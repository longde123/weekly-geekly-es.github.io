<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏼‍🏭 🌨️ 〰️ Apache Ignite + Apache Spark Data Frames: ensemble plus de plaisir 🌎 🐾 🤱🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr! Je m'appelle Nikolai Izhikov, je travaille pour Sberbank Technologies dans l'équipe de développement de solutions Open Source. Derrière...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Apache Ignite + Apache Spark Data Frames: ensemble plus de plaisir</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/sberbank/blog/427297/">  Bonjour, Habr!  Je m'appelle Nikolai Izhikov, je travaille pour Sberbank Technologies dans l'équipe de développement de solutions Open Source.  Derrière 15 ans de développement commercial en Java.  Je suis un contributeur Apache Ignite et un contributeur Apache Kafka. <br><br>  Sous le chat, vous trouverez une version vidéo et texte de mon rapport sur Apache Ignite Meetup sur la façon d'utiliser Apache Ignite avec Apache Spark et les fonctionnalités que nous avons mises en œuvre pour cela. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f42/5f3/df5/f425f3df59ff99d03d4a3e6aff3b2655.png"><br><a name="habracut"></a><br><h2>  Ce qu'Apache Spark peut faire </h2><br>  Qu'est-ce que Apache Spark?  Il s'agit d'un produit qui vous permet d'effectuer rapidement des requêtes de calcul et d'analyse distribuées.  Fondamentalement, Apache Spark est écrit en Scala. <br><br>  Apache Spark possède une API riche pour se connecter à différents systèmes de stockage ou recevoir des données.  L'une des caractéristiques du produit est un moteur de requête universel de type SQL pour les données reçues de diverses sources.  Si vous avez plusieurs sources d'informations, que vous souhaitez les combiner et obtenir des résultats, Apache Spark est ce dont vous avez besoin. <br><br>  L'une des abstractions clés que Spark fournit est Data Frame, DataSet.  En termes de base de données relationnelle, il s'agit d'une table, une source qui fournit des données de manière structurée.  La structure, le type de chaque colonne, son nom, etc., est connu.  Les trames de données peuvent être créées à partir de diverses sources.  Les exemples incluent les fichiers json, les bases de données relationnelles, divers systèmes hadoop et Apache Ignite. <br><br>  Spark prend en charge les jointures dans les requêtes SQL.  Vous pouvez combiner des données provenant de diverses sources et obtenir des résultats, effectuer des requêtes analytiques.  De plus, il existe une API pour enregistrer les données.  Lorsque vous avez terminé les requêtes, effectué une étude, Spark offre la possibilité d'enregistrer les résultats sur le récepteur qui prend en charge cette fonctionnalité et, en conséquence, de résoudre le problème du traitement des données. <br><br><h2>  Quelles fonctionnalités avons-nous mises en œuvre pour intégrer Apache Spark à Apache Ignite </h2><br><ol><li>  Lecture des données des tables SQL Apache Ignite. </li><li>  Écriture de données dans des tables SQL Apache Ignite. </li><li>  IgniteCatalog dans IgniteSparkSession - la possibilité d'utiliser toutes les tables Ignite SQL existantes sans s'enregistrer «à la main». </li><li>  Optimisation SQL - la possibilité d'exécuter des instructions SQL dans Ignite. </li></ol><br>  Apache Spark peut lire les données des tables SQL Apache Ignite et les écrire sous la forme d'une telle table.  Tout DataFrame formé dans Spark peut être enregistré en tant que table Apache Ignite SQL. <br><br>  Apache Ignite vous permet d'utiliser toutes les tables SQL Ignite existantes dans Spark Session sans vous inscrire «à la main» - en utilisant IgniteCatalog dans l'extension SparkSession standard - IgniteSparkSession. <br><br>  Ici, vous devez aller un peu plus loin dans l'appareil Spark.  En termes de base de données régulière, un répertoire est un endroit où les méta-informations sont stockées: quelles tables sont disponibles, quelles colonnes s'y trouvent, etc.  Lorsqu'une demande arrive, les méta-informations sont extraites du catalogue et le moteur SQL fait quelque chose avec les tables et les données.  Par défaut, dans Spark, toutes les tables de lecture (peu importe, à partir d'une base de données relationnelle, Ignite, Hadoop) doivent être enregistrées manuellement dans la session.  Par conséquent, vous avez la possibilité d'effectuer une requête SQL sur ces tables.  Spark les découvre. <br><br>  Pour travailler avec les données que nous avons téléchargées sur Ignite, nous devons enregistrer les tables.  Mais au lieu d'enregistrer chaque table avec nos mains, nous avons implémenté la possibilité d'accéder automatiquement à toutes les tables Ignite. <br><br>  Quelle est la fonctionnalité ici?  Pour une raison inconnue, le répertoire dans Spark est une API interne, c'est-à-dire  un étranger ne peut pas venir créer sa propre implémentation de catalogue.  Et, depuis que Spark est sorti de Hadoop, il ne prend en charge que Hive.  Et vous devez enregistrer tout le reste avec vos mains.  Les utilisateurs demandent souvent comment contourner ce problème et effectuer immédiatement des requêtes SQL.  J'ai implémenté un répertoire qui vous permet de parcourir et d'accéder aux tables Ignite sans enregistrer ~ et sms ~, et j'ai initialement proposé ce patch dans la communauté Spark, auquel j'ai reçu une réponse: un tel patch n'est pas intéressant pour certaines raisons internes.  Et ils n'ont pas donné l'API interne. <br><br>  Désormais, le catalogue Ignite est une fonctionnalité intéressante implémentée à l'aide de l'API interne de Spark.  Pour utiliser ce répertoire, nous avons notre propre implémentation de la session, c'est la SparkSession habituelle, dans laquelle vous pouvez faire des requêtes, traiter des données.  Les différences sont que nous y avons intégré ExternalCatalog pour travailler avec les tables Ignite, ainsi que IgniteOptimization, qui sera décrit ci-dessous. <br><br>  <b>Optimisation SQL</b> - la possibilité d'exécuter des instructions SQL dans Ignite.  Par défaut, lors de l'exécution d'une jointure, d'un regroupement, d'un calcul d'agrégation et d'autres requêtes SQL complexes, Spark lit les données ligne par ligne.  La seule chose que la source de données peut faire est de filtrer efficacement les lignes. <br><br>  Si vous utilisez la jointure ou le regroupement, Spark extrait toutes les données de la table dans sa mémoire vers le programme de travail, à l'aide des filtres spécifiés, puis les regroupe ou effectue d'autres opérations SQL.  Dans le cas d'Ignite, ce n'est pas optimal, car Ignite lui-même a une architecture distribuée et a connaissance des données qui y sont stockées.  Par conséquent, Ignite lui-même peut calculer efficacement les agrégats et effectuer le regroupement.  De plus, il peut y avoir beaucoup de données, et pour les regrouper, vous devrez tout soustraire, augmenter toutes les données dans Spark, ce qui est assez cher. <br><br>  Spark fournit une API avec laquelle vous pouvez modifier le plan initial de la requête SQL, effectuer une optimisation et transférer la partie de la requête SQL qui peut y être exécutée dans Ignite.  Cela sera efficace en termes de vitesse et de consommation de mémoire, car nous ne l'utiliserons pas pour extraire des données qui seront immédiatement regroupées. <br><br><h2>  Comment ça marche </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/b28/1df/0ef/b281df0ef5f2ea2a08f73267ef7f5edb.png"><br><br>  Nous avons un cluster Ignite - c'est la moitié inférieure de l'image.  Il n'y a pas de gardien de zoo, car il n'y a que cinq nœuds.  Il y a des ouvriers spark, à l'intérieur de chaque ouvrier le nœud client Ignite est levé.  Grâce à lui, nous pouvons faire une demande et lire les données, interagir avec le cluster.  En outre, le nœud client monte dans IgniteSparkSession pour que le répertoire fonctionne. <br><br><h2>  Allumer la trame de données </h2><br>  Passons au code: comment lire les données d'une table SQL?  Dans le cas de Spark, tout est assez simple et bon: nous disons que nous voulons calculer certaines données, indiquer le format - c'est une certaine constante.  De plus, nous avons plusieurs options - le chemin d'accès au fichier de configuration pour le nœud client, qui démarre lors de la lecture des données.  Nous indiquons la table que nous voulons lire et demandons à Spark de charger.  Nous obtenons les données et nous pouvons en faire ce que nous voulons. <br><br><pre><code class="scala hljs">spark.read .format(<span class="hljs-type"><span class="hljs-type">FORMAT_IGNITE</span></span>) .option(<span class="hljs-type"><span class="hljs-type">OPTION_CONFIG_FILE</span></span>, <span class="hljs-type"><span class="hljs-type">TEST_CONFIG_FILE</span></span>) .option(<span class="hljs-type"><span class="hljs-type">OPTION_TABLE</span></span>, <span class="hljs-string"><span class="hljs-string">"person"</span></span>) .load()</code> </pre> <br>  Après avoir généré les données - éventuellement depuis Ignite, à partir de n'importe quelle source - nous pouvons tout aussi facilement tout sauvegarder en spécifiant le format et le tableau correspondant.  Nous demandons à Spark d'écrire, nous spécifions un format.  Dans la configuration, nous prescrivons à quel cluster se connecter.  Spécifiez la table dans laquelle nous voulons enregistrer.  De plus, nous pouvons prescrire des options d'utilitaire - spécifiez la clé primaire que nous créons sur ce tableau.  Si les données bouleversent simplement sans créer de table, ce paramètre n'est pas nécessaire.  À la fin, cliquez sur Enregistrer et les données sont écrites. <br><br><pre> <code class="scala hljs">tbl.write. format(<span class="hljs-type"><span class="hljs-type">FORMAT_IGNITE</span></span>). option(<span class="hljs-type"><span class="hljs-type">OPTION_CONFIG_FILE</span></span>, <span class="hljs-type"><span class="hljs-type">CFG_PATH</span></span>). option(<span class="hljs-type"><span class="hljs-type">OPTION_TABLE</span></span>, tableName). option(<span class="hljs-type"><span class="hljs-type">OPTION_CREATE_TABLE_PRIMARY_KEY_FIELDS</span></span>, pk). save</code> </pre><br>  Voyons maintenant comment tout cela fonctionne. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b35/41a/b86/b3541ab86eca15cd240765bf15907979.png"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">LoadDataExample.scala</a> <br><br>  Cette application évidente démontrera d'abord les capacités d'enregistrement.  Par exemple, j'ai choisi les données sur les matchs de football, les statistiques téléchargées à partir d'une ressource bien connue.  Il contient des informations sur les tournois: ligues, matches, joueurs, équipes, attributs des joueurs, attributs des équipes - données qui décrivent les matchs de football dans les ligues des pays européens (Angleterre, France, Espagne, etc.). <br><br>  Je veux les télécharger sur Ignite.  Nous créons une session Spark, spécifions l'adresse de l'assistant et appelons le chargement de ces tables en passant des paramètres.  L'exemple est en Scala, pas en Java, car Scala est moins verbeux et donc meilleur par exemple. <br><br>  Nous transférons le nom du fichier, le lisons, indiquons qu'il est multiligne, il s'agit d'un fichier json standard.  Ensuite, nous écrivons dans Ignite.  La structure de notre fichier n'est nulle part à décrire - Spark lui-même détermine quelles données nous avons et quelle est leur structure.  Si tout se passe bien, une table est créée dans laquelle se trouvent tous les champs nécessaires des types de données requis.  C'est ainsi que nous pouvons tout charger dans Ignite. <br><br>  Lorsque les données sont chargées, nous pouvons les voir dans Ignite et les utiliser immédiatement.  À titre d'exemple simple, une requête qui vous permet de savoir quelle équipe a joué le plus de matchs.  Nous avons deux colonnes: hometeam et awayteam, hôtes et invités.  Nous sélectionnons, groupons, comptons, additionnons et joignons les données de la commande - pour entrer le nom de la commande.  Ta-dam - et les données de json-chiks que nous avons obtenues dans Ignite.  Nous voyons Paris Saint-Germain, Toulouse - nous avons beaucoup de données sur les équipes françaises. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8a4/202/52b/8a420252be6fb8df3a9083d7411911a9.png"><br><br>  Nous résumons.  Nous avons maintenant téléchargé des données depuis la source, le fichier json, vers Ignite, et assez rapidement.  Peut-être, du point de vue des mégadonnées, ce n'est pas trop grand, mais décent pour un ordinateur local.  Le schéma de la table est extrait du fichier json dans sa forme d'origine.  La table a été créée, les noms des colonnes ont été copiés à partir du fichier source, la clé primaire a été créée.  L'ID est partout et la clé primaire est l'ID.  Ces données sont entrées dans Ignite, nous pouvons les utiliser. <br><br><h2>  IgniteSparkSession et IgniteCatalog </h2><br>  Voyons comment cela fonctionne. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/654/24a/4ee/65424a4eeda4a4c2c6cce7038e13d1a9.png"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">CatalogExample.scala</a> <br><br>  D'une manière assez simple, vous pouvez accéder à toutes vos données et les interroger.  Dans le dernier exemple, nous avons démarré la session spark standard.  Et il n'y avait aucune spécificité Ignite là-bas - sauf que vous devez mettre un pot avec la bonne source de données - un travail complètement standard via l'API publique.  Mais, si vous souhaitez accéder automatiquement aux tables Ignite, vous pouvez utiliser notre extension.  La différence est qu'au lieu de SparkSession, nous écrivons IgniteSparkSession. <br><br>  Dès que vous créez un objet IgniteSparkSession, vous voyez dans le répertoire toutes les tables qui viennent d'être chargées dans Ignite.  Vous pouvez voir leur diagramme et toutes les informations.  Spark connaît déjà les tables qu'Ignite possède et vous pouvez facilement obtenir toutes les données. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/dec/f1b/a0c/decf1ba0c5db2e0d84e50a0e88b6c192.png"><br><br><h2>  Igniteoptimization </h2><br>  Lorsque vous effectuez des requêtes complexes dans Ignite à l'aide de JOIN, Spark extrait d'abord les données, puis seulement JOIN les regroupe.  Pour optimiser le processus, nous avons créé la fonction IgniteOptimization - elle optimise le plan de requête Spark et vous permet de transmettre les parties de la demande qui peuvent être exécutées dans Ignite dans Ignite.  Nous montrons l'optimisation sur une demande spécifique. <br><br><pre> <code class="sql hljs">SQL Query: <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span>   city_id,   <span class="hljs-keyword"><span class="hljs-keyword">count</span></span>(*) <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span>   person p <span class="hljs-keyword"><span class="hljs-keyword">GROUP</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> city_id <span class="hljs-keyword"><span class="hljs-keyword">HAVING</span></span> <span class="hljs-keyword"><span class="hljs-keyword">count</span></span>(*) &gt; <span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br>  Nous satisfaisons la demande.  Nous avons une table de personnes - certains employés, des gens.  Chaque employé connaît l'identifiant de la ville dans laquelle il vit.  Nous voulons savoir combien de personnes vivent dans chaque ville.  Nous filtrons - dans quelle ville vit plus d'une personne.  Voici le plan initial que Spark construit: <br><br><pre> <code class="scala hljs">== <span class="hljs-type"><span class="hljs-type">Analyzed</span></span> <span class="hljs-type"><span class="hljs-type">Logical</span></span> <span class="hljs-type"><span class="hljs-type">Plan</span></span> == city_id: bigint, count(<span class="hljs-number"><span class="hljs-number">1</span></span>): bigint <span class="hljs-type"><span class="hljs-type">Project</span></span> [city_id#<span class="hljs-number"><span class="hljs-number">19</span></span>L, count(<span class="hljs-number"><span class="hljs-number">1</span></span>)#<span class="hljs-number"><span class="hljs-number">52</span></span>L] +- <span class="hljs-type"><span class="hljs-type">Filter</span></span> (count(<span class="hljs-number"><span class="hljs-number">1</span></span>)#<span class="hljs-number"><span class="hljs-number">54</span></span>L &gt; cast(<span class="hljs-number"><span class="hljs-number">1</span></span> as bigint))  +- <span class="hljs-type"><span class="hljs-type">Aggregate</span></span> [city_id#<span class="hljs-number"><span class="hljs-number">19</span></span>L], [city_id#<span class="hljs-number"><span class="hljs-number">19</span></span>L, count(<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-type"><span class="hljs-type">AS</span></span> count(<span class="hljs-number"><span class="hljs-number">1</span></span>)#<span class="hljs-number"><span class="hljs-number">52</span></span>L, count(<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-type"><span class="hljs-type">AS</span></span> count(<span class="hljs-number"><span class="hljs-number">1</span></span>)#<span class="hljs-number"><span class="hljs-number">54</span></span>L] +- <span class="hljs-type"><span class="hljs-type">SubqueryAlias</span></span> p    +- <span class="hljs-type"><span class="hljs-type">SubqueryAlias</span></span> person       +- <span class="hljs-type"><span class="hljs-type">Relation</span></span>[<span class="hljs-type"><span class="hljs-type">NAME</span></span>#<span class="hljs-number"><span class="hljs-number">11</span></span>,<span class="hljs-type"><span class="hljs-type">BIRTH_DATE</span></span>#<span class="hljs-number"><span class="hljs-number">12</span></span>,<span class="hljs-type"><span class="hljs-type">IS_RESIDENT</span></span>#<span class="hljs-number"><span class="hljs-number">13</span></span>,<span class="hljs-type"><span class="hljs-type">SALARY</span></span>#<span class="hljs-number"><span class="hljs-number">14</span></span>,<span class="hljs-type"><span class="hljs-type">PENSION</span></span>#<span class="hljs-number"><span class="hljs-number">15</span></span>,<span class="hljs-type"><span class="hljs-type">ACCOUNT</span></span>#<span class="hljs-number"><span class="hljs-number">16</span></span>,<span class="hljs-type"><span class="hljs-type">AGE</span></span>#<span class="hljs-number"><span class="hljs-number">17</span></span>,<span class="hljs-type"><span class="hljs-type">ID</span></span>#<span class="hljs-number"><span class="hljs-number">18</span></span>L,<span class="hljs-type"><span class="hljs-type">CITY_ID</span></span>#<span class="hljs-number"><span class="hljs-number">19</span></span>L]         <span class="hljs-type"><span class="hljs-type">IgniteSQLRelation</span></span>[table=<span class="hljs-type"><span class="hljs-type">PERSON</span></span>]</code> </pre><br>  La relation n'est qu'une table Ignite.  Il n'y a pas de filtres - nous pompons simplement toutes les données de la table Person sur le réseau à partir du cluster.  Ensuite, Spark agrège tout cela - conformément à la demande et renvoie le résultat de la demande. <br><br>  Il est facile de voir que tous ces sous-arbres avec filtre et agrégation peuvent être exécutés dans Ignite.  Cela sera beaucoup plus efficace que d'extraire toutes les données d'une table potentiellement grande dans Spark - c'est ce que fait notre fonction IgniteOptimization.  Après avoir analysé et optimisé l'arbre, nous obtenons le plan suivant: <br><br><pre> <code class="scala hljs">== <span class="hljs-type"><span class="hljs-type">Optimized</span></span> <span class="hljs-type"><span class="hljs-type">Logical</span></span> <span class="hljs-type"><span class="hljs-type">Plan</span></span> == <span class="hljs-type"><span class="hljs-type">Relation</span></span>[<span class="hljs-type"><span class="hljs-type">CITY_ID</span></span>#<span class="hljs-number"><span class="hljs-number">19</span></span>L,<span class="hljs-type"><span class="hljs-type">COUNT</span></span>(<span class="hljs-number"><span class="hljs-number">1</span></span>)#<span class="hljs-number"><span class="hljs-number">52</span></span>L]   <span class="hljs-type"><span class="hljs-type">IgniteSQLAccumulatorRelation</span></span>(     columns=[<span class="hljs-type"><span class="hljs-type">CITY_ID</span></span>, <span class="hljs-type"><span class="hljs-type">COUNT</span></span>(<span class="hljs-number"><span class="hljs-number">1</span></span>)], qry=<span class="hljs-type"><span class="hljs-type">SELECT</span></span> <span class="hljs-type"><span class="hljs-type">CITY_ID</span></span>, <span class="hljs-type"><span class="hljs-type">COUNT</span></span>(<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-type"><span class="hljs-type">FROM</span></span> <span class="hljs-type"><span class="hljs-type">PERSON</span></span> <span class="hljs-type"><span class="hljs-type">GROUP</span></span> <span class="hljs-type"><span class="hljs-type">BY</span></span> city_id <span class="hljs-type"><span class="hljs-type">HAVING</span></span> count(<span class="hljs-number"><span class="hljs-number">1</span></span>) &gt; <span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br>  En conséquence, nous n'obtenons qu'une seule relation, car nous avons optimisé tout l'arbre.  Et à l'intérieur, vous pouvez déjà voir qu'Ignite enverra une demande suffisamment proche de la demande d'origine. <br><br>  Supposons que nous joignions différentes sources de données: par exemple, nous avons un DataFrame d'Ignite, le second de json, le troisième d'Ignite à nouveau et le quatrième d'une sorte de base de données relationnelle.  Dans ce cas, seul le sous-arbre sera optimisé dans le plan.  Nous optimisons ce que nous pouvons, le déposons dans Ignite et Spark fera le reste.  Pour cette raison, nous obtenons un gain de vitesse. <br><br>  Un autre exemple avec JOIN: <br><br><pre> <code class="sql hljs">SQL Query - <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> jt1.id <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> id1, jt1.val1, jt2.id <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> id2, jt2.val2 <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> jt1 <span class="hljs-keyword"><span class="hljs-keyword">JOIN</span></span> jt2 <span class="hljs-keyword"><span class="hljs-keyword">ON</span></span> jt1.val1 = jt2.val2</code> </pre><br>  Nous avons deux tableaux.  Nous restons unis par valeur et sélectionnons parmi eux tous - ID, valeurs.  Spark propose un tel plan: <br><br><pre> <code class="scala hljs">== <span class="hljs-type"><span class="hljs-type">Analyzed</span></span> <span class="hljs-type"><span class="hljs-type">Logical</span></span> <span class="hljs-type"><span class="hljs-type">Plan</span></span> == id1: bigint, val1: string, id2: bigint, val2: string <span class="hljs-type"><span class="hljs-type">Project</span></span> [id#<span class="hljs-number"><span class="hljs-number">4</span></span>L <span class="hljs-type"><span class="hljs-type">AS</span></span> id1#<span class="hljs-number"><span class="hljs-number">84</span></span>L, val1#<span class="hljs-number"><span class="hljs-number">3</span></span>, id#<span class="hljs-number"><span class="hljs-number">6</span></span>L <span class="hljs-type"><span class="hljs-type">AS</span></span> id2#<span class="hljs-number"><span class="hljs-number">85</span></span>L, val2#<span class="hljs-number"><span class="hljs-number">5</span></span>] +- <span class="hljs-type"><span class="hljs-type">Join</span></span> <span class="hljs-type"><span class="hljs-type">Inner</span></span>, (val1#<span class="hljs-number"><span class="hljs-number">3</span></span> = val2#<span class="hljs-number"><span class="hljs-number">5</span></span>) :- <span class="hljs-type"><span class="hljs-type">SubqueryAlias</span></span> jt1 : +- <span class="hljs-type"><span class="hljs-type">Relation</span></span>[<span class="hljs-type"><span class="hljs-type">VAL1</span></span>#<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-type"><span class="hljs-type">ID</span></span>#<span class="hljs-number"><span class="hljs-number">4</span></span>L] <span class="hljs-type"><span class="hljs-type">IgniteSQLRelation</span></span>[table=<span class="hljs-type"><span class="hljs-type">JT1</span></span>] +- <span class="hljs-type"><span class="hljs-type">SubqueryAlias</span></span> jt2    +- <span class="hljs-type"><span class="hljs-type">Relation</span></span>[<span class="hljs-type"><span class="hljs-type">VAL2</span></span>#<span class="hljs-number"><span class="hljs-number">5</span></span>,<span class="hljs-type"><span class="hljs-type">ID</span></span>#<span class="hljs-number"><span class="hljs-number">6</span></span>L] <span class="hljs-type"><span class="hljs-type">IgniteSQLRelation</span></span>[table=<span class="hljs-type"><span class="hljs-type">JT2</span></span>]</code> </pre> <br>  Nous voyons qu'il va extraire toutes les données d'une table, toutes les données de la seconde, les joindre en lui et donner les résultats.  Après le traitement et l'optimisation, nous obtenons exactement la même demande qui va à Ignite, où elle est exécutée relativement rapidement. <br><br><pre> <code class="scala hljs">== <span class="hljs-type"><span class="hljs-type">Optimized</span></span> <span class="hljs-type"><span class="hljs-type">Logical</span></span> <span class="hljs-type"><span class="hljs-type">Plan</span></span> == <span class="hljs-type"><span class="hljs-type">Relation</span></span>[<span class="hljs-type"><span class="hljs-type">ID</span></span>#<span class="hljs-number"><span class="hljs-number">84</span></span>L,<span class="hljs-type"><span class="hljs-type">VAL1</span></span>#<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-type"><span class="hljs-type">ID</span></span>#<span class="hljs-number"><span class="hljs-number">85</span></span>L,<span class="hljs-type"><span class="hljs-type">VAL2</span></span>#<span class="hljs-number"><span class="hljs-number">5</span></span>] <span class="hljs-type"><span class="hljs-type">IgniteSQLAccumulatorRelation</span></span>(columns=[<span class="hljs-type"><span class="hljs-type">ID</span></span>, <span class="hljs-type"><span class="hljs-type">VAL1</span></span>, <span class="hljs-type"><span class="hljs-type">ID</span></span>, <span class="hljs-type"><span class="hljs-type">VAL2</span></span>], qry= <span class="hljs-type"><span class="hljs-type">SELECT</span></span> <span class="hljs-type"><span class="hljs-type">JT1</span></span>.<span class="hljs-type"><span class="hljs-type">ID</span></span> <span class="hljs-type"><span class="hljs-type">AS</span></span> id1, <span class="hljs-type"><span class="hljs-type">JT1</span></span>.<span class="hljs-type"><span class="hljs-type">VAL1</span></span>, <span class="hljs-type"><span class="hljs-type">JT2</span></span>.<span class="hljs-type"><span class="hljs-type">ID</span></span> <span class="hljs-type"><span class="hljs-type">AS</span></span> id2, <span class="hljs-type"><span class="hljs-type">JT2</span></span>.<span class="hljs-type"><span class="hljs-type">VAL2</span></span> <span class="hljs-type"><span class="hljs-type">FROM</span></span> <span class="hljs-type"><span class="hljs-type">JT1</span></span> <span class="hljs-type"><span class="hljs-type">JOIN</span></span> <span class="hljs-type"><span class="hljs-type">JT2</span></span> <span class="hljs-type"><span class="hljs-type">ON</span></span> <span class="hljs-type"><span class="hljs-type">JT1</span></span>.val1 = <span class="hljs-type"><span class="hljs-type">JT2</span></span>.val2 <span class="hljs-type"><span class="hljs-type">WHERE</span></span> <span class="hljs-type"><span class="hljs-type">JT1</span></span>.val1 <span class="hljs-type"><span class="hljs-type">IS</span></span> <span class="hljs-type"><span class="hljs-type">NOT</span></span> <span class="hljs-type"><span class="hljs-type">NULL</span></span> <span class="hljs-type"><span class="hljs-type">AND</span></span> <span class="hljs-type"><span class="hljs-type">JT2</span></span>.val2 <span class="hljs-type"><span class="hljs-type">IS</span></span> <span class="hljs-type"><span class="hljs-type">NOT</span></span> <span class="hljs-type"><span class="hljs-type">NULL</span></span>)</code> </pre> <br>  Je vais vous montrer un exemple. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ba4/39a/493/ba439a493e76dd573966cad413c07650.png"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OptimizationExample.scala</a> <br><br>  Nous créons une session IgniteSpark dans laquelle toutes nos capacités d'optimisation sont déjà automatiquement incluses.  Voici la demande: trouvez les joueurs avec la note la plus élevée et affichez leurs noms.  Dans la table des joueurs, leurs attributs et données.  Nous nous joignons, filtrons les données indésirables et affichons les joueurs avec la note la plus élevée.  Voyons quel type de plan nous avons obtenu après l'optimisation et montrons les résultats de cette requête. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c7d/c51/9ab/c7dc519abdfa6b3b1d7a8396ef9725b3.png"><br><br>  Nous commençons.  Nous voyons des noms de famille familiers: Messi, Buffon, Ronaldo, etc.  Soit dit en passant, certains pour une raison quelconque se rencontrent sous deux formes - Messi et Ronaldo.  Les amateurs de football peuvent trouver étrange que des joueurs inconnus apparaissent sur la liste.  Ce sont des gardiens de but, des joueurs avec des caractéristiques assez élevées - dans le contexte des autres joueurs.  Maintenant, nous regardons le plan de requête qui a été exécuté.  Dans Spark, presque rien n'a été fait, c'est-à-dire que nous avons de nouveau envoyé la demande entière à Ignite. <br><br><h2>  Apache Ignite Development </h2><br>  Notre projet est un produit open source, nous sommes donc toujours satisfaits des correctifs et des commentaires des développeurs.  Votre aide, vos commentaires, vos correctifs sont les bienvenus.  Nous les attendons.  90% de la communauté Ignite est russophone.  Par exemple, pour moi, jusqu'à ce que je commence à travailler sur Apache Ignite, la meilleure connaissance de l'anglais n'était pas dissuasive.  Cela ne vaut guère la peine d'écrire en russe sur une liste de développeurs, mais même si vous écrivez quelque chose de mal, ils vous répondront et vous aideront. <br><br>  Que peut-on améliorer sur cette intégration?  Comment puis-je vous aider si vous avez un tel désir?  Liste ci-dessous.  Les astérisques indiquent la complexité. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/de4/d43/ed0/de4d43ed01894ce6b02865ad9f6aef5d.png"><br>  Pour tester l'optimisation, vous devez écrire des tests avec des requêtes complexes.  Ci-dessus, j'ai montré quelques requêtes évidentes.  Il est clair que si vous écrivez beaucoup de groupements et beaucoup de jointures, alors quelque chose peut tomber.  C'est une tâche très simple - venez le faire.  Si nous trouvons des bogues basés sur les résultats des tests, ils devront être corrigés.  Ce sera plus difficile là-bas. <br><br>  Une autre tâche claire et intéressante est l'intégration de Spark avec un client léger.  Il est initialement capable de spécifier certains ensembles d'adresses IP, et cela suffit pour rejoindre le cluster Ignite, ce qui est pratique en cas d'intégration avec un système externe.  Si vous souhaitez soudainement rejoindre la solution à ce problème, je vais personnellement vous aider. <br><br>  Si vous souhaitez rejoindre la communauté Apache Ignite, voici quelques liens utiles: <br><br><ul><li>  <i>Commencez ici - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=http://yu">https://ignite.apache.org/community/resources.html</a></i> <br></li><li>  <i>Sources ici - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://github.com/apache/ignite/</a></i> <br></li><li>  <i>Docks ici - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://apacheignite.readme.io/docs</a></i> <br></li><li>  <i>Bugs ici - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://issues.apache.org/jira/browse/IGNITE</a></i> <br></li><li>  <i>Vous pouvez écrire ici - dev@ignite.apache.org, user@ignite.apache.org</i> <br></li></ul><br>  Nous avons une liste de développeurs réactifs, qui vous aidera.  C'est encore loin d'être idéal, mais en comparaison avec d'autres projets, il est vraiment vivant. <br><br>  <i>Si vous connaissez Java ou C ++, vous cherchez du travail et souhaitez développer l'Open Source (Apache Ignite, Apache Kafka, Tarantool, etc.) écrivez ici: join-open-source@sberbank.ru.</i> <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/CzbAweNKEVY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr427297/">https://habr.com/ru/post/fr427297/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr427285/index.html">École sur les bases des circuits numériques: Novossibirsk - Ok, Krasnoyarsk - préparez-vous</a></li>
<li><a href="../fr427289/index.html">Modélisation géologique 3D, diagraphie et technologie d'Aramco Innovations</a></li>
<li><a href="../fr427291/index.html">Réduisez le trafic dans les formulaires Web ASP.NET, les div cliquables et les interrogations périodiques du serveur</a></li>
<li><a href="../fr427293/index.html">Modèles de conception JavaScript</a></li>
<li><a href="../fr427295/index.html">Fonctions de currying JavaScript</a></li>
<li><a href="../fr427299/index.html">Allons chercher autre chose à collectionner? Constructeur 3 en 1 "Flotte Lunaire"</a></li>
<li><a href="../fr427301/index.html">Base de données crash de GitHub</a></li>
<li><a href="../fr427303/index.html">Ralentissement de Windows, partie 2: création de processus</a></li>
<li><a href="../fr427307/index.html">Pratique de test du backend Java + Rest-Assured</a></li>
<li><a href="../fr427309/index.html">Comment PVS-Studio s'est avéré plus attentif que trois programmeurs et demi</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>