<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèº‚Äçüè≠ üå®Ô∏è „Ä∞Ô∏è Apache Ignite + Apache Spark Data Frames: ensemble plus de plaisir üåé üêæ ü§±üèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr! Je m'appelle Nikolai Izhikov, je travaille pour Sberbank Technologies dans l'√©quipe de d√©veloppement de solutions Open Source. Derri√®re...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Apache Ignite + Apache Spark Data Frames: ensemble plus de plaisir</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/sberbank/blog/427297/">  Bonjour, Habr!  Je m'appelle Nikolai Izhikov, je travaille pour Sberbank Technologies dans l'√©quipe de d√©veloppement de solutions Open Source.  Derri√®re 15 ans de d√©veloppement commercial en Java.  Je suis un contributeur Apache Ignite et un contributeur Apache Kafka. <br><br>  Sous le chat, vous trouverez une version vid√©o et texte de mon rapport sur Apache Ignite Meetup sur la fa√ßon d'utiliser Apache Ignite avec Apache Spark et les fonctionnalit√©s que nous avons mises en ≈ìuvre pour cela. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f42/5f3/df5/f425f3df59ff99d03d4a3e6aff3b2655.png"><br><a name="habracut"></a><br><h2>  Ce qu'Apache Spark peut faire </h2><br>  Qu'est-ce que Apache Spark?  Il s'agit d'un produit qui vous permet d'effectuer rapidement des requ√™tes de calcul et d'analyse distribu√©es.  Fondamentalement, Apache Spark est √©crit en Scala. <br><br>  Apache Spark poss√®de une API riche pour se connecter √† diff√©rents syst√®mes de stockage ou recevoir des donn√©es.  L'une des caract√©ristiques du produit est un moteur de requ√™te universel de type SQL pour les donn√©es re√ßues de diverses sources.  Si vous avez plusieurs sources d'informations, que vous souhaitez les combiner et obtenir des r√©sultats, Apache Spark est ce dont vous avez besoin. <br><br>  L'une des abstractions cl√©s que Spark fournit est Data Frame, DataSet.  En termes de base de donn√©es relationnelle, il s'agit d'une table, une source qui fournit des donn√©es de mani√®re structur√©e.  La structure, le type de chaque colonne, son nom, etc., est connu.  Les trames de donn√©es peuvent √™tre cr√©√©es √† partir de diverses sources.  Les exemples incluent les fichiers json, les bases de donn√©es relationnelles, divers syst√®mes hadoop et Apache Ignite. <br><br>  Spark prend en charge les jointures dans les requ√™tes SQL.  Vous pouvez combiner des donn√©es provenant de diverses sources et obtenir des r√©sultats, effectuer des requ√™tes analytiques.  De plus, il existe une API pour enregistrer les donn√©es.  Lorsque vous avez termin√© les requ√™tes, effectu√© une √©tude, Spark offre la possibilit√© d'enregistrer les r√©sultats sur le r√©cepteur qui prend en charge cette fonctionnalit√© et, en cons√©quence, de r√©soudre le probl√®me du traitement des donn√©es. <br><br><h2>  Quelles fonctionnalit√©s avons-nous mises en ≈ìuvre pour int√©grer Apache Spark √† Apache Ignite </h2><br><ol><li>  Lecture des donn√©es des tables SQL Apache Ignite. </li><li>  √âcriture de donn√©es dans des tables SQL Apache Ignite. </li><li>  IgniteCatalog dans IgniteSparkSession - la possibilit√© d'utiliser toutes les tables Ignite SQL existantes sans s'enregistrer ¬´√† la main¬ª. </li><li>  Optimisation SQL - la possibilit√© d'ex√©cuter des instructions SQL dans Ignite. </li></ol><br>  Apache Spark peut lire les donn√©es des tables SQL Apache Ignite et les √©crire sous la forme d'une telle table.  Tout DataFrame form√© dans Spark peut √™tre enregistr√© en tant que table Apache Ignite SQL. <br><br>  Apache Ignite vous permet d'utiliser toutes les tables SQL Ignite existantes dans Spark Session sans vous inscrire ¬´√† la main¬ª - en utilisant IgniteCatalog dans l'extension SparkSession standard - IgniteSparkSession. <br><br>  Ici, vous devez aller un peu plus loin dans l'appareil Spark.  En termes de base de donn√©es r√©guli√®re, un r√©pertoire est un endroit o√π les m√©ta-informations sont stock√©es: quelles tables sont disponibles, quelles colonnes s'y trouvent, etc.  Lorsqu'une demande arrive, les m√©ta-informations sont extraites du catalogue et le moteur SQL fait quelque chose avec les tables et les donn√©es.  Par d√©faut, dans Spark, toutes les tables de lecture (peu importe, √† partir d'une base de donn√©es relationnelle, Ignite, Hadoop) doivent √™tre enregistr√©es manuellement dans la session.  Par cons√©quent, vous avez la possibilit√© d'effectuer une requ√™te SQL sur ces tables.  Spark les d√©couvre. <br><br>  Pour travailler avec les donn√©es que nous avons t√©l√©charg√©es sur Ignite, nous devons enregistrer les tables.  Mais au lieu d'enregistrer chaque table avec nos mains, nous avons impl√©ment√© la possibilit√© d'acc√©der automatiquement √† toutes les tables Ignite. <br><br>  Quelle est la fonctionnalit√© ici?  Pour une raison inconnue, le r√©pertoire dans Spark est une API interne, c'est-√†-dire  un √©tranger ne peut pas venir cr√©er sa propre impl√©mentation de catalogue.  Et, depuis que Spark est sorti de Hadoop, il ne prend en charge que Hive.  Et vous devez enregistrer tout le reste avec vos mains.  Les utilisateurs demandent souvent comment contourner ce probl√®me et effectuer imm√©diatement des requ√™tes SQL.  J'ai impl√©ment√© un r√©pertoire qui vous permet de parcourir et d'acc√©der aux tables Ignite sans enregistrer ~ et sms ~, et j'ai initialement propos√© ce patch dans la communaut√© Spark, auquel j'ai re√ßu une r√©ponse: un tel patch n'est pas int√©ressant pour certaines raisons internes.  Et ils n'ont pas donn√© l'API interne. <br><br>  D√©sormais, le catalogue Ignite est une fonctionnalit√© int√©ressante impl√©ment√©e √† l'aide de l'API interne de Spark.  Pour utiliser ce r√©pertoire, nous avons notre propre impl√©mentation de la session, c'est la SparkSession habituelle, dans laquelle vous pouvez faire des requ√™tes, traiter des donn√©es.  Les diff√©rences sont que nous y avons int√©gr√© ExternalCatalog pour travailler avec les tables Ignite, ainsi que IgniteOptimization, qui sera d√©crit ci-dessous. <br><br>  <b>Optimisation SQL</b> - la possibilit√© d'ex√©cuter des instructions SQL dans Ignite.  Par d√©faut, lors de l'ex√©cution d'une jointure, d'un regroupement, d'un calcul d'agr√©gation et d'autres requ√™tes SQL complexes, Spark lit les donn√©es ligne par ligne.  La seule chose que la source de donn√©es peut faire est de filtrer efficacement les lignes. <br><br>  Si vous utilisez la jointure ou le regroupement, Spark extrait toutes les donn√©es de la table dans sa m√©moire vers le programme de travail, √† l'aide des filtres sp√©cifi√©s, puis les regroupe ou effectue d'autres op√©rations SQL.  Dans le cas d'Ignite, ce n'est pas optimal, car Ignite lui-m√™me a une architecture distribu√©e et a connaissance des donn√©es qui y sont stock√©es.  Par cons√©quent, Ignite lui-m√™me peut calculer efficacement les agr√©gats et effectuer le regroupement.  De plus, il peut y avoir beaucoup de donn√©es, et pour les regrouper, vous devrez tout soustraire, augmenter toutes les donn√©es dans Spark, ce qui est assez cher. <br><br>  Spark fournit une API avec laquelle vous pouvez modifier le plan initial de la requ√™te SQL, effectuer une optimisation et transf√©rer la partie de la requ√™te SQL qui peut y √™tre ex√©cut√©e dans Ignite.  Cela sera efficace en termes de vitesse et de consommation de m√©moire, car nous ne l'utiliserons pas pour extraire des donn√©es qui seront imm√©diatement regroup√©es. <br><br><h2>  Comment √ßa marche </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/b28/1df/0ef/b281df0ef5f2ea2a08f73267ef7f5edb.png"><br><br>  Nous avons un cluster Ignite - c'est la moiti√© inf√©rieure de l'image.  Il n'y a pas de gardien de zoo, car il n'y a que cinq n≈ìuds.  Il y a des ouvriers spark, √† l'int√©rieur de chaque ouvrier le n≈ìud client Ignite est lev√©.  Gr√¢ce √† lui, nous pouvons faire une demande et lire les donn√©es, interagir avec le cluster.  En outre, le n≈ìud client monte dans IgniteSparkSession pour que le r√©pertoire fonctionne. <br><br><h2>  Allumer la trame de donn√©es </h2><br>  Passons au code: comment lire les donn√©es d'une table SQL?  Dans le cas de Spark, tout est assez simple et bon: nous disons que nous voulons calculer certaines donn√©es, indiquer le format - c'est une certaine constante.  De plus, nous avons plusieurs options - le chemin d'acc√®s au fichier de configuration pour le n≈ìud client, qui d√©marre lors de la lecture des donn√©es.  Nous indiquons la table que nous voulons lire et demandons √† Spark de charger.  Nous obtenons les donn√©es et nous pouvons en faire ce que nous voulons. <br><br><pre><code class="scala hljs">spark.read .format(<span class="hljs-type"><span class="hljs-type">FORMAT_IGNITE</span></span>) .option(<span class="hljs-type"><span class="hljs-type">OPTION_CONFIG_FILE</span></span>, <span class="hljs-type"><span class="hljs-type">TEST_CONFIG_FILE</span></span>) .option(<span class="hljs-type"><span class="hljs-type">OPTION_TABLE</span></span>, <span class="hljs-string"><span class="hljs-string">"person"</span></span>) .load()</code> </pre> <br>  Apr√®s avoir g√©n√©r√© les donn√©es - √©ventuellement depuis Ignite, √† partir de n'importe quelle source - nous pouvons tout aussi facilement tout sauvegarder en sp√©cifiant le format et le tableau correspondant.  Nous demandons √† Spark d'√©crire, nous sp√©cifions un format.  Dans la configuration, nous prescrivons √† quel cluster se connecter.  Sp√©cifiez la table dans laquelle nous voulons enregistrer.  De plus, nous pouvons prescrire des options d'utilitaire - sp√©cifiez la cl√© primaire que nous cr√©ons sur ce tableau.  Si les donn√©es bouleversent simplement sans cr√©er de table, ce param√®tre n'est pas n√©cessaire.  √Ä la fin, cliquez sur Enregistrer et les donn√©es sont √©crites. <br><br><pre> <code class="scala hljs">tbl.write. format(<span class="hljs-type"><span class="hljs-type">FORMAT_IGNITE</span></span>). option(<span class="hljs-type"><span class="hljs-type">OPTION_CONFIG_FILE</span></span>, <span class="hljs-type"><span class="hljs-type">CFG_PATH</span></span>). option(<span class="hljs-type"><span class="hljs-type">OPTION_TABLE</span></span>, tableName). option(<span class="hljs-type"><span class="hljs-type">OPTION_CREATE_TABLE_PRIMARY_KEY_FIELDS</span></span>, pk). save</code> </pre><br>  Voyons maintenant comment tout cela fonctionne. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b35/41a/b86/b3541ab86eca15cd240765bf15907979.png"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">LoadDataExample.scala</a> <br><br>  Cette application √©vidente d√©montrera d'abord les capacit√©s d'enregistrement.  Par exemple, j'ai choisi les donn√©es sur les matchs de football, les statistiques t√©l√©charg√©es √† partir d'une ressource bien connue.  Il contient des informations sur les tournois: ligues, matches, joueurs, √©quipes, attributs des joueurs, attributs des √©quipes - donn√©es qui d√©crivent les matchs de football dans les ligues des pays europ√©ens (Angleterre, France, Espagne, etc.). <br><br>  Je veux les t√©l√©charger sur Ignite.  Nous cr√©ons une session Spark, sp√©cifions l'adresse de l'assistant et appelons le chargement de ces tables en passant des param√®tres.  L'exemple est en Scala, pas en Java, car Scala est moins verbeux et donc meilleur par exemple. <br><br>  Nous transf√©rons le nom du fichier, le lisons, indiquons qu'il est multiligne, il s'agit d'un fichier json standard.  Ensuite, nous √©crivons dans Ignite.  La structure de notre fichier n'est nulle part √† d√©crire - Spark lui-m√™me d√©termine quelles donn√©es nous avons et quelle est leur structure.  Si tout se passe bien, une table est cr√©√©e dans laquelle se trouvent tous les champs n√©cessaires des types de donn√©es requis.  C'est ainsi que nous pouvons tout charger dans Ignite. <br><br>  Lorsque les donn√©es sont charg√©es, nous pouvons les voir dans Ignite et les utiliser imm√©diatement.  √Ä titre d'exemple simple, une requ√™te qui vous permet de savoir quelle √©quipe a jou√© le plus de matchs.  Nous avons deux colonnes: hometeam et awayteam, h√¥tes et invit√©s.  Nous s√©lectionnons, groupons, comptons, additionnons et joignons les donn√©es de la commande - pour entrer le nom de la commande.  Ta-dam - et les donn√©es de json-chiks que nous avons obtenues dans Ignite.  Nous voyons Paris Saint-Germain, Toulouse - nous avons beaucoup de donn√©es sur les √©quipes fran√ßaises. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8a4/202/52b/8a420252be6fb8df3a9083d7411911a9.png"><br><br>  Nous r√©sumons.  Nous avons maintenant t√©l√©charg√© des donn√©es depuis la source, le fichier json, vers Ignite, et assez rapidement.  Peut-√™tre, du point de vue des m√©gadonn√©es, ce n'est pas trop grand, mais d√©cent pour un ordinateur local.  Le sch√©ma de la table est extrait du fichier json dans sa forme d'origine.  La table a √©t√© cr√©√©e, les noms des colonnes ont √©t√© copi√©s √† partir du fichier source, la cl√© primaire a √©t√© cr√©√©e.  L'ID est partout et la cl√© primaire est l'ID.  Ces donn√©es sont entr√©es dans Ignite, nous pouvons les utiliser. <br><br><h2>  IgniteSparkSession et IgniteCatalog </h2><br>  Voyons comment cela fonctionne. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/654/24a/4ee/65424a4eeda4a4c2c6cce7038e13d1a9.png"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">CatalogExample.scala</a> <br><br>  D'une mani√®re assez simple, vous pouvez acc√©der √† toutes vos donn√©es et les interroger.  Dans le dernier exemple, nous avons d√©marr√© la session spark standard.  Et il n'y avait aucune sp√©cificit√© Ignite l√†-bas - sauf que vous devez mettre un pot avec la bonne source de donn√©es - un travail compl√®tement standard via l'API publique.  Mais, si vous souhaitez acc√©der automatiquement aux tables Ignite, vous pouvez utiliser notre extension.  La diff√©rence est qu'au lieu de SparkSession, nous √©crivons IgniteSparkSession. <br><br>  D√®s que vous cr√©ez un objet IgniteSparkSession, vous voyez dans le r√©pertoire toutes les tables qui viennent d'√™tre charg√©es dans Ignite.  Vous pouvez voir leur diagramme et toutes les informations.  Spark conna√Æt d√©j√† les tables qu'Ignite poss√®de et vous pouvez facilement obtenir toutes les donn√©es. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/dec/f1b/a0c/decf1ba0c5db2e0d84e50a0e88b6c192.png"><br><br><h2>  Igniteoptimization </h2><br>  Lorsque vous effectuez des requ√™tes complexes dans Ignite √† l'aide de JOIN, Spark extrait d'abord les donn√©es, puis seulement JOIN les regroupe.  Pour optimiser le processus, nous avons cr√©√© la fonction IgniteOptimization - elle optimise le plan de requ√™te Spark et vous permet de transmettre les parties de la demande qui peuvent √™tre ex√©cut√©es dans Ignite dans Ignite.  Nous montrons l'optimisation sur une demande sp√©cifique. <br><br><pre> <code class="sql hljs">SQL Query: <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span>   city_id,   <span class="hljs-keyword"><span class="hljs-keyword">count</span></span>(*) <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span>   person p <span class="hljs-keyword"><span class="hljs-keyword">GROUP</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> city_id <span class="hljs-keyword"><span class="hljs-keyword">HAVING</span></span> <span class="hljs-keyword"><span class="hljs-keyword">count</span></span>(*) &gt; <span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br>  Nous satisfaisons la demande.  Nous avons une table de personnes - certains employ√©s, des gens.  Chaque employ√© conna√Æt l'identifiant de la ville dans laquelle il vit.  Nous voulons savoir combien de personnes vivent dans chaque ville.  Nous filtrons - dans quelle ville vit plus d'une personne.  Voici le plan initial que Spark construit: <br><br><pre> <code class="scala hljs">== <span class="hljs-type"><span class="hljs-type">Analyzed</span></span> <span class="hljs-type"><span class="hljs-type">Logical</span></span> <span class="hljs-type"><span class="hljs-type">Plan</span></span> == city_id: bigint, count(<span class="hljs-number"><span class="hljs-number">1</span></span>): bigint <span class="hljs-type"><span class="hljs-type">Project</span></span> [city_id#<span class="hljs-number"><span class="hljs-number">19</span></span>L, count(<span class="hljs-number"><span class="hljs-number">1</span></span>)#<span class="hljs-number"><span class="hljs-number">52</span></span>L] +- <span class="hljs-type"><span class="hljs-type">Filter</span></span> (count(<span class="hljs-number"><span class="hljs-number">1</span></span>)#<span class="hljs-number"><span class="hljs-number">54</span></span>L &gt; cast(<span class="hljs-number"><span class="hljs-number">1</span></span> as bigint))  +- <span class="hljs-type"><span class="hljs-type">Aggregate</span></span> [city_id#<span class="hljs-number"><span class="hljs-number">19</span></span>L], [city_id#<span class="hljs-number"><span class="hljs-number">19</span></span>L, count(<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-type"><span class="hljs-type">AS</span></span> count(<span class="hljs-number"><span class="hljs-number">1</span></span>)#<span class="hljs-number"><span class="hljs-number">52</span></span>L, count(<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-type"><span class="hljs-type">AS</span></span> count(<span class="hljs-number"><span class="hljs-number">1</span></span>)#<span class="hljs-number"><span class="hljs-number">54</span></span>L] +- <span class="hljs-type"><span class="hljs-type">SubqueryAlias</span></span> p    +- <span class="hljs-type"><span class="hljs-type">SubqueryAlias</span></span> person       +- <span class="hljs-type"><span class="hljs-type">Relation</span></span>[<span class="hljs-type"><span class="hljs-type">NAME</span></span>#<span class="hljs-number"><span class="hljs-number">11</span></span>,<span class="hljs-type"><span class="hljs-type">BIRTH_DATE</span></span>#<span class="hljs-number"><span class="hljs-number">12</span></span>,<span class="hljs-type"><span class="hljs-type">IS_RESIDENT</span></span>#<span class="hljs-number"><span class="hljs-number">13</span></span>,<span class="hljs-type"><span class="hljs-type">SALARY</span></span>#<span class="hljs-number"><span class="hljs-number">14</span></span>,<span class="hljs-type"><span class="hljs-type">PENSION</span></span>#<span class="hljs-number"><span class="hljs-number">15</span></span>,<span class="hljs-type"><span class="hljs-type">ACCOUNT</span></span>#<span class="hljs-number"><span class="hljs-number">16</span></span>,<span class="hljs-type"><span class="hljs-type">AGE</span></span>#<span class="hljs-number"><span class="hljs-number">17</span></span>,<span class="hljs-type"><span class="hljs-type">ID</span></span>#<span class="hljs-number"><span class="hljs-number">18</span></span>L,<span class="hljs-type"><span class="hljs-type">CITY_ID</span></span>#<span class="hljs-number"><span class="hljs-number">19</span></span>L]         <span class="hljs-type"><span class="hljs-type">IgniteSQLRelation</span></span>[table=<span class="hljs-type"><span class="hljs-type">PERSON</span></span>]</code> </pre><br>  La relation n'est qu'une table Ignite.  Il n'y a pas de filtres - nous pompons simplement toutes les donn√©es de la table Person sur le r√©seau √† partir du cluster.  Ensuite, Spark agr√®ge tout cela - conform√©ment √† la demande et renvoie le r√©sultat de la demande. <br><br>  Il est facile de voir que tous ces sous-arbres avec filtre et agr√©gation peuvent √™tre ex√©cut√©s dans Ignite.  Cela sera beaucoup plus efficace que d'extraire toutes les donn√©es d'une table potentiellement grande dans Spark - c'est ce que fait notre fonction IgniteOptimization.  Apr√®s avoir analys√© et optimis√© l'arbre, nous obtenons le plan suivant: <br><br><pre> <code class="scala hljs">== <span class="hljs-type"><span class="hljs-type">Optimized</span></span> <span class="hljs-type"><span class="hljs-type">Logical</span></span> <span class="hljs-type"><span class="hljs-type">Plan</span></span> == <span class="hljs-type"><span class="hljs-type">Relation</span></span>[<span class="hljs-type"><span class="hljs-type">CITY_ID</span></span>#<span class="hljs-number"><span class="hljs-number">19</span></span>L,<span class="hljs-type"><span class="hljs-type">COUNT</span></span>(<span class="hljs-number"><span class="hljs-number">1</span></span>)#<span class="hljs-number"><span class="hljs-number">52</span></span>L]   <span class="hljs-type"><span class="hljs-type">IgniteSQLAccumulatorRelation</span></span>(     columns=[<span class="hljs-type"><span class="hljs-type">CITY_ID</span></span>, <span class="hljs-type"><span class="hljs-type">COUNT</span></span>(<span class="hljs-number"><span class="hljs-number">1</span></span>)], qry=<span class="hljs-type"><span class="hljs-type">SELECT</span></span> <span class="hljs-type"><span class="hljs-type">CITY_ID</span></span>, <span class="hljs-type"><span class="hljs-type">COUNT</span></span>(<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-type"><span class="hljs-type">FROM</span></span> <span class="hljs-type"><span class="hljs-type">PERSON</span></span> <span class="hljs-type"><span class="hljs-type">GROUP</span></span> <span class="hljs-type"><span class="hljs-type">BY</span></span> city_id <span class="hljs-type"><span class="hljs-type">HAVING</span></span> count(<span class="hljs-number"><span class="hljs-number">1</span></span>) &gt; <span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br>  En cons√©quence, nous n'obtenons qu'une seule relation, car nous avons optimis√© tout l'arbre.  Et √† l'int√©rieur, vous pouvez d√©j√† voir qu'Ignite enverra une demande suffisamment proche de la demande d'origine. <br><br>  Supposons que nous joignions diff√©rentes sources de donn√©es: par exemple, nous avons un DataFrame d'Ignite, le second de json, le troisi√®me d'Ignite √† nouveau et le quatri√®me d'une sorte de base de donn√©es relationnelle.  Dans ce cas, seul le sous-arbre sera optimis√© dans le plan.  Nous optimisons ce que nous pouvons, le d√©posons dans Ignite et Spark fera le reste.  Pour cette raison, nous obtenons un gain de vitesse. <br><br>  Un autre exemple avec JOIN: <br><br><pre> <code class="sql hljs">SQL Query - <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> jt1.id <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> id1, jt1.val1, jt2.id <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> id2, jt2.val2 <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> jt1 <span class="hljs-keyword"><span class="hljs-keyword">JOIN</span></span> jt2 <span class="hljs-keyword"><span class="hljs-keyword">ON</span></span> jt1.val1 = jt2.val2</code> </pre><br>  Nous avons deux tableaux.  Nous restons unis par valeur et s√©lectionnons parmi eux tous - ID, valeurs.  Spark propose un tel plan: <br><br><pre> <code class="scala hljs">== <span class="hljs-type"><span class="hljs-type">Analyzed</span></span> <span class="hljs-type"><span class="hljs-type">Logical</span></span> <span class="hljs-type"><span class="hljs-type">Plan</span></span> == id1: bigint, val1: string, id2: bigint, val2: string <span class="hljs-type"><span class="hljs-type">Project</span></span> [id#<span class="hljs-number"><span class="hljs-number">4</span></span>L <span class="hljs-type"><span class="hljs-type">AS</span></span> id1#<span class="hljs-number"><span class="hljs-number">84</span></span>L, val1#<span class="hljs-number"><span class="hljs-number">3</span></span>, id#<span class="hljs-number"><span class="hljs-number">6</span></span>L <span class="hljs-type"><span class="hljs-type">AS</span></span> id2#<span class="hljs-number"><span class="hljs-number">85</span></span>L, val2#<span class="hljs-number"><span class="hljs-number">5</span></span>] +- <span class="hljs-type"><span class="hljs-type">Join</span></span> <span class="hljs-type"><span class="hljs-type">Inner</span></span>, (val1#<span class="hljs-number"><span class="hljs-number">3</span></span> = val2#<span class="hljs-number"><span class="hljs-number">5</span></span>) :- <span class="hljs-type"><span class="hljs-type">SubqueryAlias</span></span> jt1 : +- <span class="hljs-type"><span class="hljs-type">Relation</span></span>[<span class="hljs-type"><span class="hljs-type">VAL1</span></span>#<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-type"><span class="hljs-type">ID</span></span>#<span class="hljs-number"><span class="hljs-number">4</span></span>L] <span class="hljs-type"><span class="hljs-type">IgniteSQLRelation</span></span>[table=<span class="hljs-type"><span class="hljs-type">JT1</span></span>] +- <span class="hljs-type"><span class="hljs-type">SubqueryAlias</span></span> jt2    +- <span class="hljs-type"><span class="hljs-type">Relation</span></span>[<span class="hljs-type"><span class="hljs-type">VAL2</span></span>#<span class="hljs-number"><span class="hljs-number">5</span></span>,<span class="hljs-type"><span class="hljs-type">ID</span></span>#<span class="hljs-number"><span class="hljs-number">6</span></span>L] <span class="hljs-type"><span class="hljs-type">IgniteSQLRelation</span></span>[table=<span class="hljs-type"><span class="hljs-type">JT2</span></span>]</code> </pre> <br>  Nous voyons qu'il va extraire toutes les donn√©es d'une table, toutes les donn√©es de la seconde, les joindre en lui et donner les r√©sultats.  Apr√®s le traitement et l'optimisation, nous obtenons exactement la m√™me demande qui va √† Ignite, o√π elle est ex√©cut√©e relativement rapidement. <br><br><pre> <code class="scala hljs">== <span class="hljs-type"><span class="hljs-type">Optimized</span></span> <span class="hljs-type"><span class="hljs-type">Logical</span></span> <span class="hljs-type"><span class="hljs-type">Plan</span></span> == <span class="hljs-type"><span class="hljs-type">Relation</span></span>[<span class="hljs-type"><span class="hljs-type">ID</span></span>#<span class="hljs-number"><span class="hljs-number">84</span></span>L,<span class="hljs-type"><span class="hljs-type">VAL1</span></span>#<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-type"><span class="hljs-type">ID</span></span>#<span class="hljs-number"><span class="hljs-number">85</span></span>L,<span class="hljs-type"><span class="hljs-type">VAL2</span></span>#<span class="hljs-number"><span class="hljs-number">5</span></span>] <span class="hljs-type"><span class="hljs-type">IgniteSQLAccumulatorRelation</span></span>(columns=[<span class="hljs-type"><span class="hljs-type">ID</span></span>, <span class="hljs-type"><span class="hljs-type">VAL1</span></span>, <span class="hljs-type"><span class="hljs-type">ID</span></span>, <span class="hljs-type"><span class="hljs-type">VAL2</span></span>], qry= <span class="hljs-type"><span class="hljs-type">SELECT</span></span> <span class="hljs-type"><span class="hljs-type">JT1</span></span>.<span class="hljs-type"><span class="hljs-type">ID</span></span> <span class="hljs-type"><span class="hljs-type">AS</span></span> id1, <span class="hljs-type"><span class="hljs-type">JT1</span></span>.<span class="hljs-type"><span class="hljs-type">VAL1</span></span>, <span class="hljs-type"><span class="hljs-type">JT2</span></span>.<span class="hljs-type"><span class="hljs-type">ID</span></span> <span class="hljs-type"><span class="hljs-type">AS</span></span> id2, <span class="hljs-type"><span class="hljs-type">JT2</span></span>.<span class="hljs-type"><span class="hljs-type">VAL2</span></span> <span class="hljs-type"><span class="hljs-type">FROM</span></span> <span class="hljs-type"><span class="hljs-type">JT1</span></span> <span class="hljs-type"><span class="hljs-type">JOIN</span></span> <span class="hljs-type"><span class="hljs-type">JT2</span></span> <span class="hljs-type"><span class="hljs-type">ON</span></span> <span class="hljs-type"><span class="hljs-type">JT1</span></span>.val1 = <span class="hljs-type"><span class="hljs-type">JT2</span></span>.val2 <span class="hljs-type"><span class="hljs-type">WHERE</span></span> <span class="hljs-type"><span class="hljs-type">JT1</span></span>.val1 <span class="hljs-type"><span class="hljs-type">IS</span></span> <span class="hljs-type"><span class="hljs-type">NOT</span></span> <span class="hljs-type"><span class="hljs-type">NULL</span></span> <span class="hljs-type"><span class="hljs-type">AND</span></span> <span class="hljs-type"><span class="hljs-type">JT2</span></span>.val2 <span class="hljs-type"><span class="hljs-type">IS</span></span> <span class="hljs-type"><span class="hljs-type">NOT</span></span> <span class="hljs-type"><span class="hljs-type">NULL</span></span>)</code> </pre> <br>  Je vais vous montrer un exemple. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ba4/39a/493/ba439a493e76dd573966cad413c07650.png"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OptimizationExample.scala</a> <br><br>  Nous cr√©ons une session IgniteSpark dans laquelle toutes nos capacit√©s d'optimisation sont d√©j√† automatiquement incluses.  Voici la demande: trouvez les joueurs avec la note la plus √©lev√©e et affichez leurs noms.  Dans la table des joueurs, leurs attributs et donn√©es.  Nous nous joignons, filtrons les donn√©es ind√©sirables et affichons les joueurs avec la note la plus √©lev√©e.  Voyons quel type de plan nous avons obtenu apr√®s l'optimisation et montrons les r√©sultats de cette requ√™te. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c7d/c51/9ab/c7dc519abdfa6b3b1d7a8396ef9725b3.png"><br><br>  Nous commen√ßons.  Nous voyons des noms de famille familiers: Messi, Buffon, Ronaldo, etc.  Soit dit en passant, certains pour une raison quelconque se rencontrent sous deux formes - Messi et Ronaldo.  Les amateurs de football peuvent trouver √©trange que des joueurs inconnus apparaissent sur la liste.  Ce sont des gardiens de but, des joueurs avec des caract√©ristiques assez √©lev√©es - dans le contexte des autres joueurs.  Maintenant, nous regardons le plan de requ√™te qui a √©t√© ex√©cut√©.  Dans Spark, presque rien n'a √©t√© fait, c'est-√†-dire que nous avons de nouveau envoy√© la demande enti√®re √† Ignite. <br><br><h2>  Apache Ignite Development </h2><br>  Notre projet est un produit open source, nous sommes donc toujours satisfaits des correctifs et des commentaires des d√©veloppeurs.  Votre aide, vos commentaires, vos correctifs sont les bienvenus.  Nous les attendons.  90% de la communaut√© Ignite est russophone.  Par exemple, pour moi, jusqu'√† ce que je commence √† travailler sur Apache Ignite, la meilleure connaissance de l'anglais n'√©tait pas dissuasive.  Cela ne vaut gu√®re la peine d'√©crire en russe sur une liste de d√©veloppeurs, mais m√™me si vous √©crivez quelque chose de mal, ils vous r√©pondront et vous aideront. <br><br>  Que peut-on am√©liorer sur cette int√©gration?  Comment puis-je vous aider si vous avez un tel d√©sir?  Liste ci-dessous.  Les ast√©risques indiquent la complexit√©. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/de4/d43/ed0/de4d43ed01894ce6b02865ad9f6aef5d.png"><br>  Pour tester l'optimisation, vous devez √©crire des tests avec des requ√™tes complexes.  Ci-dessus, j'ai montr√© quelques requ√™tes √©videntes.  Il est clair que si vous √©crivez beaucoup de groupements et beaucoup de jointures, alors quelque chose peut tomber.  C'est une t√¢che tr√®s simple - venez le faire.  Si nous trouvons des bogues bas√©s sur les r√©sultats des tests, ils devront √™tre corrig√©s.  Ce sera plus difficile l√†-bas. <br><br>  Une autre t√¢che claire et int√©ressante est l'int√©gration de Spark avec un client l√©ger.  Il est initialement capable de sp√©cifier certains ensembles d'adresses IP, et cela suffit pour rejoindre le cluster Ignite, ce qui est pratique en cas d'int√©gration avec un syst√®me externe.  Si vous souhaitez soudainement rejoindre la solution √† ce probl√®me, je vais personnellement vous aider. <br><br>  Si vous souhaitez rejoindre la communaut√© Apache Ignite, voici quelques liens utiles: <br><br><ul><li>  <i>Commencez ici - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=http://yu">https://ignite.apache.org/community/resources.html</a></i> <br></li><li>  <i>Sources ici - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://github.com/apache/ignite/</a></i> <br></li><li>  <i>Docks ici - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://apacheignite.readme.io/docs</a></i> <br></li><li>  <i>Bugs ici - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://issues.apache.org/jira/browse/IGNITE</a></i> <br></li><li>  <i>Vous pouvez √©crire ici - dev@ignite.apache.org, user@ignite.apache.org</i> <br></li></ul><br>  Nous avons une liste de d√©veloppeurs r√©actifs, qui vous aidera.  C'est encore loin d'√™tre id√©al, mais en comparaison avec d'autres projets, il est vraiment vivant. <br><br>  <i>Si vous connaissez Java ou C ++, vous cherchez du travail et souhaitez d√©velopper l'Open Source (Apache Ignite, Apache Kafka, Tarantool, etc.) √©crivez ici: join-open-source@sberbank.ru.</i> <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/CzbAweNKEVY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr427297/">https://habr.com/ru/post/fr427297/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr427285/index.html">√âcole sur les bases des circuits num√©riques: Novossibirsk - Ok, Krasnoyarsk - pr√©parez-vous</a></li>
<li><a href="../fr427289/index.html">Mod√©lisation g√©ologique 3D, diagraphie et technologie d'Aramco Innovations</a></li>
<li><a href="../fr427291/index.html">R√©duisez le trafic dans les formulaires Web ASP.NET, les div cliquables et les interrogations p√©riodiques du serveur</a></li>
<li><a href="../fr427293/index.html">Mod√®les de conception JavaScript</a></li>
<li><a href="../fr427295/index.html">Fonctions de currying JavaScript</a></li>
<li><a href="../fr427299/index.html">Allons chercher autre chose √† collectionner? Constructeur 3 en 1 "Flotte Lunaire"</a></li>
<li><a href="../fr427301/index.html">Base de donn√©es crash de GitHub</a></li>
<li><a href="../fr427303/index.html">Ralentissement de Windows, partie 2: cr√©ation de processus</a></li>
<li><a href="../fr427307/index.html">Pratique de test du backend Java + Rest-Assured</a></li>
<li><a href="../fr427309/index.html">Comment PVS-Studio s'est av√©r√© plus attentif que trois programmeurs et demi</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>