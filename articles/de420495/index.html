<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üì± üìØ ‚òπÔ∏è Bildschirm Wasser Rendering üëÜüèΩ üë®üèø‚Äçüíª üå¶Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Meine letzte Aufgabe im Bereich technische Grafik / Rendering war es, eine gute L√∂sung f√ºr das Rendern von Wasser zu finden. Insbesondere das Rendern ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bildschirm Wasser Rendering</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/420495/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/216/ca8/1f5/216ca81f5eb506eab2dbcfc730a904b4.png" alt="Bild"></div><br>  Meine letzte Aufgabe im Bereich technische Grafik / Rendering war es, eine gute L√∂sung f√ºr das Rendern von Wasser zu finden.  Insbesondere das Rendern von d√ºnnen und sich schnell bewegenden Wasserstrahlen auf Partikelbasis.  In der letzten Woche habe ich an gute Ergebnisse gedacht, deshalb werde ich einen Artikel dar√ºber schreiben. <br><br>  Ich mag den Ansatz der voxelisierten / marschierenden W√ºrfel beim Rendern von Wasser nicht wirklich (siehe zum Beispiel das Rendern einer Fl√ºssigkeitssimulation in Blender).  Wenn das Wasservolumen im gleichen Ma√üstab liegt wie das zum Rendern verwendete Gitter, ist die Bewegung sp√ºrbar diskret.  Dieses Problem kann durch Erh√∂hen der Aufl√∂sung des Gitters gel√∂st werden, aber f√ºr d√ºnne Jets √ºber relativ gro√üe Entfernungen in Echtzeit ist es einfach unpraktisch, da es die Ausf√ºhrungszeit und den belegten Speicher stark beeinflusst.  (Es gibt einen Pr√§zedenzfall f√ºr die Verwendung sp√§rlicher Voxelstrukturen, um die Situation zu verbessern. Ich bin mir jedoch nicht sicher, wie gut dies f√ºr dynamische Systeme funktioniert. Dies ist auch nicht der Grad an Komplexit√§t, mit dem ich arbeiten m√∂chte.) <br><br>  Die erste Alternative, die ich erkundete, war M√ºllers Screen Space Meshes.  Sie verwenden das Rendern von Wasserpartikeln in einen Tiefenpuffer, gl√§tten ihn, erkennen verbundene Fragmente √§hnlicher Tiefe und bauen aus dem Ergebnis mithilfe von Marschquadraten ein Netz auf.  Heute ist diese Methode wahrscheinlich anwendbarer geworden als 2007 (da wir jetzt ein Netz im Compute-Shader erstellen k√∂nnen), aber sie ist immer noch mit einem h√∂heren Grad an Komplexit√§t und Kosten verbunden, als ich es gerne h√§tte. <br><br>  Am Ende fand ich Simon Green's Pr√§sentation mit GDC 2010, Screen Space Fluid Rendering f√ºr Spiele.  Es beginnt genauso wie Screen Space Meshes: Partikel werden in den Tiefenpuffer gerendert und gegl√§ttet.  Aber anstatt das Netz zu konstruieren, wird der resultierende Puffer verwendet, um die Fl√ºssigkeit in der Hauptszene zu schattieren und zusammenzusetzen (indem die Tiefe explizit aufgezeichnet wird). Ich habe beschlossen, ein solches System zu implementieren. <br><a name="habracut"></a><br><h3>  Vorbereitung </h3><br>  In mehreren fr√ºheren Unity-Projekten habe ich gelernt, mich nicht mit den Einschr√§nkungen beim Rendern der Engine zu befassen.  Daher werden Fl√ºssigkeitspuffer von einer zweiten Kamera mit einer geringeren Sch√§rfentiefe gerendert, so dass sie vor der Hauptszene gerendert wird.  Jedes Fluidsystem existiert auf einer separaten Rendering-Schicht;  Die Hauptkammer schlie√üt eine Wasserschicht aus, und die zweite Kammer gibt nur Wasser ab.  Beide Kameras sind Kinder eines leeren Objekts, um ihre relative Ausrichtung sicherzustellen. <br><br>  Ein solches Schema bedeutet, dass ich fast alles in der Fl√ºssigkeitsschicht rendern kann, und es wird so aussehen, wie ich es erwartet habe.  Im Kontext meiner Demoszene bedeutet dies, dass einige Jets und Spritzer von Sub-Emittern zusammengef√ºhrt werden k√∂nnen.  Dar√ºber hinaus erm√∂glicht dies das Mischen anderer Wassersysteme, z. B. Volumina basierend auf H√∂henfeldern, die dann gleich wiedergegeben werden k√∂nnen.  (Ich habe dies noch nicht getestet.) <br><br>  Die Wasserquelle in meiner Szene ist ein Standardpartikelsystem.  Tats√§chlich wird keine Fl√ºssigkeitssimulation durchgef√ºhrt.  Dies bedeutet wiederum, dass sich die Partikel nicht vollst√§ndig physikalisch √ºberlappen, das Endergebnis jedoch in der Praxis akzeptabel erscheint. <br><br><h3>  Fl√ºssigkeitspuffer-Rendering </h3><br>  Der erste Schritt bei dieser Technik besteht darin, den Basisfl√ºssigkeitspuffer zu rendern.  Dies ist ein Off-Screen-Puffer, der (zum gegenw√§rtigen Zeitpunkt meiner Implementierung) Folgendes enth√§lt: Fluidbreite, Bewegungsvektor im Bildschirmraum und Rauschwert.  Zus√§tzlich rendern wir den Tiefenpuffer, indem wir die Tiefe des Fragment-Shaders explizit aufzeichnen, um jedes Viereck eines Partikels in eine kugelf√∂rmige (eigentlich elliptische) ‚ÄûKugel‚Äú zu verwandeln. <br><br>  Tiefen- und Breitenberechnungen sind ziemlich einfach: <br><br><pre><code class="cpp hljs">frag_out o; float3 N; N.xy = i.uv*<span class="hljs-number"><span class="hljs-number">2.0</span></span> - <span class="hljs-number"><span class="hljs-number">1.0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> r2 = dot(N.xy, N.xy); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (r2 &gt; <span class="hljs-number"><span class="hljs-number">1.0</span></span>) discard; Nz = <span class="hljs-built_in"><span class="hljs-built_in">sqrt</span></span>(<span class="hljs-number"><span class="hljs-number">1.0</span></span> - r2); float4 pixel_pos = float4(i.view_pos + N * i.size, <span class="hljs-number"><span class="hljs-number">1.0</span></span>); float4 clip_pos = mul(UNITY_MATRIX_P, pixel_pos); <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> depth = clip_pos.z / clip_pos.w; o.depth = depth; <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> thick = Nz * i.size * <span class="hljs-number"><span class="hljs-number">2</span></span>;</code> </pre> <br>  (Nat√ºrlich k√∂nnen Tiefenberechnungen vereinfacht werden; von der Clipposition ben√∂tigen wir nur z und w.) <br><br>  Wenig sp√§ter kehren wir zum Fragment-Shader f√ºr die Bewegungs- und Rauschvektoren zur√ºck. <br><br>  Der Spa√ü beginnt im Vertex-Shader, und hier weiche ich von der Green-Technik ab.  Das Ziel dieses Projekts ist es, Hochgeschwindigkeits-Wasserstrahlen zu rendern.  es kann mit Hilfe von kugelf√∂rmigen Partikeln realisiert werden, aber eine gro√üe Menge von ihnen wird ben√∂tigt, um einen kontinuierlichen Strahl zu erzeugen.  Stattdessen werde ich die Vierecke der Partikel basierend auf ihrer Geschwindigkeit dehnen, was wiederum die Tiefenkugeln streckt und sie nicht kugelf√∂rmig, sondern elliptisch macht.  (Da Tiefenberechnungen auf UV basieren, die sich nicht √§ndern, funktioniert alles nur.) <br><br>  Erfahrene Unity-Benutzer fragen sich m√∂glicherweise, warum ich den im Unity-Partikelsystem verf√ºgbaren integrierten Stretched Billboard-Modus einfach nicht verwende.  Stretched Billboard f√ºhrt eine bedingungslose Dehnung entlang des Geschwindigkeitsvektors im Weltraum der Welt durch.  Im allgemeinen Fall ist dies durchaus geeignet, f√ºhrt jedoch zu einem sehr auff√§lligen Problem, wenn der Geschwindigkeitsvektor zusammen mit dem nach vorne gerichteten Kameravektor (oder sehr nahe daran) gerichtet ist.  Die Werbetafel erstreckt sich auf dem Bildschirm, was ihre zweidimensionale Natur sehr deutlich macht. <br><br>  Stattdessen verwende ich eine Werbetafel, die auf die Kamera gerichtet ist, und projiziere den Geschwindigkeitsvektor auf die Ebene des Partikels, um das Viereck zu dehnen.  Wenn der Geschwindigkeitsvektor senkrecht zur Ebene ist (auf den Bildschirm gerichtet oder von ihm weg), bleibt das Partikel ungedehnt und sph√§risch, wie es sollte, und wenn es gekippt wird, wird das Partikel in diese Richtung gedehnt, was wir brauchen. <br><br>  Lassen wir eine lange Erkl√§rung, hier ist eine ziemlich einfache Funktion: <br><br><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-function">float3 </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">ComputeStretchedVertex</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(float3 p_world, float3 c_world, float3 vdir_world, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params"> stretch_amount)</span></span></span><span class="hljs-function"> </span></span>{ float3 center_offset = p_world - c_world; float3 stretch_offset = dot(center_offset, vdir_world) * vdir_world; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> p_world + stretch_offset * lerp(<span class="hljs-number"><span class="hljs-number">0.25f</span></span>, <span class="hljs-number"><span class="hljs-number">3.0f</span></span>, stretch_amount); }</code> </pre> <br>  Um den Bewegungsvektor des Bildschirmraums zu berechnen, berechnen wir zwei S√§tze von Positionen von Vektoren: <br><br><pre> <code class="cpp hljs">float3 vp1 = ComputeStretchedVertex( vertex_wp, center_wp, velocity_dir_w, rand); float3 vp0 = ComputeStretchedVertex( vertex_wp - velocity_w * unity_DeltaTime.x, center_wp - velocity_w * unity_DeltaTime.x, velocity_dir_w, rand); o.motion_0 = mul(_LastVP, float4(vp0, <span class="hljs-number"><span class="hljs-number">1.0</span></span>)); o.motion_1 = mul(_CurrVP, float4(vp1, <span class="hljs-number"><span class="hljs-number">1.0</span></span>));</code> </pre> <br>  Beachten Sie, dass Unity uns keine vorherige oder unverzerrte aktuelle Projektion aus der Ansicht liefert, da wir Bewegungsvektoren im Hauptdurchgang und nicht im Durchgang von Geschwindigkeitsvektoren berechnen.  Um dies zu beheben, habe ich den entsprechenden Partikelsystemen ein einfaches Skript hinzugef√ºgt: <br><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">ScreenspaceLiquidRenderer</span></span> : <span class="hljs-title"><span class="hljs-title">MonoBehaviour</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> Camera LiquidCamera; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> ParticleSystemRenderer m_ParticleRenderer; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">bool</span></span> m_First; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> Matrix4x4 m_PreviousVP; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Start</span></span></span><span class="hljs-function">(</span><span class="hljs-params"></span><span class="hljs-function"><span class="hljs-params"></span>)</span></span> { m_ParticleRenderer = GetComponent(); m_First = <span class="hljs-literal"><span class="hljs-literal">true</span></span>; } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">OnWillRenderObject</span></span></span><span class="hljs-function">(</span><span class="hljs-params"></span><span class="hljs-function"><span class="hljs-params"></span>)</span></span> { Matrix4x4 current_vp = LiquidCamera.nonJitteredProjectionMatrix * LiquidCamera.worldToCameraMatrix; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (m_First) { m_PreviousVP = current_vp; m_First = <span class="hljs-literal"><span class="hljs-literal">false</span></span>; } m_ParticleRenderer.material.SetMatrix(<span class="hljs-string"><span class="hljs-string">"_LastVP"</span></span>, GL.GetGPUProjectionMatrix(m_PreviousVP, <span class="hljs-literal"><span class="hljs-literal">true</span></span>)); m_ParticleRenderer.material.SetMatrix(<span class="hljs-string"><span class="hljs-string">"_CurrVP"</span></span>, GL.GetGPUProjectionMatrix(current_vp, <span class="hljs-literal"><span class="hljs-literal">true</span></span>)); m_PreviousVP = current_vp; } }</code> </pre> <br>  Ich speichere die vorherige Matrix manuell zwischen, da Camera.previousViewProjectionMatrix falsche Ergebnisse liefert. <br><br>  ¬Ø \ _ („ÉÑ) _ / ¬Ø <br><br>  (Au√üerdem verst√∂√üt diese Methode gegen das Rendern. In der Praxis kann es ratsam sein, globale Matrixkonstanten festzulegen, anstatt sie f√ºr jedes Material zu verwenden.) <br><br>  Kehren wir zum Fragment-Shader zur√ºck: Wir verwenden die projizierten Positionen, um die Bewegungsvektoren des Bildschirmraums zu berechnen: <br><br><pre> <code class="cpp hljs">float3 hp0 = i.motion_0.xyz / i.motion_0.w; float3 hp1 = i.motion_1.xyz / i.motion_1.w; float2 vp0 = (hp0.xy + <span class="hljs-number"><span class="hljs-number">1</span></span>) / <span class="hljs-number"><span class="hljs-number">2</span></span>; float2 vp1 = (hp1.xy + <span class="hljs-number"><span class="hljs-number">1</span></span>) / <span class="hljs-number"><span class="hljs-number">2</span></span>; <span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">if</span></span></span><span class="hljs-meta"> UNITY_UV_STARTS_AT_TOP vp0.y = 1.0 - vp0.y; vp1.y = 1.0 - vp1.y; #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">endif</span></span></span><span class="hljs-meta"> float2 vel = vp1 - vp0;</span></span></code> </pre> <br>  (Die Berechnung der Bewegungsvektoren erfolgt nahezu unver√§ndert aus <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">https://github.com/keijiro/ParticleMotionVector/blob/master/Assets/ParticleMotionVector/Shaders/Motion.cginc</a> ) <br><br>  Schlie√ülich ist der letzte Wert im Fl√ºssigkeitspuffer Rauschen.  Ich verwende eine stabile Zufallszahl f√ºr jedes Partikel, um eines von vier Ger√§uschen auszuw√§hlen (in eine einzelne Textur gepackt).  Dann wird es durch Geschwindigkeit und Einheit abz√ºglich der Partikelgr√∂√üe skaliert (daher sind schnelle und kleine Partikel lauter).  Dieser Rauschwert wird im Schattierungsdurchgang verwendet, um die Normalen zu verzerren und eine Schaumschicht hinzuzuf√ºgen.  Die Arbeit von Green verwendet dreikanaliges wei√ües Rauschen, aber eine neuere Arbeit (Screen Space Fluid Rendering mit Kr√ºmmungsfluss) schl√§gt die Verwendung von Perlin-Rauschen vor.  Ich verwende Voronoi-Rauschen / Zellenrauschen mit verschiedenen Skalen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/743/9f4/c0b/7439f4c0beebbc16cbfa536c516b2fcf.png"></div><br><h3>  Mischen von Problemen (und Problemumgehungen) </h3><br>  Und hier treten die ersten Probleme meiner Implementierung auf.  Zur korrekten Berechnung der Dicke der Partikel werden additiv gemischt.  Da das Mischen die gesamte Ausgabe beeinflusst, bedeutet dies, dass Rausch- und Bewegungsvektoren auch additiv gemischt werden.  Additives Rauschen passt gut zu uns, aber nicht zu additiven Vektoren. Wenn Sie sie so lassen, wie sie sind, erhalten Sie ekelhaftes Zeit-Anti-Aliasing (TAA) und Bewegungsunsch√§rfe.  Um dieses Problem zu l√∂sen, multipliziere ich beim Rendern eines Fl√ºssigkeitspuffers einfach die Bewegungsvektoren mit der Dicke und dividiere durch die Gesamtdicke im Schattierungsdurchlauf.  Dies gibt uns einen gewichteten durchschnittlichen Bewegungsvektor f√ºr alle √ºberlappenden Partikel;  nicht ganz das, was wir brauchen (seltsame Artefakte entstehen, wenn sich mehrere Jets schneiden), aber durchaus akzeptabel. <br><br>  Ein komplexeres Problem ist die Tiefe;  F√ºr eine ordnungsgem√§√üe Wiedergabe des Tiefenpuffers m√ºssen sowohl die Tiefenaufzeichnung als auch die Tiefenpr√ºfung aktiviert sein.  Dies kann zu Problemen f√ºhren, wenn die Partikel nicht sortiert sind (da der Unterschied in der Renderreihenfolge dazu f√ºhren kann, dass die Ausgabe von Partikeln, die von anderen √ºberlappt werden, abgeschnitten wird).  Deshalb befehlen wir dem Unity-Partikelsystem, die Partikel nach Tiefe zu sortieren, und dr√ºcken dann die Daumen und hoffen.  Diese Systeme werden auch in der Tiefe rendern.  Wir werden * F√§lle * von √ºberlappenden Systemen haben (zum Beispiel den Schnittpunkt zweier Partikelstrahlen), die nicht korrekt verarbeitet werden, was zu einer geringeren Dicke f√ºhrt.  Dies kommt jedoch nicht sehr oft vor und hat keinen gro√üen Einfluss auf das Erscheinungsbild. <br><br>  Der richtige Ansatz w√§re h√∂chstwahrscheinlich, die Tiefen- und Farbpuffer vollst√§ndig voneinander zu trennen.  Die Amortisation hierf√ºr ist das Rendern in zwei Durchg√§ngen.  Es lohnt sich, dieses Problem beim Einrichten des Systems zu untersuchen. <br><br><h3>  Tiefengl√§ttung </h3><br>  Schlie√ülich das Wichtigste in der Green-Technik.  Wir haben ein paar kugelf√∂rmige Kugeln in den Tiefenpuffer gerendert, aber in Wirklichkeit besteht Wasser nicht aus ‚ÄûKugeln‚Äú.  Nun nehmen wir diese Ann√§herung und verwischen sie, um sie der Oberfl√§che einer Fl√ºssigkeit √§hnlicher zu machen. <br><br>  Der naive Ansatz besteht darin, einfach Gau√üsche Rauschtiefen auf den gesamten Puffer anzuwenden.  Es erzeugt seltsame Ergebnisse - es gl√§ttet die entfernten Punkte mehr als die nahen und verwischt die R√§nder der Silhouetten.  Stattdessen k√∂nnen wir den Unsch√§rferadius in der Tiefe √§ndern und zweiseitige Unsch√§rfe verwenden, um die Kanten zu speichern. <br><br>  Hier tritt nur ein Problem auf: Solche √Ñnderungen machen die Unsch√§rfe ununterscheidbar.  Gemeinsame Unsch√§rfe kann in zwei Durchg√§ngen ausgef√ºhrt werden: horizontal und dann vertikal.  Die nicht unterscheidbare Unsch√§rfe erfolgt in einem Durchgang.  Dieser Unterschied ist wichtig, da die gemeinsame Unsch√§rfe linear skaliert (O (w) + O (h)) und die nicht gemeinsam genutzte Unsch√§rfe genau skaliert (O (w * h)).  Gro√üe, nicht gemeinsam genutzte Unsch√§rfen werden in der Praxis schnell nicht mehr anwendbar. <br><br>  Als Erwachsene, verantwortungsbewusste Entwickler, k√∂nnen wir den offensichtlichen Schritt machen: Schlie√üen Sie unsere Augen, tun Sie so, als ob das Zwei-Wege-Ger√§usch * geteilt * wird, und implementieren Sie es dennoch mit getrennten horizontalen und vertikalen G√§ngen. <br><br>  Green hat in seiner Pr√§sentation gezeigt, dass dieser Ansatz zwar Artefakte im resultierenden Ergebnis erzeugt (insbesondere bei der Rekonstruktion von Normalen), diese jedoch durch die Schattierungsphase gut ausgeblendet werden.  Bei der Arbeit mit den von mir erzeugten schmaleren Wasserstr√∂men sind diese Artefakte noch weniger auff√§llig und wirken sich nicht besonders auf das Ergebnis aus. <br><br><h3>  Schattierung </h3><br>  Wir haben endlich die Arbeit mit dem Fl√ºssigkeitspuffer beendet.  Fahren wir nun mit dem zweiten Teil des Effekts fort: Schattieren und Zusammensetzen des Hauptbilds. <br><br>  Hier sto√üen wir auf viele Unity-Rendering-Einschr√§nkungen.  Ich beschloss, das Wasser nur mit dem Licht der Sonne und der Skybox zu beleuchten.  Die Unterst√ºtzung zus√§tzlicher Lichtquellen erfordert entweder mehrere Durchg√§nge (dies ist verschwenderisch!) Oder den Aufbau einer Beleuchtungssuchstruktur auf der GPU-Seite (kostspielig und ziemlich kompliziert).  Da Unity keinen Zugriff auf Schattenkarten bietet und gerichtetes Licht Bildschirmschatten verwendet (basierend auf einem Tiefenpuffer, der durch undurchsichtige Geometrie gerendert wird), haben wir keinen Zugriff auf Schatteninformationen von einer Sonnenlichtquelle.  Sie k√∂nnen einen Befehlspuffer an eine Sonnenlichtquelle anh√§ngen, um eine Schattenkarte des Bildschirmbereichs speziell f√ºr Wasser zu erstellen. Bisher habe ich dies jedoch noch nicht getan. <br><br>  Die letzte Stufe der Schattierung wird √ºber ein Skript gesteuert und verwendet den Befehlspuffer zum Senden von Zeichnungsaufrufen.  Dies ist <i>erforderlich,</i> da die Bewegungsvektortextur (die f√ºr tempor√§res Anti-Aliasing (TAA) und Bewegungsunsch√§rfe verwendet wird) nicht f√ºr das direkte Rendern mit Graphics.SetRenderTarget () verwendet werden kann.  In dem an die Hauptkamera angeh√§ngten Skript schreiben wir Folgendes: <br><br><pre> <code class="cs hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Start</span></span></span><span class="hljs-function">(</span><span class="hljs-params"></span><span class="hljs-function"><span class="hljs-params"></span>)</span></span> { <span class="hljs-comment"><span class="hljs-comment">//... m_QuadMesh = new Mesh(); m_QuadMesh.subMeshCount = 1; m_QuadMesh.vertices = new Vector3[] { new Vector3(0, 0, 0.1f), new Vector3(1, 0, 0.1f), new Vector3(1, 1, 0.1f), new Vector3(0, 1, 0.1f), }; m_QuadMesh.uv = new Vector2[] { new Vector2(0, 0), new Vector2(1, 0), new Vector2(1, 1), new Vector2(0, 1), }; m_QuadMesh.triangles = new int[] { 0, 1, 2, 0, 2, 3, }; m_QuadMesh.UploadMeshData(false); m_CommandBuffer = new CommandBuffer(); m_CommandBuffer.Clear(); m_CommandBuffer.SetProjectionMatrix( GL.GetGPUProjectionMatrix( Matrix4x4.Ortho(0, 1, 0, 1, -1, 100), false)); m_CommandBuffer.SetRenderTarget( BuiltinRenderTextureType.CameraTarget, BuiltinRenderTextureType.CameraTarget); m_CommandBuffer.DrawMesh( m_QuadMesh, Matrix4x4.identity, m_Mat, 0, m_Mat.FindPass("LIQUIDCOMPOSITE")); m_CommandBuffer.SetRenderTarget( BuiltinRenderTextureType.MotionVectors, BuiltinRenderTextureType.Depth); m_CommandBuffer.DrawMesh( m_QuadMesh, Matrix4x4.identity, m_Mat, 0, m_Mat.FindPass("MOTION")); }</span></span></code> </pre> <br>  Farbpuffer und Bewegungsvektoren k√∂nnen nicht gleichzeitig mit MRT (Multi-Rendering-Ziele) gerendert werden.  Ich konnte den Grund nicht herausfinden.  Au√üerdem m√ºssen sie an verschiedene Tiefenpuffer gebunden werden.  Gl√ºcklicherweise schreiben wir die Tiefe in diese <i>beiden</i> Tiefenpuffer, sodass das erneute Projizieren von tempor√§rem Anti-Aliasing gut funktioniert (oh, es ist eine Freude, mit der Black-Box-Engine zu arbeiten). <br><br>  In jedem Frame wird ein zusammengesetztes Rendering aus OnPostRender () ausgegeben: <br><br><pre> <code class="cs hljs"><span class="hljs-function"><span class="hljs-function">RenderTexture </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">GenerateRefractionTexture</span></span></span><span class="hljs-function">(</span><span class="hljs-params"></span><span class="hljs-function"><span class="hljs-params"></span>)</span></span> { RenderTexture result = RenderTexture.GetTemporary(m_MainCamera.activeTexture.descriptor); Graphics.Blit(m_MainCamera.activeTexture, result); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> result; } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">OnPostRender</span></span></span><span class="hljs-function">(</span><span class="hljs-params"></span><span class="hljs-function"><span class="hljs-params"></span>)</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (ScreenspaceLiquidCamera &amp;&amp; ScreenspaceLiquidCamera.IsReady()) { RenderTexture refraction_texture = GenerateRefractionTexture(); m_Mat.SetTexture(<span class="hljs-string"><span class="hljs-string">"_MainTex"</span></span>, ScreenspaceLiquidCamera.GetColorBuffer()); m_Mat.SetVector(<span class="hljs-string"><span class="hljs-string">"_MainTex_TexelSize"</span></span>, ScreenspaceLiquidCamera.GetTexelSize()); m_Mat.SetTexture(<span class="hljs-string"><span class="hljs-string">"_LiquidRefractTexture"</span></span>, refraction_texture); m_Mat.SetTexture(<span class="hljs-string"><span class="hljs-string">"_MainDepth"</span></span>, ScreenspaceLiquidCamera.GetDepthBuffer()); m_Mat.SetMatrix(<span class="hljs-string"><span class="hljs-string">"_DepthViewFromClip"</span></span>, ScreenspaceLiquidCamera.GetProjection().inverse); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (SunLight) { m_Mat.SetVector(<span class="hljs-string"><span class="hljs-string">"_SunDir"</span></span>, transform.InverseTransformVector(-SunLight.transform.forward)); m_Mat.SetColor(<span class="hljs-string"><span class="hljs-string">"_SunColor"</span></span>, SunLight.color * SunLight.intensity); } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { m_Mat.SetVector(<span class="hljs-string"><span class="hljs-string">"_SunDir"</span></span>, transform.InverseTransformVector(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Vector3(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>))); m_Mat.SetColor(<span class="hljs-string"><span class="hljs-string">"_SunColor"</span></span>, Color.white); } m_Mat.SetTexture(<span class="hljs-string"><span class="hljs-string">"_ReflectionProbe"</span></span>, ReflectionProbe.defaultTexture); m_Mat.SetVector(<span class="hljs-string"><span class="hljs-string">"_ReflectionProbe_HDR"</span></span>, ReflectionProbe.defaultTextureHDRDecodeValues); Graphics.ExecuteCommandBuffer(m_CommandBuffer); RenderTexture.ReleaseTemporary(refraction_texture); } }</code> </pre> <br>  Und hier endet die CPU-Beteiligung, sp√§ter gehen nur noch Shader. <br><br>  Beginnen wir mit dem Durchgang von Bewegungsvektoren.  So sieht der gesamte Shader aus: <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"UnityCG.cginc"</span></span></span><span class="hljs-meta"> sampler2D _MainDepth; sampler2D _MainTex; struct appdata { float4 vertex : POSITION; float2 uv : TEXCOORD0; }; struct v2f { float2 uv : TEXCOORD0; float4 vertex : SV_POSITION; }; v2f vert(appdata v) { v2f o; o.vertex = mul(UNITY_MATRIX_P, v.vertex); o.uv = v.uv; return o; } struct frag_out { float4 color : SV_Target; float depth : SV_Depth; }; frag_out frag(v2f i) { frag_out o; float4 fluid = tex2D(_MainTex, i.uv); </span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">if</span></span></span><span class="hljs-meta"> (fluid.a == 0) discard; o.depth = tex2D(_MainDepth, i.uv).r; float2 vel = fluid.gb / fluid.a; o.color = float4(vel, 0, 1); return o; }</span></span></code> </pre> <br>  Die Geschwindigkeit im Bildschirmbereich wird im gr√ºnen und blauen Kanal des Fl√ºssigkeitspuffers gespeichert.  Da wir die Geschwindigkeit beim Rendern des Puffers durch die Dicke skaliert haben, teilen wir erneut die Gesamtdicke (im Alphakanal), um eine gewichtete Durchschnittsgeschwindigkeit zu erhalten. <br><br>  Es ist anzumerken, dass beim Arbeiten mit gro√üen Wassermengen m√∂glicherweise eine andere Methode zur Verarbeitung des Geschwindigkeitspuffers erforderlich ist.  Da wir ohne Mischen rendern, gehen die Bewegungsvektoren f√ºr alles <i>hinter dem</i> Wasser verloren, wodurch die TAA und die Bewegungsunsch√§rfe dieser Objekte zerst√∂rt werden.  Wenn Sie mit d√ºnnen Wasserstr√∂men arbeiten, ist dies kein Problem, kann jedoch bei der Arbeit mit einem Pool oder See st√∂ren, wenn TAA- oder Bewegungsunsch√§rfeobjekte erforderlich sind, um durch die Oberfl√§che deutlich sichtbar zu sein. <br><br>  Interessanter ist der Hauptschattierungspass.  Unsere erste Priorit√§t nach dem Maskieren mit der Dicke der Fl√ºssigkeit ist die Rekonstruktion der Position und Normalen des Betrachtungsraums (Sichtraum). <br><br><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-function">float3 </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">ViewPosition</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(float2 uv)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> clip_z = tex2D(_MainDepth, uv).r; <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> clip_x = uv.x * <span class="hljs-number"><span class="hljs-number">2.0</span></span> - <span class="hljs-number"><span class="hljs-number">1.0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> clip_y = <span class="hljs-number"><span class="hljs-number">1.0</span></span> - uv.y * <span class="hljs-number"><span class="hljs-number">2.0</span></span>; float4 clip_p = float4(clip_x, clip_y, clip_z, <span class="hljs-number"><span class="hljs-number">1.0</span></span>); float4 view_p = mul(_DepthViewFromClip, clip_p); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (view_p.xyz / view_p.w); } <span class="hljs-function"><span class="hljs-function">float3 </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">ReconstructNormal</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(float2 uv, float3 vp11)</span></span></span><span class="hljs-function"> </span></span>{ float3 vp12 = ViewPosition(uv + _MainTex_TexelSize.xy * float2(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)); float3 vp10 = ViewPosition(uv + _MainTex_TexelSize.xy * float2(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>)); float3 vp21 = ViewPosition(uv + _MainTex_TexelSize.xy * float2(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>)); float3 vp01 = ViewPosition(uv + _MainTex_TexelSize.xy * float2(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>)); float3 dvpdx0 = vp11 - vp12; float3 dvpdx1 = vp10 - vp11; float3 dvpdy0 = vp11 - vp21; float3 dvpdy1 = vp01 - vp11; <span class="hljs-comment"><span class="hljs-comment">// Pick the closest float3 dvpdx = dot(dvpdx0, dvpdx0) &gt; dot(dvpdx1, dvpdx1) ? dvpdx1 : dvpdx0; float3 dvpdy = dot(dvpdy0, dvpdy0) &gt; dot(dvpdy1, dvpdy1) ? dvpdy1 : dvpdy0; return normalize(cross(dvpdy, dvpdx)); }</span></span></code> </pre> <br>  Dies ist eine kostspielige Methode, um die Position des Betrachtungsraums zu rekonstruieren: Wir nehmen die Position im Clipraum ein und f√ºhren den umgekehrten Vorgang der Projektion durch. <br><br>  Nachdem wir eine M√∂glichkeit zur Rekonstruktion von Positionen erhalten haben, sind die Normalen einfacher: Wir berechnen die Position benachbarter Punkte im Tiefenpuffer und konstruieren daraus eine Tangentenbasis.  Um mit den Kanten von Silhouetten zu arbeiten, probieren wir in beide Richtungen und w√§hlen den Punkt aus, der dem Ansichtsraum am n√§chsten liegt, um die Normalen zu rekonstruieren.  Diese Methode funktioniert √ºberraschend gut und verursacht nur bei sehr d√ºnnen Objekten Probleme. <br><br>  Dies bedeutet, dass wir f√ºnf separate R√ºckprojektionsoperationen pro Pixel ausf√ºhren (f√ºr den aktuellen Punkt und vier benachbarte).  Es gibt einen g√ºnstigeren Weg, aber dieser Beitrag ist bereits zu lang, sodass ich ihn f√ºr sp√§ter belassen werde. <br><br>  Die resultierenden Normalen sind: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b2c/0ed/7f6/b2c0ed7f6cfcc66473ff4e5380c46894.png"></div><br>  Ich verzerre diese berechnete Normalen unter Verwendung der Ableitungen des Rauschwerts aus dem Fl√ºssigkeitspuffer, skaliert durch den Kraftparameter und normalisiert durch Teilen durch die Dicke des Strahls (aus dem gleichen Grund wie f√ºr die Geschwindigkeit): <br><br><pre> <code class="cpp hljs">N.xy += NoiseDerivatives(i.uv, fluid.r) * (_NoiseStrength / fluid.a); N = normalize(N);</code> </pre> <br>  Wir k√∂nnen endlich mit der Schattierung selbst fortfahren.  Wasserschattierung besteht aus drei Hauptteilen: Spiegelreflexion, Spiegelrefraktion und Schaum. <br><br>  Reflection ist ein Standard-GGX, der vollst√§ndig aus dem Standard-Unity-Shader stammt.  (Bei einer Korrektur wird das korrekte F0 von 2% f√ºr Wasser verwendet.) <br><br>  Mit der Brechung ist alles interessanter.  Die korrekte Brechung erfordert Raytracing (oder Raymarching f√ºr ein ungef√§hres Ergebnis).  Gl√ºcklicherweise ist die Brechung f√ºr das Auge weniger intuitiv als die Reflexion, und daher sind falsche Ergebnisse nicht so auff√§llig.  Daher verschieben wir die UV-Probe f√ºr die Brechungstextur um x- und y-Normalen, skaliert nach dem Parameter Dicke und Kraft: <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">float</span></span> aspect = _MainTex_TexelSize.y * _MainTex_TexelSize.z; float2 refract_uv = (i.grab_pos.xy + N.xy * float2(<span class="hljs-number"><span class="hljs-number">1</span></span>, -aspect) * fluid.a * _RefractionMultiplier) / i.grab_pos.w; float4 refract_color = tex2D(_LiquidRefractTexture, refract_uv);</code> </pre> <br>  (Beachten Sie, dass die Korrelationskorrektur verwendet wird. Sie ist <i>optional</i> - schlie√ülich handelt es sich nur um eine Ann√§herung, aber das Hinzuf√ºgen ist recht einfach.) <br><br>  Dieses gebrochene Licht geht durch die Fl√ºssigkeit, so dass ein Teil davon absorbiert wird: <br><br><pre> <code class="cpp hljs">float3 water_color = _AbsorptionColor.rgb * _AbsorptionIntensity; refract_color.rgb *= <span class="hljs-built_in"><span class="hljs-built_in">exp</span></span>(-water_color * fluid.a);</code> </pre> <br>  Beachten Sie, dass _AbsorptionColor genau umgekehrt wie erwartet bestimmt wird: Die Werte jedes Kanals geben die Menge des <i>absorbierten</i> und nicht des durchgelassenen Lichts an.  Daher ergibt _AbsorptionColor mit einem Wert von (1, 0, 0) kein Rot, sondern eine t√ºrkisfarbene Farbe (blaugr√ºn). <br><br>  Reflexion und Brechung werden unter Verwendung von Fresnel-Koeffizienten gemischt: <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">float</span></span> spec_blend = lerp(<span class="hljs-number"><span class="hljs-number">0.02</span></span>, <span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">pow</span></span>(<span class="hljs-number"><span class="hljs-number">1.0</span></span> - ldoth, <span class="hljs-number"><span class="hljs-number">5</span></span>)); float4 clear_color = lerp(refract_color, spec, spec_blend);</code> </pre> <br>  Bis zu diesem Moment haben wir uns (meistens) an die Regeln gehalten und physische Schattierungen verwendet. <br><br>  Er ist ziemlich gut, aber er hat ein Problem mit Wasser.  Es ist ein wenig schwer zu sehen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ff2/4f2/ba2/ff24f2ba2dad5dc6fe7714ad3fd55124.png"></div><br>  Um das Problem zu beheben, f√ºgen wir etwas Schaum hinzu. <br><br>  Schaum entsteht, wenn Wasser turbulent ist und sich Luft mit Wasser unter Bildung von Blasen vermischt.  Solche Blasen erzeugen alle Arten von Variationen in Reflexion und Brechung, was dem gesamten Wasser ein Gef√ºhl diffuser Beleuchtung verleiht.  Ich werde dieses Verhalten mit umwickeltem Umgebungslicht modellieren: <br><br><pre> <code class="cpp hljs">float3 foam_color = _SunColor * saturate((dot(N, L)*<span class="hljs-number"><span class="hljs-number">0.25f</span></span> + <span class="hljs-number"><span class="hljs-number">0.25f</span></span>));</code> </pre> <br>  Es wird der endg√ºltigen Farbe unter Verwendung eines speziellen Faktors hinzugef√ºgt, abh√§ngig vom Ger√§usch der Fl√ºssigkeit und dem erweichten Fresnel-Koeffizienten: <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">float</span></span> foam_blend = saturate(fluid.r * _NoiseStrength) * lerp(<span class="hljs-number"><span class="hljs-number">0.05f</span></span>, <span class="hljs-number"><span class="hljs-number">0.5f</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">pow</span></span>(<span class="hljs-number"><span class="hljs-number">1.0f</span></span> - ndotv, <span class="hljs-number"><span class="hljs-number">3</span></span>)); clear_color.rgb += foam_color * saturate(foam_blend);</code> </pre> <br>  Eingewickeltes Umgebungslicht wird normalisiert, um Energie zu sparen, sodass es als Ann√§herung an die Diffusion verwendet werden kann.  Das Mischen der Farbe des Schaums ist deutlicher.  Es ist ein ziemlich klarer Versto√ü gegen das Energieerhaltungsgesetz. <br><br>  Aber im Allgemeinen sieht alles gut aus und macht den Stream auff√§lliger: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/216/ca8/1f5/216ca81f5eb506eab2dbcfc730a904b4.png"></div><br><h3>  Weitere Arbeiten und Verbesserungen </h3><br>  In dem erstellten System kann viel verbessert werden. <br><br><ul><li>  Mehrere Farben verwenden.  Im Moment wird die Absorption erst in der letzten Stufe der Schattierung berechnet und verwendet eine konstante Farbe und Helligkeit f√ºr die gesamte Fl√ºssigkeit auf dem Bildschirm.  Die Unterst√ºtzung f√ºr verschiedene Farben ist m√∂glich, erfordert jedoch einen zweiten Farbpuffer und die L√∂sung des Absorptionsintegrals f√ºr jedes Partikel beim Rendern des Basisfl√ºssigkeitspuffers.  Dies k√∂nnte m√∂glicherweise eine kostspielige Operation sein. </li><li>  Volle Abdeckung.  Durch den Zugriff auf die Beleuchtungssuchstruktur auf der GPU-Seite (entweder von Hand oder dank der Bindung an die neue Unity HD-Rendering-Pipeline) k√∂nnen wir Wasser mit einer beliebigen Anzahl von Lichtquellen richtig beleuchten und die richtige Umgebungsbeleuchtung erzeugen. </li><li>  Verbesserte Brechung.  Mit den verschwommenen Mip-Texturen der Hintergrundtextur k√∂nnen wir die Brechung f√ºr raue Oberfl√§chen besser simulieren.  In der Praxis ist dies f√ºr kleine Fl√ºssigkeitssprays nicht sehr n√ºtzlich, kann jedoch f√ºr gr√∂√üere Volumina n√ºtzlich sein. </li></ul><br>  Wenn ich die Gelegenheit h√§tte, w√ºrde ich dieses System bis zum Verlust eines Pulses verbessern, aber im Moment kann es als vollst√§ndig bezeichnet werden. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de420495/">https://habr.com/ru/post/de420495/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de420479/index.html">Abh√§ngigkeitsinjektion im Apache Ignite.NET-Dienst</a></li>
<li><a href="../de420487/index.html">Unternehmen bitten um das Recht auf personenbezogene Daten der Nutzer</a></li>
<li><a href="../de420489/index.html">Neue ARM-Prozessoren k√∂nnen mit Core i5 konkurrieren</a></li>
<li><a href="../de420491/index.html">Mein Weg als Krieger oder wie ich einen Antrag f√ºr das Leben in Sailfish vorbereitet habe</a></li>
<li><a href="../de420493/index.html">Kann der amerikanische Bestellservice f√ºr Lebensmittel zu Amazon in der Welt der Restaurants werden?</a></li>
<li><a href="../de420497/index.html">Gem√ºse-Singularit√§t: Kroger bringt Robocouriers f√ºr Obst- und Gem√ºsekunden in Arizona auf den Markt</a></li>
<li><a href="../de420499/index.html">Anatomie von Empfehlungssystemen. Teil eins</a></li>
<li><a href="../de420501/index.html">Linux im RAM: Debirf Way 2018</a></li>
<li><a href="../de420503/index.html">JS Developer Day, verschiedene St√§dte und Gemeinden - ein Feiertag</a></li>
<li><a href="../de420505/index.html">Wird OpenAI Five das professionelle Team von The International gewinnen?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>