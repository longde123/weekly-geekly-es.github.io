<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üñºÔ∏è üí§ üòá 7 Jahre Hype um neuronale Netze in Grafiken und inspirierenden Perspektiven von Deep Learning 2020 ‚òÇÔ∏è üë®‚Äçüéì üë©üèø‚Äçüéì</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Das neue Jahr r√ºckt n√§her, die 2010er werden bald enden und der Welt die sensationelle Renaissance neuronaler Netze bescheren. Ich war durch einen ein...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>7 Jahre Hype um neuronale Netze in Grafiken und inspirierenden Perspektiven von Deep Learning 2020</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/481844/"><img src="https://habrastorage.org/getpro/habr/post_images/f4c/0a2/971/f4c0a297160b156ef22379a9555bd5fd.png"><br><br>  Das neue Jahr r√ºckt n√§her, die 2010er werden bald enden und der Welt die sensationelle Renaissance neuronaler Netze bescheren.  Ich war <s>durch einen</s> einfachen Gedanken beunruhigt <s>und konnte nicht schlafen</s> : ‚ÄûWie k√∂nnen wir die Entwicklungsgeschwindigkeit neuronaler Netze r√ºckblickend absch√§tzen?‚Äú F√ºr ‚ÄûWer die Vergangenheit kennt, kennt die Zukunft‚Äú.  Wie schnell haben sich verschiedene Algorithmen durchgesetzt?  Wie kann man die Geschwindigkeit des Fortschritts in diesem Bereich einsch√§tzen und die Geschwindigkeit des Fortschritts im n√§chsten Jahrzehnt absch√§tzen? <br><br><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/cad/e4a/11b/cade4a11b5be7a57182eddbf3765ba4c.png"><br><br>  Es ist klar, dass Sie die Anzahl der Artikel in verschiedenen Bereichen grob berechnen k√∂nnen.  Die Methode ist nicht ideal, Sie m√ºssen Subdomains ber√ºcksichtigen, aber im Allgemeinen k√∂nnen Sie es versuchen.  Ich gebe eine Idee, auf <a href="https://scholar.google.com/scholar%3Fhl%3Den%26as_sdt%3D0%252C5%26q%3Dbatch%2Bnormalization" rel="nofollow">Google Scholar (BatchNorm) ist</a> es ganz real!  Sie k√∂nnen neue Datens√§tze ber√ºcksichtigen, Sie k√∂nnen neue Kurse.  Nachdem Ihr bescheidener Diener mehrere Optionen ausgew√§hlt hatte, entschied er sich f√ºr <a href="https://trends.google.com/trends/explore%3Fdate%3Dall%26q%3Dbatch%2520normalization" rel="nofollow">Google Trends (BatchNorm)</a> . <br><br>  Meine Kollegen und ich haben Anfragen von den wichtigsten ML / DL-Technologien, z. B. <a href="https://en.wikipedia.org/wiki/Batch_normalization" rel="nofollow">Batch-Normalisierung, entgegengenommen</a> , wie in der Abbildung oben dargestellt, das Ver√∂ffentlichungsdatum des Artikels in regelm√§√üigen Abst√§nden hinzugef√ºgt und einen recht guten Zeitplan f√ºr die Popularit√§t des Themas erstellt.  Aber nicht f√ºr alle, der <s>Weg ist mit Rosen √ºbers√§t, der</s> Start ist so offensichtlich und wundersch√∂n wie beim Fledermaus.  Einige Begriffe, z. B. Regularisierung oder √úberspringen von Verbindungen, konnten aufgrund von Datenrauschen √ºberhaupt nicht erstellt werden.  Generell ist es uns jedoch gelungen, Trends zu sammeln. <br><br>  Wen k√ºmmert es, was passiert ist - willkommen beim Schnitt! <br><a name="habracut"></a><br><h1>  Anstatt einzuf√ºhren oder √ºber Bilderkennung </h1><br>  Also!  Die anf√§nglichen Daten waren ziemlich verrauscht, manchmal gab es scharfe Spitzen. <img src="https://habrastorage.org/webt/wn/ej/-p/wnej-pvivjixvw84l9sibvriuno.png"><br>  <i>Quelle: <a href="https://twitter.com/karpathy/status/849338608297406465" rel="nofollow">Andrei Karpaty twitter - Studenten stehen vor einem riesigen Publikum und h√∂ren einen Vortrag √ºber Faltungsnetzwerke</a></i> <br><br>  Konventionell reichte es f√ºr <a href="https://en.wikipedia.org/wiki/Andrej_Karpathy" rel="nofollow">Andrey Karpaty</a> , einen Vortrag √ºber das legend√§re <a href="http://cs231n.stanford.edu/" rel="nofollow">CS231n zu halten: Convolutional Neural Networks for Visual Recognition</a> f√ºr 750 Personen mit der Popularisierung des Konzepts, wie ein scharfer Gipfel verl√§uft.  Daher wurden die Daten mit einem einfachen <a href="http://nghiaho.com/%3Fp%3D1159" rel="nofollow">Box-Filter</a> gegl√§ttet (alle gegl√§tteten Outs sind auf der Achse als gegl√§ttet markiert).  Da wir daran interessiert waren, die Wachstumsrate der Popularit√§t zu vergleichen, wurden nach dem Gl√§tten alle Daten normalisiert.  Es ist ziemlich lustig geworden.  Hier ist eine Grafik der Hauptarchitekturen, die auf ImageNet konkurrieren: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0fa/4d6/f7b/0fa4d6f7bad7b232ec50e2f0bde6559b.png"><br>  <i>Quelle: Nachstehend - Berechnungen des Autors gem√§√ü Google Trends</i> <br><br>  Die Grafik zeigt sehr deutlich, dass nach der sensationellen Publikation <a href="https://en.wikipedia.org/wiki/AlexNet" rel="nofollow">AlexNet</a> , die Ende 2012 den Brei des aktuellen Hype neuronaler Netze braute, f√ºr fast zwei Jahre <s>entgegen den Behauptungen des Haufens</s> nur ein relativ enger Kreis von Spezialisten <s>hinzukam</s> .  Das Thema wurde erst im Winter 2014‚Äì2015 der √ñffentlichkeit zug√§nglich gemacht.  Achten Sie darauf, wie periodisch der Zeitplan ab 2017 wird: Weitere Spitzen jeden Fr√ºhling.  <s>In der Psychiatrie spricht man von einer Versch√§rfung des Fr√ºhlings ...</s> Dies ist ein sicheres Zeichen daf√ºr, dass der Begriff derzeit haupts√§chlich von Studenten verwendet wird und das Interesse an AlexNet im Vergleich zum H√∂hepunkt der Popularit√§t im Durchschnitt abnimmt. <br><br>  In der zweiten Jahresh√§lfte 2014 ist die <a href="https://towardsdatascience.com/vgg-neural-networks-the-next-step-after-alexnet-3f91fa9ffe2c" rel="nofollow">VGG</a> hinzugekommen.  √úbrigens hat <a href="" rel="nofollow">VGG</a> zusammen mit der <a href="" rel="nofollow">Studienleiterin</a> meiner ehemaligen Studentin <a href="https://scholar.google.com/citations%3Fuser%3DL7lMQkQAAAAJ%26hl%3Den" rel="nofollow">Karen Simonyan geschrieben</a> , die jetzt in Google DeepMind ( <a href="https://en.wikipedia.org/wiki/AlphaGo" rel="nofollow">AlphaGo</a> , <a href="https://en.wikipedia.org/wiki/AlphaZero" rel="nofollow">AlphaZero</a> usw.) arbeitet.  W√§hrend ihres Studiums an der Moskauer Staatlichen Universit√§t im 3. Jahr implementierte Karen einen guten <a href="https://www.compression.ru/video/motion_estimation/index_en.html" rel="nofollow">Algorithmus</a> zur <a href="https://www.compression.ru/video/motion_estimation/index_en.html" rel="nofollow">Bewegungssch√§tzung</a> , der seit 12 Jahren als Referenz f√ºr zweij√§hrige Studenten dient.  Au√üerdem sind die Aufgaben dort ziemlich √§hnlich.  Vergleichen Sie: <br><br><img width="50%" src="https://habrastorage.org/webt/mo/w0/9w/mow09w8lyvaxyw3b1ztxm2wxtxe.png"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/01a/c45/c94/01ac45c944275e9045557c9d453ff938.png"><br>  <i>Quelle: Verlustfunktion f√ºr Bewegungssch√§tzungsaufgaben (Autorenmaterialien) und <a href="https://arxiv.org/abs/1712.09913" rel="nofollow">VGG-56</a></i> <br><br>  Links m√ºssen Sie den tiefsten Punkt in einer nichttrivialen Oberfl√§che in Abh√§ngigkeit von den Eingabedaten f√ºr die minimale Anzahl von Messungen finden (viele lokale Minima sind m√∂glich), und rechts m√ºssen Sie einen niedrigeren Punkt mit minimalen Berechnungen finden (und auch eine Reihe von lokalen Minima, und die Oberfl√§che h√§ngt auch von den Daten ab). .  Links erhalten wir den vorhergesagten Bewegungsvektor und rechts das trainierte Netzwerk.  Der Unterschied besteht darin, dass links nur eine implizite Messung des Farbraums erfolgt und rechts zwei Messungen von Hunderten von Millionen.  Die rechnerische Komplexit√§t auf der rechten Seite ist ungef√§hr 12 Gr√∂√üenordnungen (!) H√∂her.  Ein bisschen wie das ... Aber das zweite Jahr, auch mit einer einfachen Aufgabe, schwankt wie ... [durch Zensur herausgeschnitten].  Und das Programmniveau der gestrigen Sch√ºler ist aus unbekannten Gr√ºnden in den letzten 15 Jahren deutlich gesunken.  Sie m√ºssen sagen: "Du wirst es gut machen, sie werden dich zu DeepMind bringen!"  Man k√∂nnte sagen "erfinde VGG", aber "sie werden zu DeepMind" motiviert aus irgendeinem Grund besser.  Dies ist offensichtlich ein modernes, fortgeschrittenes Analogon des Klassikers "Sie werden Grie√ü essen, Sie werden Astronaut!".  Wenn wir jedoch die Anzahl der Kinder im Land und die Gr√∂√üe des Kosmonauten-Corps z√§hlen, sind die Chancen millionenfach h√∂her, da zwei von uns bereits von unserem Labor aus bei DeepMind arbeiten. <br><br>  Als n√§chstes kam <a href="https://en.wikipedia.org/wiki/Residual_neural_network" rel="nofollow">ResNet</a> , das die Messlatte f√ºr die Anzahl der Schichten durchbrach und nach sechs Monaten zu starten begann.  Und schlie√ülich startete DenseNet, das zu Beginn des Hype stand <a href="https://towardsdatascience.com/densenet-2810936aeebb" rel="nofollow">,</a> fast sofort, sogar noch cooler als ResNet. <br><br>  Wenn wir √ºber Popularit√§t sprechen, m√∂chte ich einige Worte √ºber die Eigenschaften des Netzwerks und die Leistung hinzuf√ºgen, von denen auch die Popularit√§t abh√§ngt.  Wenn Sie sich ansehen, wie die <a href="https://en.wikipedia.org/wiki/ImageNet" rel="nofollow">ImageNet-</a> Klasse in Abh√§ngigkeit von der Anzahl der Operationen im Netzwerk vorhergesagt wird, sieht das Layout folgenderma√üen aus (h√∂her und links - besser): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c08/f56/f11/c08f56f11908ffb9f78a0d5d66a71342.png"><br>  <i>Quelle: <a href="https://www.researchgate.net/publication/328509150_Benchmark_Analysis_of_Representative_Deep_Neural_Network_Architectures" rel="nofollow">Benchmark-Analyse repr√§sentativer Deep Neural Network-Architekturen</a></i> <br><br>  Typ AlexNet ist kein Kinderspiel mehr und sie regieren Netzwerke, die auf ResNet basieren.  Wenn Sie sich jedoch die praktische Bewertung von <abbr title="Anzahl der pro Sekunde verarbeiteten Bilder">FPS</abbr> n√§her an meinem Herzen ansehen, k√∂nnen Sie deutlich erkennen, dass VGG hier n√§her am Optimum ist und sich die Ausrichtung im Allgemeinen merklich √§ndert.  Einf√ºgen von AlexNet unerwartet in die paretooptimale H√ºllkurve (horizontale Skala ist logarithmisch, besser oben und rechts): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/642/3b9/412/6423b941235280be5ec1182b91cf6d6e.png"><br>  <i>Quelle: <a href="https://www.researchgate.net/publication/328509150_Benchmark_Analysis_of_Representative_Deep_Neural_Network_Architectures" rel="nofollow">Benchmark-Analyse repr√§sentativer Deep Neural Network-Architekturen</a></i> <br><br>  <b>Gesamt:</b> <b><br><br></b> <ul><li>  In den kommenden Jahren wird sich die Ausrichtung von Architekturen mit hoher Wahrscheinlichkeit aufgrund des <a href="https://habr.com/post/455353/">Fortschritts der Beschleuniger f√ºr neuronale Netze</a> erheblich √§ndern, wenn einige Architekturen in K√∂rbe gehen und andere pl√∂tzlich abheben, einfach weil es besser ist, sich auf neue Hardware zu legen.  <a href="https://www.researchgate.net/publication/328509150_Benchmark_Analysis_of_Representative_Deep_Neural_Network_Architectures" rel="nofollow">In dem erw√§hnten Artikel</a> wird beispielsweise ein Vergleich zwischen dem NVIDIA Titan X Pascal und dem NVIDIA Jetson TX1-Board durchgef√ºhrt, und das Layout √§ndert sich merklich.  Gleichzeitig hat der Fortschritt von TPU, NPU und anderen gerade erst begonnen. <br></li><li>  Als Praktiker kann ich nur bemerken, dass der Vergleich in ImageNet standardm√§√üig in ImageNet-1k und nicht in ImageNet-22k durchgef√ºhrt wird, einfach weil die meisten ihre Netzwerke in ImageNet-1k trainieren, wo es 22-mal weniger Klassen gibt (dies) einfacher und schneller).  Der Wechsel zu ImageNet-22k, das f√ºr viele praktische Anwendungen relevanter ist, √§ndert auch die Ausrichtung (f√ºr diejenigen, die um 1k gesch√§rft sind - viel). <br></li></ul><br><h1>  Tiefer in Technologie und Architektur </h1><br>  Zur√ºck zur Technik.  Der Begriff <a href="https://en.wikipedia.org/wiki/Dropout_(neural_networks)" rel="nofollow">Dropout</a> als Suchwort ist ziemlich laut, aber das f√ºnffache Wachstum ist eindeutig mit neuronalen Netzen verbunden.  Und der R√ºckgang des Interesses daran ist h√∂chstwahrscheinlich mit einem <a href="https://patents.google.com/patent/US9406017B2/en" rel="nofollow">Google-Patent</a> und dem Aufkommen neuer Methoden verbunden.  Bitte beachten Sie, dass von der Ver√∂ffentlichung des <a href="http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="nofollow">Originalartikels</a> ungef√§hr anderthalb Jahre vergangen sind, bis das Interesse an der Methode gewachsen ist: <br><img src="https://habrastorage.org/getpro/habr/post_images/162/fca/4e4/162fca4e4b15574f66f3df40f4230090.png"><br><br>  Wenn wir jedoch √ºber die Zeit vor dem Anstieg der Popularit√§t sprechen, dann wird in DL einer der ersten Pl√§tze eindeutig von <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B5%25D0%25BA%25D1%2583%25D1%2580%25D1%2580%25D0%25B5%25D0%25BD%25D1%2582%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C" rel="nofollow">wiederkehrenden Netzwerken</a> und <a href="https://ru.wikipedia.org/wiki/%25D0%2594%25D0%25BE%25D0%25BB%25D0%25B3%25D0%25B0%25D1%258F_%25D0%25BA%25D1%2580%25D0%25B0%25D1%2582%25D0%25BA%25D0%25BE%25D1%2581%25D1%2580%25D0%25BE%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BF%25D0%25B0%25D0%25BC%25D1%258F%25D1%2582%25D1%258C" rel="nofollow">LSTM eingenommen</a> : <br><img src="https://habrastorage.org/getpro/habr/post_images/9bf/423/789/9bf423789d111f7b95352dd9a7b062c1.png"><br><br>  Lange vor 20 Jahren, vor dem H√∂hepunkt der Popularit√§t, wurden die maschinellen √úbersetzungen und die Genomanalysen jetzt radikal verbessert. In naher Zukunft wird der Netflix-Datenverkehr bei YouTube bei gleicher Bildqualit√§t zweimal sinken.  Wenn Sie die Lehren aus der Geschichte richtig ziehen, ist es offensichtlich, dass ein Teil der Ideen aus dem aktuellen Artikelschacht erst nach 20 Jahren ‚Äûabhebt‚Äú.  F√ºhre einen gesunden Lebensstil, pass auf dich auf und du wirst es pers√∂nlich sehen! <br><br>  Nun n√§her an dem versprochenen Hype.  <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network" rel="nofollow">So starteten die GANs</a> : <br> <a href="https://videoprocessing.ml/" rel="nofollow"><img src="https://habrastorage.org/getpro/habr/post_images/80c/272/c0d/80c272c0d6207b79b2736559480aea3d.png"></a> <br><br>  Es ist deutlich zu erkennen, dass es fast ein Jahr lang zu einer v√∂lligen Stille kam und erst 2016, nach 2 Jahren, ein starker Anstieg einsetzte (die Ergebnisse wurden sp√ºrbar verbessert).  Dieser Start ein Jahr sp√§ter brachte den sensationellen DeepFake, der jedoch auch 1,5 Jahre lang startete.  Das hei√üt, selbst vielversprechende Technologien ben√∂tigen viel Zeit, um von einer Idee zu Anwendungen zu gelangen, die jeder nutzen kann. <br><br>  Wenn Sie sich ansehen, welche Bilder die GAN im <a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" rel="nofollow">Originalartikel</a> erzeugt hat und was mit <a href="https://en.wikipedia.org/wiki/StyleGAN" rel="nofollow">StyleGAN erstellt werden kann</a> , wird deutlich, warum es so still war.  Im Jahr 2014 konnten nur Spezialisten bewerten, wie cool es war - im Wesentlichen ein weiteres Netzwerk als Verlustfunktion zu erstellen und gemeinsam zu trainieren.  Und im Jahr 2019 konnte jedes Schulkind erkennen, wie cool das ist (ohne genau zu verstehen, wie das gemacht wird): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5ee/6ca/51a/5ee6ca51ac69315327c2dc31f8f80c15.png"><br><br>  Heutzutage <a href="https://www.eff.org/ai/metrics" rel="nofollow">gibt es viele</a> verschiedene Probleme, die von neuronalen Netzen erfolgreich gel√∂st werden. Sie k√∂nnen die besten Netze verwenden und Beliebtheitsgraphen f√ºr jede Richtung erstellen, mit Rauschen und Spitzenwerten von Suchanfragen umgehen usw.  Um meine Gedanken nicht √ºber den Baum zu verbreiten, beenden wir diese Auswahl mit dem Thema Segmentierungsalgorithmen, bei denen sich die Ideen der <a href="https://medium.com/%40sh.tsang/review-deeplabv3-atrous-separable-convolution-semantic-segmentation-a625f6e83b90" rel="nofollow">atrous / dilated Convolution</a> und des <a href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d" rel="nofollow">ASPP</a> in den letzten eineinhalb Jahren <a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb_main.php%3Fchallengeid%3D11%26compid%3D6" rel="nofollow">im Algorithmus-Benchmark</a> ziemlich verbreitet haben: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f52/6c8/6d5/f526c86d5b81454b2a62f77193be0335.png"><br>  Es sollte auch beachtet werden, dass wenn <a href="https://arxiv.org/pdf/1412.7062.pdf" rel="nofollow">DeepLabv1</a> mehr als ein Jahr auf den Anstieg der Popularit√§t ‚Äûgewartet‚Äú hat, <a href="https://arxiv.org/pdf/1606.00915.pdf" rel="nofollow">DeepLabv2</a> in einem Jahr <a href="https://arxiv.org/pdf/1706.05587.pdf" rel="nofollow">gestartet</a> ist und <a href="https://arxiv.org/pdf/1706.05587.pdf" rel="nofollow">DeepLabv3</a> fast sofort.  Das hei√üt  Im Allgemeinen k√∂nnen wir √ºber die Beschleunigung des Interessenswachstums im Laufe der Zeit sprechen (oder √ºber die Beschleunigung des Interessenswachstums an Technologien renommierter Autoren). <br><br>  All dies zusammen f√ºhrte zur Entstehung des folgenden globalen Problems - einer explosionsartigen Zunahme der Ver√∂ffentlichungen zu diesem Thema: <br><br><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/99b/bd7/66e/99bbd766ebd801adb403efa6ee5efb4e.png"><br>  <i>Quelle: <a href="http://data-mining.philippe-fournier-viger.com/too-many-machine-learning-papers/" rel="nofollow">Zu viele maschinelle Lernpapiere?</a></i> <br><br>  In diesem Jahr erhalten wir ungef√§hr 150-200 Artikel pro Tag, da nicht alle auf arXiv-e ver√∂ffentlicht sind.  Es ist heute v√∂llig unm√∂glich, Artikel auch in ihrem eigenen Unterbereich zu lesen.  Infolgedessen werden sicherlich viele interessante Ideen in den Tr√ºmmern neuer Ver√∂ffentlichungen vergraben sein, die sich auf den Zeitpunkt ihres ‚ÄûStarts‚Äú auswirken werden.  Aber auch der <i>explosionsartige</i> Zuwachs an kompetenten Fachkr√§ften in der Region l√§sst <s>wenig</s> Hoffnung, das Problem zu bew√§ltigen. <br><br>  <b>Gesamt:</b> <b><br><br></b> <ul><li>  Zus√§tzlich zu ImageNet und der Geschichte hinter den Kulissen der Spieleerfolge von DeepMind haben GANs eine neue Welle der Popularisierung neuronaler Netze ausgel√∂st.  Mit ihnen war es wirklich m√∂glich, <a href="https://www.youtube.com/watch%3Fv%3D5rPKeUXjEvE" rel="nofollow">Schauspieler</a> ohne <a href="https://www.youtube.com/watch%3Fv%3DWm3squcz7Aw" rel="nofollow">Kamera</a> zu <a href="https://www.youtube.com/watch%3Fv%3D5rPKeUXjEvE" rel="nofollow">‚Äûdrehen‚Äú</a> .  Und ob es noch mehr geben wird!  Unter diesem Informationsrauschen werden weniger sonore, aber durchaus funktionierende Verarbeitungs- und Erkennungstechnologien finanziert. <br></li><li>  Da es zu viele Ver√∂ffentlichungen gibt, freuen wir uns auf die Entwicklung neuer neuronaler Netzwerkmethoden f√ºr die schnelle Analyse von Artikeln, denn nur sie werden uns retten (ein Witz mit einem Bruchteil eines Witzes!). <br></li></ul><br><h1>  Arbeitsroboter, gl√ºcklicher Mann </h1><br>  AutoML erfreut sich seit 2 Jahren wachsender Beliebtheit <s>auf den Zeitungsseiten</s> .  Begonnen hat alles traditionell mit ImageNet, in dem er in Top-1 Accuracy die ersten Pl√§tze fest einnahm: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e6b/9a1/498/e6b9a14988b3d708bdbc73aebbf567d2.png"><br>  Die Essenz von AutoML ist sehr einfach, ein jahrhundertealter Traum von Datenwissenschaftlern ist darin wahr geworden - f√ºr ein neuronales Netzwerk zur Auswahl von Hyperparametern.  Die Idee wurde mit einem Knall begr√º√üt: <br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/4f8/c12/56a/4f8c1256ab40bc119fa0c971a8f8bbc1.png"></div><br>  Unten in der Grafik sehen wir eine eher seltene Situation, in der sie nach der Ver√∂ffentlichung von <a href="https://arxiv.org/pdf/1707.07012.pdf" rel="nofollow">Quellartikeln</a> auf <a href="https://arxiv.org/pdf/1707.07012.pdf" rel="nofollow">NASNet</a> und <a href="https://arxiv.org/pdf/1802.01548.pdf" rel="nofollow">AmoebaNet</a> im <a href="https://arxiv.org/pdf/1707.07012.pdf" rel="nofollow">Vergleich zu</a> fr√ºheren Ideen fast augenblicklich an Popularit√§t gewinnen (ein gro√ües Interesse am Thema wirkt sich aus): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0aa/81b/662/0aa81b6628967bc6e77468365cce8554.png"><br>  Das idyllische Bild wird durch zwei Punkte etwas verdorben.  Erstens beginnt jedes Gespr√§ch √ºber AutoML mit dem Satz: "Wenn Sie ein GPU-Dofigalion haben ...".  Und das ist das Problem.  Google behauptet nat√ºrlich, dass dies mit seiner <a href="https://cloud.google.com/automl/" rel="nofollow">Cloud AutoML</a> leicht zu l√∂sen ist. <s>Hauptsache, Sie haben genug Geld</s> , aber nicht jeder stimmt diesem Ansatz zu.  Zweitens funktioniert es soweit <a href="https://towardsdatascience.com/automl-is-overhyped-1b5511ded65f" rel="nofollow">unvollkommen</a> .  Auf der anderen Seite sind, unter Hinweis auf die GANs, noch keine f√ºnf Jahre vergangen, und die Idee selbst sieht sehr vielversprechend aus. <br><br>  In jedem Fall beginnt der Hauptstart von AutoML mit der n√§chsten Generation von Hardwarebeschleunigern f√ºr neuronale Netze und in der Tat mit verbesserten Algorithmen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7ec/e67/3c5/7ece673c519b57348f556aa7f7b9dfa6.png"><br>  <i>Quelle: Bild von Dmitry Konovalchuk, Materialien des Autors</i> <br><br>  <b>Total: Tats√§chlich werden Datenwissenschaftler nat√ºrlich keinen ewigen Urlaub haben, da die Daten f√ºr eine sehr lange Zeit gro√üe Kopfschmerzen bereiten werden.</b>  <b>Aber warum nicht vor dem neuen Jahr und dem Beginn der 2020er Jahre tr√§umen?</b> <br><br><h1>  Ein paar Worte zu Werkzeugen </h1><br>  Die Wirksamkeit der Forschung h√§ngt stark von den Instrumenten ab.  Wenn Sie f√ºr die Programmierung von AlexNet nicht-triviale Programmierung ben√∂tigten, kann ein solches Netzwerk heute in mehreren Zeilen in neuen Frameworks gesammelt werden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b7c/002/004/b7c0020046f4f5a5b021c6e4a943c9a5.png"><br>  Es ist deutlich zu sehen, wie sich die Popularit√§t in Wellen ver√§ndert.  Heute ist <a href="https://pytorch.org/" rel="nofollow">PyTorch</a> das beliebteste (auch <a href="https://paperswithcode.com/trends" rel="nofollow">laut PapersWithCode</a> ).  Und einmal verl√§sst der beliebte <a href="http://caffe.berkeleyvision.org/" rel="nofollow">Caffe</a> wunderbar ganz reibungslos.  (Hinweis: Thema und Software bedeuten, dass beim Plotten die Themenfilterung von Google verwendet wurde.) <br><br>  Nun, da wir uns mit Entwicklungstools befasst haben, sollten Bibliotheken erw√§hnt werden, um die Netzwerkausf√ºhrung zu beschleunigen: <br> <a href="https://videoprocessing.ml/" rel="nofollow"><img src="https://habrastorage.org/getpro/habr/post_images/de3/fb6/8af/de3fb68af88d9afabe23929e1b060309.png"></a> <br><br>  Das √§lteste Thema ist (in Bezug auf NVIDIA) <a href="https://developer.nvidia.com/cudnn" rel="nofollow">cuDNN</a> , und zum Gl√ºck f√ºr Entwickler ist die Anzahl der Bibliotheken in den letzten Jahren um ein Vielfaches gestiegen, und der Beginn ihrer Beliebtheit ist deutlich steiler geworden.  Und es scheint, dass dies alles nur der Anfang ist. <br><br>  <b>Insgesamt: Auch in den letzten 3 Jahren haben sich die Werkzeuge stark zum Besseren gewandelt.</b>  <b>Und vor 3 Jahren waren sie nach heutigen Ma√üst√§ben √ºberhaupt nicht.</b>  <b>Der Fortschritt ist sehr gut!</b> <br><br><h1>  Versprochene Perspektiven f√ºr neuronale Netze </h1><br>  Aber der Spa√ü beginnt sp√§ter.  In diesem Sommer habe ich in einem <a href="https://habr.com/post/455353/">separaten gro√üen Artikel</a> ausf√ºhrlich beschrieben, warum die CPU und sogar die GPU nicht effizient genug sind, um mit neuronalen Netzen zu arbeiten, warum Milliarden von Dollar in die Entwicklung neuer Chips flie√üen und welche Aussichten bestehen.  Ich werde mich nicht wiederholen.  Nachfolgend finden Sie eine Verallgemeinerung und Erg√§nzung des vorherigen Textes. <br><br>  Zun√§chst m√ºssen Sie die Unterschiede zwischen neuronalen Netzwerkberechnungen und Berechnungen in der bekannten von Neumann-Architektur verstehen (in der sie nat√ºrlich berechnet werden k√∂nnen, aber weniger effizient sind): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a1c/b6a/2f7/a1cb6a2f730e5d5f66a0e29b3fa7d1ac.png"><br>  <i>Quelle: Bild von Dmitry Konovalchuk, Materialien des Autors</i> <br><div class="scrollable-table"><table><tbody><tr><td>  <b>Von Neumann Architektur</b> <br></td><td>  <b>Neuronale Netze</b> <br></td></tr><tr><td>  Die meisten Berechnungen sind sequentielle Operationen. <br></td><td>  Massively Parallel Computing (Sie ben√∂tigen eine Architektur mit einer gro√üen Anzahl von Rechenmodulen und einer Beschleunigung des Tensor Computing) <br></td></tr><tr><td>  Der Ablauf der Berechnungen √§ndert sich <br>  abh√§ngig von den Bedingungen ( <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D1%2583%25D0%25BF%25D0%25B5%25D1%2580%25D1%2581%25D0%25BA%25D0%25B0%25D0%25BB%25D1%258F%25D1%2580%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D1%258C" rel="nofollow">Superskalarit√§t</a> erforderlich) <br></td><td>  Die Rechenstruktur ist fast immer fest und im Voraus bekannt (Superskalarit√§t ist ineffizient) <br></td></tr><tr><td>  Es gibt Lokalit√§t nach den Daten (Cache funktioniert gut) <br></td><td>  Keine Datenlokalit√§t (Cache erw√§rmt die Luft) <br></td></tr><tr><td>  Genaue Berechnungen <br></td><td>  Berechnungen sind m√∂glicherweise nicht genau. <br></td></tr><tr><td>  Die Daten √§ndern sich f√ºr verschiedene Algorithmen unterschiedlich <br></td><td>  Dutzende von Megabyte Netzwerkkoeffizienten bleiben unver√§ndert, wenn Daten wiederholt durch ein neuronales Netzwerk geleitet werden <br></td></tr></tbody></table></div><br>  Beim vorherigen Mal drehte sich die Hauptdiskussion um FPGA / ASIC, und ungenaue Berechnungen blieben fast unbemerkt. Lassen Sie uns daher n√§her darauf eingehen.  Die enormen Aussichten f√ºr die Reduzierung der Chips der n√§chsten Generationen liegen genau in der F√§higkeit, ungenau zu lesen (und Koeffizientendaten lokal zu speichern).  Tats√§chlich wird die Vergr√∂berung auch in der exakten Arithmetik verwendet, wenn die Netzwerkgewichte in ganze Zahlen umgewandelt und quantisiert werden, jedoch auf einer neuen Ebene.  Betrachten Sie als Beispiel einen Einzelbit-Addierer (das Beispiel ist ziemlich abstrakt): <br><br><img width="60%" src="https://habrastorage.org/getpro/habr/post_images/959/bbf/f16/959bbff160d7f0b33e4d46ce7a5be453.png"><br>  <i>Quelle: <a href="https://www.researchgate.net/publication/270898651_A_High_Speed_and_Low_Power_8_Bit_x_8_Bit_Multiplier_Design_using_Novel_Two_Transistor_2T_XOR_Gates" rel="nofollow">Ein 8-Bit-x-8-Bit-Multiplikator-Design mit hoher Geschwindigkeit und geringem Stromverbrauch unter Verwendung neuartiger 2-Transistor-XOR-Gatter</a></i> <br><br>  Er braucht 6 Transistoren (es gibt verschiedene Ans√§tze, die Anzahl der ben√∂tigten Transistoren kann immer geringer sein, aber im Allgemeinen ungef√§hr so).  F√ºr 8 Bits sind ungef√§hr <a href="https://www.semanticscholar.org/paper/A-novel-approach-for-reversible-realization-of-with-Shukla-Singh/147db51cf0f054b260d980950c01146649483aa1" rel="nofollow">48 Transistoren</a> erforderlich.  In diesem Fall ben√∂tigt der Analogaddierer nur 2 (zwei!) Transistoren, d.h.  24 mal weniger: <br><br><img width="60%" src="https://habrastorage.org/getpro/habr/post_images/f34/85f/2a5/f3485f2a5ac6fa241d2e59cd80ed1064.png"><br>  <i>Quelle: <a href="http://www.iitk.ac.in/eclub/ee381/AnalogMultipliers.pdf" rel="nofollow">Analoge Multiplikatoren (Analyse und Entwurf analoger integrierter Schaltungen)</a></i> <br><br>  Wenn die Genauigkeit h√∂her ist (z. B. entsprechend 10 oder 16 Bit), ist die Differenz noch gr√∂√üer.  Noch interessanter ist die Situation mit der Multiplikation!  Wenn ein digitaler 8-Bit-Multiplexer ungef√§hr <a href="https://www.semanticscholar.org/paper/A-novel-approach-for-reversible-realization-of-with-Shukla-Singh/147db51cf0f054b260d980950c01146649483aa1" rel="nofollow">400 Transistoren ben√∂tigt</a> , dann wird eine analoge 6, d.h.  67 mal (!) Weniger.  Nat√ºrlich unterscheiden sich "analoge" und "digitale" Transistoren vom Standpunkt der Schaltung her erheblich, aber die Idee ist klar: Wenn es uns gelingt, die Genauigkeit analoger Berechnungen zu erh√∂hen, erreichen wir leicht die Situation, in der wir zwei Gr√∂√üenordnungen weniger Transistoren ben√∂tigen.  Dabei geht es nicht so sehr um die Reduzierung der Gr√∂√üe (was im Zusammenhang mit der ‚ÄûVerlangsamung von Moores Gesetz‚Äú wichtig ist), sondern um die Reduzierung des Stromverbrauchs, der f√ºr mobile Plattformen von entscheidender Bedeutung ist.  Und f√ºr Rechenzentren wird es nicht √ºberfl√ºssig sein. <br><br><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/077/0b2/42b/0770b242bb2693acbe79a2165a4e8c68.png"><br>  <i>Quelle: <a href="https://blocksandfiles.com/2019/02/11/ibms-ai-chips-change-phase/" rel="nofollow">IBM denkt an analoge Chips, um das maschinelle Lernen zu beschleunigen</a></i> <br><br>  Der Schl√ºssel zum Erfolg wird hier eine Verringerung der Genauigkeit sein, und auch hier steht IBM an vorderster Front: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a87/570/9f1/a875709f1c76de4e228a567f293a6618.png"><br>  <i>Quelle: <a href="https://www.ibm.com/blogs/research/2018/12/8-bit-precision-training/" rel="nofollow">IBM Research Blog: 8-Bit-Pr√§zision f√ºr das Training von Deep Learning-Systemen</a></i> <br><br>  Sie besch√§ftigen sich bereits mit spezialisierten ASICs f√ºr neuronale Netze, die eine mehr als 10-fache √úberlegenheit gegen√ºber der GPU aufweisen, und planen, in den kommenden Jahren eine 100-fache √úberlegenheit zu erreichen.  Es sieht sehr ermutigend aus, wir freuen uns sehr darauf, denn ich wiederhole, dies wird ein Durchbruch f√ºr mobile Ger√§te sein. <br><br>  Bisher ist die Situation nicht so magisch, obwohl es ernsthafte Erfolge gibt.  Hier ist ein interessanter Test der aktuellen mobilen Hardwarebeschleuniger von neuronalen Netzen (das Bild ist anklickbar und das erw√§rmt die Seele des Autors erneut, auch in Bildern pro Sekunde): <br><br> <a href="" rel="nofollow"><img src="https://habrastorage.org/getpro/habr/post_images/175/fe0/10d/175fe010d6d9742ddabe6d20d50edb67.png"></a> <br>  <i>Quelle: <a href="https://www.groundai.com/project/ai-benchmark-all-about-deep-learning-on-smartphones-in-2019/1" rel="nofollow">Leistungsentwicklung mobiler AI-Beschleuniger: Bilddurchsatz f√ºr das Float-Inception-V3-Modell (FP16-Modell unter Verwendung von TensorFlow Lite und NNAPI)</a></i> <br><br>  Gr√ºn zeigt mobile Chips an, blau zeigt CPU an, orange zeigt GPU an.  Es ist deutlich zu sehen, dass die aktuellen mobilen Chips und vor allem der Top-End-Chip von Huawei bereits die zehnfach gr√∂√üere CPU (und den zehnfach gr√∂√üeren Stromverbrauch) √ºberholen.  Und es ist stark!  Mit der GPU ist bisher alles nicht so magisch, aber es wird noch etwas anderes geben.  Sie k√∂nnen die Ergebnisse detaillierter auf einer separaten Website unter <a href="http://ai-benchmark.com/" rel="nofollow">http://ai-benchmark.com/</a> ansehen. <a href="http://ai-benchmark.com/" rel="nofollow">Beachten Sie</a> den dortigen Testabschnitt. Sie haben eine gute Reihe von Algorithmen zum Vergleich ausgew√§hlt. <br><br>  <b>Insgesamt: Der Fortschritt analoger Beschleuniger ist heute recht schwer zu bewerten.</b>  <b>Es gibt ein Rennen.</b>  <b>Die Produkte sind jedoch noch nicht erschienen, so dass es <a href="https://www.google.com/search%3Fq%3Danalog%2Bdnn%2Baccelerator%2Bfiletype%253Apdf" rel="nofollow">relativ wenige</a> Ver√∂ffentlichungen gibt.</b>  <b>Sie k√∂nnen Patente √ºberwachen, die verz√∂gert angezeigt werden (z. B. dichter Datenfluss <a href="https://patents.google.com/%3Fq%3D%2522resistive%2Bprocessing%2Bunit%2522%26q%3DRPU%26oq%3D%2522resistive%2Bprocessing%2Bunit%2522%2BRPU" rel="nofollow">von IBM</a> ), oder <a href="https://patents.google.com/%3Fq%3Danalog%26q%3Ddnn%26q%3Daccelerator%26oq%3Danalog%2Bdnn%2Baccelerator" rel="nofollow">nach seltenen Patenten</a> anderer Hersteller suchen.</b>  <b>Es scheint, dass dies eine sehr ernste Revolution sein wird, vor allem bei Smartphones und Server-TPUs.</b> <br><br><h1>  Anstelle einer Schlussfolgerung </h1><br>  ML / DL hei√üt heute eine neue Programmiertechnologie, wenn wir kein Programm schreiben, sondern einen Block einf√ºgen und trainieren.  Das hei√üt  Wie anfangs gab es einen Assembler, dann C, dann C ++, und jetzt, nach langen 30 Jahren des Wartens, ist der n√§chste Schritt ML / DL: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cfd/8ae/5f7/cfd8ae5f7236a0c9781199044966d2c5.png"></div><br>  Das macht Sinn.  In j√ºngster Zeit werden in fortgeschrittenen Unternehmen Entscheidungsorte in Programmen durch neuronale Netze ersetzt.  Das hei√üt      ¬´ IF-¬ª            (!)         ,        3-5        .   ,   ,     .     ,  <s>,  </s> ,   ,  ,   ,      .   -! <br><br> ,   .  ,  -  ,  : ¬´ ,    !¬ª     <s></s>      ,   , ,               ,       (, !) .          : ¬´    , !¬ª              ,     .           ¬´¬ª          ¬´     !¬ª (  ).      . ! <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Als mathematisches Werkzeug ist ML / DL im Allgemeinen und neuronale Netze im Besonderen jedoch eindeutig mehr als die n√§chste Programmiertechnologie. </font><font style="vertical-align: inherit;">Die gleichen neuronalen Netze werden jetzt einfach bei jedem Schritt gefunden:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Das Smartphone macht Bilder des Textes und erkennt ihn - das sind neuronale Netze, </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ein Smartphone l√§sst sich im Handumdrehen von einer Sprache in eine andere √ºbersetzen und spricht eine √úbersetzung - neuronale Netze und wiederum neuronale Netze. </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Der Navigator und der intelligente Sprecher erkennen die Sprache recht gut - wieder neuronale Netze, </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Der Fernseher zeigt ein helles Kontrastbild von 8K aus dem eingegebenen 2K-Video - ebenfalls ein neuronales Netzwerk. </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Roboter in der Produktion wurden genauer, sie begannen, abnormale Situationen besser zu sehen und zu erkennen - wieder neuronale Netze, </font></font><br></li><li>   10       ,       <s> </s>        ,    - 90- ‚Äî   , <br></li><li>             ‚Äî   , <br></li><li>    ‚Äî               -   <a href="https://www.cgevent.ru/archives/28741" rel="nofollow">     </a> ‚Äî    , <br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Im Allgemeinen - neuronale Netze sind jetzt absolut √ºberall! </font></font> ) <br></li></ul><br><img width="60%" src="https://habrastorage.org/getpro/habr/post_images/f28/b47/999/f28b47999ec31c004bc0b917fed05633.png"><br><br>  Es sind nur 4 Jahre vergangen, seit Menschen dank BatchNorm (2015) und Skip Connections (2015) in vielerlei Hinsicht gelernt haben, wirklich tiefe neuronale Netze zu trainieren, und 3 Jahre sind vergangen, seit sie ‚Äûabgehoben‚Äú haben, und wir lesen wirklich die Ergebnisse ihrer Arbeit habe nicht gesehen.  Und jetzt werden sie die Produkte erreichen.  Etwas sagt uns, dass in den kommenden Jahren viele interessante Dinge auf uns warten.  Vor allem, wenn Beschleuniger "abheben" ... <br><br><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/a88/6b9/3dc/a886b93dc708002de50fac65c7d84a7a.png"><br><br>  Es war einmal, wenn sich jemand daran erinnert, dass Prometheus dem Olymp das Feuer gestohlen und es den Menschen √ºbergeben hat.  Der zornige Zeus mit anderen G√∂ttern schuf die erste Sch√∂nheit einer Frau namens Pandora, die mit vielen wundervollen weiblichen Eigenschaften ausgestattet war <s>(mir wurde pl√∂tzlich klar, dass die politisch korrekte Nacherz√§hlung einiger Mythen des antiken Griechenlands √§u√üerst schwierig ist)</s> .  Pandora wurde zu Leuten geschickt, aber Prometheus, der vermutete, dass etwas nicht stimmte, widerstand ihrem Zauber und sein Bruder Epimetheus nicht.  Als Geschenk f√ºr die Hochzeit sandte Zeus einen sch√∂nen Sarg mit Merkur und Merkur, eine g√ºtige Seele, erf√ºllte den Befehl - er gab den Sarg Epimetheus, warnte ihn aber, ihn auf keinen Fall zu √∂ffnen.  Die neugierige Pandora hat ihrem Mann den Sarg gestohlen und ge√∂ffnet, aber es gab nur S√ºnden, Krankheiten, Kriege und andere Probleme der Menschheit.  Sie versuchte den Sarg zu schlie√üen, aber es war zu sp√§t: <br><br><img src="https://habrastorage.org/webt/fi/hk/wh/fihkwhcd3uam5uyejc6thklshcm.png"><br>  <i>Quelle: <a href="https://regnum.ru/pictures/2414568/6.html" rel="nofollow">K√ºnstler Frederick Stuart Church, B√ºchse der Pandora</a></i> <br><br>  Seitdem ist der Ausdruck "√ñffne die B√ºchse der Pandora" verschwunden, das hei√üt, <s>aus Neugier</s> eine irreversible Handlung <s>auszuf√ºhren</s> , deren Folgen m√∂glicherweise nicht so sch√∂n sind wie die Verzierungen des Sarges auf der Au√üenseite. <br><br>  Je tiefer ich in die neuronalen Netze eintauche, desto ausgepr√§gter ist das Gef√ºhl, dass dies eine andere B√ºchse von Pandora ist.  Die Menschheit hat jedoch die gr√∂√üte Erfahrung mit dem √ñffnen solcher Kisten!  Aus der j√ºngsten Vergangenheit - das ist Kernenergie und das Internet.  Ich denke also, wir k√∂nnen zusammen fertig werden.  Kein Wunder, dass ein Haufen rauer <s>B√§rtiger</s> unter den Er√∂ffnern ist.  Nun, ein Sarg ist wundersch√∂n, stimme zu!  Und es ist nicht wahr, dass es nur Probleme gibt, sie haben bereits ein paar gute Dinge.  Deshalb kamen sie zusammen und ... wir √∂ffnen weiter! <br><br>  <b>Gesamt:</b> <b><br><br></b> <ul><li>  <b>Der Artikel enthielt nicht viele interessante Themen, zum Beispiel klassische ML-Algorithmen, Transferlernen, Best√§rkungslernen, die Beliebtheit von Datens√§tzen usw.</b>  <b>(Meine Herren, Sie k√∂nnen das Thema fortsetzen!)</b> <b><br></b> </li><li>  <b>Zur Frage zum Sarg: Ich pers√∂nlich finde <a href="https://habr.com/ru/post/411323/">die Google-Programmierer</a> , die es Google <a href="https://tproger.ru/news/google-drops-pentagon/" rel="nofollow">erm√∂glicht haben, den 10-Milliarden-Dollar-Pentagon-Vertrag aufzugeben,</a> gro√üartig und ansehnlich.</b>  <b>Sie respektieren und respektieren.</b>  <b>Beachten Sie jedoch, dass jemand diese Hauptausschreibung gewonnen hat.</b> <b><br></b> </li></ul><br>  Lesen Sie auch: <br><br><ul><li>  <a href="https://habr.com/post/455353/">Hardwarebeschleunigung tiefer neuronaler Netze: GPU, FPGA, ASIC, TPU, VPU, IPU, DPU, NPU, RPU, NNP und andere Briefe</a> - der Autorentext √ºber den aktuellen Stand und die Perspektiven der Hardwarebeschleunigung neuronaler Netze im Vergleich zu aktuellen Ans√§tzen. <br></li><li>  <a href="https://habr.com/post/480348/">Deep Fake Science, die Krise der Reproduzierbarkeit und woher leere Repositories kommen</a> - √ºber die Probleme in der Wissenschaft, die durch ML / DL erzeugt werden. <br></li><li>  <a href="https://habr.com/post/451664/">Street Magic Codec Vergleich.</a>  <a href="https://habr.com/post/451664/">Wir enth√ºllen Geheimnisse</a> - ein Beispiel f√ºr eine F√§lschung, die auf neuronalen Netzen basiert. <br></li></ul><br><h3>  Alles eine gro√üe Anzahl <i>neuer interessanter Entdeckungen</i> in den 2020er Jahren im Allgemeinen und im Neuen Jahr im Besonderen! </h3><br><h2>  Danksagung </h2><br>  Ich m√∂chte mich herzlich bedanken bei: <br><br><ul><li>  Labor f√ºr Computergrafik und Multimedia VMK Moscow State University  M.V.  Lomonosov f√ºr seinen Beitrag zur Entwicklung des tiefen Lernens in Russland und nicht nur <br></li><li>  pers√∂nlich Konstantin Kozhemyakov und Dmitry Konovalchuk, die viel getan haben, um diesen Artikel besser und visueller zu machen, <br></li><li>  und zum Schluss vielen Dank an Kirill Malyshev, Jegor Sklyarov, Nikolai Oplachko, Andrej Moskalenko, Ivan Molodetsky, Evgeny Lyapustin, Roman Kazantsev, Alexander Yakovenko und Dmitry Klepikov f√ºr viele n√ºtzliche Kommentare und Korrekturen, die diesen Text viel besser gemacht haben! <br></li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de481844/">https://habr.com/ru/post/de481844/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de481828/index.html">Die Geschichte der Lernsoftware: Lernmanagementsysteme und der Aufstieg der Online-Bildung</a></li>
<li><a href="../de481836/index.html">Pizza as a Service: Wie Amazon zu Redshift migrierte</a></li>
<li><a href="../de481838/index.html">WireGuard, Einrichtung mehrerer Clients f√ºr NAT und wohin geht STUN?</a></li>
<li><a href="../de481840/index.html">Sch√ºtzen Sie Ihre GraphQL-API vor Sicherheitsl√ºcken</a></li>
<li><a href="../de481842/index.html">Umstieg auf reinen Speicher: Unser neuer Speicher</a></li>
<li><a href="../de481846/index.html">Verwenden von GitHub CI f√ºr Elixir-Projekte</a></li>
<li><a href="../de481848/index.html">Erfahrene Mitarbeiterschulung</a></li>
<li><a href="../de481850/index.html">Die spanische Inquisition und der Roboter f√ºr Dem√ºtigung: Was sind die "r√§uberischen" Konferenzen um des Geldes willen?</a></li>
<li><a href="../de481852/index.html">3D Anet N4 Printer Review // Wie man einen Dark Souls Charakter realistisch f√§rbt</a></li>
<li><a href="../de481854/index.html">Testen von Ideen durch Dashboard-Prototyping</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>