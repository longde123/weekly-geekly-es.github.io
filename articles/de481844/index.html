<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🖼️ 💤 😇 7 Jahre Hype um neuronale Netze in Grafiken und inspirierenden Perspektiven von Deep Learning 2020 ☂️ 👨‍🎓 👩🏿‍🎓</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Das neue Jahr rückt näher, die 2010er werden bald enden und der Welt die sensationelle Renaissance neuronaler Netze bescheren. Ich war durch einen ein...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>7 Jahre Hype um neuronale Netze in Grafiken und inspirierenden Perspektiven von Deep Learning 2020</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/481844/"><img src="https://habrastorage.org/getpro/habr/post_images/f4c/0a2/971/f4c0a297160b156ef22379a9555bd5fd.png"><br><br>  Das neue Jahr rückt näher, die 2010er werden bald enden und der Welt die sensationelle Renaissance neuronaler Netze bescheren.  Ich war <s>durch einen</s> einfachen Gedanken beunruhigt <s>und konnte nicht schlafen</s> : „Wie können wir die Entwicklungsgeschwindigkeit neuronaler Netze rückblickend abschätzen?“ Für „Wer die Vergangenheit kennt, kennt die Zukunft“.  Wie schnell haben sich verschiedene Algorithmen durchgesetzt?  Wie kann man die Geschwindigkeit des Fortschritts in diesem Bereich einschätzen und die Geschwindigkeit des Fortschritts im nächsten Jahrzehnt abschätzen? <br><br><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/cad/e4a/11b/cade4a11b5be7a57182eddbf3765ba4c.png"><br><br>  Es ist klar, dass Sie die Anzahl der Artikel in verschiedenen Bereichen grob berechnen können.  Die Methode ist nicht ideal, Sie müssen Subdomains berücksichtigen, aber im Allgemeinen können Sie es versuchen.  Ich gebe eine Idee, auf <a href="https://scholar.google.com/scholar%3Fhl%3Den%26as_sdt%3D0%252C5%26q%3Dbatch%2Bnormalization" rel="nofollow">Google Scholar (BatchNorm) ist</a> es ganz real!  Sie können neue Datensätze berücksichtigen, Sie können neue Kurse.  Nachdem Ihr bescheidener Diener mehrere Optionen ausgewählt hatte, entschied er sich für <a href="https://trends.google.com/trends/explore%3Fdate%3Dall%26q%3Dbatch%2520normalization" rel="nofollow">Google Trends (BatchNorm)</a> . <br><br>  Meine Kollegen und ich haben Anfragen von den wichtigsten ML / DL-Technologien, z. B. <a href="https://en.wikipedia.org/wiki/Batch_normalization" rel="nofollow">Batch-Normalisierung, entgegengenommen</a> , wie in der Abbildung oben dargestellt, das Veröffentlichungsdatum des Artikels in regelmäßigen Abständen hinzugefügt und einen recht guten Zeitplan für die Popularität des Themas erstellt.  Aber nicht für alle, der <s>Weg ist mit Rosen übersät, der</s> Start ist so offensichtlich und wunderschön wie beim Fledermaus.  Einige Begriffe, z. B. Regularisierung oder Überspringen von Verbindungen, konnten aufgrund von Datenrauschen überhaupt nicht erstellt werden.  Generell ist es uns jedoch gelungen, Trends zu sammeln. <br><br>  Wen kümmert es, was passiert ist - willkommen beim Schnitt! <br><a name="habracut"></a><br><h1>  Anstatt einzuführen oder über Bilderkennung </h1><br>  Also!  Die anfänglichen Daten waren ziemlich verrauscht, manchmal gab es scharfe Spitzen. <img src="https://habrastorage.org/webt/wn/ej/-p/wnej-pvivjixvw84l9sibvriuno.png"><br>  <i>Quelle: <a href="https://twitter.com/karpathy/status/849338608297406465" rel="nofollow">Andrei Karpaty twitter - Studenten stehen vor einem riesigen Publikum und hören einen Vortrag über Faltungsnetzwerke</a></i> <br><br>  Konventionell reichte es für <a href="https://en.wikipedia.org/wiki/Andrej_Karpathy" rel="nofollow">Andrey Karpaty</a> , einen Vortrag über das legendäre <a href="http://cs231n.stanford.edu/" rel="nofollow">CS231n zu halten: Convolutional Neural Networks for Visual Recognition</a> für 750 Personen mit der Popularisierung des Konzepts, wie ein scharfer Gipfel verläuft.  Daher wurden die Daten mit einem einfachen <a href="http://nghiaho.com/%3Fp%3D1159" rel="nofollow">Box-Filter</a> geglättet (alle geglätteten Outs sind auf der Achse als geglättet markiert).  Da wir daran interessiert waren, die Wachstumsrate der Popularität zu vergleichen, wurden nach dem Glätten alle Daten normalisiert.  Es ist ziemlich lustig geworden.  Hier ist eine Grafik der Hauptarchitekturen, die auf ImageNet konkurrieren: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0fa/4d6/f7b/0fa4d6f7bad7b232ec50e2f0bde6559b.png"><br>  <i>Quelle: Nachstehend - Berechnungen des Autors gemäß Google Trends</i> <br><br>  Die Grafik zeigt sehr deutlich, dass nach der sensationellen Publikation <a href="https://en.wikipedia.org/wiki/AlexNet" rel="nofollow">AlexNet</a> , die Ende 2012 den Brei des aktuellen Hype neuronaler Netze braute, für fast zwei Jahre <s>entgegen den Behauptungen des Haufens</s> nur ein relativ enger Kreis von Spezialisten <s>hinzukam</s> .  Das Thema wurde erst im Winter 2014–2015 der Öffentlichkeit zugänglich gemacht.  Achten Sie darauf, wie periodisch der Zeitplan ab 2017 wird: Weitere Spitzen jeden Frühling.  <s>In der Psychiatrie spricht man von einer Verschärfung des Frühlings ...</s> Dies ist ein sicheres Zeichen dafür, dass der Begriff derzeit hauptsächlich von Studenten verwendet wird und das Interesse an AlexNet im Vergleich zum Höhepunkt der Popularität im Durchschnitt abnimmt. <br><br>  In der zweiten Jahreshälfte 2014 ist die <a href="https://towardsdatascience.com/vgg-neural-networks-the-next-step-after-alexnet-3f91fa9ffe2c" rel="nofollow">VGG</a> hinzugekommen.  Übrigens hat <a href="" rel="nofollow">VGG</a> zusammen mit der <a href="" rel="nofollow">Studienleiterin</a> meiner ehemaligen Studentin <a href="https://scholar.google.com/citations%3Fuser%3DL7lMQkQAAAAJ%26hl%3Den" rel="nofollow">Karen Simonyan geschrieben</a> , die jetzt in Google DeepMind ( <a href="https://en.wikipedia.org/wiki/AlphaGo" rel="nofollow">AlphaGo</a> , <a href="https://en.wikipedia.org/wiki/AlphaZero" rel="nofollow">AlphaZero</a> usw.) arbeitet.  Während ihres Studiums an der Moskauer Staatlichen Universität im 3. Jahr implementierte Karen einen guten <a href="https://www.compression.ru/video/motion_estimation/index_en.html" rel="nofollow">Algorithmus</a> zur <a href="https://www.compression.ru/video/motion_estimation/index_en.html" rel="nofollow">Bewegungsschätzung</a> , der seit 12 Jahren als Referenz für zweijährige Studenten dient.  Außerdem sind die Aufgaben dort ziemlich ähnlich.  Vergleichen Sie: <br><br><img width="50%" src="https://habrastorage.org/webt/mo/w0/9w/mow09w8lyvaxyw3b1ztxm2wxtxe.png"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/01a/c45/c94/01ac45c944275e9045557c9d453ff938.png"><br>  <i>Quelle: Verlustfunktion für Bewegungsschätzungsaufgaben (Autorenmaterialien) und <a href="https://arxiv.org/abs/1712.09913" rel="nofollow">VGG-56</a></i> <br><br>  Links müssen Sie den tiefsten Punkt in einer nichttrivialen Oberfläche in Abhängigkeit von den Eingabedaten für die minimale Anzahl von Messungen finden (viele lokale Minima sind möglich), und rechts müssen Sie einen niedrigeren Punkt mit minimalen Berechnungen finden (und auch eine Reihe von lokalen Minima, und die Oberfläche hängt auch von den Daten ab). .  Links erhalten wir den vorhergesagten Bewegungsvektor und rechts das trainierte Netzwerk.  Der Unterschied besteht darin, dass links nur eine implizite Messung des Farbraums erfolgt und rechts zwei Messungen von Hunderten von Millionen.  Die rechnerische Komplexität auf der rechten Seite ist ungefähr 12 Größenordnungen (!) Höher.  Ein bisschen wie das ... Aber das zweite Jahr, auch mit einer einfachen Aufgabe, schwankt wie ... [durch Zensur herausgeschnitten].  Und das Programmniveau der gestrigen Schüler ist aus unbekannten Gründen in den letzten 15 Jahren deutlich gesunken.  Sie müssen sagen: "Du wirst es gut machen, sie werden dich zu DeepMind bringen!"  Man könnte sagen "erfinde VGG", aber "sie werden zu DeepMind" motiviert aus irgendeinem Grund besser.  Dies ist offensichtlich ein modernes, fortgeschrittenes Analogon des Klassikers "Sie werden Grieß essen, Sie werden Astronaut!".  Wenn wir jedoch die Anzahl der Kinder im Land und die Größe des Kosmonauten-Corps zählen, sind die Chancen millionenfach höher, da zwei von uns bereits von unserem Labor aus bei DeepMind arbeiten. <br><br>  Als nächstes kam <a href="https://en.wikipedia.org/wiki/Residual_neural_network" rel="nofollow">ResNet</a> , das die Messlatte für die Anzahl der Schichten durchbrach und nach sechs Monaten zu starten begann.  Und schließlich startete DenseNet, das zu Beginn des Hype stand <a href="https://towardsdatascience.com/densenet-2810936aeebb" rel="nofollow">,</a> fast sofort, sogar noch cooler als ResNet. <br><br>  Wenn wir über Popularität sprechen, möchte ich einige Worte über die Eigenschaften des Netzwerks und die Leistung hinzufügen, von denen auch die Popularität abhängt.  Wenn Sie sich ansehen, wie die <a href="https://en.wikipedia.org/wiki/ImageNet" rel="nofollow">ImageNet-</a> Klasse in Abhängigkeit von der Anzahl der Operationen im Netzwerk vorhergesagt wird, sieht das Layout folgendermaßen aus (höher und links - besser): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c08/f56/f11/c08f56f11908ffb9f78a0d5d66a71342.png"><br>  <i>Quelle: <a href="https://www.researchgate.net/publication/328509150_Benchmark_Analysis_of_Representative_Deep_Neural_Network_Architectures" rel="nofollow">Benchmark-Analyse repräsentativer Deep Neural Network-Architekturen</a></i> <br><br>  Typ AlexNet ist kein Kinderspiel mehr und sie regieren Netzwerke, die auf ResNet basieren.  Wenn Sie sich jedoch die praktische Bewertung von <abbr title="Anzahl der pro Sekunde verarbeiteten Bilder">FPS</abbr> näher an meinem Herzen ansehen, können Sie deutlich erkennen, dass VGG hier näher am Optimum ist und sich die Ausrichtung im Allgemeinen merklich ändert.  Einfügen von AlexNet unerwartet in die paretooptimale Hüllkurve (horizontale Skala ist logarithmisch, besser oben und rechts): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/642/3b9/412/6423b941235280be5ec1182b91cf6d6e.png"><br>  <i>Quelle: <a href="https://www.researchgate.net/publication/328509150_Benchmark_Analysis_of_Representative_Deep_Neural_Network_Architectures" rel="nofollow">Benchmark-Analyse repräsentativer Deep Neural Network-Architekturen</a></i> <br><br>  <b>Gesamt:</b> <b><br><br></b> <ul><li>  In den kommenden Jahren wird sich die Ausrichtung von Architekturen mit hoher Wahrscheinlichkeit aufgrund des <a href="https://habr.com/post/455353/">Fortschritts der Beschleuniger für neuronale Netze</a> erheblich ändern, wenn einige Architekturen in Körbe gehen und andere plötzlich abheben, einfach weil es besser ist, sich auf neue Hardware zu legen.  <a href="https://www.researchgate.net/publication/328509150_Benchmark_Analysis_of_Representative_Deep_Neural_Network_Architectures" rel="nofollow">In dem erwähnten Artikel</a> wird beispielsweise ein Vergleich zwischen dem NVIDIA Titan X Pascal und dem NVIDIA Jetson TX1-Board durchgeführt, und das Layout ändert sich merklich.  Gleichzeitig hat der Fortschritt von TPU, NPU und anderen gerade erst begonnen. <br></li><li>  Als Praktiker kann ich nur bemerken, dass der Vergleich in ImageNet standardmäßig in ImageNet-1k und nicht in ImageNet-22k durchgeführt wird, einfach weil die meisten ihre Netzwerke in ImageNet-1k trainieren, wo es 22-mal weniger Klassen gibt (dies) einfacher und schneller).  Der Wechsel zu ImageNet-22k, das für viele praktische Anwendungen relevanter ist, ändert auch die Ausrichtung (für diejenigen, die um 1k geschärft sind - viel). <br></li></ul><br><h1>  Tiefer in Technologie und Architektur </h1><br>  Zurück zur Technik.  Der Begriff <a href="https://en.wikipedia.org/wiki/Dropout_(neural_networks)" rel="nofollow">Dropout</a> als Suchwort ist ziemlich laut, aber das fünffache Wachstum ist eindeutig mit neuronalen Netzen verbunden.  Und der Rückgang des Interesses daran ist höchstwahrscheinlich mit einem <a href="https://patents.google.com/patent/US9406017B2/en" rel="nofollow">Google-Patent</a> und dem Aufkommen neuer Methoden verbunden.  Bitte beachten Sie, dass von der Veröffentlichung des <a href="http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="nofollow">Originalartikels</a> ungefähr anderthalb Jahre vergangen sind, bis das Interesse an der Methode gewachsen ist: <br><img src="https://habrastorage.org/getpro/habr/post_images/162/fca/4e4/162fca4e4b15574f66f3df40f4230090.png"><br><br>  Wenn wir jedoch über die Zeit vor dem Anstieg der Popularität sprechen, dann wird in DL einer der ersten Plätze eindeutig von <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B5%25D0%25BA%25D1%2583%25D1%2580%25D1%2580%25D0%25B5%25D0%25BD%25D1%2582%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C" rel="nofollow">wiederkehrenden Netzwerken</a> und <a href="https://ru.wikipedia.org/wiki/%25D0%2594%25D0%25BE%25D0%25BB%25D0%25B3%25D0%25B0%25D1%258F_%25D0%25BA%25D1%2580%25D0%25B0%25D1%2582%25D0%25BA%25D0%25BE%25D1%2581%25D1%2580%25D0%25BE%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BF%25D0%25B0%25D0%25BC%25D1%258F%25D1%2582%25D1%258C" rel="nofollow">LSTM eingenommen</a> : <br><img src="https://habrastorage.org/getpro/habr/post_images/9bf/423/789/9bf423789d111f7b95352dd9a7b062c1.png"><br><br>  Lange vor 20 Jahren, vor dem Höhepunkt der Popularität, wurden die maschinellen Übersetzungen und die Genomanalysen jetzt radikal verbessert. In naher Zukunft wird der Netflix-Datenverkehr bei YouTube bei gleicher Bildqualität zweimal sinken.  Wenn Sie die Lehren aus der Geschichte richtig ziehen, ist es offensichtlich, dass ein Teil der Ideen aus dem aktuellen Artikelschacht erst nach 20 Jahren „abhebt“.  Führe einen gesunden Lebensstil, pass auf dich auf und du wirst es persönlich sehen! <br><br>  Nun näher an dem versprochenen Hype.  <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network" rel="nofollow">So starteten die GANs</a> : <br> <a href="https://videoprocessing.ml/" rel="nofollow"><img src="https://habrastorage.org/getpro/habr/post_images/80c/272/c0d/80c272c0d6207b79b2736559480aea3d.png"></a> <br><br>  Es ist deutlich zu erkennen, dass es fast ein Jahr lang zu einer völligen Stille kam und erst 2016, nach 2 Jahren, ein starker Anstieg einsetzte (die Ergebnisse wurden spürbar verbessert).  Dieser Start ein Jahr später brachte den sensationellen DeepFake, der jedoch auch 1,5 Jahre lang startete.  Das heißt, selbst vielversprechende Technologien benötigen viel Zeit, um von einer Idee zu Anwendungen zu gelangen, die jeder nutzen kann. <br><br>  Wenn Sie sich ansehen, welche Bilder die GAN im <a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" rel="nofollow">Originalartikel</a> erzeugt hat und was mit <a href="https://en.wikipedia.org/wiki/StyleGAN" rel="nofollow">StyleGAN erstellt werden kann</a> , wird deutlich, warum es so still war.  Im Jahr 2014 konnten nur Spezialisten bewerten, wie cool es war - im Wesentlichen ein weiteres Netzwerk als Verlustfunktion zu erstellen und gemeinsam zu trainieren.  Und im Jahr 2019 konnte jedes Schulkind erkennen, wie cool das ist (ohne genau zu verstehen, wie das gemacht wird): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5ee/6ca/51a/5ee6ca51ac69315327c2dc31f8f80c15.png"><br><br>  Heutzutage <a href="https://www.eff.org/ai/metrics" rel="nofollow">gibt es viele</a> verschiedene Probleme, die von neuronalen Netzen erfolgreich gelöst werden. Sie können die besten Netze verwenden und Beliebtheitsgraphen für jede Richtung erstellen, mit Rauschen und Spitzenwerten von Suchanfragen umgehen usw.  Um meine Gedanken nicht über den Baum zu verbreiten, beenden wir diese Auswahl mit dem Thema Segmentierungsalgorithmen, bei denen sich die Ideen der <a href="https://medium.com/%40sh.tsang/review-deeplabv3-atrous-separable-convolution-semantic-segmentation-a625f6e83b90" rel="nofollow">atrous / dilated Convolution</a> und des <a href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d" rel="nofollow">ASPP</a> in den letzten eineinhalb Jahren <a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb_main.php%3Fchallengeid%3D11%26compid%3D6" rel="nofollow">im Algorithmus-Benchmark</a> ziemlich verbreitet haben: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f52/6c8/6d5/f526c86d5b81454b2a62f77193be0335.png"><br>  Es sollte auch beachtet werden, dass wenn <a href="https://arxiv.org/pdf/1412.7062.pdf" rel="nofollow">DeepLabv1</a> mehr als ein Jahr auf den Anstieg der Popularität „gewartet“ hat, <a href="https://arxiv.org/pdf/1606.00915.pdf" rel="nofollow">DeepLabv2</a> in einem Jahr <a href="https://arxiv.org/pdf/1706.05587.pdf" rel="nofollow">gestartet</a> ist und <a href="https://arxiv.org/pdf/1706.05587.pdf" rel="nofollow">DeepLabv3</a> fast sofort.  Das heißt  Im Allgemeinen können wir über die Beschleunigung des Interessenswachstums im Laufe der Zeit sprechen (oder über die Beschleunigung des Interessenswachstums an Technologien renommierter Autoren). <br><br>  All dies zusammen führte zur Entstehung des folgenden globalen Problems - einer explosionsartigen Zunahme der Veröffentlichungen zu diesem Thema: <br><br><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/99b/bd7/66e/99bbd766ebd801adb403efa6ee5efb4e.png"><br>  <i>Quelle: <a href="http://data-mining.philippe-fournier-viger.com/too-many-machine-learning-papers/" rel="nofollow">Zu viele maschinelle Lernpapiere?</a></i> <br><br>  In diesem Jahr erhalten wir ungefähr 150-200 Artikel pro Tag, da nicht alle auf arXiv-e veröffentlicht sind.  Es ist heute völlig unmöglich, Artikel auch in ihrem eigenen Unterbereich zu lesen.  Infolgedessen werden sicherlich viele interessante Ideen in den Trümmern neuer Veröffentlichungen vergraben sein, die sich auf den Zeitpunkt ihres „Starts“ auswirken werden.  Aber auch der <i>explosionsartige</i> Zuwachs an kompetenten Fachkräften in der Region lässt <s>wenig</s> Hoffnung, das Problem zu bewältigen. <br><br>  <b>Gesamt:</b> <b><br><br></b> <ul><li>  Zusätzlich zu ImageNet und der Geschichte hinter den Kulissen der Spieleerfolge von DeepMind haben GANs eine neue Welle der Popularisierung neuronaler Netze ausgelöst.  Mit ihnen war es wirklich möglich, <a href="https://www.youtube.com/watch%3Fv%3D5rPKeUXjEvE" rel="nofollow">Schauspieler</a> ohne <a href="https://www.youtube.com/watch%3Fv%3DWm3squcz7Aw" rel="nofollow">Kamera</a> zu <a href="https://www.youtube.com/watch%3Fv%3D5rPKeUXjEvE" rel="nofollow">„drehen“</a> .  Und ob es noch mehr geben wird!  Unter diesem Informationsrauschen werden weniger sonore, aber durchaus funktionierende Verarbeitungs- und Erkennungstechnologien finanziert. <br></li><li>  Da es zu viele Veröffentlichungen gibt, freuen wir uns auf die Entwicklung neuer neuronaler Netzwerkmethoden für die schnelle Analyse von Artikeln, denn nur sie werden uns retten (ein Witz mit einem Bruchteil eines Witzes!). <br></li></ul><br><h1>  Arbeitsroboter, glücklicher Mann </h1><br>  AutoML erfreut sich seit 2 Jahren wachsender Beliebtheit <s>auf den Zeitungsseiten</s> .  Begonnen hat alles traditionell mit ImageNet, in dem er in Top-1 Accuracy die ersten Plätze fest einnahm: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e6b/9a1/498/e6b9a14988b3d708bdbc73aebbf567d2.png"><br>  Die Essenz von AutoML ist sehr einfach, ein jahrhundertealter Traum von Datenwissenschaftlern ist darin wahr geworden - für ein neuronales Netzwerk zur Auswahl von Hyperparametern.  Die Idee wurde mit einem Knall begrüßt: <br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/4f8/c12/56a/4f8c1256ab40bc119fa0c971a8f8bbc1.png"></div><br>  Unten in der Grafik sehen wir eine eher seltene Situation, in der sie nach der Veröffentlichung von <a href="https://arxiv.org/pdf/1707.07012.pdf" rel="nofollow">Quellartikeln</a> auf <a href="https://arxiv.org/pdf/1707.07012.pdf" rel="nofollow">NASNet</a> und <a href="https://arxiv.org/pdf/1802.01548.pdf" rel="nofollow">AmoebaNet</a> im <a href="https://arxiv.org/pdf/1707.07012.pdf" rel="nofollow">Vergleich zu</a> früheren Ideen fast augenblicklich an Popularität gewinnen (ein großes Interesse am Thema wirkt sich aus): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0aa/81b/662/0aa81b6628967bc6e77468365cce8554.png"><br>  Das idyllische Bild wird durch zwei Punkte etwas verdorben.  Erstens beginnt jedes Gespräch über AutoML mit dem Satz: "Wenn Sie ein GPU-Dofigalion haben ...".  Und das ist das Problem.  Google behauptet natürlich, dass dies mit seiner <a href="https://cloud.google.com/automl/" rel="nofollow">Cloud AutoML</a> leicht zu lösen ist. <s>Hauptsache, Sie haben genug Geld</s> , aber nicht jeder stimmt diesem Ansatz zu.  Zweitens funktioniert es soweit <a href="https://towardsdatascience.com/automl-is-overhyped-1b5511ded65f" rel="nofollow">unvollkommen</a> .  Auf der anderen Seite sind, unter Hinweis auf die GANs, noch keine fünf Jahre vergangen, und die Idee selbst sieht sehr vielversprechend aus. <br><br>  In jedem Fall beginnt der Hauptstart von AutoML mit der nächsten Generation von Hardwarebeschleunigern für neuronale Netze und in der Tat mit verbesserten Algorithmen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7ec/e67/3c5/7ece673c519b57348f556aa7f7b9dfa6.png"><br>  <i>Quelle: Bild von Dmitry Konovalchuk, Materialien des Autors</i> <br><br>  <b>Total: Tatsächlich werden Datenwissenschaftler natürlich keinen ewigen Urlaub haben, da die Daten für eine sehr lange Zeit große Kopfschmerzen bereiten werden.</b>  <b>Aber warum nicht vor dem neuen Jahr und dem Beginn der 2020er Jahre träumen?</b> <br><br><h1>  Ein paar Worte zu Werkzeugen </h1><br>  Die Wirksamkeit der Forschung hängt stark von den Instrumenten ab.  Wenn Sie für die Programmierung von AlexNet nicht-triviale Programmierung benötigten, kann ein solches Netzwerk heute in mehreren Zeilen in neuen Frameworks gesammelt werden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b7c/002/004/b7c0020046f4f5a5b021c6e4a943c9a5.png"><br>  Es ist deutlich zu sehen, wie sich die Popularität in Wellen verändert.  Heute ist <a href="https://pytorch.org/" rel="nofollow">PyTorch</a> das beliebteste (auch <a href="https://paperswithcode.com/trends" rel="nofollow">laut PapersWithCode</a> ).  Und einmal verlässt der beliebte <a href="http://caffe.berkeleyvision.org/" rel="nofollow">Caffe</a> wunderbar ganz reibungslos.  (Hinweis: Thema und Software bedeuten, dass beim Plotten die Themenfilterung von Google verwendet wurde.) <br><br>  Nun, da wir uns mit Entwicklungstools befasst haben, sollten Bibliotheken erwähnt werden, um die Netzwerkausführung zu beschleunigen: <br> <a href="https://videoprocessing.ml/" rel="nofollow"><img src="https://habrastorage.org/getpro/habr/post_images/de3/fb6/8af/de3fb68af88d9afabe23929e1b060309.png"></a> <br><br>  Das älteste Thema ist (in Bezug auf NVIDIA) <a href="https://developer.nvidia.com/cudnn" rel="nofollow">cuDNN</a> , und zum Glück für Entwickler ist die Anzahl der Bibliotheken in den letzten Jahren um ein Vielfaches gestiegen, und der Beginn ihrer Beliebtheit ist deutlich steiler geworden.  Und es scheint, dass dies alles nur der Anfang ist. <br><br>  <b>Insgesamt: Auch in den letzten 3 Jahren haben sich die Werkzeuge stark zum Besseren gewandelt.</b>  <b>Und vor 3 Jahren waren sie nach heutigen Maßstäben überhaupt nicht.</b>  <b>Der Fortschritt ist sehr gut!</b> <br><br><h1>  Versprochene Perspektiven für neuronale Netze </h1><br>  Aber der Spaß beginnt später.  In diesem Sommer habe ich in einem <a href="https://habr.com/post/455353/">separaten großen Artikel</a> ausführlich beschrieben, warum die CPU und sogar die GPU nicht effizient genug sind, um mit neuronalen Netzen zu arbeiten, warum Milliarden von Dollar in die Entwicklung neuer Chips fließen und welche Aussichten bestehen.  Ich werde mich nicht wiederholen.  Nachfolgend finden Sie eine Verallgemeinerung und Ergänzung des vorherigen Textes. <br><br>  Zunächst müssen Sie die Unterschiede zwischen neuronalen Netzwerkberechnungen und Berechnungen in der bekannten von Neumann-Architektur verstehen (in der sie natürlich berechnet werden können, aber weniger effizient sind): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a1c/b6a/2f7/a1cb6a2f730e5d5f66a0e29b3fa7d1ac.png"><br>  <i>Quelle: Bild von Dmitry Konovalchuk, Materialien des Autors</i> <br><div class="scrollable-table"><table><tbody><tr><td>  <b>Von Neumann Architektur</b> <br></td><td>  <b>Neuronale Netze</b> <br></td></tr><tr><td>  Die meisten Berechnungen sind sequentielle Operationen. <br></td><td>  Massively Parallel Computing (Sie benötigen eine Architektur mit einer großen Anzahl von Rechenmodulen und einer Beschleunigung des Tensor Computing) <br></td></tr><tr><td>  Der Ablauf der Berechnungen ändert sich <br>  abhängig von den Bedingungen ( <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D1%2583%25D0%25BF%25D0%25B5%25D1%2580%25D1%2581%25D0%25BA%25D0%25B0%25D0%25BB%25D1%258F%25D1%2580%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D1%258C" rel="nofollow">Superskalarität</a> erforderlich) <br></td><td>  Die Rechenstruktur ist fast immer fest und im Voraus bekannt (Superskalarität ist ineffizient) <br></td></tr><tr><td>  Es gibt Lokalität nach den Daten (Cache funktioniert gut) <br></td><td>  Keine Datenlokalität (Cache erwärmt die Luft) <br></td></tr><tr><td>  Genaue Berechnungen <br></td><td>  Berechnungen sind möglicherweise nicht genau. <br></td></tr><tr><td>  Die Daten ändern sich für verschiedene Algorithmen unterschiedlich <br></td><td>  Dutzende von Megabyte Netzwerkkoeffizienten bleiben unverändert, wenn Daten wiederholt durch ein neuronales Netzwerk geleitet werden <br></td></tr></tbody></table></div><br>  Beim vorherigen Mal drehte sich die Hauptdiskussion um FPGA / ASIC, und ungenaue Berechnungen blieben fast unbemerkt. Lassen Sie uns daher näher darauf eingehen.  Die enormen Aussichten für die Reduzierung der Chips der nächsten Generationen liegen genau in der Fähigkeit, ungenau zu lesen (und Koeffizientendaten lokal zu speichern).  Tatsächlich wird die Vergröberung auch in der exakten Arithmetik verwendet, wenn die Netzwerkgewichte in ganze Zahlen umgewandelt und quantisiert werden, jedoch auf einer neuen Ebene.  Betrachten Sie als Beispiel einen Einzelbit-Addierer (das Beispiel ist ziemlich abstrakt): <br><br><img width="60%" src="https://habrastorage.org/getpro/habr/post_images/959/bbf/f16/959bbff160d7f0b33e4d46ce7a5be453.png"><br>  <i>Quelle: <a href="https://www.researchgate.net/publication/270898651_A_High_Speed_and_Low_Power_8_Bit_x_8_Bit_Multiplier_Design_using_Novel_Two_Transistor_2T_XOR_Gates" rel="nofollow">Ein 8-Bit-x-8-Bit-Multiplikator-Design mit hoher Geschwindigkeit und geringem Stromverbrauch unter Verwendung neuartiger 2-Transistor-XOR-Gatter</a></i> <br><br>  Er braucht 6 Transistoren (es gibt verschiedene Ansätze, die Anzahl der benötigten Transistoren kann immer geringer sein, aber im Allgemeinen ungefähr so).  Für 8 Bits sind ungefähr <a href="https://www.semanticscholar.org/paper/A-novel-approach-for-reversible-realization-of-with-Shukla-Singh/147db51cf0f054b260d980950c01146649483aa1" rel="nofollow">48 Transistoren</a> erforderlich.  In diesem Fall benötigt der Analogaddierer nur 2 (zwei!) Transistoren, d.h.  24 mal weniger: <br><br><img width="60%" src="https://habrastorage.org/getpro/habr/post_images/f34/85f/2a5/f3485f2a5ac6fa241d2e59cd80ed1064.png"><br>  <i>Quelle: <a href="http://www.iitk.ac.in/eclub/ee381/AnalogMultipliers.pdf" rel="nofollow">Analoge Multiplikatoren (Analyse und Entwurf analoger integrierter Schaltungen)</a></i> <br><br>  Wenn die Genauigkeit höher ist (z. B. entsprechend 10 oder 16 Bit), ist die Differenz noch größer.  Noch interessanter ist die Situation mit der Multiplikation!  Wenn ein digitaler 8-Bit-Multiplexer ungefähr <a href="https://www.semanticscholar.org/paper/A-novel-approach-for-reversible-realization-of-with-Shukla-Singh/147db51cf0f054b260d980950c01146649483aa1" rel="nofollow">400 Transistoren benötigt</a> , dann wird eine analoge 6, d.h.  67 mal (!) Weniger.  Natürlich unterscheiden sich "analoge" und "digitale" Transistoren vom Standpunkt der Schaltung her erheblich, aber die Idee ist klar: Wenn es uns gelingt, die Genauigkeit analoger Berechnungen zu erhöhen, erreichen wir leicht die Situation, in der wir zwei Größenordnungen weniger Transistoren benötigen.  Dabei geht es nicht so sehr um die Reduzierung der Größe (was im Zusammenhang mit der „Verlangsamung von Moores Gesetz“ wichtig ist), sondern um die Reduzierung des Stromverbrauchs, der für mobile Plattformen von entscheidender Bedeutung ist.  Und für Rechenzentren wird es nicht überflüssig sein. <br><br><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/077/0b2/42b/0770b242bb2693acbe79a2165a4e8c68.png"><br>  <i>Quelle: <a href="https://blocksandfiles.com/2019/02/11/ibms-ai-chips-change-phase/" rel="nofollow">IBM denkt an analoge Chips, um das maschinelle Lernen zu beschleunigen</a></i> <br><br>  Der Schlüssel zum Erfolg wird hier eine Verringerung der Genauigkeit sein, und auch hier steht IBM an vorderster Front: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a87/570/9f1/a875709f1c76de4e228a567f293a6618.png"><br>  <i>Quelle: <a href="https://www.ibm.com/blogs/research/2018/12/8-bit-precision-training/" rel="nofollow">IBM Research Blog: 8-Bit-Präzision für das Training von Deep Learning-Systemen</a></i> <br><br>  Sie beschäftigen sich bereits mit spezialisierten ASICs für neuronale Netze, die eine mehr als 10-fache Überlegenheit gegenüber der GPU aufweisen, und planen, in den kommenden Jahren eine 100-fache Überlegenheit zu erreichen.  Es sieht sehr ermutigend aus, wir freuen uns sehr darauf, denn ich wiederhole, dies wird ein Durchbruch für mobile Geräte sein. <br><br>  Bisher ist die Situation nicht so magisch, obwohl es ernsthafte Erfolge gibt.  Hier ist ein interessanter Test der aktuellen mobilen Hardwarebeschleuniger von neuronalen Netzen (das Bild ist anklickbar und das erwärmt die Seele des Autors erneut, auch in Bildern pro Sekunde): <br><br> <a href="" rel="nofollow"><img src="https://habrastorage.org/getpro/habr/post_images/175/fe0/10d/175fe010d6d9742ddabe6d20d50edb67.png"></a> <br>  <i>Quelle: <a href="https://www.groundai.com/project/ai-benchmark-all-about-deep-learning-on-smartphones-in-2019/1" rel="nofollow">Leistungsentwicklung mobiler AI-Beschleuniger: Bilddurchsatz für das Float-Inception-V3-Modell (FP16-Modell unter Verwendung von TensorFlow Lite und NNAPI)</a></i> <br><br>  Grün zeigt mobile Chips an, blau zeigt CPU an, orange zeigt GPU an.  Es ist deutlich zu sehen, dass die aktuellen mobilen Chips und vor allem der Top-End-Chip von Huawei bereits die zehnfach größere CPU (und den zehnfach größeren Stromverbrauch) überholen.  Und es ist stark!  Mit der GPU ist bisher alles nicht so magisch, aber es wird noch etwas anderes geben.  Sie können die Ergebnisse detaillierter auf einer separaten Website unter <a href="http://ai-benchmark.com/" rel="nofollow">http://ai-benchmark.com/</a> ansehen. <a href="http://ai-benchmark.com/" rel="nofollow">Beachten Sie</a> den dortigen Testabschnitt. Sie haben eine gute Reihe von Algorithmen zum Vergleich ausgewählt. <br><br>  <b>Insgesamt: Der Fortschritt analoger Beschleuniger ist heute recht schwer zu bewerten.</b>  <b>Es gibt ein Rennen.</b>  <b>Die Produkte sind jedoch noch nicht erschienen, so dass es <a href="https://www.google.com/search%3Fq%3Danalog%2Bdnn%2Baccelerator%2Bfiletype%253Apdf" rel="nofollow">relativ wenige</a> Veröffentlichungen gibt.</b>  <b>Sie können Patente überwachen, die verzögert angezeigt werden (z. B. dichter Datenfluss <a href="https://patents.google.com/%3Fq%3D%2522resistive%2Bprocessing%2Bunit%2522%26q%3DRPU%26oq%3D%2522resistive%2Bprocessing%2Bunit%2522%2BRPU" rel="nofollow">von IBM</a> ), oder <a href="https://patents.google.com/%3Fq%3Danalog%26q%3Ddnn%26q%3Daccelerator%26oq%3Danalog%2Bdnn%2Baccelerator" rel="nofollow">nach seltenen Patenten</a> anderer Hersteller suchen.</b>  <b>Es scheint, dass dies eine sehr ernste Revolution sein wird, vor allem bei Smartphones und Server-TPUs.</b> <br><br><h1>  Anstelle einer Schlussfolgerung </h1><br>  ML / DL heißt heute eine neue Programmiertechnologie, wenn wir kein Programm schreiben, sondern einen Block einfügen und trainieren.  Das heißt  Wie anfangs gab es einen Assembler, dann C, dann C ++, und jetzt, nach langen 30 Jahren des Wartens, ist der nächste Schritt ML / DL: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cfd/8ae/5f7/cfd8ae5f7236a0c9781199044966d2c5.png"></div><br>  Das macht Sinn.  In jüngster Zeit werden in fortgeschrittenen Unternehmen Entscheidungsorte in Programmen durch neuronale Netze ersetzt.  Das heißt      « IF-»            (!)         ,        3-5        .   ,   ,     .     ,  <s>,  </s> ,   ,  ,   ,      .   -! <br><br> ,   .  ,  -  ,  : « ,    !»     <s></s>      ,   , ,               ,       (, !) .          : «    , !»              ,     .           «»          «     !» (  ).      . ! <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Als mathematisches Werkzeug ist ML / DL im Allgemeinen und neuronale Netze im Besonderen jedoch eindeutig mehr als die nächste Programmiertechnologie. </font><font style="vertical-align: inherit;">Die gleichen neuronalen Netze werden jetzt einfach bei jedem Schritt gefunden:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Das Smartphone macht Bilder des Textes und erkennt ihn - das sind neuronale Netze, </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ein Smartphone lässt sich im Handumdrehen von einer Sprache in eine andere übersetzen und spricht eine Übersetzung - neuronale Netze und wiederum neuronale Netze. </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Der Navigator und der intelligente Sprecher erkennen die Sprache recht gut - wieder neuronale Netze, </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Der Fernseher zeigt ein helles Kontrastbild von 8K aus dem eingegebenen 2K-Video - ebenfalls ein neuronales Netzwerk. </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Roboter in der Produktion wurden genauer, sie begannen, abnormale Situationen besser zu sehen und zu erkennen - wieder neuronale Netze, </font></font><br></li><li>   10       ,       <s> </s>        ,    - 90- —   , <br></li><li>             —   , <br></li><li>    —               -   <a href="https://www.cgevent.ru/archives/28741" rel="nofollow">     </a> —    , <br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Im Allgemeinen - neuronale Netze sind jetzt absolut überall! </font></font> ) <br></li></ul><br><img width="60%" src="https://habrastorage.org/getpro/habr/post_images/f28/b47/999/f28b47999ec31c004bc0b917fed05633.png"><br><br>  Es sind nur 4 Jahre vergangen, seit Menschen dank BatchNorm (2015) und Skip Connections (2015) in vielerlei Hinsicht gelernt haben, wirklich tiefe neuronale Netze zu trainieren, und 3 Jahre sind vergangen, seit sie „abgehoben“ haben, und wir lesen wirklich die Ergebnisse ihrer Arbeit habe nicht gesehen.  Und jetzt werden sie die Produkte erreichen.  Etwas sagt uns, dass in den kommenden Jahren viele interessante Dinge auf uns warten.  Vor allem, wenn Beschleuniger "abheben" ... <br><br><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/a88/6b9/3dc/a886b93dc708002de50fac65c7d84a7a.png"><br><br>  Es war einmal, wenn sich jemand daran erinnert, dass Prometheus dem Olymp das Feuer gestohlen und es den Menschen übergeben hat.  Der zornige Zeus mit anderen Göttern schuf die erste Schönheit einer Frau namens Pandora, die mit vielen wundervollen weiblichen Eigenschaften ausgestattet war <s>(mir wurde plötzlich klar, dass die politisch korrekte Nacherzählung einiger Mythen des antiken Griechenlands äußerst schwierig ist)</s> .  Pandora wurde zu Leuten geschickt, aber Prometheus, der vermutete, dass etwas nicht stimmte, widerstand ihrem Zauber und sein Bruder Epimetheus nicht.  Als Geschenk für die Hochzeit sandte Zeus einen schönen Sarg mit Merkur und Merkur, eine gütige Seele, erfüllte den Befehl - er gab den Sarg Epimetheus, warnte ihn aber, ihn auf keinen Fall zu öffnen.  Die neugierige Pandora hat ihrem Mann den Sarg gestohlen und geöffnet, aber es gab nur Sünden, Krankheiten, Kriege und andere Probleme der Menschheit.  Sie versuchte den Sarg zu schließen, aber es war zu spät: <br><br><img src="https://habrastorage.org/webt/fi/hk/wh/fihkwhcd3uam5uyejc6thklshcm.png"><br>  <i>Quelle: <a href="https://regnum.ru/pictures/2414568/6.html" rel="nofollow">Künstler Frederick Stuart Church, Büchse der Pandora</a></i> <br><br>  Seitdem ist der Ausdruck "Öffne die Büchse der Pandora" verschwunden, das heißt, <s>aus Neugier</s> eine irreversible Handlung <s>auszuführen</s> , deren Folgen möglicherweise nicht so schön sind wie die Verzierungen des Sarges auf der Außenseite. <br><br>  Je tiefer ich in die neuronalen Netze eintauche, desto ausgeprägter ist das Gefühl, dass dies eine andere Büchse von Pandora ist.  Die Menschheit hat jedoch die größte Erfahrung mit dem Öffnen solcher Kisten!  Aus der jüngsten Vergangenheit - das ist Kernenergie und das Internet.  Ich denke also, wir können zusammen fertig werden.  Kein Wunder, dass ein Haufen rauer <s>Bärtiger</s> unter den Eröffnern ist.  Nun, ein Sarg ist wunderschön, stimme zu!  Und es ist nicht wahr, dass es nur Probleme gibt, sie haben bereits ein paar gute Dinge.  Deshalb kamen sie zusammen und ... wir öffnen weiter! <br><br>  <b>Gesamt:</b> <b><br><br></b> <ul><li>  <b>Der Artikel enthielt nicht viele interessante Themen, zum Beispiel klassische ML-Algorithmen, Transferlernen, Bestärkungslernen, die Beliebtheit von Datensätzen usw.</b>  <b>(Meine Herren, Sie können das Thema fortsetzen!)</b> <b><br></b> </li><li>  <b>Zur Frage zum Sarg: Ich persönlich finde <a href="https://habr.com/ru/post/411323/">die Google-Programmierer</a> , die es Google <a href="https://tproger.ru/news/google-drops-pentagon/" rel="nofollow">ermöglicht haben, den 10-Milliarden-Dollar-Pentagon-Vertrag aufzugeben,</a> großartig und ansehnlich.</b>  <b>Sie respektieren und respektieren.</b>  <b>Beachten Sie jedoch, dass jemand diese Hauptausschreibung gewonnen hat.</b> <b><br></b> </li></ul><br>  Lesen Sie auch: <br><br><ul><li>  <a href="https://habr.com/post/455353/">Hardwarebeschleunigung tiefer neuronaler Netze: GPU, FPGA, ASIC, TPU, VPU, IPU, DPU, NPU, RPU, NNP und andere Briefe</a> - der Autorentext über den aktuellen Stand und die Perspektiven der Hardwarebeschleunigung neuronaler Netze im Vergleich zu aktuellen Ansätzen. <br></li><li>  <a href="https://habr.com/post/480348/">Deep Fake Science, die Krise der Reproduzierbarkeit und woher leere Repositories kommen</a> - über die Probleme in der Wissenschaft, die durch ML / DL erzeugt werden. <br></li><li>  <a href="https://habr.com/post/451664/">Street Magic Codec Vergleich.</a>  <a href="https://habr.com/post/451664/">Wir enthüllen Geheimnisse</a> - ein Beispiel für eine Fälschung, die auf neuronalen Netzen basiert. <br></li></ul><br><h3>  Alles eine große Anzahl <i>neuer interessanter Entdeckungen</i> in den 2020er Jahren im Allgemeinen und im Neuen Jahr im Besonderen! </h3><br><h2>  Danksagung </h2><br>  Ich möchte mich herzlich bedanken bei: <br><br><ul><li>  Labor für Computergrafik und Multimedia VMK Moscow State University  M.V.  Lomonosov für seinen Beitrag zur Entwicklung des tiefen Lernens in Russland und nicht nur <br></li><li>  persönlich Konstantin Kozhemyakov und Dmitry Konovalchuk, die viel getan haben, um diesen Artikel besser und visueller zu machen, <br></li><li>  und zum Schluss vielen Dank an Kirill Malyshev, Jegor Sklyarov, Nikolai Oplachko, Andrej Moskalenko, Ivan Molodetsky, Evgeny Lyapustin, Roman Kazantsev, Alexander Yakovenko und Dmitry Klepikov für viele nützliche Kommentare und Korrekturen, die diesen Text viel besser gemacht haben! <br></li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de481844/">https://habr.com/ru/post/de481844/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de481828/index.html">Die Geschichte der Lernsoftware: Lernmanagementsysteme und der Aufstieg der Online-Bildung</a></li>
<li><a href="../de481836/index.html">Pizza as a Service: Wie Amazon zu Redshift migrierte</a></li>
<li><a href="../de481838/index.html">WireGuard, Einrichtung mehrerer Clients für NAT und wohin geht STUN?</a></li>
<li><a href="../de481840/index.html">Schützen Sie Ihre GraphQL-API vor Sicherheitslücken</a></li>
<li><a href="../de481842/index.html">Umstieg auf reinen Speicher: Unser neuer Speicher</a></li>
<li><a href="../de481846/index.html">Verwenden von GitHub CI für Elixir-Projekte</a></li>
<li><a href="../de481848/index.html">Erfahrene Mitarbeiterschulung</a></li>
<li><a href="../de481850/index.html">Die spanische Inquisition und der Roboter für Demütigung: Was sind die "räuberischen" Konferenzen um des Geldes willen?</a></li>
<li><a href="../de481852/index.html">3D Anet N4 Printer Review // Wie man einen Dark Souls Charakter realistisch färbt</a></li>
<li><a href="../de481854/index.html">Testen von Ideen durch Dashboard-Prototyping</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>