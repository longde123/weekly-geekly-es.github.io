<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üîÉ ü§ö üë®‚Äç‚öñÔ∏è Quel est le probl√®me avec l'apprentissage par renforcement? üë©üèø‚Äçüè≠ üë®üèø‚Äç‚úàÔ∏è üíÉ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="D√©but 2018, un article a √©t√© publi√©. L'apprentissage par renforcement profond ne fonctionne pas encore ("L'apprentissage par renforcement ne fonctionn...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Quel est le probl√®me avec l'apprentissage par renforcement?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/437020/"><p><img src="https://habrastorage.org/webt/hv/1l/vs/hv1lvsyszoctmnrbxex7valfo8a.jpeg"></p><br><p>  D√©but 2018, un article a √©t√© publi√©. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">L'apprentissage par renforcement profond ne fonctionne pas encore</a> ("L'apprentissage par renforcement ne fonctionne pas encore.").  La principale plainte √©tait que les algorithmes d'apprentissage modernes avec renforcement n√©cessitent environ le m√™me temps pour r√©soudre un probl√®me qu'une recherche al√©atoire r√©guli√®re. </p><br><p>  Depuis, quelque chose a-t-il chang√©?  Non. </p><br><p>  L'apprentissage renforc√© est consid√©r√© comme l'un des trois principaux moyens de construire une IA forte.  Mais les difficult√©s rencontr√©es dans ce domaine de l'apprentissage automatique et les m√©thodes que les scientifiques tentent de r√©soudre ces difficult√©s sugg√®rent qu'il peut y avoir des probl√®mes fondamentaux avec cette approche elle-m√™me. </p><a name="habracut"></a><br><h2 id="postoyte-chto-znachit-odin-iz-treh-a-ostalnye-dva-kakie">  Attendez, qu'est-ce que l'un des trois signifie?  Quels sont les deux autres? </h2><br><p>  Compte tenu du succ√®s des r√©seaux de neurones ces derni√®res ann√©es et de l'analyse de leur fonctionnement avec des capacit√©s cognitives de haut niveau, qui √©taient auparavant consid√©r√©es comme caract√©ristiques uniquement des humains et des animaux sup√©rieurs, aujourd'hui, dans la communaut√© scientifique, il existe une opinion selon laquelle il existe trois approches principales pour cr√©er une IA forte sur la base de r√©seaux de neurones, qui peuvent √™tre consid√©r√©s comme plus ou moins r√©alistes: </p><br><h2 id="1-obrabotka-tekstov">  1. Traitement de texte </h2><br><p>  Le monde a accumul√© un grand nombre de livres et de textes sur Internet, y compris des manuels et des livres de r√©f√©rence.  Le texte est pratique et rapide pour un traitement sur ordinateur.  Th√©oriquement, cet ensemble de textes devrait suffire √† former une IA conversationnelle solide. </p><br><p>  Il est sous-entendu que dans ces tableaux textuels, la structure compl√®te du monde se refl√®te (au moins, elle est d√©crite dans les manuels et ouvrages de r√©f√©rence).  Mais ce n'est pas du tout un fait.  Les textes en tant que forme de pr√©sentation de l'information sont fortement dissoci√©s du monde tridimensionnel r√©el et du cours du temps dans lequel nous vivons. </p><br><p>  De bons exemples d'IA form√©s aux tableaux de texte sont les robots de discussion et les traducteurs automatiques.  Puisque pour traduire le texte, vous devez comprendre le sens de la phrase et la redire dans de nouveaux mots (dans une autre langue).  Il existe une id√©e fausse commune selon laquelle les r√®gles de grammaire et de syntaxe, y compris une description de toutes les exceptions possibles, d√©crivent compl√®tement un langage particulier.  Ce n'est pas le cas.  La langue n'est qu'un outil auxiliaire dans la vie, elle change facilement et s'adapte √† de nouvelles situations. </p><br><p>  Le probl√®me avec le traitement de texte (m√™me par des syst√®mes experts, m√™me des r√©seaux de neurones) est qu'il <strong>n'y a pas d'</strong> ensemble de r√®gles, quelles phrases doivent √™tre appliqu√©es dans quelles situations.  Veuillez noter - pas les r√®gles de construction des phrases elles-m√™mes (ce que font la grammaire et la syntaxe), mais quelles phrases dans quelles situations.  Dans la m√™me situation, les gens prononcent des phrases dans diff√©rentes langues qui ne sont g√©n√©ralement pas li√©es les unes aux autres en termes de structure de la langue.  Comparez les phrases avec une extr√™me surprise: "oh mon dieu!"  et "√¥ merde!".  Eh bien, et comment faire une correspondance entre eux, en connaissant le mod√®le de langage?  Pas question.  C'est arriv√© par hasard historiquement.  Vous devez conna√Ætre la situation et ce qu'ils parlent habituellement dans une langue particuli√®re.  C'est √† cause de cela que les traducteurs automatiques sont si imparfaits. </p><br><p>  On ne sait pas si ces connaissances peuvent √™tre distingu√©es purement d'un ensemble de textes.  Mais si les traducteurs automatiques traduisent parfaitement sans faire d'erreurs idiotes et ridicules, alors ce sera la preuve que la cr√©ation d'une IA forte uniquement bas√©e sur du texte est possible. </p><br><h2 id="2-raspoznavanie-izobrazheniy">  2. Reconnaissance d'image </h2><br><p>  Regardez cette image </p><br><p><img src="https://habrastorage.org/webt/pa/od/nd/paodndrl6p5dkuhig3rwo68cu-q.jpeg"></p><br><p>  En regardant cette photo, nous comprenons que la prise de vue a √©t√© effectu√©e la nuit.  A en juger par les drapeaux, le vent souffle de droite √† gauche.  Et √† en juger par la circulation √† droite, l'affaire ne se produit pas en Angleterre ou en Australie.  Aucune de ces informations n'est indiqu√©e explicitement dans les pixels de l'image, il s'agit de connaissances externes.  Sur la photo, il n'y a que des signes par lesquels nous pouvons utiliser les connaissances obtenues d'autres sources. </p><br><div class="spoiler">  <b class="spoiler_title">Savez-vous autre chose en regardant cette photo?</b> <div class="spoiler_text"><p>  A propos de cela et du discours ... Et trouvez-vous enfin une fille </p></div></div><br><p>  Par cons√©quent, on pense que si vous entra√Ænez un r√©seau de neurones √† reconna√Ætre des objets dans une image, il aura alors une id√©e interne du fonctionnement du monde r√©el.  Et cette vue, obtenue √† partir des photographies, correspondra certainement √† notre monde r√©el et r√©el.  Contrairement aux tableaux de textes o√π cela n'est pas garanti. </p><br><p>  La valeur des r√©seaux de neurones form√©s sur un r√©seau de photographies ImageNet (et maintenant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OpenImages V4</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">COCO</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">KITTI</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">BDD100K</a> et autres) n'est pas du tout le fait de la reconnaissance d'un chat sur une photo.  Et cela est stock√© dans l'avant-derni√®re couche.  C'est l√† que se trouve un ensemble de fonctionnalit√©s de haut niveau qui d√©crivent notre monde.  Un vecteur de 1024 nombres suffit pour obtenir une description de 1000 cat√©gories d'objets diff√©rentes avec une pr√©cision de 80% (et dans 95% des cas, la bonne r√©ponse sera dans les 5 options les plus proches).  Pensez-y. </p><br><p>  C'est pourquoi ces fonctionnalit√©s de l'avant-derni√®re couche sont si bien utilis√©es dans des t√¢ches compl√®tement diff√©rentes en vision par ordinateur.  Gr√¢ce √† l'apprentissage par transfert et au r√©glage fin.  √Ä partir de ce vecteur en 1024 nombres, vous pouvez obtenir, par exemple, une carte de profondeur de l'image </p><br><p><img src="https://habrastorage.org/webt/vs/k6/lm/vsk6lmod2grqjzous7knxl5ekaq.jpeg"></p><br><p>  (un exemple du <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">travail</a> o√π un r√©seau Densenet-169 pr√©-form√© pratiquement inchang√© est utilis√©) </p><br><p>  Ou d√©terminez la pose d'une personne.  Il existe de nombreuses applications. </p><br><p><img src="https://habrastorage.org/webt/id/rs/sp/idrsspge5oaq0dae1-li5pghf3s.jpeg"></p><br><p>  En cons√©quence, la reconnaissance d'image peut potentiellement √™tre utilis√©e pour cr√©er une IA forte, car elle refl√®te vraiment le mod√®le de notre monde r√©el.  Une √©tape de la photographie √† la vid√©o, et la vid√©o est notre vie, car nous obtenons environ 99% des informations visuellement. </p><br><p>  Mais √† partir de la photographie, il est compl√®tement incompr√©hensible de motiver le r√©seau neuronal √† penser et √† tirer des conclusions.  Elle peut √™tre form√©e pour r√©pondre √† des questions comme "combien de crayons sont sur la table?"  (cette classe de t√¢ches s'appelle Visual Question Answering, un exemple d'un tel ensemble de donn√©es: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://visualqa.org</a> ).  Ou donnez une description textuelle de ce qui se passe sur la photo.  Il s'agit de la classe de t√¢ches de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sous-titrage d'image</a> . </p><br><p><img src="https://habrastorage.org/webt/mp/lz/0y/mplz0y9uleukwz68u-lyc35wlqk.jpeg"></p><br><p>  Mais est-ce cette intelligence?  Ayant d√©velopp√© cette approche, dans un avenir proche, les r√©seaux de neurones pourront r√©pondre √† des questions vid√©o telles que "Deux moineaux assis sur les fils, l'un d'eux s'est envol√©, combien de moineaux restaient?".  Il s'agit de v√©ritables math√©matiques, dans des cas un peu plus compliqu√©s, inaccessibles aux animaux et au niveau de l'√©ducation scolaire humaine.  Surtout si, √† l'exception des moineaux, il y aura des seins assis √† c√¥t√© d'eux, mais ils n'ont pas besoin d'√™tre pris en compte, car la question ne concernait que les moineaux.  Oui, ce sera certainement de l'intelligence. </p><br><h2 id="3-obuchenie-s-podkrepleniem-reinforcement-learning">  3. Apprentissage par renforcement </h2><br><p>  L'id√©e est tr√®s simple: encourager les actions menant √† la r√©compense et √©viter de conduire √† l'√©chec.  Il s'agit d'un moyen d'apprentissage universel et, bien s√ªr, il peut certainement conduire √† la cr√©ation d'une IA forte.  Par cons√©quent, il y a eu tellement d'int√©r√™t pour l'apprentissage par renforcement ces derni√®res ann√©es. </p><br><div class="spoiler">  <b class="spoiler_title">M√©langez mais ne secouez pas</b> <div class="spoiler_text"><p>  Bien s√ªr, il est pr√©f√©rable de cr√©er une IA forte en combinant les trois approches.  En images et avec une formation de renforcement, vous pouvez obtenir une IA de niveau animal.  Et en ajoutant des noms textuels d'objets aux images (une blague, bien s√ªr - for√ßant l'IA √† regarder des vid√©os o√π les gens interagissent et parlent, comme lors de l'enseignement √† un b√©b√©), et se recycler sur un tableau de texte pour acqu√©rir des connaissances (un analogue de notre √©cole et universit√©), en th√©orie, vous pouvez obtenir IA au niveau humain.  Capable de parler. </p></div></div><br><p>  L'apprentissage renforc√© a un gros plus.  Dans le simulateur, vous pouvez cr√©er un mod√®le simplifi√© du monde.  Ainsi, pour une figure humaine, seulement 17 degr√©s de libert√© suffisent, au lieu de 700 chez une personne vivante (nombre approximatif de muscles).  Par cons√©quent, dans le simulateur, vous pouvez r√©soudre le probl√®me dans une tr√®s petite dimension. </p><br><p>  √Ä l'avenir, les algorithmes modernes d'apprentissage par renforcement ne sont pas en mesure de contr√¥ler arbitrairement le mod√®le d'une personne, m√™me avec 17 degr√©s de libert√©.  Autrement dit, ils ne peuvent pas r√©soudre le probl√®me d'optimisation, o√π il y a 44 nombres √† l'entr√©e et 17 √† l'entr√©e. Il n'est possible de le faire que dans des cas tr√®s simples, avec un r√©glage fin des conditions initiales et des hyperparam√®tres.  Et m√™me dans ce cas, par exemple, pour enseigner un mod√®le humano√Øde avec 17 degr√©s de libert√© pour fonctionner, et √† partir d'une position debout (ce qui est beaucoup plus simple), vous avez besoin de plusieurs jours de calculs sur un GPU puissant.  Et des cas un peu plus compliqu√©s, par exemple, apprendre √† se lever d'une pose arbitraire, peuvent ne jamais apprendre du tout.  C'est un √©chec. </p><br><p>  En outre, tous les algorithmes d'apprentissage par renforcement fonctionnent avec des r√©seaux de neurones d√©primants, mais ils ne peuvent pas faire face √† l'apprentissage de grands r√©seaux.  Les grands r√©seaux de convolution ne sont utilis√©s que pour r√©duire la dimension de l'image √† plusieurs fonctionnalit√©s, qui sont transmises aux algorithmes d'apprentissage avec renforcement.  Le m√™me humano√Øde en marche est contr√¥l√© par un r√©seau Feed Forward avec deux ou trois couches de 128 neurones.  Vraiment?  Et sur cette base, essayons-nous de construire une IA forte? </p><br><p>  Pour essayer de comprendre pourquoi cela se produit et ce qui ne va pas avec l'apprentissage par renforcement, vous devez d'abord vous familiariser avec les architectures de base de l'apprentissage par renforcement moderne. </p><br><p>  La structure physique du cerveau et du syst√®me nerveux est adapt√©e par l'√©volution au type sp√©cifique d'animal et √† ses conditions de vie.  Ainsi, au cours de l'√©volution, une mouche a d√©velopp√© un tel syst√®me nerveux et un tel travail de neurotransmetteurs dans les ganglions (un analogue du cerveau chez les insectes) pour esquiver rapidement une tapette √† mouches.  Eh bien, non pas d'une tapette √† mouches, mais d'oiseaux qui ont p√™ch√© pendant 400 millions d'ann√©es (je plaisante, les oiseaux eux-m√™mes sont apparus il y a 150 millions d'ann√©es, tr√®s probablement √† partir de grenouilles 360 millions d'ann√©es).  Un rhinoc√©ros assez tel un syst√®me nerveux et un cerveau pour se tourner lentement vers la cible et commencer √† courir.  Et l√†, comme on dit, le rhinoc√©ros a une mauvaise vue, mais ce n'est pas son probl√®me. </p><br><p>  Mais en plus de l'√©volution, chaque individu sp√©cifique, √† partir de la naissance et tout au long de la vie, travaille pr√©cis√©ment le m√©canisme d'apprentissage habituel avec renforcement.  Dans le cas des mammif√®res <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">et des insectes aussi</a> , le syst√®me de dopamine fait ce travail.  Son travail est plein de secrets et de nuances, mais tout se r√©sume au fait qu'en cas de r√©compense, le syst√®me dopaminergique, gr√¢ce √† des m√©canismes de m√©moire, fixe en quelque sorte les connexions entre les neurones qui √©taient actifs imm√©diatement avant.  C'est ainsi que se forme la m√©moire associative. </p><br><p>  Qui, en raison de son associativit√©, est ensuite utilis√© dans la prise de d√©cision.  Autrement dit, si la situation actuelle (les neurones actifs actuels dans cette situation) √† travers la m√©moire associative activent les neurones de plaisir, alors l'individu s√©lectionne les actions qu'elle a faites dans une situation similaire et dont elle se souvient.  ¬´Choisir des actions¬ª est une mauvaise d√©finition.  Il n'y a pas d'autre choix.  Les neurones de m√©moire du plaisir simplement activ√©s, fix√©s par le syst√®me dopaminergique pour une situation donn√©e, activent automatiquement les motoneurones, entra√Ænant une contraction musculaire.  C'est si une action imm√©diate est n√©cessaire. </p><br><p>  L'apprentissage artificiel avec renforcement, en tant que domaine de connaissance, il est n√©cessaire de r√©soudre ces deux probl√®mes: </p><br><h3 id="1-podobrat-arhitekturu-neyroseti-chto-dlya-nas-uzhe-sdelala-evolyuciya">  1. Choisir l'architecture du r√©seau neuronal (ce que l'√©volution a d√©j√† fait pour nous) </h3><br><p>  La bonne nouvelle est que les fonctions cognitives sup√©rieures r√©alis√©es dans le n√©ocortex chez les mammif√®res (et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dans le striatum chez les corvid√©s</a> ) sont r√©alis√©es dans une structure approximativement uniforme.  Apparemment, cela n'a pas besoin d'une "architecture" rigoureusement prescrite. </p><br><p>  La diversit√© des r√©gions c√©r√©brales est probablement due √† des raisons purement historiques.  Lorsque, au fur et √† mesure de leur √©volution, de nouvelles parties du cerveau se sont d√©velopp√©es au-dessus de celles de base laiss√©es par les tout premiers animaux.  Par le principe, cela fonctionne - ne touchez pas.  D'un autre c√¥t√©, chez diff√©rentes personnes, les m√™mes parties du cerveau r√©agissent aux m√™mes situations.  Cela peut s'expliquer √† la fois par l'associativit√© (caract√©ristiques et ¬´neurones de grand-m√®re¬ª naturellement form√©s √† ces endroits au cours du processus d'apprentissage) et la physiologie.  Que les voies de signalisation cod√©es dans les g√®nes conduisent pr√©cis√©ment √† ces zones.  Il n'y a pas de consensus, mais vous pouvez lire, par exemple, cet article r√©cent: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">"Intelligence biologique et artificielle"</a> . </p><br><h3 id="2-nauchitsya-obuchat-neyronnye-seti-po-principam-obucheniya-s-podkrepleniem">  2. Apprenez √† former des r√©seaux de neurones selon les principes de l'apprentissage avec renforcement </h3><br><p>  C'est principalement ce que fait l'apprentissage par renforcement moderne.  Et quels sont les succ√®s?  Pas vraiment. </p><br><h1 id="naivnyy-podhod">  Approche na√Øve </h1><br><p>  Il semblerait qu'il soit tr√®s simple de former un r√©seau de neurones avec renforcement: nous faisons des actions al√©atoires, et si nous obtenons une r√©compense, alors nous consid√©rons les actions prises comme ¬´r√©f√©rence¬ª.  Nous les mettons sur la sortie du r√©seau neuronal en tant qu'√©tiquettes standard et entra√Ænons le r√©seau neuronal par la m√©thode de la propagation arri√®re de l'erreur, afin qu'il produise exactement une telle sortie.  Eh bien, la formation de r√©seau neuronal la plus courante.  Et si les actions ont conduit √† un √©chec, alors ignorez ce cas ou supprimez ces actions (nous en d√©finissons d'autres comme sortie, par exemple, toute autre action al√©atoire).  En g√©n√©ral, cette id√©e r√©p√®te le syst√®me dopaminergique. </p><br><p>  Mais si vous essayez de former n'importe quel r√©seau de neurones de cette mani√®re, quelle que soit la complexit√© de l'architecture, la r√©cursivit√©, la convolution ou la distribution directe ordinaire, alors ... Cela ne fonctionnera pas! </p><br><p>  Pourquoi?  Inconnu </p><br><p>  On pense que le signal utile est si petit qu'il est perdu sur le fond du bruit.  Par cons√©quent, le r√©seau n'apprend pas la m√©thode standard de propagation de l'erreur.  Une r√©compense arrive tr√®s rarement, peut-√™tre une fois en centaines ou m√™me en milliers d'√©tapes.  Et m√™me LSTM se souvient d'un maximum de 100-500 points dans l'histoire, et seulement dans des t√¢ches tr√®s simples.  Mais sur les plus complexes, s'il y a 10-20 points dans l'histoire, alors c'est d√©j√† bien. </p><br><p>  Mais la racine du probl√®me r√©side pr√©cis√©ment dans les r√©compenses tr√®s rares (au moins dans les t√¢ches de valeur pratique).  Pour le moment, nous ne savons pas comment former des r√©seaux de neurones qui se souviendraient de cas isol√©s.  Ce que le cerveau fait avec brio.  Vous vous souvenez de quelque chose qui ne s'est produit qu'une seule fois dans la vie.  Et en passant, la plupart de la formation et du travail de l'intellect sont construits sur de tels cas. </p><br><p>  Cela ressemble √† un terrible d√©s√©quilibre des classes dans le domaine de la reconnaissance d'image.  Il n'y a tout simplement aucun moyen de r√©soudre ce probl√®me.  Le mieux qu'ils ont pu trouver jusqu'√† pr√©sent est simplement de soumettre √† l'entr√©e du r√©seau, avec de nouvelles situations, des situations r√©ussies du pass√© stock√©es dans un tampon sp√©cial artificiel.  Autrement dit, pour enseigner constamment non seulement les nouveaux cas, mais aussi les anciens r√©ussis.  Naturellement, un tel tampon ne peut pas √™tre augment√© √† l'infini, et on ne sait pas exactement quoi y stocker.  J'essaie toujours de corriger temporairement les chemins √† l'int√©rieur du r√©seau neuronal, qui √©taient actifs lors d'un cas r√©ussi, afin que la formation ult√©rieure ne les √©crase pas.  Une analogie assez proche de ce qui se passe dans le cerveau, √† mon avis, m√™me s'ils n'ont pas encore beaucoup de succ√®s dans cette direction.  √âtant donn√© que les nouvelles t√¢ches entra√Æn√©es dans leur calcul utilisent les r√©sultats des neurones quittant les chemins gel√©s, en cons√©quence, le signal n'interf√®re qu'avec les nouvelles gel√©es et les anciennes t√¢ches cessent de fonctionner.  Il existe une autre approche curieuse: former le r√©seau avec de nouveaux exemples / t√¢ches uniquement dans le sens orthogonal aux t√¢ches pr√©c√©dentes ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://arxiv.org/abs/1810.01256</a> ).  Cela n'√©crase pas l'exp√©rience pr√©c√©dente, mais limite consid√©rablement la capacit√© du r√©seau. </p><br><p>  Une classe distincte d'algorithmes con√ßus pour faire face √† cette catastrophe (et en m√™me temps donner l'espoir de parvenir √† une IA forte) est en cours de d√©veloppement dans Meta-Learning.  Ce sont des tentatives pour enseigner √† un r√©seau de neurones plusieurs t√¢ches √† la fois.  Pas dans le sens o√π il reconna√Æt diff√©rentes images dans une m√™me t√¢che, √† savoir diff√©rentes t√¢ches dans diff√©rents domaines (chacune avec sa propre distribution et son propre paysage de solutions).  Dites, reconnaissez les images et faites du v√©lo en m√™me temps.  Jusqu'√† pr√©sent, le succ√®s n'est pas tr√®s bon non plus, car il s'agit g√©n√©ralement de pr√©parer un r√©seau de neurones √† l'avance avec des poids universels g√©n√©raux, puis rapidement, en quelques √©tapes de descente de gradient, de les adapter √† une t√¢che sp√©cifique.  Des exemples d'algorithmes de m√©ta-apprentissage sont MAML et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Reptile</a> . </p><br><p>  En g√©n√©ral, seul ce probl√®me (l'incapacit√© d'apprendre √† partir d'exemples r√©ussis) met un terme √† la formation moderne avec renforcement.  Toute la puissance des r√©seaux de neurones devant ce triste fait est jusqu'√† pr√©sent impuissante. </p><br><p>  Ce fait, que la mani√®re la plus simple et la plus √©vidente ne fonctionne pas, a forc√© les chercheurs √† revenir √† l'apprentissage par renforcement classique bas√© sur une table.  Ce qui, en tant que science, est apparu dans l'antiquit√©, alors que les r√©seaux de neurones n'√©taient m√™me pas dans le projet.  Mais maintenant, au lieu de calculer manuellement les valeurs dans les tableaux et les formules, utilisons un approximateur aussi puissant que les r√©seaux de neurones comme fonctions objectives!  C'est l'essence m√™me de l'apprentissage par renforcement moderne.  Et sa principale diff√©rence avec la formation habituelle des r√©seaux de neurones. </p><br><h1 id="q-learning-i-dqn">  Q-learning et DQN </h1><br><p>  L'apprentissage par renforcement (avant m√™me les r√©seaux de neurones) est n√© comme une id√©e assez simple et originale: faisons des actions al√©atoires, puis pour chaque cellule du tableau et chaque direction de mouvement, nous calculons selon une formule sp√©ciale (appel√©e l'√©quation de Bellman, ce mot vous √† rencontrer dans presque tous les travaux avec une formation de renforcement) √† quel point cette cellule et la direction choisie sont bonnes.  Plus ce nombre est √©lev√©, plus ce chemin m√®ne √† la victoire. </p><br><p><img src="https://habrastorage.org/webt/nx/zm/-7/nxzm-7q1_oc-igaim3j0mrr7vki.png"></p><br><p>  Quelle que soit la cellule dans laquelle vous apparaissez, d√©placez-vous dans le vert en pleine croissance!  (vers le nombre maximum sur les c√¥t√©s de la cellule actuelle). </p><br><p>  Ce nombre est appel√© Q (du mot qualit√© - qualit√© de choix, √©videmment), et la m√©thode est Q-learning.  Rempla√ßant la formule de calcul de ce nombre par un r√©seau neuronal, ou plut√¥t enseignant le r√©seau neuronal √† l'aide de cette formule (plus quelques astuces li√©es uniquement aux math√©matiques de la formation des r√©seaux neuronaux), Deepmind a obtenu la m√©thode <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">DQN</a> .  C'est qui, en 2015, a remport√© le tas de jeux Atari et a inaugur√© une r√©volution dans l'apprentissage par renforcement profond. </p><br><p>  Malheureusement, cette m√©thode dans son architecture ne fonctionne qu'avec des actions discr√®tes discr√®tes.  Dans le DQN, l'√©tat actuel (la situation actuelle) est transmis √† l'entr√©e du r√©seau neuronal, et √† la sortie, le r√©seau neuronal pr√©dit le nombre Q.Et comme la sortie du r√©seau r√©pertorie toutes les actions possibles √† la fois (chacune avec son propre Q pr√©dit), il s'av√®re que le r√©seau neuronal dans DQN impl√©mente la fonction classique Q (s, a) de Q-learning.  Q  state  action (  Q(s,a)    s  a).     argmax          Q   ,     . </p><br><p>        Q,      .        ,    Q- (..    Q   ,   ).    .      ,        (Exploration),       ,     ,        .         ,        . </p><br><p>   ,    ?    5     Atari,  continuous    ? ,    -1..1      0.1,          ,     Atari.         . ,        .       10    .  -      ,    10       .     .   DQN     ,      17     .  ,    ,  . </p><br><p>             DQN, ,   ,   continuous  (      ): DDQN, DuDQN, BDQN, CDQN, NAF, Rainbow. ,     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Direct Future Prediction (DFP)</a> ,    DQN     .    Q   , DFP          ,    .          .                     ,     . ,   ,       ,     . </p><br><p>    ,          Reinforcement Learning. </p><br><p><img src="https://habrastorage.org/webt/f3/lc/3t/f3lc3tno4mpvwren4rfocva9iv8.png"></p><br><h1 id="policy-gradient"> Policy Gradient </h1><br><p>       state,       (  ,        ).   ,  actions,  .   ,   R   .        (   ),   (  ).        .     . </p><br><p> ,    R   ,   ,        .       !       .   ""       labels (       ),      .     ,   ,      R. </p><br><p>   Policy Gradient.      ‚Äî    ,     R,        .    ‚Äî     ,       ,      .     ,    . </p><br><h1 id="actor-critic-ddpg"> Actor-critic, DDPG </h1><br><p>   ,       ‚Äî      ,       .  ,  Q-   ,    DQN.      state,    action(s).       state,     action,   ,      Q     : Q(s,a). </p><br><p> ,   Q(s,a),    (  critic, ),       ,      (  , actor),       R.       ,    .     actor-critic.       Policy Gradient,        ,    .   . </p><br><p>      DDPG.       actions,     continuous . DDPG   continuous  DQN    . </p><br><p><img src="https://habrastorage.org/webt/9b/th/fk/9bthfkh7cfpymc6_f6xrt7sica0.png"></p><br><h1 id="advantage-actor-critic-a3ca2c"> Advantage Actor Critic (A3C/A2C) </h1><br><p>             critic  Q(s,a) ‚Äî   ,   actor,     DDPG.         ,   . </p><br><p>     ,     .   ,           ,    <strong></strong> ,    . ,    ,    ,      (     ,   ). </p><br><p>          Q(s,a),    Advantage: A(s,a) = Q(s,a) ‚Äî V(s).  A(s,a)     Q(s,a)  ,    ‚Äî      ,    V(s).  A(s,a) &gt; 0,      ,    .  A(s,a) &lt; 0,      ,     , ..   . </p><br><p>    V(s)     state   ,     (    s,  a).         ‚Äî     state,   V(s).       ,      state,   V(s). </p><br><p>  ,    Q(s,a)     r,     ,         A = r ‚Äî V(s). </p><br><p>   ,    V(s) (          ),    ‚Äî actor  critic,    !     state,        head:    actions,    V(s).     c , ..       state. ,      . </p><br><p><img src="https://habrastorage.org/webt/eo/ph/5y/eoph5ypzawg11tachwn-nt_7nyg.png"></p><br><p>        V(s)      .     V(s),          action (     ),      .    Dueling Q-Network (DuDQN),  Q(s,a)      Q(s,a) = V(s) + A(a),    . </p><br><p> Asynchronous Advantage Actor Critic (A3C)   ,   ,     actor.        batch  .  ,     actor.     ,     ,   .   ,   A2C ‚Äî   A3C,         actor       ( ). A2C    ,    ,     . </p><br><h1 id="trpo-ppo-sac"> TRPO, PPO, SAC </h1><br><p> ,    . </p><br><p>        ,     .   Reinforcement Learning     ,      ,   ,          ‚Äî      ,  .   . </p><br><p>   ‚Äî TRPO  PPO,   state-of-the-art,   Actor-Critic.  PPO         RL.  ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OpenAI Five</a>    Dota 2. </p><br><p>   ,       TRPO  PPO ‚Äî        ,     .   ,   A3C/A2C   ,    .  ,   policy     ,     . -  gradient clipping        ,     .   ,         (       ,      ),      ,   ,    -  . </p><br><p> R√©cemment, l'algorithme Soft-Actor-Critic (SAC) a gagn√© en popularit√©.  Il n'est pas tr√®s diff√©rent de PPO, seul un objectif a √©t√© ajout√© lors de l'apprentissage de l'augmentation de l'entropie dans la politique.  Rendre le comportement des agents plus al√©atoire.  Non, pas comme √ßa.  Que l'agent a pu agir dans des situations plus al√©atoires.  Cela augmente automatiquement la fiabilit√© de la politique, une fois que l'agent est pr√™t pour toutes les situations al√©atoires.  De plus, le SAC n√©cessite un peu moins d'exemples de formation que PPO et est moins sensible aux param√®tres d'hyperparam√®tre, ce qui est √©galement un plus.  Cependant, m√™me avec SAC, pour former un humano√Øde √† courir avec 17 degr√©s de libert√©, √† partir d'une position debout, vous avez besoin d'environ 20 millions d'images et d'environ une journ√©e de calcul sur un GPU.  Des conditions initiales plus difficiles, par exemple, pour apprendre √† un humano√Øde √† se lever d'une pose arbitraire, peuvent ne pas √™tre enseign√©es du tout. </p><br><p>  Total, la recommandation g√©n√©rale de l'apprentissage par renforcement moderne: utilisez SAC, PPO, DDPG, DQN (dans cet ordre, d√©croissant). </p><br><h1 id="model-based">  Bas√© sur un mod√®le </h1><br><p>  Il existe une autre approche int√©ressante, indirectement li√©e √† l'apprentissage par renforcement.  Il s'agit de construire un mod√®le de l'environnement et de l'utiliser pour pr√©dire ce qui se passera si nous prenons des mesures. </p><br><p>  Son inconv√©nient est qu'il ne dit en aucune mani√®re quelles mesures doivent √™tre prises.  Seulement sur leur r√©sultat.  Mais un tel r√©seau de neurones est facile √† former - il suffit de s'entra√Æner sur toutes les statistiques.  Il se r√©v√®le quelque chose comme un simulateur mondial bas√© sur un r√©seau de neurones. </p><br><p>  Apr√®s cela, nous g√©n√©rons un grand nombre d'actions al√©atoires, et chacune est conduite via ce simulateur (via un r√©seau de neurones).  Et nous regardons lequel apportera la r√©compense maximale.  Il y a une petite optimisation - pour g√©n√©rer non seulement des actions al√©atoires, mais s'√©cartant selon la loi normale de la trajectoire actuelle.  Et en effet, si nous levons la main, alors avec une forte probabilit√©, nous devons continuer √† la lever.  Par cons√©quent, vous devez d'abord v√©rifier les √©carts minimaux par rapport √† la trajectoire actuelle. </p><br><p>  L'astuce ici est que m√™me un simulateur physique primitif comme MuJoCo ou pyBullet produit environ 200 FPS.  Et si vous entra√Ænez un r√©seau neuronal √† pr√©dire en avant au moins quelques √©tapes, alors pour des environnements simples, vous pouvez facilement obtenir des lots de pr√©dictions de 2000 √† 5000 √† la fois.  Selon la puissance du GPU, vous pouvez obtenir une pr√©vision pour des dizaines de milliers d'actions al√©atoires par seconde en raison de la parall√©lisation dans le GPU et de la vitesse de calcul dans le r√©seau neuronal.  Le r√©seau neuronal agit ici simplement comme un simulateur tr√®s rapide de la r√©alit√©. </p><br><p>  De plus, √©tant donn√© que le r√©seau neuronal peut pr√©dire le monde r√©el (il s'agit d'une approche bas√©e sur un mod√®le, au sens g√©n√©ral), l'entra√Ænement peut √™tre effectu√© enti√®rement en imagination, pour ainsi dire.  Ce concept dans l'apprentissage par renforcement s'appelle Dream Worlds, ou World Models.  Cela fonctionne bien, une bonne description est ici: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://worldmodels.github.io</a> .  De plus, il a une contrepartie naturelle - des r√™ves ordinaires.  Et le d√©filement multiple des √©v√©nements r√©cents ou pr√©vus dans la t√™te. </p><br><h1 id="imitation-learning">  Apprentissage par imitation </h1><br><p>  En raison de l'impuissance que les algorithmes d'apprentissage par renforcement ne fonctionnent pas sur les grandes dimensions et les t√¢ches complexes, les gens se sont efforc√©s de r√©p√©ter au moins les actions des experts sous forme de personnes.  Ici, de bons r√©sultats ont √©t√© obtenus (inaccessibles par l'apprentissage par renforcement conventionnel).  Ainsi, OpenAI s'est av√©r√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">r√©ussir le jeu Montezuma's Revenge</a> .  L'astuce s'est av√©r√©e simple: placer l'agent imm√©diatement √† la fin de la partie (√† la fin de la trajectoire indiqu√©e par la personne).  L√†, avec l'aide de PPO, gr√¢ce √† la proximit√© de la r√©compense finale, l'agent apprend rapidement √† marcher le long de la trajectoire.  Apr√®s cela, nous le remettons un peu en arri√®re, o√π il apprend rapidement √† atteindre le lieu qu'il a d√©j√† √©tudi√©.  Ainsi, en d√©pla√ßant progressivement le point de "r√©apparition" le long de la trajectoire jusqu'au tout d√©but du jeu, l'agent apprend √† passer / simuler la trajectoire experte tout au long du jeu. </p><br><p>  Un autre r√©sultat impressionnant est la r√©p√©tition des mouvements pour les personnes film√©es sur Motion Capture: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">DeepMimic</a> .  La recette est similaire √† la m√©thode OpenAI: chaque √©pisode ne commence pas au d√©but du chemin, mais √† partir d'un point al√©atoire le long du chemin.  PPO √©tudie ensuite avec succ√®s les environs de ce point. </p><br><p>  Je dois dire que l'algorithme sensationnel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Go-Explore</a> d'Uber, qui a d√©pass√© Montezuma's Revenge avec des points d'enregistrement, n'est pas du tout un algorithme d'apprentissage par renforcement.  Il s'agit d'une recherche al√©atoire r√©guli√®re, mais en commen√ßant par une cellule de cellule visit√©e au hasard (une cellule grossi√®re dans laquelle plusieurs √©tats tombent).  Et ce n'est que lorsque la trajectoire jusqu'√† la fin du jeu est trouv√©e par une telle recherche al√©atoire, que le r√©seau neuronal est entra√Æn√© √† l'aide de Imitation Learning.  D'une mani√®re similaire √† OpenAI, c'est-√†-dire  √† partir de la fin de la trajectoire. </p><br><h1 id="curiosity-lyubopytstvo">  Curiosit√© (Curiosit√©) </h1><br><p>  Un concept tr√®s important dans l'apprentissage par renforcement est la curiosit√©.  Dans la nature, c'est un moteur de recherche environnementale. </p><br><p>  Le probl√®me est que, par mesure de curiosit√©, vous ne pouvez pas utiliser une simple erreur de pr√©diction de r√©seau, ce qui se passera ensuite.  Sinon, un tel r√©seau sera suspendu devant le premier arbre au feuillage oscillant.  Ou devant un t√©l√©viseur avec commutation al√©atoire des canaux.  Car le r√©sultat d√ª √† la complexit√© sera impossible √† pr√©voir et l'erreur sera toujours importante.  Cependant, c'est pr√©cis√©ment la raison pour laquelle nous (les gens) aimons regarder le feuillage, l'eau et le feu.  Et comment les autres travaillent =).  Mais nous avons des m√©canismes de protection pour ne pas pendre pour toujours. </p><br><p>  L'un de ces m√©canismes a √©t√© invent√© comme le mod√®le inverse dans l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">exploration ax√©e</a> sur la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">curiosit√© par</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><br></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Pr√©diction auto-supervis√©e</a> .  En bref, un agent (r√©seau de neurones), en plus de pr√©dire quelles actions sont les mieux ex√©cut√©es dans une situation donn√©e, essaie √©galement de pr√©dire ce qui va arriver au monde apr√®s les actions prises.  Et il utilise cette pr√©diction du monde pour l'√©tape suivante, afin que lui et l'√©tape actuelle puissent pr√©dire ses actions prises plus t√¥t (oui, c'est difficile, vous ne pouvez pas le comprendre sans une pinte). </p><br><p>  Cela conduit √† un effet curieux: l'agent ne devient curieux que de ce qu'il peut influencer par ses actions.  Il ne peut pas influencer les branches d'un arbre qui se balancent, elles deviennent donc sans int√©r√™t pour lui.  Mais il peut se promener dans le quartier, il est donc curieux de marcher et d'explorer le monde. </p><br><p>  Cependant, si l'agent dispose d'une t√©l√©commande TV qui commute des canaux al√©atoires, il peut l'affecter!  Et il sera curieux de cliquer sur les canaux √† l'infini (puisqu'il ne peut pas pr√©dire quel sera le prochain canal, car il est al√©atoire).  Google a tent√© de contourner ce probl√®me dans le cadre de son travail sur la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">curiosit√© √©pisodique gr√¢ce √† l'accessibilit√©</a> . </p><br><p>  Mais peut-√™tre que le meilleur r√©sultat de pointe est d√ª √† la curiosit√©, OpenAI est actuellement propri√©taire de l'id√©e de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">distillation en r√©seau al√©atoire (RND)</a> .  Son essence est qu'il faut un deuxi√®me r√©seau, compl√®tement initialis√© de mani√®re al√©atoire, et que l'√©tat actuel lui est transmis.  Et notre principal r√©seau de neurones actif essaie de deviner la sortie de ce r√©seau de neurones.  Le deuxi√®me r√©seau n'est pas form√©, il reste fixe tout le temps comme il a √©t√© initialis√©. </p><br><p>  √Ä quoi √ßa sert?  Le fait est que si un √©tat a d√©j√† √©t√© visit√© et √©tudi√© par notre r√©seau de travail, il pourra plus ou moins r√©ussir √† pr√©dire la sortie de ce deuxi√®me r√©seau.  Et s'il s'agit d'un nouvel √©tat, o√π nous n'avons jamais √©t√©, alors notre r√©seau de neurones ne pourra pas pr√©dire la sortie de ce r√©seau RND.  Cette erreur de pr√©diction de la sortie de ce r√©seau initialis√© au hasard est utilis√©e comme un indicateur de curiosit√© (elle donne des r√©compenses √©lev√©es si nous ne pouvons pas pr√©dire sa sortie dans cette situation). </p><br><p>  Pourquoi cela fonctionne n'est pas enti√®rement clair.  Mais ils √©crivent que cela √©limine le probl√®me lorsque la cible de pr√©diction est stochastique et lorsqu'il n'y a pas suffisamment de donn√©es pour faire une pr√©diction de ce qui se passera ensuite (ce qui donne une grosse erreur de pr√©diction dans les algorithmes de curiosit√© ordinaires).  D'une mani√®re ou d'une autre, mais RND a vraiment montr√© d'excellents r√©sultats de recherche bas√©s sur la curiosit√© dans les jeux.  Et fait face au probl√®me de la t√©l√©vision al√©atoire. </p><br><p>  Avec RND, la curiosit√© d'OpenAI pour la premi√®re fois honn√™tement (et non par une recherche al√©atoire pr√©liminaire, comme dans Uber) a franchi le premier niveau de la vengeance de Montezuma.  Pas √† chaque fois et de mani√®re non fiable, mais de temps en temps, cela se r√©v√®le. </p><br><p><img src="https://habrastorage.org/webt/iw/jm/kb/iwjmkbze4r8efybc5-01pbqzjf8.png"></p><br><h1 id="chto-v-itoge">  Quel est le r√©sultat? </h1><br><p>  Comme vous pouvez le voir, en quelques ann√©es seulement, l'apprentissage par renforcement a parcouru un long chemin.  Pas seulement quelques solutions r√©ussies, comme dans les r√©seaux convolutionnels, o√π les connexions de resudal et de saut ont permis de former des r√©seaux √† des centaines de couches en profondeur, au lieu d'une douzaine de couches avec la seule fonction d'activation Relu, qui a surmont√© le probl√®me de la disparition des gradients dans sigmoid et tanh.  Dans l'apprentissage par renforcement, des progr√®s ont √©t√© accomplis dans les concepts et dans la compr√©hension des raisons pour lesquelles telle ou telle version na√Øve de la mise en ≈ìuvre n'a pas fonctionn√©.  Le mot cl√© "n'a pas fonctionn√©". </p><br><p>  Mais du point de vue technique, tout repose toujours sur les pr√©dictions des m√™mes valeurs Q, V ou A.  Il n'y a pas de d√©pendances temporelles √† diff√©rentes √©chelles, comme dans le cerveau (Hierarchical Reinforcement Learning ne compte pas, la hi√©rarchie y est trop primitive par rapport √† l'associativit√© dans le cerveau vivant).  Aucune tentative de proposer une architecture de r√©seau sp√©cialement con√ßue pour l'apprentissage par renforcement, comme cela s'est produit avec LSTM et d'autres r√©seaux r√©currents pour les s√©quences temporelles.  Renforcement Apprendre √† pi√©tiner sur place, √† se r√©jouir de petits succ√®s ou √† se d√©placer dans une direction compl√®tement fausse. </p><br><p>  Je voudrais croire qu'une fois dans l'apprentissage par renforcement, il y aura une perc√©e dans l'architecture des r√©seaux de neurones, similaire √† ce qui s'est pass√© dans les r√©seaux convolutionnels.  Et nous verrons vraiment un apprentissage par renforcement efficace.  Apprendre sur des exemples isol√©s, avec travailler la m√©moire associative et travailler sur diff√©rentes √©chelles de temps. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr437020/">https://habr.com/ru/post/fr437020/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr437006/index.html">Test de l'imprimante 3D Wanhao Duplicator 10</a></li>
<li><a href="../fr437008/index.html">PNL. Les bases. Techniques. D√©veloppement personnel. Partie 1</a></li>
<li><a href="../fr437010/index.html">√âchos du pass√©: l'exp√©rience de Young √† la base de la nouvelle m√©thode de spectroscopie aux rayons X</a></li>
<li><a href="../fr437014/index.html">La t√¢che de N corps ou comment faire sauter une galaxie sans quitter la cuisine</a></li>
<li><a href="../fr437018/index.html">Quelques pi√®ges de la saisie statique en Python</a></li>
<li><a href="../fr437022/index.html">Bit de s√©curit√© contre le bruit 0x22 (attaques par injection de d√©fauts, 35C3 et Wallet.fail)</a></li>
<li><a href="../fr437026/index.html">Google en France inflige une amende de 50 millions d'euros au RGPD pour abus de donn√©es √† caract√®re personnel</a></li>
<li><a href="../fr437030/index.html">Automatisation de l'infrastructure d'un bureau de luxe: √† quoi elle ressemble</a></li>
<li><a href="../fr437032/index.html">Instructions d'installation de NGINX ModSecurity</a></li>
<li><a href="../fr437034/index.html">Universal Whistles: Snom A230 et A210 USB Dongle Review</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>