<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëéüèæ üìà üëÉüèΩ Teoria de sharding üë®‚Äçüî¨ üê∂ üíÉüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Parece que estamos t√£o imersos na selva do desenvolvimento de grandes cargas que simplesmente n√£o pensamos nos problemas b√°sicos. Tome, por exemplo, s...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Teoria de sharding</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/433370/">  Parece que estamos t√£o imersos na selva do desenvolvimento de grandes cargas que simplesmente n√£o pensamos nos problemas b√°sicos.  Tome, por exemplo, sharding.  O que entender se √© poss√≠vel escrever shards = n condicionalmente nas configura√ß√µes do banco de dados e tudo ser√° feito por si s√≥.  Est√° certo, ele est√°, mas se, ao contr√°rio, quando algo der errado, os recursos come√ßarem a ser realmente escassos, eu gostaria de entender qual √© o motivo e como corrigi-lo. <br><br>  Em resumo, se voc√™ estava contribuindo com sua implementa√ß√£o alternativa de hash no Cassandra, dificilmente haver√° revela√ß√µes para voc√™.  Mas se a carga nos seus servi√ßos j√° estiver chegando e o conhecimento do sistema n√£o a acompanhar, ser√° bem-vindo.  O grande e terr√≠vel <strong>Andrei Aksyonov</strong> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">shodan</a> ), em sua maneira usual, dir√° que o <strong>sharding √© ruim, n√£o o sharding tamb√©m √© ruim</strong> , e como √© organizado dentro.  E por acidente, uma das partes da hist√≥ria sobre sharding n√£o √© realmente sobre sharding, mas o diabo sabe sobre o que - como mapear objetos para shards. <br><img src="https://habrastorage.org/webt/c9/ju/s6/c9jus6tadexnz4aih4q95bl7ega.jpeg"><br>  A foto das focas (apesar de acidentalmente serem filhotes) parece responder √† pergunta de por que isso √© tudo, mas vamos come√ßar em sequ√™ncia. <br><a name="habracut"></a><br><h2>  O que √© sharding? <br></h2><br>  Se voc√™ persistentemente pesquisa no Google, verifica-se que h√° uma borda bastante desfocada entre o chamado particionamento e o chamado sharding.  Todo mundo chama tudo o que ele quer do que ele quer.  Algumas pessoas distinguem entre particionamento horizontal e sharding.  Outros dizem que o sharding √© um certo tipo de particionamento horizontal. <br><br>  N√£o encontrei um √∫nico padr√£o terminol√≥gico que fosse aprovado pelos pais fundadores e certificado na ISO.  Uma cren√ßa pessoal interna √© mais ou menos assim: <strong>particionar</strong> em m√©dia √© "cortar a base em peda√ßos" de uma maneira arbitr√°ria. <br><br><ul><li>  Particionamento <strong>vertical</strong>  Por exemplo, h√° uma tabela gigante com alguns bilh√µes de entradas em 60 colunas.  Em vez de manter uma dessas tabelas gigantescas, mantemos 60 tabelas gigantescas com 2 bilh√µes de registros cada - e isso n√£o √© de meio per√≠odo, mas de particionamento vertical (como um exemplo de terminologia). <br></li><li>  Particionamento <strong>horizontal</strong> - cortamos linha por linha, talvez dentro do servidor. <br></li></ul><br>  O momento embara√ßoso aqui √© a diferen√ßa sutil entre particionamento horizontal e sharding.  Voc√™ pode me cortar em peda√ßos, mas n√£o vou lhe dizer com certeza em que consiste.  H√° um sentimento de que sharding e particionamento horizontal s√£o praticamente a mesma coisa. <br><br>  O sharding √© geralmente quando uma tabela grande em termos de bancos de dados ou uma cole√ß√£o de documentos, objetos, se voc√™ n√£o possui um banco de dados, mas um armazenamento de documentos, √© cortada especificamente para objetos.  Ou seja, pe√ßas de 2 bilh√µes de objetos s√£o selecionadas, independentemente do tamanho.  Os objetos por si mesmos dentro de cada objeto n√£o s√£o cortados em peda√ßos, n√£o nos decompomos em colunas separadas, ou seja, colocamos feixes em lugares diferentes. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/xx_Lv1P_X_I" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Link</a> para a apresenta√ß√£o para mais detalhes.</i> <br><br>  J√° existem diferen√ßas terminol√≥gicas sutis.  Por exemplo, relativamente falando, os desenvolvedores do Postgres podem dizer que o particionamento horizontal ocorre quando todas as tabelas nas quais a tabela principal est√° dividida est√£o no mesmo esquema e, quando em m√°quinas diferentes, ele est√° compartilhando. <br><br>  De um modo geral, sem estar vinculado √† terminologia de um banco de dados espec√≠fico e de um sistema de gerenciamento de dados espec√≠fico, existe a sensa√ß√£o de que o sharding est√° apenas cortando linha por linha e assim por diante - e √© tudo: <br><br><blockquote>  Sharding (~ =, \ in ...) Particionamento Horizontal == √© t√≠pico. <br></blockquote><br>  Eu enfatizo, tipicamente.  No sentido de que fazemos tudo isso n√£o apenas para cortar 2 bilh√µes de documentos em 20 tabelas, cada um dos quais seria mais gerenci√°vel, mas para distribu√≠-lo em v√°rios n√∫cleos, muitos discos ou muitos servidores f√≠sicos ou virtuais . <br><br>  Entende-se que estamos fazendo isso para que todos os shard - todos os dados shatka - sejam replicados muitas vezes.  Mas na verdade n√£o. <br><br><pre><code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> docs00 <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> documents <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">id</span></span>%<span class="hljs-number"><span class="hljs-number">16</span></span>)=<span class="hljs-number"><span class="hljs-number">0</span></span> ... <span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> docs15 <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> documents <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">id</span></span>%<span class="hljs-number"><span class="hljs-number">16</span></span>)=<span class="hljs-number"><span class="hljs-number">15</span></span></code> </pre> <br>  De fato, se voc√™ fizer uma fatia desses dados e de uma tabela gigante do SQL no MySQL, voc√™ gerar√° 16 pequenas tabelas no seu valioso laptop, sem ir al√©m de um √∫nico laptop, nem de um esquema, nem de um banco de dados, etc.  etc.  - tudo, voc√™ j√° tem sharding. <br><br>  Lembrando a ilustra√ß√£o com filhotes, isso leva ao seguinte: <br><br><ul><li>  A largura de banda est√° aumentando. <br></li><li>  A lat√™ncia n√£o muda, ou seja, cada trabalhador ou consumidor, por assim dizer, nesse caso, recebe o seu.  N√£o se sabe o que os filhotes aparecem na foto, mas os pedidos s√£o atendidos aproximadamente ao mesmo tempo, como se o filhote estivesse sozinho. </li><li>  Ou ambos, e outro, e ainda alta disponibilidade (replica√ß√£o). <br></li></ul><br>  <strong>Por que largura de banda?</strong>  √Äs vezes, podemos ter esses volumes de dados que n√£o se encaixam - n√£o est√° claro onde, mas eles n√£o se encaixam - por 1 {core |  dirigir |  servidor |  ...}.  Simplesmente n√£o h√° recursos suficientes e √© isso.  Para trabalhar com esse grande conjunto de dados, √© necess√°rio cort√°-lo. <br><br>  <strong>Por que lat√™ncia?</strong>  Em um n√∫cleo, a varredura de uma tabela de 2 bilh√µes de linhas √© 20 vezes mais lenta que a varredura de 20 tabelas em 20 kernels, fazendo isso em paralelo.  Os dados est√£o sendo processados ‚Äã‚Äãmuito lentamente em um recurso. <br><br>  <strong>Por que alta disponibilidade?</strong>  Ou cortamos os dados para fazer uma e outra ao mesmo tempo e, ao mesmo tempo, v√°rias c√≥pias de cada shard - replication fornecem alta disponibilidade. <br><br><h2>  Um exemplo simples de "como fazer isso com as m√£os" <br></h2><br>  O sharding condicional pode ser cortado usando a tabela de teste test.documents para 32 documentos e gerando a partir desta tabela 16 tabelas de teste para cerca de 2 documentos test.docs00, 01, 02, ..., 15 cada. <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> docs00 <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> documents <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">id</span></span>%<span class="hljs-number"><span class="hljs-number">16</span></span>)=<span class="hljs-number"><span class="hljs-number">0</span></span> ... <span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> docs15 <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> documents <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">id</span></span>%<span class="hljs-number"><span class="hljs-number">16</span></span>)=<span class="hljs-number"><span class="hljs-number">15</span></span></code> </pre><br>  Porque?  Como a priori, n√£o sabemos como o ID √© distribu√≠do, se de 1 a 32 inclusive, haver√° exatamente 2 documentos cada, caso contr√°rio, n√£o. <br><br>  <strong>Estamos fazendo isso por qu√™.</strong>  Depois de fazermos 16 tabelas, podemos ‚Äúcapturar‚Äù 16 do que precisamos.  Independentemente do que apoiamos, podemos paralelizar esses recursos.  Por exemplo, se n√£o houver espa√ßo em disco suficiente, far√° sentido decompor essas tabelas em discos separados. <br><br>  Infelizmente, tudo isso n√£o √© gratuito.  Eu suspeito que, no caso do padr√£o SQL can√¥nico (n√£o reli o padr√£o SQL por um longo tempo, talvez ele n√£o tenha sido atualizado por muito tempo), n√£o h√° sintaxe padronizada oficial para dizer a qualquer servidor SQL: ‚ÄúCaro servidor SQL, fa√ßa-me 32 shards e coloque-os em 4 discos ".  Mas em implementa√ß√µes individuais, geralmente h√° uma sintaxe espec√≠fica para fazer o mesmo em princ√≠pio.  O PostgreSQL possui mecanismos de particionamento, o MySQL MariaDB possui, a Oracle provavelmente j√° fez tudo isso h√° muito tempo. <br><br>  No entanto, se fizermos isso manualmente, sem suporte ao banco de dados e dentro da estrutura do padr√£o, <strong>pagaremos condicionalmente a complexidade do acesso aos dados</strong> .  Onde havia um simples SELECT * FROM documentos WHERE id = 123, agora 16 x SELECT * FROM docsXX.  E bem, se tent√°ssemos obter o registro por chave.  Significativamente mais interessante se tent√°ssemos obter uma faixa inicial de registros.  Agora (se, enfatizo, como se eu fosse tolo, e permane√ßo dentro do padr√£o), os resultados desses 16 SELECT * FROM dever√£o ser combinados no aplicativo. <br><br>  <strong>Que mudan√ßa de desempenho esperar?</strong> <br><br><ul><li>  Intuitivamente linear. </li><li>  Teoricamente - sublinear, porque a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">lei de Amdahl</a> . </li><li>  Na pr√°tica - talvez quase linearmente, talvez n√£o. </li></ul><br>  De fato, a resposta correta √© desconhecida.  Com a aplica√ß√£o inteligente da t√©cnica de sharding, voc√™ pode obter uma deteriora√ß√£o super-linear significativa na opera√ß√£o do seu aplicativo, e at√© o DBA estar√° funcionando com um p√¥quer em brasa. <br><br>  Vamos ver como isso pode ser alcan√ßado.  √â claro que apenas definir a configura√ß√£o como shards do PostgreSQL = 16 e depois decolar - isso n√£o √© interessante.  Vamos pensar em como poder√≠amos conseguir que <em>reduzir√≠amos a fragmenta√ß√£o em 32 vezes</em> , o que √© interessante do ponto de vista de como n√£o fazer isso. <br><br>  Nossas tentativas de acelerar ou desacelerar sempre ficar√£o contra os cl√°ssicos - a boa e velha lei da Amdahl, que diz que n√£o h√° paraleliza√ß√£o perfeita de nenhuma solicita√ß√£o, sempre h√° uma parte consistente. <br><br><h2>  Lei Amdahl <br></h2><br><blockquote>  <strong><em>Sempre</em></strong> h√° uma pe√ßa serializada. <br></blockquote><br>  Sempre h√° uma parte da execu√ß√£o da solicita√ß√£o que √© paralela e sempre h√° uma parte que n√£o √© paralela.  Mesmo que pare√ßa para voc√™ uma consulta perfeitamente paralela, pelo menos coletando uma linha do resultado que voc√™ enviar√° ao cliente, a partir das linhas recebidas de cada shard, sempre haver√° e sempre ser√° consistente. <br><br>  Sempre h√° algum tipo de parte seq√ºencial.  Pode ser min√∫sculo, absolutamente invis√≠vel no contexto geral, pode ser gigantesco e, consequentemente, afetar fortemente a paraleliza√ß√£o, mas est√° sempre l√°. <br><br>  Al√©m disso, sua influ√™ncia est√° <strong><em>mudando</em></strong> e pode crescer significativamente, por exemplo, se reduzirmos nossa tabela - vamos aumentar as taxas - de 64 registros para 16 tabelas de 4 registros, essa parte ser√° alterada.  Obviamente, a julgar por quantidades t√£o gigantescas de dados, trabalhamos em um telefone celular e processador de 86 MHz, n√£o temos arquivos suficientes que possam ser mantidos abertos ao mesmo tempo.  Aparentemente, com essa entrada, abrimos um arquivo por vez. <br><br><ul><li>  Foi <strong>Total =</strong> <strong>Serial +</strong> <strong>Paralelo</strong> .  Onde, por exemplo, paralelo √© todo o trabalho dentro do banco de dados e o serial est√° enviando o resultado ao cliente. <br></li><li>  Tornou-se <strong>Total2 = Serial + Paralelo / N + Xserial.</strong>  Por exemplo, quando o geral ORDER BY, Xserial&gt; 0. <br></li></ul><br>  Com este exemplo simples, tento mostrar que algum Xserial aparece.  Al√©m do fato de sempre haver uma parte serializada e de estarmos tentando trabalhar com dados em paralelo, uma parte adicional aparece para garantir o corte desses dados.  Grosso modo, podemos precisar de: <br><br><ul><li>  encontre essas 16 tabelas no dicion√°rio interno do banco de dados; </li><li>  abrir arquivos; </li><li>  alocar mem√≥ria; </li><li>  realocar mem√≥ria; </li><li>  manchar os resultados; </li><li>  sincronizar entre n√∫cleos; </li></ul><br>  Os efeitos fora de sincronia sempre aparecem.  Eles podem ser insignificantes e ocupar um bilion√©simo do tempo total, mas s√£o sempre diferentes de zero e sempre existem.  Com a ajuda deles, podemos perder drasticamente a produtividade ap√≥s o sharding. <br><br><img src="https://habrastorage.org/webt/fh/mx/yh/fhmxyh9tozfrbd4yszxj2va9a1g.jpeg"><br><br>  Esta √© uma imagem padr√£o da lei de Amdahl.  N√£o √© muito leg√≠vel, mas √© importante que as linhas, que idealmente sejam retas e cres√ßam linearmente, encostem na ass√≠ntota.  Mas como o gr√°fico da Internet √© ileg√≠vel, criei, na minha opini√£o, mais tabelas visuais com n√∫meros. <br><br>  Suponha que tenhamos uma parte serializada do processamento da solicita√ß√£o, que leva apenas 5%: <strong>serial = 0,05 = 1/20.</strong> <br><br>  Intuitivamente, parece que, com a parte serializada, que leva apenas 1/20 do processamento da solicita√ß√£o, se paralelizarmos o processamento da solicita√ß√£o a 20 n√∫cleos, ele se tornar√° cerca de 20, na pior das hip√≥teses, 18 vezes mais r√°pido. <br><br>  De fato, a <b>matem√°tica √© uma coisa sem cora√ß√£o</b> : <br><br> <code>wall = 0.05 + 0.95/num_cores, speedup = 1 / (0.05 + 0.95/num_cores)</code> <br> <br>  Acontece que, se voc√™ calcular cuidadosamente, com uma parte serializada de 5%, a acelera√ß√£o ser√° 10 vezes (10,3), e isso √© 51% comparado ao ideal te√≥rico. <br><br><table><tbody><tr><td>  8 n√∫cleos </td><td>  = 5,9 </td><td>  <font color="#c45911">= 74%</font> </td></tr><tr><td>  10 n√∫cleos </td><td>  = 6,9 </td><td>  <font color="#c45911">= 69%</font> </td></tr><tr><td>  <strong>20 n√∫cleos</strong> </td><td>  <strong>= 10,3</strong> </td><td>  <strong><font color="#c45911">= 51%</font></strong> </td></tr><tr><td>  40 n√∫cleos </td><td>  = 13,6 </td><td>  <font color="#ff0000">= 34%</font> </td></tr><tr><td>  128 n√∫cleos </td><td>  = 17,4 </td><td>  <font color="#ff0000">= 14%</font> </td></tr></tbody></table><br>  Usando 20 n√∫cleos (20 discos, se voc√™ preferir) para a tarefa em que trabalhamos antes, teoricamente nunca obteremos acelera√ß√£o mais de 20 vezes, mas praticamente muito menos.  Al√©m disso, com um aumento no n√∫mero de paralelos, a inefici√™ncia est√° crescendo rapidamente. <br><br>  Quando apenas 1% do trabalho serializado permanece e 99% √© paralelo, os valores de acelera√ß√£o s√£o um pouco melhorados: <br><br><table><tbody><tr><td>  8 n√∫cleos </td><td>  = 7,5 </td><td>  <font color="#538135">= 93%</font> </td></tr><tr><td>  16 n√∫cleos </td><td>  = 13,9 </td><td>  <font color="#538135">= 87%</font> </td></tr><tr><td>  32 n√∫cleos </td><td>  = 24,4 </td><td>  <font color="#c45911">= 76%</font> </td></tr><tr><td>  64 n√∫cleos </td><td>  = 39,3 </td><td>  <font color="#c45911">= 61%</font> </td></tr></tbody></table><br>  Para uma consulta completamente termonuclear, que dura naturalmente por horas, e o trabalho preparat√≥rio e a montagem do resultado levam muito pouco tempo (serial = 0,001), j√° veremos uma boa efici√™ncia: <br><br><table><tbody><tr><td>  8 n√∫cleos </td><td>  = 7,94 </td><td>  <font color="#538135">= 99%</font> </td></tr><tr><td>  16 n√∫cleos </td><td>  = 15,76 </td><td>  <font color="#538135">= 99%</font> </td></tr><tr><td>  32 n√∫cleos </td><td>  = 31,04 </td><td>  <font color="#538135">= 97%</font> </td></tr><tr><td>  64 n√∫cleos </td><td>  = 60,20 </td><td>  <font color="#538135">= 94%</font> </td></tr></tbody></table><br>  Observe que <strong>nunca veremos 100%</strong> .  Em casos particularmente bons, √© poss√≠vel ver, por exemplo, 99,999%, mas n√£o exatamente 100%. <br><br><h2>  Como embaralhar e quebrar N vezes? <br></h2><br>  Voc√™ pode embaralhar e interromper exatamente N vezes: <br><br><ol><li>  Envie solicita√ß√µes docs00 ... docs15 <strong>sequencialmente</strong> , n√£o paralelamente. </li><li>  Em consultas simples, <strong>n√£o</strong> selecione <strong>por chave</strong> , WHERE algo = 234. </li></ol><br>  Nesse caso, a parte serializada (serial) ocupa n√£o 1% e n√£o 5%, mas cerca de 20% nos bancos de dados modernos.  Voc√™ pode obter 50% da parte serializada se acessar o banco de dados usando um protocolo bin√°rio extremamente eficiente ou vincul√°-lo como uma biblioteca din√¢mica a um script Python. <br><br>  O restante do tempo de processamento de uma solicita√ß√£o simples ser√° ocupado por opera√ß√µes n√£o paralelas de an√°lise da solicita√ß√£o, prepara√ß√£o do plano etc.  Ou seja, fica mais lento sem ler o registro. <br><br>  Se dividirmos os dados em 16 tabelas e execut√°-los seq√ºencialmente, como √© habitual na linguagem de programa√ß√£o PHP, por exemplo, (ele n√£o sabe como executar processos ass√≠ncronos muito bem), obtemos a desacelera√ß√£o em 16 vezes.  E, talvez, ainda mais, porque as viagens de ida e volta da rede tamb√©m ser√£o adicionadas. <br><br><blockquote>  De repente, ao compartilhar, a escolha de uma linguagem de programa√ß√£o √© importante. <br></blockquote><br>  Lembramos da escolha de uma linguagem de programa√ß√£o, porque se voc√™ envia consultas ao banco de dados (ou servidor de pesquisa) sequencialmente, de onde vem a acelera√ß√£o?  Em vez disso, uma desacelera√ß√£o aparecer√°. <br><br><h3>  Bicicleta da vida <br></h3><br>  Se voc√™ escolher C ++, <strong>escreva para Threads POSIX</strong> , n√£o Boost I / O.  Eu vi uma excelente biblioteca de desenvolvedores experientes da Oracle e do pr√≥prio MySQL, que escreveram a comunica√ß√£o com o servidor MySQL no Boost.  Aparentemente, eles foram for√ßados a escrever em C puro no trabalho, mas depois conseguiram se virar, usar o Boost com E / S ass√≠ncrona etc.  Um problema - essa E / S ass√≠ncrona, que teoricamente deveria ter direcionado 10 solicita√ß√µes em paralelo, por algum motivo, possu√≠a um ponto de sincroniza√ß√£o invis√≠vel.  Ao iniciar 10 solicita√ß√µes em paralelo, elas foram executadas exatamente 20 vezes mais devagar que uma, porque 10 vezes para as pr√≥prias solicita√ß√µes e uma vez para o ponto de sincroniza√ß√£o. <br><br>  <strong>Conclus√£o:</strong> escreva em linguagens que implementem execu√ß√£o paralela e aguardem bem solicita√ß√µes diferentes.  Para ser sincero, n√£o sei o que exatamente h√° para aconselhar al√©m do Go.  N√£o s√≥ porque eu realmente amo o Go, mas porque n√£o sei nada mais adequado. <br><br>  <strong>N√£o escreva em idiomas inadequados</strong> nos quais voc√™ n√£o pode executar 20 consultas paralelas no banco de dados.  Ou em todas as oportunidades, n√£o fa√ßa tudo com as m√£os - entenda como funciona, mas n√£o fa√ßa manualmente. <br><br><h2>  Bicicleta de teste A / B <br></h2><br>  √Äs vezes, voc√™ pode desacelerar porque est√° acostumado com o fato de que tudo funciona e n√£o percebeu que a pe√ßa serializada, em primeiro lugar, √©, em segundo, grande. <br><br><ul><li>  Imediatamente ~ 60 shards de √≠ndice de pesquisa, categorias </li><li>  Estes s√£o fragmentos corretos e corretos, sob uma √°rea de assunto. </li><li>  Havia at√© 1000 documentos e 50.000 documentos. </li></ul><br>  Esta √© uma bicicleta de produ√ß√£o, quando as consultas de pesquisa foram ligeiramente alteradas e come√ßaram a selecionar muito mais documentos de 60 shards do √≠ndice de pesquisa.  Tudo funcionou rapidamente e com o princ√≠pio: ‚ÄúFunciona - n√£o toque‚Äù, todos esqueceram, que na verdade est√° dentro de 60 estilha√ßos.  Aumentamos o limite de amostragem para cada fragmento de mil para 50 mil documentos.  De repente, come√ßou a desacelerar e o paralelismo cessou.  Os pedidos em si, que foram executados de acordo com os fragmentos, voaram muito bem, e o palco ficou mais lento, quando 50 mil documentos foram coletados de 60 fragmentos.  Esses 3 milh√µes de documentos finais em um n√∫cleo foram mesclados, classificados e os 3 milh√µes foram selecionados e entregues ao cliente.  A mesma parte serial desacelerou, a mesma lei implac√°vel de Amdal funcionou. <br><br>  <em>Ent√£o talvez voc√™ n√£o deva fazer sharding com as m√£os, mas apenas humanamente</em> <em><br></em>  <em>diga ao banco de dados: "Fa√ßa!"</em> <em><br></em> <br>  <strong>Disclaimer:</strong> Eu realmente n√£o sei como fazer algo certo.  Eu sou do andar errado !!! <br><br>  Eu tenho promovido uma religi√£o chamada "fundamentalismo algor√≠tmico" durante toda a minha vida consciente.  √â brevemente formulado de maneira muito simples: <br><br><blockquote>  Voc√™ realmente n√£o quer fazer nada com as m√£os, mas √© extremamente √∫til saber como est√° organizado dentro.  Para que, no momento em que algo d√™ errado no banco de dados, voc√™ pelo menos entenda o que deu errado l√°, como ele est√° organizado por dentro e aproximadamente como ele pode ser reparado. <br></blockquote><br>  Vejamos as op√ß√µes: <br><br><ol><li>  <strong>"M√£os"</strong> .  Antes, dividimos os dados manualmente em 16 tabelas virtuais e reescrevemos todas as consultas com nossas m√£os - isso √© extremamente desconfort√°vel.  <strong>Se houver uma oportunidade de n√£o embaralhar as m√£os - n√£o embaralhe as m√£os!</strong>  Mas √†s vezes isso n√£o √© poss√≠vel, por exemplo, voc√™ possui o MySQL 3.23 e, em seguida, √© necess√°rio. </li><li>  <strong>"Autom√°tico".</strong>  Acontece que voc√™ pode embaralhar automaticamente ou quase automaticamente, quando o banco de dados pode distribuir os dados em si, basta escrever aproximadamente em algum lugar uma configura√ß√£o espec√≠fica.  Existem muitas bases e elas t√™m v√°rias configura√ß√µes diferentes.  Estou certo de que em todo banco de dados em que √© poss√≠vel gravar shards = 16 (qualquer que seja a sintaxe), muitas outras configura√ß√µes s√£o coladas a esse caso pelo mecanismo. </li><li>  <strong>"Semi-autom√°tico"</strong> - um modo completamente c√≥smico, na minha opini√£o, e brutal.  Ou seja, a pr√≥pria base parece n√£o conseguir, mas existem patches adicionais externos. </li></ol><br>  √â dif√≠cil dizer algo sobre a m√°quina, exceto envi√°-la para a documenta√ß√£o no banco de dados correspondente (MongoDB, Elastic, Cassandra, ... em geral, o chamado NoSQL).  Se voc√™ tiver sorte, basta pressionar o bot√£o "me fa√ßa 16 shards" e tudo funcionar√°.  Nesse momento, quando n√£o funciona, o restante do artigo pode ser necess√°rio. <br><br><h2>  Sobre dispositivo semiautom√°tico <br></h2><br>  Em alguns lugares, tecnologias sofisticadas de informa√ß√£o inspiram horror ct√¥nico.  Por exemplo, o MySQL pronto para uso n√£o tinha implementa√ß√£o de sharding para determinadas vers√µes, com certeza, no entanto, o tamanho das bases operadas em batalha cresce para valores indecentes. <br><br>  O sofrimento da humanidade diante dos DBAs individuais tem sido atormentado por anos e cria v√°rias solu√ß√µes de fragmenta√ß√£o ruins criadas sem motivo.  Depois disso, uma solu√ß√£o de sharding mais ou menos decente √© escrita chamada ProxySQL (MariaDB / Spider, PG / pg_shard / Citus, ...).  Este √© um exemplo bem conhecido desse mesmo manto. <br><br>  O ProxySQL como um todo, √© claro, √© uma solu√ß√£o completa de classe empresarial para c√≥digo aberto, para roteamento e muito mais.  Mas uma das tarefas a serem resolvidas √© o sharding para um banco de dados, que por si s√≥ n√£o sabe como fragmentar humanamente.  Veja bem, n√£o existe uma op√ß√£o "shards = 16", voc√™ precisa reescrever cada solicita√ß√£o no aplicativo e existem muitos locais ou colocar uma camada intermedi√°ria entre o aplicativo e o banco de dados com a seguinte apar√™ncia: "Hmm ... SELECT * FROM documentos?  Sim, ele deve ser dividido em 16 pequenos SELECT * FROM server1.document1, SELECT * FROM server2.document2 - para este servidor com esse nome de usu√°rio / senha, para este com outro.  Se algu√©m n√£o respondeu, ent√£o ... "etc. <br><br>  Exatamente isso pode ser feito por patches intermedi√°rios.  Eles s√£o um pouco menores do que para todos os bancos de dados.  Para o PostgreSQL, como eu o entendo, ao mesmo tempo, existem algumas solu√ß√µes embutidas (PostgresForeign Data Wrappers, na minha opini√£o, est√° embutido no pr√≥prio PostgreSQL), existem patches externos. <br><br>  A configura√ß√£o de cada patch espec√≠fico √© um t√≥pico gigante separado que n√£o cabe em um relat√≥rio, portanto discutiremos apenas conceitos b√°sicos. <br><br>  Vamos falar um pouco mais sobre a teoria do zumbido. <br><br><h2>  Automa√ß√£o perfeita absoluta? <br></h2><br>  Toda a teoria do zumbido no caso do sharding nesta letra F (), o princ√≠pio b√°sico √© <strong>sempre</strong> o mesmo bruto: <code>shard_id = F(object).</code> <br><br>  Sharding √© geralmente sobre o que?  Temos 2 bilh√µes de registros (ou 64).  Queremos dividi-los em v√°rios peda√ßos.  Surge uma pergunta inesperada - como?  Por que princ√≠pio devo espalhar meus 2 bilh√µes de registros (ou 64) em 16 servidores dispon√≠veis para mim? <br><br>  O matem√°tico latente em n√≥s deve sugerir que, no final, sempre exista uma fun√ß√£o m√°gica que, para cada documento (objeto, linha, etc.), determine em que pe√ßa coloc√°-lo. <br><br>  Se aprofundarmos na matem√°tica, essa fun√ß√£o sempre depende n√£o apenas do objeto em si (da pr√≥pria linha), mas tamb√©m de configura√ß√µes externas, como o n√∫mero total de fragmentos.  A fun√ß√£o, que para cada objeto deve dizer onde coloc√°-lo, n√£o pode retornar um valor mais do que existem servidores no sistema.  E as fun√ß√µes s√£o um pouco diferentes: <br><br><ul><li>  shard_func = <strong>F1</strong> (objeto); <br></li><li>  shard_id = <strong>F2</strong> (shard_func, ...); </li><li>  shard_id = <strong>F2</strong> ( <strong>F1</strong> (objeto), current_num_shards, ...). </li></ul><br>  Mas, al√©m disso, n√£o vamos nos aprofundar nessas selvas de fun√ß√µes individuais, apenas falamos sobre quais s√£o as fun√ß√µes m√°gicas F (). <br><br><h2>  O que s√£o F ()? <br></h2><br>  Eles podem apresentar muitos mecanismos de implementa√ß√£o diferentes e diferentes.  Resumo da amostra: <br><br><ul><li>  F = <strong>rand</strong> ()% nums_shards </li><li>  F = <strong>somehash</strong> (object.id)% num_shards </li><li>  F = object.date% num_shards </li><li>  F = object.user_id% num_shards </li><li>  ... </li><li>  F = shard_table [somehash () | ... object.date | ...] </li></ul><br>  Um fato interessante - voc√™ pode naturalmente espalhar todos os dados aleatoriamente - lan√ßamos o pr√≥ximo registro em um servidor arbitr√°rio, em um n√∫cleo arbitr√°rio, em uma tabela arbitr√°ria.  N√£o haver√° muita felicidade nisso, mas funcionar√°. <br><br>  Existem m√©todos um pouco mais inteligentes de scamming para fun√ß√µes de hash reproduz√≠veis ou mesmo consistentes, ou scamming para algum atributo.  Vamos passar por cada m√©todo. <br><br><h3>  F = rand () <br></h3><br>  A dispers√£o n√£o √© um m√©todo muito correto.  Um problema: dispersamos nossos 2 bilh√µes de registros por mil servidores aleatoriamente e n√£o sabemos onde est√° o registro.  Precisamos puxar user_1, mas n√£o sabemos onde ele est√°.  Vamos a milhares de servidores e resolvemos tudo - de alguma forma, √© ineficiente. <br><br><h3>  F = somehash () <br></h3><br>  Vamos espalhar os usu√°rios de uma maneira adulta: leia a fun√ß√£o de hash reproduzida em user_id, pegue o restante da divis√£o pelo n√∫mero de servidores e acesse o servidor desejado imediatamente. <br><br>  <em>Por que estamos fazendo isso?</em>  <em>E ent√£o, temos carga alta e n√£o colocamos nada em um servidor.</em>  <em>Se interferir, a vida seria t√£o simples.</em> <br><br>  Bem, a situa√ß√£o j√° melhorou, para obter um registro, vamos a um servidor conhecido.  Mas se tivermos um intervalo de chaves, em todo esse intervalo, precisamos classificar todos os valores de chave e, no limite, ir para o n√∫mero de shards que tivermos chaves no intervalo ou para cada servidor em geral.  A situa√ß√£o, √© claro, melhorou, mas n√£o para todos os pedidos.  Alguns pedidos foram afetados. <br><br><h3>  Fragmento natural (F = object.date% num_shards) <br></h3><br>  √Äs vezes, ou seja, 95% do tr√°fego e 95% da carga s√£o solicita√ß√µes que possuem algum tipo de fragmenta√ß√£o natural. , 95%  -       1 , 3 , 7 ,   5%     .  95% ,  ,    ,        . <br><br>        , ,   ,         -           . <br><br>   ‚Äî        ,      .       ,    , , ,    .        5 %  . <br><br>       ,    : <br><br><ol><li>      ,  95%     . </li><li>  95%    ,       ,     .   ,           .     ,     . </li></ol><br>  ,      ‚Äî    ,         - . <br><br>   ,   ,         ,     ,         .       ¬´   -      ¬ª. <br><br> <strong>     ¬´¬ª.</strong> ,            . <br><br><h3> 1.  :   <br></h3><br>    ,      ,  . <br><br><ul><li>    ,   ! </li><li> <strong><em></em></strong>  () . </li></ul><br>   , /  , ,  , PM    (       ,  PM   ),     .     . <br><br>  ,    .      ,       ,    100   .        . <br><br>   ,  ,   ,            ,    - . <br><br><h3> 2. ¬´¬ª : , join <br></h3><br>   ,             ? <br><br><ul><li>  ¬´¬ª ‚Ä¶ WHERE randcol BETWEEN aaa AND bbb? <br></li><li>  ¬´¬ª ‚Ä¶ users_32shards JOIN posts_1024 shards? </li></ul><br>  : , ! <br><br>           ,    ,       ,           .      .       (, , document store    ),     ,     . <br><br>   ‚Äî <strong>-       </strong> .     .  ,          .     ,       ,    ,   .       - , ,         ,   ,         ‚Äî    . <br><br>       ,             . <br><br><h3> 3. / :  <br></h3><br> :         ,          . <br><br><blockquote>    ,   . <br></blockquote><br>      ,  , ,  .     ,     ,   ,    10 , -        30,       100   .    .          ‚Äî       ,  -   ‚Äî  , -  . <br><br> ,      :  16 -,  32. ,   17,  23 ‚Äî    .      ,  ,    -  ? <br><br>  : ,    ,     . <br><br>  ,    ¬´¬ª,   ¬´ ¬ª. <br><br><h4>   #1.   <br></h4><br><ul><li>     NewF(object),    . </li><li>   NewF()=OldF() . </li><li>   <strong> .</strong> </li><li>  Ai. </li></ul><br>  ,    2       ,  ,  .   :  17 ,  6   ,  2  ,    17   23 .   10  , ,    .      . <br><br><h4>   #2.   <br></h4><br>    ‚Äî       ‚Äî  17    23,     16   32 !         ,        . <br><br><ul><li>     NewF(object),    . </li><li> <strong>  2^N,   2^(N+1) .</strong> </li><li>   NewF()=OldF()  0,5. </li><li>   50% . </li><li> ,   <strong>   .</strong> </li></ul><br>  ,  ,         .   ,   ,  . <br><br>  ,            .   ,  16     16,      ‚Äî    . <br><br> ,        ‚Äî     . <br><br><h4>  #3. Consistent hashing <br></h4><br> ,       consistent hashing <br><img src="https://habrastorage.org/webt/il/ml/rt/ilmlrt9xy-c3wuyfaafntagufay.jpeg"><br><br>   ¬´consistent hashing¬ª,    ,    . <br><br> :    ()   ,      .    ,     ,  ,      (  ,     ), . <br><br><ul><li>   :  <strong><em> </em></strong> ,   2 ¬´¬ª,    1/n. <br></li><li>   :    ,   .  . </li></ul><br>          ,         .  ,      ,      ,     :     ,          . <br><br>        .  ,        .  ,   ..,    .  ,   - , ,        . <br><br>       ,  , ,  Cassandra   .  ,         , ,      , ,  . <br><br>   ,        ‚Äî     /    ,   ,    . <br><br> , :    ?       ? ‚Äî ,  ! <br><br><h4>  #4. Rendezvous/HRW <br></h4><br>    (  ,   ): <strong>shard_id = arg max hash(object_id, shard_id).</strong> <br><br>    Rendezvous hashing,   ,  ,    Highest Random Weight.      : <br><img src="https://habrastorage.org/webt/0t/dt/rm/0tdtrm0iftxxb5ors5a2wxcex8s.jpeg"><br>   , , 16 .    (),   - ,  16 ,      .      -,   . <br><br>    HRW-hashing,   Rendezvous hashing.       , -,        ,   . <br><br>    ,       .  ,        - -        .      . <br><br>   ,       . <br><br><h4>  #5.   <br></h4><br> ,        Google    -   : <br><br><ul><li> Jump Hash ‚Äî Google '2014. </li><li> Multi Probe ‚ÄîGoogle '2015. </li><li> Maglev ‚Äî Google '2016. </li></ul><br>    ,    .      ,   ,    , -,       .      . <br><br><h4>  #6.  <br></h4><br>      ‚Äî  .     ?   ,     2  ,          object_id  2  ,     . <br><br>  ,       ?    ? <br><br>     . ,   -     ,   ,  .  ,      , ,  ,     . <br><br> : <br><br><ul><li>  1  . </li><li>      /  /  /       : min/max_id =&gt; shard_id. </li><li>    8    4    (4      !) ‚Äî  20    . </li><li>      -   ,        20  ‚Äî     . </li><li> 20  ‚Äî                 . </li></ul><br>     2     -    16  ‚Äî   100   -   .       : ,         ,   ‚Äî  1 .     ,  ,   . <br><br> ,    ,     ,    - ,     . <br><br><h1>  Conclus√µes <br></h1><br>            : ¬´  ,   !¬ª.       ,     20 . <br><br>   ,   ,     .   ,  <strong>   </strong> ‚Äî   .     100$        ,     .          -,    .     ‚Äî   . <br><br> <strong>    </strong> , ,  ¬´¬ª (, DFS, ...)   .   ,   , highload   -   .  ,        ,     - .     ‚Äî <strong> ,    </strong> . <br><br>     <strong> </strong> <strong>F()</strong> ,   , ,  ..  , ,    2   <strong>     </strong> . <br><br><h2>   <br></h2><br> ,      ,        .       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> HighLoad++</a> ,  ,     ‚ÄîSphinx‚Äîhighload  ,   . <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/qpGljUyIht8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h2>    <br></h2><br>         Highload User Group.  ,    . <br><br>  , ,     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">HighLoad++</a>     .         , ,  .  ,            , .     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a>   highload-,   . <br><br>        ,  ,     ,  . ,           , ,        . <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a> <strong>24   -</strong>      ¬´¬ª, ¬´ ¬ª.  ,        .      ,     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> . <br><br><blockquote>         , ,  <strong>8  9   -  </strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><strong>HighLoad++</strong></a>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> early bird . <br></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt433370/">https://habr.com/ru/post/pt433370/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt433360/index.html">Cientistas tentaram prever quando avi√µes el√©tricos se tornar√£o realidade</a></li>
<li><a href="../pt433362/index.html">9 princ√≠pios de beleza, simplicidade e cuidado no UX</a></li>
<li><a href="../pt433364/index.html">LDraw + Unity. Como eu gerei Lego</a></li>
<li><a href="../pt433366/index.html">Trabalhando com recursos externos no Unity3D</a></li>
<li><a href="../pt433368/index.html">Como aplicar o pensamento do supermercado ao mundo: um exemplo de moletom</a></li>
<li><a href="../pt433372/index.html">Bicicleta do carro</a></li>
<li><a href="../pt433374/index.html">Toda a verdade sobre o RTOS. Artigo 26. Canais: servi√ßos auxiliares e estruturas de dados</a></li>
<li><a href="../pt433376/index.html">Curso MIT "Seguran√ßa de sistemas de computadores". Aula 21: Rastreamento de Dados, Parte 1</a></li>
<li><a href="../pt433378/index.html">Curso MIT "Seguran√ßa de sistemas de computadores". Aula 21: Rastreamento de Dados, Parte 2</a></li>
<li><a href="../pt433380/index.html">Curso MIT "Seguran√ßa de sistemas de computadores". Aula 21: Rastreamento de Dados, Parte 3</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>