<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🔘 👨‍👩‍👦 🛀🏽 Starten des RabbitMQ-Clusters in Kubernetes 🧑🏼‍🤝‍🧑🏼 🗾 👩🏽‍✈️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Im Fall der Microservice-Organisation der Anwendung beruht ein erheblicher Arbeitsaufwand auf den Mechanismen der Integrationsverbindung von Microserv...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Starten des RabbitMQ-Clusters in Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/true_engineering/blog/419817/">  Im Fall der Microservice-Organisation der Anwendung beruht ein erheblicher Arbeitsaufwand auf den Mechanismen der Integrationsverbindung von Microservices.  Darüber hinaus sollte diese Integration fehlertolerant sein und einen hohen Grad an Verfügbarkeit aufweisen. <br><br>  In unseren Lösungen verwenden wir die Integration mit Kafka, gRPC und RabbitMQ. <br><br>  In diesem Artikel werden wir unsere Erfahrungen mit dem Clustering von RabbitMQ teilen, dessen Knoten auf Kubernetes gehostet werden. <br><br><img src="https://habrastorage.org/webt/dx/ll/-h/dxll-hzomoco0zcfp7esju8pena.jpeg" alt="Bild"><br><br>  Vor RabbitMQ Version 3.7 war das Clustering in K8S keine sehr triviale Aufgabe, mit vielen Hacks und nicht sehr schönen Lösungen.  In Version 3.6 wurde ein Autocluster-Plugin von RabbitMQ Community verwendet.  Und in 3.7 erschien das Kubernetes Peer Discovery Backend.  Es ist durch das Plug-In in der Basisauslieferung von RabbitMQ integriert und erfordert keine separate Montage und Installation. <br><br>  Wir werden die endgültige Konfiguration als Ganzes beschreiben und kommentieren, was passiert. <br><a name="habracut"></a><br><h2>  In der Theorie </h2><br>  Das Plugin verfügt über ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Repository auf dem Github</a> , in dem es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ein Beispiel für die grundlegende Verwendung gibt</a> . <br>  Dieses Beispiel ist nicht für die Produktion gedacht, was in seiner Beschreibung klar angegeben ist, und außerdem sind einige der darin enthaltenen Einstellungen entgegen der Verwendungslogik im Produkt festgelegt.  In diesem Beispiel wird die Persistenz des Speichers überhaupt nicht erwähnt, sodass sich unser Cluster in jeder Notsituation in einen Zilch verwandelt. <br><br><h2>  In der Praxis </h2><br>  Jetzt erfahren Sie, was Sie selbst gesehen haben und wie Sie RabbitMQ installieren und konfigurieren. <br><br>  Beschreiben wir die Konfigurationen aller Teile von RabbitMQ als Service in K8s.  Wir werden sofort klarstellen, dass wir RabbitMQ in K8s als StatefulSet installiert haben.  Auf jedem Knoten des K8s-Clusters funktioniert immer eine Instanz von RabbitMQ (ein Knoten in der klassischen Clusterkonfiguration).  Wir werden auch das RabbitMQ-Control Panel in K8s installieren und Zugriff auf dieses Panel außerhalb des Clusters gewähren. <br><br><h3>  Rechte und Rollen: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_rbac.yaml</b> <div class="spoiler_text"><pre><code class="plaintext hljs">--- apiVersion: v1 kind: ServiceAccount metadata: name: rabbitmq --- kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: endpoint-reader rules: - apiGroups: [""] resources: ["endpoints"] verbs: ["get"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: endpoint-reader subjects: - kind: ServiceAccount name: rabbitmq roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: endpoint-reader</code> </pre> </div></div><br>  Die Zugriffsrechte für RabbitMQ stammen vollständig aus dem Beispiel, es sind keine Änderungen erforderlich.  Wir erstellen ein ServiceAccount für unseren Cluster und erteilen ihm Endberechtigungen für Endpoints K8s. <br><br><h3>  Dauerhafte Speicherung: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_pv.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: PersistentVolume apiVersion: v1 metadata: name: rabbitmq-data-sigma labels: type: local annotations: volume.alpha.kubernetes.io/storage-class: rabbitmq-data-sigma spec: storageClassName: rabbitmq-data-sigma capacity: storage: 10Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Recycle hostPath: path: "/opt/rabbitmq-data-sigma"</code> </pre> </div></div><br>  Hier haben wir den einfachsten Fall als persistenten Speicher genommen - hostPath (ein regulärer Ordner auf jedem K8s-Knoten), aber Sie können jeden der vielen Arten von persistenten Volumes verwenden, die von K8s unterstützt werden. <br><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_pvc.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: PersistentVolumeClaim apiVersion: v1 metadata: name: rabbitmq-data spec: storageClassName: rabbitmq-data-sigma accessModes: - ReadWriteMany resources: requests: storage: 10Gi</code> </pre> </div></div><br>  Volume erstellen Anspruch auf das im vorherigen Schritt erstellte Volume.  Dieser Anspruch wird dann in StatefulSet als persistenter Datenspeicher verwendet. <br><br><h3>  Dienstleistungen: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_service.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: rabbitmq-internal labels: app: rabbitmq spec: clusterIP: None ports: - name: http protocol: TCP port: 15672 - name: amqp protocol: TCP port: 5672 selector: app: rabbitmq</code> </pre> </div></div><br>  Wir erstellen einen internen Headless-Service, über den das Peer Discovery-Plugin funktioniert. <br><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_service_ext.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: rabbitmq labels: app: rabbitmq type: LoadBalancer spec: type: NodePort ports: - name: http protocol: TCP port: 15672 targetPort: 15672 nodePort: 31673 - name: amqp protocol: TCP port: 5672 targetPort: 5672 nodePort: 30673 selector: app: rabbitmq</code> </pre> </div></div><br>  Damit Anwendungen in K8s mit unserem Cluster funktionieren, erstellen wir einen Balancer-Service. <br><br>  Da wir außerhalb von K8s Zugriff auf den RabbitMQ-Cluster benötigen, rollen wir über NodePort.  RabbitMQ ist verfügbar, wenn auf einen beliebigen Knoten des K8s-Clusters an den Ports 31673 und 30673 zugegriffen wird. In der Praxis besteht hierfür kein großer Bedarf.  Die Frage nach der Benutzerfreundlichkeit des RabbitMQ-Admin-Panels. <br><br>  Beim Erstellen eines Dienstes mit dem NodePort-Typ in K8s wird implizit auch ein Dienst mit dem ClusterIP-Typ erstellt, um ihn bereitzustellen.  Daher können Anwendungen in K8s, die mit unserem RabbitMQ arbeiten müssen, unter <i>amqp: // rabbitmq: 5672</i> auf den Cluster <i>zugreifen</i> <br><br><h3>  Konfiguration: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_configmap.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: v1 kind: ConfigMap metadata: name: rabbitmq-config data: enabled_plugins: | [rabbitmq_management,rabbitmq_peer_discovery_k8s]. rabbitmq.conf: | cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s cluster_formation.k8s.host = kubernetes.default.svc.cluster.local cluster_formation.k8s.port = 443 ### cluster_formation.k8s.address_type = ip cluster_formation.k8s.address_type = hostname cluster_formation.node_cleanup.interval = 10 cluster_formation.node_cleanup.only_log_warning = true cluster_partition_handling = autoheal queue_master_locator=min-masters cluster_formation.randomized_startup_delay_range.min = 0 cluster_formation.randomized_startup_delay_range.max = 2 cluster_formation.k8s.service_name = rabbitmq-internal cluster_formation.k8s.hostname_suffix = .rabbitmq-internal.our-namespace.svc.cluster.local</code> </pre> </div></div><br>  Wir erstellen RabbitMQ-Konfigurationsdateien.  Die Hauptmagie. <br><br><pre> <code class="plaintext hljs">enabled_plugins: | [rabbitmq_management,rabbitmq_peer_discovery_k8s].</code> </pre><br>  Fügen Sie die erforderlichen Plugins zu den zum Herunterladen zugelassenen hinzu.  Jetzt können wir die automatische Peer Discovery im K8S verwenden. <br><br><pre> <code class="plaintext hljs">cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s</code> </pre><br>  Wir stellen das erforderliche Plugin als Backend für die Peer-Erkennung zur Verfügung. <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.host = kubernetes.default.svc.cluster.local cluster_formation.k8s.port = 443</code> </pre><br>  Geben Sie die Adresse und den Port an, über die Sie kubernetes apiserver erreichen können.  Hier können Sie die IP-Adresse direkt angeben, aber es ist schöner, dies zu tun. <br><br>  In der Standardeinstellung für Namespaces wird normalerweise ein Dienst mit dem Namen kubernetes erstellt, der zu k8-apiserver führt.  In verschiedenen K8S-Installationsoptionen können Namespace, Dienstname und Port unterschiedlich sein.  Wenn etwas in einer bestimmten Installation anders ist, müssen Sie es entsprechend beheben. <br><br>  Zum Beispiel sind wir mit der Tatsache konfrontiert, dass sich der Dienst in einigen Clustern auf Port 443 und in einigen auf 6443 befindet. Es ist möglich zu verstehen, dass in den RabbitMQ-Startprotokollen etwas nicht stimmt. Die Verbindungszeit zu der hier angegebenen Adresse ist dort deutlich hervorgehoben. <br><br><pre> <code class="plaintext hljs">### cluster_formation.k8s.address_type = ip cluster_formation.k8s.address_type = hostname</code> </pre><br>  Standardmäßig wurde im Beispiel der Adresstyp des RabbitMQ-Clusterknotens nach IP-Adresse angegeben.  Wenn Sie den Pod jedoch neu starten, erhält er jedes Mal eine neue IP.  Überraschung!  Der Cluster stirbt vor Qual. <br><br>  Ändern Sie die Adressierung in Hostname.  StatefulSet garantiert uns die Unveränderlichkeit des Hostnamens innerhalb des Lebenszyklus des gesamten StatefulSet, was vollständig zu uns passt. <br><br><pre> <code class="plaintext hljs">cluster_formation.node_cleanup.interval = 10 cluster_formation.node_cleanup.only_log_warning = true</code> </pre><br>  Wenn wir davon ausgehen, dass einer der Knoten früher oder später wiederhergestellt wird, deaktivieren wir die Selbstlöschung durch einen Cluster unzugänglicher Knoten.  In diesem Fall tritt der Knoten, sobald er online ist, in den Cluster ein, ohne seinen vorherigen Status zu verlieren. <br><br><pre> <code class="plaintext hljs">cluster_partition_handling = autoheal</code> </pre> <br>  Dieser Parameter bestimmt die Aktionen des Clusters bei Verlust des Quorums.  Hier müssen Sie nur die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation zu diesem Thema</a> lesen und selbst verstehen, was einem bestimmten Anwendungsfall näher kommt. <br><br><pre> <code class="plaintext hljs">queue_master_locator=min-masters</code> </pre> <br>  Bestimmen Sie die Assistentenauswahl für neue Warteschlangen.  Mit dieser Einstellung wählt der Assistent den Knoten mit der geringsten Anzahl von Warteschlangen aus, sodass die Warteschlangen gleichmäßig auf die Clusterknoten verteilt werden. <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.service_name = rabbitmq-internal</code> </pre> <br>  Wir nennen den kopflosen K8-Dienst (von uns zuvor erstellt), über den die RabbitMQ-Knoten miteinander kommunizieren. <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.hostname_suffix = .rabbitmq-internal.our-namespace.svc.cluster.local</code> </pre> <br>  Eine wichtige Sache für die Adressierung in einem Cluster ist der Hostname.  Der FQDN des K8-Herdes wird als Kurzname (rabbitmq-0, rabbitmq-1) + Suffix (Domänenteil) gebildet.  Hier geben wir dieses Suffix an.  In K8S sieht es so aus <b>: &lt;Dienstname&gt;. &lt;Namespace-Name&gt; .svc.cluster.local</b> <br><br>  kube-dns löst Namen der Form rabbitmq-0.rabbitmq-internal.our-namespace.svc.cluster.local ohne zusätzliche Konfiguration in die IP-Adresse eines bestimmten Pods auf, wodurch die Magie des Clustering nach Hostnamen möglich wird. <br><br><h3>  StatefulSet RabbitMQ-Konfiguration: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_statefulset.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: rabbitmq spec: serviceName: rabbitmq-internal replicas: 3 template: metadata: labels: app: rabbitmq annotations: scheduler.alpha.kubernetes.io/affinity: &gt; { "podAntiAffinity": { "requiredDuringSchedulingIgnoredDuringExecution": [{ "labelSelector": { "matchExpressions": [{ "key": "app", "operator": "In", "values": ["rabbitmq"] }] }, "topologyKey": "kubernetes.io/hostname" }] } } spec: serviceAccountName: rabbitmq terminationGracePeriodSeconds: 10 containers: - name: rabbitmq-k8s image: rabbitmq:3.7 volumeMounts: - name: config-volume mountPath: /etc/rabbitmq - name: rabbitmq-data mountPath: /var/lib/rabbitmq/mnesia ports: - name: http protocol: TCP containerPort: 15672 - name: amqp protocol: TCP containerPort: 5672 livenessProbe: exec: command: ["rabbitmqctl", "status"] initialDelaySeconds: 60 periodSeconds: 10 timeoutSeconds: 10 readinessProbe: exec: command: ["rabbitmqctl", "status"] initialDelaySeconds: 10 periodSeconds: 10 timeoutSeconds: 10 imagePullPolicy: Always env: - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: HOSTNAME valueFrom: fieldRef: fieldPath: metadata.name - name: NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: RABBITMQ_USE_LONGNAME value: "true" - name: RABBITMQ_NODENAME value: "rabbit@$(HOSTNAME).rabbitmq-internal.$(NAMESPACE).svc.cluster.local" - name: K8S_SERVICE_NAME value: "rabbitmq-internal" - name: RABBITMQ_ERLANG_COOKIE value: "mycookie" volumes: - name: config-volume configMap: name: rabbitmq-config items: - key: rabbitmq.conf path: rabbitmq.conf - key: enabled_plugins path: enabled_plugins - name: rabbitmq-data persistentVolumeClaim: claimName: rabbitmq-data</code> </pre> </div></div><br>  Eigentlich StatefulSet selbst.  Wir stellen interessante Punkte fest. <br><br><pre> <code class="plaintext hljs">serviceName: rabbitmq-internal</code> </pre> <br>  Wir schreiben den Namen des kopflosen Dienstes, über den Pods in StatefulSet kommunizieren. <br><br><pre> <code class="plaintext hljs">replicas: 3</code> </pre> <br>  Legen Sie die Anzahl der Replikate im Cluster fest.  In unserem Land entspricht dies der Anzahl der K8-Arbeitsknoten. <br><br><pre> <code class="plaintext hljs">annotations: scheduler.alpha.kubernetes.io/affinity: &gt; { "podAntiAffinity": { "requiredDuringSchedulingIgnoredDuringExecution": [{ "labelSelector": { "matchExpressions": [{ "key": "app", "operator": "In", "values": ["rabbitmq"] }] }, "topologyKey": "kubernetes.io/hostname" }] } }</code> </pre> <br>  Wenn einer der K8s-Knoten ausfällt, versucht das Statefulset, die Anzahl der Instanzen in der Menge beizubehalten. Daher werden mehrere Herde auf demselben K8s-Knoten erstellt.  Dieses Verhalten ist völlig unerwünscht und grundsätzlich sinnlos.  Daher schreiben wir eine Anti-Affinitätsregel für Herd-Sets von Statefulset vor.  Wir machen die Regel hart (Erforderlich), damit der Kube-Scheduler sie bei der Planung von Pods nicht brechen kann. <br><br>  Das Wesentliche ist einfach: Es ist dem Scheduler untersagt, (innerhalb des Namespace) mehr als einen Pod mit dem <i>Tag app: rabbitmq</i> auf jedem Knoten zu platzieren.  Wir unterscheiden die <i>Knoten</i> durch den Wert der Bezeichnung <i>kubernetes.io/hostname</i> .  Wenn nun aus irgendeinem Grund die Anzahl der funktionierenden K8S-Knoten geringer ist als die erforderliche Anzahl von Replikaten in StatefulSet, werden neue Replikate erst erstellt, wenn wieder ein freier Knoten angezeigt wird. <br><br><pre> <code class="plaintext hljs">serviceAccountName: rabbitmq</code> </pre> <br>  Wir registrieren ServiceAccount, unter dem unsere Pods arbeiten. <br><br><pre> <code class="plaintext hljs">image: rabbitmq:3.7</code> </pre> <br>  Das Image von RabbitMQ ist vollständig Standard und stammt aus dem Docker-Hub. Es erfordert keine Neuerstellung und Dateirevision. <br><br><pre> <code class="plaintext hljs">- name: rabbitmq-data mountPath: /var/lib/rabbitmq/mnesia</code> </pre><br>  Persistente Daten von RabbitMQ werden in / var / lib / rabbitmq / mnesia gespeichert.  Hier stellen wir unseren Persistent Volume Claim in diesen Ordner, damit beim Neustart der Herde / Knoten oder sogar des gesamten StatefulSet die Daten (sowohl der Dienst, einschließlich des zusammengestellten Clusters, als auch die Benutzerdaten) sicher und zuverlässig sind.  Es gibt einige Beispiele, bei denen der gesamte Ordner / var / lib / rabbitmq / persistent gemacht wird.  Wir kamen zu dem Schluss, dass dies nicht die beste Idee ist, da gleichzeitig alle von den Kaninchenkonfigurationen gesetzten Informationen in Erinnerung bleiben.  Das heißt, um etwas in der Konfigurationsdatei zu ändern, müssen Sie den persistenten Speicher bereinigen, was im Betrieb sehr unpraktisch ist. <br><br><pre> <code class="plaintext hljs"> - name: HOSTNAME valueFrom: fieldRef: fieldPath: metadata.name - name: NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: RABBITMQ_USE_LONGNAME value: "true" - name: RABBITMQ_NODENAME value: "rabbit@$(HOSTNAME).rabbitmq-internal.$(NAMESPACE).svc.cluster.local"</code> </pre><br>  Mit diesem Satz von Umgebungsvariablen weisen wir RabbitMQ zunächst an, den FQDN-Namen als Kennung für die Clustermitglieder zu verwenden, und zweitens legen wir das Format dieses Namens fest.  Das Format wurde bereits beim Parsen der Konfiguration beschrieben. <br><br><pre> <code class="plaintext hljs">- name: K8S_SERVICE_NAME value: "rabbitmq-internal"</code> </pre> <br>  Der Name des kopflosen Dienstes für die Kommunikation zwischen Clustermitgliedern. <br><br><pre> <code class="plaintext hljs">- name: RABBITMQ_ERLANG_COOKIE value: "mycookie"</code> </pre> <br>  Der Inhalt des Erlang-Cookies sollte auf allen Knoten des Clusters gleich sein. Sie müssen Ihren eigenen Wert registrieren.  Ein Knoten mit einem anderen Cookie kann den Cluster nicht betreten. <br><br><pre> <code class="plaintext hljs">volumes: - name: rabbitmq-data persistentVolumeClaim: claimName: rabbitmq-data</code> </pre> <br>  Definieren Sie das zugeordnete Volume aus dem zuvor erstellten Persistent Volume Claim. <br><br>  Hier sind wir mit dem Setup in den K8 fertig.  Das Ergebnis ist ein RabbitMQ-Cluster, der die Warteschlangen gleichmäßig auf die Knoten verteilt und gegen Probleme in der Laufzeitumgebung resistent ist. <br><br><img src="https://habrastorage.org/webt/_j/ky/mw/_jkymwmxe7syyjfa7h2idbcxosc.png" alt="Bild"><br><br>  Wenn einer der Clusterknoten nicht verfügbar ist, ist der Zugriff auf die darin enthaltenen Warteschlangen nicht mehr möglich, alles andere funktioniert weiterhin.  Sobald der Knoten wieder in Betrieb ist, kehrt er zum Cluster zurück, und die Warteschlangen, für die er ein Master war, werden wieder betriebsbereit, wobei alle darin enthaltenen Daten erhalten bleiben (wenn der persistente Speicher natürlich nicht beschädigt wurde).  Alle diese Prozesse sind vollautomatisch und erfordern kein Eingreifen. <br><br><h2>  Bonus: HA anpassen </h2><br>  Eines der Projekte war eine Nuance.  Die Anforderungen ergaben eine vollständige Spiegelung aller im Cluster enthaltenen Daten.  Dies ist erforderlich, damit in einer Situation, in der mindestens ein Clusterknoten betriebsbereit ist, aus Sicht der Anwendung weiterhin alles funktioniert.  Dieser Moment hat nichts mit K8s zu tun, wir beschreiben ihn einfach als Mini-How-to. <br><br>  Um die vollständige HA zu aktivieren, müssen Sie im RabbitMQ-Dashboard auf der Registerkarte <i>Admin -&gt; Richtlinien</i> eine Richtlinie erstellen.  Der Name ist beliebig, das Muster ist leer (alle Warteschlangen), in den Definitionen werden zwei Parameter hinzugefügt: <i>ha-Modus: alle</i> , <i>ha-Sync-Modus: automatisch</i> . <br><br><img src="https://habrastorage.org/webt/jz/tn/vu/jztnvu5zygtv56hurbyss1w9ljm.png" alt="Bild"><br><br><img src="https://habrastorage.org/webt/_6/in/om/_6inoma38lvluhpaet1g66uus_u.png" alt="Bild"><br><br>  Danach befinden sich alle im Cluster erstellten Warteschlangen im Hochverfügbarkeitsmodus: Wenn der Masterknoten nicht verfügbar ist, wird einer der Slaves vom neuen Assistenten automatisch ausgewählt.  Die in die Warteschlange eingehenden Daten werden auf alle Knoten des Clusters gespiegelt.  Was in der Tat erforderlich war, um zu erhalten. <br><br><img src="https://habrastorage.org/webt/0v/m3/je/0vm3jem0bi4fqckj8ucmiy5zcxe.png" alt="Bild"><br><br>  Lesen Sie hier mehr über HA in RabbitMQ <br><br><h2>  Nützliche Literatur: </h2><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Repository RabbitMQ Peer Discovery Kubernetes Plugin</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Konfigurationsbeispiel für die RabbitMQ-Bereitstellung in K8S</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beschreibung der Prinzipien der Clusterbildung, des Peer Discovery-Mechanismus und der Plugins dafür</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Epische Diskussion über das richtige Setup auf Hostnamenbasis</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RabbitMQ-Clustering-Handbuch</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beschreibung von Split-Brain-Clustering-Problemen und -Lösungen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hochverfügbarkeitswarteschlangen bei RabbitMQ</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Richtlinien konfigurieren</a> </li></ul><br>  Viel Glück! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de419817/">https://habr.com/ru/post/de419817/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de419805/index.html">Der Super Tiny Compiler - jetzt in russischer Sprache</a></li>
<li><a href="../de419807/index.html">Glaukom - wie man nicht erblindet: Sprechen wir über die Behandlung ...</a></li>
<li><a href="../de419811/index.html">Die Entwicklung flexibler Displays</a></li>
<li><a href="../de419813/index.html">Skillbox-Webinare: Auswahl am Freitag</a></li>
<li><a href="../de419815/index.html">Geheimnisse der Fehlertoleranz unseres Front Office</a></li>
<li><a href="../de419819/index.html">Biomarker des Alterns. Panel Gebrechlichkeit. Teil 2</a></li>
<li><a href="../de419823/index.html">Ungewöhnliche Duett - Passphrasen und Gedächtnisbilder</a></li>
<li><a href="../de419825/index.html">Testen der Leistung verschiedener Laufwerkstypen in einer virtuellen Umgebung</a></li>
<li><a href="../de419829/index.html">Die Standardschlüsselverschlüsselung von OpenSSH ist schlechter als keine</a></li>
<li><a href="../de419831/index.html">So funktioniert JS: Benutzerdefinierte Elemente</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>