<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üîò üë®‚Äçüë©‚Äçüë¶ üõÄüèΩ Starten des RabbitMQ-Clusters in Kubernetes üßëüèº‚Äçü§ù‚Äçüßëüèº üóæ üë©üèΩ‚Äç‚úàÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Im Fall der Microservice-Organisation der Anwendung beruht ein erheblicher Arbeitsaufwand auf den Mechanismen der Integrationsverbindung von Microserv...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Starten des RabbitMQ-Clusters in Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/true_engineering/blog/419817/">  Im Fall der Microservice-Organisation der Anwendung beruht ein erheblicher Arbeitsaufwand auf den Mechanismen der Integrationsverbindung von Microservices.  Dar√ºber hinaus sollte diese Integration fehlertolerant sein und einen hohen Grad an Verf√ºgbarkeit aufweisen. <br><br>  In unseren L√∂sungen verwenden wir die Integration mit Kafka, gRPC und RabbitMQ. <br><br>  In diesem Artikel werden wir unsere Erfahrungen mit dem Clustering von RabbitMQ teilen, dessen Knoten auf Kubernetes gehostet werden. <br><br><img src="https://habrastorage.org/webt/dx/ll/-h/dxll-hzomoco0zcfp7esju8pena.jpeg" alt="Bild"><br><br>  Vor RabbitMQ Version 3.7 war das Clustering in K8S keine sehr triviale Aufgabe, mit vielen Hacks und nicht sehr sch√∂nen L√∂sungen.  In Version 3.6 wurde ein Autocluster-Plugin von RabbitMQ Community verwendet.  Und in 3.7 erschien das Kubernetes Peer Discovery Backend.  Es ist durch das Plug-In in der Basisauslieferung von RabbitMQ integriert und erfordert keine separate Montage und Installation. <br><br>  Wir werden die endg√ºltige Konfiguration als Ganzes beschreiben und kommentieren, was passiert. <br><a name="habracut"></a><br><h2>  In der Theorie </h2><br>  Das Plugin verf√ºgt √ºber ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Repository auf dem Github</a> , in dem es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ein Beispiel f√ºr die grundlegende Verwendung gibt</a> . <br>  Dieses Beispiel ist nicht f√ºr die Produktion gedacht, was in seiner Beschreibung klar angegeben ist, und au√üerdem sind einige der darin enthaltenen Einstellungen entgegen der Verwendungslogik im Produkt festgelegt.  In diesem Beispiel wird die Persistenz des Speichers √ºberhaupt nicht erw√§hnt, sodass sich unser Cluster in jeder Notsituation in einen Zilch verwandelt. <br><br><h2>  In der Praxis </h2><br>  Jetzt erfahren Sie, was Sie selbst gesehen haben und wie Sie RabbitMQ installieren und konfigurieren. <br><br>  Beschreiben wir die Konfigurationen aller Teile von RabbitMQ als Service in K8s.  Wir werden sofort klarstellen, dass wir RabbitMQ in K8s als StatefulSet installiert haben.  Auf jedem Knoten des K8s-Clusters funktioniert immer eine Instanz von RabbitMQ (ein Knoten in der klassischen Clusterkonfiguration).  Wir werden auch das RabbitMQ-Control Panel in K8s installieren und Zugriff auf dieses Panel au√üerhalb des Clusters gew√§hren. <br><br><h3>  Rechte und Rollen: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_rbac.yaml</b> <div class="spoiler_text"><pre><code class="plaintext hljs">--- apiVersion: v1 kind: ServiceAccount metadata: name: rabbitmq --- kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: endpoint-reader rules: - apiGroups: [""] resources: ["endpoints"] verbs: ["get"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: endpoint-reader subjects: - kind: ServiceAccount name: rabbitmq roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: endpoint-reader</code> </pre> </div></div><br>  Die Zugriffsrechte f√ºr RabbitMQ stammen vollst√§ndig aus dem Beispiel, es sind keine √Ñnderungen erforderlich.  Wir erstellen ein ServiceAccount f√ºr unseren Cluster und erteilen ihm Endberechtigungen f√ºr Endpoints K8s. <br><br><h3>  Dauerhafte Speicherung: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_pv.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: PersistentVolume apiVersion: v1 metadata: name: rabbitmq-data-sigma labels: type: local annotations: volume.alpha.kubernetes.io/storage-class: rabbitmq-data-sigma spec: storageClassName: rabbitmq-data-sigma capacity: storage: 10Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Recycle hostPath: path: "/opt/rabbitmq-data-sigma"</code> </pre> </div></div><br>  Hier haben wir den einfachsten Fall als persistenten Speicher genommen - hostPath (ein regul√§rer Ordner auf jedem K8s-Knoten), aber Sie k√∂nnen jeden der vielen Arten von persistenten Volumes verwenden, die von K8s unterst√ºtzt werden. <br><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_pvc.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: PersistentVolumeClaim apiVersion: v1 metadata: name: rabbitmq-data spec: storageClassName: rabbitmq-data-sigma accessModes: - ReadWriteMany resources: requests: storage: 10Gi</code> </pre> </div></div><br>  Volume erstellen Anspruch auf das im vorherigen Schritt erstellte Volume.  Dieser Anspruch wird dann in StatefulSet als persistenter Datenspeicher verwendet. <br><br><h3>  Dienstleistungen: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_service.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: rabbitmq-internal labels: app: rabbitmq spec: clusterIP: None ports: - name: http protocol: TCP port: 15672 - name: amqp protocol: TCP port: 5672 selector: app: rabbitmq</code> </pre> </div></div><br>  Wir erstellen einen internen Headless-Service, √ºber den das Peer Discovery-Plugin funktioniert. <br><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_service_ext.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: rabbitmq labels: app: rabbitmq type: LoadBalancer spec: type: NodePort ports: - name: http protocol: TCP port: 15672 targetPort: 15672 nodePort: 31673 - name: amqp protocol: TCP port: 5672 targetPort: 5672 nodePort: 30673 selector: app: rabbitmq</code> </pre> </div></div><br>  Damit Anwendungen in K8s mit unserem Cluster funktionieren, erstellen wir einen Balancer-Service. <br><br>  Da wir au√üerhalb von K8s Zugriff auf den RabbitMQ-Cluster ben√∂tigen, rollen wir √ºber NodePort.  RabbitMQ ist verf√ºgbar, wenn auf einen beliebigen Knoten des K8s-Clusters an den Ports 31673 und 30673 zugegriffen wird. In der Praxis besteht hierf√ºr kein gro√üer Bedarf.  Die Frage nach der Benutzerfreundlichkeit des RabbitMQ-Admin-Panels. <br><br>  Beim Erstellen eines Dienstes mit dem NodePort-Typ in K8s wird implizit auch ein Dienst mit dem ClusterIP-Typ erstellt, um ihn bereitzustellen.  Daher k√∂nnen Anwendungen in K8s, die mit unserem RabbitMQ arbeiten m√ºssen, unter <i>amqp: // rabbitmq: 5672</i> auf den Cluster <i>zugreifen</i> <br><br><h3>  Konfiguration: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_configmap.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: v1 kind: ConfigMap metadata: name: rabbitmq-config data: enabled_plugins: | [rabbitmq_management,rabbitmq_peer_discovery_k8s]. rabbitmq.conf: | cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s cluster_formation.k8s.host = kubernetes.default.svc.cluster.local cluster_formation.k8s.port = 443 ### cluster_formation.k8s.address_type = ip cluster_formation.k8s.address_type = hostname cluster_formation.node_cleanup.interval = 10 cluster_formation.node_cleanup.only_log_warning = true cluster_partition_handling = autoheal queue_master_locator=min-masters cluster_formation.randomized_startup_delay_range.min = 0 cluster_formation.randomized_startup_delay_range.max = 2 cluster_formation.k8s.service_name = rabbitmq-internal cluster_formation.k8s.hostname_suffix = .rabbitmq-internal.our-namespace.svc.cluster.local</code> </pre> </div></div><br>  Wir erstellen RabbitMQ-Konfigurationsdateien.  Die Hauptmagie. <br><br><pre> <code class="plaintext hljs">enabled_plugins: | [rabbitmq_management,rabbitmq_peer_discovery_k8s].</code> </pre><br>  F√ºgen Sie die erforderlichen Plugins zu den zum Herunterladen zugelassenen hinzu.  Jetzt k√∂nnen wir die automatische Peer Discovery im K8S verwenden. <br><br><pre> <code class="plaintext hljs">cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s</code> </pre><br>  Wir stellen das erforderliche Plugin als Backend f√ºr die Peer-Erkennung zur Verf√ºgung. <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.host = kubernetes.default.svc.cluster.local cluster_formation.k8s.port = 443</code> </pre><br>  Geben Sie die Adresse und den Port an, √ºber die Sie kubernetes apiserver erreichen k√∂nnen.  Hier k√∂nnen Sie die IP-Adresse direkt angeben, aber es ist sch√∂ner, dies zu tun. <br><br>  In der Standardeinstellung f√ºr Namespaces wird normalerweise ein Dienst mit dem Namen kubernetes erstellt, der zu k8-apiserver f√ºhrt.  In verschiedenen K8S-Installationsoptionen k√∂nnen Namespace, Dienstname und Port unterschiedlich sein.  Wenn etwas in einer bestimmten Installation anders ist, m√ºssen Sie es entsprechend beheben. <br><br>  Zum Beispiel sind wir mit der Tatsache konfrontiert, dass sich der Dienst in einigen Clustern auf Port 443 und in einigen auf 6443 befindet. Es ist m√∂glich zu verstehen, dass in den RabbitMQ-Startprotokollen etwas nicht stimmt. Die Verbindungszeit zu der hier angegebenen Adresse ist dort deutlich hervorgehoben. <br><br><pre> <code class="plaintext hljs">### cluster_formation.k8s.address_type = ip cluster_formation.k8s.address_type = hostname</code> </pre><br>  Standardm√§√üig wurde im Beispiel der Adresstyp des RabbitMQ-Clusterknotens nach IP-Adresse angegeben.  Wenn Sie den Pod jedoch neu starten, erh√§lt er jedes Mal eine neue IP.  √úberraschung!  Der Cluster stirbt vor Qual. <br><br>  √Ñndern Sie die Adressierung in Hostname.  StatefulSet garantiert uns die Unver√§nderlichkeit des Hostnamens innerhalb des Lebenszyklus des gesamten StatefulSet, was vollst√§ndig zu uns passt. <br><br><pre> <code class="plaintext hljs">cluster_formation.node_cleanup.interval = 10 cluster_formation.node_cleanup.only_log_warning = true</code> </pre><br>  Wenn wir davon ausgehen, dass einer der Knoten fr√ºher oder sp√§ter wiederhergestellt wird, deaktivieren wir die Selbstl√∂schung durch einen Cluster unzug√§nglicher Knoten.  In diesem Fall tritt der Knoten, sobald er online ist, in den Cluster ein, ohne seinen vorherigen Status zu verlieren. <br><br><pre> <code class="plaintext hljs">cluster_partition_handling = autoheal</code> </pre> <br>  Dieser Parameter bestimmt die Aktionen des Clusters bei Verlust des Quorums.  Hier m√ºssen Sie nur die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation zu diesem Thema</a> lesen und selbst verstehen, was einem bestimmten Anwendungsfall n√§her kommt. <br><br><pre> <code class="plaintext hljs">queue_master_locator=min-masters</code> </pre> <br>  Bestimmen Sie die Assistentenauswahl f√ºr neue Warteschlangen.  Mit dieser Einstellung w√§hlt der Assistent den Knoten mit der geringsten Anzahl von Warteschlangen aus, sodass die Warteschlangen gleichm√§√üig auf die Clusterknoten verteilt werden. <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.service_name = rabbitmq-internal</code> </pre> <br>  Wir nennen den kopflosen K8-Dienst (von uns zuvor erstellt), √ºber den die RabbitMQ-Knoten miteinander kommunizieren. <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.hostname_suffix = .rabbitmq-internal.our-namespace.svc.cluster.local</code> </pre> <br>  Eine wichtige Sache f√ºr die Adressierung in einem Cluster ist der Hostname.  Der FQDN des K8-Herdes wird als Kurzname (rabbitmq-0, rabbitmq-1) + Suffix (Dom√§nenteil) gebildet.  Hier geben wir dieses Suffix an.  In K8S sieht es so aus <b>: &lt;Dienstname&gt;. &lt;Namespace-Name&gt; .svc.cluster.local</b> <br><br>  kube-dns l√∂st Namen der Form rabbitmq-0.rabbitmq-internal.our-namespace.svc.cluster.local ohne zus√§tzliche Konfiguration in die IP-Adresse eines bestimmten Pods auf, wodurch die Magie des Clustering nach Hostnamen m√∂glich wird. <br><br><h3>  StatefulSet RabbitMQ-Konfiguration: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_statefulset.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: rabbitmq spec: serviceName: rabbitmq-internal replicas: 3 template: metadata: labels: app: rabbitmq annotations: scheduler.alpha.kubernetes.io/affinity: &gt; { "podAntiAffinity": { "requiredDuringSchedulingIgnoredDuringExecution": [{ "labelSelector": { "matchExpressions": [{ "key": "app", "operator": "In", "values": ["rabbitmq"] }] }, "topologyKey": "kubernetes.io/hostname" }] } } spec: serviceAccountName: rabbitmq terminationGracePeriodSeconds: 10 containers: - name: rabbitmq-k8s image: rabbitmq:3.7 volumeMounts: - name: config-volume mountPath: /etc/rabbitmq - name: rabbitmq-data mountPath: /var/lib/rabbitmq/mnesia ports: - name: http protocol: TCP containerPort: 15672 - name: amqp protocol: TCP containerPort: 5672 livenessProbe: exec: command: ["rabbitmqctl", "status"] initialDelaySeconds: 60 periodSeconds: 10 timeoutSeconds: 10 readinessProbe: exec: command: ["rabbitmqctl", "status"] initialDelaySeconds: 10 periodSeconds: 10 timeoutSeconds: 10 imagePullPolicy: Always env: - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: HOSTNAME valueFrom: fieldRef: fieldPath: metadata.name - name: NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: RABBITMQ_USE_LONGNAME value: "true" - name: RABBITMQ_NODENAME value: "rabbit@$(HOSTNAME).rabbitmq-internal.$(NAMESPACE).svc.cluster.local" - name: K8S_SERVICE_NAME value: "rabbitmq-internal" - name: RABBITMQ_ERLANG_COOKIE value: "mycookie" volumes: - name: config-volume configMap: name: rabbitmq-config items: - key: rabbitmq.conf path: rabbitmq.conf - key: enabled_plugins path: enabled_plugins - name: rabbitmq-data persistentVolumeClaim: claimName: rabbitmq-data</code> </pre> </div></div><br>  Eigentlich StatefulSet selbst.  Wir stellen interessante Punkte fest. <br><br><pre> <code class="plaintext hljs">serviceName: rabbitmq-internal</code> </pre> <br>  Wir schreiben den Namen des kopflosen Dienstes, √ºber den Pods in StatefulSet kommunizieren. <br><br><pre> <code class="plaintext hljs">replicas: 3</code> </pre> <br>  Legen Sie die Anzahl der Replikate im Cluster fest.  In unserem Land entspricht dies der Anzahl der K8-Arbeitsknoten. <br><br><pre> <code class="plaintext hljs">annotations: scheduler.alpha.kubernetes.io/affinity: &gt; { "podAntiAffinity": { "requiredDuringSchedulingIgnoredDuringExecution": [{ "labelSelector": { "matchExpressions": [{ "key": "app", "operator": "In", "values": ["rabbitmq"] }] }, "topologyKey": "kubernetes.io/hostname" }] } }</code> </pre> <br>  Wenn einer der K8s-Knoten ausf√§llt, versucht das Statefulset, die Anzahl der Instanzen in der Menge beizubehalten. Daher werden mehrere Herde auf demselben K8s-Knoten erstellt.  Dieses Verhalten ist v√∂llig unerw√ºnscht und grunds√§tzlich sinnlos.  Daher schreiben wir eine Anti-Affinit√§tsregel f√ºr Herd-Sets von Statefulset vor.  Wir machen die Regel hart (Erforderlich), damit der Kube-Scheduler sie bei der Planung von Pods nicht brechen kann. <br><br>  Das Wesentliche ist einfach: Es ist dem Scheduler untersagt, (innerhalb des Namespace) mehr als einen Pod mit dem <i>Tag app: rabbitmq</i> auf jedem Knoten zu platzieren.  Wir unterscheiden die <i>Knoten</i> durch den Wert der Bezeichnung <i>kubernetes.io/hostname</i> .  Wenn nun aus irgendeinem Grund die Anzahl der funktionierenden K8S-Knoten geringer ist als die erforderliche Anzahl von Replikaten in StatefulSet, werden neue Replikate erst erstellt, wenn wieder ein freier Knoten angezeigt wird. <br><br><pre> <code class="plaintext hljs">serviceAccountName: rabbitmq</code> </pre> <br>  Wir registrieren ServiceAccount, unter dem unsere Pods arbeiten. <br><br><pre> <code class="plaintext hljs">image: rabbitmq:3.7</code> </pre> <br>  Das Image von RabbitMQ ist vollst√§ndig Standard und stammt aus dem Docker-Hub. Es erfordert keine Neuerstellung und Dateirevision. <br><br><pre> <code class="plaintext hljs">- name: rabbitmq-data mountPath: /var/lib/rabbitmq/mnesia</code> </pre><br>  Persistente Daten von RabbitMQ werden in / var / lib / rabbitmq / mnesia gespeichert.  Hier stellen wir unseren Persistent Volume Claim in diesen Ordner, damit beim Neustart der Herde / Knoten oder sogar des gesamten StatefulSet die Daten (sowohl der Dienst, einschlie√ülich des zusammengestellten Clusters, als auch die Benutzerdaten) sicher und zuverl√§ssig sind.  Es gibt einige Beispiele, bei denen der gesamte Ordner / var / lib / rabbitmq / persistent gemacht wird.  Wir kamen zu dem Schluss, dass dies nicht die beste Idee ist, da gleichzeitig alle von den Kaninchenkonfigurationen gesetzten Informationen in Erinnerung bleiben.  Das hei√üt, um etwas in der Konfigurationsdatei zu √§ndern, m√ºssen Sie den persistenten Speicher bereinigen, was im Betrieb sehr unpraktisch ist. <br><br><pre> <code class="plaintext hljs"> - name: HOSTNAME valueFrom: fieldRef: fieldPath: metadata.name - name: NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: RABBITMQ_USE_LONGNAME value: "true" - name: RABBITMQ_NODENAME value: "rabbit@$(HOSTNAME).rabbitmq-internal.$(NAMESPACE).svc.cluster.local"</code> </pre><br>  Mit diesem Satz von Umgebungsvariablen weisen wir RabbitMQ zun√§chst an, den FQDN-Namen als Kennung f√ºr die Clustermitglieder zu verwenden, und zweitens legen wir das Format dieses Namens fest.  Das Format wurde bereits beim Parsen der Konfiguration beschrieben. <br><br><pre> <code class="plaintext hljs">- name: K8S_SERVICE_NAME value: "rabbitmq-internal"</code> </pre> <br>  Der Name des kopflosen Dienstes f√ºr die Kommunikation zwischen Clustermitgliedern. <br><br><pre> <code class="plaintext hljs">- name: RABBITMQ_ERLANG_COOKIE value: "mycookie"</code> </pre> <br>  Der Inhalt des Erlang-Cookies sollte auf allen Knoten des Clusters gleich sein. Sie m√ºssen Ihren eigenen Wert registrieren.  Ein Knoten mit einem anderen Cookie kann den Cluster nicht betreten. <br><br><pre> <code class="plaintext hljs">volumes: - name: rabbitmq-data persistentVolumeClaim: claimName: rabbitmq-data</code> </pre> <br>  Definieren Sie das zugeordnete Volume aus dem zuvor erstellten Persistent Volume Claim. <br><br>  Hier sind wir mit dem Setup in den K8 fertig.  Das Ergebnis ist ein RabbitMQ-Cluster, der die Warteschlangen gleichm√§√üig auf die Knoten verteilt und gegen Probleme in der Laufzeitumgebung resistent ist. <br><br><img src="https://habrastorage.org/webt/_j/ky/mw/_jkymwmxe7syyjfa7h2idbcxosc.png" alt="Bild"><br><br>  Wenn einer der Clusterknoten nicht verf√ºgbar ist, ist der Zugriff auf die darin enthaltenen Warteschlangen nicht mehr m√∂glich, alles andere funktioniert weiterhin.  Sobald der Knoten wieder in Betrieb ist, kehrt er zum Cluster zur√ºck, und die Warteschlangen, f√ºr die er ein Master war, werden wieder betriebsbereit, wobei alle darin enthaltenen Daten erhalten bleiben (wenn der persistente Speicher nat√ºrlich nicht besch√§digt wurde).  Alle diese Prozesse sind vollautomatisch und erfordern kein Eingreifen. <br><br><h2>  Bonus: HA anpassen </h2><br>  Eines der Projekte war eine Nuance.  Die Anforderungen ergaben eine vollst√§ndige Spiegelung aller im Cluster enthaltenen Daten.  Dies ist erforderlich, damit in einer Situation, in der mindestens ein Clusterknoten betriebsbereit ist, aus Sicht der Anwendung weiterhin alles funktioniert.  Dieser Moment hat nichts mit K8s zu tun, wir beschreiben ihn einfach als Mini-How-to. <br><br>  Um die vollst√§ndige HA zu aktivieren, m√ºssen Sie im RabbitMQ-Dashboard auf der Registerkarte <i>Admin -&gt; Richtlinien</i> eine Richtlinie erstellen.  Der Name ist beliebig, das Muster ist leer (alle Warteschlangen), in den Definitionen werden zwei Parameter hinzugef√ºgt: <i>ha-Modus: alle</i> , <i>ha-Sync-Modus: automatisch</i> . <br><br><img src="https://habrastorage.org/webt/jz/tn/vu/jztnvu5zygtv56hurbyss1w9ljm.png" alt="Bild"><br><br><img src="https://habrastorage.org/webt/_6/in/om/_6inoma38lvluhpaet1g66uus_u.png" alt="Bild"><br><br>  Danach befinden sich alle im Cluster erstellten Warteschlangen im Hochverf√ºgbarkeitsmodus: Wenn der Masterknoten nicht verf√ºgbar ist, wird einer der Slaves vom neuen Assistenten automatisch ausgew√§hlt.  Die in die Warteschlange eingehenden Daten werden auf alle Knoten des Clusters gespiegelt.  Was in der Tat erforderlich war, um zu erhalten. <br><br><img src="https://habrastorage.org/webt/0v/m3/je/0vm3jem0bi4fqckj8ucmiy5zcxe.png" alt="Bild"><br><br>  Lesen Sie hier mehr √ºber HA in RabbitMQ <br><br><h2>  N√ºtzliche Literatur: </h2><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Repository RabbitMQ Peer Discovery Kubernetes Plugin</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Konfigurationsbeispiel f√ºr die RabbitMQ-Bereitstellung in K8S</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beschreibung der Prinzipien der Clusterbildung, des Peer Discovery-Mechanismus und der Plugins daf√ºr</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Epische Diskussion √ºber das richtige Setup auf Hostnamenbasis</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RabbitMQ-Clustering-Handbuch</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beschreibung von Split-Brain-Clustering-Problemen und -L√∂sungen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hochverf√ºgbarkeitswarteschlangen bei RabbitMQ</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Richtlinien konfigurieren</a> </li></ul><br>  Viel Gl√ºck! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de419817/">https://habr.com/ru/post/de419817/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de419805/index.html">Der Super Tiny Compiler - jetzt in russischer Sprache</a></li>
<li><a href="../de419807/index.html">Glaukom - wie man nicht erblindet: Sprechen wir √ºber die Behandlung ...</a></li>
<li><a href="../de419811/index.html">Die Entwicklung flexibler Displays</a></li>
<li><a href="../de419813/index.html">Skillbox-Webinare: Auswahl am Freitag</a></li>
<li><a href="../de419815/index.html">Geheimnisse der Fehlertoleranz unseres Front Office</a></li>
<li><a href="../de419819/index.html">Biomarker des Alterns. Panel Gebrechlichkeit. Teil 2</a></li>
<li><a href="../de419823/index.html">Ungew√∂hnliche Duett - Passphrasen und Ged√§chtnisbilder</a></li>
<li><a href="../de419825/index.html">Testen der Leistung verschiedener Laufwerkstypen in einer virtuellen Umgebung</a></li>
<li><a href="../de419829/index.html">Die Standardschl√ºsselverschl√ºsselung von OpenSSH ist schlechter als keine</a></li>
<li><a href="../de419831/index.html">So funktioniert JS: Benutzerdefinierte Elemente</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>