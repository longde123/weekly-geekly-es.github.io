<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë∏üèº üåÄ üëèüèª Requ√™tes parall√®les dans PostgreSQL ‚öΩÔ∏è ‚òéÔ∏è üå†</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Il y a beaucoup de c≈ìurs dans les processeurs modernes. Pendant des ann√©es, les applications ont envoy√© des requ√™tes aux bases de donn√©es en parall√®le...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Requ√™tes parall√®les dans PostgreSQL</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/446706/"><p><img src="https://habrastorage.org/webt/kx/ht/dl/kxhtdlsry_f8p1jv2ve_1ziks7e.jpeg"></p><br><p>  Il y a beaucoup de c≈ìurs dans les processeurs modernes.  Pendant des ann√©es, les applications ont envoy√© des requ√™tes aux bases de donn√©es en parall√®le.  S'il s'agit d'une requ√™te de g√©n√©ration de rapports pour plusieurs lignes d'une table, elle s'ex√©cute plus rapidement lorsqu'elle utilise plusieurs processeurs, et dans PostgreSQL, il est possible √† partir de la version 9.6. </p><br><p>  Il a fallu 3 ans pour impl√©menter la fonction de requ√™te parall√®le - j'ai d√ª r√©√©crire le code √† diff√©rentes √©tapes de l'ex√©cution de la requ√™te.  PostgreSQL 9.6 a introduit une infrastructure pour am√©liorer encore le code.  Dans les versions ult√©rieures, d'autres types de requ√™tes sont ex√©cut√©es en parall√®le. </p><a name="habracut"></a><br><h3 id="ogranicheniya">  Limitations </h3><br><ul><li>  N'activez pas l'ex√©cution parall√®le si tous les c≈ìurs sont d√©j√† pris, sinon les autres requ√™tes ralentiront. </li><li>  Plus important encore, le traitement parall√®le avec des valeurs WORK_MEM √©lev√©es consomme beaucoup de m√©moire - chaque jointure ou tri de hachage occupe de la m√©moire dans la quantit√© de work_mem. </li><li>  Les requ√™tes OLTP √† faible latence ne peuvent pas √™tre acc√©l√©r√©es par une ex√©cution parall√®le.  Et si la requ√™te renvoie une ligne, le traitement parall√®le ne fera que la ralentir. </li><li>  Les d√©veloppeurs aiment utiliser le benchmark TPC-H.  Vous avez peut-√™tre des requ√™tes similaires pour une ex√©cution parall√®le parfaite. </li><li>  Seules les requ√™tes SELECT sans verrous de pr√©dicat sont ex√©cut√©es en parall√®le. </li><li>  Parfois, une indexation correcte est meilleure que des analyses de table s√©quentielles en parall√®le. </li><li>  Les requ√™tes d'interruption et les curseurs ne sont pas pris en charge. </li><li>  Les fonctions de fen√™tre et les fonctions d'agr√©gation des ensembles ordonn√©s ne sont pas parall√®les. </li><li>  Vous ne gagnez rien dans la charge de travail d'E / S. </li><li>  Il n'y a pas d'algorithme de tri parall√®le.  Mais les requ√™tes tri√©es peuvent √™tre ex√©cut√©es en parall√®le √† certains √©gards. </li><li> Remplacez CTE (WITH ...) par un SELECT imbriqu√© pour activer le traitement parall√®le. </li><li>  Les wrappers de donn√©es tiers ne prennent pas encore en charge le traitement parall√®le (mais ils pourraient!) </li><li>  FULL OUTER JOIN n'est pas pris en charge. </li><li>  max_rows d√©sactive le traitement parall√®le. </li><li>  Si la demande a une fonction qui n'est pas marqu√©e comme PARALLEL SAFE, elle sera monothread. </li><li>  Le niveau d'isolement des transactions SERIALIZABLE d√©sactive le traitement parall√®le. </li></ul><br><h3 id="testovaya-sreda">  Environnement de test </h3><br><p>  Les d√©veloppeurs de PostgreSQL ont essay√© de r√©duire le temps de r√©ponse des requ√™tes de benchmark TPC-H.  T√©l√©chargez le benchmark et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">adaptez-le √† PostgreSQL</a> .  Il s'agit d'une utilisation non officielle du benchmark TPC-H - pas pour comparer des bases de donn√©es ou du mat√©riel. </p><br><ol><li>  T√©l√©chargez TPC-H_Tools_v2.17.3.zip (ou une version plus r√©cente) √† <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">partir du TPC hors site</a> . </li><li>  Renommez makefile.suite en Makefile et modifiez comme d√©crit ici: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://github.com/tvondra/pg_tpch</a> .  Compilez le code avec la commande make. </li><li> G√©n√©rez des donn√©es: <code>./dbgen -s 10</code> cr√©e une base de donn√©es de 23 Go.  Cela suffit pour voir la diff√©rence de performances des requ√™tes parall√®les et non parall√®les. </li><li>  Convertissez les fichiers <code>tbl</code> en <code>csv  for</code> et <code>sed</code> . </li><li>  Clonez le r√©f√©rentiel pg_tpch et copiez les <code>csv</code> dans <code>pg_tpch/dss/data</code> . </li><li>  Cr√©ez des requ√™tes avec la commande <code>qgen</code> . </li><li>  T√©l√©chargez les donn√©es dans la base de donn√©es avec la commande <code>./tpch.sh</code> . </li></ol><br><h3 id="parallelnoe-posledovatelnoe-skanirovanie">  Balayage s√©quentiel parall√®le </h3><br><p>  Il peut √™tre plus rapide non pas en raison de la lecture parall√®le, mais parce que les donn√©es sont dispers√©es sur de nombreux c≈ìurs de processeur.  Sur les syst√®mes d'exploitation modernes, les fichiers de donn√©es PostgreSQL sont bien mis en cache.  Avec la lecture anticip√©e, vous pouvez obtenir plus du stockage que les demandes du d√©mon PG.  Par cons√©quent, les performances des requ√™tes ne sont pas limit√©es par les E / S de disque.  Il consomme des cycles CPU pour: </p><br><ul><li>  lire les lignes une par une dans les pages du tableau; </li><li>  Comparez les valeurs de cha√Æne et les clauses <code>WHERE</code> . </li></ul><br><p>  Ex√©cutons une simple requ√™te de <code>select</code> : </p><br><pre> <code class="plaintext hljs">tpch=# explain analyze select l_quantity as sum_qty from lineitem where l_shipdate &lt;= date '1998-12-01' - interval '105' day; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------- Seq Scan on lineitem (cost=0.00..1964772.00 rows=58856235 width=5) (actual time=0.014..16951.669 rows=58839715 loops=1) Filter: (l_shipdate &lt;= '1998-08-18 00:00:00'::timestamp without time zone) Rows Removed by Filter: 1146337 Planning Time: 0.203 ms Execution Time: 19035.100 ms</code> </pre> <br><p>  Une analyse s√©quentielle produit trop de lignes sans agr√©gation, de sorte que la demande est ex√©cut√©e par un seul c≈ìur de processeur. </p><br><p>  Si vous ajoutez <code>SUM()</code> , vous pouvez voir que deux workflows aideront √† acc√©l√©rer la demande: </p><br><pre> <code class="plaintext hljs">explain analyze select sum(l_quantity) as sum_qty from lineitem where l_shipdate &lt;= date '1998-12-01' - interval '105' day; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------------------------------------- Finalize Aggregate (cost=1589702.14..1589702.15 rows=1 width=32) (actual time=8553.365..8553.365 rows=1 loops=1) -&gt; Gather (cost=1589701.91..1589702.12 rows=2 width=32) (actual time=8553.241..8555.067 rows=3 loops=1) Workers Planned: 2 Workers Launched: 2 -&gt; Partial Aggregate (cost=1588701.91..1588701.92 rows=1 width=32) (actual time=8547.546..8547.546 rows=1 loops=3) -&gt; Parallel Seq Scan on lineitem (cost=0.00..1527393.33 rows=24523431 width=5) (actual time=0.038..5998.417 rows=19613238 loops=3) Filter: (l_shipdate &lt;= '1998-08-18 00:00:00'::timestamp without time zone) Rows Removed by Filter: 382112 Planning Time: 0.241 ms Execution Time: 8555.131 ms</code> </pre> <br><h3 id="parallelnaya-agregaciya">  Agr√©gation parall√®le </h3><br><p>  Le n≈ìud Parallel Seq Scan produit des cha√Ænes pour une agr√©gation partielle.  Le n≈ìud Partial Aggregate tronque ces lignes √† l'aide de <code>SUM()</code> .  √Ä la fin, le compteur SUM de chaque workflow est collect√© par le n≈ìud Gather. </p><br><p>  Le r√©sultat final est calcul√© par le n≈ìud "Finalize Aggregate".  Si vous avez vos propres fonctions d'agr√©gation, assurez-vous de les marquer comme "coffre-fort parall√®le". </p><br><h3 id="kolichestvo-rabochih-processov">  Nombre de workflows </h3><br><p>  Le nombre de workflows peut √™tre augment√© sans red√©marrer le serveur: </p><br><pre> <code class="plaintext hljs">alter system set max_parallel_workers_per_gather=4; select * from pg_reload_conf();</code> </pre> <br><p>  Maintenant, nous voyons 4 travailleurs dans la sortie d'explication: </p><br><pre> <code class="plaintext hljs">tpch=# explain analyze select sum(l_quantity) as sum_qty from lineitem where l_shipdate &lt;= date '1998-12-01' - interval '105' day; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------------------------------------- Finalize Aggregate (cost=1440213.58..1440213.59 rows=1 width=32) (actual time=5152.072..5152.072 rows=1 loops=1) -&gt; Gather (cost=1440213.15..1440213.56 rows=4 width=32) (actual time=5151.807..5153.900 rows=5 loops=1) Workers Planned: 4 Workers Launched: 4 -&gt; Partial Aggregate (cost=1439213.15..1439213.16 rows=1 width=32) (actual time=5147.238..5147.239 rows=1 loops=5) -&gt; Parallel Seq Scan on lineitem (cost=0.00..1402428.00 rows=14714059 width=5) (actual time=0.037..3601.882 rows=11767943 loops=5) Filter: (l_shipdate &lt;= '1998-08-18 00:00:00'::timestamp without time zone) Rows Removed by Filter: 229267 Planning Time: 0.218 ms Execution Time: 5153.967 ms</code> </pre> <br><p>  Que se passe-t-il ici?  Il y avait 2 fois plus de workflows et la demande n'√©tait que 1,6599 fois plus rapide.  Les calculs sont int√©ressants.  Nous avions 2 processus de travail et 1 leader.  Apr√®s le changement, il est devenu 4 + 1. </p><br><p>  Notre acc√©l√©ration maximale du traitement parall√®le: 5/3 = 1,66 (6) fois. </p><br><h2 id="kak-eto-rabotaet">  Comment √ßa marche? </h2><br><h3 id="processy">  Les processus </h3><br><p>  L'ex√©cution d'une demande commence toujours par un processus pilote.  Le leader fait tout ce qui n'est pas parall√®le et fait partie du traitement parall√®le.  Les autres processus qui ex√©cutent les m√™mes requ√™tes sont appel√©s workflows.  Le traitement parall√®le utilise une infrastructure de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">workflows d'arri√®re-plan dynamiques</a> (depuis la version 9.4).  √âtant donn√© que d'autres parties de PostgreSQL utilisent des processus plut√¥t que des threads, une requ√™te avec 3 workflows pourrait √™tre 4 fois plus rapide que le traitement traditionnel. </p><br><h3 id="vzaimodeystvie">  L'interaction </h3><br><p>  Les workflows communiquent avec le leader via une file d'attente de messages (bas√©e sur la m√©moire partag√©e).  Chaque processus a 2 files d'attente: pour les erreurs et pour les tuples. </p><br><h3 id="skolko-nuzhno-rabochih-processov">  De combien de processus de travail avez-vous besoin? </h3><br><p>  La limite minimale est d√©finie par le param√®tre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>max_parallel_workers_per_gather</code></a> .  Ensuite, l'ex√©cuteur de requ√™te prend les workflows du pool limit√© par le param√®tre de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>max_parallel_workers size</code></a> .  La derni√®re limitation est <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>max_worker_processes</code></a> , c'est-√†-dire le nombre total de processus d'arri√®re-plan. </p><br><p>  S'il n'a pas √©t√© possible d'allouer un workflow, le traitement se fera en un seul processus. </p><br><p>  Le planificateur de requ√™tes peut raccourcir les workflows en fonction de la taille de la table ou de l'index.  Il existe des param√®tres <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>min_parallel_table_scan_size</code></a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>min_parallel_index_scan_size</code></a> pour cela. </p><br><pre> <code class="plaintext hljs">set min_parallel_table_scan_size='8MB' 8MB table =&gt; 1 worker 24MB table =&gt; 2 workers 72MB table =&gt; 3 workers x =&gt; log(x / min_parallel_table_scan_size) / log(3) + 1 worker</code> </pre> <br><p>  Chaque fois qu'une table est 3 fois plus grande que <code>min_parallel_(index|table)_scan_size</code> , Postgres ajoute un workflow.  Le nombre de processus de travail n'est pas bas√© sur les co√ªts.  La d√©pendance circulaire complique les impl√©mentations complexes.  Au lieu de cela, le planificateur utilise des r√®gles simples. </p><br><p>  En pratique, ces r√®gles ne sont pas toujours adapt√©es √† la production, vous pouvez donc modifier le nombre de workflows pour une table particuli√®re: ALTER TABLE ... SET ( <code>parallel_workers = N</code> ). </p><br><h3 id="pochemu-parallelnaya-obrabotka-ne-ispolzuetsya">  Pourquoi le traitement parall√®le n'est-il pas utilis√©? </h3><br><p>  En plus d'une longue liste de restrictions, il existe √©galement des contr√¥les des co√ªts: </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>parallel_setup_cost</code></a> - pour se passer du traitement parall√®le des requ√™tes courtes.  Ce param√®tre estime le temps de pr√©paration de la m√©moire, de d√©marrage du processus et d'√©change de donn√©es initial. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>parallel_tuple_cost</code></a> : la communication entre un leader et des travailleurs peut √™tre retard√©e proportionnellement au nombre de tuples des processus de travail.  Ce param√®tre calcule les co√ªts d'√©change de donn√©es. </p><br><h3 id="soedineniya-vlozhennyh-ciklov--nested-loop-join">  Jointure de boucle imbriqu√©e </h3><br><pre> <code class="plaintext hljs">PostgreSQL 9.6+      ‚Äî   . explain (costs off) select c_custkey, count(o_orderkey) from customer left outer join orders on c_custkey = o_custkey and o_comment not like '%special%deposits%' group by c_custkey; QUERY PLAN -------------------------------------------------------------------------------------- Finalize GroupAggregate Group Key: customer.c_custkey -&gt; Gather Merge Workers Planned: 4 -&gt; Partial GroupAggregate Group Key: customer.c_custkey -&gt; Nested Loop Left Join -&gt; Parallel Index Only Scan using customer_pkey on customer -&gt; Index Scan using idx_orders_custkey on orders Index Cond: (customer.c_custkey = o_custkey) Filter: ((o_comment)::text !~~ '%special%deposits%'::text)</code> </pre> <br><p>  La collecte a lieu √† la derni√®re √©tape, donc la jointure gauche de la boucle imbriqu√©e est une op√©ration parall√®le.  Parallel Index Only Scan n'est apparu que dans la version 10. Il fonctionne de mani√®re similaire √† la num√©risation s√©rie parall√®le.  La condition <code>c_custkey = o_custkey</code> lit une commande pour chaque ligne client.  Ce n'est donc pas parall√®le. </p><br><h3 id="hesh-soedinenie--hash-join">  Hash Join - Hash Join </h3><br><p>  Chaque flux de travail cr√©e sa propre table de hachage avant PostgreSQL 11. Et s'il existe plus de quatre de ces processus, les performances ne s'am√©lioreront pas.  Dans la nouvelle version, la table de hachage est partag√©e.  Chaque flux de travail peut utiliser WORK_MEM pour cr√©er une table de hachage. </p><br><pre> <code class="plaintext hljs">select l_shipmode, sum(case when o_orderpriority = '1-URGENT' or o_orderpriority = '2-HIGH' then 1 else 0 end) as high_line_count, sum(case when o_orderpriority &lt;&gt; '1-URGENT' and o_orderpriority &lt;&gt; '2-HIGH' then 1 else 0 end) as low_line_count from orders, lineitem where o_orderkey = l_orderkey and l_shipmode in ('MAIL', 'AIR') and l_commitdate &lt; l_receiptdate and l_shipdate &lt; l_commitdate and l_receiptdate &gt;= date '1996-01-01' and l_receiptdate &lt; date '1996-01-01' + interval '1' year group by l_shipmode order by l_shipmode LIMIT 1; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Limit (cost=1964755.66..1964961.44 rows=1 width=27) (actual time=7579.592..7922.997 rows=1 loops=1) -&gt; Finalize GroupAggregate (cost=1964755.66..1966196.11 rows=7 width=27) (actual time=7579.590..7579.591 rows=1 loops=1) Group Key: lineitem.l_shipmode -&gt; Gather Merge (cost=1964755.66..1966195.83 rows=28 width=27) (actual time=7559.593..7922.319 rows=6 loops=1) Workers Planned: 4 Workers Launched: 4 -&gt; Partial GroupAggregate (cost=1963755.61..1965192.44 rows=7 width=27) (actual time=7548.103..7564.592 rows=2 loops=5) Group Key: lineitem.l_shipmode -&gt; Sort (cost=1963755.61..1963935.20 rows=71838 width=27) (actual time=7530.280..7539.688 rows=62519 loops=5) Sort Key: lineitem.l_shipmode Sort Method: external merge Disk: 2304kB Worker 0: Sort Method: external merge Disk: 2064kB Worker 1: Sort Method: external merge Disk: 2384kB Worker 2: Sort Method: external merge Disk: 2264kB Worker 3: Sort Method: external merge Disk: 2336kB -&gt; Parallel Hash Join (cost=382571.01..1957960.99 rows=71838 width=27) (actual time=7036.917..7499.692 rows=62519 loops=5) Hash Cond: (lineitem.l_orderkey = orders.o_orderkey) -&gt; Parallel Seq Scan on lineitem (cost=0.00..1552386.40 rows=71838 width=19) (actual time=0.583..4901.063 rows=62519 loops=5) Filter: ((l_shipmode = ANY ('{MAIL,AIR}'::bpchar[])) AND (l_commitdate &lt; l_receiptdate) AND (l_shipdate &lt; l_commitdate) AND (l_receiptdate &gt;= '1996-01-01'::date) AND (l_receiptdate &lt; '1997-01-01 00:00:00'::timestamp without time zone)) Rows Removed by Filter: 11934691 -&gt; Parallel Hash (cost=313722.45..313722.45 rows=3750045 width=20) (actual time=2011.518..2011.518 rows=3000000 loops=5) Buckets: 65536 Batches: 256 Memory Usage: 3840kB -&gt; Parallel Seq Scan on orders (cost=0.00..313722.45 rows=3750045 width=20) (actual time=0.029..995.948 rows=3000000 loops=5) Planning Time: 0.977 ms Execution Time: 7923.770 ms</code> </pre> <br><p>  La requ√™te 12 de TPC-H illustre une connexion de hachage parall√®le.  Chaque flux de travail est impliqu√© dans la cr√©ation d'une table de hachage partag√©e. </p><br><h3 id="soedinenie-sliyaniem--merge-join">  Fusionner rejoindre </h3><br><p>  Une jointure de fusion n'est pas de nature parall√®le.  Ne vous inqui√©tez pas s'il s'agit de la derni√®re √©tape de la demande - elle peut toujours √™tre ex√©cut√©e en parall√®le. </p><br><pre> <code class="plaintext hljs">-- Query 2 from TPC-H explain (costs off) select s_acctbal, s_name, n_name, p_partkey, p_mfgr, s_address, s_phone, s_comment from part, supplier, partsupp, nation, region where p_partkey = ps_partkey and s_suppkey = ps_suppkey and p_size = 36 and p_type like '%BRASS' and s_nationkey = n_nationkey and n_regionkey = r_regionkey and r_name = 'AMERICA' and ps_supplycost = ( select min(ps_supplycost) from partsupp, supplier, nation, region where p_partkey = ps_partkey and s_suppkey = ps_suppkey and s_nationkey = n_nationkey and n_regionkey = r_regionkey and r_name = 'AMERICA' ) order by s_acctbal desc, n_name, s_name, p_partkey LIMIT 100; QUERY PLAN ---------------------------------------------------------------------------------------------------------- Limit -&gt; Sort Sort Key: supplier.s_acctbal DESC, nation.n_name, supplier.s_name, part.p_partkey -&gt; Merge Join Merge Cond: (part.p_partkey = partsupp.ps_partkey) Join Filter: (partsupp.ps_supplycost = (SubPlan 1)) -&gt; Gather Merge Workers Planned: 4 -&gt; Parallel Index Scan using &lt;strong&gt;part_pkey&lt;/strong&gt; on part Filter: (((p_type)::text ~~ '%BRASS'::text) AND (p_size = 36)) -&gt; Materialize -&gt; Sort Sort Key: partsupp.ps_partkey -&gt; Nested Loop -&gt; Nested Loop Join Filter: (nation.n_regionkey = region.r_regionkey) -&gt; Seq Scan on region Filter: (r_name = 'AMERICA'::bpchar) -&gt; Hash Join Hash Cond: (supplier.s_nationkey = nation.n_nationkey) -&gt; Seq Scan on supplier -&gt; Hash -&gt; Seq Scan on nation -&gt; Index Scan using idx_partsupp_suppkey on partsupp Index Cond: (ps_suppkey = supplier.s_suppkey) SubPlan 1 -&gt; Aggregate -&gt; Nested Loop Join Filter: (nation_1.n_regionkey = region_1.r_regionkey) -&gt; Seq Scan on region region_1 Filter: (r_name = 'AMERICA'::bpchar) -&gt; Nested Loop -&gt; Nested Loop -&gt; Index Scan using idx_partsupp_partkey on partsupp partsupp_1 Index Cond: (part.p_partkey = ps_partkey) -&gt; Index Scan using supplier_pkey on supplier supplier_1 Index Cond: (s_suppkey = partsupp_1.ps_suppkey) -&gt; Index Scan using nation_pkey on nation nation_1 Index Cond: (n_nationkey = supplier_1.s_nationkey)</code> </pre> <br><p>  Le n≈ìud Fusionner la jointure est situ√© au-dessus de la collecte de fusion.  La fusion n'utilise donc pas de traitement parall√®le.  Mais le n≈ìud Parallel Index Scan aide toujours avec le segment <code>part_pkey</code> . </p><br><h3 id="soedinenie-po-sekciyam">  Connexion de section </h3><br><p>  Dans PostgreSQL 11, le partitionnement est d√©sactiv√© par d√©faut: il a une planification tr√®s co√ªteuse.  Les tables avec un partitionnement similaire peuvent √™tre jointes section par section.  Postgres utilisera donc des tables de hachage plus petites.  Chaque connexion de section peut √™tre parall√®le. </p><br><pre> <code class="plaintext hljs">tpch=# set enable_partitionwise_join=t; tpch=# explain (costs off) select * from prt1 t1, prt2 t2 where t1.a = t2.b and t1.b = 0 and t2.b between 0 and 10000; QUERY PLAN --------------------------------------------------- Append -&gt; Hash Join Hash Cond: (t2.b = t1.a) -&gt; Seq Scan on prt2_p1 t2 Filter: ((b &gt;= 0) AND (b &lt;= 10000)) -&gt; Hash -&gt; Seq Scan on prt1_p1 t1 Filter: (b = 0) -&gt; Hash Join Hash Cond: (t2_1.b = t1_1.a) -&gt; Seq Scan on prt2_p2 t2_1 Filter: ((b &gt;= 0) AND (b &lt;= 10000)) -&gt; Hash -&gt; Seq Scan on prt1_p2 t1_1 Filter: (b = 0) tpch=# set parallel_setup_cost = 1; tpch=# set parallel_tuple_cost = 0.01; tpch=# explain (costs off) select * from prt1 t1, prt2 t2 where t1.a = t2.b and t1.b = 0 and t2.b between 0 and 10000; QUERY PLAN ----------------------------------------------------------- Gather Workers Planned: 4 -&gt; Parallel Append -&gt; Parallel Hash Join Hash Cond: (t2_1.b = t1_1.a) -&gt; Parallel Seq Scan on prt2_p2 t2_1 Filter: ((b &gt;= 0) AND (b &lt;= 10000)) -&gt; Parallel Hash -&gt; Parallel Seq Scan on prt1_p2 t1_1 Filter: (b = 0) -&gt; Parallel Hash Join Hash Cond: (t2.b = t1.a) -&gt; Parallel Seq Scan on prt2_p1 t2 Filter: ((b &gt;= 0) AND (b &lt;= 10000)) -&gt; Parallel Hash -&gt; Parallel Seq Scan on prt1_p1 t1 Filter: (b = 0)</code> </pre> <br><p>  L'essentiel est que la connexion dans les sections ne soit parall√®le que si ces sections sont suffisamment grandes. </p><br><h3 id="parallelnoe-dopolnenie--parallel-append">  Parallel Append - Parallel Append </h3><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Parallel Append</a> peut √™tre utilis√© √† la place de diff√©rents blocs dans diff√©rents workflows.  Cela se produit g√©n√©ralement avec les requ√™tes UNION ALL.  L'inconv√©nient est moins de parall√©lisme, car chaque flux de travail ne traite qu'une seule demande. </p><br><p>  2 workflows s'ex√©cutent ici, bien que 4 soient inclus. </p><br><pre> <code class="plaintext hljs">tpch=# explain (costs off) select sum(l_quantity) as sum_qty from lineitem where l_shipdate &lt;= date '1998-12-01' - interval '105' day union all select sum(l_quantity) as sum_qty from lineitem where l_shipdate &lt;= date '2000-12-01' - interval '105' day; QUERY PLAN ------------------------------------------------------------------------------------------------ Gather Workers Planned: 2 -&gt; Parallel Append -&gt; Aggregate -&gt; Seq Scan on lineitem Filter: (l_shipdate &lt;= '2000-08-18 00:00:00'::timestamp without time zone) -&gt; Aggregate -&gt; Seq Scan on lineitem lineitem_1 Filter: (l_shipdate &lt;= '1998-08-18 00:00:00'::timestamp without time zone)</code> </pre> <br><h3 id="samye-vazhnye-peremennye">  Les variables les plus importantes </h3><br><ul><li>  WORK_MEM limite la quantit√© de m√©moire pour chaque processus, pas seulement pour les requ√™tes: <em>processus de</em> connexion work_mem = beaucoup de m√©moire. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>max_parallel_workers_per_gather</code></a> - combien de processus de travail le programme d'ex√©cution utilisera pour le traitement parall√®le √† partir du plan. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>max_worker_processes</code></a> - ajuste le nombre total de processus de travail au nombre de c≈ìurs de processeur sur le serveur. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>max_parallel_workers</code></a> est le m√™me, mais pour les workflows parall√®les. </li></ul><br><h3 id="itogi">  R√©sum√© </h3><br><p>  √Ä partir de la version 9.6, le traitement parall√®le peut consid√©rablement am√©liorer les performances des requ√™tes complexes qui analysent de nombreuses lignes ou index.  Dans PostgreSQL 10, le traitement parall√®le est activ√© par d√©faut.  N'oubliez pas de le d√©sactiver sur les serveurs avec une charge de travail OLTP importante.  Les analyses s√©quentielles ou les analyses d'index consomment beaucoup de ressources.  Si vous ne g√©n√©rez pas de rapports sur l'ensemble des donn√©es, les requ√™tes peuvent √™tre rendues plus efficaces en ajoutant simplement les index manquants ou en utilisant le partitionnement correct. </p><br><h3 id="ssylki">  Les r√©f√©rences </h3><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://www.postgresql.org/docs/11/how-parallel-query-works.html</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://www.postgresql.org/docs/11/parallel-plans.html</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://ashutoshpg.blogspot.com/2017/12/partition-wise-joins-divide-and-conquer.html</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://rhaas.blogspot.com/2016/04/postgresql-96-with-parallel-query-vs.html</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://amitkapila16.blogspot.com/2015/11/parallel-sequential-scans-in-play.html</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://write-skew.blogspot.com/2018/01/parallel-hash-for-postgresql.html</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://rhaas.blogspot.com/2017/03/parallel-query-v2.html</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://blog.2ndquadrant.com/parallel-monster-benchmark/</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://blog.2ndquadrant.com/parallel-aggregate/</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://www.depesz.com/2018/02/12/waiting-for-postgresql-11-support-parallel-btree-index-builds/</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Concurrence dans PostgreSQL 11</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr446706/">https://habr.com/ru/post/fr446706/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr446690/index.html">Electrolux lance un purificateur d'air intelligent pour les villes les plus pollu√©es</a></li>
<li><a href="../fr446694/index.html">Stockage modulaire JBOD et degr√©s de libert√©</a></li>
<li><a href="../fr446696/index.html">Mythes sur 152-FZ, ce qui peut co√ªter cher √† l'op√©rateur de donn√©es personnelles</a></li>
<li><a href="../fr446700/index.html">Lazydocker - GUI pour Docker directement dans le terminal</a></li>
<li><a href="../fr446702/index.html">Et un autre casque bizarre - pour dormir</a></li>
<li><a href="../fr446708/index.html">Comparaison des syst√®mes de communication spatiale</a></li>
<li><a href="../fr446710/index.html">Quatre vraies histoires de travail avec une architecture de microservices - rapport du Backend United 3 mitap: Kholodets</a></li>
<li><a href="../fr446712/index.html">HTTPS n'est pas toujours aussi s√©curis√© qu'il n'y para√Æt. Vuln√©rabilit√©s trouv√©es dans 5,5% des sites HTTPS</a></li>
<li><a href="../fr446714/index.html">Curieuses perversions du monde informatique - 4</a></li>
<li><a href="../fr446716/index.html">Conscience et argumentation de la fin du monde</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>