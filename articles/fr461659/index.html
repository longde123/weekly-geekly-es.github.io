<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üìÄ ‚úçüèª üë©üèæ‚Äçü§ù‚Äçüë©üèº R√©seaux de neurones et apprentissage profond, Chapitre 4: Preuve visuelle que les r√©seaux de neurones peuvent calculer n'importe quelle fonction üàÇÔ∏è üë®üèΩ‚Äç‚úàÔ∏è üõê</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dans ce chapitre, je donne une explication simple et surtout visuelle du th√©or√®me de l'universalit√©. Pour suivre le contenu de ce chapitre, il n'est p...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>R√©seaux de neurones et apprentissage profond, Chapitre 4: Preuve visuelle que les r√©seaux de neurones peuvent calculer n'importe quelle fonction</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/461659/">  Dans ce chapitre, je donne une explication simple et surtout visuelle du th√©or√®me de l'universalit√©.  Pour suivre le contenu de ce chapitre, il n'est pas n√©cessaire de lire les pr√©c√©dents.  Il est structur√© comme un essai ind√©pendant.  Si vous avez la compr√©hension la plus √©l√©mentaire de NS, vous devriez √™tre capable de comprendre les explications. <br><br><div class="spoiler">  <b class="spoiler_title">Table des mati√®res</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 1: utiliser les r√©seaux de neurones pour reconna√Ætre les nombres manuscrits</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 2: comment fonctionne l'algorithme de r√©tropropagation</a> </li><li>  Chapitre 3: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 1: am√©liorer la m√©thode de formation des r√©seaux de neurones</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 2: Pourquoi la r√©gularisation contribue-t-elle √† r√©duire le recyclage?</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 3: comment choisir les hyperparam√®tres de r√©seau neuronal?</a> <br></li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 4: preuve visuelle que les r√©seaux de neurones sont capables de calculer n'importe quelle fonction</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 5: Pourquoi les r√©seaux de neurones profonds sont-ils si difficiles √† former?</a> </li><li>  Chapitre 6: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 1: Deep Learning</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 2: progr√®s r√©cents dans la reconnaissance d'images</a> </li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Postface: existe-t-il un algorithme simple pour cr√©er de l'intelligence?</a> </li></ul></div></div><br>  L'un des faits les plus √©tonnants sur les r√©seaux de neurones est qu'ils peuvent calculer n'importe quelle fonction.  Autrement dit, supposons que quelqu'un vous donne une sorte de fonction complexe et sinueuse f (x): <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><a name="habracut"></a><br>  Et quelle que soit cette fonction, il est garanti un r√©seau de neurones tel que pour toute entr√©e x la valeur f (x) (ou une approximation proche) sera la sortie de ce r√©seau, c'est-√†-dire: <br><br><img src="https://habrastorage.org/webt/eb/ud/zc/ebudzc72xyiytdvkf4ulx-u7onq.png"><br><br>  Cela fonctionne m√™me si c'est une fonction de nombreuses variables f = f (x <sub>1</sub> , ..., x <sub>m</sub> ), et avec de nombreuses valeurs.  Par exemple, voici un r√©seau calculant une fonction avec m = 3 entr√©es et n = 2 sorties: <br><br><img src="https://habrastorage.org/webt/1g/01/i7/1g01i7vpnwo-mlm1r2brhd9al9m.png"><br><br>  Ce r√©sultat sugg√®re que les r√©seaux de neurones ont une certaine universalit√©.  Quelle que soit la fonction que nous voulons calculer, nous savons qu'il existe un r√©seau neuronal qui peut le faire. <br><br>  De plus, le th√©or√®me d'universalit√© tient m√™me si nous limitons le r√©seau √† une seule couche entre les neurones entrants et sortants - ce qu'on appelle  dans une couche cach√©e.  Ainsi, m√™me les r√©seaux avec une architecture tr√®s simple peuvent √™tre extr√™mement puissants. <br><br>  Le th√©or√®me de l'universalit√© est bien connu des personnes utilisant des r√©seaux de neurones.  Mais bien qu'il en soit ainsi, la compr√©hension de ce fait n'est pas si r√©pandue.  Et la plupart des explications √† cela sont trop complexes sur le plan technique.  Par exemple, l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">un des premiers articles</a> prouvant ce r√©sultat a utilis√© le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">th√©or√®me de Hahn-Banach</a> , le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">th√©or√®me de repr√©sentation de Riesz</a> et une analyse de Fourier.  Si vous √™tes math√©maticien, il est facile pour vous de comprendre ces preuves, mais pour la plupart des gens, ce n'est pas si facile.  C‚Äôest dommage, car les raisons fondamentales de l‚Äôuniversalit√© sont simples et belles. <br><br>  Dans ce chapitre, je donne une explication simple et surtout visuelle du th√©or√®me de l'universalit√©.  Nous allons parcourir pas √† pas les id√©es qui le sous-tendent.  Vous comprendrez pourquoi les r√©seaux de neurones peuvent vraiment calculer n'importe quelle fonction.  Vous comprendrez certaines des limites de ce r√©sultat.  Et vous comprendrez comment le r√©sultat est associ√© √† une NS profonde. <br><br>  Pour suivre le contenu de ce chapitre, il n'est pas n√©cessaire de lire les pr√©c√©dents.  Il est structur√© comme un essai ind√©pendant.  Si vous avez la compr√©hension la plus √©l√©mentaire de NS, vous devriez √™tre capable de comprendre les explications.  Mais je fournirai parfois des liens vers des documents pr√©c√©dents pour aider √† combler les lacunes dans les connaissances. <br><br>  Les th√©or√®mes d'universalit√© se trouvent souvent en informatique, alors parfois nous oublions m√™me √† quel point ils sont incroyables.  Mais cela vaut la peine de vous le rappeler: la capacit√© de calculer n'importe quelle fonction arbitraire est vraiment incroyable.  Presque tous les processus que vous pouvez imaginer peuvent √™tre r√©duits au calcul d'une fonction.  Consid√©rez la t√¢che de trouver le nom d'une composition musicale sur la base d'un bref passage.  Cela peut √™tre consid√©r√© comme un calcul de fonction.  Ou consid√©rez la t√¢che de traduire un texte chinois en anglais.  Et cela peut √™tre consid√©r√© comme un calcul de fonction (en fait, de nombreuses fonctions, car il existe de nombreuses options acceptables pour traduire un seul texte).  Ou consid√©rez la t√¢che de g√©n√©rer une description de l'intrigue du film et de la qualit√© du jeu sur la base du fichier mp4.  Cela aussi peut √™tre consid√©r√© comme le calcul d'une certaine fonction (la remarque faite sur les options de traduction de texte est √©galement correcte ici).  L'universalit√© signifie qu'en principe, les SN peuvent effectuer toutes ces t√¢ches et bien d'autres. <br><br>  Bien s√ªr, seulement du fait que nous savons qu'il existe des NS capables de traduire, par exemple, du chinois vers l'anglais, il ne s'ensuit pas que nous ayons de bonnes techniques pour cr√©er ou m√™me reconna√Ætre un tel r√©seau.  Cette restriction s'applique √©galement aux th√©or√®mes d'universalit√© traditionnels pour les mod√®les tels que les sch√©mas bool√©ens.  Mais, comme nous l'avons d√©j√† vu dans ce livre, le NS dispose de puissants algorithmes d'apprentissage des fonctions.  La combinaison d'algorithmes d'apprentissage et de polyvalence est un m√©lange attrayant.  Jusqu'√† pr√©sent, dans le livre, nous nous sommes concentr√©s sur les algorithmes de formation.  Dans ce chapitre, nous nous concentrerons sur la polyvalence et ce que cela signifie. <br><br><h2>  Deux astuces </h2><br>  Avant d'expliquer pourquoi le th√©or√®me d'universalit√© est vrai, je veux mentionner deux astuces contenues dans l'√©nonc√© informel ¬´un r√©seau de neurones peut calculer n'importe quelle fonction¬ª. <br><br>  Premi√®rement, cela ne signifie pas que le r√©seau peut √™tre utilis√© pour calculer avec pr√©cision n'importe quelle fonction.  Nous pouvons obtenir une approximation aussi bonne que n√©cessaire.  En augmentant le nombre de neurones cach√©s, nous am√©liorons l'approximation.  Par exemple, j'ai pr√©c√©demment illustr√© un r√©seau calculant une certaine fonction f (x) en utilisant trois neurones cach√©s.  Pour la plupart des fonctions, en utilisant trois neurones, seule une approximation de faible qualit√© peut √™tre obtenue.  En augmentant le nombre de neurones cach√©s (disons, jusqu'√† cinq), nous pouvons g√©n√©ralement obtenir une meilleure approximation: <br><br><img src="https://habrastorage.org/webt/x2/nt/zw/x2ntzw4ykxb450nexszfsd-qz08.png"><br><br>  Et pour am√©liorer la situation en augmentant encore le nombre de neurones cach√©s. <br><br>  Pour clarifier cette affirmation, disons qu'on nous a donn√© une fonction f (x), que nous voulons calculer avec une certaine pr√©cision n√©cessaire Œµ&gt; 0.  Il est garanti qu'en utilisant un nombre suffisant de neurones cach√©s, nous pouvons toujours trouver un NS dont la sortie g (x) satisfait l'√©quation | g (x) ‚àíf (x) | &lt;Œµ pour tout x.  En d'autres termes, l'approximation sera r√©alis√©e avec la pr√©cision souhait√©e pour toute valeur d'entr√©e possible. <br><br>  Le deuxi√®me probl√®me est que les fonctions qui peuvent √™tre approxim√©es par la m√©thode d√©crite appartiennent √† une classe continue.  Si la fonction est interrompue, c'est-√†-dire qu'elle fait des sauts brusques et soudains, alors dans le cas g√©n√©ral, il sera impossible de se rapprocher √† l'aide de NS.  Et cela n'est pas surprenant, car nos NS calculent les fonctions continues des donn√©es d'entr√©e.  Cependant, m√™me si la fonction que nous devons vraiment calculer est discontinue, l'approximation est souvent assez continue.  Si oui, alors nous pouvons utiliser NS.  En pratique, cette limitation n'est g√©n√©ralement pas importante. <br><br>  En cons√©quence, une d√©claration plus pr√©cise du th√©or√®me d'universalit√© sera que NS avec une couche cach√©e peut √™tre utilis√©e pour approximer n'importe quelle fonction continue avec la pr√©cision souhait√©e.  Dans ce chapitre, nous prouvons une version l√©g√®rement moins rigoureuse de ce th√©or√®me, utilisant deux couches cach√©es au lieu d'une.  Dans les t√¢ches, je d√©crirai bri√®vement comment cette explication peut √™tre adapt√©e, avec des modifications mineures, √† une preuve qui utilise une seule couche cach√©e. <br><br><h2>  Polyvalence avec une entr√©e et une valeur de sortie </h2><br>  Pour comprendre pourquoi le th√©or√®me d'universalit√© est vrai, nous commen√ßons par comprendre comment cr√©er une fonction d'approximation NS avec une seule entr√©e et une seule valeur de sortie: <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><br>  Il s'av√®re que c'est l'essence m√™me de la t√¢che d'universalit√©.  Une fois que nous aurons compris ce cas particulier, il sera assez facile de l'√©tendre √† des fonctions avec de nombreuses valeurs d'entr√©e et de sortie. <br><br>  Pour comprendre comment construire un r√©seau pour compter f, nous commen√ßons avec un r√©seau contenant une seule couche cach√©e avec deux neurones cach√©s, et avec une couche de sortie contenant un neurone de sortie: <br><br><img src="https://habrastorage.org/webt/b7/cz/ql/b7czqllzyyxbpzeq7gs3h6a2338.png"><br><br>  Pour imaginer comment fonctionnent les composants du r√©seau, nous nous concentrons sur le neurone sup√©rieur cach√©.  Dans le diagramme de l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article d'origine,</a> vous pouvez modifier le poids de mani√®re interactive avec la souris en cliquant sur ¬´w¬ª et voir imm√©diatement comment la fonction calcul√©e par le neurone sup√©rieur cach√© change: <br><br><img src="https://habrastorage.org/webt/mh/r0/dz/mhr0dzpmf_zop4a3bwi2of04qtu.png"><br><br>  Comme nous l'avons appris plus t√¥t dans le livre, un neurone cach√© compte œÉ (wx + b), o√π œÉ (z) ‚â° 1 / (1 + e <sup>‚àíz</sup> ) est un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sigmo√Øde</a> .  Jusqu'√† pr√©sent, nous avons utilis√© cette forme alg√©brique assez souvent.  Cependant, pour prouver l'universalit√©, il serait pr√©f√©rable d'ignorer compl√®tement cette alg√®bre et de manipuler et d'observer la forme sur le graphique.  Cela vous aidera non seulement √† mieux ressentir ce qui se passe, mais nous donnera √©galement une preuve d'universalit√© applicable √† d'autres fonctions d'activation en plus de sigmo√Øde. <br><br>  √Ä strictement parler, l'approche visuelle que j'ai choisie n'est traditionnellement pas consid√©r√©e comme une preuve.  Mais je crois que l'approche visuelle fournit plus de perspicacit√© dans la v√©rit√© du r√©sultat final que la preuve traditionnelle.  Et, bien s√ªr, une telle compr√©hension est le v√©ritable objectif de la preuve.  Dans les preuves que je propose, des lacunes se pr√©senteront parfois;  Je fournirai des preuves visuelles raisonnables, mais pas toujours rigoureuses.  Si cela vous d√©range, consid√©rez qu'il est de votre devoir de combler ces lacunes.  Cependant, ne perdons pas de vue l'objectif principal: comprendre pourquoi le th√©or√®me de l'universalit√© est vrai. <br><br>  Pour commencer avec cette √©preuve, cliquez sur le d√©calage b dans le diagramme d'origine et faites glisser vers la droite pour l'agrandir.  Vous verrez qu'avec une augmentation du d√©calage, le graphique se d√©place vers la gauche, mais ne change pas de forme. <br><br>  Faites-le ensuite glisser vers la gauche pour r√©duire le d√©calage.  Vous verrez que le graphique se d√©place vers la droite sans changer de forme. <br><br>  R√©duisez le poids √† 2-3.  Vous verrez qu'√† mesure que le poids diminue, la courbe se redresse.  Pour que la courbe ne s'√©chappe pas du graphique, vous devrez peut-√™tre corriger le d√©calage. <br><br>  Enfin, augmentez le poids √† des valeurs sup√©rieures √† 100. La courbe deviendra plus raide et finira par s'approcher de l'√©tape.  Essayez d'ajuster le d√©calage de sorte que son angle soit dans la r√©gion du point x = 0,3.  La vid√©o ci-dessous montre ce qui devrait arriver: <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Votre navigateur ne prend pas en charge la vid√©o HTML5. <source src="http://neuralnetworksanddeeplearning.com/movies/create_step_function.mp4" type="video/mp4"></video></div></div></div><br><br>  Nous pouvons grandement simplifier notre analyse en augmentant le poids afin que la sortie soit vraiment une bonne approximation de la fonction de pas.  Ci-dessous, j'ai construit la sortie du neurone cach√© sup√©rieur pour le poids w = 999.  Ceci est une image statique: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/94c/24e/8a8/94c24e8a8a262c06c102b97bef033e99.jpg"><br><br>  L'utilisation des fonctions pas √† pas est un peu plus facile qu'avec un sigmo√Øde typique.  La raison en est que les contributions de tous les neurones cach√©s sont additionn√©es dans la couche de sortie.  La somme d'un ensemble de fonctions pas √† pas est facile √† analyser, mais il est plus difficile de parler de ce qui se passe lorsqu'un groupe de courbes est ajout√© en tant que sigmo√Øde.  Par cons√©quent, il sera beaucoup plus simple de supposer que nos neurones cach√©s produisent des fonctions pas √† pas.  Plus pr√©cis√©ment, nous le faisons en fixant le poids w √† une valeur tr√®s √©lev√©e, puis en affectant la position du pas √† travers le d√©calage.  Bien s√ªr, travailler avec une sortie en tant que fonction pas √† pas est une approximation, mais c'est tr√®s bien, et jusqu'√† pr√©sent, nous traiterons la fonction comme une vraie fonction pas √† pas.  Plus tard, je reviendrai sur la discussion de l'effet des √©carts par rapport √† cette approximation. <br><br>  Quelle valeur de x est le pas?  En d'autres termes, comment la position de la marche d√©pend-elle du poids et du d√©placement? <br><br>  Pour r√©pondre √† la question, essayez de modifier le poids et le d√©calage dans le graphique interactif.  Pouvez-vous comprendre comment la position de l'√©tape d√©pend de w et b?  En pratiquant un peu, vous pouvez vous convaincre que sa position est proportionnelle √† b et inversement proportionnelle √† w. <br><br>  En fait, le pas est √† s = ‚Äã‚Äã‚àíb / w, comme on le verra si on ajuste le poids et le d√©placement aux valeurs suivantes: <br><br><img src="https://habrastorage.org/webt/ee/d9/zo/eed9zodaxp8ot33ip8y8j-sdcqg.png"><br><br>  Notre vie sera grandement simplifi√©e si nous d√©crivons des neurones cach√©s avec un seul param√®tre, s, c'est-√†-dire par la position du pas, s = ‚àíb / w.  Dans le diagramme interactif suivant, vous pouvez simplement modifier s: <br><br><img src="https://habrastorage.org/webt/uy/g6/9h/uyg69hokiufnmt7zwwmysjof5uc.png"><br><br>  Comme indiqu√© ci-dessus, nous avons sp√©cialement attribu√© un poids w √† l'entr√©e √† une tr√®s grande valeur - suffisamment grande pour que la fonction de pas devienne une bonne approximation.  Et nous pouvons facilement ramener le neurone param√©tr√© de cette mani√®re √† sa forme habituelle en choisissant le biais b = ‚àíws. <br><br>  Jusqu'√† pr√©sent, nous nous sommes concentr√©s sur la sortie du seul neurone cach√© sup√©rieur.  Examinons le comportement de l'ensemble du r√©seau.  Supposons que les neurones cach√©s calculent les fonctions de pas d√©finies par les param√®tres des pas s <sub>1</sub> (neurone sup√©rieur) et s <sub>2</sub> (neurone inf√©rieur).  Leurs poids de sortie respectifs sont w <sub>1</sub> et w <sub>2</sub> .  Voici notre r√©seau: <br><br><img src="https://habrastorage.org/webt/6u/ot/ns/6uotnsmlecwfh8iaqz1eb5p5tjo.png"><br><br>  √Ä droite, un graphique de la sortie pond√©r√©e w <sub>1</sub> a <sub>1</sub> + w <sub>2</sub> a <sub>2 de la</sub> couche cach√©e.  Ici, a <sub>1</sub> et a <sub>2</sub> sont respectivement les sorties des neurones cach√©s sup√©rieur et inf√©rieur.  Ils sont d√©sign√©s par ¬´a¬ª, car ils sont souvent appel√©s activations neuronales. <br><br>  Soit dit en passant, nous notons que la sortie de l'ensemble du r√©seau est œÉ (w <sub>1</sub> a <sub>1</sub> + w <sub>2</sub> a <sub>2</sub> + b), o√π b est le biais du neurone de sortie.  Ce n'est √©videmment pas la m√™me chose que la sortie pond√©r√©e de la couche cach√©e, dont nous construisons le graphique.  Mais pour l'instant, nous nous concentrerons sur la sortie √©quilibr√©e de la couche cach√©e et ne penserons que plus tard √† la fa√ßon dont elle se rapporte √† la sortie de l'ensemble du r√©seau. <br><br>  Essayez d'augmenter et de diminuer l'√©tape s <sub>1 du</sub> neurone sup√©rieur cach√© sur le diagramme interactif <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dans l'article d'origine</a> .  D√©couvrez comment cela modifie la sortie pond√©r√©e du calque masqu√©.  Il est particuli√®rement utile de comprendre ce qui se passe lorsque s <sub>1</sub> d√©passe s <sub>2</sub> .  Vous verrez que le graphique dans ces cas change de forme, lorsque nous passons d'une situation dans laquelle le neurone cach√© sup√©rieur est activ√© en premier √† une situation dans laquelle le neurone cach√© inf√©rieur est activ√© en premier. <br><br>  De m√™me, essayez de manipuler le pas s <sub>2 du</sub> neurone cach√© inf√©rieur et voyez comment cela modifie la sortie globale des neurones cach√©s. <br><br>  Essayez de r√©duire et d'augmenter les poids de sortie.  Remarquez comment cela modifie la contribution des neurones cach√©s correspondants.  Que se passe-t-il si l'un des poids est √©gal √† 0? <br><br>  Enfin, essayez de r√©gler w <sub>1</sub> √† 0,8 et w <sub>2</sub> √† -0,8.  Le r√©sultat est une fonction de ¬´saillie¬ª, avec un d√©but √† s <sub>1</sub> , une fin √† s <sub>2</sub> et une hauteur de 0,8.  Par exemple, une sortie pond√©r√©e pourrait ressembler √† ceci: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/84d/9be/e75/84d9bee755d8a57bbdb3aed20d07da78.jpg"><br><br>  Bien s√ªr, la saillie peut √™tre mise √† l'√©chelle √† n'importe quelle hauteur.  Utilisons un param√®tre, h, indiquant la hauteur.  Aussi, pour simplifier, je vais me d√©barrasser des notations "s <sub>1</sub> = ..." et "w <sub>1</sub> = ...". <br><br><img src="https://habrastorage.org/webt/io/by/7l/ioby7lcd1whqowsw0ak9j1td16q.png"><br><br>  Essayez d'augmenter et de diminuer la valeur h pour voir comment la hauteur de la saillie change.  Essayez de rendre h n√©gatif.  Essayez de modifier les points des √©tapes pour observer comment cela modifie la forme de la saillie. <br><br>  Vous verrez que nous utilisons nos neurones non seulement comme des primitives graphiques, mais aussi comme des unit√©s plus famili√®res aux programmeurs - quelque chose comme une instruction si-alors-autre dans la programmation: <br><br>  si entr√©e&gt; = d√©but de l'√©tape: <br>  ajouter 1 √† la sortie pond√©r√©e <br>  sinon: <br>  ajouter 0 √† la sortie pond√©r√©e <br><br>  Pour la plupart, je m'en tiendrai √† la notation graphique.  Cependant, il vous sera parfois utile de passer √† la vue si-alors-autre et de r√©fl√©chir √† ce qui se passe en ces termes. <br><br>  Nous pouvons utiliser notre astuce de protrusion en collant deux parties de neurones cach√©s ensemble sur le m√™me r√©seau: <br><br><img src="https://habrastorage.org/webt/4w/4p/pz/4w4ppzryydmyz3f3dglgzcwisfm.png"><br><br>  Ici, j'ai baiss√© les poids en √©crivant simplement les valeurs h pour chaque paire de neurones cach√©s.  Essayez de jouer avec les deux valeurs h et voyez comment cela change le graphique.  D√©placez les onglets, en changeant les points des √©tapes. <br><br>  Dans un cas plus g√©n√©ral, cette id√©e peut √™tre utilis√©e pour obtenir un nombre souhait√© de pics de n'importe quelle hauteur.  En particulier, nous pouvons diviser l'intervalle [0,1] en un grand nombre de (N) sous-intervalles, et utiliser N paires de neurones cach√©s pour obtenir des pics de n'importe quelle hauteur souhait√©e.  Voyons comment cela fonctionne pour N = 5.  C'est d√©j√† beaucoup de neurones, donc je suis une pr√©sentation un peu plus √©troite.  D√©sol√© pour le diagramme complexe - j'ai pu cacher la complexit√© derri√®re des abstractions suppl√©mentaires, mais il me semble que cela vaut la peine d'√™tre tourment√© par la complexit√© afin de mieux ressentir le fonctionnement des r√©seaux de neurones. <br><br><img src="https://habrastorage.org/webt/do/2t/x-/do2tx-fp-h-w83rnboapp8w-o98.png"><br><br>  Vous voyez, nous avons cinq paires de neurones cach√©s.  Les points des pas des paires correspondantes sont situ√©s √† 0,1 / 5, puis 1 / 5,2 / 5, et ainsi de suite, jusqu'√† 4 / 5,5 / 5.  Ces valeurs sont fixes - nous obtenons cinq protub√©rances de largeur √©gale sur le graphique. <br><br>  Chaque paire de neurones a une valeur h qui lui est associ√©e.  N'oubliez pas que les connexions des neurones de sortie ont des poids h et ‚Äìh.  Dans l'article d'origine du graphique, vous pouvez cliquer sur les valeurs h et les d√©placer de gauche √† droite.  Avec un changement de hauteur, l'horaire change √©galement.  En modifiant les poids de sortie, nous construisons la fonction finale! <br><br>  Sur le diagramme, vous pouvez toujours cliquer sur le graphique et faire glisser la hauteur des marches vers le haut ou vers le bas.  Lorsque vous modifiez sa hauteur, vous voyez comment la hauteur du h correspondant change.  Les poids de sortie + h et ‚Äìh changent en cons√©quence.  En d'autres termes, nous manipulons directement une fonction dont le graphique est affich√© √† droite et voyons ces changements dans les valeurs de h √† gauche.  Vous pouvez √©galement maintenir le bouton de la souris enfonc√© sur l'une des saillies, puis faire glisser la souris vers la gauche ou la droite, et les saillies s'ajusteront √† la hauteur actuelle. <br><br>  Il est temps de faire le travail. <br><br>  Rappelez-vous la fonction que j'ai dessin√©e au tout d√©but du chapitre: <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><br>  Ensuite, je n'ai pas mentionn√© cela, mais en fait, cela ressemble √† ceci: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo>+</mo><mn>0</mn><mo>,</mo><mn>4</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>0</mn><mo>,</mo><mn>3</mn><mi>x</mi><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>i</mi><mi>n</mi><mo stretchy=&quot;false&quot;>(</mo><mn>15</mn><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><mn>0</mn><mo>,</mo><mn>05</mn><mtext>&amp;#xA0;</mtext><mi>c</mi><mi>o</mi><mi>s</mi><mo stretchy=&quot;false&quot;>(</mo><mn>50</mn><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>113</mn></mrow></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="60.204ex" height="3.021ex" viewBox="0 -987.6 25921 1300.8" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-66" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-28" x="550" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-78" x="940" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-29" x="1512" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-3D" x="2179" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-30" x="3236" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-2C" x="3736" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-32" x="4181" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-2B" x="4904" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-30" x="5905" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-2C" x="6405" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-34" x="6850" y="0"></use><g transform="translate(7351,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-32" x="809" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-2B" x="8599" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-30" x="9600" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-2C" x="10101" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-33" x="10546" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-78" x="11046" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-73" x="11869" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-69" x="12338" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-6E" x="12684" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-28" x="13284" y="0"></use><g transform="translate(13674,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-35" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-78" x="14675" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-29" x="15247" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-2B" x="15859" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-30" x="16860" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-2C" x="17360" y="0"></use><g transform="translate(17805,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-30"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-35" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-63" x="19056" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-6F" x="19490" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-73" x="19975" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-28" x="20445" y="0"></use><g transform="translate(20834,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-35"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-30" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-78" x="21835" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-29" x="22408" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-74" x="23047" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-61" x="23409" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-67" x="23938" y="0"></use><g transform="translate(24419,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-31" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-33" x="1001" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">f</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(</font></font></mo><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">)</font></font></mo><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">=</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></mn><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">,</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></mn><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></mn><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">,</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4</font></font></mn><msup><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></mi><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></mn></msup><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></mn><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">,</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3</font></font></mn><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></mi><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">je</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">n</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">15</font></font></mn><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">)</font></font></mo><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></mn><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">,</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">05</font></font></mn><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">c</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">50</font></font></mn><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">)</font></font></mo><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">g</font></font></mi><mrow class="MJX-TeXAtom-ORD"><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">113</font></font></mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> f (x) = 0,2 + 0,4 x ^ 2 + 0,3x \ sin (15 x) + 0,05 \ cos (50 x) \ tag {113} </script></p><br><br>  Il est construit pour x valeurs de 0 √† 1, et les valeurs le long de l'axe y varient de 0 √† 1. <br><br>  De toute √©vidence, cette fonction n'est pas triviale.  Et vous devez comprendre comment le calculer en utilisant des r√©seaux de neurones. <br><br>  Dans nos r√©seaux de neurones ci-dessus, nous avons analys√© une combinaison pond√©r√©e ‚àë <sub>j</sub> w <sub>j</sub> a <sub>j</sub> de sortie de neurones cach√©s.  Nous savons comment contr√¥ler significativement cette valeur.  Mais, comme je l'ai not√© pr√©c√©demment, cette valeur n'est pas √©gale √† la sortie r√©seau.  La sortie du r√©seau est œÉ (‚àë <sub>j</sub> w <sub>j</sub> a <sub>j</sub> + b), o√π b est le d√©placement du neurone de sortie.  Pouvons-nous prendre le contr√¥le directement sur la sortie du r√©seau? <br><br>  La solution consiste √† d√©velopper un r√©seau neuronal dans lequel la sortie pond√©r√©e de la couche cach√©e est donn√©e par l'√©quation œÉ <sup>‚àí1</sup> ‚ãÖf (x), o√π œÉ <sup>‚àí1</sup> est la fonction inverse de œÉ.  Autrement dit, nous voulons que la sortie pond√©r√©e de la couche cach√©e soit la suivante: <br><br><img src="https://habrastorage.org/webt/sk/bu/bw/skbubwnwkrrpukeblqe9a1qo8cw.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Si cela r√©ussit, alors la sortie de l'ensemble du r√©seau sera une bonne approximation de f (x) (j'ai r√©gl√© le d√©calage du neurone de sortie sur 0). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ensuite, votre t√¢che consiste √† d√©velopper une NS approximative de la fonction objectif indiqu√©e ci-dessus. Pour mieux comprendre ce qui se passe, je vous recommande de r√©soudre ce probl√®me deux fois. Pour la premi√®re fois dans l' </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">article original,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> cliquez sur le graphique et ajustez directement les hauteurs des diff√©rentes saillies. Il vous sera assez facile d'obtenir une bonne approximation de la fonction objectif. Le degr√© d'approximation est estim√© par l'√©cart moyen, la diff√©rence entre la fonction objectif et la fonction que le r√©seau calcule. Votre t√¢che consiste √† ramener l'√©cart moyen √† une valeur minimale. La t√¢che est consid√©r√©e comme termin√©e lorsque l'√©cart moyen ne d√©passe pas 0,40.</font></font><br><br><img src="https://habrastorage.org/webt/jb/qh/j8/jbqhj8kul1dtc6o-nyrm0qxhh_0.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Apr√®s avoir r√©ussi, appuyez sur le bouton R√©initialiser, qui modifie les onglets de mani√®re al√©atoire. La deuxi√®me fois, ne touchez pas le graphique, mais modifiez les valeurs h sur le c√¥t√© gauche du diagramme, en essayant de ramener l'√©cart moyen √† une valeur de 0,40 ou moins. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Et donc, vous avez trouv√© tous les √©l√©ments n√©cessaires au r√©seau pour calculer approximativement la fonction f (x)! L'approximation s'est av√©r√©e grossi√®re, mais nous pouvons facilement am√©liorer le r√©sultat en augmentant simplement le nombre de paires de neurones cach√©s, ce qui augmentera le nombre de protub√©rances. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En particulier, il est facile de reconvertir toutes les donn√©es trouv√©es dans la vue standard avec le param√©trage utilis√© pour NS. Permettez-moi de vous rappeler rapidement comment cela fonctionne. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dans la premi√®re couche, tous les poids ont une grande valeur constante, par exemple, w = 1000.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les d√©placements des neurones cach√©s sont calcul√©s par b = ‚àíws. Ainsi, par exemple, pour le deuxi√®me neurone cach√©, s = 0,2 se transforme en b = ‚àí1000 √ó 0,2 = ‚àí200. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La derni√®re couche de l'√©chelle est d√©termin√©e par les valeurs de h. Ainsi, par exemple, la valeur que vous choisissez pour le premier h, h = -0,2, signifie que les poids de sortie des deux neurones sup√©rieurs cach√©s sont respectivement -0,2 et 0,2. Et ainsi de suite, pour toute la couche de poids de sortie. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Enfin, le d√©calage du neurone de sortie est 0. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Et c'est tout: nous avons une description compl√®te du NS, qui calcule bien la fonction objectif initiale. Et nous comprenons comment am√©liorer la qualit√© de l'approximation en am√©liorant le nombre de neurones cach√©s. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">De plus, dans notre fonction objectif d'origine f (x) = 0,2 + 0,4x </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ 0.3sin (15x) + 0.05cos (50x) n'a rien de sp√©cial. </font><font style="vertical-align: inherit;">Une proc√©dure similaire pourrait √™tre utilis√©e pour toute fonction continue sur les intervalles de [0,1] √† [0,1]. </font><font style="vertical-align: inherit;">En fait, nous utilisons notre NS √† couche unique pour construire une table de recherche pour une fonction. </font><font style="vertical-align: inherit;">Et nous pouvons prendre cette id√©e comme base pour obtenir une preuve g√©n√©ralis√©e d'universalit√©.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Fonction de nombreux param√®tres </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nous √©tendons nos r√©sultats au cas d'un ensemble de variables d'entr√©e. Cela semble compliqu√©, mais toutes les id√©es dont nous avons besoin peuvent d√©j√† √™tre comprises pour le cas avec seulement deux variables entrantes. Par cons√©quent, nous consid√©rons le cas avec deux variables entrantes. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Commen√ßons par regarder ce qui se passe quand un neurone a deux entr√©es: </font></font><br><br><img src="https://habrastorage.org/webt/k5/cm/a9/k5cma9i-bgfwxnp2ao9h1wiwfz0.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nous avons les entr√©es x et y, avec les poids correspondants w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et le d√©calage b du neurone. D√©finissez le poids de w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> sur 0 et jouez avec le premier, w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et le d√©calage b pour voir comment ils affectent la sortie du neurone: </font></font><br><br><img src="https://habrastorage.org/webt/bl/71/6p/bl716pdfanpkwighwk2dc6m20ey.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Comme vous pouvez le voir, avec w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0, l'entr√©e y n'affecte pas la sortie du neurone. Tout se passe comme si x √©tait la seule entr√©e.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Compte tenu de cela, que pensez-vous qu'il se passera lorsque nous augmenterons le poids de w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> √† w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 100 et que w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> laissera 0? Si cela ne vous est pas imm√©diatement clair, r√©fl√©chissez un peu √† cette question. Regardez ensuite la vid√©o suivante, qui montre ce qui va se passer:</font></font><br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Votre navigateur ne prend pas en charge la vid√©o HTML5. <source src="http://neuralnetworksanddeeplearning.com/movies/step_3d.mp4" type="video/mp4"></video></div></div></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Comme pr√©c√©demment, avec une augmentation du poids d'entr√©e, la sortie se rapproche de la forme de l'√©tape. La diff√©rence est que notre fonction pas √† pas est maintenant situ√©e en trois dimensions. Comme pr√©c√©demment, nous pouvons d√©placer l'emplacement des √©tapes en modifiant le d√©calage. L'angle sera au point s </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚â° - b / w1. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reprenons le diagramme pour que le param√®tre soit l'emplacement de l'√©tape: </font></font><br><br><img src="https://habrastorage.org/webt/aw/qs/59/awqs59ahvnac-1i9piafzg2jbpi.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nous supposons que le poids d'entr√©e de x est d'une grande importance - j'ai utilis√© w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 1000 - et le poids w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0. Le nombre sur le neurone est la position du pas, et le x au-dessus nous rappelle que nous d√©pla√ßons le pas le long de l'axe x. Naturellement, il est tout √† fait possible d'obtenir une fonction de pas le long de l'axe y, ce qui rend le poids entrant pour y grand (par exemple, w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">= 1000), et le poids pour x est 0, w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0: </font></font><br><br><img src="https://habrastorage.org/webt/xg/zw/y0/xgzwy0jgsj5q1gl3oqgygzqv0nq.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le nombre sur le neurone, encore une fois, indique la position du pas, et y au-dessus nous rappelle que nous d√©pla√ßons le pas le long de l'axe y. Je pourrais directement d√©signer les poids pour x et y, mais je ne l'ai pas fait, car cela jetterait le graphique. Mais gardez √† l'esprit que le marqueur y indique que le poids pour y est grand et pour x est 0. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nous pouvons utiliser les fonctions de pas que nous venons de concevoir pour calculer la fonction de saillie tridimensionnelle. Pour ce faire, nous prenons deux neurones, chacun calculant une fonction de pas le long de l'axe x. Ensuite, nous combinons ces fonctions de pas avec les poids h et ‚Äìh, o√π h est la hauteur de saillie souhait√©e. Tout cela peut √™tre vu dans le diagramme suivant:</font></font><br><br><img src="https://habrastorage.org/webt/5s/qn/wk/5sqnwkfmm7_uzs3jchov90ylyz8.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Essayez de modifier la valeur de h. Voyez comment cela se rapporte aux poids du r√©seau. Et comment elle modifie la hauteur de la fonction de saillie √† droite. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Essayez √©galement de modifier le point de l'√©tape, dont la valeur est d√©finie sur 0,30 dans le neurone sup√©rieur cach√©. Voyez comment cela change la forme de la saillie. Que se passe-t-il si vous le d√©placez au-del√† du point 0,70 associ√© au neurone cach√© inf√©rieur? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nous avons appris √† construire la fonction de protrusion le long de l'axe x. Naturellement, nous pouvons facilement faire la fonction de saillie le long de l'axe y, en utilisant deux fonctions de pas le long de l'axe y. Rappelons que nous pouvons le faire en faisant de grands poids √† l'entr√©e y et en r√©glant le poids 0 √† l'entr√©e x. Et alors, que se passe-t-il:</font></font><br><br><img src="https://habrastorage.org/webt/ic/uu/fq/icuufqisf9gjv8zccnkg0f0bevc.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Il semble presque identique au r√©seau pr√©c√©dent! Le seul changement visible est de petits marqueurs y sur les neurones cach√©s. Ils nous rappellent qu'ils produisent des fonctions pas √† pas pour y, et non pour x, donc le poids √† l'entr√©e y est tr√®s grand, et √† l'entr√©e x il est nul, et non l'inverse. Comme pr√©c√©demment, j'ai d√©cid√© de ne pas le montrer directement, afin de ne pas encombrer l'image. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Voyons ce qui se passe si nous ajoutons deux fonctions de protrusion, l'une le long de l'axe x, l'autre le long de l'axe y, toutes deux de hauteur h: </font></font><br><br><img src="https://habrastorage.org/webt/7f/u7/fc/7fu7fcn8xnl5r4zffk3tpunuedg.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pour simplifier le sch√©ma de connexion avec un poids nul, j'ai omis. Jusqu'√† pr√©sent, j'ai laiss√© de petits marqueurs x et y sur les neurones cach√©s pour me rappeler dans quelles directions les fonctions de protrusion sont calcul√©es. Plus tard, nous les refuserons, car ils sont impliqu√©s par la variable entrante.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Essayez de changer le param√®tre h. </font><font style="vertical-align: inherit;">Comme vous pouvez le voir, pour cette raison, les poids de sortie changent, ainsi que les poids des deux fonctions de saillie, x et y. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Notre </font></font><br><br><img src="https://habrastorage.org/webt/ad/le/ww/adlewwyzmc3zhrk-fm9a9yvx6zo.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cr√©ation est un </font><font style="vertical-align: inherit;">peu comme une ¬´fonction tour¬ª: </font><font style="vertical-align: inherit;">si nous pouvons cr√©er de telles fonctions tour, nous pouvons les utiliser pour approximer des fonctions arbitraires en ajoutant simplement des tours de diff√©rentes hauteurs √† diff√©rents endroits: </font></font><br><br><img src="https://habrastorage.org/webt/u1/lv/xv/u1lvxvdmfi4xxsqgpjabiwofr2k.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">bien s√ªr, nous n'avons pas encore atteint la cr√©ation d'une fonction tour arbitraire. </font><font style="vertical-align: inherit;">Jusqu'√† pr√©sent, nous avons construit quelque chose comme une tour centrale de hauteur 2h avec un plateau de hauteur h qui l'entoure. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mais nous pouvons faire fonctionner une tour. </font><font style="vertical-align: inherit;">Rappelons que nous avons pr√©c√©demment montr√© comment les neurones peuvent √™tre utilis√©s pour impl√©menter l'instruction if-then-else:</font></font><br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span>  &gt;= :  <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>:  <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre> <br>  C'√©tait un neurone √† une entr√©e.  Et nous devons appliquer une id√©e similaire √† la sortie combin√©e des neurones cach√©s: <br><br><pre> <code class="python hljs"> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>     &gt;= :  <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>:  <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre> <br>  Si nous choisissons le bon seuil - par exemple, 3h / 2, coinc√© entre la hauteur du plateau et la hauteur de la tour centrale - nous pouvons √©craser le plateau √† z√©ro et ne laisser qu'une seule tour. <br><br>  Imaginez comment faire cela?  Essayez d'exp√©rimenter avec le r√©seau suivant.  Maintenant, nous tra√ßons la sortie de l'ensemble du r√©seau, et pas seulement la sortie pond√©r√©e de la couche cach√©e.  Cela signifie que nous ajoutons le terme de d√©calage √† la sortie pond√©r√©e de la couche cach√©e et appliquons le sigmo√Øde.  Pouvez-vous trouver les valeurs de h et b pour lesquelles vous obtenez une tour?  Si vous √™tes bloqu√© √† ce stade, voici deux conseils: (1) pour que le neurone sortant d√©montre le comportement correct dans le style if-then-else, nous avons besoin que les poids entrants (tous h ou ‚Äìh) soient grands;  (2) la valeur de b d√©termine l'√©chelle du seuil si-alors-sinon. <br><br><img src="https://habrastorage.org/webt/ys/k-/1u/ysk-1uvu-jo68ikk5rqu274u7wc.png"><br><br>  Avec les param√®tres par d√©faut, la sortie est similaire √† une version aplatie du diagramme pr√©c√©dent, avec une tour et un plateau.  Pour obtenir le comportement souhait√©, vous devez augmenter la valeur de h.  Cela nous donnera le comportement de seuil de if-then-else.  Deuxi√®mement, pour fixer correctement le seuil, il faut choisir b ‚âà ‚àí3h / 2. <br><br>  Voici √† quoi cela ressemble pour h = 10: <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Votre navigateur ne prend pas en charge la vid√©o HTML5. <source src="http://neuralnetworksanddeeplearning.com/movies/tower_construction.mp4" type="video/mp4"></video></div></div></div><br>  M√™me pour des valeurs relativement modestes de h, nous obtenons une belle fonction de tour.  Et, bien s√ªr, nous pouvons obtenir un r√©sultat arbitrairement beau en augmentant encore h et en maintenant le biais au niveau b = ‚àí3h / 2. <br><br>  Essayons de coller deux r√©seaux ensemble pour compter deux fonctions de tour diff√©rentes.  Pour clarifier les r√¥les respectifs des deux sous-r√©seaux, je les mets dans des rectangles s√©par√©s: chacun d'eux calcule la fonction tour en utilisant la technique d√©crite ci-dessus.  Le graphique de droite montre la sortie pond√©r√©e de la deuxi√®me couche cach√©e, c'est-√†-dire la combinaison pond√©r√©e des fonctions de la tour. <br><br><img src="https://habrastorage.org/webt/8-/cl/ke/8-clkebo6vphf1-0_jgoaf3exts.png"><br><br>  En particulier, on peut voir qu'en changeant le poids dans la derni√®re couche, vous pouvez changer la hauteur des tours de sortie. <br><br>  La m√™me id√©e vous permet de calculer autant de tours que vous le souhaitez.  Nous pouvons les rendre arbitrairement minces et hauts.  Par cons√©quent, nous garantissons que la sortie pond√©r√©e de la deuxi√®me couche cach√©e se rapproche de toute fonction souhait√©e de deux variables: <br><br><img src="https://habrastorage.org/webt/ig/0u/5z/ig0u5zbzifftdfq4ww4y9a4r7du.png"><br><br>  En particulier, en faisant bien approcher la sortie pond√©r√©e de la deuxi√®me couche cach√©e œÉ <sup>‚àí1</sup> hiddenf, nous garantissons que la sortie de notre r√©seau sera une bonne approximation de la fonction f souhait√©e. <br><br>  Qu'en est-il des fonctions de nombreuses variables? <br><br>  Essayons de prendre trois variables, x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> .  Le r√©seau suivant peut-il √™tre utilis√© pour calculer la fonction de la tour en quatre dimensions? <br><br><img src="https://habrastorage.org/webt/wg/ki/kn/wgkiknicnzeoaept0d-cemw0sd0.png"><br><br>  Ici x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> d√©signent l'entr√©e r√©seau.  s <sub>1</sub> , t <sub>1,</sub> et ainsi de suite - les points de pas pour les neurones - c'est-√†-dire que tous les poids dans la premi√®re couche sont grands, et les d√©calages sont assign√©s de sorte que les points de pas soient s <sub>1</sub> , t <sub>1</sub> , s <sub>2</sub> , ... Les poids dans la deuxi√®me couche alternent, + h, ‚àíh, o√π h est un tr√®s grand nombre.  Le d√©calage de sortie est de -5h / 2. <br><br>  Le r√©seau calcule une fonction √©gale √† 1 dans trois conditions: x <sub>1</sub> est compris entre s <sub>1</sub> et t <sub>1</sub> ;  x <sub>2</sub> est compris entre s <sub>2</sub> et t <sub>2</sub> ;  x <sub>3</sub> est compris entre s <sub>3</sub> et t <sub>3</sub> .  Le r√©seau est 0 dans tous les autres endroits.  Il s'agit d'une tour dans laquelle 1 est une petite partie de l'espace d'entr√©e et 0 est tout le reste. <br><br>  En collant un grand nombre de ces r√©seaux, nous pouvons obtenir autant de tours que nous le souhaitons et approcher une fonction arbitraire de trois variables.  La m√™me id√©e fonctionne en m dimensions.  Seul le d√©calage de sortie (‚àím + 1/2) h est modifi√© pour comprimer correctement les valeurs souhait√©es et supprimer le plateau. <br><br>  Eh bien, maintenant nous savons comment utiliser NS pour approximer la fonction r√©elle de nombreuses variables.  Qu'en est-il des fonctions vectorielles f (x <sub>1</sub> , ..., x <sub>m</sub> ) ‚àà R <sup>n</sup> ?  Bien entendu, une telle fonction peut √™tre consid√©r√©e simplement comme n fonctions r√©elles distinctes f1 (x <sub>1</sub> , ..., x <sub>m</sub> ), f2 (x <sub>1</sub> , ..., x <sub>m</sub> ), etc.  Et puis nous collons juste tous les r√©seaux ensemble.  Il est donc facile de le comprendre. <br><br><h3>  D√©fi </h3><br><ul><li>  Nous avons vu comment utiliser des r√©seaux de neurones avec deux couches cach√©es pour approximer une fonction arbitraire.  Pouvez-vous prouver que cela est possible avec une seule couche cach√©e?  Astuce - essayez de travailler avec seulement deux variables de sortie et montrez que: (a) il est possible d'obtenir les fonctions des √©tapes non seulement le long des axes x ou y, mais aussi dans une direction arbitraire;  (b) en additionnant de nombreuses constructions de l'√©tape (a), il est possible d'approcher la fonction d'une tour ronde plut√¥t que rectangulaire;  ¬© √† l'aide de tours rondes, il est possible d'approximer une fonction arbitraire.  L'√©tape ¬© sera plus facile √† r√©aliser en utilisant le mat√©riel pr√©sent√© dans ce chapitre un peu plus bas. </li></ul><br><h2>  Aller au-del√† des neurones sigmo√Ødes </h2><br>  Nous avons prouv√© qu'un r√©seau de neurones sigmo√Ødes peut calculer n'importe quelle fonction.  Rappelons que dans un neurone sigmo√Øde, les entr√©es x <sub>1</sub> , x <sub>2</sub> , ... se transforment √† la sortie en œÉ (‚àë <sub>j</sub> w <sub>j</sub> x <sub>j j</sub> + b), o√π w <sub>j</sub> sont les poids, b est le d√©placement, œÉ est le sigmo√Øde. <br><br><img src="https://habrastorage.org/webt/0h/ut/93/0hut93wneejtjvxvxiwnfwpmo40.png"><br><br>  Et si nous regardons un autre type de neurone utilisant une fonction d'activation diff√©rente, s (z): <br><br><img src="https://habrastorage.org/webt/ua/0-/it/ua0-itpxz-uwkpnptxfsvszqabg.png"><br><br>  Autrement dit, nous supposons que si un neurone a x <sub>1</sub> , x <sub>2</sub> , ... les poids w <sub>1</sub> , w <sub>2</sub> , ... et le biais b √† l'entr√©e, alors la sortie aura s ( <sub>j j</sub> w <sub>j</sub> x <sub>j j</sub> + b). <br><br>  On peut utiliser cette fonction d'activation pour se faire marcher, tout comme dans le cas du sigmo√Øde.  Essayez (dans l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article d'origine</a> ) sur le diagramme de soulever le poids pour, disons, w = 100: <br><br><img src="https://habrastorage.org/webt/vz/-v/mu/vz-vmulc79w1g7xxom_btmnfyow.png"><br><br><img src="https://habrastorage.org/webt/nf/aq/iz/nfaqiznhl2klhebfc33iiy6htrg.png"><br><br>  Comme dans le cas du sigmo√Øde, de ce fait, la fonction d'activation est compress√©e et, par cons√©quent, se transforme en une tr√®s bonne approximation de la fonction de pas.  Essayez de changer le d√©calage, et vous verrez que nous pouvons changer l'emplacement de l'√©tape √† n'importe quel.  Par cons√©quent, nous pouvons utiliser toutes les m√™mes astuces qu'auparavant pour calculer n'importe quelle fonction souhait√©e. <br><br>  Quelles propri√©t√©s doit avoir s (z) pour que cela fonctionne?  Nous devons supposer que s (z) est bien d√©fini comme z ‚Üí ‚àí‚àû et z ‚Üí ‚àû.  Ces limites sont deux valeurs accept√©es par notre fonction pas √† pas.  Nous devons √©galement supposer que ces limites sont diff√©rentes.  S'ils ne diff√©raient pas, les √©tapes ne fonctionneraient pas; il y aurait simplement un horaire fixe!  Mais si la fonction d'activation s (z) satisfait √† ces propri√©t√©s, les neurones bas√©s sur celle-ci sont universellement adapt√©s aux calculs. <br><br><h3>  Les t√¢ches </h3><br><ul><li>  Plus t√¥t dans le livre, nous avons rencontr√© un type diff√©rent de neurone - un neurone lin√©aire redress√© ou une unit√© lin√©aire rectifi√©e, ReLU.  Expliquez pourquoi ces neurones ne remplissent pas les conditions n√©cessaires √† l'universalit√©.  Trouvez des preuves de polyvalence montrant que les ReLU sont universellement adapt√©s √† l'informatique. </li><li>  Supposons que nous consid√©rons des neurones lin√©aires, avec la fonction d'activation s (z) = z.  Expliquez pourquoi les neurones lin√©aires ne satisfont pas aux conditions d'universalit√©.  Montrez que ces neurones ne peuvent pas √™tre utilis√©s pour l'informatique universelle. </li></ul><br><h2>  Fonction √©tape fixe </h2><br>  Pour le moment, nous avons suppos√© que nos neurones produisent des fonctions de pas pr√©cises.  C'est une bonne approximation, mais seulement une approximation.  En fait, il existe un √©troit intervalle d'√©chec, illustr√© dans le graphique suivant, o√π les fonctions ne se comportent pas du tout comme une fonction pas √† pas: <br><br><img src="https://habrastorage.org/webt/mr/0t/ng/mr0tng4l1giob-gsuhyo_oh_vk0.png"><br><br>  En cette p√©riode d'√©chec, mon explication de l'universalit√© ne fonctionne pas. <br><br>  L'√©chec n'est pas si effrayant.  En fixant des poids d'entr√©e suffisamment grands, nous pouvons rendre ces √©carts arbitrairement petits.  Nous pouvons les rendre beaucoup plus petits que sur la carte, invisibles √† l'≈ìil nu.  Alors peut-√™tre que nous n'avons pas √† nous soucier de ce probl√®me. <br><br>  N√©anmoins, j'aimerais avoir un moyen de le r√©soudre. <br><br>  Il s'av√®re que c'est facile √† r√©soudre.  Voyons cette solution pour calculer les fonctions NS avec une seule entr√©e et une seule sortie.  Les m√™mes id√©es fonctionneront pour r√©soudre le probl√®me avec un grand nombre d'entr√©es et de sorties. <br><br>  Supposons en particulier que nous voulons que notre r√©seau calcule une fonction f.  Comme pr√©c√©demment, nous essayons de le faire en concevant le r√©seau de sorte que la sortie pond√©r√©e de la couche cach√©e de neurones soit œÉ <sup>‚àí1</sup> ‚ãÖf (x): <br><br><img src="https://habrastorage.org/webt/sk/bu/bw/skbubwnwkrrpukeblqe9a1qo8cw.png"><br><br>  Si nous le faisons en utilisant la technique d√©crite ci-dessus, nous forcerons les neurones cach√©s √† produire une s√©quence de fonctions de protrusion: <br><br><img src="https://habrastorage.org/webt/71/uc/x_/71ucx_26mzx0_isj6dlob9wdacq.png"><br><br>  Bien s√ªr, j'ai exag√©r√© la taille des intervalles d'√©chec, pour que ce soit plus facile √† voir.  Il devrait √™tre clair que si nous additionnons toutes ces fonctions des protub√©rances, nous obtenons une assez bonne approximation de œÉ <sup>‚àí1</sup> ‚ãÖf (x) partout sauf pour les intervalles de d√©faillance. <br><br>  Mais, supposons qu'au lieu d'utiliser l'approximation qui vient d'√™tre d√©crite, nous utilisons un ensemble de neurones cach√©s pour calculer l'approximation de la moiti√© de notre fonction objective d'origine, c'est-√†-dire œÉ <sup>‚àí1</sup> ‚ãÖf (x) / 2.  Bien s√ªr, cela ressemblera √† une version √† l'√©chelle du dernier graphique: <br><br><img src="https://habrastorage.org/webt/8-/0b/fv/8-0bfvrf5njiwum-w6d8edu4dro.png"><br><br>  Et, supposons que nous faisons un autre ensemble de neurones cach√©s calculer l'approximation de œÉ <sup>‚àí1</sup> ‚àíf (x) / 2, cependant, √† sa base, les protub√©rances seront d√©cal√©es de la moiti√© de leur largeur: <br><br><img src="https://habrastorage.org/webt/dj/cz/7n/djcz7nxhhm98yhiq94uluglvm-m.png"><br><br>  Nous avons maintenant deux approximations diff√©rentes pour œÉ - 1‚ãÖf (x) / 2.  Si nous additionnons ces deux approximations, nous obtenons une approximation g√©n√©rale de œÉ - 1‚ãÖf (x).  Cette approximation g√©n√©rale comportera toujours des inexactitudes √† petits intervalles.  Mais le probl√®me sera moindre qu'auparavant - car les points tombant dans les intervalles de l'√©chec de la premi√®re approximation ne tomberont pas dans les intervalles de l'√©chec de la seconde approximation.  Par cons√©quent, l'approximation de ces intervalles sera environ 2 fois meilleure. <br><br>  Nous pouvons am√©liorer la situation en ajoutant un grand nombre, M, d'approximations qui se chevauchent de la fonction œÉ - 1‚ãÖf (x) / M.  Si tous leurs intervalles de d√©faillance sont suffisamment √©troits, aucun courant ne sera dans l'un d'eux.  Si vous utilisez un nombre suffisamment grand d'approximations de M qui se chevauchent, le r√©sultat est une excellente approximation g√©n√©rale. <br><br><h2>  Conclusion </h2><br>  L'explication de l'universalit√© discut√©e ici ne peut certainement pas √™tre appel√©e une description pratique de la fa√ßon de compter les fonctions √† l'aide de r√©seaux de neurones!  En ce sens, cela ressemble plus √† une preuve de la polyvalence des portes logiques NAND et plus encore.  Par cons√©quent, j'ai essentiellement essay√© de rendre ce design clair et facile √† suivre sans optimiser ses d√©tails.  Cependant, essayer d'optimiser cette conception peut √™tre un exercice int√©ressant et instructif pour vous. <br><br>  Bien que le r√©sultat obtenu ne puisse pas √™tre directement utilis√© pour cr√©er NS, il est important car il supprime la question de la calculabilit√© d'une fonction particuli√®re utilisant NS.  La r√©ponse √† une telle question sera toujours positive.  Par cons√©quent, il est correct de demander si une fonction est calculable, mais quelle est la bonne fa√ßon de la calculer. <br><br>  Notre conception universelle utilise seulement deux couches cach√©es pour calculer une fonction arbitraire.  Comme nous l'avons vu, il est possible d'obtenir le m√™me r√©sultat avec une seule couche cach√©e.  Compte tenu de cela, vous vous demandez peut-√™tre pourquoi nous avons besoin de r√©seaux profonds, c'est-√†-dire des r√©seaux avec un grand nombre de couches cach√©es.  Ne pouvons-nous pas simplement remplacer ces r√©seaux par des r√©seaux peu profonds qui ont une couche cach√©e? <br><br>  Bien que, en principe, cela soit possible, il existe de bonnes raisons pratiques d'utiliser des r√©seaux de neurones profonds.  Comme d√©crit dans le chapitre 1, les NS profonds ont une structure hi√©rarchique qui leur permet de bien s'adapter pour √©tudier les connaissances hi√©rarchiques, qui sont utiles pour r√©soudre des probl√®mes r√©els.  Plus pr√©cis√©ment, lors de la r√©solution de probl√®mes tels que la reconnaissance de formes, il est utile d'utiliser un syst√®me qui comprend non seulement des pixels individuels, mais aussi des concepts de plus en plus complexes: des bordures aux formes g√©om√©triques simples, et au-del√†, aux sc√®nes complexes impliquant plusieurs objets.  Dans les chapitres suivants, nous verrons des preuves en faveur du fait que les SN profonds seront mieux en mesure de faire face √† l'√©tude de telles hi√©rarchies de connaissances que les NS peu profondes.  Pour r√©sumer: l'universalit√© nous dit que NS peut calculer n'importe quelle fonction;  des donn√©es empiriques sugg√®rent que les NS profonds sont mieux adapt√©s √† l'√©tude des fonctions utiles pour r√©soudre de nombreux probl√®mes du monde r√©el. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr461659/">https://habr.com/ru/post/fr461659/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr461649/index.html">Comment vais-je sauver le monde</a></li>
<li><a href="../fr461651/index.html">Frontend Weekly Digest (22-28 juillet 2019)</a></li>
<li><a href="../fr461653/index.html">Radio d√©finie par logiciel - comment √ßa marche? Partie 10</a></li>
<li><a href="../fr461655/index.html">Le condens√© de mati√®res fra√Æches du monde du frontend pour la derni√®re semaine n ¬∞ 373 (22-28 juillet 2019)</a></li>
<li><a href="../fr461657/index.html">Acheter Red Hat: cela aidera-t-il le g√©ant bleu √† se battre pour le leadership du cloud hybride</a></li>
<li><a href="../fr461661/index.html">Guide de d√©veloppement bas√© sur les composants</a></li>
<li><a href="../fr461663/index.html">L'histoire de la fa√ßon dont Linux a introduit Windows</a></li>
<li><a href="../fr461665/index.html">Zen2. L'√©volution de la plateforme AM4 sur l'exemple de Ryzen 7 3700x</a></li>
<li><a href="../fr461669/index.html">PHP Digest n ¬∞ 161 (15-29 juillet 2019)</a></li>
<li><a href="../fr461673/index.html">8 conseils pour les programmeurs d√©butants ou une r√©trospective de ma carri√®re</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>