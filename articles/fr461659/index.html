<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>📀 ✍🏻 👩🏾‍🤝‍👩🏼 Réseaux de neurones et apprentissage profond, Chapitre 4: Preuve visuelle que les réseaux de neurones peuvent calculer n'importe quelle fonction 🈂️ 👨🏽‍✈️ 🛐</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dans ce chapitre, je donne une explication simple et surtout visuelle du théorème de l'universalité. Pour suivre le contenu de ce chapitre, il n'est p...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Réseaux de neurones et apprentissage profond, Chapitre 4: Preuve visuelle que les réseaux de neurones peuvent calculer n'importe quelle fonction</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/461659/">  Dans ce chapitre, je donne une explication simple et surtout visuelle du théorème de l'universalité.  Pour suivre le contenu de ce chapitre, il n'est pas nécessaire de lire les précédents.  Il est structuré comme un essai indépendant.  Si vous avez la compréhension la plus élémentaire de NS, vous devriez être capable de comprendre les explications. <br><br><div class="spoiler">  <b class="spoiler_title">Table des matières</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 1: utiliser les réseaux de neurones pour reconnaître les nombres manuscrits</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 2: comment fonctionne l'algorithme de rétropropagation</a> </li><li>  Chapitre 3: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 1: améliorer la méthode de formation des réseaux de neurones</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 2: Pourquoi la régularisation contribue-t-elle à réduire le recyclage?</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 3: comment choisir les hyperparamètres de réseau neuronal?</a> <br></li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 4: preuve visuelle que les réseaux de neurones sont capables de calculer n'importe quelle fonction</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 5: Pourquoi les réseaux de neurones profonds sont-ils si difficiles à former?</a> </li><li>  Chapitre 6: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 1: Deep Learning</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 2: progrès récents dans la reconnaissance d'images</a> </li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Postface: existe-t-il un algorithme simple pour créer de l'intelligence?</a> </li></ul></div></div><br>  L'un des faits les plus étonnants sur les réseaux de neurones est qu'ils peuvent calculer n'importe quelle fonction.  Autrement dit, supposons que quelqu'un vous donne une sorte de fonction complexe et sinueuse f (x): <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><a name="habracut"></a><br>  Et quelle que soit cette fonction, il est garanti un réseau de neurones tel que pour toute entrée x la valeur f (x) (ou une approximation proche) sera la sortie de ce réseau, c'est-à-dire: <br><br><img src="https://habrastorage.org/webt/eb/ud/zc/ebudzc72xyiytdvkf4ulx-u7onq.png"><br><br>  Cela fonctionne même si c'est une fonction de nombreuses variables f = f (x <sub>1</sub> , ..., x <sub>m</sub> ), et avec de nombreuses valeurs.  Par exemple, voici un réseau calculant une fonction avec m = 3 entrées et n = 2 sorties: <br><br><img src="https://habrastorage.org/webt/1g/01/i7/1g01i7vpnwo-mlm1r2brhd9al9m.png"><br><br>  Ce résultat suggère que les réseaux de neurones ont une certaine universalité.  Quelle que soit la fonction que nous voulons calculer, nous savons qu'il existe un réseau neuronal qui peut le faire. <br><br>  De plus, le théorème d'universalité tient même si nous limitons le réseau à une seule couche entre les neurones entrants et sortants - ce qu'on appelle  dans une couche cachée.  Ainsi, même les réseaux avec une architecture très simple peuvent être extrêmement puissants. <br><br>  Le théorème de l'universalité est bien connu des personnes utilisant des réseaux de neurones.  Mais bien qu'il en soit ainsi, la compréhension de ce fait n'est pas si répandue.  Et la plupart des explications à cela sont trop complexes sur le plan technique.  Par exemple, l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">un des premiers articles</a> prouvant ce résultat a utilisé le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">théorème de Hahn-Banach</a> , le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">théorème de représentation de Riesz</a> et une analyse de Fourier.  Si vous êtes mathématicien, il est facile pour vous de comprendre ces preuves, mais pour la plupart des gens, ce n'est pas si facile.  C’est dommage, car les raisons fondamentales de l’universalité sont simples et belles. <br><br>  Dans ce chapitre, je donne une explication simple et surtout visuelle du théorème de l'universalité.  Nous allons parcourir pas à pas les idées qui le sous-tendent.  Vous comprendrez pourquoi les réseaux de neurones peuvent vraiment calculer n'importe quelle fonction.  Vous comprendrez certaines des limites de ce résultat.  Et vous comprendrez comment le résultat est associé à une NS profonde. <br><br>  Pour suivre le contenu de ce chapitre, il n'est pas nécessaire de lire les précédents.  Il est structuré comme un essai indépendant.  Si vous avez la compréhension la plus élémentaire de NS, vous devriez être capable de comprendre les explications.  Mais je fournirai parfois des liens vers des documents précédents pour aider à combler les lacunes dans les connaissances. <br><br>  Les théorèmes d'universalité se trouvent souvent en informatique, alors parfois nous oublions même à quel point ils sont incroyables.  Mais cela vaut la peine de vous le rappeler: la capacité de calculer n'importe quelle fonction arbitraire est vraiment incroyable.  Presque tous les processus que vous pouvez imaginer peuvent être réduits au calcul d'une fonction.  Considérez la tâche de trouver le nom d'une composition musicale sur la base d'un bref passage.  Cela peut être considéré comme un calcul de fonction.  Ou considérez la tâche de traduire un texte chinois en anglais.  Et cela peut être considéré comme un calcul de fonction (en fait, de nombreuses fonctions, car il existe de nombreuses options acceptables pour traduire un seul texte).  Ou considérez la tâche de générer une description de l'intrigue du film et de la qualité du jeu sur la base du fichier mp4.  Cela aussi peut être considéré comme le calcul d'une certaine fonction (la remarque faite sur les options de traduction de texte est également correcte ici).  L'universalité signifie qu'en principe, les SN peuvent effectuer toutes ces tâches et bien d'autres. <br><br>  Bien sûr, seulement du fait que nous savons qu'il existe des NS capables de traduire, par exemple, du chinois vers l'anglais, il ne s'ensuit pas que nous ayons de bonnes techniques pour créer ou même reconnaître un tel réseau.  Cette restriction s'applique également aux théorèmes d'universalité traditionnels pour les modèles tels que les schémas booléens.  Mais, comme nous l'avons déjà vu dans ce livre, le NS dispose de puissants algorithmes d'apprentissage des fonctions.  La combinaison d'algorithmes d'apprentissage et de polyvalence est un mélange attrayant.  Jusqu'à présent, dans le livre, nous nous sommes concentrés sur les algorithmes de formation.  Dans ce chapitre, nous nous concentrerons sur la polyvalence et ce que cela signifie. <br><br><h2>  Deux astuces </h2><br>  Avant d'expliquer pourquoi le théorème d'universalité est vrai, je veux mentionner deux astuces contenues dans l'énoncé informel «un réseau de neurones peut calculer n'importe quelle fonction». <br><br>  Premièrement, cela ne signifie pas que le réseau peut être utilisé pour calculer avec précision n'importe quelle fonction.  Nous pouvons obtenir une approximation aussi bonne que nécessaire.  En augmentant le nombre de neurones cachés, nous améliorons l'approximation.  Par exemple, j'ai précédemment illustré un réseau calculant une certaine fonction f (x) en utilisant trois neurones cachés.  Pour la plupart des fonctions, en utilisant trois neurones, seule une approximation de faible qualité peut être obtenue.  En augmentant le nombre de neurones cachés (disons, jusqu'à cinq), nous pouvons généralement obtenir une meilleure approximation: <br><br><img src="https://habrastorage.org/webt/x2/nt/zw/x2ntzw4ykxb450nexszfsd-qz08.png"><br><br>  Et pour améliorer la situation en augmentant encore le nombre de neurones cachés. <br><br>  Pour clarifier cette affirmation, disons qu'on nous a donné une fonction f (x), que nous voulons calculer avec une certaine précision nécessaire ε&gt; 0.  Il est garanti qu'en utilisant un nombre suffisant de neurones cachés, nous pouvons toujours trouver un NS dont la sortie g (x) satisfait l'équation | g (x) −f (x) | &lt;ε pour tout x.  En d'autres termes, l'approximation sera réalisée avec la précision souhaitée pour toute valeur d'entrée possible. <br><br>  Le deuxième problème est que les fonctions qui peuvent être approximées par la méthode décrite appartiennent à une classe continue.  Si la fonction est interrompue, c'est-à-dire qu'elle fait des sauts brusques et soudains, alors dans le cas général, il sera impossible de se rapprocher à l'aide de NS.  Et cela n'est pas surprenant, car nos NS calculent les fonctions continues des données d'entrée.  Cependant, même si la fonction que nous devons vraiment calculer est discontinue, l'approximation est souvent assez continue.  Si oui, alors nous pouvons utiliser NS.  En pratique, cette limitation n'est généralement pas importante. <br><br>  En conséquence, une déclaration plus précise du théorème d'universalité sera que NS avec une couche cachée peut être utilisée pour approximer n'importe quelle fonction continue avec la précision souhaitée.  Dans ce chapitre, nous prouvons une version légèrement moins rigoureuse de ce théorème, utilisant deux couches cachées au lieu d'une.  Dans les tâches, je décrirai brièvement comment cette explication peut être adaptée, avec des modifications mineures, à une preuve qui utilise une seule couche cachée. <br><br><h2>  Polyvalence avec une entrée et une valeur de sortie </h2><br>  Pour comprendre pourquoi le théorème d'universalité est vrai, nous commençons par comprendre comment créer une fonction d'approximation NS avec une seule entrée et une seule valeur de sortie: <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><br>  Il s'avère que c'est l'essence même de la tâche d'universalité.  Une fois que nous aurons compris ce cas particulier, il sera assez facile de l'étendre à des fonctions avec de nombreuses valeurs d'entrée et de sortie. <br><br>  Pour comprendre comment construire un réseau pour compter f, nous commençons avec un réseau contenant une seule couche cachée avec deux neurones cachés, et avec une couche de sortie contenant un neurone de sortie: <br><br><img src="https://habrastorage.org/webt/b7/cz/ql/b7czqllzyyxbpzeq7gs3h6a2338.png"><br><br>  Pour imaginer comment fonctionnent les composants du réseau, nous nous concentrons sur le neurone supérieur caché.  Dans le diagramme de l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article d'origine,</a> vous pouvez modifier le poids de manière interactive avec la souris en cliquant sur «w» et voir immédiatement comment la fonction calculée par le neurone supérieur caché change: <br><br><img src="https://habrastorage.org/webt/mh/r0/dz/mhr0dzpmf_zop4a3bwi2of04qtu.png"><br><br>  Comme nous l'avons appris plus tôt dans le livre, un neurone caché compte σ (wx + b), où σ (z) ≡ 1 / (1 + e <sup>−z</sup> ) est un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sigmoïde</a> .  Jusqu'à présent, nous avons utilisé cette forme algébrique assez souvent.  Cependant, pour prouver l'universalité, il serait préférable d'ignorer complètement cette algèbre et de manipuler et d'observer la forme sur le graphique.  Cela vous aidera non seulement à mieux ressentir ce qui se passe, mais nous donnera également une preuve d'universalité applicable à d'autres fonctions d'activation en plus de sigmoïde. <br><br>  À strictement parler, l'approche visuelle que j'ai choisie n'est traditionnellement pas considérée comme une preuve.  Mais je crois que l'approche visuelle fournit plus de perspicacité dans la vérité du résultat final que la preuve traditionnelle.  Et, bien sûr, une telle compréhension est le véritable objectif de la preuve.  Dans les preuves que je propose, des lacunes se présenteront parfois;  Je fournirai des preuves visuelles raisonnables, mais pas toujours rigoureuses.  Si cela vous dérange, considérez qu'il est de votre devoir de combler ces lacunes.  Cependant, ne perdons pas de vue l'objectif principal: comprendre pourquoi le théorème de l'universalité est vrai. <br><br>  Pour commencer avec cette épreuve, cliquez sur le décalage b dans le diagramme d'origine et faites glisser vers la droite pour l'agrandir.  Vous verrez qu'avec une augmentation du décalage, le graphique se déplace vers la gauche, mais ne change pas de forme. <br><br>  Faites-le ensuite glisser vers la gauche pour réduire le décalage.  Vous verrez que le graphique se déplace vers la droite sans changer de forme. <br><br>  Réduisez le poids à 2-3.  Vous verrez qu'à mesure que le poids diminue, la courbe se redresse.  Pour que la courbe ne s'échappe pas du graphique, vous devrez peut-être corriger le décalage. <br><br>  Enfin, augmentez le poids à des valeurs supérieures à 100. La courbe deviendra plus raide et finira par s'approcher de l'étape.  Essayez d'ajuster le décalage de sorte que son angle soit dans la région du point x = 0,3.  La vidéo ci-dessous montre ce qui devrait arriver: <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Votre navigateur ne prend pas en charge la vidéo HTML5. <source src="http://neuralnetworksanddeeplearning.com/movies/create_step_function.mp4" type="video/mp4"></video></div></div></div><br><br>  Nous pouvons grandement simplifier notre analyse en augmentant le poids afin que la sortie soit vraiment une bonne approximation de la fonction de pas.  Ci-dessous, j'ai construit la sortie du neurone caché supérieur pour le poids w = 999.  Ceci est une image statique: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/94c/24e/8a8/94c24e8a8a262c06c102b97bef033e99.jpg"><br><br>  L'utilisation des fonctions pas à pas est un peu plus facile qu'avec un sigmoïde typique.  La raison en est que les contributions de tous les neurones cachés sont additionnées dans la couche de sortie.  La somme d'un ensemble de fonctions pas à pas est facile à analyser, mais il est plus difficile de parler de ce qui se passe lorsqu'un groupe de courbes est ajouté en tant que sigmoïde.  Par conséquent, il sera beaucoup plus simple de supposer que nos neurones cachés produisent des fonctions pas à pas.  Plus précisément, nous le faisons en fixant le poids w à une valeur très élevée, puis en affectant la position du pas à travers le décalage.  Bien sûr, travailler avec une sortie en tant que fonction pas à pas est une approximation, mais c'est très bien, et jusqu'à présent, nous traiterons la fonction comme une vraie fonction pas à pas.  Plus tard, je reviendrai sur la discussion de l'effet des écarts par rapport à cette approximation. <br><br>  Quelle valeur de x est le pas?  En d'autres termes, comment la position de la marche dépend-elle du poids et du déplacement? <br><br>  Pour répondre à la question, essayez de modifier le poids et le décalage dans le graphique interactif.  Pouvez-vous comprendre comment la position de l'étape dépend de w et b?  En pratiquant un peu, vous pouvez vous convaincre que sa position est proportionnelle à b et inversement proportionnelle à w. <br><br>  En fait, le pas est à s = ​​−b / w, comme on le verra si on ajuste le poids et le déplacement aux valeurs suivantes: <br><br><img src="https://habrastorage.org/webt/ee/d9/zo/eed9zodaxp8ot33ip8y8j-sdcqg.png"><br><br>  Notre vie sera grandement simplifiée si nous décrivons des neurones cachés avec un seul paramètre, s, c'est-à-dire par la position du pas, s = −b / w.  Dans le diagramme interactif suivant, vous pouvez simplement modifier s: <br><br><img src="https://habrastorage.org/webt/uy/g6/9h/uyg69hokiufnmt7zwwmysjof5uc.png"><br><br>  Comme indiqué ci-dessus, nous avons spécialement attribué un poids w à l'entrée à une très grande valeur - suffisamment grande pour que la fonction de pas devienne une bonne approximation.  Et nous pouvons facilement ramener le neurone paramétré de cette manière à sa forme habituelle en choisissant le biais b = −ws. <br><br>  Jusqu'à présent, nous nous sommes concentrés sur la sortie du seul neurone caché supérieur.  Examinons le comportement de l'ensemble du réseau.  Supposons que les neurones cachés calculent les fonctions de pas définies par les paramètres des pas s <sub>1</sub> (neurone supérieur) et s <sub>2</sub> (neurone inférieur).  Leurs poids de sortie respectifs sont w <sub>1</sub> et w <sub>2</sub> .  Voici notre réseau: <br><br><img src="https://habrastorage.org/webt/6u/ot/ns/6uotnsmlecwfh8iaqz1eb5p5tjo.png"><br><br>  À droite, un graphique de la sortie pondérée w <sub>1</sub> a <sub>1</sub> + w <sub>2</sub> a <sub>2 de la</sub> couche cachée.  Ici, a <sub>1</sub> et a <sub>2</sub> sont respectivement les sorties des neurones cachés supérieur et inférieur.  Ils sont désignés par «a», car ils sont souvent appelés activations neuronales. <br><br>  Soit dit en passant, nous notons que la sortie de l'ensemble du réseau est σ (w <sub>1</sub> a <sub>1</sub> + w <sub>2</sub> a <sub>2</sub> + b), où b est le biais du neurone de sortie.  Ce n'est évidemment pas la même chose que la sortie pondérée de la couche cachée, dont nous construisons le graphique.  Mais pour l'instant, nous nous concentrerons sur la sortie équilibrée de la couche cachée et ne penserons que plus tard à la façon dont elle se rapporte à la sortie de l'ensemble du réseau. <br><br>  Essayez d'augmenter et de diminuer l'étape s <sub>1 du</sub> neurone supérieur caché sur le diagramme interactif <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dans l'article d'origine</a> .  Découvrez comment cela modifie la sortie pondérée du calque masqué.  Il est particulièrement utile de comprendre ce qui se passe lorsque s <sub>1</sub> dépasse s <sub>2</sub> .  Vous verrez que le graphique dans ces cas change de forme, lorsque nous passons d'une situation dans laquelle le neurone caché supérieur est activé en premier à une situation dans laquelle le neurone caché inférieur est activé en premier. <br><br>  De même, essayez de manipuler le pas s <sub>2 du</sub> neurone caché inférieur et voyez comment cela modifie la sortie globale des neurones cachés. <br><br>  Essayez de réduire et d'augmenter les poids de sortie.  Remarquez comment cela modifie la contribution des neurones cachés correspondants.  Que se passe-t-il si l'un des poids est égal à 0? <br><br>  Enfin, essayez de régler w <sub>1</sub> à 0,8 et w <sub>2</sub> à -0,8.  Le résultat est une fonction de «saillie», avec un début à s <sub>1</sub> , une fin à s <sub>2</sub> et une hauteur de 0,8.  Par exemple, une sortie pondérée pourrait ressembler à ceci: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/84d/9be/e75/84d9bee755d8a57bbdb3aed20d07da78.jpg"><br><br>  Bien sûr, la saillie peut être mise à l'échelle à n'importe quelle hauteur.  Utilisons un paramètre, h, indiquant la hauteur.  Aussi, pour simplifier, je vais me débarrasser des notations "s <sub>1</sub> = ..." et "w <sub>1</sub> = ...". <br><br><img src="https://habrastorage.org/webt/io/by/7l/ioby7lcd1whqowsw0ak9j1td16q.png"><br><br>  Essayez d'augmenter et de diminuer la valeur h pour voir comment la hauteur de la saillie change.  Essayez de rendre h négatif.  Essayez de modifier les points des étapes pour observer comment cela modifie la forme de la saillie. <br><br>  Vous verrez que nous utilisons nos neurones non seulement comme des primitives graphiques, mais aussi comme des unités plus familières aux programmeurs - quelque chose comme une instruction si-alors-autre dans la programmation: <br><br>  si entrée&gt; = début de l'étape: <br>  ajouter 1 à la sortie pondérée <br>  sinon: <br>  ajouter 0 à la sortie pondérée <br><br>  Pour la plupart, je m'en tiendrai à la notation graphique.  Cependant, il vous sera parfois utile de passer à la vue si-alors-autre et de réfléchir à ce qui se passe en ces termes. <br><br>  Nous pouvons utiliser notre astuce de protrusion en collant deux parties de neurones cachés ensemble sur le même réseau: <br><br><img src="https://habrastorage.org/webt/4w/4p/pz/4w4ppzryydmyz3f3dglgzcwisfm.png"><br><br>  Ici, j'ai baissé les poids en écrivant simplement les valeurs h pour chaque paire de neurones cachés.  Essayez de jouer avec les deux valeurs h et voyez comment cela change le graphique.  Déplacez les onglets, en changeant les points des étapes. <br><br>  Dans un cas plus général, cette idée peut être utilisée pour obtenir un nombre souhaité de pics de n'importe quelle hauteur.  En particulier, nous pouvons diviser l'intervalle [0,1] en un grand nombre de (N) sous-intervalles, et utiliser N paires de neurones cachés pour obtenir des pics de n'importe quelle hauteur souhaitée.  Voyons comment cela fonctionne pour N = 5.  C'est déjà beaucoup de neurones, donc je suis une présentation un peu plus étroite.  Désolé pour le diagramme complexe - j'ai pu cacher la complexité derrière des abstractions supplémentaires, mais il me semble que cela vaut la peine d'être tourmenté par la complexité afin de mieux ressentir le fonctionnement des réseaux de neurones. <br><br><img src="https://habrastorage.org/webt/do/2t/x-/do2tx-fp-h-w83rnboapp8w-o98.png"><br><br>  Vous voyez, nous avons cinq paires de neurones cachés.  Les points des pas des paires correspondantes sont situés à 0,1 / 5, puis 1 / 5,2 / 5, et ainsi de suite, jusqu'à 4 / 5,5 / 5.  Ces valeurs sont fixes - nous obtenons cinq protubérances de largeur égale sur le graphique. <br><br>  Chaque paire de neurones a une valeur h qui lui est associée.  N'oubliez pas que les connexions des neurones de sortie ont des poids h et –h.  Dans l'article d'origine du graphique, vous pouvez cliquer sur les valeurs h et les déplacer de gauche à droite.  Avec un changement de hauteur, l'horaire change également.  En modifiant les poids de sortie, nous construisons la fonction finale! <br><br>  Sur le diagramme, vous pouvez toujours cliquer sur le graphique et faire glisser la hauteur des marches vers le haut ou vers le bas.  Lorsque vous modifiez sa hauteur, vous voyez comment la hauteur du h correspondant change.  Les poids de sortie + h et –h changent en conséquence.  En d'autres termes, nous manipulons directement une fonction dont le graphique est affiché à droite et voyons ces changements dans les valeurs de h à gauche.  Vous pouvez également maintenir le bouton de la souris enfoncé sur l'une des saillies, puis faire glisser la souris vers la gauche ou la droite, et les saillies s'ajusteront à la hauteur actuelle. <br><br>  Il est temps de faire le travail. <br><br>  Rappelez-vous la fonction que j'ai dessinée au tout début du chapitre: <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><br>  Ensuite, je n'ai pas mentionné cela, mais en fait, cela ressemble à ceci: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo>+</mo><mn>0</mn><mo>,</mo><mn>4</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>0</mn><mo>,</mo><mn>3</mn><mi>x</mi><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>i</mi><mi>n</mi><mo stretchy=&quot;false&quot;>(</mo><mn>15</mn><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><mn>0</mn><mo>,</mo><mn>05</mn><mtext>&amp;#xA0;</mtext><mi>c</mi><mi>o</mi><mi>s</mi><mo stretchy=&quot;false&quot;>(</mo><mn>50</mn><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>113</mn></mrow></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="60.204ex" height="3.021ex" viewBox="0 -987.6 25921 1300.8" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-66" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-28" x="550" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-78" x="940" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-29" x="1512" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-3D" x="2179" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-30" x="3236" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-2C" x="3736" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-32" x="4181" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-2B" x="4904" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-30" x="5905" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-2C" x="6405" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-34" x="6850" y="0"></use><g transform="translate(7351,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-32" x="809" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-2B" x="8599" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-30" x="9600" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-2C" x="10101" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-33" x="10546" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-78" x="11046" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-73" x="11869" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-69" x="12338" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-6E" x="12684" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-28" x="13284" y="0"></use><g transform="translate(13674,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-35" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-78" x="14675" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-29" x="15247" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-2B" x="15859" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-30" x="16860" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-2C" x="17360" y="0"></use><g transform="translate(17805,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-30"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-35" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-63" x="19056" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-6F" x="19490" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-73" x="19975" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-28" x="20445" y="0"></use><g transform="translate(20834,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-35"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-30" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-78" x="21835" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-29" x="22408" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-74" x="23047" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-61" x="23409" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMATHI-67" x="23938" y="0"></use><g transform="translate(24419,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-31" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhi63kwfRG6T9e1BE-8JK_8RHTFcwA#MJMAIN-33" x="1001" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">f</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(</font></font></mo><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">)</font></font></mo><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">=</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></mn><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">,</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></mn><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></mn><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">,</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4</font></font></mn><msup><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></mi><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></mn></msup><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></mn><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">,</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3</font></font></mn><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></mi><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">je</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">n</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">15</font></font></mn><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">)</font></font></mo><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></mn><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">,</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">05</font></font></mn><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">c</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">50</font></font></mn><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">)</font></font></mo><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">g</font></font></mi><mrow class="MJX-TeXAtom-ORD"><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">113</font></font></mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> f (x) = 0,2 + 0,4 x ^ 2 + 0,3x \ sin (15 x) + 0,05 \ cos (50 x) \ tag {113} </script></p><br><br>  Il est construit pour x valeurs de 0 à 1, et les valeurs le long de l'axe y varient de 0 à 1. <br><br>  De toute évidence, cette fonction n'est pas triviale.  Et vous devez comprendre comment le calculer en utilisant des réseaux de neurones. <br><br>  Dans nos réseaux de neurones ci-dessus, nous avons analysé une combinaison pondérée ∑ <sub>j</sub> w <sub>j</sub> a <sub>j</sub> de sortie de neurones cachés.  Nous savons comment contrôler significativement cette valeur.  Mais, comme je l'ai noté précédemment, cette valeur n'est pas égale à la sortie réseau.  La sortie du réseau est σ (∑ <sub>j</sub> w <sub>j</sub> a <sub>j</sub> + b), où b est le déplacement du neurone de sortie.  Pouvons-nous prendre le contrôle directement sur la sortie du réseau? <br><br>  La solution consiste à développer un réseau neuronal dans lequel la sortie pondérée de la couche cachée est donnée par l'équation σ <sup>−1</sup> ⋅f (x), où σ <sup>−1</sup> est la fonction inverse de σ.  Autrement dit, nous voulons que la sortie pondérée de la couche cachée soit la suivante: <br><br><img src="https://habrastorage.org/webt/sk/bu/bw/skbubwnwkrrpukeblqe9a1qo8cw.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Si cela réussit, alors la sortie de l'ensemble du réseau sera une bonne approximation de f (x) (j'ai réglé le décalage du neurone de sortie sur 0). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ensuite, votre tâche consiste à développer une NS approximative de la fonction objectif indiquée ci-dessus. Pour mieux comprendre ce qui se passe, je vous recommande de résoudre ce problème deux fois. Pour la première fois dans l' </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">article original,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> cliquez sur le graphique et ajustez directement les hauteurs des différentes saillies. Il vous sera assez facile d'obtenir une bonne approximation de la fonction objectif. Le degré d'approximation est estimé par l'écart moyen, la différence entre la fonction objectif et la fonction que le réseau calcule. Votre tâche consiste à ramener l'écart moyen à une valeur minimale. La tâche est considérée comme terminée lorsque l'écart moyen ne dépasse pas 0,40.</font></font><br><br><img src="https://habrastorage.org/webt/jb/qh/j8/jbqhj8kul1dtc6o-nyrm0qxhh_0.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Après avoir réussi, appuyez sur le bouton Réinitialiser, qui modifie les onglets de manière aléatoire. La deuxième fois, ne touchez pas le graphique, mais modifiez les valeurs h sur le côté gauche du diagramme, en essayant de ramener l'écart moyen à une valeur de 0,40 ou moins. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Et donc, vous avez trouvé tous les éléments nécessaires au réseau pour calculer approximativement la fonction f (x)! L'approximation s'est avérée grossière, mais nous pouvons facilement améliorer le résultat en augmentant simplement le nombre de paires de neurones cachés, ce qui augmentera le nombre de protubérances. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En particulier, il est facile de reconvertir toutes les données trouvées dans la vue standard avec le paramétrage utilisé pour NS. Permettez-moi de vous rappeler rapidement comment cela fonctionne. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dans la première couche, tous les poids ont une grande valeur constante, par exemple, w = 1000.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les déplacements des neurones cachés sont calculés par b = −ws. Ainsi, par exemple, pour le deuxième neurone caché, s = 0,2 se transforme en b = −1000 × 0,2 = −200. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La dernière couche de l'échelle est déterminée par les valeurs de h. Ainsi, par exemple, la valeur que vous choisissez pour le premier h, h = -0,2, signifie que les poids de sortie des deux neurones supérieurs cachés sont respectivement -0,2 et 0,2. Et ainsi de suite, pour toute la couche de poids de sortie. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Enfin, le décalage du neurone de sortie est 0. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Et c'est tout: nous avons une description complète du NS, qui calcule bien la fonction objectif initiale. Et nous comprenons comment améliorer la qualité de l'approximation en améliorant le nombre de neurones cachés. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">De plus, dans notre fonction objectif d'origine f (x) = 0,2 + 0,4x </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ 0.3sin (15x) + 0.05cos (50x) n'a rien de spécial. </font><font style="vertical-align: inherit;">Une procédure similaire pourrait être utilisée pour toute fonction continue sur les intervalles de [0,1] à [0,1]. </font><font style="vertical-align: inherit;">En fait, nous utilisons notre NS à couche unique pour construire une table de recherche pour une fonction. </font><font style="vertical-align: inherit;">Et nous pouvons prendre cette idée comme base pour obtenir une preuve généralisée d'universalité.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Fonction de nombreux paramètres </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nous étendons nos résultats au cas d'un ensemble de variables d'entrée. Cela semble compliqué, mais toutes les idées dont nous avons besoin peuvent déjà être comprises pour le cas avec seulement deux variables entrantes. Par conséquent, nous considérons le cas avec deux variables entrantes. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Commençons par regarder ce qui se passe quand un neurone a deux entrées: </font></font><br><br><img src="https://habrastorage.org/webt/k5/cm/a9/k5cma9i-bgfwxnp2ao9h1wiwfz0.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nous avons les entrées x et y, avec les poids correspondants w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et le décalage b du neurone. Définissez le poids de w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> sur 0 et jouez avec le premier, w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et le décalage b pour voir comment ils affectent la sortie du neurone: </font></font><br><br><img src="https://habrastorage.org/webt/bl/71/6p/bl716pdfanpkwighwk2dc6m20ey.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Comme vous pouvez le voir, avec w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0, l'entrée y n'affecte pas la sortie du neurone. Tout se passe comme si x était la seule entrée.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Compte tenu de cela, que pensez-vous qu'il se passera lorsque nous augmenterons le poids de w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> à w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 100 et que w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> laissera 0? Si cela ne vous est pas immédiatement clair, réfléchissez un peu à cette question. Regardez ensuite la vidéo suivante, qui montre ce qui va se passer:</font></font><br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Votre navigateur ne prend pas en charge la vidéo HTML5. <source src="http://neuralnetworksanddeeplearning.com/movies/step_3d.mp4" type="video/mp4"></video></div></div></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Comme précédemment, avec une augmentation du poids d'entrée, la sortie se rapproche de la forme de l'étape. La différence est que notre fonction pas à pas est maintenant située en trois dimensions. Comme précédemment, nous pouvons déplacer l'emplacement des étapes en modifiant le décalage. L'angle sera au point s </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ≡ - b / w1. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reprenons le diagramme pour que le paramètre soit l'emplacement de l'étape: </font></font><br><br><img src="https://habrastorage.org/webt/aw/qs/59/awqs59ahvnac-1i9piafzg2jbpi.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nous supposons que le poids d'entrée de x est d'une grande importance - j'ai utilisé w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 1000 - et le poids w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0. Le nombre sur le neurone est la position du pas, et le x au-dessus nous rappelle que nous déplaçons le pas le long de l'axe x. Naturellement, il est tout à fait possible d'obtenir une fonction de pas le long de l'axe y, ce qui rend le poids entrant pour y grand (par exemple, w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">= 1000), et le poids pour x est 0, w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0: </font></font><br><br><img src="https://habrastorage.org/webt/xg/zw/y0/xgzwy0jgsj5q1gl3oqgygzqv0nq.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le nombre sur le neurone, encore une fois, indique la position du pas, et y au-dessus nous rappelle que nous déplaçons le pas le long de l'axe y. Je pourrais directement désigner les poids pour x et y, mais je ne l'ai pas fait, car cela jetterait le graphique. Mais gardez à l'esprit que le marqueur y indique que le poids pour y est grand et pour x est 0. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nous pouvons utiliser les fonctions de pas que nous venons de concevoir pour calculer la fonction de saillie tridimensionnelle. Pour ce faire, nous prenons deux neurones, chacun calculant une fonction de pas le long de l'axe x. Ensuite, nous combinons ces fonctions de pas avec les poids h et –h, où h est la hauteur de saillie souhaitée. Tout cela peut être vu dans le diagramme suivant:</font></font><br><br><img src="https://habrastorage.org/webt/5s/qn/wk/5sqnwkfmm7_uzs3jchov90ylyz8.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Essayez de modifier la valeur de h. Voyez comment cela se rapporte aux poids du réseau. Et comment elle modifie la hauteur de la fonction de saillie à droite. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Essayez également de modifier le point de l'étape, dont la valeur est définie sur 0,30 dans le neurone supérieur caché. Voyez comment cela change la forme de la saillie. Que se passe-t-il si vous le déplacez au-delà du point 0,70 associé au neurone caché inférieur? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nous avons appris à construire la fonction de protrusion le long de l'axe x. Naturellement, nous pouvons facilement faire la fonction de saillie le long de l'axe y, en utilisant deux fonctions de pas le long de l'axe y. Rappelons que nous pouvons le faire en faisant de grands poids à l'entrée y et en réglant le poids 0 à l'entrée x. Et alors, que se passe-t-il:</font></font><br><br><img src="https://habrastorage.org/webt/ic/uu/fq/icuufqisf9gjv8zccnkg0f0bevc.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Il semble presque identique au réseau précédent! Le seul changement visible est de petits marqueurs y sur les neurones cachés. Ils nous rappellent qu'ils produisent des fonctions pas à pas pour y, et non pour x, donc le poids à l'entrée y est très grand, et à l'entrée x il est nul, et non l'inverse. Comme précédemment, j'ai décidé de ne pas le montrer directement, afin de ne pas encombrer l'image. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Voyons ce qui se passe si nous ajoutons deux fonctions de protrusion, l'une le long de l'axe x, l'autre le long de l'axe y, toutes deux de hauteur h: </font></font><br><br><img src="https://habrastorage.org/webt/7f/u7/fc/7fu7fcn8xnl5r4zffk3tpunuedg.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pour simplifier le schéma de connexion avec un poids nul, j'ai omis. Jusqu'à présent, j'ai laissé de petits marqueurs x et y sur les neurones cachés pour me rappeler dans quelles directions les fonctions de protrusion sont calculées. Plus tard, nous les refuserons, car ils sont impliqués par la variable entrante.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Essayez de changer le paramètre h. </font><font style="vertical-align: inherit;">Comme vous pouvez le voir, pour cette raison, les poids de sortie changent, ainsi que les poids des deux fonctions de saillie, x et y. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Notre </font></font><br><br><img src="https://habrastorage.org/webt/ad/le/ww/adlewwyzmc3zhrk-fm9a9yvx6zo.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">création est un </font><font style="vertical-align: inherit;">peu comme une «fonction tour»: </font><font style="vertical-align: inherit;">si nous pouvons créer de telles fonctions tour, nous pouvons les utiliser pour approximer des fonctions arbitraires en ajoutant simplement des tours de différentes hauteurs à différents endroits: </font></font><br><br><img src="https://habrastorage.org/webt/u1/lv/xv/u1lvxvdmfi4xxsqgpjabiwofr2k.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">bien sûr, nous n'avons pas encore atteint la création d'une fonction tour arbitraire. </font><font style="vertical-align: inherit;">Jusqu'à présent, nous avons construit quelque chose comme une tour centrale de hauteur 2h avec un plateau de hauteur h qui l'entoure. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mais nous pouvons faire fonctionner une tour. </font><font style="vertical-align: inherit;">Rappelons que nous avons précédemment montré comment les neurones peuvent être utilisés pour implémenter l'instruction if-then-else:</font></font><br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span>  &gt;= :  <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>:  <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre> <br>  C'était un neurone à une entrée.  Et nous devons appliquer une idée similaire à la sortie combinée des neurones cachés: <br><br><pre> <code class="python hljs"> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>     &gt;= :  <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>:  <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre> <br>  Si nous choisissons le bon seuil - par exemple, 3h / 2, coincé entre la hauteur du plateau et la hauteur de la tour centrale - nous pouvons écraser le plateau à zéro et ne laisser qu'une seule tour. <br><br>  Imaginez comment faire cela?  Essayez d'expérimenter avec le réseau suivant.  Maintenant, nous traçons la sortie de l'ensemble du réseau, et pas seulement la sortie pondérée de la couche cachée.  Cela signifie que nous ajoutons le terme de décalage à la sortie pondérée de la couche cachée et appliquons le sigmoïde.  Pouvez-vous trouver les valeurs de h et b pour lesquelles vous obtenez une tour?  Si vous êtes bloqué à ce stade, voici deux conseils: (1) pour que le neurone sortant démontre le comportement correct dans le style if-then-else, nous avons besoin que les poids entrants (tous h ou –h) soient grands;  (2) la valeur de b détermine l'échelle du seuil si-alors-sinon. <br><br><img src="https://habrastorage.org/webt/ys/k-/1u/ysk-1uvu-jo68ikk5rqu274u7wc.png"><br><br>  Avec les paramètres par défaut, la sortie est similaire à une version aplatie du diagramme précédent, avec une tour et un plateau.  Pour obtenir le comportement souhaité, vous devez augmenter la valeur de h.  Cela nous donnera le comportement de seuil de if-then-else.  Deuxièmement, pour fixer correctement le seuil, il faut choisir b ≈ −3h / 2. <br><br>  Voici à quoi cela ressemble pour h = 10: <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Votre navigateur ne prend pas en charge la vidéo HTML5. <source src="http://neuralnetworksanddeeplearning.com/movies/tower_construction.mp4" type="video/mp4"></video></div></div></div><br>  Même pour des valeurs relativement modestes de h, nous obtenons une belle fonction de tour.  Et, bien sûr, nous pouvons obtenir un résultat arbitrairement beau en augmentant encore h et en maintenant le biais au niveau b = −3h / 2. <br><br>  Essayons de coller deux réseaux ensemble pour compter deux fonctions de tour différentes.  Pour clarifier les rôles respectifs des deux sous-réseaux, je les mets dans des rectangles séparés: chacun d'eux calcule la fonction tour en utilisant la technique décrite ci-dessus.  Le graphique de droite montre la sortie pondérée de la deuxième couche cachée, c'est-à-dire la combinaison pondérée des fonctions de la tour. <br><br><img src="https://habrastorage.org/webt/8-/cl/ke/8-clkebo6vphf1-0_jgoaf3exts.png"><br><br>  En particulier, on peut voir qu'en changeant le poids dans la dernière couche, vous pouvez changer la hauteur des tours de sortie. <br><br>  La même idée vous permet de calculer autant de tours que vous le souhaitez.  Nous pouvons les rendre arbitrairement minces et hauts.  Par conséquent, nous garantissons que la sortie pondérée de la deuxième couche cachée se rapproche de toute fonction souhaitée de deux variables: <br><br><img src="https://habrastorage.org/webt/ig/0u/5z/ig0u5zbzifftdfq4ww4y9a4r7du.png"><br><br>  En particulier, en faisant bien approcher la sortie pondérée de la deuxième couche cachée σ <sup>−1</sup> hiddenf, nous garantissons que la sortie de notre réseau sera une bonne approximation de la fonction f souhaitée. <br><br>  Qu'en est-il des fonctions de nombreuses variables? <br><br>  Essayons de prendre trois variables, x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> .  Le réseau suivant peut-il être utilisé pour calculer la fonction de la tour en quatre dimensions? <br><br><img src="https://habrastorage.org/webt/wg/ki/kn/wgkiknicnzeoaept0d-cemw0sd0.png"><br><br>  Ici x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> désignent l'entrée réseau.  s <sub>1</sub> , t <sub>1,</sub> et ainsi de suite - les points de pas pour les neurones - c'est-à-dire que tous les poids dans la première couche sont grands, et les décalages sont assignés de sorte que les points de pas soient s <sub>1</sub> , t <sub>1</sub> , s <sub>2</sub> , ... Les poids dans la deuxième couche alternent, + h, −h, où h est un très grand nombre.  Le décalage de sortie est de -5h / 2. <br><br>  Le réseau calcule une fonction égale à 1 dans trois conditions: x <sub>1</sub> est compris entre s <sub>1</sub> et t <sub>1</sub> ;  x <sub>2</sub> est compris entre s <sub>2</sub> et t <sub>2</sub> ;  x <sub>3</sub> est compris entre s <sub>3</sub> et t <sub>3</sub> .  Le réseau est 0 dans tous les autres endroits.  Il s'agit d'une tour dans laquelle 1 est une petite partie de l'espace d'entrée et 0 est tout le reste. <br><br>  En collant un grand nombre de ces réseaux, nous pouvons obtenir autant de tours que nous le souhaitons et approcher une fonction arbitraire de trois variables.  La même idée fonctionne en m dimensions.  Seul le décalage de sortie (−m + 1/2) h est modifié pour comprimer correctement les valeurs souhaitées et supprimer le plateau. <br><br>  Eh bien, maintenant nous savons comment utiliser NS pour approximer la fonction réelle de nombreuses variables.  Qu'en est-il des fonctions vectorielles f (x <sub>1</sub> , ..., x <sub>m</sub> ) ∈ R <sup>n</sup> ?  Bien entendu, une telle fonction peut être considérée simplement comme n fonctions réelles distinctes f1 (x <sub>1</sub> , ..., x <sub>m</sub> ), f2 (x <sub>1</sub> , ..., x <sub>m</sub> ), etc.  Et puis nous collons juste tous les réseaux ensemble.  Il est donc facile de le comprendre. <br><br><h3>  Défi </h3><br><ul><li>  Nous avons vu comment utiliser des réseaux de neurones avec deux couches cachées pour approximer une fonction arbitraire.  Pouvez-vous prouver que cela est possible avec une seule couche cachée?  Astuce - essayez de travailler avec seulement deux variables de sortie et montrez que: (a) il est possible d'obtenir les fonctions des étapes non seulement le long des axes x ou y, mais aussi dans une direction arbitraire;  (b) en additionnant de nombreuses constructions de l'étape (a), il est possible d'approcher la fonction d'une tour ronde plutôt que rectangulaire;  © à l'aide de tours rondes, il est possible d'approximer une fonction arbitraire.  L'étape © sera plus facile à réaliser en utilisant le matériel présenté dans ce chapitre un peu plus bas. </li></ul><br><h2>  Aller au-delà des neurones sigmoïdes </h2><br>  Nous avons prouvé qu'un réseau de neurones sigmoïdes peut calculer n'importe quelle fonction.  Rappelons que dans un neurone sigmoïde, les entrées x <sub>1</sub> , x <sub>2</sub> , ... se transforment à la sortie en σ (∑ <sub>j</sub> w <sub>j</sub> x <sub>j j</sub> + b), où w <sub>j</sub> sont les poids, b est le déplacement, σ est le sigmoïde. <br><br><img src="https://habrastorage.org/webt/0h/ut/93/0hut93wneejtjvxvxiwnfwpmo40.png"><br><br>  Et si nous regardons un autre type de neurone utilisant une fonction d'activation différente, s (z): <br><br><img src="https://habrastorage.org/webt/ua/0-/it/ua0-itpxz-uwkpnptxfsvszqabg.png"><br><br>  Autrement dit, nous supposons que si un neurone a x <sub>1</sub> , x <sub>2</sub> , ... les poids w <sub>1</sub> , w <sub>2</sub> , ... et le biais b à l'entrée, alors la sortie aura s ( <sub>j j</sub> w <sub>j</sub> x <sub>j j</sub> + b). <br><br>  On peut utiliser cette fonction d'activation pour se faire marcher, tout comme dans le cas du sigmoïde.  Essayez (dans l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article d'origine</a> ) sur le diagramme de soulever le poids pour, disons, w = 100: <br><br><img src="https://habrastorage.org/webt/vz/-v/mu/vz-vmulc79w1g7xxom_btmnfyow.png"><br><br><img src="https://habrastorage.org/webt/nf/aq/iz/nfaqiznhl2klhebfc33iiy6htrg.png"><br><br>  Comme dans le cas du sigmoïde, de ce fait, la fonction d'activation est compressée et, par conséquent, se transforme en une très bonne approximation de la fonction de pas.  Essayez de changer le décalage, et vous verrez que nous pouvons changer l'emplacement de l'étape à n'importe quel.  Par conséquent, nous pouvons utiliser toutes les mêmes astuces qu'auparavant pour calculer n'importe quelle fonction souhaitée. <br><br>  Quelles propriétés doit avoir s (z) pour que cela fonctionne?  Nous devons supposer que s (z) est bien défini comme z → −∞ et z → ∞.  Ces limites sont deux valeurs acceptées par notre fonction pas à pas.  Nous devons également supposer que ces limites sont différentes.  S'ils ne différaient pas, les étapes ne fonctionneraient pas; il y aurait simplement un horaire fixe!  Mais si la fonction d'activation s (z) satisfait à ces propriétés, les neurones basés sur celle-ci sont universellement adaptés aux calculs. <br><br><h3>  Les tâches </h3><br><ul><li>  Plus tôt dans le livre, nous avons rencontré un type différent de neurone - un neurone linéaire redressé ou une unité linéaire rectifiée, ReLU.  Expliquez pourquoi ces neurones ne remplissent pas les conditions nécessaires à l'universalité.  Trouvez des preuves de polyvalence montrant que les ReLU sont universellement adaptés à l'informatique. </li><li>  Supposons que nous considérons des neurones linéaires, avec la fonction d'activation s (z) = z.  Expliquez pourquoi les neurones linéaires ne satisfont pas aux conditions d'universalité.  Montrez que ces neurones ne peuvent pas être utilisés pour l'informatique universelle. </li></ul><br><h2>  Fonction étape fixe </h2><br>  Pour le moment, nous avons supposé que nos neurones produisent des fonctions de pas précises.  C'est une bonne approximation, mais seulement une approximation.  En fait, il existe un étroit intervalle d'échec, illustré dans le graphique suivant, où les fonctions ne se comportent pas du tout comme une fonction pas à pas: <br><br><img src="https://habrastorage.org/webt/mr/0t/ng/mr0tng4l1giob-gsuhyo_oh_vk0.png"><br><br>  En cette période d'échec, mon explication de l'universalité ne fonctionne pas. <br><br>  L'échec n'est pas si effrayant.  En fixant des poids d'entrée suffisamment grands, nous pouvons rendre ces écarts arbitrairement petits.  Nous pouvons les rendre beaucoup plus petits que sur la carte, invisibles à l'œil nu.  Alors peut-être que nous n'avons pas à nous soucier de ce problème. <br><br>  Néanmoins, j'aimerais avoir un moyen de le résoudre. <br><br>  Il s'avère que c'est facile à résoudre.  Voyons cette solution pour calculer les fonctions NS avec une seule entrée et une seule sortie.  Les mêmes idées fonctionneront pour résoudre le problème avec un grand nombre d'entrées et de sorties. <br><br>  Supposons en particulier que nous voulons que notre réseau calcule une fonction f.  Comme précédemment, nous essayons de le faire en concevant le réseau de sorte que la sortie pondérée de la couche cachée de neurones soit σ <sup>−1</sup> ⋅f (x): <br><br><img src="https://habrastorage.org/webt/sk/bu/bw/skbubwnwkrrpukeblqe9a1qo8cw.png"><br><br>  Si nous le faisons en utilisant la technique décrite ci-dessus, nous forcerons les neurones cachés à produire une séquence de fonctions de protrusion: <br><br><img src="https://habrastorage.org/webt/71/uc/x_/71ucx_26mzx0_isj6dlob9wdacq.png"><br><br>  Bien sûr, j'ai exagéré la taille des intervalles d'échec, pour que ce soit plus facile à voir.  Il devrait être clair que si nous additionnons toutes ces fonctions des protubérances, nous obtenons une assez bonne approximation de σ <sup>−1</sup> ⋅f (x) partout sauf pour les intervalles de défaillance. <br><br>  Mais, supposons qu'au lieu d'utiliser l'approximation qui vient d'être décrite, nous utilisons un ensemble de neurones cachés pour calculer l'approximation de la moitié de notre fonction objective d'origine, c'est-à-dire σ <sup>−1</sup> ⋅f (x) / 2.  Bien sûr, cela ressemblera à une version à l'échelle du dernier graphique: <br><br><img src="https://habrastorage.org/webt/8-/0b/fv/8-0bfvrf5njiwum-w6d8edu4dro.png"><br><br>  Et, supposons que nous faisons un autre ensemble de neurones cachés calculer l'approximation de σ <sup>−1</sup> −f (x) / 2, cependant, à sa base, les protubérances seront décalées de la moitié de leur largeur: <br><br><img src="https://habrastorage.org/webt/dj/cz/7n/djcz7nxhhm98yhiq94uluglvm-m.png"><br><br>  Nous avons maintenant deux approximations différentes pour σ - 1⋅f (x) / 2.  Si nous additionnons ces deux approximations, nous obtenons une approximation générale de σ - 1⋅f (x).  Cette approximation générale comportera toujours des inexactitudes à petits intervalles.  Mais le problème sera moindre qu'auparavant - car les points tombant dans les intervalles de l'échec de la première approximation ne tomberont pas dans les intervalles de l'échec de la seconde approximation.  Par conséquent, l'approximation de ces intervalles sera environ 2 fois meilleure. <br><br>  Nous pouvons améliorer la situation en ajoutant un grand nombre, M, d'approximations qui se chevauchent de la fonction σ - 1⋅f (x) / M.  Si tous leurs intervalles de défaillance sont suffisamment étroits, aucun courant ne sera dans l'un d'eux.  Si vous utilisez un nombre suffisamment grand d'approximations de M qui se chevauchent, le résultat est une excellente approximation générale. <br><br><h2>  Conclusion </h2><br>  L'explication de l'universalité discutée ici ne peut certainement pas être appelée une description pratique de la façon de compter les fonctions à l'aide de réseaux de neurones!  En ce sens, cela ressemble plus à une preuve de la polyvalence des portes logiques NAND et plus encore.  Par conséquent, j'ai essentiellement essayé de rendre ce design clair et facile à suivre sans optimiser ses détails.  Cependant, essayer d'optimiser cette conception peut être un exercice intéressant et instructif pour vous. <br><br>  Bien que le résultat obtenu ne puisse pas être directement utilisé pour créer NS, il est important car il supprime la question de la calculabilité d'une fonction particulière utilisant NS.  La réponse à une telle question sera toujours positive.  Par conséquent, il est correct de demander si une fonction est calculable, mais quelle est la bonne façon de la calculer. <br><br>  Notre conception universelle utilise seulement deux couches cachées pour calculer une fonction arbitraire.  Comme nous l'avons vu, il est possible d'obtenir le même résultat avec une seule couche cachée.  Compte tenu de cela, vous vous demandez peut-être pourquoi nous avons besoin de réseaux profonds, c'est-à-dire des réseaux avec un grand nombre de couches cachées.  Ne pouvons-nous pas simplement remplacer ces réseaux par des réseaux peu profonds qui ont une couche cachée? <br><br>  Bien que, en principe, cela soit possible, il existe de bonnes raisons pratiques d'utiliser des réseaux de neurones profonds.  Comme décrit dans le chapitre 1, les NS profonds ont une structure hiérarchique qui leur permet de bien s'adapter pour étudier les connaissances hiérarchiques, qui sont utiles pour résoudre des problèmes réels.  Plus précisément, lors de la résolution de problèmes tels que la reconnaissance de formes, il est utile d'utiliser un système qui comprend non seulement des pixels individuels, mais aussi des concepts de plus en plus complexes: des bordures aux formes géométriques simples, et au-delà, aux scènes complexes impliquant plusieurs objets.  Dans les chapitres suivants, nous verrons des preuves en faveur du fait que les SN profonds seront mieux en mesure de faire face à l'étude de telles hiérarchies de connaissances que les NS peu profondes.  Pour résumer: l'universalité nous dit que NS peut calculer n'importe quelle fonction;  des données empiriques suggèrent que les NS profonds sont mieux adaptés à l'étude des fonctions utiles pour résoudre de nombreux problèmes du monde réel. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr461659/">https://habr.com/ru/post/fr461659/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr461649/index.html">Comment vais-je sauver le monde</a></li>
<li><a href="../fr461651/index.html">Frontend Weekly Digest (22-28 juillet 2019)</a></li>
<li><a href="../fr461653/index.html">Radio définie par logiciel - comment ça marche? Partie 10</a></li>
<li><a href="../fr461655/index.html">Le condensé de matières fraîches du monde du frontend pour la dernière semaine n ° 373 (22-28 juillet 2019)</a></li>
<li><a href="../fr461657/index.html">Acheter Red Hat: cela aidera-t-il le géant bleu à se battre pour le leadership du cloud hybride</a></li>
<li><a href="../fr461661/index.html">Guide de développement basé sur les composants</a></li>
<li><a href="../fr461663/index.html">L'histoire de la façon dont Linux a introduit Windows</a></li>
<li><a href="../fr461665/index.html">Zen2. L'évolution de la plateforme AM4 sur l'exemple de Ryzen 7 3700x</a></li>
<li><a href="../fr461669/index.html">PHP Digest n ° 161 (15-29 juillet 2019)</a></li>
<li><a href="../fr461673/index.html">8 conseils pour les programmeurs débutants ou une rétrospective de ma carrière</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>