<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏾‍💼 🗡️ 👵🏼 Erstellen von Client-Routing / semantischer Suche bei Profi.ru. 👨🏾‍🏫 🏂🏽 🍲</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Erstellen von Client-Routing / semantischer Suche und Clustering beliebiger externer Korpusse bei Profi.ru. 
 TLDR 


 Dies ist eine sehr kurze Zusamm...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Erstellen von Client-Routing / semantischer Suche bei Profi.ru.</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/428674/"><h1 id="building-client-routing--semantic-search-and-clustering-arbitrary-external-corpuses-at-profiru">  Erstellen von Client-Routing / semantischer Suche und Clustering beliebiger externer Korpusse bei Profi.ru. </h1><br><h2 id="tldr">  <strong>TLDR</strong> </h2><br><p>  Dies ist eine sehr kurze Zusammenfassung (oder ein Teaser) darüber, was wir in ungefähr 2 Monaten in der Profi.ru DS-Abteilung geschafft haben (ich war etwas länger dort, aber mich und mein Team einzubinden war eine separate Sache zuerst gemacht). </p><a name="habracut"></a><br><h2 id="projected-goals">  Projizierte Ziele </h2><br><ol><li> Verstehen Sie die Eingabe / Absicht des Clients und leiten Sie die Clients entsprechend weiter (wir haben uns am Ende für einen agnostischen Klassifikator für die Eingabequalität entschieden, obwohl wir auch Modelle mit Char-Level und Sprachmodellen in Betracht gezogen haben. Einfachheitsregeln); </li><li>  Finden Sie völlig neue Dienste und Synonyme für die vorhandenen Dienste. </li><li>  Als Teilziel von (2) - lernen, richtige Cluster auf beliebigen externen Korpussen aufzubauen; </li></ol><br><h2 id="achieved-goals">  Ziele erreicht </h2><br><p>  Offensichtlich wurden einige dieser Ergebnisse nicht nur von unserem Team erzielt, sondern auch von mehreren Teams (d. H. Wir haben den Scraping-Teil für Domain-Korpusse und die manuelle Annotation offensichtlich nicht durchgeführt, obwohl ich glaube, dass das Scraping auch von unserem Team gelöst werden kann - Sie brauchen es nur genug Proxies + wahrscheinlich etwas Erfahrung mit Selen). </p><br><p>  <strong>Geschäftsziele:</strong> </p><br><ol><li> ~ <code>88+%</code> (gegenüber ~ <code>60%</code> bei elastischer Suche) Genauigkeit bei Client-Routing / Intent-Klassifizierung (~ <code>5k</code> Klassen); </li><li>  Die Suche ist unabhängig von der Eingabequalität (Druckfehler / Teileingabe). </li><li>  Der Klassifikator verallgemeinert, die morphologische Struktur der Sprache wird ausgenutzt; </li><li>  Der Klassifikator schlägt die elastische Suche bei verschiedenen Benchmarks deutlich (siehe unten). </li><li>  Um auf der sicheren Seite zu sein - es wurden mindestens <code>1,000</code> neue Dienste gefunden + mindestens <code>15,000</code> Synonyme (gegenüber dem aktuellen Stand von <code>5,000</code> + ~ <code>30,000</code> ).  Ich erwarte, dass sich diese Zahl verdoppelt oder sogar verdreifacht; </li></ol><br><p>  Die letzte Kugel ist eine Schätzung des Baseballstadions, aber eine konservative. <br>  Auch AB-Tests werden folgen.  Aber ich bin von diesen Ergebnissen überzeugt. </p><br><p>  <strong>"Wissenschaftliche" Ziele:</strong> </p><br><ol><li>  Wir haben viele moderne Satzeinbettungstechniken unter Verwendung einer nachgeschalteten Klassifizierungsaufgabe + KNN gründlich mit einer Datenbank von Dienstsynonymen verglichen. </li><li>  Wir haben es geschafft, die schwach überwachte elastische Suche in diesem Benchmark (siehe Details unten) mit <strong>UNSUPERVISED-</strong> Methoden zu schlagen (im Wesentlichen ist ihr Klassifikator ein Beutel mit Gramm). </li><li>  Wir haben eine neuartige Methode entwickelt, um angewandte NLP-Modelle zu erstellen (eine einfache Vanille-Bi-LSTM + -Tüte mit Einbettungen, im Wesentlichen schneller Text entspricht RNN) - dies berücksichtigt die Morphologie der russischen Sprache und verallgemeinert sich gut; </li><li>  Wir haben gezeigt, dass unsere endgültige Einbettungstechnik (eine Flaschenhalsschicht des besten Klassifikators) in Kombination mit modernsten unbeaufsichtigten Algorithmen (UMAP + HDBSCAN) Sternhaufen erzeugen kann. </li><li>  Wir haben in der Praxis die Möglichkeit, Durchführbarkeit und Verwendbarkeit von: <br><ul><li>  Wissensdestillation; </li><li>  Erweiterungen für Textdaten (sic!); </li></ul></li><li>  Das Training textbasierter Klassifikatoren mit dynamischen Erweiterungen reduzierte die Konvergenzzeit drastisch (10x) im Vergleich zur Erzeugung größerer statischer Datensätze (d. H. Das CNN lernt, den Fehler zu verallgemeinern, der angezeigt wird, wenn drastisch weniger erweiterte Sätze angezeigt werden). </li></ol><br><h2 id="overall-project-structure">  Gesamtprojektstruktur </h2><br><p>  Dies schließt den endgültigen Klassifikator nicht ein. <br>  Auch am Ende haben wir gefälschte RNN- und Triplett-Verlust-Modelle zugunsten des Klassifikator-Engpasses aufgegeben. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/1c2/449/157/1c24491576ed703ebc571dfd4d7d8da3.png"></p><br><h2 id="what-works-in-nlp-now">  Was funktioniert jetzt in NLP? </h2><br><p>  Eine Vogelperspektive: <br><img src="https://habrastorage.org/getpro/habr/post_images/5a1/8f5/df1/5a18f5df1e133bef082edf9315011da7.png"></p><br><p>  Vielleicht wissen Sie auch, dass NLP <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">jetzt</a> den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Imagenet-Moment</a> erlebt. </p><br><h2 id="large-scale-umap-hack">  UMAP-Hack in großem Maßstab </h2><br><p>  Beim Erstellen von Clustern stießen wir auf einen Weg / Hack, um UMAP im Wesentlichen auf Datensätze mit einer Größe von über 100 m (oder vielleicht sogar 1 Mrd.) anzuwenden.  Erstellen Sie im Wesentlichen ein KNN-Diagramm mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">FAISS</a> und schreiben <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sie</a> dann einfach die UMAP-Hauptschleife mithilfe Ihrer GPU in PyTorch neu.  Wir haben das nicht gebraucht und das Konzept aufgegeben (wir hatten immerhin nur 10-15 Millionen Punkte), aber bitte folgen Sie diesem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Thread</a> für Details. </p><br><h2 id="what-works-best">  Was funktioniert am besten </h2><br><ul><li>  Für die überwachte Klassifizierung erfüllt Fast-Text die RNN (Bi-LSTM) + sorgfältig ausgewählte Menge von n-Gramm; </li><li>  Implementierung - einfaches Python für <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">n-Gramm</a> + PyTorch Embedding Bag Layer; </li><li>  Für das Clustering - die Engpassschicht dieses Modells + UMAP + HDBSCAN; </li></ul><br><h2 id="best-classifier-benchmarks">  <strong>Beste Klassifikator-Benchmarks</strong> </h2><br><p>  <strong>Manuell kommentierter Entwickler-Set</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/04b/7cc/e7e/04b7cce7e8cee9cee4b066b6a353bed9.jpg"></p><br><p>  <strong>Von links nach rechts:</strong> <br>  (Top1 Genauigkeit) </p><br><ul><li>  Aktueller Algorithmus (elastische Suche); </li><li>  Erste RNN; </li><li>  Neue Anmerkung; </li><li>  Tuning </li><li>  Fast-Text-Einbettungsbeutelschicht; </li><li>  Hinzufügen von Tippfehlern und Teileingaben; </li><li>  Dynamische Erzeugung von Fehlern und Teileingabe ( <strong>Trainingszeit um das 10-fache reduziert</strong> ); </li><li>  Endergebnis; </li></ul><br><p>  <strong>Manuell kommentierte Entwicklermenge + 1-3 Fehler pro Abfrage</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/ae2/a31/040/ae2a31040dbd77402d6b6dfee9eeba28.jpg"></p><br><p>  <strong>Von links nach rechts:</strong> <br>  (Top1 Genauigkeit) </p><br><ul><li>  Aktueller Algorithmus (elastische Suche); </li><li>  Fast-Text-Einbettungsbeutelschicht; </li><li>  Hinzufügen von Tippfehlern und Teileingaben; </li><li>  Dynamische Erzeugung von Fehlern und Teileingabe; </li><li>  Endergebnis; </li></ul><br><p>  <strong>Manuell kommentierter Dev Set + Teileingabe</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/c3c/680/681/c3c680681dd3166b95246930f1f1b1a8.jpg"></p><br><p>  <strong>Von links nach rechts:</strong> <br>  (Top1 Genauigkeit) </p><br><ul><li>  Aktueller Algorithmus (elastische Suche); </li><li>  Fast-Text-Einbettungsbeutelschicht; </li><li>  Hinzufügen von Tippfehlern und Teileingaben; </li><li>  Dynamische Erzeugung von Fehlern und Teileingabe; </li><li>  Endergebnis; </li></ul><br><h2 id="large-scale-corpuses--n-gram-selection">  Große Korpusse / n-Gramm-Auswahl </h2><br><ul><li>  Wir haben die größten Korpusse für die russische Sprache gesammelt: <br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Areneum</a> - eine verarbeitete Version ist hier verfügbar - die Autoren des Datensatzes haben nicht geantwortet. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Taiga</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Allgemeines Crawlen</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Wiki</a> - bitte folgen Sie diesen Artikeln; </li></ul></li><li>  Wir haben ein <code>100m</code> Wörterbuch mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">1 TB Crawl</a> gesammelt. </li><li>  Verwenden Sie diesen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hack auch</a> , um solche Dateien schneller (über Nacht) herunterzuladen. </li><li>  Wir haben einen optimalen Satz von <code>1m</code> n-Gramm für unseren Klassifikator ausgewählt, um ihn am besten zu verallgemeinern ( <code>500k</code> beliebteste n-Gramm aus schnellem Text, der auf russischer Wikipedia trainiert wurde, + <code>500k</code> beliebteste n-Gramm in unseren Domain-Daten). </li></ul><br><p>  <strong>Stresstest unserer 1M n-Gramm auf 100M Vokabeln:</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/198/1fe/38b/1981fe38b03b4cdf76022f4ff6ef0074.png" alt="Bild"></p><br><h2 id="text-augmentations">  Texterweiterungen </h2><br><p>  Kurz gesagt: </p><br><ul><li>  Nehmen Sie ein großes Wörterbuch mit Fehlern (z. B. 10-100 m eindeutige Wörter); </li><li>  Generieren Sie einen Fehler (lassen Sie einen Buchstaben fallen, tauschen Sie einen Buchstaben mit berechneten Wahrscheinlichkeiten aus, fügen Sie einen zufälligen Buchstaben ein, verwenden Sie möglicherweise das Tastaturlayout usw.); </li><li>  Überprüfen Sie, ob das neue Wort im Wörterbuch enthalten ist. </li></ul><br><p>  Wir haben viele Anfragen an Dienste wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesen brutal gestellt</a> (um ihren Datensatz im Wesentlichen rückzuentwickeln), und sie enthalten ein sehr kleines Wörterbuch (auch dieser Dienst wird von einem Baumklassifizierer mit n-Gramm-Funktionen unterstützt).  Es war irgendwie lustig zu sehen, dass <strong>sie nur 30-50% der Wörter abdeckten, die wir auf einigen Korpussen hatten</strong> . </p><br><p>  <strong>Unser Ansatz ist weit überlegen, wenn Sie Zugriff auf ein großes Domain-Vokabular haben</strong> . </p><br><h2 id="best-unsupervised--semi-supervised-results">  Beste unbeaufsichtigte / halbüberwachte Ergebnisse </h2><br><p>  KNN diente als Benchmark zum Vergleich verschiedener Einbettungsmethoden. </p><br><p>  (Vektorgröße) Liste der getesteten Modelle: </p><br><ul><li>  (512) Falscher Satzdetektor in großem Maßstab, der auf 200 GB allgemeiner Durchforstungsdaten trainiert ist; </li><li>  (300) Fälschungssatzdetektor, der darauf trainiert ist, einen zufälligen Satz aus Wikipedia von einem Dienst zu unterscheiden; </li><li>  (300) Von hier erhaltener Fast-Text, der auf Araneum Corpus vorab trainiert wurde; </li><li>  (200) Fast-Text, der auf unseren Domain-Daten trainiert wurde; </li><li>  (300) Fast-Text, der auf 200 GB Common Crawl-Daten trainiert wurde; </li><li>  (300) Ein siamesisches Netzwerk mit Triplettverlust, das mit Diensten / Synonymen / zufälligen Sätzen aus Wikipedia trainiert wurde; </li><li>  (200) Erste Iteration der Einbettungsschicht des Einbettungsbeutels RNN, ein Satz wird als ein ganzer Beutel mit Einbettungen codiert; </li><li>  (200) Gleich, aber zuerst wird der Satz in Wörter aufgeteilt, dann wird jedes Wort eingebettet, dann wird der Durchschnitt genommen; </li><li>  (300) Wie oben, jedoch für das endgültige Modell; </li><li>  (300) Wie oben, jedoch für das endgültige Modell; </li><li>  (250) Engpassschicht des endgültigen Modells (250 Neuronen); </li><li>  Schwach überwachte elastische Suchgrundlinie; </li></ul><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ca1/e0b/e9c/ca1e0be9c152d092d4149f9986b87289.png" alt="Standard"></p><br><p>  Um Undichtigkeiten zu vermeiden, wurden alle zufälligen Sätze zufällig ausgewählt.  Ihre Länge in Worten entsprach der Länge der Dienste / Synonyme, mit denen sie verglichen wurden.  Es wurden auch Maßnahmen ergriffen, um sicherzustellen, dass Modelle nicht nur durch Trennen von Vokabeln lernten (Einbettungen wurden eingefroren, Wikipedia wurde unterabgetastet, um sicherzustellen, dass in jedem Wikipedia-Satz mindestens ein Domain-Wort enthalten war). </p><br><h2 id="cluster-visualization">  Cluster-Visualisierung </h2><br><p>  <strong>3D</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/4b7/f10/d19/4b7f10d19a785b5f690a28f2e2a039e6.gif"></p><br><p>  <strong>2D</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/ad7/0ad/441/ad70ad441ecae6f396c8bb76826484df.png"></p><br><h2 id="cluster-exploration-interface">  Cluster-Exploration "Schnittstelle" </h2><br><p>  Grün - neues Wort / Synonym. <br>  Grauer Hintergrund - wahrscheinlich neues Wort. <br>  Grauer Text - vorhandenes Synonym. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/cda/d17/00f/cdad1700ff0701ff6643a4aa14041d31.jpg"></p><br><h2 id="ablation-tests-and-what-works-what-we-tried-and-what-we-did-not">  Ablationstests und was funktioniert, was wir versucht haben und was nicht </h2><br><ol><li>  Siehe die obigen Tabellen; </li><li>  Einfacher Durchschnitt / tf-idf Durchschnitt der Einbettungen in schnellen Text - eine <strong>SEHR beeindruckende Basislinie</strong> ; </li><li>  Fast-Text&gt; Word2Vec für Russisch; </li><li>  Satzeinbettung durch falsche Satzerkennung funktioniert, verblasst aber im Vergleich zu anderen Methoden; </li><li>  BPE (Satzstück) zeigte keine Verbesserung unserer Domain; </li><li>  Char-Level-Modelle hatten trotz des widerrufenen Papiers von Google Schwierigkeiten, sich zu verallgemeinern. </li><li>  Wir haben versucht, einen Mehrkopf-Transformator (mit Klassifikator- und Sprachmodellierungsköpfen) zu verwenden, aber die zur Verfügung stehende Anmerkung ergab ungefähr die gleiche Leistung wie einfache Vanille-LSTM-basierte Modelle.  Als wir zur Einbettung schlechter Ansätze übergingen, gaben wir diese Forschungsrichtung auf, da der Transformator weniger praktisch und unpraktisch war, einen LM-Kopf zusammen mit einer Einbettungsbeutelschicht zu haben. </li><li>  <strong>BERT</strong> - scheint <strong>übertrieben</strong> zu sein, auch einige Leute behaupten, dass Transformatoren buchstäblich wochenlang trainieren; </li><li>  <strong>ELMO</strong> - Die Verwendung einer Bibliothek wie AllenNLP erscheint meiner Meinung nach sowohl in Forschungs- / Produktions- als auch in Bildungsumgebungen aus Gründen, die ich hier nicht angeben werde, kontraproduktiv. </li></ol><br><h2 id="deploy">  Bereitstellen </h2><br><p>  Fertig mit: </p><br><ul><li>  Docker-Container mit einem einfachen Web-Service; </li><li>  Nur CPU für Inferenz ist ausreichend; </li><li>  ~ <code>2.5 ms</code> pro Abfrage auf der CPU, Batching nicht wirklich notwendig; </li><li>  ~ <code>1GB</code> RAM-Speicherplatz; </li><li>  Fast keine Abhängigkeiten, außer <code>PyTorch</code> , <code>numpy</code> und <code>pandas</code> (und Webserver ofc). </li><li>  Imitieren Sie eine solche Fast-Text-N-Gramm-Generation. </li><li>  Einbetten von Bag Layer + Indizes, wie sie gerade in einem Wörterbuch gespeichert wurden; </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de428674/">https://habr.com/ru/post/de428674/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de428662/index.html">Programmieraufgabe für den Einzelhandel</a></li>
<li><a href="../de428664/index.html">Linux-Kernel-Boot. Teil 1</a></li>
<li><a href="../de428666/index.html">Wie ich mit CSS-Masken stimmungsverändernde Animationen erstellt habe</a></li>
<li><a href="../de428668/index.html">Blizzard kündigte die Veröffentlichung der Neuveröffentlichung von WarCraft III im Jahr 2019 an. Vorbestellung öffnen</a></li>
<li><a href="../de428672/index.html">QuietOn Active Squelch Übersicht</a></li>
<li><a href="../de428676/index.html">Grundlegendes zu C #: Zuweisen von Speicher für einen Referenztyp auf dem Stapel</a></li>
<li><a href="../de428680/index.html">Erstellen und Integrieren eines VK-Bots in eine Gruppe über VkBotLongPoll [Python]</a></li>
<li><a href="../de428682/index.html">Selbstzerstörender Beta-Fallout 76</a></li>
<li><a href="../de428688/index.html">Einrichten der Arbeitsumgebung in Docker für die yii-Framework-Anwendung</a></li>
<li><a href="../de428690/index.html">Wie Sie Ihrer Freundin das Programmieren beibringen, wenn Sie kein Lehrer sind, aber sie an Sie glaubt</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>