<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèº‚Äçüè´ üëâ üë©üèæ‚Äçüöí Nano-neur√¥nio - 7 fun√ß√µes JavaScript simples, mostrando como a m√°quina pode "aprender" üßùüèº üßö üå©Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Um nano-neur√¥nio √© uma vers√£o simplificada de um neur√¥nio a partir do conceito de rede neural. O nano-neur√¥nio executa a tarefa mais simples e √© trein...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Nano-neur√¥nio - 7 fun√ß√µes JavaScript simples, mostrando como a m√°quina pode "aprender"</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/479220/"><p>  <a href="https://github.com/trekhleb/nano-neuron" rel="nofollow"><strong>Um nano-neur√¥nio</strong></a> √© uma vers√£o <em>simplificada</em> de um neur√¥nio a partir do conceito de rede neural.  O nano-neur√¥nio executa a tarefa mais simples e √© treinado para converter a temperatura de graus Celsius em graus Fahrenheit. </p><br><p>  O c√≥digo <a href="" rel="nofollow"><strong>NanoNeuron.js</strong></a> consiste em 7 fun√ß√µes JavaScript simples que envolvem aprendizado, treinamento, previs√£o e propaga√ß√£o direta e reversa do sinal do modelo.  O objetivo de escrever essas fun√ß√µes era fornecer ao leitor uma explica√ß√£o b√°sica e m√≠nima (intui√ß√£o) de como, afinal, uma m√°quina pode "aprender".  O c√≥digo n√£o usa bibliotecas de terceiros.  Como diz o ditado, apenas fun√ß√µes JavaScript "baunilha" simples. </p><br><p>  Essas fun√ß√µes n√£o s√£o de forma <strong>alguma</strong> um guia completo para o aprendizado de m√°quina.  Muitos conceitos de aprendizado de m√°quina est√£o ausentes ou simplificados!  Essa simplifica√ß√£o √© permitida com o √∫nico objetivo - fornecer ao leitor a compreens√£o e intui√ß√£o mais <strong>b√°sicas</strong> sobre como uma m√°quina pode "aprender" em princ√≠pio, de modo que, como resultado, "MAGIC of machine learning" pare√ßa cada vez mais para o leitor como "MATEM√ÅTICA DO aprendizado de m√°quina". </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/98d/6c4/69e/98d6c469e1facbf97154fe29f698cd12.png" alt="Nanoneuron"></p><a name="habracut"></a><br><h2 id="chto-vyuchit-nash-nano-neyron">  O que nosso nano-neur√¥nio ‚Äúaprender√°‚Äù </h2><br><p>  Voc√™ pode ter ouvido falar de neur√¥nios no contexto de <a href="https://ru.wikipedia.org/wiki/%25D0%2598%25D1%2581%25D0%25BA%25D1%2583%25D1%2581%25D1%2581%25D1%2582%25D0%25B2%25D0%25B5%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C" rel="nofollow">redes neurais</a> .  Um nano-neur√¥nio √© uma vers√£o simplificada desse mesmo neur√¥nio.  Neste exemplo, escreveremos sua implementa√ß√£o do zero.  Por simplicidade, n√£o construiremos uma rede de nano-neur√¥nios.  Vamos nos concentrar na cria√ß√£o de um √∫nico nano-neur√¥nio e tentar ensin√°-lo a converter a temperatura de graus Celsius em graus Fahrenheit.  Em outras palavras, vamos ensin√°-lo a <strong>prever a</strong> temperatura em graus Fahrenheit com base na temperatura em graus Celsius. </p><br><p>  A prop√≥sito, a f√≥rmula para converter graus Celsius em graus Fahrenheit √© a seguinte: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/9fa/2e8/8b5/9fa2e88b5a7324c8b9fc359b274ba091.png" alt="Celsius em Fahrenheit"></p><br><p>  Mas, no momento, nosso nano-neur√¥nio n√£o sabe nada sobre essa f√≥rmula ... </p><br><h3 id="model-nano-neyrona">  Modelo nano-neur√¥nio </h3><br><p> Vamos come√ßar criando uma fun√ß√£o que descreve o modelo do nosso nano neur√¥nio.  Este modelo √© uma rela√ß√£o linear simples entre <code>x</code> e <code>y</code> , com a seguinte apar√™ncia: <code>y = w * x + b</code> .  Simplificando, nosso nano-neur√¥nio √© uma crian√ßa que pode desenhar uma linha reta no sistema de coordenadas <code>XY</code> . </p><br><p>  As vari√°veis <code>w</code> e <code>b</code> s√£o <strong>par√¢metros do</strong> modelo.  Um nano-neur√¥nio conhece apenas esses dois par√¢metros de uma fun√ß√£o linear.  Esses par√¢metros s√£o precisamente o que nosso nano-neur√¥nio aprender√° durante o processo de treinamento. </p><br><p>  A √∫nica coisa que um nano-neur√¥nio pode fazer nesta fase √© simular rela√ß√µes lineares.  Ele faz isso no m√©todo <code>predict()</code> , que pega uma vari√°vel <code>x</code> na entrada e prediz a vari√°vel <code>y</code> na sa√≠da.  Nenhuma m√°gica. </p><br><pre> <code class="javascript hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">NanoNeuron</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">w, b</span></span></span><span class="hljs-function">) </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.w = w; <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.b = b; <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.predict = <span class="hljs-function"><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">x</span></span></span><span class="hljs-function">) =&gt;</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> x * <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.w + <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.b; } }</code> </pre> <br><p>  _ (... espere ... <a href="https://en.wikipedia.org/wiki/Linear_regression" rel="nofollow">regress√£o linear</a> √© voc√™, ou o qu√™?) _ </p><br><h3 id="konvertaciya-gradusov-celsiya-v-gradusy-farengeyta">  Converter graus Celsius em graus Fahrenheit </h3><br><p>  A temperatura em graus Celsius pode ser convertida em graus Fahrenheit de acordo com a f√≥rmula: <code>f = 1.8 * c + 32</code> , onde <code>c</code> √© a temperatura em graus Celsius <code>f</code> √© a temperatura em graus Fahrenheit. </p><br><pre> <code class="javascript hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">celsiusToFahrenheit</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">c</span></span></span><span class="hljs-function">) </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> w = <span class="hljs-number"><span class="hljs-number">1.8</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> b = <span class="hljs-number"><span class="hljs-number">32</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> f = c * w + b; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> f; };</code> </pre> <br><p>  Como resultado, queremos que nosso nano-neur√¥nio seja capaz de simular essa fun√ß√£o espec√≠fica.  Ele ter√° que adivinhar (aprender) que o par√¢metro <code>w = 1.8</code> <code>b = 32</code> sem conhec√™-lo com anteced√™ncia. </p><br><p>  √â assim que a fun√ß√£o de convers√£o aparece no gr√°fico.  √â isso que nosso "beb√™" nano-neural deve aprender a "desenhar": </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/68b/0d6/8bc/68b0d68bcc7be00ec9526867b2fcecf3.png" alt="Convers√£o de Celsius para Fahrenheit"></p><br><h3 id="generirovanie-dannyh">  Gera√ß√£o de dados </h3><br><p>  Na programa√ß√£o cl√°ssica, conhecemos os dados de entrada ( <code>x</code> ) e o algoritmo para converter esses dados (par√¢metros <code>w</code> ), mas os dados de sa√≠da ( <code>y</code> ) s√£o desconhecidos.  A sa√≠da √© calculada com base na entrada usando um algoritmo conhecido.  No aprendizado de m√°quina, pelo contr√°rio, apenas os dados de entrada e sa√≠da ( <code>x</code> e <code>y</code> ) s√£o conhecidos, mas o algoritmo para alternar de <code>x</code> para <code>y</code> desconhecido (par√¢metros <code>w</code> e <code>b</code> ). </p><br><p>  √â a gera√ß√£o de entrada e sa√≠da que vamos fazer agora.  Precisamos gerar dados para <strong>treinar</strong> nosso modelo e dados para <strong>testar o</strong> modelo.  A fun√ß√£o auxiliar <code>celsiusToFahrenheit()</code> nos ajudar√° com isso.  Cada um dos conjuntos de dados de treinamento e teste √© um conjunto de pares <code>x</code> e <code>y</code> .  Por exemplo, se <code>x = 2</code> , <code>y = 35,6</code> e assim por diante. </p><br><blockquote>  No mundo real, a maioria dos dados provavelmente ser√° <em>coletada</em> , n√£o <em>gerada</em> .  Por exemplo, esses dados coletados podem ser um conjunto de pares de "fotos de rosto" -&gt; "nome da pessoa". </blockquote><p>  Usaremos o conjunto de dados TRAINING para treinar nosso nano-neur√¥nio.  Antes que ele cres√ßa e seja capaz de tomar decis√µes por conta pr√≥pria, devemos ensinar a ele o que √© "verdadeiro" e o que √© "falso" usando dados "corretos" de um conjunto de treinamento. </p><br><blockquote>  A prop√≥sito, aqui o princ√≠pio da vida ‚Äúlixo na entrada - lixo na sa√≠da‚Äù √© claramente tra√ßado.  Se um nano-neur√¥nio joga uma "mentira" no kit de treinamento que 5 ¬∞ C √© convertido em 1000 ¬∞ F, depois de muitas itera√ß√µes de treinamento, ele acreditar√° nisso e converter√° corretamente todos os valores de temperatura, <strong>exceto</strong> 5 ¬∞ C.  Precisamos ter muito cuidado com os dados de treinamento que carregamos todos os dias em nossa rede neural cerebral. </blockquote><p>  Distra√≠do.  Vamos continuar. </p><br><p>  Usaremos o conjunto de dados TEST para avaliar qu√£o bem nosso nano neur√¥nio foi treinado e podemos fazer previs√µes corretas sobre novos dados que ele n√£o viu durante o treinamento. </p><br><pre> <code class="javascript hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">generateDataSets</span></span></span><span class="hljs-function">(</span><span class="hljs-params"></span><span class="hljs-function"><span class="hljs-params"></span>) </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// xTrain -&gt; [0, 1, 2, ...], // yTrain -&gt; [32, 33.8, 35.6, ...] const xTrain = []; const yTrain = []; for (let x = 0; x &lt; 100; x += 1) { const y = celsiusToFahrenheit(x); xTrain.push(x); yTrain.push(y); } // xTest -&gt; [0.5, 1.5, 2.5, ...] // yTest -&gt; [32.9, 34.7, 36.5, ...] const xTest = []; const yTest = []; //   0.5    1,       //   ,       . for (let x = 0.5; x &lt; 100; x += 1) { const y = celsiusToFahrenheit(x); xTest.push(x); yTest.push(y); } return [xTrain, yTrain, xTest, yTest]; }</span></span></code> </pre> <br><h3 id="ocenka-pogreshnosti-predskazaniy">  Estimativa de erro de previs√£o </h3><br><p>  Precisamos de uma certa m√©trica (medida, n√∫mero, classifica√ß√£o) que mostre qu√£o pr√≥xima √© a previs√£o de um nano neur√¥nio.  Em outras palavras, esse n√∫mero / m√©trica / fun√ß√£o deve mostrar qu√£o certo ou errado o nano neur√¥nio est√°.  √â como na escola, um aluno pode obter uma nota de <code>5</code> ou <code>2</code> por seu controle. </p><br><p>  No caso de um nano-neur√¥nio, seu erro (erro) entre o valor verdadeiro de <code>y</code> e o valor previsto de <code>prediction</code> ser√° produzido pela f√≥rmula: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/8d8/e50/ac1/8d8e50ac12d03614e65975f7b5d36931.png" alt="Custo de previs√£o"></p><br><p>  Como pode ser visto na f√≥rmula, consideraremos o erro como uma simples diferen√ßa entre os dois valores.  Quanto mais pr√≥ximos os valores estiverem, menor a diferen√ßa.  Usamos o quadrado aqui para se livrar do sinal, para que no final <code>(1 - 2) ^ 2</code> equivalente a <code>(2 - 1) ^ 2</code> .  A divis√£o por <code>2</code> ocorre apenas para simplificar o significado da derivada dessa fun√ß√£o na f√≥rmula para propaga√ß√£o reversa de um sinal (mais sobre isso abaixo). </p><br><p>  A fun√ß√£o de erro, neste caso, ter√° a seguinte apar√™ncia: </p><br><pre> <code class="javascript hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">predictionCost</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">y, prediction</span></span></span><span class="hljs-function">) </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (y - prediction) ** <span class="hljs-number"><span class="hljs-number">2</span></span> / <span class="hljs-number"><span class="hljs-number">2</span></span>; <span class="hljs-comment"><span class="hljs-comment">// ie -&gt; 235.6 }</span></span></code> </pre> <br><h3 id="pryamoe-rasprostranenie-signala">  Propaga√ß√£o direta de sinal </h3><br><p>  A propaga√ß√£o direta do sinal atrav√©s do nosso modelo significa fazer previs√µes para todos os pares do conjunto de dados de treinamento <code>yTrain</code> e <code>yTrain</code> e calcular o erro m√©dio (erro) dessas previs√µes. </p><br><p>  N√≥s apenas deixamos nosso nano neur√¥nio "falar", permitindo que ele fizesse previs√µes (convertesse a temperatura).  Ao mesmo tempo, um nano-neur√¥nio nesta fase pode estar muito errado.  O valor m√©dio do erro de previs√£o nos mostrar√° at√© que ponto nosso modelo est√° / est√° pr√≥ximo da verdade no momento.  O valor do erro √© muito importante aqui, pois, alterando os par√¢metros <code>w</code> <code>b</code> propaga√ß√£o direta do sinal, podemos avaliar se nosso nano neur√¥nio se tornou "mais inteligente" com novos par√¢metros ou n√£o. </p><br><p>  O erro m√©dio de previs√£o de um nano neur√¥nio ser√° realizado usando a seguinte f√≥rmula: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/575/db3/e0a/575db3e0a0c872b29582147e41231344.png" alt="Custo m√©dio"></p><br><p>  Onde <code>m</code> √© o n√∫mero de c√≥pias de treinamento (no nosso caso, temos <code>100</code> pares de dados). </p><br><p>  Veja como podemos implementar isso no c√≥digo: </p><br><pre> <code class="javascript hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forwardPropagation</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">model, xTrain, yTrain</span></span></span><span class="hljs-function">) </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> m = xTrain.length; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> predictions = []; <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> cost = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">let</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; m; i += <span class="hljs-number"><span class="hljs-number">1</span></span>) { <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> prediction = nanoNeuron.predict(xTrain[i]); cost += predictionCost(yTrain[i], prediction); predictions.push(prediction); } <span class="hljs-comment"><span class="hljs-comment">//     . cost /= m; return [predictions, cost]; }</span></span></code> </pre> <br><h3 id="obratnoe-rasprostranenie-signala">  Propaga√ß√£o Reversa de Sinais </h3><br><p>  Agora que sabemos como nosso nano-neur√¥nio est√° certo ou errado em suas previs√µes (com base no valor m√©dio do erro), como podemos tornar as previs√µes mais precisas? </p><br><p>  A propaga√ß√£o reversa do sinal nos ajudar√° com isso.  A propaga√ß√£o do sinal de retorno √© o processo de avaliar o erro de um nano-neur√¥nio e, em seguida, ajustar seus par√¢metros <code>w</code> para que as pr√≥ximas previs√µes do nano-neur√¥nio para todo o conjunto de dados de treinamento se tornem um pouco mais precisas. </p><br><p>  √â aqui que o aprendizado de m√°quina se torna como m√°gica.  O conceito-chave aqui √© uma <strong>derivada da fun√ß√£o</strong> , que mostra qual etapa de tamanho e qual caminho precisamos seguir para abordar o m√≠nimo da fun√ß√£o (no nosso caso, o m√≠nimo da fun√ß√£o de erro). </p><br><p>  O objetivo final do treinamento de um nano-neur√¥nio √© encontrar o m√≠nimo da fun√ß√£o de erro (veja a fun√ß√£o acima).  Se pudermos encontrar esses valores de <code>w</code> e <code>b</code> nos quais o valor m√©dio da fun√ß√£o de erro √© pequeno, isso significa que nosso nano neur√¥nio lida bem com as previs√µes de temperatura em graus Fahrenheit. </p><br><p>  Derivativos s√£o um t√≥pico amplo e separado que n√£o abordaremos neste artigo.  <a href="https://www.mathsisfun.com/calculus/derivatives-introduction.html" rel="nofollow">MathIsFun</a> √© um √≥timo recurso que pode fornecer uma compreens√£o b√°sica de derivadas. </p><br><p>  Uma coisa que precisamos aprender com a ess√™ncia da derivada e que nos ajudar√° a entender como a retropropaga√ß√£o do sinal funciona √© que a derivada de uma fun√ß√£o em um ponto espec√≠fico <code>x</code> e <code>y</code> , por defini√ß√£o, √© uma linha tangente √† curva dessa fun√ß√£o em <code>x</code> e <code>y</code> <em>indica a dire√ß√£o para o m√≠nimo da fun√ß√£o</em> . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/66d/bfd/49a/66dbfd49aaf1ced48d7f6b5917fddb12.svg" alt="Inclina√ß√£o derivada"></p><br><p>  <em>Imagem tirada do <a href="https://www.mathsisfun.com/calculus/derivatives-introduction.html" rel="nofollow">MathIsFun</a></em> </p><br><p>  Por exemplo, no gr√°fico acima, voc√™ v√™ que, no ponto <code>(x=2, y=4)</code> inclina√ß√£o da tangente nos mostra que precisamos nos mover para a <code></code> e para <code></code> para obter o m√≠nimo da fun√ß√£o.  Observe tamb√©m que quanto maior a inclina√ß√£o da tangente, mais r√°pido devemos mover para o ponto m√≠nimo. </p><br><p>  As derivadas de nossa fun√ß√£o de erro m√©dio <code>averageCost</code> com <code>averageCost</code> aos par√¢metros <code>w</code> e <code>b</code> ter√£o a seguinte apar√™ncia: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/4cc/bda/ba1/4ccbdaba120c399c1528e2bc38cf0efd.png" alt="dW"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/e02/0cb/125/e020cb125449849009a9f565a32ff46f.png" alt="dB"></p><br><p>  Onde <code>m</code> √© o n√∫mero de c√≥pias de treinamento (no nosso caso, temos <code>100</code> pares de dados). </p><br><p>  <em>Voc√™ pode ler mais detalhadamente sobre como obter a derivada de fun√ß√µes complexas <a href="https://www.mathsisfun.com/calculus/derivatives-rules.html" rel="nofollow">aqui</a> .</em> </p><br><pre> <code class="javascript hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">backwardPropagation</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">predictions, xTrain, yTrain</span></span></span><span class="hljs-function">) </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> m = xTrain.length; <span class="hljs-comment"><span class="hljs-comment">//           'w'  'b'. //      0. let dW = 0; let dB = 0; for (let i = 0; i &lt; m; i += 1) { dW += (yTrain[i] - predictions[i]) * xTrain[i]; dB += yTrain[i] - predictions[i]; } //    . dW /= m; dB /= m; return [dW, dB]; }</span></span></code> </pre> <br><h3 id="trenirovka-modeli">  Modelo de treinamento </h3><br><p>  Agora sabemos como estimar o erro / erro das previs√µes do nosso modelo de nano-neur√¥nios para todos os dados de treinamento (propaga√ß√£o direta do sinal).  Tamb√©m sabemos como ajustar os par√¢metros <code>w</code> e <code>b</code> modelo de nano-neur√¥nios (propaga√ß√£o reversa do sinal) para melhorar a precis√£o das previs√µes.  O problema √© que, se realizarmos a propaga√ß√£o para frente e para tr√°s do sinal apenas uma vez, isso n√£o ser√° suficiente para o nosso modelo identificar e aprender as depend√™ncias e leis nos dados de treinamento.  Voc√™ pode comparar isso com a visita escolar de um dia de um aluno.  Ele / ela deve frequentar a escola regularmente, dia ap√≥s dia, ano ap√≥s ano, para aprender todo o material. </p><br><p>  Portanto, devemos <em>repetir a</em> propaga√ß√£o para frente e para tr√°s do sinal muitas vezes.  √â <code>trainModel()</code> fun√ß√£o <code>trainModel()</code> .  Ela √© como uma "professora" para o modelo do nosso nano neur√¥nio: </p><br><ul><li>  ela passar√° algum tempo ( <code>epochs</code> ) com nosso nano neur√¥nio ainda bobo, tentando trein√°-lo, </li><li>  ela usar√° livros especiais (conjuntos de dados <code>yTrain</code> e <code>yTrain</code> ) para treinamento, </li><li>  incentiva nosso "aluno" a estudar com mais dilig√™ncia (mais r√°pido) usando o par√¢metro <code>alpha</code> , que controla essencialmente a velocidade do aprendizado. </li></ul><br><p>  Algumas palavras sobre o par√¢metro <code>alpha</code> .  Este √© apenas um coeficiente (multiplicador) para os valores das vari√°veis <code>dW</code> e <code>dB</code> , que calculamos durante a propaga√ß√£o posterior do sinal.  Portanto, a derivada nos mostrou a dire√ß√£o para o m√≠nimo da fun√ß√£o de erro (os sinais dos valores de <code>dW</code> e <code>dB</code> nos dizem isso).  A derivada tamb√©m nos mostrou a rapidez com que precisamos avan√ßar para o m√≠nimo da fun√ß√£o (os valores absolutos de <code>dW</code> e <code>dB</code> nos dizem isso).  Agora, precisamos multiplicar o tamanho da etapa por <code>alpha</code> , a fim de ajustar a velocidade de nossa abordagem a um m√≠nimo (o tamanho total da etapa).  √Äs vezes, se usarmos valores grandes para <code>alpha</code> , podemos executar etapas t√£o grandes que podemos simplesmente <em>ultrapassar o</em> m√≠nimo da fun√ß√£o, ignorando-a. </p><br><p>  Por analogia com o ‚Äúprofessor‚Äù, quanto mais forte ela for√ßaria nosso ‚Äúnano-aluno‚Äù a aprender, mais r√°pido ele aprenderia, MAS, se voc√™ for√ßar e pression√°-lo muito, ent√£o nosso ‚Äúnano-aluno‚Äù poder√° sofrer um colapso nervoso e apatia completa e ele n√£o aprender√° nada. </p><br><p>  Atualizaremos os par√¢metros do nosso modelo <code>w</code> e <code>b</code> seguinte maneira: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/c7b/db8/84f/c7bdb884f2a940d62332246cdbcb44bc.png" alt="w"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/b57/622/0ab/b576220ab6515d44255ef56699077bab.png" alt="b"></p><br><p>  E √© assim que o treinamento em si se parece: </p><br><pre> <code class="javascript hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">trainModel</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">{model, epochs, alpha, xTrain, yTrain}</span></span></span><span class="hljs-function">) </span></span>{ <span class="hljs-comment"><span class="hljs-comment">//     -.  . const costHistory = []; //    ()  for (let epoch = 0; epoch &lt; epochs; epoch += 1) { //   . const [predictions, cost] = forwardPropagation(model, xTrain, yTrain); costHistory.push(cost); //   . const [dW, dB] = backwardPropagation(predictions, xTrain, yTrain); //    -,    . nanoNeuron.w += alpha * dW; nanoNeuron.b += alpha * dB; } return costHistory; }</span></span></code> </pre> <br><h3 id="soberem-vse-funkcii-vmeste">  Juntando todos os recursos </h3><br><p>  Hora de usar todas as fun√ß√µes criadas anteriormente juntas. </p><br><p>  Crie uma inst√¢ncia do modelo nano-neur√¥nio.  No momento, o nano-neur√¥nio n√£o sabe nada sobre quais devem ser os par√¢metros <code>w</code> e <code>b</code> .  Ent√£o, vamos definir <code>w</code> e <code>b</code> aleatoriamente. </p><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> w = <span class="hljs-built_in"><span class="hljs-built_in">Math</span></span>.random(); <span class="hljs-comment"><span class="hljs-comment">// ie -&gt; 0.9492 const b = Math.random(); // ie -&gt; 0.4570 const nanoNeuron = new NanoNeuron(w, b);</span></span></code> </pre> <br><p>  Geramos conjuntos de dados de treinamento e teste. </p><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> [xTrain, yTrain, xTest, yTest] = generateDataSets();</code> </pre> <br><p>  Agora vamos tentar treinar nosso modelo usando pequenos passos ( <code>0.0005</code> ) para <code>70000</code> √©pocas.  Voc√™ pode experimentar com esses par√¢metros, eles s√£o determinados empiricamente. </p><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> epochs = <span class="hljs-number"><span class="hljs-number">70000</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> alpha = <span class="hljs-number"><span class="hljs-number">0.0005</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> trainingCostHistory = trainModel({<span class="hljs-attr"><span class="hljs-attr">model</span></span>: nanoNeuron, epochs, alpha, xTrain, yTrain});</code> </pre> <br><p>  Vamos verificar como o valor do erro do nosso modelo mudou durante o treinamento.  Esperamos que o valor do erro ap√≥s o treinamento seja significativamente menor do que antes do treinamento.  Isso significa que o nosso nano-neur√¥nio √© mais s√°bio.  A op√ß√£o oposta tamb√©m √© poss√≠vel quando, ap√≥s o treinamento, o erro de previs√£o apenas aumenta (por exemplo, grandes valores da etapa de aprendizado <code>alpha</code> ). </p><br><pre> <code class="javascript hljs"><span class="hljs-built_in"><span class="hljs-built_in">console</span></span>.log(<span class="hljs-string"><span class="hljs-string">'  :'</span></span>, trainingCostHistory[<span class="hljs-number"><span class="hljs-number">0</span></span>]); <span class="hljs-comment"><span class="hljs-comment">// ie -&gt; 4694.3335043 console.log('  :', trainingCostHistory[epochs - 1]); // ie -&gt; 0.0000024</span></span></code> </pre> <br><p>  E aqui est√° como o valor do erro do modelo mudou durante o treinamento.  No eixo <code>x</code> s√£o √©pocas (em milhares).  Esperamos que o gr√°fico esteja diminuindo. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/191/860/d6f/191860d6f0cd8cf7d24127f04f779462.png" alt="Processo de treinamento"></p><br><p>  Vejamos quais par√¢metros nosso nano-neur√¥nio ‚Äúaprendeu‚Äù.  Esperamos que os par√¢metros <code>w</code> e <code>b</code> sejam semelhantes aos par√¢metros com o mesmo nome da fun√ß√£o <code>celsiusToFahrenheit()</code> ( <code>w = 1.8</code> <code>b = 32</code> ), porque foi o nano-neur√¥nio dela que tentei simular. </p><br><pre> <code class="javascript hljs"><span class="hljs-built_in"><span class="hljs-built_in">console</span></span>.log(<span class="hljs-string"><span class="hljs-string">' -:'</span></span>, {<span class="hljs-attr"><span class="hljs-attr">w</span></span>: nanoNeuron.w, <span class="hljs-attr"><span class="hljs-attr">b</span></span>: nanoNeuron.b}); <span class="hljs-comment"><span class="hljs-comment">// ie -&gt; {w: 1.8, b: 31.99}</span></span></code> </pre> <br><p>  Como voc√™ pode ver, o nano-neur√¥nio est√° muito pr√≥ximo da fun√ß√£o <code>celsiusToFahrenheit()</code> . </p><br><p>  Agora vamos ver qu√£o precisas s√£o as previs√µes de nosso nano-neur√¥nio para dados de teste que ele n√£o viu durante o treinamento.  O erro de previs√£o para os dados de teste deve estar pr√≥ximo do erro de previs√£o para os dados de treinamento.  Isso significa que o nano-neur√¥nio aprendeu as depend√™ncias corretas e pode abstrair corretamente sua experi√™ncia a partir de dados anteriormente desconhecidos (esse √© todo o valor do modelo). </p><br><pre> <code class="javascript hljs">[testPredictions, testCost] = forwardPropagation(nanoNeuron, xTest, yTest); <span class="hljs-built_in"><span class="hljs-built_in">console</span></span>.log(<span class="hljs-string"><span class="hljs-string">'   :'</span></span>, testCost); <span class="hljs-comment"><span class="hljs-comment">// ie -&gt; 0.0000023</span></span></code> </pre> <br><p>  Agora, como nosso "nano-beb√™" foi bem treinado na "escola" e agora sabe como converter com precis√£o graus Celsius em graus Fahrenheit, mesmo para dados que ele n√£o viu, podemos cham√°-lo de "inteligente".  Agora podemos at√© pedir conselhos a ele sobre convers√£o de temperatura, e esse foi o objetivo de todo o treinamento. </p><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> tempInCelsius = <span class="hljs-number"><span class="hljs-number">70</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> customPrediction = nanoNeuron.predict(tempInCelsius); <span class="hljs-built_in"><span class="hljs-built_in">console</span></span>.log(<span class="hljs-string"><span class="hljs-string">`- "",  </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${tempInCelsius}</span></span></span><span class="hljs-string">¬∞C   :`</span></span>, customPrediction); <span class="hljs-comment"><span class="hljs-comment">// -&gt; 158.0002 console.log('  :', celsiusToFahrenheit(tempInCelsius)); // -&gt; 158</span></span></code> </pre> <br><p>  Muito perto!  Como as pessoas, nosso nano-neur√¥nio √© bom, mas n√£o √© perfeito :) </p><br><p>  Codifica√ß√£o bem sucedida! </p><br><h2 id="kak-zapustit-i-protestirovat-nano-neyron">  Como executar e testar um nano-neur√¥nio </h2><br><p>  Voc√™ pode clonar o reposit√≥rio e executar o nano neur√¥nio localmente: </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/trekhleb/nano-neuron.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> nano-neuron</code> </pre> <br><pre> <code class="bash hljs">node ./NanoNeuron.js</code> </pre> <br><h2 id="upuschennye-koncepcii">  Conceitos perdidos </h2><br><p>  Os seguintes conceitos de aprendizado de m√°quina foram omitidos ou simplificados para facilitar a explica√ß√£o. </p><br><p>  <strong>Separa√ß√£o de conjuntos de dados de treinamento e teste</strong> </p><br><p>  Geralmente voc√™ tem um grande conjunto de dados.  Dependendo do n√∫mero de c√≥pias deste conjunto, sua divis√£o em conjuntos de treinamento e teste pode ser realizada na propor√ß√£o de 70/30.  Os dados no conjunto devem ser misturados aleatoriamente antes de serem divididos.  Se a quantidade de dados for grande (por exemplo, milh√µes), a divis√£o em conjuntos de teste e treinamento poder√° ser realizada em propor√ß√µes pr√≥ximas a 90/10 ou 95/5. </p><br><p>  <strong>Poder online</strong> </p><br><p>  Normalmente, voc√™ n√£o encontrar√° casos em que apenas um neur√¥nio seja usado.  A for√ßa est√° na <a href="https://en.wikipedia.org/wiki/Neural_network" rel="nofollow">rede de</a> tais neur√¥nios.  Uma rede neural pode aprender depend√™ncias muito mais complexas. </p><br><p>  Tamb√©m no exemplo acima, nosso nano-neur√¥nio pode parecer mais uma <a href="https://en.wikipedia.org/wiki/Linear_regression" rel="nofollow">regress√£o linear</a> simples do que uma rede neural. </p><br><p>  <strong>Normaliza√ß√£o de entrada</strong> </p><br><p>  Antes do treinamento, √© habitual <a href="https://www.jeremyjordan.me/batch-normalization/" rel="nofollow">normalizar os dados de entrada</a> . </p><br><p>  <strong>Implementa√ß√£o vetorial</strong> </p><br><p>  Para redes neurais, os c√°lculos de vetor (matriz) s√£o muito mais r√°pidos que os c√°lculos <code>for</code> loops.  Geralmente, a propaga√ß√£o direta e reversa do sinal √© realizada usando opera√ß√µes de matriz, por exemplo, a biblioteca Python <a href="https://numpy.org/" rel="nofollow">Numpy</a> . </p><br><p>  <strong>Fun√ß√£o de erro m√≠nimo</strong> </p><br><p>  A fun√ß√£o de erro que usamos para o nano neur√¥nio √© muito simplificada.  Ele deve conter <a href="https://stackoverflow.com/questions/32986123/why-the-cost-function-of-logistic-regression-has-a-logarithmic-expression/32998675" rel="nofollow">componentes logar√≠tmicos</a> .  Uma mudan√ßa na f√≥rmula para a fun√ß√£o de erro tamb√©m implicar√° uma mudan√ßa nas f√≥rmulas para a propaga√ß√£o para frente e para tr√°s do sinal. </p><br><p>  <strong>Fun√ß√£o de ativa√ß√£o</strong> </p><br><p>  Normalmente, o valor de sa√≠da do neur√¥nio passa pela fun√ß√£o de ativa√ß√£o.  Para a ativa√ß√£o, fun√ß√µes como <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="nofollow">Sigmoid</a> , <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="nofollow">ReLU</a> e outras podem ser usadas. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt479220/">https://habr.com/ru/post/pt479220/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt479202/index.html">C ++ e m√©todos num√©ricos: integra√ß√£o aproximada de Newton-Cotes</a></li>
<li><a href="../pt479210/index.html">O que acontecer√° com as compras em lojas on-line estrangeiras a partir de 1¬∫ de janeiro de 2020</a></li>
<li><a href="../pt479214/index.html">Uma sele√ß√£o dos pr√≥ximos eventos gratuitos para desenvolvedores em Moscou # 2</a></li>
<li><a href="../pt479216/index.html">Segundo vento Pandora DXL 3000 ou como eu estraguei minha pr√≥pria telemetria</a></li>
<li><a href="../pt479218/index.html">Como fazer um bot que transforma uma foto em uma hist√≥ria em quadrinhos: instru√ß√µes passo a passo para manequins</a></li>
<li><a href="../pt479222/index.html">O resumo de materiais interessantes para o desenvolvedor m√≥vel # 325 (de 2 a 8 de dezembro)</a></li>
<li><a href="../pt479226/index.html">An√°lise Habr: o que os usu√°rios pedem como presente da Habr</a></li>
<li><a href="../pt479230/index.html">Documente sua API expressa com anota√ß√µes de arrog√¢ncia</a></li>
<li><a href="../pt479232/index.html">Desenvolvimento de aplicativo MQ JMS no Spring Boot</a></li>
<li><a href="../pt479234/index.html">Not√≠cias do mundo do OpenStreetMap n¬∫ 488 (19/11/2019 - 25/11/2019)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>