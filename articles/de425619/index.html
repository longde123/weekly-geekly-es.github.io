<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßñüèø üë®‚ÄçüöÄ üë®‚Äçüåæ Das mehrarmige Banditenproblem - Vergleichen Sie die Epsilon-Greedy-Strategie und die Thompson-Probenahme üé£ üíáüèª ü§∏</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Habr! Ich pr√§sentiere Ihnen die Titelseite der mehrarmigen Banditen l√∂sen: Ein Vergleich von epsilon-gierigen und Thompson-Sampling- Artikeln. 
...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Das mehrarmige Banditenproblem - Vergleichen Sie die Epsilon-Greedy-Strategie und die Thompson-Probenahme</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/425619/">  <i>Hallo Habr!</i>  <i>Ich pr√§sentiere Ihnen die Titelseite der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">mehrarmigen Banditen l√∂sen: Ein Vergleich von epsilon-gierigen und Thompson-Sampling-</a> Artikeln.</i> <br><br><h1>  Das mehrarmige Banditenproblem </h1><br><p>  Das mehrarmige Banditenproblem ist eine der grundlegendsten Aufgaben in der Wissenschaft der L√∂sungen.  Dies ist n√§mlich das Problem der optimalen Allokation von Ressourcen unter Unsicherheitsbedingungen.  Der Name "mehrarmiger Bandit" selbst stammt von alten Spielautomaten, die von Griffen gesteuert werden.  Diese Sturmgewehre wurden als ‚ÄûBanditen‚Äú bezeichnet, da sich die Menschen nach einem Gespr√§ch normalerweise ausgeraubt f√ºhlten.  Stellen Sie sich nun vor, dass es mehrere solcher Maschinen gibt und die Chance, gegen verschiedene Autos zu gewinnen, unterschiedlich ist.  Seit wir mit diesen Maschinen spielen, m√∂chten wir feststellen, welche Chance h√∂her ist, und diese Maschine h√§ufiger als andere verwenden. </p><br><p>  Das Problem ist folgendes: Wie k√∂nnen wir am effizientesten verstehen, welche Maschine am besten geeignet ist, und gleichzeitig viele Funktionen in Echtzeit ausprobieren?  Dies ist kein theoretisches Problem, sondern ein Problem, mit dem ein Unternehmen st√§ndig konfrontiert ist.  Ein Unternehmen verf√ºgt beispielsweise √ºber mehrere Optionen f√ºr Nachrichten, die Benutzern angezeigt werden m√ºssen (z. B. enthalten Nachrichten Anzeigen, Websites, Bilder), damit die ausgew√§hlten Nachrichten eine bestimmte Gesch√§ftsaufgabe maximieren (Conversion, Klickbarkeit usw.). </p><br><a name="habracut"></a><p>  Ein typischer Weg, um dieses Problem zu l√∂sen, besteht darin, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">A / B-Tests</a> mehrmals <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">durchzuf√ºhren</a> .  Das hei√üt, mehrere Wochen lang, um jede der Optionen gleich oft zu zeigen, und dann anhand statistischer Tests zu entscheiden, welche Option besser ist.  Diese Methode eignet sich, wenn es nur wenige Optionen gibt, z. B. 2 oder 4. Wenn es jedoch viele Optionen gibt, wird dieser Ansatz unwirksam - sowohl bei Zeitverlust als auch bei entgangenem Gewinn. </p><br><p>  Woher die verlorene Zeit kommt, sollte leicht zu verstehen sein.  Mehr Optionen - mehr A / B-Tests sind erforderlich - mehr Zeit ist erforderlich, um eine Entscheidung zu treffen.  Das Auftreten von entgangenen Gewinnen ist nicht so offensichtlich.  Opportunit√§tsverlust (Opportunit√§tskosten) - die Kosten, die mit der Tatsache verbunden sind, dass wir anstelle einer Aktion eine andere durchgef√ºhrt haben, dh in einfachen Worten, dies ist das, was wir durch die Investition in A anstelle von B verloren haben. Die Investition in B ist der verlorene Gewinn aus der Investition in A. Das gleiche gilt f√ºr die Optionspr√ºfung.  A / B-Tests sollten erst nach Abschluss unterbrochen werden.  Dies bedeutet, dass der Experimentator nicht wei√ü, welche Option besser ist, bis der Test beendet ist.  Es wird jedoch immer noch angenommen, dass eine Option besser ist als die andere.  Dies bedeutet, dass wir durch die Verl√§ngerung der A / B-Tests nicht die besten Optionen f√ºr eine ausreichend gro√üe Anzahl von Besuchern anzeigen (obwohl wir nicht wissen, welche Optionen nicht die besten sind), wodurch wir unseren Gewinn verlieren.  Dies ist der verlorene Vorteil von A / B-Tests.  Wenn es nur einen A / B-Test gibt, ist der entgangene Gewinn vielleicht √ºberhaupt nicht gro√ü.  Eine gro√üe Anzahl von A / B-Tests bedeutet, dass wir unseren Kunden lange Zeit viele der nicht besten Optionen zeigen m√ºssen.  Es w√§re besser, wenn Sie die schlechten Optionen schnell in Echtzeit wegwerfen k√∂nnten und nur dann, wenn nur noch wenige Optionen verf√ºgbar sind, A / B-Tests f√ºr sie verwenden k√∂nnten. </p><br><p>  Mit Samplern oder Agenten k√∂nnen Sie die Verteilung von Optionen schnell testen und optimieren.  In diesem Artikel werde ich Ihnen die <i>Thompson-Probenahme</i> und ihre Eigenschaften vorstellen.  Ich werde auch die Thompson-Abtastung mit dem epsilon-gierigen Algorithmus vergleichen, einer weiteren beliebten Option f√ºr das Problem der mehrarmigen Banditen.  Alles wird von Grund auf in Python implementiert - der gesamte Code ist hier zu finden. </p><br><h2>  Kurzes W√∂rterbuch der Konzepte </h2><br><p></p><ul><li>  Agent, Sampler, Bandit ( <i>Agent, Sampler, Bandit</i> ) - ein Algorithmus, der Entscheidungen dar√ºber trifft, welche Option angezeigt werden soll. </li><li>  Variante - eine andere Variante der Nachricht, die der Besucher sieht. </li><li>  Aktion - Die Aktion, die der Algorithmus ausgew√§hlt hat (welche Option angezeigt werden soll). </li><li>  Verwenden ( <i>Exploit</i> ) - Treffen Sie eine Auswahl, um die Gesamtbelohnung basierend auf den verf√ºgbaren Daten zu maximieren. </li><li>  Entdecken, <i>erkunden</i> - treffen Sie Entscheidungen, um die Amortisation f√ºr jede Option besser zu verstehen. </li><li>  Auszeichnung, Punkte ( <i>Punktzahl, Belohnung</i> ) - eine Gesch√§ftsaufgabe, zum Beispiel Conversion oder Klickbarkeit.  Der Einfachheit halber glauben wir, dass es binomial verteilt ist und entweder 1 oder 0 entspricht - geklickt oder nicht. </li><li>  Umgebung - der Kontext, in dem der Agent arbeitet - Optionen und deren ‚ÄûR√ºckzahlung‚Äú f√ºr den Benutzer verborgen. </li><li>  Amortisation, Erfolgswahrscheinlichkeit ( <i>Auszahlungsrate</i> ) - eine versteckte Variable, die der Wahrscheinlichkeit entspricht, eine Punktzahl von 1 zu erhalten, f√ºr jede Option ist sie unterschiedlich.  Aber der Benutzer sieht sie nicht. </li><li>  Versuchen Sie es ( <i>Testversion</i> ) - der Benutzer besucht die Seite. </li><li>  Bedauern ist der Unterschied zwischen dem besten Ergebnis aller verf√ºgbaren Optionen und dem Ergebnis der im aktuellen Versuch verf√ºgbaren Option.  Je weniger Bedauern die bereits ergriffenen Ma√ünahmen sind, desto besser. </li><li>  Nachricht ( <i>Nachricht</i> ) - ein Banner, eine Seitenoption und mehr, von denen wir verschiedene Versionen ausprobieren m√∂chten. </li><li>  Stichprobe - Die Erzeugung einer Stichprobe aus einer bestimmten Verteilung. </li></ul><br><h2>  Entdecken und ausnutzen </h2><br><p>  Agenten sind Algorithmen, die nach einem Ansatz f√ºr die Entscheidungsfindung in Echtzeit suchen, um ein Gleichgewicht zwischen der Erkundung des Optionsraums und der Verwendung der besten Option zu erreichen.  Dieses Gleichgewicht ist sehr wichtig.  Der Optionsbereich muss untersucht werden, um eine Vorstellung davon zu erhalten, welche Option die beste ist.  Wenn wir diese optimale Option zuerst entdeckt haben und sie dann die ganze Zeit verwenden, maximieren wir die Gesamtbelohnung, die uns aus der Umgebung zur Verf√ºgung steht.  Andererseits m√∂chten wir auch andere m√∂gliche Optionen untersuchen - was ist, wenn sie sich in Zukunft als besser herausstellen werden, aber wir wissen es noch nicht?  Mit anderen Worten, wir m√∂chten uns gegen m√∂gliche Verluste versichern und versuchen, ein wenig mit suboptimalen Optionen zu experimentieren, um ihre Amortisation f√ºr sich selbst zu kl√§ren.  Wenn ihre Amortisation tats√§chlich h√∂her ist, k√∂nnen sie h√§ufiger angezeigt werden.  Ein weiteres Plus bei der Untersuchung von Optionen besteht darin, dass wir nicht nur die durchschnittliche Amortisation besser verstehen k√∂nnen, sondern auch, wie grob die Amortisation verteilt ist, d. H. Wir k√∂nnen die Unsicherheit besser absch√§tzen. <br>  Das Hauptproblem ist daher die L√∂sung - was ist der beste Ausweg aus dem Dilemma zwischen Exploration und Exploitation (Kompromiss zwischen Exploration und Exploitation). </p><br><h2>  Epsilon-gieriger Algorithmus </h2><br><p>  Ein typischer Ausweg aus diesem Dilemma ist der epsilon-gierige Algorithmus.  "Gierig" bedeutet genau das, was Sie gedacht haben.  Nach einer anf√§nglichen Periode, in der wir versehentlich Versuche unternehmen - beispielsweise 1000 Mal - w√§hlt der Algorithmus eifrig die beste Option k in <i>e</i> Prozent der Versuche aus.  Wenn beispielsweise <i>e</i> = 0,05 ist, w√§hlt der Algorithmus in 95% der F√§lle die beste Option aus und in den verbleibenden 5% der F√§lle w√§hlt er zuf√§llige Versuche aus.  Tats√§chlich ist dies ein ziemlich effektiver Algorithmus, der jedoch m√∂glicherweise nicht ausreicht, um den Raum der Optionen zu erkunden, und daher nicht ausreicht, um zu bewerten, welche Option die beste ist, um an einer suboptimalen Option festzuhalten.  Lassen Sie uns im Code zeigen, wie dieser Algorithmus funktioniert. </p><br><p>  Aber zuerst einige Abh√§ngigkeiten.  Wir m√ºssen die Umwelt definieren.  Dies ist der Kontext, in dem die Algorithmen ausgef√ºhrt werden.  In diesem Fall ist der Kontext sehr einfach.  Er ruft den Agenten an, damit der Agent entscheidet, welche Aktion er ausw√§hlen soll. Anschlie√üend startet der Kontext diese Aktion und gibt die daf√ºr erhaltenen Punkte an den Agenten zur√ºck (der seinen Status irgendwie aktualisiert). </p><br><pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Environment</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, variants, payouts, n_trials, variance=False)</span></span></span><span class="hljs-function">:</span></span> self.variants = variants <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> variance: self.payouts = np.clip(payouts + np.random.normal(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0.04</span></span>, size=len(variants)), <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">.2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: self.payouts = payouts <span class="hljs-comment"><span class="hljs-comment">#self.payouts[5] = self.payouts[5] if i &lt; n_trials/2 else 0.1 self.n_trials = n_trials self.total_reward = 0 self.n_k = len(variants) self.shape = (self.n_k, n_trials) def run(self, agent): """Run the simulation with the agent. agent must be a class with choose_k and update methods.""" for i in range(self.n_trials): # agent makes a choice x_chosen = agent.choose_k() # Environment returns reward reward = np.random.binomial(1, p=self.payouts[x_chosen]) # agent learns of reward agent.reward = reward # agent updates parameters based on the data agent.update() self.total_reward += reward agent.collect_data() return self.total_reward</span></span></code> </pre> <br>  Punkte werden binomial mit der Wahrscheinlichkeit p in Abh√§ngigkeit von der Anzahl der Aktionen verteilt (so wie sie kontinuierlich verteilt werden k√∂nnten, h√§tte sich die Essenz nicht ge√§ndert).  Ich werde auch die BaseSampler-Klasse definieren - sie wird nur zum Speichern von Protokollen und verschiedenen Attributen ben√∂tigt. <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">BaseSampler</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, env, n_samples=None, n_learning=None, e=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.05</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> self.env = env self.shape = (env.n_k, n_samples) self.variants = env.variants self.n_trials = env.n_trials self.payouts = env.payouts self.ad_i = np.zeros(env.n_trials) self.r_i = np.zeros(env.n_trials) self.thetas = np.zeros(self.n_trials) self.regret_i = np.zeros(env.n_trials) self.thetaregret = np.zeros(self.n_trials) self.a = np.ones(env.n_k) self.b = np.ones(env.n_k) self.theta = np.zeros(env.n_k) self.data = <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> self.reward = <span class="hljs-number"><span class="hljs-number">0</span></span> self.total_reward = <span class="hljs-number"><span class="hljs-number">0</span></span> self.k = <span class="hljs-number"><span class="hljs-number">0</span></span> self.i = <span class="hljs-number"><span class="hljs-number">0</span></span> self.n_samples = n_samples self.n_learning = n_learning self.e = e self.ep = np.random.uniform(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, size=env.n_trials) self.exploit = (<span class="hljs-number"><span class="hljs-number">1</span></span> - e) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">collect_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.data = pd.DataFrame(dict(ad=self.ad_i, reward=self.r_i, regret=self.regret_i))</code> </pre> <br>  Nachfolgend definieren wir 10 Optionen und Payback f√ºr jede.  Die beste Option ist Option 9 mit einer Amortisation von 0,11%. <br><br><pre> <code class="python hljs">variants = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>] payouts = [<span class="hljs-number"><span class="hljs-number">0.023</span></span>, <span class="hljs-number"><span class="hljs-number">0.03</span></span>, <span class="hljs-number"><span class="hljs-number">0.029</span></span>, <span class="hljs-number"><span class="hljs-number">0.001</span></span>, <span class="hljs-number"><span class="hljs-number">0.05</span></span>, <span class="hljs-number"><span class="hljs-number">0.06</span></span>, <span class="hljs-number"><span class="hljs-number">0.0234</span></span>, <span class="hljs-number"><span class="hljs-number">0.035</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, <span class="hljs-number"><span class="hljs-number">0.11</span></span>]</code> </pre> <br>  Um etwas aufzubauen, definieren wir auch die RandomSampler-Klasse.  Diese Klasse wird als Basismodell ben√∂tigt.  Er w√§hlt einfach bei jedem Versuch rein zuf√§llig eine Option und aktualisiert seine Parameter nicht. <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">RandomSampler</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(BaseSampler)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, env)</span></span></span><span class="hljs-function">:</span></span> super().__init__(env) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">choose_k</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.k = np.random.choice(self.variants) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.k <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">update</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># nothing to update #self.thetaregret[self.i] = self.thetaregret[self.i] #self.regret_i[self.i] = np.max(self.thetaregret) - self.theta[self.k] #self.thetas[self.i] = self.theta[self.k] self.thetaregret[self.i] = np.max(self.theta) - self.theta[self.k] self.a[self.k] += self.reward self.b[self.k] += 1 self.theta = self.a/self.b self.ad_i[self.i] = self.k self.r_i[self.i] = self.reward self.i += 1</span></span></code> </pre> <br><p>  Andere Modelle haben die folgende Struktur.  Alle haben select_k und Update-Methoden.  select_k implementiert die Methode, mit der der Agent eine Option ausw√§hlt.  update aktualisiert die Parameter des Agenten - diese Methode charakterisiert, wie sich die F√§higkeit des Agenten, die Option auszuw√§hlen, √§ndert (mit RandomSampler √§ndert sich diese F√§higkeit in keiner Weise).  Wir f√ºhren den Agenten in der Umgebung nach dem folgenden Muster aus. </p><br><pre> <code class="python hljs">en0 = Environment(machines, payouts, n_trials=<span class="hljs-number"><span class="hljs-number">10000</span></span>) rs = RandomSampler(env=en0) en0.run(agent=rs)</code> </pre> <br><p>  Das Wesen des epsilon-gierigen Algorithmus ist wie folgt. <br><br></p><ol><li>  W√§hlen Sie zuf√§llig k f√ºr n Versuche. </li><li>  Bewerten Sie bei jedem Versuch f√ºr jede Option die Gewinne. </li><li>  Nach allen n Versuchen: </li><li>  Mit der Wahrscheinlichkeit 1 - <i>e</i> w√§hlen Sie k mit der h√∂chsten Verst√§rkung; </li><li>  Mit der Wahrscheinlichkeit <i>e</i> w√§hle K zuf√§llig. </li></ol><br>  Epsilon-gieriger Code: <br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">eGreedy</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(BaseSampler)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, env, n_learning, e)</span></span></span><span class="hljs-function">:</span></span> super().__init__(env, n_learning, e) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">choose_k</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># e% of the time take a random draw from machines # random k for n learning trials, then the machine with highest theta self.k = np.random.choice(self.variants) if self.i &lt; self.n_learning else np.argmax(self.theta) # with 1 - e probability take a random sample (explore) otherwise exploit self.k = np.random.choice(self.variants) if self.ep[self.i] &gt; self.exploit else self.k return self.k # every 100 trials update the successes # update the count of successes for the chosen machine def update(self): # update the probability of payout for each machine self.a[self.k] += self.reward self.b[self.k] += 1 self.theta = self.a/self.b #self.total_reward += self.reward #self.regret_i[self.i] = np.max(self.theta) - self.theta[self.k] #self.thetaregret[self.i] = self.thetaregret[self.i] self.thetas[self.i] = self.theta[self.k] self.thetaregret[self.i] = np.max(self.thetas) - self.theta[self.k] self.ad_i[self.i] = self.k self.r_i[self.i] = self.reward self.i += 1</span></span></code> </pre> <br><br><p>  Unten in der Grafik sehen Sie die Ergebnisse einer rein zuf√§lligen Stichprobe, dh es gibt hier kein Modell.  Die Grafik zeigt, welche Auswahl der Algorithmus bei jedem Versuch getroffen hat, wenn es 10.000 Versuche gab.  Der Algorithmus versucht nur, lernt aber nicht.  Insgesamt erzielte er 418 Punkte. <br> <a href=""><img src="https://habrastorage.org/webt/sn/ql/2r/snql2roqbdiruuskxsathithz8i.jpeg"></a> </p><br><p>  Mal sehen, wie sich der epsilon-gierige Algorithmus in derselben Umgebung verh√§lt.  F√ºhren Sie den Algorithmus f√ºr zehntausend Versuche mit <i>e</i> = 0,1 und n_learning = 500 aus (der Agent versucht einfach die ersten 500 Versuche, dann versucht er es mit der Wahrscheinlichkeit <i>e</i> = 0,1).  Bewerten wir den Algorithmus anhand der Gesamtzahl der Punkte, die er in der Umgebung erzielt. </p><br><pre> <code class="python hljs">en1 = Environment(machines, payouts, n_trials) eg = eGreedy(env=en1, n_learning=<span class="hljs-number"><span class="hljs-number">500</span></span>, e=<span class="hljs-number"><span class="hljs-number">0.1</span></span>) en1.run(agent=eg)</code> </pre> <br> <a href=""><img src="https://habrastorage.org/webt/-f/zf/14/-fzf14djbqtdl5-0vyrapuhcp5c.jpeg"></a> <br><p>  Der Epsilon-gierige Algorithmus erzielte 788 Punkte, fast zweimal besser als der Zufallsalgorithmus - super!  Das zweite Diagramm erkl√§rt diesen Algorithmus ziemlich gut.  Wir sehen, dass f√ºr die ersten 500 Schritte die Aktionen ungef√§hr gleichm√§√üig verteilt sind und K zuf√§llig ausgew√§hlt wird.  Dann beginnt es jedoch, Option 5 stark auszunutzen - dies ist eine ziemlich starke Option, aber nicht die beste.  Wir sehen auch, dass der Agent immer noch 10% der Zeit zuf√§llig ausw√§hlt. </p><br><p>  Das ist ziemlich cool - wir haben nur ein paar Codezeilen geschrieben und jetzt haben wir bereits einen ziemlich leistungsf√§higen Algorithmus, der den Raum der Optionen erkunden und Entscheidungen treffen kann, die nahezu optimal sind.  Andererseits fand der Algorithmus nicht die beste Option.  Ja, wir k√∂nnen die Anzahl der Lernschritte erh√∂hen, aber auf diese Weise verbringen wir noch mehr Zeit mit einer zuf√§lligen Suche, was das Endergebnis weiter verschlechtert.  Au√üerdem wird standardm√§√üig Zuf√§lligkeit in diesen Prozess eingen√§ht - der beste Algorithmus wird m√∂glicherweise nicht gefunden. </p><br><p>  Sp√§ter werde ich jeden der Algorithmen viele Male ausf√ºhren, damit wir sie relativ zueinander vergleichen k√∂nnen.  Aber jetzt schauen wir uns die Thompson-Probenahme an und testen sie in derselben Umgebung. </p><br><h2>  Thompson-Probenahme </h2><br><p>  Die Thompson-Abtastung unterscheidet sich grundlegend vom epsilon-gierigen Algorithmus durch drei Hauptpunkte: <br><br></p><ol><li>  Es ist nicht gierig. </li><li>  Es macht Versuche auf raffiniertere Weise. </li><li>  Es ist Bayesianisch. </li></ol><br>  Der Hauptpunkt ist Absatz 3, daraus folgen die Abs√§tze 1 und 2. <br><p>  Das Wesentliche des Algorithmus ist: <br><br></p><ol><li>  Stellen Sie die anf√§ngliche Beta-Verteilung f√ºr die R√ºckzahlung jeder Option zwischen 0 und 1 ein. </li><li>  Probieren Sie die Optionen aus dieser Verteilung aus und w√§hlen Sie den maximalen Theta-Parameter aus. </li><li>  W√§hlen Sie die Option k, die dem gr√∂√üten Theta zugeordnet ist. </li><li>  Zeigen Sie an, wie viele Punkte erzielt wurden, und aktualisieren Sie die Verteilungsparameter. </li></ol><br>  Lesen Sie hier mehr √ºber die Beta-Distribution. <br>  Und √ºber seine Verwendung in Python - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> . <br><p>  Algorithmuscode: <br><br></p><pre> <code class="python hljs"> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ThompsonSampler</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(BaseSampler)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, env)</span></span></span><span class="hljs-function">:</span></span> super().__init__(env) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">choose_k</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># sample from posterior (this is the thompson sampling approach) # this leads to more exploration because machines with &gt; uncertainty can then be selected as the machine self.theta = np.random.beta(self.a, self.b) # select machine with highest posterior p of payout self.k = self.variants[np.argmax(self.theta)] #self.k = np.argmax(self.a/(self.a + self.b)) return self.k def update(self): #update dist (a, b) = (a, b) + (r, 1 - r) self.a[self.k] += self.reward self.b[self.k] += 1 - self.reward # ie only increment b when it's a swing and a miss. 1 - 0 = 1, 1 - 1 = 0 #self.thetaregret[self.i] = self.thetaregret[self.i] #self.regret_i[self.i] = np.max(self.theta) - self.theta[self.k] self.thetas[self.i] = self.theta[self.k] self.thetaregret[self.i] = np.max(self.thetas) - self.theta[self.k] self.ad_i[self.i] = self.k self.r_i[self.i] = self.reward self.i += 1</span></span></code> </pre> <br>  Die formale Notation des Algorithmus sieht so aus. <br> <a href=""><img src="https://habrastorage.org/webt/5f/n6/xe/5fn6xew2i7v1jjh_10h9jqkjdzu.png"></a> <br><p>  Programmieren wir diesen Algorithmus.  Wie andere Agenten erbt ThompsonSampler von BaseSampler und definiert seine eigenen Methoden select_k und update.  Starten Sie jetzt unseren neuen Agenten. </p><br><pre> <code class="python hljs"> en2 = Environment(machines, payouts, n_trials) tsa = ThompsonSampler(env=en2) en2.run(agent=tsa)</code> </pre> <br> <a href=""><img src="https://habrastorage.org/webt/ml/kj/1p/mlkj1pvs8xnpxtkqmehmm_3xhgo.jpeg"></a> <br><p>  Wie Sie sehen k√∂nnen, erzielte er mehr als der epsilon-gierige Algorithmus.  Gro√üartig!  Schauen wir uns das Diagramm der Auswahl der Versuche an.  Darauf sind zwei interessante Dinge zu sehen.  Erstens hat der Agent die beste Option (Option 9) korrekt erkannt und in vollem Umfang genutzt.  Zweitens verwendete der Agent andere Optionen, jedoch auf schwierigere Weise - nach etwa 1000 Versuchen verwendete der Agent neben der Hauptoption haupts√§chlich die leistungsst√§rksten Optionen unter den anderen.  Mit anderen Worten, er w√§hlte nicht zuf√§llig, sondern kompetenter. </p><br><p>  Warum funktioniert das?  Es ist ganz einfach: Die Unsicherheit in der posterioren Verteilung des erwarteten Nutzens f√ºr jede Option bedeutet, dass jede Option mit einer Wahrscheinlichkeit ausgew√§hlt wird, die ungef√§hr proportional zu ihrer Form ist und durch die Alpha- und Beta-Parameter bestimmt wird.  Mit anderen Worten, bei jedem Versuch l√∂st die Thompson-Stichprobe die Option entsprechend der posterioren Wahrscheinlichkeit aus, dass sie den maximalen Nutzen hat.  Grob gesagt entscheidet der Agent anhand der Verteilungsinformationen √ºber die Unsicherheit, wann die Umgebung untersucht und wann die Informationen verwendet werden sollen.  Beispielsweise kann eine schwache Option mit hoher posteriorer Unsicherheit f√ºr diesen Versuch am meisten zahlen.  Bei den meisten Versuchen ist jedoch die Wahrscheinlichkeit einer Auswahl umso gr√∂√üer, je st√§rker die posteriore Verteilung ist, je gr√∂√üer der Durchschnitt und je geringer die Standardabweichung ist. </p><br><p>  Eine weitere bemerkenswerte Eigenschaft des Thompson-Algorithmus: Da es sich um einen Bayes'schen Algorithmus handelt, k√∂nnen wir die Unsicherheit in der Amortisationssch√§tzung f√ºr jede Option anhand ihrer Parameter absch√§tzen.  Die folgende Grafik zeigt die posterioren Verteilungen an 6 verschiedenen Punkten und in 20.000 Versuchen.  Sie sehen, wie Verteilungen allm√§hlich zur Option mit der besten Amortisation konvergieren. </p><br> <a href=""><img src="https://habrastorage.org/webt/bb/ka/fb/bbkafb4nv1pajwkygxy2brtmowy.jpeg"></a> <br><p>  Vergleichen Sie nun alle 3 Agenten in 100 Simulationen.  1 Simulation ist ein Agentenstart bei 10.000 Versuchen. </p><br> <a href=""><img src="https://habrastorage.org/webt/j6/v1/sm/j6v1smcrwkwyhlo27ffhly13pwk.jpeg"></a> <br><p>  Wie Sie in der Grafik sehen k√∂nnen, funktionieren sowohl die epsilon-gierige Strategie als auch die Thompson-Stichprobe viel besser als die Zufallsstichprobe.  Sie werden √ºberrascht sein, dass die epsilon-gierige Strategie und die Thompson-Stichprobe hinsichtlich ihrer Leistung tats√§chlich vergleichbar sind.  Eine Epsilon-gierige Strategie kann sehr effektiv sein, ist jedoch riskanter, da sie bei einer suboptimalen Option h√§ngen bleiben kann - dies ist an den Fehlern in der Grafik zu erkennen.  Aber Thompson Sampling kann nicht, weil es die Auswahl im Bereich der Optionen auf komplexere Weise trifft. </p><br><h2>  Bedauern </h2><br><p>  Eine andere M√∂glichkeit, um zu bewerten, wie gut der Algorithmus funktioniert, besteht darin, das Bedauern zu bewerten.  Grob gesagt ist es umso besser, je kleiner es im Verh√§ltnis zu den bereits ergriffenen Ma√ünahmen ist.  Unten finden Sie eine grafische Darstellung des gesamten Bedauerns und des Bedauerns f√ºr den Fehler.  Noch einmal - je weniger Bedauern, desto besser. </p><br> <a href=""><img src="https://habrastorage.org/webt/8p/kd/o3/8pkdo3bilrde28bwsimdnbesqwg.jpeg"></a> <br><p>  In der oberen Grafik sehen wir das totale Bedauern und in der unteren den Versuch.  Wie aus den Grafiken ersichtlich ist, konvergiert die Thompson-Stichprobe viel schneller zu minimalem Bedauern als die epsilon-gierige Strategie.  Und es konvergiert auf eine niedrigere Ebene.  Bei der Thompson-Stichprobe bereut der Agent weniger, da er die beste Option besser erkennen und die vielversprechendsten Optionen besser ausprobieren kann. Die Thompson-Stichprobe eignet sich daher besonders f√ºr fortgeschrittenere Anwendungsf√§lle wie statistische Modelle oder neuronale Netze zur Auswahl von k. </p><br><h2>  Schlussfolgerungen </h2><br><p>  Dies ist ein ziemlich langer technischer Beitrag.  Zusammenfassend k√∂nnen wir ziemlich ausgefeilte Stichprobenmethoden verwenden, wenn wir viele Optionen haben, die wir in Echtzeit testen m√∂chten.  Eine der sehr guten Eigenschaften der Thompson-Probenahme ist, dass sie die Nutzung und Erkundung auf ziemlich knifflige Weise in Einklang bringt.  Das hei√üt, wir k√∂nnen ihn die Verteilung der L√∂sungsoptionen in Echtzeit optimieren lassen.  Dies sind coole Algorithmen, die f√ºr ein Unternehmen n√ºtzlicher sein sollten als A / B-Tests. </p><br><p>  <b>Wichtig!</b>  <b>Thompson-Stichproben bedeuten nicht, dass Sie keine A / B-Tests durchf√ºhren m√ºssen.</b>  <b>Normalerweise finden sie zuerst mit seiner Hilfe die besten Optionen und f√ºhren dann bereits A / B-Tests durch.</b> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de425619/">https://habr.com/ru/post/de425619/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de425605/index.html">Annahme von Zahlungen von einer Karte ohne jur. Gesichter auf Yandex.Money</a></li>
<li><a href="../de425607/index.html">Identifizieren Sie Betrug mithilfe des Enron-Datensatzes. Teil 2, das beste Modell finden</a></li>
<li><a href="../de425609/index.html">Spieltheorie: Entscheidungsfindung mit Beispielen bei Kotlin</a></li>
<li><a href="../de425611/index.html">Frontend-Architektur der oberen Ebene. Yandex Vortrag</a></li>
<li><a href="../de425613/index.html">Wie ich die Daten des Tempo-Plugins f√ºr Jira Server und Jira Cloud kombiniert und zur√ºck in Jira Cloud migriert habe</a></li>
<li><a href="../de425621/index.html">Ein Unternehmen, das atmosph√§risches Kohlendioxid verwendet, startet die Methanproduktion</a></li>
<li><a href="../de425623/index.html">Coworking Fototour ‚ÄúKey‚Äù</a></li>
<li><a href="../de425625/index.html">Verbraucht oder warum Lokalisierer Spiele schlecht √ºbersetzen</a></li>
<li><a href="../de425627/index.html">IaaS f√ºr die Entwicklung von Diensten: Wer und warum hat auf virtuelle Infrastruktur umgestellt?</a></li>
<li><a href="../de425629/index.html">Wie wir ein Brettspiel mit Fernbedienung gemacht haben</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>