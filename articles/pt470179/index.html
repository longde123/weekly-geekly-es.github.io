<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üï£ ‚öõÔ∏è üöã PDDM - Novo algoritmo de aprendizado de refor√ßo baseado em modelo com agendador avan√ßado üö© üò± üëÉ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="O aprendizado por refor√ßo √© dividido em duas grandes classes: sem modelo e com base em modelo. No primeiro caso, as a√ß√µes s√£o otimizadas diretamente p...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>PDDM - Novo algoritmo de aprendizado de refor√ßo baseado em modelo com agendador avan√ßado</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/470179/"><p><img src="https://habrastorage.org/webt/az/gq/1u/azgq1uy2qpj7dwfvwxzq6qmsv2a.gif"></p><br><p>  O aprendizado por refor√ßo √© dividido em duas grandes classes: sem modelo e com base em modelo.  No primeiro caso, as a√ß√µes s√£o otimizadas diretamente pelo sinal de recompensa e, no segundo, a rede neural √© apenas um modelo de realidade e as a√ß√µes ideais s√£o selecionadas usando um agendador externo.  Cada abordagem tem suas pr√≥prias vantagens e desvantagens. </p><br><p>  Os desenvolvedores de Berkeley e Google Brain introduziram o algoritmo PDDM baseado em modelo com um agendador aprimorado que permite aprender efetivamente movimentos complexos com um grande n√∫mero de graus de liberdade com um pequeno n√∫mero de exemplos.  Para aprender a girar as bolas em um bra√ßo rob√≥tico com articula√ß√µes dos dedos realistas com 24 graus de liberdade, foram necess√°rias apenas 4 horas de pr√°tica em um rob√¥ f√≠sico real. </p><a name="habracut"></a><br><p>  Aprendizado por Refor√ßo √© o treinamento de rob√¥s com um sinal de recompensa.  √â semelhante a como os seres vivos aprendem.  Mas o problema √© complicado pelo fato de n√£o se saber como alterar os pesos da rede neural, de modo que suas a√ß√µes propostas levem a um aumento nas recompensas.  Portanto, no aprendizado por refor√ßo, os m√©todos convencionais de treinamento em redes neurais n√£o s√£o adequados.  Afinal, n√£o se sabe exatamente o que ela deve divulgar em sua sa√≠da, o que significa que √© imposs√≠vel encontrar um erro entre sua previs√£o e o estado real das coisas.  Para pular essa diferen√ßa de volta pelas camadas da rede neural e alterar os pesos entre os neur√¥nios para minimizar esse erro.  Este √© um algoritmo cl√°ssico de propaga√ß√£o traseira, ensinado por redes neurais. </p><br><p>  Portanto, os cientistas inventaram v√°rias maneiras de resolver esse problema. </p><br><h2 id="model-free">  Sem modelo </h2><br><p>  Uma das abordagens mais eficazes foi o modelo ator-cr√≠tico.  Deixe uma rede neural (ator) em sua entrada receber o estado do ambiente do estado e, na sa√≠da, emitir a√ß√µes que devem levar a um aumento nas recompensas de recompensa.  At√© o momento, essas a√ß√µes s√£o aleat√≥rias e simplesmente dependem do fluxo de sinal na rede, uma vez que a rede neural ainda n√£o foi treinada.  E a segunda rede neural (cr√≠tica), permite que a entrada tamb√©m receba o estado do ambiente do estado, mas tamb√©m a√ß√µes da sa√≠da da primeira rede.  E na sa√≠da, deixe apenas a recompensa de recompensa prevista, que ser√° recebida se essas a√ß√µes forem aplicadas. </p><br><p>  Agora preste aten√ß√£o: n√£o sabemos quais devem ser as melhores a√ß√µes na sa√≠da da primeira rede, levando a um aumento na recompensa.  Portanto, usando o algoritmo de propaga√ß√£o traseira, n√£o podemos trein√°-lo.  Mas a segunda rede neural pode muito bem prever o valor exato da recompensa da recompensa (ou melhor, geralmente sua mudan√ßa), que receber√° se as a√ß√µes forem aplicadas agora.  Ent√£o, vamos pegar o gradiente de altera√ß√£o de erro da segunda rede e aplic√°-lo √† primeira!  Assim, voc√™ pode treinar a primeira rede neural pelo m√©todo cl√°ssico de propaga√ß√£o traseira do erro.  Simplesmente tomamos o erro n√£o das sa√≠das da primeira rede, mas das sa√≠das da segunda. </p><br><p>  Como resultado, a primeira rede neural aprende a emitir a√ß√µes ideais, levando a um aumento nas recompensas.  Porque se o cr√≠tico cr√≠tico cometeu um erro e previu uma recompensa menor do que na realidade, o gradiente dessa diferen√ßa mover√° as a√ß√µes do ator ator na dire√ß√£o, de modo que o cr√≠tico preveja com mais precis√£o a recompensa.  E isso significa a√ß√µes mais √≥timas (afinal, elas levar√£o ao fato de que o cr√≠tico prev√™ com precis√£o um pr√™mio maior).  Um princ√≠pio semelhante funciona na dire√ß√£o oposta: se o cr√≠tico superestimar a recompensa esperada, a diferen√ßa entre expectativa e realidade reduzir√° os resultados das a√ß√µes da primeira rede neural, o que levou a essa indica√ß√£o superestimada de recompensa da segunda rede. </p><br><p>  Como voc√™ pode ver, neste caso, as a√ß√µes s√£o otimizadas diretamente pelo sinal de recompensa.  Essa √© a ess√™ncia comum de todos os algoritmos sem modelo no Aprendizado por Refor√ßo.  Eles s√£o o estado da arte no momento. </p><br><p>  Sua vantagem √© que as a√ß√µes ideais s√£o procuradas por descida gradiente; portanto, no final, as mais √≥timas s√£o encontradas.  O que significa mostrar o melhor resultado.  Outra vantagem √© a capacidade de usar redes neurais pequenas (e, portanto, mais f√°ceis de aprender).  Se dentre toda a variedade de fatores ambientais, alguns espec√≠ficos s√£o fundamentais para resolver o problema, a descida gradiente √© capaz de identific√°-los.  E use para resolver o problema.  Essas duas vantagens garantiram o sucesso com m√©todos diretos sem modelo. </p><br><p>  Mas eles tamb√©m t√™m desvantagens.  Como as a√ß√µes s√£o ensinadas diretamente pelo sinal de recompensa, muitos exemplos de treinamento s√£o necess√°rios.  Dezenas de milh√µes, mesmo para casos muito simples.  Eles trabalham mal em tarefas com um grande n√∫mero de graus de liberdade.  Se o algoritmo n√£o conseguir identificar imediatamente os principais fatores no cen√°rio de alta dimens√£o, provavelmente n√£o aprender√° nada.  Os m√©todos sem modelo tamb√©m podem explorar vulnerabilidades no sistema, concentrando-se em a√ß√µes n√£o ideais (se a descida gradiente convergir para ele), ignorando outros fatores ambientais.  Para tarefas um pouco livres de modelo, mesmo que ligeiramente diferentes, os m√©todos precisam ser treinados completamente novamente. </p><br><h2 id="model-based">  Baseado em modelo </h2><br><p>  Os m√©todos baseados em modelos no Aprendizado por Refor√ßo s√£o fundamentalmente diferentes da abordagem descrita acima.  Em Baseado em Modelo, uma rede neural prev√™ apenas o que acontecer√° a seguir.  N√£o oferecendo nenhuma a√ß√£o.  Ou seja, √© simplesmente um modelo de realidade (da√≠ o "Modelo" - Baseado no nome).  E nem um sistema de tomada de decis√£o. </p><br><p>  As redes neurais baseadas em modelo s√£o alimentadas com o estado atual do ambiente de estado e com as a√ß√µes que queremos executar.  E a rede neural prev√™ como o estado mudar√° no futuro ap√≥s a aplica√ß√£o dessas a√ß√µes.  Ela tamb√©m pode prever qual ser√° a recompensa como resultado dessas a√ß√µes.  Mas isso n√£o √© necess√°rio, pois a recompensa geralmente pode ser calculada a partir de um estado conhecido.  Al√©m disso, esse estado de sa√≠da pode ser retornado √† entrada da rede neural (junto com as novas a√ß√µes propostas) e, assim, prever recursivamente as mudan√ßas no ambiente externo muitos passos adiante. </p><br><p>  As redes neurais baseadas em modelo s√£o muito f√°ceis de aprender.  Como eles simplesmente prev√™em como o mundo mudar√°, sem fazer sugest√µes de quais a√ß√µes √≥timas devem ser para que a recompensa aumente.  Portanto, a rede neural baseada em modelo usa todos os exemplos existentes para seu treinamento, e n√£o apenas aqueles que levam a um aumento ou diminui√ß√£o de recompensas, como √© o caso do Modelo-Livre.  Essa √© a raz√£o pela qual as redes neurais baseadas em modelo precisam de muito menos exemplos de treinamento. </p><br><p>  A √∫nica desvantagem √© que a rede neural baseada em modelo deve estudar a din√¢mica real do sistema e, portanto, deve ter capacidade suficiente para isso.  Uma rede neural sem modelo pode convergir em fatores-chave, ignorando o restante, e, portanto, ser uma rede pequena e simples (se a tarefa for, em princ√≠pio, resolvida com menos recursos). </p><br><p>  Outra grande vantagem, al√©m do treinamento em um n√∫mero menor de exemplos, √© que, como modelo universal do mundo, uma √∫nica rede neural baseada em modelo pode ser usada para resolver qualquer n√∫mero de problemas neste mundo. </p><br><p>  O principal problema da abordagem baseada em modelo √© que a√ß√µes devem ser aplicadas √†s a√ß√µes das redes neurais?  Afinal, a pr√≥pria rede neural n√£o oferece nenhuma a√ß√£o ideal. </p><br><p>  A maneira mais f√°cil √© conduzir atrav√©s de uma rede neural dezenas de milhares de a√ß√µes aleat√≥rias e escolher aquelas pelas quais a rede neural ir√° prever a maior recompensa.  Este √© um aprendizado cl√°ssico de refor√ßo baseado em modelo.  No entanto, com grandes dimens√µes e longas cadeias de tempo, o n√∫mero de a√ß√µes poss√≠veis acaba sendo muito grande para classificar todas elas (ou at√© adivinhar pelo menos um pouco de ideal). </p><br><p>  Por esse motivo, os m√©todos baseados em modelo geralmente s√£o inferiores aos livres de modelo, que por descida gradiente convergem diretamente para as a√ß√µes mais ideais. </p><br><p>  Uma vers√£o aprimorada aplic√°vel aos movimentos em rob√≥tica n√£o √© usar a√ß√µes aleat√≥rias, mas manter o movimento anterior, adicionando aleatoriedade √† distribui√ß√£o normal.  Como os movimentos dos rob√¥s geralmente s√£o suaves, isso reduz o n√∫mero de bustos.  Mas, ao mesmo tempo, uma importante mudan√ßa acentuada pode ser perdida. </p><br><p>  A op√ß√£o final de desenvolvimento para essa abordagem pode ser considerada a op√ß√£o CEM, que usa n√£o uma distribui√ß√£o normal fixa que introduz a aleatoriedade no caminho atual das a√ß√µes, mas seleciona os par√¢metros da distribui√ß√£o aleat√≥ria usando entropia cruzada.  Para isso, √© lan√ßada uma popula√ß√£o de c√°lculos de a√ß√µes e os melhores s√£o usados ‚Äã‚Äãpara refinar a dissemina√ß√£o de par√¢metros na pr√≥xima gera√ß√£o.  Algo como um algoritmo evolutivo. </p><br><h2 id="pddm">  PDDM </h2><br><p>  Uma introdu√ß√£o t√£o longa foi necess√°ria para explicar o que est√° acontecendo no novo algoritmo de aprendizado de refor√ßo baseado em modelo PDDM proposto.  Depois de ler um artigo no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">blog da AI de Berkeley</a> (ou uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">vers√£o estendida</a> ) e at√© mesmo o artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">arxiv.org/abs/1909.11652</a> original, isso pode n√£o ter sido √≥bvio. </p><br><p>  O m√©todo PDDM repete a ideia do CEM ao escolher a√ß√µes aleat√≥rias que precisam ser executadas atrav√©s de uma rede neural baseada em modelo para selecionar a√ß√µes com a maior recompensa previs√≠vel.  Somente em vez de selecionar par√¢metros de distribui√ß√£o aleat√≥ria, como √© feito no CEM, o PDDM usa uma correla√ß√£o temporal entre a√ß√µes e uma regra mais suave para atualizar a distribui√ß√£o aleat√≥ria.  A f√≥rmula √© dada no artigo original.  Isso permite verificar um n√∫mero maior de a√ß√µes adequadas por longas dist√¢ncias, principalmente se os movimentos exigirem coordena√ß√£o precisa.  Al√©m disso, os autores do algoritmo filtram os candidatos a a√ß√µes, obtendo uma trajet√≥ria mais suave dos movimentos. </p><br><p>  Simplificando, os desenvolvedores simplesmente propuseram uma f√≥rmula melhor para escolher a√ß√µes aleat√≥rias para testar no cl√°ssico Aprendizado de refor√ßo baseado em modelo. </p><br><p>  Mas o resultado foi muito bom. </p><br><p>  Em apenas 4 horas de treinamento em um rob√¥ real, um rob√¥ com 24 graus de liberdade aprendeu a segurar duas bolas e gir√°-las nas palmas sem cair.  Um resultado inating√≠vel para qualquer m√©todo moderno sem modelo, com um n√∫mero t√£o pequeno de exemplos. </p><br><p>  Curiosamente, para o treinamento, eles usaram um segundo bra√ßo do rob√¥ com 7 graus de liberdade, que pegou as bolas ca√≠das e as devolveu ao bra√ßo principal do rob√¥: </p><br><p><img src="https://habrastorage.org/webt/7s/yf/wq/7syfwqr2vg2-rlzupbccu-06mic.gif"></p><br><p>  Como resultado, ap√≥s 1-2 horas, o roboruk podia segurar as bolas com confian√ßa e mov√™-las na palma da m√£o, e 4 horas eram suficientes para o treinamento completo. </p><br><p><img src="https://habrastorage.org/webt/az/gq/1u/azgq1uy2qpj7dwfvwxzq6qmsv2a.gif"></p><br><p>  Preste aten√ß√£o aos movimentos espasm√≥dicos dos dedos.  Esse √© um recurso das abordagens baseadas em modelo.  Como as a√ß√µes pretendidas s√£o escolhidas aleatoriamente, elas nem sempre coincidem com as melhores.  O algoritmo sem modelo pode potencialmente convergir para movimentos suaves realmente ideais. </p><br><p>  No entanto, a abordagem baseada em modelo permite que, com uma rede neural treinada, modele o mundo, resolva problemas diferentes sem reciclagem.  Existem v√°rios exemplos no artigo, por exemplo, voc√™ pode alterar facilmente a dire√ß√£o de rota√ß√£o das bolas na m√£o (no modelo livre, voc√™ teria que treinar novamente a rede neural para isso).  Ou segure a bola em um ponto espec√≠fico na palma da sua m√£o, seguindo o ponto vermelho. </p><br><p><img src="https://habrastorage.org/webt/np/2a/4v/np2a4vibrlvwohr_kimp7w4o_1u.gif"></p><br><p>  Voc√™ tamb√©m pode fazer Roboruk tra√ßar trajet√≥rias arbitr√°rias com um l√°pis, aprendendo que, para os m√©todos sem modelo, √© uma tarefa muito dif√≠cil. </p><br><p><img src="https://habrastorage.org/webt/iz/a_/v5/iza_v5a2jkohqpcl8vb99_yfwri.gif"></p><br><p>  Embora o algoritmo proposto n√£o seja uma panac√©ia e nem sequer seja um algoritmo de IA no sentido completo da palavra (no PDDM, a rede neural simplesmente substitui o modelo anal√≠tico e as decis√µes s√£o tomadas por pesquisa aleat√≥ria com uma regra complicada que reduz o n√∫mero de enumera√ß√£o de op√ß√µes), pode ser √∫til na rob√≥tica.  Uma vez que mostrou uma melhoria not√°vel nos resultados e √© treinado em um n√∫mero muito pequeno de exemplos. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt470179/">https://habr.com/ru/post/pt470179/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt470165/index.html">A filosofia de dividir por ... ou confiss√£o de um louco</a></li>
<li><a href="../pt470167/index.html">Confer√™ncia para os interessados ‚Äã‚Äãem ci√™ncia antes de se tornar mainstream</a></li>
<li><a href="../pt470169/index.html">Como impedir que a id√©ia morra e reunir uma equipe que n√£o a mate</a></li>
<li><a href="../pt470173/index.html">Plataforma de integra√ß√£o como servi√ßo</a></li>
<li><a href="../pt470175/index.html">Adicione o login com a Apple ao back-end</a></li>
<li><a href="../pt470181/index.html">Como o m√©todo Levenberg-Marquardt funciona</a></li>
<li><a href="../pt470187/index.html">A faixa de pre√ßo para o design e o design de um servi√ßo on-line √© de 100 mil a 5 milh√µes de rublos. Raz√µes</a></li>
<li><a href="../pt470189/index.html">Enviando mensagens ponto a ponto com o PeerJS</a></li>
<li><a href="../pt470191/index.html">Web Solu√ß√£o de problemas com r0ot-mi. Parte 1</a></li>
<li><a href="../pt470193/index.html">Prote√ß√£o universal contra ataques xss e inje√ß√µes de sql</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>