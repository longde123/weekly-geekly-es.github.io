<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>⛔️ 🛌 👨🏿‍🏭 Proses ganda dan rekonsiliasi data dari berbagai sumber 🤟🏾 👩🏼‍🤝‍👨🏾 🍩</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Halo, Habr! 

 Mengingat beragamnya sistem terdistribusi, ketersediaan informasi terverifikasi dalam penyimpanan target adalah kriteria penting untuk ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Proses ganda dan rekonsiliasi data dari berbagai sumber</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/480076/"> Halo, Habr! <br><br>  Mengingat beragamnya sistem terdistribusi, ketersediaan informasi terverifikasi dalam penyimpanan target adalah kriteria penting untuk konsistensi data. <br><br>  Ada banyak pendekatan dan metode untuk efek ini, dan kami akan fokus pada rekonsiliasi, aspek teoretis yang dibahas <a href="https://habr.com/ru/post/428443/">di sini dalam artikel ini.</a>  Saya mengusulkan untuk mempertimbangkan implementasi praktis dari sistem ini, dapat diskalakan dan disesuaikan dengan sejumlah besar data. <br><br>  Bagaimana menerapkan kasus ini pada Python lama yang baik - baca di bawah cut!  Ayo pergi! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ic/zx/hg/iczxhgu9zvlumwggetuoblxm1ra.jpeg"></div><br>  <a href="https://www.megapixl.com/alexdobysh-stock-images-videos-portfolio" rel="nofollow">(Sumber gambar)</a> <br><a name="habracut"></a><br><h2>  Pendahuluan </h2><br>  Mari kita bayangkan bahwa lembaga keuangan memiliki beberapa sistem terdistribusi dan kita dihadapkan dengan tugas memverifikasi transaksi dalam sistem ini dan mengunggah data yang direkonsiliasi ke penyimpanan target. <br><br>  Sebagai sumber data, ambil file teks besar dan tabel di database PostgreSQL.  Misalkan data dalam sumber-sumber ini memiliki transaksi yang sama, tetapi mereka dapat memiliki perbedaan, dan oleh karena itu mereka perlu diverifikasi dan ditulis ke data yang diverifikasi dalam penyimpanan akhir untuk analisis. <br><br>  Selain itu, perlu untuk menyediakan peluncuran paralel dari beberapa rekonsiliasi pada database yang sama dan mengadaptasi sistem ke volume besar menggunakan multiprocessing. <br><br>  Modul <a href="https://docs.python.org/dev/library/multiprocessing.html" rel="nofollow">multiprosesing</a> sangat bagus untuk memparalelkan operasi dengan Python dan, dalam arti tertentu, menghindari kekurangan GIL tertentu.  Kami akan menggunakan kemampuan perpustakaan ini di bawah ini. <br><br><h2>  Arsitektur sistem dalam pengembangan </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/je/dm/hu/jedmhumxsx9d-mxu-bbfbzqbulq.png"></div><br>  Komponen yang Digunakan: <br><br><ul><li>  <b>Pembuat data acak</b> - skrip Python yang menghasilkan file CSV dan atas dasar itu mengisi tabel dalam database; </li><li>  <b>Sumber data</b> - file dan tabel CSV dalam database PostgreSQL; </li><li>  <b>Adapters</b> - dalam hal ini, kami menggunakan dua adapter yang akan mengekstraksi data dari sumbernya (CSV atau DB) dan memasukkan informasi ke dalam database perantara; </li><li>  <b>Basis data</b> - dalam jumlah tiga bagian: data mentah, database perantara yang menyimpan informasi yang ditangkap oleh adaptor, dan basis data "bersih" yang berisi transaksi yang direkonsiliasi dari kedua sumber. </li></ul><br><h2>  Pelatihan awal </h2><br>  Sebagai alat penyimpanan data, kami akan menggunakan <a href="https://hub.docker.com/_/postgres" rel="nofollow">basis data PostgreSQL di wadah Docker</a> dan berinteraksi dengan basis data kami melalui <a href="https://hub.docker.com/r/dpage/pgadmin4/" rel="nofollow">pgAdmin yang berjalan di wadah</a> : <br><br><pre><code class="bash hljs">docker run --name pg -d -e <span class="hljs-string"><span class="hljs-string">"POSTGRES_USER=my_user"</span></span> -e <span class="hljs-string"><span class="hljs-string">"POSTGRES_PASSWORD=my_password"</span></span> postgres</code> </pre> <br>  Menjalankan pgAdmin: <br><br><pre> <code class="bash hljs">docker run -p 80:80 -e <span class="hljs-string"><span class="hljs-string">"PGADMIN_DEFAULT_EMAIL=user@domain.com"</span></span> -e <span class="hljs-string"><span class="hljs-string">"PGADMIN_DEFAULT_PASSWORD=12345"</span></span> -d dpage/pgadmin4</code> </pre> <br>  Setelah semuanya dimulai, jangan lupa untuk menentukan di file konfigurasi (conf / db.ini) string koneksi ke database (untuk contoh pelatihan, Anda bisa!): <br><br><pre> <code class="bash hljs">[POSTGRESQL] db_url=postgresql://my_user:my_password@172.17.0.2:5432/my_user</code> </pre><br>  Pada prinsipnya, penggunaan wadah adalah opsional dan Anda dapat menggunakan server basis data Anda. <br><br><h2>  Pembuatan Input </h2><br>  Script Python <b>generate_test_data</b> bertanggung jawab atas pembuatan data uji, yang mengambil jumlah entri yang diinginkan untuk dihasilkan.  Urutan operasi dapat dengan mudah dilacak oleh fungsi utama dari kelas <b>GenerateTestData</b> : <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta"> @m.timing def run(self, num_rows): """ Run the process """ m.info('START!') self.create_db_schema() self.create_folder('data') self.create_csv_file(num_rows) self.bulk_copy_to_db() self.random_delete_rows() self.random_update_rows() m.info('END!')</span></span></code> </pre> <br>  Jadi, fungsi melakukan langkah-langkah berikut: <br><br><ul><li>  Membuat skema dalam database (kami membuat semua skema dan tabel utama); </li><li>  Membuat folder untuk menyimpan file uji; </li><li>  Membuat file uji dengan sejumlah baris tertentu; </li><li>  Sisipkan massal data ke tabel target transaction_db_raw.transaction_log; </li><li>  Penghapusan tak disengaja dari beberapa baris dalam tabel ini; </li><li>  Pembaruan acak dari beberapa baris dalam tabel ini. </li></ul><br>  Penghapusan dan modifikasi diperlukan agar objek yang dibandingkan memiliki setidaknya beberapa perbedaan.  Penting untuk dapat mencari perbedaan ini! <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@m.timing @m.wrapper(m.entering, m.exiting) def random_delete_rows(self): """ Random deleting some rows from the table """ sql_command = sql.SQL(""" delete from {0}.{1} where ctid = any(array( select ctid from {0}.{1} tablesample bernoulli (1) ))""").format(sql.Identifier(self.schema_raw), sql.Identifier(self.raw_table_name)) try: rows = self.database.execute(sql_command) m.info('Has been deleted [%s rows] from table %s' % (rows, self.raw_table_name)) except psycopg2.Error as err: m.error('Oops! Delete random rows has been FAILED. Reason: %s' % err.pgerror) @m.timing @m.wrapper(m.entering, m.exiting) def random_update_rows(self): """ Random update some rows from the table """ sql_command = sql.SQL(""" update {0}.{1} set transaction_amount = round(random()::numeric, 2) where ctid = any(array( select ctid from {0}.{1} tablesample bernoulli (1) ))""").format(sql.Identifier(self.schema_raw), sql.Identifier(self.raw_table_name)) try: rows = self.database.execute(sql_command) m.info('Has been updated [%s rows] from table %s' % (rows, self.raw_table_name)) except psycopg2.Error as err: m.error('Oops! Delete random rows has been FAILED. Reason: %s' % err.pgerror)</span></span></code> </pre> <br>  Pembuatan kumpulan data uji dan perekaman selanjutnya ke file teks dalam format CSV adalah sebagai berikut: <br><br><ul><li>  UID transaksi acak dibuat; </li><li>  Nomor akun UID acak dibuat (secara default, kami mengambil sepuluh akun unik, tetapi nilai ini dapat diubah menggunakan file konfigurasi dengan mengubah parameter "random_accounts"); </li><li>  Tanggal transaksi - tanggal acak dari tanggal yang ditentukan dalam file konfigurasi (initial_date); </li><li>  Jenis transaksi (transaksi / komisi); </li><li>  Jumlah transaksi; </li><li>  Pekerjaan utama dalam pembuatan data dilakukan oleh metode <i>generate_test_data_by_chunk</i> dari kelas <b>TestDataCreator</b> : </li></ul><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@m.timing def generate_test_data_by_chunk(self, chunk_start, chunk_end): """ Generating and saving to the file """ num_rows_mp = chunk_end - chunk_start new_rows = [] for _ in range(num_rows_mp): transaction_uid = uuid.uuid4() account_uid = choice(self.list_acc) transaction_date = (self.get_random_date(self.date_in, 0) .__next__() .strftime('%Y-%m-%d %H:%M:%S')) type_deal = choice(self.list_type_deal) transaction_amount = randint(-1000, 1000) new_rows.append([transaction_uid, account_uid, transaction_date, type_deal, transaction_amount]) self.write_in_file(new_rows, chunk_start, chunk_end)</span></span></code> </pre> <br><blockquote>  Fitur dari fungsi ini adalah peluncuran dalam beberapa proses asinkron yang diparalelkan, yang masing-masing menghasilkan bagiannya sendiri dari catatan 50K.  "Chip" ini akan memungkinkan Anda untuk membuat file pada beberapa juta baris dengan cukup cepat </blockquote><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">run_csv_writing</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" Writing the test data into csv file """</span></span> pool = mp.Pool(mp.cpu_count()) jobs = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> chunk_start, chunk_end <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.divide_into_chunks(<span class="hljs-number"><span class="hljs-number">0</span></span>, self.num_rows): jobs.append(pool.apply_async(self.generate_test_data_by_chunk, (chunk_start, chunk_end))) <span class="hljs-comment"><span class="hljs-comment"># wait for all jobs to finish for job in jobs: job.get() # clean up pool.close() pool.join()</span></span></code> </pre> <br>  Setelah file teks selesai, perintah bulk_insert diproses dan semua data dari file ini masuk ke tabel <b>transaction_db_raw.transaction_log.</b> <br><br>  Selanjutnya, kedua sumber akan berisi data yang persis sama dan rekonsiliasi tidak akan menemukan sesuatu yang menarik, jadi kami menghapus dan mengubah beberapa baris acak dalam database. <br><br>  Jalankan skrip dan hasilkan file uji CSV dengan transaksi pada 10 ribu baris: <br><br><pre> <code class="bash hljs">./generate_test_data.py 10000</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/4c/cp/hm/4ccphmc5dcjcgxuy54p_9limlz4.png"></div><br>  Tangkapan layar menunjukkan bahwa file 10K baris diterima, 10K dimuat ke dalam database, tetapi kemudian 112 baris dihapus dari database dan 108 lainnya diubah. Hasil: file dan tabel dalam database berbeda dengan 220 entri. <br><br>  "Nah, di mana multiprocessing?", Anda bertanya. <br>  Dan pekerjaannya dapat dilihat saat Anda menghasilkan file yang lebih besar, bukan dengan catatan 10 ribu, tetapi, misalnya, sebesar 1 juta.  Akankah kita mencoba? <br><br><pre> <code class="bash hljs">./generate_test_data.py 1000000</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/rw/_a/ne/rw_aneqnairixqk-wglpqxgjvkc.png"></div><br>  Setelah memuat data, menghapus dan mengubah catatan acak, kita melihat perbedaan file teks dari tabel: 19.939 baris (yang 10.022 dihapus secara acak, dan 9.917 diubah). <br><br><blockquote>  Gambar menunjukkan bahwa pembuatan rekaman tidak sinkron, tidak konsisten.  Ini berarti bahwa proses selanjutnya dapat dimulai tanpa memperhitungkan urutan mulai segera setelah yang sebelumnya selesai.  Tidak ada jaminan bahwa hasilnya akan berada dalam urutan yang sama dengan input. </blockquote><br><div class="spoiler">  <b class="spoiler_title">Apakah ini pasti lebih cepat?</b> <div class="spoiler_text">  Satu juta baris yang bukan pada mesin virtual tercepat "diciptakan" dalam 15,5 detik - dan ini adalah opsi yang layak.  Setelah memulai generasi yang sama secara berurutan, tanpa menggunakan multiprosesor, saya mendapatkan hasilnya: pembuatan file lebih dari tiga kali lebih lambat (lebih dari 52 detik, bukan 15,5): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/sb/kb/ck/sbkbckylzluoyyslyflwak5udrq.png"></div><br></div></div><br><h2>  Adaptor untuk CSV </h2><br>  Adaptor ini hash baris, hanya menyisakan kolom pertama, pengidentifikasi transaksi, tidak berubah dan menyimpan data yang diterima ke file <i>data / transaction_hashed.csv</i> .  Langkah terakhir dari karyanya adalah memuat file ini menggunakan perintah COPY ke dalam tabel sementara skema <b>reconciliation_db.</b> <br><br>  Pembacaan file yang optimal dilakukan oleh beberapa proses paralel.  Kami membaca baris demi baris, masing-masing dalam ukuran 5 megabyte.  Angka "5 megabyte" diperoleh dengan metode empiris.  Dengan ukuran selembar teks inilah kami bisa mendapatkan waktu terkecil untuk membaca file besar di mesin virtual kami.  Anda dapat melakukan percobaan pada lingkungan Anda dengan parameter ini dan melihat bagaimana waktu operasi akan berubah: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@m.timing def process_wrapper(self, chunk_start, chunk_size): """ Read a particular chunk """ with open(self.file_name_raw, newline='\n') as file: file.seek(chunk_start) lines = file.read(chunk_size).splitlines() for line in lines: self.process(line) def chunkify(self, size=1024*1024*5): """ Return a new chunk """ with open(self.file_name_raw, 'rb') as file: chunk_end = file.tell() while True: chunk_start = chunk_end file.seek(size, 1) file.readline() chunk_end = file.tell() if chunk_end &gt; self.file_end: chunk_end = self.file_end yield chunk_start, chunk_end - chunk_start break else: yield chunk_start, chunk_end - chunk_start @m.timing def run_reading(self): """ The main method for the reading """ # init objects pool = mp.Pool(mp.cpu_count()) jobs = [] m.info('Run csv reading...') # create jobs for chunk_start, chunk_size in self.chunkify(): jobs.append(pool.apply_async(self.process_wrapper, (chunk_start, chunk_size))) # wait for all jobs to finish for job in jobs: job.get() # clean up pool.close() pool.join() m.info('CSV file reading has been completed')</span></span></code> </pre> <br>  Contoh membaca file yang dibuat sebelumnya pada catatan 1M: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9p/z1/zr/9pz1zrkzeelnep_r8oppk0sxhok.png"></div><br>  Tangkapan layar memperlihatkan pembuatan tabel sementara dengan nama unik untuk menjalankan rekonsiliasi saat ini.  Selanjutnya adalah pembacaan asinkron file di bagian dan mengambil hash dari setiap baris.  Memasukkan data dari adaptor ke tabel target menyelesaikan pekerjaan dengan adaptor ini. <br><blockquote>  Menggunakan tabel sementara dengan nama unik untuk setiap proses rekonsiliasi memungkinkan Anda untuk memparalelkan proses rekonsiliasi dalam satu basis data. </blockquote><br><h2>  Adaptor untuk PostgreSQL </h2><br>  Adaptor untuk memproses data yang disimpan dalam tabel bekerja kira-kira dengan logika yang sama dengan adaptor untuk file: <br><br><ul><li>  membaca bagian tabel (jika besar, lebih dari 100K entri) dan mengambil hash untuk semua kolom kecuali pengenal transaksi; </li><li>  kemudian ada penyisipan data yang diproses ke dalam tabel <b>reconciliation_db.</b>  <b>storage _ $ (int (time.time ())</b> . </li></ul><br>  Fitur yang menarik dari adaptor ini adalah ia menggunakan kumpulan koneksi ke database, yang akan mencari berdasarkan indeks untuk data yang diperlukan dalam tabel dan memprosesnya. <br><br>  Berdasarkan ukuran tabel, jumlah proses yang diperlukan untuk pemrosesan dihitung dan dalam setiap proses ada pembagian menjadi 10 tugas. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">read_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" Read the data from the postgres and shared those records with each processor to perform their operation using threads """</span></span> threads_array = self.get_threads(<span class="hljs-number"><span class="hljs-number">0</span></span>, self.max_id_num_row, self.pid_max) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> pid <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, len(threads_array) + <span class="hljs-number"><span class="hljs-number">1</span></span>): m.info(<span class="hljs-string"><span class="hljs-string">'Process %s'</span></span> % pid) <span class="hljs-comment"><span class="hljs-comment"># Getting connection from the connection pool select_conn = self._select_conn_pool.getconn() select_conn.autocommit = 1 # Creating 10 process to perform the operation process = Process(target=self.process_data, args=(self.data_queque, pid, threads_array[pid-1][0], threads_array[pid-1][1], select_conn)) process.daemon = True process.start() process.join() select_conn.close()</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pw/kt/kk/pwktkkisxg3sud4dyss_gtyi4gy.png"></div><br><h2>  Cari perbedaan </h2><br>  Kami melanjutkan ke verifikasi data yang diterima dari dua adapter. <br><br>  Rekonsiliasi (atau menerima laporan perbedaan) terjadi di sisi server dari database, menggunakan semua kekuatan bahasa SQL. <br><br>  Query SQL cukup rumit - hanya sebuah tabel yang digabung dengan data dari adapter ke dirinya sendiri dengan ID transaksi: <br><br><pre> <code class="python hljs">sql_command = sql.SQL(<span class="hljs-string"><span class="hljs-string">""" select s1.adapter_name, count(s1.transaction_uid) as tran_count from {0}.{1} s1 full join {0}.{1} s2 on s2.transaction_uid = s1.transaction_uid and s2.adapter_name != s1.adapter_name and s2.hash = s1.hash where s2.transaction_uid is null group by s1.adapter_name;"""</span></span>).format(sql.Identifier(self.schema_target), sql.Identifier(self.storage_table))</code> </pre><br>  Outputnya adalah laporan: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/5c/ou/gy/5cougys1gkflsplq2hvkoleooto.png"></div><br>  Periksa apakah semuanya benar pada gambar di atas.  Kami ingat bahwa 9917 dihapus dari tabel dalam database dan 10.022 baris diubah.  Total 19939 baris, yang terbukti dalam laporan. <br><br><h2>  Tabel ringkasan </h2><br>  Tetap hanya memasukkan transaksi "bersih" ke dalam tabel penyimpanan, yang bertepatan dalam semua hal (dengan hash) di adaptor yang berbeda.  Proses ini dilakukan oleh kueri SQL berikut: <br><br><pre> <code class="python hljs">sql_command = sql.SQL(<span class="hljs-string"><span class="hljs-string">""" with reconcil_data as ( select s1.transaction_uid from {0}.{1} s1 join {0}.{1} s2 on s2.transaction_uid = s1.transaction_uid and s2.adapter_name != s1.adapter_name where s2.hash = s1.hash and s1.adapter_name = 'postresql_adapter' ) insert into {2}.transaction_log select t.transaction_uid, t.account_uid, t.transaction_date, t.type_deal, t.transaction_amount from {3}.transaction_log t join reconcil_data r on t.transaction_uid = r.transaction_uid where not exists ( select 1 from {2}.transaction_log tl where tl.transaction_uid = t.transaction_uid ) """</span></span>).format(sql.Identifier(self.schema_target), sql.Identifier(self.storage_table), sql.Identifier(self.schema_db_clean), sql.Identifier(self.schema_raw))</code> </pre><br>  Tabel sementara yang kami gunakan sebagai penyimpanan data antara dari adaptor dapat dihapus. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uq/sr/te/uqsrte2g0thu2woasaxqojdbc88.png"></div><br><h2>  Kesimpulan </h2><br>  Dalam perjalanan pekerjaan yang dilakukan, sistem untuk merekonsiliasi data dari berbagai sumber dikembangkan: file teks dan tabel dalam database.  Minimal menggunakan alat tambahan. <br><br>  Mungkin pembaca yang canggih mungkin memperhatikan bahwa menggunakan kerangka kerja seperti Apache Spark, ditambah dengan mengubah data sumber ke format parket, dapat secara signifikan mempercepat proses ini, terutama untuk volume besar.  Tetapi tujuan utama dari pekerjaan ini adalah untuk menulis sebuah sistem dengan Python kosong dan untuk mempelajari pemrosesan data multi-pemrosesan.  Dengan apa yang kami, menurut saya, telah atasi. <br><br>  Kode sumber dari seluruh proyek terletak <a href="https://github.com/igorgorbenko/transact_reconciliation" rel="nofollow">di repositori saya di GitHub</a> , saya sarankan Anda membiasakan diri dengannya. <br><br>  Saya akan dengan senang hati menjawab semua pertanyaan dan berkenalan dengan komentar Anda. <br><br>  Semoga sukses! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id480076/">https://habr.com/ru/post/id480076/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id480062/index.html">10 sistem kontrol. Di mana lebih nyaman untuk berkomunikasi tentang tugas dan berbagi file?</a></li>
<li><a href="../id480064/index.html">Belajar kata-kata dikelompokkan secara tematis</a></li>
<li><a href="../id480068/index.html">[Perbarui] Orang-orang kami dipukuli, dan kami akan diam?</a></li>
<li><a href="../id480070/index.html">Bereaksi manfaat: Sebuah berkah untuk Bisnis?</a></li>
<li><a href="../id480072/index.html">Kubernetes: mengapa begitu penting untuk mengatur manajemen sumber daya sistem?</a></li>
<li><a href="../id480078/index.html">Perpustakaan front-end baru di React peripherals</a></li>
<li><a href="../id480080/index.html">Apa yang Anda butuhkan dalam mencatat aplikasi?</a></li>
<li><a href="../id480082/index.html">Menggunakan partisi di MySQL untuk Zabbix dengan sejumlah besar objek pemantauan</a></li>
<li><a href="../id480086/index.html">Cara mematuhi persyaratan 152-FZ, melindungi data pribadi pelanggan kami dan tidak menginjak kami</a></li>
<li><a href="../id480088/index.html">DevOps - OK, tapi apa yang harus dilakukan? Cara mengurangi tenaga kerja manual dan mencapai hasil yang diinginkan</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>