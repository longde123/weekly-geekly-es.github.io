<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü¶Ü ü§µüèº üßëüèæ‚Äçü§ù‚Äçüßëüèº Mensagens in√∫teis e sem bloqueio adiadas no MPI: an√°lise de luz e tutorial para aqueles que est√£o um pouco "no assunto" üêö üõÇ üêò</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Mais recentemente, tive que resolver outra tarefa trivial de treinamento do meu professor. No entanto, resolvendo isso, consegui chamar a aten√ß√£o para...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Mensagens in√∫teis e sem bloqueio adiadas no MPI: an√°lise de luz e tutorial para aqueles que est√£o um pouco "no assunto"</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/427219/">  Mais recentemente, tive que resolver outra tarefa trivial de treinamento do meu professor.  No entanto, resolvendo isso, consegui chamar a aten√ß√£o para coisas sobre as quais nunca havia pensado antes; talvez voc√™ tamb√©m n√£o tenha pensado nisso.  √â mais prov√°vel que este artigo seja √∫til para estudantes e para todos que iniciam sua jornada no mundo da programa√ß√£o paralela usando o MPI. <br><br><img src="https://habrastorage.org/webt/v0/ik/2-/v0ik2-rxhe1gexlrsumqpwhslbu.jpeg"><br><br><h2>  Nosso "dado:" </h2><br>  Portanto, a ess√™ncia de nossa tarefa essencialmente computacional √© comparar quantas vezes um programa que usa transfer√™ncias ponto a ponto atrasadas e sem bloqueio √© mais r√°pido do que aquele que usa transfer√™ncias ponto a ponto de bloqueio.  Realizaremos medi√ß√µes para matrizes de entrada das dimens√µes 64, 256, 1024, 4096, 8192, 16384, 65536, 262144, 1048576, 4194304, 16777216, 33554432 elementos.  Por padr√£o, prop√µe-se resolv√™-lo por quatro processos.  E aqui, de fato, √© o que consideraremos: <br><br><a name="habracut"></a><img src="https://habrastorage.org/webt/rt/vc/vo/rtvcvob3gfonidkax7qtskxfbc0.png"><cut></cut><br><br>  Na sa√≠da, devemos obter tr√™s vetores: Y1, Y2 e Y3, que o processo zero coletar√°.  Vou testar tudo isso no meu sistema com base em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">um processador Intel</a> com 16 GB de RAM.  Para desenvolver programas, usaremos a implementa√ß√£o do padr√£o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MPI da Microsoft vers√£o 9.0.1</a> (no momento da reda√ß√£o deste documento, √© relevante), o Visual Studio Community 2017 e n√£o o Fortran. <br><br><h2>  Material </h2><br>  Eu n√£o gostaria de descrever em detalhes como as fun√ß√µes MPI que ser√£o usadas funcionam. Voc√™ sempre pode <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">consultar a documenta√ß√£o para isso</a> , portanto, darei apenas uma breve vis√£o geral do que usaremos. <br><br><h4>  Bloqueio de troca </h4><br>  <b>Para bloquear mensagens ponto a ponto, usaremos as fun√ß√µes:</b> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MPI_Send</a> - implementa o bloqueio do envio de mensagens, ou seja,  depois de chamar a fun√ß√£o, o processo √© bloqueado at√© que os dados enviados sejam gravados da mem√≥ria no buffer interno do sistema MPI, ap√≥s o qual o processo continua a trabalhar mais; <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MPI_Recv</a> - executa o bloqueio da recep√ß√£o de mensagens, ou seja,  Ap√≥s chamar a fun√ß√£o, o processo √© bloqueado at√© que os dados do processo de envio cheguem e at√© que esses dados sejam completamente gravados no buffer do processo de recebimento pelo ambiente MPI. <br><br><h4>  Troca n√£o bloqueada diferida </h4><br>  <b>Para mensagens ponto a ponto sem bloqueio adiadas, usaremos as fun√ß√µes:</b> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MPI_Send_init</a> - em segundo plano prepara o ambiente para o envio de dados que ocorrer√£o em algum futuro e sem bloqueios; <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MPI_Recv_init</a> - esta fun√ß√£o funciona de maneira semelhante √† anterior, somente desta vez para receber dados; <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MPI_Start</a> - inicia o processo de recebimento ou transmiss√£o de uma mensagem, tamb√©m √© executado em segundo plano de a.k.a.  sem bloqueio; <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MPI_Wait</a> - √© usado para verificar e, se necess√°rio, aguardar a conclus√£o do envio ou recebimento de uma mensagem, mas apenas bloqueia o processo, se necess√°rio (se os dados "n√£o forem enviados" ou "n√£o recebidos").  Por exemplo, um processo deseja usar dados que ainda n√£o foram alcan√ßados - nada bom, portanto, inserimos o MPI_Wait na frente do local em que esses dados ser√£o necess√°rios (os inserimos mesmo que exista apenas um risco de corrup√ß√£o de dados).  Outro exemplo: o processo iniciou a transfer√™ncia de dados em segundo plano e, ap√≥s iniciar a transfer√™ncia, imediatamente come√ßou a alterar esses dados de alguma forma - nada bom, ent√£o inserimos MPI_Wait na frente do local no programa em que come√ßa a alterar esses dados (aqui tamb√©m os inserimos, mesmo que existe simplesmente um risco de corrup√ß√£o de dados). <br><br>  Assim, <i>semanticamente</i> , <i>a</i> sequ√™ncia de chamadas com uma troca n√£o bloqueada adiada √© a seguinte: <br><br><ol><li>  MPI_Send_init / MPI_Recv_init - preparando o ambiente para receber ou transmitir </li><li>  MPI_Start - inicia o processo de recebimento / transmiss√£o </li><li>  MPI_Wait - corremos o risco de danos (incluindo "envio insuficiente" e "subnotifica√ß√£o") de dados transmitidos ou recebidos </li></ol><br>  Tamb√©m usei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MPI_Startall</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MPI_Waitall</a> em meus programas de teste, o significado deles √© basicamente o mesmo que MPI_Start e MPI_Wait, respectivamente, apenas eles operam em v√°rios pacotes e / ou transmiss√µes.  Mas essa n√£o √© a lista completa das fun√ß√µes de in√≠cio e espera, existem v√°rias outras fun√ß√µes para verificar a conclus√£o das opera√ß√µes. <br><br><h2>  Arquitetura entre processos </h2><br>  Para maior clareza, constru√≠mos um gr√°fico para realizar c√°lculos por quatro processos.  Nesse caso, deve-se tentar distribuir todas as opera√ß√µes aritm√©ticas de vetor de maneira relativamente uniforme nos processos.  Aqui est√° o que eu tenho: <br><br><img src="https://habrastorage.org/webt/bq/_b/q4/bq_bq43yrgkgjayi8p5uy0g9ckk.png"><br><br>  Veja essas matrizes T0-T2?  Esses s√£o buffers para armazenar resultados intermedi√°rios de opera√ß√µes.  Al√©m disso, no gr√°fico ao enviar mensagens de um processo para outro, no in√≠cio da seta est√° o nome da matriz cujos dados s√£o transmitidos e no final da seta est√° a matriz que recebe esses dados. <br><br>  Bem, quando finalmente respondemos √†s perguntas: <br><br><ol><li>  Que tipo de problema estamos resolvendo? </li><li>  Quais ferramentas usaremos para resolv√™-lo? </li><li>  Como vamos resolver isso? </li></ol><br>  Resta apenas resolv√™-lo ... <br><br><h2>  Nossa "solu√ß√£o:" </h2><br>  A seguir, apresentarei os c√≥digos dos dois programas discutidos acima, mas, para come√ßar, darei mais algumas explica√ß√µes sobre o que e como. <br><br>  Tirei todas as opera√ß√µes aritm√©ticas de vetores em procedimentos separados (add, sub, mul, div) para aumentar a legibilidade do c√≥digo.  Todas as matrizes de entrada s√£o inicializadas de acordo com as f√≥rmulas que eu indiquei <i>quase</i> aleatoriamente.  Como o processo zero coleta os resultados do trabalho de todos os outros processos, portanto, funciona por mais tempo, portanto, √© l√≥gico considerar o tempo de seu trabalho igual ao tempo de execu√ß√£o do programa (como lembramos, estamos interessados ‚Äã‚Äãem: aritm√©tica + mensagens) no primeiro e no segundo casos.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Mediremos</a> os intervalos de tempo usando a fun√ß√£o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MPI_Wtime</a> e, ao mesmo tempo, decidi exibir qual resolu√ß√£o dos rel√≥gios que tenho l√° usando <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MPI_Wtick</a> (em algum lugar da minha alma espero que eles se encaixem no meu TSC invariante, neste caso, at√© estou pronto para perdo√°-los pelo erro associado ao hor√°rio em que a fun√ß√£o foi chamada MPI_Wtime).  Ent√£o, reuniremos tudo o que escrevi acima e, de acordo com o gr√°fico, finalmente desenvolveremos esses programas (e depuraremos, √© claro tamb√©m). <br><br><hr><br>  Quem se importa em ver o c√≥digo: <br><br><div class="spoiler">  <b class="spoiler_title">Programa com bloqueio de transfer√™ncias de dados</b> <div class="spoiler_text"><pre><code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"pch.h"</span></span></span><span class="hljs-meta"> #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;iostream&gt; #include &lt;iomanip&gt; #include &lt;fstream&gt; #include &lt;mpi.h&gt; using namespace std; void add(double *A, double *B, double *C, int n); void sub(double *A, double *B, double *C, int n); void mul(double *A, double *B, double *C, int n); void div(double *A, double *B, double *C, int n); int main(int argc, char **argv) { if (argc &lt; 2) { return 1; } int n = atoi(argv[1]); int rank; double start_time, end_time; MPI_Status status; double *A = new double[n]; double *B = new double[n]; double *C = new double[n]; double *D = new double[n]; double *E = new double[n]; double *G = new double[n]; double *T0 = new double[n]; double *T1 = new double[n]; double *T2 = new double[n]; for (int i = 0; i &lt; n; i++) { A[i] = double (2 * i + 1); B[i] = double(2 * i); C[i] = double(0.003 * (i + 1)); D[i] = A[i] * 0.001; E[i] = B[i]; G[i] = C[i]; } cout.setf(ios::fixed); cout &lt;&lt; fixed &lt;&lt; setprecision(9); MPI_Init(&amp;argc, &amp;argv); MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank); if (rank == 0) { start_time = MPI_Wtime(); sub(A, B, T0, n); MPI_Send(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD); MPI_Send(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD); div(T0, G, T1, n); MPI_Recv(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;status); add(T1, T2, T0, n); mul(T0, T1, T2, n); MPI_Recv(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;status); MPI_Send(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD); add(T0, T2, T1, n); MPI_Recv(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;status); MPI_Recv(T2, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;status); end_time = MPI_Wtime(); cout &lt;&lt; "Clock resolution: " &lt;&lt; MPI_Wtick() &lt;&lt; " secs" &lt;&lt; endl; cout &lt;&lt; "Thread " &lt;&lt; rank &lt;&lt; " execution time: " &lt;&lt; end_time - start_time &lt;&lt; endl; } if (rank == 1) { add(C, C, T0, n); MPI_Recv(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;status); MPI_Send(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD); mul(T1, G, T2, n); add(T2, C, T0, n); MPI_Recv(T1, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;status); MPI_Send(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD); sub(T1, T0, T2, n); MPI_Recv(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;status); add(T0, T2, T1, n); MPI_Send(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD); } if (rank == 2) { mul(C, C, T0, n); MPI_Recv(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;status); MPI_Recv(T2, n, MPI_DOUBLE, 3, 0, MPI_COMM_WORLD, &amp;status); MPI_Send(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD); MPI_Send(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD); add(T1, T2, T0, n); mul(T0, G, T1, n); MPI_Recv(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;status); mul(T1, T2, T0, n); MPI_Recv(T1, n, MPI_DOUBLE, 3, 0, MPI_COMM_WORLD, &amp;status); mul(T0, T1, T2, n); MPI_Send(T2, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD); } if (rank == 3) { mul(E, D, T0, n); MPI_Send(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD); sub(T0, B, T1, n); mul(T1, T1, T2, n); sub(T1, G, T0, n); mul(T0, T2, T1, n); MPI_Send(T1, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD); } MPI_Finalize(); delete[] A; delete[] B; delete[] C; delete[] D; delete[] E; delete[] G; delete[] T0; delete[] T1; delete[] T2; return 0; } void add(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] + B[i]; } } void sub(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] - B[i]; } } void mul(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] * B[i]; } } void div(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] / B[i]; } }</span></span></span></span></code> </pre> </div></div><br><div class="spoiler">  <b class="spoiler_title">Programa com transfer√™ncias de dados n√£o bloqueadas adiadas</b> <div class="spoiler_text"><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"pch.h"</span></span></span><span class="hljs-meta"> #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;iostream&gt; #include &lt;iomanip&gt; #include &lt;fstream&gt; #include &lt;mpi.h&gt; using namespace std; void add(double *A, double *B, double *C, int n); void sub(double *A, double *B, double *C, int n); void mul(double *A, double *B, double *C, int n); void div(double *A, double *B, double *C, int n); int main(int argc, char **argv) { if (argc &lt; 2) { return 1; } int n = atoi(argv[1]); int rank; double start_time, end_time; MPI_Request request[7]; MPI_Status statuses[4]; double *A = new double[n]; double *B = new double[n]; double *C = new double[n]; double *D = new double[n]; double *E = new double[n]; double *G = new double[n]; double *T0 = new double[n]; double *T1 = new double[n]; double *T2 = new double[n]; for (int i = 0; i &lt; n; i++) { A[i] = double(2 * i + 1); B[i] = double(2 * i); C[i] = double(0.003 * (i + 1)); D[i] = A[i] * 0.001; E[i] = B[i]; G[i] = C[i]; } cout.setf(ios::fixed); cout &lt;&lt; fixed &lt;&lt; setprecision(9); MPI_Init(&amp;argc, &amp;argv); MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank); if (rank == 0) { start_time = MPI_Wtime(); MPI_Send_init(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[0]);// MPI_Send_init(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[1]);// MPI_Recv_init(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[2]);// MPI_Recv_init(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[3]);// MPI_Send_init(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[4]);// MPI_Recv_init(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[5]);// MPI_Recv_init(T2, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[6]);// MPI_Start(&amp;request[2]); sub(A, B, T0, n); MPI_Startall(2, &amp;request[0]); div(T0, G, T1, n); MPI_Waitall(3, &amp;request[0], statuses); add(T1, T2, T0, n); mul(T0, T1, T2, n); MPI_Startall(2, &amp;request[3]); MPI_Wait(&amp;request[3], &amp;statuses[0]); add(T0, T2, T1, n); MPI_Startall(2, &amp;request[5]); MPI_Wait(&amp;request[4], &amp;statuses[0]); MPI_Waitall(2, &amp;request[5], statuses); end_time = MPI_Wtime(); cout &lt;&lt; "Clock resolution: " &lt;&lt; MPI_Wtick() &lt;&lt; " secs" &lt;&lt; endl; cout &lt;&lt; "Thread " &lt;&lt; rank &lt;&lt; " execution time: " &lt;&lt; end_time - start_time &lt;&lt; endl; } if (rank == 1) { MPI_Recv_init(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[0]);// MPI_Send_init(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[1]);// MPI_Recv_init(T1, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[2]);// MPI_Send_init(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[3]);// MPI_Recv_init(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[4]);// MPI_Send_init(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[5]);// MPI_Start(&amp;request[0]); add(C, C, T0, n); MPI_Start(&amp;request[1]); MPI_Wait(&amp;request[0], &amp;statuses[0]); mul(T1, G, T2, n); MPI_Start(&amp;request[2]); MPI_Wait(&amp;request[1], &amp;statuses[0]); add(T2, C, T0, n); MPI_Start(&amp;request[3]); MPI_Wait(&amp;request[2], &amp;statuses[0]); sub(T1, T0, T2, n); MPI_Wait(&amp;request[3], &amp;statuses[0]); MPI_Start(&amp;request[4]); MPI_Wait(&amp;request[4], &amp;statuses[0]); add(T0, T2, T1, n); MPI_Start(&amp;request[5]); MPI_Wait(&amp;request[5], &amp;statuses[0]); } if (rank == 2) { MPI_Recv_init(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[0]);// MPI_Recv_init(T2, n, MPI_DOUBLE, 3, 0, MPI_COMM_WORLD, &amp;request[1]);// MPI_Send_init(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[2]);// MPI_Send_init(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[3]);// MPI_Recv_init(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[4]);// MPI_Recv_init(T1, n, MPI_DOUBLE, 3, 0, MPI_COMM_WORLD, &amp;request[5]);// MPI_Send_init(T2, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[6]);// MPI_Startall(2, &amp;request[0]); mul(C, C, T0, n); MPI_Startall(2, &amp;request[2]); MPI_Waitall(4, &amp;request[0], statuses); add(T1, T2, T0, n); MPI_Start(&amp;request[4]); mul(T0, G, T1, n); MPI_Wait(&amp;request[4], &amp;statuses[0]); mul(T1, T2, T0, n); MPI_Start(&amp;request[5]); MPI_Wait(&amp;request[5], &amp;statuses[0]); mul(T0, T1, T2, n); MPI_Start(&amp;request[6]); MPI_Wait(&amp;request[6], &amp;statuses[0]); } if (rank == 3) { MPI_Send_init(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[0]); MPI_Send_init(T1, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[1]); mul(E, D, T0, n); MPI_Start(&amp;request[0]); sub(T0, B, T1, n); mul(T1, T1, T2, n); MPI_Wait(&amp;request[0], &amp;statuses[0]); sub(T1, G, T0, n); mul(T0, T2, T1, n); MPI_Start(&amp;request[1]); MPI_Wait(&amp;request[1], &amp;statuses[0]); } MPI_Finalize(); delete[] A; delete[] B; delete[] C; delete[] D; delete[] E; delete[] G; delete[] T0; delete[] T1; delete[] T2; return 0; } void add(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] + B[i]; } } void sub(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] - B[i]; } } void mul(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] * B[i]; } } void div(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] / B[i]; } }</span></span></span></span></code> </pre></div></div><br><hr><br><h2>  Teste e an√°lise </h2><br>  Vamos executar nossos programas para matrizes de tamanhos diferentes e ver o que acontece.  Os resultados do teste est√£o resumidos na tabela, na √∫ltima coluna da qual calculamos e escrevemos o coeficiente de acelera√ß√£o, que definimos da seguinte forma: K <sub>accele</sub> = T <sub>ex.</sub>  <sub>sem bloqueio.</sub>  <sub>Bloco</sub> / T. <br><br><img src="https://habrastorage.org/webt/ba/hg/zv/bahgzvsgz67vnkfsji-swsyy_cg.png"><br><br>  Se voc√™ observar esta tabela com um pouco mais de cuidado do que o habitual, notar√° que, com um aumento no n√∫mero de elementos processados, o coeficiente de acelera√ß√£o diminui de alguma forma da seguinte maneira: <br><br><img src="https://habrastorage.org/webt/qq/to/hv/qqtohvv7azbc05r6x4mwtfbbxfe.png"><br><br>  Vamos tentar determinar qual √© o problema?  Para fazer isso, proponho escrever um pequeno programa de teste que medir√° o tempo de cada opera√ß√£o aritm√©tica de vetor e reduzir√° cuidadosamente os resultados para um arquivo de texto comum. <br><br><hr><br>  Aqui, de fato, o pr√≥prio programa: <br><br><div class="spoiler">  <b class="spoiler_title">Medi√ß√£o do tempo</b> <div class="spoiler_text"><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"pch.h"</span></span></span><span class="hljs-meta"> #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;iostream&gt; #include &lt;iomanip&gt; #include &lt;Windows.h&gt; #include &lt;fstream&gt; using namespace std; void add(double *A, double *B, double *C, int n); void sub(double *A, double *B, double *C, int n); void mul(double *A, double *B, double *C, int n); void div(double *A, double *B, double *C, int n); int main() { struct res { double add; double sub; double mul; double div; }; int i, j, k, n, loop; LARGE_INTEGER start_time, end_time, freq; ofstream fout("test_measuring.txt"); int N[12] = { 64, 256, 1024, 4096, 8192, 16384, 65536, 262144, 1048576, 4194304, 16777216, 33554432 }; SetConsoleOutputCP(1251); cout &lt;&lt; "   loop: "; cin &gt;&gt; loop; fout &lt;&lt; setiosflags(ios::fixed) &lt;&lt; setiosflags(ios::right) &lt;&lt; setprecision(9); fout &lt;&lt; " : " &lt;&lt; loop &lt;&lt; endl; fout &lt;&lt; setw(10) &lt;&lt; "\n " &lt;&lt; setw(30) &lt;&lt; ".   (c)" &lt;&lt; setw(30) &lt;&lt; ".   (c)" &lt;&lt; setw(30) &lt;&lt; ".  (c)" &lt;&lt; setw(30) &lt;&lt; ".   (c)" &lt;&lt; endl; QueryPerformanceFrequency(&amp;freq); cout &lt;&lt; "\n : " &lt;&lt; freq.QuadPart &lt;&lt; " " &lt;&lt; endl; for (k = 0; k &lt; sizeof(N) / sizeof(int); k++) { res output = {}; n = N[k]; double *A = new double[n]; double *B = new double[n]; double *C = new double[n]; for (i = 0; i &lt; n; i++) { A[i] = 2.0 * i; B[i] = 2.0 * i + 1; C[i] = 0; } for (j = 0; j &lt; loop; j++) { QueryPerformanceCounter(&amp;start_time); add(A, B, C, n); QueryPerformanceCounter(&amp;end_time); output.add += double(end_time.QuadPart - start_time.QuadPart) / double(freq.QuadPart); QueryPerformanceCounter(&amp;start_time); sub(A, B, C, n); QueryPerformanceCounter(&amp;end_time); output.sub += double(end_time.QuadPart - start_time.QuadPart) / double(freq.QuadPart); QueryPerformanceCounter(&amp;start_time); mul(A, B, C, n); QueryPerformanceCounter(&amp;end_time); output.mul += double(end_time.QuadPart - start_time.QuadPart) / double(freq.QuadPart); QueryPerformanceCounter(&amp;start_time); div(A, B, C, n); QueryPerformanceCounter(&amp;end_time); output.div += double(end_time.QuadPart - start_time.QuadPart) / double(freq.QuadPart); } fout &lt;&lt; setw(10) &lt;&lt; n &lt;&lt; setw(30) &lt;&lt; output.add / loop &lt;&lt; setw(30) &lt;&lt; output.sub / loop &lt;&lt; setw(30) &lt;&lt; output.mul / loop &lt;&lt; setw(30) &lt;&lt; output.div / loop &lt;&lt; endl; delete[] A; delete[] B; delete[] C; } fout.close(); cout &lt;&lt; endl; system("pause"); return 0; } void add(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] + B[i]; } } void sub(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] - B[i]; } } void mul(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] * B[i]; } } void div(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] / B[i]; } }</span></span></span></span></code> </pre></div></div><br><hr><br>  Na inicializa√ß√£o, ele solicita que voc√™ insira o n√∫mero de ciclos de medi√ß√£o, testei por 10.000 ciclos.  Na sa√≠da, obtemos o resultado m√©dio para cada opera√ß√£o: <br><br><img src="https://habrastorage.org/webt/iz/0g/bs/iz0gbs8ilynlmbchxtga_0at61s.png"><br><br>  Para medir o tempo, usei o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">QueryPerformanceCounter de</a> alto n√≠vel.  Eu recomendo fortemente a leitura <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">desta FAQ,</a> para que a maioria das perguntas sobre a medi√ß√£o do tempo com esta fun√ß√£o desapare√ßa por conta pr√≥pria.  De acordo com minhas observa√ß√µes, ele se apega ao TSC (mas teoricamente pode n√£o ser o caso), mas retorna, de acordo com a ajuda, o n√∫mero atual de ticks do contador.  Mas o fato √© que meu contador fisicamente n√£o pode medir o intervalo de tempo de 32 ns (consulte a primeira linha da tabela de resultados).  Esse resultado se deve ao fato de que entre as duas chamadas dos ticks QueryPerformanceCounter 0. ou 1. Os ticks passam 1. Para a primeira linha da tabela, podemos concluir apenas que aproximadamente um ter√ßo dos 10.000 resultados s√£o iguais a 1 tick.  <i>Portanto, os dados nesta tabela para 64, 256 e at√© 1024 elementos s√£o algo bastante aproximados.</i>  Agora, vamos abrir qualquer um dos programas e calcular quantas opera√ß√µes totais de cada tipo encontramos, tradicionalmente "espalharemos" tudo de acordo com a pr√≥xima tabela: <br><br><img src="https://habrastorage.org/webt/et/vo/w2/etvow2fj-vxgtusc9aez__9egxq.png"><br><br>  Por fim, sabemos o tempo de cada opera√ß√£o aritm√©tica vetorial e quanto est√° em nosso programa, tentamos descobrir quanto tempo √© gasto nessas opera√ß√µes em programas paralelos e quanto tempo √© gasto no bloqueio e na troca de dados n√£o-bloqueados adiados entre processos e, novamente, por quest√£o de clareza, reduziremos isso para tabela: <br><br><img src="https://habrastorage.org/webt/no/k2/-0/nok2-0p1kpw8g1ahveqog4q9lzi.png"><br><br>  Com base nos resultados dos dados obtidos, constru√≠mos um gr√°fico de tr√™s fun√ß√µes: a primeira descreve a mudan√ßa no tempo gasto no bloqueio de transfer√™ncias entre processos, a partir do n√∫mero de elementos da matriz, a segunda descreve a mudan√ßa no tempo gasto em transfer√™ncias n√£o bloqueadas adiadas entre processos, no n√∫mero de elementos da matriz e a terceira descreve a altera√ß√£o no tempo, gastos em opera√ß√µes aritm√©ticas, a partir do n√∫mero de elementos de matrizes: <br><br><img src="https://habrastorage.org/webt/4e/w6/5h/4ew65htbb-s3vh7anlo0txafxpg.png"><br><br>  Como voc√™ j√° notou, a escala vertical do gr√°fico √© logar√≠tmica, √© uma medida necess√°ria, porque  a dispers√£o dos tempos √© muito grande e em um gr√°fico regular nada teria sido vis√≠vel.  Preste aten√ß√£o √† fun√ß√£o da depend√™ncia do tempo gasto em aritm√©tica no n√∫mero de elementos, ele ultrapassa com seguran√ßa as outras duas fun√ß√µes em cerca de 1 milh√£o de elementos.  O fato √© que ele cresce no infinito mais r√°pido que seus dois oponentes.  Portanto, com um aumento no n√∫mero de elementos processados, o tempo de execu√ß√£o dos programas √© cada vez mais determinado pela aritm√©tica, e n√£o pelas transfer√™ncias.  Suponha que voc√™ tenha aumentado o n√∫mero de transfer√™ncias entre processos; conceitualmente, voc√™ ver√° apenas que o momento em que a fun√ß√£o aritm√©tica ultrapassa as outras duas acontecer√° mais tarde. <br><br><h2>  Sum√°rio </h2><br>  Assim, continuando a aumentar o comprimento das matrizes, voc√™ chegar√° √† conclus√£o de que um programa com transfer√™ncias n√£o bloqueadas adiadas ser√° apenas um pouco mais r√°pido do que o que usa a troca de bloqueio.  E se voc√™ direcionar o comprimento das matrizes para o infinito (bem, ou apenas realizar matrizes muito longas), o tempo de opera√ß√£o do seu programa ser√° 100% determinado por c√°lculos, e o coeficiente de acelera√ß√£o tender√° a 1. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt427219/">https://habr.com/ru/post/pt427219/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt427207/index.html">Funcion√°rios da Rockstar defendem empresa ap√≥s cr√≠ticas por semanas de trabalho de 100 horas</a></li>
<li><a href="../pt427209/index.html">GeoPuzzle - fa√ßa o mundo pe√ßa por pe√ßa</a></li>
<li><a href="../pt427211/index.html">Electron √© um flash para desktop</a></li>
<li><a href="../pt427215/index.html">Os microsservi√ßos precisam crescer, n√£o come√ßar com eles</a></li>
<li><a href="../pt427217/index.html">An√°lise de desempenho de servidores WSGI: parte dois</a></li>
<li><a href="../pt427221/index.html">O que eu percebi no caminho para o meu sonho de intelig√™ncia artificial</a></li>
<li><a href="../pt427223/index.html">Qual √© a responsabilidade do desenvolvedor l√≠der</a></li>
<li><a href="../pt427225/index.html">Lan√ßamento do Oracle Database 18c XE</a></li>
<li><a href="../pt427227/index.html">Como fizemos um jogo de tabuleiro com controle remoto - Parte 2</a></li>
<li><a href="../pt427229/index.html">4 anos de programa de Gerenciamento de Projetos de Jogos</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>