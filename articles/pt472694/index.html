<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üö¥üèº üë©üèª‚Äçüé® üíÖ Mais do que Ceph: MCS Block Cloud Storage üëî üë®‚Äçüíº üí§</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Carrinho Voador, Afu Chan 

 Trabalho no Mail.ru Cloud Solutons como arquiteto e desenvolvedor, incluindo minha nuvem. Sabe-se que uma infraestrutura ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Mais do que Ceph: MCS Block Cloud Storage</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/472694/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/wx/av/di/wxavdimhgftb4rj-bjzsjjbdwts.jpeg"></div> <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Carrinho Voador, Afu Chan</a></i> <br><br>  Trabalho no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Mail.ru Cloud Solutons como</a> arquiteto e desenvolvedor, incluindo minha nuvem.  Sabe-se que uma infraestrutura de nuvem distribu√≠da precisa de um armazenamento em bloco produtivo, do qual depende a opera√ß√£o dos servi√ßos e solu√ß√µes PaaS criados usando eles. <br><br>  Inicialmente, ao implantar essa infraestrutura, usamos apenas Ceph, mas gradualmente o armazenamento em bloco evoluiu.  Quer√≠amos que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">nossos bancos de dados</a> , armazenamento de arquivos e v√°rios servi√ßos funcionassem com o desempenho m√°ximo, por isso adicionamos armazenamentos localizados e configuramos o monitoramento avan√ßado do Ceph. <br><br>  Vou contar como foi - talvez essa hist√≥ria, os problemas que encontramos e nossas solu√ß√µes sejam √∫teis para quem tamb√©m usa o Ceph.  A prop√≥sito, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui est√° uma</a> vers√£o em v√≠deo deste relat√≥rio. <br><a name="habracut"></a><br><h2>  Dos processos do DevOps √† sua pr√≥pria nuvem </h2><br>  As pr√°ticas de DevOps visam implantar o produto o mais r√°pido poss√≠vel: <br><br><ul><li>  Automa√ß√£o de processos - todo o ciclo de vida: montagem, teste, entrega ao teste e produtivo.  Automatize os processos gradualmente, come√ßando com pequenas etapas. <br></li><li>  A infraestrutura como um c√≥digo √© um modelo quando o processo de configura√ß√£o da infraestrutura √© semelhante ao processo de programa√ß√£o de software.  Primeiro eles testam o produto, o produto possui certos requisitos para a infraestrutura e a infraestrutura precisa ser testada.  Nesta fase, com o desejo de que ela apare√ßa, eu quero ‚Äúajustar‚Äù a infraestrutura - primeiro no ambiente de teste, depois no supermercado.  No primeiro est√°gio, isso pode ser feito manualmente, mas eles passam para a automa√ß√£o - para o modelo "infraestrutura como c√≥digo". <br></li><li>  Virtualiza√ß√£o e cont√™ineres - aparecem na empresa quando fica claro que voc√™ precisa colocar processos em uma trilha industrial, implantar novos recursos mais rapidamente, com o m√≠nimo de interven√ß√£o manual. <br></li></ul><br><img src="https://habrastorage.org/webt/wz/im/rw/wzimrwndijvqdzrmn6pjlunr_ci.jpeg">  <i>A arquitetura de todos os ambientes virtuais √© semelhante: m√°quinas convidadas com cont√™ineres, aplicativos, redes p√∫blicas e privadas, armazenamento.</i> <br><br>  Gradualmente, mais e mais servi√ßos s√£o implantados na infraestrutura virtual integrada nos processos DevOps e o ambiente virtual est√° se tornando n√£o apenas um teste (usado para desenvolvimento e teste), mas tamb√©m produtivo. <br><br>  Como regra, nos est√°gios iniciais, eles s√£o ignorados pelas ferramentas b√°sicas de automa√ß√£o mais simples.  Por√©m, √† medida que novas ferramentas s√£o atra√≠das, mais cedo ou mais tarde, √© necess√°rio implantar uma plataforma em nuvem completa para usar as ferramentas mais avan√ßadas como a Terraform. <br><br>  Nesse est√°gio, a infraestrutura virtual de "hipervisores, redes e armazenamento" se transforma em uma infraestrutura de nuvem completa com ferramentas e componentes desenvolvidos para orquestrar processos.  Em seguida, aparece a pr√≥pria nuvem, na qual ocorrem os processos de teste e entrega automatizada de atualiza√ß√µes dos servi√ßos existentes e a implanta√ß√£o de novos servi√ßos. <br><br>  A segunda maneira de sua pr√≥pria nuvem √© a necessidade de n√£o depender de recursos externos e provedores de servi√ßos externos, ou seja, fornecendo alguma independ√™ncia t√©cnica para seus pr√≥prios servi√ßos. <br><br><img src="https://habrastorage.org/webt/5y/wb/pm/5ywbpmuwrruww-bfep4f34p4eja.jpeg">  <i>A primeira nuvem parece quase uma infraestrutura virtual - um hipervisor (um ou v√°rios), m√°quinas virtuais com cont√™ineres, armazenamento compartilhado: se voc√™ construir a nuvem n√£o em solu√ß√µes propriet√°rias, geralmente √© Ceph ou DRBD.</i> <br><br><h2>  Resili√™ncia e desempenho da nuvem privada </h2><br>  A nuvem est√° crescendo, os neg√≥cios dependem cada vez mais, a empresa come√ßa a exigir maior confiabilidade. <br><br>  Aqui, a distribui√ß√£o √© adicionada √† nuvem privada, a infraestrutura de nuvem distribu√≠da aparece: pontos adicionais em que o equipamento est√° localizado.  A nuvem gerencia duas, tr√™s ou mais instala√ß√µes constru√≠das para fornecer uma solu√ß√£o tolerante a falhas. <br><br>  Ao mesmo tempo, s√£o necess√°rios dados de todos os sites, e h√° um problema: em um site n√£o h√° grandes atrasos na transfer√™ncia de dados, mas entre sites, os dados s√£o transmitidos mais lentamente. <br><br><img src="https://habrastorage.org/webt/q6/__/3m/q6__3mcgjw-wsknmxy-etas8oky.jpeg">  <i>Sites de instala√ß√£o e armazenamento comum.</i>  <i>Ret√¢ngulos vermelhos s√£o gargalos no n√≠vel da rede.</i> <br><br>  A parte externa da infraestrutura do ponto de vista da rede de gerenciamento ou da rede p√∫blica n√£o est√° t√£o ocupada, mas na rede interna os volumes de dados transferidos s√£o muito maiores.  E em sistemas distribu√≠dos, os problemas come√ßam, expressos em um longo tempo de servi√ßo.  Se o cliente chegar a um grupo de n√≥s de armazenamento, os dados dever√£o ser replicados instantaneamente para o segundo grupo para que as altera√ß√µes n√£o sejam perdidas. <br><br>  Para alguns processos, a lat√™ncia de replica√ß√£o de dados √© aceit√°vel, mas em casos como o processamento de transa√ß√µes, as transa√ß√µes n√£o podem ser perdidas.  Se a replica√ß√£o ass√≠ncrona for usada, ocorrer√° um intervalo de tempo que poder√° levar √† perda de parte dos dados, se uma das "falhas" do sistema de armazenamento (sistema de armazenamento de dados) falhar.  Se a replica√ß√£o s√≠ncrona for usada, o tempo de servi√ßo aumentar√°. <br><br>  Tamb√©m √© bastante natural que, quando o tempo de processamento (lat√™ncia) do armazenamento aumentar, os bancos de dados comecem a ficar mais lentos e houver efeitos negativos que precisam ser combatidos. <br><br>  Em nossa nuvem, buscamos solu√ß√µes equilibradas para manter a confiabilidade e o desempenho.  A t√©cnica mais simples √© localizar os dados - e adicionamos clusters Ceph localizados adicionais. <br><br><img src="https://habrastorage.org/webt/qf/_g/k3/qf_gk33i3esefh0xgnjuhegczia.jpeg">  <i>A cor verde indica clusters Ceph localizados adicionais.</i> <br><br>  A vantagem de uma arquitetura t√£o complexa √© que aqueles que precisam de entrada / sa√≠da r√°pida de dados podem usar armazenamentos localizados.  Os dados para os quais a disponibilidade total √© cr√≠tica em dois sites est√£o em um cluster distribu√≠do.  Funciona mais devagar - mas os dados s√£o replicados nos dois sites.  Se seu desempenho n√£o for suficiente, voc√™ poder√° usar clusters Ceph localizados. <br><br>  A maioria das nuvens p√∫blicas e privadas acaba chegando ao mesmo padr√£o de trabalho, quando, dependendo dos requisitos, a carga √© implantada em diferentes tipos de armazenamento (tipos diferentes de discos). <br><br><h2>  Diagn√≥stico ceph: como criar monitoramento </h2><br>  Quando implantamos e lan√ßamos a infraestrutura, era hora de garantir seu funcionamento, minimizar o tempo e o n√∫mero de falhas.  Portanto, o pr√≥ximo passo no desenvolvimento da infraestrutura foi a constru√ß√£o de diagn√≥sticos e monitoramento. <br><br>  Considere toda a tarefa de monitoramento - temos uma pilha de aplicativos em um ambiente de nuvem virtual: um aplicativo, um sistema operacional convidado, um dispositivo de bloco, os drivers desse dispositivo de bloco em um hipervisor, uma rede de armazenamento e o sistema de armazenamento real (sistema de armazenamento).  E tudo isso ainda n√£o foi coberto pelo monitoramento. <br><br><img src="https://habrastorage.org/webt/z1/nk/nk/z1nknkrnannwnfn2fa7jwf6w0jq.jpeg">  <i>Elementos n√£o cobertos pelo monitoramento.</i> <br><br>  O monitoramento √© implementado em v√°rias etapas, come√ßamos com discos.  Obtemos o n√∫mero de opera√ß√µes de leitura / grava√ß√£o, com certa precis√£o, o tempo de servi√ßo (megabytes por segundo), a profundidade da fila e outras caracter√≠sticas, e tamb√©m coletamos a SMART sobre o estado dos discos. <br><br><img src="https://habrastorage.org/webt/2m/b_/7s/2mb_7s2vda6qq6dtem9iwsnlup8.jpeg">  <i>A primeira etapa: cobrimos os discos de monitoramento.</i> <br><br>  O monitoramento de disco n√£o √© suficiente para obter uma imagem completa do que est√° acontecendo no sistema.  Portanto, passamos a monitorar um elemento cr√≠tico da infraestrutura - a rede do sistema de armazenamento.  Na verdade, existem dois deles - o cluster interno e o cliente, que conecta os clusters de armazenamento aos hipervisores.  Aqui temos as taxas de transfer√™ncia de pacotes de dados (megabytes por segundo, pacotes por segundo), o tamanho das filas de rede, buffers e possivelmente caminhos de dados. <br><br><img src="https://habrastorage.org/webt/pn/dd/wu/pnddwuxah0r9sngg8awzcxhzlom.jpeg">  <i>Segunda etapa: monitoramento de rede.</i> <br><br>  Eles costumam parar nisso, mas isso n√£o pode ser feito, porque a maior parte da infraestrutura ainda n√£o foi fechada pelo monitoramento. <br><br>  Todo o armazenamento distribu√≠do usado em nuvens p√∫blicas e privadas √© SDS, armazenamento definido por software.  Eles podem ser implementados nas solu√ß√µes de um fornecedor espec√≠fico, solu√ß√µes de c√≥digo aberto, voc√™ pode fazer algo usando uma pilha de tecnologias familiares.  Mas √© sempre SDS, e o trabalho dessas partes do software deve ser monitorado. <br><br><img src="https://habrastorage.org/webt/mt/o5/cb/mto5cbnz456tvs6mg_6i-b5lm8o.jpeg">  <i>Terceira etapa: monitorando o daemon de armazenamento.</i> <br><br>  A maioria dos operadores do Ceph usa dados coletados dos daemons de monitoramento e controle do Ceph (monitor e gerente, tamb√©m conhecido como mgr).  Inicialmente, seguimos o mesmo caminho, mas rapidamente percebemos que essas informa√ß√µes n√£o eram suficientes - os avisos sobre solicita√ß√µes suspensas aparecem atrasados: a solicita√ß√£o foi interrompida por 30 segundos e s√≥ ent√£o a vimos.  Enquanto se trata de monitoramento, enquanto o monitoramento dispara o alarme, pelo menos tr√™s minutos se passam.  Na melhor das hip√≥teses, isso significa que parte do armazenamento e aplicativos ficar√° inativa por tr√™s minutos. <br><br>  Naturalmente, decidimos expandir o monitoramento e passamos ao elemento principal do Ceph - o daemon OSD.  Ao monitorar o daemon de armazenamento de objetos, obtemos o tempo aproximado de opera√ß√£o conforme o OSD o v√™, al√©m de estat√≠sticas sobre solicita√ß√µes suspensas - quem, quando, em que PG e por quanto tempo. <br><br><h2>  Por que apenas Ceph n√£o √© suficiente e o que fazer sobre isso </h2><br>  O Ceph por si s√≥ n√£o √© suficiente por v√°rias raz√µes.  Por exemplo, temos um cliente com um perfil de banco de dados.  Ele implantou todos os bancos de dados no cluster all-flash, a lat√™ncia das opera√ß√µes que foram emitidas no local lhe convinha, no entanto, houve queixas de tempo de inatividade. <br><br>  O sistema de monitoramento n√£o permite que voc√™ veja o que est√° acontecendo dentro dos clientes do ambiente virtual.  Como resultado, para identificar o problema, usamos a an√°lise avan√ßada, que foi solicitada usando o utilit√°rio blktrace de sua m√°quina virtual. <br><br><img src="https://habrastorage.org/webt/uj/ch/vk/ujchvkl3ozoarbjooedieuqzf80.jpeg">  <i>O resultado de uma an√°lise estendida.</i> <br><br>  Os resultados da an√°lise cont√™m opera√ß√µes marcadas com os sinalizadores W e WS.  O sinalizador W √© um registro, o sinalizador WS √© um registro s√≠ncrono, aguardando o dispositivo concluir a opera√ß√£o.  Quando trabalhamos com bancos de dados, quase todos os bancos de dados SQL t√™m um gargalo - WAL (log write-ahead). <br><br>  O banco de dados sempre grava primeiro os dados no log, recebe a confirma√ß√£o do disco com buffers de libera√ß√£o e, em seguida, grava os dados no pr√≥prio banco de dados.  Se ela n√£o recebeu a confirma√ß√£o de uma redefini√ß√£o de buffer, ela acredita que uma redefini√ß√£o de energia pode apagar uma transa√ß√£o confirmada pelo cliente.  Isso √© inaceit√°vel para o banco de dados, por isso ele exibe "write SYNC / FLUSH" e depois grava os dados.  Quando os logs est√£o cheios, ocorre a troca e tudo o que entra no cache da p√°gina tamb√©m √© piscado √† for√ßa.  <i>Adicionado: n√£o h√° redefini√ß√£o na pr√≥pria imagem - ou seja, opera√ß√µes com o sinalizador de pr√©-descarga.</i>  <i>Eles se parecem com o FWS - pr√©-libera√ß√£o + grava√ß√£o + sincroniza√ß√£o ou FWSF - pr√©-libera√ß√£o + grava√ß√£o + sincroniza√ß√£o + FUA</i> <br><br>  Quando um cliente tem muitas transa√ß√µes pequenas, praticamente todas as suas E / S se transformam em uma cadeia seq√ºencial: write-flush - write-flush.  Como voc√™ n√£o pode fazer algo com o banco de dados, come√ßamos a trabalhar com o sistema de armazenamento.  Neste momento, entendemos que os recursos do Ceph n√£o s√£o suficientes. <br><br>  Para n√≥s, nesse est√°gio, a melhor solu√ß√£o era adicionar reposit√≥rios locais pequenos e r√°pidos que n√£o foram implementados usando as ferramentas Ceph (basicamente esgotamos seus recursos).  E transformamos o armazenamento em nuvem em algo mais que o Ceph.  No nosso caso, adicionamos muitas hist√≥rias locais (local em termos do data center, n√£o o hipervisor). <br><br><img src="https://habrastorage.org/webt/4o/hh/rr/4ohhrre0loyyk4gmxijlpwy6pry.jpeg">  <i>Reposit√≥rios localizados adicionais Destino A e B.</i> <br><br>  O tempo de servi√ßo desse armazenamento local √© de cerca de 0,3 ms por fluxo.  Se estiver em outro datacenter, funcionar√° mais lentamente - com um desempenho de aproximadamente 0,7 ms.  Esse √© um aumento significativo em compara√ß√£o com o Ceph, que produz 1,2 ms e distribu√≠do pelos data centers - 2 ms.  O desempenho dessas pequenas f√°bricas, das quais temos mais de uma d√∫zia, √© de cerca de 100 mil por m√≥dulo, 100 mil IOPS por registro. <br><br>  Ap√≥s essa altera√ß√£o na infraestrutura, nossa nuvem reduz menos de um milh√£o de IOPS para grava√ß√£o ou cerca de dois a tr√™s milh√µes de IOPS para leitura no total de todos os clientes: <br><br><img src="https://habrastorage.org/webt/hw/zq/aj/hwzqajyzsmewq-s2h6iw8mnf4bk.jpeg"><br><br>  √â importante observar que esse tipo de armazenamento n√£o √© o principal m√©todo de expans√£o, fazemos a aposta principal no Ceph e a presen√ßa de armazenamento r√°pido √© importante apenas para servi√ßos que exigem tempo de resposta do disco. <br><br><h2>  Novas itera√ß√µes: aprimoramentos de c√≥digo e infraestrutura </h2><br>  Todas as nossas hist√≥rias s√£o recursos compartilhados.  Essa infraestrutura exige que <b>implementemos uma pol√≠tica de n√≠vel de servi√ßo</b> : devemos fornecer um certo n√≠vel de servi√ßo e n√£o permitir que um cliente interfira no outro por acidente ou de prop√≥sito, desativando o armazenamento. <br><br>  Para fazer isso, tivemos que fazer a finaliza√ß√£o e a implanta√ß√£o n√£o trivial - entrega iterativa ao produtivo. <br><br>  Esse lan√ßamento foi diferente das pr√°ticas usuais do DevOps, quando todos os processos: montagem, teste, lan√ßamento de c√≥digo, servi√ßo de reinicializa√ß√£o, se necess√°rio, come√ßam com um clique de um bot√£o e tudo funciona.  Se voc√™ implementar pr√°ticas de DevOps na infraestrutura, ela permanecer√° at√© o primeiro erro. <br><br>  √â por isso que a ‚Äúautoma√ß√£o total‚Äù n√£o se enraizou particularmente na equipe de infraestrutura.  Obviamente, existe uma certa abordagem para automa√ß√£o de testes e entrega - mas ela √© sempre controlada e a entrega √© iniciada pelos engenheiros de SRE da equipe em nuvem. <br><br>  Implementamos altera√ß√µes em v√°rios servi√ßos: no back-end do Cinder, no front-end do Cinder (cliente do Cinder) e no servi√ßo Nova.  As altera√ß√µes foram aplicadas em v√°rias itera√ß√µes - uma itera√ß√£o por vez.  Ap√≥s a terceira itera√ß√£o, as altera√ß√µes correspondentes foram aplicadas √†s m√°quinas convidadas dos clientes: algu√©m migrou, algu√©m reiniciou a VM (reinicializa√ß√£o for√ßada) ou migra√ß√£o planejada para atender os hipervisores. <br><br>  O pr√≥ximo problema que surgiu √© o <b>salto na velocidade de grava√ß√£o</b> .  Quando trabalhamos com armazenamento conectado √† rede, o hipervisor padr√£o considera a rede lenta e, portanto, armazena em cache todos os dados.  Ele escreve rapidamente, at√© v√°rias dezenas de megabytes, e depois come√ßa a liberar o cache.  Houve muitos momentos desagrad√°veis ‚Äã‚Äãpor causa de tais saltos. <br><br>  Descobrimos que se voc√™ ativar o cache, o desempenho do SSD diminui em 15% e se voc√™ desativar o cache, o desempenho do HDD diminui em 35%.  Foi preciso outro desenvolvimento, implementado o gerenciamento de cache gerenciado, quando o armazenamento em cache √© explicitamente designado para cada tipo de disco.  Isso nos permitiu dirigir SSD sem cache e HDD - com um cache, como resultado, paramos de perder desempenho. <br><br>  A pr√°tica de entregar desenvolvimento a um produtivo √© semelhante - itera√ß√µes.  Implementamos o c√≥digo, reiniciamos o daemon e, conforme necess√°rio, reinicie ou migre m√°quinas virtuais convidadas, que devem estar sujeitas a altera√ß√µes.  A VM do cliente migrou do HDD, seu cache ativado - tudo funciona ou, pelo contr√°rio, o cliente migrou com SSD, seu cache desativado - tudo funciona. <br><br>  O terceiro problema √© a <b>opera√ß√£o incorreta de m√°quinas virtuais implantadas das imagens GOLD no HDD</b> . <br><br>  Existem muitos clientes, e a peculiaridade da situa√ß√£o √© que o trabalho da VM foi ajustado por si s√≥: o problema foi garantido durante a implanta√ß√£o, mas foi resolvido enquanto o cliente alcan√ßava o suporte t√©cnico.  Inicialmente, pedimos aos clientes que esperassem meia hora at√© a VM estar estabilizada, mas depois come√ßamos a trabalhar na qualidade do servi√ßo. <br><br>  No processo de pesquisa, percebemos que os recursos de nossa infraestrutura de monitoramento ainda n√£o s√£o suficientes. <br><br><img src="https://habrastorage.org/webt/bl/tj/ft/bltjftnwasbvxd_iep-wwfsepkw.jpeg">  <i>O monitoramento fechou a parte azul e o problema estava no topo da infraestrutura, n√£o coberto pelo monitoramento.</i> <br><br>  Come√ßamos a lidar com o que est√° acontecendo na parte da infraestrutura que n√£o era coberta pelo monitoramento.  Para fazer isso, usamos os diagn√≥sticos avan√ßados do Ceph (ou melhor, uma das variedades do cliente Ceph - librbd).  Utilizando ferramentas de automa√ß√£o, fizemos altera√ß√µes na configura√ß√£o do cliente Ceph para acessar estruturas de dados internas por meio do soquete do dom√≠nio Unix e come√ßamos a coletar estat√≠sticas dos clientes Ceph no hipervisor. <br><br>  O que vimos?  N√£o vimos estat√≠sticas no cluster Ceph / OSD / cluster, mas estat√≠sticas em cada disco da m√°quina virtual do cliente cujos discos estavam no Ceph - ou seja, estat√≠sticas associadas ao dispositivo. <br><br><img src="https://habrastorage.org/webt/us/hr/5j/ushr5jfx40pvedrcqwpvj9ai-qy.jpeg">  <i>Resultados avan√ßados de estat√≠sticas de monitoramento.</i> <br><br>  Foram as estat√≠sticas expandidas que deixaram claro que o problema ocorre apenas em discos clonados de outros discos. <br><br>  Em seguida, analisamos as estat√≠sticas das opera√ß√µes, em particular as opera√ß√µes de leitura e grava√ß√£o.  Descobriu-se que a carga nas imagens de n√≠vel superior √© relativamente pequena e, nas iniciais, de onde o clone prov√©m, √© grande, mas sem equil√≠brio: uma grande quantidade de leitura sem grava√ß√£o. <br><br>  O problema est√° localizado, agora √© necess√°ria uma solu√ß√£o - c√≥digo ou infraestrutura? <br><br>  Nada pode ser feito com o c√≥digo Ceph, √© "dif√≠cil".  Al√©m disso, a seguran√ßa dos dados do cliente depende disso.  Mas h√° um problema, ele deve ser resolvido e alteramos a arquitetura do reposit√≥rio.  O cluster do HDD se transformou em um cluster h√≠brido - uma certa quantidade de SSD foi adicionada ao HDD; as prioridades dos daemons do OSD foram alteradas para que o SSD estivesse sempre em prioridade e se tornasse o OSD principal dentro do grupo de posicionamento (PG). <br><br>  Agora, quando o cliente implanta a m√°quina virtual a partir do disco clonado, suas opera√ß√µes de leitura v√£o para o SSD.  Como resultado, a recupera√ß√£o do disco tornou-se r√°pida e apenas os dados do cliente, exceto a imagem original, s√£o gravados no disco r√≠gido.  Recebemos um aumento de tr√™s vezes na produtividade quase gratuitamente (em rela√ß√£o ao custo inicial da infraestrutura). <br><br><h1>  Por que o monitoramento da infraestrutura √© importante </h1><br><ol><li>  A infraestrutura de monitoramento deve ser inclu√≠da ao m√°ximo em toda a pilha, come√ßando com a m√°quina virtual e terminando com o disco.  Afinal, enquanto um cliente que usa uma nuvem p√∫blica ou privada acessa sua infraestrutura e fornece as informa√ß√µes necess√°rias, o problema muda ou muda para outro local. <br></li><li>  O monitoramento de todo o hipervisor, m√°quina virtual ou cont√™iner "em sua totalidade" produz quase nada.  Tentamos entender do tr√°fego de rede o que est√° acontecendo com o Ceph - √© in√∫til, os dados voam em alta velocidade (de 500 megabytes por segundo), √© extremamente dif√≠cil selecionar os necess√°rios.  √â necess√°rio um volume monstruoso de discos para armazenar essas estat√≠sticas e muito tempo para analis√°-las. <br></li><li>       ,     - .   :     ,           ,   ‚Äî     ,        . <br></li><li>   ‚Äî     .   ,   .   ‚Äî     ,      .        ,   .             ‚Äî    ,   ,      . <br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cloud MCS Cloud Solutions √© uma infraestrutura cujas decis√µes de evolu√ß√£o s√£o tomadas em grande parte com base nos dados acumulados pelo monitoramento. </font><font style="vertical-align: inherit;">Melhoramos o monitoramento e usamos seus dados para melhorar o n√≠vel de servi√ßo dos clientes.</font></font><br></li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt472694/">https://habr.com/ru/post/pt472694/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt472682/index.html">Retardando o envelhecimento com sinergias medicamentosas em C. elegans</a></li>
<li><a href="../pt472684/index.html">Surpresa fsync () PostgreSQL</a></li>
<li><a href="../pt472686/index.html">Est√∫dio de V√≠deo Baseado no i486</a></li>
<li><a href="../pt472688/index.html">Como funciona a renderiza√ß√£o de jogos em 3D: processamento de v√©rtices</a></li>
<li><a href="../pt472690/index.html">O que h√° de novo no Zabbix 4.4</a></li>
<li><a href="../pt472702/index.html">JH Rainwater ‚ÄúComo pastar gatos‚Äù: ra√ßas de programadores e caracter√≠sticas de sua cria√ß√£o</a></li>
<li><a href="../pt472708/index.html">Imperva revelou detalhes t√©cnicos do Cloud WAF hack</a></li>
<li><a href="../pt472714/index.html">Onde o trabalhador front-end deve procurar trabalho e n√£o cair: Telegrama, Slack e n√£o apenas</a></li>
<li><a href="../pt472716/index.html">Obtendo o Spring Bean de um contexto de aplicativo de terceiros corretamente</a></li>
<li><a href="../pt472720/index.html">ERP n√£o funciona ... Qual √© a alternativa? ou na hora certa. Para a R√∫ssia?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>