<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëî üôãüèª üåø Pixel 3 aprende a determinar la profundidad en fotos ü§úüèø üôãüèΩ ü•Ç</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El modo retrato en los tel√©fonos inteligentes Pixel le permite tomar fotos de aspecto profesional que llaman la atenci√≥n sobre el sujeto con desenfoqu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Pixel 3 aprende a determinar la profundidad en fotos</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/433600/"> El modo retrato en los tel√©fonos inteligentes Pixel le permite tomar fotos de aspecto profesional que llaman la atenci√≥n sobre el sujeto con desenfoque del fondo.  El a√±o pasado, describimos c√≥mo calculamos la profundidad usando una c√°mara √∫nica y el enfoque autom√°tico de detecci√≥n de fase (enfoque autom√°tico de detecci√≥n de fase, PDAF), tambi√©n conocido como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enfoque autom√°tico de doble p√≠xel</a> .  Este proceso utiliz√≥ un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">algoritmo est√©reo tradicional</a> sin entrenamiento.  Este a√±o en Pixel 3, adoptamos el aprendizaje autom√°tico para mejorar la evaluaci√≥n de profundidad y producir resultados a√∫n mejores en modo vertical. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/505/531/899/505531899e63adc78fbd74d94f1c3a3a.gif"><br>  <i>Izquierda: la imagen original capturada en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">HDR +</a> .</i>  <i>A la derecha hay una comparaci√≥n de los resultados de disparo en modo vertical usando la profundidad del est√©reo tradicional y el aprendizaje autom√°tico.</i>  <i>Los resultados del aprendizaje producen menos errores.</i>  <i>En el resultado est√©reo tradicional, la profundidad de muchas l√≠neas horizontales detr√°s del hombre se estima incorrectamente igual a la profundidad del hombre mismo, como resultado de lo cual permanecen n√≠tidas.</i> <br><a name="habracut"></a><br><h2>  Una breve excursi√≥n al material anterior. </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">El a√±o pasado,</a> describimos que el modo retrato usa una red neuronal para separar los p√≠xeles que pertenecen a las im√°genes de las personas y una imagen de fondo, y complementa esta m√°scara de dos niveles con informaci√≥n de profundidad derivada de los p√≠xeles PDAF.  Todo esto se hizo para obtener desenfoque, dependiendo de la profundidad, cerca de lo que puede ofrecer una c√°mara profesional. <br><br>  Para trabajar, la PDAF toma dos tomas ligeramente diferentes de la escena.  Al cambiar de imagen, puede ver que la persona no se mueve y el fondo se mueve horizontalmente; este efecto se llama <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">paralaje</a> .  Dado que el paralaje es una funci√≥n de la distancia de un punto desde la c√°mara y la distancia entre dos puntos de vista, podemos determinar la profundidad comparando cada punto en una imagen con su punto correspondiente en otra. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e15/41c/044/e1541c044baa5454ddee71ef45c9a96c.gif"><br>  <i>Las im√°genes PDAF a la izquierda y en el centro se ven similares, pero se puede ver paralaje en el fragmento ampliado a la derecha.</i>  <i>Es m√°s f√°cil notarlo por la estructura redonda en el centro de aumento.</i> <br><br>  Sin embargo, encontrar tales correspondencias en las im√°genes PDAF (este m√©todo se llama profundidad est√©reo) es una tarea extremadamente dif√≠cil, ya que los puntos entre las fotos se mueven muy d√©bilmente.  Adem√°s, todas las tecnolog√≠as est√©reo sufren problemas de apertura.  Si observa la escena a trav√©s de una peque√±a abertura, no ser√° posible encontrar la correspondencia de puntos para l√≠neas paralelas a la l√≠nea de base est√©reo, es decir, la l√≠nea que conecta las dos c√°maras.  En otras palabras, cuando se estudian l√≠neas horizontales en la foto presentada (o l√≠neas verticales en im√°genes con orientaci√≥n vertical), todos los cambios en una imagen en relaci√≥n con otra se ven aproximadamente iguales.  En el modo de retrato del a√±o pasado, todos estos factores podr√≠an conducir a errores en la determinaci√≥n de la profundidad y la aparici√≥n de artefactos desagradables. <br><br><h2>  Mejora de la evaluaci√≥n de profundidad </h2><br>  Con el modo retrato de Pixel 3, solucionamos estos errores usando el hecho de que el paralaje de las fotos est√©reo es solo una de las muchas pistas en las im√°genes.  Por ejemplo, los puntos que est√°n lejos del plano de enfoque parecen menos n√≠tidos, y esto ser√° una pista de la profundidad desenfocada.  Adem√°s, incluso al ver una imagen en una pantalla plana, podemos estimar f√°cilmente la distancia a los objetos, ya que conocemos el tama√±o aproximado de los objetos cotidianos (es decir, puede usar la cantidad de p√≠xeles que representan la cara de una persona para evaluar qu√© tan lejos est√° ubicada).  Esta ser√° una pista sem√°ntica. <br><br>  Desarrollar manualmente un algoritmo que combine estos consejos es extremadamente dif√≠cil, pero usando MO, podemos hacerlo mientras mejoramos el rendimiento de los consejos de paralaje PDAF.  Espec√≠ficamente, entrenamos una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">red neuronal convolucional</a> escrita en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">TensorFlow</a> , que recibe p√≠xeles de PDAF como entrada, y aprende a predecir la profundidad.  Este nuevo m√©todo mejorado para estimar la profundidad basado en MO se usa en el modo vertical Pixel 3. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7db/0ff/5e3/7db0ff5e3063425f703aaf2b3f30fd77.png"><br>  <i>Nuestra red neuronal convolucional recibe im√°genes PDAF y proporciona un mapa de profundidad.</i>  <i>La red utiliza una arquitectura de estilo codificador-decodificador con <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">conexiones de omisi√≥n</a> adicionales y bloques residuales.</i> <br><br><h2>  Entrenamiento de redes neuronales </h2><br>  Para entrenar la red, necesitamos muchas im√°genes PDAF y mapas de profundidad de alta calidad correspondientes.  Y dado que necesitamos que las predicciones de profundidad sean √∫tiles en el modo vertical, necesitamos que los datos de entrenamiento sean similares a las fotos que los usuarios toman con los tel√©fonos inteligentes. <br><br>  Para hacer esto, dise√±amos un dispositivo Frankenfon especial, en el que combinamos cinco tel√©fonos Pixel 3 y establecimos una conexi√≥n WiFi entre ellos, lo que nos permiti√≥ tomar fotos simult√°neamente desde todos los tel√©fonos (con una diferencia de no m√°s de 2 ms).  Con este dispositivo, calculamos mapas de profundidad de alta calidad basados ‚Äã‚Äãen fotograf√≠as, utilizando tanto movimiento como est√©reo desde m√∫ltiples √°ngulos. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/995/827/05b/99582705b2c447fa6faf95c5c114d20f.gif"><br>  <i>Izquierda: un dispositivo para recopilar datos de entrenamiento.</i>  <i>En el medio: un ejemplo de cambio entre cinco fotograf√≠as.</i>  <i>La sincronizaci√≥n de la c√°mara garantiza la capacidad de calcular la profundidad en escenas din√°micas.</i>  <i>Derecha: profundidad total.</i>  <i>Los puntos con poca confianza, donde la comparaci√≥n de p√≠xeles en diferentes fotos era incierta debido a la debilidad de las texturas, est√°n pintados de negro y no se usan en el entrenamiento.</i> <br><br>  Los datos obtenidos con este dispositivo fueron ideales para entrenar la red por las siguientes razones: <br><br><ul><li>  Cinco puntos de vista garantizan la presencia de paralaje en varias direcciones, lo que nos salva del problema de la apertura. </li><li>  La ubicaci√≥n de las c√°maras garantiza que cualquier punto de la imagen se repita en al menos dos fotograf√≠as, lo que reduce el n√∫mero de puntos que no pueden coincidir. </li><li>  La l√≠nea base, es decir, la distancia entre las c√°maras, es mayor que la del PDAF, lo que garantiza una estimaci√≥n m√°s precisa de la profundidad. </li><li>  La sincronizaci√≥n de la c√°mara garantiza la capacidad de calcular la profundidad en escenas din√°micas. </li><li>  La portabilidad del dispositivo garantiza la posibilidad de tomar fotos en la naturaleza, simulando fotos que los usuarios toman con tel√©fonos inteligentes. </li></ul><br>  Sin embargo, a pesar de la idealidad de los datos obtenidos con este dispositivo, todav√≠a es extremadamente dif√≠cil predecir la profundidad absoluta de los objetos de la escena: cualquier par PDAF puede corresponder a varios mapas de profundidad (todo depende de las caracter√≠sticas de las lentes, la distancia focal, etc.).  Para tener todo esto en cuenta, estimamos la profundidad relativa de los objetos de la escena, que es suficiente para obtener resultados satisfactorios en modo retrato. <br><br><h2>  Combinamos todo esto </h2><br>  La estimaci√≥n de la profundidad utilizando MO en el Pixel 3 deber√≠a funcionar r√°pidamente para que los usuarios no tengan que esperar demasiado para obtener los resultados de los retratos.  Sin embargo, para obtener buenas estimaciones de profundidad utilizando un peque√±o desenfoque y paralaje, debe alimentar las redes neuronales de la foto en resoluci√≥n completa.  Para garantizar resultados r√°pidos, utilizamos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">TensorFlow Lite</a> , una soluci√≥n multiplataforma para lanzar modelos MO en dispositivos m√≥viles e integrados, as√≠ como una potente GPU Pixel 3, que le permite calcular r√°pidamente la profundidad para datos de entrada inusualmente grandes.  Luego combinamos las estimaciones de profundidad obtenidas con m√°scaras de nuestra red neuronal, que distingue a las personas, para obtener los resultados m√°s bellos de disparar en modo retrato. <br><br><h2>  Pru√©balo t√∫ mismo </h2><br>  En la aplicaci√≥n Google Camera versi√≥n 6.1 y superior, nuestros mapas de profundidad est√°n integrados en im√°genes en modo retrato.  Esto significa que podemos usar el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Editor de profundidad de Google Photos</a> para cambiar el grado de desenfoque y el punto de enfoque despu√©s de tomar una foto.  Tambi√©n puede usar programas de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">terceros</a> para extraer mapas de profundidad de jpeg y estudiarlos usted mismo.  Tambi√©n puede tomar un √°lbum <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">del enlace</a> , que muestra mapas de profundidad relativa e im√°genes correspondientes en modo vertical, para comparar el enfoque est√©reo tradicional y MO. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es433600/">https://habr.com/ru/post/es433600/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es433586/index.html">C√≥mo no ganamos el hackathon</a></li>
<li><a href="../es433588/index.html">Incre√≠ble rendimiento de algoritmos paralelos C ++ 17. ¬øMito o realidad?</a></li>
<li><a href="../es433592/index.html">Informaci√≥n: Yandex.Phone</a></li>
<li><a href="../es433596/index.html">Error de Magellan: desbordamiento de b√∫fer o expedici√≥n alrededor del mundo usando SQLite FTS</a></li>
<li><a href="../es433598/index.html">C√≥mo LLVM optimiza la funci√≥n</a></li>
<li><a href="../es433602/index.html">La simplicidad matem√°tica puede ser la base de la velocidad de la evoluci√≥n.</a></li>
<li><a href="../es433604/index.html">Trabajo c√≥modo con Android Studio</a></li>
<li><a href="../es433606/index.html">Profundidades SIEM: correlaciones listas para usar. Parte 3.2. Metodolog√≠a de normalizaci√≥n de eventos</a></li>
<li><a href="../es433608/index.html">El auto del futuro. ¬øPantallas en lugar de vidrio autom√°tico?</a></li>
<li><a href="../es433610/index.html">Notas de un fitoqu√≠mico. Caqui</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>