<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚õπüèº ‚ûï ‚öΩÔ∏è Grokay PyTorch üë®‚Äçüéì üòµ üéôÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° Habr! 

 Temos uma encomenda antecipada de um livro h√° muito aguardado sobre a biblioteca PyTorch . 



 Como voc√™ aprender√° todo o material b√°sic...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Grokay PyTorch</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/471228/"> Ol√° Habr! <br><br>  Temos uma encomenda antecipada de um livro h√° muito aguardado sobre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">a biblioteca PyTorch</a> . <br><br><img src="https://habrastorage.org/webt/bt/am/vl/btamvlvzqw01qwk-_2mq08t-sh4.jpeg"><br><br>  Como voc√™ aprender√° todo o material b√°sico necess√°rio sobre o PyTorch neste livro, lembramos os <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">benef√≠cios de um</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">processo</a> chamado "grokking" ou "compreens√£o profunda" do t√≥pico que voc√™ deseja aprender.  Na postagem de hoje, contaremos como Kai Arulkumaran bateu o PyTorch (sem foto).  Bem-vindo ao gato. <br><a name="habracut"></a><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O PyTorch</a> √© uma estrutura flex√≠vel de aprendizado profundo que distingue automaticamente objetos usando redes neurais din√¢micas (ou seja, redes usando controle din√¢mico de fluxo, como <code>if</code> e loops <code>while</code> ).  O PyTorch suporta acelera√ß√£o de GPU, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">treinamento distribu√≠do</a> , v√°rios tipos de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">otimiza√ß√£o</a> e muitos outros recursos interessantes.  Aqui, expus algumas reflex√µes sobre como, na minha opini√£o, deve usar o PyTorch;  todos os aspectos da biblioteca e pr√°ticas recomendadas n√£o s√£o abordados aqui, mas espero que este texto seja √∫til para voc√™. <br><br>  Redes neurais s√£o uma subclasse de gr√°ficos computacionais.  Os gr√°ficos de computa√ß√£o recebem dados como entrada e, em seguida, esses dados s√£o roteados (e podem ser convertidos) nos n√≥s em que s√£o processados.  No aprendizado profundo, os neur√¥nios (n√≥s) geralmente transformam dados aplicando par√¢metros e fun√ß√µes diferenci√°veis ‚Äã‚Äãa eles, para que os par√¢metros possam ser otimizados para minimizar as perdas pelo m√©todo da descida do gradiente.  Em um sentido mais amplo, observo que as fun√ß√µes podem ser estoc√°sticas e din√¢micas de gr√°fico.  Assim, enquanto as redes neurais se encaixam bem no paradigma de programa√ß√£o de fluxo de dados, a API do PyTorch se concentra no paradigma de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">programa√ß√£o imperativa</a> , e essa maneira de interpretar os programas criados √© muito mais familiar.  √â por isso que o c√≥digo PyTorch √© mais f√°cil de ler, √© mais f√°cil julgar o design de programas complexos, o que, no entanto, n√£o exige comprometimento s√©rio no desempenho: na verdade, o PyTorch √© r√°pido o suficiente e oferece muitas otimiza√ß√µes que voc√™, como usu√°rio final, n√£o pode se preocupar com nada. (no entanto, se voc√™ estiver realmente interessado neles, poder√° se aprofundar um pouco mais e conhec√™-los). <br><br>  O restante deste artigo √© uma an√°lise do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">exemplo oficial no conjunto de dados MNIST</a> .  Aqui <i>jogamos</i> PyTorch, portanto, recomendo entender o artigo somente ap√≥s o conhecimento <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">dos manuais oficiais para iniciantes</a> .  Por conveni√™ncia, o c√≥digo √© apresentado na forma de pequenos fragmentos equipados com coment√°rios, ou seja, n√£o √© distribu√≠do em fun√ß√µes / arquivos separados que voc√™ costuma ver em c√≥digo modular puro. <br><br><h4>  Importa√ß√µes </h4><br><pre> <code class="plaintext hljs">import argparse import os import torch from torch import nn, optim from torch.nn import functional as F from torch.utils.data import DataLoader from torchvision import datasets, transforms</code> </pre> <br>  Tudo isso √© uma importa√ß√£o bastante padr√£o, com exce√ß√£o dos m√≥dulos de <code>torchvision</code> da <code>torchvision</code> , que s√£o usados ‚Äã‚Äãativamente para resolver tarefas relacionadas √† vis√£o computacional. <br><br><h4>  Personaliza√ß√£o </h4><br><pre> <code class="python hljs">parser = argparse.ArgumentParser(description=<span class="hljs-string"><span class="hljs-string">'PyTorch MNIST Example'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--batch-size'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">64</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'N'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'input batch size for training (default: 64)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--epochs'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">10</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'N'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'number of epochs to train (default: 10)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--lr'</span></span>, type=float, default=<span class="hljs-number"><span class="hljs-number">0.01</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'LR'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'learning rate (default: 0.01)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--momentum'</span></span>, type=float, default=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'M'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'SGD momentum (default: 0.5)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--no-cuda'</span></span>, action=<span class="hljs-string"><span class="hljs-string">'store_true'</span></span>, default=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, help=<span class="hljs-string"><span class="hljs-string">'disables CUDA training'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--seed'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">1</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'S'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'random seed (default: 1)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--save-interval'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">10</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'N'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'how many batches to wait before checkpointing'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--resume'</span></span>, action=<span class="hljs-string"><span class="hljs-string">'store_true'</span></span>, default=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, help=<span class="hljs-string"><span class="hljs-string">'resume training from checkpoint'</span></span>) args = parser.parse_args() use_cuda = torch.cuda.is_available() <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> args.no_cuda device = torch.device(<span class="hljs-string"><span class="hljs-string">'cuda'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">'cpu'</span></span>) torch.manual_seed(args.seed) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: torch.cuda.manual_seed(args.seed)</code> </pre> <br>  <code>argparse</code> √© a maneira padr√£o de lidar com argumentos de linha de comando no Python. <br><br>  Se voc√™ precisar escrever um c√≥digo projetado para funcionar em dispositivos diferentes (usando a acelera√ß√£o da GPU, quando dispon√≠vel, mas se n√£o for revertido para os c√°lculos na CPU), selecione e salve o <code>torch.device</code> apropriado, com o qual voc√™ pode determinar onde deve tensores s√£o armazenados.  Para mais informa√ß√µes sobre como criar esse c√≥digo, consulte a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o oficial</a> .  A abordagem do PyTorch √© levar a sele√ß√£o de dispositivos ao controle do usu√°rio, o que pode parecer indesej√°vel em exemplos simples.  No entanto, essa abordagem simplifica bastante o trabalho quando voc√™ precisa lidar com tensores, o que: a) √© conveniente para a depura√ß√£o; b) permite que voc√™ use efetivamente os dispositivos manualmente. <br><br>  Para a reprodutibilidade das experi√™ncias, √© necess√°rio definir valores iniciais aleat√≥rios para todos os componentes que usam gera√ß√£o aleat√≥ria de n√∫meros (incluindo <code>random</code> ou <code>numpy</code> , se voc√™ <code>numpy</code> os usar).  Observe: cuDNN usa algoritmos n√£o determin√≠sticos e, opcionalmente, √© desativado usando <code>torch.backends.cudnn.enabled = False</code> . <br><br><h4>  Dados </h4><br><pre> <code class="python hljs">data_path = os.path.join(os.path.expanduser(<span class="hljs-string"><span class="hljs-string">'~'</span></span>), <span class="hljs-string"><span class="hljs-string">'.torch'</span></span>, <span class="hljs-string"><span class="hljs-string">'datasets'</span></span>, <span class="hljs-string"><span class="hljs-string">'mnist'</span></span>) train_data = datasets.MNIST(data_path, train=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, download=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((<span class="hljs-number"><span class="hljs-number">0.1307</span></span>,), (<span class="hljs-number"><span class="hljs-number">0.3081</span></span>,))])) test_data = datasets.MNIST(data_path, train=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((<span class="hljs-number"><span class="hljs-number">0.1307</span></span>,), (<span class="hljs-number"><span class="hljs-number">0.3081</span></span>,))])) train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, num_workers=<span class="hljs-number"><span class="hljs-number">4</span></span>, pin_memory=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) test_loader = DataLoader(test_data, batch_size=args.batch_size, num_workers=<span class="hljs-number"><span class="hljs-number">4</span></span>, pin_memory=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><br>  Como os modelos de <code>torchvision</code> armazenados em <code>~/.torch/models/</code> , eu prefiro armazenar os <code>torchvision</code> vis√£o da tocha em <code>~/.torch/datasets</code> .  Este √© meu contrato de direitos autorais, mas √© muito conveniente usar em projetos desenvolvidos com base no MNIST, CIFAR-10, etc.  Em geral, os conjuntos de dados devem ser armazenados separadamente do c√≥digo se voc√™ pretende reutilizar v√°rios conjuntos de dados. <br><br>  <code>torchvision.transforms</code> cont√©m muitas op√ß√µes de convers√£o convenientes para imagens individuais, como corte e normaliza√ß√£o. <br><br>  Existem muitas op√ß√µes no <code>batch_size</code> , mas, al√©m de <code>batch_size</code> e <code>shuffle</code> , voc√™ tamb√©m deve ter em mente <code>num_workers</code> e <code>pin_memory</code> , eles ajudam a aumentar a efici√™ncia.  <code>num_workers &gt; 0</code> usa subprocessos para carregamento ass√≠ncrono de dados e n√£o bloqueia o processo principal para isso.  Um caso de uso t√≠pico √© carregar dados (por exemplo, imagens) de um disco e, possivelmente, convert√™-los;  tudo isso pode ser feito em paralelo, juntamente com o processamento de dados da rede.  O grau de processamento pode precisar ser ajustado para: a) minimizar o n√∫mero de trabalhadores e, consequentemente, a quantidade de CPU e RAM usada (cada trabalhador carrega um lote separado, em vez de amostras individuais inclu√≠das no lote); b) minimizar o tempo que os dados aguardam na rede.  <code>pin_memory</code> usa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">mem√≥ria fixada</a> (em oposi√ß√£o a paginada) para acelerar qualquer opera√ß√£o de transfer√™ncia de dados da RAM para a GPU (e n√£o faz nada com o c√≥digo espec√≠fico da CPU). <br><br><h4>  Modelo </h4><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Net</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> super(Net, self).__init__() self.conv1 = nn.Conv2d(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, kernel_size=<span class="hljs-number"><span class="hljs-number">5</span></span>) self.conv2 = nn.Conv2d(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>, kernel_size=<span class="hljs-number"><span class="hljs-number">5</span></span>) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(<span class="hljs-number"><span class="hljs-number">320</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>) self.fc2 = nn.Linear(<span class="hljs-number"><span class="hljs-number">50</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x)</span></span></span><span class="hljs-function">:</span></span> x = F.relu(F.max_pool2d(self.conv1(x), <span class="hljs-number"><span class="hljs-number">2</span></span>)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), <span class="hljs-number"><span class="hljs-number">2</span></span>)) x = x.view(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">320</span></span>) x = F.relu(self.fc1(x)) x = self.fc2(x) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> F.log_softmax(x, dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) model = Net().to(device) optimiser = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> args.resume: model.load_state_dict(torch.load(<span class="hljs-string"><span class="hljs-string">'model.pth'</span></span>)) optimiser.load_state_dict(torch.load(<span class="hljs-string"><span class="hljs-string">'optimiser.pth'</span></span>))</code> </pre> <br>  A inicializa√ß√£o da rede geralmente se estende a vari√°veis-membro, camadas que cont√™m par√¢metros de aprendizado e, possivelmente, par√¢metros de aprendizado individuais e buffers n√£o treinados.  Ent√£o, com um passe direto, eles s√£o usados ‚Äã‚Äãem combina√ß√£o com fun√ß√µes de <code>F</code> que s√£o puramente funcionais e n√£o cont√™m par√¢metros.  Algumas pessoas gostam de trabalhar com redes puramente funcionais (por exemplo, mant√™m par√¢metros e usam <code>F.conv2d</code> vez de <code>nn.Conv2d</code> ) ou redes inteiramente constitu√≠das por camadas (por exemplo, <code>nn.ReLU</code> vez de <code>F.relu</code> ). <br><br>  <code>.to(device)</code> √© uma maneira conveniente de enviar par√¢metros (e buffers) para a GPU se o <code>device</code> definido como GPU, porque, caso contr√°rio (se o dispositivo estiver definido como CPU), nada ser√° feito.  √â importante transferir os par√¢metros do dispositivo para o dispositivo apropriado antes de pass√°-los para o otimizador;  caso contr√°rio, o otimizador n√£o poder√° rastrear os par√¢metros corretamente! <br><br>  As redes neurais ( <code>nn.Module</code> ) e os otimizadores ( <code>optim.Optimizer</code> ) podem salvar e carregar seu estado interno, e √© recomend√°vel fazer isso com <code>.load_state_dict(state_dict)</code> - √© necess√°rio recarregar o estado de ambos para retomar o treinamento com base em dicion√°rios salvos anteriormente estados.  Salvar o objeto inteiro pode estar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cheio de erros</a> .  Se voc√™ salvou os tensores na GPU e deseja carreg√°-los na CPU ou em outra GPU, a maneira mais f√°cil √© carreg√°-los diretamente na CPU usando a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">op√ß√£o</a> <code>map_location</code> , por exemplo, <code>torch.load('model.pth'</code> , <code>map_location='cpu'</code> ). <br><br>  Aqui est√£o alguns outros pontos que n√£o s√£o mostrados aqui, mas dignos de men√ß√£o, de que voc√™ pode usar o fluxo de controle com uma passagem direta (por exemplo, a execu√ß√£o da <code>if</code> pode depender da vari√°vel do membro ou dos pr√≥prios dados. Al√©m disso, √© perfeitamente aceit√°vel exibir no meio do processo ( <code>print</code> ) tensors, o que simplifica bastante a depura√ß√£o.Finalmente, com uma passagem direta, muitos argumentos podem ser usados.Ilustrarei esse ponto com uma pequena lista que n√£o est√° vinculada a nenhuma id√©ia espec√≠fica: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, hx, drop=False)</span></span></span><span class="hljs-function">:</span></span> hx2 = self.rnn(x, hx) print(hx.mean().item(), hx.var().item()) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> hx.max.item() &gt; <span class="hljs-number"><span class="hljs-number">10</span></span> <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> self.can_drop <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> drop: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> hx <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> hx2</code> </pre> <br><h4>  Treinamento </h4><br><pre> <code class="python hljs">model.train() train_losses = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, (data, target) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(train_loader): data = data.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) target = target.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) optimiser.zero_grad() output = model(data) loss = F.nll_loss(output, target) loss.backward() train_losses.append(loss.item()) optimiser.step() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i % <span class="hljs-number"><span class="hljs-number">10</span></span> == <span class="hljs-number"><span class="hljs-number">0</span></span>: print(i, loss.item()) torch.save(model.state_dict(), <span class="hljs-string"><span class="hljs-string">'model.pth'</span></span>) torch.save(optimiser.state_dict(), <span class="hljs-string"><span class="hljs-string">'optimiser.pth'</span></span>) torch.save(train_losses, <span class="hljs-string"><span class="hljs-string">'train_losses.pth'</span></span>)</code> </pre> <br>  Os m√≥dulos de rede s√£o colocados no modo de treinamento por padr√£o - o que afeta, em certa medida, a opera√ß√£o dos m√≥dulos, principalmente - a dilui√ß√£o e a normaliza√ß√£o de lotes.  De uma forma ou de outra, √© melhor definir essas coisas manualmente usando <code>.train()</code> , que filtra o sinalizador "training" para todos os m√≥dulos filhos. <br><br>  Aqui, o m√©todo <code>.to()</code> n√£o apenas aceita o dispositivo, mas tamb√©m define <code>non_blocking=True</code> , garantindo assim a c√≥pia ass√≠ncrona de dados para a GPU da mem√≥ria confirmada, permitindo que a CPU permane√ßa operacional durante a transfer√™ncia de dados;  caso contr√°rio, <code>non_blocking=True</code> simplesmente n√£o √© uma op√ß√£o. <br><br>  Antes de criar um novo conjunto de gradientes usando <code>loss.backward()</code> e <code>optimiser.step()</code> usando <code>optimiser.step()</code> , voc√™ deve redefinir manualmente os gradientes dos par√¢metros a serem otimizados usando optimiser.zero_grad <code>optimiser.zero_grad()</code> .  Por padr√£o, o PyTorch acumula gradientes, o que √© muito conveniente se voc√™ n√£o tiver recursos suficientes para calcular todos os gradientes necess√°rios em uma √∫nica passagem. <br><br>  O PyTorch usa um sistema de "fita" de gradientes autom√°ticos - coleta informa√ß√µes sobre quais opera√ß√µes e em que ordem foram executadas nos tensores e as reproduz na dire√ß√£o oposta para realizar a diferencia√ß√£o na ordem inversa (diferencia√ß√£o no modo reverso).  √â por isso que √© t√£o super flex√≠vel e permite gr√°ficos computacionais arbitr√°rios.  Se nenhum desses tensores exigir gradientes (voc√™ precisa definir <code>requires_grad=True</code> , criando um tensor para essa finalidade), nenhum gr√°fico ser√° salvo!  No entanto, as redes geralmente t√™m par√¢metros que exigem gradientes; portanto, todos os c√°lculos realizados com base na sa√≠da da rede ser√£o armazenados no gr√°fico.  Portanto, se voc√™ quiser salvar os dados resultantes desta etapa, ser√° necess√°rio desativar manualmente os gradientes ou (uma abordagem mais comum), salve essas informa√ß√µes como um n√∫mero Python (usando <code>.item()</code> no escalar do PyTorch) ou uma matriz <code>numpy</code> .  Leia mais sobre o <code>autograd</code> na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o oficial</a> . <br><br>  Uma maneira de reduzir o gr√°fico computacional √© usar <code>.detach()</code> quando o estado oculto √© passado ao aprender RNN com uma vers√£o truncada da retropropaga√ß√£o ao longo do tempo.  Tamb√©m √© conveniente ao diferenciar perdas, quando um dos componentes √© a sa√≠da de outra rede, mas essa outra rede n√£o deve ser otimizada com rela√ß√£o √†s perdas.  Como exemplo, ensinarei a parte discriminat√≥ria do material de sa√≠da gerado ao trabalhar com a GAN ou o treinamento de pol√≠ticas no algoritmo de ator-cr√≠tico, usando a fun√ß√£o objetivo como a fun√ß√£o base (por exemplo, A2C).  Outra t√©cnica que impede o c√°lculo de gradientes √© eficaz no treinamento GAN (treinamento da parte geradora em material discriminante) e t√≠pica no ajuste fino √© a enumera√ß√£o c√≠clica de par√¢metros de rede para os quais <code>param.requires_grad = False</code> . <br><br>  √â importante n√£o apenas registrar os resultados no arquivo de console / log, mas tamb√©m definir pontos de controle nos par√¢metros do modelo (e no estado do otimizador) apenas por precau√ß√£o.  Voc√™ tamb√©m pode usar <code>torch.save()</code> para salvar objetos Python regulares ou usar outra solu√ß√£o padr√£o - o <code>pickle</code> . <br><br><h4>  Teste </h4><br><pre> <code class="python hljs">model.eval() test_loss, correct = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> torch.no_grad(): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> data, target <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> test_loader: data = data.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) target = target.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) output = model(data) test_loss += F.nll_loss(output, target, reduction=<span class="hljs-string"><span class="hljs-string">'sum'</span></span>).item() pred = output.argmax(<span class="hljs-number"><span class="hljs-number">1</span></span>, keepdim=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_data) acc = correct / len(test_data) print(acc, test_loss)</code> </pre> <br>  Em resposta a <code>.train()</code> redes precisam ser explicitamente colocadas no modo de avalia√ß√£o usando <code>.eval()</code> . <br><br>  Como mencionado acima, ao usar uma rede, geralmente √© compilado um gr√°fico computacional.  Para evitar isso, use o <code>no_grad</code> contexto <code>with torch.no_grad()</code> . <br><br><h4>  Um pouco mais </h4><br>  Esta √© uma se√ß√£o adicional, na qual fiz algumas digress√µes mais √∫teis. <br>  Aqui est√° a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o oficial que</a> explica como trabalhar com mem√≥ria. <br><br>  Erros CUDA?  √â dif√≠cil corrigi-los e, geralmente, eles est√£o conectados com inconsist√™ncias l√≥gicas, de acordo com as quais mensagens de erro mais sens√≠veis s√£o exibidas na CPU do que na GPU.  O melhor de tudo √© que, se voc√™ planeja trabalhar com a GPU, pode alternar rapidamente entre a CPU e a GPU.  Uma dica de desenvolvimento mais geral √© organizar o c√≥digo para que ele possa ser verificado rapidamente antes de iniciar uma tarefa completa.  Por exemplo, prepare um conjunto de dados pequeno ou sint√©tico, execute um teste de trem da era +, etc.  Se o problema for um erro CUDA ou voc√™ n√£o puder alternar para a CPU, defina CUDA_LAUNCH_BLOCKING = 1.  Isso far√° com que o kernel CUDA inicie de forma s√≠ncrona e voc√™ receber√° mensagens de erro mais precisas. <br><br>  Uma observa√ß√£o sobre <code>torch.multiprocessing</code> ou apenas executando v√°rios scripts PyTorch ao mesmo tempo.  Como o PyTorch usa bibliotecas BLAS com v√°rios threads para acelerar os c√°lculos de √°lgebra linear na CPU, geralmente v√°rios n√∫cleos est√£o envolvidos.  Se voc√™ deseja fazer v√°rias coisas ao mesmo tempo, usando processamento multithread ou v√°rios scripts, pode ser aconselh√°vel reduzir manualmente seu n√∫mero, configurando a vari√°vel de ambiente <code>OMP_NUM_THREADS</code> para 1 ou outro valor baixo.  Assim, a probabilidade de escorregar no processador √© reduzida.  A documenta√ß√£o oficial possui outros coment√°rios sobre o processamento multithread. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt471228/">https://habr.com/ru/post/pt471228/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt471212/index.html">10 dicas e truques para ajudar voc√™ a se tornar o melhor desenvolvedor do VueJS</a></li>
<li><a href="../pt471216/index.html">A longa hist√≥ria do guia - como eu escrevi um servi√ßo para trilhas inteligentes por 5 anos</a></li>
<li><a href="../pt471220/index.html">Cockpit - simplifique tarefas administrativas t√≠picas no Linux atrav√©s de uma interface web conveniente</a></li>
<li><a href="../pt471222/index.html">Compreender as pol√≠ticas de privacidade de aplicativos e servi√ßos ajudar√° redes neurais</a></li>
<li><a href="../pt471226/index.html">O Linux tem muitas faces: como trabalhar em qualquer distribui√ß√£o</a></li>
<li><a href="../pt471232/index.html">Minha experi√™ncia conectando o LPS331AP ao Omega Onion2</a></li>
<li><a href="../pt471236/index.html">Dos√≠metro para Seryozha. Parte III Radi√≥metro nacional</a></li>
<li><a href="../pt471240/index.html">‚ÄúBitchy Betty‚Äù e interfaces de √°udio modernas: por que eles falam com uma voz feminina?</a></li>
<li><a href="../pt471242/index.html">Introdu√ß√£o ao Bash Shell</a></li>
<li><a href="../pt471244/index.html">C√≥digo Rosetta: mede o comprimento do c√≥digo em um grande n√∫mero de linguagens de programa√ß√£o, estuda a proximidade das linguagens entre si</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>