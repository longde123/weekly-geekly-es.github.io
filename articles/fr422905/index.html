<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèª‚Äçüè´ üé´ üêÑ Mais vous dites Ceph ... est-il si bon? üë®‚Äç‚öïÔ∏è üë©üèø‚Äçü§ù‚Äçüë®üèº üí≥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="J'adore Ceph. Je travaille avec lui depuis 4 ans (0.80.x -  12.2.6  , 12.2.5). Parfois, je suis tellement passionn√© par lui que je passe des soir√©es e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Mais vous dites Ceph ... est-il si bon?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/croccloudteam/blog/422905/"><p><img src="https://habrastorage.org/webt/fm/pp/3b/fmpp3bma4xf_j2pxwystnjgezc0.png"></p><br><p>  J'adore Ceph.  Je travaille avec lui depuis 4 ans (0.80.x - <del>  12.2.6 </del> , 12.2.5).  Parfois, je suis tellement passionn√© par lui que je passe des soir√©es et des nuits en sa compagnie, et pas avec ma copine.  J'ai rencontr√© divers probl√®mes avec ce produit et je continue d'en vivre avec certains √† ce jour.  Parfois, je me r√©jouissais de d√©cisions faciles et parfois je r√™vais de rencontrer des d√©veloppeurs pour exprimer mon indignation.  Mais Ceph est toujours utilis√© dans notre projet et il est possible qu'il soit utilis√© dans de nouvelles t√¢ches, au moins par moi.  Dans cette histoire, je partagerai notre exp√©rience de fonctionnement de Ceph, d'une certaine mani√®re je m'exprimerai sur le sujet de ce que je n'aime pas dans cette solution et peut-√™tre aider ceux qui ne font que la regarder.  Les √©v√©nements qui ont commenc√© il y a environ un an lorsque j'ai apport√© le Dell EMC ScaleIO, maintenant connu sous le nom de Dell EMC VxFlex OS, m'ont pouss√© √† √©crire cet article. </p><br><p>  Ce n'est en aucun cas une publicit√© pour Dell EMC ou leur produit!  Personnellement, je ne suis pas tr√®s bon avec les grandes entreprises et les bo√Ætes noires comme VxFlex OS.  Mais comme vous le savez, tout dans le monde est relatif et en utilisant l'exemple de VxFlex OS, il est tr√®s pratique de montrer ce qu'est Ceph en termes de fonctionnement, et je vais essayer de le faire. <a name="habracut"></a></p><br><h2 id="parametry-rech-idet-o-4-znachnyh-chislah">  Param√®tres  Il s'agit de nombres √† 4 chiffres! </h2><br><p>  Services Ceph tels que MON, OSD, etc.  ont diff√©rents param√®tres pour configurer toutes sortes de sous-syst√®mes.  Les param√®tres sont d√©finis dans le fichier de configuration, les d√©mons les lisent au moment du lancement.  Certaines valeurs peuvent √™tre facilement modifi√©es √† la vol√©e en utilisant le m√©canisme "d'injection", qui est d√©crit ci-dessous.  Tout est presque super, si vous omettez le moment o√π il y a des centaines de param√®tres: <br><br>  Marteau: </p><br><pre><code class="html hljs xml">&gt; ceph daemon mon.a config show | wc -l 863</code> </pre> <br><p>  Lumineux: </p><br><pre> <code class="html hljs xml">&gt; ceph daemon mon.a config show | wc -l 1401</code> </pre> <br><p>  Il s'av√®re que ~ 500 nouveaux param√®tres en deux ans.  En g√©n√©ral, le param√©trage est cool, ce n'est pas cool qu'il y ait des difficult√©s √† comprendre 80% de cette liste.  La documentation d√©crit par mes estimations ~ 20% et √† certains endroits est ambigu√´.  Une compr√©hension de la signification de la plupart des param√®tres doit √™tre trouv√©e dans le github du projet ou dans les listes de diffusion, mais cela n'aide pas toujours. </p><br><p>  Voici un exemple de plusieurs param√®tres qui m'int√©ressaient tout r√©cemment, je les ai trouv√©s sur le blog d'un Ceph-gadfly: </p><br><pre> <code class="html hljs xml">throttler_perf_counter = false // enable/disable throttler perf counter osd_enable_op_tracker = false // enable/disable OSD op tracking</code> </pre> <br><p>  Codez les commentaires dans l'esprit des meilleures pratiques.  Comme si, je comprends les mots et m√™me √† peu pr√®s de quoi ils parlent, mais ce que cela me donnera ne l'est pas. </p><br><p>  Ou ici: <strong>osd_op_threads</strong> dans Luminous avait disparu et seul le code source a aid√© √† trouver un nouveau nom: <strong>threads osd_peering_wq</strong> </p><br><p>  J'aime aussi le fait qu'il existe des options particuli√®rement holistiques.  Ici, mec montre que l'augmentation de <strong>rgw_num _rados_handles</strong> est <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">bonne</a> : </p><br><p>  et l'autre mec pense que&gt; 1 est impossible et m√™me <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dangereux</a> . </p><br><p>  Et ma chose pr√©f√©r√©e est lorsque les d√©butants donnent des exemples de configuration dans leurs articles de blog, o√π tous les param√®tres sont copi√©s sans r√©fl√©chir (il me semble) √† partir d'un autre blog du m√™me genre, et donc un tas de param√®tres que personne ne conna√Æt, sauf l'auteur du code. config to config. </p><br><p>  Je br√ªle aussi sauvagement avec ce qu'ils ont fait dans Luminous.  Il y a une fonctionnalit√© super cool - changer les param√®tres √† la vol√©e, sans red√©marrer les processus.  Vous pouvez, par exemple, modifier le param√®tre d'un OSD sp√©cifique: </p><br><pre> <code class="html hljs xml">&gt; ceph tell osd.12 injectargs '--filestore_fd_cache_size=512'</code> </pre> <br><p>  ou mettez '*' au lieu de 12 et la valeur sera modifi√©e sur tous les OSD.  C'est vraiment cool, vraiment.  Mais, comme beaucoup √† Ceph, cela se fait avec le pied gauche.  La conception de Bai ne permet pas de modifier toutes les valeurs des param√®tres √† la vol√©e.  Plus pr√©cis√©ment, ils peuvent √™tre d√©finis et ils appara√Ætront modifi√©s dans la sortie, mais en fait, seuls quelques-uns sont relus et r√©appliqu√©s.  Par exemple, vous ne pouvez pas modifier la taille du pool de threads sans red√©marrer le processus.  Pour que l'ex√©cuteur de l'√©quipe comprenne qu'il est inutile de modifier le param√®tre de cette mani√®re - ils ont d√©cid√© d'imprimer un message.  Bonjour </p><br><p>  Par exemple: </p><br><pre> <code class="html hljs xml">&gt; ceph tell mon.* injectargs '--mon_allow_pool_delete=true' mon.c: injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart) mon.a: injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart) mon.b: injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart)</code> </pre> <br><p>  Ambigu.  En effet, le retrait des bassins devient possible apr√®s l'injection.  Autrement dit, cet avertissement n'est pas pertinent pour ce param√®tre.  D'accord, mais il existe encore des centaines de param√®tres, y compris des param√®tres tr√®s utiles, qui comportent √©galement un avertissement et il n'y a aucun moyen de v√©rifier leur applicabilit√© r√©elle.  Pour le moment, je ne peux m√™me pas comprendre par le code quels param√®tres sont appliqu√©s apr√®s l'injection et lesquels ne le sont pas.  Pour la fiabilit√©, vous devez red√©marrer les services et cela, vous le savez, exasp√®re.  Enrag√©s parce que je sais qu'il y a un m√©canisme d'injection. </p><br><p>  Et VxFlex OS?  Des processus similaires comme MON (dans VxFlex c'est MDM), OSD (SDS dans VxFlex) ont √©galement des fichiers de configuration, dans lesquels il y a des dizaines de param√®tres pour tous.  Certes, leurs noms ne disent rien non plus, mais la bonne nouvelle est que nous n'avons jamais eu recours √† eux pour br√ªler autant qu'avec Ceph. </p><br><h2 id="tehnicheskiy-dolg">  Dette technique </h2><br><p>  Lorsque vous commencez √† vous familiariser avec Ceph avec la version la plus pertinente pour aujourd'hui, alors tout semble aller bien et vous voulez √©crire un article positif.  Mais quand vous vivez avec lui dans la prod de la version 0.80, alors tout ne semble pas si rose. </p><br><p>  Avant Jewel, les processus Ceph s'ex√©cutaient en tant que root.  Jewel a d√©cid√© de travailler √† partir de l'utilisateur ¬´ceph¬ª, ce qui a n√©cessit√© un changement de propri√©taire pour tous les r√©pertoires utilis√©s par les services Ceph.  Il semblerait que cela?  Imaginez un OSD qui dessert un disque magn√©tique SATA pleine capacit√© de 2 To.  Ainsi, le d√©montage d'un tel disque, en parall√®le (vers diff√©rents sous-r√©pertoires) avec une utilisation compl√®te du disque prend 3-4 heures.  Imaginez, par exemple, que vous ayez 3 centaines de tels disques.  M√™me si vous mettez √† jour les n≈ìuds (visualisez imm√©diatement 8 √† 12 disques), vous obtenez une mise √† jour assez longue, dans laquelle le cluster aura un OSD de diff√©rentes versions et une r√©plique de donn√©es est inf√©rieure au moment de la mise √† jour du serveur.  En g√©n√©ral, nous pensions que c'√©tait absurde, reconstruit les packages Ceph et laiss√© OSD en cours d'ex√©cution en tant que root.  Nous avons d√©cid√© qu'en entrant ou en rempla√ßant l'OSD, nous les transf√©rerions √† un nouvel utilisateur.  Maintenant, nous changeons 2-3 disques par mois et en ajoutons 1 √† 2, je pense que nous pouvons le g√©rer d'ici 2022). </p><br><p>  CRUSH Tunables </p><br><p>  <strong>CRUSH</strong> est le c≈ìur de Ceph, tout tourne autour de lui.  Il s'agit de l'algorithme par lequel, de mani√®re pseudo-al√©atoire, l'emplacement des donn√©es est s√©lectionn√© et gr√¢ce auquel les clients travaillant avec le cluster RADOS sauront sur quel OSD les donn√©es (objets) dont ils ont besoin sont stock√©es.  La principale caract√©ristique de CRUSH est qu'il n'y a pas besoin de serveurs de m√©tadonn√©es, tels que Luster ou IBM GPFS (maintenant Spectrum Scale).  CRUSH permet aux clients et √† l'OSD d'interagir directement entre eux.  Bien que, bien s√ªr, il soit difficile de comparer le stockage d'objets RADOS primitif et les syst√®mes de fichiers, que j'ai donn√©s en exemple, mais je pense que l'id√©e est claire. </p><br><p>  Les ajustables CRUSH, √† leur tour, sont un ensemble de param√®tres / drapeaux qui affectent le fonctionnement de CRUSH, le rendant plus efficace, au moins en th√©orie. </p><br><p>  Ainsi, lors de la mise √† niveau de Hammer vers Jewel (testez naturellement), un avertissement est apparu, disant que le profil ajustable a des param√®tres qui ne sont pas optimaux pour la version actuelle (Jewel) et il est recommand√© de basculer le profil sur l'optimal.  En g√©n√©ral, tout est clair.  Le dock dit que c'est tr√®s important et c'est la bonne fa√ßon, mais il est √©galement dit qu'apr√®s le changement de donn√©es, il y aura une r√©bellion de 10% des donn√©es.  10% - cela ne semble pas effrayant, mais nous avons d√©cid√© de le tester.  Pour un cluster, c'est environ 10 fois moins que sur un prod, avec le m√™me nombre de PG par OSD, rempli de donn√©es de test, on a une r√©bellion de 60%!  Imaginez, par exemple, avec 100 To de donn√©es, 60 To commencent √† se d√©placer entre les OSD et cela avec une charge client en constante √©volution exigeant une latence!  Si je n'ai pas encore dit, nous fournissons s3 et nous n'avons pas beaucoup moins de charge sur rgw m√™me la nuit, dont il y en a 8 et 4 de plus sous les sites Web statiques.  En g√©n√©ral, nous avons d√©cid√© que ce n'√©tait pas notre chemin, d'autant plus que faire une telle reconstruction sur la nouvelle version, avec laquelle nous n'avions pas travaill√© dans la prod, √©tait au moins trop optimiste.  De plus, nous avions de grands index de bucket qui sont tr√®s mal reconstruits et c'√©tait aussi la raison du retard dans le changement de profil.  √Ä propos des indices seront s√©par√©ment un peu plus bas.  Au final, nous avons simplement supprim√© l'avertissement et d√©cid√© d'y revenir plus tard. </p><br><p>  Et lors du changement de profil lors des tests, les clients cephfs qui se trouvent dans les noyaux CentOS 7.2 sont tomb√©s car ils ne pouvaient pas fonctionner avec le nouvel algorithme de hachage du nouveau profil fourni.  Nous n'utilisons pas cephfs dans la prod, mais si nous en avions l'habitude, ce serait une autre raison de ne pas changer de profil. </p><br><p>  Soit dit en passant, le dock dit que si ce qui se passe pendant la r√©bellion ne vous convient pas, vous pouvez annuler le profil.  En fait, apr√®s une nouvelle installation de la version Hammer et une mise √† niveau vers Jewel, le profil ressemble √† ceci: </p><br><pre> <code class="html hljs xml">&gt; ceph osd crush show-tunables { ... "straw_calc_version": 1, "allowed_bucket_algs": 22, "profile": "unknown", "optimal_tunables": 0, ... }</code> </pre> <br><p>  Il est important qu'il soit "inconnu" et si vous essayez d'arr√™ter la reconstruction en le basculant sur "h√©rit√©" (comme indiqu√© dans le dock) ou m√™me sur "marteau", alors la r√©bellion ne s'arr√™tera pas, elle continuera simplement conform√©ment aux autres param√®tres ajustables, et non " optimal. "  En g√©n√©ral, tout doit √™tre soigneusement v√©rifi√© et rev√©rifi√©, ceph n'est pas fiable. </p><br><p>  CRUSH trade-of </p><br><p>  Comme vous le savez, tout dans ce monde est √©quilibr√© et des inconv√©nients sont appliqu√©s √† tous les avantages.  L'inconv√©nient de CRUSH est que les PG sont in√©galement r√©partis sur diff√©rents OSD m√™me avec le m√™me poids de ces derniers.  De plus, rien n'emp√™che diff√©rents PG de cro√Ætre √† des vitesses diff√©rentes, tandis que la fonction de hachage tombera.  Plus pr√©cis√©ment, nous avons une plage d'utilisation de l'OSD de 48 √† 84%, malgr√© le fait qu'ils ont la m√™me taille et, par cons√©quent, le poids.  Nous essayons m√™me de rendre les serveurs √©gaux en poids, mais c'est ainsi, juste notre perfectionnisme, pas plus.  Et avec le fait que les E / S sont r√©parties de mani√®re in√©gale sur les disques, le pire est que lorsque vous atteignez le statut complet (95%) d'au moins un OSD dans le cluster, l'enregistrement entier s'arr√™te et le cluster passe en lecture seule.  L'ensemble du cluster!  Et peu importe que le cluster soit toujours plein d'espace.  Tout, la finale, sortez!  Il s'agit d'une caract√©ristique architecturale de CRUSH.  Imaginez que vous √™tes en vacances, certains OSD ont franchi la barre des 85% (le premier avertissement par d√©faut), et vous avez 10% en stock pour emp√™cher l'enregistrement de s'arr√™ter.  Et 10% avec un enregistrement actif n'est pas tellement / long.  Id√©alement, avec une telle conception, Ceph a besoin d'une personne de service qui puisse suivre les instructions pr√©par√©es dans de tels cas. </p><br><p>  Nous avons donc d√©cid√© que cela signifie d√©s√©quilibrer les donn√©es du cluster, car  plusieurs OSD √©taient proches du seuil proche (85%). </p><br><p>  Il existe plusieurs fa√ßons: </p><br><ul><li>  Ajouter des lecteurs </li></ul><br><p>  Le moyen le plus simple est l√©g√®rement inutile et peu efficace, car  les donn√©es elles-m√™mes peuvent ne pas bouger de l'OSD bond√© ou le mouvement sera n√©gligeable. </p><br><ul><li>  Modifier le poids permanent de l'OSD (POIDS) </li></ul><br><p>  Cela entra√Æne une modification du poids de toute la hi√©rarchie des compartiments sup√©rieurs (terminologie CRUSH), du serveur OSD, du centre de donn√©es, etc.  et, par cons√©quent, au mouvement des donn√©es, y compris √† partir des OSD dont elles ne sont pas n√©cessaires. <br>  Nous avons essay√©, r√©duit le poids d'un OSD, apr√®s que les donn√©es en reconstruisant un autre ont √©t√© remplies, nous l'avons r√©duit, puis le troisi√®me et nous avons r√©alis√© que nous jouerions cela pendant longtemps. </p><br><ul><li>  Modifier le poids OSD non permanent (REWEIGHT) </li></ul><br><p>  C'est ce que l'on fait en appelant ¬´ceph osd reweight-by-use¬ª.  Cela entra√Æne une modification du poids d'ajustement dit OSD, et le poids du godet sup√©rieur ne change pas.  En cons√©quence, les donn√©es sont √©quilibr√©es entre diff√©rents OSD d'un serveur, pour ainsi dire, sans d√©passer les limites du compartiment CRUSH.  Nous avons vraiment aim√© cette approche, nous avons examin√© la marche √† sec quels changements seraient et effectu√©s sur la prod.  Tout allait bien jusqu'√† ce que le processus de r√©√©quilibrage prenne une place interm√©diaire.  Encore une fois sur Google, en lisant les newsletters, en exp√©rimentant diff√©rentes options, et finalement, il s'av√®re que l'arr√™t a √©t√© caus√© par le manque de certains param√®tres ajustables dans le profil mentionn√© ci-dessus.  Encore une fois, nous √©tions pris de dettes techniques.  En cons√©quence, nous avons suivi la voie de l'ajout de disques et de la reconstruction la plus inefficace.  Heureusement, nous devions encore le faire car  Il √©tait pr√©vu de changer de profil CRUSH avec une marge de capacit√© suffisante. </p><br><p>  Oui, nous connaissons l'√©quilibreur (lumineux et sup√©rieur), qui fait partie de mgr, qui est con√ßu pour r√©soudre le probl√®me de la distribution in√©gale des donn√©es en d√©pla√ßant PG entre les OSD, par exemple, la nuit.  Mais je n'ai pas encore entendu de critiques positives sur son travail, m√™me dans le Mimic actuel. </p><br><p>  Vous direz probablement que la dette technique est purement notre probl√®me et je serais probablement d'accord.  Mais pendant quatre ans avec Ceph dans la prod, nous n'avons enregistr√© qu'un seul temps d'arr√™t s3, qui a dur√© une heure enti√®re.  Et puis, le probl√®me n'√©tait pas dans RADOS, mais dans RGW, qui, apr√®s avoir tap√© leurs 100 threads par d√©faut, s'est accroch√© et la plupart des utilisateurs n'ont pas r√©pondu aux demandes.  C'√©tait toujours sur Hammer.  √Ä mon avis, c'est un bon indicateur et il est atteint car nous ne faisons pas de mouvements brusques et sommes plut√¥t sceptiques √† propos de tout chez Ceph. </p><br><h2 id="dikiy-gc">  GC sauvage </h2><br><p>  Comme vous le savez, la suppression de donn√©es directement du disque est une t√¢che assez exigeante et dans les syst√®mes avanc√©s, la suppression est retard√©e ou ne se fait pas du tout.  Ceph est √©galement un syst√®me avanc√©, et dans le cas de RGW, lors de la suppression d'un objet s3, les objets RADOS correspondants ne sont pas imm√©diatement supprim√©s du disque.  RGW marque les objets s3 comme supprim√©s, et un flux gc s√©par√© supprime les objets directement des pools RADOS et, en cons√©quence, est report√© des disques.  Apr√®s la mise √† jour vers Luminous, le comportement de gc a consid√©rablement chang√©, il a commenc√© √† fonctionner de mani√®re plus agressive, bien que les param√®tres de gc soient rest√©s les m√™mes.  Par le mot sensiblement, je veux dire que nous avons commenc√© √† voir gc travailler sur la surveillance externe du service pour sauter la latence.  Cela a √©t√© accompagn√© d'un haut E / S dans le pool rgw.gc.  Mais le probl√®me auquel nous sommes confront√©s est bien plus √©pique que le simple E / S.  Lorsque gc est en cours d'ex√©cution, de nombreux journaux du formulaire sont g√©n√©r√©s: </p><br><pre> <code class="html hljs xml">0 <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">cls</span></span></span><span class="hljs-tag">&gt;</span></span> /builddir/build/BUILD/ceph-12.2.5/src/cls/rgw/cls_rgw.cc:3284: gc_iterate_entries end_key=1_01530264199.726582828</code> </pre> <br><p>  O√π 0 au d√©but est le niveau de journalisation auquel ce message est imprim√©.  Pour ainsi dire, il n'y a nulle part o√π abaisser la journalisation en dessous de z√©ro.  En cons√©quence, ~ 1 Go de journaux ont √©t√© g√©n√©r√©s en nous par un OSD en quelques heures, et tout aurait √©t√© bien si les n≈ìuds ceph n'√©taient pas sans disque ... Nous chargeons l'OS via PXE directement dans la m√©moire et n'utilisons pas de disque local ou NFS, NBD pour la partition syst√®me (/).  Il s'av√®re que les serveurs sont sans √©tat.  Apr√®s un red√©marrage, l'√©tat entier est roul√© par automatisation.  Comment cela fonctionne, je vais en quelque sorte d√©crire dans un article s√©par√©, maintenant il est important que 6 Go de m√©moire soient allou√©s pour "/", dont ~ 4 est g√©n√©ralement libre.  Nous envoyons tous les journaux √† Graylog et utilisons une politique de rotation des journaux plut√¥t agressive et ne rencontrons g√©n√©ralement aucun probl√®me de d√©bordement de disque / RAM.  Mais nous n'√©tions pas pr√™ts pour cela, avec 12 OSD, le serveur "/" s'est rempli tr√®s rapidement, les pr√©pos√©s √† l'heure n'ont pas r√©pondu au d√©clencheur dans Zabbix et OSD a juste commenc√© √† s'arr√™ter en raison de l'impossibilit√© d'√©crire un journal.  En cons√©quence, nous avons r√©duit l'intensit√© de gc, le ticket n'a pas commenc√© car  Il √©tait d√©j√† l√†, et nous avons ajout√© un script √† cron, dans lequel nous for√ßons les journaux OSD √† tronquer lorsqu'un certain montant est d√©pass√© sans attendre la rotation du journal.  Soit dit en passant, le niveau de journalisation a √©t√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">augment√©</a> . </p><br><h2 id="placement-groups-i-hvalyonaya-masshtabiruemost">  Groupes de placement et √©volutivit√© lou√©e </h2><br><p>  √Ä mon avis, PG est l'abstraction la plus difficile √† comprendre.  PG est n√©cessaire pour rendre CRUSH plus efficace.  L'objectif principal de PG est de regrouper des objets pour r√©duire la consommation de ressources, augmenter la productivit√© et l'√©volutivit√©.  Adresser des objets directement, individuellement, sans les combiner en PG serait tr√®s co√ªteux. </p><br><p>  Le principal probl√®me de PG est de d√©terminer leur nombre pour un nouveau pool.  Depuis le blog Ceph: </p><br><blockquote>  "Choisir le bon nombre de PG pour votre cluster est un peu un art noir et un cauchemar pour la convivialit√©." </blockquote><p>  Ceci est toujours tr√®s sp√©cifique √† une installation particuli√®re et n√©cessite beaucoup de r√©flexion et de calcul. </p><br><p>  Recommandations cl√©s: </p><br><ul><li>  Trop de PG sur l'OSD sont mauvais; il y aura un d√©passement des ressources pour leur maintenance et leurs freins pendant le r√©√©quilibrage / r√©cup√©ration. </li><li>  Peu de PG sur OSD sont mauvais, les performances en souffriront et les OSD seront remplis de mani√®re in√©gale. </li><li>  Le nombre PG doit √™tre un multiple de degr√© 2. Cela aidera √† obtenir "la puissance de CRUSH". </li></ul><br><p>  Et ici √ßa br√ªle avec moi.  Les PG ne sont pas limit√©s en volume ou en nombre d'objets.  Combien de ressources (en nombres r√©els) sont n√©cessaires pour desservir un PG?  Cela d√©pend-il de sa taille?  Cela d√©pend-il du nombre de r√©pliques de cette PG?  Dois-je prendre un bain de vapeur si j'ai suffisamment de m√©moire, des processeurs rapides et un bon r√©seau? <br>  Et vous devez √©galement penser √† la croissance future du cluster.  Le nombre PG ne peut pas √™tre r√©duit - seulement augment√©.  En m√™me temps, il n'est pas recommand√© de le faire, car cela impliquera, en substance, de diviser une partie de PG en une reconstruction nouvelle et sauvage. </p><br><blockquote>  "L'augmentation du nombre de PG d'un pool est l'un des √©v√©nements les plus percutants d'un cluster Ceph et doit √™tre √©vit√© si possible pour les clusters de production." </blockquote><p>  Par cons√©quent, vous devez penser √† l'avenir imm√©diatement, si possible. </p><br><p>  Un vrai exemple. </p><br><p>  Un cluster de 3 serveurs avec 14x2 To OSD chacun, un total de 42 OSD.  R√©plique 3, endroit utile ~ 28 To.  Pour √™tre utilis√© sous S3, vous devez calculer le nombre de PG pour le pool de donn√©es et le pool d'index.  RGW utilise plus de pools, mais les deux sont principaux. </p><br><p>  Nous allons dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la calculatrice PG</a> (il y a une telle calculatrice), nous consid√©rons qu'avec les 100 PG recommand√©s sur l'OSD, nous n'obtenons que 1312 PG.  Mais tout n'est pas si simple: nous en avons un en introduction - le cluster va certainement cro√Ætre trois fois en un an, mais le fer sera achet√© un peu plus tard.  Nous augmentons trois fois les "PG cibles par OSD", √† 300 et nous obtenons 4480 PG. </p><br><p><img src="https://habrastorage.org/webt/ce/o3/bo/ceo3boailgnmcx_2_w6un6_covi.png"></p><br><p>  D√©finissez le nombre de PG pour les pools correspondants - nous recevons un avertissement: trop de PG par OSD ... sont arriv√©s.  Re√ßu ~ 300 PG sur OSD avec une limite de 200 (Lumineux).  Auparavant, c'√©tait 300.  Et la chose la plus int√©ressante est que tous les PG inutiles ne sont pas autoris√©s √† regarder, c'est-√†-dire qu'il ne s'agit pas simplement d'un avertissement.  En cons√©quence, nous pensons que nous faisons tout correctement, en √©levant les limites, en d√©sactivant l'avertissement et en poursuivant. </p><br><p>  Un autre exemple r√©el est plus int√©ressant. </p><br><p>  S3, volume utile de 152 To, 252 OSD √† 1,81 To, ~ 105 PG sur OSD.  Le cluster a grandi progressivement, tout allait bien jusqu'√† ce que les nouvelles lois de notre pays imposent une croissance √† 1 PB, soit + ~ 850 To, et en m√™me temps, vous devez maintenir des performances, ce qui est maintenant assez bon pour S3.  Supposons que nous prenions des disques de 6 (5,7 r√©els) To et en tenant compte de la r√©plique 3, nous obtenons + 447 OSD.  En tenant compte des actuels, nous obtenons 699 OSD avec 37 PG chacun, et si nous prenons en compte des poids diff√©rents, il s'av√®re que les anciens OSD n'ont qu'une douzaine de PG.  Vous me dites donc dans quelle mesure cela fonctionnera?  Les performances d'un cluster avec un nombre diff√©rent de PG sont assez difficiles √† mesurer synth√©tiquement, mais les tests que j'ai men√©s montrent que pour des performances optimales, il est n√©cessaire de 50 PG √† 2 To OSD.  Et qu'en est-il de la poursuite de la croissance?  Sans augmenter le nombre de PG, vous pouvez acc√©der au mappage de PG √† OSD 1: 1.  Peut-√™tre que je ne comprends pas quelque chose? </p><br><p>  Oui, vous pouvez cr√©er un nouveau pool pour RGW avec le nombre de PG souhait√© et lui associer une r√©gion S3 distincte.  Ou m√™me cr√©ez un nouveau cluster √† proximit√©.  Mais vous devez admettre que ce sont toutes des b√©quilles.  Et il s'av√®re qu'il semble √™tre un Ceph bien √©volutif en raison de son concept, PG √©volue avec des r√©serves.  Vous devrez soit vivre avec des vorings d√©sactiv√©s en pr√©paration de la croissance, soit √† un moment donn√© reconstruire toutes les donn√©es du cluster, ou marquer des performances et vivre avec ce qui se passe.  Ou passez par tout cela. </p><br><p>  Je suis heureux que les d√©veloppeurs de Ceph <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">comprennent</a> que PG est une abstraction complexe et superflue pour l'utilisateur et qu'il vaut mieux ne pas le savoir. </p><br><blockquote>  "Dans Luminous, nous avons pris des mesures majeures pour √©liminer d√©finitivement l'un des moyens les plus courants de conduire votre cluster dans un foss√©, et nous nous effor√ßons de masquer compl√®tement les PG afin qu'ils ne soient pas quelque chose que la plupart des utilisateurs devront jamais savoir ou pensez √† ". </blockquote><p>  Dans vxFlex, il n'y a pas de concept de PG ou d'analogues.  Vous ajoutez simplement des disques au pool et c'est tout.  Et ainsi de suite jusqu'√† 16 PB.  Imaginez, rien n'a besoin d'√™tre calcul√©, il n'y a pas de tas de statuts de ces PG, les disques sont √©limin√©s uniform√©ment tout au long de la croissance.  Parce que  les disques sont donn√©s √† vxFlex dans son ensemble (il n'y a pas de syst√®me de fichiers au-dessus d'eux), il n'y a aucun moyen d'√©valuer la pl√©nitude et il n'y a pas de probl√®me du tout.  Je ne sais m√™me pas comment vous dire √† quel point c'est agr√©able. </p><br><h2 id="nuzhno-zhdat-sp1">  "Besoin d'attendre SP1" </h2><br><p>  Une autre histoire de ¬´succ√®s¬ª.  Comme vous le savez, RADOS est le stockage de valeur-cl√© le plus primitif.  S3, impl√©ment√© au-dessus de RADOS, est √©galement primitif, mais toujours un peu plus fonctionnel. ,  S3      .  ,   , RGW       .   ‚Äî  RADOS-,      OSD.         .   ,               . OSD            down.   ,      ,   .  ,   scrub'          .      ,    -  503,      . </p><br><p> <strong>Bucket Index resharding</strong> ‚Äî  ,       (RADOS-) , ,    OSD,         . </p><br><p> ,  ,        Jewel        !  Hammer,      ..    -.       ? </p><br><p>  Hammer       20+  ,       ,     OSD     Graylog ,    .     , ..   IO   .    Luminous, ..         .    Luminous,    , .   ,      . IO    index-,   ,         .    ,  IO     ,      . ,     ‚Ä¶   ; <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> : </p><br><p>  ,      .       , ..           ,   . </p><br><p> ,  Hammer-&gt;Jewel   -   . OSD     -  .        ,    OSD       . </p><br><p>    ‚Äî   ,     ,       .   Hammer    s3,   .      ,  .       ,    ,      etag,   body,     .           .      ,     . Suspend    .   ""           .            ,        . </p><br><h2 id="holivary-na-temu-chisla-replik">      </h2><br><p>       ,    2 ‚Äî  ,         Cloudmouse. ,    Ceph, , . </p><br><p>  vxFlex OS   2    . ,             .       ,    .           ,         .        ,    ,    ,     Dell EMC. </p><br><h2 id="proizvoditelnost">  Performances </h2><br><p>    .       ,       ?  . ,      .   ,     Ceph, vxFlex          .       -  .        ,               . </p><br><p>   9   ceph-devel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> : ,     CPU  (  Xeon'  !)   IOPS  All-NVMe   Ceph 12.2.7  bluestore. </p><br><p> ,       ,  "" Ceph    .    (  Hammer)         Ceph    ,        s3     .  ,  ScaleIO  Ceph RBD   .   Ceph,     ‚Äî      CPU.       RDMA  InfiniBand, jemalloc   . ,    10-20 ,       iops,      io, Ceph      . vxFlex          .    ‚Äî  Ceph  system time,  scaleio ‚Äî io wait. ,    bluestore,      ,    ,         -, ,     Ceph.    ScaleIO  .  ,      , Ceph           Dell EMC. </p><br><p> ,       ,         PG.        (),     IO. -   PG       IO,     ,      . ,               nearfull.       ,    . </p><br><p>  vxFlex     -   ,      .       (   ceph-volume),         ,     . </p><br><h2 id="scrub"> Scrub </h2><br><p> , .  , ,      Ceph. </p><br><p>             ,      . " "    ‚Äî   -    ,    . ,      2 TB     &gt;50%,       Ceph,     .            .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> ,       . </p><br><p>  vxFlex OS         ,    ,     .        ‚Äî bandwidth  .            .         ,        . </p><br><p> ,  , vxFlex     scrub-error. Ceph      2   . </p><br><h2 id="monitoring">  </h2><br><p> Luminous ‚Äî     .        .    MGR-     Zabbix              (3 ).       .   ,   ,  -         IO  ,     gc, .   ‚Äî   RGW . </p><br><p><img src="https://habrastorage.org/webt/ys/ya/xq/ysyaxqffjukjtgkv3ecnu0_q2ro.png"></p><br><p>       .     . <br>      S3,    "" : </p><br><p><img src="https://habrastorage.org/webt/ax/ge/u5/axgeu5iyyjami3qszjo97akj6oc.png"></p><br><p>   Ceph  , ,   ,       ,    . </p><br><p>  ,   eph   Graylog   GELF    .  , ,  OSD down, out, failed  .          , ,   OSD    down  ,     . </p><br><p><img src="https://habrastorage.org/webt/sd/hz/av/sdhzavl-jyjrmajz5zlsjamujzo.png"></p><br><p> - ,    OSD       heartbeat      failed (.  ).    <code>vm.zone_reclaim_mode=1</code>     NUMA. </p><br><p>     Ceph.  c vxFlex   .       : </p><br><p><img src="https://habrastorage.org/webt/hu/nz/e2/hunze2e6ucygc4anzyep2wku050.png"></p><br><p>     : </p><br><p><img src="https://habrastorage.org/webt/jv/fa/ey/jvfaeyd7ql3kalcrcabrckekuqg.png"></p><br><p>  IO    : </p><br><p><img src="https://habrastorage.org/webt/sy/vp/lq/syvplqbfmjtek_wii033vszoq9s.png"></p><br><p>              IO,      Ceph. </p><br><p>    : </p><br><p><img src="https://habrastorage.org/webt/h2/8j/ff/h28jff_jzpstuucf5wfx8vrrmay.png"></p><br><p>   Ceph,    Luminous     .   2.0,    Mimic  ,      . </p><br><h2 id="vxflex-tozhe-ne-idealen"> vxFlex    </h2><br><p>     <strong>Degraded state</strong> ,          . </p><br><p>  vxFlex ‚Äî        RH   .   7.5  , .  Ceph   RBD  cephfs ‚Äî          . </p><br><p> vxFlex       Ceph. vxFlex ‚Äî    ,   , , . </p><br><p>     16 PB,     .  eph     2 PB ‚Ä¶ </p><br><h2 id="zaklyuchenie">  Conclusion </h2><br><p>  ,  Ceph       ,     ,      ,      Ceph ‚Äî .      . </p><br><p>       ,  Ceph       " ".       ,  "  ,   ,     R&amp;D,  - ".    .       " ",  Ceph      ,    ,     . </p><br><p>      Ceph  2k18  ,    .      24/7       ( S3, ,  EBS),             ,   Ceph    .  ,    .        ‚Äî       .         /     maintenance        backfilling  ,  c Ceph   , ,       . </p><br><p>      Ceph    ?  , "     ".      Ceph.    .      ,         ,   ,  ,    ‚Ä¶ <br>    ! <br>  HEALTH_OK! </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr422905/">https://habr.com/ru/post/fr422905/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr422895/index.html">Google veut tuer les URL</a></li>
<li><a href="../fr422897/index.html">Mauvais, mais le mien: comment √©crire du CSS vraiment horrible</a></li>
<li><a href="../fr422899/index.html">Sous surveillance vigilante: comment contr√¥ler les tarifs d'h√©bergement et tenir √† jour le catalogue VPS</a></li>
<li><a href="../fr422901/index.html">Un moniteur de fr√©quence cardiaque pour Poutine, ou qu'est-ce qu'un Ritmer</a></li>
<li><a href="../fr422903/index.html">Comment et pourquoi nous avons √©crit un service √©volutif hautement charg√© pour 1C: Enterprise: Java, PostgreSQL, Hazelcast</a></li>
<li><a href="../fr422907/index.html">Aide-m√©moire du robot aspirateur 2018</a></li>
<li><a href="../fr422909/index.html">10 vid√©os de discussion r√©tro 404 Festival les plus populaires</a></li>
<li><a href="../fr422915/index.html">Je recherche un senior sans bureau ni cookies: comment nous avons organis√© une recherche d'employ√©s 100% distants</a></li>
<li><a href="../fr422917/index.html">Je n'ai pas de bouche, mais je dois crier. R√©flexions sur l'IA et l'√©thique</a></li>
<li><a href="../fr422919/index.html">SIP √† v√©lo et conversation t√©l√©phonique dans le cloud</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>