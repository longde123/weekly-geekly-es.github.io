<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ôíÔ∏è üèúÔ∏è üéç Implantar aplicativos usando o docker swarm üêΩ üçï üòî</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="O sistema de recomenda√ß√£o de conte√∫do de v√≠deo on-line em que estamos trabalhando √© um desenvolvimento comercial fechado e, tecnicamente, √© um cluster...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Implantar aplicativos usando o docker swarm</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/471528/">  O sistema de recomenda√ß√£o de conte√∫do de v√≠deo on-line em que estamos trabalhando √© um desenvolvimento comercial fechado e, tecnicamente, √© um cluster multicomponente de seus pr√≥prios componentes de c√≥digo aberto.  O objetivo deste artigo √© descrever a introdu√ß√£o de um sistema de clustering swarm de docker para uma plataforma de teste, sem violar o fluxo de trabalho existente de nossos processos em um tempo limitado.  A narrativa apresentada a sua aten√ß√£o est√° dividida em duas partes.  A primeira parte descreve o CI / CD antes de usar o docker swarm e a segunda descreve o processo de implementa√ß√£o.  Aqueles que n√£o est√£o interessados ‚Äã‚Äãem ler a primeira parte podem passar com seguran√ßa para a segunda. <br><a name="habracut"></a><br><h3>  Parte I </h3><br>  No ano distante, distante, era necess√°rio configurar o processo de CI / CD o mais r√°pido poss√≠vel.  Uma das condi√ß√µes era n√£o usar o Docker <i>para implantar os</i> componentes desenvolvidos por v√°rios motivos: <br><br><ul><li>  para uma opera√ß√£o mais confi√°vel e est√°vel dos componentes na Produ√ß√£o (isto √©, de fato, o requisito de n√£o usar a virtualiza√ß√£o) </li><li>  Os principais desenvolvedores n√£o queriam trabalhar com o Docker (estranho, mas era apenas isso) </li><li>  por raz√µes ideol√≥gicas, gest√£o de P&amp;D </li></ul><br>  A infraestrutura, a pilha e os requisitos iniciais de amostra para MVP foram os seguintes: <br><br><ul><li>  4 servidores Intel¬Æ X5650 com Debian (mais uma m√°quina poderosa para desenvolvimento) </li><li>  O desenvolvimento de componentes personalizados √© realizado em C ++, Python3 </li><li>  As principais ferramentas utilizadas por terceiros: Kafka, Clickhouse, Airflow, Redis, Grafana, Postgresql, Mysql, ... </li><li>  Montagem de tubula√ß√µes e teste de componentes separadamente para depura√ß√£o e libera√ß√£o </li></ul><br>  Um dos primeiros problemas a serem resolvidos no est√°gio inicial √© como implantar componentes personalizados em qualquer ambiente (CI / CD). <br><br>  Componentes de terceiros decidiram instalar sistematicamente e atualiz√°-los sistemicamente.  Aplicativos personalizados desenvolvidos em C ++ ou Python podem ser implantados de v√°rias maneiras.  Entre eles, por exemplo: cria√ß√£o de pacotes do sistema, enviando-os para o reposit√≥rio de imagens coletadas e sua instala√ß√£o subsequente em servidores.  Por um motivo desconhecido, outro m√©todo foi escolhido, a saber: usando o IC, os arquivos de aplicativos execut√°veis ‚Äã‚Äãs√£o compilados, um ambiente de projeto virtual √© criado, py-modules a partir de requirements.txt s√£o instalados e todos esses artefatos s√£o enviados juntamente com configura√ß√µes, scripts e o ambiente de aplicativo que os acompanha aos servidores.  Em seguida, os aplicativos s√£o iniciados a partir de um usu√°rio virtual sem direitos de administrador. <br><br>  O Gitlab-CI foi escolhido como o sistema CI / CD.  O pipeline resultante ficou mais ou menos assim: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/460/406/c27/460406c27d873dc28928ac3ba901f903.png" alt="imagem"><br><div class="spoiler">  <b class="spoiler_title">Estruturalmente, o gitlab-ci.yml ficou assim</b> <div class="spoiler_text"><pre><code class="xml hljs">--- variables: #     ,    CMAKE_CPUTYPE: "westmere" DEBIAN: "MYREGISTRY:5000/debian:latest" before_script: - eval $(ssh-agent -s) - ssh-add <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">(echo</span></span></span><span class="hljs-tag"> "$</span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">SSH_PRIVATE_KEY</span></span></span><span class="hljs-tag">") </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">-</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">mkdir</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">-p</span></span></span><span class="hljs-tag"> ~/</span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">.ssh</span></span></span><span class="hljs-tag"> &amp;&amp; </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">echo</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">-e</span></span></span><span class="hljs-tag"> "</span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">Host</span></span></span><span class="hljs-tag"> *\</span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">n</span></span></span><span class="hljs-tag">\</span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">tStrictHostKeyChecking</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">no</span></span></span><span class="hljs-tag">\</span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">n</span></span></span><span class="hljs-tag">\</span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">n</span></span></span><span class="hljs-tag">" &gt;</span></span> ~/.ssh/config stages: - build - testing - deploy debug.debian: stage: build image: $DEBIAN script: - cd builds/release &amp;&amp; ./build.sh paths: - bin/ - builds/release/bin/ when: always release.debian: stage: build image: $DEBIAN script: - cd builds/release &amp;&amp; ./build.sh paths: - bin/ - builds/release/bin/ when: always ## testing stage tests.codestyle: stage: testing image: $DEBIAN dependencies: - release.debian script: - /bin/bash run_tests.sh -t codestyle -b "${CI_COMMIT_REF_NAME}_codestyle" tests.debug.debian: stage: testing image: $DEBIAN dependencies: - debug.debian script: - /bin/bash run_tests.sh -e codestyle/test_pylint.py -b "${CI_COMMIT_REF_NAME}_debian_debug" artifacts: paths: - run_tests/username/ when: always expire_in: 1 week tests.release.debian: stage: testing image: $DEBIAN dependencies: - release.debian script: - /bin/bash run_tests.sh -e codestyle/test_pylint.py -b "${CI_COMMIT_REF_NAME}_debian_release" artifacts: paths: - run_tests/username/ when: always expire_in: 1 week ## staging stage deploy_staging: stage: deploy environment: staging image: $DEBIAN dependencies: - release.debian script: - cd scripts/deploy/ &amp;&amp; python3 createconfig.py -s $CI_ENVIRONMENT_NAME &amp;&amp; /bin/bash install_venv.sh -d -r ../../requirements.txt &amp;&amp; python3 prepare_init.d.py &amp;&amp; python3 deploy.py -s $CI_ENVIRONMENT_NAME when: manual</code> </pre> <br></div></div><br>  Vale ressaltar que a montagem e o teste s√£o feitos em sua pr√≥pria imagem, onde todos os pacotes de sistema necess√°rios j√° est√£o instalados e outras configura√ß√µes s√£o feitas. <br><br>  Embora cada um desses scripts no trabalho seja interessante √† sua maneira, <s>certamente n√£o falarei sobre eles</s> , mas a descri√ß√£o de cada um deles levar√° um tempo consider√°vel e esse n√£o √© o objetivo do artigo.  Prestarei aten√ß√£o apenas ao fato de que o est√°gio de implanta√ß√£o consiste em uma sequ√™ncia de chamadas de script: <br><br><ol><li>  <b>createconfig.py</b> - cria o arquivo settings.ini com as configura√ß√µes dos componentes em um ambiente diferente para a implanta√ß√£o subsequente (pr√©-produ√ß√£o, produ√ß√£o, teste, ...) </li><li>  <b>install_venv.sh</b> - cria um ambiente virtual para componentes py em um diret√≥rio espec√≠fico e o copia para servidores remotos </li><li>  <b>prepare_init.d.py</b> - prepara scripts de in√≠cio / parada do componente com base em um modelo </li><li>  <b>deploy.py</b> - <b>descomprima</b> e reinicie novos componentes </li></ol><br>  O tempo passou.  O est√°gio foi substitu√≠do pela pr√©-produ√ß√£o e produ√ß√£o.  O suporte ao produto foi adicionado em outro kit de distribui√ß√£o (CentOS).  Foram adicionados 5 servidores f√≠sicos mais poderosos e uma d√∫zia de servidores virtuais.  E ficou cada vez mais dif√≠cil para desenvolvedores e testadores executar suas tarefas em um ambiente mais ou menos pr√≥ximo do estado de trabalho.  Neste momento, ficou claro que √© imposs√≠vel ficar sem ele ... <br><br><h3>  Parte II </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/f0e/673/166/f0e67316667fed0c0d673653419e8b8a.png" alt="imagem"><br><br>  Portanto, nosso cluster <s>ainda</s> √© o <s>espet√°culo de um</s> sistema de duas d√∫zias de componentes separados que n√£o s√£o descritos pelo Dockerfiles.  Voc√™ pode configur√°-lo para implanta√ß√£o em um ambiente espec√≠fico apenas como um todo.  Nossa tarefa √© implantar o cluster em um ambiente intermedi√°rio para execut√°-lo antes do teste de pr√©-lan√ßamento. <br><br>  Teoricamente, pode haver v√°rios clusters trabalhando simultaneamente: tantas tarefas est√£o no estado conclu√≠do ou pr√≥ximas da conclus√£o.  As capacidades dispon√≠veis para nossos servidores nos permitem executar v√°rios clusters em cada servidor.  Cada cluster de temporariedade deve ser isolado (n√£o deve haver interse√ß√£o em portas, diret√≥rios etc.). <br><br>  O recurso mais valioso √© o nosso tempo, e n√£o t√≠nhamos muito. <br><br>  Para um in√≠cio mais r√°pido, eles escolheram o Docker Swarm devido √† sua simplicidade e flexibilidade de arquitetura.  A primeira coisa que fizemos foi criar nos servidores do gerenciador remoto e em v√°rios n√≥s: <br><br><pre> <code class="bash hljs">$ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION kilqc94pi2upzvabttikrfr5d nop-test-1 Ready Active 19.03.2 jilwe56pl2zvabupryuosdj78 nop-test-2 Ready Active 19.03.2 j5a4yz1kr2xke6b1ohoqlnbq5 * nop-test-3 Ready Active Leader 19.03.2</code> </pre><br>  Em seguida, criamos uma rede: <br><br><pre> <code class="bash hljs">$ docker network create --driver overlay --subnet 10.10.10.0/24 nw_swarm</code> </pre><br>  Em seguida, eles conectaram os n√≥s Gitlab-CI e Swarm em termos de gerenciamento remoto de n√≥s de CI: instala√ß√£o de certificados, configura√ß√£o de vari√°veis ‚Äã‚Äãsecretas e instala√ß√£o do servi√ßo Docker no servidor de gerenciamento.  Este <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo</a> economizou muito tempo. <br><br>  Em seguida, adicionamos trabalhos para criar e destruir a pilha no .gitlab-ci .yml. <br><br><div class="spoiler">  <b class="spoiler_title">Mais alguns trabalhos foram adicionados ao .gitlab-ci .yml</b> <div class="spoiler_text"><pre> <code class="xml hljs">## staging stage deploy_staging: stage: testing before_script: - echo "override global 'before_script'" image: "REGISTRY:5000/docker:latest" environment: staging dependencies: [] variables: DOCKER_CERT_PATH: "/certs" DOCKER_HOST: tcp://10.50.173.107:2376 DOCKER_TLS_VERIFY: 1 CI_BIN_DEPENDENCIES_JOB: "release.centos.7" script: - mkdir -p $DOCKER_CERT_PATH - echo "$TLSCACERT" &gt; $DOCKER_CERT_PATH/ca.pem - echo "$TLSCERT" &gt; $DOCKER_CERT_PATH/cert.pem - echo "$TLSKEY" &gt; $DOCKER_CERT_PATH/key.pem - docker stack deploy -c docker-compose.yml ${CI_ENVIRONMENT_NAME}_${CI_COMMIT_REF_NAME} --with-registry-auth - rm -rf $DOCKER_CERT_PATH when: manual ## stop staging stage stop_staging: stage: testing before_script: - echo "override global 'before_script'" image: "REGISTRY:5000/docker:latest" environment: staging dependencies: [] variables: DOCKER_CERT_PATH: "/certs" DOCKER_HOST: tcp://10.50.173.107:2376 DOCKER_TLS_VERIFY: 1 script: - mkdir -p $DOCKER_CERT_PATH - echo "$TLSCACERT" &gt; $DOCKER_CERT_PATH/ca.pem - echo "$TLSCERT" &gt; $DOCKER_CERT_PATH/cert.pem - echo "$TLSKEY" &gt; $DOCKER_CERT_PATH/key.pem - docker stack rm ${CI_ENVIRONMENT_NAME}_${CI_COMMIT_REF_NAME} # TODO: need check that stopped when: manual</code> </pre><br></div></div><br>  No snippet de c√≥digo acima, fica claro que dois bot√µes (deploy_staging, stop_staging) foram adicionados aos pipelines que requerem interven√ß√£o manual. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2de/e3c/790/2dee3c7907f75f793499665d8a60de8a.png" alt="imagem"><br>  O nome da pilha corresponde ao nome da ramifica√ß√£o e essa exclusividade deve ser suficiente.  Os servi√ßos na pilha recebem endere√ßos IP exclusivos, portas, diret√≥rios etc.  ser√° isolado, mas o mesmo de pilha para pilha (porque o arquivo de configura√ß√£o √© o mesmo para todas as pilhas) - foi o que alcan√ßamos.  Implementamos <b>a pilha</b> (cluster) usando o <b>docker-compose.yml</b> , que descreve nosso cluster. <br><br><div class="spoiler">  <b class="spoiler_title">docker-compose.yml</b> <div class="spoiler_text"><pre> <code class="xml hljs">--- version: '3' services: userprop: image: redis:alpine deploy: replicas: 1 placement: constraints: [node.id == kilqc94pi2upzvabttikrfr5d] restart_policy: condition: none networks: nw_swarm: celery_bcd: image: redis:alpine deploy: replicas: 1 placement: constraints: [node.id == kilqc94pi2upzvabttikrfr5d] restart_policy: condition: none networks: nw_swarm: schedulerdb: image: mariadb:latest environment: MYSQL_ALLOW_EMPTY_PASSWORD: 'yes' MYSQL_DATABASE: schedulerdb MYSQL_USER: **** MYSQL_PASSWORD: **** command: ['--character-set-server=utf8mb4', '--collation-server=utf8mb4_unicode_ci', '--explicit_defaults_for_timestamp=1'] deploy: replicas: 1 placement: constraints: [node.id == kilqc94pi2upzvabttikrfr5d] restart_policy: condition: none networks: nw_swarm: celerydb: image: mariadb:latest environment: MYSQL_ALLOW_EMPTY_PASSWORD: 'yes' MYSQL_DATABASE: celerydb MYSQL_USER: **** MYSQL_PASSWORD: **** deploy: replicas: 1 placement: constraints: [node.id == kilqc94pi2upzvabttikrfr5d] restart_policy: condition: none networks: nw_swarm: cluster: image: $CENTOS7 environment: - CENTOS - CI_ENVIRONMENT_NAME - CI_API_V4_URL - CI_REPOSITORY_URL - CI_PROJECT_ID - CI_PROJECT_URL - CI_PROJECT_PATH - CI_PROJECT_NAME - CI_COMMIT_REF_NAME - CI_BIN_DEPENDENCIES_JOB command: &gt; sudo -u myusername -H /bin/bash -c ". /etc/profile &amp;&amp; mkdir -p /storage1/$CI_COMMIT_REF_NAME/$CI_PROJECT_NAME &amp;&amp; cd /storage1/$CI_COMMIT_REF_NAME/$CI_PROJECT_NAME &amp;&amp; git clone -b $CI_COMMIT_REF_NAME $CI_REPOSITORY_URL . &amp;&amp; curl $CI_API_V4_URL/projects/$CI_PROJECT_ID/jobs/artifacts/$CI_COMMIT_REF_NAME/download?job=$CI_BIN_DEPENDENCIES_JOB -o artifacts.zip &amp;&amp; unzip artifacts.zip ; cd /storage1/$CI_COMMIT_REF_NAME/$CI_PROJECT_NAME/scripts/deploy/ &amp;&amp; python3 createconfig.py -s $CI_ENVIRONMENT_NAME &amp;&amp; /bin/bash install_venv.sh -d -r ../../requirements.txt &amp;&amp; python3 prepare_init.d.py &amp;&amp; python3 deploy.py -s $CI_ENVIRONMENT_NAME" deploy: replicas: 1 placement: constraints: [node.id == kilqc94pi2upzvabttikrfr5d] restart_policy: condition: none tty: true stdin_open: true networks: nw_swarm: networks: nw_swarm: external: true</code> </pre><br></div></div><br>  Aqui voc√™ pode ver que os componentes est√£o conectados por uma rede (nw_swarm) e s√£o acess√≠veis um ao outro. <br><br>  Os componentes do sistema (baseados em redis, mysql) s√£o separados do pool comum de componentes customizados (os planos e customizados s√£o divididos como servi√ßos).  O est√°gio de implanta√ß√£o de nosso cluster se parece com a transfer√™ncia de CMD para uma imagem grande configurada e, como um todo, praticamente n√£o difere da implanta√ß√£o descrita na Parte I. Destaco as diferen√ßas: <br><br><ul><li>  <b>git clone ...</b> - obtemos os arquivos necess√°rios para fazer uma implementa√ß√£o (createconfig.py, install_venv.sh, etc.) </li><li>  <b>enrolar ... &amp;&amp; descompactar ...</b> - baixar e descompactar artefatos de montagem (utilit√°rios compilados) </li></ul><br>  H√° apenas um problema que ainda n√£o foi descrito: os componentes que possuem uma interface da web n√£o podem ser acessados ‚Äã‚Äãpelos navegadores de desenvolvedor.  Resolvemos esse problema usando o proxy reverso, assim: <br><br>  Em .gitlab-ci.yml, ap√≥s implantar a pilha do cluster, adicione a linha de implanta√ß√£o do balanceador (que, ao confirmar, atualiza apenas sua configura√ß√£o (cria novos arquivos de configura√ß√£o nginx usando o modelo: /etc/nginx/conf.d/${CI_COMMIT_REF_NAME‚ñ∫.conf) - consulte o c√≥digo docker-compose-nginx.yml) <br><br><pre> <code class="xml hljs"> - docker stack deploy -c docker-compose-nginx.yml ${CI_ENVIRONMENT_NAME} --with-registry-auth</code> </pre><br><div class="spoiler">  <b class="spoiler_title">docker-compose-nginx.yml</b> <div class="spoiler_text"><pre> <code class="xml hljs">--- version: '3' services: nginx: image: nginx:latest environment: CI_COMMIT_REF_NAME: ${CI_COMMIT_REF_NAME} NGINX_CONFIG: |- server { listen 8080; server_name staging_${CI_COMMIT_REF_NAME}_cluster.dev; location / { proxy_pass http://staging_${CI_COMMIT_REF_NAME}_cluster:8080; } } server { listen 5555; server_name staging_${CI_COMMIT_REF_NAME}_cluster.dev; location / { proxy_pass http://staging_${CI_COMMIT_REF_NAME}_cluster:5555; } } volumes: - /tmp/staging/nginx:/etc/nginx/conf.d command: /bin/bash -c "echo -e \"$$NGINX_CONFIG\" &gt; /etc/nginx/conf.d/${CI_COMMIT_REF_NAME}.conf; nginx -g \"daemon off;\"; /etc/init.d/nginx reload" ports: - 8080:8080 - 5555:5555 - 3000:3000 - 443:443 - 80:80 deploy: replicas: 1 placement: constraints: [node.id == kilqc94pi2upzvabttikrfr5d] restart_policy: condition: none networks: nw_swarm: networks: nw_swarm: external: true</code> </pre><br></div></div><br>  Nos computadores de desenvolvimento, atualize / etc / hosts;  registrar url no nginx: <br><br> <code>10.50.173.106 staging_BRANCH-1831_cluster.dev <br></code> <br>  Portanto, a implanta√ß√£o de clusters de armazenamento tempor√°rio isolados foi implementada e os desenvolvedores agora podem inici√°-los em quantidade suficiente para testar suas tarefas. <br><br>  Planos adicionais: <br><br><ul><li>  Separe nossos componentes como servi√ßos </li><li>  Crie para todos os arquivos do Docker </li><li>  Detectar automaticamente n√≥s menos carregados na pilha </li><li>  Definir n√≥s por padr√£o de nome (em vez de usar o id como no artigo) </li><li>  Adicione verifica√ß√£o de que a pilha est√° destru√≠da </li><li>  ... </li></ul><br>  Agradecimentos especiais para o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt471528/">https://habr.com/ru/post/pt471528/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt471516/index.html">Intel 665p - SSD com QLC NAND de 96 camadas</a></li>
<li><a href="../pt471518/index.html">Apple em 2019 √© Linux em 2000</a></li>
<li><a href="../pt471520/index.html">O livro "Tarefas cl√°ssicas de ci√™ncia da computa√ß√£o em Python"</a></li>
<li><a href="../pt471522/index.html">Askozia. Como funciona o Plug & Play de provisionamento autom√°tico</a></li>
<li><a href="../pt471524/index.html">Tradu√ß√£o completa de instru√ß√µes para avaliadores do Google</a></li>
<li><a href="../pt471530/index.html">O GitLab percorreu um caminho incomum para CI / CD e Kubernetes</a></li>
<li><a href="../pt471532/index.html">Adeus PCB; ol√° interconex√£o de silicone</a></li>
<li><a href="../pt471536/index.html">Previs√£o de inunda√ß√£o do Google: um olhar por dentro</a></li>
<li><a href="../pt471538/index.html">Da id√©ia de um aplicativo m√≥vel ao MVP no qual os investidores investir√£o</a></li>
<li><a href="../pt471542/index.html">Reconhecimento de texto OCR</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>