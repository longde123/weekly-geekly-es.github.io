<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèæ‚Äçü§ù‚Äçüë®üèΩ üîÑ üë®üèΩ‚ÄçüöÄ Immersion dans les r√©seaux de neurones convolutifs: transfert d'apprentissage üêÆ üë®üèΩ‚Äçüé§ üë©‚ÄçüöÄ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Le cours complet de russe se trouve sur ce lien . 
 Le cours d'anglais original est disponible sur ce lien . 



 Table des mati√®res 


1. Entretien a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Immersion dans les r√©seaux de neurones convolutifs: transfert d'apprentissage</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/467967/"><p>  Le cours complet de russe se trouve sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ce lien</a> . <br>  Le cours d'anglais original est disponible sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ce lien</a> . </p><br><p><img src="https://habrastorage.org/webt/wu/ie/7c/wuie7cgpktklm4bytoweu7ki0oq.jpeg"></p><a name="habracut"></a><br><h1>  Table des mati√®res </h1><br><ol><li>  Entretien avec Sebastian Trun </li><li>  Pr√©sentation </li><li>  Transfert du mod√®le d'apprentissage </li><li>  MobileNet </li><li>  CoLab: Cats Vs Dogs with Transfer Training </li><li>  Plonger dans des r√©seaux de neurones convolutifs </li><li>  Partie pratique: d√©termination des couleurs avec transfert de formation </li><li>  R√©sum√© </li></ol><br><h1>  Entretien avec Sebastian Trun </h1><br><p>  - Il s'agit de la le√ßon 6 et elle est enti√®rement d√©di√©e au transfert d'apprentissage.  Le transfert d'apprentissage est le processus d'utilisation d'un mod√®le existant avec peu de raffinement pour de nouvelles t√¢ches.  Le transfert de la formation contribue √† r√©duire le temps de formation du mod√®le en donnant une certaine augmentation de l'efficacit√© lors de l'apprentissage au tout d√©but.  S√©bastien, que penses-tu du transfert de formation?  Avez-vous d√©j√† pu utiliser la m√©thodologie de transfert d'enseignement dans votre travail et votre recherche? <br>  - Ma th√®se √©tait uniquement consacr√©e au th√®me du transfert de formation et s'intitulait " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Explication sur la base du transfert de formation</a> ".  Lorsque nous travaillions sur une dissertation, l'id√©e √©tait qu'il √©tait possible d'enseigner √† distinguer tous les autres objets de ce type sur un seul objet (ensemble de donn√©es, entit√©) dans diverses variations et formats.  Dans le travail, nous avons utilis√© l'algorithme d√©velopp√©, qui a distingu√© les principales caract√©ristiques (attributs) de l'objet et a pu les comparer avec un autre objet.  Les biblioth√®ques comme Tensorflow sont d√©j√† livr√©es avec des mod√®les pr√©-form√©s. <br>  - Oui, chez Tensorflow, nous avons un ensemble complet de mod√®les pr√©-form√©s que vous pouvez utiliser pour r√©soudre des probl√®mes pratiques.  Nous parlerons des sets pr√™ts √† l'emploi un peu plus tard. <br>  - Oui, oui!  Si vous y r√©fl√©chissez, alors les gens sont impliqu√©s dans le transfert de formation tout au long de leur vie. <br>  - Peut-on dire que gr√¢ce √† la m√©thode de transfert de la formation, nos nouveaux √©tudiants n'auront pas √† un moment ou √† un autre la connaissance de l'apprentissage automatique car il suffira de connecter un mod√®le d√©j√† pr√©par√© et de l'utiliser? <br>  - La programmation consiste √† √©crire ligne par ligne, nous donnons des commandes √† l'ordinateur.  Notre objectif est de nous assurer que tout le monde sur la plan√®te est en mesure de programmer en fournissant √† l'ordinateur uniquement des exemples de donn√©es d'entr√©e.  D'accord, si vous voulez apprendre √† un ordinateur √† distinguer les chats des chiens, il est assez difficile de trouver 100 000 images diff√©rentes de chats et 100 000 images diff√©rentes de chiens, et gr√¢ce au transfert de formation, vous pouvez r√©soudre ce probl√®me en plusieurs lignes. <br>  - Oui, c'est vraiment √ßa!  Merci pour les r√©ponses et passons enfin √† l'apprentissage. </p><br><h1>  Pr√©sentation </h1><br><p>  - Bonjour et bon retour! <br>  - La derni√®re fois, nous avons form√© un r√©seau neuronal convolutif pour classer les chats et les chiens dans l'image.  Notre premier r√©seau de neurones a √©t√© recycl√©, donc son r√©sultat n'√©tait pas si √©lev√© - une pr√©cision d'environ 70%.  Apr√®s cela, nous avons impl√©ment√© l'extension et le d√©crochage des donn√©es (d√©connexion arbitraire des neurones), ce qui nous a permis d'augmenter la pr√©cision des pr√©dictions jusqu'√† 80%. <br>  - Malgr√© le fait que 80% peut sembler un excellent indicateur, l'erreur de 20% est encore trop importante.  Non?  Que pouvons-nous faire pour augmenter la pr√©cision de la classification?  Dans cette le√ßon, nous utiliserons la technique de transfert de connaissances (transfert du mod√®le de connaissances), qui nous permettra d'utiliser le mod√®le d√©velopp√© par des experts et form√© sur d'√©normes matrices de donn√©es.  Comme nous le verrons dans la pratique, en transf√©rant le mod√®le de connaissances, nous pouvons atteindre une pr√©cision de classification de 95%.  Commen√ßons! </p><br><h1>  Transfert de mod√®le d'apprentissage </h1><br><p>  En 2012, le r√©seau neuronal AlexNet a r√©volutionn√© le monde de l'apprentissage automatique et popularis√© l'utilisation des r√©seaux de neurones convolutifs pour la classification en remportant le d√©fi de reconnaissance visuelle √† grande √©chelle ImageNet. </p><br><p><img src="https://habrastorage.org/webt/fs/xx/di/fsxxdicwkitfkxibie8kkjcyexu.png"></p><br><p>  Apr√®s cela, la lutte a commenc√© √† d√©velopper des r√©seaux de neurones plus pr√©cis et efficaces qui pourraient d√©passer AlexNet dans les t√¢ches de classification des images de l'ensemble de donn√©es ImageNet. </p><br><p><img src="https://habrastorage.org/webt/oe/t0/ov/oet0ovye40p1cziih64hpmbhuus.png"></p><br><p>  Depuis plusieurs ann√©es, des r√©seaux de neurones ont √©t√© d√©velopp√©s pour mieux g√©rer la t√¢che de classification qu'AlexNet - Inception et ResNet. <br>  √ätes-vous d'accord pour dire que ce serait formidable de pouvoir profiter de ces r√©seaux de neurones d√©j√† form√©s sur d'√©normes ensembles de donn√©es d'ImageNet et de les utiliser dans votre classificateur pour chats et chiens? </p><br><p>  Il s'av√®re que nous pouvons le faire!  La technique est appel√©e apprentissage par transfert.  L'id√©e principale de la m√©thode de transfert du mod√®le d'apprentissage est bas√©e sur le fait qu'ayant form√© un r√©seau neuronal sur un grand ensemble de donn√©es, nous pouvons appliquer le mod√®le obtenu √† un ensemble de donn√©es que ce mod√®le n'a pas encore rencontr√©.  C'est pourquoi la technique est appel√©e transfert d'apprentissage - transfert du processus d'apprentissage d'un ensemble de donn√©es √† un autre. </p><br><p>  Afin d'appliquer la m√©thodologie de transfert du mod√®le d'apprentissage, nous devons changer la derni√®re couche de notre r√©seau neuronal convolutionnel: </p><br><p><img src="https://habrastorage.org/webt/3j/-g/g3/3j-gg3yxm9kvrtphiswnho2jgtc.png"></p><br><p>  Nous effectuons cette op√©ration car chaque ensemble de donn√©es se compose d'un nombre diff√©rent de classes de sortie.  Par exemple, les ensembles de donn√©es dans ImageNet contiennent 1000 classes de sortie diff√©rentes.  FashionMNIST contient 10 classes.  Notre ensemble de donn√©es de classification se compose de seulement 2 classes - chats et chiens. </p><br><p><img src="https://habrastorage.org/webt/5e/pm/ej/5epmejbamklkdfgzzw9v8rzb1ts.png"></p><br><p>  C'est pourquoi il est n√©cessaire de changer la derni√®re couche de notre r√©seau de neurones convolutionnels afin qu'elle contienne le nombre de sorties qui correspondrait au nombre de classes dans le nouvel ensemble. </p><br><p><img src="https://habrastorage.org/webt/cg/mk/fz/cgmkfzxqmyqbdzsmvfdppjhsl9e.png"></p><br><p>  Nous devons √©galement nous assurer de ne pas modifier le mod√®le pr√©-form√© pendant le processus de formation.  La solution consiste √† d√©sactiver les variables du mod√®le pr√©-form√© - nous interdisons simplement √† l'algorithme de mettre √† jour les valeurs pendant la propagation avant et arri√®re pour les modifier. <br>  Ce processus est appel√© le ¬´gel du mod√®le¬ª. </p><br><p><img src="https://habrastorage.org/webt/8z/oz/0n/8zoz0nenaad4-lgezhhia25k_18.png"></p><br><p>  En ¬´gelant¬ª les param√®tres du mod√®le pr√©-form√©, nous permettons d'apprendre uniquement la derni√®re couche du r√©seau de classification, les valeurs des variables du pr√©-form√© restent inchang√©es. </p><br><p>  Un autre avantage incontestable des mod√®les pr√©-form√©s est que nous r√©duisons le temps de formation en formant uniquement la derni√®re couche avec un nombre de variables significativement plus petit, et non le mod√®le entier. </p><br><p>  Si nous ne ¬´gelons¬ª pas les variables du mod√®le pr√©-form√©, alors pendant le processus de formation, les valeurs des variables changeront sur le nouvel ensemble de donn√©es.  En effet, les valeurs des variables de la derni√®re couche de la classification seront remplies de valeurs al√©atoires.  En raison de valeurs al√©atoires sur la derni√®re couche, notre mod√®le fera de grosses erreurs dans la classification, ce qui entra√Ænera √† son tour de forts changements dans les poids initiaux du mod√®le pr√©-form√©, ce qui est extr√™mement ind√©sirable pour nous. </p><br><p><img src="https://habrastorage.org/webt/wp/uz/km/wpuzkmnan5rahfg3irexdsvrstm.png"></p><br><p>  C'est pour cette raison que nous devons toujours nous rappeler que lors de l'utilisation de mod√®les existants, les valeurs des variables doivent √™tre ¬´gel√©es¬ª et la n√©cessit√© de former un mod√®le pr√©-form√© doit √™tre d√©sactiv√©e. </p><br><p>  Maintenant que nous savons comment fonctionne le transfert du mod√®le de formation, il nous suffit de choisir un r√©seau neuronal pr√©-form√© √† utiliser dans notre propre classificateur!  C'est ce que nous ferons dans la partie suivante. </p><br><h1>  MobileNet </h1><br><p>  Comme nous l'avons mentionn√© pr√©c√©demment, des r√©seaux de neurones extr√™mement efficaces ont √©t√© d√©velopp√©s qui ont montr√© des r√©sultats √©lev√©s sur les ensembles de donn√©es ImageNet - AlexNet, Inception, Resonant.  Ces r√©seaux de neurones sont des r√©seaux tr√®s profonds et contiennent des milliers voire des millions de param√®tres.  Un grand nombre de param√®tres permet au r√©seau d'apprendre des mod√®les plus complexes et ainsi d'obtenir une pr√©cision de classification accrue.  Un grand nombre de param√®tres d'apprentissage du r√©seau neuronal affecte la vitesse d'apprentissage, la quantit√© de m√©moire requise pour stocker le r√©seau et la complexit√© des calculs. </p><br><p>  Dans cette le√ßon, nous utiliserons le r√©seau neuronal convolutionnel moderne MobileNet.  MobileNet est une architecture de r√©seau neuronal convolutif efficace qui r√©duit la quantit√© de m√©moire utilis√©e pour le calcul tout en maintenant une grande pr√©cision des pr√©dictions.  C'est pourquoi MobileNet est id√©al pour une utilisation sur des appareils mobiles avec une quantit√© limit√©e de m√©moire et de ressources informatiques. </p><br><p>  MobileNet a √©t√© d√©velopp√© par Google et form√© sur l'ensemble de donn√©es ImageNet. </p><br><p>  √âtant donn√© que MobileNet a √©t√© form√© dans 1 000 classes √† partir de l'ensemble de donn√©es ImageNet, MobileNet a 1 000 classes de sortie, au lieu des deux dont nous avons besoin - un chat et un chien. </p><br><p><img src="https://habrastorage.org/webt/he/2f/ox/he2foxizd_xmt7rijxumteg-94k.png"></p><br><p>  Pour terminer le transfert de la formation, nous pr√©chargeons le vecteur d'entit√©s sans couche de classification: </p><br><p><img src="https://habrastorage.org/webt/1b/o2/no/1bo2nop9cpz3ebag_jcyfrgje7w.png"></p><br><p>  Dans Tensorflow, un vecteur d'entit√© charg√© peut √™tre utilis√© comme une couche Keras r√©guli√®re avec des donn√©es d'entr√©e d'une certaine taille. </p><br><p>  √âtant donn√© que MobileNet a √©t√© form√© sur l'ensemble de donn√©es ImageNet, nous devrons apporter la taille des donn√©es d'entr√©e √† celles qui ont √©t√© utilis√©es dans le processus de formation.  Dans notre cas, MobileNet a √©t√© form√© sur des images RVB de taille fixe 224x224px. </p><br><p>  TensorFlow contient un r√©f√©rentiel pr√©-form√© appel√© TensorFlow Hub. </p><br><p><img src="https://habrastorage.org/webt/o5/we/9d/o5we9dvkdtaomahwj4nzomgquca.png"></p><br><p>  TensorFlow Hub contient des mod√®les pr√©-form√©s dans lesquels la derni√®re couche de classification a √©t√© exclue de l'architecture du r√©seau neuronal pour une r√©utilisation ult√©rieure. </p><br><p>  Vous pouvez utiliser le TensorFlow Hub dans le code sur plusieurs lignes: </p><br><p><img src="https://habrastorage.org/webt/1h/ai/hg/1haihgg1llinsfv_wyhde4z0egy.png"></p><br><p>  Il suffit de sp√©cifier l'URL du vecteur d'entit√©s du mod√®le d'apprentissage souhait√©, puis d'int√©grer le mod√®le dans notre classificateur avec la derni√®re couche avec le nombre de classes de sortie souhait√©.  C'est la derni√®re couche qui sera soumise √† la formation et √† la modification des valeurs des param√®tres.  La compilation et la formation de notre nouveau mod√®le s'effectuent de la m√™me mani√®re que pr√©c√©demment: </p><br><p><img src="https://habrastorage.org/webt/uh/cr/5e/uhcr5esktfnxtxwxfgutvttxbxk.png"></p><br><p>  Voyons comment cela fonctionnera r√©ellement et √©crivons le code appropri√©. </p><br><h1>  CoLab: Cats Vs Dogs with Transfer Training </h1><br><p>  Lien vers <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">CoLab en russe</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">CoLab en anglais</a> . </p><br><p>  TensorFlow Hub est un r√©f√©rentiel avec des mod√®les pr√©-form√©s que nous pouvons utiliser. </p><br><p>  Le transfert d'apprentissage est un processus dans lequel nous prenons un mod√®le pr√©-form√© et l'√©largissons pour effectuer une t√¢che sp√©cifique.  Dans le m√™me temps, nous laissons la partie du mod√®le pr√©-form√© que nous int√©grons dans le r√©seau neuronal intacte, mais ne formons que les derni√®res couches de sortie pour obtenir le r√©sultat souhait√©. </p><br><p>  Dans cette partie pratique, nous allons tester les deux options. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Ce lien</a> vous permet d'explorer la liste compl√®te des mod√®les disponibles. </p><br><p>  <strong>Dans cette partie de Colab</strong> </p><br><ol><li>  Nous utiliserons le mod√®le TensorFlow Hub pour les pr√©dictions; </li><li>  Nous utiliserons le mod√®le TensorFlow Hub pour l'ensemble de donn√©es des chats et des chiens; </li><li>  Transf√©rons la formation en utilisant le mod√®le du TensorFlow Hub. </li></ol><br><p> Avant de proc√©der √† la mise en ≈ìuvre de la partie pratique actuelle, nous vous recommandons de r√©initialiser le <code>Runtime -&gt; Reset all runtimes...</code> </p><br><p>  <strong>Importations de biblioth√®que</strong> </p><br><p>  Dans cette partie pratique, nous utiliserons un certain nombre de fonctionnalit√©s de la biblioth√®que TensorFlow qui ne sont pas encore dans la version officielle.  C'est pourquoi nous allons d'abord installer les versions TensorFlow et TensorFlow Hub pour les d√©veloppeurs. </p><br><p>  L'installation de la version de d√©veloppement de TensorFlow active automatiquement la derni√®re version install√©e.  Apr√®s avoir fini de traiter cette partie pratique, nous vous recommandons de restaurer les param√®tres TensorFlow et de revenir √† la version stable via l'√©l√©ment de menu <code>Runtime -&gt; Reset all runtimes...</code>  L'ex√©cution de cette commande r√©initialisera tous les param√®tres d'environnement aux param√®tres d'origine. </p><br><pre> <code class="python hljs">!pip install tf-nightly-gpu !pip install <span class="hljs-string"><span class="hljs-string">"tensorflow_hub==0.4.0"</span></span> !pip install -U tensorflow_datasets</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">Requirement already satisfied: absl-py&gt;=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.8.0) Requirement already satisfied: protobuf&gt;=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.7.1) Requirement already satisfied: google-pasta&gt;=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.1.7) Collecting tf-estimator-nightly (from tf-nightly-gpu) Downloading https://files.pythonhosted.org/packages/ea/72/f092fc631ef2602fd0c296dcc4ef6ef638a6a773cb9fdc6757fecbfffd33/tf_estimator_nightly-1.14.0.dev2019092201-py2.py3-none-any.whl (450kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 450kB 45.9MB/s Requirement already satisfied: numpy&lt;2.0,&gt;=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.16.5) Requirement already satisfied: wrapt&gt;=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.11.2) Requirement already satisfied: astor&gt;=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.8.0) Requirement already satisfied: opt-einsum&gt;=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.0.1) Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.33.6) Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications&gt;=1.0.8-&gt;tf-nightly-gpu) (2.8.0) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly&lt;1.16.0a0,&gt;=1.15.0a0-&gt;tf-nightly-gpu) (3.1.1) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly&lt;1.16.0a0,&gt;=1.15.0a0-&gt;tf-nightly-gpu) (41.2.0) Requirement already satisfied: werkzeug&gt;=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly&lt;1.16.0a0,&gt;=1.15.0a0-&gt;tf-nightly-gpu) (0.15.6) Installing collected packages: tb-nightly, tf-estimator-nightly, tf-nightly-gpu Successfully installed tb-nightly-1.15.0a20190911 tf-estimator-nightly-1.14.0.dev2019092201 tf-nightly-gpu-1.15.0.dev20190821 Collecting tensorflow_hub==0.4.0 Downloading https://files.pythonhosted.org/packages/10/5c/6f3698513cf1cd730a5ea66aec665d213adf9de59b34f362f270e0bd126f/tensorflow_hub-0.4.0-py2.py3-none-any.whl (75kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81kB 5.0MB/s Requirement already satisfied: protobuf&gt;=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.4.0) (3.7.1) Requirement already satisfied: numpy&gt;=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.4.0) (1.16.5) Requirement already satisfied: six&gt;=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.4.0) (1.12.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf&gt;=3.4.0-&gt;tensorflow_hub==0.4.0) (41.2.0) Installing collected packages: tensorflow-hub Found existing installation: tensorflow-hub 0.6.0 Uninstalling tensorflow-hub-0.6.0: Successfully uninstalled tensorflow-hub-0.6.0 Successfully installed tensorflow-hub-0.4.0 Collecting tensorflow_datasets Downloading https://files.pythonhosted.org/packages/6c/34/ff424223ed4331006aaa929efc8360b6459d427063dc59fc7b75d7e4bab3/tensorflow_datasets-1.2.0-py3-none-any.whl (2.3MB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.3MB 4.9MB/s Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.16.0) Requirement already satisfied, skipping upgrade: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.11.2) Requirement already satisfied, skipping upgrade: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.3.0) Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.16.5) Requirement already satisfied, skipping upgrade: requests&gt;=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.21.0) Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (4.28.1) Requirement already satisfied, skipping upgrade: protobuf&gt;=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (3.7.1) Requirement already satisfied, skipping upgrade: psutil in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (5.4.8) Requirement already satisfied, skipping upgrade: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.2.1) Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.8.0) Requirement already satisfied, skipping upgrade: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.14.0) Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.12.0) Requirement already satisfied, skipping upgrade: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.1.0) Requirement already satisfied, skipping upgrade: attrs in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (19.1.0) Requirement already satisfied, skipping upgrade: idna&lt;2.9,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (2.8) Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (2019.6.16) Requirement already satisfied, skipping upgrade: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (3.0.4) Requirement already satisfied, skipping upgrade: urllib3&lt;1.25,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (1.24.3) Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf&gt;=3.6.1-&gt;tensorflow_datasets) (41.2.0) Requirement already satisfied, skipping upgrade: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata-&gt;tensorflow_datasets) (1.6.0) Installing collected packages: tensorflow-datasets Successfully installed tensorflow-datasets-1.2.0</code> </pre> <br><p>  Nous avons d√©j√† vu et utilis√© quelques importations auparavant.  Du nouveau - import <code>tensorflow_hub</code> , que nous avons install√© et que nous utiliserons dans cette partie pratique. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> __future__ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> absolute_import, division, print_function, unicode_literals <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pylab <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf tf.enable_eager_execution() <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow_hub <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> hub <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow_datasets <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tfds <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> layers</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">WARNING:tensorflow: TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0. Please upgrade your code to TensorFlow 2.0: * https://www.tensorflow.org/beta/guide/migration_guide Or install the latest stable TensorFlow 1.X release: * `pip install -U "tensorflow==1.*"` Otherwise your code may be broken by the change.</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> logging logger = tf.get_logger() logger.setLevel(logging.ERROR)</code> </pre> <br><p>  <strong>Partie 1: utilisez TensorFlow Hub MobileNet pour les pr√©visions</strong> </p><br><p>  Dans cette partie de CoLab, nous allons prendre un mod√®le pr√©-form√©, le t√©l√©charger sur Keras et le tester. </p><br><p>  Le mod√®le que nous utilisons est MobileNet v2 (au lieu de MobileNet, tout autre mod√®le de classificateur d'images compatible tf2 avec tfhub.dev peut √™tre utilis√©). </p><br><p>  <strong>T√©l√©charger le classificateur</strong> </p><br><p>  T√©l√©chargez le mod√®le MobileNet et cr√©ez-en un mod√®le Keras.  MobileNet √† l'entr√©e s'attend √† recevoir une image de 224x224 pixels avec 3 canaux de couleur (RVB). </p><br><pre> <code class="python hljs">CLASSIFIER_URL = <span class="hljs-string"><span class="hljs-string">"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/2"</span></span> IMAGE_RES = <span class="hljs-number"><span class="hljs-number">224</span></span> model = tf.keras.Sequential([ hub.KerasLayer(CLASSIFIER_URL, input_shape=(IMAGE_RES, IMAGE_RES, <span class="hljs-number"><span class="hljs-number">3</span></span>)) ])</code> </pre> <br><p>  <strong>Ex√©cutez le classificateur sur une seule image</strong> </p><br><p>  MobileNet a √©t√© form√© sur l'ensemble de donn√©es ImageNet.  ImageNet contient 1000 classes de sortie et l'une de ces classes est un uniforme militaire.  Trouvons l'image sur laquelle l'uniforme militaire sera situ√© et qui ne fera pas partie du kit de formation ImageNet pour v√©rifier la pr√©cision de la classification. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PIL.Image <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> Image grace_hopper = tf.keras.utils.get_file(<span class="hljs-string"><span class="hljs-string">'image.jpg'</span></span>, <span class="hljs-string"><span class="hljs-string">'https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg'</span></span>) grace_hopper = Image.open(grace_hopper).resize((IMAGE_RES, IMAGE_RES)) grace_hopper</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg 65536/61306 [================================] - 0s 0us/step</code> </pre> <br><p><img src="https://habrastorage.org/webt/8g/gn/0u/8ggn0ur3pmnr_rxvezfdo4vrwcc.png"></p><br><pre> <code class="python hljs">grace_hopper = np.array(grace_hopper)/<span class="hljs-number"><span class="hljs-number">255.0</span></span> grace_hopper.shape</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">(224, 224, 3)</code> </pre> <br><p>  Gardez √† l'esprit que les mod√®les re√ßoivent toujours un ensemble (bloc) d'images √† traiter en entr√©e.  Dans le code ci-dessous, nous ajoutons une nouvelle dimension - la taille du bloc. </p><br><pre> <code class="python hljs">result = model.predict(grace_hopper[np.newaxis, ...]) result.shape</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">(1, 1001)</code> </pre> <br><p>  Le r√©sultat de la pr√©diction √©tait un vecteur d'une taille de 1 001 √©l√©ments, o√π chaque valeur repr√©sente la probabilit√© que l'objet dans l'image appartient √† une certaine classe. </p><br><p>  La position de la valeur de probabilit√© maximale peut √™tre trouv√©e en utilisant la fonction <code>argmax</code> .  Cependant, il y a une question √† laquelle nous n'avons toujours pas r√©pondu - comment pouvons-nous d√©terminer √† quelle classe un √©l√©ment appartient avec une probabilit√© maximale? </p><br><pre> <code class="python hljs">predicted_class = np.argmax(result[<span class="hljs-number"><span class="hljs-number">0</span></span>], axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) predicted_class</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">653</code> </pre> <br><p>  <strong>D√©chiffrer les pr√©dictions</strong> </p><br><p>  Afin de d√©terminer la classe √† laquelle se rapportent les pr√©dictions, nous t√©l√©chargeons la liste des balises ImageNet et par l'indice avec une fid√©lit√© maximale, nous d√©terminons la classe √† laquelle se rapporte la pr√©diction. </p><br><pre> <code class="python hljs">labels_path = tf.keras.utils.get_file(<span class="hljs-string"><span class="hljs-string">'ImageNetLabels.txt'</span></span>,<span class="hljs-string"><span class="hljs-string">'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt'</span></span>) imagenet_labels = np.array(open(labels_path).read().splitlines()) plt.imshow(grace_hopper) plt.axis(<span class="hljs-string"><span class="hljs-string">'off'</span></span>) predicted_class_name = imagenet_labels[predicted_class] _ = plt.title(<span class="hljs-string"><span class="hljs-string">"Prediction: "</span></span> + predicted_class_name.title())</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt 16384/10484 [==============================================] - 0s 0us/step</code> </pre> <br><p><img src="https://habrastorage.org/webt/ai/1k/rt/ai1krt6mkpcafhb5jx8ozf6mq8s.png"></p><br><p>  Bingo!  Notre mod√®le a correctement identifi√© l'uniforme militaire. </p><br><p>  <strong>Partie 2: utiliser le mod√®le TensorFlow Hub pour un jeu de donn√©es chat et chien</strong> </p><br><p>  Nous allons maintenant utiliser la version compl√®te du mod√®le MobileNet et voir comment il s'adaptera √† l'ensemble de donn√©es des chats et des chiens. </p><br><p>  <strong>Jeu de donn√©es</strong> </p><br><p>  Nous pouvons utiliser les jeux de donn√©es TensorFlow pour t√©l√©charger un jeu de donn√©es pour chats et chiens. </p><br><pre> <code class="python hljs">splits = tfds.Split.ALL.subsplit(weighted=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>)) splits, info = tfds.load(<span class="hljs-string"><span class="hljs-string">'cats_vs_dogs'</span></span>, with_info=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, as_supervised=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, split = splits) (train_examples, validation_examples) = splits num_examples = info.splits[<span class="hljs-string"><span class="hljs-string">'train'</span></span>].num_examples num_classes = info.features[<span class="hljs-string"><span class="hljs-string">'label'</span></span>].num_classes</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">Downloading and preparing dataset cats_vs_dogs (786.68 MiB) to /root/tensorflow_datasets/cats_vs_dogs/2.0.1... /usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings InsecureRequestWarning) WARNING:absl:1738 images were corrupted and were skipped Dataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/2.0.1. Subsequent calls will reuse this data.</code> </pre><br><p>  Toutes les images d'un jeu de donn√©es chat et chien n'ont pas la m√™me taille. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, example_image <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(train_examples.take(<span class="hljs-number"><span class="hljs-number">3</span></span>)): print(<span class="hljs-string"><span class="hljs-string">"Image {} shape: {}"</span></span>.format(i+<span class="hljs-number"><span class="hljs-number">1</span></span>, example_image[<span class="hljs-number"><span class="hljs-number">0</span></span>].shape))</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">Image 1 shape: (500, 343, 3) Image 2 shape: (375, 500, 3) Image 3 shape: (375, 500, 3)</code> </pre> <br><p>  Par cons√©quent, les images de l'ensemble de donn√©es obtenu doivent √™tre r√©duites √† une seule taille, ce que le mod√®le MobileNet attend √† l'entr√©e - 224 x 224. </p><br><p>  La fonction <code>.repeat()</code> et <code>steps_per_epoch</code> ne sont pas n√©cessaires ici, mais elles vous permettent d'√©conomiser environ 15 secondes par it√©ration d'entra√Ænement, car  le tampon temporaire ne doit √™tre initialis√© qu'une seule fois au tout d√©but du processus d'apprentissage. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">format_image</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(image, label)</span></span></span><span class="hljs-function">:</span></span> image = tf.image.resize(image, (IMAGE_RES, IMAGE_RES)) / <span class="hljs-number"><span class="hljs-number">255.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> image, label BATCH_SIZE = <span class="hljs-number"><span class="hljs-number">32</span></span> train_batches = train_examples.shuffle(num_examples//<span class="hljs-number"><span class="hljs-number">4</span></span>).map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>) validation_batches = validation_examples.map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><p>  <strong>Ex√©cutez le classificateur sur les ensembles d'images</strong> </p><br><p>  Permettez-moi de vous rappeler qu'√† ce stade, il existe toujours une version compl√®te du r√©seau MobileNet pr√©-form√©, qui contient 1 000 classes de sortie possibles.  ImageNet contient un grand nombre d'images de chiens et de chats, essayons donc de saisir l'une des images de test de notre ensemble de donn√©es et de voir quelle pr√©diction le mod√®le nous donnera. </p><br><pre> <code class="python hljs">image_batch, label_batch = next(iter(train_batches.take(<span class="hljs-number"><span class="hljs-number">1</span></span>))) image_batch = image_batch.numpy() label_batch = label_batch.numpy() result_batch = model.predict(image_batch) predicted_class_names = imagenet_labels[np.argmax(result_batch, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>)] predicted_class_names</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">array(['Persian cat', 'mink', 'Siamese cat', 'tabby', 'Bouvier des Flandres', 'dishwasher', 'Yorkshire terrier', 'tiger cat', 'tabby', 'Egyptian cat', 'Egyptian cat', 'tabby', 'dalmatian', 'Persian cat', 'Border collie', 'Newfoundland', 'tiger cat', 'Siamese cat', 'Persian cat', 'Egyptian cat', 'tabby', 'tiger cat', 'Labrador retriever', 'German shepherd', 'Eskimo dog', 'kelpie', 'mink', 'Norwegian elkhound', 'Labrador retriever', 'Egyptian cat', 'computer keyboard', 'boxer'], dtype='&lt;U30')</code> </pre> <br><p>  Les √©tiquettes sont similaires aux noms des races de chats et de chiens.  Voyons maintenant quelques images de notre jeu de donn√©es chats et chiens et pla√ßons une √©tiquette pr√©dite sur chacun d'eux. </p><br><pre> <code class="python hljs">plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">30</span></span>): plt.subplot(<span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, n+<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.subplots_adjust(hspace=<span class="hljs-number"><span class="hljs-number">0.3</span></span>) plt.imshow(image_batch[n]) plt.title(predicted_class_names[n]) plt.axis(<span class="hljs-string"><span class="hljs-string">'off'</span></span>) _ = plt.suptitle(<span class="hljs-string"><span class="hljs-string">"ImageNet predictions"</span></span>)</code> </pre> <br><p><img src="https://habrastorage.org/webt/fc/af/w-/fcafw-kdtzotffq7k-nihlntuda.png"></p><br><p>  <strong>Partie 3: impl√©menter le transfert d'apprentissage avec le concentrateur TensorFlow</strong> </p><br><p>  Utilisons maintenant le TensorFlow Hub pour transf√©rer l'apprentissage d'un mod√®le √† un autre. </p><br><p>  Dans le processus de transfert de la formation, nous r√©utilisons un mod√®le pr√©-form√© en modifiant sa derni√®re couche, ou plusieurs couches, puis recommen√ßons le processus de formation sur un nouvel ensemble de donn√©es. </p><br><p>  Dans le TensorFlow Hub, vous pouvez trouver non seulement des mod√®les pr√©-form√©s complets (avec la derni√®re couche), mais aussi des mod√®les sans la derni√®re couche de classification.  Ce dernier peut √™tre facilement utilis√© pour transf√©rer la formation.  Nous continuerons d'utiliser MobileNet v2 pour la simple raison que dans les parties suivantes de notre cours, nous transf√©rerons ce mod√®le et le lancerons sur un appareil mobile √† l'aide de TensorFlow Lite. </p><br><p>  Nous continuerons √©galement √† utiliser l'ensemble de donn√©es sur les chats et les chiens, nous aurons donc la possibilit√© de comparer les performances de ce mod√®le avec celles que nous avons mises en ≈ìuvre √† partir de z√©ro. </p><br><p>  Notez que nous avons appel√© le mod√®le partiel avec le TensorFlow Hub (sans la derni√®re couche de classification) <code>feature_extractor</code> .  Ce nom s'explique par le fait que le mod√®le accepte les donn√©es en entr√©e et les transforme en un ensemble fini de propri√©t√©s (caract√©ristiques) s√©lectionn√©es.  Ainsi, notre mod√®le a fait le travail d'identification du contenu de l'image, mais n'a pas produit la distribution de probabilit√© finale sur les classes de sortie.  Le mod√®le a extrait un ensemble de propri√©t√©s de l'image. </p><br><pre> <code class="python hljs">URL = <span class="hljs-string"><span class="hljs-string">'https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2'</span></span> feature_extractor = hub.KerasLayer(URL, input_shape=(IMAGE_RES, IMAGE_RES, <span class="hljs-number"><span class="hljs-number">3</span></span>))</code> </pre> <br><p>  Ex√©cutons un ensemble d'images via <code>feature_extractor</code> et examinons le formulaire r√©sultant (format de sortie).  32 - le nombre d'images, 1280 - le nombre de neurones dans la derni√®re couche du mod√®le pr√©-form√© avec le TensorFlow Hub. </p><br><pre> <code class="python hljs">feature_batch = feature_extractor(image_batch) print(feature_batch.shape)</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">(32, 1280)</code> </pre> <br><p>  Nous ¬´figons¬ª les variables dans la couche d'extraction des propri√©t√©s de sorte que seules les valeurs des variables de la couche de classification changent au cours du processus d'apprentissage. </p><br><pre> <code class="python hljs">feature_extractor.trainable = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span></code> </pre> <br><p>  <strong>Ajouter une couche de classification</strong> </p><br><p>  Enveloppez maintenant la couche du TensorFlow Hub dans le mod√®le <code>tf.keras.Sequential</code> et ajoutez une couche de classification. </p><br><pre> <code class="python hljs">model = tf.keras.Sequential([ feature_extractor, layers.Dense(<span class="hljs-number"><span class="hljs-number">2</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>) ]) model.summary()</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">Model: "sequential_1" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= keras_layer_1 (KerasLayer) (None, 1280) 2257984 _________________________________________________________________ dense (Dense) (None, 2) 2562 ================================================================= Total params: 2,260,546 Trainable params: 2,562 Non-trainable params: 2,257,984 _________________________________________________________________</code> </pre> <br><p>  <strong>Mod√®le de train</strong> </p><br><p>  Maintenant, nous entra√Ænons le mod√®le r√©sultant comme nous le faisions avant d'appeler la <code>compile</code> suivi de l' <code>fit</code> pour la formation. </p><br><pre> <code class="python hljs">model.compile( optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>] ) EPOCHS = <span class="hljs-number"><span class="hljs-number">6</span></span> history = model.fit(train_batches, epochs=EPOCHS, validation_data=validation_batches)</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">Epoch 1/6 582/582 [==============================] - 77s 133ms/step - loss: 0.2381 - acc: 0.9346 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 Epoch 2/6 582/582 [==============================] - 70s 120ms/step - loss: 0.1827 - acc: 0.9618 - val_loss: 0.1629 - val_acc: 0.9670 Epoch 3/6 582/582 [==============================] - 69s 119ms/step - loss: 0.1733 - acc: 0.9660 - val_loss: 0.1623 - val_acc: 0.9666 Epoch 4/6 582/582 [==============================] - 69s 118ms/step - loss: 0.1677 - acc: 0.9676 - val_loss: 0.1627 - val_acc: 0.9677 Epoch 5/6 582/582 [==============================] - 68s 118ms/step - loss: 0.1636 - acc: 0.9689 - val_loss: 0.1634 - val_acc: 0.9675 Epoch 6/6 582/582 [==============================] - 69s 118ms/step - loss: 0.1604 - acc: 0.9701 - val_loss: 0.1643 - val_acc: 0.9668</code> </pre> <br><p>  Comme vous l'avez probablement remarqu√©, nous avons pu atteindre une pr√©cision d'environ 97% des pr√©visions sur l'ensemble de donn√©es de validation.  G√©nial!  L'approche actuelle a consid√©rablement augment√© la pr√©cision de classification par rapport au premier mod√®le que nous avons nous-m√™mes form√© et a obtenu une pr√©cision de classification de ~ 87%.  La raison en est que MobileNet a √©t√© con√ßu par des experts et soigneusement d√©velopp√© sur une longue p√©riode de temps, puis form√© sur un ensemble de donn√©es ImageNet incroyablement grand. </p><br><p>  Vous pouvez voir comment cr√©er votre propre MobileNet dans Keras √† <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ce lien</a> . </p><br><p>  Construisons des graphiques des changements dans les valeurs de pr√©cision et de perte sur les ensembles de donn√©es de formation et de validation. </p><br><pre> <code class="python hljs">acc = history.history[<span class="hljs-string"><span class="hljs-string">'acc'</span></span>] val_acc = history.history[<span class="hljs-string"><span class="hljs-string">'val_acc'</span></span>] loss = history.history[<span class="hljs-string"><span class="hljs-string">'loss'</span></span>] val_loss = history.history[<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>] epochs_range = range(EPOCHS) plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>)) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(epochs_range, acc, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.plot(epochs_range, val_acc, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'lower right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'     '</span></span>) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) plt.plot(epochs_range, loss, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.plot(epochs_range, val_loss, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'upper right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'     '</span></span>) plt.show()</code> </pre> <br><p><img src="https://habrastorage.org/webt/o7/5c/7a/o75c7aieqvmudmyglkroiddrm90.png"></p><br><p>  Ce qui est int√©ressant ici, c'est que les r√©sultats sur l'ensemble de donn√©es de validation sont meilleurs que les r√©sultats sur l'ensemble de donn√©es de formation du tout d√©but √† la fin du processus d'apprentissage. </p><br><p>  L'une des raisons de ce comportement est que la pr√©cision de l'ensemble de donn√©es de validation est mesur√©e √† la fin de l'it√©ration d'apprentissage et que l'exactitude de l'ensemble de donn√©es d'apprentissage est consid√©r√©e comme la valeur moyenne parmi toutes les it√©rations d'apprentissage. </p><br><p>  La principale raison de ce comportement est l'utilisation du sous-r√©seau MobileNet pr√©-form√©, qui √©tait auparavant form√© sur un grand ensemble de donn√©es de chats et de chiens.  Dans le processus d'apprentissage, notre r√©seau continue d'√©largir l'ensemble de donn√©es de formation d'entr√©e (la m√™me augmentation), mais pas l'ensemble de validation.  Cela signifie que les images g√©n√©r√©es sur l'ensemble de donn√©es d'apprentissage sont plus difficiles √† classer que les images normales de l'ensemble de donn√©es valid√©. </p><br><p>  <strong>V√©rifier les r√©sultats de pr√©diction</strong> </p><br><p>  Pour r√©p√©ter le graphique de la section pr√©c√©dente, vous devez d'abord obtenir une liste tri√©e des noms de classe: </p><br><pre> <code class="python hljs">class_names = np.array(info.features[<span class="hljs-string"><span class="hljs-string">'label'</span></span>].names) class_names</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">array(['cat', 'dog'], dtype='&lt;U3')</code> </pre> <br><p>  Passez le bloc avec des images √† travers le mod√®le et convertissez les index r√©sultants en noms de classe: </p><br><pre> <code class="python hljs">predicted_batch = model.predict(image_batch) predicted_batch = tf.squeeze(predicted_batch).numpy() predicted_ids = np.argmax(predicted_batch, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) predicted_class_names = class_names[predicted_ids] predicted_class_names</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">array(['cat', 'cat', 'cat', 'cat', 'dog', 'cat', 'dog', 'cat', 'cat', 'cat', 'cat', 'cat', 'dog', 'cat', 'cat', 'dog', 'cat', 'cat', 'cat', 'cat', 'cat', 'cat', 'dog', 'dog', 'dog', 'dog', 'cat', 'cat', 'dog', 'cat', 'cat', 'dog'], dtype='&lt;U3')</code> </pre> <br><p>  Jetons un coup d'≈ìil aux v√©ritables √©tiquettes et pr√©dit: </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">": "</span></span>, label_batch) print(<span class="hljs-string"><span class="hljs-string">": "</span></span>, predicted_ids)</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">: [0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1] : [0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1]</code> </pre> <br><pre> <code class="python hljs">plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">30</span></span>): plt.subplot(<span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, n+<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.subplots_adjust(hspace=<span class="hljs-number"><span class="hljs-number">0.3</span></span>) plt.imshow(image_batch[n]) color = <span class="hljs-string"><span class="hljs-string">"blue"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> predicted_ids[n] == label_batch[n] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">"red"</span></span> plt.title(predicted_class_names[n].title(), color=color) plt.axis(<span class="hljs-string"><span class="hljs-string">'off'</span></span>) _ = plt.suptitle(<span class="hljs-string"><span class="hljs-string">"  (: , : )"</span></span>)</code> </pre> <br><p><img src="https://habrastorage.org/webt/dc/ox/4w/dcox4wk3fek_1e2j9ltjmvai1ia.png"></p><br><h1>  Plonger dans des r√©seaux de neurones convolutifs </h1><br><p>  √Ä l'aide de r√©seaux de neurones convolutifs, nous avons r√©ussi √† nous assurer qu'ils r√©pondent bien √† la t√¢che de classification des images.  Cependant, pour le moment, nous pouvons √† peine imaginer comment ils fonctionnent vraiment.  Si nous pouvions comprendre comment se d√©roule le processus d'apprentissage, alors, en principe, nous pourrions encore am√©liorer le travail de classification.  Une fa√ßon de comprendre le fonctionnement des r√©seaux de neurones convolutifs est de visualiser les couches et les r√©sultats de leur travail.  Nous vous recommandons fortement d'√©tudier les mat√©riaux <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> pour mieux comprendre comment visualiser les r√©sultats des couches convolutives. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/50f/d7a/3eb/50fd7a3eb31650740807d84eb8ff1da2.gif" alt="image"></p><br><p>  Le champ de la vision par ordinateur a vu le jour au bout du tunnel et a fait des progr√®s significatifs depuis l'av√®nement des r√©seaux de neurones convolutifs.  La vitesse incroyable avec laquelle les recherches sont men√©es dans ce domaine et les √©normes matrices d'images publi√©es sur Internet ont donn√© des r√©sultats incroyables au cours des derni√®res ann√©es.  La mont√©e en puissance des r√©seaux de neurones convolutifs a commenc√© avec AlexNet en 2012, cr√©√© par Alex Krizhevsky, Ilya Sutskever et Jeffrey Hinton et a remport√© le c√©l√®bre d√©fi de reconnaissance visuelle √† grande √©chelle ImageNet.  Depuis lors, il n'y avait aucun doute dans un avenir brillant en utilisant des r√©seaux de neurones convolutifs, et le domaine de la vision par ordinateur et les r√©sultats de son travail ne faisaient que confirmer ce fait.  Commen√ßant par reconna√Ætre votre visage sur un t√©l√©phone mobile et se terminant par la reconnaissance d'objets dans des voitures autonomes, les r√©seaux de neurones convolutionnels ont d√©j√† r√©ussi √† montrer et √† prouver leur force et √† r√©soudre de nombreux probl√®mes du monde r√©el. </p><br><p>  Malgr√© le grand nombre d'ensembles de donn√©es volumineux et de mod√®les pr√©-form√©s de r√©seaux de neurones convolutionnels, il est parfois extr√™mement difficile de comprendre comment le r√©seau fonctionne et √† quoi exactement ce r√©seau est form√©, en particulier pour les personnes qui n'ont pas suffisamment de connaissances dans le domaine de l'apprentissage automatique.                 ,            , ,   Inception,   .                     .            ,    ,         ,         ,         . </p><br><p>       "   Python"  <br> Fran√ßois Chollet.   ,        .    Keras,     ,   " " TensorFlow, MXNET  Theano.   ,        ,            .           ,       . </p><br><p> <strong>  </strong> </p><br><p>            ,    ,           . </p><br><p>             (training accuracy)     .         ,          ,        , ,   Inception,                . </p><br><p>           ,       ,      .   Inception v3 (     ImageNet)     ,    Kaggle.         Inception,       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">    </a> ,        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Inception v3</a>      . </p><br><p>     10  ()     32 ,    2292293.           0.3195,     ‚Äî 0.6377.     <code>ImageDataGenerator</code>     ,      .       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">GitHub </a> . </p><br><p> <strong>  </strong> </p><br><p>             ,    ""   ,      .               . </p><br><p> ,              Inception v3 ,        . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/d4c/ac1/6f5/d4cac16f506f3d14ab4cca070f4d876b.jpg" alt="image"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/222/102/eda/222102eda480f7108db0c9869ab46057.jpg" alt="image"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ca8/176/7c0/ca81767c0f5cae4748f1db6cee625e01.jpg" alt="image"></p><br><p>    ‚Äî     .             . </p><br><p>         ,              ()   .        (),       , ,  ,      .          ,      ,        ,     ,       . </p><br><p>   ReLU-    .    ,     <code>ReLU(z) = max(0, z)</code>     . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/9e3/b87/e17/9e3b87e175577fe97da51fd1a2b50eac.png" alt="image"></p><br><p>          ,   ,   ,        ,      ,           ,   , ,   ..            ,            .     "" ()     ,            ,     ,             . </p><br><p> <strong>   </strong> </p><br><p>         ""        .               . </p><br><p>   ,     Inveption V3      : </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/00e/a03/d78/00ea03d78e161d7f6fff59ba1a133309.jpg" alt="image"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/247/875/0da/2478750da1a4eb167f8dc1c9c55252d6.jpg" alt="image"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/4a9/635/bd8/4a9635bd81a0d2e46435a05c39d3457a.jpg" alt="image"></p><br><p>              ,         . ,                   ,      ,           ,   ..          ,       ,                .            ,       ,  ,           "" ( ,      ). </p><br><p> <strong>    </strong> </p><br><p>       ,         , ,      .      ,                  . </p><br><p>     Class Activation Map (  ).      CAM       .       2D              ,                 . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/2f8/95e/f8b/2f895ef8b9086c9ea56745ce0f441ef9.jpg" alt="image"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/6ff/392/e60/6ff392e60f007791bee52e439099759f.jpg" alt="image"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/fe9/896/210/fe9896210055693195c21a96b74f3188.jpg" alt="image"></p><br><p>       ,     .    ,    ,        Mixed-  Inception V3-,        .        () ,           . </p><br><p>     ,          ,        .          <strong></strong> ,            ,        .       ,          .   ,                 ,        ,    ,        . </p><br><p>           ,      ""  -          .               .             . </p><br><p>    ,           ,                   . </p><br><h1>  :      </h1><br><p> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Colab  </a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Colab  </a> . </p><br><p> <strong>TensorFlow Hub</strong> </p><br><p> TensorFlow Hub      ,      . </p><br><p>                  .     ,      ,   ,          . </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a>      . </p><br><p>              <code>Runtime -&gt; Reset all runtimes...</code> </p><br><p> <strong></strong> </p><br><p>  ,    : </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> __future__ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> absolute_import, division, print_function, unicode_literals <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf tf.enable_eager_execution() <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow_hub <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> hub <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow_datasets <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tfds <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> layers</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">WARNING:tensorflow: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see: * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md * https://github.com/tensorflow/addons * https://github.com/tensorflow/io (for I/O related ops) If you depend on functionality not listed there, please file an issue.</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> logging logger = tf.get_logger() logger.setLevel(logging.ERROR)</code> </pre> <br><p> <strong>      TensorFlow Datasets</strong> </p><br><p>            TensorFlow Datasets.   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a> ,       ‚Äî <code>tf_flowers</code> .        ,       .                <code>tfds.splits</code>   (70%)   (30%).        <code>tfds.load</code> .    <code>tfds.load</code> ,            ,      . </p><br><pre> <code class="python hljs">splits = tfds.Split.TRAIN.subsplit([<span class="hljs-number"><span class="hljs-number">70</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>]) (training_set, validation_set), dataset_info = tfds.load(<span class="hljs-string"><span class="hljs-string">'tf_flowers'</span></span>, with_info=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, as_supervised=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, split=splits)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Downloading and preparing dataset tf_flowers (218.21 MiB) to /root/tensorflow_datasets/tf_flowers/1.0.0... Dl Completed... 1/|/100% 1/1 [00:07&lt;00:00, 3.67s/ url] Dl Size... 218/|/100% 218/218 [00:07&lt;00:00, 30.69 MiB/s] Extraction completed... 1/|/100% 1/1 [00:07&lt;00:00, 7.05s/ file] Dataset tf_flowers downloaded and prepared to /root/tensorflow_datasets/tf_flowers/1.0.0. Subsequent calls will reuse this data.</code> </pre> <br><p> <strong>     </strong> </p><br><p> ,      ,    ()         ,      ,          ‚Äî   . </p><br><pre> <code class="python hljs">num_classes = dataset_info.features[<span class="hljs-string"><span class="hljs-string">'label'</span></span>].num_classes num_training_examples = <span class="hljs-number"><span class="hljs-number">0</span></span> num_validation_examples = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> example <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> training_set: num_training_examples += <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> example <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> validation_set: num_validation_examples += <span class="hljs-number"><span class="hljs-number">1</span></span> print(<span class="hljs-string"><span class="hljs-string">'Total Number of Classes: {}'</span></span>.format(num_classes)) print(<span class="hljs-string"><span class="hljs-string">'Total Number of Training Images: {}'</span></span>.format(num_training_examples)) print(<span class="hljs-string"><span class="hljs-string">'Total Number of Validation Images: {} \n'</span></span>.format(num_validation_examples))</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Total Number of Classes: 5 Total Number of Training Images: 2590 Total Number of Validation Images: 1080</code> </pre> <br><p>        ‚Äî  . </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, example <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(training_set.take(<span class="hljs-number"><span class="hljs-number">5</span></span>)): print(<span class="hljs-string"><span class="hljs-string">'Image {} shape: {} label: {}'</span></span>.format(i+<span class="hljs-number"><span class="hljs-number">1</span></span>, example[<span class="hljs-number"><span class="hljs-number">0</span></span>].shape, example[<span class="hljs-number"><span class="hljs-number">1</span></span>]))</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Image 1 shape: (226, 240, 3) label: 0 Image 2 shape: (240, 145, 3) label: 2 Image 3 shape: (331, 500, 3) label: 2 Image 4 shape: (240, 320, 3) label: 0 Image 5 shape: (333, 500, 3) label: 1</code> </pre> <br><p> <strong>     </strong> </p><br><p>          ‚Äî ,   MobilNet v2     ‚Äî 224224     (grayscale).     <code>image</code> ()  <code>label</code> ()       . </p><br><pre> <code class="python hljs">IMAGE_RES = <span class="hljs-number"><span class="hljs-number">224</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">format_image</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(image, label)</span></span></span><span class="hljs-function">:</span></span> image = tf.image.resize(image, (IMAGE_RES, IMAGE_RES))/<span class="hljs-number"><span class="hljs-number">255.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> image, label BATCH_SIZE = <span class="hljs-number"><span class="hljs-number">32</span></span> train_batches = training_set.shuffle(num_training_examples//<span class="hljs-number"><span class="hljs-number">4</span></span>).map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>) validation_batches = validation_set.map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><p> <strong>    TensorFlow Hub</strong> </p><br><p>    TensorFlow Hub   . ,                      ,         . </p><br><p> <strong>   </strong> </p><br><p>      <code>feature_extractor</code>  MobileNet v2. ,      TensorFlow Hub (   )   .          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> .   <code>tf2-preview/mobilenet_v2/feature_vector</code> ,     URL       MobileNet v2 .   <code>feature_extractor</code>   <code>hub.KerasLayer</code>      <code>input_shape</code> . </p><br><pre> <code class="python hljs">URL = <span class="hljs-string"><span class="hljs-string">"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4"</span></span> feature_extractor = hub.KerasLayer(URL, input_shape=(IMAGE_RES, IMAGE_RES, <span class="hljs-number"><span class="hljs-number">3</span></span>))</code> </pre> <br><p> <strong>   </strong> </p><br><p>                   ,   : </p><br><pre> <code class="python hljs">feature_extractor.trainable = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span></code> </pre> <br><p> <strong>  </strong> </p><br><p>               ,   .            .          . </p><br><pre> <code class="python hljs">model = tf.keras.Sequential([ feature_extractor, layers.Dense(num_classes, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>) ]) model.summary()</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Model: "sequential" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= keras_layer (KerasLayer) (None, 1280) 2257984 _________________________________________________________________ dense (Dense) (None, 5) 6405 ================================================================= Total params: 2,264,389 Trainable params: 6,405 Non-trainable params: 2,257,984</code> </pre> <br><p> <strong> </strong> </p><br><p>            ,           . </p><br><pre> <code class="python hljs">model.compile( optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>]) EPOCHS = <span class="hljs-number"><span class="hljs-number">6</span></span> history = model.fit(train_batches, epochs=EPOCHS, validation_data=validation_batches)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Epoch 1/6 81/81 [==============================] - 17s 216ms/step - loss: 0.7765 - acc: 0.7170 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 Epoch 2/6 81/81 [==============================] - 12s 147ms/step - loss: 0.3806 - acc: 0.8757 - val_loss: 0.3485 - val_acc: 0.8833 Epoch 3/6 81/81 [==============================] - 12s 146ms/step - loss: 0.3011 - acc: 0.9031 - val_loss: 0.3190 - val_acc: 0.8907 Epoch 4/6 81/81 [==============================] - 12s 147ms/step - loss: 0.2527 - acc: 0.9205 - val_loss: 0.3031 - val_acc: 0.8917 Epoch 5/6 81/81 [==============================] - 12s 148ms/step - loss: 0.2177 - acc: 0.9371 - val_loss: 0.2933 - val_acc: 0.8972 Epoch 6/6 81/81 [==============================] - 12s 146ms/step - loss: 0.1905 - acc: 0.9456 - val_loss: 0.2870 - val_acc: 0.9000</code> </pre> <br><p>         ~90%  6  ,     !   ,    ,           ~76%  80  .        ,  MobilNet v2                . </p><br><p> <strong>         </strong> </p><br><p>               . </p><br><pre> <code class="python hljs">acc = history.history[<span class="hljs-string"><span class="hljs-string">'acc'</span></span>] val_acc = history.history[<span class="hljs-string"><span class="hljs-string">'val_acc'</span></span>] loss = history.history[<span class="hljs-string"><span class="hljs-string">'loss'</span></span>] val_loss = history.history[<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>] epochs_range = range(EPOCHS) plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>)) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(epochs_range, acc, label=<span class="hljs-string"><span class="hljs-string">'Training Accuracy'</span></span>) plt.plot(epochs_range, val_acc, label=<span class="hljs-string"><span class="hljs-string">'Validation Accuracy'</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'lower right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'Training and Validation Accuracy'</span></span>) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) plt.plot(epochs_range, loss, label=<span class="hljs-string"><span class="hljs-string">'Training Loss'</span></span>) plt.plot(epochs_range, val_loss, label=<span class="hljs-string"><span class="hljs-string">'Validation Loss'</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'upper right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'Training and Validation Loss'</span></span>) plt.show()</code> </pre> <br><p><img src="https://habrastorage.org/webt/ox/nb/hy/oxnbhyqanark0qrt1xmeqg3qv4a.png"></p><br><p>   ,   ,                    . </p><br><p>        ,           ,              . </p><br><p>        - MobileNet,           .              (  augmentation),    .                  . </p><br><p> <strong> </strong> </p><br><p>              NumPy.     ,      . </p><br><pre> <code class="python hljs">class_names = np.array(dataset_info.features[<span class="hljs-string"><span class="hljs-string">'label'</span></span>].names) print(class_names)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">['dandelion' 'daisy' 'tulips' 'sunflowers' 'roses']</code> </pre> <br><p> <strong>        </strong> </p><br><p>   <code>next()</code>   <code>image_batch</code> ( )   <code>label_batch</code> ( ).   <code>image_batch</code>  <code>label_batch</code>  NumPy     <code>.numpy()</code> .    <code>.predict()</code>      .        <code>np.argmax()</code>   .            . </p><br><pre> <code class="python hljs">image_batch, label_batch = next(iter(train_batches)) image_batch = image_batch.numpy() label_batch = label_batch.numpy() predicted_batch = model.predict(image_batch) predicted_batch = tf.squeeze(predicted_batch).numpy() predicted_ids = np.argmax(predicted_batch, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) predicted_class_names = class_names[predicted_ids] print(predicted_class_names)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">['sunflowers' 'roses' 'tulips' 'tulips' 'daisy' 'dandelion' 'tulips' 'sunflowers' 'daisy' 'daisy' 'tulips' 'daisy' 'daisy' 'tulips' 'tulips' 'tulips' 'dandelion' 'dandelion' 'tulips' 'tulips' 'dandelion' 'roses' 'daisy' 'daisy' 'dandelion' 'roses' 'daisy' 'tulips' 'dandelion' 'dandelion' 'roses' 'dandelion']</code> </pre> <br><p> <strong>     </strong> </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">"Labels: "</span></span>, label_batch) print(<span class="hljs-string"><span class="hljs-string">"Predicted labels: "</span></span>, predicted_ids)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Labels: [3 4 2 2 1 0 2 3 1 1 2 1 1 2 2 2 0 0 2 2 0 4 1 1 0 4 1 2 0 0 4 0] Predicted labels: [3 4 2 2 1 0 2 3 1 1 2 1 1 2 2 2 0 0 2 2 0 4 1 1 0 4 1 2 0 0 4 0]</code> </pre> <br><p> <strong>  </strong> </p><br><pre> <code class="python hljs">plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>,<span class="hljs-number"><span class="hljs-number">9</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">30</span></span>): plt.subplot(<span class="hljs-number"><span class="hljs-number">6</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>,n+<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.subplots_adjust(hspace = <span class="hljs-number"><span class="hljs-number">0.3</span></span>) plt.imshow(image_batch[n]) color = <span class="hljs-string"><span class="hljs-string">"blue"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> predicted_ids[n] == label_batch[n] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">"red"</span></span> plt.title(predicted_class_names[n].title(), color=color) plt.axis(<span class="hljs-string"><span class="hljs-string">'off'</span></span>) _ = plt.suptitle(<span class="hljs-string"><span class="hljs-string">"Model predictions (blue: correct, red: incorrect)"</span></span>)</code> </pre> <br><p><img src="https://habrastorage.org/webt/e1/q-/rg/e1q-rgvnr6qns8-vvrcr8yzbexi.png"></p><br><p> <strong>     Inception-</strong> </p><br><p>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> TensorFlow Hub</a>     <code>tf2-preview/inception_v3/feature_vector</code> .        Inception V3 .      ,      Inception V3     .  ,  Inception V3       299299 .   Inception V3    MobileNet V2. </p><br><pre> <code class="python hljs">IMAGE_RES = <span class="hljs-number"><span class="hljs-number">299</span></span> (training_set, validation_set), dataset_info = tfds.load(<span class="hljs-string"><span class="hljs-string">'tf_flowers'</span></span>, with_info=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, as_supervised=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, split=splits) train_batches = training_set.shuffle(num_training_examples//<span class="hljs-number"><span class="hljs-number">4</span></span>).map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>) validation_batches = validation_set.map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>) URL = <span class="hljs-string"><span class="hljs-string">"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4"</span></span> feature_extractor = hub.KerasLayer(URL, input_shape=(IMAGE_RES, IMAGE_RES, <span class="hljs-number"><span class="hljs-number">3</span></span>), trainable=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) model_inception = tf.keras.Sequential([ feature_extractor, tf.keras.layers.Dense(num_classes, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>) ]) model_inception.summary()</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Model: "sequential_1" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= keras_layer_1 (KerasLayer) (None, 2048) 21802784 _________________________________________________________________ dense_1 (Dense) (None, 5) 10245 ================================================================= Total params: 21,813,029 Trainable params: 10,245 Non-trainable params: 21,802,784</code> </pre> <br><pre> <code class="python hljs">model_inception.compile( optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>]) EPOCHS = <span class="hljs-number"><span class="hljs-number">6</span></span> history = model_inception.fit(train_batches, epochs=EPOCHS, validation_data=validation_batches)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Epoch 1/6 81/81 [==============================] - 44s 541ms/step - loss: 0.7594 - acc: 0.7309 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 Epoch 2/6 81/81 [==============================] - 35s 434ms/step - loss: 0.3927 - acc: 0.8772 - val_loss: 0.3945 - val_acc: 0.8657 Epoch 3/6 81/81 [==============================] - 35s 434ms/step - loss: 0.3074 - acc: 0.9120 - val_loss: 0.3586 - val_acc: 0.8769 Epoch 4/6 81/81 [==============================] - 35s 434ms/step - loss: 0.2588 - acc: 0.9282 - val_loss: 0.3385 - val_acc: 0.8796 Epoch 5/6 81/81 [==============================] - 35s 436ms/step - loss: 0.2252 - acc: 0.9375 - val_loss: 0.3256 - val_acc: 0.8824 Epoch 6/6 81/81 [==============================] - 35s 435ms/step - loss: 0.1996 - acc: 0.9440 - val_loss: 0.3164 - val_acc: 0.8861</code> </pre> <br><h1>  R√©sum√© </h1><br><p>                   .          : </p><br><ul><li> <strong> :</strong> ,                .              . </li><li> <strong> :</strong>       . ""     ,       ,      . </li><li> <strong>MobileNet:</strong>       Google,                  . MobileNet              . </li></ul><br><p>              MobileNet      .                     .                   MobileNet    . </p><br><p> ‚Ä¶   call-to-action ‚Äî ,     share :) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">YouTube</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">T√©l√©gramme</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">VKontakte</a> <br>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Ojok</a> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr467967/">https://habr.com/ru/post/fr467967/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr467951/index.html">Habrastatistique: analyser les commentaires des lecteurs. Partie 2, r√©ponses aux questions</a></li>
<li><a href="../fr467953/index.html">Test de serveurs virtuels bon march√©</a></li>
<li><a href="../fr467959/index.html">Cosmologie et fluctuations quantiques dans le navigateur</a></li>
<li><a href="../fr467961/index.html">Probl√®mes et nuances lors du d√©veloppement de SmartTV √† l'aide de React.js</a></li>
<li><a href="../fr467965/index.html">Vivaldi 2.8 - Menu, s'il vous pla√Æt</a></li>
<li><a href="../fr467969/index.html">Pr√©sentations d'√©cran modal modal dans iOS 13</a></li>
<li><a href="../fr467973/index.html">Naissance de la plateforme</a></li>
<li><a href="../fr467975/index.html">Huawei Dorado V6: Sichuan Heat</a></li>
<li><a href="../fr467977/index.html">Cr√©ation d'une application √† l'aide de composants de style dans Vue.js</a></li>
<li><a href="../fr467979/index.html">Int√©grations publicitaires: comment √ßa marche?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>