<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üèåÔ∏è ‚òÇÔ∏è üèûÔ∏è Aprendizaje de refuerzo en Python üë©üèæ‚Äçüé® üõå üïî</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola colegas 



 En la √∫ltima publicaci√≥n del a√±o saliente, quer√≠amos mencionar Reinforcement Learning, un tema en el que ya estamos traduciendo un l...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Aprendizaje de refuerzo en Python</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/434738/">  Hola colegas <br><br><img src="https://habrastorage.org/webt/8s/-m/om/8s-mommciij8mkqdkjm62glthy4.jpeg"><br><br>  En la √∫ltima publicaci√≥n del a√±o saliente, quer√≠amos mencionar Reinforcement Learning, un tema en el que ya estamos traduciendo un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">libro</a> . <br><br>  Juzgue usted mismo: hab√≠a un art√≠culo elemental con Medium, que describ√≠a el contexto del problema, describ√≠a el algoritmo m√°s simple con implementaci√≥n en Python.  El art√≠culo tiene varios gifs.  Y la motivaci√≥n, la recompensa y la elecci√≥n de la estrategia correcta en el camino hacia el √©xito son cosas que ser√°n extremadamente √∫tiles para cada uno de nosotros en el pr√≥ximo a√±o. <br><br>  Que tengas una buena lectura! <br><a name="habracut"></a><br>  El aprendizaje reforzado es una forma de aprendizaje autom√°tico en el que el agente aprende a actuar en el entorno, realizando acciones y desarrollando as√≠ la intuici√≥n, despu√©s de lo cual observa los resultados de sus acciones.  En este art√≠culo te dir√© c√≥mo entender y formular el problema de aprender con refuerzo, y luego resolverlo en Python. <br><br><br>  Recientemente, nos hemos acostumbrado al hecho de que las computadoras juegan juegos contra humanos, ya sea como bots en juegos multijugador o como rivales en juegos uno a uno: digamos, en Dota2, PUB-G, Mario.  La compa√±√≠a de investigaci√≥n <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Deepmind</a> hizo un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">esc√°ndalo</a> por las noticias cuando su programa AlphaGo 2016 venci√≥ al campe√≥n surcoreano en 2016.  Si eres un √°vido jugador, podr√≠as escuchar acerca de los cinco partidos de Dota 2 OpenAI Five, donde los autos lucharon contra las personas y derrotaron a los mejores jugadores en Dota2 en varios partidos.  (Si est√° interesado en los detalles, el algoritmo se analiza en detalle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> y se examina c√≥mo funcionaban las m√°quinas). <br><br><img src="https://habrastorage.org/webt/da/l7/3j/dal73jd7dacspz0co83f6zuano0.png"><br><br>  La √∫ltima versi√≥n de OpenAI Five <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">lleva a Roshan</a> . <br><br>  Entonces, comencemos con la pregunta central.  ¬øPor qu√© necesitamos entrenamiento reforzado?  ¬øSe usa solo en juegos o es aplicable en escenarios realistas para resolver problemas aplicados?  Si es la primera vez que lees el entrenamiento de refuerzo, simplemente no puedes imaginar la respuesta a estas preguntas.  De hecho, el aprendizaje reforzado es una de las tecnolog√≠as m√°s utilizadas y de r√°pido desarrollo en el campo de la inteligencia artificial. <br>  Aqu√≠ hay una serie de √°reas tem√°ticas en las que los sistemas de aprendizaje por refuerzo son especialmente demandados: <br><br><ol><li>  Veh√≠culos no tripulados </li><li>  Industria del juego </li><li>  Rob√≥tica </li><li>  Sistemas de recomendaci√≥n </li><li>  Publicidad y mercadeo </li></ol><br>  <b>Resumen y antecedentes del aprendizaje por refuerzo</b> <br><br>  Entonces, ¬øc√≥mo se form√≥ el fen√≥meno de aprendizaje con refuerzo cuando tenemos tantos m√©todos de aprendizaje profundo y m√°quinas a nuestra disposici√≥n?  "Fue inventado por Rich Sutton y Andrew Barto, el supervisor de investigaci√≥n de Rich, quienes lo ayudaron a preparar su doctorado".  El paradigma se form√≥ por primera vez en la d√©cada de 1980 y luego fue arcaico.  Posteriormente, Rich cre√≠a que ella ten√≠a un gran futuro, y que eventualmente recibir√≠a reconocimiento. <br><br>  El aprendizaje reforzado admite la automatizaci√≥n en el entorno donde se implementa.  Tanto la m√°quina como el aprendizaje profundo operan aproximadamente de la misma manera: est√°n organizados estrat√©gicamente de manera diferente, pero ambos paradigmas admiten la automatizaci√≥n.  Entonces, ¬øpor qu√© surgi√≥ el entrenamiento de refuerzo? <br><br>  Es una reminiscencia del proceso de aprendizaje natural en el que el proceso / modelo act√∫a y recibe comentarios sobre c√≥mo se las arregla para hacer frente a la tarea: buena y no. <br><br>  La m√°quina y el aprendizaje profundo tambi√©n son opciones de capacitaci√≥n, sin embargo, est√°n m√°s dise√±ados para identificar patrones en los datos disponibles.  En el aprendizaje por refuerzo, por otro lado, dicha experiencia se obtiene mediante prueba y error;  el sistema encuentra gradualmente las opciones correctas u √≥ptimo global.  Una gran ventaja adicional del aprendizaje reforzado es que, en este caso, no es necesario proporcionar un amplio conjunto de datos de capacitaci√≥n, como ocurre con la ense√±anza con un maestro.  Unos peque√±os fragmentos ser√°n suficientes. <br><br>  <b>El concepto de aprendizaje por refuerzo.</b> <br><br>  Imagina ense√±ar a tus gatos nuevos trucos;  pero, desafortunadamente, los gatos no entienden el lenguaje humano, por lo que no puedes tomar y decirles lo que vas a jugar con ellos.  Por lo tanto, actuar√° de manera diferente: imite la situaci√≥n y el gato tratar√° de responder de una forma u otra en respuesta.  Si el gato reaccion√≥ de la manera que quer√≠a, entonces le vierte leche.  ¬øEntiendes lo que suceder√° despu√©s?  Una vez m√°s, en una situaci√≥n similar, el gato volver√° a realizar la acci√≥n deseada, y con un entusiasmo a√∫n mayor, con la esperanza de que se alimente a√∫n mejor.  As√≠ es como se lleva a cabo el aprendizaje en un ejemplo positivo;  pero, si intentas "educar" a un gato con incentivos negativos, por ejemplo, m√≠ralo estrictamente y frunce el ce√±o, por lo general no aprende en tales situaciones. <br><br>  El aprendizaje reforzado funciona de manera similar.  Le decimos a la m√°quina algunas entradas y acciones, y luego recompensamos la m√°quina dependiendo de la salida.  Nuestro objetivo final es maximizar las recompensas.  Ahora veamos c√≥mo reformular el problema anterior en t√©rminos de aprendizaje por refuerzo. <br><br><ul><li>  El gato act√∫a como un "agente" expuesto al "medio ambiente". </li><li>  El entorno es un hogar o √°rea de juego, dependiendo de lo que le est√©s ense√±ando al gato. </li><li>  Las situaciones que surgen de la capacitaci√≥n se denominan "estados".  En el caso de un gato, ejemplos de condiciones son cuando el gato "corre" o "se arrastra debajo de la cama". </li><li>  Los agentes reaccionan realizando acciones y movi√©ndose de un "estado" a otro. </li><li>  Despu√©s de que el estado cambia, el agente recibe una "recompensa" o una "multa" dependiendo de la acci√≥n que haya tomado. </li><li>  La "estrategia" es una metodolog√≠a para elegir una acci√≥n para obtener los mejores resultados. </li></ul><br>  Ahora que hemos descubierto qu√© es el aprendizaje por refuerzo, hablemos en detalle sobre los or√≠genes y la evoluci√≥n del aprendizaje por refuerzo y el aprendizaje profundo con refuerzo, discutamos c√≥mo este paradigma nos permite resolver problemas que son imposibles de aprender con o sin un maestro, y tambi√©n tenga en cuenta lo siguiente Dato curioso: en la actualidad, el motor de b√∫squeda de Google est√° optimizado mediante algoritmos de aprendizaje por refuerzo. <br><br>  <b>Comprender la terminolog√≠a del aprendizaje por refuerzo</b> <br><br>  El agente y el entorno desempe√±an papeles clave en el algoritmo de aprendizaje de refuerzo.  El entorno es el mundo en el que el Agente tiene que sobrevivir.  Adem√°s, el Agente recibe se√±ales de refuerzo del Medio Ambiente (recompensa): este es un n√∫mero que describe cu√°n bueno o malo puede considerarse el estado actual del mundo.  El prop√≥sito del Agente es maximizar la recompensa total, la llamada "ganancia".  Antes de escribir nuestros primeros algoritmos de aprendizaje de refuerzo, debe comprender la siguiente terminolog√≠a. <br><br><img src="https://habrastorage.org/webt/6j/vx/cp/6jvxcpcpr52v252wa9eze1mehx4.gif"><br><br><ol><li>  <b>Estados</b> : un estado es una descripci√≥n completa de un mundo en el que no falta un solo fragmento de la informaci√≥n que lo caracteriza.  Puede ser una posici√≥n, fija o din√°mica.  Como regla general, dichos estados se escriben en forma de matrices, matrices o tensores de orden superior. </li><li>  <b>Acci√≥n</b> : la acci√≥n generalmente depende de las condiciones ambientales, y en diferentes entornos el agente tomar√° diferentes acciones.  Muchas acciones v√°lidas del agente se registran en un espacio llamado "espacio de acci√≥n".  T√≠picamente, el n√∫mero de acciones en el espacio es finito. </li><li>  <b>Entorno</b> : este es el lugar en el que existe el agente y con el que interact√∫a.  Se utilizan diferentes tipos de recompensas, estrategias, etc. para diferentes entornos. </li><li>  <b>Recompensas</b> y <b>ganancias</b> : debes controlar constantemente la funci√≥n de recompensa R cuando entrenas con refuerzos.  Es fundamental cuando configura un algoritmo, lo optimiza y tambi√©n cuando deja de aprender.  Depende del estado actual del mundo, la acci√≥n que se acaba de tomar y el pr√≥ximo estado del mundo. </li><li>  <b>Estrategias</b> : una estrategia es una regla seg√∫n la cual un agente elige la siguiente acci√≥n.  El conjunto de estrategias tambi√©n se conoce como el "cerebro" del agente. </li></ol><br><img src="https://habrastorage.org/webt/ur/lb/u-/urlbu-ifbred1iqfkhvqkv7bqds.png"><br><br>  Ahora que nos hemos familiarizado con la terminolog√≠a de aprendizaje por refuerzo, solucionemos el problema utilizando los algoritmos apropiados.  Antes de esto, debe comprender c√≥mo formular un problema de este tipo y, al resolverlo, confiar en la terminolog√≠a del entrenamiento con refuerzo. <br><br>  <b>Soluci√≥n de taxi</b> <br><br>  Entonces, pasamos a resolver el problema con el uso de algoritmos de refuerzo. <br>  Supongamos que tenemos una zona de entrenamiento para un taxi no tripulado, que entrenamos para entregar pasajeros al estacionamiento en cuatro puntos diferentes ( <code>R,G,Y,B</code> ).  Antes de eso, debe comprender y establecer el entorno en el que comenzamos a programar en Python.  Si reci√©n comienza a aprender Python, le recomiendo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este art√≠culo</a> . <br><br>  El entorno para resolver un problema de taxi se puede configurar con OpenAI's <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Gym</a> : esta es una de las bibliotecas m√°s populares para resolver problemas con el entrenamiento de refuerzo.  Bueno, antes de usar el gimnasio, debe instalarlo en su m√°quina, y un administrador de paquetes de Python llamado pip es conveniente para esto.  El siguiente es el comando de instalaci√≥n. <br><br> <code>pip install gym</code> <br> <br>  A continuaci√≥n, veamos c√≥mo se mostrar√° nuestro entorno.  Todos los modelos y la interfaz para esta tarea ya est√°n configurados en el gimnasio y nombrados bajo <code>Taxi-V2</code> .  El fragmento de c√≥digo a continuaci√≥n se utiliza para mostrar este entorno. <br><br>  ‚ÄúTenemos 4 ubicaciones (indicadas por letras diferentes);  nuestra tarea es recoger a un pasajero en un punto y dejarlo en otro.  Obtenemos +20 puntos por un aterrizaje exitoso de pasajeros y perdemos 1 punto por cada paso gastado en √©l.  Tambi√©n hay una penalizaci√≥n de 10 puntos por cada embarque y desembarque involuntario de un pasajero ".  (Fuente: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">gym.openai.com/envs/Taxi-v2</a> ) <br><br>  Aqu√≠ est√° la salida que veremos en nuestra consola: <br><br><img src="https://habrastorage.org/webt/n8/xj/lw/n8xjlwkbla6unwn2k6tworczc8o.png"><br><br>  Taxi V2 ENV <br><br>  Genial, <code>env</code> es el coraz√≥n de OpenAi Gym, es una interfaz de entorno unificada.  Los siguientes son m√©todos env que nos parecen √∫tiles: <br><br>  <code>env.reset</code> : restablece el entorno y devuelve un estado inicial aleatorio. <br>  <code>env.step(action)</code> : <code>env.step(action)</code> desarrollo del entorno un paso en el tiempo. <br>  <code>env.step(action)</code> : devuelve las siguientes variables <br><br><ul><li>  <code>observation</code> : Observaci√≥n del medio ambiente. </li><li>  <code>reward</code> : <code>reward</code> si tu acci√≥n fue beneficiosa. </li><li>  <code>done</code> : indica si logramos recoger y dejar al pasajero correctamente, tambi√©n denominado "un episodio". </li><li>  <code>info</code> : informaci√≥n adicional como el rendimiento y la latencia necesarios para la depuraci√≥n. </li><li>  <code>env.render</code> : muestra un cuadro del entorno (√∫til para renderizar) </li></ul><br>  Entonces, despu√©s de examinar el entorno, intentemos comprender mejor el problema.  Los taxis son el √∫nico autom√≥vil en este estacionamiento.  El estacionamiento se puede dividir en una cuadr√≠cula de <code>5x5</code> , donde obtenemos 25 posibles ubicaciones de taxis.  Estos 25 valores son uno de los elementos de nuestro espacio de estado.  Tenga en cuenta: en este momento, nuestro taxi se encuentra en el punto con coordenadas (3, 1). <br><br>  Hay 4 puntos en el entorno donde los pasajeros pueden abordar: estos son: <code>R, G, Y, B</code> o <code>[(0,0), (0,4), (4,0), (4,3)]</code> en coordenadas ( horizontalmente; verticalmente), si fuera posible interpretar el entorno anterior en coordenadas cartesianas.  Si tambi√©n tiene en cuenta un (1) estado m√°s del pasajero: dentro del taxi, puede tomar todas las combinaciones de ubicaciones de pasajeros y sus destinos para calcular el n√∫mero total de estados en nuestro entorno para la formaci√≥n de taxis: tenemos cuatro (4) destinos y cinco (4+ 1) ubicaciones de pasajeros. <br><br>  Entonces, en nuestro entorno para un taxi, hay 5 √ó 5 √ó 5 √ó 4 = 500 estados posibles.  Un agente maneja una de las 500 condiciones y toma medidas.  En nuestro caso, las opciones son las siguientes: moverse en una direcci√≥n u otra, o la decisi√≥n de recoger / dejar al pasajero.  En otras palabras, tenemos a nuestra disposici√≥n seis acciones posibles: <br>  recogida, ca√≠da, norte, este, sur, oeste (los √∫ltimos cuatro valores son direcciones en las que puede moverse un taxi). <br><br>  Este es el <code>action space</code> : el conjunto de todas las acciones que nuestro agente puede realizar en un estado determinado. <br><br>  Como queda claro en la ilustraci√≥n anterior, un taxi no puede realizar ciertas acciones en algunas situaciones (las paredes interfieren).  En el c√≥digo que describe el entorno, simplemente asignamos una penalizaci√≥n de -1 por cada golpe en la pared y un taxi que colisiona con la pared.  Por lo tanto, tales multas se acumular√°n, por lo que el taxi intentar√° no golpear las paredes. <br><br>  Tabla de recompensas: al crear un entorno de taxi, tambi√©n se puede crear una tabla de recompensas primaria llamada P. Puede considerarla una matriz, donde el n√∫mero de estados corresponde al n√∫mero de filas y el n√∫mero de acciones al n√∫mero de columnas.  Es decir, estamos hablando de la matriz de <code>states √ó actions</code> . <br><br>  Como absolutamente todas las condiciones se registran en esta matriz, puede ver los valores de recompensa predeterminados asignados al estado que hemos elegido para ilustrar: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym &gt;&gt;&gt; env = gym.make(<span class="hljs-string"><span class="hljs-string">"Taxi-v2"</span></span>).env &gt;&gt;&gt; env.P[<span class="hljs-number"><span class="hljs-number">328</span></span>] {<span class="hljs-number"><span class="hljs-number">0</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">433</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">1</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">233</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">2</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">353</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">3</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">4</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">5</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)] }</code> </pre> <br>  La estructura de este diccionario es la siguiente: <code>{action: [(probability, nextstate, reward, done)]}</code> . <br><br><ul><li>  Los valores 0‚Äì5 corresponden a las acciones (sur, norte, este, oeste, recogida, devoluci√≥n) que un taxi puede realizar en el estado actual que se muestra en la ilustraci√≥n. </li><li>  done le permite juzgar cu√°ndo dejamos con √©xito al pasajero en el punto deseado. </li></ul><br>  Para resolver este problema sin ning√∫n entrenamiento con refuerzo, puede establecer el estado objetivo, hacer una selecci√≥n de espacios y luego, si puede alcanzar el estado objetivo durante un cierto n√∫mero de iteraciones, suponga que este momento corresponde a la recompensa m√°xima.  En otros estados, el valor de la recompensa se aproxima al m√°ximo si el programa act√∫a correctamente (se acerca a la meta) o acumula multas si comete errores.  Adem√°s, el valor de la multa no puede llegar a menos de -10. <br><br>  Escribamos c√≥digo para resolver este problema sin entrenamiento de refuerzo. <br>  Dado que tenemos una tabla P con valores de recompensa predeterminados para cada estado, podemos intentar organizar la navegaci√≥n de nuestro taxi solo en base a esta tabla. <br><br>  Creamos un bucle sin fin, desplaz√°ndonos hasta que el pasajero llegue al destino (un episodio) o, en otras palabras, hasta que la tasa de recompensa alcance 20. El m√©todo <code>env.action_space.sample()</code> selecciona autom√°ticamente una acci√≥n aleatoria del conjunto de todas las acciones disponibles. .  Considera lo que sucede: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sleep <span class="hljs-comment"><span class="hljs-comment">#  thr env env = gym.make("Taxi-v2").env env.s = 328 #     ,   , epochs = 0 penalties, reward = 0, 0 frames = [] done = False while not done: action = env.action_space.sample() state, reward, done, info = env.step(action) if reward == -10: penalties += 1 #         frames.append({ 'frame': env.render(mode='ansi'), 'state': state, 'action': action, 'reward': reward } ) epochs += 1 print("Timesteps taken: {}".format(epochs)) print("Penalties incurred: {}".format(penalties)) #    , ,  def frames(frames): for i, frame in enumerate(frames): clear_output(wait=True) print(frame['frame'].getvalue()) print(f"Timestep: {i + 1}") print(f"State: {frame['state']}") print(f"Action: {frame['action']}") print(f"Reward: {frame['reward']}") sleep(.1) frames(frames)</span></span></code> </pre><br>  Conclusi√≥n <br><br><img src="https://habrastorage.org/webt/b3/kd/z_/b3kdz_kejhninocffgc2_3ytr_u.gif"><br><br>  cr√©ditos: OpenAI <br><br>  El problema est√° resuelto, pero no est√° optimizado, o este algoritmo no funcionar√° en todos los casos.  Necesitamos un agente de interacci√≥n adecuado para que el n√∫mero de iteraciones gastadas por la m√°quina / algoritmo para resolver el problema siga siendo m√≠nimo.  Aqu√≠ el algoritmo de Q-learning nos ayudar√°, cuya implementaci√≥n consideraremos en la siguiente secci√≥n. <br><br>  <b>Introduciendo Q-Learning</b> <br><br>  A continuaci√≥n se muestran los algoritmos de aprendizaje por refuerzo m√°s populares y uno de los m√°s simples.  El entorno recompensa al agente por su entrenamiento gradual y por el hecho de que, en un estado particular, da el paso m√°s √≥ptimo.  En la implementaci√≥n discutida anteriormente, ten√≠amos una tabla de recompensas "P", seg√∫n la cual nuestro agente aprender√°.  Seg√∫n la tabla de recompensas, elige la siguiente acci√≥n seg√∫n lo √∫til que sea, y luego actualiza otro valor, llamado valor Q.  Como resultado, se crea una nueva tabla, llamada Q-table, que se muestra en la combinaci√≥n (Estado, Acci√≥n).  Si los valores Q son mejores, obtenemos recompensas m√°s optimizadas. <br><br>  Por ejemplo, si un taxi se encuentra en un estado en el que el pasajero se encuentra en el mismo punto que el taxi, es extremadamente probable que el valor Q para la acci√≥n "recoger" sea mayor que para otras acciones, por ejemplo, "dejar al pasajero" o "ir al norte". ". <br>  Los valores Q se inicializan con valores aleatorios y, a medida que el agente interact√∫a con el entorno y recibe varias recompensas al realizar ciertas acciones, los valores Q se actualizan de acuerdo con la siguiente ecuaci√≥n: <br><br><img src="https://habrastorage.org/webt/ed/fv/br/edfvbr7xz2terdw8meeftimstx0.png"><br><br>  Esto plantea la pregunta: c√≥mo inicializar los valores Q y c√≥mo calcularlos.  A medida que se realizan acciones, los valores Q se ejecutan en esta ecuaci√≥n. <br><br>  Aqu√≠, Alpha y Gamma son los par√°metros del algoritmo Q-learning.  Alfa es el ritmo de aprendizaje, y gamma es el factor de descuento.  Ambos valores pueden variar de 0 a 1 y a veces son iguales a uno.  Gamma puede ser igual a cero, pero alfa no, porque el valor de las p√©rdidas durante la actualizaci√≥n debe compensarse (la tasa de aprendizaje es positiva).  El valor alfa aqu√≠ es el mismo que cuando se ense√±a con un maestro.  Gamma determina cu√°n importante queremos dar las recompensas que nos esperan en el futuro. <br><br>  Este algoritmo se resume a continuaci√≥n: <br><br><ul><li>  Paso 1: inicialice la tabla Q, rellen√°ndola con ceros, y para los valores Q establecemos constantes arbitrarias. </li><li>  Paso 2: Ahora deje que el agente responda al entorno y pruebe diferentes acciones.  Para cada cambio de estado, seleccionamos una de todas las acciones posibles en este estado (S). </li><li>  Paso 3: vaya al siguiente estado (S ') en funci√≥n de los resultados de la acci√≥n anterior (a). </li><li>  Paso 4: Para todas las acciones posibles desde el estado (S '), seleccione una con el valor Q m√°s alto. </li><li>  Paso 5: Actualice los valores de la tabla Q de acuerdo con la ecuaci√≥n anterior. </li><li>  Paso 6: Convierte el siguiente estado en el actual. </li><li>  Paso 7: Si se alcanza el estado objetivo, completamos el proceso y luego repetimos. </li></ul><br>  <b>Q-learning en Python</b> <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> IPython.display <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> clear_output <span class="hljs-comment"><span class="hljs-comment">#  Taxi-V2 Env env = gym.make("Taxi-v2").env #    q_table = np.zeros([env.observation_space.n, env.action_space.n]) #  alpha = 0.1 gamma = 0.6 epsilon = 0.1 all_epochs = [] all_penalties = [] for i in range(1, 100001): state = env.reset() #   epochs, penalties, reward, = 0, 0, 0 done = False while not done: if random.uniform(0, 1) &lt; epsilon: #    action = env.action_space.sample() else: #    action = np.argmax(q_table[state]) next_state, reward, done, info = env.step(action) old_value = q_table[state, action] next_max = np.max(q_table[next_state]) #    new_value = (1 - alpha) * old_value + alpha * \ (reward + gamma * next_max) q_table[state, action] = new_value if reward == -10: penalties += 1 state = next_state epochs += 1 if i % 100 == 0: clear_output(wait=True) print("Episode: {i}") print("Training finished.")</span></span></code> </pre><br>  Genial, ahora todos sus valores se almacenar√°n en la variable <code>q_table</code> . <br><br>  Por lo tanto, su modelo est√° entrenado en condiciones ambientales y ahora sabe c√≥mo seleccionar pasajeros con mayor precisi√≥n.  Y se familiariz√≥ con el fen√≥meno del aprendizaje por refuerzo, y puede programar el algoritmo para resolver un nuevo problema. <br><br>  Otras t√©cnicas de aprendizaje de refuerzo: <br><br><ul><li>  Procesos de toma de decisiones de Markov (MDP) y ecuaciones de Bellman </li><li>  Programaci√≥n din√°mica: RL basado en modelos, iteraci√≥n de estrategia e iteraci√≥n de valor </li><li>  Q-Training profundo </li><li>  M√©todos de descenso de gradiente de estrategia </li><li>  Sarsa </li></ul><br>  El c√≥digo para este ejercicio se encuentra en: <br><br>  vihar / python-refuerzo-aprendizaje </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es434738/">https://habr.com/ru/post/es434738/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es434728/index.html">Personas y procesos: ¬øpor qu√© udalenka no es adecuado para todas las empresas?</a></li>
<li><a href="../es434730/index.html">Bases de datos en memoria: aplicaci√≥n, escalado y adiciones importantes</a></li>
<li><a href="../es434732/index.html">Vida a 6200 DPI. Revisi√≥n de HyperX Pulsefire Core</a></li>
<li><a href="../es434734/index.html">Transformada de Fourier. El r√°pido y el furioso</a></li>
<li><a href="../es434736/index.html">Usando la base de datos de registro de Mikrotik para suprimir la fuerza bruta</a></li>
<li><a href="../es434740/index.html">Red neuronal ense√±ada a detectar paneles solares en im√°genes satelitales y predecir el nivel de su distribuci√≥n</a></li>
<li><a href="../es434742/index.html">Parte 2: uso de los controladores UDB PSoC de Cypress para reducir el n√∫mero de interrupciones en una impresora 3D</a></li>
<li><a href="../es434744/index.html">Samsung SSD 860 QVO 1 TB y 4 TB: el primer consumidor SATA QLC (2 partes)</a></li>
<li><a href="../es434746/index.html">BLE bajo microscopio 4</a></li>
<li><a href="../es434750/index.html">C√≥mo tomar el control de su infraestructura de red. CAPITULO DOS Limpieza y documentacion</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>