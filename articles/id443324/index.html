<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸš£ğŸ¿ ğŸ§—ğŸ¾ ğŸ¤§ Python vs. Scala for Apache Spark - benchmark yang diharapkan dengan hasil yang tidak terduga ğŸˆ´ ğŸ‘¶ğŸ¿ ğŸ‘ˆğŸ¾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Apache Spark saat ini mungkin merupakan platform paling populer untuk menganalisis data volume besar. Kontribusi yang cukup besar terhadap popularitas...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Python vs. Scala for Apache Spark - benchmark yang diharapkan dengan hasil yang tidak terduga</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/odnoklassniki/blog/443324/"><p><img src="https://habrastorage.org/webt/uk/hc/yv/ukhcyvudckoey9ttm1libhqkyv8.jpeg"></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Apache Spark</a> saat ini mungkin merupakan platform paling populer untuk menganalisis data volume besar.  Kontribusi yang cukup besar terhadap popularitasnya dibuat oleh kemungkinan menggunakannya dari bawah Python.  Pada saat yang sama, semua orang setuju bahwa, dalam kerangka API standar, kinerja kode Python dan Scala / Java dapat dibandingkan, tetapi tidak ada sudut pandang tunggal mengenai fungsi yang ditentukan pengguna (User Defined Function, UDF).  Mari kita coba mencari tahu bagaimana biaya overhead meningkat dalam kasus ini, menggunakan contoh tugas memeriksa solusi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">SNA Hackathon 2019</a> . </p><a name="habracut"></a><br><p>  Sebagai bagian dari kompetisi, peserta memecahkan masalah pengurutan feed berita dari jejaring sosial dan mengunggah solusi dalam bentuk serangkaian daftar yang diurutkan.  Untuk memeriksa kualitas solusi yang diperoleh, pertama, untuk masing-masing daftar yang dimuat, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ROC AUC</a> dihitung, dan kemudian nilai rata-rata ditampilkan.  Harap dicatat bahwa Anda perlu menghitung bukan satu ROC AUC yang umum, tetapi yang pribadi untuk setiap pengguna - tidak ada desain yang siap pakai untuk menyelesaikan masalah ini, jadi Anda harus menulis fungsi khusus.  Alasan yang baik untuk membandingkan kedua pendekatan tersebut dalam praktik. </p><br><p> Sebagai platform perbandingan, kami akan menggunakan wadah cloud dengan empat core dan Spark diluncurkan dalam mode lokal, dan kami akan bekerja dengannya melalui <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Apache Zeppelin</a> .  Untuk membandingkan fungsinya, kita akan meniru <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">kode yang</a> sama di PySpark dan Scala Spark.  [di sini] Mari kita mulai dengan memuat data. </p><br><pre><code class="python hljs">data = sqlContext.read.csv(<span class="hljs-string"><span class="hljs-string">"sna2019/modelCappedSubmit"</span></span>) trueData = sqlContext.read.csv(<span class="hljs-string"><span class="hljs-string">"sna2019/collabGt"</span></span>) toValidate = data.withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c1"</span></span>, <span class="hljs-string"><span class="hljs-string">"submit"</span></span>) \ .join(trueData.withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c1"</span></span>, <span class="hljs-string"><span class="hljs-string">"real"</span></span>), <span class="hljs-string"><span class="hljs-string">"_c0"</span></span>) \ .withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c0"</span></span>, <span class="hljs-string"><span class="hljs-string">"user"</span></span>) \ .repartition(<span class="hljs-number"><span class="hljs-number">4</span></span>).cache() toValidate.count()</code> </pre> <br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> data = sqlContext.read.csv(<span class="hljs-string"><span class="hljs-string">"sna2019/modelCappedSubmit"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> trueData = sqlContext.read.csv(<span class="hljs-string"><span class="hljs-string">"sna2019/collabGt"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> toValidate = data.withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c1"</span></span>, <span class="hljs-string"><span class="hljs-string">"submit"</span></span>) .join(trueData.withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c1"</span></span>, <span class="hljs-string"><span class="hljs-string">"real"</span></span>), <span class="hljs-string"><span class="hljs-string">"_c0"</span></span>) .withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c0"</span></span>, <span class="hljs-string"><span class="hljs-string">"user"</span></span>) .repartition(<span class="hljs-number"><span class="hljs-number">4</span></span>).cache() toValidate.count()</code> </pre> <br><p>  Saat menggunakan API standar, identitas kode yang hampir lengkap perlu diperhatikan, hingga kata kunci <code>val</code> .  Waktu operasi tidak berbeda secara signifikan.  Sekarang mari kita coba menentukan UDF yang kita butuhkan. </p><br><pre> <code class="python hljs">parse = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"parse"</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: [int(s.strip()) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> x[<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">-1</span></span>].split(<span class="hljs-string"><span class="hljs-string">","</span></span>)], ArrayType(IntegerType())) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">auc</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(submit, real)</span></span></span><span class="hljs-function">:</span></span> trueSet = set(real) scores = [<span class="hljs-number"><span class="hljs-number">1.0</span></span> / (i + <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i,x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(submit)] labels = [<span class="hljs-number"><span class="hljs-number">1.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> trueSet <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">0.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> submit] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> float(roc_auc_score(labels, scores)) auc_udf = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"auc"</span></span>, auc, DoubleType())</code> </pre> <br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> parse = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"parse"</span></span>, (x : <span class="hljs-type"><span class="hljs-type">String</span></span>) =&gt; x.slice(<span class="hljs-number"><span class="hljs-number">1</span></span>,x.size - <span class="hljs-number"><span class="hljs-number">1</span></span>).split(<span class="hljs-string"><span class="hljs-string">","</span></span>).map(_.trim.toInt)) <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">AucAccumulator</span></span></span><span class="hljs-class">(</span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">height: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">Int</span></span></span></span><span class="hljs-class"><span class="hljs-params">, area: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">Int</span></span></span></span><span class="hljs-class"><span class="hljs-params">, negatives: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">Int</span></span></span></span></span><span class="hljs-class">) </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">val</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">auc_udf</span></span></span><span class="hljs-class"> </span></span>= sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"auc"</span></span>, (byScore: <span class="hljs-type"><span class="hljs-type">Seq</span></span>[<span class="hljs-type"><span class="hljs-type">Int</span></span>], gt: <span class="hljs-type"><span class="hljs-type">Seq</span></span>[<span class="hljs-type"><span class="hljs-type">Int</span></span>]) =&gt; { <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> byLabel = gt.toSet <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> accumulator = byScore.foldLeft(<span class="hljs-type"><span class="hljs-type">AucAccumulator</span></span>(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>))((accumulated, current) =&gt; { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (byLabel.contains(current)) { accumulated.copy(height = accumulated.height + <span class="hljs-number"><span class="hljs-number">1</span></span>) } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { accumulated.copy(area = accumulated.area + accumulated.height, negatives = accumulated.negatives + <span class="hljs-number"><span class="hljs-number">1</span></span>) } }) (accumulator.area).toDouble / (accumulator.negatives * accumulator.height) })</code> </pre> <br><p>  Ketika mengimplementasikan fungsi tertentu, jelas bahwa Python lebih ringkas, terutama karena kemampuan untuk menggunakan fungsi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">scikit-learn bawaan</a> .  Namun, ada saat-saat yang tidak menyenangkan - Anda harus secara eksplisit menentukan jenis nilai pengembalian, sedangkan di Scala ditentukan secara otomatis.  Mari kita lakukan operasi: </p><br><pre> <code class="python hljs">toValidate.select(auc_udf(parse(<span class="hljs-string"><span class="hljs-string">"submit"</span></span>), parse(<span class="hljs-string"><span class="hljs-string">"real"</span></span>))).groupBy().avg().show()</code> </pre> <br><pre> <code class="scala hljs">toValidate.select(auc_udf(parse($<span class="hljs-string"><span class="hljs-string">"submit"</span></span>), parse($<span class="hljs-string"><span class="hljs-string">"real"</span></span>))).groupBy().avg().show()</code> </pre> <br><p>  Kode ini terlihat hampir identik, tetapi hasilnya mengecewakan. </p><br><p><img src="https://habrastorage.org/webt/vc/jw/y_/vcjwy_zwvfbkb_jlw6mimlogdwi.png"></p><br><p>  Implementasi di PySpark berhasil satu setengah menit, bukan dua detik pada Scala, yaitu, <strong>Python ternyata 45 kali lebih lambat</strong> .  Saat berjalan, top menunjukkan 4 proses Python aktif yang berjalan dengan kecepatan penuh, dan ini menunjukkan bahwa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Global Interpreter Lock</a> tidak menciptakan masalah di sini.  Tapi!  Mungkin masalahnya ada pada implementasi internal scikit-learn - mari kita coba mereproduksi kode Python secara harfiah, tanpa menggunakan perpustakaan standar. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">auc</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(submit, real)</span></span></span><span class="hljs-function">:</span></span> trueSet = set(real) height = <span class="hljs-number"><span class="hljs-number">0</span></span> area = <span class="hljs-number"><span class="hljs-number">0</span></span> negatives = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> candidate <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> submit: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> candidate <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> trueSet: height = height + <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: area = area + height negatives = negatives + <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> float(area) / (negatives * height) auc_udf_modified = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"auc_modified"</span></span>, auc, DoubleType()) toValidate.select(auc_udf_modified(parse(<span class="hljs-string"><span class="hljs-string">"submit"</span></span>), parse(<span class="hljs-string"><span class="hljs-string">"real"</span></span>))).groupBy().avg().show()</code> </pre> <br><p><img src="https://habrastorage.org/webt/cx/o2/io/cxo2ioxdl18a6djwsmgr6dravhy.png"></p><br><p>  Eksperimen menunjukkan hasil yang menarik.  Di satu sisi, dengan pendekatan ini, produktivitas diratakan, tetapi di sisi lain, laconicism menghilang.  Hasil yang diperoleh dapat menunjukkan bahwa ketika bekerja di Python menggunakan modul C ++ tambahan, overhead signifikan muncul untuk beralih antar konteks.  Tentu saja, ada overhead serupa ketika menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">JNI</a> di Java / Scala, namun, saya tidak harus berurusan dengan contoh degradasi 45 kali saat menggunakannya. </p><br><p>  Untuk analisis yang lebih rinci, kami akan melakukan dua percobaan tambahan: menggunakan Python murni tanpa Spark untuk mengukur kontribusi dari paket doa, dan dengan peningkatan ukuran data di Spark untuk mengamortisasi overhead dan mendapatkan perbandingan yang lebih akurat. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">parse</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [int(s.strip()) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> x[<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">-1</span></span>].split(<span class="hljs-string"><span class="hljs-string">","</span></span>)] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">auc</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(submit, real)</span></span></span><span class="hljs-function">:</span></span> trueSet = set(real) height = <span class="hljs-number"><span class="hljs-number">0</span></span> area = <span class="hljs-number"><span class="hljs-number">0</span></span> negatives = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> candidate <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> submit: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> candidate <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> trueSet: height = height + <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: area = area + height negatives = negatives + <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> float(area) / (negatives * height) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sklearn_auc</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(submit, real)</span></span></span><span class="hljs-function">:</span></span> trueSet = set(real) scores = [<span class="hljs-number"><span class="hljs-number">1.0</span></span> / (i + <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i,x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(submit)] labels = [<span class="hljs-number"><span class="hljs-number">1.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> trueSet <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">0.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> submit] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> float(roc_auc_score(labels, scores))</code> </pre> <br><p><img src="https://habrastorage.org/webt/bk/ul/m1/bkulm1xarplv2-f4ilyg5dtxixw.png"></p><br><p>  Eksperimen dengan Python dan Pandas lokal mengkonfirmasi asumsi overhead yang signifikan ketika menggunakan paket tambahan - ketika menggunakan scikit-learn, kecepatan berkurang lebih dari 20 kali.  Namun, 20 bukan 45 - mari kita coba "mengembang" data dan membandingkan kinerja Spark lagi. </p><br><pre> <code class="python hljs">k4 = toValidate.union(toValidate) k8 = k4.union(k4) m1 = k8.union(k8) m2 = m1.union(m1) m4 = m2.union(m2).repartition(<span class="hljs-number"><span class="hljs-number">4</span></span>).cache() m4.count()</code> </pre> <br><p><img src="https://habrastorage.org/webt/zp/sf/po/zpsfpoabev7w3_xjcn0vqobwqf4.png"></p><br><p>  Perbandingan baru menunjukkan keunggulan kecepatan implementasi Scala atas Python oleh 7-8 kali - 7 detik versus 55. Akhirnya, mari kita coba "tercepat yang ada di Python" - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">numpy</a> untuk menghitung jumlah array: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy numpy_sum = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"numpy_sum"</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: float(numpy.sum(x)), DoubleType())</code> </pre> <br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> my_sum = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"my_sum"</span></span>, (x: <span class="hljs-type"><span class="hljs-type">Seq</span></span>[<span class="hljs-type"><span class="hljs-type">Int</span></span>]) =&gt; x.map(_.toDouble).sum)</code> </pre> <br><p><img src="https://habrastorage.org/webt/og/tr/gx/ogtrgx-kkuzx4kmkh4pd5cbyd-k.png"></p><br><p>  Lagi-lagi pelambatan signifikan - 5 detik Scala versus 80 detik Python.  Kesimpulannya, kita bisa menarik kesimpulan berikut: </p><br><ul><li>  Sementara PySpark beroperasi dalam kerangka API standar, ini benar-benar dapat dibandingkan dalam kecepatan dengan Scala. </li><li>  Ketika logika spesifik muncul dalam bentuk Fungsi yang Ditentukan Pengguna, kinerja PySpark menurun secara nyata.  Dengan informasi yang cukup, ketika waktu pemrosesan blok data melebihi beberapa detik, implementasi Python 5-10 lebih lambat karena kebutuhan untuk memindahkan data antara proses dan sumber daya limbah pada menafsirkan Python. </li><li>  Jika penggunaan fungsi tambahan yang diterapkan dalam modul C ++ muncul, maka biaya panggilan tambahan muncul, dan perbedaan antara Python dan Scala meningkat hingga 10-50 kali. </li></ul><br><p>  Akibatnya, terlepas dari semua pesona Python, penggunaannya bersama dengan Spark tidak selalu terlihat benar.  Jika tidak ada begitu banyak data untuk membuat overhead Python signifikan, maka Anda harus berpikir tentang apakah Spark diperlukan di sini?  Jika ada banyak data, tetapi pemrosesan terjadi dalam kerangka standar SQL API Spark, maka apakah Python diperlukan di sini? </p><br><p>  Jika ada banyak data dan sering harus berurusan dengan tugas-tugas yang melampaui batas SQL API, maka untuk melakukan jumlah pekerjaan yang sama ketika menggunakan PySpark, Anda harus menambah kluster di kali.  Misalnya, untuk Odnoklassniki, biaya pengeluaran modal untuk cluster Spark akan meningkat ratusan juta rubel.  Dan jika Anda mencoba untuk mengambil keuntungan dari kemampuan canggih dari pustaka ekosistem Python, yaitu, risiko melambat tidak hanya pada waktu, tetapi urutan besarnya. </p><br><p>  Beberapa akselerasi dapat diperoleh dengan menggunakan fungsi vektorisasi yang relatif baru.  Dalam hal ini, tidak satu baris pun dimasukkan ke input UDF, tetapi paket beberapa baris dalam bentuk Bingkai Data Pandas.  Namun, pengembangan fungsi ini <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">belum selesai</a> , dan bahkan dalam kasus ini perbedaannya akan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">signifikan</a> . </p><br><p>  Alternatifnya adalah dengan mempertahankan tim insinyur data yang luas, dapat dengan cepat mengatasi kebutuhan ilmuwan data dengan fungsi tambahan.  Atau untuk membenamkan diri di dunia Scala, karena tidak begitu sulit: banyak alat yang diperlukan sudah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ada</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">program pelatihan</a> muncul yang melampaui PySpark. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id443324/">https://habr.com/ru/post/id443324/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id443312/index.html">Manajemen memori python</a></li>
<li><a href="../id443316/index.html">Pengujian ReactJS: Seberapa Dalam Lubang Kelinci itu</a></li>
<li><a href="../id443318/index.html">Menulis loader wasm untuk Ghidra. Bagian 1: Pernyataan masalah dan pengaturan lingkungan</a></li>
<li><a href="../id443320/index.html">Sistem manajemen dokumen elektronik "Wazir"</a></li>
<li><a href="../id443322/index.html">GitLab 11.8 dirilis dengan SAST untuk JavaScript, GitLab Pages untuk subkelompok dan pelacakan bug</a></li>
<li><a href="../id443326/index.html">Python & Arduino. Sederhana, cepat dan cantik</a></li>
<li><a href="../id443330/index.html">Minggu Keamanan 11: RSA 2019 dan masa depan yang lebih cerah</a></li>
<li><a href="../id443332/index.html">Robot dapur</a></li>
<li><a href="../id443334/index.html">Wajah kuning</a></li>
<li><a href="../id443336/index.html">Bekerja dengan kamera dalam Flutter</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>