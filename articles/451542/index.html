<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üìÜ üëêüèª ‚úåüèΩ C√≥mo y por qu√© hicimos el reconocimiento de hitos en Mail.ru Cloud ü¶Ö üí® ‚óÄÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Con el advenimiento de las c√°maras de alta calidad en los tel√©fonos m√≥viles, estamos fotografiando cada vez m√°s y filmando videos de los momentos bril...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>C√≥mo y por qu√© hicimos el reconocimiento de hitos en Mail.ru Cloud</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/451542/"><img src="https://habrastorage.org/getpro/habr/post_images/a5d/eeb/615/a5deeb615e9a57fb482a1a5464ab6fcc.png"><br><br>  Con el advenimiento de las c√°maras de alta calidad en los tel√©fonos m√≥viles, estamos fotografiando cada vez m√°s y filmando videos de los momentos brillantes e importantes de nuestras vidas.  Muchos de nosotros tenemos archivos fotogr√°ficos que datan de decenas de a√±os y miles de fotograf√≠as, en los que cada vez es m√°s dif√≠cil navegar.  Recuerde cu√°nto tiempo llev√≥ encontrar la foto correcta hace varios a√±os. <br><br>  Uno de los objetivos de Mail.ru Cloud es proporcionar el acceso y la b√∫squeda m√°s convenientes en su archivo de fotos y videos.  Para hacer esto, nosotros, el equipo de visi√≥n artificial de Mail.ru, hemos creado e implementado sistemas inteligentes de procesamiento de fotos: b√∫squeda por objetos, escenas, caras, etc. Otra tecnolog√≠a tan llamativa es el reconocimiento de im√°genes.  Y hoy hablar√© sobre c√≥mo resolvimos este problema con la ayuda de Deep Learning. <br><a name="habracut"></a><br>  Imagina la situaci√≥n: te fuiste de vacaciones y trajiste un mont√≥n de fotos.  Y en una conversaci√≥n con amigos, te pidieron que mostraras c√≥mo visitaste un palacio, castillo, pir√°mide, templo, lago, cascada, monta√±a, etc.  Empiezas a desplazarte fren√©ticamente por la carpeta con fotos, tratando de encontrar la correcta.  Lo m√°s probable es que no lo encuentre entre cientos de im√°genes y diga que lo mostrar√° m√°s tarde. <br><br>  Resolvemos este problema agrupando fotos personalizadas en √°lbumes.  Esto facilita encontrar las im√°genes correctas con solo unos pocos clics.  Ahora tenemos √°lbumes en caras, en objetos y escenas, as√≠ como en atracciones. <br><br>  Las fotos con puntos de referencia son importantes porque a menudo muestran momentos importantes de nuestras vidas (por ejemplo, viajes).  Estas pueden ser fotograf√≠as en el fondo de alguna estructura arquitect√≥nica o un rinc√≥n de la naturaleza que el hombre no haya tocado.  Por lo tanto, necesitamos encontrar estas fotos y darles a los usuarios un acceso f√°cil y r√°pido a ellas. <br><br><h1>  Reconocimiento de funciones </h1><br>  Pero hay un matiz: no puedes simplemente tomar y entrenar alg√∫n modelo para reconocer las vistas, hay muchas dificultades. <br><br><ul><li>  En primer lugar, no podemos describir claramente qu√© es un "hito".  No podemos decir por qu√© un edificio es un hito, y estar junto a √©l no lo es.  Este no es un concepto formalizado, lo que complica la formulaci√≥n del problema de reconocimiento. <br></li><li>  En segundo lugar, las vistas son extremadamente diversas.  Pueden ser edificios hist√≥ricos o culturales: templos, palacios, castillos.  Estos pueden ser los monumentos m√°s diversos.  Pueden ser objetos naturales: lagos, ca√±ones, cascadas.  Y un modelo debe poder encontrar todas estas atracciones. <br></li><li>  En tercer lugar, hay muy pocas im√°genes con vistas, seg√∫n nuestros c√°lculos, solo se encuentran en el 1-3% de las fotos de los usuarios.  Por lo tanto, no podemos permitirnos errores en el reconocimiento, porque si mostramos a una persona una foto sin un punto de inter√©s, se notar√° de inmediato y causar√° desconcierto y reacci√≥n negativa.  O, por el contrario, le mostramos a la persona una foto con un punto de referencia en Nueva York, y nunca hab√≠a estado en Estados Unidos.  Por lo tanto, el modelo de reconocimiento debe tener una FPR baja (tasa de falsos positivos). <br></li><li>  Cuarto, aproximadamente el 50% de los usuarios, o incluso m√°s, apagan el almacenamiento de informaci√≥n geogr√°fica al fotografiar.  Necesitamos tener esto en cuenta y determinar el lugar √∫nicamente a partir de la imagen.  La mayor√≠a de los servicios que hoy en d√≠a logran trabajar con lugares de inter√©s lo hacen gracias a los datos geogr√°ficos.  Nuestros requisitos iniciales fueron m√°s estrictos. <br></li></ul><br>  Mostrar√© ahora con ejemplos. <br><br>  Aqu√≠ hay objetos similares, tres catedrales g√≥ticas francesas.  A la izquierda est√° la Catedral de Amiens, en el centro de la Catedral de Reims, a la derecha est√° Notre Dame de Paris. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bd4/da6/7f7/bd4da67f77caa21c8afdd344e1e1a05a.png"><br><br>  Incluso una persona necesita algo de tiempo para mirarlos y comprender que se trata de catedrales diferentes, y que la m√°quina tambi√©n debe ser capaz de manejarlo, y m√°s r√°pido que una persona. <br><br>  Y aqu√≠ hay un ejemplo de otra dificultad: las tres fotos en la diapositiva son Notre Dame de Paris, tomadas desde diferentes √°ngulos.  Las fotos resultaron ser muy diferentes, pero todas necesitan ser reconocidas y encontradas. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/837/0a4/f9b/8370a4f9b8785814cd4cc7368770aac2.png"><br><br>  Los objetos naturales son completamente diferentes de los arquitect√≥nicos.  A la izquierda est√° Cesarea en Israel, a la derecha est√° el Parque Ingl√©s en Munich. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/55d/632/245/55d632245cf529e935e25a78de40f020.png"><br><br>  En estas fotograf√≠as hay muy pocos detalles caracter√≠sticos para los cuales el modelo puede "captar". <br><br><h1>  Nuestro metodo </h1><br>  Nuestro m√©todo se basa completamente en redes neuronales convolucionales profundas.  Como enfoque del aprendizaje, eligieron el llamado aprendizaje curricular: aprendizaje en varias etapas.  Para trabajar de manera m√°s eficiente tanto en presencia de geodatos como en ausencia de ellos, hicimos una inferencia especial (conclusi√≥n).  Te contar√© sobre cada una de las etapas con m√°s detalle. <br><br><h1>  Datacet </h1><br>  El combustible para el aprendizaje autom√°tico son los datos.  Y antes que nada, necesit√°bamos recopilar un conjunto de datos para la capacitaci√≥n modelo. <br><br>  Dividimos el mundo en 4 regiones, cada una de las cuales se utiliza en diferentes etapas de entrenamiento.  Luego, se tomaron pa√≠ses en cada regi√≥n, para cada pa√≠s se compil√≥ una lista de ciudades y se compil√≥ una base de datos de fotograf√≠as de sus atracciones.  A continuaci√≥n se presentan ejemplos de datos. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3f6/e54/629/3f6e546299616e59efd6a1a1464b9b65.png"><br><br>  Primero, tratamos de entrenar nuestro modelo en la base resultante.  Los resultados fueron malos.  Comenzaron a analizar, y result√≥ que los datos est√°n muy "sucios".  Cada atracci√≥n ten√≠a una gran cantidad de basura.  Que hacer  La revisi√≥n manual de toda la gran cantidad de datos es costosa, triste y poco inteligente.  Por lo tanto, hicimos una limpieza autom√°tica de la base, durante la cual el procesamiento manual se usa solo en un paso: para cada atracci√≥n, seleccionamos manualmente 3-5 fotograf√≠as de referencia que contienen con precisi√≥n la atracci√≥n deseada en una perspectiva m√°s o menos correcta.  Resulta bastante r√°pido, porque el volumen de dichos datos de referencia es peque√±o en relaci√≥n con toda la base de datos.  Luego, ya se realiza una limpieza autom√°tica basada en redes neuronales convolucionales profundas. <br><br>  Adem√°s, usar√© el t√©rmino "incrustaci√≥n", por el cual entender√© lo siguiente.  Tenemos una red neuronal convolucional, la entrenamos para la clasificaci√≥n, cortamos la √∫ltima capa de clasificaci√≥n, tomamos algunas im√°genes, atravesamos la red y obtuvimos un vector num√©rico en la salida.  Lo llamar√© incrustaci√≥n. <br><br>  Como dije, nuestra capacitaci√≥n se llev√≥ a cabo en varias etapas, correspondientes a partes de nuestra base de datos.  Por lo tanto, primero tomamos una red neuronal de la etapa anterior o una red de inicializaci√≥n. <br><br>  Realizaremos las fotos de los lugares de inter√©s a trav√©s de la red y realizaremos varias incrustaciones.  Ahora puedes limpiar la base.  Tomamos todas las im√°genes del conjunto de datos para esta atracci√≥n, y tambi√©n manejamos cada imagen a trav√©s de la red.  Obtenemos un mont√≥n de incrustaciones y para cada una de ellas consideramos las distancias a la incrustaci√≥n de est√°ndares.  Luego calculamos la distancia promedio, y si es m√°s de un cierto umbral, que es el par√°metro del algoritmo, consideramos que esta no es una atracci√≥n tur√≠stica.  Si la distancia promedio es menor que el umbral, entonces dejamos esta foto. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/305/300/a87/305300a8734289229c57e0d1876fa74e.png"><br><br>  Como resultado, obtuvimos una base de datos que contiene m√°s de 11 mil atracciones de m√°s de 500 ciudades en 70 pa√≠ses del mundo, m√°s de 2.3 millones de fotos.  Ahora es el momento de recordar que la mayor√≠a de las fotos no contienen atracciones en absoluto.  Esta informaci√≥n debe ser compartida de alguna manera con nuestros modelos.  Por lo tanto, agregamos 900 mil fotos sin vistas a nuestra base de datos y capacitamos a nuestro modelo en el conjunto de datos resultante. <br><br>  Para medir la calidad de la capacitaci√≥n, presentamos una prueba fuera de l√≠nea.  Basado en el hecho de que las vistas se encuentran solo en aproximadamente el 1-3% de las fotograf√≠as, compilamos manualmente un conjunto de 290 fotograf√≠as que muestran vistas.  Estas son fotograf√≠as diferentes y bastante complejas con una gran cantidad de objetos tomados desde diferentes √°ngulos, por lo que la prueba es lo m√°s dif√≠cil posible para el modelo.  Por el mismo principio, seleccionamos 11 mil fotograf√≠as sin vistas, que tambi√©n son bastante complejas, e intentamos encontrar objetos que sean muy similares a las vistas disponibles en nuestra base de datos. <br><br>  Para evaluar la calidad de la capacitaci√≥n, medimos la precisi√≥n de nuestro modelo a partir de fotograf√≠as con y sin vistas.  Estas son nuestras dos m√©tricas principales. <br><br><h1>  Enfoques existentes </h1><br>  Hay relativamente poca informaci√≥n sobre el reconocimiento de la vista en la literatura cient√≠fica.  La mayor√≠a de las soluciones se basan en caracter√≠sticas locales.  La idea es que tenemos una determinada imagen de solicitud y una imagen de la base de datos.  En estas im√°genes encontramos signos locales, puntos clave y los comparamos.  Si el n√∫mero de coincidencias es lo suficientemente grande, creemos que hemos encontrado un punto de inter√©s. <br><br>  Hasta la fecha, el mejor m√©todo es el m√©todo propuesto por Google, DELF (funciones locales profundas), en el que una comparaci√≥n de las funciones locales se combina con el aprendizaje profundo.  Al ejecutar la imagen de entrada a trav√©s de la red de convoluci√≥n, obtenemos algunos signos DELF. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e17/a2d/803/e17a2d803135ba5a7821842f035b1cf8.png"><br><br>  ¬øC√≥mo es el reconocimiento de atracciones?  Tenemos una base de datos de fotos y una imagen de entrada, y queremos entender si hay una atracci√≥n tur√≠stica en ella o no.  Corremos todas las im√°genes a trav√©s de DELF, obtenemos los signos correspondientes para la base y para la imagen de entrada.  Luego realizamos una b√∫squeda utilizando el m√©todo de vecinos m√°s cercanos y en la salida obtenemos im√°genes candidatas con signos.  Comparamos estos signos con la ayuda de la verificaci√≥n geom√©trica: si lo pasan con √©xito, entonces creemos que hay un punto de inter√©s en la imagen. <br><br><h1>  Red neuronal convolucional </h1><br>  Para el aprendizaje profundo, la capacitaci√≥n previa es crucial.  Por lo tanto, tomamos la base de escenas y preentrenamos en ella nuestra red neuronal.  Por qu√©  Una escena es un objeto complejo que incluye una gran cantidad de otros objetos.  Y la atracci√≥n es un caso especial de la escena.  Como modelo de pre-entrenamiento sobre esa base, podemos darle al modelo una idea de algunas caracter√≠sticas de bajo nivel que luego pueden generalizarse para el reconocimiento exitoso de las atracciones. <br><br>  Como modelo, utilizamos una red neuronal de la familia de redes residuales.  Su caracter√≠stica principal es que usan un bloque residual, que incluye una conexi√≥n de salto, que permite que la se√±al pase libremente sin meterse en capas con pesas.  Con esta arquitectura, puede entrenar cualitativamente redes profundas y lidiar con el efecto de desenfoque de gradiente, que es muy importante al aprender. <br><br>  Nuestro modelo es Wide ResNet 50-2, una modificaci√≥n de ResNet 50, en la que el n√∫mero de convoluciones en el bloque de cuello de botella interno se duplica. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/008/a1f/544/008a1f544843103423b5bcdb4c8b1fe5.png"><br><br>  La red es muy eficiente.  Realizamos pruebas en nuestra base de datos de escenas y esto es lo que obtuvimos: <br><br><div class="scrollable-table"><table><tbody><tr><th>  Modelo <br></th><th>  Top 1 err <br></th><th>  Top 5 err <br></th></tr><tr><td>  ResNet-50 <br></td><td>  46,1% <br></td><td>  15,7% <br></td></tr><tr><td>  ResNet-200 <br></td><td>  42,6% <br></td><td>  12,9% <br></td></tr><tr><td>  SE-ResNext-101 <br></td><td>  42% <br></td><td>  12,1% <br></td></tr><tr><td>  WRN-50-2 (¬°r√°pido!) <br></td><td>  41,8% <br></td><td>  11,8% <br></td></tr></tbody></table></div><br>  Wide ResNet result√≥ ser casi el doble de r√°pido que la red ResNet 200 bastante grande. Y la velocidad de operaci√≥n es muy importante para la operaci√≥n.  En base a la totalidad de estas circunstancias, tomamos Wide ResNet 50-2 como nuestra red neuronal principal. <br><br><h2>  Entrenamiento </h2><br>  Para entrenar la red, necesitamos p√©rdida (funci√≥n de p√©rdida).  Para seleccionarlo, decidimos utilizar el enfoque de aprendizaje m√©trico: se entrena una red neuronal para que los representantes de la misma clase se unan en un grupo.  Al mismo tiempo, los grupos para diferentes clases deben estar lo m√°s separados posible.  Para las atracciones, utilizamos la p√©rdida de centro, que re√∫ne puntos de la misma clase en un determinado centro.  Una caracter√≠stica importante de este enfoque es que no requiere muestreo negativo, que en las √∫ltimas etapas del entrenamiento es un procedimiento bastante dif√≠cil. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/27a/ed1/cfb/27aed1cfbe8d7cffa90bc1a96b578eff.png"><br><br>  Perm√≠tame recordarle que tenemos n clases de atracciones y otra clase de "no atracciones", la p√©rdida de Centro no se utiliza para ello.  Queremos decir que un punto de referencia es el mismo objeto y que tiene una estructura, por lo tanto, es aconsejable considerar un centro para √©l.  Pero una atracci√≥n tur√≠stica no puede ser nada, y considerar el centro para √©l no es razonable. <br><br>  Luego lo armamos todo y obtuvimos un modelo para el entrenamiento.  Consta de tres partes principales: <br><br><ul><li>  Red neuronal convolucional Wide ResNet 50-2, pre-entrenada en base a escenas; <br></li><li>  Partes de incrustaci√≥n que consisten en una capa completamente conectada y una capa de norma de Batch; <br></li><li>  Un clasificador, que es una capa totalmente conectada, seguido de un par de p√©rdida Softmax y p√©rdida central. <br></li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/6dd/7bf/338/6dd7bf33835ea1d84565a04c5ff1e281.jpg"><br><br>  Como recordar√°, nuestra base est√° dividida en 4 partes por regi√≥n del mundo.  Utilizamos estas 4 partes como parte del paradigma de aprendizaje curricular.  En cada etapa, tenemos el conjunto de datos actual, le agregamos otra parte del mundo y obtenemos un nuevo conjunto de datos de capacitaci√≥n. <br><br>  El modelo consta de tres partes, y para cada una de ellas utilizamos nuestra propia tasa de aprendizaje en la capacitaci√≥n.  Esto es necesario para que la red no solo pueda aprender las vistas de la nueva parte del conjunto de datos que agregamos, sino tambi√©n que no olvide los datos ya aprendidos.  Despu√©s de muchos experimentos, este enfoque result√≥ ser el m√°s efectivo. <br><br>  Entonces, entrenamos al modelo.  Necesitas entender c√≥mo funciona.  Usemos el mapa de activaci√≥n de clase para ver qu√© parte de la imagen responde mejor a nuestra red neuronal.  En la siguiente imagen, en la primera fila, las im√°genes de entrada, y en la segunda, se superponen el mapa de activaci√≥n de clase de la cuadr√≠cula, que hemos entrenado en el paso anterior. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5aa/4aa/264/5aa4aa26413082931c28f1a393a64b95.png"><br><br>  El mapa de calor muestra en qu√© partes de la imagen se est√° enfocando la red.  Del mapa de activaci√≥n de clase se puede ver que nuestra red neuronal ha aprendido con √©xito el concepto de atracci√≥n. <br><br><h2>  Inferencia </h2><br>  Ahora necesita utilizar de alguna manera este conocimiento para obtener el resultado.  Como utilizamos la p√©rdida del Centro para el entrenamiento, parece l√≥gico deducir que tambi√©n se calcula el tserotoide para las atracciones. <br><br>  Para esto, tomamos parte de las im√°genes del conjunto de entrenamiento para alg√∫n tipo de atracci√≥n, por ejemplo, para el Jinete de Bronce.  Los ejecutamos a trav√©s de la red, obtenemos incrustaciones, promediamos y obtenemos un centroide. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c49/b0f/69e/c49b0f69e33581f30c05985a98702f6f.png"><br><br>  Pero surge la pregunta: ¬øcu√°ntos centroides para una atracci√≥n tiene sentido calcular?  Al principio, la respuesta parece clara y l√≥gica: un centroide.  Pero esto result√≥ no ser as√≠.  Al principio, tambi√©n decidimos hacer un centroide y obtuvimos un resultado bastante bueno.  Entonces, ¬øpor qu√© necesitas tomar algunos centroides? <br><br>  En primer lugar, nuestros datos no est√°n completamente limpios.  Aunque limpiamos el conjunto de datos, solo eliminamos la basura.  Y podr√≠amos tener im√°genes que no podr√≠an considerarse basura, pero que empeorar√≠an el resultado. <br><br>  Por ejemplo, tengo una clase hist√≥rica de Palacio de Invierno.  Quiero contar un centroide para √©l.  Pero el conjunto inclu√≠a una serie de fotograf√≠as con la Plaza del Palacio y el arco del Edificio del Estado Mayor.  Si consideramos el centroide en todas las im√°genes, resultar√° no demasiado estable.  Es necesario agrupar de alguna manera sus incrustaciones, que se obtienen de la cuadr√≠cula habitual, tomar solo el centroide que es responsable del Palacio de Invierno y calcular el promedio de acuerdo con estos datos. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/000/7b7/8ef/0007b78efb3d15296bab0ebef0cf4dfb.png"><br><br>  En segundo lugar, se pueden tomar fotograf√≠as desde diferentes √°ngulos. <br><br>  Citar√© el campanario de Belfort en Brujas como una ilustraci√≥n de este comportamiento.  Se cuentan dos centroides para ella.  En la fila superior de la imagen est√°n las fotos que est√°n m√°s cerca del primer centroide, y en la segunda fila, las que est√°n m√°s cerca del segundo centroide: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d13/5f4/105/d135f4105f2823a612e458f946e25221.png"><br><br>  El primer centroide es responsable de las fotograf√≠as de primeros planos m√°s "inteligentes" tomadas desde la Plaza del Mercado de Brujas.  Y el segundo centroide es responsable de las fotograf√≠as tomadas desde lejos, desde las calles adyacentes. <br><br>  Resulta que al calcular varios centroides por clase de un punto de inter√©s, podemos mostrar diferentes √°ngulos de este punto de inter√©s en inferencia. <br><br>  Entonces, ¬øc√≥mo encontramos estos conjuntos para calcular los centroides?  Aplicamos agrupamiento jer√°rquico a los conjuntos de datos para cada punto de inter√©s: enlace completo.  Con su ayuda, encontramos grupos v√°lidos mediante los cuales calcularemos los centroides.  Por grupos v√°lidos nos referimos a aquellos que, como resultado de la agrupaci√≥n, contienen al menos 50 fotograf√≠as.  Los grupos restantes se descartan.  Como resultado, result√≥ que aproximadamente el 20% de las vistas tienen m√°s de un centroide. <br><br>  Ahora inferencia.  Lo calculamos en dos etapas: primero, ejecutamos la imagen de entrada a trav√©s de nuestra red neuronal convolucional y realizamos la incrustaci√≥n, y luego, usando el producto escalar, comparamos la incrustaci√≥n con los centroides.  Si las im√°genes contienen geodatos, restringimos la b√∫squeda a los centroides, que se relacionan con las atracciones ubicadas en un cuadrado de 1 por 1 km desde el lugar de disparo.  Esto le permite buscar con mayor precisi√≥n, elegir un umbral m√°s bajo para la comparaci√≥n posterior.  Si la distancia obtenida es mayor que el umbral, que es un par√°metro del algoritmo, entonces decimos que en la foto hay un punto de inter√©s con el valor m√°ximo del producto escalar.  Si menos, entonces esto no es una atracci√≥n tur√≠stica. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d01/75c/5a5/d0175c5a5142647bf9446f7d65df0509.png"><br><br>  Supongamos que la foto contiene un hito.     ,       .   ,    .    ,        .      ,          -.    - ,   ,     .  ,         ,     . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/14b/ffe/04f/14bffe04fe6ca1b3d09876432e319e53.png"><br><br><h1>  Resultados de la prueba </h1><br>      DELF,    ,         .    . <br><br><div class="scrollable-table"><table><tbody><tr><th>  <br></th><th>  <br></th><th>   <br></th></tr><tr><td>  <br></td><td> 80 % <br></td><td> 99 % <br></td></tr><tr><td> DELF <br></td><td> 80,1 % <br></td><td> 99 % <br></td></tr></tbody></table></div><br>       :  ( 100 ),   87 %     ,  .      :  85,3 %.      46 %,     ‚Äî          . <br><br><div class="scrollable-table"><table><tbody><tr><th>  <br></th><th>  <br></th><th>     <br></th></tr><tr><td>  <br></td><td> 85,3 % <br></td><td> 87 % <br></td></tr><tr><td>  <br></td><td> 46 % <br></td><td> 13 % <br></td></tr></tbody></table></div><br>    /B-   .          10 %,       3 %,       13 %. <br><br>       DELF.  GPU DELF  7  ,    7  ,      1 .  CPU DELF             .      CPU   15  .        ,     . <br><br><h1> :    </h1><br>               .  . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/72c/df4/52e/72cdf452e2105c18f8e4375a1b69d497.png"><br><br>   ,        .   ¬´¬ª, ¬´¬ª, ¬´¬ª.      ,    .      ,        . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ec6/7f8/929/ec67f892964ee568754a86cd48e16b51.png"><br><br>   :   , ,   .       Instagram      ,      ,    ‚Äî     . <br><br><h1>  </h1><br>      . <br><br><ol><li>   .      ,      .           . <br></li><li>        deep metric learning,        . <br></li><li>       curriculum learning ‚Äî   .      .  inference    ,           . <br></li></ol><br>  ,   ‚Äî    .     ,     ,    .         -   .      ! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/451542/">https://habr.com/ru/post/451542/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../451528/index.html">‚ÄúY as√≠ sigue‚Äù: que los proveedores de la nube no est√°n de acuerdo con los datos personales</a></li>
<li><a href="../451532/index.html">Noticias del mundo de OpenStreetMap No. 459 (30/04/2019 - 06/05/2019)</a></li>
<li><a href="../451534/index.html">12 principios de animaci√≥n en el desarrollo de videojuegos</a></li>
<li><a href="../451538/index.html">Gu√≠a de escalado paralelo de Amazon Redshift y resultados de la prueba</a></li>
<li><a href="../451540/index.html">Cu√°ntos desarrolladores necesitan para crear servicios como Airbnb</a></li>
<li><a href="../451552/index.html">Creamos canales de venta en red del gadget DO-RA</a></li>
<li><a href="../451556/index.html">Flutter: localizaci√≥n de aplicaciones con Android Studio</a></li>
<li><a href="../451558/index.html">Un d√≠a en la vida de la automatizaci√≥n del control de calidad.</a></li>
<li><a href="../451560/index.html">Estimado cliente, es por eso que este cambio tom√≥ tanto tiempo.</a></li>
<li><a href="../451562/index.html">¬øC√≥mo escapar de una secta?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>