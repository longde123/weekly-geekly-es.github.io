<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèº‚Äçü§ù‚Äçüë©üèª ü•° üçù C√≥mo aumentamos la productividad de servicio de Tensorflow en un 70% üë©üèæ‚Äçü§ù‚Äçüë©üèª ‚ÜòÔ∏è üíÜüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Tensorflow se ha convertido en la plataforma est√°ndar para el aprendizaje autom√°tico (ML), popular tanto en la industria como en la investigaci√≥n. Se ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>C√≥mo aumentamos la productividad de servicio de Tensorflow en un 70%</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/445928/"> Tensorflow se ha convertido en la plataforma est√°ndar para el aprendizaje autom√°tico (ML), popular tanto en la industria como en la investigaci√≥n.  Se han creado muchas bibliotecas, herramientas y marcos gratuitos para capacitar y mantener modelos de ML.  El proyecto de Tensorflow Serving ayuda a mantener los modelos ML en un entorno de producci√≥n distribuido. <br><br>  Nuestro servicio Mux utiliza Tensorflow Serving en varias partes de la infraestructura, ya hemos discutido el uso de Tensorflow Serving en la codificaci√≥n de t√≠tulos de video.  Hoy nos centraremos en m√©todos que mejoran la latencia optimizando tanto el servidor de pron√≥stico como el cliente.  Los pron√≥sticos de modelos suelen ser operaciones "en l√≠nea" (en la ruta cr√≠tica de solicitud de una aplicaci√≥n), por lo tanto, los objetivos principales de la optimizaci√≥n son procesar grandes vol√∫menes de solicitudes con el menor retraso posible. <br><a name="habracut"></a><br><h1>  ¬øQu√© es el servicio Tensorflow? </h1><br>  Tensorflow Serving proporciona una arquitectura de servidor flexible para implementar y mantener modelos ML.  Una vez que el modelo est√° entrenado y listo para ser utilizado para el pron√≥stico, Tensorflow Serving requiere exportarlo a un formato compatible (de servicio). <br><br>  <i>Servable</i> es una abstracci√≥n central que envuelve los objetos de Tensorflow.  Por ejemplo, un modelo se puede representar como uno o m√°s objetos Servable.  Por lo tanto, Servable son los objetos b√°sicos que el cliente usa para realizar c√°lculos.  El tama√±o de servicio es importante: los modelos m√°s peque√±os ocupan menos espacio, usan menos memoria y se cargan m√°s r√°pido.  Para descargar y mantener utilizando la API de predicci√≥n, los modelos deben estar en formato de modelo guardado. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/20b/9f9/47e/20b9f947ea0f318dc7c2eba619b3901f.png"><br><br>  Tensorflow Serving combina los componentes b√°sicos para crear un servidor gRPC / HTTP que sirve a varios modelos ML (o varias versiones), proporciona componentes de monitoreo y una arquitectura personalizada. <br><br><h1>  Tensorflow Sirviendo con Docker </h1><br>  Echemos un vistazo a las m√©tricas b√°sicas de latencia en el pron√≥stico del rendimiento con la configuraci√≥n est√°ndar de Servidor de Tensorflow (sin optimizaci√≥n de CPU). <br><br>  Primero, descargue la √∫ltima imagen del centro TensorFlow Docker: <br><br><pre><code class="bash hljs">docker pull tensorflow/serving:latest</code> </pre> <br>  En este art√≠culo, todos los contenedores se ejecutan en un host con cuatro n√∫cleos, 15 GB, Ubuntu 16.04. <br><br><h3>  Exportar modelo de Tensorflow a modelo guardado </h3><br>  Cuando un modelo se entrena utilizando Tensorflow, la salida se puede guardar como puntos de control variables (archivos en el disco).  La salida se realiza directamente restaurando los puntos de control del modelo o en un formato de gr√°fico congelado congelado (archivo binario). <br><br>  Para el servicio Tensorflow, este gr√°fico congelado debe exportarse al formato GuardadoModelo.  La <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentaci√≥n de Tensorflow</a> contiene ejemplos de exportaci√≥n de modelos entrenados al formato SavedModel. <br><br>  Tensorflow tambi√©n proporciona muchos modelos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">oficiales y de investigaci√≥n</a> como punto de partida para la experimentaci√≥n, la investigaci√≥n o la producci√≥n. <br><br>  Como ejemplo, utilizaremos el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">modelo de red neuronal residual profunda (ResNet)</a> para clasificar un conjunto de datos ImageNet de 1000 clases.  Descargue el modelo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">pre</a> - <code>ResNet-50 v2</code> , espec√≠ficamente la <i>opci√≥n</i> Channels_last (NHWC) en <i>SavedModel</i> : como regla, funciona mejor en la CPU. <br><br>  Copie el directorio del modelo RestNet en la siguiente estructura: <br><br><pre> <code class="plaintext hljs">models/ 1/ saved_model.pb variables/ variables.data-00000-of-00001 variables.index</code> </pre> <br>  Tensorflow Serving espera una estructura de directorio ordenada num√©ricamente para el control de versiones.  En nuestro caso, el directorio <code>1/</code> corresponde al modelo de la versi√≥n 1, que contiene la arquitectura del modelo <code>saved_model.pb</code> con una instant√°nea de los pesos del modelo (variables). <br><br><h3>  Cargando y procesando el modelo guardado </h3><br>  El siguiente comando inicia el servidor modelo Tensorflow Serving en un contenedor Docker.  Para cargar SavedModel, debe montar el directorio del modelo en el directorio del contenedor esperado. <br><br><pre> <code class="plaintext hljs">docker run -d -p 9000:8500 \ -v $(pwd)/models:/models/resnet -e MODEL_NAME=resnet \ -t tensorflow/serving:latest</code> </pre> <br>  La comprobaci√≥n de los registros del contenedor muestra que ModelServer est√° en funcionamiento para manejar las solicitudes de salida para el modelo de <code>resnet</code> en los puntos finales gRPC y HTTP: <br><br><pre> <code class="plaintext hljs">... I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: resnet version: 1} I tensorflow_serving/model_servers/server.cc:286] Running gRPC ModelServer at 0.0.0.0:8500 ... I tensorflow_serving/model_servers/server.cc:302] Exporting HTTP/REST API at:localhost:8501 ...</code> </pre> <br><h3>  Cliente de pron√≥stico </h3><br>  Tensorflow Serving define un esquema de API en formato de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">buffers de protocolo</a> (protobufs).  Las implementaciones de cliente GRPC para la API de pron√≥stico se empaquetan como un paquete de Python <code>tensorflow_serving.apis</code> .  Necesitaremos otro paquete de <code>tensorflow</code> de Python para funciones de utilidad. <br><br>  Instale las dependencias para crear un cliente simple: <br><br><pre> <code class="plaintext hljs">virtualenv .env &amp;&amp; source .env/bin/activate &amp;&amp; \ pip install numpy grpcio opencv-python tensorflow tensorflow-serving-api</code> </pre> <br>  El modelo <code>ResNet-50 v2</code> espera la entrada de tensores de coma flotante en una estructura de datos formateada channel_last (NHWC).  Por lo tanto, la imagen de entrada se lee usando opencv-python y se carga en la matriz numpy (alto √ó ancho √ó canales) como un tipo de datos float32.  El siguiente script crea un c√≥digo auxiliar de cliente de predicci√≥n y carga los datos JPEG en una matriz numpy, los convierte en tensor_proto para hacer una solicitud de pron√≥stico para gRPC: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/env python from __future__ import print_function import argparse import numpy as np import time tt = time.time() import cv2 import tensorflow as tf from grpc.beta import implementations from tensorflow_serving.apis import predict_pb2 from tensorflow_serving.apis import prediction_service_pb2 parser = argparse.ArgumentParser(description='incetion grpc client flags.') parser.add_argument('--host', default='0.0.0.0', help='inception serving host') parser.add_argument('--port', default='9000', help='inception serving port') parser.add_argument('--image', default='', help='path to JPEG image file') FLAGS = parser.parse_args() def main(): # create prediction service client stub channel = implementations.insecure_channel(FLAGS.host, int(FLAGS.port)) stub = prediction_service_pb2.beta_create_PredictionService_stub(channel) # create request request = predict_pb2.PredictRequest() request.model_spec.name = 'resnet' request.model_spec.signature_name = 'serving_default' # read image into numpy array img = cv2.imread(FLAGS.image).astype(np.float32) # convert to tensor proto and make request # shape is in NHWC (num_samples x height x width x channels) format tensor = tf.contrib.util.make_tensor_proto(img, shape=[1]+list(img.shape)) request.inputs['input'].CopyFrom(tensor) resp = stub.Predict(request, 30.0) print('total time: {}s'.format(time.time() - tt)) if __name__ == '__main__': main()</span></span></code> </pre> <br>  Habiendo recibido una entrada JPEG, un cliente que trabaja producir√° el siguiente resultado: <br><br><pre> <code class="bash hljs">python tf_serving_client.py --image=images/pupper.jpg total time: 2.56152906418s</code> </pre> <br>  El tensor resultante contiene un pron√≥stico en forma de valor entero y probabilidad de signos. <br><br><pre> <code class="python hljs">outputs { key: <span class="hljs-string"><span class="hljs-string">"classes"</span></span> value { dtype: DT_INT64 tensor_shape { dim { size: <span class="hljs-number"><span class="hljs-number">1</span></span> } } int64_val: <span class="hljs-number"><span class="hljs-number">238</span></span> } } outputs { key: <span class="hljs-string"><span class="hljs-string">"probabilities"</span></span> ...</code> </pre> <br>  Para una sola solicitud, dicho retraso no es aceptable.  Pero nada sorprendente: el binario Tensorflow Serving est√° dise√±ado por defecto para la gama m√°s amplia de equipos para la mayor√≠a de los casos de uso.  Probablemente haya notado las siguientes l√≠neas en los registros del contenedor est√°ndar de Tensorflow Serving: <br><br><pre> <code class="plaintext hljs">I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</code> </pre> <br>  Esto indica un binario de TensorFlow Serving que se ejecuta en una plataforma de CPU para la que no ha sido optimizado. <br><br><h3>  Construye un binario optimizado </h3><br>  De acuerdo con la <a href="">documentaci√≥n de</a> Tensorflow, se recomienda compilar Tensorflow desde la fuente con todas las optimizaciones disponibles para la CPU en el host donde funcionar√° el binario.  Al ensamblar, los indicadores especiales permiten la activaci√≥n de conjuntos de instrucciones de CPU para una plataforma espec√≠fica: <br><br><div class="scrollable-table"><table><tbody><tr><th>  Conjunto de instrucciones </th><th>  Banderas </th></tr><tr><td>  AVX </td><td>  --copt = -mavx </td></tr><tr><td>  AVX2 </td><td>  --copt = -mavx2 </td></tr><tr><td>  Fma </td><td>  --copt = -mfma </td></tr><tr><td>  SSE 4.1 </td><td>  --copt = -msse4.1 </td></tr><tr><td>  SSE 4.2 </td><td>  --copt = -msse4.2 </td></tr><tr><td>  Todo soportado por procesador </td><td>  --copt = -march = nativo </td></tr></tbody></table></div><br>  Clonar un Tensorflow Sirviendo de una versi√≥n espec√≠fica.  En nuestro caso, esto es 1.13 (el √∫ltimo en el momento de la publicaci√≥n de este art√≠culo): <br><br><pre> <code class="bash hljs">USER=<span class="hljs-variable"><span class="hljs-variable">$1</span></span> TAG=<span class="hljs-variable"><span class="hljs-variable">$2</span></span> TF_SERVING_VERSION_GIT_BRANCH=<span class="hljs-string"><span class="hljs-string">"r1.13"</span></span> git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> --branch=<span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$TF_SERVING_VERSION_GIT_BRANCH</span></span></span><span class="hljs-string">"</span></span> https://github.com/tensorflow/serving</code> </pre> <br>  La imagen de desarrollo de Tensorflow Serving utiliza la herramienta de Basilea para construir.  Lo configuramos para conjuntos espec√≠ficos de instrucciones de CPU: <br><br><pre> <code class="bash hljs">TF_SERVING_BUILD_OPTIONS=<span class="hljs-string"><span class="hljs-string">"--copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2"</span></span></code> </pre> <br>  Si no hay suficiente memoria, limite el consumo de memoria durante el proceso de compilaci√≥n con el indicador <code>--local_resources=2048,.5,1.0</code> .  Para obtener informaci√≥n sobre indicadores, consulte la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ayuda de Tensorflow Serving y Docker</a> , as√≠ como la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentaci√≥n de Bazel</a> . <br><br>  Cree una imagen de trabajo basada en la existente: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash USER=$1 TAG=$2 TF_SERVING_VERSION_GIT_BRANCH="r1.13" git clone --branch="${TF_SERVING_VERSION_GIT_BRANCH}" https://github.com/tensorflow/serving TF_SERVING_BUILD_OPTIONS="--copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2" cd serving &amp;&amp; \ docker build --pull -t $USER/tensorflow-serving-devel:$TAG \ --build-arg TF_SERVING_VERSION_GIT_BRANCH="${TF_SERVING_VERSION_GIT_BRANCH}" \ --build-arg TF_SERVING_BUILD_OPTIONS="${TF_SERVING_BUILD_OPTIONS}" \ -f tensorflow_serving/tools/docker/Dockerfile.devel . cd serving &amp;&amp; \ docker build -t $USER/tensorflow-serving:$TAG \ --build-arg TF_SERVING_BUILD_IMAGE=$USER/tensorflow-serving-devel:$TAG \ -f tensorflow_serving/tools/docker/Dockerfile .</span></span></code> </pre> <br>  ModelServer se configura utilizando <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">indicadores TensorFlow</a> para admitir concurrencia.  Las siguientes opciones configuran dos grupos de subprocesos para la operaci√≥n en paralelo: <br><br><pre> <code class="plaintext hljs">intra_op_parallelism_threads</code> </pre> <br><ul><li>  controla el n√∫mero m√°ximo de subprocesos para la ejecuci√≥n paralela de una operaci√≥n; <br></li><li>  se usa para paralelizar operaciones que tienen suboperaciones que son de naturaleza independiente. </li></ul><br><pre> <code class="plaintext hljs">inter_op_parallelism_threads</code> </pre> <br><ul><li>  controla el n√∫mero m√°ximo de subprocesos para la ejecuci√≥n paralela de operaciones independientes; <br></li><li>  Las operaciones de Tensorflow Graph, que son independientes entre s√≠ y, por lo tanto, se pueden realizar en diferentes subprocesos. </li></ul><br>  Por defecto, ambos par√°metros est√°n establecidos en <code>0</code> .  Esto significa que el sistema mismo selecciona el n√∫mero apropiado, lo que a menudo significa un subproceso por n√∫cleo.  Sin embargo, el par√°metro se puede cambiar manualmente para la concurrencia de m√∫ltiples n√∫cleos. <br><br>  Luego, ejecute el contenedor Serving de la misma manera que el anterior, esta vez con una imagen Docker compilada de las fuentes y con indicadores de optimizaci√≥n de Tensorflow para un procesador espec√≠fico: <br><br><pre> <code class="bash hljs">docker run -d -p 9000:8500 \ -v $(<span class="hljs-built_in"><span class="hljs-built_in">pwd</span></span>)/models:/models/resnet -e MODEL_NAME=resnet \ -t <span class="hljs-variable"><span class="hljs-variable">$USER</span></span>/tensorflow-serving:<span class="hljs-variable"><span class="hljs-variable">$TAG</span></span> \ --tensorflow_intra_op_parallelism=4 \ --tensorflow_inter_op_parallelism=4</code> </pre> <br>  Los registros de contenedores ya no deber√≠an mostrar advertencias sobre una CPU indefinida.  Sin cambiar el c√≥digo en la misma solicitud de pron√≥stico, el retraso se reduce en aproximadamente un 35,8%: <br><br><pre> <code class="bash hljs">python tf_serving_client.py --image=images/pupper.jpg total time: 1.64234706879s</code> </pre> <br><h3>  Aumente la velocidad en el pron√≥stico del cliente. </h3><br>  ¬øTodav√≠a es posible acelerar?  Hemos optimizado el lado del servidor para nuestra CPU, pero un retraso de m√°s de 1 segundo todav√≠a parece demasiado grande. <br><br>  Dio la casualidad de que cargar las bibliotecas <code>tensorflow_serving</code> y <code>tensorflow</code> hace una contribuci√≥n significativa al retraso.  Cada llamada innecesaria a <code>tf.contrib.util.make_tensor_proto</code> tambi√©n agrega una fracci√≥n de segundo. <br><br>  Puede preguntar: "¬øNo necesitamos paquetes TensorFlow Python para hacer solicitudes de predicci√≥n al servidor Tensorflow?"  De hecho, no hay una <i>necesidad</i> real de paquetes <code>tensorflow_serving</code> y <code>tensorflow</code> . <br><br>  Como se se√±al√≥ anteriormente, las API de predicci√≥n de Tensorflow se definen como proto-buffers.  Por lo tanto, se pueden reemplazar dos dependencias externas con los <code>tensorflow</code> y <code>tensorflow_serving</code> correspondientes, y luego no es necesario extraer toda la biblioteca (pesada) de Tensorflow en el cliente. <br><br>  Primero, elimine las <code>tensorflow_serving</code> <code>tensorflow</code> y <code>tensorflow_serving</code> y agregue el paquete <code>grpcio-tools</code> . <br><br><pre> <code class="bash hljs">pip uninstall tensorflow tensorflow-serving-api &amp;&amp; \ pip install grpcio-tools==1.0.0</code> </pre> <br>  Clone los <code>tensorflow/tensorflow</code> y <code>tensorflow/serving</code> y copie los siguientes archivos protobuf en el proyecto del cliente: <br><br><pre> <code class="plaintext hljs">tensorflow/serving/ tensorflow_serving/apis/model.proto tensorflow_serving/apis/predict.proto tensorflow_serving/apis/prediction_service.proto tensorflow/tensorflow/ tensorflow/core/framework/resource_handle.proto tensorflow/core/framework/tensor_shape.proto tensorflow/core/framework/tensor.proto tensorflow/core/framework/types.proto</code> </pre> <br>  Copie estos archivos protobuf en el directorio <code>protos/</code> con las rutas originales conservadas: <br><br><pre> <code class="plaintext hljs">protos/ tensorflow_serving/ apis/ *.proto tensorflow/ core/ framework/ *.proto</code> </pre> <br>  Por simplicidad, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">prediction_service.proto</a> puede simplificarse para implementar solo Predict RPC para no descargar las dependencias anidadas de otros RPC especificados en el servicio.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Aqu√≠ hay</a> un ejemplo de <code>prediction_service.</code> simplificado. <br><br>  Cree implementaciones de Python gRPC utilizando <code>grpcio.tools.protoc</code> : <br><br><pre> <code class="plaintext hljs">PROTOC_OUT=protos/ PROTOS=$(find . | grep "\.proto$") for p in $PROTOS; do python -m grpc.tools.protoc -I . --python_out=$PROTOC_OUT --grpc_python_out=$PROTOC_OUT $p done</code> </pre> <br>  Ahora se puede eliminar todo el m√≥dulo <code>tensorflow_serving</code> : <br><br><pre> <code class="plaintext hljs">from tensorflow_serving.apis import predict_pb2 from tensorflow_serving.apis import prediction_service_pb2</code> </pre> <br>  ... y reemplace con los protobuffers generados desde <code>protos/tensorflow_serving/apis</code> : <br><br><pre> <code class="plaintext hljs">from protos.tensorflow_serving.apis import predict_pb2 from protos.tensorflow_serving.apis import prediction_service_pb2</code> </pre> <br>  La biblioteca Tensorflow se importa para usar la funci√≥n auxiliar <code>make_tensor_proto</code> , que es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">necesaria para envolver</a> un objeto python / numpy como un objeto TensorProto. <br><br>  Por lo tanto, podemos reemplazar la siguiente dependencia y fragmento de c√≥digo: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf ... tensor = tf.contrib.util.make_tensor_proto(features) request.inputs[<span class="hljs-string"><span class="hljs-string">'inputs'</span></span>].CopyFrom(tensor)</code> </pre> <br>  importar protobuffers y construir un objeto TensorProto: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> protos.tensorflow.core.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensor_pb2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> protos.tensorflow.core.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensor_shape_pb2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> protos.tensorflow.core.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> types_pb2 ... <span class="hljs-comment"><span class="hljs-comment"># ensure NHWC shape and build tensor proto tensor_shape = [1]+list(img.shape) dims = [tensor_shape_pb2.TensorShapeProto.Dim(size=dim) for dim in tensor_shape] tensor_shape = tensor_shape_pb2.TensorShapeProto(dim=dims) tensor = tensor_pb2.TensorProto( dtype=types_pb2.DT_FLOAT, tensor_shape=tensor_shape, float_val=list(img.reshape(-1))) request.inputs['inputs'].CopyFrom(tensor)</span></span></code> </pre> <br>  El script completo de Python est√° <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> .  Ejecute un cliente de inicio actualizado que realice una solicitud de predicci√≥n para el servicio optimizado de Tensorflow: <br><br><pre> <code class="bash hljs">python tf_inception_grpc_client.py --image=images/pupper.jpg total time: 0.58314920859s</code> </pre> <br>  El siguiente diagrama muestra el tiempo de ejecuci√≥n del pron√≥stico en la versi√≥n optimizada de Tensorflow Serving en comparaci√≥n con el est√°ndar, m√°s de 10 ejecuciones: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/48d/990/b83/48d990b83dc54762fec809a580c450c8.png"><br><br>  El retraso promedio disminuy√≥ en aproximadamente 3.38 veces. <br><br><h1>  Optimizaci√≥n de ancho de banda </h1><br>  Tensorflow Serving se puede configurar para manejar grandes cantidades de datos.  La optimizaci√≥n del ancho de banda generalmente se realiza para el procesamiento por lotes "aut√≥nomo", donde los l√≠mites de latencia ajustados no son un requisito estricto. <br><br><h3>  Procesamiento por lotes del lado del servidor </h3><br>  Como se indica en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentaci√≥n</a> , el procesamiento por lotes del lado del servidor es compatible de forma nativa en Tensorflow Serving. <br><br>  Las compensaciones entre latencia y rendimiento se determinan mediante par√°metros de procesamiento por lotes.  Le permiten alcanzar el rendimiento m√°ximo que los aceleradores de hardware son capaces de hacer. <br><br>  Para habilitar el empaquetado, configure los <code>--batching_parameters_file</code> <code>--enable_batching</code> y <code>--batching_parameters_file</code> .  Los par√°metros se establecen de acuerdo con <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SessionBundleConfig</a> .  Para sistemas en la CPU, establezca <code>num_batch_threads</code> en el n√∫mero de n√∫cleos disponibles.  Para la GPU, vea los par√°metros apropiados <a href="">aqu√≠</a> . <br><br>  Despu√©s de completar todo el paquete en el lado del servidor, las solicitudes de emisi√≥n se combinan en una solicitud grande (tensor) y se env√≠an a la sesi√≥n de Tensorflow con una solicitud combinada.  En esta situaci√≥n, el paralelismo CPU / GPU est√° realmente involucrado. <br><br>  Algunos usos comunes para el procesamiento por lotes de Tensorflow: <br><br><ul><li>  Uso de solicitudes de cliente as√≠ncronas para llenar paquetes del lado del servidor <br></li><li>  Procesamiento por lotes m√°s r√°pido mediante la transferencia de los componentes del gr√°fico del modelo a la CPU / GPU <br></li><li>  Sirviendo solicitudes de m√∫ltiples modelos desde un √∫nico servidor <br></li><li>  El procesamiento por lotes es muy recomendable para el procesamiento "fuera de l√≠nea" de una gran cantidad de solicitudes </li></ul><br><h3>  Procesamiento por lotes del lado del cliente </h3><br>  El procesamiento por lotes del lado del cliente agrupa varias solicitudes entrantes en una. <br><br>  Dado que el modelo ResNet est√° esperando entrada en formato NHWC (la primera dimensi√≥n es la cantidad de entradas), podemos combinar varias im√°genes de entrada en una solicitud RPC: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">... </span></span>batch = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> jpeg <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> os.listdir(FLAGS.images_path): path = os.path.join(FLAGS.images_path, jpeg) img = cv2.imread(path).astype(np.float32) batch.append(img) ... batch_np = np.array(batch).astype(np.float32) dims = [tensor_shape_pb2.TensorShapeProto.Dim(size=dim) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> dim <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> batch_np.shape] t_shape = tensor_shape_pb2.TensorShapeProto(dim=dims) tensor = tensor_pb2.TensorProto( dtype=types_pb2.DT_FLOAT, tensor_shape=t_shape, float_val=list(batched_np.reshape(<span class="hljs-number"><span class="hljs-number">-1</span></span>))) request.inputs[<span class="hljs-string"><span class="hljs-string">'inputs'</span></span>].CopyFrom(tensor)</code> </pre> <br>  Para un paquete de N im√°genes, el tensor de salida en la respuesta contendr√° los resultados de predicci√≥n para el mismo n√∫mero de entradas.  En nuestro caso, N = 2: <br><br><pre> <code class="python hljs">outputs { key: <span class="hljs-string"><span class="hljs-string">"classes"</span></span> value { dtype: DT_INT64 tensor_shape { dim { size: <span class="hljs-number"><span class="hljs-number">2</span></span> } } int64_val: <span class="hljs-number"><span class="hljs-number">238</span></span> int64_val: <span class="hljs-number"><span class="hljs-number">121</span></span> } } ...</code> </pre> <br><h1>  Aceleraci√≥n de hardware </h1><br>  Algunas palabras sobre las GPU. <br><br>  El proceso de aprendizaje utiliza naturalmente la paralelizaci√≥n en la GPU, ya que la construcci√≥n de redes neuronales profundas requiere c√°lculos masivos para lograr la soluci√≥n √≥ptima. <br><br>  Pero para generar resultados, la paralelizaci√≥n no es tan obvia.  A menudo, puede acelerar la salida de una red neuronal a una GPU, pero debe seleccionar y probar cuidadosamente el equipo y realizar un an√°lisis t√©cnico y econ√≥mico en profundidad.  La paralelizaci√≥n de hardware es m√°s valiosa para el procesamiento por lotes de conclusiones "aut√≥nomas" (vol√∫menes masivos). <br><br>  Antes de pasar a una GPU, considere los requisitos comerciales con un an√°lisis cuidadoso de los costos (monetario, operativo, t√©cnico) para obtener el mayor beneficio (latencia reducida, alto rendimiento). </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/445928/">https://habr.com/ru/post/445928/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../445918/index.html">20 a√±os de RollerCoaster Tycoon: una entrevista con el creador del juego</a></li>
<li><a href="../445920/index.html">Live: c√≥mo frenar el desarrollo de iOS en equipos grandes</a></li>
<li><a href="../445922/index.html">¬øPor qu√© ver transmisiones en l√≠nea si puedes leer a Habr?</a></li>
<li><a href="../445924/index.html">TESOROS: cuando los relojes inteligentes se vuelven raros</a></li>
<li><a href="../445926/index.html">El Programa secreto de ovnis de EE. UU. Tambi√©n ha estado investigando agujeros de gusano y dimensiones adicionales.</a></li>
<li><a href="../445932/index.html">Seguridad de la aplicaci√≥n del cliente: consejos pr√°cticos para un desarrollador front-end</a></li>
<li><a href="../445936/index.html">Desarrollo de electr√≥nica. Sobre microcontroladores en los dedos</a></li>
<li><a href="../445940/index.html">AMA con Habr, v 7.0. Lim√≥n, Donuts y Noticias</a></li>
<li><a href="../445946/index.html">MWC: instrucciones de uso</a></li>
<li><a href="../445948/index.html">Herencia en C ++: principiante, intermedio, avanzado</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>