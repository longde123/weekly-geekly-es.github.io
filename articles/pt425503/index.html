<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíÜüèª üïö üõÖ Cassandra Sink para Spark Structured Streaming üßïüèª üíù ‚ù£Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="H√° alguns meses, comecei a estudar o Spark e, em algum momento, tive o problema de salvar os c√°lculos do Structured Streaming no banco de dados do Cas...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cassandra Sink para Spark Structured Streaming</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/425503/">  H√° alguns meses, comecei a estudar o Spark e, em algum momento, tive o problema de salvar os c√°lculos do Structured Streaming no banco de dados do Cassandra. <br><br>  Neste post, dou um exemplo simples de cria√ß√£o e uso de Cassandra Sink para o Spark Structured Streaming.  Espero que a publica√ß√£o seja √∫til para aqueles que recentemente come√ßaram a trabalhar com o Spark Structured Streaming e est√£o se perguntando como fazer upload dos resultados do c√°lculo no banco de dados. <br><br>  A id√©ia do aplicativo √© muito simples - receber e analisar mensagens do Kafka, realizar transforma√ß√µes simples em um par e salvar os resultados no cassandra. <br><a name="habracut"></a><br><h3>  Profissionais de streaming estruturado </h3><br>  Voc√™ pode ler mais sobre o Streaming estruturado na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o</a> .  Em resumo, o Structured Streaming √© um mecanismo de processamento de informa√ß√µes de streaming bem escal√°vel, baseado no mecanismo Spark SQL.  Ele permite que voc√™ use um conjunto de dados / DataFrame para agregar dados, calcular fun√ß√µes de janela, conex√µes etc. Ou seja, o Streaming estruturado permite que voc√™ use o bom e velho SQL para trabalhar com fluxos de dados. <br><br><h3>  Qual √© o problema? </h3><br>  A vers√£o est√°vel do Spark Structured Streaming foi lan√ßada em 2017.  Ou seja, essa √© uma API relativamente nova que implementa a funcionalidade b√°sica, mas algumas coisas ter√£o que ser feitas por n√≥s mesmos.  Por exemplo, o Structured Streaming possui fun√ß√µes padr√£o para gravar a sa√≠da em um arquivo, bloco, console ou mem√≥ria, mas para salvar os dados no banco de dados, voc√™ deve usar o receptor <i>foreach</i> dispon√≠vel no Structured Streaming e implementar a interface <i>ForeachWriter</i> .  <b>A partir do Spark 2.3.1, essa funcionalidade pode ser implementada apenas no Scala e Java</b> . <br><br>  Suponho que o leitor j√° saiba como o Streaming Estruturado funciona em termos gerais, saiba como implementar as transforma√ß√µes necess√°rias e agora esteja pronto para carregar os resultados no banco de dados.  Se algumas das etapas acima n√£o forem claras, a documenta√ß√£o oficial pode servir como um bom ponto de partida para aprender o Streaming Estruturado.  Neste artigo, gostaria de focar na √∫ltima etapa quando voc√™ precisar salvar os resultados em um banco de dados. <br><br>  Abaixo, descreverei um exemplo de implementa√ß√£o do coletor Cassandra para o Structured Streaming e explicarei como execut√°-lo em um cluster.  O c√≥digo completo est√° dispon√≠vel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> . <br><br>  Quando eu encontrei o problema acima, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">este projeto</a> acabou sendo muito √∫til.  No entanto, pode parecer um pouco complicado se o leitor acabou de come√ßar a trabalhar com o Structured Streaming e est√° procurando um exemplo simples de como fazer upload de dados para o cassandra.  Al√©m disso, o projeto foi gravado para funcionar no modo local e requer algumas altera√ß√µes para executar no cluster. <br><br>  Tamb√©m quero dar exemplos de como salvar dados no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MongoDB</a> e em qualquer outro banco de dados usando o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">JDBC</a> . <br><br><h3>  Solu√ß√£o simples </h3><br>  Para fazer upload de dados para um sistema externo, voc√™ deve usar o receptor <i>foreach</i> .  Leia mais sobre isso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> .  Em resumo, a interface <i>ForeachWriter</i> deve ser implementada.  Ou seja, √© necess√°rio determinar como abrir a conex√£o, como processar cada parte dos dados e como fechar a conex√£o no final do processamento.  O c√≥digo fonte √© o seguinte: <br><br><pre><code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CassandraSinkForeach</span></span></span><span class="hljs-class">(</span><span class="hljs-params"></span><span class="hljs-class"><span class="hljs-params"></span>) </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ForeachWriter</span></span></span><span class="hljs-class">[org.apache.spark.sql.</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">Row</span></span></span><span class="hljs-class">] </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// This class implements the interface ForeachWriter, which has methods that get called // whenever there is a sequence of rows generated as output val cassandraDriver = new CassandraDriver(); def open(partitionId: Long, version: Long): Boolean = { // open connection println(s"Open connection") true } def process(record: org.apache.spark.sql.Row) = { println(s"Process new $record") cassandraDriver.connector.withSessionDo(session =&gt; session.execute(s""" insert into ${cassandraDriver.namespace}.${cassandraDriver.foreachTableSink} (fx_marker, timestamp_ms, timestamp_dt) values('${record(0)}', '${record(1)}', '${record(2)}')""") ) } def close(errorOrNull: Throwable): Unit = { // close the connection println(s"Close connection") } }</span></span></code> </pre> <br>  Descreverei a defini√ß√£o de <i>CassandraDriver</i> e a estrutura da tabela de sa√≠da posteriormente, mas, por enquanto, vamos dar uma olhada em como o c√≥digo acima funciona.  Para conectar-se ao Kasandra a partir do Spark, crio um objeto <i>CassandraDriver</i> que fornece acesso ao <i>CassandraConnector</i> , um conector desenvolvido pela <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">DataStax</a> .  O CassandraConnector √© respons√°vel por abrir e fechar a conex√£o com o banco de dados, portanto, apenas mostro mensagens de depura√ß√£o nos m√©todos de <i>abertura</i> e <i>fechamento</i> da classe <i>CassandraSinkForeach</i> . <br><br>  O c√≥digo acima √© chamado do aplicativo principal da seguinte maneira: <br><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> sink = parsed .writeStream .queryName(<span class="hljs-string"><span class="hljs-string">"KafkaToCassandraForeach"</span></span>) .outputMode(<span class="hljs-string"><span class="hljs-string">"update"</span></span>) .foreach(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">CassandraSinkForeach</span></span>()) .start()</code> </pre><br>  <i>CassandraSinkForeach</i> √© criado para cada linha de dados, portanto, cada n√≥ de trabalho insere sua parte das linhas no banco de dados.  Ou seja, cada n√≥ de trabalho executa <i>val cassandraDriver = new CassandraDriver ();</i>  √â assim que o CassandraDriver se parece: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CassandraDriver</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// This object will be used in CassandraSinkForeach to connect to Cassandra DB from an executor. // It extends SparkSessionBuilder so to use the same SparkSession on each node. val spark = buildSparkSession import spark.implicits._ val connector = CassandraConnector(spark.sparkContext.getConf) // Define Cassandra's table which will be used as a sink /* For this app I used the following table: CREATE TABLE fx.spark_struct_stream_sink ( fx_marker text, timestamp_ms timestamp, timestamp_dt date, primary key (fx_marker)); */ val namespace = "fx" val foreachTableSink = "spark_struct_stream_sink" }</span></span></code> </pre><br>  Vamos dar uma olhada no objeto <i>fa√≠sca</i> .  O c√≥digo para <i>SparkSessionBuilder √©</i> o seguinte: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Serializable</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// Build a spark session. Class is made serializable so to get access to SparkSession in a driver and executors. // Note here the usage of @transient lazy val def buildSparkSession: SparkSession = { @transient lazy val conf: SparkConf = new SparkConf() .setAppName("Structured Streaming from Kafka to Cassandra") .set("spark.cassandra.connection.host", "ec2-52-23-103-178.compute-1.amazonaws.com") .set("spark.sql.streaming.checkpointLocation", "checkpoint") @transient lazy val spark = SparkSession .builder() .config(conf) .getOrCreate() spark } }</span></span></code> </pre><br>  Em cada n√≥ de trabalho, o <i>SparkSessionBuilder</i> fornece acesso ao <i>SparkSession</i> que foi criado no driver.  Para tornar esse acesso poss√≠vel, √© necess√°rio serializar o <i>SparkSessionBuilder</i> e usar um valor <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">transit√≥rio</a> pregui√ßoso</i> , que permite ao sistema de serializa√ß√£o ignorar objetos <i>conf</i> e <i>spark</i> quando o programa √© inicializado e at√© que os objetos sejam acessados.  Portanto, quando o programa <i>buildSparkSession</i> √© <i>iniciado, ele √©</i> serializado e enviado para cada n√≥ de trabalho, mas objetos <i>conf</i> e <i>spark</i> s√£o permitidos apenas quando o n√≥ de trabalho os acessa. <br><br>  Agora vamos ver o c√≥digo principal do aplicativo: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">object</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">KafkaToCassandra</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// Main body of the app. It also extends SparkSessionBuilder. def main(args: Array[String]) { val spark = buildSparkSession import spark.implicits._ // Define location of Kafka brokers: val broker = "ec2-18-209-75-68.compute-1.amazonaws.com:9092,ec2-18-205-142-57.compute-1.amazonaws.com:9092,ec2-50-17-32-144.compute-1.amazonaws.com:9092" /*Here is an example massage which I get from a Kafka stream. It contains multiple jsons separated by \n {"timestamp_ms": "1530305100936", "fx_marker": "EUR/GBP"} {"timestamp_ms": "1530305100815", "fx_marker": "USD/CHF"} {"timestamp_ms": "1530305100969", "fx_marker": "EUR/CHF"} {"timestamp_ms": "1530305100011", "fx_marker": "USD/CAD"} */ // Read incoming stream val dfraw = spark .readStream .format("kafka") .option("kafka.bootstrap.servers", broker) .option("subscribe", "currency_exchange") .load() val schema = StructType( Seq( StructField("fx_marker", StringType, false), StructField("timestamp_ms", StringType, false) ) ) val df = dfraw .selectExpr("CAST(value AS STRING)").as[String] .flatMap(_.split("\n")) val jsons = df.select(from_json($"value", schema) as "data").select("data.*") // Process data. Create a new date column val parsed = jsons .withColumn("timestamp_dt", to_date(from_unixtime($"timestamp_ms"/1000.0, "yyyy-MM-dd HH:mm:ss.SSS"))) .filter("fx_marker != ''") // Output results into a database val sink = parsed .writeStream .queryName("KafkaToCassandraForeach") .outputMode("update") .foreach(new CassandraSinkForeach()) .start() sink.awaitTermination() } }</span></span></code> </pre><br>  Quando o aplicativo √© enviado para execu√ß√£o, o <i>buildSparkSession √©</i> serializado e enviado aos n√≥s de trabalho, no entanto, os objetos <i>conf</i> e <i>spark</i> permanecem sem solu√ß√£o.  Em seguida, o driver cria um objeto spark dentro do <i>KafkaToCassandra</i> e distribui o trabalho entre os n√≥s em funcionamento.  Cada n√≥ de trabalho l√™ dados do Kafka, faz transforma√ß√µes simples na parte recebida dos registros e, quando o n√≥ de trabalho est√° pronto para gravar os resultados no banco de dados, permite objetos <i>conf</i> e <i>spark</i> , obtendo acesso ao <i>SparkSession</i> criado no driver. <br><br><h3>  Como criar e executar o aplicativo? </h3><br>  Quando mudei do PySpark para o Scala, demorei um pouco para descobrir como criar o aplicativo.  Portanto, inclu√≠ o Maven <i>pom.xml</i> no meu projeto.  O leitor pode criar o aplicativo usando o Maven executando o comando <i>mvn package</i> .  Depois que o aplicativo pode ser enviado para execu√ß√£o usando <br><br><pre> <code class="bash hljs">./bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.1,datastax:spark-cassandra-connector:2.3.0-s_2.11 --class com.insight.app.CassandraSink.KafkaToCassandra --master spark://ec2-18-232-26-53.compute-1.amazonaws.com:7077 target/cassandra-sink-0.0.1-SNAPSHOT.jar</code> </pre><br>  Para criar e executar o aplicativo, √© necess√°rio substituir os nomes das minhas m√°quinas da AWS por suas pr√≥prias (ou seja, substituir tudo que se parece com ec2-xx-xxx-xx-xx.comxxpute-1.amazonaws.com). <br><br>  O Spark e o Structured Streaming, em particular, s√£o um novo t√≥pico para mim, por isso serei muito grato aos leitores por coment√°rios, discuss√µes e corre√ß√µes. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt425503/">https://habr.com/ru/post/pt425503/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt425489/index.html">Automa√ß√£o: amea√ßa exagerada de rob√¥s</a></li>
<li><a href="../pt425493/index.html">Configurando o MikroTik hAP mini para IPTV Beeline</a></li>
<li><a href="../pt425497/index.html">Tutu PHP Meetup # 2: Transmiss√£o de eventos ao vivo</a></li>
<li><a href="../pt425499/index.html">HyperX Impact DDR4 - SO-DIMM que poderia! Ou por que, em um laptop, 64 GB de mem√≥ria com uma frequ√™ncia de 3200 MHz?</a></li>
<li><a href="../pt425501/index.html">Testes A / B no Android de A a Z</a></li>
<li><a href="../pt425505/index.html">An√°lise do processo de inicializa√ß√£o do kernel Linux</a></li>
<li><a href="../pt425507/index.html">Wikipedia Parsim para tarefas de PNL em 4 equipes</a></li>
<li><a href="../pt425511/index.html">Recursos n√£o √≥bvios do aplicativo Rotativa para gerar PDF no aplicativo ASP.NET MVC</a></li>
<li><a href="../pt425515/index.html">Apple bloqueia reparo independente de novos modelos de MacBook</a></li>
<li><a href="../pt425517/index.html">Como o Yandex criou uma previs√£o global de precipita√ß√£o usando radares e sat√©lites</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>