<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßï üßöüèª üî¨ Introdu√ß√£o ao Airflow para gerenciar Spark Jobs no ivi: esperan√ßas e muletas üî∑ üà≥ üåÅ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A tarefa de implantar modelos de aprendizado de m√°quina na produ√ß√£o √© sempre dolorosa, porque √© muito desconfort√°vel sair de um notebook jupyter acolh...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Introdu√ß√£o ao Airflow para gerenciar Spark Jobs no ivi: esperan√ßas e muletas</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ivi/blog/456630/">  A tarefa de implantar modelos de aprendizado de m√°quina na produ√ß√£o √© sempre dolorosa, porque √© muito desconfort√°vel sair de um notebook jupyter acolhedor para o mundo do monitoramento e da toler√¢ncia a falhas. <br><br>  J√° escrevemos sobre a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">primeira itera√ß√£o de refatora√ß√£o do</a> sistema de recomenda√ß√£o do cinema online ivi.  No ano passado, quase n√£o finalizamos a arquitetura do aplicativo (do global - apenas passando do obsoleto python 2.7 e python 3.4 para o "fresh" python 3.6), mas adicionamos alguns novos modelos de ML e imediatamente enfrentamos o problema de implantar novos algoritmos na produ√ß√£o.  No artigo, contarei sobre nossa experi√™ncia na implementa√ß√£o de uma ferramenta de gerenciamento de fluxo de tarefas como o Apache Airflow: por que a equipe teve essa necessidade, o que n√£o se adequava √† solu√ß√£o existente, quais muletas tiveram que ser cortadas ao longo do caminho e o que veio dela. <br><br>  ‚Üí A vers√£o em v√≠deo do relat√≥rio pode ser assistida no YouTube (a partir das 03:00:00) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qc/bw/tt/qcbwttjzivyzuedihk88u6nwn5o.png"></div><br><a name="habracut"></a><br><br><h2>  <font color="#fd004c">Hydra Team</font> </h2><br>  Vou falar um pouco sobre o projeto: ivi √© v√°rias dezenas de milhares de unidades de conte√∫do, temos um dos maiores diret√≥rios jur√≠dicos do RuNet.  A p√°gina principal da vers√£o web ivi √© um recorte personalizado do cat√°logo, projetado para fornecer ao usu√°rio o conte√∫do mais rico e relevante com base em seus coment√°rios (visualiza√ß√µes, classifica√ß√µes e assim por diante). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/vj/-g/nh/vj-gnheaviq-z7tibuzawo-qdqs.png"></div><br>  A parte on-line do sistema de recomenda√ß√£o √© um aplicativo de back-end do Flask com uma carga de at√© 600 RPS.  Off-line, o modelo √© treinado em mais de 250 milh√µes de visualiza√ß√µes de conte√∫do por m√™s.  Os pipelines de prepara√ß√£o de dados para treinamento s√£o implementados no Spark, que √© executado no topo do reposit√≥rio Hive. <br><br>  A equipe agora tem 7 desenvolvedores que est√£o envolvidos na cria√ß√£o de modelos e na sua implanta√ß√£o em produ√ß√£o - essa √© uma equipe bastante grande que requer ferramentas convenientes para gerenciar fluxos de tarefas. <br><br><h2>  <font color="#fd004c">Arquitetura offline</font> </h2><br>  Abaixo, voc√™ v√™ o diagrama da infraestrutura de fluxo de dados para o sistema de recomenda√ß√£o. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xs/10/nv/xs10nvc8r3mz9bd8osjrn6sqn3u.png"></div><br>  Dois armazenamentos de dados s√£o representados aqui - Hive para feedback do usu√°rio (visualiza√ß√µes, classifica√ß√µes) e Postgres para v√°rias informa√ß√µes comerciais (tipos de monetiza√ß√£o de conte√∫do etc.), enquanto a transfer√™ncia do Postgres para o Hive √© ajustada.  Um pacote de aplicativos Spark suga dados do Hive: e treina nossos modelos nesses dados (ALS para recomenda√ß√µes pessoais, v√°rios modelos colaborativos de similaridade de conte√∫do). <br><br>  Os aplicativos Spark s√£o tradicionalmente gerenciados a partir de uma m√°quina virtual dedicada, que chamamos de hydra-updater usando um monte de scripts cron + shell.  Este pacote foi criado no departamento de opera√ß√µes da ivi em tempos imemoriais e funcionou muito bem.  O script de shell era um ponto de entrada √∫nico para o lan√ßamento de aplicativos spark - ou seja, cada novo modelo come√ßava a girar no produto somente depois que os administradores terminavam esse script. <br><br>  Alguns dos artefatos do treinamento do modelo s√£o armazenados no HDFS para armazenamento eterno (e aguardando algu√©m fazer o download deles e transferi-los para o servidor em que a parte on-line est√° girando), e alguns s√£o gravados diretamente do driver Spark para o armazenamento r√°pido Redis, que geralmente usamos mem√≥ria para v√°rias dezenas de processos python da parte online. <br><br>  Essa arquitetura acumulou uma s√©rie de desvantagens ao longo do tempo: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ri/x2/b0/rix2b0qjxi1qa0igp-bpl23bn04.png"></div><br>  O diagrama mostra que os fluxos de dados t√™m uma estrutura bastante complicada - sem uma ferramenta simples e clara para gerenciar esse bem, o desenvolvimento e a opera√ß√£o se transformar√£o em horror, decad√™ncia e sofrimento. <br><br>  Al√©m de gerenciar aplicativos spark, o script admin faz muitas coisas √∫teis: reiniciar os servi√ßos em batalha, um despejo de Redis e outras coisas do sistema.  Obviamente, durante um longo per√≠odo de opera√ß√£o, o script cresceu demais com muitas fun√ß√µes, pois cada novo modelo nosso gerou algumas dezenas de linhas.  O script come√ßou a parecer sobrecarregado demais em termos de funcionalidade; portanto, como uma equipe do sistema de recomenda√ß√£o, quer√≠amos remover em algum lugar parte da funcionalidade que diz respeito ao lan√ßamento e gerenciamento de aplicativos Spark.  Para esses prop√≥sitos, decidimos usar o Airflow. <br><br><h2>  <font color="#fd004c">Muletas para fluxo de ar</font> </h2><br>  Al√©m de resolver todos esses problemas, √© claro, a maneira como criamos novos para n√≥s mesmos - implantar o Airflow para iniciar e monitorar os aplicativos Spark acabou sendo dif√≠cil. <br><br>  A principal dificuldade era que ningu√©m iria remodelar toda a infraestrutura para n√≥s, porque  O recurso devops √© uma coisa escassa.  Por esse motivo, tivemos que n√£o apenas implementar o Airflow, mas integr√°-lo ao sistema existente, o que √© muito mais dif√≠cil de ser visto do zero. <br><br>  Quero falar sobre as dores que encontramos durante o processo de implementa√ß√£o e as muletas que tivemos que cortar para obter o Airflow. <br><br>  <b>A primeira e principal dor</b> : como integrar o Airflow em um grande script de shell do departamento de opera√ß√µes. <br><br>  Aqui a solu√ß√£o √© a mais √≥bvia - come√ßamos a disparar gr√°ficos diretamente do script shell usando o bin√°rio do fluxo de ar com a tecla trigger_dag.  Com essa abordagem, n√£o usamos o sheduler Airflow e, de fato, o aplicativo Spark √© iniciado com a mesma coroa - isso n√£o √© religiosamente muito correto.  Mas conseguimos uma integra√ß√£o perfeita com uma solu√ß√£o existente.  Aqui est√° a apar√™ncia do in√≠cio do script de shell do nosso principal aplicativo Spark, que √© chamado historicamente de hidramatrizes. <br><br><pre><code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">log</span></span> <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$FUNCNAME</span></span></span><span class="hljs-string"> started"</span></span> <span class="hljs-built_in"><span class="hljs-built_in">local</span></span> RETVAL=0 <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> AIRFLOW_CONFIG=/opt/airflow/airflow.cfg AIRFLOW_API=api/dag_last_run/hydramatrices/all <span class="hljs-built_in"><span class="hljs-built_in">log</span></span> <span class="hljs-string"><span class="hljs-string">"run /var/www/airflow/bin/airflow trigger_dag hydramatrices"</span></span> /var/www/airflow/bin/airflow trigger_dag hydramatrices 2&gt;&amp;1 | tee -a <span class="hljs-variable"><span class="hljs-variable">$LOGFILE</span></span></code> </pre> <br>  <b>Dor: O</b> script de shell do departamento de opera√ß√µes deve, de alguma forma, determinar o status do gr√°fico Airflow para controlar seu pr√≥prio fluxo de execu√ß√£o. <br><br>  Muleta: estendemos a API REST do Airflow com um ponto de extremidade para o monitoramento do DAG dentro dos scripts do shell.  Agora, cada gr√°fico possui tr√™s estados: EXECUTANDO, SUCEDIDO, FALHADO. <br><br>  De fato, depois de iniciar os c√°lculos no Airflow, simplesmente pesquisamos regularmente o gr√°fico em execu√ß√£o: marcamos a solicita√ß√£o GET para determinar se o DAG foi conclu√≠do ou n√£o.  Quando o terminal de monitoramento responde sobre a execu√ß√£o bem-sucedida do gr√°fico, o script de shell continua executando seu fluxo. <br>  Quero dizer que a API REST do Airflow √© apenas uma coisa impetuosa que permite configurar flexivelmente seus pipelines - por exemplo, voc√™ pode encaminhar par√¢metros POST para gr√°ficos. <br><br>  A extens√£o da API Airflow √© apenas uma classe Python que se parece com isso: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> json <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> settings <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> DagBag, DagRun <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> flask <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Blueprint, request, Response airflow_api_blueprint = Blueprint(<span class="hljs-string"><span class="hljs-string">'airflow_api'</span></span>, __name__, url_prefix=<span class="hljs-string"><span class="hljs-string">'/api'</span></span>) AIRFLOW_DAGS = <span class="hljs-string"><span class="hljs-string">'{}/dags'</span></span>.format( os.path.dirname(os.path.dirname(os.path.abspath(__file__))) ) <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ApiResponse</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    GET """</span></span> STATUS_OK = <span class="hljs-number"><span class="hljs-number">200</span></span> STATUS_NOT_FOUND = <span class="hljs-number"><span class="hljs-number">404</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">pass</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">standard_response</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(status: int, payload: dict)</span></span></span><span class="hljs-function"> -&gt; Response:</span></span> json_data = json.dumps(payload) resp = Response(json_data, status=status, mimetype=<span class="hljs-string"><span class="hljs-string">'application/json'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> resp <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">success</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, payload: dict)</span></span></span><span class="hljs-function"> -&gt; Response:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.standard_response(self.STATUS_OK, payload) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">error</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, status: int, message: str)</span></span></span><span class="hljs-function"> -&gt; Response:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.standard_response(status, {<span class="hljs-string"><span class="hljs-string">'error'</span></span>: message}) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">not_found</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, message: str = </span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'Resource not found'</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function"> -&gt; Response:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.error(self.STATUS_NOT_FOUND, message)</code> </pre><br>  Usamos a API no script de shell - pesquisamos o endpoint a cada 10 minutos: <br><br><pre> <code class="bash hljs"> TRIGGER=$? [ <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$TRIGGER</span></span></span><span class="hljs-string">"</span></span> -eq <span class="hljs-string"><span class="hljs-string">"0"</span></span> ] &amp;&amp; <span class="hljs-built_in"><span class="hljs-built_in">log</span></span> <span class="hljs-string"><span class="hljs-string">"trigger airflow DAG succeeded"</span></span> || { <span class="hljs-built_in"><span class="hljs-built_in">log</span></span> <span class="hljs-string"><span class="hljs-string">"trigger airflow DAG failed"</span></span>; <span class="hljs-built_in"><span class="hljs-built_in">return</span></span> 1; } CMD=<span class="hljs-string"><span class="hljs-string">"curl -s http://</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$HYDRA_SERVER</span></span></span><span class="hljs-string">/</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$AIRFLOW_API</span></span></span><span class="hljs-string"> | jq .dag_last_run.state"</span></span> STATE=$(<span class="hljs-built_in"><span class="hljs-built_in">eval</span></span> <span class="hljs-variable"><span class="hljs-variable">$CMD</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> [ <span class="hljs-variable"><span class="hljs-variable">$STATE</span></span> == \<span class="hljs-string"><span class="hljs-string">"running\" ]; do log "</span></span>Generating matrices <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> progress...<span class="hljs-string"><span class="hljs-string">" sleep 600 STATE=</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$(eval $CMD)</span></span></span><span class="hljs-string"> done [ </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$STATE</span></span></span><span class="hljs-string"> == \"success\" ] &amp;&amp; RETVAL=0 || RETVAL=1 [ </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$RETVAL</span></span></span><span class="hljs-string"> -eq 0 ] &amp;&amp; log "</span></span><span class="hljs-variable"><span class="hljs-variable">$FUNCNAME</span></span> succeeded<span class="hljs-string"><span class="hljs-string">" || log "</span></span><span class="hljs-variable"><span class="hljs-variable">$FUNCNAME</span></span> failed<span class="hljs-string"><span class="hljs-string">" return </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$RETVAL</span></span></span></span></code> </pre><br>  <b>Problema</b> : se voc√™ executar um trabalho do Spark usando o envio de spark no modo de cluster, saber√° que os logs no STDOUT s√£o uma folha n√£o informativa com as linhas "SPARK APPLICATION_ID IS RUNNING".  Os logs do pr√≥prio aplicativo Spark podem ser visualizados, por exemplo, usando o comando yarn logs.  No script de shell, esse problema foi resolvido simplesmente: um t√∫nel SSH foi aberto em uma das m√°quinas de cluster e o envio de spark foi executado no modo cliente para esta m√°quina.  Nesse caso, o STDOUT ter√° logs leg√≠veis e compreens√≠veis.  No Airflow, decidimos sempre usar a decis√£o de cluster e esse n√∫mero n√£o funcionar√°. <br><br>  Muleta: ap√≥s o envio do spark, n√≥s extra√≠mos os logs do driver do HDFS por application_id e o exibimos na interface Airflow simplesmente atrav√©s do operador Python print ().  O √∫nico aspecto negativo - na interface Airflow, os logs aparecem somente ap√≥s o envio do spark, voc√™ precisa seguir o tempo real em outros lugares - por exemplo, o focinho da web do YARN. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_logs</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(config: BaseConfig, app_id: str)</span></span></span><span class="hljs-function"> -&gt; </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">None</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""   :param config: :param app_id: """</span></span> hdfs = HDFSInteractor(config) logs_path = <span class="hljs-string"><span class="hljs-string">'/tmp/logs/{username}/logs/{app_id}'</span></span>.format(username=config.CURRENT_USERNAME, app_id=app_id) logs_files = hdfs.files_in_folder(logs_path) logs_files = [file <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> file <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> logs_files <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> file[<span class="hljs-number"><span class="hljs-number">-4</span></span>:] != <span class="hljs-string"><span class="hljs-string">'.tmp'</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> file <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> logs_files: <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> hdfs.hdfs_client.read(os.path.join(logs_path, file), encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>, delimiter=<span class="hljs-string"><span class="hljs-string">'\n'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> reader: print_line = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> reader: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> re.search(<span class="hljs-string"><span class="hljs-string">'stdout'</span></span>, line) <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> len(line) &gt; <span class="hljs-number"><span class="hljs-number">30</span></span>: print_line = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> re.search(<span class="hljs-string"><span class="hljs-string">'stderr'</span></span>, line): print_line = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> print_line: print(line)</code> </pre><br>  <b>Dor</b> : para testadores e desenvolvedores, seria bom ter uma bancada de testes do Airflow, mas estamos economizando recursos de devops, por isso pensamos em como implantar o ambiente de teste por um longo tempo. <br><br>  Muleta: empacotamos o Airflow em um cont√™iner de encaixe e o Dockerfile o colocou no reposit√≥rio com trabalhos de fa√≠sca.  Assim, cada desenvolvedor ou testador pode aumentar seu pr√≥prio fluxo de ar em uma m√°quina local.  Devido ao fato de os aplicativos serem executados no modo de cluster, quase n√£o s√£o necess√°rios recursos locais para o docker. <br><br>  Uma instala√ß√£o local do spark estava oculta no cont√™iner do docker e em toda a sua configura√ß√£o por meio de vari√°veis ‚Äã‚Äãde ambiente - voc√™ n√£o precisa mais gastar v√°rias horas configurando o ambiente.  Abaixo, dei um exemplo com um fragmento de arquivo docker para um cont√™iner com Airflow, onde voc√™ pode ver como o Airflow √© configurado usando vari√°veis ‚Äã‚Äãde ambiente: <br><br><pre> <code class="bash hljs">FROM ubuntu:16.04 ARG AIRFLOW_VERSION=1.9.0 ARG AIRFLOW_HOME ARG USERNAME=airflow ARG USER_ID ARG GROUP_ID ARG LOCALHOST ARG AIRFLOW_PORT ARG PIPENV_PATH ARG PROJECT_HYDRAMATRICES_DOCKER_PATH RUN apt-get update \ &amp;&amp; apt-get install -y \ python3.6 \ python3.6-dev \ &amp;&amp; update-alternatives --install /usr/bin/python3 python3.6 /usr/bin/python3.6 0 \ &amp;&amp; apt-get -y install python3-pip RUN mv /root/.pydistutils.cf /root/.pydistutils.cfg RUN pip3 install pandas==0.20.3 \ apache-airflow==<span class="hljs-variable"><span class="hljs-variable">$AIRFLOW_VERSION</span></span> \ psycopg2==2.7.5 \ ldap3==2.5.1 \ cryptography <span class="hljs-comment"><span class="hljs-comment">#   ,       ENV PROJECT_HYDRAMATRICES_DOCKER_PATH=${PROJECT_HYDRAMATRICES_DOCKER_PATH} ENV PIPENV_PATH=${PIPENV_PATH} ENV SPARK_HOME=/usr/lib/spark2 ENV HADOOP_CONF_DIR=$PROJECT_HYDRAMATRICES_DOCKER_PATH/etc/hadoop-conf-preprod ENV PYTHONPATH=${SPARK_HOME}/python/lib/py4j-0.10.4-src.zip:${SPARK_HOME}/python/lib/pyspark.zip:${SPARK_HOME}/python/lib ENV PIP_NO_BINARY=numpy ENV AIRFLOW_HOME=${AIRFLOW_HOME} ENV AIRFLOW_DAGS=${AIRFLOW_HOME}/dags ENV AIRFLOW_LOGS=${AIRFLOW_HOME}/logs ENV AIRFLOW_PLUGINS=${AIRFLOW_HOME}/plugins #      Airflow (log url) BASE_URL="http://${AIRFLOW_CURRENT_HOST}:${AIRFLOW_PORT}" ; #   Airflow ENV AIRFLOW__WEBSERVER__BASE_URL=${BASE_URL} ENV AIRFLOW__WEBSERVER__ENDPOINT_URL=${BASE_URL} ENV AIRFLOW__CORE__AIRFLOW_HOME=${AIRFLOW_HOME} ENV AIRFLOW__CORE__DAGS_FOLDER=${AIRFLOW_DAGS} ENV AIRFLOW__CORE__BASE_LOG_FOLDER=${AIRFLOW_LOGS} ENV AIRFLOW__CORE__PLUGINS_FOLDER=${AIRFLOW_PLUGINS} ENV AIRFLOW__SCHEDULER__CHILD_PROCESS_LOG_DIRECTORY=${AIRFLOW_LOGS}/scheduler</span></span></code> </pre><br>  Como resultado da implementa√ß√£o do Airflow, alcan√ßamos os seguintes resultados: <br><br><ul><li>  Reduzido o ciclo de lan√ßamento: a implanta√ß√£o de um novo modelo (ou um pipeline de prepara√ß√£o de dados) agora se resume √† cria√ß√£o de um novo gr√°fico do Airflow; os pr√≥prios gr√°ficos s√£o armazenados no reposit√≥rio e implantados com o c√≥digo.  Esse processo est√° inteiramente nas m√£os do desenvolvedor.  Os administradores est√£o felizes, j√° n√£o os usamos em ninharias. </li><li>  Os logs de aplicativos Spark que costumavam ir direto ao inferno agora s√£o armazenados no Aiflow com uma interface de acesso conveniente.  Voc√™ pode ver os logs de qualquer dia sem escolher os diret√≥rios HDFS. </li><li>  O c√°lculo com falha pode ser reiniciado com um bot√£o na interface, √© muito conveniente, at√© junho pode lidar com isso. </li><li>  Voc√™ pode marcar tarefas do spark na interface sem precisar executar as configura√ß√µes do Spark na m√°quina local.  Os testadores est√£o satisfeitos - todas as configura√ß√µes do envio de spark para funcionar corretamente j√° foram feitas no Dockerfile </li><li>  Aiflow standard buns - agenda, reinicia trabalhos interrompidos, belos gr√°ficos (por exemplo, tempo de execu√ß√£o do aplicativo, estat√≠sticas de lan√ßamentos bem-sucedidos e sem √™xito). </li></ul><br>  Para onde ir a seguir?  Agora, temos um grande n√∫mero de fontes de dados e sumidouros, cujo n√∫mero aumentar√°.  Altera√ß√µes em qualquer classe de reposit√≥rio hydramatrices podem falhar em outro pipeline (ou mesmo na parte online): <br><br><ul><li>  Estouros de Clickhouse ‚Üí Hive </li><li>  pr√©-processamento de dados: Hive ‚Üí Hive </li><li>  implantar modelos c2c: Hive ‚Üí Redis </li><li>  prepara√ß√£o de diret√≥rios (como o tipo de monetiza√ß√£o do conte√∫do): Postgres ‚Üí Redis </li><li>  prepara√ß√£o do modelo: FS local ‚Üí HDFS </li></ul><br>  Em tal situa√ß√£o, precisamos realmente de um suporte para testes autom√°ticos de tubula√ß√µes na prepara√ß√£o de dados.  Isso reduzir√° bastante o custo das altera√ß√µes de teste no reposit√≥rio, acelerar√° o lan√ßamento de novos modelos na produ√ß√£o e aumentar√° drasticamente o n√≠vel de endorfinas nos testadores.  Mas sem o Airflow, seria imposs√≠vel implantar um suporte para esse tipo de teste autom√°tico! <br><br>  Eu escrevi este artigo para contar sobre a nossa experi√™ncia na implementa√ß√£o do Airflow, que pode ser √∫til para outras equipes em uma situa√ß√£o semelhante - voc√™ j√° possui um grande sistema de trabalho e deseja experimentar algo novo, elegante e jovem.  N√£o √© preciso ter medo de nenhuma atualiza√ß√£o do sistema em funcionamento; voc√™ precisa experimentar - essas experi√™ncias geralmente abrem novos horizontes para o desenvolvimento. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt456630/">https://habr.com/ru/post/pt456630/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt456614/index.html">Como um conto de praga: o quadro da inoc√™ncia √© processado</a></li>
<li><a href="../pt456616/index.html">3 milh√µes de rublos para quem pode codificar</a></li>
<li><a href="../pt456618/index.html">Larabeer Moscou - 21 de junho</a></li>
<li><a href="../pt456622/index.html">Como criar um sistema operacional certificado de acordo com a prote√ß√£o classe I</a></li>
<li><a href="../pt456624/index.html">Ferramentas √∫teis em Python</a></li>
<li><a href="../pt456632/index.html">Estamos construindo o quarto andar de modelos C ++ no RESTinio. Por que e como?</a></li>
<li><a href="../pt456634/index.html">Receitas Nginx: CAS (Servi√ßo Central de Autoriza√ß√£o)</a></li>
<li><a href="../pt456638/index.html">Comparando o mesmo projeto em Rust, Haskell, C ++, Python, Scala e OCaml</a></li>
<li><a href="../pt456640/index.html">An√°lise do concurso de intelig√™ncia competitiva no PHDays 9</a></li>
<li><a href="../pt456642/index.html">A primeira gradua√ß√£o do programa de mestrado JetBrains Corporate e da Universidade ITMO</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>