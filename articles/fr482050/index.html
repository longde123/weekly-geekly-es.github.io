<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👲 👩‍👩‍👧‍👦 👸🏿 Technique de réduction du réseau de convolution Jedi - élagage 🏾 👰🏽 🏞️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Avant de nouveau, la tâche de détecter des objets. Priorité - vitesse avec une précision acceptable. Vous prenez l'architecture de YOLOv3 et la formez...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Technique de réduction du réseau de convolution Jedi - élagage</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/482050/"><p><img src="https://habrastorage.org/webt/tf/oa/br/tfoabr16w_dawnzb9hnjndyv_bg.png" alt="image"></p><br><p>  Avant de nouveau, la tâche de détecter des objets.  Priorité - vitesse avec une précision acceptable.  Vous prenez l'architecture de YOLOv3 et la formez.  La précision (mAp75) est supérieure à 0,95.  Mais la vitesse de course est encore faible.  L'enfer </p><br><p>  Aujourd'hui, nous contournerons la quantification.  Et sous la coupe, pensez à l' <strong>élagage du modèle</strong> - couper les parties de réseau redondantes pour accélérer l'inférence sans perdre en précision.  Visuellement - où, combien et comment couper.  Voyons comment le faire manuellement et où vous pouvez automatiser.  À la fin se trouve un référentiel sur les keras. </p><a name="habracut"></a><br><h3 id="vvedenie">  Présentation </h3><br><p>  Au dernier lieu de travail, Perm Macroscop, j'ai eu une habitude - toujours de surveiller le temps d'exécution des algorithmes.  Et la durée d'exécution du réseau doit toujours être vérifiée via le filtre d'adéquation.  Habituellement, l'état de l'art dans la prod ne passe pas ce filtre, ce qui m'a conduit à l'élagage. </p><br><p>  L'élagage est un thème ancien qui a été abordé dans les <a href="https://www.youtube.com/watch%3Fv%3DeZdOkDtYMoo" rel="nofollow">conférences de Stanford</a> en 2017.  L'idée principale est de réduire la taille du réseau formé sans perdre en précision en supprimant divers nœuds.  Ça a l'air cool, mais j'entends rarement parler de son utilisation.  Probablement, il n'y a pas assez d'implémentations, il n'y a pas d'articles en russe, ou tout le monde considère que l'élagage du savoir-faire est silencieux. <br>  Mais va démonter </p><br><h3 id="vzglyad-v-biologiyu">  Un regard sur la biologie </h3><br><p>  J'aime quand dans le Deep Learning les idées viennent de la biologie.  On peut leur faire confiance, comme l'évolution (saviez-vous que ReLU est très similaire à la <a href="http://www.gatsby.ucl.ac.uk/~lmate/biblio/dayanabbott.pdf" rel="nofollow">fonction d'activation des neurones dans le cerveau</a> ?) </p><br><p>  Le processus d'élagage du modèle est également proche de la biologie.  La réponse du réseau ici peut être comparée à la plasticité du cerveau.  Quelques exemples intéressants se trouvent dans <a href="https://www.litres.ru/norman-doydzh/plastichnost-mozga/%3Futm_medium%3Dcpc%26utm_source%3Dgoogle%26utm_campaign%3DDSA%257C149839530%26utm_term%3D%26utm_content%3Dk50id%257Caud-499675211712%253Adsa-179513627318%257Ccid%257C149839530%257Caid%257C248455294996%257Cgid%257C6837176850%257Cpos%257C1t1%257Csrc%257Cg_%257Cdvc%257Cc%257Creg%257C1011993%257Crin%257C%257C%26k50id%3D6837176850%257Caud-499675211712%253Adsa-179513627318%26gclid%3DCj0KCQiA0ZHwBRCRARIsAK0Tr-oKPqkmL7_Oxg62JZO8Jlk9zO-9nYKIRFxHi_lgoCvsQQadvUGxUzkaApgpEALw_wcB" rel="nofollow">le</a> livre de <a href="https://www.litres.ru/norman-doydzh/plastichnost-mozga/%3Futm_medium%3Dcpc%26utm_source%3Dgoogle%26utm_campaign%3DDSA%257C149839530%26utm_term%3D%26utm_content%3Dk50id%257Caud-499675211712%253Adsa-179513627318%257Ccid%257C149839530%257Caid%257C248455294996%257Cgid%257C6837176850%257Cpos%257C1t1%257Csrc%257Cg_%257Cdvc%257Cc%257Creg%257C1011993%257Crin%257C%257C%26k50id%3D6837176850%257Caud-499675211712%253Adsa-179513627318%26gclid%3DCj0KCQiA0ZHwBRCRARIsAK0Tr-oKPqkmL7_Oxg62JZO8Jlk9zO-9nYKIRFxHi_lgoCvsQQadvUGxUzkaApgpEALw_wcB" rel="nofollow">Norman Dodge</a> : </p><br><ol><li>  Le cerveau d'une femme qui n'avait qu'une moitié de naissance s'est reprogrammée pour remplir les fonctions de la moitié manquante </li><li>  Le gars s'est tiré une balle dans la partie du cerveau responsable de la vision.  Au fil du temps, d'autres parties du cerveau ont repris ces fonctions.  (ne réessayez pas) </li></ol><br><p>  Ainsi, à partir de votre modèle, vous pouvez supprimer certains des paquets faibles.  Dans les cas extrêmes, les faisceaux restants aideront à remplacer ceux coupés. </p><br><h3 id="lyubish-transfer-learning-ili-uchish-s-nulya">  Aimez-vous le transfert d'apprentissage ou apprenez à partir de zéro? </h3><br><p>  <strong>Option numéro un.</strong>  Vous utilisez Transfer Learning sur Yolov3.  Retina, Mask-RCNN ou U-Net.  Mais le plus souvent, nous n'avons pas besoin de reconnaître 80 classes d'objets, comme dans COCO.  Dans ma pratique, tout est limité à 1-2 classes.  On peut supposer que l'architecture pour 80 classes est redondante ici.  Cela laisse penser que l'architecture doit être réduite.  De plus, je voudrais le faire sans perdre les poids pré-formés existants. </p><br><p>  <strong>Option numéro deux.</strong>  Peut-être que vous avez beaucoup de données et de ressources informatiques, ou que vous avez juste besoin d'une architecture super-personnalisée.  Peu importe.  Mais vous apprenez le réseau à partir de zéro.  L'ordre habituel consiste à examiner la structure des données, à sélectionner une architecture dont la puissance est RÉDUITE et à empêcher les abandons de se recycler.  J'ai vu des décrocheurs 0,6, Carl. </p><br><p>  Dans les deux cas, le réseau peut être réduit.  Promu.  Voyons maintenant quel genre de <del>  circoncision </del>  élagage </p><br><h3 id="obschiy-algoritm">  Algorithme général </h3><br><p>  Nous avons décidé de supprimer la convolution.  Cela semble très simple: </p><br><p><img src="https://habrastorage.org/webt/ey/yt/-g/eyyt-g-b6pfzjrbnim_ssosyqqk.png"></p><br><p>  La suppression de toute convolution est une contrainte pour le réseau, ce qui entraîne généralement une augmentation des erreurs.  D'une part, cette croissance d'erreur est un indicateur de la façon dont nous supprimons correctement les convolutions (par exemple, une croissance importante indique que nous faisons quelque chose de mal).  Mais une petite croissance est tout à fait acceptable et est souvent éliminée par un entraînement ultérieur facile et ultérieur avec un petit LR.  Nous ajoutons une étape de recyclage: </p><br><p><img src="https://habrastorage.org/webt/kb/ui/5d/kbui5dm1k8sgflm5xzu0wggbbhs.png"></p><br><p>  Maintenant, nous devons comprendre quand nous voulons arrêter notre cycle d'apprentissage &lt;-&gt; Élagage.  Il peut y avoir des options exotiques lorsque nous devons réduire le réseau à une certaine taille et vitesse d'exécution (par exemple, pour les appareils mobiles).  Cependant, l'option la plus courante consiste à poursuivre le cycle jusqu'à ce que l'erreur devienne supérieure à celle autorisée.  Ajouter une condition: </p><br><p><img src="https://habrastorage.org/webt/1i/pi/52/1ipi52uqhkciw2ne-1zt2rbdmje.png"></p><br><p>  Ainsi, l'algorithme devient clair.  Reste à savoir comment déterminer les circonvolutions supprimées. </p><br><h3 id="poisk-udalyaemyh-svertok">  Rechercher la convolution à supprimer </h3><br><p>  Nous devons supprimer certaines circonvolutions.  Arracher et «tirer» sur n'importe quelle est une mauvaise idée, même si cela fonctionnera.  Mais si vous avez une tête, vous pouvez penser et essayer de sélectionner des convolutions «faibles» pour l'enlèvement.  Il existe plusieurs options: </p><br><ol><li>  <a href="https://openreview.net/pdf%3Fid%3DrJqFGTslg" rel="nofollow">La plus petite mesure L1 ou élagage à faible amplitude</a> .  L'idée que les convolutions de petits poids contribuent peu à la décision finale </li><li>  La plus petite mesure L1 tenant compte de la moyenne et de l'écart-type.  Nous complétons l'appréciation de la nature de la distribution. </li><li>  <a href="https://arxiv.org/abs/1512.08571" rel="nofollow">Masquer les circonvolutions et éliminer le moins possible la précision résultante</a> .  Une définition plus précise des convolutions insignifiantes, mais très chronophage et gourmande en ressources. </li><li>  Autre </li></ol><br><p>  Chacune des options a droit à la vie et possède ses propres fonctionnalités de mise en œuvre.  Ici, nous considérons la variante avec la plus petite mesure L1 </p><br><h3 id="ruchnoy-process-dlya-yolov3">  Processus manuel pour YOLOv3 </h3><br><p>  L'architecture d'origine contient des blocs résiduels.  Mais peu importe à quel point ils sont cool pour les réseaux profonds, ils nous gêneront quelque peu.  La difficulté est que vous ne pouvez pas supprimer les rapprochements avec différents indices dans ces couches: </p><br><p><img src="https://habrastorage.org/webt/mh/p-/-k/mhp--ksk3ifgurz5jx6exgcrm5c.png"></p><br><p>  Par conséquent, nous sélectionnons les couches dont nous pouvons supprimer librement les rapprochements: </p><br><p><img src="https://habrastorage.org/webt/qy/ek/zo/qyekzofcur-q0auqurg3egxnato.png"></p><br><p>  Construisons maintenant un cycle de travail: </p><br><ol><li>  Décharger l'activation </li><li>  On se demande combien couper </li><li>  Découper </li><li>  Apprenez 10 époques avec LR = 1e-4 </li><li>  Test </li></ol><br><p>  Le déchargement des convolutions est utile pour évaluer quelle partie nous pouvons retirer à une certaine étape.  Exemples de déchargement: </p><br><p><img src="https://habrastorage.org/webt/rp/jo/pk/rpjopk6dzfrl6psoucr8tgj0log.png"></p><br><p>  Nous voyons que presque partout 5% des circonvolutions ont une norme L1 très basse et nous pouvons les supprimer.  À chaque étape, ce déchargement a été répété et une évaluation a été faite des couches et de la quantité pouvant être coupée. </p><br><p>  L'ensemble du processus s'est déroulé en 4 étapes (ici et partout les chiffres du RTX 2060 Super): </p><br><div class="scrollable-table"><table><thead><tr><th>  Étape </th><th>  mAp75 </th><th>  Le nombre de paramètres, millions </th><th>  Taille du réseau, mb </th><th>  De l'original,% </th><th>  Temps d'exécution, ms </th><th>  Condition de circoncision </th></tr></thead><tbody><tr><td>  0 </td><td>  0,9656 </td><td>  60 </td><td>  241 </td><td>  100 </td><td>  180 </td><td>  - </td></tr><tr><td>  1 </td><td>  0,9622 </td><td>  55 </td><td>  218 </td><td>  91 </td><td>  175 </td><td>  5% de tous </td></tr><tr><td>  2 </td><td>  0,9625 </td><td>  50 </td><td>  197 </td><td>  83 </td><td>  168 </td><td>  5% de tous </td></tr><tr><td>  3 </td><td>  0,9633 </td><td>  39 </td><td>  155 </td><td>  64 </td><td>  155 </td><td>  15% pour les couches avec plus de 400 convolutions </td></tr><tr><td><del>  4 </del></td><td><del>  0,9555 </del></td><td><del>  31 </del></td><td><del>  124 </del></td><td><del>  51 </del></td><td><del>  146 </del></td><td><del>  10% pour les couches avec 100+ convolution </del></td></tr></tbody></table></div><br><p>  À l'étape 2, un effet positif a été ajouté: le patch de taille 4 est entré en mémoire, ce qui a considérablement accéléré le processus de recyclage. <br>  À l'étape 4, le processus a été arrêté, car  même une formation continue prolongée n'a pas élevé le mAp75 à ses anciennes valeurs. <br>  En conséquence, nous avons réussi à accélérer l'inférence de <strong>15%</strong> , à réduire la taille de <strong>35%</strong> et à ne pas perdre en précision. </p><br><h3 id="avtomatizaciya-dlya-bolee-prostyh-arhitektur">  Automatisation pour des architectures plus simples </h3><br><p>  Pour les architectures de réseau plus simples (sans ajout conditionnel, concaténation et blocs résiduels), il est tout à fait possible de se concentrer sur le traitement de toutes les couches convolutives et d'automatiser le processus de découpe des convolutions. </p><br><p>  J'ai implémenté <a href="https://github.com/PaginDm/keras-L1-pruning" rel="nofollow">cette</a> option <a href="https://github.com/PaginDm/keras-L1-pruning" rel="nofollow">ici</a> . <br>  C'est simple: vous n'avez qu'une fonction de perte, un optimiseur et des générateurs batch: </p><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pruning <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.optimizers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Adam <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequence train_batch_generator = BatchGenerator... score_batch_generator = BatchGenerator... opt = Adam(lr=<span class="hljs-number"><span class="hljs-number">1e-4</span></span>) pruner = pruning.Pruner(<span class="hljs-string"><span class="hljs-string">"config.json"</span></span>, <span class="hljs-string"><span class="hljs-string">"categorical_crossentropy"</span></span>, opt) pruner.prune(train_batch, valid_batch)</code> </pre> <br><p>  Si nécessaire, vous pouvez modifier les paramètres de configuration: </p><br><pre> <code class="json hljs">{ <span class="hljs-attr"><span class="hljs-attr">"input_model_path"</span></span>: <span class="hljs-string"><span class="hljs-string">"model.h5"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"output_model_path"</span></span>: <span class="hljs-string"><span class="hljs-string">"model_pruned.h5"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"finetuning_epochs"</span></span>: <span class="hljs-number"><span class="hljs-number">10</span></span>, # the number of epochs for train between pruning steps <span class="hljs-attr"><span class="hljs-attr">"stop_loss"</span></span>: <span class="hljs-number"><span class="hljs-number">0.1</span></span>, # loss for stopping process <span class="hljs-attr"><span class="hljs-attr">"pruning_percent_step"</span></span>: <span class="hljs-number"><span class="hljs-number">0.05</span></span>, # part of convs for delete on every pruning step <span class="hljs-attr"><span class="hljs-attr">"pruning_standart_deviation_part"</span></span>: <span class="hljs-number"><span class="hljs-number">0.2</span></span> # shift for limit pruning part }</code> </pre> <br><p>  De plus, une restriction basée sur l'écart-type est mise en œuvre.  L'objectif est de limiter une partie de celles supprimées, à l'exclusion des convolutions avec des mesures L1 déjà "suffisantes": </p><br><p><img src="https://habrastorage.org/webt/bh/_9/nq/bh_9nqasnp91xifixn7ilhco0mw.png"></p><br><p>  Ainsi, nous ne pouvons supprimer que des convolutions faibles de distributions similaires à la droite et ne pas affecter la suppression de distributions comme la gauche: </p><br><p><img src="https://habrastorage.org/webt/pr/r5/zp/prr5zpjrdvh1wow6slejnn6axya.png"></p><br><p>  Lorsque la distribution s'approche de la normale, le coefficient pruning_standart_deviation_part peut être sélectionné parmi: </p><br><p><img src="https://habrastorage.org/webt/dl/yl/7d/dlyl7dub216jsr67dcbnhez5fl8.png"><br>  Je recommande une hypothèse de 2 sigma.  Ou vous ne pouvez pas vous concentrer sur cette fonctionnalité, laissant la valeur &lt;1.0. </p><br><p>  La sortie est un graphique de la taille du réseau, de la perte et de la durée d'exécution du réseau pour l'ensemble du test, normalisé à 1,0.  Par exemple, ici, la taille du réseau a été réduite de près de 2 fois sans perte de qualité (un petit réseau de convolution pour des poids de 100k): </p><br><p><img src="https://habrastorage.org/webt/ig/hu/x_/ighux_gyoaptm71iu2hk_txga_g.png"></p><br><p>  La vitesse de course est sujette à des fluctuations normales et n'a pas beaucoup changé.  Il y a une explication à cela: </p><br><ol><li>  Le nombre de circonvolutions passe de pratique (32, 64, 128) à pas le plus pratique pour les cartes vidéo - 27, 51, etc.  Ici, je peux me tromper, mais cela affecte très probablement. </li><li>  L'architecture n'est pas large, mais cohérente.  En réduisant la largeur, on ne touche pas la profondeur.  Ainsi, nous réduisons la charge, mais ne modifions pas la vitesse. </li></ol><br><p>  Par conséquent, l'amélioration s'est traduite par une diminution de la charge de CUDA pendant l'analyse de 20 à 30%, mais pas par une diminution de la durée d'exécution. </p><br><h3 id="itogi">  Résumé </h3><br><p>  Réfléchissez.  Nous avons envisagé 2 options d'élagage - pour YOLOv3 (lorsque vous devez travailler avec vos mains) et pour les réseaux avec des architectures plus faciles.  On peut voir que dans les deux cas, il est possible d'obtenir une réduction de la taille et de l'accélération du réseau sans perte de précision.  Résultats: </p><br><ul><li>  Réduction des effectifs </li><li>  Accélération de la course </li><li>  Réduction de charge CUDA </li><li>  En conséquence, respect de l'environnement (Nous optimisons l'utilisation future des ressources informatiques. Quelque part <a href="https://meduza.io/feature/2019/12/12/kto-takaya-greta-tunberg-i-pochemu-ona-stala-chelovekom-goda-zhurnal-time" rel="nofollow">Greta Tunberg</a> se réjouit seule) </li></ul><br><h3 id="appendix">  Appendice </h3><br><ul><li>  Après l'étape d'élagage, vous pouvez également tordre la quantification (par exemple, avec TensorRT) </li><li>  Tensorflow fournit des fonctionnalités pour <a href="https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras" rel="nofollow">low_magnitude_pruning</a> .  Ça marche. </li><li>  Je souhaite développer le <a href="https://github.com/PaginDm/keras-L1-pruning" rel="nofollow">référentiel</a> et je serai heureux de vous aider </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr482050/">https://habr.com/ru/post/fr482050/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr482034/index.html">Yandex: il y a tout ... sur les utilisateurs</a></li>
<li><a href="../fr482038/index.html">Nous résumons les résultats de 2019 à Haber Career</a></li>
<li><a href="../fr482040/index.html">Caractéristiques des programmes de profilage en C ++</a></li>
<li><a href="../fr482042/index.html">Travailler avec la bibliothèque Newtonsoft.Json avec un exemple réel. 2e partie</a></li>
<li><a href="../fr482044/index.html">10 meilleures pratiques pour sécuriser les images Docker. 2e partie</a></li>
<li><a href="../fr482052/index.html">Jeu de données du Nouvel An 2019: dictionnaire tonal ouvert de la langue russe</a></li>
<li><a href="../fr482054/index.html">3. Pile élastique: analyse du journal de sécurité. Tableaux de bord</a></li>
<li><a href="../fr482058/index.html">Prédateur ou proie? Qui protégera les autorités de certification</a></li>
<li><a href="../fr482060/index.html">Modèle de mandat de contrôle d'accès (MAC): présentation et applications</a></li>
<li><a href="../fr482064/index.html">Facilité de développement de sites multilingues sur CMS Umbraco 8</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>