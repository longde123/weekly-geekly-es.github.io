<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§∏üèΩ üèà üôçüèΩ Le probl√®me des bandits √† bras multiples - Comparez la strat√©gie Epsilon-Greedy et l'√©chantillonnage de Thompson üëû üôéüèø üßóüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr! Je vous pr√©sente la premi√®re page des bandits multi-bras Solving: une comparaison de l'√©chantillonnage epsilon-greedy et Thompson . 

 ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Le probl√®me des bandits √† bras multiples - Comparez la strat√©gie Epsilon-Greedy et l'√©chantillonnage de Thompson</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/425619/">  <i>Bonjour, Habr!</i>  <i>Je vous pr√©sente la premi√®re page des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">bandits multi-bras Solving: une comparaison de l'√©chantillonnage epsilon-greedy et Thompson</a> .</i> <br><br><h1>  Le probl√®me des bandits arm√©s </h1><br><p>  Le probl√®me des bandits √† plusieurs bras est l'une des t√¢ches les plus fondamentales de la science des solutions.  A savoir, c'est le probl√®me de l'allocation optimale des ressources dans des conditions d'incertitude.  Le nom de ¬´bandit multi-arm√©¬ª lui-m√™me vient de vieilles machines √† sous contr√¥l√©es par des poign√©es.  Ces fusils d'assaut √©taient surnomm√©s ¬´bandits¬ª, car apr√®s leur avoir parl√©, les gens se sentaient g√©n√©ralement vol√©s.  Imaginez maintenant qu'il existe plusieurs machines de ce type et que les chances de gagner contre diff√©rentes voitures sont diff√©rentes.  Depuis que nous avons commenc√© √† jouer avec ces machines, nous voulons d√©terminer quelle chance est la plus √©lev√©e et utiliser cette machine plus souvent que les autres. </p><br><p>  Le probl√®me est le suivant: comment pouvons-nous mieux comprendre quelle machine est la mieux adapt√©e et en m√™me temps essayer de nombreuses fonctionnalit√©s en temps r√©el?  Ce n'est pas une sorte de probl√®me th√©orique, c'est un probl√®me auquel une entreprise est confront√©e tout le temps.  Par exemple, une entreprise dispose de plusieurs options pour les messages qui doivent √™tre affich√©s aux utilisateurs (par exemple, les messages incluent des publicit√©s, des sites, des images) afin que les messages s√©lectionn√©s maximisent une certaine t√¢che commerciale (conversion, cliquabilit√©, etc.) </p><br><a name="habracut"></a><p>  Une fa√ßon typique de r√©soudre ce probl√®me consiste √† ex√©cuter plusieurs fois les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tests A / B.</a>  C'est-√†-dire, pendant plusieurs semaines, pour montrer chacune des options √©galement souvent, puis, sur la base de tests statistiques, d√©cider quelle option est la meilleure.  Cette m√©thode convient quand il y a peu d'options, disons 2 ou 4. Mais quand il y a beaucoup d'options, cette approche devient inefficace - √† la fois en temps perdu et en perte de profit. </p><br><p>  La provenance du temps perdu doit √™tre facile √† comprendre.  Plus d'options - plus de tests A / B sont n√©cessaires - plus de temps est n√©cessaire pour prendre une d√©cision.  La survenance de profits perdus n'est pas si √©vidente.  Perte d'opportunit√© (co√ªt d'opportunit√©) - les co√ªts associ√©s au fait qu'au lieu d'une action, nous avons effectu√© une autre, c'est-√†-dire, c'est ce que nous avons perdu en investissant dans A au lieu de B. Investir dans B est le manque √† gagner de l'investissement en A. Idem avec v√©rification des options.  Les tests A / B ne doivent pas √™tre interrompus tant qu'ils ne sont pas termin√©s.  Cela signifie que l'exp√©rimentateur ne sait pas quelle option est meilleure jusqu'√† la fin des tests.  Cependant, on pense toujours qu'une option sera meilleure que l'autre.  Cela signifie qu'en prolongeant les tests A / B, nous ne montrons pas les meilleures options pour un nombre suffisamment important de visiteurs (bien que nous ne sachions pas quelles options ne sont pas les meilleures), perdant ainsi nos b√©n√©fices.  C'est l'avantage perdu des tests A / B.  S'il n'y a qu'un seul test A / B, alors le profit perdu n'est peut-√™tre pas du tout grand.  Un grand nombre de tests A / B signifie que pendant longtemps nous devons montrer aux clients beaucoup des meilleures options.  Il serait pr√©f√©rable que vous puissiez rapidement √©liminer les mauvaises options en temps r√©el, et seulement alors, s'il reste peu d'options, utilisez des tests A / B pour elles. </p><br><p>  Les √©chantillonneurs ou agents sont des moyens de tester et d'optimiser rapidement la distribution des options.  Dans cet article, je vais vous pr√©senter l' <i>√©chantillonnage de Thompson</i> et ses propri√©t√©s.  Je comparerai √©galement l'√©chantillonnage de Thompson avec l'algorithme epsilon-greedy, une autre option populaire pour le probl√®me des bandits √† plusieurs bras.  Tout sera impl√©ment√© en Python √† partir de z√©ro - tout le code peut √™tre trouv√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . </p><br><h2>  Bref dictionnaire de concepts </h2><br><p></p><ul><li>  Agent, √©chantillonneur, bandit ( <i>agent, √©chantillonneur, bandit</i> ) - un algorithme qui d√©cide quelles options afficher. </li><li>  Variante - une variante diff√©rente du message que le visiteur voit. </li><li>  Action - l'action que l'algorithme a choisie (quelle option afficher). </li><li>  Utiliser ( <i>exploiter</i> ) - faites un choix pour maximiser la r√©compense totale en fonction des donn√©es disponibles. </li><li>  Explorez, <i>explorez</i> - faites des choix pour mieux comprendre le retour sur investissement de chaque option. </li><li>  R√©compense, points ( <i>score, r√©compense</i> ) - une t√¢che commerciale, par exemple, la conversion ou la cliquabilit√©.  Pour simplifier, nous pensons qu'il est distribu√© binomialement et est √©gal √† 1 ou 0 - cliqu√© ou non. </li><li>  Environnement - le contexte dans lequel l'agent op√®re - les options et leur ¬´retour sur investissement¬ª cach√©s pour l'utilisateur. </li><li>  Remboursement, probabilit√© de succ√®s ( <i>taux de versement</i> ) - une variable cach√©e √©gale √† la probabilit√© d'obtenir un score = 1, pour chaque option, elle est diff√©rente.  Mais l'utilisateur ne la voit pas. </li><li>  Essayez ( <i>essai</i> ) - l'utilisateur visite la page. </li><li>  Le regret est la diff√©rence entre ce qui serait le meilleur r√©sultat de toutes les options disponibles et ce qui √©tait le r√©sultat de l'option disponible dans la tentative actuelle.  Moins on regrette les actions d√©j√† prises, mieux c'est. </li><li>  Message ( <i>message</i> ) - une banni√®re, une option de page et plus encore, diff√©rentes versions dont nous voulons essayer. </li><li>  √âchantillonnage - la g√©n√©ration d'un √©chantillon √† partir d'une distribution donn√©e. </li></ul><br><h2>  Explorer et exploiter </h2><br><p>  Les agents sont des algorithmes qui recherchent une approche de la prise de d√©cision en temps r√©el afin d'atteindre un √©quilibre entre l'exploration de l'espace des options et l'utilisation de la meilleure option.  Cet √©quilibre est tr√®s important.  L'espace des options doit √™tre √©tudi√© afin d'avoir une id√©e de quelle option est la meilleure.  Si nous d√©couvrons d'abord cette option la plus optimale, puis que nous l'utilisons tout le temps, nous maximiserons la r√©compense totale qui nous est offerte par l'environnement.  D'un autre c√¥t√©, nous voulons √©galement explorer d'autres options possibles - et si elles se r√©v√©laient meilleures √† l'avenir, mais nous ne le savons tout simplement pas encore?  En d'autres termes, nous voulons nous assurer contre les pertes possibles, en essayant d'exp√©rimenter un peu avec des options sous-optimales afin de clarifier par elles-m√™mes leur retour sur investissement.  Si leur retour sur investissement est en fait plus √©lev√©, ils peuvent √™tre affich√©s plus souvent.  Un autre avantage des recherches sur les options est que nous pouvons mieux comprendre non seulement le retour sur investissement moyen, mais aussi la r√©partition approximative du retour sur investissement, c'est-√†-dire que nous pouvons mieux estimer l'incertitude. <br>  Le principal probl√®me est donc de r√©soudre - quelle est la meilleure fa√ßon de sortir du dilemme entre le compromis entre l'exploration et l'exploitation. </p><br><h2>  Algorithme Epsilon-gourmand </h2><br><p>  Un moyen typique de sortir de ce dilemme est l'algorithme epsilon-gourmand.  ¬´Gourmand¬ª signifie exactement ce que vous pensiez.  Apr√®s une p√©riode initiale, lorsque nous faisons accidentellement des tentatives - disons, 1000 fois, l'algorithme choisit avec empressement la meilleure option k dans <i>e</i> pour cent des tentatives.  Par exemple, si <i>e</i> = 0,05, l'algorithme 95% du temps s√©lectionne la meilleure option, et dans les 5% restants du temps, il s√©lectionne les tentatives al√©atoires.  En fait, c'est un algorithme plut√¥t efficace, cependant, il ne suffit peut-√™tre pas d'explorer l'espace des options, et donc, il ne sera pas suffisant d'√©valuer quelle option est la meilleure, de rester bloqu√© sur une option sous-optimale.  Montrons dans le code comment fonctionne cet algorithme. </p><br><p>  Mais d'abord, quelques d√©pendances.  Nous devons d√©finir l'environnement.  C'est le contexte dans lequel les algorithmes s'ex√©cuteront.  Dans ce cas, le contexte est tr√®s simple.  Il appelle l'agent pour que l'agent d√©cide quelle action choisir, puis le contexte lance cette action et renvoie les points re√ßus pour cela √† l'agent (qui met √† jour son √©tat). </p><br><pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Environment</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, variants, payouts, n_trials, variance=False)</span></span></span><span class="hljs-function">:</span></span> self.variants = variants <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> variance: self.payouts = np.clip(payouts + np.random.normal(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0.04</span></span>, size=len(variants)), <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">.2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: self.payouts = payouts <span class="hljs-comment"><span class="hljs-comment">#self.payouts[5] = self.payouts[5] if i &lt; n_trials/2 else 0.1 self.n_trials = n_trials self.total_reward = 0 self.n_k = len(variants) self.shape = (self.n_k, n_trials) def run(self, agent): """Run the simulation with the agent. agent must be a class with choose_k and update methods.""" for i in range(self.n_trials): # agent makes a choice x_chosen = agent.choose_k() # Environment returns reward reward = np.random.binomial(1, p=self.payouts[x_chosen]) # agent learns of reward agent.reward = reward # agent updates parameters based on the data agent.update() self.total_reward += reward agent.collect_data() return self.total_reward</span></span></code> </pre> <br>  Les points sont distribu√©s binomialement avec une probabilit√© p d√©pendant du nombre de l'action (tout comme ils pourraient √™tre distribu√©s en continu, l'essence n'aurait pas chang√©).  Je d√©finirai √©galement la classe BaseSampler - elle est n√©cessaire uniquement pour stocker les journaux et divers attributs. <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">BaseSampler</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, env, n_samples=None, n_learning=None, e=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.05</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> self.env = env self.shape = (env.n_k, n_samples) self.variants = env.variants self.n_trials = env.n_trials self.payouts = env.payouts self.ad_i = np.zeros(env.n_trials) self.r_i = np.zeros(env.n_trials) self.thetas = np.zeros(self.n_trials) self.regret_i = np.zeros(env.n_trials) self.thetaregret = np.zeros(self.n_trials) self.a = np.ones(env.n_k) self.b = np.ones(env.n_k) self.theta = np.zeros(env.n_k) self.data = <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> self.reward = <span class="hljs-number"><span class="hljs-number">0</span></span> self.total_reward = <span class="hljs-number"><span class="hljs-number">0</span></span> self.k = <span class="hljs-number"><span class="hljs-number">0</span></span> self.i = <span class="hljs-number"><span class="hljs-number">0</span></span> self.n_samples = n_samples self.n_learning = n_learning self.e = e self.ep = np.random.uniform(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, size=env.n_trials) self.exploit = (<span class="hljs-number"><span class="hljs-number">1</span></span> - e) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">collect_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.data = pd.DataFrame(dict(ad=self.ad_i, reward=self.r_i, regret=self.regret_i))</code> </pre> <br>  Ci-dessous, nous d√©finissons 10 options et r√©cup√©ration pour chacune.  La meilleure option est l'option 9 avec un retour sur investissement de 0,11%. <br><br><pre> <code class="python hljs">variants = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>] payouts = [<span class="hljs-number"><span class="hljs-number">0.023</span></span>, <span class="hljs-number"><span class="hljs-number">0.03</span></span>, <span class="hljs-number"><span class="hljs-number">0.029</span></span>, <span class="hljs-number"><span class="hljs-number">0.001</span></span>, <span class="hljs-number"><span class="hljs-number">0.05</span></span>, <span class="hljs-number"><span class="hljs-number">0.06</span></span>, <span class="hljs-number"><span class="hljs-number">0.0234</span></span>, <span class="hljs-number"><span class="hljs-number">0.035</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, <span class="hljs-number"><span class="hljs-number">0.11</span></span>]</code> </pre> <br>  Afin d'avoir quelque chose √† construire, nous d√©finissons √©galement la classe RandomSampler.  Cette classe est n√©cessaire comme mod√®le de base.  Il choisit simplement au hasard une option √† chaque tentative et ne met pas √† jour ses param√®tres. <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">RandomSampler</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(BaseSampler)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, env)</span></span></span><span class="hljs-function">:</span></span> super().__init__(env) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">choose_k</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.k = np.random.choice(self.variants) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.k <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">update</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># nothing to update #self.thetaregret[self.i] = self.thetaregret[self.i] #self.regret_i[self.i] = np.max(self.thetaregret) - self.theta[self.k] #self.thetas[self.i] = self.theta[self.k] self.thetaregret[self.i] = np.max(self.theta) - self.theta[self.k] self.a[self.k] += self.reward self.b[self.k] += 1 self.theta = self.a/self.b self.ad_i[self.i] = self.k self.r_i[self.i] = self.reward self.i += 1</span></span></code> </pre> <br><p>  D'autres mod√®les ont la structure suivante.  Tous ont des m√©thodes choose_k et update.  choose_k impl√©mente la m√©thode par laquelle l'agent s√©lectionne une option.  update met √† jour les param√®tres de l'agent - cette m√©thode caract√©rise comment la capacit√© de l'agent √† choisir l'option change (avec RandomSampler, cette capacit√© ne change en rien).  Nous ex√©cutons l'agent dans l'environnement en utilisant le mod√®le suivant. </p><br><pre> <code class="python hljs">en0 = Environment(machines, payouts, n_trials=<span class="hljs-number"><span class="hljs-number">10000</span></span>) rs = RandomSampler(env=en0) en0.run(agent=rs)</code> </pre> <br><p>  L'essence de l'algorithme epsilon-greedy est la suivante. <br><br></p><ol><li>  S√©lectionnez al√©atoirement k pour n tentatives. </li><li>  √Ä chaque tentative, pour chaque option, √©valuez les gains. </li><li>  Apr√®s toutes les n tentatives: </li><li>  Avec la probabilit√© 1 - <i>e</i> choisissez k avec le gain le plus √©lev√©; </li><li>  Avec la probabilit√© <i>e,</i> choisissez K au hasard. </li></ol><br>  Code Epsilon-gourmand: <br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">eGreedy</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(BaseSampler)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, env, n_learning, e)</span></span></span><span class="hljs-function">:</span></span> super().__init__(env, n_learning, e) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">choose_k</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># e% of the time take a random draw from machines # random k for n learning trials, then the machine with highest theta self.k = np.random.choice(self.variants) if self.i &lt; self.n_learning else np.argmax(self.theta) # with 1 - e probability take a random sample (explore) otherwise exploit self.k = np.random.choice(self.variants) if self.ep[self.i] &gt; self.exploit else self.k return self.k # every 100 trials update the successes # update the count of successes for the chosen machine def update(self): # update the probability of payout for each machine self.a[self.k] += self.reward self.b[self.k] += 1 self.theta = self.a/self.b #self.total_reward += self.reward #self.regret_i[self.i] = np.max(self.theta) - self.theta[self.k] #self.thetaregret[self.i] = self.thetaregret[self.i] self.thetas[self.i] = self.theta[self.k] self.thetaregret[self.i] = np.max(self.thetas) - self.theta[self.k] self.ad_i[self.i] = self.k self.r_i[self.i] = self.reward self.i += 1</span></span></code> </pre> <br><br><p>  Ci-dessous sur le graphique, vous pouvez voir les r√©sultats d'un √©chantillonnage purement al√©atoire, c'est-√†-dire qu'il n'y a pas de mod√®le ici.  Le graphique montre quel choix l'algorithme a fait √† chaque tentative, s'il y a eu 10 000 tentatives.  L'algorithme essaie seulement, mais n'apprend pas.  Au total, il a marqu√© 418 points. <br> <a href=""><img src="https://habrastorage.org/webt/sn/ql/2r/snql2roqbdiruuskxsathithz8i.jpeg"></a> </p><br><p>  Voyons comment l'algorithme epsilon-greedy se comporte dans le m√™me environnement.  Ex√©cutez l'algorithme pour 10 000 tentatives avec <i>e</i> = 0,1 et n_learning = 500 (l'agent essaie simplement les 500 premi√®res tentatives, puis il essaie avec la probabilit√© <i>e</i> = 0,1).  √âvaluons l'algorithme en fonction du nombre total de points qu'il marque dans l'environnement. </p><br><pre> <code class="python hljs">en1 = Environment(machines, payouts, n_trials) eg = eGreedy(env=en1, n_learning=<span class="hljs-number"><span class="hljs-number">500</span></span>, e=<span class="hljs-number"><span class="hljs-number">0.1</span></span>) en1.run(agent=eg)</code> </pre> <br> <a href=""><img src="https://habrastorage.org/webt/-f/zf/14/-fzf14djbqtdl5-0vyrapuhcp5c.jpeg"></a> <br><p>  L'algorithme gourmand d'Epsilon a marqu√© 788 points, pr√®s de 2 fois mieux que l'algorithme al√©atoire - super!  Le deuxi√®me graphique explique assez bien cet algorithme.  Nous voyons que pour les 500 premi√®res √©tapes, les actions sont r√©parties approximativement √©galement et K est choisi au hasard.  Cependant, il commence alors √† exploiter fortement l'option 5 - c'est une option assez forte, mais pas la meilleure.  Nous constatons √©galement que l'agent s√©lectionne toujours au hasard 10% du temps. </p><br><p>  C'est assez cool - nous n'avons √©crit que quelques lignes de code, et maintenant nous avons d√©j√† un algorithme assez puissant qui peut explorer l'espace des options et prendre des d√©cisions presque optimales.  En revanche, l'algorithme n'a pas trouv√© la meilleure option.  Oui, nous pouvons augmenter le nombre d'√©tapes d'apprentissage, mais de cette fa√ßon, nous passerons encore plus de temps sur une recherche al√©atoire, ce qui aggravera encore le r√©sultat final.  De plus, l'al√©atoire est int√©gr√© √† ce processus par d√©faut - le meilleur algorithme peut ne pas √™tre trouv√©. </p><br><p>  Plus tard, je lancerai chacun des algorithmes plusieurs fois afin de pouvoir les comparer les uns par rapport aux autres.  Mais pour l'instant, examinons l'√©chantillonnage de Thompson et testons-le dans le m√™me environnement. </p><br><h2>  √âchantillonnage de Thompson </h2><br><p>  L'√©chantillonnage de Thompson est fondamentalement diff√©rent de l'algorithme epsilon-greedy par trois points principaux: <br><br></p><ol><li>  Ce n'est pas gourmand. </li><li>  Il fait des tentatives d'une mani√®re plus sophistiqu√©e. </li><li>  C'est bay√©sien. </li></ol><br>  Le point principal est le paragraphe 3, les paragraphes 1 et 2 en d√©coulent. <br><p>  L'essence de l'algorithme est la suivante: <br><br></p><ol><li>  D√©finissez la distribution b√™ta initiale entre 0 et 1 pour le retour sur investissement de chaque option. </li><li>  √âchantillonnez les options de cette distribution, s√©lectionnez le param√®tre Th√™ta maximum. </li><li>  Choisissez l'option k associ√©e √† la plus grande th√™ta. </li><li>  Voir combien de points ont √©t√© marqu√©s, mettre √† jour les param√®tres de distribution. </li></ol><br>  En savoir plus sur la distribution b√™ta <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . <br>  Et √† propos de son utilisation en Python - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . <br><p>  Code d'algorithme: <br><br></p><pre> <code class="python hljs"> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ThompsonSampler</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(BaseSampler)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, env)</span></span></span><span class="hljs-function">:</span></span> super().__init__(env) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">choose_k</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># sample from posterior (this is the thompson sampling approach) # this leads to more exploration because machines with &gt; uncertainty can then be selected as the machine self.theta = np.random.beta(self.a, self.b) # select machine with highest posterior p of payout self.k = self.variants[np.argmax(self.theta)] #self.k = np.argmax(self.a/(self.a + self.b)) return self.k def update(self): #update dist (a, b) = (a, b) + (r, 1 - r) self.a[self.k] += self.reward self.b[self.k] += 1 - self.reward # ie only increment b when it's a swing and a miss. 1 - 0 = 1, 1 - 1 = 0 #self.thetaregret[self.i] = self.thetaregret[self.i] #self.regret_i[self.i] = np.max(self.theta) - self.theta[self.k] self.thetas[self.i] = self.theta[self.k] self.thetaregret[self.i] = np.max(self.thetas) - self.theta[self.k] self.ad_i[self.i] = self.k self.r_i[self.i] = self.reward self.i += 1</span></span></code> </pre> <br>  La notation formelle de l'algorithme ressemble √† ceci. <br> <a href=""><img src="https://habrastorage.org/webt/5f/n6/xe/5fn6xew2i7v1jjh_10h9jqkjdzu.png"></a> <br><p>  Programmons cet algorithme.  Comme d'autres agents, ThompsonSampler h√©rite de BaseSampler et d√©finit ses propres m√©thodes choose_k et update.  Lancez maintenant notre nouvel agent. </p><br><pre> <code class="python hljs"> en2 = Environment(machines, payouts, n_trials) tsa = ThompsonSampler(env=en2) en2.run(agent=tsa)</code> </pre> <br> <a href=""><img src="https://habrastorage.org/webt/ml/kj/1p/mlkj1pvs8xnpxtkqmehmm_3xhgo.jpeg"></a> <br><p>  Comme vous pouvez le voir, il a marqu√© plus que l'algorithme epsilon-greedy.  Super!  Regardons le graphique de la s√©lection des tentatives.  On y voit deux choses int√©ressantes.  Premi√®rement, l'agent a correctement d√©couvert la meilleure option (option 9) et l'a utilis√©e au maximum.  Deuxi√®mement, l'agent a utilis√© d'autres options, mais de mani√®re plus d√©licate - apr√®s environ 1000 tentatives, l'agent, en plus de l'option principale, a principalement utilis√© les options les plus puissantes parmi les autres.  En d'autres termes, il n'a pas choisi au hasard, mais de mani√®re plus comp√©tente. </p><br><p>  Pourquoi √ßa marche?  C'est simple - l'incertitude dans la distribution post√©rieure des avantages attendus pour chaque option signifie que chaque option est s√©lectionn√©e avec une probabilit√© approximativement proportionnelle √† sa forme, d√©termin√©e par les param√®tres alpha et b√™ta.  En d'autres termes, √† chaque tentative, l'√©chantillonnage de Thompson d√©clenche l'option en fonction de la probabilit√© a posteriori qu'il pr√©sente l'avantage maximal.  En gros, √† partir de la distribution des informations sur l'incertitude, l'agent d√©cide quand examiner l'environnement et quand utiliser les informations.  Par exemple, une option faible avec une incertitude post√©rieure √©lev√©e peut payer le plus pour cette tentative.  Mais pour la plupart des tentatives, plus sa distribution post√©rieure est forte, plus sa moyenne et son √©cart-type sont faibles, et donc, plus les chances de le choisir sont grandes. </p><br><p>  Autre propri√©t√© remarquable de l'algorithme de Thompson: puisqu'il est bay√©sien, nous pouvons estimer l'incertitude de l'estimation de la rentabilit√© de chaque option en utilisant ses param√®tres.  Le graphique ci-dessous montre les distributions post√©rieures en 6 points diff√©rents et en 20 000 tentatives.  Vous voyez comment les distributions commencent progressivement √† converger vers l'option avec le meilleur retour sur investissement. </p><br> <a href=""><img src="https://habrastorage.org/webt/bb/ka/fb/bbkafb4nv1pajwkygxy2brtmowy.jpeg"></a> <br><p>  Comparez maintenant les 3 agents dans 100 simulations.  1 simulation est un lancement d'agent sur 10 000 tentatives. </p><br> <a href=""><img src="https://habrastorage.org/webt/j6/v1/sm/j6v1smcrwkwyhlo27ffhly13pwk.jpeg"></a> <br><p>  Comme vous pouvez le voir sur le graphique, la strat√©gie epsilon-greedy et l'√©chantillonnage de Thompson fonctionnent beaucoup mieux que l'√©chantillonnage al√©atoire.  Vous serez peut-√™tre surpris que la strat√©gie epsilon-greedy et l'√©chantillonnage de Thompson soient en fait comparables en termes de performances.  La strat√©gie gourmande d'Epsilon peut √™tre tr√®s efficace, mais elle est plus risqu√©e, car elle peut rester bloqu√©e sur une option sous-optimale - cela peut √™tre vu dans les √©checs du graphique.  Mais l'√©chantillonnage de Thompson ne peut pas, car il fait le choix dans l'espace des options d'une mani√®re plus complexe. </p><br><h2>  Regret </h2><br><p>  Une autre fa√ßon d'√©valuer l'efficacit√© de l'algorithme est d'√©valuer le regret.  En gros, plus elle est petite par rapport aux actions d√©j√† entreprises, mieux c'est.  Voici un graphique du regret total et du regret de l'erreur.  Encore une fois - moins il y a de regrets, mieux c'est. </p><br> <a href=""><img src="https://habrastorage.org/webt/8p/kd/o3/8pkdo3bilrde28bwsimdnbesqwg.jpeg"></a> <br><p>  Sur le graphique sup√©rieur, nous voyons le regret total, et sur le regret inf√©rieur la tentative.  Comme le montrent les graphiques, l'√©chantillonnage de Thompson converge vers un regret minimal beaucoup plus rapidement que la strat√©gie epsilon-cupide.  Et il converge vers un niveau inf√©rieur.  Avec l'√©chantillonnage de Thompson, l'agent regrette moins car il peut mieux d√©tecter la meilleure option et mieux essayer les options les plus prometteuses.L'√©chantillonnage de Thompson est donc particuli√®rement adapt√© aux cas d'utilisation plus avanc√©s, tels que les mod√®les statistiques ou les r√©seaux de neurones pour choisir k. </p><br><h2>  Conclusions </h2><br><p>  Il s'agit d'un poste technique assez long.  Pour r√©sumer, nous pouvons utiliser des m√©thodes d'√©chantillonnage assez sophistiqu√©es si nous avons de nombreuses options que nous voulons tester en temps r√©el.  L'une des tr√®s bonnes caract√©ristiques de l'√©chantillonnage de Thompson est qu'il √©quilibre l'utilisation et l'exploration de mani√®re assez d√©licate.  Autrement dit, nous pouvons le laisser optimiser la distribution des options de solution en temps r√©el.  Ce sont des algorithmes sympas, et ils devraient √™tre plus utiles pour une entreprise que les tests A / B. </p><br><p>  <b>Important!</b>  <b>L'√©chantillonnage de Thompson ne signifie pas que vous n'avez pas besoin de faire des tests A / B.</b>  <b>Habituellement, ils trouvent d'abord les meilleures options avec son aide, puis font d√©j√† des tests A / B sur eux.</b> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr425619/">https://habr.com/ru/post/fr425619/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr425605/index.html">Acceptation des paiements d'une carte sans jur. visages sur Yandex.Money</a></li>
<li><a href="../fr425607/index.html">Identifiez la fraude √† l'aide de l'ensemble de donn√©es Enron. Partie 2, trouver le meilleur mod√®le</a></li>
<li><a href="../fr425609/index.html">Th√©orie des jeux: prise de d√©cision avec exemples √† Kotlin</a></li>
<li><a href="../fr425611/index.html">Architecture frontale de niveau sup√©rieur. Conf√©rence Yandex</a></li>
<li><a href="../fr425613/index.html">Comment j'ai combin√© les donn√©es du plugin Tempo pour Jira Server et Jira Cloud et les ai migr√©es vers Jira Cloud</a></li>
<li><a href="../fr425621/index.html">Une entreprise qui utilise le dioxyde de carbone atmosph√©rique lance la production de m√©thane</a></li>
<li><a href="../fr425623/index.html">Visite photo de coworking ¬´Key¬ª</a></li>
<li><a href="../fr425625/index.html">D√©pens√©, ou pourquoi les localisateurs traduisent mal les jeux</a></li>
<li><a href="../fr425627/index.html">IaaS pour le d√©veloppement de services: qui et pourquoi sont pass√©s √† une infrastructure virtuelle</a></li>
<li><a href="../fr425629/index.html">Comment nous avons cr√©√© un jeu de soci√©t√© avec t√©l√©commande</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>