<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üö¶ üë©üèº‚Äçüíª üë©üèæ‚Äçüéì Mensajer√≠a diferida in√∫til no bloqueada en MPI: an√°lisis ligero y tutorial para aquellos que est√°n un poco "en el tema" üéí üôç üôáüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="M√°s recientemente, tuve que resolver otra tarea trivial de capacitaci√≥n de mi maestro. Sin embargo, al resolverlo, logr√© llamar la atenci√≥n sobre cosa...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Mensajer√≠a diferida in√∫til no bloqueada en MPI: an√°lisis ligero y tutorial para aquellos que est√°n un poco "en el tema"</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/427219/">  M√°s recientemente, tuve que resolver otra tarea trivial de capacitaci√≥n de mi maestro.  Sin embargo, al resolverlo, logr√© llamar la atenci√≥n sobre cosas en las que no hab√≠a pensado antes, y es posible que tampoco lo hayas pensado.  Es m√°s probable que este art√≠culo sea √∫til para los estudiantes y para todos los que comienzan su viaje al mundo de la programaci√≥n paralela utilizando MPI. <br><br><img src="https://habrastorage.org/webt/v0/ik/2-/v0ik2-rxhe1gexlrsumqpwhslbu.jpeg"><br><br><h2>  Nuestro "Dado:" </h2><br>  Entonces, la esencia de nuestra tarea esencialmente computacional es comparar cu√°ntas veces un programa que usa transferencias punto a punto retardadas sin bloqueo es m√°s r√°pido que el que usa transferencias punto a punto de bloqueo.  Realizaremos mediciones para matrices de entrada de 64, 256, 1024, 4096, 8192, 16384, 65536, 262144, 1048576, 4194304, 16777216, 33554432 elementos.  Por defecto, se propone resolverlo mediante cuatro procesos.  Y aqu√≠, de hecho, es lo que consideraremos: <br><br><a name="habracut"></a><img src="https://habrastorage.org/webt/rt/vc/vo/rtvcvob3gfonidkax7qtskxfbc0.png"><cut></cut><br><br>  En la salida, deber√≠amos obtener tres vectores: Y1, Y2 e Y3, que el proceso cero recopilar√°.  Probar√© todo esto en mi sistema basado en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">un procesador Intel</a> con 16 GB de RAM.  Para desarrollar programas, utilizaremos la implementaci√≥n del est√°ndar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MPI de Microsoft versi√≥n 9.0.1</a> (en el momento de la redacci√≥n, es relevante), Visual Studio Community 2017 y no Fortran. <br><br><h2>  Materiel </h2><br>  No me gustar√≠a describir en detalle c√≥mo funcionan las funciones MPI que se utilizar√°n, siempre puede ir a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ver la documentaci√≥n para esto</a> , por lo que solo dar√© una breve descripci√≥n de lo que usaremos. <br><br><h4>  Bloqueo de intercambio </h4><br>  <b>Para bloquear la mensajer√≠a punto a punto, utilizaremos las funciones:</b> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MPI_Send</a> : implementa el env√≠o de mensajes de bloqueo, es decir  despu√©s de llamar a la funci√≥n, el proceso se bloquea hasta que los datos que se le env√≠an se escriben desde su memoria en el b√∫fer interno del sistema MPI, despu√©s de lo cual el proceso contin√∫a trabajando m√°s; <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MPI_Recv</a> : realiza la recepci√≥n de mensajes de bloqueo, es decir  Despu√©s de llamar a la funci√≥n, el proceso se bloquea hasta que lleguen los datos del proceso de env√≠o y hasta que el entorno MPI escriba completamente estos datos en el b√∫fer del proceso de recepci√≥n. <br><br><h4>  Intercambio diferido sin bloqueo </h4><br>  <b>Para mensajes diferidos de punto a punto sin bloqueo, utilizaremos las funciones:</b> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MPI_Send_init</a> : en segundo plano, prepara el entorno para enviar datos que suceder√°n en el futuro y sin bloqueos; <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MPI_Recv_init</a> : esta funci√≥n funciona de manera similar a la anterior, solo que esta vez para recibir datos; <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MPI_Start</a> : inicia el proceso de recibir o transmitir un mensaje, tambi√©n se ejecuta en segundo plano a.k.a.  sin bloqueo <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MPI_Wait</a> : se utiliza para verificar y, si es necesario, esperar a que finalice el env√≠o o la recepci√≥n de un mensaje, pero simplemente bloquea el proceso si es necesario (si los datos no se "env√≠an" o "no se reciben").  Por ejemplo, un proceso quiere usar datos que a√∫n no lo hayan alcanzado; no es bueno, por lo tanto, insertamos MPI_Wait delante del lugar donde necesitar√° estos datos (los insertamos incluso si simplemente existe el riesgo de corrupci√≥n de datos).  Otro ejemplo, el proceso comenz√≥ la transferencia de datos en segundo plano, y despu√©s de comenzar la transferencia de datos, inmediatamente comenz√≥ a cambiar estos datos de alguna manera, no es bueno, por lo que insertamos MPI_Wait delante del lugar en el programa donde comienza a cambiar estos datos (aqu√≠ tambi√©n los insertamos incluso si simplemente existe el riesgo de corrupci√≥n de datos). <br><br>  Por lo tanto, <i>sem√°nticamente la</i> secuencia de llamadas con un intercambio diferido sin bloqueo es la siguiente: <br><br><ol><li>  MPI_Send_init / MPI_Recv_init: preparaci√≥n del entorno para recibir o transmitir </li><li>  MPI_Start - comienza el proceso de recepci√≥n / transmisi√≥n </li><li>  MPI_Wait: llamamos a riesgo de da√±os (incluyendo "comprensi√≥n" y "recepci√≥n insuficiente") de datos transmitidos o recibidos </li></ol><br>  Tambi√©n utilic√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MPI_Startall</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MPI_Waitall</a> en mis programas de prueba, su significado es b√°sicamente el mismo que MPI_Start y MPI_Wait, respectivamente, solo que operan en varios paquetes y / o transmisiones.  Pero esta no es la lista completa de las funciones de inicio y espera, hay varias funciones m√°s para verificar la integridad de las operaciones. <br><br><h2>  Arquitectura entre procesos </h2><br>  Para mayor claridad, construimos un gr√°fico para realizar c√°lculos mediante cuatro procesos.  En este caso, uno deber√≠a tratar de distribuir todas las operaciones aritm√©ticas vectoriales de manera relativamente uniforme sobre los procesos.  Esto es lo que obtuve: <br><br><img src="https://habrastorage.org/webt/bq/_b/q4/bq_bq43yrgkgjayi8p5uy0g9ckk.png"><br><br>  Ver estos arreglos T0-T2?  Estos son buffers para almacenar resultados intermedios de operaciones.  Adem√°s, en un gr√°fico al enviar mensajes de un proceso a otro, al comienzo de la flecha se encuentra el nombre de la matriz cuyos datos se transmiten, y al final de la flecha se encuentra la matriz que recibe estos datos. <br><br>  Bueno, cuando finalmente respondimos las preguntas: <br><br><ol><li>  ¬øQu√© tipo de problema estamos resolviendo? </li><li>  ¬øQu√© herramientas usaremos para resolverlo? </li><li>  ¬øC√≥mo lo resolveremos? </li></ol><br>  Solo queda resolverlo ... <br><br><h2>  Nuestra "soluci√≥n" </h2><br>  A continuaci√≥n, presentar√© los c√≥digos de los dos programas discutidos anteriormente, pero para empezar, dar√© algunas explicaciones m√°s de qu√© y c√≥mo. <br><br>  Saqu√© todas las operaciones aritm√©ticas de vectores en procedimientos separados (add, sub, mul, div) para aumentar la legibilidad del c√≥digo.  Todas las matrices de entrada se inicializan de acuerdo con las f√≥rmulas que indiqu√© <i>casi</i> al azar.  Dado que el proceso cero recopila los resultados del trabajo de todos los dem√°s procesos, por lo tanto, funciona m√°s tiempo, por lo tanto, es l√≥gico considerar el tiempo de su trabajo igual al tiempo de ejecuci√≥n del programa (como recordamos, estamos interesados ‚Äã‚Äãen: aritm√©tica + mensajer√≠a) en el primer y segundo casos.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Mediremos</a> los intervalos de tiempo usando la funci√≥n <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MPI_Wtime,</a> y al mismo tiempo decid√≠ mostrar qu√© resoluci√≥n de los relojes tengo all√≠ usando <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MPI_Wtick</a> (en alg√∫n lugar de mi alma espero que encajen en mi TSC invariante, en este caso, incluso estoy listo para perdonarles el error asociado con el momento en que la funci√≥n se llamaba MPI_Wtime).  Entonces, reuniremos todo lo que escrib√≠ anteriormente y de acuerdo con el gr√°fico finalmente desarrollaremos estos programas (y depuraci√≥n, por supuesto). <br><br><hr><br>  A qui√©n le importa ver el c√≥digo: <br><br><div class="spoiler">  <b class="spoiler_title">Programa con bloqueo de transferencias de datos</b> <div class="spoiler_text"><pre><code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"pch.h"</span></span></span><span class="hljs-meta"> #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;iostream&gt; #include &lt;iomanip&gt; #include &lt;fstream&gt; #include &lt;mpi.h&gt; using namespace std; void add(double *A, double *B, double *C, int n); void sub(double *A, double *B, double *C, int n); void mul(double *A, double *B, double *C, int n); void div(double *A, double *B, double *C, int n); int main(int argc, char **argv) { if (argc &lt; 2) { return 1; } int n = atoi(argv[1]); int rank; double start_time, end_time; MPI_Status status; double *A = new double[n]; double *B = new double[n]; double *C = new double[n]; double *D = new double[n]; double *E = new double[n]; double *G = new double[n]; double *T0 = new double[n]; double *T1 = new double[n]; double *T2 = new double[n]; for (int i = 0; i &lt; n; i++) { A[i] = double (2 * i + 1); B[i] = double(2 * i); C[i] = double(0.003 * (i + 1)); D[i] = A[i] * 0.001; E[i] = B[i]; G[i] = C[i]; } cout.setf(ios::fixed); cout &lt;&lt; fixed &lt;&lt; setprecision(9); MPI_Init(&amp;argc, &amp;argv); MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank); if (rank == 0) { start_time = MPI_Wtime(); sub(A, B, T0, n); MPI_Send(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD); MPI_Send(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD); div(T0, G, T1, n); MPI_Recv(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;status); add(T1, T2, T0, n); mul(T0, T1, T2, n); MPI_Recv(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;status); MPI_Send(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD); add(T0, T2, T1, n); MPI_Recv(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;status); MPI_Recv(T2, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;status); end_time = MPI_Wtime(); cout &lt;&lt; "Clock resolution: " &lt;&lt; MPI_Wtick() &lt;&lt; " secs" &lt;&lt; endl; cout &lt;&lt; "Thread " &lt;&lt; rank &lt;&lt; " execution time: " &lt;&lt; end_time - start_time &lt;&lt; endl; } if (rank == 1) { add(C, C, T0, n); MPI_Recv(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;status); MPI_Send(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD); mul(T1, G, T2, n); add(T2, C, T0, n); MPI_Recv(T1, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;status); MPI_Send(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD); sub(T1, T0, T2, n); MPI_Recv(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;status); add(T0, T2, T1, n); MPI_Send(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD); } if (rank == 2) { mul(C, C, T0, n); MPI_Recv(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;status); MPI_Recv(T2, n, MPI_DOUBLE, 3, 0, MPI_COMM_WORLD, &amp;status); MPI_Send(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD); MPI_Send(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD); add(T1, T2, T0, n); mul(T0, G, T1, n); MPI_Recv(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;status); mul(T1, T2, T0, n); MPI_Recv(T1, n, MPI_DOUBLE, 3, 0, MPI_COMM_WORLD, &amp;status); mul(T0, T1, T2, n); MPI_Send(T2, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD); } if (rank == 3) { mul(E, D, T0, n); MPI_Send(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD); sub(T0, B, T1, n); mul(T1, T1, T2, n); sub(T1, G, T0, n); mul(T0, T2, T1, n); MPI_Send(T1, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD); } MPI_Finalize(); delete[] A; delete[] B; delete[] C; delete[] D; delete[] E; delete[] G; delete[] T0; delete[] T1; delete[] T2; return 0; } void add(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] + B[i]; } } void sub(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] - B[i]; } } void mul(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] * B[i]; } } void div(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] / B[i]; } }</span></span></span></span></code> </pre> </div></div><br><div class="spoiler">  <b class="spoiler_title">Programa con transferencias de datos no bloqueadas diferidas</b> <div class="spoiler_text"><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"pch.h"</span></span></span><span class="hljs-meta"> #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;iostream&gt; #include &lt;iomanip&gt; #include &lt;fstream&gt; #include &lt;mpi.h&gt; using namespace std; void add(double *A, double *B, double *C, int n); void sub(double *A, double *B, double *C, int n); void mul(double *A, double *B, double *C, int n); void div(double *A, double *B, double *C, int n); int main(int argc, char **argv) { if (argc &lt; 2) { return 1; } int n = atoi(argv[1]); int rank; double start_time, end_time; MPI_Request request[7]; MPI_Status statuses[4]; double *A = new double[n]; double *B = new double[n]; double *C = new double[n]; double *D = new double[n]; double *E = new double[n]; double *G = new double[n]; double *T0 = new double[n]; double *T1 = new double[n]; double *T2 = new double[n]; for (int i = 0; i &lt; n; i++) { A[i] = double(2 * i + 1); B[i] = double(2 * i); C[i] = double(0.003 * (i + 1)); D[i] = A[i] * 0.001; E[i] = B[i]; G[i] = C[i]; } cout.setf(ios::fixed); cout &lt;&lt; fixed &lt;&lt; setprecision(9); MPI_Init(&amp;argc, &amp;argv); MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank); if (rank == 0) { start_time = MPI_Wtime(); MPI_Send_init(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[0]);// MPI_Send_init(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[1]);// MPI_Recv_init(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[2]);// MPI_Recv_init(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[3]);// MPI_Send_init(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[4]);// MPI_Recv_init(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[5]);// MPI_Recv_init(T2, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[6]);// MPI_Start(&amp;request[2]); sub(A, B, T0, n); MPI_Startall(2, &amp;request[0]); div(T0, G, T1, n); MPI_Waitall(3, &amp;request[0], statuses); add(T1, T2, T0, n); mul(T0, T1, T2, n); MPI_Startall(2, &amp;request[3]); MPI_Wait(&amp;request[3], &amp;statuses[0]); add(T0, T2, T1, n); MPI_Startall(2, &amp;request[5]); MPI_Wait(&amp;request[4], &amp;statuses[0]); MPI_Waitall(2, &amp;request[5], statuses); end_time = MPI_Wtime(); cout &lt;&lt; "Clock resolution: " &lt;&lt; MPI_Wtick() &lt;&lt; " secs" &lt;&lt; endl; cout &lt;&lt; "Thread " &lt;&lt; rank &lt;&lt; " execution time: " &lt;&lt; end_time - start_time &lt;&lt; endl; } if (rank == 1) { MPI_Recv_init(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[0]);// MPI_Send_init(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[1]);// MPI_Recv_init(T1, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[2]);// MPI_Send_init(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[3]);// MPI_Recv_init(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[4]);// MPI_Send_init(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[5]);// MPI_Start(&amp;request[0]); add(C, C, T0, n); MPI_Start(&amp;request[1]); MPI_Wait(&amp;request[0], &amp;statuses[0]); mul(T1, G, T2, n); MPI_Start(&amp;request[2]); MPI_Wait(&amp;request[1], &amp;statuses[0]); add(T2, C, T0, n); MPI_Start(&amp;request[3]); MPI_Wait(&amp;request[2], &amp;statuses[0]); sub(T1, T0, T2, n); MPI_Wait(&amp;request[3], &amp;statuses[0]); MPI_Start(&amp;request[4]); MPI_Wait(&amp;request[4], &amp;statuses[0]); add(T0, T2, T1, n); MPI_Start(&amp;request[5]); MPI_Wait(&amp;request[5], &amp;statuses[0]); } if (rank == 2) { MPI_Recv_init(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[0]);// MPI_Recv_init(T2, n, MPI_DOUBLE, 3, 0, MPI_COMM_WORLD, &amp;request[1]);// MPI_Send_init(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[2]);// MPI_Send_init(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[3]);// MPI_Recv_init(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[4]);// MPI_Recv_init(T1, n, MPI_DOUBLE, 3, 0, MPI_COMM_WORLD, &amp;request[5]);// MPI_Send_init(T2, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[6]);// MPI_Startall(2, &amp;request[0]); mul(C, C, T0, n); MPI_Startall(2, &amp;request[2]); MPI_Waitall(4, &amp;request[0], statuses); add(T1, T2, T0, n); MPI_Start(&amp;request[4]); mul(T0, G, T1, n); MPI_Wait(&amp;request[4], &amp;statuses[0]); mul(T1, T2, T0, n); MPI_Start(&amp;request[5]); MPI_Wait(&amp;request[5], &amp;statuses[0]); mul(T0, T1, T2, n); MPI_Start(&amp;request[6]); MPI_Wait(&amp;request[6], &amp;statuses[0]); } if (rank == 3) { MPI_Send_init(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[0]); MPI_Send_init(T1, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[1]); mul(E, D, T0, n); MPI_Start(&amp;request[0]); sub(T0, B, T1, n); mul(T1, T1, T2, n); MPI_Wait(&amp;request[0], &amp;statuses[0]); sub(T1, G, T0, n); mul(T0, T2, T1, n); MPI_Start(&amp;request[1]); MPI_Wait(&amp;request[1], &amp;statuses[0]); } MPI_Finalize(); delete[] A; delete[] B; delete[] C; delete[] D; delete[] E; delete[] G; delete[] T0; delete[] T1; delete[] T2; return 0; } void add(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] + B[i]; } } void sub(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] - B[i]; } } void mul(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] * B[i]; } } void div(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] / B[i]; } }</span></span></span></span></code> </pre></div></div><br><hr><br><h2>  Pruebas y an√°lisis. </h2><br>  Ejecutemos nuestros programas para matrices de diferentes tama√±os y veamos qu√© sucede.  Los resultados de la prueba se resumen en la tabla, en la √∫ltima columna de la cual calculamos y escribimos el coeficiente de aceleraci√≥n, que definimos de la siguiente manera: K <sub>accele</sub> = T <sub>ex.</sub>  <sub>sin bloque.</sub>  / T <sub>bloque.</sub> <br><br><img src="https://habrastorage.org/webt/ba/hg/zv/bahgzvsgz67vnkfsji-swsyy_cg.png"><br><br>  Si observa esta tabla con un poco m√°s de cuidado de lo habitual, notar√° que con un aumento en el n√∫mero de elementos procesados, el coeficiente de aceleraci√≥n disminuye de alguna manera de esta manera: <br><br><img src="https://habrastorage.org/webt/qq/to/hv/qqtohvv7azbc05r6x4mwtfbbxfe.png"><br><br>  ¬øIntentamos determinar cu√°l es el problema?  Para hacer esto, propongo escribir un peque√±o programa de prueba que mida el tiempo de cada operaci√≥n aritm√©tica vectorial y reduzca cuidadosamente los resultados a un archivo de texto ordinario. <br><br><hr><br>  Aqu√≠, de hecho, el programa en s√≠: <br><br><div class="spoiler">  <b class="spoiler_title">Medida del tiempo</b> <div class="spoiler_text"><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"pch.h"</span></span></span><span class="hljs-meta"> #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;iostream&gt; #include &lt;iomanip&gt; #include &lt;Windows.h&gt; #include &lt;fstream&gt; using namespace std; void add(double *A, double *B, double *C, int n); void sub(double *A, double *B, double *C, int n); void mul(double *A, double *B, double *C, int n); void div(double *A, double *B, double *C, int n); int main() { struct res { double add; double sub; double mul; double div; }; int i, j, k, n, loop; LARGE_INTEGER start_time, end_time, freq; ofstream fout("test_measuring.txt"); int N[12] = { 64, 256, 1024, 4096, 8192, 16384, 65536, 262144, 1048576, 4194304, 16777216, 33554432 }; SetConsoleOutputCP(1251); cout &lt;&lt; "   loop: "; cin &gt;&gt; loop; fout &lt;&lt; setiosflags(ios::fixed) &lt;&lt; setiosflags(ios::right) &lt;&lt; setprecision(9); fout &lt;&lt; " : " &lt;&lt; loop &lt;&lt; endl; fout &lt;&lt; setw(10) &lt;&lt; "\n " &lt;&lt; setw(30) &lt;&lt; ".   (c)" &lt;&lt; setw(30) &lt;&lt; ".   (c)" &lt;&lt; setw(30) &lt;&lt; ".  (c)" &lt;&lt; setw(30) &lt;&lt; ".   (c)" &lt;&lt; endl; QueryPerformanceFrequency(&amp;freq); cout &lt;&lt; "\n : " &lt;&lt; freq.QuadPart &lt;&lt; " " &lt;&lt; endl; for (k = 0; k &lt; sizeof(N) / sizeof(int); k++) { res output = {}; n = N[k]; double *A = new double[n]; double *B = new double[n]; double *C = new double[n]; for (i = 0; i &lt; n; i++) { A[i] = 2.0 * i; B[i] = 2.0 * i + 1; C[i] = 0; } for (j = 0; j &lt; loop; j++) { QueryPerformanceCounter(&amp;start_time); add(A, B, C, n); QueryPerformanceCounter(&amp;end_time); output.add += double(end_time.QuadPart - start_time.QuadPart) / double(freq.QuadPart); QueryPerformanceCounter(&amp;start_time); sub(A, B, C, n); QueryPerformanceCounter(&amp;end_time); output.sub += double(end_time.QuadPart - start_time.QuadPart) / double(freq.QuadPart); QueryPerformanceCounter(&amp;start_time); mul(A, B, C, n); QueryPerformanceCounter(&amp;end_time); output.mul += double(end_time.QuadPart - start_time.QuadPart) / double(freq.QuadPart); QueryPerformanceCounter(&amp;start_time); div(A, B, C, n); QueryPerformanceCounter(&amp;end_time); output.div += double(end_time.QuadPart - start_time.QuadPart) / double(freq.QuadPart); } fout &lt;&lt; setw(10) &lt;&lt; n &lt;&lt; setw(30) &lt;&lt; output.add / loop &lt;&lt; setw(30) &lt;&lt; output.sub / loop &lt;&lt; setw(30) &lt;&lt; output.mul / loop &lt;&lt; setw(30) &lt;&lt; output.div / loop &lt;&lt; endl; delete[] A; delete[] B; delete[] C; } fout.close(); cout &lt;&lt; endl; system("pause"); return 0; } void add(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] + B[i]; } } void sub(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] - B[i]; } } void mul(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] * B[i]; } } void div(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] / B[i]; } }</span></span></span></span></code> </pre></div></div><br><hr><br>  Al inicio, le pide que ingrese el n√∫mero de ciclos de medici√≥n, prob√© durante 10,000 ciclos.  En la salida, obtenemos el resultado promedio para cada operaci√≥n: <br><br><img src="https://habrastorage.org/webt/iz/0g/bs/iz0gbs8ilynlmbchxtga_0at61s.png"><br><br>  Para medir el tiempo, utilic√© el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">QueryPerformanceCounter de</a> alto nivel.  Recomiendo leer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">estas preguntas frecuentes</a> para que la mayor√≠a de las preguntas sobre la medici√≥n del tiempo con esta funci√≥n desaparezcan por s√≠ mismas.  Seg√∫n mis observaciones, se aferra al TSC (pero en teor√≠a puede que no sea por ello), pero devuelve, seg√∫n la ayuda, el n√∫mero actual de tics del contador.  Pero el hecho es que mi contador f√≠sicamente no puede medir el intervalo de tiempo de 32 ns (vea la primera fila de la tabla de resultados).  Este resultado se debe al hecho de que entre las dos llamadas del QueryPerformanceCounter pasan 0 ticks o 1 ticks. Para la primera fila de la tabla, solo podemos concluir que aproximadamente un tercio de los 10,000 resultados son 1 tick.  <i>Entonces, los datos en esta tabla para 64, 256 e incluso para 1024 elementos son algo aproximados.</i>  Ahora, abramos cualquiera de los programas y calculemos cu√°ntas operaciones totales de cada tipo encuentra, tradicionalmente "distribuiremos" todo seg√∫n la siguiente tabla: <br><br><img src="https://habrastorage.org/webt/et/vo/w2/etvow2fj-vxgtusc9aez__9egxq.png"><br><br>  Finalmente, sabemos el tiempo de cada operaci√≥n aritm√©tica de vectores y cu√°nto es en nuestro programa, tratamos de averiguar cu√°nto tiempo se dedica a estas operaciones en programas paralelos y cu√°nto tiempo se dedica al bloqueo y al intercambio diferido de datos sin bloqueo entre procesos y nuevamente, para mayor claridad, reduciremos esto a tabla: <br><br><img src="https://habrastorage.org/webt/no/k2/-0/nok2-0p1kpw8g1ahveqog4q9lzi.png"><br><br>  Con base en los resultados de los datos obtenidos, construimos un gr√°fico de tres funciones: la primera describe el cambio en el tiempo dedicado a bloquear transferencias entre procesos, a partir del n√∫mero de elementos de la matriz, la segunda describe el cambio en el tiempo dedicado a transferencias diferidas sin bloqueo entre procesos, en la cantidad de elementos de la matriz y la tercera describe el cambio en el tiempo, gastado en operaciones aritm√©ticas, a partir del n√∫mero de elementos de las matrices: <br><br><img src="https://habrastorage.org/webt/4e/w6/5h/4ew65htbb-s3vh7anlo0txafxpg.png"><br><br>  Como ya ha notado, la escala vertical del gr√°fico es logar√≠tmica, es una medida necesaria, porque  la dispersi√≥n de los tiempos es demasiado grande y, en un gr√°fico regular, nada habr√≠a sido visible.  Preste atenci√≥n a la funci√≥n de la dependencia del tiempo dedicado a la aritm√©tica en el n√∫mero de elementos, ya que supera con seguridad las otras dos funciones en aproximadamente 1 mill√≥n de elementos.  El caso es que crece al infinito m√°s r√°pido que sus dos oponentes.  Por lo tanto, con un aumento en el n√∫mero de elementos procesados, el tiempo de ejecuci√≥n de los programas est√° cada vez m√°s determinado por la aritm√©tica en lugar de las transferencias.  Suponga que aumenta el n√∫mero de transferencias entre procesos, conceptualmente solo ver√° que el momento en que la funci√≥n aritm√©tica supere a las otras dos suceder√° m√°s tarde. <br><br><h2>  Resumen </h2><br>  Por lo tanto, si contin√∫a aumentando la longitud de las matrices, llegar√° a la conclusi√≥n de que un programa con transferencias diferidas sin bloqueo ser√° solo un poco m√°s r√°pido que el que usa el intercambio de bloqueo.  Y si dirige la longitud de las matrices al infinito (bueno, o simplemente toma matrices muy largas), entonces el tiempo de funcionamiento de su programa estar√° determinado al 100% por los c√°lculos, y el coeficiente de aceleraci√≥n tender√° a 1 de manera segura. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es427219/">https://habr.com/ru/post/es427219/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es427207/index.html">Los empleados de Rockstar defienden a la compa√±√≠a despu√©s de las cr√≠ticas durante 100 horas de trabajo</a></li>
<li><a href="../es427209/index.html">GeoPuzzle: crea el mundo pieza por pieza</a></li>
<li><a href="../es427211/index.html">Electron es un flash para escritorio</a></li>
<li><a href="../es427215/index.html">Los microservicios necesitan crecer, no comenzar con ellos.</a></li>
<li><a href="../es427217/index.html">An√°lisis de rendimiento de los servidores WSGI: segunda parte</a></li>
<li><a href="../es427221/index.html">Lo que me di cuenta de camino a mi sue√±o de inteligencia artificial</a></li>
<li><a href="../es427223/index.html">¬øCu√°l es la responsabilidad del desarrollador principal?</a></li>
<li><a href="../es427225/index.html">Oracle Database 18c XE lanzado</a></li>
<li><a href="../es427227/index.html">C√≥mo hicimos un juego de mesa con control remoto - Parte 2</a></li>
<li><a href="../es427229/index.html">Programa de gesti√≥n de proyectos de juegos de 4 a√±os</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>