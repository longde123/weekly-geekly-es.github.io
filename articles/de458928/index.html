<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏽‍🏭 🧑🏻‍🤝‍🧑🏻 🍒 XLNet gegen BERT 🏝️ 💅 🤟🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ende Juni zeigte uns ein Team der Carnegie Mellon University XLNet und legte sofort die Veröffentlichung , den Code und das fertige Modell vor ( XLNet...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>XLNet gegen BERT</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/458928/"><img src="https://habrastorage.org/webt/py/g0/es/pyg0es7u25w7xb0cc8z49aczcls.png"><br><br>  Ende Juni zeigte uns ein Team der Carnegie Mellon University XLNet und legte sofort die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Veröffentlichung</a> , den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Code</a> und das fertige Modell vor ( <a href="">XLNet-Large</a> , Gehäuse: 24- <a href="">lagig</a> , 1024-versteckt, 16-Kopf).  Dies ist ein vorab trainiertes Modell zur Lösung verschiedener Probleme der Verarbeitung natürlicher Sprache. <br><br>  In der Veröffentlichung gaben sie sofort einen Vergleich ihres Modells mit Googles <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">BERT an</a> .  Sie schreiben, dass XLNet BERT in einer Vielzahl von Aufgaben überlegen ist.  Und zeigt Ergebnisse in 18 Aufgaben auf dem neuesten Stand der Technik. <br><a name="habracut"></a><br><h2>  BERT, XLNet und Transformatoren </h2><br>  Einer der jüngsten Trends beim Deep Learning ist das Transferlernen.  Wir trainieren Modelle, um einfache Probleme mit einer großen Datenmenge zu lösen, und verwenden dann diese vorab trainierten Modelle, um jedoch andere, spezifischere Probleme zu lösen.  BERT und XLNet sind genau solche vorgefertigten Netzwerke, mit denen Probleme bei der Verarbeitung natürlicher Sprache gelöst werden können. <br><br>  Diese Modelle entwickeln die Idee von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Transformatoren</a> - den derzeit vorherrschenden Ansatz zum Erstellen von Modellen für die Arbeit mit Sequenzen.  Sehr detailliert und mit Beispielen für Code auf Transformatoren und dem Aufmerksamkeitsmechanismus ist in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">The Annotated Transformer geschrieben</a> . <br><br>  Wenn Sie sich das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GLUE-Benchmark-Leaderboard (General Language Understanding Evaluation) ansehen</a> , sehen Sie von oben viele Modelle, die auf Transformatoren basieren.  Einschließlich beider Modelle, die bessere Ergebnisse als Menschen zeigen.  Wir können sagen, dass wir mit Transformatoren eine Mini-Revolution in der Verarbeitung natürlicher Sprache erleben. <br><br><h2>  BERT Nachteile </h2><br>  BERT ist ein Auto-Encoder (Autoencoder, AE).  Er versteckt und verdirbt einige Wörter in der Sequenz und versucht, die ursprüngliche Sequenz von Wörtern aus dem Kontext wiederherzustellen. <br><br>  Dies führt zu Nachteilen des Modells: <br><br><ul><li>  Jedes versteckte Wort wird einzeln vorhergesagt.  Wir verlieren Informationen über die möglichen Beziehungen zwischen maskierten Wörtern.  Der Artikel enthält ein Beispiel namens "New York".  Wenn wir versuchen, diese Wörter unabhängig im Kontext vorherzusagen, werden wir die Beziehung zwischen ihnen nicht berücksichtigen. </li><li>  Inkonsistenz zwischen den Trainingsphasen des BERT-Modells und der Verwendung des vorab trainierten BERT-Modells.  Wenn wir das Modell trainieren - wir haben versteckte Wörter ([MASK] -Token), wenn wir das vorab trainierte Modell verwenden, liefern wir solche Token noch nicht an die Eingabe. </li></ul><br>  Trotz dieser Probleme zeigte BERT bei vielen Aufgaben der Verarbeitung natürlicher Sprache die neuesten Ergebnisse. <br><br><h2>  XLNet-Funktionen </h2><br>  XLNet ist eine autoregressive Sprachmodellierung, AR LM.  Sie versucht, den nächsten Token aus der Reihenfolge der vorherigen vorherzusagen.  In klassischen autoregressiven Modellen wird diese Kontextsequenz unabhängig von zwei Richtungen der ursprünglichen Zeichenfolge verwendet. <br><br>  XLNet verallgemeinert diese Methode und bildet Kontext von verschiedenen Stellen in der Quellsequenz.  Wie macht er das?  Er nimmt alle (theoretisch) möglichen Permutationen der ursprünglichen Sequenz und sagt jedes Token in der Sequenz aus den vorherigen voraus. <br><br>  Hier ist ein Beispiel aus dem Artikel, wie das x3-Token aus verschiedenen Permutationen der ursprünglichen Sequenz vorhergesagt wird. <br><br><img src="https://habrastorage.org/webt/yq/mb/fa/yqmbfas9mcnfkciq6pmew_-4hh8.png"><br><br>  Darüber hinaus ist der Kontext keine Tüte mit Worten.  Informationen zur Erstbestellung von Token werden ebenfalls an das Modell geliefert. <br><br>  Wenn wir Analogien zum BERT ziehen, stellt sich heraus, dass wir die Token nicht im Voraus maskieren, sondern unterschiedliche Sätze versteckter Token für unterschiedliche Permutationen verwenden.  Gleichzeitig verschwindet das zweite Problem von BERT - das Fehlen versteckter Token bei Verwendung des vorab trainierten Modells.  Bei XLNet ist bereits die gesamte Sequenz ohne Masken eingegeben. <br><br>  Woher kommt der XL im Namen?  XL - weil XLNet den Aufmerksamkeitsmechanismus und die Ideen des Transformer-XL-Modells verwendet.  Obwohl böse Sprachen behaupten, dass XL auf die Menge an Ressourcen hinweist, die zum Trainieren des Netzwerks benötigt werden. <br><br><img src="https://habrastorage.org/webt/hs/fb/u-/hsfbu-ufj-9e-me1agkauoa389c.png"><br><br>  Und über die Ressourcen.  Auf Twitter veröffentlichten sie die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Berechnung der</a> Kosten für das Training des Netzwerks mit den Parametern aus dem Artikel.  Es stellte sich heraus, 245.000 Dollar.  Es stimmt, dann kam ein Ingenieur von Google und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">korrigierte,</a> dass in dem Artikel 512 TPU-Chips erwähnt werden, von denen sich vier auf dem Gerät befinden.  Das heißt, die Kosten betragen bereits 62.440 Dollar oder sogar 32.720 Dollar angesichts der 512 Kerne, die ebenfalls im Artikel erwähnt werden. <br><br><h2>  XLNet gegen BERT </h2><br>  Bisher wurde nur ein vorab geschultes Modell für Englisch für den Artikel ausgelegt (XLNet-Large, Cased).  Der Artikel erwähnt aber auch Experimente mit kleineren Modellen.  Bei vielen Aufgaben zeigen XLNet-Modelle im Vergleich zu ähnlichen BERT-Modellen bessere Ergebnisse. <br><br><img src="https://habrastorage.org/webt/ac/p_/th/acp_thyxqwgcuyhvfvkkixhwj_y.png"><br><br>  Das Aufkommen von BERT und insbesondere vorgefertigten Modellen zog die Aufmerksamkeit der Forscher auf sich und führte zu einer Vielzahl verwandter Arbeiten.  Jetzt ist hier XLNet.  Es ist interessant zu sehen, ob es für einige Zeit zum De-facto-Standard in NLP wird oder umgekehrt, um Forscher bei der Suche nach neuen Architekturen und Ansätzen für die Verarbeitung natürlicher Sprache anzuregen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de458928/">https://habr.com/ru/post/de458928/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de458918/index.html">Was Sie aus dem Design von Hyper-Casual-Spielen lernen können</a></li>
<li><a href="../de458920/index.html">Konferenz für DevOps-Fans</a></li>
<li><a href="../de458922/index.html">So wechseln Sie von ESXi zu KVM / LXD und verlieren nicht den Verstand</a></li>
<li><a href="../de458924/index.html">Unfälle helfen Ihnen beim Lernen</a></li>
<li><a href="../de458926/index.html">Die Tragödie kommt nicht allein</a></li>
<li><a href="../de458930/index.html">Wie Perm-Studenten das Finale der internationalen Datenanalyse-Meisterschaft des Data Mining Cup 2019 erreichten</a></li>
<li><a href="../de458932/index.html">Yota - oder wie Sie alles herausfinden können</a></li>
<li><a href="../de458934/index.html">Bereitstellen von Anwendungen auf mehreren Kubernetes-Clustern mit Helm</a></li>
<li><a href="../de458936/index.html">"Es ist einfacher zu antworten als zu schweigen" - ein großes Interview mit dem Vater des Transaktionsgedächtnisses, Maurice Herlichi</a></li>
<li><a href="../de458938/index.html">C ++ 20 wird gebündelt, C ++ 23 wird gestartet. Ergebnisse des Treffens in Köln</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>