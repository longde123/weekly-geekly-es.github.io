<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§ü üéóÔ∏è üîñ Autos werden schlauer und lernen fast so viel wie wir üé° üë≤üèø üíä</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Studien zeigen, dass Computermodelle, die als neuronale Netze bekannt sind und in immer mehr Anwendungen verwendet werden, lernen k√∂nnen, Sequenzen in...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Autos werden schlauer und lernen fast so viel wie wir</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/401931/"><h3>  Studien zeigen, dass Computermodelle, die als neuronale Netze bekannt sind und in immer mehr Anwendungen verwendet werden, lernen k√∂nnen, Sequenzen in Daten mit denselben Algorithmen wie das menschliche Gehirn zu erkennen. </h3><br><img src="https://habrastorage.org/getpro/geektimes/post_images/c2c/1bf/8e1/c2c1bf8e1a76f607108a26cfb43a4858.jpg" alt="Bild"><br><br>  Das Gehirn l√∂st sein kanonisches Problem - das Training -, indem es viele seiner Verbindungen nach einem unbekannten Regelwerk anpasst.  Um diese Regeln aufzudecken, begannen Wissenschaftler vor 30 Jahren mit der Entwicklung von Computermodellen, um den Lernprozess zu reproduzieren.  In einer wachsenden Anzahl von Experimenten wird heute deutlich, dass sich diese Modelle bei bestimmten Aufgaben sehr √§hnlich wie das reale Gehirn verhalten.  Forscher sagen, dass diese √Ñhnlichkeit eine grundlegende √úbereinstimmung zwischen Gehirn- und Computer-Lernalgorithmen nahe legt. <br><br>  Der vom Computermodell verwendete Algorithmus wird als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Boltzmann-Maschine bezeichnet</a> .  Es wurde 1983 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">von</a> Jeffrey Hinton und Terry Seinowski <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erfunden</a> [tats√§chlich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">1985</a> - ca.  √ºbersetzt.].  Es sieht sehr vielversprechend aus als einfache theoretische Erkl√§rung mehrerer Prozesse im Gehirn - Entwicklung, Ged√§chtnisbildung, Erkennung von Objekten und Ger√§uschen, Schlaf- und Wachzyklen. <br><a name="habracut"></a><br>  "Dies ist die beste Gelegenheit, das Gehirn heute zu verstehen", sagt Sue Becker, Professorin f√ºr Psychologie, Neurobiologie und Verhalten an der Universit√§t.  McMaster in Hamilton, Ontario.  "Ich kenne kein Modell, das ein breiteres Spektrum von Ph√§nomenen beschreibt, die mit Lernen und Gehirnstruktur zusammenh√§ngen." <br><br>  Hinton, ein Pionier auf dem Gebiet der KI, wollte immer die Regeln verstehen, nach denen das Gehirn die Kommunikation verbessert oder schw√§cht - das hei√üt, den Lernalgorithmus.  ‚ÄûIch habe beschlossen, dass etwas gebaut werden muss, um es zu verstehen‚Äú, sagt er.  Nach dem reduktionistischen Ansatz der Physiker plant er, einfache Computermodelle des Gehirns unter Verwendung verschiedener Lernalgorithmen zu erstellen und zu sehen, ‚Äûwelche funktionieren werden‚Äú, sagt Hinton, teils Professor f√ºr Informatik an der Universit√§t Toronto, teils bei Google. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/e09/ca7/de6/e09ca7de64952922dee2dc5c0e51d154.png" alt="Bild"><br>  <i>Mehrschichtige neuronale Netze bestehen aus Schichten k√ºnstlicher Neuronen mit gewichteten Verbindungen zwischen ihnen.</i>  <i>Ich sende den eingehenden Daten eine Kaskade von Signalen in Schichten, und der Algorithmus bestimmt die √Ñnderung der Gewichte jeder Verbindung.</i> <br><br>  In den 1980er und 1990er Jahren erfand Hinton, der Ururenkel der Logik des 19. Jahrhunderts, George Boole, dessen Arbeit die Grundlage der modernen Informatik bildete, mehrere Algorithmen f√ºr maschinelles Lernen.  Algorithmen, die steuern, wie ein Computer aus Daten lernt, werden in Computermodellen verwendet, die als "k√ºnstliche neuronale Netze" bezeichnet werden - Netze miteinander verbundener virtueller Neuronen, die Signale an ihre Nachbarn senden, ein- oder ausschalten oder "ausl√∂sen".  Wenn Daten in das Netzwerk eingespeist werden, f√ºhrt dies zu einer Kaskade von Antworten, und der Algorithmus w√§hlt basierend auf dem Bild dieser Antworten aus, die Gewichte der Verbindungen oder Synapsen zwischen jedem Neuronenpaar zu erh√∂hen oder zu verringern. <br><br>  Seit Jahrzehnten vegetieren viele Hinton-Computermodelle.  Dank der Fortschritte bei der Prozessorleistung, des Fortschritts beim Verst√§ndnis des Gehirns und der Algorithmen spielen neuronale Netze in der Neurobiologie eine immer gr√∂√üere Rolle.  Sejnowski [Sejnowski], Leiter des Computerlabors f√ºr Neurobiologie am Institut f√ºr biologische Forschung.  Salka in La Jolla, Kalifornien, sagt: ‚ÄûVor drei√üig Jahren hatten wir sehr grobe Ideen.  Jetzt fangen wir an, einige davon zu testen. ‚Äú <br><br><h2>  Gehirnmaschinen </h2><br>  Hintons fr√ºhe Versuche, das Gehirn zu reproduzieren, waren begrenzt.  Computer k√∂nnten ihre Lernalgorithmen in kleinen neuronalen Netzen ausf√ºhren, aber Skalierungsmodelle √ºberlasteten die Prozessoren sehr schnell.  Im Jahr 2005 entdeckte Hinton, dass der Prozess effizienter wird, wenn Sie die neuronalen Netze in Schichten unterteilen und die Algorithmen auf jeder Schicht separat ausf√ºhren und dabei die Struktur und Entwicklung des Gehirns ungef√§hr wiederholen. <br><br>  Obwohl Hinton seine Entdeckung in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zwei</a> <a href="">bekannten Magazinen ver√∂ffentlichte</a> , waren neuronale Netze bis dahin aus der Mode gekommen, und er "k√§mpfte darum, die Leute zu interessieren", sagte Lee Deng, leitender Forscher bei Microsoft Research.  Deng kannte Hinton jedoch und beschloss 2009, seine ‚ÄûDeep Learning‚Äú -Methode zu testen, um sein Potenzial schnell zu erkennen.  In den folgenden Jahren werden Lernalgorithmen in der Praxis in einer wachsenden Anzahl von Anwendungen verwendet, z. B. im pers√∂nlichen Assistenten von Google Now oder in der Sprachsuchfunktion von Microsoft Windows-Telefonen. <br><br>  Einer der vielversprechendsten Algorithmen, die Boltzmann-Maschine, ist nach dem √∂sterreichischen Physiker Ludwig Boltzmann aus dem 19. Jahrhundert benannt, der einen Zweig der Physik entwickelte, der sich mit einer gro√üen Anzahl von Teilchen befasst, der so genannten statistischen Mechanik.  Boltzmann entdeckte eine Gleichung, die die Wahrscheinlichkeit angibt, dass ein molekulares Gas eine bestimmte Energie hat, wenn es das Gleichgewicht erreicht.  Wenn Sie die Molek√ºle durch Neuronen ersetzen, tendiert das Ergebnis zur gleichen Gleichung. <br><br>  Die Netzwerksynapsen beginnen mit einer zuf√§lligen Verteilung von Gewichten, und die Gewichte werden nach einem relativ einfachen Verfahren schrittweise angepasst: Die erzeugte Antwortschaltung beim Empfangen von Daten (wie Bildern oder T√∂nen) durch die Maschine wird mit der zuf√§lligen Antwortschaltung der Maschine verglichen, die auftritt, wenn keine Daten eingegeben werden. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/68c/dfb/775/68cdfb7753fc04e80a2940469564726e.jpg" alt="Bild"><br>  <i>Joffrey Hinton glaubt, dass der beste Ansatz zum Verst√§ndnis der Lernprozesse im Gehirn darin besteht, Computer zu bauen, die auf die gleiche Weise lernen.</i> <br><br>  Jede virtuelle Synapse verfolgt beide statistischen S√§tze.  Wenn die damit verbundenen Neuronen beim Empfang von Daten h√§ufiger in enger Reihenfolge ausgel√∂st werden als w√§hrend des Zufallsbetriebs, erh√∂ht sich das Gewicht der Synapse um einen Wert, der proportional zur Differenz ist.  Wenn jedoch zwei Neuronen w√§hrend einer zuf√§lligen Operation h√§ufiger zusammen ausgel√∂st werden, wird die sie verbindende Synapse als zu stark und geschw√§cht angesehen. <br><br>  Die am h√§ufigsten verwendete Version der Boltzmann-Maschine funktioniert nach dem ‚ÄûTraining‚Äú besser, da Tausende von Probendaten nacheinander auf jeder Schicht verarbeitet wurden.  Erstens empf√§ngt die untere Schicht des Netzwerks Rohdaten in Form von Bildern oder T√∂nen, und in der Art von Netzhautzellen werden Neuronen ausgel√∂st, wenn sie Kontraste in ihrem Datenbereich erkennen, beispielsweise das Umschalten von hell auf dunkel.  Ihre Ausl√∂sung kann die Ausl√∂sung der mit ihnen verbundenen Neuronen ausl√∂sen, abh√§ngig vom Gewicht der Synapse, die sie verbindet.  Da die Ausl√∂sung von Paaren virtueller Neuronen st√§ndig mit Hintergrundstatistiken verglichen wird, treten nach und nach sinnvolle Verbindungen zwischen Neuronen auf und verst√§rken sich.  Die Gewichte der Synapsen werden angegeben, und Kategorien von T√∂nen und Bildern werden in die Verbindungen eingebaut.  Jede nachfolgende Ebene wird auf √§hnliche Weise trainiert, wobei Daten aus der darunter liegenden Ebene verwendet werden. <br><br>  Wenn Sie ein Bild eines Autos einem neuronalen Netzwerk zuf√ºhren, das darauf trainiert ist, bestimmte Objekte in Bildern zu erkennen, funktioniert die untere Ebene, wenn ein Kontrast erkannt wird, der ein Gesicht oder einen Endpunkt anzeigt.  Diese Signale werden an Neuronen h√∂herer Ebene gesendet, die Winkel, Teile der R√§der usw. bestimmen.  In der oberen Ebene werden Neuronen nur als Reaktion auf das Bild des Autos ausgel√∂st. <br><br>  "Die Magie des Geschehens im Web besteht darin, dass es zusammengefasst werden kann", sagt Yann LeCun, Direktor des Data Science Center an der New York University.  "Wenn Sie ihr ein Auto zeigen, das sie zuvor noch nicht gesehen hat, und wenn das Auto einige Formen und Merkmale aufweist, die mit den ihr w√§hrend des Trainings gezeigten Maschinen gemeinsam sind, kann sie feststellen, dass es sich um ein Auto handelt." <br><br>  Neuronale Netze haben k√ºrzlich ihre Entwicklung dank des Hinton-Mehrschichtmodus, der Verwendung von Hochgeschwindigkeits-Computerchips zur Verarbeitung von Grafiken und des explosionsartigen Anstiegs der Anzahl der f√ºr das Training verf√ºgbaren Bilder und Sprachaufzeichnungen beschleunigt.  Netzwerke k√∂nnen 88% der W√∂rter in englischer Sprache korrekt erkennen, w√§hrend die durchschnittliche Person 96% erkennt.  Sie k√∂nnen Autos und Tausende anderer Objekte in Bildern mit √§hnlicher Genauigkeit erkennen und haben in den letzten Jahren eine dominierende Position bei Wettbewerben f√ºr maschinelles Lernen eingenommen. <br><br><h2>  Ein Gehirn aufbauen </h2><br>  Niemand wei√ü, wie man die Regeln, nach denen das Gehirn trainiert wird, direkt herausfindet, aber es gibt viele indirekte √úbereinstimmungen zwischen dem Verhalten des Gehirns und der Boltzmann-Maschine. <br><br>  Beide werden ohne Aufsicht trainiert, wobei nur vorhandene Muster in den Daten verwendet werden.  "Ihre Mutter erz√§hlt Ihnen nicht millionenfach, was auf dem Bild gezeigt wird", sagt Hinton.  - Man muss lernen, Dinge ohne den Rat anderer zu erkennen.  Nachdem Sie die Kategorien studiert haben, werden Ihnen die Namen dieser Kategorien mitgeteilt.  So lernen die Kinder etwas √ºber Hunde und Katzen, und dann lernen sie, dass Hunde "Hunde" und Katzen "Katzen" genannt werden. <br><br>  Das Gehirn eines Erwachsenen ist nicht so flexibel wie das junge, genau wie die Boltzmann-Maschine, die f√ºr 100.000 Bilder von Autos trainiert hat. Es wird sich nicht viel √§ndern, nachdem sie ein anderes gesehen hat.  Ihre Synapsen haben bereits die richtigen Gewichte f√ºr die Kategorisierung von Autos.  Aber das Training endet nicht.  Neue Informationen k√∂nnen in die Struktur der Gehirn- und Boltzmann-Maschinen integriert werden. <br><br>  In den letzten zwei Jahrzehnten hat eine Untersuchung der Gehirnaktivit√§t in einem Traum den ersten Beweis erbracht, dass das Gehirn einen dem Boltzmann-Algorithmus √§hnlichen Algorithmus verwendet, um neue Informationen und Erinnerungen in seine Struktur aufzunehmen.  Neurowissenschaftler wissen seit langem, dass der Schlaf eine wichtige Rolle bei der Ged√§chtniskonsolidierung spielt und zur Integration neuer Informationen beitr√§gt.  1995 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">schlugen</a> Hinton und Kollegen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vor,</a> dass der Schlaf die Rolle einer Grundebene im Algorithmus spielt und die Aktivit√§t von Neuronen ohne Eingabedaten bezeichnet. <br><br>  "W√§hrend des Schlafes m√ºssen Sie nur die Grundfrequenz der Neuronen herausfinden", sagt Hinton.  - Sie finden die Korrelation ihrer Arbeit f√ºr den Fall heraus, dass das System alleine arbeitet.  Und wenn die Neuronen mehr korrelieren, erh√∂hen Sie einfach die Gewichte zwischen ihnen.  Und wenn weniger, reduzieren Sie das Gewicht. " <br><br>  Auf Synapsenebene kann ‚Äûdieser Algorithmus auf verschiedene Arten bereitgestellt werden‚Äú, sagt Sezhnowski, Berater der Pr√§sidialverwaltung im Rahmen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der BRAIN-Initiative</a> , einer Studie mit einem Zuschuss von 100 Millionen US-Dollar zur Entwicklung neuer Gehirnforschungstechniken. <br><br>  Er sagt, dass es f√ºr das Gehirn am einfachsten ist, mit dem Boltzmann-Algorithmus zu arbeiten, indem es tags√ºber Synapsen baut und nachts verringert.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Giulio Tononi</a> , Leiter des Zentrums f√ºr Schlaf- und Bewusstseinsstudien an der Universit√§t von Wisconsin-Madison, fand heraus, dass die Genexpression in Synapsen sie gem√§√ü dieser Hypothese ver√§ndert: Gene, die am Synapsenwachstum beteiligt sind, sind tags√ºber aktiver und Gene, die an der Kontraktion beteiligt sind Synapsen - nachts. <br><br>  Bei einer anderen Option kann ‚Äûdie Basislinie in einem Traum berechnet und dann tags√ºber relativ dazu ge√§ndert werden‚Äú, sagt Sezhnowski.  In seinem Labor werden detaillierte Computermodelle von Synapsen und die von ihnen unterst√ºtzten Netzwerke erstellt, um zu bestimmen, wie sie Statistiken √ºber Wachheit und Schlafmuster sammeln und wann sich die St√§rke von Synapsen √§ndert, um diesen Unterschied anzuzeigen. <br><br><h2>  Schwierigkeiten mit dem Gehirn </h2><br><img src="https://habrastorage.org/getpro/geektimes/post_images/784/a3b/721/784a3b7213087f712c156f1ad62a346d.jpg" alt="Bild"><br>  <i>Bild der Netzhaut, in der verschiedene Zelltypen durch unterschiedliche Farben gekennzeichnet sind.</i>  <i>Farbempfindlich (violett) verbindet sich mit horizontal (orange), das mit bipolar (gr√ºn) verbunden ist, und mit den Zellen der Netzhaut und des Ganglions (lila).</i> <br><br>  Der Boltzmann-Algorithmus kann einer von vielen sein, die vom Gehirn zur Feinabstimmung von Synapsen verwendet werden.  In den 1990er Jahren entwickelten mehrere unabh√§ngige Gruppen ein theoretisches Modell daf√ºr, wie das visuelle System den Informationsfluss zur Netzhaut effizient codiert.  Die Theorie postulierte, dass es in den unteren Schichten des visuellen Kortex einen Prozess der ‚ÄûStreukodierung‚Äú gibt, √§hnlich der Bildkomprimierung, wodurch die sp√§ten Stadien des visuellen Systems effizienter arbeiten. <br><br>  Vorhersagen des Modells bestehen nach und nach immer strengere Tests.  In einem in PLOS Computational Biology ver√∂ffentlichten Artikel stellten Computational Neuroscientists aus Gro√übritannien und Australien fest, dass neuronale Netze, die den von Hinton 2002 erfundenen Streupodierungsalgorithmus Products of Experts verwenden, dieselben ungew√∂hnlichen visuellen Daten verarbeiten, die lebende Katzen erhalten (Zum Beispiel untersuchen Katzen und neuronale Netze gestreifte Bilder), ihre Neuronen produzieren fast identische ungew√∂hnliche Verbindungen. <br><br>  "Wenn die Informationen den visuellen Kortex erreichen, pr√§sentiert das Gehirn sie unserer Meinung nach als verstreuten Code", sagte Bruno Olshausen, Computational Neuroscientist und Direktor des Redwood Center for Theoretical Neurobiology an der University of California-Berkeley, der an der Entwicklung mitwirkte Streukodierungstheorie.  "Als ob eine Boltzmann-Maschine in Ihrem Kopf sitzt und versucht, die Verbindungen zu verstehen, die zwischen den Elementen des verstreuten Codes bestehen." <br><br>  Olshausen und das Team verwendeten neuronale Netzwerkmodelle der h√∂heren Schichten des visuellen Kortex, um zu zeigen, wie das Gehirn trotz der Bewegung von Bildern <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">eine stabile Wahrnehmung visueller Eingaben aufrechterhalten</a> kann.  In einer anderen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Studie fanden</a> sie heraus, dass die Aktivit√§t von Neuronen im visuellen Kortex von Katzen, die einen Schwarzwei√üfilm betrachten, von der Boltzmann-Maschine sehr gut beschrieben wird. <br><br>  Eine der m√∂glichen Anwendungen dieser Arbeit ist die Herstellung von Neuroprothesen, beispielsweise einer k√ºnstlichen Netzhaut.  Wenn Sie sich ansehen, wie ‚ÄûInformationen im Gehirn formatiert werden, k√∂nnen Sie verstehen, wie Sie das Gehirn dazu anregen k√∂nnen, zu glauben, dass es ein Bild sieht‚Äú, sagt Olshausen. <br><br>  Laut Sezhnowski k√∂nnen Forscher durch das Verst√§ndnis der Wachstums- und Reduktionsalgorithmen von Synapsen diese √§ndern und lernen, wie die Funktionsweise des neuronalen Netzwerks gest√∂rt wird.  "Dann k√∂nnen sie mit den bekannten Problemen der Menschen verglichen werden", sagt er.  - Fast alle psychischen St√∂rungen k√∂nnen durch Probleme mit Synapsen erkl√§rt werden.  Wenn wir Synapsen besser verstehen k√∂nnen, k√∂nnen wir verstehen, wie das Gehirn normal funktioniert, wie es Informationen verarbeitet, wie es lernt und was schief geht, wenn Sie beispielsweise Schizophrenie entwickeln. ‚Äú <br><br>  Der Ansatz zur Untersuchung des Gehirns mithilfe neuronaler Netze steht in starkem Kontrast zum Ansatz des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Human Brain Project</a> .  Dies ist der ver√∂ffentlichte Plan des Schweizer Neurowissenschaftlers Henry Marcram, mit einem Supercomputer eine genaue Simulation des menschlichen Gehirns zu erstellen.  Im Gegensatz zu Hintons Ansatz, der mit einem stark vereinfachten Modell beginnt und dem Weg der allm√§hlichen Komplikation folgt, m√∂chte Markram sofort die gr√∂√ütm√∂gliche Datenmenge bis hin zu einzelnen Molek√ºlen einbeziehen und hofft, dass er dadurch die volle Funktionalit√§t und das volle Bewusstsein hat. <br><br>  Das Projekt wurde von der Europ√§ischen Kommission mit 1,3 Milliarden US-Dollar finanziert, aber Hinton glaubt, dass diese Megasimulation fehlschlagen wird und in zu vielen beweglichen Teilen stecken bleibt, die noch niemand versteht. <br><br>  Dar√ºber hinaus glaubt Hinton nicht, dass das Gehirn nur durch seine Bilder verstanden werden kann.  Solche Daten sollten verwendet werden, um Algorithmen zu erstellen und zu verfeinern.  "Theoretisches Denken und die Erforschung des Raums der Lernalgorithmen sind erforderlich, um eine Theorie wie diese zu erstellen", sagt Boltzmanns Maschine.  Der n√§chste Schritt f√ºr Hinton ist die Entwicklung von Algorithmen zum Trainieren noch hirn√§hnlicherer neuronaler Netze, bei denen Synapsen Neuronen innerhalb einer Schicht und nicht nur zwischen verschiedenen Schichten verbinden.  ‚ÄûDas Hauptziel ist es, die Vorteile zu verstehen, die durch die Komplikation der Berechnungen in jeder Phase erzielt werden k√∂nnen‚Äú, sagt er. <br><br>  Die Hypothese ist, dass mehr Verbindungen zu st√§rkeren R√ºckschleifen f√ºhren, die laut Olshausen h√∂chstwahrscheinlich dazu beitragen, dass das Gehirn "die fehlenden Details ausf√ºllt".  Die oberen Schichten st√∂ren die Arbeit von Neuronen aus den unteren Schichten, die sich mit Teilinformationen befassen.  "All dies h√§ngt eng mit dem Bewusstsein zusammen", sagt er. <br><br>  Das menschliche Gehirn ist immer noch viel komplexer als jedes Modell.  Es ist gr√∂√üer, dichter, effizienter, hat mehr Verbindungen und komplexe Neuronen - und arbeitet gleichzeitig mit mehreren Algorithmen.  Olshausen schl√§gt vor, dass wir ungef√§hr 15% der Aktivit√§t des visuellen Kortex verstehen.  Obwohl die Modelle voranschreiten, sind die Neurowissenschaften immer noch "der Physik vor Newton √§hnlich", sagt er.  Dennoch ist er zuversichtlich, dass der Prozess der Arbeit auf der Grundlage dieser Algorithmen eines Tages das Hauptr√§tsel des Gehirns erkl√§ren kann - wie Daten aus den Sinnesorganen in einen subjektiven Realit√§tssinn umgewandelt werden.  Bewusstsein, sagt Olshausen, "ist etwas, das aus der Realit√§t hervorgeht, eine sehr komplexe Boltzmann-Maschine." </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de401931/">https://habr.com/ru/post/de401931/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de401919/index.html">ONO 3D-Drucker. Stereolithographie - f√ºr die Massen</a></li>
<li><a href="../de401923/index.html">5 Mythen √ºber Projektoren. Mythos Nr. 2 - ‚ÄûFarbhelligkeit‚Äú - ein Projektormerkmal eines von Vermarktern erfundenen Projektors</a></li>
<li><a href="../de401925/index.html">Eingef√ºhrt von Pi Zero W.</a></li>
<li><a href="../de401927/index.html">Rocket Fuel Saga - R√ºckseite der M√ºnze</a></li>
<li><a href="../de401929/index.html">SpaceX: N√§chstes Jahr werden wir zwei Weltraumtouristen zum Mond schicken</a></li>
<li><a href="../de401935/index.html">Mozilla erwirbt Pocket Lazy Reading Service</a></li>
<li><a href="../de401937/index.html">Yandex ignoriert die 3D-Sicherheits√ºberpr√ºfung beim Bezahlen von Anzeigen in Yandex.Direct mit Bankkarten</a></li>
<li><a href="../de401939/index.html">Kinderschreiner PLAYMAT: Holzarbeiten zum Selbermachen sind interessant</a></li>
<li><a href="../de401941/index.html">√úbersicht √ºber die neue Oco2-√úberwachungskamera</a></li>
<li><a href="../de401943/index.html">Intel Atom Family Brief</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>