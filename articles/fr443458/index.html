<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèΩ‚Äçüíº ‚öúÔ∏è üï° 6 bugs syst√®me divertissants dans le fonctionnement de Kubernetes [et leur solution] ü§±üèº üèÅ üêøÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Au fil des ann√©es d'exploitation de Kubernetes en production, nous avons accumul√© de nombreuses histoires int√©ressantes, car les bogues dans divers co...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>6 bugs syst√®me divertissants dans le fonctionnement de Kubernetes [et leur solution]</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/443458/"><img src="https://habrastorage.org/webt/7o/mz/o2/7omzo2vcqpqijlxsewel9gyhcsq.png"><br><br>  Au fil des ann√©es d'exploitation de Kubernetes en production, nous avons accumul√© de nombreuses histoires int√©ressantes, car les bogues dans divers composants du syst√®me ont entra√Æn√© des cons√©quences d√©sagr√©ables et / ou incompr√©hensibles qui affectent le fonctionnement des conteneurs et des pods.  Dans cet article, nous avons s√©lectionn√© quelques-unes des plus fr√©quentes ou int√©ressantes.  M√™me si vous n'√™tes jamais assez chanceux pour rencontrer de telles situations, lire √† propos de ces brefs d√©tectives - d'autant plus, de premi√®re main - est toujours amusant, n'est-ce pas? <a name="habracut"></a><br><br><h2>  Historique 1. Docker supercronique et glacial </h2><br>  Sur l'un des clusters, nous recevions p√©riodiquement un Docker ¬´gel√©¬ª, qui interf√©rait avec le fonctionnement normal du cluster.  Dans le m√™me temps, ce qui suit a √©t√© observ√© dans les journaux Docker <br><br><pre><code class="plaintext hljs">level=error msg="containerd: start init process" error="exit status 2: \"runtime/cgo: pthread_create failed: No space left on device SIGABRT: abort PC=0x7f31b811a428 m=0 goroutine 0 [idle]: goroutine 1 [running]: runtime.systemstack_switch() /usr/local/go/src/runtime/asm_amd64.s:252 fp=0xc420026768 sp=0xc420026760 runtime.main() /usr/local/go/src/runtime/proc.go:127 +0x6c fp=0xc4200267c0 sp=0xc420026768 runtime.goexit() /usr/local/go/src/runtime/asm_amd64.s:2086 +0x1 fp=0xc4200267c8 sp=0xc4200267c0 goroutine 17 [syscall, locked to thread]: runtime.goexit() /usr/local/go/src/runtime/asm_amd64.s:2086 +0x1 ‚Ä¶</code> </pre> <br>  Dans cette erreur, le message nous int√©resse le plus: <code>pthread_create failed: No space left on device</code> .  Une √©tude rapide de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation a</a> expliqu√© que Docker ne pouvait pas bifurquer le processus, ce qui provoquait un ¬´gel¬ª p√©riodique. <br><br>  Dans le suivi de ce qui se passe, l'image suivante correspond: <br><br><img src="https://habrastorage.org/webt/7r/cq/gv/7rcqgvafvtlmis1kl7maxyz5jgk.png"><br><br>  Une situation similaire est observ√©e sur d'autres n≈ìuds: <br><br><img src="https://habrastorage.org/webt/lj/mw/-h/ljmw-hrrlyukwgmigltivjwyxig.png"><br><br><img src="https://habrastorage.org/webt/ap/tw/5u/aptw5ufxl-9woo5zszegfg1nqvc.png"><br><br>  Sur les m√™mes n≈ìuds, nous voyons: <br><br><pre> <code class="bash hljs">root@kube-node-1 ~ <span class="hljs-comment"><span class="hljs-comment"># ps auxfww | grep curl -c 19782 root@kube-node-1 ~ # ps auxfww | grep curl | head root 16688 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 17398 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 16852 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 9473 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 4664 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 30571 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 24113 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 16475 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 7176 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 1090 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt;</span></span></code> </pre> <br>  Il s'est av√©r√© que ce comportement est une cons√©quence du travail du <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pod</a> avec <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">supercronic</a> (l'utilitaire sur Go que nous utilisons pour ex√©cuter des t√¢ches cron dans les pods): <br><br><pre> <code class="plaintext hljs"> \_ docker-containerd-shim 833b60bb9ff4c669bb413b898a5fd142a57a21695e5dc42684235df907825567 /var/run/docker/libcontainerd/833b60bb9ff4c669bb413b898a5fd142a57a21695e5dc42684235df907825567 docker-runc | \_ /usr/local/bin/supercronic -json /crontabs/cron | \_ /usr/bin/newrelic-daemon --agent --pidfile /var/run/newrelic-daemon.pid --logfile /dev/stderr --port /run/newrelic.sock --tls --define utilization.detect_aws=true --define utilization.detect_azure=true --define utilization.detect_gcp=true --define utilization.detect_pcf=true --define utilization.detect_docker=true | | \_ /usr/bin/newrelic-daemon --agent --pidfile /var/run/newrelic-daemon.pid --logfile /dev/stderr --port /run/newrelic.sock --tls --define utilization.detect_aws=true --define utilization.detect_azure=true --define utilization.detect_gcp=true --define utilization.detect_pcf=true --define utilization.detect_docker=true -no-pidfile | \_ [newrelic-daemon] &lt;defunct&gt; | \_ [curl] &lt;defunct&gt; | \_ [curl] &lt;defunct&gt; | \_ [curl] &lt;defunct&gt; ‚Ä¶</code> </pre> <br>  Le probl√®me est le suivant: lorsqu'une t√¢che d√©marre en supercronic, le processus g√©n√©r√© par elle <b>ne peut pas se terminer correctement</b> , se transformant en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">zombie</a> . <br><br>  <i><b>Remarque</b> : pour √™tre plus pr√©cis, les processus sont g√©n√©r√©s par des t√¢ches cron, cependant, le supercronic n'est pas un syst√®me init et ne peut pas ¬´adopter¬ª les processus que ses enfants ont engendr√©s.</i>  <i>Lorsque des signaux SIGHUP ou SIGTERM se produisent, ils ne sont pas transmis aux processus g√©n√©r√©s, √† la suite de quoi les processus enfants ne se terminent pas, restant dans le statut zombie.</i>  <i>Vous pouvez en savoir plus sur tout cela, par exemple, dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">un tel article</a> .</i> <br><br>  Il existe deux fa√ßons de r√©soudre les probl√®mes: <br><br><ol><li>  Solution de contournement temporaire: augmentez le nombre de PID dans le syst√®me √† un moment donn√©: <br><br><pre> <code class="plaintext hljs"> /proc/sys/kernel/pid_max (since Linux 2.5.34) This file specifies the value at which PIDs wrap around (ie, the value in this file is one greater than the maximum PID). PIDs greater than this value are not allo‚Äê cated; thus, the value in this file also acts as a system-wide limit on the total number of processes and threads. The default value for this file, 32768, results in the same range of PIDs as on earlier kernels</code> </pre> </li><li>  Ou, faites le lancement de t√¢ches en supercronic pas directement, mais avec l'aide du m√™me <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tini</a> , qui est capable de terminer correctement les processus et de ne pas g√©n√©rer de zombie. </li></ol><br><h2>  Historique 2. "Zombies" lors de la suppression de cgroup </h2><br>  Kubelet a commenc√© √† consommer beaucoup de CPU: <br><br><img src="https://habrastorage.org/webt/ns/lh/mp/nslhmpwfnmennya-btg5icbkh8e.png"><br><br>  Personne n'aime cela, alors nous nous sommes arm√©s de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">perf</a> et avons commenc√© √† r√©gler le probl√®me.  Les r√©sultats de l'enqu√™te sont les suivants: <br><br><ul><li>  Kubelet passe plus d'un tiers du temps CPU √† extraire les donn√©es de la m√©moire de tous les groupes de contr√¥le: <br><br><img src="https://habrastorage.org/webt/yf/u7/qv/yfu7qvvcnryl5iknz4zosagh0is.png"></li><li>  Dans la liste de diffusion des d√©veloppeurs du noyau, vous pouvez trouver une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">discussion sur le probl√®me</a> .  En bref, l'essentiel est que <b>diff√©rents fichiers tmpfs et autres choses similaires ne sont pas compl√®tement supprim√©s du syst√®me</b> lorsque cgroup est supprim√© - le soi-disant <b>zombie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">memcg</a></b> reste.  T√¥t ou tard, ils seront n√©anmoins supprim√©s du cache de pages, cependant, la m√©moire sur le serveur est volumineuse et le noyau ne voit pas l'int√©r√™t de perdre du temps √† les supprimer.  Par cons√©quent, ils continuent de s'accumuler.  Pourquoi cela se produit-il m√™me?  Il s'agit d'un serveur avec des t√¢ches cron qui cr√©e constamment de nouvelles t√¢ches et avec elles de nouveaux pods.  Ainsi, de nouveaux groupes de contr√¥le sont cr√©√©s pour les conteneurs qu'ils contiennent, qui seront bient√¥t supprim√©s. </li><li>  Pourquoi cAdvisor dans Kubelet passe autant de temps?  Ceci est facilement visible par l'ex√©cution la plus simple du <code>time cat /sys/fs/cgroup/memory/memory.stat</code> .  Si l'op√©ration prend 0,01 seconde sur une machine saine, puis 1,2 seconde sur un cron02 probl√©matique.  Le fait est que cAdvisor, qui lit tr√®s lentement les donn√©es des sysfs, essaie √©galement de prendre en compte la m√©moire utilis√©e dans les groupes de zombies. </li><li>  Pour supprimer de force les zombies, nous avons essay√© de vider les caches, comme recommand√© dans LKML: <code>sync; echo 3 &gt; /proc/sys/vm/drop_caches</code>  <code>sync; echo 3 &gt; /proc/sys/vm/drop_caches</code> , mais le noyau s'est av√©r√© plus compliqu√© et a bloqu√© la machine. </li></ul><br>  Que faire?  Le probl√®me est r√©solu ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">commit</a> , et la description, voir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le message de sortie</a> ) en mettant √† jour le noyau Linux vers la version 4.16. <br><br><h2>  Historique 3. Systemd et sa monture </h2><br>  Encore une fois, kubelet consomme trop de ressources sur certains n≈ìuds, mais cette fois, c'est d√©j√† de la m√©moire: <br><br><img src="https://habrastorage.org/webt/ud/l3/vl/udl3vlr9r5c6hzxoamdmnzvvm5m.png"><br><br>  Il s'est av√©r√© qu'il y avait un probl√®me dans le systemd utilis√© dans Ubuntu 16.04, et cela se produit lors du contr√¥le des montages qui sont cr√©√©s pour connecter les sous- <code>subPath</code> depuis ConfigMaps ou secrets.  Une fois le pod termin√©, le <b>service systemd et son montage de service restent</b> sur le syst√®me.  Au fil du temps, ils accumulent une √©norme quantit√©.  Il y a m√™me des probl√®mes sur ce sujet: <br><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">kops # 5916</a> ; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">kubernetes # 57345</a> . </li></ol><br>  ... dans le dernier, se r√©f√©rer √† PR dans systemd: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="># 7811</a> (le probl√®me dans systemd est <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="># 7798</a> ). <br><br>  Le probl√®me n'est plus dans Ubuntu 18.04, mais si vous souhaitez continuer √† utiliser Ubuntu 16.04, notre solution de contournement sur ce sujet peut √™tre utile. <br><br>  Nous avons donc cr√©√© le DaemonSet suivant: <br><br><pre> <code class="plaintext hljs">--- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: labels: app: systemd-slices-cleaner name: systemd-slices-cleaner namespace: kube-system spec: updateStrategy: type: RollingUpdate selector: matchLabels: app: systemd-slices-cleaner template: metadata: labels: app: systemd-slices-cleaner spec: containers: - command: - /usr/local/bin/supercronic - -json - /app/crontab Image: private-registry.org/systemd-slices-cleaner/systemd-slices-cleaner:v0.1.0 imagePullPolicy: Always name: systemd-slices-cleaner resources: {} securityContext: privileged: true volumeMounts: - name: systemd mountPath: /run/systemd/private - name: docker mountPath: /run/docker.sock - name: systemd-etc mountPath: /etc/systemd - name: systemd-run mountPath: /run/systemd/system/ - name: lsb-release mountPath: /etc/lsb-release-host imagePullSecrets: - name: antiopa-registry priorityClassName: cluster-low tolerations: - operator: Exists volumes: - name: systemd hostPath: path: /run/systemd/private - name: docker hostPath: path: /run/docker.sock - name: systemd-etc hostPath: path: /etc/systemd - name: systemd-run hostPath: path: /run/systemd/system/ - name: lsb-release hostPath: path: /etc/lsb-release</code> </pre> <br>  ... et il utilise le script suivant: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash # we will work only on xenial hostrelease="/etc/lsb-release-host" test -f ${hostrelease} &amp;&amp; grep xenial ${hostrelease} &gt; /dev/null || exit 0 # sleeping max 30 minutes to dispense load on kube-nodes sleep $((RANDOM % 1800)) stoppedCount=0 # counting actual subpath units in systemd countBefore=$(systemctl list-units | grep subpath | grep "run-" | wc -l) # let's go check each unit for unit in $(systemctl list-units | grep subpath | grep "run-" | awk '{print $1}'); do # finding description file for unit (to find out docker container, who born this unit) DropFile=$(systemctl status ${unit} | grep Drop | awk -F': ' '{print $2}') # reading uuid for docker container from description file DockerContainerId=$(cat ${DropFile}/50-Description.conf | awk '{print $5}' | cut -d/ -f6) # checking container status (running or not) checkFlag=$(docker ps | grep -c ${DockerContainerId}) # if container not running, we will stop unit if [[ ${checkFlag} -eq 0 ]]; then echo "Stopping unit ${unit}" # stoping unit in action systemctl stop $unit # just counter for logs ((stoppedCount++)) # logging current progress echo "Stopped ${stoppedCount} systemd units out of ${countBefore}" fi done</span></span></code> </pre> <br>  ... et √ßa commence toutes les 5 minutes avec le supercronic d√©j√† mentionn√©.  Son Dockerfile ressemble √† ceci: <br><br><pre> <code class="plaintext hljs">FROM ubuntu:16.04 COPY rootfs / WORKDIR /app RUN apt-get update &amp;&amp; \ apt-get upgrade -y &amp;&amp; \ apt-get install -y gnupg curl apt-transport-https software-properties-common wget RUN add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable" &amp;&amp; \ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - &amp;&amp; \ apt-get update &amp;&amp; \ apt-get install -y docker-ce=17.03.0* RUN wget https://github.com/aptible/supercronic/releases/download/v0.1.6/supercronic-linux-amd64 -O \ /usr/local/bin/supercronic &amp;&amp; chmod +x /usr/local/bin/supercronic ENTRYPOINT ["/bin/bash", "-c", "/usr/local/bin/supercronic -json /app/crontab"]</code> </pre> <br><h2>  Historique 4. Comp√©tition dans la planification des modules </h2><br>  Il a √©t√© not√© que: si un pod est plac√© sur notre n≈ìud et que son image est pomp√©e pendant tr√®s longtemps, alors l'autre pod qui est "arriv√©" au m√™me n≈ìud <b>ne commence</b> tout simplement <b>pas √† tirer l'image du nouveau pod</b> .  Au lieu de cela, il attend que l'image du pod pr√©c√©dent soit tir√©e.  En cons√©quence, un pod qui a d√©j√† √©t√© planifi√© et dont l'image pourrait √™tre t√©l√©charg√©e en une minute se retrouvera longtemps dans l'√©tat <code>containerCreating</code> . <br><br>  Dans les √©v√©nements, il y aura quelque chose comme √ßa: <br><br><pre> <code class="plaintext hljs">Normal Pulling 8m kubelet, ip-10-241-44-128.ap-northeast-1.compute.internal pulling image "registry.example.com/infra/openvpn/openvpn:master"</code> </pre> <br>  Il s'av√®re qu'une <b>seule image du registre lent peut bloquer le d√©ploiement</b> sur le n≈ìud. <br><br>  Malheureusement, il n'y a pas tant de fa√ßons de sortir de la situation: <br><br><ol><li>  Essayez d'utiliser votre Docker Registry directement dans le cluster ou directement avec le cluster (par exemple, GitLab Registry, Nexus, etc.); </li><li>  Utilisez des utilitaires comme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">kraken</a> . </li></ol><br><h2>  Historique 5. N≈ìuds suspendus avec m√©moire insuffisante </h2><br>  Pendant le fonctionnement de diverses applications, nous avons √©galement re√ßu une situation o√π le n≈ìud cesse compl√®tement d'√™tre accessible: SSH ne r√©pond pas, tous les d√©mons de surveillance tombent, puis rien (ou presque rien) n'est anormal dans les journaux. <br><br>  Je vais vous dire dans les images sur l'exemple d'un n≈ìud o√π MongoDB a fonctionn√©. <br><br>  Voici √† quoi ressemble au sommet <b>avant le</b> crash: <br><br><img src="https://habrastorage.org/webt/l5/ef/az/l5efazbhjmzsuxdv6puz1tlvbzs.png"><br><br>  Et donc - <b>apr√®s l'</b> accident: <br><br><img src="https://habrastorage.org/webt/wx/mh/q7/wxmhq71060dhvxsxk-dh8m--pas.png"><br><br>  Dans la surveillance aussi, il y a un saut brusque dans lequel le n≈ìud cesse d'√™tre accessible: <br><br><img src="https://habrastorage.org/webt/tp/fm/wf/tpfmwftsui_eoz91ojyy5rzektc.png"><br><br>  Ainsi, les captures d'√©cran montrent que: <br><br><ol><li>  La RAM sur la machine est proche de la fin; </li><li>  On observe une forte augmentation de la consommation de RAM, apr√®s quoi l'acc√®s √† l'ensemble de la machine est fortement d√©sactiv√©; </li><li>  Une grosse t√¢che arrive chez Mongo, qui oblige le processus SGBD √† utiliser plus de m√©moire et √† lire activement √† partir du disque. </li></ol><br>  Il s'av√®re que si Linux manque de m√©moire libre (une pression de m√©moire se produit) et qu'il n'y a pas d'√©change, puis <b>avant que</b> le tueur OOM n'arrive, un √©quilibre peut se produire entre le lancement de pages dans le cache de pages et leur r√©√©criture sur le disque.  Ceci est fait par kswapd, qui lib√®re courageusement autant de pages de m√©moire que possible pour une distribution ult√©rieure. <br><br>  Malheureusement, avec une grande charge d'E / S, coupl√©e √† une petite quantit√© de m√©moire libre, <b>kswapd devient le goulot d'√©tranglement de tout le syst√®me</b> , car <b>tous les</b> d√©fauts de page des pages de m√©moire du syst√®me lui sont li√©s.  Cela peut durer tr√®s longtemps si les processus ne veulent plus utiliser de m√©moire, mais sont fix√©s au bord m√™me de l'ab√Æme OOM-killer. <br><br>  La question logique est: pourquoi le tueur OOM arrive si tard?  Dans l'it√©ration OOM actuelle, killer est extr√™mement stupide: il ne tuera le processus que lorsque la tentative d'allocation d'une page m√©moire √©choue, c'est-√†-dire  si l'erreur de page √©choue.  Cela ne se produit pas pendant longtemps, car kswapd lib√®re courageusement des pages de m√©moire en vidant le cache de pages (toutes les E / S de disque du syst√®me, en fait) sur disque.  Plus en d√©tail, avec une description des √©tapes n√©cessaires pour √©liminer de tels probl√®mes dans le noyau, vous pouvez lire <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . <br><br>  Ce comportement <a href="">devrait s'am√©liorer</a> avec le noyau Linux 4.6+. <br><br><h2>  Histoire 6. Les pods sont en attente </h2><br>  Dans certains clusters, dans lesquels il y a vraiment beaucoup de pods, nous avons commenc√© √† remarquer que la plupart d'entre eux √©taient suspendus dans l'√©tat <code>Pending</code> depuis tr√®s longtemps, bien que les conteneurs Docker eux-m√™mes soient d√©j√† en cours d'ex√©cution sur les n≈ìuds et que vous puissiez travailler manuellement avec eux. <br><br>  Il n'y a rien de mal √† <code>describe</code> : <br><br><pre> <code class="plaintext hljs"> Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 1m default-scheduler Successfully assigned sphinx-0 to ss-dev-kub07 Normal SuccessfulAttachVolume 1m attachdetach-controller AttachVolume.Attach succeeded for volume "pvc-6aaad34f-ad10-11e8-a44c-52540035a73b" Normal SuccessfulMountVolume 1m kubelet, ss-dev-kub07 MountVolume.SetUp succeeded for volume "sphinx-config" Normal SuccessfulMountVolume 1m kubelet, ss-dev-kub07 MountVolume.SetUp succeeded for volume "default-token-fzcsf" Normal SuccessfulMountVolume 49s (x2 over 51s) kubelet, ss-dev-kub07 MountVolume.SetUp succeeded for volume "pvc-6aaad34f-ad10-11e8-a44c-52540035a73b" Normal Pulled 43s kubelet, ss-dev-kub07 Container image "registry.example.com/infra/sphinx-exporter/sphinx-indexer:v1" already present on machine Normal Created 43s kubelet, ss-dev-kub07 Created container Normal Started 43s kubelet, ss-dev-kub07 Started container Normal Pulled 43s kubelet, ss-dev-kub07 Container image "registry.example.com/infra/sphinx/sphinx:v1" already present on machine Normal Created 42s kubelet, ss-dev-kub07 Created container Normal Started 42s kubelet, ss-dev-kub07 Started container</code> </pre> <br>  Apr√®s avoir fouill√©, nous avons fait l'hypoth√®se que kubelet n'a tout simplement pas le temps d'envoyer au serveur API toutes les informations sur l'√©tat des pods, les √©chantillons de vivacit√© / pr√©paration. <br><br>  Et apr√®s avoir √©tudi√© l'aide, nous avons trouv√© les param√®tres suivants: <br><br><pre> <code class="plaintext hljs">--kube-api-qps - QPS to use while talking with kubernetes apiserver (default 5) --kube-api-burst - Burst to use while talking with kubernetes apiserver (default 10) --event-qps - If &gt; 0, limit event creations per second to this value. If 0, unlimited. (default 5) --event-burst - Maximum size of a bursty event records, temporarily allows event records to burst to this number, while still not exceeding event-qps. Only used if --event-qps &gt; 0 (default 10) --registry-qps - If &gt; 0, limit registry pull QPS to this value. --registry-burst - Maximum size of bursty pulls, temporarily allows pulls to burst to this number, while still not exceeding registry-qps. Only used if --registry-qps &gt; 0 (default 10)</code> </pre> <br>  Comme vous pouvez le voir, les <b>valeurs par d√©faut sont assez petites</b> , et √† 90% elles couvrent tous les besoins ... Cependant, dans notre cas ce n'√©tait pas suffisant.  Par cons√©quent, nous d√©finissons ces valeurs: <br><br><pre> <code class="plaintext hljs">--event-qps=30 --event-burst=40 --kube-api-burst=40 --kube-api-qps=30 --registry-qps=30 --registry-burst=40</code> </pre> <br><br>  ... et red√©marr√© les kubelets, apr√®s quoi ils ont vu l'image suivante sur les graphiques d'acc√®s au serveur API: <br><br><img src="https://habrastorage.org/webt/nq/-i/oq/nq-ioqoyt6_qudmacm5dwfe8hnk.png"><br><br>  ... et oui, tout a commenc√© √† voler! <br><br><h2>  PS </h2><br>  Pour l'aide √† la collecte des bugs et √† la pr√©paration de l'article, j'exprime ma profonde gratitude aux nombreux ing√©nieurs de notre entreprise, et en particulier √† Andrei Klimentyev (coll√®gue de notre √©quipe R&amp;D) ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">zuzzas</a> ). <br><br><h2>  PPS </h2><br>  Lisez aussi dans notre blog: <br><br><ul><li>  ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Plugin Kubectl-debug pour le d√©bogage dans les pods Kubernetes</a> ¬ª; </li><li>  ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Surveillance et Kubernetes (revue et rapport vid√©o)</a> ¬ª; </li><li>  Cycle de trucs et astuces Kubernetes: <ul><li>  ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Transfert des ressources travaillant dans un cluster vers la gestion de Helm 2</a> ¬ª; </li><li>  ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Sur l'allocation des n≈ìuds et la charge sur l'application web</a> ¬ª; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Acc√®s aux sites de d√©veloppement</a> "; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Acc√©l√©rer le bootstrap des grandes bases de donn√©es.</a> " </li></ul></li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr443458/">https://habr.com/ru/post/fr443458/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr443438/index.html">7 extensions Firefox utiles pour apprendre l'anglais</a></li>
<li><a href="../fr443440/index.html">Module PHP pour travailler avec des donn√©es hi√©rarchiques dans InterSystems IRIS</a></li>
<li><a href="../fr443450/index.html">Pourquoi les pauvres ne peuvent pas √™tre en bonne sant√©</a></li>
<li><a href="../fr443452/index.html">L'arm√©e russe cr√©era son propre Internet ferm√©</a></li>
<li><a href="../fr443456/index.html">Nous vous invitons √† Yandex NLP pendant une semaine</a></li>
<li><a href="../fr443460/index.html">11 r√©ponses sur Yandex.Directory</a></li>
<li><a href="../fr443462/index.html">Piratage de cam√©ras: vecteurs d'attaque, outils de recherche de vuln√©rabilit√©s et anti-tracking</a></li>
<li><a href="../fr443464/index.html">Guide complet pour changer d'expressions dans Java 12</a></li>
<li><a href="../fr443466/index.html">Roi du d√©veloppement</a></li>
<li><a href="../fr443468/index.html">Quels outils de surveillance de r√©seau sont devenus des leaders dans la version de Gartner</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>