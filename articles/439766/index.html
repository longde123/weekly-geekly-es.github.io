<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏼‍✈️ 😧 👩‍🏫 Aumentarlo! Aumento de la resolución moderna 🌖 ✊🏼 🤙🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ya he dejado de estremecerme y preguntarme cuándo suena el teléfono y suena una voz fuerte y segura en el receptor: "¿Es este el capitán que te molest...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Aumentarlo! Aumento de la resolución moderna</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/439766/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b88/1db/675/b881db675f3ef17c06a45cc67a8c1cb2.png"></div><br>  Ya he dejado de estremecerme y preguntarme cuándo suena el teléfono y suena una voz fuerte y segura en el receptor: "¿Es este el capitán que te molesta (mayor o menor), ¿puedes responder un par de preguntas?"  ¿Por qué no hablar con tu propia policía ... <br><br>  Las preguntas son siempre las mismas.  "Tenemos un video con el sospechoso, por favor ayude a restaurar la cara" ... "Ayude a aumentar el número del DVR" ... "No hay manos humanas aquí, por favor ayude a aumentar" ... Y así sucesivamente. <br><br>  Para dejar en claro de qué se trata, aquí hay un ejemplo real de un video muy comprimido enviado donde solicitan restaurar una cara borrosa (cuyo tamaño es equivalente a aproximadamente 8 píxeles): <br><br><div style="text-align:center;"><img width="400" src="https://habrastorage.org/webt/k-/bw/5h/k-bw5hsws_7bijnrykr3vrvhk_i.png"></div><br>  Y bueno, solo los tíos rusos de Stepa molestarían, escriben los Pinkertones occidentales. <br><a name="habracut"></a>  Aquí, por ejemplo, hay una carta de la policía de Inglaterra &lt;***** @ *****. Fsnet.co.uk&gt;: <br><blockquote>  He usado sus filtros en privado durante algún tiempo para rescatar mis videos pobres de vacaciones familiares, pero me gustaría usar los filtros comerciales para mi trabajo.  Actualmente soy un oficial de policía en una pequeña fuerza policial y estamos recibiendo una gran cantidad de video de CCTV, que a veces es de muy baja calidad y puedo ver cómo sus filtros marcarían una verdadera diferencia.  ¿Me puede decir el costo y si podría usarlos? <br><br>  Gracias <br><br><div class="spoiler">  <b class="spoiler_title">Traducción</b> <div class="spoiler_text">  Ya utilicé tus filtros para fines personales para guardar mis videos malos de vacaciones familiares.  Pero me gustaría usar filtros comerciales en mi trabajo.  Actualmente soy un oficial de policía en una pequeña unidad.  Obtenemos una gran cantidad de video de cámaras CCTV, a veces de muy baja calidad, y sus filtros realmente ayudarán.  ¿Podría decirme su costo y puedo usarlos? <br><br>  Gracias </div></div></blockquote>  O aquí hay un policía de Australia que escribe: <br><blockquote>  Hola <br>  Trabajo para la policía de Victoria en Australia, en la unidad forense de video y audio.  Ocasionalmente, recibimos videos de cámaras portátiles o montadas en vehículos.  A menudo, estos capturan imágenes entrelazadas de eventos de rápido movimiento.  En particular, el metraje que generalmente tiene más "promesa" es el metraje de las matrículas de los vehículos.  A menudo encontramos que el vehículo sujeto se habrá movido significativamente entre el primer y el último campo capturado.  Como resultado, tratamos de reconstruir todo el cuadro a partir de los dos campos, con el segundo traducido, a veces rotado, y ocasionalmente el tamaño también será diferente (a medida que el vehículo se aleja o se dirige hacia la cámara). Casar estos dos campos , preferiblemente a una precisión de subpíxel, y reconstruir el marco que contiene la matrícula, puede ser difícil. <br>  Por lo que he visto de ti desentrelazando imágenes, puede ser que tu filtro pueda hacer algo, si no todo, de lo que necesitamos.  Para ser honesto, como nuestro presupuesto es bastante pequeño, es poco probable que podamos pagar una licencia comercial.  No vendemos el producto, por supuesto, lo usamos como evidencia en casos policiales.  En cualquier caso, pensé en escribir un correo electrónico y preguntar de todos modos.  ¿Cuánto costaría una licencia?  ¿Es posible probar el producto en imágenes para ver si es apropiado?  ¿Hace algo de lo que necesitamos?  Por último, ¿se ha publicado el algoritmo?  Trabajar con algoritmos desconocidos es una práctica peligrosa para un tribunal de justicia.  Si la evidencia da como resultado que un hombre vaya a la cárcel por 20 años, ¡es una buena práctica saber por qué! <br><br>  Cualquier información que pueda ofrecer sería apreciada. <br><br>  Saludos <br>  Trabajador social <br>  Unidad Audiovisual <br>  Departamento de servicios forenses de la policía de Victoria <br><br><div class="spoiler">  <b class="spoiler_title">Traducción</b> <div class="spoiler_text">  Hola <br>  Trabajo para la Policía de Victoria en Australia en el departamento de video y audio forense.  De vez en cuando, recibimos videos de cámaras de mano y DVR.  A menudo, estos videos son disparos entrelazados de objetos que se mueven rápidamente.  En particular, el material más importante son las matrículas de vehículos.  A menudo encontramos que el vehículo en cuestión se mueve fuertemente entre el primer y el último campo capturado.  Como resultado, estamos tratando de restaurar un cuadro completo a partir de dos campos, el segundo se desplaza, a veces gira y a veces tiene un tamaño diferente (cuando el automóvil viaja hacia o desde la cámara).  Combinar estos dos campos, preferiblemente con una precisión de medio píxel, y restaurar un cuadro completo que contenga una placa de matrícula puede ser difícil. <br><br>  Veo cómo aplicas el desentrelazado a los marcos, y tal vez tus filtros puedan hacer algo, si no todo lo que necesitamos.  Honestamente, es posible que no podamos pagar una licencia comercial, porque nuestro presupuesto es bastante pequeño.  No vendemos el producto, por supuesto, lo usamos como evidencia en casos policiales.  En cualquier caso, pensé que escribiría una carta y seguiría preguntando.  ¿Cuánto costará la licencia?  ¿Es posible probar el producto en el material para saber si es adecuado?  ¿Hace parte de lo que necesitamos?  Finalmente, ¿se ha publicado el algoritmo? Trabajar con algoritmos desconocidos es una práctica peligrosa en los tribunales.  Si la evidencia lleva a una persona a ir a prisión por 20 años, es útil saber por qué. <br><br>  Agradeceremos cualquier información que nos pueda proporcionar. <br><br>  Saludos <br>  Investigador <br>  División de Audio y Video <br>  Departamento forense de la policía de Victoria </div></div></blockquote>  Tenga en cuenta que la carta es muy cuidadosa, una persona está preocupada por el algoritmo que se publica y por la responsabilidad de una recuperación incorrecta. <br><br>  A veces, solo en el proceso de correspondencia admiten que son de la policía.  Por ejemplo, a los carabinieri de Italia les gustaría ayuda: <br><blockquote>  Dr.  Vatolin <br>  Gracias por la respuesta <br>  La respuesta también vale para las fuerzas policiales (investigación de carabineros <br>  científico para PARMA ITALIA)? <br>  A qué software le han asociado sus algoritmos. <br>  Estaríamos mucho <br><br><div class="spoiler">  <b class="spoiler_title">Traducción</b> <div class="spoiler_text">  Dr.  Batolin <br>  Gracias por la respuesta <br>  ¿Es esto adecuado para la policía (Unidad de Investigación de Carabineros para PARMA ITALIA)? <br>  ¿Están interesados ​​en qué software utilizan sus algoritmos? <br>  Estaremos agradecidos </div></div></blockquote>  Y, por supuesto, muchos atractivos de la gente común ... <br><br><h2>  Aumentarlo!  ¿Qué, sientes pena por presionar el botón correcto? </h2><br>  Está claro que todo este flujo de llamadas no aparece desde cero. <br><br>  "Culpa" principalmente películas y programas de televisión. <br><br>  Por ejemplo, aquí, en 3 segundos, el marco del video comprimido aumenta 50 veces y, a partir del reflejo en las gafas, ven evidencia: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/I_8ZH1Ggjk0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Y hay muchos de esos momentos en las películas y series modernas.  Por ejemplo, en este video, hemos recopilado episodios absolutamente épicos de un paquete de programas de televisión, no se tome dos minutos para mirar: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/LhF_56SxrGk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Y cuando ves esto en cada película, el último erizo se vuelve claro que todo lo que necesitas es tener un genio informático competente, una combinación de algoritmos modernos, y solo queda <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"¡DETENERSE!"</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">y "¡Mejora!"</a>  .  Y voila!  ¡Un milagro sucederá! <br><br>  Sin embargo, los guionistas no se detienen en esta recepción ya trillada, y su imaginación desenfrenada va más allá.  Aquí hay un ejemplo muy monstruoso.  Los valientes detectives para reflejar en la pupila de la víctima recibieron una foto del delincuente.  De hecho, el reflejo en las gafas ya estaba allí.  Esto es un lugar común.  ¡Sigamos adelante!  Es solo que la resolución de la cámara CCTV en el hueco de la escalera resultó ser bastante aleatoria como el telescopio del Hubble: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/3uoM5kfZIQ0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  En el "Profeta" (00:38:07): <div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e82/109/09e/e8210909e53e8966a6484d35b096bf0f.gif"></div><br>  En “Avatar” (1: 41: 04–1: 41: 05), el algoritmo de nitidez, por cierto, es algo inusual en comparación con otras películas: primero se afila en ciertos lugares, y después de una fracción de segundo, muestra el resto de la imagen, t .e.  primero la mitad izquierda de la boca, y luego la derecha: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fff/54d/603/fff54d603344fb7b80c8a5c93aaca7ab.gif"></div><br>  En general, en películas muy populares que son vistas por cientos de millones, la nitidez de la imagen se realiza con un solo clic.  <b>¡Todas las personas (en las películas) lo hacen!</b>  Entonces, ¿por qué ustedes, expertos tan inteligentes, no pueden hacer esto? <br><br><hr>  "¡Sé que esto es fácil!"  ¡Y definitivamente me dijeron que estás haciendo esto!  ¿Eres demasiado vago para presionar este botón? <br><br>  <i>// Oh querido ... Malditos guionistas con su imaginación salvaje ...</i> <br><br>  - ¡Entiendo que estás ocupado, pero se trata de tu ayuda al estado para resolver un crimen importante! <br><br>  <i>// Lo entendemos.</i> <br><br>  - Tal vez se trata del dinero?  ¿Cuánto tienes que pagar? <br><br>  <i>// Bueno, cómo explicar brevemente que no es que no necesitemos dinero ... Y luego otra vez, y luego otra vez ...</i> <hr><br>  Cualquier coincidencia de las citas anteriores con diálogos reales es completamente aleatoria, pero, en particular, este texto está escrito con el fin de enviar a una persona a leerlo cuidadosamente primero, y solo luego devolver la llamada. <br><blockquote>  <b>Conclusión:</b> Debido al hecho de que la escena con la ampliación de las imágenes de las cámaras de CCTV con un solo clic se ha convertido en un sello del cine moderno, un gran número de personas está sinceramente convencido de que es muy simple ampliar un fragmento de un marco de una cámara barata o una grabadora de video barata.  Lo principal es cómo preguntar (bueno, o mandar, así de afortunado). </blockquote><h2>  ¿De dónde crecen las piernas? </h2><br>  Está claro que esta secuencia completa de llamadas no se toma desde cero.  Realmente hemos estado involucrados en la mejora del video durante aproximadamente 20 años, incluidos varios tipos de recuperación de video (y hay varios tipos de ellos, por cierto), y nuestros ejemplos serán más bajos en esta sección. <br><br>  Un aumento "inteligente" en la resolución en artículos científicos generalmente se denomina Super Resolución (SR para abreviar).  Google Scholar a pedido <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Super Resolution</a> encuentra 2.9 millones de artículos, es decir  el tema fue, por así decirlo, bastante bien desenterrado, y un gran número de personas se ocupó de él.  Si sigues <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el enlace</a> , entonces hay un mar de resultados, uno más hermoso que el otro.  Sin embargo, vale la pena cavar más profundo, la imagen, como de costumbre, se vuelve no tan pastoral.  El tema SR tiene dos direcciones: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Súper resolución de video</a> (0,4 millones de artículos): la restauración real utilizando fotogramas anteriores (y a veces posteriores), <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Superresolución de imagen</a> (2,2 millones de artículos): aumento "inteligente" de la resolución con solo un fotograma.  Dado que en el caso de una imagen para tomar información sobre lo que realmente no estaba en ningún lugar en este lugar, los algoritmos están completando (o, relativamente hablando, "completando") la imagen de una manera u otra, lo que podría estar allí.  El criterio principal para esto es que el resultado debe verse lo más natural posible, o estar lo más cerca posible del original.  Y está claro que tales métodos no son adecuados para restaurar lo que era "realmente", aunque agrandan la imagen para que se vea mejor, por ejemplo, al imprimir (cuando tiene una foto única, pero no hay una versión con mayor resolución ) Tales métodos son muy posibles. <br></li></ul><br>  Como puede ver, 0.4 millones versus 2.2, es decir, 5 veces menos personas participan en la recuperación real.  Afortunadamente, el tema "hazlo más grande, simplemente hermoso" tiene una gran demanda, incluso en la industria (el notorio zoom digital de teléfonos inteligentes y jaboneras digitales).  Además, si profundiza aún más, rápidamente queda claro que un número significativo de artículos sobre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Video Super Resolution</a> también es un aumento en la resolución de video sin recuperación, porque la recuperación es difícil.  Como resultado, podemos decir que aquellos que "lo hacen maravillosamente" son aproximadamente 10 veces más que aquellos que realmente están tratando de restaurar.  Una situación bastante común en la vida, por cierto. <br><br>  Vamos aún más profundo.  Muy a menudo, los resultados del algoritmo son muy buenos, pero necesita, por ejemplo, 20 cuadros hacia adelante y 20 cuadros hacia atrás, y la velocidad de procesamiento de un cuadro es de aproximadamente <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">15 minutos</a> cuando se usa la GPU más avanzada.  Es decir  durante 1 minuto, el video necesita 450 horas (casi 19 días).  Oops-ss ... De acuerdo, esto no se parece en nada al instante "¡Zoom!"  del cine  Regularmente hay algoritmos que funcionan durante varios días por fotograma.  Para los artículos, un mejor resultado suele ser más importante que el tiempo de funcionamiento, porque la aceleración es una tarea difícil por separado y es más fácil comer un elefante grande en partes.  Esta es la diferencia entre la vida y el cine ... <br><br>  La solicitud de algoritmos que se ejecutan en video a una velocidad razonable dio lugar a una dirección separada de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Super Resolución Rápida de Video</a> : 0.18 millones de artículos, incluidos artículos "lentos" que se comparan con los "rápidos", es decir.  El número real de artículos sobre tales métodos es exagerado.  Tenga en cuenta que entre los enfoques "rápidos", el porcentaje de especulación, es decir  sin recuperación real, más alto.  En consecuencia, el porcentaje de recuperación honesta es menor. <br><br>  La imagen, como ves, se está volviendo clara.  Pero esto, por supuesto, está lejos de todo. <br><br>  ¿Qué otros puntos afectan significativamente la obtención de un buen resultado? <br><br>  En primer lugar, el ruido es muy influyente.  A continuación se muestra un ejemplo de una doble restauración de la resolución en un video muy ruidoso: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/461/c9c/bd5/461c9cbd5b7b8d0a7b50f75471ebe941.png"></div><br>  <i>Fuente: materiales del autor.</i> <br><br>  El principal problema en este fragmento no es ni siquiera con los ruidos habituales, sino con el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">muaré de</a> color en la camisa, que es difícil de procesar.  Algunos podrían decir que los grandes ruidos no son un problema hoy.  Esto no es asi.  Mire los datos de los DVR de los automóviles y las cámaras de CCTV en la oscuridad (justo cuando tienen más demanda). <br><br>  Sin embargo, el muaré también puede ocurrir en video relativamente "limpio" en términos de ruido, como la ciudad a continuación (los ejemplos a continuación se <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">basan en nuestro trabajo</a> ): <br><br> <a href=""><img src="https://habrastorage.org/webt/0a/-x/0z/0a-x0zj47mmkgsa79aiojqtrfco.gif"></a> <br>  <i>Fuente: materiales del autor.</i> <br><br>  En segundo lugar, para una recuperación óptima, se necesita una predicción cercana al ideal del movimiento entre cuadros.  Por qué esto es difícil es un gran tema aparte, pero esto explica por qué las escenas con un movimiento de cámara panorámica a menudo se restauran muy bien, y las escenas con un movimiento relativamente caótico son extremadamente difíciles de recuperar, pero con ellas puede obtener un resultado bastante bueno en algunas situaciones: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4bd/7bb/3da/4bd7bb3da22d2904311d2da762331b5e.gif"></div>  <i>Fuente: materiales del autor.</i> <br><br>  Y finalmente, aquí hay un ejemplo de recuperación de texto: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a98/ad0/a00/a98ad0a0062a2fa1a7b4c602d26e2edd.png"></div><br>  <i>Fuente: materiales del autor.</i> <br><br>  Aquí, el fondo se mueve con bastante suavidad, y el algoritmo tiene la capacidad de "deambular": <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a5e/1a4/542/a5e1a4542ea2b0a441c6160e4c9eaba0.png"></div><br><br>  En particular, si comparamos una inscripción muy pequeña a la derecha de la mano, incluida la ampliación con la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">interpolación bicúbica</a> clásica, entonces la diferencia es muy claramente visible: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9d4/d00/dfc/9d4d00dfc6a3364dfae37e8637708993.png"></div><br>  Se puede ver que para la interpolación bicúbica es casi imposible leer el año, para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Lanczos4</a> , que es amado por aquellos que cambian semi-profesionalmente la resolución del video por la nitidez, los bordes son más claros, por supuesto, pero aún es imposible leer el año.  No hacemos comentarios sobre el Topacio comercial, pero leemos claramente la inscripción y puede ver que probablemente sea 1809. <br><blockquote>  <b>Conclusiones:</b> <br><br><ul><li>  Miles de investigadores en el mundo se dedican a aumentar la resolución, y se han publicado millones de artículos sobre este tema.  Debido a esto, cada teléfono inteligente tiene un "zoom digital", que generalmente es objetivamente mejor que los algoritmos para aumentar los programas convencionales, y cada televisor FullHD puede mostrar video SD, a menudo incluso sin artefactos característicos de cambio de resolución. <br></li><li>  La recuperación de una imagen real de un video es mucho menos del 10% de los involucrados en la Súper Resolución, además, la mayoría de los algoritmos de recuperación son extremadamente lentos (hasta varios días de cálculos por cuadro). <br></li><li>  En la mayoría de los casos, la recuperación está diseñada para garantizar que las altas frecuencias en el video se conserven más o menos y, por lo tanto, no funcionen en video con artefactos de compresión significativos.  Y dado que en la configuración de las cámaras de CCTV, la relación de compresión a menudo se elige en función del deseo de ahorrar más horas (es decir, el video se comprime más fuertemente y las altas frecuencias se "matan"), se hace casi imposible restaurar dicho video. <br></li></ul></blockquote><br><h2>  Cómo se ve SR en la industria </h2><br>  Para ser justos, observamos que hoy en día todos los algoritmos de resolución de resolución (y al menos comprados) están disponibles para todos los fabricantes de televisores (necesita hacer imágenes HD a partir de imágenes SD sobre la marcha), para todos los fabricantes de teléfonos inteligentes (lo que se llama "zoom digital" en publicidad), etc. .d.  Hablaremos sobre los resultados de Google (y no solo).  En primer lugar, porque Google es muy agradable y sin mucho pathos y marketing describe los resultados en su blog, y esto es extremadamente agradable.  En segundo lugar, porque los fabricantes de teléfonos inteligentes (por ejemplo, una empresa coreana muy conocida) no rehúsan usar, por ejemplo, Photoshop en la publicidad de sus tecnologías (cuál es la diferencia, la gente todavía se traga), y esto es desagradable.  En general, hablemos de aquellos que describen su tecnología con bastante honestidad. <br><br>  En <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">2016, Google publicó</a> resultados bastante interesantes del algoritmo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RAISR (Superresolución de imagen rápida y precisa)</a> utilizado en el teléfono inteligente Pixel 2. En las imágenes más exitosas, el resultado se veía genial: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/78c/785/e21/78c785e214d760cb58f8ea2e2d78b70a.png"></div><br>  <i>Fuente: <a href="">Blog de Google AI</a></i> <br><br>  El algoritmo fue un conjunto de filtros utilizados después de la clasificación ML, y en comparación con la interpolación bicúbica (niño de azotes tradicional), el resultado complació: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/67b/1f3/a53/67b1f3a5335bbced94d53be35ba3881d.png"></div><br>  <i>En orden: original, interpolación bicúbica, RAISR</i> <br><br>  Pero fue la Interpolación de un solo cuadro, y en ejemplos "infructuosos", como el follaje de abajo, la imagen se distorsionó muy desagradablemente, después de la ampliación la imagen se volvió notablemente "sintética".  Mostró exactamente el efecto por el cual no le gusta el zoom digital de los teléfonos inteligentes modernos: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/g0/y7/xw/g0y7xw8cipif-g-cw3yuljfmivg.png"></div><br>  El milagro, de hecho, no sucedió, y Google honesta e inmediatamente publicó un contraejemplo, es decir.  Inmediatamente describió los límites de aplicabilidad de su enfoque y salvó a las personas de expectativas excesivas (típico del marketing convencional). <br><br>  Sin embargo, menos de dos años después, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">se publicó</a> la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">continuación del trabajo</a> utilizado en Google Pixel 3 y mejora drásticamente la calidad de su disparo, que ya es una súper resolución honesta de múltiples cuadros, es decir.  algoritmo de recuperación de resolución de múltiples cuadros: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/83b/3ff/950/83b3ff950cf9959b2acb3d11a134ef50.gif"></div><br>  <i>Fuente: <a href="">Blog de Google AI</a></i> <br><br>  La imagen de arriba muestra una comparación de los resultados de Pixel 2 y Pixel 3, y los resultados se ven muy bien: la imagen realmente se volvió mucho más clara y se puede ver claramente que esto no está "pensando", sino realmente restaurando los detalles.  Además, un lector profesional atento tendrá preguntas sobre dos tubos gemelos verticales a la izquierda.  La resolución ha aumentado claramente, mientras que el paso de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">alias</a> (un signo de resolución real) parece extrañamente cercano.  Que fue eso <br><br>  En pocas palabras, analizaremos el algoritmo.  Los colegas pasaron de cambiar la interpolación del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">patrón de Bayer</a> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/355/0ba/e72/3550bae72349a1d9d9705e1e8c69cdd3.png"></div><br>  El hecho es que 2/3 de la información en una imagen real es en realidad información interpolada.  Es decir  su imagen YA está borrosa y "borrosa", pero con un nivel de ruido real esto no es tan significativo.  Por cierto, la capacidad de utilizar algoritmos de interpolación más sofisticados ha hecho que los programas populares de conversión RAW de la más alta calidad para fotografías (la diferencia entre el algoritmo simple integrado en cada cámara y el algoritmo complejo de un programa especializado generalmente se note a simple vista cuando la imagen se amplía). <br><br>  Los colegas de Google usan el hecho de que la gran mayoría de las fotos de teléfonos inteligentes se toman con las manos, es decir,  la cámara temblará ligeramente: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/864/59b/d05/86459bd058ec0edfdd5f3b1d0c9b287b.gif"></div><br>  <i>Fuente: <a href="">Blog de Google AI</a> (imagen de varios cuadros alineada a nivel de píxel para mostrar el desplazamiento de subpíxel)</i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como resultado, si toma unos pocos cuadros y evalúa el cambio (y el hierro, que puede construir un mapa de estimación de movimiento con una precisión de un cuarto de píxel, está en cualquier teléfono inteligente con soporte H.264), obtenemos un mapa de cambio. Fiel a la animación anterior, se ve claramente que con un nivel de ruido real, construir un mapa de desplazamiento con precisión de subpíxeles es una tarea muy poco trivial, pero han aparecido muy buenos algoritmos en esta área durante los últimos 20 años. Por supuesto, a veces, y les cuesta mucho. Por ejemplo, en el ejemplo anterior, algo parpadea en un marco en la parte superior de la barandilla de la escalera. Y esto sigue siendo una escena estática, no hay objetos en movimiento que a veces no solo se muevan, sino que giren, cambien de forma, se muevan rápidamente, dejando grandes áreas de apertura (cuyo bucle no debe ser visible después del procesamiento). El siguiente ejemplo muestra claramentequé sucede con los objetos que se mueven rápidamente, si desactiva el procesamiento especial de tales casos (deshabilitado a la izquierda, habilitado a la derecha, si hace clic, los bloques de procesamiento son claramente visibles):</font></font><br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/2e8/3d1/bb2/2e83d1bb215c546de4eb6d55524f865f.png"></a> <br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fuente: </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Blog de Google AI (se recomienda hacer clic y ver en alta resolución)</font></font></a></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ejemplos duros son las llamas, las ondas, el resplandor del sol en el agua, etc. En general, incluso en el problema "simple" de determinar el cambio, hay muchos momentos no triviales que complican significativamente la vida del algoritmo. Sin embargo, ahora no se trata de eso. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Curiosamente, incluso si la cámara está completamente estacionaria (por ejemplo, montada en un trípode), puede hacer que el sensor se mueva a través del control del </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">módulo de estabilización óptica</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (OIS - Estabilización óptica de imagen). Como resultado, obtenemos los cambios de subpíxeles deseados. En Pixel 3, se implementa la compatibilidad con OIS, y puede presionar el teléfono contra el cristal y observar con interés cómo OIS comienza </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a mover la imagen a lo largo de una elipse (aproximadamente, como este enlace)</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, es decir, incluso en este caso de montaje en un trípode, difícil para él, la Super Resolución podrá resolver y mejorar la calidad. </font><font style="vertical-align: inherit;">Sin embargo, la mayor parte de los disparos desde teléfonos inteligentes son disparos manuales. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como resultado, tenemos información adicional para construir una foto de mayor resolución:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/11d/67a/0a2/11d67a0a2127af61576cadffbfeba49b.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Como se mencionó anteriormente, la consecuencia directa de SR es una disminución significativa en el nivel de ruido, en algunos casos es muy notable: </font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3e1/cd0/1b1/3e1cd01b1e996fc11a6309dca47b0e17.png"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fuente: </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Blog de Google AI</font></font></a></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Tenga en cuenta que la recuperación también significa la restauración por el número de bits por componente.</font></font> Es decir<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Al resolver formalmente el problema de aumentar la resolución, el mismo motor en ciertas condiciones no solo puede suprimir el ruido, sino también convertir el marco en HDR. Está claro que hoy HDR rara vez se usa, pero esto, como puede ver, es una buena ventaja. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El siguiente ejemplo muestra una comparación de imágenes obtenidas al disparar en el Pixel 2 y en el Pixel 3 después de SR con una calidad de sensor comparable. La diferencia en el ruido y la diferencia en la claridad son claramente visibles:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/74c/17b/e38/74c17be383c81603e91124d9b4fd04c9.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para aquellos a quienes les gusta ver los detalles, hay </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un álbum</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en el que se puede apreciar la súper resolución de Google (nombre comercial Super Res Zoom) en todo su esplendor en el espectro de la escala de zoom de la imagen en un teléfono inteligente (cambio de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">FoV</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ): </font><font style="vertical-align: inherit;">cómo escriben modestamente: dieron un paso más cerca de la calidad de disparo de los teléfonos inteligentes a la calidad de las cámaras profesionales. </font><font style="vertical-align: inherit;">Para ser justos, observamos que las cámaras profesionales tampoco se quedan quietas. Otra cosa es que con ventas más pequeñas, las mismas tecnologías le costarán más al usuario. Sin embargo, SR ya está apareciendo en cámaras profesionales. </font><b><font style="vertical-align: inherit;">UPD:</font></b><font style="vertical-align: inherit;"> Como ejemplo (el último enlace es una comparación):</font></font><br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/321/e5e/40d/321e5e40d89b28612139434735c76fe0.png"></a> <br><br><font style="vertical-align: inherit;"></font><br><br><font style="vertical-align: inherit;"></font><br><br> <b><font style="vertical-align: inherit;"></font></b> <br><font style="vertical-align: inherit;"></font><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Testing Sony's New Pixel Shift Feature in the a7R III</a> ,     2     ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> </a>    ,               ), <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">   Olympus E-M5 Mark II</a>  16  40 , <br></li><li>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> Super Resolution   Pentax K-1</a> , <br></li><li>  : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pixel-Shift Shootout: Olympus vs. Pentax vs. Sony vs. Panasonic</a> —    <b>Pentax K-1, Sony a7R III, Olympus OM-D E-M1 Mark II  Panasonic Lumix DC-G9.</b> ,    ,         ,    Pentax K-1. <br></li></ul><br><blockquote> <b>:</b> <br><br><ul><li>  Super Resolution    ,        ,         . <br></li><li>  SR: Image Super Resolution —    ( ),     . <br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Los principales beneficios de los algoritmos de recuperación son la reducción de ruido, el refinamiento de los detalles, el HDR "más honesto", una calidad de imagen claramente visible y más alta en televisores de pantalla grande. </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Toda esta magnificencia fue posible gracias a un aumento cardinal (aproximadamente 3 órdenes de magnitud en número de operaciones) en la complejidad de los algoritmos de procesamiento de fotos, o más precisamente, un cuadro de video. </font></font><br></li></ul></blockquote><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Resultados Yandex </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Como todavía preguntarán en los comentarios, diré algunas palabras sobre Yandex, que publicó su versión de Super Resolution el año pasado: </font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/02a/ec9/09f/02aec909f0cfd75bf9902a25ee85acc9.png"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fuente: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://yandex.ru/blog/company/oldfilms</font></font></a></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Y aquí hay algunos ejemplos de dibujos animados:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d6a/e9e/2cb/d6ae9e2cba814e1c02a4060ccb545280.png"></div><br>  <i>Fuente: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://yandex.ru/blog/company/soyuzmultfilm</a></i> <br><br>  Que fue eso  Yandex repitió la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tecnología de Google en 2016</a> ? <br><br>  En <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">la página de descripción de</a> tecnología de Yandex (nombre comercial DeepHD) solo se vincula a Image Super Resolution.  Esto significa que obviamente hay contraejemplos en los que el algoritmo estropea la imagen y son más comunes que los algoritmos de recuperación honestos.  Pero alrededor del 80% de los artículos están dedicados al tema y el algoritmo es más fácil de implementar. <br><br>  Esta tecnología <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">también se describió</a> en un centro (es interesante que el autor del artículo sea un graduado de nuestro laboratorio), pero, como puede ver en los comentarios, los autores no respondieron ninguna de mis preguntas, mientras que respondieron las otras.  Y estos, más bien, no son los autores de los villanos, sino la política de la compañía (en otras publicaciones, si se mira de cerca, a menudo tampoco hay respuestas a las preguntas de los expertos).  Para los blogs de empresas de tecnología son reacios a profundizar en la discusión de la implementación o los detalles tecnológicos.  Especialmente si esto crea una mejor impresión de la tecnología / producto.  O los competidores pueden cortar lo mismo más rápido.  Una vez más, el marketing es responsable de las publicaciones, y este es su trabajo directo: crear una impresión favorable de los productos de la compañía, independientemente de la calidad de los productos mismos.  De ahí la frecuente desconfianza de la información proveniente del marketing. <br><br>  En general, vale la pena ser muy escéptico sobre las imágenes de las compañías de la serie "cómo hicimos todo bien" por las siguientes razones: <br><br><ul><li>  Los autores de algoritmos de procesamiento son conscientes de que prácticamente <b>no hay algoritmos que en algunos casos no generarían artefactos.</b>  Y, de hecho, una de las tareas clave del desarrollador es reducir el porcentaje de tales casos (o la visibilidad de los artefactos en tales casos) mientras se mantiene la calidad en otros casos.  Y muy a menudo esto NO tiene éxito: <br><br><ul><li>  O los artefactos son tan fuertes y difíciles de arreglar que se rechaza todo el enfoque.  En realidad este es el caso, quizás (¡sorpresa-sorpresa!), De la mayoría de los artículos.  Imágenes divinas en algunos casos (que fueron puestas a tierra) y "no funciona en absoluto" en el resto. <br></li><li>  O (y esta es una situación común para las empresas de tecnología práctica), tiene que sacrificar algo de calidad en promedio para que se puedan tolerar los artefactos en los peores casos. <br></li></ul><br></li></ul>  En consecuencia, cuando no se publican malos ejemplos (clásicos para empresas) o se publican de forma limitada y con valores predeterminados (clásicos para artículos), este es el caso más común de engañar a las personas sobre las propiedades de una tecnología / algoritmo. <br><br><ul><li>  <b>Otro concepto erróneo común con respecto a los algoritmos de procesamiento es el uso de parámetros (incluidos los parámetros internos) del algoritmo.</b>  Sucedió que los algoritmos tienen parámetros, y los usuarios, y esta es también la norma, prefieren tener a lo sumo un botón de "habilitar".  E incluso si hay configuraciones, el usuario masivo no las usa.  Es por eso que, cuando compran tecnología, "se detienen cien veces", vuelven a preguntar: "¿Es esta una máquina completa?"  y pide muchos ejemplos. <br><br><ul><li>  En consecuencia, una historia común es la publicación de un resultado que se obtuvo con ciertos parámetros.  Afortunadamente, el desarrollador los conoce bien, e incluso cuando hay cincuenta de ellos (¡la situación real!), Los recoge muy rápidamente para que la imagen sea mágica.  Exactamente estas imágenes a menudo van a la publicidad. <br></li><li>  Además, el desarrollador puede incluso estar en contra.  Marketing ve los nuevos ejemplos enviados y dice: "no hay nada visible en ellos, ¡en la última presentación tuvo ejemplos normales!"  Y luego pueden tratar de explicarles que los nuevos ejemplos son lo que la gente realmente ve, y en la última presentación, se mostraron resultados potenciales que pueden lograrse mediante estudios preliminares del inicio del proyecto.  Esto no molesta a nadie.  La gente obtendrá la imagen "donde puedes ver".  En algunos casos, incluso las grandes empresas usan Photoshop.  ¡El lío está servido, caballeros!  ) <br></li></ul><br></li></ul><ul><li>  Además, <b>cuando se trata de video, abre simplemente grandes espacios abiertos para la <s>máquina ... ¡</s> buen marketing!</b>  Por lo general, los cuadros se presentan y la calidad del video comprimido siempre oscila y depende de la masa de los parámetros.  Nuevamente, varias tecnologías pueden aplicarse correctamente, el tiempo de procesamiento, nuevamente, puede ser diferente.  Y eso no es todo, el alcance es excelente. <br><br><ul><li>  La publicidad de Yandex afirma que la tecnología <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DeepHD funciona en tiempo real, por lo que hoy puede ver canales de televisión usándola</a> .  Se explicó anteriormente que la velocidad de operación es el talón de Aquiles de Super Resolución.  La ventaja de las redes neuronales, por supuesto, es que al estudiar durante mucho tiempo, pueden funcionar muy rápidamente en algunos casos, pero aún así (con gran interés profesional) buscaría en qué resolución y calidad funciona el algoritmo en tiempo real.  Por lo general, se crean varias modificaciones del algoritmo y a altas resoluciones en tiempo real, <b>muchos</b> "chips" (críticos para la calidad) tienen que ser desactivados.  <b>Demasiados</b> <br></li><li>  En <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ejemplos en blanco y negro</a> , una mirada más cercana revela que el brillo local está cambiando.  Dado que el SR correcto no cambia el brillo, parece que algún otro algoritmo funcionó, tal vez no uno (los resultados muestran que esto no es Procesamiento de un solo cuadro, o más bien, parece que no solo).  Si observa una pieza más grande (al menos 100 cuadros), la imagen será clara.  Sin embargo, medir la calidad del video es un tema separado y muy importante. <br></li></ul><br></li></ul><blockquote>  <b>Conclusiones:</b> <br><br><ul><li>  Debe comprender que los especialistas en marketing a menudo usan sus trucos precisamente porque funciona (¡y cómo!).  La gran mayoría de las personas <s>no leen el centro</s>  Lo que regularmente conduce a todo tipo de distorsiones.  ¡Deseo que todos se anuncien menos, especialmente cuando la narración de cuentos está en su mejor momento y realmente querrá creer en un milagro! <br></li><li>  Y, por supuesto, es muy bueno que Yandex también trabaje en el tema y haga su propio SR (más precisamente, su propia familia SR). <br></li></ul></blockquote><br><h2>  Perspectivas </h2><br>  Volvamos a donde empezamos.  ¿Qué hacer para aquellos que desean aumentar el video comprimido?  ¿Es todo esto malo? <br><br>  Como se describió anteriormente, incluso un ligero cambio en la imagen en la región, literalmente en el nivel de ruido, es crítico para los algoritmos de recuperación "honesta".  Es decir, las altas frecuencias en la imagen y su cambio entre cuadros son críticos. <br><br>  En este caso, lo principal debido a que se realiza la compresión de video es la eliminación del ruido entre cuadros.  En el ejemplo a continuación, la diferencia entre cuadros de un video ruidoso antes de la compensación de movimiento, después de la compensación (con compresión débil) y después de una compresión notable: sienta la diferencia (el contraste se eleva aproximadamente 6 veces para que se puedan ver los detalles): <br><img src="https://habrastorage.org/getpro/habr/post_images/4a0/867/c18/4a0867c1839ba6acea0aa16a7a5a408c.png"><br>  <i>Fuente: conferencias del autor sobre algoritmos de compresión.</i> <br><br>  Se puede ver claramente que desde el punto de vista del códec, el área ideal es el área en la que el movimiento en el que se compensó por completo y en el que no es necesario gastar más bits.  Bueno, se puede gastar un poco, algo mínimamente corregido.  Y puede haber bastantes áreas de este tipo.  Por lo tanto, Super Resolution pierde su "pan principal": información sobre lo que hay en este lugar en otros cuadros, teniendo en cuenta el desplazamiento de subpíxeles. <br><br>  Si mira los artículos, incluso para un JPEG relativamente simple, la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">restauración de jpeg</a> contiene 26 mil resultados, y para la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">recuperación de jpeg</a> : 52 mil, y esto junto con la restauración de archivos rotos, etc.  Para el video, la situación es peor que la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">restauración de MPEG</a> : 22 mil, es decir  El trabajo, por supuesto, está en marcha, pero la escala del trabajo en Super Resolución no es comparable.  Hay aproximadamente un orden de magnitud menos trabajo que restaurar la resolución de video y dos órdenes de magnitud menos que la Superresolución de imagen.  Dos pedidos es mucho.  También hicimos un acercamiento al proyectil (ya que hemos estado haciendo compresión y procesamiento durante mucho tiempo), hay algo con lo que trabajar, especialmente si la calidad es oscilante o se usa algo como M-JPEG (más recientemente, una imagen común en video vigilancia).  Pero todos estos serán casos especiales. <br><br>  Los resultados de los artículos de los enlaces anteriores también muestran que los resultados a veces son muy hermosos, pero se obtienen para casos muy especiales.  Es decir  mañana, en todos los teléfonos inteligentes, esta función, por desgracia, no aparecerá.  Estas son malas noticias.  Bien, pasado mañana y en una computadora con una buena GPU, aparecerá con seguridad. <br><br>  Razones: <br><br><ul><li>  Los dispositivos de almacenamiento (tarjetas SD para registradores, discos para cámaras de CCTV, etc.) se están volviendo gradualmente más baratos y la tasa de bits promedio para guardar video está aumentando. <br></li><li>  Además, durante la compresión, cambian gradualmente a los estándares de las próximas generaciones (por ejemplo, en HEVC), lo que significa una mejora notable en la calidad con la misma tasa de bits.  Los últimos 2 puntos significan que gradualmente la calidad del video será más alta y, a partir de cierto punto, los algoritmos de Superresolución de video bien desarrollados comenzarán a funcionar. <br></li><li>  Finalmente, se están mejorando los algoritmos.  Los logros de los algoritmos basados ​​en el aprendizaje automático en los últimos 4 años son especialmente buenos.  En este sentido, con alta probabilidad podemos esperar algo como esto: <br></li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/fce/82a/6a9/fce82a6a91aa6c5d654bd22c26bbf3d4.png"><br><br>  Es decir  el algoritmo usará explícitamente la información de movimiento recibida del códec, y luego estos datos se enviarán a una red neuronal capacitada para recuperar artefactos específicos de códecs específicos.  Tal esquema actualmente parece bastante factible. <br><br>  Pero en cualquier caso, debe comprender claramente que la recuperación actual es, por regla general, un aumento de 2 veces en la resolución.  Con menos frecuencia, en algunos casos, cuando el material de origen no se comprimió o casi no se comprimió, podemos hablar de 3 a 4 veces.  Como puede ver, esto ni siquiera es cerca de 100-1000 veces el aumento de las películas, cuando 1,5 píxeles de una grabación nocturna con ruido se convierten en un número de automóvil de excelente calidad.  Al género de "ciencia ficción" se le debe asignar, de hecho, un mayor porcentaje de películas y programas de televisión. <br><br>  Y, por supuesto, habrá intentos de hacer algo universal, en el marco de la tendencia de la moda "lo principal es cortar más capas".  Y aquí vale la pena advertir contra las reacciones de "aplausos" a los materiales publicitarios sobre este tema.  Las redes neuronales son el marco más conveniente para demostrar milagros y todo tipo de especulaciones.  Lo principal es elegir correctamente la muestra de entrenamiento y los ejemplos finales.  Y voila!  ¡Mira el milagro!  Muy conveniente en términos de apresurar a los inversores, por cierto.  Es decir, es extremadamente importante que la eficiencia de las tecnologías sea confirmada por alguien independiente en una gran cantidad de ejemplos heterogéneos, lo que rara vez se demuestra.  Para las empresas, incluso dar uno o dos ejemplos cuando la tecnología no funciona, hoy se equipara a una hazaña civil. <br><br>  Bueno, para que la vida no parezca cariño, te recordaré que la llamada transcodificación es popular hoy en día, cuando de hecho tienes que trabajar con un video que originalmente fue reducido por un algoritmo y luego reducido por otro, mientras se usan otros vectores de movimiento, los altos se destruyen nuevamente. frecuencias etc.  Y el hecho de que una persona vea todo bien allí no significa que el algoritmo que procesa tal video realmente haga milagros.  No será posible restaurar videos muy pellizcados, aunque en general la Super Resolución se desarrollará rápidamente en los próximos 10 años. <br><br><blockquote>  <b>Conclusiones:</b> <br><br><ul><li>  Recuerda que lo que ves en las películas y cómo es en la vida real es muy diferente.  ¡Y no solo en términos de recuperación de video altamente comprimido! <br></li><li>  Por lo general, los algoritmos modernos aumentan las resoluciones 2 veces, con menos frecuencia, un poco más, es decir  No 50 veces, conocido por las películas, pronto tendrá que esperar. <br></li><li>  El área de Superresolución está en auge y puede esperar el desarrollo activo de Video Restoration en los próximos años, incluida la recuperación después de la compresión. <br></li><li>  Pero lo primero que veremos es todo tipo de especulaciones sobre el tema, cuando los resultados demostrados exagerarán en gran medida las capacidades reales de los algoritmos.  Ten cuidado <br></li></ul></blockquote>  A finales del año pasado, dimos una conferencia "Redes neuronales en el procesamiento de video: mitos y realidad".  Quizás podamos ponerla aquí. <br><br>  Estén atentos! <br><br><h2>  Agradecimientos </h2><br>  Me gustaría agradecerle cordialmente: <br><br><ul><li>  Laboratorio de Computación Gráfica VMK Universidad Estatal de Moscú  MV Lomonosov para potencia informática y no solo <br></li><li>  nuestros colegas del grupo de videos, gracias a quienes se crearon los algoritmos anteriores, y especialmente a Karen Simonyan, la autora del artículo cuyos resultados se mostraron anteriormente y que ahora trabaja en Google DeepMind, <br></li><li>  personalmente Konstantin Kozhemyakov, quien hizo mucho para hacer este artículo mejor y más visual, <br></li><li>  Google por un excelente blog y descripciones relativamente correctas de las tecnologías creadas, y Yandex por competir muy bien en un frente amplio: Google es prácticamente el único ejemplo exitoso en un país donde los servicios de Google no están prohibidos, <br></li><li>  Habrovchan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" class="user_link">denisshabr</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" class="user_link">JamboJet</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" class="user_link">iMADik</a> por la sugerencia y los enlaces a cámaras profesionales SR de fotogramas múltiples, <br></li><li>  y finalmente, muchas gracias a Vyacheslav Napadovsky, Evgeny Kuptsov, Stanislav Grokholsky, Ivan Molodetsky, Alexei Soloviev, Evgeny Lyapustin, Yegor Sklyarov, Denis Kondranin, Alexandra Anzina, Roman Kazantsev y Gleb Ishelev por esta gran cantidad de comentarios útiles. mejor! <br></li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/439766/">https://habr.com/ru/post/439766/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../439756/index.html">¿Por qué necesito un generador termoacústico?</a></li>
<li><a href="../439758/index.html">A medio camino "Juno"</a></li>
<li><a href="../439760/index.html">Los ingenieros "torcieron" la luz en la fibra: una nueva tecnología acelerará la transferencia de datos cientos de veces</a></li>
<li><a href="../439762/index.html">En educación superior, programadores y trabajo de cuello azul</a></li>
<li><a href="../439764/index.html">Frutas corporativas</a></li>
<li><a href="../439768/index.html">¿Cómo funciona un código de barras?</a></li>
<li><a href="../439772/index.html">Escribir pruebas unitarias en Swift para probar tareas asincrónicas</a></li>
<li><a href="../439774/index.html">Prueba automática de selectores redux en la aplicación</a></li>
<li><a href="../439776/index.html">Frontend Weekly Digest (4-10 de febrero de 2019)</a></li>
<li><a href="../439778/index.html">El resumen de materiales frescos del mundo del front-end para la última semana No. 351 (del 4 al 10 de febrero de 2019)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>