<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘¨â€âœˆï¸ ğŸ§œğŸ½ ğŸ‘‡ğŸ» CuriositÃ© et procrastination dans l'apprentissage automatique ğŸ‘©ğŸ¿â€ğŸ’¼ ğŸ‘¨â€â¤ï¸â€ğŸ‘¨ ğŸ¥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="L'apprentissage renforcÃ© (RL) est l'une des techniques d'apprentissage automatique les plus prometteuses qui est activement dÃ©veloppÃ©e. Ici, l'agent I...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>CuriositÃ© et procrastination dans l'apprentissage automatique</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/427847/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">L'apprentissage renforcÃ©</a> (RL) est l'une des techniques d'apprentissage automatique les plus prometteuses qui est activement dÃ©veloppÃ©e.  Ici, l'agent IA reÃ§oit une rÃ©compense positive pour les bonnes actions et une rÃ©compense nÃ©gative pour les mauvaises.  Cette mÃ©thode de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">carotte et de bÃ¢ton</a> est simple et universelle.  Avec cela, DeepMind a enseignÃ© l'algorithme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">DQN</a> pour jouer Ã  d'anciens jeux vidÃ©o Atari et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">AlphaGoZero</a> pour jouer Ã  l'ancien jeu Go.  OpenAI a donc appris Ã  l'algorithme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OpenAI-Five</a> Ã  jouer au jeu vidÃ©o Dota moderne, et Google a appris aux mains robotiques Ã  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">capturer de nouveaux objets</a> .  MalgrÃ© le succÃ¨s de RL, il existe encore de nombreux problÃ¨mes qui rÃ©duisent l'efficacitÃ© de cette technique. <br><br>  Les algorithmes RL ont du <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">mal Ã  travailler</a> dans un environnement oÃ¹ l'agent reÃ§oit rarement des commentaires.  Mais c'est typique du monde rÃ©el.  Par exemple, imaginez chercher votre fromage prÃ©fÃ©rÃ© dans un grand labyrinthe, comme un supermarchÃ©.  Vous cherchez et cherchez un dÃ©partement avec des fromages, mais vous ne pouvez pas le trouver.  Si Ã  chaque pas vous nâ€™obtenez ni Â«bÃ¢tonÂ» ni Â«carotteÂ», alors il est impossible de dire si vous allez dans la bonne direction.  En l'absence de rÃ©compense, qu'est-ce qui vous empÃªche de vous promener pour toujours?  Rien que peut-Ãªtre votre curiositÃ©.  Cela motive le passage au service d'Ã©picerie, qui ne semble pas familier. <br><a name="habracut"></a><br>  L'ouvrage scientifique, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Â«CuriositÃ© Ã©pisodique grÃ¢ce Ã  l'accessibilitÃ©Â»,</a> est le rÃ©sultat d'une collaboration entre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">l'Ã©quipe de Google Brain</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">DeepMind,</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la Swiss Higher Technical School de Zurich</a> .  Nous proposons un nouveau modÃ¨le de rÃ©compense RL basÃ© sur la mÃ©moire Ã©pisodique.  Elle ressemble Ã  une curiositÃ© qui vous permet d'explorer l'environnement.  Ã‰tant donnÃ© que l'agent doit non seulement Ã©tudier l'environnement, mais Ã©galement rÃ©soudre le problÃ¨me initial, notre modÃ¨le ajoute un bonus Ã  la rÃ©compense initialement clairsemÃ©e.  La rÃ©compense combinÃ©e n'est plus rare, ce qui permet aux algorithmes RL standard d'en tirer des leÃ§ons.  Ainsi, notre mÃ©thode de curiositÃ© Ã©largit la gamme des tÃ¢ches qui peuvent Ãªtre rÃ©solues en utilisant RL. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4e9/462/46c/4e946246c97aafab60f3dbe954ca6ed2.png"><br>  <i><font color="gray">CuriositÃ© occasionnelle par l'accessibilitÃ©: des donnÃ©es d'observation sont ajoutÃ©es Ã  la mÃ©moire, la rÃ©compense est calculÃ©e en fonction de la distance entre l'observation actuelle et des observations similaires en mÃ©moire.</font></i>  <i><font color="gray">L'agent reÃ§oit une plus grande rÃ©compense pour les observations qui ne sont pas encore prÃ©sentÃ©es en mÃ©moire.</font></i> <br><br>  L'idÃ©e principale de la mÃ©thode est de stocker les observations de l'agent sur l'environnement dans la mÃ©moire Ã©pisodique, ainsi que de rÃ©compenser l'agent pour la visualisation des observations non encore prÃ©sentÃ©es en mÃ©moire.  Â«Manque de mÃ©moireÂ» est la dÃ©finition de la nouveautÃ© dans notre mÃ©thode.  La recherche de telles observations signifie la recherche d'un Ã©tranger.  Une telle envie de rechercher un Ã©tranger mÃ¨nera l'agent IA vers de nouveaux endroits, empÃªchant ainsi l'errance dans un cercle, et l'aidera finalement Ã  tomber sur la cible.  Comme nous le verrons plus tard, notre libellÃ© peut dissuader l'agent du comportement indÃ©sirable auquel sont soumis certains autres libellÃ©s.  Ã€ notre grande surprise, ce comportement prÃ©sente certaines similitudes avec ce qu'un profane appellerait Â«procrastinationÂ». <br><br><h4>  CuriositÃ© antÃ©rieure </h4><br>  Bien qu'il y ait eu de nombreuses tentatives pour formuler la curiositÃ© dans le passÃ© <sup>[1] [2] [3] [4]</sup> , dans cet article, nous nous concentrerons sur une approche naturelle et trÃ¨s populaire: la curiositÃ© par la surprise basÃ©e sur les prÃ©visions.  Cette technique est dÃ©crite dans un article rÃ©cent, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Â«EnquÃªter sur un environnement utilisant la curiositÃ© en prÃ©disant sous son propre contrÃ´leÂ»</a> (gÃ©nÃ©ralement appelÃ© ICM).  Pour illustrer le lien entre surprise et curiositÃ©, nous utilisons Ã  nouveau l'analogie de trouver du fromage dans un supermarchÃ©. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b93/003/a3c/b93003a3c6790968f3922777926a494a.jpg"><br>  <i><font color="gray">Illustration d' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Indira Pasko</a> , sous licence <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">CC BY-NC-ND 4.0</a></font></i> <br><br>  En vous promenant dans le magasin, vous essayez de prÃ©dire l'avenir ( <i>"Maintenant, je suis dans le rayon viande, donc je pense que le rayon du coin est le rayon poisson, ils sont gÃ©nÃ©ralement Ã  proximitÃ© dans cette chaÃ®ne de supermarchÃ©s"</i> ).  Si la prÃ©vision est incorrecte, vous Ãªtes surpris ( <i>"En fait, il y a un dÃ©partement de lÃ©gumes. Je ne m'y attendais pas!"</i> ) - et de cette faÃ§on, vous obtenez une rÃ©compense.  Cela augmente la motivation Ã  l'avenir pour regarder Ã  nouveau au coin de la rue, en explorant de nouveaux endroits uniquement pour vÃ©rifier que vos attentes sont vraies (et Ã©ventuellement tomber sur du fromage). <br><br>  De mÃªme, la mÃ©thode ICM construit un modÃ¨le prÃ©dictif de la dynamique du monde et donne Ã  l'agent une rÃ©compense si le modÃ¨le ne parvient pas Ã  faire de bonnes prÃ©dictions - un marqueur de surprise ou de nouveautÃ©.  Veuillez noter que l'exploration de nouveaux endroits n'est pas directement articulÃ©e dans la curiositÃ© de l'ICM.  Pour la mÃ©thode ICM, y assister n'est qu'un moyen d'obtenir plus de Â«surprisesÂ» et ainsi maximiser votre rÃ©compense globale.  Il s'avÃ¨re que dans certains environnements, il peut y avoir d'autres faÃ§ons de se surprendre, ce qui conduit Ã  des rÃ©sultats inattendus. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a56/253/b56/a56253b56c9a991ce20b2c744f42d5a9.gif"></div><br>  <i><font color="gray">Un agent avec un systÃ¨me de curiositÃ© basÃ© sur la surprise se fige lorsqu'il rencontre un tÃ©lÃ©viseur.</font></i>  <i><font color="gray">Animation de la vidÃ©o de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Deepak Patak</a> , sous licence <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">CC BY 2.0</a></font></i> <br><br><h4>  Le danger de la procrastination </h4><br>  Dans l'article <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Â«A Large-Scale Study of Curiosity-Based LearningÂ», les</a> auteurs de la mÃ©thode ICM, avec les chercheurs d'OpenAI, montrent un danger cachÃ© de maximiser la surprise: les agents peuvent apprendre Ã  se laisser aller Ã  la procrastination au lieu de faire quelque chose d'utile pour la tÃ¢che.  Pour comprendre pourquoi cela se produit, envisagez une expÃ©rience de pensÃ©e que les auteurs appellent le Â«problÃ¨me du bruit de la tÃ©lÃ©visionÂ».  Ici, l'agent est placÃ© dans un labyrinthe avec la tÃ¢che de trouver un article trÃ¨s utile (comme Â«fromageÂ» dans notre exemple).  L'environnement a une tÃ©lÃ©vision et l'agent a une tÃ©lÃ©commande.  Le nombre de chaÃ®nes est limitÃ© (chaque chaÃ®ne a une transmission distincte) et chaque pression sur la tÃ©lÃ©commande fait passer le tÃ©lÃ©viseur sur une chaÃ®ne alÃ©atoire.  Comment un agent agira-t-il dans un tel environnement? <br><br>  Si la curiositÃ© se forme sur la base de la surprise, un changement de chaÃ®ne donnera plus de rÃ©compenses, car chaque changement est imprÃ©visible et inattendu.  Il est important de noter que mÃªme aprÃ¨s un balayage cyclique de toutes les chaÃ®nes disponibles, une sÃ©lection alÃ©atoire d'une chaÃ®ne garantit que chaque nouveau changement sera toujours inattendu - l'agent fait une prÃ©diction qu'il affichera la tÃ©lÃ©vision aprÃ¨s avoir changÃ© de chaÃ®ne, et trÃ¨s probablement les prÃ©visions se rÃ©vÃ©leront incorrectes, ce qui surprendra.  Il est important de noter que mÃªme si l'agent a dÃ©jÃ  vu chaque transmission sur chaque canal, le changement est toujours imprÃ©visible.  Pour cette raison, l'agent, au lieu de chercher un article trÃ¨s utile, restera finalement devant le tÃ©lÃ©viseur - semblable Ã  la procrastination.  Comment changer la formulation de la curiositÃ© pour Ã©viter ce comportement? <br><br><h4>  CuriositÃ© Ã©pisodique </h4><br>  Dans l'article <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Â«CuriositÃ© Ã©pisodique grÃ¢ce Ã  l'accessibilitÃ©Â»,</a> nous examinons un modÃ¨le de curiositÃ© basÃ© sur la mÃ©moire Ã©pisodique qui est moins sujet au plaisir instantanÃ©.  Pourquoi  Si nous prenons l'exemple ci-dessus, aprÃ¨s un certain temps de commutation de canaux, toutes les transmissions finiront par finir en mÃ©moire.  Ainsi, le tÃ©lÃ©viseur perdra de son attrait: mÃªme si l'ordre dans lequel les programmes apparaissent Ã  l'Ã©cran est alÃ©atoire et imprÃ©visible, ils sont tous en mÃ©moire!  C'est la principale diffÃ©rence avec la mÃ©thode basÃ©e sur la surprise: notre mÃ©thode n'essaie mÃªme pas de prÃ©dire l'avenir, elle est difficile Ã  prÃ©voir (voire impossible).  Au lieu de cela, l'agent examine le passÃ© et vÃ©rifie s'il y a des observations dans la mÃ©moire <i>comme celle en</i> cours.  Ainsi, notre agent n'est pas sujet aux plaisirs instantanÃ©s, ce qui donne un "bruit de tÃ©lÃ©vision".  L'agent devra aller explorer le monde en dehors du tÃ©lÃ©viseur pour obtenir plus de rÃ©compenses. <br><br>  Mais comment dÃ©cider si un agent voit la mÃªme chose qui est stockÃ©e en mÃ©moire?  La vÃ©rification exacte des correspondances est inutile: dans un environnement rÃ©el, un agent voit rarement la mÃªme chose deux fois.  Par exemple, mÃªme si l'agent retourne dans la mÃªme piÃ¨ce, il verra toujours cette piÃ¨ce sous un angle diffÃ©rent. <br><br>  Au lieu de vÃ©rifier les correspondances exactes, nous utilisons un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">rÃ©seau neuronal profond</a> qui est formÃ© pour mesurer la similitude de deux expÃ©riences.  Pour former ce rÃ©seau, il faut deviner Ã  quel point les observations se sont produites dans le temps.  La proximitÃ© dans le temps est un bon indicateur pour savoir si deux observations doivent Ãªtre considÃ©rÃ©es comme faisant partie de la mÃªme.  Un tel apprentissage conduit Ã  un concept gÃ©nÃ©ral de nouveautÃ© par l'accessibilitÃ©, qui est illustrÃ© ci-dessous. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/52c/f43/629/52cf436298b7bbf44550ba9770e84f1b.png"><br>  <i><font color="gray">Le graphique d'accessibilitÃ© dÃ©finit la nouveautÃ©.</font></i>  <i><font color="gray">En pratique, ce graphique n'est pas disponible - par consÃ©quent, nous formons l'approximateur de rÃ©seau neuronal pour estimer le nombre d'Ã©tapes entre les observations</font></i> <br><br><h4>  RÃ©sultats expÃ©rimentaux </h4><br>  Pour comparer les performances de diffÃ©rentes approches pour dÃ©crire la curiositÃ©, nous les avons testÃ©es dans deux environnements 3D visuellement riches: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ViZDoom</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">DMLab</a> .  Dans ces conditions, l'agent s'est vu confier diverses tÃ¢ches, telles que trouver une cible dans le labyrinthe, collecter les bons objets et Ã©viter les mauvais.  Dans l'environnement DMLab, l'agent est Ã©quipÃ© par dÃ©faut d'un gadget fantastique comme un laser, mais si le gadget n'est pas nÃ©cessaire pour une tÃ¢che spÃ©cifique, l'agent ne peut pas l'utiliser librement.  Fait intÃ©ressant, basÃ© sur la surprise, l'agent ICM a utilisÃ© le laser trÃ¨s souvent, mÃªme s'il Ã©tait inutile de terminer la tÃ¢che!  Comme dans le cas de la tÃ©lÃ©vision, au lieu de chercher un objet de valeur dans le labyrinthe, il a prÃ©fÃ©rÃ© passer du temps Ã  tirer sur les murs, car cela donnait beaucoup de rÃ©compenses sous forme de surprise.  ThÃ©oriquement, le rÃ©sultat d'une photo murale devrait Ãªtre prÃ©visible, mais en pratique, il est trop difficile Ã  prÃ©voir.  Cela nÃ©cessite probablement une connaissance plus approfondie de la physique que celle dont dispose l'agent IA standard. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/497/4e6/be3/4974e6be39718b8ab6dbd9cdfcdab273.gif"></div><br>  <i><font color="gray">Un agent ICM surpris tire constamment dans le mur au lieu d'explorer le labyrinthe</font></i> <br><br>  Contrairement Ã  lui, notre agent maÃ®trise un comportement raisonnable pour Ã©tudier l'environnement.  Cela s'est produit parce qu'il n'essaie pas de prÃ©dire le rÃ©sultat de ses actions, mais cherche plutÃ´t des observations qui sont Â«plus Ã©loignÃ©esÂ» de celles qui sont dans la mÃ©moire Ã©pisodique.  En d'autres termes, l'agent poursuit implicitement des objectifs qui nÃ©cessitent plus d'efforts qu'un simple coup de feu contre le mur. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b5a/002/381/b5a00238194970be7a6d6cf8969ac7f7.gif"></div><br>  <i><font color="gray">Notre mÃ©thode dÃ©montre un comportement d'exploration environnementale intelligent.</font></i> <br><br>  Il est intÃ©ressant d'observer comment notre approche de la rÃ©compense punit un agent s'exÃ©cutant dans un cercle, car aprÃ¨s l'achÃ¨vement du premier cercle, l'agent ne rencontre pas de nouvelles observations et, par consÃ©quent, ne reÃ§oit aucune rÃ©compense: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/489/ef2/a8b/489ef2a8b67c8c087cff940bdfeeee9f.gif"></div><br>  <i><font color="gray">Visualisation des rÃ©compenses: le rouge correspond Ã  la rÃ©compense nÃ©gative, le vert au positif.</font></i>  <i><font color="gray">De gauche Ã  droite: carte de rÃ©compense, carte avec emplacements en mÃ©moire, vue Ã  la premiÃ¨re personne</font></i> <br><br>  Dans le mÃªme temps, notre mÃ©thode contribue Ã  une bonne Ã©tude de l'environnement: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/075/ae3/952/075ae39528e77477099fad13610d2e4a.gif"></div><br>  <i><font color="gray">Visualisation des rÃ©compenses: le rouge correspond Ã  la rÃ©compense nÃ©gative, le vert au positif.</font></i>  <i><font color="gray">De gauche Ã  droite: carte de rÃ©compense, carte avec emplacements en mÃ©moire, vue Ã  la premiÃ¨re personne</font></i> <br><br>  Nous espÃ©rons que nos travaux contribueront Ã  une nouvelle vague de recherche qui dÃ©passe le cadre de la technique de la surprise afin d'Ã©duquer les agents sur des comportements plus intelligents.  Pour une analyse approfondie de notre mÃ©thode, veuillez consulter la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">prÃ©impression du travail scientifique</a> . <br><br><h4>  Remerciements: </h4><br>  Ce projet est le rÃ©sultat d'une collaboration entre l'Ã©quipe de Google Brain, DeepMind, et la Swiss Higher Technical School de Zurich.  Groupe de recherche principal: Nikolay Savinov, Anton Raichuk, Rafael Marinier, Damien Vincent, Mark Pollefeys, Timothy Lillirap et Sylvain Zheli.  Nous remercions Olivier Pietkin, Carlos Riquelme, Charles Blundell et Sergey Levine d'avoir discutÃ© de ce document.  Nous remercions Indira Pasco pour son aide avec les illustrations. <br><br><h4>  RÃ©fÃ©rences Ã  la littÃ©rature: </h4><br>  [1] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Â«L'Ã©tude de l'environnement basÃ©e sur le comptage avec des modÃ¨les de densitÃ© neuronaleÂ»</a> , Georg Ostrovsky, Mark G. Bellemar, Aaron Van den Oord, Remy Munoz <br>  [2] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Â«Environnements d'apprentissage basÃ©s sur le</a> comptage <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pour l'apprentissage en profondeur avec renforcementÂ»</a> , Khaoran Tan, Rain Huthuft, Davis Foot, Adam Knock, Xi Chen, Yan Duan, John Schulman, Philip de Turk, Peter Abbel <br>  [3] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Â«Apprendre sans professeur pour localiser les objectifs d'une recherche Ã  motivation interneÂ»,</a> Alexander Pere, SÃ©bastien Forestier, Olivier Sigot, Pierre-Yves Udaye <br>  [4] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Â«VIME: l'intelligence pour maximiser les changements d'informationÂ»,</a> Rein Huthuft, Xi Chen, Yan Duan, John Schulman, Philippe de Turk, Peter Abbel </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr427847/">https://habr.com/ru/post/fr427847/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr427837/index.html">Les premiers jours dans l'Ã©quipe de dÃ©veloppement - comme cela arrive avec nous</a></li>
<li><a href="../fr427839/index.html">Autorisation utilisateur dans Django via GSSAPI et dÃ©lÃ©gation de droits utilisateur au serveur</a></li>
<li><a href="../fr427841/index.html">Escroquerie Magic Leap</a></li>
<li><a href="../fr427843/index.html">Comment bien dormir et bien dormir</a></li>
<li><a href="../fr427845/index.html">Comment installer un million d'Ã©toiles dans un iPhone</a></li>
<li><a href="../fr427849/index.html">Ligne droite avec TM. v3.0</a></li>
<li><a href="../fr427853/index.html">RÃ©flexions sur TDD. Pourquoi cette mÃ©thodologie n'est pas largement reconnue</a></li>
<li><a href="../fr427855/index.html">Mitaps MOSDROID dans FunCorp</a></li>
<li><a href="../fr427857/index.html">Questions fiscales et juridiques pour les pigistes dÃ©butants</a></li>
<li><a href="../fr427859/index.html">Pourquoi des compÃ©tences techniques au chef de projet: expliquer les cas</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>