<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤵🏿 🍱 🧓 Classificação da cobertura do solo usando o eo-learn. Parte 3 👧🏾 🥌 😌</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Quando você precisa de melhores resultados do que satisfatórios 


 Parte 1 
 Parte 2 





 A transição da zona do inverno para o verão é composta po...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Classificação da cobertura do solo usando o eo-learn. Parte 3</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/453354/"><p>  Quando você precisa de melhores resultados do que satisfatórios </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 1</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 2</a> </p><br><p><img src="https://habrastorage.org/webt/c0/ls/b2/c0lsb2it_c9qwggm74kdk3uglw4.png"></p><br><p> <em>A transição da zona do inverno para o verão é composta por imagens do Sentinel-2.</em>  <em>Você pode notar algumas diferenças nos tipos de cobertura na neve, descritas em um artigo anterior.</em> </p><a name="habracut"></a><br><h2 id="predislovie">  Prefácio </h2><br><p> As duas últimas semanas foram muito difíceis.  Publicamos a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">primeira</a> e a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">segunda</a> partes de nossos artigos sobre a classificação da capa em todo o país, usando a estrutura <code>eo-learn</code> .  <code>eo-learn</code> é uma biblioteca de código aberto para criar uma camada entre o recebimento e o processamento de imagens de satélite e o aprendizado de máquina.  Nos artigos anteriores dos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">exemplos,</a> indicamos apenas um pequeno subconjunto dos dados e mostramos os resultados apenas em uma pequena porcentagem de toda a área de interesse (AOI - área de interesse).  Eu sei que isso parece pelo menos não muito impressionante, e talvez muito rude da nossa parte.  Todo esse tempo você foi atormentado por questões de como você pode usar esse conhecimento e transferi-lo para o <em>próximo</em> nível. </p><br><p>  Não se preocupe, é para isso que serve o terceiro artigo desta série!  Pegue uma xícara de café e sente-se ... </p><br><h2 id="all-our-data-are-belong-to-you">  Todos os nossos dados pertencem a você! </h2><br><p>  Você já está sentado?  Talvez deixe o café na mesa por mais um segundo, porque agora você ouvirá as melhores notícias de hoje ... <br>  Na Sinergise, decidimos publicar o conjunto completo de dados para a Eslovênia para 2017.  De graça.  Você pode acessar livremente 200 GB de dados na forma de ~ 300 fragmentos de EOPatch, cada um aproximadamente do tamanho de 1000x1000, em uma resolução de 10m!  Você pode ler mais sobre o formato EOPatch na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">última postagem</a> sobre o <code>eo-learn</code> , mas, na verdade, é um contêiner para dados <em>geo-temporais de</em> EO (Observação da Terra) e não-EO: por exemplo, imagens de satélite, máscaras, mapas etc. </p><br><p><img src="https://habrastorage.org/webt/dc/nt/gy/dcntgywsu4la7pdpwegv5m6eskc.png"><br>  <em>Estrutura do EOPatch</em> ) </p><br><p>  Não invadimos quando baixamos esses dados.  Cada EOPatch contém imagens do Sentinel-2 L1C, a máscara <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">s2cloudless</a> correspondente e o mapa oficial da cobertura terrestre em formato raster! </p><br><p>  Os dados são armazenados no AWS S3 em: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">http://eo-learn.sentinel-hub.com/</a> </p><br><p>  Desserializar um objeto EOPatch é bastante simples: </p><br><pre> <code class="python hljs">EOPatch.load(<span class="hljs-string"><span class="hljs-string">'path_to_eopatches/eopatch-0x6/'</span></span>)</code> </pre> <br><p>  Como resultado, você obtém um objeto da seguinte estrutura: </p><br><pre> <code class="python hljs">EOPatch( data: { BANDS: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>), dtype=float32) } mask: { CLM: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=uint8) IS_DATA: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=uint8) IS_VALID: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=bool) } mask_timeless: { LULC: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=uint8) VALID_COUNT: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=int64) } meta_info: { maxcc: <span class="hljs-number"><span class="hljs-number">0.8</span></span> service_type: <span class="hljs-string"><span class="hljs-string">'wcs'</span></span> size_x: <span class="hljs-string"><span class="hljs-string">'10m'</span></span> size_y: <span class="hljs-string"><span class="hljs-string">'10m'</span></span> time_difference: datetime.timedelta(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">86399</span></span>) time_interval: (datetime.datetime(<span class="hljs-number"><span class="hljs-number">2017</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>), datetime.datetime(<span class="hljs-number"><span class="hljs-number">2017</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">31</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>)) } bbox: BBox(((<span class="hljs-number"><span class="hljs-number">370230.5261411405</span></span>, <span class="hljs-number"><span class="hljs-number">5085303.344972428</span></span>), (<span class="hljs-number"><span class="hljs-number">380225.31836121203</span></span>, <span class="hljs-number"><span class="hljs-number">5095400.767924464</span></span>)), crs=EPSG:<span class="hljs-number"><span class="hljs-number">32633</span></span>) timestamp: [datetime.datetime(<span class="hljs-number"><span class="hljs-number">2017</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>), ..., datetime.datetime(<span class="hljs-number"><span class="hljs-number">2017</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">25</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">15</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>)], length=<span class="hljs-number"><span class="hljs-number">80</span></span> )</code> </pre> <br><p>  O acesso aos vários atributos do EOPatch é o seguinte: </p><br><pre> <code class="python hljs">eopatch.timestamp eopatch.mask[<span class="hljs-string"><span class="hljs-string">'LULC'</span></span>] eopatch.data[<span class="hljs-string"><span class="hljs-string">'CLM'</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] eopatch.data[<span class="hljs-string"><span class="hljs-string">'BANDS'</span></span>][<span class="hljs-number"><span class="hljs-number">5</span></span>][..., [<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>]]</code> </pre> <br><h3 id="eoexecute-order-66">  Pedido EOExecute 66 </h3><br><p>  Ótimo, os dados estão carregando.  Enquanto aguardamos a conclusão desse processo, vamos dar uma olhada nos recursos de uma classe que ainda não foi discutida nesses artigos - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><code>EOExecutor</code></a> .  Este módulo está envolvido na execução e no monitoramento do pipeline e permite que o multi-threading seja usado sem esforços desnecessários.  Chega de pesquisas no Stack Overflow sobre como paralelizar o pipeline corretamente ou como fazer a barra de progresso funcionar nesse modo - já fizemos tudo por você! </p><br><p>  Além disso, ele lida com os erros que ocorrem e pode gerar um breve resumo do processo de execução.  Este último é o momento mais importante para garantir a repetibilidade dos seus resultados no futuro, para que o usuário não precise gastar um tempo precioso de trabalho procurando parâmetros que ele usou na última quinta-feira às 9 horas, após uma noite inteira de folia (não misture álcool e programação). vale a pena!).  Essa classe também pode gerar um bom gráfico de dependência para o pipeline, que você pode mostrar ao seu chefe! </p><br><p><img src="https://habrastorage.org/webt/_o/x7/0q/_ox70q41_uiebqp7opyqbeu0nx0.png"><br>  <em>Gráfico de dependência de pipeline gerado pelo <code>eo-learn</code></em> </p><br><h3 id="eksperimenty-s-mashinnym-obucheniem">  Experiências de aprendizado de máquina </h3><br><p>  Conforme prometido, este artigo tem como objetivo principal estudar diferentes modelos com o <code>eo-learn</code> usando os dados que fornecemos.  Abaixo, preparamos dois experimentos onde estudamos o efeito das nuvens e diferentes algoritmos de reamostragem durante a interpolação temporal no resultado final.  Depois de tudo isso, começaremos a trabalhar com redes de convolução (CNN) e compararemos os resultados de duas abordagens - análise pixel por pixel da árvore de decisão e aprendizado profundo usando redes neurais convolucionais. </p><br><p>  Infelizmente, não se pode dar uma resposta inequívoca a respeito de quais decisões devem ser tomadas durante os experimentos.  Você pode estudar a área de assunto mais profundamente e fazer suposições para decidir se o jogo vale a pena, mas, no final das contas, o trabalho continuará sendo por tentativa e erro. </p><br><h3 id="igraem-s-oblakami">  Brinque com as nuvens </h3><br><p>  As nuvens são uma grande dor no mundo da EO, especialmente quando se trata de algoritmos de aprendizado de máquina, onde você deseja determiná-las e removê-las do conjunto de dados para interpolação com base nos valores ausentes.  Mas qual é o tamanho do benefício desse procedimento?  Vale a pena?  Rußwurm e Körner, em seu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo Classificação</a> multitemporal de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cobertura da terra com codificadores recorrentes sequenciais,</a> até mostraram que, para aprendizado profundo, o processo de filtragem de nuvens é provavelmente absolutamente sem importância, pois o próprio classificador é capaz de detectar nuvens e ignorá-las. </p><br><p><img src="https://habrastorage.org/webt/gz/c8/zs/gzc8zsp0nrdjtgbewqqysxulaiu.png"><br>  Ativação da camada de entrada (superior) e da camada de modulação (inferior) na sequência de imagens de um fragmento específico para uma rede neural.  Você pode perceber que esse fragmento de rede aprendeu a criar máscaras na nuvem e filtrar os resultados obtidos.  (Página 9 em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://www.researchgate.net/publication/322975904_Multi-Temporal_Land_Cover_Classification_with_Sequential_Recurrent_Encoders</a> ) </p><br><p>  Recordamos brevemente a estrutura da etapa de filtragem de dados (para detalhes, consulte [artigo anterior] ()).  Depois de tirar os instantâneos do Sentinel-2, começamos a filtrar os instantâneos da nuvem.  Todas as imagens nas quais o número de pixels não nublados não exceda 80% estão sujeitas a triagem (os valores limite podem diferir para diferentes áreas de interesse).  Depois disso, para obter valores de pixel em dias arbitrários, máscaras de nuvem são usadas para não levar esses dados em consideração. </p><br><p>  No total, são possíveis quatro comportamentos: </p><br><ol><li>  <strong>com</strong> filtro de imagem, <strong>com</strong> máscaras de nuvem </li><li>  <strong>sem</strong> filtro de instantâneo, <strong>dadas</strong> máscaras de nuvem </li><li>  <strong>com</strong> filtro de imagem, excluindo máscaras de nuvens </li><li>  <strong>sem</strong> filtro de imagem, <strong>sem incluir</strong> máscaras de nuvem </li></ol><br><p><img src="https://habrastorage.org/webt/rd/3i/ne/rd3ineypd8f0akhs41yve8mtgso.png"><br>  <em>Exibição visual da pilha de imagens do satélite Sentinel-2.</em>  <em>Pixels transparentes à esquerda significam pixels ausentes devido à cobertura de nuvens.</em>  <em>A pilha central mostra os valores de pixel após filtrar as imagens e interpolá-las com uma máscara de nuvem (Caso 4), e a pilha à direita mostra o resultado da interpolação no caso sem filtrar as imagens e sem máscaras de nuvem (1).</em>  <em>(Note lane - aparentemente, o artigo contém um erro de digitação, e foi o contrário - caso 1 no centro e 4 à direita).</em> </p><br><p>  No último artigo, já executamos uma variação do caso 1 e mostramos os resultados, por isso vamos usá-los para comparação.  Preparar outros transportadores e treinar o modelo parece uma tarefa simples - você só precisa ter certeza de que estamos comparando os valores corretos.  Para fazer isso, basta usar o mesmo conjunto de pixels para treinar e validar o modelo. </p><br><p>  Os resultados são mostrados na tabela abaixo.  Você pode ver que, em geral, a influência das nuvens no resultado do modelo é bastante baixa!  Isso pode ser devido ao fato de o cartão de referência ser de muito boa qualidade e o modelo poder ignorar a maioria das imagens.  De qualquer forma, esse comportamento não pode ser garantido por nenhuma AOI, portanto, dedique um tempo para dar esse passo fora de seus modelos! </p><br><div class="scrollable-table"><table><thead><tr><th>  Modelo </th><th>  Precisão [%] </th><th>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">F_1</a> [%] </th></tr></thead><tbody><tr><td>  Sem filtros, sem máscara </td><td>  92,8 </td><td>  92,6 </td></tr><tr><td>  Sem filtros, com máscara </td><td>  94,2 </td><td>  93,9 </td></tr><tr><td>  Com filtro, sem máscara </td><td>  94,0 </td><td>  93,8 </td></tr><tr><td>  Com filtro, com máscara </td><td>  94,4 </td><td>  94,1 </td></tr></tbody></table></div><br><h3 id="vliyanie-raznyh-podhodov-k-resemplingu">  O impacto de diferentes abordagens de reamostragem </h3><br><p>  A escolha das opções de reamostragem temporal não é óbvia.  Por um lado, precisamos de uma matriz detalhada de imagens que mostre bem os detalhes das imagens de origem - queremos incluir o número mais próximo possível de imagens nos dados de origem.  Por outro lado, estamos limitados por recursos de computação.  A redução da etapa de reamostragem dobra o número de quadros após a interpolação e, portanto, aumenta o número de atributos usados ​​no treinamento.  Essa melhoria vale o custo dos recursos?  É isso que temos que descobrir. </p><br><p>  Para este experimento, usaremos a variação 1 da etapa anterior.  Após a interpolação, fazemos uma nova amostra com as seguintes variações: </p><br><ol><li>  Reamostragem uniforme com um intervalo de 16 dias </li><li>  Reamostragem uniforme com um intervalo de 8 dias </li><li>  Na escolha das "melhores" datas, o número coincide com o caso 2. </li></ol><br><p>  A amostra no caso 3 é baseada no maior número de datas comuns para todo o EOPatch na AOI selecionada <br><img src="https://habrastorage.org/webt/xg/qa/9w/xgqa9w17-oe4dbtxca22yejwhzo.png"><br>  <em>O gráfico mostra o número de fragmentos do EOPatch que contêm dados para cada dia de 2017 (azul).</em>  <em>As linhas vermelhas mostram as datas ideais para a reamostragem, que são baseadas nas datas das imagens do Sentinel-2 para a AOI de 2017.</em> </p><br><p>  Observando a tabela abaixo, você pode ver que os resultados não são muito impressionantes, como na experiência anterior.  Nos casos 2 e 3, a quantidade de tempo gasto dobra, mas a diferença com a abordagem inicial é menor que 1%.  Essas melhorias são muito discretas para uso prático, para que possamos considerar o intervalo de 16 dias adequado para a tarefa. </p><br><div class="scrollable-table"><table><thead><tr><th>  Modelo </th><th>  Precisão [%] </th><th>  F_1 [%] </th></tr></thead><tbody><tr><td>  Uniformemente a cada 16 dias </td><td>  94,4 </td><td>  94,1 </td></tr><tr><td>  Uniformemente a cada 8 dias </td><td>  94,5 </td><td>  94,3 </td></tr><tr><td>  Escolhendo as melhores datas </td><td>  94,6 </td><td>  94,4 </td></tr></tbody></table></div><br><p>  <em>Resultados de precisão geral e F1 ponderada para diferentes tubulações com uma alteração na abordagem da reamostragem.</em> </p><br><h2 id="glubokoe-obuchenie-ispolzuem-svyortochnuyu-neyronnuyu-set-cnn">  Aprendizagem Profunda: Usando a Rede Neural Convolucional (CNN) </h2><br><p>  O aprendizado profundo tornou-se a abordagem padrão para muitas tarefas, como visão computacional, processamento de palavras em linguagem natural e processamento de sinais.  Isso se deve à sua capacidade de extrair padrões de entradas multidimensionais complexas.  Abordagens clássicas de aprendizado de máquina (como árvores de decisão) têm sido usadas em muitas tarefas de dados geográficos temporais.  Redes convolucionais, por outro lado, foram usadas para analisar a correlação espacial entre imagens adjacentes.  Basicamente, seu uso foi limitado ao trabalho com imagens únicas. </p><br><p>  Queríamos estudar a arquitetura de modelos de aprendizagem profunda e tentar escolher uma que seja capaz de analisar os aspectos espaciais e temporais dos dados de satélite ao mesmo tempo. </p><br><p>  Para fazer isso, usamos o Netvork Totalmente Convolucional Temporal, TFCN, ou melhor, a extensão temporal para o U-Net, implementada no TensorFlow.  Mais especificamente, a arquitetura usa correlações espaço-temporais para melhorar o resultado.  Uma vantagem adicional é que a estrutura da rede permite representar melhor as relações espaciais em diferentes escalas, graças ao processo de codificação / decodificação no U-net.  Como nos modelos clássicos, na saída obtemos uma matriz bidimensional de rótulos, que compararemos com a verdade. </p><br><p><img src="https://habrastorage.org/webt/p0/jl/mg/p0jlmgxi9euwvodwonx4zrmezsw.png"></p><br><p>  Utilizamos o modelo treinado para prever as notas no conjunto de testes, e os valores obtidos foram verificados com a verdade.  No geral, a precisão foi de 84,4% e a F1 foi de 85,4%. </p><br><p><img src="https://habrastorage.org/webt/ol/z2/zj/olz2zjp3waghaak9hnirzcwa258.png"></p><br><p>  <em>Comparação de diferentes previsões para nossa tarefa.</em>  <em>Imagem visual (canto superior esquerdo), mapa de referência verdadeiro (canto superior direito), previsão do modelo LightGBM (canto inferior esquerdo) e previsão de rede U (canto inferior direito)</em> </p><br><p>  Esses resultados mostram apenas o trabalho inicial desse protótipo, que não é altamente otimizado para a tarefa atual.  Apesar disso, os resultados concordam com algumas estatísticas obtidas na região.  Para liberar o potencial de uma rede neural, é necessário otimizar a arquitetura (conjunto de atributos, profundidade da rede, número de circunvoluções), além de definir hiper parâmetros (velocidade de aprendizado, número de épocas, ponderação de classe).  Esperamos nos aprofundar ainda mais neste tópico (ha ha) e planejamos distribuir nosso código quando ele estiver de uma forma aceitável. </p><br><h3 id="drugie-eksperimenty">  Outras experiências </h3><br><p>  Você pode encontrar <em>várias</em> maneiras de melhorar seus resultados atuais, mas não podemos resolver nem experimentar todos eles.  É nesse momento que você aparece em cena!  Mostre o que você pode fazer com esse conjunto de dados e ajude-nos a melhorar os resultados! </p><br><p>  Por exemplo, em um futuro próximo, um de nossos colegas participará da classificação da capa com base na pilha temporal de imagens <em>individuais</em> usando redes de convolução.  A ideia é que algumas superfícies, por exemplo, artificiais, possam ser distinguidas sem características temporais - bastante espaciais.  Ficaremos felizes em escrever um artigo separado quando este trabalho levar a resultados! </p><br><h3 id="ot-perevodchika">  Do tradutor </h3><br><p>  Infelizmente, a próxima parte desta série de artigos não foi publicada, o que significa que os autores não mostraram exemplos de código fonte na criação da U-Net.  Como alternativa, posso oferecer as seguintes fontes: </p><br><ol><li>  <em>U-Net: Redes Convolucionais para Segmentação de Imagem Biomédica - Olaf Ronneberger, Philipp Fischer, Thomas Brox</em> é um dos artigos básicos sobre arquitetura de U-Net que não envolve dados temporais. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html</a> - A página de documentação do eo-learn, onde (possivelmente) está localizada uma versão mais recente dos pipelines de 1,2 partes. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://github.com/divamgupta/image-segmentation-keras</a> - Um repositório com várias redes implementadas usando keras.  Eu tenho algumas perguntas sobre implementações (elas são um pouco diferentes das descritas nos artigos originais), mas, em geral, as soluções são facilmente adaptadas para fins pessoais e estão funcionando. </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt453354/">https://habr.com/ru/post/pt453354/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt453342/index.html">Mitos sobre funcionários remotos que nos destruímos</a></li>
<li><a href="../pt453346/index.html">Tecnologias de armazenamento e proteção de dados - o terceiro dia no VMware EMPOWER 2019</a></li>
<li><a href="../pt453348/index.html">O que há dentro de assíncio</a></li>
<li><a href="../pt453350/index.html">Transmissão aberta do salão principal do RIT ++ 2019</a></li>
<li><a href="../pt453352/index.html">Como os drones fornecem medicamentos vitais no Gana</a></li>
<li><a href="../pt453356/index.html">Tendências atuais e recomendações sobre aglomeração de grandes instituições financeiras</a></li>
<li><a href="../pt453360/index.html">Cidade sem engarrafamentos</a></li>
<li><a href="../pt453362/index.html">HabraConf # 1 - para trás para back-end</a></li>
<li><a href="../pt453364/index.html">Uma história de lançamento que afetou tudo</a></li>
<li><a href="../pt453366/index.html">Como usar vírgulas em inglês: 15 regras e exemplos de erros</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>