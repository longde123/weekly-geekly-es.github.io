<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§µüèø üç± üßì Classifica√ß√£o da cobertura do solo usando o eo-learn. Parte 3 üëßüèæ ü•å üòå</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Quando voc√™ precisa de melhores resultados do que satisfat√≥rios 


 Parte 1 
 Parte 2 





 A transi√ß√£o da zona do inverno para o ver√£o √© composta po...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Classifica√ß√£o da cobertura do solo usando o eo-learn. Parte 3</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/453354/"><p>  Quando voc√™ precisa de melhores resultados do que satisfat√≥rios </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 1</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 2</a> </p><br><p><img src="https://habrastorage.org/webt/c0/ls/b2/c0lsb2it_c9qwggm74kdk3uglw4.png"></p><br><p> <em>A transi√ß√£o da zona do inverno para o ver√£o √© composta por imagens do Sentinel-2.</em>  <em>Voc√™ pode notar algumas diferen√ßas nos tipos de cobertura na neve, descritas em um artigo anterior.</em> </p><a name="habracut"></a><br><h2 id="predislovie">  Pref√°cio </h2><br><p> As duas √∫ltimas semanas foram muito dif√≠ceis.  Publicamos a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">primeira</a> e a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">segunda</a> partes de nossos artigos sobre a classifica√ß√£o da capa em todo o pa√≠s, usando a estrutura <code>eo-learn</code> .  <code>eo-learn</code> √© uma biblioteca de c√≥digo aberto para criar uma camada entre o recebimento e o processamento de imagens de sat√©lite e o aprendizado de m√°quina.  Nos artigos anteriores dos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">exemplos,</a> indicamos apenas um pequeno subconjunto dos dados e mostramos os resultados apenas em uma pequena porcentagem de toda a √°rea de interesse (AOI - √°rea de interesse).  Eu sei que isso parece pelo menos n√£o muito impressionante, e talvez muito rude da nossa parte.  Todo esse tempo voc√™ foi atormentado por quest√µes de como voc√™ pode usar esse conhecimento e transferi-lo para o <em>pr√≥ximo</em> n√≠vel. </p><br><p>  N√£o se preocupe, √© para isso que serve o terceiro artigo desta s√©rie!  Pegue uma x√≠cara de caf√© e sente-se ... </p><br><h2 id="all-our-data-are-belong-to-you">  Todos os nossos dados pertencem a voc√™! </h2><br><p>  Voc√™ j√° est√° sentado?  Talvez deixe o caf√© na mesa por mais um segundo, porque agora voc√™ ouvir√° as melhores not√≠cias de hoje ... <br>  Na Sinergise, decidimos publicar o conjunto completo de dados para a Eslov√™nia para 2017.  De gra√ßa.  Voc√™ pode acessar livremente 200 GB de dados na forma de ~ 300 fragmentos de EOPatch, cada um aproximadamente do tamanho de 1000x1000, em uma resolu√ß√£o de 10m!  Voc√™ pode ler mais sobre o formato EOPatch na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">√∫ltima postagem</a> sobre o <code>eo-learn</code> , mas, na verdade, √© um cont√™iner para dados <em>geo-temporais de</em> EO (Observa√ß√£o da Terra) e n√£o-EO: por exemplo, imagens de sat√©lite, m√°scaras, mapas etc. </p><br><p><img src="https://habrastorage.org/webt/dc/nt/gy/dcntgywsu4la7pdpwegv5m6eskc.png"><br>  <em>Estrutura do EOPatch</em> ) </p><br><p>  N√£o invadimos quando baixamos esses dados.  Cada EOPatch cont√©m imagens do Sentinel-2 L1C, a m√°scara <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">s2cloudless</a> correspondente e o mapa oficial da cobertura terrestre em formato raster! </p><br><p>  Os dados s√£o armazenados no AWS S3 em: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">http://eo-learn.sentinel-hub.com/</a> </p><br><p>  Desserializar um objeto EOPatch √© bastante simples: </p><br><pre> <code class="python hljs">EOPatch.load(<span class="hljs-string"><span class="hljs-string">'path_to_eopatches/eopatch-0x6/'</span></span>)</code> </pre> <br><p>  Como resultado, voc√™ obt√©m um objeto da seguinte estrutura: </p><br><pre> <code class="python hljs">EOPatch( data: { BANDS: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>), dtype=float32) } mask: { CLM: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=uint8) IS_DATA: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=uint8) IS_VALID: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=bool) } mask_timeless: { LULC: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=uint8) VALID_COUNT: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=int64) } meta_info: { maxcc: <span class="hljs-number"><span class="hljs-number">0.8</span></span> service_type: <span class="hljs-string"><span class="hljs-string">'wcs'</span></span> size_x: <span class="hljs-string"><span class="hljs-string">'10m'</span></span> size_y: <span class="hljs-string"><span class="hljs-string">'10m'</span></span> time_difference: datetime.timedelta(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">86399</span></span>) time_interval: (datetime.datetime(<span class="hljs-number"><span class="hljs-number">2017</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>), datetime.datetime(<span class="hljs-number"><span class="hljs-number">2017</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">31</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>)) } bbox: BBox(((<span class="hljs-number"><span class="hljs-number">370230.5261411405</span></span>, <span class="hljs-number"><span class="hljs-number">5085303.344972428</span></span>), (<span class="hljs-number"><span class="hljs-number">380225.31836121203</span></span>, <span class="hljs-number"><span class="hljs-number">5095400.767924464</span></span>)), crs=EPSG:<span class="hljs-number"><span class="hljs-number">32633</span></span>) timestamp: [datetime.datetime(<span class="hljs-number"><span class="hljs-number">2017</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>), ..., datetime.datetime(<span class="hljs-number"><span class="hljs-number">2017</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">25</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">15</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>)], length=<span class="hljs-number"><span class="hljs-number">80</span></span> )</code> </pre> <br><p>  O acesso aos v√°rios atributos do EOPatch √© o seguinte: </p><br><pre> <code class="python hljs">eopatch.timestamp eopatch.mask[<span class="hljs-string"><span class="hljs-string">'LULC'</span></span>] eopatch.data[<span class="hljs-string"><span class="hljs-string">'CLM'</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] eopatch.data[<span class="hljs-string"><span class="hljs-string">'BANDS'</span></span>][<span class="hljs-number"><span class="hljs-number">5</span></span>][..., [<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>]]</code> </pre> <br><h3 id="eoexecute-order-66">  Pedido EOExecute 66 </h3><br><p>  √ìtimo, os dados est√£o carregando.  Enquanto aguardamos a conclus√£o desse processo, vamos dar uma olhada nos recursos de uma classe que ainda n√£o foi discutida nesses artigos - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><code>EOExecutor</code></a> .  Este m√≥dulo est√° envolvido na execu√ß√£o e no monitoramento do pipeline e permite que o multi-threading seja usado sem esfor√ßos desnecess√°rios.  Chega de pesquisas no Stack Overflow sobre como paralelizar o pipeline corretamente ou como fazer a barra de progresso funcionar nesse modo - j√° fizemos tudo por voc√™! </p><br><p>  Al√©m disso, ele lida com os erros que ocorrem e pode gerar um breve resumo do processo de execu√ß√£o.  Este √∫ltimo √© o momento mais importante para garantir a repetibilidade dos seus resultados no futuro, para que o usu√°rio n√£o precise gastar um tempo precioso de trabalho procurando par√¢metros que ele usou na √∫ltima quinta-feira √†s 9 horas, ap√≥s uma noite inteira de folia (n√£o misture √°lcool e programa√ß√£o). vale a pena!).  Essa classe tamb√©m pode gerar um bom gr√°fico de depend√™ncia para o pipeline, que voc√™ pode mostrar ao seu chefe! </p><br><p><img src="https://habrastorage.org/webt/_o/x7/0q/_ox70q41_uiebqp7opyqbeu0nx0.png"><br>  <em>Gr√°fico de depend√™ncia de pipeline gerado pelo <code>eo-learn</code></em> </p><br><h3 id="eksperimenty-s-mashinnym-obucheniem">  Experi√™ncias de aprendizado de m√°quina </h3><br><p>  Conforme prometido, este artigo tem como objetivo principal estudar diferentes modelos com o <code>eo-learn</code> usando os dados que fornecemos.  Abaixo, preparamos dois experimentos onde estudamos o efeito das nuvens e diferentes algoritmos de reamostragem durante a interpola√ß√£o temporal no resultado final.  Depois de tudo isso, come√ßaremos a trabalhar com redes de convolu√ß√£o (CNN) e compararemos os resultados de duas abordagens - an√°lise pixel por pixel da √°rvore de decis√£o e aprendizado profundo usando redes neurais convolucionais. </p><br><p>  Infelizmente, n√£o se pode dar uma resposta inequ√≠voca a respeito de quais decis√µes devem ser tomadas durante os experimentos.  Voc√™ pode estudar a √°rea de assunto mais profundamente e fazer suposi√ß√µes para decidir se o jogo vale a pena, mas, no final das contas, o trabalho continuar√° sendo por tentativa e erro. </p><br><h3 id="igraem-s-oblakami">  Brinque com as nuvens </h3><br><p>  As nuvens s√£o uma grande dor no mundo da EO, especialmente quando se trata de algoritmos de aprendizado de m√°quina, onde voc√™ deseja determin√°-las e remov√™-las do conjunto de dados para interpola√ß√£o com base nos valores ausentes.  Mas qual √© o tamanho do benef√≠cio desse procedimento?  Vale a pena?  Ru√üwurm e K√∂rner, em seu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo Classifica√ß√£o</a> multitemporal de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cobertura da terra com codificadores recorrentes sequenciais,</a> at√© mostraram que, para aprendizado profundo, o processo de filtragem de nuvens √© provavelmente absolutamente sem import√¢ncia, pois o pr√≥prio classificador √© capaz de detectar nuvens e ignor√°-las. </p><br><p><img src="https://habrastorage.org/webt/gz/c8/zs/gzc8zsp0nrdjtgbewqqysxulaiu.png"><br>  Ativa√ß√£o da camada de entrada (superior) e da camada de modula√ß√£o (inferior) na sequ√™ncia de imagens de um fragmento espec√≠fico para uma rede neural.  Voc√™ pode perceber que esse fragmento de rede aprendeu a criar m√°scaras na nuvem e filtrar os resultados obtidos.  (P√°gina 9 em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://www.researchgate.net/publication/322975904_Multi-Temporal_Land_Cover_Classification_with_Sequential_Recurrent_Encoders</a> ) </p><br><p>  Recordamos brevemente a estrutura da etapa de filtragem de dados (para detalhes, consulte [artigo anterior] ()).  Depois de tirar os instant√¢neos do Sentinel-2, come√ßamos a filtrar os instant√¢neos da nuvem.  Todas as imagens nas quais o n√∫mero de pixels n√£o nublados n√£o exceda 80% est√£o sujeitas a triagem (os valores limite podem diferir para diferentes √°reas de interesse).  Depois disso, para obter valores de pixel em dias arbitr√°rios, m√°scaras de nuvem s√£o usadas para n√£o levar esses dados em considera√ß√£o. </p><br><p>  No total, s√£o poss√≠veis quatro comportamentos: </p><br><ol><li>  <strong>com</strong> filtro de imagem, <strong>com</strong> m√°scaras de nuvem </li><li>  <strong>sem</strong> filtro de instant√¢neo, <strong>dadas</strong> m√°scaras de nuvem </li><li>  <strong>com</strong> filtro de imagem, excluindo m√°scaras de nuvens </li><li>  <strong>sem</strong> filtro de imagem, <strong>sem incluir</strong> m√°scaras de nuvem </li></ol><br><p><img src="https://habrastorage.org/webt/rd/3i/ne/rd3ineypd8f0akhs41yve8mtgso.png"><br>  <em>Exibi√ß√£o visual da pilha de imagens do sat√©lite Sentinel-2.</em>  <em>Pixels transparentes √† esquerda significam pixels ausentes devido √† cobertura de nuvens.</em>  <em>A pilha central mostra os valores de pixel ap√≥s filtrar as imagens e interpol√°-las com uma m√°scara de nuvem (Caso 4), e a pilha √† direita mostra o resultado da interpola√ß√£o no caso sem filtrar as imagens e sem m√°scaras de nuvem (1).</em>  <em>(Note lane - aparentemente, o artigo cont√©m um erro de digita√ß√£o, e foi o contr√°rio - caso 1 no centro e 4 √† direita).</em> </p><br><p>  No √∫ltimo artigo, j√° executamos uma varia√ß√£o do caso 1 e mostramos os resultados, por isso vamos us√°-los para compara√ß√£o.  Preparar outros transportadores e treinar o modelo parece uma tarefa simples - voc√™ s√≥ precisa ter certeza de que estamos comparando os valores corretos.  Para fazer isso, basta usar o mesmo conjunto de pixels para treinar e validar o modelo. </p><br><p>  Os resultados s√£o mostrados na tabela abaixo.  Voc√™ pode ver que, em geral, a influ√™ncia das nuvens no resultado do modelo √© bastante baixa!  Isso pode ser devido ao fato de o cart√£o de refer√™ncia ser de muito boa qualidade e o modelo poder ignorar a maioria das imagens.  De qualquer forma, esse comportamento n√£o pode ser garantido por nenhuma AOI, portanto, dedique um tempo para dar esse passo fora de seus modelos! </p><br><div class="scrollable-table"><table><thead><tr><th>  Modelo </th><th>  Precis√£o [%] </th><th>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">F_1</a> [%] </th></tr></thead><tbody><tr><td>  Sem filtros, sem m√°scara </td><td>  92,8 </td><td>  92,6 </td></tr><tr><td>  Sem filtros, com m√°scara </td><td>  94,2 </td><td>  93,9 </td></tr><tr><td>  Com filtro, sem m√°scara </td><td>  94,0 </td><td>  93,8 </td></tr><tr><td>  Com filtro, com m√°scara </td><td>  94,4 </td><td>  94,1 </td></tr></tbody></table></div><br><h3 id="vliyanie-raznyh-podhodov-k-resemplingu">  O impacto de diferentes abordagens de reamostragem </h3><br><p>  A escolha das op√ß√µes de reamostragem temporal n√£o √© √≥bvia.  Por um lado, precisamos de uma matriz detalhada de imagens que mostre bem os detalhes das imagens de origem - queremos incluir o n√∫mero mais pr√≥ximo poss√≠vel de imagens nos dados de origem.  Por outro lado, estamos limitados por recursos de computa√ß√£o.  A redu√ß√£o da etapa de reamostragem dobra o n√∫mero de quadros ap√≥s a interpola√ß√£o e, portanto, aumenta o n√∫mero de atributos usados ‚Äã‚Äãno treinamento.  Essa melhoria vale o custo dos recursos?  √â isso que temos que descobrir. </p><br><p>  Para este experimento, usaremos a varia√ß√£o 1 da etapa anterior.  Ap√≥s a interpola√ß√£o, fazemos uma nova amostra com as seguintes varia√ß√µes: </p><br><ol><li>  Reamostragem uniforme com um intervalo de 16 dias </li><li>  Reamostragem uniforme com um intervalo de 8 dias </li><li>  Na escolha das "melhores" datas, o n√∫mero coincide com o caso 2. </li></ol><br><p>  A amostra no caso 3 √© baseada no maior n√∫mero de datas comuns para todo o EOPatch na AOI selecionada <br><img src="https://habrastorage.org/webt/xg/qa/9w/xgqa9w17-oe4dbtxca22yejwhzo.png"><br>  <em>O gr√°fico mostra o n√∫mero de fragmentos do EOPatch que cont√™m dados para cada dia de 2017 (azul).</em>  <em>As linhas vermelhas mostram as datas ideais para a reamostragem, que s√£o baseadas nas datas das imagens do Sentinel-2 para a AOI de 2017.</em> </p><br><p>  Observando a tabela abaixo, voc√™ pode ver que os resultados n√£o s√£o muito impressionantes, como na experi√™ncia anterior.  Nos casos 2 e 3, a quantidade de tempo gasto dobra, mas a diferen√ßa com a abordagem inicial √© menor que 1%.  Essas melhorias s√£o muito discretas para uso pr√°tico, para que possamos considerar o intervalo de 16 dias adequado para a tarefa. </p><br><div class="scrollable-table"><table><thead><tr><th>  Modelo </th><th>  Precis√£o [%] </th><th>  F_1 [%] </th></tr></thead><tbody><tr><td>  Uniformemente a cada 16 dias </td><td>  94,4 </td><td>  94,1 </td></tr><tr><td>  Uniformemente a cada 8 dias </td><td>  94,5 </td><td>  94,3 </td></tr><tr><td>  Escolhendo as melhores datas </td><td>  94,6 </td><td>  94,4 </td></tr></tbody></table></div><br><p>  <em>Resultados de precis√£o geral e F1 ponderada para diferentes tubula√ß√µes com uma altera√ß√£o na abordagem da reamostragem.</em> </p><br><h2 id="glubokoe-obuchenie-ispolzuem-svyortochnuyu-neyronnuyu-set-cnn">  Aprendizagem Profunda: Usando a Rede Neural Convolucional (CNN) </h2><br><p>  O aprendizado profundo tornou-se a abordagem padr√£o para muitas tarefas, como vis√£o computacional, processamento de palavras em linguagem natural e processamento de sinais.  Isso se deve √† sua capacidade de extrair padr√µes de entradas multidimensionais complexas.  Abordagens cl√°ssicas de aprendizado de m√°quina (como √°rvores de decis√£o) t√™m sido usadas em muitas tarefas de dados geogr√°ficos temporais.  Redes convolucionais, por outro lado, foram usadas para analisar a correla√ß√£o espacial entre imagens adjacentes.  Basicamente, seu uso foi limitado ao trabalho com imagens √∫nicas. </p><br><p>  Quer√≠amos estudar a arquitetura de modelos de aprendizagem profunda e tentar escolher uma que seja capaz de analisar os aspectos espaciais e temporais dos dados de sat√©lite ao mesmo tempo. </p><br><p>  Para fazer isso, usamos o Netvork Totalmente Convolucional Temporal, TFCN, ou melhor, a extens√£o temporal para o U-Net, implementada no TensorFlow.  Mais especificamente, a arquitetura usa correla√ß√µes espa√ßo-temporais para melhorar o resultado.  Uma vantagem adicional √© que a estrutura da rede permite representar melhor as rela√ß√µes espaciais em diferentes escalas, gra√ßas ao processo de codifica√ß√£o / decodifica√ß√£o no U-net.  Como nos modelos cl√°ssicos, na sa√≠da obtemos uma matriz bidimensional de r√≥tulos, que compararemos com a verdade. </p><br><p><img src="https://habrastorage.org/webt/p0/jl/mg/p0jlmgxi9euwvodwonx4zrmezsw.png"></p><br><p>  Utilizamos o modelo treinado para prever as notas no conjunto de testes, e os valores obtidos foram verificados com a verdade.  No geral, a precis√£o foi de 84,4% e a F1 foi de 85,4%. </p><br><p><img src="https://habrastorage.org/webt/ol/z2/zj/olz2zjp3waghaak9hnirzcwa258.png"></p><br><p>  <em>Compara√ß√£o de diferentes previs√µes para nossa tarefa.</em>  <em>Imagem visual (canto superior esquerdo), mapa de refer√™ncia verdadeiro (canto superior direito), previs√£o do modelo LightGBM (canto inferior esquerdo) e previs√£o de rede U (canto inferior direito)</em> </p><br><p>  Esses resultados mostram apenas o trabalho inicial desse prot√≥tipo, que n√£o √© altamente otimizado para a tarefa atual.  Apesar disso, os resultados concordam com algumas estat√≠sticas obtidas na regi√£o.  Para liberar o potencial de uma rede neural, √© necess√°rio otimizar a arquitetura (conjunto de atributos, profundidade da rede, n√∫mero de circunvolu√ß√µes), al√©m de definir hiper par√¢metros (velocidade de aprendizado, n√∫mero de √©pocas, pondera√ß√£o de classe).  Esperamos nos aprofundar ainda mais neste t√≥pico (ha ha) e planejamos distribuir nosso c√≥digo quando ele estiver de uma forma aceit√°vel. </p><br><h3 id="drugie-eksperimenty">  Outras experi√™ncias </h3><br><p>  Voc√™ pode encontrar <em>v√°rias</em> maneiras de melhorar seus resultados atuais, mas n√£o podemos resolver nem experimentar todos eles.  √â nesse momento que voc√™ aparece em cena!  Mostre o que voc√™ pode fazer com esse conjunto de dados e ajude-nos a melhorar os resultados! </p><br><p>  Por exemplo, em um futuro pr√≥ximo, um de nossos colegas participar√° da classifica√ß√£o da capa com base na pilha temporal de imagens <em>individuais</em> usando redes de convolu√ß√£o.  A ideia √© que algumas superf√≠cies, por exemplo, artificiais, possam ser distinguidas sem caracter√≠sticas temporais - bastante espaciais.  Ficaremos felizes em escrever um artigo separado quando este trabalho levar a resultados! </p><br><h3 id="ot-perevodchika">  Do tradutor </h3><br><p>  Infelizmente, a pr√≥xima parte desta s√©rie de artigos n√£o foi publicada, o que significa que os autores n√£o mostraram exemplos de c√≥digo fonte na cria√ß√£o da U-Net.  Como alternativa, posso oferecer as seguintes fontes: </p><br><ol><li>  <em>U-Net: Redes Convolucionais para Segmenta√ß√£o de Imagem Biom√©dica - Olaf Ronneberger, Philipp Fischer, Thomas Brox</em> √© um dos artigos b√°sicos sobre arquitetura de U-Net que n√£o envolve dados temporais. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html</a> - A p√°gina de documenta√ß√£o do eo-learn, onde (possivelmente) est√° localizada uma vers√£o mais recente dos pipelines de 1,2 partes. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://github.com/divamgupta/image-segmentation-keras</a> - Um reposit√≥rio com v√°rias redes implementadas usando keras.  Eu tenho algumas perguntas sobre implementa√ß√µes (elas s√£o um pouco diferentes das descritas nos artigos originais), mas, em geral, as solu√ß√µes s√£o facilmente adaptadas para fins pessoais e est√£o funcionando. </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt453354/">https://habr.com/ru/post/pt453354/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt453342/index.html">Mitos sobre funcion√°rios remotos que nos destru√≠mos</a></li>
<li><a href="../pt453346/index.html">Tecnologias de armazenamento e prote√ß√£o de dados - o terceiro dia no VMware EMPOWER 2019</a></li>
<li><a href="../pt453348/index.html">O que h√° dentro de ass√≠ncio</a></li>
<li><a href="../pt453350/index.html">Transmiss√£o aberta do sal√£o principal do RIT ++ 2019</a></li>
<li><a href="../pt453352/index.html">Como os drones fornecem medicamentos vitais no Gana</a></li>
<li><a href="../pt453356/index.html">Tend√™ncias atuais e recomenda√ß√µes sobre aglomera√ß√£o de grandes institui√ß√µes financeiras</a></li>
<li><a href="../pt453360/index.html">Cidade sem engarrafamentos</a></li>
<li><a href="../pt453362/index.html">HabraConf # 1 - para tr√°s para back-end</a></li>
<li><a href="../pt453364/index.html">Uma hist√≥ria de lan√ßamento que afetou tudo</a></li>
<li><a href="../pt453366/index.html">Como usar v√≠rgulas em ingl√™s: 15 regras e exemplos de erros</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>