<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏼‍💼 🍿 🧖🏾 Como reconhecer imagens e textos no seu telefone usando o ML Kit 😃 🕑 👉🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Há dois anos, Sundar Pichai, diretor do Google, disse que a empresa do mobile-first se torna a IA e se concentra no aprendizado de máquina. Um ano dep...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Como reconhecer imagens e textos no seu telefone usando o ML Kit</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yamoney/blog/461867/"><p><img src="https://habrastorage.org/webt/6u/ek/co/6uekco-kgxxahafq0864aq6rmsw.png"></p><br><p>  Há dois anos, Sundar Pichai, diretor do Google, disse que a empresa do mobile-first se torna a IA e se concentra no aprendizado de máquina.  Um ano depois, o Machine Learning Kit foi lançado - um conjunto de ferramentas com as quais você pode usar efetivamente o ML no iOS e no Android. </p><br><p>  Fala-se muito sobre o ML Kit nos EUA, mas quase não há informações em russo.  E como o usamos para algumas tarefas no Yandex.Money, decidi compartilhar minha experiência e mostrar com exemplos como usá-lo para fazer coisas interessantes. </p><br><p>  Meu nome é Yura, e no ano passado trabalhei na equipe Yandex.Money em uma carteira móvel.  Falaremos sobre aprendizado de máquina no celular. </p><a name="habracut"></a><br><hr><br><p>  Nota  Editorial: Este post é uma recontagem do relatório de Yuri Chechetkin “Do celular primeiro à IA primeiro” do mitap Yanox.Money <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Android Paranoid</a> . </p><br><h2 id="chto-takoe-ml-kit">  O que é o kit ML? </h2><br><p>  Esse é o SDK para celular do Google que facilita o uso do aprendizado de máquina em dispositivos Android e iOS.  Não é necessário ser um especialista em ML ou em inteligência artificial, porque em algumas linhas de código você pode implementar coisas muito complexas.  Além disso, não é necessário saber como funcionam as redes neurais ou a otimização de modelos. </p><br><h2 id="chto-zhe-mozhet-ml-kit">  O que o kit ML pode fazer? </h2><br><p>  Os recursos básicos são bastante amplos.  Por exemplo, você pode reconhecer texto, faces, localizar e rastrear objetos, criar etiquetas para imagens e seus próprios modelos de classificação, digitalizar códigos de barras e tags QR. </p><br><p>  Já usamos o reconhecimento de código QR no aplicativo Yandex.Money. </p><br><p>  Há também um kit ML </p><br><ol><li>  Reconhecimento de referência; </li><li>  Definição do idioma em que o texto está escrito; </li><li>  Tradução de textos no dispositivo; </li><li>  Resposta rápida a uma carta ou mensagem. </li></ol><br><p>  Além de um grande número de métodos prontos para uso, há suporte para modelos personalizados, o que praticamente oferece inúmeras possibilidades - por exemplo, você pode colorir fotografias em preto e branco e fazê-las coloridas. </p><br><p>  É importante que você não precise usar nenhum serviço, API ou back-end para isso.  Tudo pode ser feito diretamente no dispositivo, para não carregar o tráfego do usuário, não recebermos muitos erros relacionados à rede, não precisamos processar vários casos, por exemplo, falta de Internet, perda de conexão e assim por diante.  Além disso, no dispositivo, ele funciona muito mais rápido do que através de uma rede. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/024/fb0/78d/024fb078d1cd1b8f8b81f8be44122aad.png" alt="1"></p><br><h2 id="raspoznavanie-teksta">  Reconhecimento de texto </h2><br><p>  <strong>Tarefa: dada uma fotografia, você precisa colocar o texto circulado em um retângulo.</strong> </p><br><p>  Começamos com a dependência em Gradle.  Basta conectar uma dependência e estamos prontos para trabalhar. </p><br><pre><code class="kotlin hljs">dependencies { <span class="hljs-comment"><span class="hljs-comment">// ... implementation'com.google.firebase:firebase-ml-vision:20.0.0' }</span></span></code> </pre> <br><p>  Vale a pena especificar os metadados que informam que o modelo será baixado no dispositivo durante o download do aplicativo no Play Market.  Se você não fizer isso e acessar a API sem um modelo, obteremos um erro e o modelo deverá ser baixado em segundo plano.  Se você precisar usar vários modelos, é aconselhável especificá-los separados por vírgulas.  Em nosso exemplo, usamos o modelo de OCR, e o nome do restante pode ser encontrado na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documentação</a> . </p><br><pre> <code class="kotlin hljs">&lt;application ...&gt; ... &lt;meta-<span class="hljs-keyword"><span class="hljs-keyword">data</span></span> android:name=<span class="hljs-string"><span class="hljs-string">"com.google.firebase.ml.vision.DEPENDENCIES"</span></span> android:value=<span class="hljs-string"><span class="hljs-string">"ocr"</span></span> /&gt; &lt;!-- To use multiple models: android:value=<span class="hljs-string"><span class="hljs-string">"ocr,model2,model3"</span></span> --&gt; &lt;/application&gt;</code> </pre> <br><p>  Após a configuração do projeto, você precisa definir os valores de entrada.  O ML Kit funciona com o tipo FirebaseVisionImage, temos cinco métodos, cuja assinatura eu escrevi abaixo.  Eles convertem os tipos usuais de Android e Java nos tipos de ML Kit, com os quais é conveniente trabalhar. </p><br><pre> <code class="kotlin hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromMediaImage</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(image: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Image</span></span></span></span><span class="hljs-function"><span class="hljs-params">, rotation: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Int</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromBitmap</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(bitmap: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Bitmap</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromFilePath</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(context: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Context</span></span></span></span><span class="hljs-function"><span class="hljs-params">, uri: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Uri</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromByteBuffer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( byteBuffer: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">ByteBuffer</span></span></span></span><span class="hljs-function"><span class="hljs-params">, metadata: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">FirebaseVisionImageMetadata</span></span></span></span><span class="hljs-function"><span class="hljs-params"> )</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromByteArray</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( bytes: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">ByteArray</span></span></span></span><span class="hljs-function"><span class="hljs-params">, metadata: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">FirebaseVisionImageMetadata</span></span></span></span><span class="hljs-function"><span class="hljs-params"> )</span></span></span></span>: FirebaseVisionImage</code> </pre> <br><p>  Preste atenção nos dois últimos - eles funcionam com uma matriz de bytes e com um buffer de bytes, e precisamos especificar metadados para que o ML Kit entenda como lidar com tudo isso.  Os metadados, de fato, descrevem o formato, neste caso, a largura e a altura, o formato padrão, IMAGE_FORMAT_NV21 e rotação. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> metadata = FirebaseVisionImageMetadata.Builder() .setWidth(<span class="hljs-number"><span class="hljs-number">480</span></span>) .setHeight(<span class="hljs-number"><span class="hljs-number">360</span></span>) .setFormat(FirebaseVisionImageMetadata.IMAGE_FORMAT_NV21) .setRotation(rotation) .build() <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> image = FirebaseVisionImage.fromByteBuffer(buffer, metadata)</code> </pre> <br><p>  Quando os dados de entrada são coletados, crie um detector que reconheça o texto. </p><br><p>  Existem dois tipos de detectores, no dispositivo e na nuvem, eles são criados literalmente em uma linha.  Vale ressaltar que o detector no dispositivo funciona apenas em inglês.  O detector de nuvem suporta mais de 20 idiomas; eles devem ser especificados no método especial setLanguageHints. </p><br><pre> <code class="kotlin hljs"><span class="hljs-comment"><span class="hljs-comment">//  onDevice val detector = FirebaseVision.getInstance().getOnDeviceTextRecognizer() // onCloud with options val options = FirebaseVisionCloudTextRecognizerOptions.Builder() .setLanguageHints(arrayOf("en", "ru")) .build() val detector = FirebaseVision.getInstance().getCloudTextRecognizer(options)</span></span></code> </pre> <br><p>  O número de idiomas suportados é superior a 20, todos estão no site oficial.  No nosso exemplo, apenas inglês e russo. </p><br><p>  Depois de inserir um detector, basta chamar o método processImage neste detector.  Obtemos o resultado na forma de uma tarefa, na qual penduramos dois retornos de chamada - para obter sucesso e para erro.  A exceção padrão ocorre com erro e o tipo FirebaseVisionText obtém êxito com o onSuccessListener. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> result: Task&lt;FirebaseVisionText&gt; = detector.processImage(image) .addOnSuccessListener { result: FirebaseVisionText -&gt; <span class="hljs-comment"><span class="hljs-comment">// Task completed successfully // ... } .addOnFailureListener { exception: Exception -&gt; // Task failed with an exception // ... }</span></span></code> </pre> <br><h2 id="kak-rabotat-s-tipom-firebasevisiontext">  Como trabalhar com o tipo FirebaseVisionText? </h2><br><p>  Consiste em blocos de texto (TextBlock), aqueles por sua vez consistem em linhas (Linha) e linhas de elementos (Elemento).  Eles estão aninhados um no outro. </p><br><p>  Além disso, cada uma dessas classes possui cinco métodos que retornam dados diferentes sobre o objeto.  Um retângulo é a área em que o texto está localizado, a confiança é a precisão do texto reconhecido, os pontos de canto são os pontos de canto no sentido horário, começando no canto superior esquerdo, nos idiomas reconhecidos e no próprio texto. </p><br><pre> <code class="kotlin hljs">FirebaseVisionText contains a list of FirebaseVisionText.TextBlock which contains a list of FirebaseVisionText.Line which <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> composed of a list of FirebaseVisionText.Element. <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getBoundingBox</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span>: Rect <span class="hljs-comment"><span class="hljs-comment">// axis-aligned bounding rectangle of the detected text fun getConfidence(): Float // confidence of the recognized text fun getCornerPoints(): Array&lt;Point&gt; // four corner points in clockwise direction fun getRecognizedLanguages(): List&lt;RecognizedLanguage&gt; // a list of recognized languages fun getText(): String //recognized text as a string</span></span></code> </pre> <br><h2 id="dlya-chego-eto-nuzhno">  Para que é isso? </h2><br><p>  Podemos reconhecer o texto inteiro na figura e seus parágrafos, partes, linhas ou apenas palavras.  E como exemplo, podemos iterar sobre, em cada estágio, pegar um texto, pegar as bordas desse texto e desenhar.  Muito confortável </p><br><p>  Planejamos usar essa ferramenta em nosso aplicativo para reconhecer cartões bancários, cujas etiquetas estão localizadas fora do padrão.  Nem todas as bibliotecas de reconhecimento de cartões funcionam bem e, para cartões personalizados, o ML Kit seria muito útil.  Como há pouco texto, é muito fácil processar dessa maneira. </p><br><h2 id="raspoznavanie-obektov-na-foto">  Reconhecimento de objetos na foto </h2><br><p><img src="https://habrastorage.org/getpro/habr/post_images/766/526/977/766526977ec9b1a83419e2aa6bdac37f.png" alt="2"></p><br><p>  Usando a ferramenta a seguir como exemplo, eu gostaria de mostrar que o princípio de operação é aproximadamente o mesmo.  Nesse caso, reconhecimento do que é representado no objeto.  Também criamos dois detectores, um no dispositivo e outro na nuvem. Podemos especificar a precisão mínima como parâmetros.  O padrão é 0,5, indicado 0,7 e pronto para uso.  Também obtemos o resultado na forma de FirebaseImageLabel, esta é uma lista de rótulos, cada um dos quais contém um ID, descrição e precisão. </p><br><pre> <code class="kotlin hljs"><span class="hljs-comment"><span class="hljs-comment">// onDevice val detector: FirebaseVisionImageLabeler = FirebaseVision .getInstance() .getOnDeviceImageLabeler() // onCloud with minimum confidence val options = FirebaseVisionCloudImageLabelerOptions.Builder() .setConfidenceThreshold(0.7f) .build() val detector: FirebaseVisionImageLabeler = FirebaseVision .getInstance() .getCloudImageLabeler(options)</span></span></code> </pre> <br><h2 id="garold-skryvayuschiy-schaste">  Harold escondendo a felicidade </h2><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ec4/223/a1c/ec4223a1c8dcab56e489c6b0a4a4ca5b.jpg" alt="3"></p><br><p>  Você pode tentar entender como Harold esconde a dor e se ele é feliz ao mesmo tempo.  Usamos uma ferramenta de reconhecimento de rosto, que, além de reconhecer as características faciais, pode dizer o quão feliz é uma pessoa.  Como se viu, Harold está 93% feliz.  Ou ele esconde a dor muito bem. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/237/352/dee/237352dee824e16c69ba131fe814fc8b.png" alt="4"></p><br><h2 id="ot-legkogo-k-legkomu-no-chut-bolee-slozhnomu-kastomnye-modeli">  De fácil a fácil, mas um pouco mais complicado.  Modelos personalizados. </h2><br><p>  <strong>Tarefa: classificação do que é retratado na foto.</strong> </p><br><p>  Tirei uma foto do laptop e reconheci o modem, o computador de mesa e o teclado.  Parece a verdade.  Existem mil classificadores, e ele leva três deles que melhor descrevem esta foto. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/5fb/0d1/cf1/5fb0d1cf1b1ae43bdb0bd2151937229a.png" alt="7"></p><br><p>  Ao trabalhar com modelos personalizados, também podemos trabalhar com eles no dispositivo e na nuvem. </p><br><p>  Se trabalharmos na nuvem, você precisará acessar o Firebase Console, a guia ML Kit e o toque personalizado, onde podemos fazer upload de nosso modelo para o TensorFlow Lite, porque o ML Kit funciona com modelos com esta resolução.  Se o usarmos em um dispositivo, podemos simplesmente colocar o modelo em qualquer parte do projeto como um ativo. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/cd6/70e/aae/cd670eaae79abc02c358dc6583c9061e.png" alt="6"></p><br><p>  Apontamos a dependência do intérprete, que pode trabalhar com modelos personalizados, e não esquecemos a permissão para trabalhar com a Internet. </p><br><pre> <code class="kotlin hljs">&lt;uses-permission android:name=<span class="hljs-string"><span class="hljs-string">"android.permission.INTERNET"</span></span> /&gt; dependencies { <span class="hljs-comment"><span class="hljs-comment">// ... implementation 'com.google.firebase:firebase-ml-model-interpreter:19.0.0' }</span></span></code> </pre> <br><p>  Para os modelos que estão no dispositivo, você deve indicar no Gradle que o modelo não deve ser compactado, pois pode estar distorcido. </p><br><pre> <code class="kotlin hljs">android { <span class="hljs-comment"><span class="hljs-comment">// ... aaptOptions { noCompress "tflite" // Your model's file extension: "tflite" } }</span></span></code> </pre> <br><p>  Quando configuramos tudo em nosso ambiente, precisamos definir condições especiais, que incluem, por exemplo, o uso de Wi-Fi, também exigem carregamento e exigem que o dispositivo ocioso esteja disponível no Android N - essas condições indicam que o telefone está carregando ou está no modo de espera. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">var</span></span> conditionsBuilder: FirebaseModelDownloadConditions.Builder = FirebaseModelDownloadConditions.Builder().requireWifi() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.N) { <span class="hljs-comment"><span class="hljs-comment">// Enable advanced conditions on Android Nougat and newer. conditionsBuilder = conditionsBuilder .requireCharging() .requireDeviceIdle() } val conditions: FirebaseModelDownloadConditions = conditionsBuilder.build()</span></span></code> </pre> <br><p>  Quando criamos um modelo remoto, definimos as condições de inicialização e atualização, além de sinalizar se nosso modelo deve ser atualizado.  O nome do modelo deve corresponder ao nome especificado no console do Firebase.  Quando criamos o modelo remoto, devemos registrá-lo no Firebase Model Manager. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> cloudSource: FirebaseRemoteModel = FirebaseRemoteModel.Builder(<span class="hljs-string"><span class="hljs-string">"my_cloud_model"</span></span>) .enableModelUpdates(<span class="hljs-literal"><span class="hljs-literal">true</span></span>) .setInitialDownloadConditions(conditions) .setUpdatesDownloadConditions(conditions) .build() FirebaseModelManager.getInstance().registerRemoteModel(cloudSource)</code> </pre> <br><p>  Realizamos as mesmas etapas para o modelo local, especificamos seu nome, o caminho para o modelo e o registramos no Firebase Model Manager. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> localSource: FirebaseLocalModel = FirebaseLocalModel.Builder(<span class="hljs-string"><span class="hljs-string">"my_local_model"</span></span>) .setAssetFilePath(<span class="hljs-string"><span class="hljs-string">"my_model.tflite"</span></span>) .build() FirebaseModelManager.getInstance().registerLocalModel(localSource)</code> </pre> <br><p>  Depois disso, você precisa criar essas opções onde especificamos os nomes de nossos modelos, instalamos o modelo remoto, instalamos o modelo local e criamos um intérprete com essas opções.  Podemos especificar um modelo remoto ou apenas local, e o intérprete entenderá com quem trabalhar. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> options: FirebaseModelOptions = FirebaseModelOptions.Builder() .setRemoteModelName(<span class="hljs-string"><span class="hljs-string">"my_cloud_model"</span></span>) .setLocalModelName(<span class="hljs-string"><span class="hljs-string">"my_local_model"</span></span>) .build() <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> interpreter = FirebaseModelInterpreter.getInstance(options)</code> </pre> <br><p>  O ML Kit não sabe nada sobre o formato dos dados de entrada e saída dos modelos personalizados, portanto, é necessário especificá-los. </p><br><p>  Os dados de entrada são uma matriz multidimensional, em que 1 é o número de imagens, 224x224 é a resolução e 3 é uma imagem RGB de três canais.  Bem, o tipo de dados é bytes. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> input = intArrayOf(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>) <span class="hljs-comment"><span class="hljs-comment">//one 224x224 three-channel (RGB) image val output = intArrayOf(1, 1000) val inputOutputOptions = FirebaseModelInputOutputOptions.Builder() .setInputFormat(0, FirebaseModelDataType.BYTE, input) .setOutputFormat(0, FirebaseModelDataType.BYTE, output) .build()</span></span></code> </pre> <br><p>  Os valores de saída são 1000 classificadores.  Definimos o formato dos valores de entrada e saída em bytes com as matrizes multidimensionais especificadas.  Além de bytes, float, long, int também estão disponíveis. </p><br><p>  Agora vamos definir os valores de entrada.  Pegamos o Bitmap, compactamos para 224 por 224, convertemos para ByteBuffer e criamos valores de entrada usando FirebaseModelInput usando um construtor especial. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> bitmap = Bitmap.createScaledBitmap(yourInputImage, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-literal"><span class="hljs-literal">true</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> imgData = convertBitmapToByteBuffer(bitmap) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> inputs: FirebaseModelInputs = FirebaseModelInputs.Builder() .add(imageData) .build()</code> </pre> <br><p>  E agora, quando há um intérprete, o formato dos valores de entrada e saída e os próprios valores de entrada, podemos executar a solicitação usando o método run.  Transferimos todos os itens acima como parâmetros e, como resultado, obtemos o FirebaseModelOutput, que contém um genérico do tipo especificado.  Nesse caso, era uma matriz de bytes, após a qual podemos iniciar o processamento.  São exatamente os milhares de classificadores que solicitamos e exibimos, por exemplo, os três primeiros mais adequados. </p><br><pre> <code class="kotlin hljs">interpreter.run(inputs, inputOutputOptions) .addOnSuccessListener { result: FirebaseModelOutputs -&gt; <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> labelProbArray = result.getOutput&lt;Array&lt;ByteArray&gt;&gt;(<span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-comment"><span class="hljs-comment">//handle labelProbArray } .addOnFailureListener( object : OnFailureListener { override fun onFailure(e: Exception) { // Task failed with an exception } })</span></span></code> </pre> <br><h2 id="realizaciya-za-odin-den">  Implementação de um dia </h2><br><p>  Tudo é muito fácil de implementar e o reconhecimento de objetos com ferramentas integradas pode ser realizado em apenas um dia.  A ferramenta está disponível no iOS e Android. Além disso, você pode usar o mesmo modelo TensorFlow para ambas as plataformas. </p><br><p>  Além disso, existem vários métodos disponíveis imediatamente, que podem cobrir muitos casos.  A maioria das APIs está disponível no dispositivo, ou seja, o reconhecimento funcionará mesmo sem a Internet. </p><br><p>  E o mais importante - suporte para modelos personalizados que podem ser usados ​​como você deseja em qualquer tarefa. </p><br><h2 id="poleznye-ssylki">  Links úteis </h2><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Documentação do kit ML</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Projeto de demonstração do kit Github ML</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Machine Learning para celular com Firebase (Google I / O'19)</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Machine Learning SDK para desenvolvedores de dispositivos móveis (Google I / O'18)</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Criando um scanner de cartão de crédito usando o Firebase ML Kit (Medium.com)</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt461867/">https://habr.com/ru/post/pt461867/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt461851/index.html">Blockchain e eletricidade</a></li>
<li><a href="../pt461855/index.html">Salários em TI no primeiro semestre de 2019: de acordo com a calculadora de salários My Circle</a></li>
<li><a href="../pt461859/index.html">Você não sabe nada sobre tecnologia alimentar</a></li>
<li><a href="../pt461861/index.html">Office 365 Cloud Security: teste de ponto de verificação CloudGuard SaaS</a></li>
<li><a href="../pt461865/index.html">Curso em vídeo “Introdução à reversão do zero usando o IDA PRO. Capítulo 1</a></li>
<li><a href="../pt461871/index.html">101 dicas para se tornar um bom programador (e humano)</a></li>
<li><a href="../pt461873/index.html">ViewPager 2 - nova funcionalidade no antigo invólucro</a></li>
<li><a href="../pt461875/index.html">5 nm vs 3 nm</a></li>
<li><a href="../pt461877/index.html">Java vs Kotlin para Android: opiniões dos desenvolvedores</a></li>
<li><a href="../pt461879/index.html">O livro "Linux em ação"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>