<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèº‚Äçüíº üçø üßñüèæ Como reconhecer imagens e textos no seu telefone usando o ML Kit üòÉ üïë üëâüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="H√° dois anos, Sundar Pichai, diretor do Google, disse que a empresa do mobile-first se torna a IA e se concentra no aprendizado de m√°quina. Um ano dep...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Como reconhecer imagens e textos no seu telefone usando o ML Kit</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yamoney/blog/461867/"><p><img src="https://habrastorage.org/webt/6u/ek/co/6uekco-kgxxahafq0864aq6rmsw.png"></p><br><p>  H√° dois anos, Sundar Pichai, diretor do Google, disse que a empresa do mobile-first se torna a IA e se concentra no aprendizado de m√°quina.  Um ano depois, o Machine Learning Kit foi lan√ßado - um conjunto de ferramentas com as quais voc√™ pode usar efetivamente o ML no iOS e no Android. </p><br><p>  Fala-se muito sobre o ML Kit nos EUA, mas quase n√£o h√° informa√ß√µes em russo.  E como o usamos para algumas tarefas no Yandex.Money, decidi compartilhar minha experi√™ncia e mostrar com exemplos como us√°-lo para fazer coisas interessantes. </p><br><p>  Meu nome √© Yura, e no ano passado trabalhei na equipe Yandex.Money em uma carteira m√≥vel.  Falaremos sobre aprendizado de m√°quina no celular. </p><a name="habracut"></a><br><hr><br><p>  Nota  Editorial: Este post √© uma recontagem do relat√≥rio de Yuri Chechetkin ‚ÄúDo celular primeiro √† IA primeiro‚Äù do mitap Yanox.Money <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Android Paranoid</a> . </p><br><h2 id="chto-takoe-ml-kit">  O que √© o kit ML? </h2><br><p>  Esse √© o SDK para celular do Google que facilita o uso do aprendizado de m√°quina em dispositivos Android e iOS.  N√£o √© necess√°rio ser um especialista em ML ou em intelig√™ncia artificial, porque em algumas linhas de c√≥digo voc√™ pode implementar coisas muito complexas.  Al√©m disso, n√£o √© necess√°rio saber como funcionam as redes neurais ou a otimiza√ß√£o de modelos. </p><br><h2 id="chto-zhe-mozhet-ml-kit">  O que o kit ML pode fazer? </h2><br><p>  Os recursos b√°sicos s√£o bastante amplos.  Por exemplo, voc√™ pode reconhecer texto, faces, localizar e rastrear objetos, criar etiquetas para imagens e seus pr√≥prios modelos de classifica√ß√£o, digitalizar c√≥digos de barras e tags QR. </p><br><p>  J√° usamos o reconhecimento de c√≥digo QR no aplicativo Yandex.Money. </p><br><p>  H√° tamb√©m um kit ML </p><br><ol><li>  Reconhecimento de refer√™ncia; </li><li>  Defini√ß√£o do idioma em que o texto est√° escrito; </li><li>  Tradu√ß√£o de textos no dispositivo; </li><li>  Resposta r√°pida a uma carta ou mensagem. </li></ol><br><p>  Al√©m de um grande n√∫mero de m√©todos prontos para uso, h√° suporte para modelos personalizados, o que praticamente oferece in√∫meras possibilidades - por exemplo, voc√™ pode colorir fotografias em preto e branco e faz√™-las coloridas. </p><br><p>  √â importante que voc√™ n√£o precise usar nenhum servi√ßo, API ou back-end para isso.  Tudo pode ser feito diretamente no dispositivo, para n√£o carregar o tr√°fego do usu√°rio, n√£o recebermos muitos erros relacionados √† rede, n√£o precisamos processar v√°rios casos, por exemplo, falta de Internet, perda de conex√£o e assim por diante.  Al√©m disso, no dispositivo, ele funciona muito mais r√°pido do que atrav√©s de uma rede. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/024/fb0/78d/024fb078d1cd1b8f8b81f8be44122aad.png" alt="1"></p><br><h2 id="raspoznavanie-teksta">  Reconhecimento de texto </h2><br><p>  <strong>Tarefa: dada uma fotografia, voc√™ precisa colocar o texto circulado em um ret√¢ngulo.</strong> </p><br><p>  Come√ßamos com a depend√™ncia em Gradle.  Basta conectar uma depend√™ncia e estamos prontos para trabalhar. </p><br><pre><code class="kotlin hljs">dependencies { <span class="hljs-comment"><span class="hljs-comment">// ... implementation'com.google.firebase:firebase-ml-vision:20.0.0' }</span></span></code> </pre> <br><p>  Vale a pena especificar os metadados que informam que o modelo ser√° baixado no dispositivo durante o download do aplicativo no Play Market.  Se voc√™ n√£o fizer isso e acessar a API sem um modelo, obteremos um erro e o modelo dever√° ser baixado em segundo plano.  Se voc√™ precisar usar v√°rios modelos, √© aconselh√°vel especific√°-los separados por v√≠rgulas.  Em nosso exemplo, usamos o modelo de OCR, e o nome do restante pode ser encontrado na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o</a> . </p><br><pre> <code class="kotlin hljs">&lt;application ...&gt; ... &lt;meta-<span class="hljs-keyword"><span class="hljs-keyword">data</span></span> android:name=<span class="hljs-string"><span class="hljs-string">"com.google.firebase.ml.vision.DEPENDENCIES"</span></span> android:value=<span class="hljs-string"><span class="hljs-string">"ocr"</span></span> /&gt; &lt;!-- To use multiple models: android:value=<span class="hljs-string"><span class="hljs-string">"ocr,model2,model3"</span></span> --&gt; &lt;/application&gt;</code> </pre> <br><p>  Ap√≥s a configura√ß√£o do projeto, voc√™ precisa definir os valores de entrada.  O ML Kit funciona com o tipo FirebaseVisionImage, temos cinco m√©todos, cuja assinatura eu escrevi abaixo.  Eles convertem os tipos usuais de Android e Java nos tipos de ML Kit, com os quais √© conveniente trabalhar. </p><br><pre> <code class="kotlin hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromMediaImage</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(image: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Image</span></span></span></span><span class="hljs-function"><span class="hljs-params">, rotation: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Int</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromBitmap</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(bitmap: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Bitmap</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromFilePath</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(context: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Context</span></span></span></span><span class="hljs-function"><span class="hljs-params">, uri: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Uri</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromByteBuffer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( byteBuffer: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">ByteBuffer</span></span></span></span><span class="hljs-function"><span class="hljs-params">, metadata: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">FirebaseVisionImageMetadata</span></span></span></span><span class="hljs-function"><span class="hljs-params"> )</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromByteArray</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( bytes: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">ByteArray</span></span></span></span><span class="hljs-function"><span class="hljs-params">, metadata: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">FirebaseVisionImageMetadata</span></span></span></span><span class="hljs-function"><span class="hljs-params"> )</span></span></span></span>: FirebaseVisionImage</code> </pre> <br><p>  Preste aten√ß√£o nos dois √∫ltimos - eles funcionam com uma matriz de bytes e com um buffer de bytes, e precisamos especificar metadados para que o ML Kit entenda como lidar com tudo isso.  Os metadados, de fato, descrevem o formato, neste caso, a largura e a altura, o formato padr√£o, IMAGE_FORMAT_NV21 e rota√ß√£o. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> metadata = FirebaseVisionImageMetadata.Builder() .setWidth(<span class="hljs-number"><span class="hljs-number">480</span></span>) .setHeight(<span class="hljs-number"><span class="hljs-number">360</span></span>) .setFormat(FirebaseVisionImageMetadata.IMAGE_FORMAT_NV21) .setRotation(rotation) .build() <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> image = FirebaseVisionImage.fromByteBuffer(buffer, metadata)</code> </pre> <br><p>  Quando os dados de entrada s√£o coletados, crie um detector que reconhe√ßa o texto. </p><br><p>  Existem dois tipos de detectores, no dispositivo e na nuvem, eles s√£o criados literalmente em uma linha.  Vale ressaltar que o detector no dispositivo funciona apenas em ingl√™s.  O detector de nuvem suporta mais de 20 idiomas; eles devem ser especificados no m√©todo especial setLanguageHints. </p><br><pre> <code class="kotlin hljs"><span class="hljs-comment"><span class="hljs-comment">//  onDevice val detector = FirebaseVision.getInstance().getOnDeviceTextRecognizer() // onCloud with options val options = FirebaseVisionCloudTextRecognizerOptions.Builder() .setLanguageHints(arrayOf("en", "ru")) .build() val detector = FirebaseVision.getInstance().getCloudTextRecognizer(options)</span></span></code> </pre> <br><p>  O n√∫mero de idiomas suportados √© superior a 20, todos est√£o no site oficial.  No nosso exemplo, apenas ingl√™s e russo. </p><br><p>  Depois de inserir um detector, basta chamar o m√©todo processImage neste detector.  Obtemos o resultado na forma de uma tarefa, na qual penduramos dois retornos de chamada - para obter sucesso e para erro.  A exce√ß√£o padr√£o ocorre com erro e o tipo FirebaseVisionText obt√©m √™xito com o onSuccessListener. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> result: Task&lt;FirebaseVisionText&gt; = detector.processImage(image) .addOnSuccessListener { result: FirebaseVisionText -&gt; <span class="hljs-comment"><span class="hljs-comment">// Task completed successfully // ... } .addOnFailureListener { exception: Exception -&gt; // Task failed with an exception // ... }</span></span></code> </pre> <br><h2 id="kak-rabotat-s-tipom-firebasevisiontext">  Como trabalhar com o tipo FirebaseVisionText? </h2><br><p>  Consiste em blocos de texto (TextBlock), aqueles por sua vez consistem em linhas (Linha) e linhas de elementos (Elemento).  Eles est√£o aninhados um no outro. </p><br><p>  Al√©m disso, cada uma dessas classes possui cinco m√©todos que retornam dados diferentes sobre o objeto.  Um ret√¢ngulo √© a √°rea em que o texto est√° localizado, a confian√ßa √© a precis√£o do texto reconhecido, os pontos de canto s√£o os pontos de canto no sentido hor√°rio, come√ßando no canto superior esquerdo, nos idiomas reconhecidos e no pr√≥prio texto. </p><br><pre> <code class="kotlin hljs">FirebaseVisionText contains a list of FirebaseVisionText.TextBlock which contains a list of FirebaseVisionText.Line which <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> composed of a list of FirebaseVisionText.Element. <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getBoundingBox</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span>: Rect <span class="hljs-comment"><span class="hljs-comment">// axis-aligned bounding rectangle of the detected text fun getConfidence(): Float // confidence of the recognized text fun getCornerPoints(): Array&lt;Point&gt; // four corner points in clockwise direction fun getRecognizedLanguages(): List&lt;RecognizedLanguage&gt; // a list of recognized languages fun getText(): String //recognized text as a string</span></span></code> </pre> <br><h2 id="dlya-chego-eto-nuzhno">  Para que √© isso? </h2><br><p>  Podemos reconhecer o texto inteiro na figura e seus par√°grafos, partes, linhas ou apenas palavras.  E como exemplo, podemos iterar sobre, em cada est√°gio, pegar um texto, pegar as bordas desse texto e desenhar.  Muito confort√°vel </p><br><p>  Planejamos usar essa ferramenta em nosso aplicativo para reconhecer cart√µes banc√°rios, cujas etiquetas est√£o localizadas fora do padr√£o.  Nem todas as bibliotecas de reconhecimento de cart√µes funcionam bem e, para cart√µes personalizados, o ML Kit seria muito √∫til.  Como h√° pouco texto, √© muito f√°cil processar dessa maneira. </p><br><h2 id="raspoznavanie-obektov-na-foto">  Reconhecimento de objetos na foto </h2><br><p><img src="https://habrastorage.org/getpro/habr/post_images/766/526/977/766526977ec9b1a83419e2aa6bdac37f.png" alt="2"></p><br><p>  Usando a ferramenta a seguir como exemplo, eu gostaria de mostrar que o princ√≠pio de opera√ß√£o √© aproximadamente o mesmo.  Nesse caso, reconhecimento do que √© representado no objeto.  Tamb√©m criamos dois detectores, um no dispositivo e outro na nuvem. Podemos especificar a precis√£o m√≠nima como par√¢metros.  O padr√£o √© 0,5, indicado 0,7 e pronto para uso.  Tamb√©m obtemos o resultado na forma de FirebaseImageLabel, esta √© uma lista de r√≥tulos, cada um dos quais cont√©m um ID, descri√ß√£o e precis√£o. </p><br><pre> <code class="kotlin hljs"><span class="hljs-comment"><span class="hljs-comment">// onDevice val detector: FirebaseVisionImageLabeler = FirebaseVision .getInstance() .getOnDeviceImageLabeler() // onCloud with minimum confidence val options = FirebaseVisionCloudImageLabelerOptions.Builder() .setConfidenceThreshold(0.7f) .build() val detector: FirebaseVisionImageLabeler = FirebaseVision .getInstance() .getCloudImageLabeler(options)</span></span></code> </pre> <br><h2 id="garold-skryvayuschiy-schaste">  Harold escondendo a felicidade </h2><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ec4/223/a1c/ec4223a1c8dcab56e489c6b0a4a4ca5b.jpg" alt="3"></p><br><p>  Voc√™ pode tentar entender como Harold esconde a dor e se ele √© feliz ao mesmo tempo.  Usamos uma ferramenta de reconhecimento de rosto, que, al√©m de reconhecer as caracter√≠sticas faciais, pode dizer o qu√£o feliz √© uma pessoa.  Como se viu, Harold est√° 93% feliz.  Ou ele esconde a dor muito bem. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/237/352/dee/237352dee824e16c69ba131fe814fc8b.png" alt="4"></p><br><h2 id="ot-legkogo-k-legkomu-no-chut-bolee-slozhnomu-kastomnye-modeli">  De f√°cil a f√°cil, mas um pouco mais complicado.  Modelos personalizados. </h2><br><p>  <strong>Tarefa: classifica√ß√£o do que √© retratado na foto.</strong> </p><br><p>  Tirei uma foto do laptop e reconheci o modem, o computador de mesa e o teclado.  Parece a verdade.  Existem mil classificadores, e ele leva tr√™s deles que melhor descrevem esta foto. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/5fb/0d1/cf1/5fb0d1cf1b1ae43bdb0bd2151937229a.png" alt="7"></p><br><p>  Ao trabalhar com modelos personalizados, tamb√©m podemos trabalhar com eles no dispositivo e na nuvem. </p><br><p>  Se trabalharmos na nuvem, voc√™ precisar√° acessar o Firebase Console, a guia ML Kit e o toque personalizado, onde podemos fazer upload de nosso modelo para o TensorFlow Lite, porque o ML Kit funciona com modelos com esta resolu√ß√£o.  Se o usarmos em um dispositivo, podemos simplesmente colocar o modelo em qualquer parte do projeto como um ativo. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/cd6/70e/aae/cd670eaae79abc02c358dc6583c9061e.png" alt="6"></p><br><p>  Apontamos a depend√™ncia do int√©rprete, que pode trabalhar com modelos personalizados, e n√£o esquecemos a permiss√£o para trabalhar com a Internet. </p><br><pre> <code class="kotlin hljs">&lt;uses-permission android:name=<span class="hljs-string"><span class="hljs-string">"android.permission.INTERNET"</span></span> /&gt; dependencies { <span class="hljs-comment"><span class="hljs-comment">// ... implementation 'com.google.firebase:firebase-ml-model-interpreter:19.0.0' }</span></span></code> </pre> <br><p>  Para os modelos que est√£o no dispositivo, voc√™ deve indicar no Gradle que o modelo n√£o deve ser compactado, pois pode estar distorcido. </p><br><pre> <code class="kotlin hljs">android { <span class="hljs-comment"><span class="hljs-comment">// ... aaptOptions { noCompress "tflite" // Your model's file extension: "tflite" } }</span></span></code> </pre> <br><p>  Quando configuramos tudo em nosso ambiente, precisamos definir condi√ß√µes especiais, que incluem, por exemplo, o uso de Wi-Fi, tamb√©m exigem carregamento e exigem que o dispositivo ocioso esteja dispon√≠vel no Android N - essas condi√ß√µes indicam que o telefone est√° carregando ou est√° no modo de espera. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">var</span></span> conditionsBuilder: FirebaseModelDownloadConditions.Builder = FirebaseModelDownloadConditions.Builder().requireWifi() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.N) { <span class="hljs-comment"><span class="hljs-comment">// Enable advanced conditions on Android Nougat and newer. conditionsBuilder = conditionsBuilder .requireCharging() .requireDeviceIdle() } val conditions: FirebaseModelDownloadConditions = conditionsBuilder.build()</span></span></code> </pre> <br><p>  Quando criamos um modelo remoto, definimos as condi√ß√µes de inicializa√ß√£o e atualiza√ß√£o, al√©m de sinalizar se nosso modelo deve ser atualizado.  O nome do modelo deve corresponder ao nome especificado no console do Firebase.  Quando criamos o modelo remoto, devemos registr√°-lo no Firebase Model Manager. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> cloudSource: FirebaseRemoteModel = FirebaseRemoteModel.Builder(<span class="hljs-string"><span class="hljs-string">"my_cloud_model"</span></span>) .enableModelUpdates(<span class="hljs-literal"><span class="hljs-literal">true</span></span>) .setInitialDownloadConditions(conditions) .setUpdatesDownloadConditions(conditions) .build() FirebaseModelManager.getInstance().registerRemoteModel(cloudSource)</code> </pre> <br><p>  Realizamos as mesmas etapas para o modelo local, especificamos seu nome, o caminho para o modelo e o registramos no Firebase Model Manager. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> localSource: FirebaseLocalModel = FirebaseLocalModel.Builder(<span class="hljs-string"><span class="hljs-string">"my_local_model"</span></span>) .setAssetFilePath(<span class="hljs-string"><span class="hljs-string">"my_model.tflite"</span></span>) .build() FirebaseModelManager.getInstance().registerLocalModel(localSource)</code> </pre> <br><p>  Depois disso, voc√™ precisa criar essas op√ß√µes onde especificamos os nomes de nossos modelos, instalamos o modelo remoto, instalamos o modelo local e criamos um int√©rprete com essas op√ß√µes.  Podemos especificar um modelo remoto ou apenas local, e o int√©rprete entender√° com quem trabalhar. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> options: FirebaseModelOptions = FirebaseModelOptions.Builder() .setRemoteModelName(<span class="hljs-string"><span class="hljs-string">"my_cloud_model"</span></span>) .setLocalModelName(<span class="hljs-string"><span class="hljs-string">"my_local_model"</span></span>) .build() <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> interpreter = FirebaseModelInterpreter.getInstance(options)</code> </pre> <br><p>  O ML Kit n√£o sabe nada sobre o formato dos dados de entrada e sa√≠da dos modelos personalizados, portanto, √© necess√°rio especific√°-los. </p><br><p>  Os dados de entrada s√£o uma matriz multidimensional, em que 1 √© o n√∫mero de imagens, 224x224 √© a resolu√ß√£o e 3 √© uma imagem RGB de tr√™s canais.  Bem, o tipo de dados √© bytes. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> input = intArrayOf(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>) <span class="hljs-comment"><span class="hljs-comment">//one 224x224 three-channel (RGB) image val output = intArrayOf(1, 1000) val inputOutputOptions = FirebaseModelInputOutputOptions.Builder() .setInputFormat(0, FirebaseModelDataType.BYTE, input) .setOutputFormat(0, FirebaseModelDataType.BYTE, output) .build()</span></span></code> </pre> <br><p>  Os valores de sa√≠da s√£o 1000 classificadores.  Definimos o formato dos valores de entrada e sa√≠da em bytes com as matrizes multidimensionais especificadas.  Al√©m de bytes, float, long, int tamb√©m est√£o dispon√≠veis. </p><br><p>  Agora vamos definir os valores de entrada.  Pegamos o Bitmap, compactamos para 224 por 224, convertemos para ByteBuffer e criamos valores de entrada usando FirebaseModelInput usando um construtor especial. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> bitmap = Bitmap.createScaledBitmap(yourInputImage, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-literal"><span class="hljs-literal">true</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> imgData = convertBitmapToByteBuffer(bitmap) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> inputs: FirebaseModelInputs = FirebaseModelInputs.Builder() .add(imageData) .build()</code> </pre> <br><p>  E agora, quando h√° um int√©rprete, o formato dos valores de entrada e sa√≠da e os pr√≥prios valores de entrada, podemos executar a solicita√ß√£o usando o m√©todo run.  Transferimos todos os itens acima como par√¢metros e, como resultado, obtemos o FirebaseModelOutput, que cont√©m um gen√©rico do tipo especificado.  Nesse caso, era uma matriz de bytes, ap√≥s a qual podemos iniciar o processamento.  S√£o exatamente os milhares de classificadores que solicitamos e exibimos, por exemplo, os tr√™s primeiros mais adequados. </p><br><pre> <code class="kotlin hljs">interpreter.run(inputs, inputOutputOptions) .addOnSuccessListener { result: FirebaseModelOutputs -&gt; <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> labelProbArray = result.getOutput&lt;Array&lt;ByteArray&gt;&gt;(<span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-comment"><span class="hljs-comment">//handle labelProbArray } .addOnFailureListener( object : OnFailureListener { override fun onFailure(e: Exception) { // Task failed with an exception } })</span></span></code> </pre> <br><h2 id="realizaciya-za-odin-den">  Implementa√ß√£o de um dia </h2><br><p>  Tudo √© muito f√°cil de implementar e o reconhecimento de objetos com ferramentas integradas pode ser realizado em apenas um dia.  A ferramenta est√° dispon√≠vel no iOS e Android. Al√©m disso, voc√™ pode usar o mesmo modelo TensorFlow para ambas as plataformas. </p><br><p>  Al√©m disso, existem v√°rios m√©todos dispon√≠veis imediatamente, que podem cobrir muitos casos.  A maioria das APIs est√° dispon√≠vel no dispositivo, ou seja, o reconhecimento funcionar√° mesmo sem a Internet. </p><br><p>  E o mais importante - suporte para modelos personalizados que podem ser usados ‚Äã‚Äãcomo voc√™ deseja em qualquer tarefa. </p><br><h2 id="poleznye-ssylki">  Links √∫teis </h2><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Documenta√ß√£o do kit ML</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Projeto de demonstra√ß√£o do kit Github ML</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Machine Learning para celular com Firebase (Google I / O'19)</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Machine Learning SDK para desenvolvedores de dispositivos m√≥veis (Google I / O'18)</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Criando um scanner de cart√£o de cr√©dito usando o Firebase ML Kit (Medium.com)</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt461867/">https://habr.com/ru/post/pt461867/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt461851/index.html">Blockchain e eletricidade</a></li>
<li><a href="../pt461855/index.html">Sal√°rios em TI no primeiro semestre de 2019: de acordo com a calculadora de sal√°rios My Circle</a></li>
<li><a href="../pt461859/index.html">Voc√™ n√£o sabe nada sobre tecnologia alimentar</a></li>
<li><a href="../pt461861/index.html">Office 365 Cloud Security: teste de ponto de verifica√ß√£o CloudGuard SaaS</a></li>
<li><a href="../pt461865/index.html">Curso em v√≠deo ‚ÄúIntrodu√ß√£o √† revers√£o do zero usando o IDA PRO. Cap√≠tulo 1</a></li>
<li><a href="../pt461871/index.html">101 dicas para se tornar um bom programador (e humano)</a></li>
<li><a href="../pt461873/index.html">ViewPager 2 - nova funcionalidade no antigo inv√≥lucro</a></li>
<li><a href="../pt461875/index.html">5 nm vs 3 nm</a></li>
<li><a href="../pt461877/index.html">Java vs Kotlin para Android: opini√µes dos desenvolvedores</a></li>
<li><a href="../pt461879/index.html">O livro "Linux em a√ß√£o"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>