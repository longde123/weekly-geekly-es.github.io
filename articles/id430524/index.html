<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>™️ 👨🏻‍🏭 🍀 Arsitektur Jaringan Saraf Tiruan 🧑🏿‍🤝‍🧑🏿 👆🏼 🍀</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Terjemahan Arsitektur Jaringan Saraf Tiruan 

 Algoritma jaringan saraf yang dalam telah mendapatkan popularitas besar hari ini, yang sebagian besar d...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Arsitektur Jaringan Saraf Tiruan</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/nix/blog/430524/">  <i>Terjemahan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Arsitektur Jaringan Saraf Tiruan</a></i> <br><br>  Algoritma jaringan saraf yang dalam telah mendapatkan popularitas besar hari ini, yang sebagian besar dipastikan oleh arsitektur yang dipikirkan dengan matang.  Mari kita lihat sejarah perkembangan mereka selama beberapa tahun terakhir.  Jika Anda tertarik pada analisis yang lebih dalam, lihat <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">karya ini</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/29b/f51/960/29bf5196085373528be31e27f2489bdd.jpg"><br>  <i>Perbandingan arsitektur populer untuk akurasi satu-potong Top-1 dan jumlah operasi yang diperlukan untuk satu lintasan langsung.</i>  <i>Lebih detail di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> .</i> <br><a name="habracut"></a><br><h3>  Lenet5 </h3><br>  Pada tahun 1994, salah satu jaringan saraf convolutional pertama dikembangkan, yang meletakkan dasar untuk pembelajaran yang mendalam.  Karya perintis oleh Yann LeCun ini, setelah banyak iterasi yang sukses sejak 1988, disebut <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">LeNet5</a> ! <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b19/9c3/8f2/b199c38f21a72c44d7cd3afbca1c94eb.jpg"><br><br>  Arsitektur LeNet5 telah menjadi dasar bagi pembelajaran mendalam, terutama dalam hal distribusi properti gambar di seluruh gambar.  Konvolusi dengan parameter pembelajaran diizinkan menggunakan beberapa parameter untuk secara efisien mengekstraksi properti yang sama dari tempat yang berbeda.  Pada tahun-tahun itu, tidak ada kartu video yang dapat mempercepat proses pembelajaran, dan bahkan prosesor sentral pun lambat.  Oleh karena itu, keuntungan utama dari arsitektur adalah kemampuan untuk menyimpan parameter dan hasil perhitungan, berbeda dengan menggunakan setiap piksel sebagai data input terpisah untuk jaringan saraf multilayer besar.  Di LeNet5, piksel tidak digunakan di lapisan pertama, karena gambar sangat berkorelasi spasial, jadi menggunakan piksel individual sebagai properti input tidak akan memungkinkan Anda memanfaatkan korelasi ini. <br><br>  Fitur LeNet5: <br><br><ul><li>  Jaringan saraf convolutional yang menggunakan urutan tiga lapisan: lapisan konvolusi, lapisan penyatuan, dan lapisan non-linearitas -&gt; sejak publikasi karya Lekun, ini mungkin salah satu fitur utama pembelajaran mendalam dalam kaitannya dengan gambar. </li><li>  Menggunakan konvolusi untuk mengambil properti spasial. </li><li>  Subsampling menggunakan rata-rata peta spasial. </li><li>  Nonlinier dalam bentuk tangen hiperbolik atau sigmoid. </li><li>  Pengklasifikasi akhir dalam bentuk jaringan saraf multilayer (MLP). </li><li>  Matriks yang jarang dari konektivitas antara lapisan mengurangi jumlah perhitungan. </li></ul><br>  Jaringan saraf ini membentuk dasar dari banyak arsitektur berikutnya dan menginspirasi banyak peneliti. <br><br><h3>  Pengembangan </h3><br>  Dari tahun 1998 hingga 2010, jaringan saraf berada dalam kondisi inkubasi.  Kebanyakan orang tidak memperhatikan kemampuan mereka yang terus meningkat, meskipun banyak pengembang secara bertahap mengasah algoritme mereka.  Berkat masa kejayaan kamera ponsel dan semakin murahnya kamera digital, semakin banyak data pelatihan yang tersedia bagi kami.  Pada saat yang sama, kemampuan komputasi tumbuh, prosesor menjadi lebih kuat, dan kartu video berubah menjadi alat komputasi utama.  Semua proses ini memungkinkan pengembangan jaringan saraf, meskipun agak lambat.  Ketertarikan pada tugas yang bisa diselesaikan dengan bantuan jaringan saraf tumbuh, dan akhirnya situasinya menjadi jelas ... <br><br><h3>  Dan ciresan net </h3><br>  Pada tahun 2010, Dan Claudiu Ciresan dan Jurgen Schmidhuber menerbitkan salah satu deskripsi pertama dari implementasi <a href="">jaringan saraf GPU</a> .  Pekerjaan mereka berisi implementasi langsung dan mundur dari jaringan saraf 9-layer pada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">NVIDIA GTX 280</a> . <br><br><h3>  Alexnet </h3><br>  Pada 2012, Alexei Krizhevsky menerbitkan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">AlexNet</a> , versi LeNet yang mendalam dan diperpanjang, yang dimenangkan dengan margin lebar dalam kompetisi ImageNet. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/aad/4ad/3ca/aad4ad3ca7345f8d7e198c2b131298d1.png"><br><br>  Di AlexNet, hasil perhitungan LeNet diskalakan menjadi jaringan saraf yang jauh lebih besar, yang mampu mempelajari objek yang jauh lebih kompleks dan hierarki mereka.  Fitur dari solusi ini: <br><br><ul><li>  Penggunaan linear rectification units (ReLU) sebagai non-linearitas. </li><li>  Penggunaan teknik membuang untuk mengabaikan selektif neuron individu selama pelatihan, yang menghindari pelatihan model yang berlebihan. </li><li>  Overlap max pooling, yang menghindari efek rata-rata pooling rata-rata. </li><li>  Menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">NVIDIA GTX 580</a> untuk mempercepat pembelajaran. </li></ul><br>  Pada saat itu, jumlah core dalam kartu video telah tumbuh secara signifikan, yang memungkinkan mereka untuk mengurangi waktu pelatihan sekitar 10 kali, dan sebagai hasilnya menjadi mungkin untuk menggunakan dataset dan gambar yang jauh lebih besar. <br><br>  Keberhasilan AlexNet meluncurkan revolusi kecil, jaringan saraf convolutional telah menjadi pekerja keras pembelajaran mendalam - istilah ini sekarang berarti "jaringan saraf besar yang dapat menyelesaikan tugas-tugas yang bermanfaat." <br><br><h3>  Makan berlebihan </h3><br>  Pada Desember 2013, laboratorium NYU Jan Lekun menerbitkan deskripsi <a href="">Overfeat</a> , varian dari AlexNet.  Juga, artikel tersebut menggambarkan kotak pembatas yang terlatih, dan kemudian banyak karya lain tentang topik ini diterbitkan.  Kami percaya bahwa lebih baik mempelajari cara membagi objek, daripada menggunakan kotak pembatas buatan. <br><br><h3>  Vgg </h3><br>  Jaringan <a href="">VGG</a> dikembangkan di Oxford di setiap lapisan konvolusional yang digunakan untuk filter 3x3 pertama kali, dan bahkan menggabungkan lapisan ini dalam urutan konvolusi. <br><br>  Ini bertentangan dengan prinsip-prinsip yang ditetapkan dalam LeNet, yang menurutnya konvolusi besar digunakan untuk mengekstraksi properti gambar yang sama.  Alih-alih filter 9x9 dan 11x11 yang digunakan di AlexNet, mereka mulai menggunakan filter yang jauh lebih kecil yang hampir mendekati konvolusi 1x1, yang coba dihindari penulis LeNet, setidaknya di lapisan pertama jaringan.  Tetapi keuntungan besar dari VGG adalah menemukan bahwa beberapa konvolusi 3x3 yang digabungkan dalam suatu urutan dapat meniru bidang reseptif yang lebih besar, misalnya, 5x5 atau 7x7.  Ide-ide ini nantinya akan digunakan dalam arsitektur Inception dan ResNet. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/dec/e8b/308/dece8b308f74450222deece6fcf9d357.jpg"><br><br>  Jaringan VGG menggunakan beberapa lapisan konvolusional 3x3 untuk merepresentasikan properti kompleks.  Perhatikan blok 3, 4 dan 5 di VGG-E: untuk mengekstraksi properti yang lebih kompleks dan menggabungkannya, digunakan sekuens filter 256 × 256 dan 512 × 512 3 × 3.  Ini setara dengan classifier konvolusional besar 512x512 dengan tiga lapisan!  Ini memberi kita sejumlah besar parameter dan kemampuan belajar yang sangat baik.  Tetapi sulit untuk mempelajari jaringan seperti itu, saya harus memecahnya menjadi yang lebih kecil, menambahkan lapisan satu per satu.  Alasannya adalah kurangnya cara yang efektif untuk mengatur model atau beberapa metode membatasi ruang pencarian yang besar, yang dipromosikan oleh banyak parameter. <br><br>  VGG di banyak lapisan menggunakan sejumlah besar properti, sehingga pelatihan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">mahal secara komputasi</a> .  Beban dapat dikurangi dengan mengurangi jumlah properti, seperti yang dilakukan pada lapisan bottleneck dari arsitektur Inception. <br><br><h3>  Jaringan-dalam-jaringan </h3><br>  Arsitektur <a href="">Network-in-network</a> (NiN) didasarkan pada ide sederhana: menggunakan konvolusi 1x1 untuk meningkatkan kombinatorialitas properti di lapisan convolutional. <br><br>  Di NiN, setelah setiap konvolusi, lapisan MLP spasial digunakan untuk menggabungkan properti lebih baik sebelum diumpankan ke lapisan berikutnya.  Tampaknya penggunaan konvolusi 1x1 bertentangan dengan prinsip-prinsip LeNet asli, tetapi dalam kenyataannya memungkinkan menggabungkan properti lebih baik daripada hanya menjejalkan lebih banyak lapisan convolutional.  Pendekatan ini berbeda dari menggunakan piksel kosong sebagai input untuk lapisan berikutnya.  Dalam hal ini, konvolusi 1x1 digunakan untuk kombinasi spasial properti setelah konvolusi dalam kerangka peta properti, sehingga Anda dapat menggunakan parameter yang jauh lebih sedikit yang umum untuk semua piksel properti ini! <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d9a/d08/e5c/d9ad08e5c699a2a9cf320c4b8b622ba3.jpg"><br><br>  MLP dapat sangat meningkatkan efektivitas lapisan konvolusional individu dengan menggabungkan mereka ke dalam kelompok yang lebih kompleks.  Gagasan ini kemudian digunakan dalam arsitektur lain, seperti ResNet, Inception, dan variannya. <br><br><h3>  GoogLeNet dan Inception </h3><br>  Google Christian Szegedy khawatir tentang menurunkan perhitungan dalam jaringan saraf yang dalam, dan sebagai hasilnya menciptakan <a href="">GoogLeNet, arsitektur Inception pertama</a> . <br><br>  Pada musim gugur 2014, model pembelajaran dalam menjadi sangat berguna dalam mengkategorikan konten gambar dan bingkai dari video.  Banyak skeptis telah mengakui manfaat dari pembelajaran yang dalam dan jaringan saraf, dan raksasa internet, termasuk Google, telah menjadi sangat tertarik dalam menyebarkan jaringan yang efisien dan besar pada kapasitas server mereka. <br><br>  Christian sedang mencari cara untuk mengurangi beban komputasi dalam jaringan saraf, mencapai kinerja tertinggi (misalnya, di ImageNet).  Atau mempertahankan jumlah perhitungan, tetapi tetap meningkatkan produktivitas. <br><br>  Akibatnya, perintah itu membuat modul Inception: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/abf/d01/a92/abfd01a92262ff6e5b9f23380ba8d9cc.jpg"><br><br>  Sepintas, ini adalah kombinasi paralel dari filter konvolusional 1x1, 3x3 dan 5x5.  Tetapi yang paling penting adalah penggunaan blok konvolusi 1x1 (NiN) untuk mengurangi jumlah properti sebelum melayani di blok paralel "mahal".  Biasanya bagian ini disebut bottleneck, itu dijelaskan lebih rinci dalam bab berikutnya. <br><br>  GoogLeNet menggunakan batang tanpa modul Inception sebagai lapisan awal, dan juga menggunakan penyatuan rata-rata dan pengklasifikasi softmax mirip dengan NiN.  Klasifikasi ini melakukan sangat sedikit operasi dibandingkan dengan AlexNet dan VGG.  Ini juga membantu menciptakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">arsitektur jaringan saraf yang sangat efisien</a> . <br><br><h3>  Lapisan bottleneck </h3><br>  Lapisan ini mengurangi jumlah properti (dan karena itu operasi) di setiap lapisan, sehingga kecepatan memperoleh hasil dapat dipertahankan pada tingkat tinggi.  Sebelum mentransfer data ke modul konvolusional “mahal”, jumlah properti dikurangi, katakanlah, 4 kali.  Ini sangat mengurangi jumlah perhitungan, yang telah membuat arsitektur populer. <br><br>  Mari kita cari tahu.  Misalkan kita memiliki 256 properti pada input dan 256 pada output, dan biarkan layer Inception hanya melakukan konvolusi 3x3.  Kami mendapatkan konvolusi 256x256x3x3 (589.000 operasi multiplikasi akumulasi, yaitu operasi MAC).  Ini mungkin melampaui persyaratan kecepatan komputasi kami, misalkan sebuah layer diproses dalam 0,5 milidetik di Google Server.  Kemudian kurangi jumlah properti untuk dilipat menjadi 64 (256/4).  Dalam hal ini, pertama-tama lakukan konvolusi 1x1 256 -&gt; 64, lalu konvolusi 64 lainnya di semua cabang Inception, dan kemudian terapkan konvolusi 1x1 dengan 64 -&gt; 256 properti.  Jumlah operasi: <br><br><ul><li>  256 × 64 × 1 × 1 = 16.000 </li><li>  64 × 64 × 3 × 3 = 36.000 </li><li>  64 × 256 × 1 × 1 = 16.000 </li></ul><br>  Hanya sekitar 70.000, jumlah operasi berkurang hampir 10 kali lipat!  Tetapi pada saat yang sama, kami tidak kehilangan generalisasi di lapisan ini.  Lapisan bottleneck telah menunjukkan kinerja yang sangat baik pada dataset ImageNet, dan telah digunakan dalam arsitektur selanjutnya seperti ResNet.  Alasan kesuksesan mereka adalah bahwa properti input berkorelasi, yang berarti bahwa Anda dapat menghilangkan redundansi dengan menggabungkan properti dengan benar dengan konvolusi 1x1.  Dan setelah melipat dengan properti lebih sedikit, Anda dapat kembali menyebarkannya ke dalam kombinasi yang signifikan pada lapisan berikutnya. <br><br><h3>  Inception V3 (dan V2) </h3><br>  Christian dan timnya telah terbukti menjadi peneliti yang sangat efektif.  Pada bulan Februari 2015, arsitektur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Inception yang dinormalisasi Batch</a> diperkenalkan sebagai versi kedua <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Inception</a> .  Batch-normalisasi menghitung rata-rata dan standar deviasi dari semua peta distribusi properti di lapisan output, dan menormalkan tanggapan mereka dengan nilai-nilai ini.  Ini sesuai dengan "pemutihan" data, yaitu, respons dari semua peta saraf terletak pada kisaran yang sama dan dengan rata-rata nol.  Pendekatan ini membuat pembelajaran lebih mudah, karena lapisan berikutnya tidak diperlukan untuk mengingat offset data input dan hanya dapat mencari kombinasi properti terbaik. <br><br>  Pada Desember 2015, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">versi baru modul Inception dan arsitektur yang sesuai dirilis</a> .  Artikel penulis lebih baik menjelaskan arsitektur GoogLeNet asli, yang menceritakan lebih banyak tentang keputusan yang dibuat.  Ide-ide kunci: <br><br><ul><li>  Memaksimalkan aliran informasi dalam jaringan karena keseimbangan antara kedalaman dan lebarnya.  Sebelum setiap penyatuan, peta properti bertambah. </li><li>  Dengan meningkatnya kedalaman, jumlah properti atau lebar lapisan juga meningkat secara sistematis. </li><li>  Lebar setiap lapisan meningkat untuk meningkatkan kombinasi properti sebelum lapisan berikutnya. </li><li>  Sedapat mungkin, hanya 3x3 konvolusi yang digunakan.  Mengingat bahwa filter 5x5 dan 7x7 dapat didekomposisi menggunakan beberapa 3x3 <br><br><img src="https://habrastorage.org/getpro/habr/post_images/849/96f/d8c/84996fd8cb1040fbf0a18187313a8a81.jpg"><br><br>  Modul Inception baru terlihat seperti ini: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/975/b0b/ad7/975b0bad7d65a0b37aedf0dc119d03b8.jpg"></li><li>  Filter juga dapat didekomposisi menggunakan <a href="">konvolusi yang dihaluskan</a> menjadi modul yang lebih kompleks: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bb5/c32/21c/bb5c3221cc8f478de3ac5ef504a13357.jpg"></li><li>  Modul Inception dapat mengurangi ukuran data menggunakan pooling selama perhitungan Inception.  Ini mirip dengan melakukan konvolusi dengan langkah secara paralel dengan lapisan penyatuan sederhana: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f8b/4c1/263/f8b4c1263b3883d751c7dfe3788110ca.jpg"></li></ul><br>  Inception menggunakan lapisan penyatuan dengan softmax sebagai penggolong akhir. <br><br><h3>  Resnet </h3><br>  Pada bulan Desember 2015, pada waktu yang hampir bersamaan ketika arsitektur Inception v3 diperkenalkan, sebuah revolusi terjadi - mereka menerbitkan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ResNet</a> .  Ini berisi ide-ide sederhana: serahkan output dari dua lapisan konvolusional yang sukses dan memotong input untuk lapisan berikutnya! <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b8a/05d/8b8/b8a05d8b89f55e8d06bb2eae79bd648b.jpg"><br><br>  Ide-ide semacam itu telah diajukan, misalnya, di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> .  Tetapi dalam kasus ini, penulis memotong DUA lapisan dan menerapkan pendekatan dalam skala besar.  Memotong satu layer tidak memberi banyak manfaat, dan mem-bypass dua adalah kunci.  Ini dapat dilihat sebagai penggolong kecil, sebagai jaringan-dalam-jaringan! <br><br>  Itu juga contoh pertama dari pelatihan jaringan beberapa ratus, bahkan ribuan lapisan. <br>  Multilayer ResNet menggunakan lapisan bottleneck mirip dengan yang digunakan dalam Inception: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0d0/ecf/124/0d0ecf1248874511ae4dbca5f23afcec.jpg"><br><br>  Lapisan ini mengurangi jumlah properti di setiap lapisan, pertama menggunakan konvolusi 1x1 dengan output yang lebih rendah (biasanya seperempat dari input), kemudian lapisan 3x3, dan kemudian kembali melilitkan 1x1 ke dalam sejumlah besar properti.  Seperti dalam kasus modul Inception, ini menghemat sumber daya komputasi sambil mempertahankan banyak kombinasi properti.  Bandingkan dengan batang yang lebih kompleks dan kurang jelas dalam Inception V3 dan V4. <br><br>  ResNet menggunakan lapisan penyatuan dengan softmax sebagai penggolong akhir. <br>  Setiap hari, informasi tambahan tentang arsitektur ResNet muncul: <br><br><ul><li>  Hal ini dapat dianggap sebagai sistem modul paralel dan serial secara simultan: dalam banyak modul sinyal keluar datang paralel, dan sinyal output dari masing-masing modul dihubungkan secara seri. </li><li>  ResNet dapat dianggap sebagai beberapa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ansambel modul paralel atau serial</a> . </li><li>  Ternyata ResNet biasanya beroperasi dengan blok kedalaman 20-30 yang relatif kecil yang bekerja secara paralel, daripada berjalan secara berurutan di sepanjang seluruh jaringan. </li><li>  Karena sinyal output kembali dan dimasukkan sebagai input, seperti yang dilakukan dalam RNN, ResNet dapat dianggap sebagai <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">model yang lebih baik dari korteks serebral</a> . </li></ul><br><h3>  Inception V4 </h3><br>  Christian dan timnya unggul lagi dengan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Inception versi baru</a> . <br><br>  Modul awal batang berikut sama dengan dalam Inception V3: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/48b/955/f38/48b955f385c72d21a20af8517d941580.jpg"><br><br>  Dalam hal ini, modul Inception dikombinasikan dengan modul ResNet: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f4c/5f2/bd7/f4c5f2bd765082fe56dac5710fc30221.jpg"><br><br>  Arsitektur ini, menurut selera saya, lebih rumit, kurang elegan, dan juga diisi dengan solusi heuristik yang buram.  Sulit untuk memahami mengapa penulis membuat keputusan ini atau itu, dan sama sulitnya untuk memberi mereka penilaian apa pun. <br><br>  Oleh karena itu, hadiah untuk jaringan saraf yang bersih dan sederhana, mudah dipahami dan dimodifikasi, diberikan kepada ResNet. <br><br><h3>  Pemerasan </h3><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">SqueezeNet</a> diterbitkan baru-baru ini.  Ini adalah pembuatan ulang dengan cara baru dari banyak konsep dari ResNet dan Inception.  Para penulis menunjukkan bahwa meningkatkan arsitektur mengurangi ukuran jaringan dan jumlah parameter tanpa algoritma kompresi yang kompleks. <br><br><h3>  ENET </h3><br>  Semua fitur arsitektur terbaru digabungkan menjadi jaringan yang sangat efisien dan kompak, menggunakan sangat sedikit parameter dan daya komputasi, tetapi pada saat yang sama memberikan hasil yang sangat baik.  Arsitekturnya disebut <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ENet</a> , dikembangkan oleh Adam Paszke ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Adam Paszke</a> ).  Sebagai contoh, kami menggunakannya untuk menandai objek yang sangat akurat di layar dan mem-parsing adegan.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Beberapa contoh Enet</a> .  Video-video ini tidak terkait dengan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dataset pelatihan</a> . <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Di sini</a> Anda dapat menemukan detail teknis ENet.  Ini adalah jaringan berdasarkan encoder dan decoder.  Encoder dibangun pada skema kategorisasi CNN biasa, dan decoder adalah netowrk upampling yang dirancang untuk segmentasi dengan menyebarkan kategori kembali ke gambar ukuran asli.  Untuk segmentasi gambar, hanya jaringan saraf yang digunakan, tidak ada algoritma lain. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/18d/c54/7fa/18dc547fade22215961a848b2170b104.png"><br><br>  Seperti yang Anda lihat, ENet memiliki akurasi spesifik tertinggi dibandingkan dengan semua jaringan saraf lainnya. <br><br>  ENet dirancang untuk menggunakan sumber daya sesedikit mungkin dari awal.  Akibatnya, encoder dan decoder bersama-sama hanya menempati 0,7 MB dengan presisi fp16.  Dan dengan ukuran sekecil itu, ENet tidak kalah dengan akurasi segmentasi atau lebih unggul daripada solusi jaringan saraf murni lainnya. <br><br><h3>  Analisis modul </h3><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Diterbitkan</a> penilaian sistematis modul CNN.  Ternyata bermanfaat: <br><br><ul><li>  Gunakan ELU non-linearitas tanpa batch normalisasi (batchnorm) atau ReLU dengan normalisasi. </li><li>  Terapkan transformasi ruang warna RGB yang dipelajari. </li><li>  Gunakan kebijakan peluruhan tingkat pembelajaran linier. </li><li>  Gunakan jumlah layer pooling tengah dan maksimum. </li><li>  Gunakan paket mini 128 atau 256. Jika ini terlalu banyak untuk kartu video Anda, kurangi kecepatan belajar sebanding dengan ukuran paket. </li><li>  Gunakan lapisan yang sepenuhnya terhubung sebagai lapisan konvolusional dan perkiraan rata-rata untuk memberikan solusi akhir. </li><li>  Jika Anda menambah ukuran dataset pelatihan, pastikan Anda belum mencapai dataran tinggi dalam pelatihan.  Kebersihan data lebih penting daripada ukuran. </li><li>  Jika Anda tidak dapat meningkatkan ukuran gambar input, mengurangi langkah di lapisan berikutnya, efeknya akan kurang lebih sama. </li><li>  Jika jaringan Anda memiliki arsitektur yang kompleks dan sangat optimal, seperti di GoogLeNet, ubahlah dengan hati-hati. </li></ul><br><h3>  Xception </h3><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Xception</a> memperkenalkan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">arsitektur</a> yang lebih sederhana dan lebih elegan ke dalam modul Inception, yang tidak kalah efisien dari ResNet dan Inception V4. <br>  Seperti inilah modul Xception: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/632/40a/deb/63240adebe962726f6d035b5a5d16099.jpg"><br><br>  Siapa pun akan menyukai jaringan ini karena kesederhanaan dan keanggunan arsitekturnya: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d64/f09/933/d64f099330f0b9290a99202a50863868.jpg"><br><br>  Ini berisi 36 langkah konvolusi, dan ini mirip dengan ResNet-34.  Pada saat yang sama, model dan kode sederhana, seperti di ResNet, dan jauh lebih menyenangkan daripada di Inception V4. <br><br>  Implementasi torch7 dari jaringan ini tersedia di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> , sementara implementasi Keras / TF tersedia di sini. <br><br>  Anehnya, penulis arsitektur Xception baru-baru ini juga terinspirasi oleh <a href="">pekerjaan kami pada filter convolutional yang terpisah</a> . <br><br><h3>  MobileNets </h3><br>  Arsitektur baru M <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">obileNets</a> dirilis pada April 2017.  Untuk mengurangi jumlah parameter, ia menggunakan konvolusi yang dapat dilepas, sama seperti pada Xception.  Juga dinyatakan dalam karya ini bahwa penulis dapat sangat mengurangi jumlah parameter: sekitar setengah dalam kasus FaceNet.   : <br><br><img src="https://habrastorage.org/getpro/habr/post_images/689/04b/c1e/68904bc1e353888d4fcd54975a064362.jpg"><br><br>         ,         1 (batch of 1)   Titan Xp.      : <br><br><ul><li> resnet18: 0,002871 </li><li> alexnet: 0,001003 </li><li> vgg16: 0,001698 </li><li> squeezenet: 0,002725 </li><li> mobilenet: 0,033251 </li></ul><br>     !        ,     . <br><br><h3>    </h3><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">FractalNet</a>   ,      ImageNet        ResNet. <br><br><h3>  </h3><br>  ,           .         ,  . <br><br>   ,         ,       ,   ,       ?  ,       . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="></a>    . <br>  ,        .      ,         . <br><br>         , . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">  </a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id430524/">https://habr.com/ru/post/id430524/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id430512/index.html">Bagaimana freelancer hidup: jangan bekerja dengan klien yang tahu segalanya dan biarkan diri Anda menunda-nunda</a></li>
<li><a href="../id430514/index.html">Blockchain Charity - DataArt Menangkan Malta Blockchain Summit Hackathon</a></li>
<li><a href="../id430518/index.html">Cara merender bingkai Middle Earth: Shadow of Mordor</a></li>
<li><a href="../id430520/index.html">Memperkenalkan MongoDB Data Musim Semi</a></li>
<li><a href="../id430522/index.html">Apakah Anda memerlukan budaya perusahaan di bidang TI? Pengakuan manajer merek studio Krasnodar Plarium</a></li>
<li><a href="../id430526/index.html">Mesin slot: dari mana asalnya di USSR dan bagaimana pengaturannya</a></li>
<li><a href="../id430528/index.html">Pemrograman dengan PyUSB 1.0</a></li>
<li><a href="../id430530/index.html">Server tiruan untuk otomatisasi pengujian seluler</a></li>
<li><a href="../id430532/index.html">Keamanan di aplikasi iOS</a></li>
<li><a href="../id430534/index.html">Membuat templat untuk Zabbix menggunakan DVR Trassir SDK sebagai contoh</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>