<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚òùüèº üçÇ üôÜüèΩ C√≥mo AWS elabora sus servicios resistentes. Escala de red üë∑ ‚úä üè©</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La red de Amazon Web Services tiene 69 ubicaciones en todo el mundo en 22 regiones: Estados Unidos, Europa, Asia, √Åfrica y Australia. En cada zona hay...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>C√≥mo AWS elabora sus servicios resistentes. Escala de red</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/471688/"> La red de Amazon Web Services tiene 69 ubicaciones en todo el mundo en 22 regiones: Estados Unidos, Europa, Asia, √Åfrica y Australia.  En cada zona hay hasta 8 centros de datos: centros de procesamiento de datos.  Cada centro de datos tiene miles o cientos de miles de servidores.  La red est√° construida de tal manera que se tienen en cuenta todos los escenarios improbables de interrupci√≥n.  Por ejemplo, todas las regiones est√°n aisladas unas de otras, y las zonas de acceso est√°n separadas por varios kil√≥metros.  Incluso si corta el cable, el sistema cambiar√° a canales de respaldo y la p√©rdida de informaci√≥n equivaldr√° a unidades de paquetes de datos.  Sobre qu√© otros principios se construye la red y c√≥mo se construye, le dir√° a Vasily Pantyukhin. <br><br><img src="https://habrastorage.org/webt/5p/_u/v_/5p_uv_g6etdeiay-nwb0r6v8ns0.png"><br><br>  <b>Vasily Pantyukhin</b> comenz√≥ como administrador de Unix en empresas .ru, pas√≥ 6 a√±os en las grandes gl√°ndulas del microsistema solar y durante 11 a√±os predic√≥ la centralizaci√≥n de datos del mundo en EMC.  Evolucion√≥ naturalmente a nubes privadas, luego se hizo p√∫blico.  Ahora, como arquitecto de Amazon Web Services, el asesoramiento t√©cnico lo ayuda a vivir y crecer en la nube de AWS. <br><br>  En la parte anterior de la trilog√≠a de dispositivos de AWS, Vasily profundiz√≥ en el dispositivo de servidores f√≠sicos y el escalado de bases de datos.  Nitro-cards, hipervisor personalizado basado en KVM, base de datos de Amazon Aurora, todo esto en el art√≠culo " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">C√≥mo AWS" cocina "sus servicios el√°sticos.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Escalado de servidor y base de datos</a> ".  Lea para sumergirse en el contexto o vea un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">video de la</a> presentaci√≥n. <br><br>  En esta parte, nos centraremos en el escalado de la red, uno de los sistemas m√°s complejos de AWS.  Evoluci√≥n de una red plana a Virtual Private Cloud y su dispositivo, servicios internos Blackfoot e HyperPlane, el problema de un vecino ruidoso y, al final, la escala de la red, la red troncal y los cables f√≠sicos.  Sobre todo esto bajo el corte. <br><br>  <i>Descargo de responsabilidad: todo lo que sigue es la opini√≥n personal de Vasily, y puede no coincidir con la posici√≥n de los servicios web de Amazon.</i> <br><a name="habracut"></a><br><h2>  Escala de red </h2><br>  AWS Cloud se lanz√≥ en 2006.  Su red era bastante primitiva, con una estructura plana.  El rango de direcciones privadas era com√∫n a todos los inquilinos de la nube.  Cuando inicia una nueva m√°quina virtual, accidentalmente recibi√≥ una direcci√≥n IP disponible de este rango. <br><br><img src="https://habrastorage.org/webt/kj/u9/jw/kju9jwz69aeoldlbn6yn_vmtzom.jpeg"><br><br>  Este enfoque fue f√°cil de implementar, pero limit√≥ fundamentalmente el uso de la nube.  En particular, fue bastante dif√≠cil desarrollar soluciones h√≠bridas que combinaran redes privadas en el terreno y en AWS.  El problema m√°s com√∫n fue la intersecci√≥n de rangos de direcciones IP. <br><br><img src="https://habrastorage.org/webt/lw/fu/tg/lwfutg75jtwalgbliyv-rmdhfbc.jpeg"><br><br><h3>  Nube privada virtual </h3><br>  La nube estaba en demanda.  Es hora de pensar en la escalabilidad y la posibilidad de su uso por decenas de millones de inquilinos.  La red plana se ha convertido en un gran obst√°culo.  Por lo tanto, pensamos en c√≥mo aislar a los usuarios entre s√≠ a nivel de red para que puedan seleccionar independientemente los rangos de IP. <br><br><img src="https://habrastorage.org/webt/fl/tw/co/fltwcomu7sr802932c3wyyaucve.jpeg"><br><br>  ¬øQu√© le viene a la mente primero cuando piensa en el aislamiento de la red?  Por supuesto, <b>VLAN</b> y <b>VRF son enrutamiento y reenv√≠o virtuales</b> . <br><br>  Lamentablemente, esto no funcion√≥.  La ID de VLAN es de solo 12 bits, lo que nos da solo 4096 segmentos aislados.  Incluso en los conmutadores m√°s grandes, puede usar un m√°ximo de 1-2 mil VRF.  El uso combinado de VRF y VLAN nos da solo unos pocos millones de subredes.  Esto definitivamente no es suficiente para decenas de millones de inquilinos, cada uno de los cuales deber√≠a poder usar varias subredes. <br><br>  A√∫n as√≠, simplemente no podemos permitirnos comprar la cantidad requerida de cajas grandes, por ejemplo, de Cisco o Juniper.  Hay dos razones: es extremadamente costoso y no queremos depender de sus pol√≠ticas de desarrollo y parches. <br><br><blockquote>  Solo hay una conclusi√≥n: cocinar tu propia decisi√≥n. </blockquote><br>  En 2009, anunciamos <b>VPC</b> - <b>Virtual Private Cloud</b> .  El nombre ha echado ra√≠ces y ahora muchos proveedores de la nube tambi√©n lo usan. <br><br>  VPC es una red virtual de red definida por software ( <b>SDN</b> ).  Decidimos no inventar protocolos especiales en los niveles L2 y L3.  La red se ejecuta en Ethernet e IP est√°ndar.  Para la transmisi√≥n a trav√©s de una red, el tr√°fico de la m√°quina virtual se encapsula en un contenedor de nuestro propio protocolo.  Indica la identificaci√≥n que pertenece a la VPC de inquilinos. <br><br><img src="https://habrastorage.org/webt/x7/yh/nc/x7yhncwzn9xfi677tpy65s3td18.jpeg"><br><br>  Eso suena facil.  Sin embargo, es necesario resolver varios problemas t√©cnicos serios.  Por ejemplo, d√≥nde y c√≥mo almacenar datos de mapeo para direcciones MAC / IP virtuales, ID de VPC y direcciones MAC / IP f√≠sicas correspondientes.  En una escala de AWS, esta es una tabla enorme que deber√≠a funcionar con una latencia m√≠nima.  El <b>servicio de mapeo</b> , que est√° manchado con una capa delgada en toda la red, es responsable de esto. <br><br>  En m√°quinas de nuevas generaciones, la encapsulaci√≥n se realiza mediante tarjetas Nitro a nivel de hierro.  En casos m√°s antiguos, encapsulaci√≥n y decapsulaci√≥n de software. <br><br><img src="https://habrastorage.org/webt/6q/pt/zr/6qptzrmyqowtaimwlbu6mqobr44.jpeg"><br><br>  Veamos c√≥mo funciona esto en t√©rminos generales.  Comencemos con el nivel L2.  Supongamos que tenemos una m√°quina virtual con IP 10.0.0.2 en un servidor f√≠sico 192.168.0.3.  Env√≠a datos a una m√°quina virtual 10.0.0.3 que vive en 192.168.1.4.  Se genera una solicitud ARP, que cae en la tarjeta de red Nitro.  Para simplificar, creemos que ambas m√°quinas virtuales viven en la misma VPC "azul". <br><br><img src="https://habrastorage.org/webt/vl/u5/hv/vlu5hvvmaufe2e2jirp0mz14ugg.png"><br><br>  La tarjeta reemplaza la direcci√≥n de origen con la suya y env√≠a la trama ARP al servicio de mapeo. <br><br><img src="https://habrastorage.org/webt/0j/d9/lx/0jd9lx41a5744ptq64tdrmgzxya.png"><br><br>  El servicio de mapeo devuelve la informaci√≥n que se necesita para la transmisi√≥n a trav√©s de la red f√≠sica L2. <br><br><img src="https://habrastorage.org/webt/3e/ir/nt/3eirntgsopwiccgdaaneovvgqxe.png"><br><br>  La tarjeta nitro en la respuesta ARP reemplaza el MAC en la red f√≠sica con la direcci√≥n en la VPC. <br><br><img src="https://habrastorage.org/webt/fv/uo/kh/fvuokhk6mswvl5i8ifguxgxrhee.png"><br><br>  Al transferir datos, envolvemos el MAC l√≥gico y la IP en un contenedor VPC.  Todo esto se transmite a trav√©s de la red f√≠sica utilizando las tarjetas IP Nitro apropiadas de origen y destino. <br><br><img src="https://habrastorage.org/webt/4k/4i/u-/4k4iu-ei5cp9cdk-vfqmmf8vtig.png"><br><br>  La m√°quina f√≠sica del paquete est√° destinada a realizar verificaciones.  Esto es para evitar la posibilidad de falsificaci√≥n.  La m√°quina env√≠a una solicitud especial al servicio de mapeo y pregunta: ‚ÄúDesde la m√°quina f√≠sica 192.168.0.3 recib√≠ un paquete dise√±ado para 10.0.0.3 en la VPC azul.  ¬øEs leg√≠timo? <br><br><img src="https://habrastorage.org/webt/vv/qn/0e/vvqn0ecobvoxvxw8-1u3hgc8k48.png"><br><br>  El servicio de mapeo verifica su tabla de asignaci√≥n de recursos y permite o niega el paso del paquete.  En todos los casos nuevos, la validaci√≥n adicional se cose en las tarjetas Nitro.  Es imposible moverse incluso te√≥ricamente.  Por lo tanto, la suplantaci√≥n de recursos en otra VPC no funcionar√°. <br><br><img src="https://habrastorage.org/webt/xx/tr/wa/xxtrwaqlrsbhdledrligrt0mfjc.png"><br><br>  Luego, los datos se env√≠an a la m√°quina virtual para la que est√°n destinados. <br><br><img src="https://habrastorage.org/webt/1i/hb/py/1ihbpywnhbngzsuzs8376qf0i8g.png"><br><br>  El servicio de mapeo tambi√©n funciona como un enrutador l√≥gico para transferir datos entre m√°quinas virtuales en diferentes subredes.  Todo es conceptualmente simple all√≠, no lo analizar√© en detalle. <br><br><img src="https://habrastorage.org/webt/rr/rj/9n/rrrj9nvl-jwgk54pzowtmqm6ynm.png"><br><br>  Resulta que durante la transmisi√≥n de cada paquete, los servidores acceden al servicio de mapeo.  ¬øC√≥mo lidiar con los retrasos inevitables?  <b>Almacenamiento en cach√©</b> , por supuesto. <br><br>  Todo el encanto es que no necesita almacenar en cach√© toda la gran mesa.  Las m√°quinas virtuales de un n√∫mero relativamente peque√±o de VPC viven en un servidor f√≠sico.  La informaci√≥n solo debe almacenarse en cach√© sobre estas VPC.  La transferencia de datos a otras VPC en la configuraci√≥n "predeterminada" a√∫n no es leg√≠tima.  Si se utiliza la funcionalidad como el emparejamiento de VPC, la informaci√≥n sobre los VPC correspondientes se carga adicionalmente en la memoria cach√©. <br><br><img src="https://habrastorage.org/webt/cj/vu/uh/cjvuuhb_xbbsrhjdzxpf0x7lnck.jpeg"><br><br>  Con la transferencia de datos a la VPC resuelto. <br><br><h3>  Blackfoot </h3><br>  ¬øQu√© hacer en los casos en que el tr√°fico debe transmitirse al exterior, por ejemplo, en Internet o a trav√©s de una VPN al suelo?  Aqu√≠ es donde <b>Blackfoot</b> , el servicio interno de AWS, nos ayuda.  Est√° dise√±ado por nuestro equipo sudafricano.  Por lo tanto, el servicio lleva el nombre del ping√ºino que vive en Sud√°frica. <br><br><img src="https://habrastorage.org/webt/af/7s/gf/af7sgf9jhvniudixr94rqhwgdy0.jpeg"><br><br>  Blackfoot decapsula el tr√°fico y hace lo que necesita con √©l.  Los datos de Internet se env√≠an tal cual. <br><br><img src="https://habrastorage.org/webt/fi/93/02/fi9302-pumvdpx70gqpo7ufaaqw.png"><br><br>  Los datos se desencapsulan y se envuelven nuevamente en un contenedor IPsec cuando se usa una VPN. <br><br><img src="https://habrastorage.org/webt/jk/ag/iu/jkagiufrbqprn50vjuxqy_hwbe8.png"><br><br>  Cuando se usa Direct Connect, el tr√°fico se etiqueta y se transmite a la VLAN correspondiente. <br><br><img src="https://habrastorage.org/webt/yj/xj/i0/yjxji0wexg4cugs-cztvnv_wvxs.png"><br><br><h3>  HyperPlane </h3><br>  Este es un servicio de control de flujo interno.  Muchos servicios de red requieren monitorear el <b>estado del flujo de datos</b> .  Por ejemplo, cuando se utiliza NAT, el control de flujo debe garantizar que cada par "IP: puerto de destino" tenga un puerto de salida √∫nico.  En el caso del equilibrador <b>NLB</b> - <b>Equilibrador de carga de red</b> , el flujo de datos siempre debe dirigirse a la misma m√°quina virtual de destino.  Grupos de seguridad es un firewall con estado.  Supervisa el tr√°fico entrante y abre impl√≠citamente puertos para la secuencia de paquetes salientes. <br><br><img src="https://habrastorage.org/webt/wq/pa/kk/wqpakkyp8v_rdhyuclzte2e2y6w.jpeg"><br><br>  En la nube de AWS, los requisitos de latencia de transmisi√≥n son extremadamente altos.  Por lo tanto, <b>HyperPlane es</b> cr√≠tico para la salud de toda la red. <br><br><img src="https://habrastorage.org/webt/ov/cr/wm/ovcrwmosat9fxborz1tmkxkjt_4.jpeg"><br><br>  Hyperplane est√° construido en m√°quinas virtuales EC2.  Aqu√≠ no hay magia, solo astucia.  El truco es que son m√°quinas virtuales con gran RAM.  Las transacciones son transaccionales y se realizan exclusivamente en la memoria.  Esto permite retrasos de solo decenas de microsegundos.  Trabajar con un disco matar√≠a todo el rendimiento. <br><br>  Hyperplane es un sistema distribuido de una gran cantidad de tales m√°quinas EC2.  Cada m√°quina virtual tiene un ancho de banda de 5 GB / s.  En toda la red regional, esto produce terabits salvajes de ancho de banda y le permite procesar <b>millones de conexiones por segundo</b> . <br><br>  HyperPlane solo funciona con hilos.  La encapsulaci√≥n de paquetes VPC es completamente transparente para √©l.  La vulnerabilidad potencial en este servicio interno a√∫n no permitir√° romper el aislamiento de VPC.  Por seguridad, los niveles a continuaci√≥n son responsables. <br><br><h3>  Vecino ruidoso </h3><br>  Tambi√©n existe el <b>problema</b> <b>vecino ruidoso</b> .  Supongamos que tenemos 8 nodos.  Estos nodos procesan los hilos de todos los usuarios de la nube.  Todo parece estar bien y la carga debe distribuirse uniformemente en todos los nodos.  Los nodos son muy potentes y dif√≠ciles de sobrecargar. <br><br>  Pero estamos construyendo nuestra arquitectura basada en escenarios incluso poco probables. <br><br><blockquote>  Baja probabilidad no significa imposibilidad. </blockquote><br>  Podemos imaginar una situaci√≥n en la que uno o m√°s usuarios generar√°n demasiada carga.  Todos los nodos HyperPlane est√°n involucrados en el procesamiento de esta carga, y otros usuarios pueden sentir alg√∫n tipo de degradaci√≥n del rendimiento.  Esto destruye el concepto de la nube, en el que los inquilinos no tienen forma de influenciarse entre s√≠. <br><br><img src="https://habrastorage.org/webt/gc/ni/_l/gcni_lqe59zlatesmuodjxmcxcm.png"><br><br>  ¬øC√≥mo resolver el problema de un vecino ruidoso?  Lo primero que viene a la mente es fragmentar.  Nuestros 8 nodos est√°n divididos l√≥gicamente en 4 fragmentos con 2 nodos en cada uno.  Ahora, un vecino ruidoso se ver√° obstaculizado por solo una cuarta parte de todos los usuarios, pero mucho m√°s. <br><br><img src="https://habrastorage.org/webt/e7/vz/-v/e7vz-vablrrhawvz2xbvb1psfxu.png"><br><br>  Hag√°moslo de manera diferente.  A cada usuario se le asignan solo 3 nodos. <br><br><img src="https://habrastorage.org/webt/md/su/px/mdsupxahouehxh6y-jffnyhjpyy.png"><br><br>  El truco es asignar nodos a diferentes usuarios al azar.  En la imagen a continuaci√≥n, el usuario azul cruza los nodos con uno de los otros dos usuarios: verde y naranja. <br><br><img src="https://habrastorage.org/webt/xw/ee/yz/xweeyztmogrwbeomfqi_sbec0u0.png"><br><br>  Con 8 nodos y 3 usuarios, la probabilidad de que un vecino ruidoso se cruce con uno de los usuarios es del 54%.  Es con esta probabilidad que el usuario azul afectar√° a otros inquilinos.  Adem√°s, solo una parte de su carga.  En nuestro ejemplo, esta influencia ser√° al menos de alguna manera imperceptible para todos, pero solo un tercio de todos los usuarios.  Este ya es un buen resultado. <br><div class="scrollable-table"><table><tbody><tr><td>  El n√∫mero de usuarios que se cruzan <br></td><td>  Probabilidad en porcentaje <br></td></tr><tr><td>  0 0 <br></td><td>  18% <br></td></tr><tr><td>  1 <br></td><td>  54% <br></td></tr><tr><td>  2 <br></td><td>  26% <br></td></tr><tr><td>  3 <br></td><td>  2% <br></td></tr></tbody></table></div><br>  Acerquemos la situaci√≥n a la real: tome 100 nodos y 5 usuarios en 5 nodos.  En este caso, ninguno de los nodos se cruza con una probabilidad del 77%. <br><div class="scrollable-table"><table><tbody><tr><td>  El n√∫mero de usuarios que se cruzan <br></td><td>  Probabilidad en porcentaje <br></td></tr><tr><td>  0 0 <br></td><td>  77% <br></td></tr><tr><td>  1 <br></td><td>  21% <br></td></tr><tr><td>  2 <br></td><td>  1,8% <br></td></tr><tr><td>  3 <br></td><td>  0,06% <br></td></tr><tr><td>  4 4 <br></td><td>  0,0006% <br></td></tr><tr><td>  5 5 <br></td><td>  0.00000013% <br></td></tr></tbody></table></div><br>  En una situaci√≥n real con una gran cantidad de nodos y usuarios de HyperPlane, el impacto potencial de un vecino ruidoso en otros usuarios es m√≠nimo.  Este m√©todo se llama <b>shuffle sharding</b> .  Minimiza el efecto negativo de la falla del nodo. <br><br>  Basado en HyperPlane, se crean muchos servicios: Network Load Balancer, NAT Gateway, Amazon EFS, AWS PrivateLink, AWS Transit Gateway. <br><br><h3>  Escala de red </h3><br>  Ahora hablemos de la escala de la red misma.  Para octubre de 2019, AWS ofrece sus servicios en <b>22 regiones</b> , y se planean 9 m√°s. <br><br><ul><li>  Cada regi√≥n contiene varias zonas de disponibilidad.  Hay 69 de ellos en el mundo. <br></li><li>  Cada AZ consta de centros de procesamiento de datos.  No hay m√°s de 8 de ellos. <br></li><li>  En el centro de datos hay una gran cantidad de servidores, algunos de hasta 300,000. <br></li></ul><br>  Ahora todo esto se promedia, multiplica y obtiene una cifra impresionante que muestra la <b>escala de la nube de Amazon</b> . <br><br>  Entre las zonas de acceso y el centro de datos, se colocan muchos canales √≥pticos.  En una de nuestras regiones m√°s grandes, solo se han establecido 388 canales para la comunicaci√≥n de AZ entre ellos y los centros de comunicaci√≥n con otras regiones (Centros de tr√°nsito).  En total, esto da un loco <b>5000 Tbit</b> . <br><br><img src="https://habrastorage.org/webt/zi/nj/bj/zinjbjkov6298ccr_kuiux7z89k.jpeg"><br><br>  Backbone AWS est√° construido espec√≠ficamente para la nube y optimizado para trabajar con √©l.  Lo construimos en canales de <b>100 GB / s</b> .  Los controlamos completamente, con la excepci√≥n de las regiones de China.  El tr√°fico no se comparte con las cargas de otras compa√±√≠as. <br><br><img src="https://habrastorage.org/webt/e-/ko/hs/e-kohsjs0wax_tmb0zd3l5xh5ys.png"><br><br>  Por supuesto, no somos el √∫nico proveedor de la nube con una red troncal privada.  Cada vez m√°s empresas grandes van por este camino.  Esto es confirmado por investigadores independientes, por ejemplo, de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Telegeography</a> . <br><br><img src="https://habrastorage.org/webt/my/en/ad/myenadxrsqlm4a58uyc6a_us7kq.jpeg"><br><br>  El gr√°fico muestra que la proporci√≥n de proveedores de contenido y proveedores en la nube est√° creciendo.  Debido a esto, la proporci√≥n del tr√°fico de Internet de los proveedores de red troncal est√° disminuyendo constantemente. <br><br>  Explicar√© por qu√© sucede esto.  Anteriormente, la mayor√≠a de los servicios web estaban disponibles y se consum√≠an directamente desde Internet.  Ahora, cada vez m√°s servidores se encuentran en la nube y est√°n disponibles a trav√©s de <b>CDN</b> - <b>Content Distribution Network</b> .  Para acceder al recurso, el usuario pasa por Internet solo hasta el <b>punto de presencia</b> CDN m√°s cercano.  Muy a menudo est√° en alg√∫n lugar cercano.  Luego deja el Internet p√∫blico y vuela a trav√©s del Atl√°ntico a trav√©s de una red troncal privada, por ejemplo, y llega directamente al recurso. <br><br>  Me pregunto c√≥mo cambiar√° Internet en 10 a√±os si esta tendencia contin√∫a. <br><br><h3>  Canales fisicos </h3><br>  Los cient√≠ficos a√∫n no han descubierto c√≥mo aumentar la velocidad de la luz en el Universo, pero han hecho grandes avances en los m√©todos de transmisi√≥n a trav√©s de la fibra √≥ptica.  Actualmente estamos utilizando cables de fibra 6912.  Esto ayuda a optimizar significativamente el costo de su instalaci√≥n. <br><br>  En algunas regiones tenemos que usar cables especiales.  Por ejemplo, en la regi√≥n de Sydney, utilizamos cables con un recubrimiento especial contra las termitas. <br><br><img src="https://habrastorage.org/webt/qc/vn/wh/qcvnwhnrrbnil48u0qqgblljgna.jpeg"><br><br>  Nadie est√° a salvo de problemas y, a veces, nuestros canales est√°n da√±ados.  La foto de la derecha muestra cables √≥pticos en una de las regiones estadounidenses que fueron rasgados por los constructores.  Como resultado del accidente, solo se perdieron 13 paquetes de datos, lo cual es sorprendente.  Una vez m√°s, ¬°solo 13!  El sistema literalmente cambi√≥ instant√°neamente a canales de respaldo: la b√°scula funciona. <br><br>  Galopamos sobre algunos servicios y tecnolog√≠as en la nube de Amazon.  Espero que tenga al menos alguna idea de la escala de las tareas que nuestros ingenieros tienen que resolver.  Personalmente, estoy muy interesado en esto. <br><br><blockquote>  Esta es la parte final de la trilog√≠a de Vasily Pantyukhin sobre el dispositivo AWS.  La <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">primera</a> parte describe la optimizaci√≥n del servidor y el escalado de la base de datos, y la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">segunda</a> describe las funciones sin servidor y Firecracker. <br><br>  En <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">HighLoad ++</a> en noviembre, Vasily Pantyukhin compartir√° nuevos detalles del dispositivo Amazon.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Hablar√°</a> sobre las causas de fallas y el dise√±o de sistemas distribuidos en Amazon.  El 24 de octubre, a√∫n puede <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">reservar un</a> boleto a un buen precio y pagar m√°s tarde.  ¬°Te esperamos en HighLoad ++, ven y habla! </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/471688/">https://habr.com/ru/post/471688/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../471670/index.html">Tratando Jetpack Compose en la batalla?</a></li>
<li><a href="../471676/index.html">Estafadores telef√≥nicos. La segunda acci√≥n, en la que me descompongo y corro al cajero autom√°tico m√°s cercano</a></li>
<li><a href="../471678/index.html">Tenga servicios a pedido</a></li>
<li><a href="../471684/index.html">¬øPor qu√© necesita crear m√≥dulos para nginx?</a></li>
<li><a href="../471686/index.html">C√≥mo AWS elabora sus servicios resistentes. Escalado de servidor y base de datos</a></li>
<li><a href="../471700/index.html">C√≥mo eleg√≠ una pila tecnol√≥gica con una base para el futuro</a></li>
<li><a href="../471702/index.html">Aplicaciones web mejoradas cibern√©ticamente</a></li>
<li><a href="../471704/index.html">El libro "Las mitocondrias ego√≠stas. C√≥mo mantener la salud y mover la vejez "</a></li>
<li><a href="../471706/index.html">9 problemas de red t√≠picos que se pueden detectar usando el an√°lisis de NetFlow (usando Flowmon como ejemplo)</a></li>
<li><a href="../471708/index.html">Los storypoints son peligrosos para el desarrollo de aplicaciones cliente-servidor</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>