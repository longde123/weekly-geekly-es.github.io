<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë≥üèø üë©üèæ‚Äçüè≠ üë©üèª‚Äçüé§ Word2vec em imagens üëàüèΩ üëä üôà</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="‚Äú Tudo esconde um padr√£o que faz parte do universo. Possui simetria, eleg√¢ncia e beleza - qualidades que antes de tudo s√£o apreendidas por todo verdad...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Word2vec em imagens</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/446530/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/85d/ad8/627/85dad8627ae6845b62f5bb965c291b19.png"></div><br><br><blockquote>  <font color="gray">‚Äú <b>Tudo esconde um padr√£o que faz parte do universo.</b></font>  <font color="gray"><b>Possui simetria, eleg√¢ncia e beleza</b> - qualidades que antes de tudo s√£o apreendidas por todo verdadeiro artista que captura o mundo.</font>  <font color="gray">Esse padr√£o pode ser detectado na mudan√ßa das esta√ß√µes, na maneira como a areia flui ao longo da encosta, nos galhos emaranhados do arbusto do creosoto, no padr√£o de suas folhas.</font> <font color="gray"><br><br></font>  <font color="gray">Estamos tentando copiar esse padr√£o em nossa vida e em nossa sociedade e, portanto, amamos ritmo, m√∫sica, dan√ßa, v√°rias formas que nos fazem felizes e nos confortam.</font>  <font color="gray">No entanto, tamb√©m se pode discernir o perigo √† espreita na busca da perfei√ß√£o absoluta, pois √© √≥bvio que o padr√£o perfeito √© inalterado.</font>  <font color="gray">E, aproximando-se da perfei√ß√£o, todas as coisas v√£o √† morte ‚Äù- <i>Dune</i> (1965)</font> </blockquote><br>  Acredito que o conceito de incorpora√ß√£o √© uma das id√©ias mais marcantes do aprendizado de m√°quina.  Se voc√™ j√° usou o Siri, o Google Assistant, o Alexa, o Google Translate ou mesmo um teclado de smartphone com a previs√£o da pr√≥xima palavra, j√° trabalhou com o modelo de processamento de idioma natural baseado em anexo.  Nas √∫ltimas d√©cadas, esse conceito evoluiu significativamente para modelos neurais (desenvolvimentos recentes incluem incorpora√ß√£o de palavras contextualizadas em modelos avan√ßados, como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">BERT</a> e GPT2). <br><a name="habracut"></a><br>  O Word2vec √© um m√©todo eficaz de cria√ß√£o de investimentos desenvolvido em 2013.  Al√©m de trabalhar com palavras, alguns de seus conceitos foram eficazes no desenvolvimento de mecanismos de recomenda√ß√£o e na atribui√ß√£o de significado aos dados, mesmo em tarefas comerciais e n√£o lingu√≠sticas.  Essa tecnologia foi usada por empresas como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Airbnb</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Alibaba</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Spotify</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Anghami</a> em seus mecanismos de recomenda√ß√£o. <br><br>  Neste artigo, veremos o conceito e a mec√¢nica de gera√ß√£o de anexos usando o word2vec.  Vamos come√ßar com um exemplo para se familiarizar com como representar objetos em forma de vetor.  Voc√™ sabe o quanto uma lista de cinco n√∫meros (vetor) pode dizer sobre sua personalidade? <br><br><h1>  Personaliza√ß√£o: o que voc√™ √©? </h1><br><blockquote>  <font color="gray">‚ÄúEu te dou o camale√£o do deserto;</font>  <font color="gray">sua capacidade de se fundir com a areia lhe dir√° tudo o que voc√™ precisa saber sobre as ra√≠zes da ecologia e as raz√µes para preservar sua personalidade. ‚Äù</font>  <font color="gray">- <i>Filhos das Dunas</i></font> </blockquote><br>  Em uma escala de 0 a 100, voc√™ tem um tipo de personalidade introvertida ou extrovertida (onde 0 √© o tipo mais introvertido e 100 √© o tipo mais extrovertido)?  Voc√™ j√° passou em um teste de personalidade: por exemplo, MBTI, ou melhor ainda <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">, os Cinco Grandes</a> ?  Voc√™ recebe uma lista de perguntas e √© avaliado em v√°rios eixos, incluindo introvers√£o / extrovers√£o. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/79f/11e/e22/79f11ee220ebf9d6f52f51a5b780b090.png"></div><br>  <i><font color="gray">Exemplo dos cinco grandes resultados do teste.</font></i>  <i><font color="gray">Ele realmente fala muito sobre personalidade e √© capaz de prever <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">o</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">sucesso</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">acad√™mico</a> , <a href="">pessoal</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">profissional</a> .</font></i>  <i><font color="gray">Por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> voc√™ pode passar por isso.</font></i> <br><br>  Suponha que eu pontuei 38 em 100 para avaliar a introvers√£o / extrovers√£o.  Isso pode ser representado da seguinte maneira: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e56/729/35d/e5672935d7de17d41e78354d3742e6bc.png"></div><br><br>  Ou em uma escala de -1 a +1: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b39/e23/ce1/b39e23ce1c036b11763e3c45c3659a3e.png"></div><br><br>  Qu√£o bem reconhecemos uma pessoa apenas dessa avalia√ß√£o?  Na verdade n√£o.  Os seres humanos s√£o criaturas complexas.  Portanto, adicionamos mais uma dimens√£o: mais uma caracter√≠stica do teste. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2aa/ab2/ebc/2aaab2ebc1ff30f1fd832e5cf5bf9cb1.png"></div><br>  <i><font color="gray">Voc√™ pode imaginar essas duas dimens√µes como um ponto no gr√°fico ou, melhor ainda, como um vetor da origem at√© esse ponto.</font></i>  <i><font color="gray">Existem √≥timas ferramentas de vetores que ser√£o √∫teis muito em breve.</font></i> <br><br>  N√£o mostro quais tra√ßos de personalidade colocamos no gr√°fico para que voc√™ n√£o se apegue a tra√ßos espec√≠ficos, mas entendo imediatamente a representa√ß√£o vetorial da personalidade da pessoa como um todo. <br><br>  Agora podemos dizer que esse vetor reflete parcialmente minha personalidade.  Esta √© uma descri√ß√£o √∫til ao comparar pessoas diferentes.  Suponha que fui atropelado por um √¥nibus vermelho e voc√™ precise me substituir por uma pessoa semelhante.  Qual das duas pessoas no gr√°fico a seguir se parece mais comigo? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/de5/380/b84/de5380b84dc9fec4bb8b52ebe6519e15.png"></div><br><br>  Ao trabalhar com vetores, a similaridade √© geralmente calculada pelo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">coeficiente Otiai</a> (coeficiente geom√©trico): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/640/e59/7dd/640e597dd741a28bcec986454633e31d.png"></div><br>  <i><font color="green">A pessoa n√∫mero 1 √©</font> <font color="gray">mais parecida comigo em car√°ter.</font></i>  <i><font color="gray">Os vetores em uma dire√ß√£o (o comprimento tamb√©m √© importante) fornecem um coeficiente Otiai maior</font></i> <br><br>  Novamente, duas dimens√µes n√£o s√£o suficientes para avaliar as pessoas.  D√©cadas de desenvolvimento da ci√™ncia psicol√≥gica levaram √† cria√ß√£o de um teste para cinco caracter√≠sticas b√°sicas da personalidade (com muitas outras adicionais).  Ent√£o, vamos usar todas as cinco dimens√µes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/df5/3ae/d7b/df53aed7b1e439561a01e69b3f765487.png"></div><br><br>  O problema com as cinco dimens√µes √© que n√£o ser√° mais poss√≠vel desenhar setas limpas em 2D.  Esse √© um problema comum no aprendizado de m√°quina, onde voc√™ geralmente precisa trabalhar em um espa√ßo multidimensional.  √â bom que o coeficiente geom√©trico funcione com qualquer n√∫mero de medi√ß√µes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/521/ab3/bf1/521ab3bf1374c5b37115441b7c2d27cc.png"></div><br>  <i><font color="gray">O coeficiente geom√©trico funciona para qualquer n√∫mero de medi√ß√µes.</font></i>  <i><font color="gray">Em cinco dimens√µes, o resultado √© muito mais preciso.</font></i> <br><br>  No final deste cap√≠tulo, quero repetir duas id√©ias principais: <br><br><ol><li>  Pessoas (e outros objetos) podem ser representados como vetores num√©ricos (o que √© √≥timo para carros!). <br></li><li>  Podemos calcular facilmente como os vetores s√£o semelhantes. </li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/195/73d/16d/19573d16de1150ac1874640c79e0b381.png"></div><br><br><h1>  Incorpora√ß√£o de palavras </h1><br><blockquote>  <font color="gray">"O dom das palavras √© o dom da decep√ß√£o e da ilus√£o."</font>  <font color="gray">- <i>Filhos das Dunas</i></font> </blockquote><br>  Com esse entendimento, passaremos para as representa√ß√µes vetoriais de palavras obtidas como resultado do treinamento (elas tamb√©m s√£o chamadas de anexos) e examinaremos suas propriedades interessantes. <br><br>  Aqui est√° o anexo da palavra "rei" (vetor GloVe, treinado na Wikipedia): <br><br> <code>[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]</code> <br> <br>  Vemos uma lista de 50 n√∫meros, mas √© dif√≠cil dizer algo.  Vamos visualiz√°-los para comparar com outros vetores.  Coloque os n√∫meros em uma linha: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/516/c90/5ac/516c905ac831fe8688db73f0a63d325b.png"></div><br><br>  Colorir as c√©lulas de acordo com seus valores (vermelho para pr√≥ximo a 2, branco para pr√≥ximo a 0, azul para pr√≥ximo de -2): <br><br><div style="text-align:center;"> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/46f/7cb/1d5/46f7cb1d5adc32bd16368b2681ab26a4.png"></a> </div><br><br>  Agora esque√ßa os n√∫meros, e somente pelas cores contrastamos o "rei" com outras palavras: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1c8/6b2/909/1c86b290963e8a42b375cb6a71245185.png"></div><br><br>  Voc√™ v√™ que "homem" e "mulher" s√£o muito mais pr√≥ximos um do outro do que o "rei"?  Diz alguma coisa.  As representa√ß√µes vetoriais capturam muitas informa√ß√µes / significados / associa√ß√µes dessas palavras. <br><br>  Aqui est√° outra lista de exemplos (compare colunas com cores semelhantes): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d61/30b/d45/d6130bd4502710631a8c812923230f47.png"></div><br><br>  H√° v√°rias coisas a serem observadas: <br><br><ol><li>  Por todas as palavras passa uma coluna vermelha.  Ou seja, essas palavras s√£o semelhantes nessa dimens√£o espec√≠fica (e n√£o sabemos o que est√° codificado nela). <br></li><li>  Voc√™ pode ver que ‚Äúmulher‚Äù e ‚Äúgarota‚Äù s√£o muito parecidas.  A mesma coisa com "homem" e "menino". <br></li><li>  "Menino" e "menina" tamb√©m s√£o semelhantes em algumas dimens√µes, mas diferem de "mulher" e "homem".  Poderia ser uma vaga id√©ia codificada de juventude?  Provavelmente. <br></li><li>  Tudo, exceto a √∫ltima palavra, s√£o id√©ias das pessoas.  Eu adicionei um objeto (√°gua) para mostrar as diferen√ßas entre as categorias.  Por exemplo, voc√™ pode ver como a coluna azul desce e para na frente do vetor da √°gua. <br></li><li>  Existem dimens√µes claras em que o "rei" e a "rainha" s√£o semelhantes entre si e diferentes de todos os outros.  Talvez um conceito vago de realeza esteja codificado l√°? </li></ol><br><h1>  Analogies </h1><br><blockquote>  <font color="gray">‚ÄúAs palavras suportam qualquer carga que desejamos.</font>  <font color="gray">Tudo o que √© necess√°rio √© um acordo sobre tradi√ß√£o, segundo o qual constru√≠mos conceitos. ‚Äù</font>  <font color="gray">- <i>Deus, o Imperador de Dune</i></font> </blockquote><br>  Exemplos famosos que mostram as incr√≠veis propriedades dos investimentos s√£o o conceito de analogias.  Podemos adicionar e subtrair vetores de palavras, obtendo resultados interessantes.  O exemplo mais famoso √© a f√≥rmula "rei - homem + mulher": <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c23/71f/ead/c2371feadc58f2f2a1236c94b6b05eff.png"></div><br>  <i><font color="gray">Usando a biblioteca <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Gensim</a> em python, podemos adicionar e subtrair vetores de palavras, e a biblioteca encontrar√° as palavras mais pr√≥ximas do vetor resultante.</font></i>  <i><font color="gray">A imagem mostra uma lista das palavras mais semelhantes, cada uma com um coeficiente de similaridade geom√©trica</font></i> <br><br>  Visualizamos essa analogia como antes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a19/84b/fea/a1984bfeab5a597c6fb6300f7d694901.png"></div><br>  <i><font color="gray">O vetor resultante do c√°lculo "rei - homem + mulher" n√£o √© exatamente igual √† "rainha", mas este √© o resultado mais pr√≥ximo de anexos de 400.000 palavras no conjunto de dados</font></i> <br><br>  Depois de considerar o anexo das palavras, vamos aprender como o aprendizado ocorre.  Mas antes de passar para o word2vec, voc√™ precisa dar uma olhada no ancestral conceitual da incorpora√ß√£o de palavras: um modelo de linguagem neural. <br><br><h1>  Modelo de linguagem </h1><br><blockquote>  <font color="gray">O profeta n√£o est√° sujeito √†s ilus√µes do passado, presente ou futuro.</font>  <font color="gray"><b>A fixidez das formas lingu√≠sticas determina tais diferen√ßas lineares.</b></font>  <font color="gray">Os profetas est√£o segurando a chave da fechadura da l√≠ngua.</font>  <font color="gray">Para eles, a imagem f√≠sica permanece apenas uma imagem f√≠sica e nada mais.</font> <font color="gray"><br><br></font>  <font color="gray">O universo deles n√£o tem as propriedades de um universo mec√¢nico.</font>  <font color="gray">Uma sequ√™ncia linear de eventos √© assumida pelo observador.</font>  <font color="gray">Causa e efeito?</font>  <font color="gray">√â uma quest√£o completamente diferente.</font>  <font color="gray">O Profeta profere palavras fat√≠dicas.</font>  <font color="gray">Voc√™ v√™ de relance um evento que deve acontecer "de acordo com a l√≥gica das coisas".</font>  <font color="gray">Mas o profeta libera instantaneamente a energia do poder milagroso infinito.</font>  <font color="gray">O universo est√° passando por uma mudan√ßa espiritual. ‚Äù</font>  <font color="gray">- <i>Deus, o Imperador de Dune</i></font> </blockquote><br>  Um exemplo de PNL (processamento de linguagem natural) √© a fun√ß√£o de previs√£o da pr√≥xima palavra no teclado de um smartphone.  Bilh√µes de pessoas o usam centenas de vezes por dia. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ca4/d48/a13/ca4d48a133d58fe3c4c11e0933ea218e.png"></div><br><br>  Prever a pr√≥xima palavra √© uma tarefa adequada para <i>um modelo de linguagem</i> .  Ela pode pegar uma lista de palavras (digamos, duas palavras) e tentar prever o seguinte. <br><br>  Na captura de tela acima, o modelo pegou essas duas palavras verdes ( <code>thou shalt</code> ) e retornou uma lista de op√ß√µes (provavelmente para a palavra <code>not</code> ): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5a7/0fc/492/5a70fc49208b501202ed188f24ad1f2c.png"></div><br><br>  Podemos imaginar o modelo como uma caixa preta: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/164/72f/a83/16472fa83e5eadf58f4bb05b50075654.png"></div><br><br>  Mas, na pr√°tica, o modelo produz mais de uma palavra.  Deriva-se uma estimativa da probabilidade de praticamente todas as palavras conhecidas (o "dicion√°rio" do modelo varia de v√°rios milhares a mais de um milh√£o de palavras).  O aplicativo de teclado encontra as palavras com as pontua√ß√µes mais altas e as mostra ao usu√°rio. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7a7/eda/ad6/7a7edaad67dd51240d90426de0b198c2.png"></div><br>  <i><font color="gray">Um modelo de linguagem neural fornece a probabilidade de todas as palavras conhecidas.</font></i>  <i><font color="gray">Indicamos a probabilidade como uma porcentagem, mas no vetor resultante 40% ser√° representado como 0,4</font></i> <br><br>  Ap√≥s o treinamento, os primeiros modelos neurais ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Bengio 2003</a> ) calcularam o progn√≥stico em tr√™s etapas: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/27b/082/4f8/27b0824f81962e2863d6d4dcccabfdd2.png"></div><br><br>  O primeiro passo para n√≥s √© o mais relevante, pois discutimos investimentos.  Como resultado do treinamento, uma matriz √© criada com os anexos de todas as palavras em nosso dicion√°rio.  Para obter o resultado, simplesmente procuramos as combina√ß√µes das palavras de entrada e executamos a previs√£o: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1d1/34b/5ac/1d134b5ac32406ea363944887ce5fc53.png"></div><br><br>  Agora, vamos analisar o processo de aprendizado e descobrir como essa matriz de investimentos √© criada. <br><br><h1>  Treinamento do modelo de linguagem </h1><br><blockquote>  <font color="gray">‚ÄúO processo n√£o pode ser entendido terminando-o.</font>  <font color="gray">O entendimento deve seguir o processo, fundir-se com o seu fluxo e fluir com ele ‚Äù- <i>Dune</i></font> </blockquote><br>  Os modelos de idiomas t√™m uma enorme vantagem sobre a maioria dos outros modelos de aprendizado de m√°quina: eles podem ser treinados em textos que temos em abund√¢ncia.  Pense em todos os livros, artigos, materiais da Wikipedia e outras formas de dados textuais que temos.  Compare com outros modelos de aprendizado de m√°quina que precisam de m√£o de obra e dados especialmente coletados. <br><br><blockquote>  <b>‚ÄúVoc√™ deve aprender a palavra pela empresa dele‚Äù - J. R. Furs</b> </blockquote><br>  Os anexos das palavras s√£o calculados de acordo com as palavras ao redor, que aparecem com mais frequ√™ncia nas proximidades.  A mec√¢nica √© a seguinte: <br><br><ol><li>  Temos muitos dados de texto (digamos, todos os artigos da Wikipedia) <br></li><li>  Defina uma janela (por exemplo, tr√™s palavras) que deslize pelo texto. <br></li><li>  Uma janela deslizante gera padr√µes para o treinamento do nosso modelo. </li></ol><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a31/fc4/626/a31fc4626de165a21c2c91844b21e7ab.png"></div><br><br>  Quando essa janela desliza sobre o texto, n√≥s (na verdade) geramos um conjunto de dados, que depois usamos para treinar o modelo.  Para entender, vamos ver como uma janela deslizante lida com esta frase: <br><br><blockquote>  <b>‚ÄúQue voc√™ n√£o construa uma m√°quina dotada da semelhan√ßa da mente humana‚Äù - <i>Dune</i></b> </blockquote><br>  Quando come√ßamos, a janela est√° localizada nas tr√™s primeiras palavras da frase: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/81c/c51/a04/81cc51a0478e1655c8f3f85641cf1e4e.png"></div><br><br>  Tomamos as duas primeiras palavras para sinais e a terceira palavra para o r√≥tulo: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/097/981/086/0979810868ca398fdcad3066294055f5.png"></div><br>  <i><font color="gray">Geramos a primeira amostra em um conjunto de dados que pode ser usado posteriormente para ensinar um modelo de linguagem</font></i> <br><br>  Em seguida, movemos a janela para a pr√≥xima posi√ß√£o e criamos uma segunda amostra: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/98c/0b3/f98/98c0b3f98ebf4790890fd2f66cf86ce9.png"></div><br><br>  E logo, estamos acumulando um conjunto de dados maior: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4e7/3ce/50d/4e73ce50d863e1cbfde92a3b595dbaa3.png"></div><br><br>  Na pr√°tica, os modelos geralmente s√£o treinados diretamente no processo de mover uma janela deslizante.  Mas logicamente, a fase "gera√ß√£o de conjunto de dados" √© separada da fase de treinamento.  Al√©m das abordagens de redes neurais, o m√©todo N-gram era frequentemente usado anteriormente para o ensino de modelos de linguagem (consulte o terceiro cap√≠tulo do livro <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">‚ÄúProcessamento de fala e linguagem‚Äù</a> ).  Para ver a diferen√ßa ao mudar de N-gramas para modelos neurais em produtos reais, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui est√° uma publica√ß√£o de 2015 no blog Swiftkey</a> , o desenvolvedor do meu teclado Android favorito, que apresenta seu modelo de linguagem neural e o compara com o modelo anterior de N-gram.  Gosto deste exemplo porque mostra como as propriedades algor√≠tmicas dos investimentos podem ser descritas em uma linguagem de marketing. <br><br><h1>  N√≥s olhamos para os dois lados </h1><br><blockquote>  <font color="gray">‚ÄúUm paradoxo √© um sinal de que devemos tentar considerar o que est√° por tr√°s dele.</font>  <font color="gray">Se o paradoxo lhe preocupa, significa que voc√™ est√° lutando pelo absoluto.</font>  <font color="gray">Os relativistas v√™em o paradoxo simplesmente como um pensamento interessante, talvez engra√ßado, √†s vezes assustador, mas muito instrutivo. ‚Äù</font>  <font color="gray"><i>Imperador Deus das Dunas</i></font> </blockquote><br>  Com base no exposto, preencha a lacuna: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/680/613/871/680613871307e53415ab86fab022276a.png"></div><br><br>  Como contexto, existem cinco palavras anteriores (e uma refer√™ncia anterior a "barramento").  Estou certo de que muitos de voc√™s imaginaram que deveria haver um "√¥nibus".  Mas se eu lhe der outra palavra depois do espa√ßo, isso mudar√° sua resposta? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/45f/1fa/af0/45f1faaf0cdd4f57ac1699d87861934a.png"></div><br><br>  Isso muda completamente a situa√ß√£o: agora a palavra que falta √© provavelmente "vermelho".  Obviamente, as palavras t√™m valor informativo antes e depois de um espa√ßo.  Acontece que a contabilidade nas duas dire√ß√µes (esquerda e direita) permite calcular melhores investimentos.  Vamos ver como configurar o treinamento do modelo em tal situa√ß√£o. <br><br><h1>  Ignorar grama </h1><br><blockquote>  <font color="gray">"Quando uma escolha absolutamente inconfund√≠vel √© desconhecida, o intelecto tem a chance de trabalhar com dados limitados na arena, onde os erros n√£o s√£o apenas poss√≠veis, mas tamb√©m necess√°rios".</font>  <font color="gray">- <i>Capitul Dunes</i></font> </blockquote><br>  Al√©m de duas palavras antes do alvo, voc√™ pode levar em conta mais duas palavras depois dele. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e2b/1f6/1a1/e2b1f61a179e7d6835b47c7149a47486.png"></div><br><br>  Em seguida, o conjunto de dados para o treinamento do modelo ter√° a seguinte apar√™ncia: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6ff/729/ed4/6ff729ed4ce86722dc9c3aa689614195.png"></div><br><br>  Isso √© chamado de arquitetura CBOW (Continuous Bag of Words) e √© descrito em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">um dos documentos word2vec</a> [pdf].  H√° outra arquitetura, que tamb√©m mostra excelentes resultados, mas √© organizada de maneira um pouco diferente: tenta adivinhar as palavras vizinhas pela palavra atual.  Uma janela deslizante √© mais ou menos assim: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc9/72d/baa/dc972dbaa78b592ba91b76e950ec56e0.png"></div><br>  <i><font color="gray">No slot verde est√° a palavra de entrada e cada campo rosa representa uma poss√≠vel sa√≠da</font></i> <br><br>  Ret√¢ngulos cor de rosa t√™m tons diferentes, porque esta janela deslizante cria quatro padr√µes separados em nosso conjunto de dados de treinamento: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/709/8ac/dde/7098acddea8266d1efd5663ed98e6303.png"></div><br><br>  Esse m√©todo √© chamado <b>de</b> arquitetura de <b>ignorar grama</b> .  Voc√™ pode visualizar uma janela deslizante da seguinte maneira: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ee2/1d8/508/ee21d850835bde9e3f14250d267d88b1.png"></div><br><br>  As quatro amostras a seguir s√£o adicionadas ao conjunto de dados de treinamento: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a40/871/f1c/a40871f1c1c7b48723d3737c05fc6284.png"></div><br><br>  Em seguida, movemos a janela para a seguinte posi√ß√£o: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/14a/429/c7b/14a429c7b2ae6ba7383d6d39be9e3031.png"></div><br><br>  O que gera mais quatro exemplos: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e9b/b3c/89a/e9bb3c89a00306b3fd18eb86d8f2160b.png"></div><br><br>  Em breve teremos muito mais amostras: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6bb/749/096/6bb749096d3329712a7c00727b4d3cff.png"></div><br><br><h1>  Revis√£o de Aprendizagem </h1><br><blockquote>  <font color="gray">‚ÄúMuad'Dib aprendeu r√°pido porque aprendeu principalmente como aprender.</font>  <font color="gray">Mas a primeira li√ß√£o foi a assimila√ß√£o da cren√ßa de que ele pode aprender, e essa √© a base de tudo.</font>  <font color="gray">√â incr√≠vel quantas pessoas n√£o acreditam que podem aprender e aprender e quantas pessoas pensam que o aprendizado √© muito dif√≠cil. "</font>  <font color="gray">- <i>Duna</i></font> </blockquote><br>  Agora que temos o conjunto de skip-gram, usamos para treinar o modelo neural b√°sico da linguagem que prediz uma palavra vizinha. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/944/fb7/70d/944fb770d3aff38f1befa40dfaa7402a.png"></div><br><br>  Vamos come√ßar com a primeira amostra do nosso conjunto de dados.  Pegamos a placa e a enviamos ao modelo n√£o treinado com a solicita√ß√£o para prever a pr√≥xima palavra. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/abb/5cb/9a3/abb5cb9a38d29f1a54176206637131dc.png"></div><br><br>  O modelo passa por tr√™s etapas e exibe um vetor de previs√£o (com probabilidade para cada palavra no dicion√°rio).  Como o modelo n√£o √© treinado, nesta fase, sua previs√£o provavelmente est√° incorreta.  Mas isso n√£o √© nada.  Sabemos qual palavra ela prediz - esta √© a c√©lula resultante na linha que atualmente usamos para treinar o modelo: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8af/4fd/c3d/8af4fdc3d3cc86ec1c81fdb3d2715529.png"></div><br>  <i><font color="gray">Um "vetor de destino" √© aquele em que a palavra de destino tem uma probabilidade de 1 e todas as outras palavras t√™m uma probabilidade de 0</font></i> <br><br>  Qu√£o errado estava o modelo?  Subtraia o vetor de previs√£o do destino e obtenha o vetor de erro: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e6d/b3e/395/e6db3e39593e9c8639d94ef4caccde58.png"></div><br><br>  Agora, esse vetor de erro pode ser usado para atualizar o modelo; portanto, da pr√≥xima vez, √© mais prov√°vel que voc√™ d√™ um resultado preciso nos mesmos dados de entrada. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7d3/6c0/476/7d36c047604b937c907a4ef38ceaaeb7.png"></div><br><br>  Aqui a primeira etapa do treinamento termina.  Continuamos a fazer o mesmo com a pr√≥xima amostra no conjunto de dados e depois com a pr√≥xima, at√© examinarmos todas as amostras.  Este √© o fim da primeira era do aprendizado.  Repetimos tudo repetidamente por v√°rias √©pocas e, como resultado, obtemos um modelo treinado: dele voc√™ pode extrair a matriz de investimento e us√°-la em qualquer aplica√ß√£o. <br><br>  Embora tenhamos aprendido muito, mas para entender completamente como o word2vec realmente aprende, faltam algumas id√©ias importantes. <br><br><h1>  Sele√ß√£o negativa </h1><br><blockquote>  <font color="gray">‚ÄúTentar entender Muad'Dib sem entender seus inimigos mortais - os Harkonnenov - √© o mesmo que tentar entender a Verdade sem entender o que √© a Falsidade.</font>  <font color="gray">Esta √© uma tentativa de conhecer a Luz sem conhecer a Escurid√£o.</font>  <font color="gray">Isso √© imposs√≠vel.</font>  <font color="gray">- <i>Duna</i></font> </blockquote><br>  Lembre-se das tr√™s etapas de como um modelo neural calcula uma previs√£o: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8dd/fe1/4ca/8ddfe14ca387bd4d16c77eb9de8ce98f.png"></div><br><br>  A terceira etapa √© muito cara do ponto de vista computacional, especialmente se voc√™ fizer isso para cada amostra no conjunto de dados (dezenas de milh√µes de vezes).  √â necess√°rio, de alguma forma, aumentar a produtividade. <br><br>  Uma maneira √© dividir o objetivo em duas etapas: <br><br><ol><li>  Crie anexos de palavras de alta qualidade (sem prever a pr√≥xima palavra). <br></li><li>  Use esses investimentos de alta qualidade para ensinar o modelo de idioma (para previs√£o). </li></ol><br>  Este artigo se concentrar√° na primeira etapa.  Para aumentar a produtividade, voc√™ pode deixar de prever uma palavra vizinha ... <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/77d/0a8/c17/77d0a8c17587248a0f790155809798fe.png"></div><br><br>  ... e mude para um modelo que use palavras de entrada e sa√≠da e calcule a probabilidade de proximidade (de 0 a 1). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/220/0e3/e06/2200e3e063f3119099d1615e59538d2a.png"></div><br><br>  Uma transi√ß√£o t√£o simples substitui a rede neural por um modelo de regress√£o log√≠stica - assim, os c√°lculos se tornam muito mais simples e r√°pidos. <br><br>  Isso requer refinamento da estrutura do nosso conjunto de dados: o r√≥tulo agora √© uma nova coluna com valores de 0 ou 1. Na nossa tabela, existem unidades em todos os lugares, porque adicionamos vizinhos l√°. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc2/d1e/874/dc2d1e87438b2492dc9b6e4b1c72162e.png"></div><br><br>  Esse modelo √© calculado a uma velocidade incr√≠vel: milh√µes de amostras em minutos.  Mas voc√™ precisa fechar uma brecha.  Se todos os nossos exemplos forem positivos (objetivo: 1), pode-se formar um modelo complicado que sempre retorna 1, demonstrando 100% de precis√£o, mas n√£o aprende nada e gera investimentos indesejados. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1ba/aa2/2d0/1baaa22d0b0c06be5398f896fa7a4c4b.png"></div><br><br>  Para resolver esse problema, √© necess√°rio inserir <i>padr√µes negativos</i> no conjunto de dados - palavras que definitivamente n√£o s√£o vizinhas.  Para eles, o modelo deve retornar 0. Agora, o modelo ter√° que trabalhar duro, mas os c√°lculos ainda ser√£o feitos em grande velocidade. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f4c/194/0d8/f4c1940d80c5203620196907a1478431.png"></div><br>  <i><font color="gray">Para cada amostra no conjunto de dados, adicione exemplos negativos com o r√≥tulo 0</font></i> <br><br>  Mas o que introduzir como as palavras de sa√≠da?  Escolha as palavras arbitrariamente: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/84e/b22/06f/84eb2206f26b053f1ea8ec4e1b76c5b6.png"></div><br><br>  Esta ideia nasceu sob a influ√™ncia do m√©todo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">compara√ß√£o</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ru√≠do</a> [pdf].  Combinamos o sinal real (exemplos positivos de palavras vizinhas) com o ru√≠do (palavras selecionadas aleatoriamente que n√£o s√£o vizinhas).  Isso fornece um excelente compromisso entre desempenho e desempenho estat√≠stico. <br><br><h1>  Amostra Negativa de Ignorar Grama (SGNS) </h1><br>  Analisamos dois conceitos centrais do word2vec: juntos, eles s√£o chamados de "ignorar com amostragem negativa". <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/873/720/fae/873720fae559ce7d6020be66ccb6c397.png"></div><br><br><h1>  Aprendendo word2vec </h1><br><blockquote>  <font color="gray">‚ÄúUma m√°quina n√£o pode prever todos os problemas importantes para uma pessoa viva.</font>  <font color="gray">H√° uma grande diferen√ßa entre espa√ßo discreto e cont√≠nuo cont√≠nuo.</font>  <font color="gray">Vivemos em um espa√ßo e as m√°quinas existem em outro. ‚Äù</font>  <font color="gray">- <i>Deus, o Imperador de Dune</i></font> </blockquote><br>  Tendo examinado as id√©ias b√°sicas do skip-gram e da amostragem negativa, podemos prosseguir com uma an√°lise mais detalhada do processo de aprendizado do word2vec. <br><br>  Primeiro, pr√©-processamos o texto no qual treinamos o modelo.  Defina o tamanho do dicion√°rio (n√≥s o chamaremos <code>vocab_size</code> ), digamos, em 10.000 anexos e os par√¢metros das palavras no dicion√°rio. <br><br>  No in√≠cio do treinamento, criamos duas matrizes: <code>Embedding</code> e <code>Context</code> .  Os anexos para cada palavra s√£o armazenados nessas matrizes em nosso dicion√°rio (portanto, <code>vocab_size</code> √© um dos par√¢metros).  O segundo par√¢metro √© a dimens√£o do anexo (geralmente <code>embedding_size</code> definido como 300, mas anteriormente vimos um exemplo com 50 dimens√µes). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1b7/8f8/018/1b78f8018d20fd36d0c5aef37d87a249.png"></div><br><br>  Primeiro, inicializamos essas matrizes com valores aleat√≥rios.  Ent√£o come√ßamos o processo de aprendizado.  Em cada est√°gio, tomamos um exemplo positivo e o negativo associado a ele.  Aqui est√° o nosso primeiro grupo: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2d2/261/806/2d22618069aa9e3f8a820cb431c6c014.png"></div><br><br>  Agora temos quatro palavras: a palavra de entrada <code>not</code> e as palavras de sa√≠da / contextuais <code>thou</code> (vizinho real), <code>aaron</code> e <code>taco</code> (exemplos negativos).  Iniciamos a busca de seus anexos nas matrizes <code>Embedding</code> (para a palavra de entrada) e <code>Context</code> (para as palavras de contexto), embora ambas as matrizes contenham anexos para todas as palavras do nosso dicion√°rio. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/23e/ff0/691/23eff069128db956ce358ae758c0b8bb.png"></div><br><br>  Em seguida, calculamos o produto escalar do anexo de entrada com cada um dos anexos contextuais.  Em cada caso, √© obtido um n√∫mero que indica a similaridade dos dados de entrada e dos anexos contextuais. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/615/319/e4a/615319e4accc7235c28fc8c76dca09f6.png"></div><br><br>  Agora precisamos de uma maneira de transformar essas estimativas em um tipo de probabilidade: todas elas devem ser n√∫meros positivos entre 0 e 1. Essa √© uma excelente tarefa para equa√ß√µes log√≠sticas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">sigm√≥ides</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/98b/125/8b2/98b1258b21744917c993e617e0844ad8.png"></div><br><br>  O resultado do c√°lculo sigm√≥ide pode ser considerado como a sa√≠da do modelo para essas amostras.  Como voc√™ pode ver, o <code>taco</code> a maior pontua√ß√£o, enquanto o <code>aaron</code> ainda tem a menor pontua√ß√£o, antes e depois do sigm√≥ide. <br><br>  Quando o modelo n√£o treinado fez uma previs√£o e tem uma marca de destino real para compara√ß√£o, vamos calcular quantos erros existem na previs√£o do modelo.  Para fazer isso, basta subtrair a pontua√ß√£o sigm√≥ide dos r√≥tulos alvo. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d49/458/333/d49458333c28225c596014df3c6fcedb.png"></div><br>  <i><font color="gray"><code>error</code> = <code>target</code> - <code>sigmoid_scores</code></font></i> <br><br>  √â aqui que a fase de "aprendizado" do termo "aprendizado de m√°quina" come√ßa.  Agora, podemos usar essa estimativa de erro para ajustar os investimentos <code>not</code> , <code>thou</code> , <code>aaron</code> e <code>taco</code> , para que na pr√≥xima vez que o resultado esteja mais pr√≥ximo das estimativas de destino. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a5d/e51/cd3/a5de51cd3a86a1ed0784a709cb979bdc.png"></div><br><br>  Isso completa uma etapa do treinamento.  Melhoramos um pouco o apego de algumas palavras ( <code>not</code> <code>thou</code> , <code>aaron</code> e <code>taco</code> ).  Agora passamos para o pr√≥ximo est√°gio (a pr√≥xima amostra positiva e a negativa associada a ela) e repetimos o processo. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/637/2ab/c3a/6372abc3a3f6b623d5b2cbab02953030.png"></div><br><br>  Os anexos continuam a melhorar √† medida que percorremos o conjunto de dados inteiro v√°rias vezes.  Em seguida, voc√™ pode interromper o processo, deixar a matriz de <code>Context</code> lado e usar a matriz <code>Embeddings</code> treinada para a pr√≥xima tarefa. <br><br><h1>  Tamanho da janela e n√∫mero de amostras negativas </h1><br>  No processo de aprendizado do word2vec, dois hiperpar√¢metros-chave s√£o o tamanho da janela e o n√∫mero de amostras negativas. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9fe/437/447/9fe4374479f547c1b324c7471cd61cbd.png"></div><br><br>  Diferentes tamanhos de janelas s√£o adequados para diferentes tarefas.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Observou-se</a> que tamanhos de janela menores (2‚Äì15) geram anexos <i>intercambi√°veis</i> com √≠ndices semelhantes (observe que os ant√¥nimos geralmente s√£o intercambi√°veis ‚Äã‚Äãquando se olha para as palavras ao redor: por exemplo, as palavras ‚Äúbom‚Äù e ‚Äúruim‚Äù s√£o frequentemente mencionadas em contextos semelhantes).  Tamanhos de janela maiores (15 a 50 ou mais) geram anexos <i>relacionados</i> com √≠ndices semelhantes.  Na pr√°tica, voc√™ geralmente precisa fornecer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">anota√ß√µes</a> para obter semelhan√ßas sem√¢nticas √∫teis em sua tarefa.  No Gensim, o tamanho padr√£o da janela √© 5 (duas palavras esquerda e direita, al√©m da pr√≥pria palavra de entrada). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4f4/b0a/45a/4f4b0a45a8552d6c19c7c9459302ac48.png"></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O n√∫mero de amostras negativas √© outro fator no processo de aprendizagem. </font><font style="vertical-align: inherit;">O documento original recomenda 5‚Äì20. </font><font style="vertical-align: inherit;">Ele tamb√©m diz que 2-5 amostras parecem suficientes quando voc√™ tem um conjunto de dados suficientemente grande. </font><font style="vertical-align: inherit;">No Gensim, o valor padr√£o √© 5 padr√µes negativos.</font></font><br><br><h1>  Conclus√£o </h1><br><blockquote> <font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúSe seu comportamento est√° al√©m dos seus padr√µes, voc√™ √© uma pessoa viva, n√£o um aut√¥mato‚Äù - </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Deus-Imperador de Dune</font></font></i></font> </blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Espero que agora voc√™ entenda a incorpora√ß√£o de palavras e a ess√™ncia do algoritmo word2vec. </font><font style="vertical-align: inherit;">Espero tamb√©m que agora voc√™ se familiarize mais com os artigos que mencionam o conceito de "pular com amostragem negativa" (SGNS), como nos sistemas de recomenda√ß√£o acima mencionados.</font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Refer√™ncias e leituras adicionais </font></font></h1><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúRepresenta√ß√µes distribu√≠das de palavras e frases e sua composi√ß√£o‚Äù</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> [pdf]</font></font></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">¬´      ¬ª</a> [pdf] </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">¬´   ¬ª</a> [pdf] </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">¬´   ¬ª</a>      ‚Äî    NLP. Word2vec    . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">¬´      ¬ª</a> by <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a> ‚Äî      . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a>        Word2vec.        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">¬´ word2vec¬ª</a> </li><li>   ?   : <ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> word2vec  Python</a>  Gensim </li><li>  <a href="">   C</a> ,    <a href="">       </a> </li></ul></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">    </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">¬´  ¬ª</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> 2</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">¬´¬ª</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt446530/">https://habr.com/ru/post/pt446530/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt446514/index.html">O Gmail tem 15 anos</a></li>
<li><a href="../pt446516/index.html">Visualiza√ß√£o do tempo de renascimento de Roshan</a></li>
<li><a href="../pt446518/index.html">Firewalls de aplicativos da Web</a></li>
<li><a href="../pt446520/index.html">Como tudo come√ßou: a hist√≥ria dos drones voadores</a></li>
<li><a href="../pt446522/index.html">Swift 5.1 - o que h√° de novo?</a></li>
<li><a href="../pt446532/index.html">Upwork introduz uma taxa pelo direito de escrever para um cliente em potencial</a></li>
<li><a href="../pt446534/index.html">Lan√ßamento do Visual Studio 2019</a></li>
<li><a href="../pt446536/index.html">Filas e JMeter: Exchange com Publicador e Assinante</a></li>
<li><a href="../pt446538/index.html">O PhotoGuru mudou para o "lado sombrio" e o "mais s√°bio"</a></li>
<li><a href="../pt446544/index.html">Microsoft expande o Azure IP Advantage Program com novos benef√≠cios de IP para inovadores e startups de IoT do Azure</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>