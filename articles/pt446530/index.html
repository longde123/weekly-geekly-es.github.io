<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👳🏿 👩🏾‍🏭 👩🏻‍🎤 Word2vec em imagens 👈🏽 👊 🙈</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="“ Tudo esconde um padrão que faz parte do universo. Possui simetria, elegância e beleza - qualidades que antes de tudo são apreendidas por todo verdad...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Word2vec em imagens</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/446530/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/85d/ad8/627/85dad8627ae6845b62f5bb965c291b19.png"></div><br><br><blockquote>  <font color="gray">“ <b>Tudo esconde um padrão que faz parte do universo.</b></font>  <font color="gray"><b>Possui simetria, elegância e beleza</b> - qualidades que antes de tudo são apreendidas por todo verdadeiro artista que captura o mundo.</font>  <font color="gray">Esse padrão pode ser detectado na mudança das estações, na maneira como a areia flui ao longo da encosta, nos galhos emaranhados do arbusto do creosoto, no padrão de suas folhas.</font> <font color="gray"><br><br></font>  <font color="gray">Estamos tentando copiar esse padrão em nossa vida e em nossa sociedade e, portanto, amamos ritmo, música, dança, várias formas que nos fazem felizes e nos confortam.</font>  <font color="gray">No entanto, também se pode discernir o perigo à espreita na busca da perfeição absoluta, pois é óbvio que o padrão perfeito é inalterado.</font>  <font color="gray">E, aproximando-se da perfeição, todas as coisas vão à morte ”- <i>Dune</i> (1965)</font> </blockquote><br>  Acredito que o conceito de incorporação é uma das idéias mais marcantes do aprendizado de máquina.  Se você já usou o Siri, o Google Assistant, o Alexa, o Google Translate ou mesmo um teclado de smartphone com a previsão da próxima palavra, já trabalhou com o modelo de processamento de idioma natural baseado em anexo.  Nas últimas décadas, esse conceito evoluiu significativamente para modelos neurais (desenvolvimentos recentes incluem incorporação de palavras contextualizadas em modelos avançados, como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">BERT</a> e GPT2). <br><a name="habracut"></a><br>  O Word2vec é um método eficaz de criação de investimentos desenvolvido em 2013.  Além de trabalhar com palavras, alguns de seus conceitos foram eficazes no desenvolvimento de mecanismos de recomendação e na atribuição de significado aos dados, mesmo em tarefas comerciais e não linguísticas.  Essa tecnologia foi usada por empresas como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Airbnb</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Alibaba</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Spotify</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Anghami</a> em seus mecanismos de recomendação. <br><br>  Neste artigo, veremos o conceito e a mecânica de geração de anexos usando o word2vec.  Vamos começar com um exemplo para se familiarizar com como representar objetos em forma de vetor.  Você sabe o quanto uma lista de cinco números (vetor) pode dizer sobre sua personalidade? <br><br><h1>  Personalização: o que você é? </h1><br><blockquote>  <font color="gray">“Eu te dou o camaleão do deserto;</font>  <font color="gray">sua capacidade de se fundir com a areia lhe dirá tudo o que você precisa saber sobre as raízes da ecologia e as razões para preservar sua personalidade. ”</font>  <font color="gray">- <i>Filhos das Dunas</i></font> </blockquote><br>  Em uma escala de 0 a 100, você tem um tipo de personalidade introvertida ou extrovertida (onde 0 é o tipo mais introvertido e 100 é o tipo mais extrovertido)?  Você já passou em um teste de personalidade: por exemplo, MBTI, ou melhor ainda <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">, os Cinco Grandes</a> ?  Você recebe uma lista de perguntas e é avaliado em vários eixos, incluindo introversão / extroversão. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/79f/11e/e22/79f11ee220ebf9d6f52f51a5b780b090.png"></div><br>  <i><font color="gray">Exemplo dos cinco grandes resultados do teste.</font></i>  <i><font color="gray">Ele realmente fala muito sobre personalidade e é capaz de prever <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">o</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">sucesso</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">acadêmico</a> , <a href="">pessoal</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">profissional</a> .</font></i>  <i><font color="gray">Por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> você pode passar por isso.</font></i> <br><br>  Suponha que eu pontuei 38 em 100 para avaliar a introversão / extroversão.  Isso pode ser representado da seguinte maneira: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e56/729/35d/e5672935d7de17d41e78354d3742e6bc.png"></div><br><br>  Ou em uma escala de -1 a +1: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b39/e23/ce1/b39e23ce1c036b11763e3c45c3659a3e.png"></div><br><br>  Quão bem reconhecemos uma pessoa apenas dessa avaliação?  Na verdade não.  Os seres humanos são criaturas complexas.  Portanto, adicionamos mais uma dimensão: mais uma característica do teste. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2aa/ab2/ebc/2aaab2ebc1ff30f1fd832e5cf5bf9cb1.png"></div><br>  <i><font color="gray">Você pode imaginar essas duas dimensões como um ponto no gráfico ou, melhor ainda, como um vetor da origem até esse ponto.</font></i>  <i><font color="gray">Existem ótimas ferramentas de vetores que serão úteis muito em breve.</font></i> <br><br>  Não mostro quais traços de personalidade colocamos no gráfico para que você não se apegue a traços específicos, mas entendo imediatamente a representação vetorial da personalidade da pessoa como um todo. <br><br>  Agora podemos dizer que esse vetor reflete parcialmente minha personalidade.  Esta é uma descrição útil ao comparar pessoas diferentes.  Suponha que fui atropelado por um ônibus vermelho e você precise me substituir por uma pessoa semelhante.  Qual das duas pessoas no gráfico a seguir se parece mais comigo? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/de5/380/b84/de5380b84dc9fec4bb8b52ebe6519e15.png"></div><br><br>  Ao trabalhar com vetores, a similaridade é geralmente calculada pelo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">coeficiente Otiai</a> (coeficiente geométrico): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/640/e59/7dd/640e597dd741a28bcec986454633e31d.png"></div><br>  <i><font color="green">A pessoa número 1 é</font> <font color="gray">mais parecida comigo em caráter.</font></i>  <i><font color="gray">Os vetores em uma direção (o comprimento também é importante) fornecem um coeficiente Otiai maior</font></i> <br><br>  Novamente, duas dimensões não são suficientes para avaliar as pessoas.  Décadas de desenvolvimento da ciência psicológica levaram à criação de um teste para cinco características básicas da personalidade (com muitas outras adicionais).  Então, vamos usar todas as cinco dimensões: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/df5/3ae/d7b/df53aed7b1e439561a01e69b3f765487.png"></div><br><br>  O problema com as cinco dimensões é que não será mais possível desenhar setas limpas em 2D.  Esse é um problema comum no aprendizado de máquina, onde você geralmente precisa trabalhar em um espaço multidimensional.  É bom que o coeficiente geométrico funcione com qualquer número de medições: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/521/ab3/bf1/521ab3bf1374c5b37115441b7c2d27cc.png"></div><br>  <i><font color="gray">O coeficiente geométrico funciona para qualquer número de medições.</font></i>  <i><font color="gray">Em cinco dimensões, o resultado é muito mais preciso.</font></i> <br><br>  No final deste capítulo, quero repetir duas idéias principais: <br><br><ol><li>  Pessoas (e outros objetos) podem ser representados como vetores numéricos (o que é ótimo para carros!). <br></li><li>  Podemos calcular facilmente como os vetores são semelhantes. </li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/195/73d/16d/19573d16de1150ac1874640c79e0b381.png"></div><br><br><h1>  Incorporação de palavras </h1><br><blockquote>  <font color="gray">"O dom das palavras é o dom da decepção e da ilusão."</font>  <font color="gray">- <i>Filhos das Dunas</i></font> </blockquote><br>  Com esse entendimento, passaremos para as representações vetoriais de palavras obtidas como resultado do treinamento (elas também são chamadas de anexos) e examinaremos suas propriedades interessantes. <br><br>  Aqui está o anexo da palavra "rei" (vetor GloVe, treinado na Wikipedia): <br><br> <code>[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]</code> <br> <br>  Vemos uma lista de 50 números, mas é difícil dizer algo.  Vamos visualizá-los para comparar com outros vetores.  Coloque os números em uma linha: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/516/c90/5ac/516c905ac831fe8688db73f0a63d325b.png"></div><br><br>  Colorir as células de acordo com seus valores (vermelho para próximo a 2, branco para próximo a 0, azul para próximo de -2): <br><br><div style="text-align:center;"> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/46f/7cb/1d5/46f7cb1d5adc32bd16368b2681ab26a4.png"></a> </div><br><br>  Agora esqueça os números, e somente pelas cores contrastamos o "rei" com outras palavras: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1c8/6b2/909/1c86b290963e8a42b375cb6a71245185.png"></div><br><br>  Você vê que "homem" e "mulher" são muito mais próximos um do outro do que o "rei"?  Diz alguma coisa.  As representações vetoriais capturam muitas informações / significados / associações dessas palavras. <br><br>  Aqui está outra lista de exemplos (compare colunas com cores semelhantes): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d61/30b/d45/d6130bd4502710631a8c812923230f47.png"></div><br><br>  Há várias coisas a serem observadas: <br><br><ol><li>  Por todas as palavras passa uma coluna vermelha.  Ou seja, essas palavras são semelhantes nessa dimensão específica (e não sabemos o que está codificado nela). <br></li><li>  Você pode ver que “mulher” e “garota” são muito parecidas.  A mesma coisa com "homem" e "menino". <br></li><li>  "Menino" e "menina" também são semelhantes em algumas dimensões, mas diferem de "mulher" e "homem".  Poderia ser uma vaga idéia codificada de juventude?  Provavelmente. <br></li><li>  Tudo, exceto a última palavra, são idéias das pessoas.  Eu adicionei um objeto (água) para mostrar as diferenças entre as categorias.  Por exemplo, você pode ver como a coluna azul desce e para na frente do vetor da água. <br></li><li>  Existem dimensões claras em que o "rei" e a "rainha" são semelhantes entre si e diferentes de todos os outros.  Talvez um conceito vago de realeza esteja codificado lá? </li></ol><br><h1>  Analogies </h1><br><blockquote>  <font color="gray">“As palavras suportam qualquer carga que desejamos.</font>  <font color="gray">Tudo o que é necessário é um acordo sobre tradição, segundo o qual construímos conceitos. ”</font>  <font color="gray">- <i>Deus, o Imperador de Dune</i></font> </blockquote><br>  Exemplos famosos que mostram as incríveis propriedades dos investimentos são o conceito de analogias.  Podemos adicionar e subtrair vetores de palavras, obtendo resultados interessantes.  O exemplo mais famoso é a fórmula "rei - homem + mulher": <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c23/71f/ead/c2371feadc58f2f2a1236c94b6b05eff.png"></div><br>  <i><font color="gray">Usando a biblioteca <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Gensim</a> em python, podemos adicionar e subtrair vetores de palavras, e a biblioteca encontrará as palavras mais próximas do vetor resultante.</font></i>  <i><font color="gray">A imagem mostra uma lista das palavras mais semelhantes, cada uma com um coeficiente de similaridade geométrica</font></i> <br><br>  Visualizamos essa analogia como antes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a19/84b/fea/a1984bfeab5a597c6fb6300f7d694901.png"></div><br>  <i><font color="gray">O vetor resultante do cálculo "rei - homem + mulher" não é exatamente igual à "rainha", mas este é o resultado mais próximo de anexos de 400.000 palavras no conjunto de dados</font></i> <br><br>  Depois de considerar o anexo das palavras, vamos aprender como o aprendizado ocorre.  Mas antes de passar para o word2vec, você precisa dar uma olhada no ancestral conceitual da incorporação de palavras: um modelo de linguagem neural. <br><br><h1>  Modelo de linguagem </h1><br><blockquote>  <font color="gray">O profeta não está sujeito às ilusões do passado, presente ou futuro.</font>  <font color="gray"><b>A fixidez das formas linguísticas determina tais diferenças lineares.</b></font>  <font color="gray">Os profetas estão segurando a chave da fechadura da língua.</font>  <font color="gray">Para eles, a imagem física permanece apenas uma imagem física e nada mais.</font> <font color="gray"><br><br></font>  <font color="gray">O universo deles não tem as propriedades de um universo mecânico.</font>  <font color="gray">Uma sequência linear de eventos é assumida pelo observador.</font>  <font color="gray">Causa e efeito?</font>  <font color="gray">É uma questão completamente diferente.</font>  <font color="gray">O Profeta profere palavras fatídicas.</font>  <font color="gray">Você vê de relance um evento que deve acontecer "de acordo com a lógica das coisas".</font>  <font color="gray">Mas o profeta libera instantaneamente a energia do poder milagroso infinito.</font>  <font color="gray">O universo está passando por uma mudança espiritual. ”</font>  <font color="gray">- <i>Deus, o Imperador de Dune</i></font> </blockquote><br>  Um exemplo de PNL (processamento de linguagem natural) é a função de previsão da próxima palavra no teclado de um smartphone.  Bilhões de pessoas o usam centenas de vezes por dia. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ca4/d48/a13/ca4d48a133d58fe3c4c11e0933ea218e.png"></div><br><br>  Prever a próxima palavra é uma tarefa adequada para <i>um modelo de linguagem</i> .  Ela pode pegar uma lista de palavras (digamos, duas palavras) e tentar prever o seguinte. <br><br>  Na captura de tela acima, o modelo pegou essas duas palavras verdes ( <code>thou shalt</code> ) e retornou uma lista de opções (provavelmente para a palavra <code>not</code> ): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5a7/0fc/492/5a70fc49208b501202ed188f24ad1f2c.png"></div><br><br>  Podemos imaginar o modelo como uma caixa preta: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/164/72f/a83/16472fa83e5eadf58f4bb05b50075654.png"></div><br><br>  Mas, na prática, o modelo produz mais de uma palavra.  Deriva-se uma estimativa da probabilidade de praticamente todas as palavras conhecidas (o "dicionário" do modelo varia de vários milhares a mais de um milhão de palavras).  O aplicativo de teclado encontra as palavras com as pontuações mais altas e as mostra ao usuário. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7a7/eda/ad6/7a7edaad67dd51240d90426de0b198c2.png"></div><br>  <i><font color="gray">Um modelo de linguagem neural fornece a probabilidade de todas as palavras conhecidas.</font></i>  <i><font color="gray">Indicamos a probabilidade como uma porcentagem, mas no vetor resultante 40% será representado como 0,4</font></i> <br><br>  Após o treinamento, os primeiros modelos neurais ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Bengio 2003</a> ) calcularam o prognóstico em três etapas: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/27b/082/4f8/27b0824f81962e2863d6d4dcccabfdd2.png"></div><br><br>  O primeiro passo para nós é o mais relevante, pois discutimos investimentos.  Como resultado do treinamento, uma matriz é criada com os anexos de todas as palavras em nosso dicionário.  Para obter o resultado, simplesmente procuramos as combinações das palavras de entrada e executamos a previsão: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1d1/34b/5ac/1d134b5ac32406ea363944887ce5fc53.png"></div><br><br>  Agora, vamos analisar o processo de aprendizado e descobrir como essa matriz de investimentos é criada. <br><br><h1>  Treinamento do modelo de linguagem </h1><br><blockquote>  <font color="gray">“O processo não pode ser entendido terminando-o.</font>  <font color="gray">O entendimento deve seguir o processo, fundir-se com o seu fluxo e fluir com ele ”- <i>Dune</i></font> </blockquote><br>  Os modelos de idiomas têm uma enorme vantagem sobre a maioria dos outros modelos de aprendizado de máquina: eles podem ser treinados em textos que temos em abundância.  Pense em todos os livros, artigos, materiais da Wikipedia e outras formas de dados textuais que temos.  Compare com outros modelos de aprendizado de máquina que precisam de mão de obra e dados especialmente coletados. <br><br><blockquote>  <b>“Você deve aprender a palavra pela empresa dele” - J. R. Furs</b> </blockquote><br>  Os anexos das palavras são calculados de acordo com as palavras ao redor, que aparecem com mais frequência nas proximidades.  A mecânica é a seguinte: <br><br><ol><li>  Temos muitos dados de texto (digamos, todos os artigos da Wikipedia) <br></li><li>  Defina uma janela (por exemplo, três palavras) que deslize pelo texto. <br></li><li>  Uma janela deslizante gera padrões para o treinamento do nosso modelo. </li></ol><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a31/fc4/626/a31fc4626de165a21c2c91844b21e7ab.png"></div><br><br>  Quando essa janela desliza sobre o texto, nós (na verdade) geramos um conjunto de dados, que depois usamos para treinar o modelo.  Para entender, vamos ver como uma janela deslizante lida com esta frase: <br><br><blockquote>  <b>“Que você não construa uma máquina dotada da semelhança da mente humana” - <i>Dune</i></b> </blockquote><br>  Quando começamos, a janela está localizada nas três primeiras palavras da frase: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/81c/c51/a04/81cc51a0478e1655c8f3f85641cf1e4e.png"></div><br><br>  Tomamos as duas primeiras palavras para sinais e a terceira palavra para o rótulo: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/097/981/086/0979810868ca398fdcad3066294055f5.png"></div><br>  <i><font color="gray">Geramos a primeira amostra em um conjunto de dados que pode ser usado posteriormente para ensinar um modelo de linguagem</font></i> <br><br>  Em seguida, movemos a janela para a próxima posição e criamos uma segunda amostra: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/98c/0b3/f98/98c0b3f98ebf4790890fd2f66cf86ce9.png"></div><br><br>  E logo, estamos acumulando um conjunto de dados maior: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4e7/3ce/50d/4e73ce50d863e1cbfde92a3b595dbaa3.png"></div><br><br>  Na prática, os modelos geralmente são treinados diretamente no processo de mover uma janela deslizante.  Mas logicamente, a fase "geração de conjunto de dados" é separada da fase de treinamento.  Além das abordagens de redes neurais, o método N-gram era frequentemente usado anteriormente para o ensino de modelos de linguagem (consulte o terceiro capítulo do livro <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">“Processamento de fala e linguagem”</a> ).  Para ver a diferença ao mudar de N-gramas para modelos neurais em produtos reais, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui está uma publicação de 2015 no blog Swiftkey</a> , o desenvolvedor do meu teclado Android favorito, que apresenta seu modelo de linguagem neural e o compara com o modelo anterior de N-gram.  Gosto deste exemplo porque mostra como as propriedades algorítmicas dos investimentos podem ser descritas em uma linguagem de marketing. <br><br><h1>  Nós olhamos para os dois lados </h1><br><blockquote>  <font color="gray">“Um paradoxo é um sinal de que devemos tentar considerar o que está por trás dele.</font>  <font color="gray">Se o paradoxo lhe preocupa, significa que você está lutando pelo absoluto.</font>  <font color="gray">Os relativistas vêem o paradoxo simplesmente como um pensamento interessante, talvez engraçado, às vezes assustador, mas muito instrutivo. ”</font>  <font color="gray"><i>Imperador Deus das Dunas</i></font> </blockquote><br>  Com base no exposto, preencha a lacuna: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/680/613/871/680613871307e53415ab86fab022276a.png"></div><br><br>  Como contexto, existem cinco palavras anteriores (e uma referência anterior a "barramento").  Estou certo de que muitos de vocês imaginaram que deveria haver um "ônibus".  Mas se eu lhe der outra palavra depois do espaço, isso mudará sua resposta? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/45f/1fa/af0/45f1faaf0cdd4f57ac1699d87861934a.png"></div><br><br>  Isso muda completamente a situação: agora a palavra que falta é provavelmente "vermelho".  Obviamente, as palavras têm valor informativo antes e depois de um espaço.  Acontece que a contabilidade nas duas direções (esquerda e direita) permite calcular melhores investimentos.  Vamos ver como configurar o treinamento do modelo em tal situação. <br><br><h1>  Ignorar grama </h1><br><blockquote>  <font color="gray">"Quando uma escolha absolutamente inconfundível é desconhecida, o intelecto tem a chance de trabalhar com dados limitados na arena, onde os erros não são apenas possíveis, mas também necessários".</font>  <font color="gray">- <i>Capitul Dunes</i></font> </blockquote><br>  Além de duas palavras antes do alvo, você pode levar em conta mais duas palavras depois dele. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e2b/1f6/1a1/e2b1f61a179e7d6835b47c7149a47486.png"></div><br><br>  Em seguida, o conjunto de dados para o treinamento do modelo terá a seguinte aparência: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6ff/729/ed4/6ff729ed4ce86722dc9c3aa689614195.png"></div><br><br>  Isso é chamado de arquitetura CBOW (Continuous Bag of Words) e é descrito em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">um dos documentos word2vec</a> [pdf].  Há outra arquitetura, que também mostra excelentes resultados, mas é organizada de maneira um pouco diferente: tenta adivinhar as palavras vizinhas pela palavra atual.  Uma janela deslizante é mais ou menos assim: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc9/72d/baa/dc972dbaa78b592ba91b76e950ec56e0.png"></div><br>  <i><font color="gray">No slot verde está a palavra de entrada e cada campo rosa representa uma possível saída</font></i> <br><br>  Retângulos cor de rosa têm tons diferentes, porque esta janela deslizante cria quatro padrões separados em nosso conjunto de dados de treinamento: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/709/8ac/dde/7098acddea8266d1efd5663ed98e6303.png"></div><br><br>  Esse método é chamado <b>de</b> arquitetura de <b>ignorar grama</b> .  Você pode visualizar uma janela deslizante da seguinte maneira: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ee2/1d8/508/ee21d850835bde9e3f14250d267d88b1.png"></div><br><br>  As quatro amostras a seguir são adicionadas ao conjunto de dados de treinamento: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a40/871/f1c/a40871f1c1c7b48723d3737c05fc6284.png"></div><br><br>  Em seguida, movemos a janela para a seguinte posição: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/14a/429/c7b/14a429c7b2ae6ba7383d6d39be9e3031.png"></div><br><br>  O que gera mais quatro exemplos: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e9b/b3c/89a/e9bb3c89a00306b3fd18eb86d8f2160b.png"></div><br><br>  Em breve teremos muito mais amostras: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6bb/749/096/6bb749096d3329712a7c00727b4d3cff.png"></div><br><br><h1>  Revisão de Aprendizagem </h1><br><blockquote>  <font color="gray">“Muad'Dib aprendeu rápido porque aprendeu principalmente como aprender.</font>  <font color="gray">Mas a primeira lição foi a assimilação da crença de que ele pode aprender, e essa é a base de tudo.</font>  <font color="gray">É incrível quantas pessoas não acreditam que podem aprender e aprender e quantas pessoas pensam que o aprendizado é muito difícil. "</font>  <font color="gray">- <i>Duna</i></font> </blockquote><br>  Agora que temos o conjunto de skip-gram, usamos para treinar o modelo neural básico da linguagem que prediz uma palavra vizinha. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/944/fb7/70d/944fb770d3aff38f1befa40dfaa7402a.png"></div><br><br>  Vamos começar com a primeira amostra do nosso conjunto de dados.  Pegamos a placa e a enviamos ao modelo não treinado com a solicitação para prever a próxima palavra. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/abb/5cb/9a3/abb5cb9a38d29f1a54176206637131dc.png"></div><br><br>  O modelo passa por três etapas e exibe um vetor de previsão (com probabilidade para cada palavra no dicionário).  Como o modelo não é treinado, nesta fase, sua previsão provavelmente está incorreta.  Mas isso não é nada.  Sabemos qual palavra ela prediz - esta é a célula resultante na linha que atualmente usamos para treinar o modelo: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8af/4fd/c3d/8af4fdc3d3cc86ec1c81fdb3d2715529.png"></div><br>  <i><font color="gray">Um "vetor de destino" é aquele em que a palavra de destino tem uma probabilidade de 1 e todas as outras palavras têm uma probabilidade de 0</font></i> <br><br>  Quão errado estava o modelo?  Subtraia o vetor de previsão do destino e obtenha o vetor de erro: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e6d/b3e/395/e6db3e39593e9c8639d94ef4caccde58.png"></div><br><br>  Agora, esse vetor de erro pode ser usado para atualizar o modelo; portanto, da próxima vez, é mais provável que você dê um resultado preciso nos mesmos dados de entrada. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7d3/6c0/476/7d36c047604b937c907a4ef38ceaaeb7.png"></div><br><br>  Aqui a primeira etapa do treinamento termina.  Continuamos a fazer o mesmo com a próxima amostra no conjunto de dados e depois com a próxima, até examinarmos todas as amostras.  Este é o fim da primeira era do aprendizado.  Repetimos tudo repetidamente por várias épocas e, como resultado, obtemos um modelo treinado: dele você pode extrair a matriz de investimento e usá-la em qualquer aplicação. <br><br>  Embora tenhamos aprendido muito, mas para entender completamente como o word2vec realmente aprende, faltam algumas idéias importantes. <br><br><h1>  Seleção negativa </h1><br><blockquote>  <font color="gray">“Tentar entender Muad'Dib sem entender seus inimigos mortais - os Harkonnenov - é o mesmo que tentar entender a Verdade sem entender o que é a Falsidade.</font>  <font color="gray">Esta é uma tentativa de conhecer a Luz sem conhecer a Escuridão.</font>  <font color="gray">Isso é impossível.</font>  <font color="gray">- <i>Duna</i></font> </blockquote><br>  Lembre-se das três etapas de como um modelo neural calcula uma previsão: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8dd/fe1/4ca/8ddfe14ca387bd4d16c77eb9de8ce98f.png"></div><br><br>  A terceira etapa é muito cara do ponto de vista computacional, especialmente se você fizer isso para cada amostra no conjunto de dados (dezenas de milhões de vezes).  É necessário, de alguma forma, aumentar a produtividade. <br><br>  Uma maneira é dividir o objetivo em duas etapas: <br><br><ol><li>  Crie anexos de palavras de alta qualidade (sem prever a próxima palavra). <br></li><li>  Use esses investimentos de alta qualidade para ensinar o modelo de idioma (para previsão). </li></ol><br>  Este artigo se concentrará na primeira etapa.  Para aumentar a produtividade, você pode deixar de prever uma palavra vizinha ... <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/77d/0a8/c17/77d0a8c17587248a0f790155809798fe.png"></div><br><br>  ... e mude para um modelo que use palavras de entrada e saída e calcule a probabilidade de proximidade (de 0 a 1). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/220/0e3/e06/2200e3e063f3119099d1615e59538d2a.png"></div><br><br>  Uma transição tão simples substitui a rede neural por um modelo de regressão logística - assim, os cálculos se tornam muito mais simples e rápidos. <br><br>  Isso requer refinamento da estrutura do nosso conjunto de dados: o rótulo agora é uma nova coluna com valores de 0 ou 1. Na nossa tabela, existem unidades em todos os lugares, porque adicionamos vizinhos lá. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc2/d1e/874/dc2d1e87438b2492dc9b6e4b1c72162e.png"></div><br><br>  Esse modelo é calculado a uma velocidade incrível: milhões de amostras em minutos.  Mas você precisa fechar uma brecha.  Se todos os nossos exemplos forem positivos (objetivo: 1), pode-se formar um modelo complicado que sempre retorna 1, demonstrando 100% de precisão, mas não aprende nada e gera investimentos indesejados. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1ba/aa2/2d0/1baaa22d0b0c06be5398f896fa7a4c4b.png"></div><br><br>  Para resolver esse problema, é necessário inserir <i>padrões negativos</i> no conjunto de dados - palavras que definitivamente não são vizinhas.  Para eles, o modelo deve retornar 0. Agora, o modelo terá que trabalhar duro, mas os cálculos ainda serão feitos em grande velocidade. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f4c/194/0d8/f4c1940d80c5203620196907a1478431.png"></div><br>  <i><font color="gray">Para cada amostra no conjunto de dados, adicione exemplos negativos com o rótulo 0</font></i> <br><br>  Mas o que introduzir como as palavras de saída?  Escolha as palavras arbitrariamente: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/84e/b22/06f/84eb2206f26b053f1ea8ec4e1b76c5b6.png"></div><br><br>  Esta ideia nasceu sob a influência do método de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">comparação</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ruído</a> [pdf].  Combinamos o sinal real (exemplos positivos de palavras vizinhas) com o ruído (palavras selecionadas aleatoriamente que não são vizinhas).  Isso fornece um excelente compromisso entre desempenho e desempenho estatístico. <br><br><h1>  Amostra Negativa de Ignorar Grama (SGNS) </h1><br>  Analisamos dois conceitos centrais do word2vec: juntos, eles são chamados de "ignorar com amostragem negativa". <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/873/720/fae/873720fae559ce7d6020be66ccb6c397.png"></div><br><br><h1>  Aprendendo word2vec </h1><br><blockquote>  <font color="gray">“Uma máquina não pode prever todos os problemas importantes para uma pessoa viva.</font>  <font color="gray">Há uma grande diferença entre espaço discreto e contínuo contínuo.</font>  <font color="gray">Vivemos em um espaço e as máquinas existem em outro. ”</font>  <font color="gray">- <i>Deus, o Imperador de Dune</i></font> </blockquote><br>  Tendo examinado as idéias básicas do skip-gram e da amostragem negativa, podemos prosseguir com uma análise mais detalhada do processo de aprendizado do word2vec. <br><br>  Primeiro, pré-processamos o texto no qual treinamos o modelo.  Defina o tamanho do dicionário (nós o chamaremos <code>vocab_size</code> ), digamos, em 10.000 anexos e os parâmetros das palavras no dicionário. <br><br>  No início do treinamento, criamos duas matrizes: <code>Embedding</code> e <code>Context</code> .  Os anexos para cada palavra são armazenados nessas matrizes em nosso dicionário (portanto, <code>vocab_size</code> é um dos parâmetros).  O segundo parâmetro é a dimensão do anexo (geralmente <code>embedding_size</code> definido como 300, mas anteriormente vimos um exemplo com 50 dimensões). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1b7/8f8/018/1b78f8018d20fd36d0c5aef37d87a249.png"></div><br><br>  Primeiro, inicializamos essas matrizes com valores aleatórios.  Então começamos o processo de aprendizado.  Em cada estágio, tomamos um exemplo positivo e o negativo associado a ele.  Aqui está o nosso primeiro grupo: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2d2/261/806/2d22618069aa9e3f8a820cb431c6c014.png"></div><br><br>  Agora temos quatro palavras: a palavra de entrada <code>not</code> e as palavras de saída / contextuais <code>thou</code> (vizinho real), <code>aaron</code> e <code>taco</code> (exemplos negativos).  Iniciamos a busca de seus anexos nas matrizes <code>Embedding</code> (para a palavra de entrada) e <code>Context</code> (para as palavras de contexto), embora ambas as matrizes contenham anexos para todas as palavras do nosso dicionário. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/23e/ff0/691/23eff069128db956ce358ae758c0b8bb.png"></div><br><br>  Em seguida, calculamos o produto escalar do anexo de entrada com cada um dos anexos contextuais.  Em cada caso, é obtido um número que indica a similaridade dos dados de entrada e dos anexos contextuais. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/615/319/e4a/615319e4accc7235c28fc8c76dca09f6.png"></div><br><br>  Agora precisamos de uma maneira de transformar essas estimativas em um tipo de probabilidade: todas elas devem ser números positivos entre 0 e 1. Essa é uma excelente tarefa para equações logísticas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">sigmóides</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/98b/125/8b2/98b1258b21744917c993e617e0844ad8.png"></div><br><br>  O resultado do cálculo sigmóide pode ser considerado como a saída do modelo para essas amostras.  Como você pode ver, o <code>taco</code> a maior pontuação, enquanto o <code>aaron</code> ainda tem a menor pontuação, antes e depois do sigmóide. <br><br>  Quando o modelo não treinado fez uma previsão e tem uma marca de destino real para comparação, vamos calcular quantos erros existem na previsão do modelo.  Para fazer isso, basta subtrair a pontuação sigmóide dos rótulos alvo. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d49/458/333/d49458333c28225c596014df3c6fcedb.png"></div><br>  <i><font color="gray"><code>error</code> = <code>target</code> - <code>sigmoid_scores</code></font></i> <br><br>  É aqui que a fase de "aprendizado" do termo "aprendizado de máquina" começa.  Agora, podemos usar essa estimativa de erro para ajustar os investimentos <code>not</code> , <code>thou</code> , <code>aaron</code> e <code>taco</code> , para que na próxima vez que o resultado esteja mais próximo das estimativas de destino. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a5d/e51/cd3/a5de51cd3a86a1ed0784a709cb979bdc.png"></div><br><br>  Isso completa uma etapa do treinamento.  Melhoramos um pouco o apego de algumas palavras ( <code>not</code> <code>thou</code> , <code>aaron</code> e <code>taco</code> ).  Agora passamos para o próximo estágio (a próxima amostra positiva e a negativa associada a ela) e repetimos o processo. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/637/2ab/c3a/6372abc3a3f6b623d5b2cbab02953030.png"></div><br><br>  Os anexos continuam a melhorar à medida que percorremos o conjunto de dados inteiro várias vezes.  Em seguida, você pode interromper o processo, deixar a matriz de <code>Context</code> lado e usar a matriz <code>Embeddings</code> treinada para a próxima tarefa. <br><br><h1>  Tamanho da janela e número de amostras negativas </h1><br>  No processo de aprendizado do word2vec, dois hiperparâmetros-chave são o tamanho da janela e o número de amostras negativas. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9fe/437/447/9fe4374479f547c1b324c7471cd61cbd.png"></div><br><br>  Diferentes tamanhos de janelas são adequados para diferentes tarefas.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Observou-se</a> que tamanhos de janela menores (2–15) geram anexos <i>intercambiáveis</i> com índices semelhantes (observe que os antônimos geralmente são intercambiáveis ​​quando se olha para as palavras ao redor: por exemplo, as palavras “bom” e “ruim” são frequentemente mencionadas em contextos semelhantes).  Tamanhos de janela maiores (15 a 50 ou mais) geram anexos <i>relacionados</i> com índices semelhantes.  Na prática, você geralmente precisa fornecer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">anotações</a> para obter semelhanças semânticas úteis em sua tarefa.  No Gensim, o tamanho padrão da janela é 5 (duas palavras esquerda e direita, além da própria palavra de entrada). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4f4/b0a/45a/4f4b0a45a8552d6c19c7c9459302ac48.png"></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O número de amostras negativas é outro fator no processo de aprendizagem. </font><font style="vertical-align: inherit;">O documento original recomenda 5–20. </font><font style="vertical-align: inherit;">Ele também diz que 2-5 amostras parecem suficientes quando você tem um conjunto de dados suficientemente grande. </font><font style="vertical-align: inherit;">No Gensim, o valor padrão é 5 padrões negativos.</font></font><br><br><h1>  Conclusão </h1><br><blockquote> <font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">“Se seu comportamento está além dos seus padrões, você é uma pessoa viva, não um autômato” - </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Deus-Imperador de Dune</font></font></i></font> </blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Espero que agora você entenda a incorporação de palavras e a essência do algoritmo word2vec. </font><font style="vertical-align: inherit;">Espero também que agora você se familiarize mais com os artigos que mencionam o conceito de "pular com amostragem negativa" (SGNS), como nos sistemas de recomendação acima mencionados.</font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Referências e leituras adicionais </font></font></h1><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">“Representações distribuídas de palavras e frases e sua composição”</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> [pdf]</font></font></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">«      »</a> [pdf] </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">«   »</a> [pdf] </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">«   »</a>      —    NLP. Word2vec    . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">«      »</a> by <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a> —      . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a>        Word2vec.        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">« word2vec»</a> </li><li>   ?   : <ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> word2vec  Python</a>  Gensim </li><li>  <a href="">   C</a> ,    <a href="">       </a> </li></ul></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">    </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">«  »</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> 2</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">«»</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt446530/">https://habr.com/ru/post/pt446530/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt446514/index.html">O Gmail tem 15 anos</a></li>
<li><a href="../pt446516/index.html">Visualização do tempo de renascimento de Roshan</a></li>
<li><a href="../pt446518/index.html">Firewalls de aplicativos da Web</a></li>
<li><a href="../pt446520/index.html">Como tudo começou: a história dos drones voadores</a></li>
<li><a href="../pt446522/index.html">Swift 5.1 - o que há de novo?</a></li>
<li><a href="../pt446532/index.html">Upwork introduz uma taxa pelo direito de escrever para um cliente em potencial</a></li>
<li><a href="../pt446534/index.html">Lançamento do Visual Studio 2019</a></li>
<li><a href="../pt446536/index.html">Filas e JMeter: Exchange com Publicador e Assinante</a></li>
<li><a href="../pt446538/index.html">O PhotoGuru mudou para o "lado sombrio" e o "mais sábio"</a></li>
<li><a href="../pt446544/index.html">Microsoft expande o Azure IP Advantage Program com novos benefícios de IP para inovadores e startups de IoT do Azure</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>