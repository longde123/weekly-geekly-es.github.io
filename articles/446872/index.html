<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤷 🍅 🍓 Explorando OpenCV en StereoPi: Mapa de profundidad del video 👌🏿 🎳 🐃</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hoy queremos compartir una serie de ejemplos en Python para los estudiantes de OpenCV en la Raspberry Pi, a saber, la placa StereoPi de doble cámara. ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Explorando OpenCV en StereoPi: Mapa de profundidad del video</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/446872/"><img src="https://habrastorage.org/webt/hb/xt/po/hbxtpox_r6mswlkshq4w3cpovr4.gif"><br><br>  Hoy queremos compartir una serie de ejemplos en Python para los estudiantes de OpenCV en la Raspberry Pi, a saber, la placa StereoPi de doble cámara.  El código terminado (más la imagen de Raspbian) lo ayudará a seguir todos los pasos, comenzando con la captura de una imagen y terminando con la obtención de un mapa de profundidad del video capturado. <br><a name="habracut"></a><br><h3>  Introductorio </h3><br>  Debo enfatizar de inmediato que estos ejemplos son para una inmersión cómoda en el tema y no para una solución de producción.  Si es un usuario avanzado de OpenCV y ha estado tratando con frambuesas, entonces sabe que para un funcionamiento adecuado es recomendable codificar una mordida e incluso usar una GPU de frambuesa.  Al final del artículo, tocaré los "cuellos de botella" de la solución de Python y el rendimiento general con más detalle. <br><br><h3>  Con que estamos trabajando </h3><br>  Tenemos una configuración como el hierro: <br><br><img src="https://habrastorage.org/webt/or/pd/9u/orpd9ufeuctr0lbmsk0kfogroao.jpeg"><br><br>  Placa StereoPi a bordo del Raspberry Pi Compute Module 3+.  Las dos cámaras más simples están conectadas para la versión Raspberry Pi V1 (en el sensor ov5647). <br><br>  Lo que está instalado: <br><br><ul><li>  Raspbian Stretch (kernel 4.14.98-v7 +) </li><li>  Python 3.5.3 </li><li>  OpenCV 3.4.4 (precompilado, 'pip' de Python Wheels) </li><li>  Picamera 1.13 </li><li>  StereoVision lib 1.0.3 (https://github.com/erget/StereoVision) </li></ul><br>  El proceso de instalación de todo el software está más allá del alcance de este artículo, y solo sugerimos descargar la imagen Raspbian terminada (enlaces al github al final del artículo). <br><br><h3>  Paso uno: captura una imagen </h3><br>  Para hacer esto, use el script 1_test.py <br><br>  Abra la consola, vaya de la carpeta de inicio a la carpeta con ejemplos: <br><br><pre><code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> stereopi-tutorial</code> </pre> <br>  Ejecute el script: <br><br><pre> <code class="bash hljs">python 1_test.py</code> </pre> <br>  Después de comenzar, se muestra una vista previa de nuestra imagen estéreo en la pantalla.  El proceso se puede interrumpir presionando el botón Q. Esto guardará la última imagen capturada, que se utilizará en uno de los siguientes scripts para configurar el mapa de profundidad. <br><br>  Este script le permite asegurarse de que todo el hardware funciona correctamente, así como obtener la primera imagen para uso futuro. <br><br>  Así es como se ve el primer script: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/wllLrNUw3SE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h3>  Paso dos: recolecte imágenes para la calibración </h3><br>  Si hablamos de un caballo esférico en el vacío, entonces para obtener un mapa de profundidad de buena calidad, necesitamos tener dos cámaras absolutamente idénticas, cuyos ejes vertical y óptico sean perfectamente paralelos y los ejes horizontales coincidan.  Pero en el mundo real, todas las cámaras son ligeramente diferentes, y no es posible organizarlas perfectamente.  Por lo tanto, se inventó un truco de calibración de software.  Usando dos cámaras del mundo real, se toma una gran cantidad de imágenes de un objeto previamente conocido (tenemos una imagen con un tablero de ajedrez), y luego un algoritmo especial calcula todas las "imperfecciones" e intenta corregir las imágenes para que estén cerca del ideal. <br><br>  Este script realiza la primera etapa del trabajo, es decir, ayuda a hacer una serie de fotos para la calibración. <br><br>  Antes de cada foto, el guión comienza una cuenta regresiva de 5 segundos.  Esta vez, como regla, es suficiente para mover el tablero a una nueva posición, para asegurarse de que en ambas cámaras no se arrastre por los bordes y fije su posición (para que no haya borrosidad en la foto).  Por defecto, el tamaño de la serie se establece en 30 fotos. <br><br>  Lanzamiento <br><br><pre> <code class="bash hljs">python 2_chess_cycle.py</code> </pre> <br>  Proceso: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/1XCAlU3k-xs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Como resultado, tenemos una serie de fotos en la carpeta / escenas. <br><br><h3>  Cortamos las imágenes en parejas. </h3><br>  El tercer script 3_pairs_cut.py corta las fotos tomadas en imágenes "izquierda" y "derecha" y las guarda en la carpeta / pares.  De hecho, podríamos excluir este script y hacer el corte sobre la marcha, pero es muy útil en futuros experimentos.  Por ejemplo, puede guardar segmentos de diferentes series, usar sus scripts para trabajar con estos pares, o incluso copiar las imágenes tomadas en otras cámaras estéreo como pares. <br><br>  Además, antes de cortar cada imagen, el script muestra su imagen, que a menudo le permite ver fotos fallidas antes del siguiente paso de calibración y simplemente eliminarlas. <br><br>  Ejecute el script: <br><br><pre> <code class="bash hljs">python 3_pairs_cut.py</code> </pre> <br>  Video corto: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/95DWmPECbDc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  En la imagen final hay un conjunto de fotografías y pares de cortes que utilizamos para nuestros experimentos. <br><br><h3>  Calibración </h3><br>  El script 4_calibration.py dibuja todos los pares con los tableros de ajedrez y calcula las correcciones necesarias para corregir las imágenes.  El guión hizo un rechazo automático de fotos en las que no se encontró un tablero de ajedrez, por lo que en caso de fotos sin éxito el trabajo no se detiene.  Después de cargar los 30 pares de imágenes, comienza el cálculo.  Nos lleva aproximadamente un minuto y medio.  Una vez finalizado, el script toma uno de los pares estéreo y, sobre la base de los parámetros de calibración calculados, los "corrige" y muestra una imagen rectificada en la pantalla.  En este punto, puede evaluar la calidad de la calibración. <br><br>  Ejecutar por comando: <br><br><pre> <code class="bash hljs">python 4_calibration.py</code> </pre> <br>  Script de calibración en el trabajo: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/vtPhu23tKGo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h3>  Configuración del mapa de profundidad </h3><br>  El script 5_dm_tune.py carga la imagen tomada por el primer script y los resultados de la calibración.  A continuación, se muestra una interfaz que le permite cambiar la configuración del mapa de profundidad y ver qué cambios.  Consejo: antes de configurar los parámetros, tome un marco en el que simultáneamente tendrá objetos a diferentes distancias: cerca (30-40 centímetros), a una distancia promedio (metro o dos) y en la distancia.  Esto le permitirá elegir los parámetros en los que los objetos cercanos serán rojos y los objetos distantes serán azul oscuro. <br><br>  La imagen contiene un archivo con nuestra configuración de mapa de profundidad.  Puede cargar nuestra configuración en un script simplemente haciendo clic en el botón "Cargar configuración" <br><br>  Lanzamos: <br><br><pre> <code class="bash hljs">python 5_dm_tune.py</code> </pre> <br>  Así es como se ve el proceso de configuración: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Z4j3NrMyeGE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h3>  Mapa de profundidad en tiempo real </h3><br>  La última secuencia de comandos 6_dm_video.py crea un mapa de profundidad a partir del video utilizando los resultados de secuencias de comandos anteriores (calibración y configuración del mapa de profundidad). <br><br>  Lanzamiento <br><br><pre> <code class="bash hljs">python 6_dm_video.py</code> </pre> <br>  En realidad el resultado: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/f29arVstfZA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  ¡Esperamos que nuestros guiones sean útiles en sus experimentos! <br><br>  Por si acaso, agregaré que todas las secuencias de comandos tienen procesamiento de pulsación de teclas, y puede interrumpir el trabajo presionando el botón Q. Si se detiene "bruscamente", por ejemplo, Ctrl + C, el proceso de interacción de Python con la cámara puede romperse y será necesario reiniciar la frambuesa. <br><br><h3>  Para avanzado </h3><br><ul><li>  La primera secuencia de comandos en el proceso muestra el tiempo promedio entre capturas de cuadros y, al finalizar, el FPS promedio.  Esta es una herramienta simple y conveniente para seleccionar dichos parámetros de imagen en los que Python todavía no se está "ahogando".  Con él, recogimos 1280x480 a 20 FPS, en el que el video se renderiza sin demora. </li><li>  Puede notar que capturamos un par estéreo en una resolución de 1280x480 y luego lo escalamos a 640x240. <br><br>  Una pregunta razonable: ¿por qué todo esto y por qué no tomar inmediatamente la miniatura y no cargar nuestra pitón reduciéndola? <br><br>  Respuesta: con la captura directa a resoluciones muy bajas, todavía hay problemas en el núcleo de frambuesa (la imagen se rompe).  Por lo tanto, tomamos una resolución más grande y luego reducimos la imagen.  Aquí usamos un pequeño truco: la imagen no se escala con Python, sino con la ayuda de la GPU, por lo que no hay carga en el núcleo del brazo. </li><li>  ¿Por qué capturar video en formato BGRA, no BGR? <br>  Utilizamos los recursos de la GPU para reducir el tamaño de la imagen, y el nativo para el módulo de cambio de tamaño es el formato BGRA.  Si usamos BGR en lugar de BGRA, tendremos dos inconvenientes.  El primero es ligeramente más bajo que el FPS final (en nuestras pruebas - 20 por ciento).  El segundo es el trabajo constante en la consola "PiCameraAlfaStripping: uso de stripping alfa para convertir a formato no alpha;  puede encontrar el formato alfa equivalente más rápido ".  Buscar en Google llevó a la sección de documentación de Picamera, que describe este truco. </li><li>  ¿Dónde está el PiRGBArray? <br><br>  Esto es como la clase nativa de Picamera para trabajar con la cámara, pero aquí no se usa.  Ya resultó que en nuestras pruebas, trabajar con una matriz numpy "hecha a mano" es mucho más rápido (aproximadamente una vez y media) que usar PiRGBArray.  Esto no significa que PiRGBArray sea malo, lo más probable es que estas sean nuestras manos torcidas. </li><li>  ¿Qué tan cargado es el porcentaje en el cálculo del mapa de profundidad? <br>  Respondamos con una imagen: <br><br><img src="https://habrastorage.org/webt/nn/ez/ef/nnezefyxuiuxx7difz1xctii16w.jpeg"><br><br>  Vemos que de los 4 núcleos del procesador, de hecho, solo uno está cargado, y eso es el 70%.  Y esto a pesar del hecho de que trabajamos con una GUI, y estamos enviando imágenes y mapas de profundidad al usuario.  Esto significa que hay un buen margen de rendimiento, y un ajuste fino de OpenCV con OpenMP y otras ventajas en C, así como un modo de "combate" sin una GUI pueden dar resultados muy interesantes. </li><li>  ¿Cuál es el mapa de profundidad de FPS máximo obtenido con esta configuración? <br><br>  El máximo alcanzado por nosotros fue de 17 FPS, al capturar 20 cuadros por segundo de la cámara.  Los más "sensibles" en términos de parámetros de velocidad en la configuración del mapa de profundidad son MinDisparity y NumOfDisparities.  Esto es lógico, ya que determinan el número de "pasos" realizados dentro del algoritmo por la ventana de búsqueda para comparar marcos.  El segundo más sensible es preFilterCap, que afecta, en particular, la "suavidad" del mapa de profundidad. </li><li>  ¿Qué pasa con la temperatura del procesador? <br><br>  En Compute Module 3+ Lite (una nueva serie, con una "tapa" de hierro en el proceso) muestra aproximadamente los siguientes resultados: <br><br><img src="https://habrastorage.org/webt/ba/p7/kw/bap7kwdbbhd0y2bmvebpqzimqpa.jpeg"></li><li>  ¿Cómo usar la GPU? <br><br>  Como mínimo, se puede utilizar para la andistorización y rectificación de imágenes en tiempo real, porque hay ejemplos ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aquí en WebGL</a> ), Python <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pi3d</a> , así como el proyecto Processing ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ejemplos para frambuesas</a> ). <br><br>  Hay otro desarrollo interesante de Koichi Nakamura, llamado <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">py-videocore</a> .  En nuestra correspondencia con él, expresó la idea de que para acelerar StereoBM puede usar su núcleo y OpenCV <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">con el soporte de Cuda</a> .  En general, para la optimización, un borde intacto, como dicen. </li></ul><br>  Gracias por su atención, y aquí está el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace prometido a la fuente</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/446872/">https://habr.com/ru/post/446872/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../446860/index.html">Historia de 3dfx Voodoo1</a></li>
<li><a href="../446862/index.html">Lo que los diseñadores esperan en DUMP-2019: descripción general de la sección de diseño</a></li>
<li><a href="../446864/index.html">Energía, calor y agua</a></li>
<li><a href="../446866/index.html">Sistemas operativos: tres piezas fáciles. Parte 2: Abstracción: Proceso (traducción)</a></li>
<li><a href="../446870/index.html">Sistemas de partículas: una historia navideña</a></li>
<li><a href="../446876/index.html">Moscú, 18 de abril - QIWI SERVER PARTY 4.0</a></li>
<li><a href="../446880/index.html">Gráficos incorrectos: nuestra experiencia</a></li>
<li><a href="../446882/index.html">MIPT recibió el derecho de organizar la Copa Mundial de Programación ICPC en 2020 en Moscú</a></li>
<li><a href="../446884/index.html">Qué leer y ver en ciencia ficción fresca: Marte, cyborgs e IA rebelde</a></li>
<li><a href="../446886/index.html">Los mejores expertos de la Expo 3D: Sunny Wong. Se pueden prevenir más de 25 millones de esguinces</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>