<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üåì üôåüèª ü§¥üèº Neurobugurt. Como ensinamos a rede neural a inventar memes um ano antes que Stanford üëä üî∫ üë©üèæ‚Äç‚öñÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Esta not√≠cia (+ pesquisa ) sobre a inven√ß√£o do gerador de memes por cientistas da Universidade de Stanford me levou a escrever um artigo. No meu artig...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Neurobugurt. Como ensinamos a rede neural a inventar memes um ano antes que Stanford</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/416379/"> Esta <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">not√≠cia</a> (+ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pesquisa</a> ) sobre a inven√ß√£o do gerador de memes por cientistas da Universidade de Stanford me levou a escrever um artigo.  No meu artigo, tentarei mostrar que voc√™ n√£o precisa ser um cientista de Stanford para fazer coisas interessantes com redes neurais.  No artigo, descrevo como, em 2017, treinamos uma rede neural em um corpo de aproximadamente 30.000 textos e a for√ßamos a gerar novos memes e memes da Internet (sinais de comunica√ß√£o) no sentido sociol√≥gico da palavra.  Descrevemos o algoritmo de aprendizado de m√°quina que usamos, as dificuldades t√©cnicas e administrativas que encontramos. <br><a name="habracut"></a><br>  Um pouco de fundo sobre como chegamos √† ideia de um neuro-escritor e em que consistia exatamente.  Em 2017, fizemos um projeto para um site p√∫blico da Vkontakte, cujo nome e captura de tela os moderadores do Habrahabr proibiram de publicar, considerando sua men√ß√£o como PR "auto".  O p√∫blico existe desde 2013 e une as postagens com a id√©ia geral de decompor o humor em uma linha e separar as linhas com o s√≠mbolo "@": <br><br> <code> <br> @ <br>   <br> @ <br> </code> <br> <br>  O n√∫mero de linhas pode variar, o gr√°fico pode ser qualquer.  Na maioria das vezes, isso √© humor ou notas sociais afiadas sobre os fatos desenfreados da realidade.  Em geral, esse design √© chamado de "buhurt". <br><br><img src="https://habrastorage.org/webt/g9/bb/4n/g9bb4nvtkhh2hkx7wybuutrzr3u.png" alt="imagem"><br><br>  <i>Um dos t√≠picos buhurts</i> <br><br>  Ao longo dos anos, o p√∫blico se transformou em conhecimento interno (caracteres, tramas, locais) e o n√∫mero de postagens excedeu 30.000.No momento de analisar as necessidades do projeto, o n√∫mero de linhas de origem do texto excedia meio milh√£o. <br><br><h3>  Parte 0. O surgimento de id√©ias e equipes </h3><br>  Ap√≥s a popularidade em massa das redes neurais, a id√©ia de treinar a RNA em nossos textos ficou no ar por cerca de seis meses, mas foi finalmente formulada com o E7su em dezembro de 2016. Ao mesmo tempo, o nome foi inventado ("Neurobugurt").  Naquela √©poca, a equipe interessada no projeto era composta por apenas tr√™s pessoas.  Todos √©ramos estudantes sem experi√™ncia pr√°tica em algoritmos e redes neurais.  Pior, nem sequer t√≠nhamos uma GPU adequada para treinamento.  Tudo o que t√≠nhamos era entusiasmo e confian√ßa de que essa hist√≥ria poderia ser interessante. <br><br><h3>  Parte 1. A formula√ß√£o da hip√≥tese e tarefas </h3><br>  Nossa hip√≥tese acabou sendo a suposi√ß√£o de que, se voc√™ misturar todos os textos publicados em tr√™s anos e meio e treinar a rede neural nesse edif√≠cio, poder√° obter: <br><br>  a) mais criativo que as pessoas <br>  b) engra√ßado <br><br>  Mesmo que as palavras ou letras no buhurt pare√ßam confundidas e dispostas aleatoriamente √† m√°quina - acredit√°vamos que isso poderia funcionar como um servi√ßo de f√£s e ainda agradaria aos leitores. <br><br>  A tarefa foi bastante simplificada pelo fato de o formato dos buhurts ser essencialmente textual.  Portanto, n√£o precisamos mergulhar na vis√£o de m√°quina e em outras coisas complexas.  Outra boa not√≠cia foi que todo o corpo de textos √© muito semelhante.  Isso tornou poss√≠vel n√£o usar o aprendizado refor√ßado - pelo menos nos est√°gios iniciais.  Ao mesmo tempo, entendemos claramente que criar um gravador de rede neural com sa√≠da leg√≠vel mais de uma vez n√£o √© t√£o f√°cil.  O risco de dar √† luz um monstro que jogaria cartas aleatoriamente era muito grande. <br><br><h3>  Parte 2. Prepara√ß√£o do corpo dos textos </h3><br>  Acredita-se que a fase de prepara√ß√£o possa demorar muito tempo, pois est√° associada √† coleta e limpeza de dados.  No nosso caso, acabou por ser bastante curto: um pequeno <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">analisador</a> foi escrito que extraiu cerca de 30k posts da parede da comunidade e os colocou em um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">arquivo txt</a> . <br><br>  N√£o limpamos os dados antes do primeiro treinamento.  No futuro, isso representou uma piada cruel conosco, porque, devido ao erro que surgiu nesta fase, n√£o conseguimos trazer os resultados de forma leg√≠vel por um longo tempo.  Mas mais sobre isso mais tarde. <br><br><img src="https://habrastorage.org/webt/jt/9v/uq/jt9vuqk2tdpoi7ycus5udn-lnpo.png" alt="imagem"><br><br>  <i>Arquivo de tela com hamb√∫rgueres</i> <br><br><h3>  Parte 3. An√∫ncio, refinamento da hip√≥tese, escolha do algoritmo </h3><br>  Usamos um recurso acess√≠vel - um grande n√∫mero de assinantes p√∫blicos.  A suposi√ß√£o era que entre 300.000 leitores existem v√°rios entusiastas que possuem redes neurais em um n√≠vel suficiente para preencher as lacunas no conhecimento de nossa equipe.  Partimos da id√©ia de anunciar amplamente a competi√ß√£o e atrair entusiastas de aprendizado de m√°quina, at√© a discuss√£o do problema formulado.  Depois de escrever os textos, contamos √†s pessoas sobre a nossa ideia e esperamos uma resposta. <br><br><img src="https://habrastorage.org/webt/73/0b/jo/730bjoajceycgzoynywra6xcupy.png" alt="imagem"><br><br>  <i>An√∫ncio de discuss√£o tem√°tica</i> <br><br>  A rea√ß√£o das pessoas excedeu nossas expectativas mais loucas.  A discuss√£o do fato de que vamos treinar uma rede neural espalhou o holivar por quase 1000 coment√°rios.  A maioria dos leitores simplesmente desapareceu e tentou imaginar como seria o resultado.  Cerca de 6.000 pessoas analisaram a discuss√£o tem√°tica e mais de 50 amadores interessados ‚Äã‚Äãdeixaram coment√°rios para os quais testamos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">814 linhas de buhurt</a> para a realiza√ß√£o de testes e treinamento iniciais.  Cada pessoa interessada pode pegar um conjunto de dados e aprender o algoritmo mais interessante para ele, e depois discutir conosco e com outros entusiastas.  Anunciamos antecipadamente que continuaremos a trabalhar com os participantes cujos resultados ser√£o mais leg√≠veis. <br><br>  O trabalho come√ßou: algu√©m montou silenciosamente um gerador nas cadeias de Markov, algu√©m tentou v√°rias implementa√ß√µes com um github e a maioria ficou louca na discuss√£o e nos convenceu com espuma na boca de que nada resultaria disso.  Isso iniciou a parte t√©cnica do projeto. <br><br><img src="https://habrastorage.org/webt/ie/cu/tz/iecutzyxvv__0a2qzo5ailsjk9s.png" alt="imagem"><br><br>  Algumas sugest√µes de entusiastas <br><br>  As pessoas ofereceram dezenas de op√ß√µes para implementa√ß√£o: <br><br><ul><li>  Cadeias de Markov. </li><li>  Encontre uma implementa√ß√£o pronta de algo semelhante ao GitHub e treine-a. </li><li>  Um gerador de frases aleat√≥rias escrito em Pascal. </li><li>  Consiga um negro liter√°rio que escrever√° bobagens aleat√≥rias, e passaremos isso como uma sa√≠da de rede neural. </li></ul><br><img src="https://habrastorage.org/webt/ki/tr/ut/kitrutzq2yxrtm_weg1ngm4beow.png" alt="imagem"><br><br>  <i>Avalia√ß√£o da complexidade do projeto de um dos assinantes</i> <br><br>  A maioria dos comentaristas concordou que nosso projeto est√° fadado ao fracasso e nem chegaremos ao est√°gio de prot√≥tipo.  Como entendemos mais adiante, as pessoas ainda tendem a perceber as redes neurais como algum tipo de magia negra que acontece na ‚Äúcabe√ßa de Zuckerberg‚Äù e nas divis√µes secretas do Google. <br><br><h3>  Parte 4. Sele√ß√£o de algoritmos, treinamento e expans√£o da equipe </h3><br>  Depois de algum tempo, a campanha que lan√ßamos para id√©ias de crowdsourcing para o algoritmo come√ßou a dar seus primeiros frutos.  Temos cerca de 30 prot√≥tipos em funcionamento, a maioria dos quais deu um absurdo completamente ileg√≠vel. <br><br>  Nesta fase, encontramos pela primeira vez uma desmotiva√ß√£o da equipe.  Todos os resultados foram muito semelhantes aos buhurts e, na maioria das vezes, representavam abracadabra de letras e s√≠mbolos.  O trabalho de dezenas de entusiastas foi ao p√≥ e isso desmotivou a eles e a n√≥s. <br><br>  O algoritmo baseado em pyTorch mostrou-se melhor que outros.  Foi decidido tomar essa implementa√ß√£o e o algoritmo LSTM como base.  Reconhecemos o assinante que o prop√¥s como vencedor e come√ßamos a trabalhar para melhorar o algoritmo junto com ele.  Nossa equipe distribu√≠da cresceu para quatro pessoas.  O fato engra√ßado aqui √© que o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">vencedor do concurso</a> , como se viu, tinha apenas 16 anos de idade.  A vit√≥ria foi seu primeiro pr√™mio real no campo da ci√™ncia de dados. <br><br>  Para o primeiro treinamento, um conjunto de 8 placas gr√°ficas GXT1080 foi alugado. <br><br><img src="https://habrastorage.org/webt/mb/_l/ql/mb_lqlodsgm8fyztu36ly0soqo4.jpeg" alt="imagem"><br><br>  <i>Console de gerenciamento de cluster de cart√µes</i> <br><br>  O reposit√≥rio original e todos os manuais do projeto Torch-rnn est√£o aqui: <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">github.com/jcjohnson/torch-rnn</a> .  Posteriormente, com base nisso, publicamos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">nosso reposit√≥rio</a> , no qual existem nossas fontes, o Leia-me para instala√ß√£o, bem como os pr√≥prios neurobugurts conclu√≠dos. <br><br>  Nas primeiras vezes em que treinamos usando uma configura√ß√£o pr√©-configurada em um cluster de GPU pago.  A configura√ß√£o acabou n√£o sendo t√£o dif√≠cil - basta apenas as instru√ß√µes do desenvolvedor do Torch e a ajuda da administra√ß√£o de hospedagem, inclu√≠da no pagamento. <br><br>  No entanto, rapidamente encontramos dificuldades: cada treinamento custava o tempo de aluguel da GPU - o que significa que simplesmente n√£o havia dinheiro no projeto.  Por isso, em janeiro-fevereiro de 2017, realizamos treinamentos nas instala√ß√µes adquiridas e tentamos lan√ßar a gera√ß√£o em nossas m√°quinas locais. <br><br>  Qualquer texto √© adequado para o treinamento do modelo.  Antes do treinamento, √© necess√°rio pr√©-process√°-lo, para o qual o Torch possui um algoritmo preprocess.py especial que converte seu my_data.txt em dois arquivos: HDF5 e JSON: <br><br>  O script de pr√©-processamento √© executado assim: <br><br><pre> <code class="python hljs">python scripts/preprocess.py \ --input_txt my_data.txt \ --output_h5 my_data.h5 \ --output_json my_data.json</code> </pre> <br><img src="https://habrastorage.org/webt/8u/s8/kj/8us8kjppqy-1wqhpjp87hdiwqyi.png" alt="imagem"><br><br>  <i>Ap√≥s o pr√©-processamento, dois arquivos aparecem nos quais a rede neural ser√° treinada no futuro</i> <br><br>  Os v√°rios sinalizadores que podem ser alterados no est√°gio de pr√©-processamento s√£o descritos <a href="">aqui</a> .  Tamb√©m √© poss√≠vel executar o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Torch no Docker</a> , mas o autor do artigo n√£o o verificou. <br><br><h4>  Treinamento em redes neurais </h4><br>  Ap√≥s o pr√©-processamento, voc√™ pode prosseguir para o treinamento do modelo.  Na pasta HDF5 e JSON, √© necess√°rio executar o utilit√°rio th, que apareceu com voc√™, se voc√™ instalou o Torch corretamente: <br><br><pre> <code class="python hljs">th train.lua -input_h5 my_data.h5 -input_json my_data.json</code> </pre> <br>  O treinamento leva uma quantidade enorme de tempo e gera arquivos no formato cv / checkpoint_1000.t7, que s√£o os "pesos" da nossa rede neural.  Esses arquivos pesam uma quantidade impressionante de megabytes e cont√™m a for√ßa dos links entre letras espec√≠ficas no seu conjunto de dados original. <br><br><img src="https://habrastorage.org/webt/qt/ts/it/qttsit4vulbqojnmxfcfdh1pdzm.png" alt="imagem"><br><br>  <i>Uma rede neural √© frequentemente comparada com o c√©rebro humano, mas me parece uma analogia muito mais clara com uma fun√ß√£o matem√°tica que pega par√¢metros na entrada (seu conjunto de dados) e fornece o resultado (novos dados) na sa√≠da.</i> <br><br>  No nosso caso, cada treinamento em um cluster de 8 GTX 1080 em um conjunto de dados de 500.000 linhas levou cerca de uma hora ou duas, e um treinamento semelhante em um tipo de CPU i3-2120 levou cerca de 80 a 100 horas.  No caso de treinamento mais longo, a rede neural come√ßou a se treinar rigidamente - os s√≠mbolos se repetiam com muita frequ√™ncia, caindo em longos ciclos de preposi√ß√µes, conjun√ß√µes e palavras introdut√≥rias. <br><br>  √â conveniente escolher a frequ√™ncia dos pontos de verifica√ß√£o e, durante um treinamento, voc√™ receber√° imediatamente muitos modelos: do menos treinado (ponto de verifica√ß√£o_1000) ao reciclado (ponto de verifica√ß√£o_1000000).  Apenas espa√ßo suficiente seria suficiente. <br><br><h4>  Nova gera√ß√£o de texto </h4><br>  Depois de receber pelo menos um arquivo pronto com pesos (ponto de verifica√ß√£o _ *******), voc√™ pode prosseguir para a pr√≥xima e mais interessante etapa: come√ßar a gerar textos.  Para n√≥s, foi um verdadeiro momento de verdade, porque pela primeira vez obtivemos algum resultado tang√≠vel - um bugurt escrito por uma m√°quina. <br><br>  Nesse ponto, finalmente paramos de usar o cluster e todas as gera√ß√µes foram realizadas em nossas m√°quinas de baixa pot√™ncia.  No entanto, ao tentar iniciar localmente, simplesmente n√£o conseguimos seguir as instru√ß√µes e instalar o Torch.  A primeira barreira foi o uso de m√°quinas virtuais.  No Ubuntu 16 virtual, o stick n√£o decola - esque√ßa.  O StackOverflow costumava ser √∫til, mas alguns erros eram t√£o pouco triviais que a resposta s√≥ podia ser encontrada com grande dificuldade. <br><br>  A instala√ß√£o do Torch em uma m√°quina local interrompeu o projeto por algumas semanas: encontramos todos os tipos de erros ao instalar v√°rios pacotes necess√°rios, tamb√©m enfrentamos problemas de virtualiza√ß√£o (virtualenv .env) e, por fim, n√£o o usamos.  V√°rias vezes o suporte foi demolido para o n√≠vel sudo rm -rf e foi simplesmente instalado novamente. <br><br>  Usando o arquivo resultante com pesos, pudemos come√ßar a gerar textos em nossa m√°quina local: <br><br><img src="https://habrastorage.org/webt/9z/ve/r_/9zver_8vvx-fknpdj_9-id70hfe.jpeg" alt="imagem"><br><br>  <i>Uma das primeiras conclus√µes</i> <br><br><h3>  Parte 5. Limpando textos </h3><br>  Outra dificuldade √≥bvia foi que o t√≥pico das postagens √© muito diferente, e nosso algoritmo n√£o envolve nenhuma divis√£o e considera todas as 500.000 linhas como um texto √∫nico.  Consideramos op√ß√µes diferentes para agrupar o conjunto de dados e at√© est√°vamos prontos para quebrar manualmente o corpo dos textos por t√≥pico ou colocar tags em v√°rios milhares de buhurts (havia um recurso humano necess√°rio para isso), mas constantemente enfrent√°vamos dificuldades t√©cnicas ao enviar clusters ao aprender LSTM.  Mudar o algoritmo e conduzir a competi√ß√£o novamente n√£o parecia ser a id√©ia mais sensata em termos de tempo do projeto e motiva√ß√£o dos participantes. <br><br>  Parecia que est√°vamos em um impasse - n√£o pod√≠amos agrupar buhurts, e o treinamento em um √∫nico conjunto de dados enorme produzia resultados duvidosos.  Eu n√£o queria dar um passo atr√°s e mudar o algoritmo e a implementa√ß√£o quase elevados - o projeto poderia simplesmente entrar em coma.  A equipe desesperadamente n√£o tinha conhecimento suficiente para resolver a situa√ß√£o normalmente, mas o bom e velho SME-KAL-OCHK-A veio em socorro.  A solu√ß√£o final para a <s>muleta</s> acabou sendo genialmente simples: no conjunto de dados original, separe os buhurts existentes um do outro com linhas vazias e treine LSTM novamente. <br><br>  Organizamos as batidas em 10 espa√ßos verticais ap√≥s cada buhurt, repetimos o treinamento e, durante a gera√ß√£o, estabelecemos um limite no volume de sa√≠da de 500 caracteres (o comprimento m√©dio de uma buhurt de ‚Äúplotagem‚Äù no conjunto de dados original). <br><br><img src="https://habrastorage.org/webt/da/2h/x4/da2hx4k9shkumjvxwickcnjfwnu.png" alt="imagem"><br><br>  <i>Como era.</i>  <i>Intervalos entre textos s√£o m√≠nimos.</i> <br><br><img src="https://habrastorage.org/webt/bb/y8/i5/bby8i5skh4u0trqrs94ppe2fal0.png" alt="imagem"><br><br>  <i>Como isso se tornou.</i>  <i>Intervalos de 10 linhas permitem que o LSTM ‚Äúentenda‚Äù que um bogurt acabou e outro come√ßou.</i> <br><br>  Assim, foi poss√≠vel alcan√ßar que cerca de 60% de todos os buhurts gerados come√ßaram a ter um gr√°fico leg√≠vel (embora muitas vezes muito ilus√≥rio) ao longo de toda a extens√£o do buhurt, do come√ßo ao fim.  O comprimento de um gr√°fico era, em m√©dia, de 9 a 13 linhas. <br><br><h3>  Parte 6. Reciclagem </h3><br>  Tendo estimado a economia do projeto, decidimos n√£o gastar mais com o aluguel de um cluster, mas investir na compra de nossos pr√≥prios cart√µes.  O tempo de aprendizado aumentaria, mas, tendo comprado um cart√£o uma vez, poder√≠amos gerar novos buhurts constantemente.  Ao mesmo tempo, frequentemente n√£o era mais necess√°rio realizar treinamento. <br><br><img src="https://habrastorage.org/webt/rr/_e/92/rr_e92il8tdd5ckjdglm7fy8cuy.jpeg" alt="imagem"><br><br>  <i>Configura√ß√µes de combate na m√°quina local</i> <br><br><h3>  Parte 7. Balanceando Resultados </h3><br>  Na virada de mar√ßo a abril de 2017, treinamos novamente a rede neural, especificando os par√¢metros de temperatura e o n√∫mero de eras de treinamento.  Como resultado, a qualidade da sa√≠da aumentou ligeiramente. <br><br><img src="https://habrastorage.org/webt/ew/q8/qu/ewq8quy68ixemttz67jkmqeqvgw.png" alt="imagem"><br><br>  <i>Velocidade de aprendizado da tocha-rnn em compara√ß√£o com o char-rnn</i> <br><br>  Testamos os dois algoritmos que acompanham o Torch: rnn e LSTM.  O segundo provou ser melhor. <br><br><h3>  Parte 8. O que alcan√ßamos? </h3><br>  O primeiro neurobugurt foi publicado em 17 de janeiro de 2017 - imediatamente ap√≥s o treinamento no cluster - e no primeiro dia foram coletados mais de 1000 coment√°rios. <br><br><img src="https://habrastorage.org/webt/t2/ng/i5/t2ngi5591hhqz7fh0n_snditsuo.png" alt="imagem"><br><br>  <i>Um dos primeiros neurobugurts</i> <br><br>  Os neurobuguristas chegaram t√£o bem ao p√∫blico que se tornaram uma se√ß√£o separada, que ao longo do ano foi publicada com a hashtag # neurobugurt e divertidos assinantes.  No total, em 2017 e in√≠cio de 2018, geramos mais de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">18.000 neurobugurtos</a> , com uma m√©dia de 500 caracteres cada.  Al√©m disso, todo um movimento de par√≥dias p√∫blicas apareceu, cujos participantes representavam neurobugures, reorganizando aleatoriamente frases em alguns lugares. <br><br><img src="https://habrastorage.org/webt/ls/k2/jb/lsk2jbbloh3e7sg-1p-04k5muvq.jpeg" alt="imagem"><br><br><h3>  Parte 9. Em vez de uma conclus√£o </h3><br>  Com este artigo, eu queria mostrar que, mesmo que voc√™ n√£o tenha experi√™ncia em redes neurais, esse sofrimento n√£o √© um problema.  Voc√™ n√£o precisa trabalhar em Stanford para fazer coisas simples, mas interessantes, com redes neurais.  Todos os participantes do nosso projeto eram estudantes comuns, com suas tarefas, diplomas, trabalhos atuais, mas a causa comum nos permitiu levar o projeto √† final.  Gra√ßas √† ideia cuidadosa, planejamento e energia dos participantes, conseguimos obter os primeiros resultados sensatos em menos de um m√™s ap√≥s a formula√ß√£o final da ideia (a maior parte do trabalho t√©cnico e organizacional ocorreu nas f√©rias de inverno de 2017). <br><br><img src="https://habrastorage.org/webt/pu/q2/yj/puq2yjjiq0ynj4ioncm_wn_obrm.png" alt="imagem"><br><br>  <i>Mais de 18.000 buhurts gerados por m√°quina</i> <br><br>  Espero que este artigo ajude algu√©m a planejar seu pr√≥prio projeto ambicioso com redes neurais.  Pe√ßo para n√£o julgar rigorosamente, j√° que este √© meu primeiro artigo sobre Habr√©.  Se voc√™, como eu, um entusiasta do ML, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">sejamos amigos</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt416379/">https://habr.com/ru/post/pt416379/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt416367/index.html">Iniciamos o ReactOS com BTRFS da se√ß√£o</a></li>
<li><a href="../pt416369/index.html">Quase complicado. Parte 2, criando uma "casa inteligente" sem fio. Baseado na tecnologia Linux, nos softwares Z-Wave e MajorDoMo</a></li>
<li><a href="../pt416371/index.html">Luz de acampamento anal√≥gica</a></li>
<li><a href="../pt416375/index.html">No√ß√µes b√°sicas de JavaScript para iniciantes</a></li>
<li><a href="../pt416377/index.html">N√≥s nos tornamos assistentes em programa√ß√£o. Parte 1</a></li>
<li><a href="../pt416381/index.html">Relat√≥rio do Club of Rome de 2018, cap√≠tulo 3.13: Filantropia, Investimento, Crowdsourcing e Blockchain</a></li>
<li><a href="../pt416385/index.html">Se a correla√ß√£o sair 100%, algum erro ocorreu em algum lugar: a experi√™ncia de est√°gio no Rambler Group</a></li>
<li><a href="../pt416387/index.html">Camar√£o: Dimensione e compartilhe imagens HTTP no C ++ moderno com ImageMagic ++, SObjectizer e RESTinio</a></li>
<li><a href="../pt416391/index.html">Otimiza√ß√£o do posicionamento de m√°quinas virtuais em servidores</a></li>
<li><a href="../pt416393/index.html">Confer√™ncia da IIDF: empresas n√£o s√£o vs startups</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>