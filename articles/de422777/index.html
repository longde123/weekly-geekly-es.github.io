<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧕🏽 😡 🎌 Eine einfache Einführung in ALU für neuronale Netze: Erklärung, physikalische Bedeutung und Implementierung 🗂️ 🐾 👨🏼‍💼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Kürzlich veröffentlichten Forscher von Google DeepMind, darunter ein bekannter Wissenschaftler für künstliche Intelligenz, Autor des Buches " Understa...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Eine einfache Einführung in ALU für neuronale Netze: Erklärung, physikalische Bedeutung und Implementierung</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/422777/">  Kürzlich veröffentlichten Forscher von Google DeepMind, darunter ein bekannter Wissenschaftler für künstliche Intelligenz, Autor des Buches " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Understanding Deep Learning</a> ", Andrew Trask, einen beeindruckenden Artikel, der ein neuronales Netzwerkmodell zur Extrapolation der Werte einfacher und komplexer numerischer Funktionen mit hoher Genauigkeit beschreibt. <br><br>  In diesem Beitrag werde ich die Architektur von <abbr title="Neuronale arithmetische Logikvorrichtung">NALU</abbr> (Neural <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Arithmetic Logic Devices</a> , NALU), ihre Komponenten und signifikante Unterschiede zu herkömmlichen neuronalen Netzen erläutern.  Das Hauptziel dieses Artikels ist es, <abbr title="Neuronale arithmetische Logikvorrichtung">NALU</abbr> (sowohl Implementierung als auch Idee) für Wissenschaftler, Programmierer und Studenten, die mit neuronalen Netzen und tiefem Lernen noch nicht <abbr title="Neuronale arithmetische Logikvorrichtung">vertraut</abbr> sind, einfach und intuitiv zu erklären. <br><br>  <b>Anmerkung des Autors</b> : Ich empfehle außerdem dringend, den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Originalartikel zu</a> lesen, um das Thema genauer zu studieren. <br><a name="habracut"></a><br><h2>  Wann sind neuronale Netze falsch? </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/6ba/260/98c/6ba26098c22a26f200ac6c7c9abce91d.jpg" alt="Klassisches neuronales Netzwerk"><br>  <i>Bild aus <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesem Artikel.</a></i> <br><br>  Theoretisch sollten neuronale Netze Funktionen gut approximieren.  Sie sind fast immer in der Lage, signifikante Übereinstimmungen zwischen Eingabedaten (Faktoren oder Merkmale) und Ausgaben (Beschriftungen oder Ziele) zu identifizieren.  Aus diesem Grund werden neuronale Netze in vielen Bereichen eingesetzt, von der Objekterkennung und ihrer Klassifizierung über die Übersetzung von Sprache in Text bis hin zur Implementierung von Spielalgorithmen, die Weltmeister schlagen können.  Es wurden bereits viele verschiedene Modelle erstellt: Faltungs- und wiederkehrende neuronale Netze, Autocodierer usw. Der Erfolg bei der Erstellung neuer Modelle für neuronale Netze und Deep Learning ist ein großes Thema für sich. <br><br>  Laut den Autoren des Artikels bewältigen neuronale Netze jedoch nicht immer Aufgaben, die für Menschen und sogar <abbr title="Link [7] aus dem Originalartikel: C. Randy Gallistel. Zahlen im Gehirn finden. Philosophische Transaktionen der Royal Society B, 373, 2017.">Bienen</abbr> offensichtlich erscheinen!  Dies ist beispielsweise ein mündlicher Bericht oder eine Operation mit Zahlen sowie die Fähigkeit, die Abhängigkeit von Beziehungen zu identifizieren.  Der Artikel zeigte, dass Standardmodelle neuronaler Netze nicht einmal mit der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">identischen Zuordnung</a> fertig werden können (eine Funktion, die ein Argument in sich selbst übersetzt). <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>x</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="8.846ex" height="2.66ex" viewBox="0 -832 3808.6 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-66" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-28" x="550" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-78" x="940" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-29" x="1512" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-3D" x="2179" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-78" x="3236" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi></math></span></span><script type="math/tex" id="MathJax-Element-1"> f (x) = x </script>  ) Ist die offensichtlichste numerische Beziehung.  Die folgende Abbildung zeigt die <abbr title="quadratischer Mittelwertfehler">MSE</abbr> verschiedener Modelle neuronaler Netze beim Lernen der Werte dieser Funktion. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f1e/e09/8e7/f1ee098e705e22745970e40e13782ef0.png" alt="Mittlerer quadratischer Fehler für Standard-Neuronale Netze"><br>  <i>Die Abbildung zeigt den mittleren quadratischen Fehler für neuronale Standardnetzwerke, die dieselbe Architektur und unterschiedliche (nichtlineare) Aktivierungsfunktionen in den inneren Schichten verwenden</i> <br><br><h2>  Warum sind neuronale Netze falsch? </h2><br>  Wie aus der Figur ersichtlich ist, ist der Hauptgrund für Fehlschläge die <b>Nichtlinearität der Aktivierungsfunktionen</b> auf den inneren Schichten des neuronalen Netzwerks.  Dieser Ansatz eignet sich hervorragend zum Bestimmen nichtlinearer Beziehungen zwischen Eingabedaten und Antworten, aber es ist schrecklich falsch, über die Daten hinauszugehen, auf denen das Netzwerk gelernt hat.  Neuronale Netze können sich also hervorragend <b>an eine</b> numerische Abhängigkeit von Trainingsdaten <b>erinnern</b> , können diese jedoch nicht extrapolieren. <br><br>  Dies ist so, als würde man eine Antwort oder ein Thema vor einer Prüfung stopfen, ohne das Thema zu verstehen.  Es ist leicht, den Test zu bestehen, wenn die Fragen den Hausaufgaben ähnlich sind, aber wenn es das Verständnis des zu testenden Themas ist und nicht die Fähigkeit, sich zu erinnern, werden wir scheitern. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/888/a65/e3b/888a65e3b7a49c678be14c5e2b16d6d4.jpg" alt="Harry Potter"><br>  <i>Dies war nicht im Kursprogramm!</i> <br><br>  Der Fehlergrad steht in direktem Zusammenhang mit dem Grad der Nichtlinearität der ausgewählten Aktivierungsfunktion.  Das vorige Diagramm zeigt deutlich, dass nichtlineare Funktionen mit harten Einschränkungen wie einem Sigmoid oder einer hyperbolischen Tangente ( <b>Tanh</b> ) die Aufgabe bewältigen können, die Abhängigkeiten viel schlimmer zu verallgemeinern als Funktionen mit weichen Einschränkungen, wie z. B. eine abgeschnittene lineare Transformation ( <b>ELU</b> , <b>PReLU</b> ). <br><br><h2>  Lösung: Neuronale Batterie (NAC) </h2><br>  Eine neuronale Batterie ( <abbr title="Neuronale Batterie">NAC</abbr> ) ist das Herzstück des <abbr title="Neuronale arithmetische Logikvorrichtung">NALU-</abbr> Modells.  Dies ist ein einfacher, aber effektiver Teil eines neuronalen Netzwerks, das mit <b>Addition und Subtraktion fertig wird</b> , was für die effiziente Berechnung linearer Beziehungen erforderlich ist. <br><br>  <abbr title="Neuronale Batterie">NAC</abbr> ist eine spezielle lineare Schicht eines neuronalen Netzwerks, für deren Gewicht eine einfache Bedingung auferlegt wird: Sie können nur 3 Werte annehmen - <b>1, 0 oder -1</b> .  Aufgrund solcher Einschränkungen kann die Batterie den Bereich der Eingabedaten nicht ändern und bleibt auf allen Ebenen des Netzwerks unabhängig von Anzahl und Verbindungen konstant.  Somit ist die Ausgabe eine <b>lineare Kombination der</b> Werte des Eingabevektors, was leicht eine Additions- und Subtraktionsoperation sein kann. <br><br>  <b>Laute Gedanken</b> : Zum besseren Verständnis dieser Aussage betrachten wir ein Beispiel für den Aufbau von Schichten eines neuronalen Netzwerks, die lineare arithmetische Operationen an Eingabedaten ausführen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/851/d6a/ac8/851d6aac8016eae4df4c594cfa2fb517.jpg" alt="Lineare Extrapolation im neuronalen Netz"><br>  <i>Die Abbildung zeigt, wie Schichten eines neuronalen Netzwerks ohne Hinzufügen einer Konstanten und mit möglichen Werten der Gewichte -1, 0 oder 1 eine lineare Extrapolation durchführen können</i> <br><br>  Wie oben im Bild der Schichten gezeigt, kann das neuronale Netzwerk lernen, die Werte solcher einfachen arithmetischen Funktionen wie Addition und Subtraktion zu extrapolieren ( <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>x</mi><mn>2</mn></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.862ex" height="2.178ex" viewBox="0 -676.4 5107.3 937.7" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-79" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-3D" x="775" y="0"></use><g transform="translate(1831,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-31" x="809" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-2B" x="3080" y="0"></use><g transform="translate(4080,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-32" x="809" y="-213"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>x</mi><mn>2</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-2"> y = x_1 + x_2 </script>  und <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>&amp;#x2212;</mo><msub><mi>x</mi><mn>2</mn></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.862ex" height="2.178ex" viewBox="0 -676.4 5107.3 937.7" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-79" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-3D" x="775" y="0"></use><g transform="translate(1831,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-31" x="809" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-2212" x="3080" y="0"></use><g transform="translate(4080,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-32" x="809" y="-213"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>−</mo><msub><mi>x</mi><mn>2</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-3"> y = x_1 - x_2 </script>  ) unter Verwendung der Beschränkungen der Gewichte mit möglichen Werten von 1, 0 und -1. <br><br>  <b>Hinweis: Die NAC-Schicht enthält in diesem Fall keinen freien Term (Konstante) und wendet keine nichtlinearen Transformationen auf die Daten an.</b> <br><br>  Da Standard-Neuronale Netze die Lösung des Problems unter ähnlichen Einschränkungen nicht bewältigen können, bieten die Autoren des Artikels eine sehr nützliche Formel zur Berechnung solcher Parameter durch klassische (unbegrenzte) Parameter <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>W</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.424ex" height="2.057ex" viewBox="0 -780.1 2766 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-57" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>W</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-4"> \ hat {W} </script>  und <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.431ex" height="2.057ex" viewBox="0 -780.1 2769 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-4D" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-5"> \ hat {M} </script>  .  Gewichtsdaten können wie alle Parameter neuronaler Netze zufällig initialisiert und während des Trainings des Netzes ausgewählt werden.  Formel zur Berechnung des Vektors <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.435ex" height="2.057ex" viewBox="0 -780.1 1048.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-57" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-6"> W </script>  durch <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-7-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>W</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.424ex" height="2.057ex" viewBox="0 -780.1 2766 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-57" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>W</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-7"> \ hat {W} </script>  und <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-8-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.431ex" height="2.057ex" viewBox="0 -780.1 2769 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-4D" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-8"> \ hat {M} </script>  sieht so aus: <br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-9-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>W</mi><mo>=</mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy=&quot;false&quot;>(</mo><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>W</mi></mrow><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mi>o</mi><mi>d</mi><mi>o</mi><mi>t</mi><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>a</mi><mo stretchy=&quot;false&quot;>(</mo><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi></mrow><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="38.562ex" height="2.66ex" viewBox="0 -832 16603.1 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-57" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-3D" x="1326" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="2382" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="2744" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6E" x="3273" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-68" x="3874" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-28" x="4450" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-68" x="5090" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="5666" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="6196" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-57" x="6557" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-29" x="7606" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6F" x="8245" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-64" x="8731" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6F" x="9254" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="9740" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-73" x="10351" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-69" x="10821" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-67" x="11166" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6D" x="11647" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="12525" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-28" x="13055" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-68" x="13694" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="14271" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="14800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-4D" x="15162" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-29" x="16213" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>W</mi><mo>=</mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>W</mi></mrow><mo stretchy="false">)</mo><mtext>&nbsp;</mtext><mi>o</mi><mi>d</mi><mi>o</mi><mi>t</mi><mtext>&nbsp;</mtext><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>a</mi><mo stretchy="false">(</mo><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-9"> W = tanh (\ hat {W}) \ odot \ sigma (\ hat {M}) </script></p>  <i>Die <a href="">Formel</a> verwendet das elementweise Matrixprodukt</i> <br><br>  Die Verwendung dieser Formel <b>stellt sicher,</b> dass der Bereich der W-Werte auf das Intervall [-1, 1] begrenzt ist, das näher an der Menge -1, 0, 1 liegt. Außerdem können Funktionen aus dieser Gleichung durch Gewichtsparameter unterschieden werden.  Somit wird es für unsere <abbr title="Neuronale Batterie">NAC-</abbr> Schicht einfacher sein, Werte zu lernen <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-10-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.435ex" height="2.057ex" viewBox="0 -780.1 1048.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-57" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-10"> W </script>  unter Verwendung von <b>Gradientenabstieg und Rückausbreitung von Fehlern</b> .  Das Folgende ist ein Diagramm der Architektur der <abbr title="Neuronale Batterie">NAC-</abbr> Schicht. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/eea/018/509/eea018509b6580a9364a7e7e91bff537.png" alt="Neuronale Batteriearchitektur"><br>  <i>Die Architektur einer neuronalen Batterie für das Training elementarer (linearer) arithmetischer Funktionen</i> <br><br><h2>  Python NAC-Implementierung mit Tensorflow </h2><br>  Wie wir bereits verstanden haben, ist <abbr title="Neuronale Batterie">NAC</abbr> ein ziemlich einfaches neuronales Netzwerk (Netzwerkschicht) mit kleinen Merkmalen.  Das Folgende ist eine Implementierung einer <abbr title="Neuronale Batterie">NAC-</abbr> Schicht in Python unter Verwendung der Bibliotheken Tensoflow und NumPy. <br><br><div class="spoiler">  <b class="spoiler_title">Python-Code</b> <div class="spoiler_text"><pre><code class="hljs haskell"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf #    (<span class="hljs-type"><span class="hljs-type">NAC</span></span>)  / # -&gt;     / def nac_simple_single_layer(<span class="hljs-title"><span class="hljs-title">x_in</span></span>, <span class="hljs-title"><span class="hljs-title">out_units</span></span>): '''  : x_in -&gt;   X out_units -&gt;     : y_out -&gt;     W -&gt;     ''' #       in_features = x_in.shape[1] #  W_hat  M_hat W_hat = tf.get_variable(<span class="hljs-title"><span class="hljs-title">shape</span></span>=[<span class="hljs-title"><span class="hljs-title">in_shape</span></span>, <span class="hljs-title"><span class="hljs-title">out_units</span></span>], <span class="hljs-title"><span class="hljs-title">initializer</span></span>=<span class="hljs-title"><span class="hljs-title">tf</span></span>.<span class="hljs-title"><span class="hljs-title">initializers</span></span>.<span class="hljs-title"><span class="hljs-title">random_uniform</span></span>(<span class="hljs-title"><span class="hljs-title">minval</span></span>=-2, <span class="hljs-title"><span class="hljs-title">maxval</span></span>=2), trainable=True, name='W_hat') M_hat = tf.get_variable(<span class="hljs-title"><span class="hljs-title">shape</span></span>=[<span class="hljs-title"><span class="hljs-title">in_shape</span></span>, <span class="hljs-title"><span class="hljs-title">out_units</span></span>], <span class="hljs-title"><span class="hljs-title">initializer</span></span>=<span class="hljs-title"><span class="hljs-title">tf</span></span>.<span class="hljs-title"><span class="hljs-title">initializers</span></span>.<span class="hljs-title"><span class="hljs-title">random_uniform</span></span>(<span class="hljs-title"><span class="hljs-title">minval</span></span>=-2, <span class="hljs-title"><span class="hljs-title">maxval</span></span>=2), trainable=True, name='M_hat') #  W   W = tf.nn.tanh(<span class="hljs-type"><span class="hljs-type">W_hat</span></span>) * tf.nn.sigmoid(<span class="hljs-type"><span class="hljs-type">M_hat</span></span>) y_out = tf.matmul(<span class="hljs-title"><span class="hljs-title">x_in</span></span>, <span class="hljs-type"><span class="hljs-type">W</span></span>) return y_out, W</code> </pre> </div></div><br>  Im obigen Code <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-11-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>W</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.424ex" height="2.057ex" viewBox="0 -780.1 2766 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-57" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>W</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-11"> \ hat {W} </script>  und <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-12-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.431ex" height="2.057ex" viewBox="0 -780.1 2769 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-4D" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-12"> \ hat {M} </script>  werden unter Verwendung einer gleichmäßigen Verteilung initialisiert, Sie können jedoch <b>jede</b> empfohlene Methode zum Generieren einer anfänglichen Näherung für diese Parameter verwenden.  Sie können die Vollversion des Codes in meinem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GitHub-Repository sehen</a> (der Link wird am Ende des Beitrags dupliziert). <br><br><h2>  Weiter geht's: Von Addition und Subtraktion zu NAC für komplexe arithmetische Ausdrücke </h2><br>  Obwohl das oben beschriebene Modell eines einfachen neuronalen Netzwerks mit den einfachsten Operationen wie Addition und Subtraktion zurechtkommt, müssen wir aus den vielen Bedeutungen komplexerer Funktionen wie Multiplikation, Division und Exponentiation lernen können. <br><br>  Im Folgenden finden Sie die modifizierte <abbr title="Neuronale Batterie">NAC-</abbr> Architektur, die für die Auswahl <b>komplexerer arithmetischer Operationen</b> durch <b>Logarithmus und die Aufnahme des Exponenten</b> in das Modell angepasst ist.  Beachten Sie die Unterschiede zwischen dieser <abbr title="Neuronale Batterie">NAC-</abbr> Implementierung und der bereits oben diskutierten. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d75/2aa/629/d752aa629eae69110dc33e828cd98430.png" alt="Bild"><br>  <i><abbr title="Neuronale Batterie">NAC-</abbr> Architektur für komplexere arithmetische Operationen</i> <br><br>  Wie aus der Abbildung ersichtlich ist, logarithmieren wir die Eingabedaten, bevor wir sie mit der Gewichtungsmatrix multiplizieren, und berechnen dann den Exponenten des Ergebnisses.  Die Formel für die Berechnungen lautet wie folgt: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-13-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>Y</mi><mo>=</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>W</mi><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>u</mi><mi>l</mi><mi>l</mi><mi>e</mi><mi>t</mi><mo stretchy=&quot;false&quot;>(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mo>+</mo><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="38.878ex" height="2.66ex" viewBox="0 -832 16739 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-59" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-3D" x="1041" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-65" x="2097" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-78" x="2564" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-70" x="3136" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-28" x="3640" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-57" x="4029" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-62" x="5328" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-75" x="5757" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6C" x="6330" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6C" x="6628" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-65" x="6927" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="7393" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-28" x="7755" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6C" x="8144" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6F" x="8443" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-67" x="8928" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-28" x="9409" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-7C" x="9798" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-78" x="10077" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-7C" x="10649" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-2B" x="11150" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-65" x="12401" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-70" x="12867" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-73" x="13371" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-69" x="13840" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6C" x="14186" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6F" x="14484" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6E" x="14970" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-29" x="15570" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-29" x="15960" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-29" x="16349" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>Y</mi><mo>=</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi>W</mi><mtext>&nbsp;</mtext><mi>b</mi><mi>u</mi><mi>l</mi><mi>l</mi><mi>e</mi><mi>t</mi><mo stretchy="false">(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mo>+</mo><mtext>&nbsp;</mtext><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-13"> Y = exp (W \ bullet (log (| x | + \ epsilon))) </script></p>  <i><a href="">Die Ausgabeformel</a> für die zweite Version von <abbr title="Neuronale Batterie">NAC</abbr> .</i> <math> </math><i><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-14-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="7.942ex" height="2.419ex" viewBox="0 -780.1 3419.5 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-65" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-70" x="716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-73" x="1220" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-69" x="1689" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6C" x="2035" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6F" x="2333" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6E" x="2819" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-14"> \ epsilon </script></i>   <i>Hier ist eine sehr kleine Zahl, um Situationen wie log (0) während des Trainings zu vermeiden</i> <br><br>  Somit gilt für beide <abbr title="Neuronale Batterie">NAC-</abbr> Modelle das Funktionsprinzip, einschließlich der Berechnung der Gewichtsmatrix mit Einschränkungen <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-15-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.435ex" height="2.057ex" viewBox="0 -780.1 1048.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-57" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-15"> W </script>  durch <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-16-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>W</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.424ex" height="2.057ex" viewBox="0 -780.1 2766 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-57" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>W</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-16"> \ hat {W} </script>  und <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-17-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.431ex" height="2.057ex" viewBox="0 -780.1 2769 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-4D" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-17"> \ hat {M} </script>  ändert sich nicht.  Der einzige Unterschied ist die Verwendung logarithmischer Operationen für die Eingabe und Ausgabe im zweiten Fall. <br><br><h2>  Zweite NAC-Version in Python mit Tensorflow </h2><br>  Der Code wird sich wie die Architektur kaum ändern, mit Ausnahme der angegebenen Verbesserungen bei der Berechnung des Tensors der Ausgabewerte. <br><br><div class="spoiler">  <b class="spoiler_title">Python-Code</b> <div class="spoiler_text"><pre> <code class="hljs pgsql">#    (NAC)     # -&gt;      ,   ,      def nac_complex_single_layer(x_in, out_units, epsilon=<span class="hljs-number"><span class="hljs-number">0.000001</span></span>): <span class="hljs-string"><span class="hljs-string">''' :param x_in:   X :param out_units:    :param epsilon:    (,    log(0)   ) :return m:     :return W:     '''</span></span> in_features = x_in.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>] W_hat = tf.get_variable(shape=[in_shape, out_units], initializer=tf.initializers.random_uniform(minval=<span class="hljs-number"><span class="hljs-number">-2</span></span>, maxval=<span class="hljs-number"><span class="hljs-number">2</span></span>), trainable=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, <span class="hljs-type"><span class="hljs-type">name</span></span>="W_hat") M_hat = tf.get_variable(shape=[in_shape, out_units], initializer=tf.initializers.random_uniform(minval=<span class="hljs-number"><span class="hljs-number">-2</span></span>, maxval=<span class="hljs-number"><span class="hljs-number">2</span></span>), trainable=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, <span class="hljs-type"><span class="hljs-type">name</span></span>="M_hat") #  W   W = tf.nn.tanh(W_hat) * tf.nn.sigmoid(M_hat) #          x_modified = tf.log(tf.abs(x_in) + epsilon) m = tf.exp(tf.matmul(x_modified, W)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> m, W</code> </pre></div></div><br><br>  Ich erinnere Sie noch einmal daran, dass sich die Vollversion des Codes in meinem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GitHub-Repository befindet</a> (der Link wird am Ende des Beitrags dupliziert). <br><br><h2>  Alles zusammen: eine neuronale arithmetische Logikeinheit (NALU) </h2><br>  Wie viele bereits vermutet haben, können wir aus fast jeder arithmetischen Operation lernen, indem wir die beiden oben diskutierten Modelle kombinieren.  Dies ist die <b>Hauptidee von</b> <abbr title="Neuronale arithmetische Logikvorrichtung">NALU</abbr> , die eine <b>gewichtete Kombination aus</b> elementarem und komplexem <abbr title="Neuronale Batterie">NAC umfasst</abbr> , die über ein Trainingssignal gesteuert wird.  Daher sind <abbr title="Neuronale Batterie">NACs</abbr> die Bausteine ​​für die <abbr title="Neuronale arithmetische Logikvorrichtung">Erstellung von NALUs</abbr> . Wenn Sie deren Design verstehen, ist die <abbr title="Neuronale arithmetische Logikvorrichtung">Erstellung von NALUs</abbr> einfach.  Wenn Sie noch Fragen haben, lesen Sie die Erklärungen für beide <abbr title="Neuronale Batterie">NAC-</abbr> Modelle erneut.  Unten sehen Sie ein Diagramm mit der <abbr title="Neuronale arithmetische Logikvorrichtung">NALU-</abbr> Architektur. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ad9/8e3/2ae/ad98e32aea75b4bb4f020338e9b87bf8.png" alt="Bild"><br>  <i><abbr title="Neuronale arithmetische Logikvorrichtung">NALU-</abbr> Architekturdiagramm mit Erläuterungen</i> <br><br>  Wie aus der obigen Abbildung ersichtlich ist, werden beide <abbr title="Neuronale Batterie">NAC-</abbr> Einheiten (lila Blöcke) innerhalb der <abbr title="Neuronale arithmetische Logikvorrichtung">NALU</abbr> durch das Trainingssignal-Sigmoid (orangefarbener Block) interpoliert (kombiniert).  Auf diese Weise können Sie die Ausgabe von jedem von ihnen abhängig von der arithmetischen Funktion, deren Werte wir zu finden versuchen, (de) aktivieren. <br><br>  Wie oben erwähnt, ist die <abbr title="Neuronale Batterie">NAC-</abbr> Elementareinheit eine Akkumulationsfunktion, die es der <abbr title="Neuronale arithmetische Logikvorrichtung">NALU</abbr> ermöglicht, elementare lineare Operationen (Addition und Subtraktion) auszuführen, während die komplexe NAC-Einheit für Multiplikation, Division und Exponentiation verantwortlich ist.  <b>Die Ausgabe</b> in <abbr title="Neuronale arithmetische Logikvorrichtung">NALU</abbr> kann als Formel dargestellt werden: <br><br><div class="spoiler">  <b class="spoiler_title">Pseudocode</b> <div class="spoiler_text"><pre> <code class="hljs perl">Simple NAC : a = WX Complex NAC: <span class="hljs-keyword"><span class="hljs-keyword">m</span></span> = <span class="hljs-keyword"><span class="hljs-keyword">exp</span></span>(W <span class="hljs-keyword"><span class="hljs-keyword">log</span></span>(|X| + e))  W = tanh(W_hat) * sigmoid(M_hat) <span class="hljs-comment"><span class="hljs-comment">#  G -       : g = sigmoid(GX) # , ,   NALU #  *      NALU: y = g * a + (1 - g) * m</span></span></code> </pre></div></div><br>  Aus der <abbr title="Neuronale arithmetische Logikvorrichtung">obigen NALU-</abbr> Formel können wir das mit schließen <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-18-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>g</mi><mo>=</mo><mn>0</mn></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="5.377ex" height="2.298ex" viewBox="0 -728.2 2315.1 989.6" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-67" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-3D" x="758" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-30" x="1814" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mo>=</mo><mn>0</mn></math></span></span><script type="math/tex" id="MathJax-Element-18"> g = 0 </script>  Das neuronale Netzwerk wählt nur Werte für komplexe arithmetische Operationen aus, nicht jedoch für elementare.  und umgekehrt - im Fall von <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-19-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>g</mi><mo>=</mo><mn>1</mn></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="5.377ex" height="2.298ex" viewBox="0 -728.2 2315.1 989.6" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-67" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-3D" x="758" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-31" x="1814" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mo>=</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-19"> g = 1 </script>  .  Somit kann <abbr title="Neuronale arithmetische Logikvorrichtung">NALU</abbr> im Allgemeinen jede arithmetische Operation lernen, die aus Addition, Subtraktion, Multiplikation, Division und Erhöhen zu einer Potenz besteht, und das Ergebnis erfolgreich über die Grenzen der Intervalle der Werte der Quelldaten hinaus extrapolieren. <br><br><h2>  Python NALU-Implementierung mit Tensorflow </h2><br>  Bei der Implementierung von <abbr title="Neuronale arithmetische Logikvorrichtung">NALU werden</abbr> wir das elementare und komplexe <abbr title="Neuronale Batterie">NAC verwenden</abbr> , das wir bereits definiert haben. <br><br><div class="spoiler">  <b class="spoiler_title">Python-Code</b> <div class="spoiler_text"><pre> <code class="hljs python"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">nalu</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x_in, out_units, epsilon=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.000001</span></span></span></span><span class="hljs-function"><span class="hljs-params">, get_weights=False)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' :param x_in:   X :param out_units:    :param epsilon:    (,    log(0)   ) :param get_weights:   True      :return y_out:     :return G: o   :return W_simple:    NAC1 ( NAC) :return W_complex:    NAC2 ( NAC) '''</span></span> in_features = x_in.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-comment"><span class="hljs-comment">#     NAC a, W_simple = nac_simple_single_layer(x_in, out_units) #     NAC m, W_complex = nac_complex_single_layer(x_in, out_units, epsilon=epsilon) #    G = tf.get_variable(shape=[in_shape, out_units], initializer=tf.random_normal_initializer(stddev=1.0), trainable=True, name="Gate_weights") g = tf.nn.sigmoid(tf.matmul(x_in, G)) y_out = g * a + (1 - g) * m if(get_weights): return y_out, G, W_simple, W_complex else: return y_out</span></span></code> </pre></div></div><br>  Ich stelle erneut fest, dass ich im obigen Code die Parametermatrix erneut initialisiert habe <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-20-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.827ex" height="2.057ex" viewBox="0 -780.1 786.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-47" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi></math></span></span><script type="math/tex" id="MathJax-Element-20"> G </script>  Verwenden Sie eine gleichmäßige Verteilung, aber Sie können <b>jede</b> empfohlene Methode verwenden, um eine anfängliche Annäherung zu generieren. <br><hr><br><h2>  Zusammenfassung </h2><br>  Für mich persönlich ist die Idee von <abbr title="Neuronale arithmetische Logikvorrichtung">NALU</abbr> ein großer Durchbruch auf dem Gebiet der KI, insbesondere in neuronalen Netzen, und sie sieht vielversprechend aus.  Dieser Ansatz kann die Tür zu den Anwendungsbereichen öffnen, in denen Standard-Neuronale Netze nicht zurechtkommen könnten. <br><br>  Die Autoren des Artikels sprechen über verschiedene Experimente mit <abbr title="Neuronale arithmetische Logikvorrichtung">NALU</abbr> : von der Auswahl der Werte elementarer arithmetischer Funktionen bis zur Zählung der Anzahl handgeschriebener Ziffern in einer bestimmten Reihe von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MNIST-</a> Bildern, mit denen neuronale Netze Computerprogramme überprüfen können! <br><br>  Die Ergebnisse machen einen beeindruckenden Eindruck und beweisen, dass <abbr title="Neuronale arithmetische Logikvorrichtung">NALU</abbr> <b>fast alle Aufgaben im</b> Zusammenhang mit der numerischen Darstellung besser bewältigt als Standardmodelle neuronaler Netze.  Ich ermutige die Leser, sich mit den Ergebnissen von Experimenten vertraut zu machen, um besser zu verstehen, wie und wo das <abbr title="Neuronale arithmetische Logikvorrichtung">NALU-</abbr> Modell nützlich sein kann. <br><br>  Es muss jedoch beachtet werden, dass weder <abbr title="Neuronale Batterie">NAC</abbr> noch <abbr title="Neuronale arithmetische Logikvorrichtung">NALU</abbr> die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ideale Lösung</a> für eine Aufgabe sind.  Sie repräsentieren vielmehr die allgemeine Idee, wie Modelle für eine bestimmte Klasse von arithmetischen Operationen erstellt werden können. <br><hr><br>  Unten finden Sie einen Link zu meinem GitHub-Repository, das die vollständige Implementierung des Codes aus dem Artikel enthält. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">github.com/faizan2786/nalu_implementation</a> <br><br>  Sie können die Funktionsweise meines Modells für verschiedene Funktionen unabhängig überprüfen, indem Sie Hyperparameter für ein neuronales Netzwerk auswählen.  Bitte stellen Sie Fragen und teilen Sie Ihre Gedanken in den Kommentaren unter diesem Beitrag, und ich werde mein Bestes tun, um Ihnen zu antworten. <br><br>  <b>PS (vom Autor): Dies ist mein erster schriftlicher Beitrag. Wenn Sie also Tipps, Vorschläge und Empfehlungen für die Zukunft haben (sowohl technische als auch allgemeine), schreiben Sie mir bitte.</b> <br><br>  PPS (vom Übersetzer): Wenn Sie Kommentare zur Übersetzung oder zum Text haben, schreiben Sie mir bitte eine persönliche Nachricht.  Ich interessiere mich besonders für den Wortlaut des gelernten Gate-Signals - ich bin mir nicht sicher, ob ich diesen Begriff genau übersetzen kann. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de422777/">https://habr.com/ru/post/de422777/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de422767/index.html">DEFCON-Konferenz 16. Fedor, InSecure.org Hacker. NMAP Scan Online</a></li>
<li><a href="../de422769/index.html">Gewinner des Startup Battlefield TechCrunch Disrupt San Francisco 2018</a></li>
<li><a href="../de422771/index.html">Die Regeln des Designs, das Erreichen einer neuen Ebene und das Designdenken</a></li>
<li><a href="../de422773/index.html">NVIDIA. Enthüllung der Geheimnisse der Turing-GPU-Architektur der nächsten Generation: Double Ray Tracing, GDDR6 und mehr</a></li>
<li><a href="../de422775/index.html">DEFCON-Konferenz 22. Andrew "Zoz" Brooks. Nicht vermasseln! Teil 1</a></li>
<li><a href="../de422781/index.html">Fintech Digest: SWIFT wird weiterhin in der Russischen Föderation arbeiten. Mit VISA können Sie Geld per Telefonnummer und teuren biometrischen Daten überweisen</a></li>
<li><a href="../de422783/index.html">Besser, schneller, leistungsfähiger: Styled-Components v4</a></li>
<li><a href="../de422785/index.html">Werksdigitalisierung: Ein Blick nach vorne</a></li>
<li><a href="../de422787/index.html">Wesentliche Änderungen in führenden Chip-Architekturen</a></li>
<li><a href="../de422789/index.html">@ Pythonetc Aug 2018</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>