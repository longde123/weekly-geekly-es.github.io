<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßïüèΩ üò° üéå Eine einfache Einf√ºhrung in ALU f√ºr neuronale Netze: Erkl√§rung, physikalische Bedeutung und Implementierung üóÇÔ∏è üêæ üë®üèº‚Äçüíº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="K√ºrzlich ver√∂ffentlichten Forscher von Google DeepMind, darunter ein bekannter Wissenschaftler f√ºr k√ºnstliche Intelligenz, Autor des Buches " Understa...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Eine einfache Einf√ºhrung in ALU f√ºr neuronale Netze: Erkl√§rung, physikalische Bedeutung und Implementierung</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/422777/">  K√ºrzlich ver√∂ffentlichten Forscher von Google DeepMind, darunter ein bekannter Wissenschaftler f√ºr k√ºnstliche Intelligenz, Autor des Buches " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Understanding Deep Learning</a> ", Andrew Trask, einen beeindruckenden Artikel, der ein neuronales Netzwerkmodell zur Extrapolation der Werte einfacher und komplexer numerischer Funktionen mit hoher Genauigkeit beschreibt. <br><br>  In diesem Beitrag werde ich die Architektur von <abbr title="Neuronale arithmetische Logikvorrichtung">NALU</abbr> (Neural <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Arithmetic Logic Devices</a> , NALU), ihre Komponenten und signifikante Unterschiede zu herk√∂mmlichen neuronalen Netzen erl√§utern.  Das Hauptziel dieses Artikels ist es, <abbr title="Neuronale arithmetische Logikvorrichtung">NALU</abbr> (sowohl Implementierung als auch Idee) f√ºr Wissenschaftler, Programmierer und Studenten, die mit neuronalen Netzen und tiefem Lernen noch nicht <abbr title="Neuronale arithmetische Logikvorrichtung">vertraut</abbr> sind, einfach und intuitiv zu erkl√§ren. <br><br>  <b>Anmerkung des Autors</b> : Ich empfehle au√üerdem dringend, den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Originalartikel zu</a> lesen, um das Thema genauer zu studieren. <br><a name="habracut"></a><br><h2>  Wann sind neuronale Netze falsch? </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/6ba/260/98c/6ba26098c22a26f200ac6c7c9abce91d.jpg" alt="Klassisches neuronales Netzwerk"><br>  <i>Bild aus <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesem Artikel.</a></i> <br><br>  Theoretisch sollten neuronale Netze Funktionen gut approximieren.  Sie sind fast immer in der Lage, signifikante √úbereinstimmungen zwischen Eingabedaten (Faktoren oder Merkmale) und Ausgaben (Beschriftungen oder Ziele) zu identifizieren.  Aus diesem Grund werden neuronale Netze in vielen Bereichen eingesetzt, von der Objekterkennung und ihrer Klassifizierung √ºber die √úbersetzung von Sprache in Text bis hin zur Implementierung von Spielalgorithmen, die Weltmeister schlagen k√∂nnen.  Es wurden bereits viele verschiedene Modelle erstellt: Faltungs- und wiederkehrende neuronale Netze, Autocodierer usw. Der Erfolg bei der Erstellung neuer Modelle f√ºr neuronale Netze und Deep Learning ist ein gro√ües Thema f√ºr sich. <br><br>  Laut den Autoren des Artikels bew√§ltigen neuronale Netze jedoch nicht immer Aufgaben, die f√ºr Menschen und sogar <abbr title="Link [7] aus dem Originalartikel: C. Randy Gallistel. Zahlen im Gehirn finden. Philosophische Transaktionen der Royal Society B, 373, 2017.">Bienen</abbr> offensichtlich erscheinen!  Dies ist beispielsweise ein m√ºndlicher Bericht oder eine Operation mit Zahlen sowie die F√§higkeit, die Abh√§ngigkeit von Beziehungen zu identifizieren.  Der Artikel zeigte, dass Standardmodelle neuronaler Netze nicht einmal mit der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">identischen Zuordnung</a> fertig werden k√∂nnen (eine Funktion, die ein Argument in sich selbst √ºbersetzt). <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>x</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="8.846ex" height="2.66ex" viewBox="0 -832 3808.6 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-66" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-28" x="550" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-78" x="940" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-29" x="1512" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-3D" x="2179" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-78" x="3236" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi></math></span></span><script type="math/tex" id="MathJax-Element-1"> f (x) = x </script>  ) Ist die offensichtlichste numerische Beziehung.  Die folgende Abbildung zeigt die <abbr title="quadratischer Mittelwertfehler">MSE</abbr> verschiedener Modelle neuronaler Netze beim Lernen der Werte dieser Funktion. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f1e/e09/8e7/f1ee098e705e22745970e40e13782ef0.png" alt="Mittlerer quadratischer Fehler f√ºr Standard-Neuronale Netze"><br>  <i>Die Abbildung zeigt den mittleren quadratischen Fehler f√ºr neuronale Standardnetzwerke, die dieselbe Architektur und unterschiedliche (nichtlineare) Aktivierungsfunktionen in den inneren Schichten verwenden</i> <br><br><h2>  Warum sind neuronale Netze falsch? </h2><br>  Wie aus der Figur ersichtlich ist, ist der Hauptgrund f√ºr Fehlschl√§ge die <b>Nichtlinearit√§t der Aktivierungsfunktionen</b> auf den inneren Schichten des neuronalen Netzwerks.  Dieser Ansatz eignet sich hervorragend zum Bestimmen nichtlinearer Beziehungen zwischen Eingabedaten und Antworten, aber es ist schrecklich falsch, √ºber die Daten hinauszugehen, auf denen das Netzwerk gelernt hat.  Neuronale Netze k√∂nnen sich also hervorragend <b>an eine</b> numerische Abh√§ngigkeit von Trainingsdaten <b>erinnern</b> , k√∂nnen diese jedoch nicht extrapolieren. <br><br>  Dies ist so, als w√ºrde man eine Antwort oder ein Thema vor einer Pr√ºfung stopfen, ohne das Thema zu verstehen.  Es ist leicht, den Test zu bestehen, wenn die Fragen den Hausaufgaben √§hnlich sind, aber wenn es das Verst√§ndnis des zu testenden Themas ist und nicht die F√§higkeit, sich zu erinnern, werden wir scheitern. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/888/a65/e3b/888a65e3b7a49c678be14c5e2b16d6d4.jpg" alt="Harry Potter"><br>  <i>Dies war nicht im Kursprogramm!</i> <br><br>  Der Fehlergrad steht in direktem Zusammenhang mit dem Grad der Nichtlinearit√§t der ausgew√§hlten Aktivierungsfunktion.  Das vorige Diagramm zeigt deutlich, dass nichtlineare Funktionen mit harten Einschr√§nkungen wie einem Sigmoid oder einer hyperbolischen Tangente ( <b>Tanh</b> ) die Aufgabe bew√§ltigen k√∂nnen, die Abh√§ngigkeiten viel schlimmer zu verallgemeinern als Funktionen mit weichen Einschr√§nkungen, wie z. B. eine abgeschnittene lineare Transformation ( <b>ELU</b> , <b>PReLU</b> ). <br><br><h2>  L√∂sung: Neuronale Batterie (NAC) </h2><br>  Eine neuronale Batterie ( <abbr title="Neuronale Batterie">NAC</abbr> ) ist das Herzst√ºck des <abbr title="Neuronale arithmetische Logikvorrichtung">NALU-</abbr> Modells.  Dies ist ein einfacher, aber effektiver Teil eines neuronalen Netzwerks, das mit <b>Addition und Subtraktion fertig wird</b> , was f√ºr die effiziente Berechnung linearer Beziehungen erforderlich ist. <br><br>  <abbr title="Neuronale Batterie">NAC</abbr> ist eine spezielle lineare Schicht eines neuronalen Netzwerks, f√ºr deren Gewicht eine einfache Bedingung auferlegt wird: Sie k√∂nnen nur 3 Werte annehmen - <b>1, 0 oder -1</b> .  Aufgrund solcher Einschr√§nkungen kann die Batterie den Bereich der Eingabedaten nicht √§ndern und bleibt auf allen Ebenen des Netzwerks unabh√§ngig von Anzahl und Verbindungen konstant.  Somit ist die Ausgabe eine <b>lineare Kombination der</b> Werte des Eingabevektors, was leicht eine Additions- und Subtraktionsoperation sein kann. <br><br>  <b>Laute Gedanken</b> : Zum besseren Verst√§ndnis dieser Aussage betrachten wir ein Beispiel f√ºr den Aufbau von Schichten eines neuronalen Netzwerks, die lineare arithmetische Operationen an Eingabedaten ausf√ºhren. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/851/d6a/ac8/851d6aac8016eae4df4c594cfa2fb517.jpg" alt="Lineare Extrapolation im neuronalen Netz"><br>  <i>Die Abbildung zeigt, wie Schichten eines neuronalen Netzwerks ohne Hinzuf√ºgen einer Konstanten und mit m√∂glichen Werten der Gewichte -1, 0 oder 1 eine lineare Extrapolation durchf√ºhren k√∂nnen</i> <br><br>  Wie oben im Bild der Schichten gezeigt, kann das neuronale Netzwerk lernen, die Werte solcher einfachen arithmetischen Funktionen wie Addition und Subtraktion zu extrapolieren ( <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>x</mi><mn>2</mn></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.862ex" height="2.178ex" viewBox="0 -676.4 5107.3 937.7" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-79" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-3D" x="775" y="0"></use><g transform="translate(1831,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-31" x="809" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-2B" x="3080" y="0"></use><g transform="translate(4080,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-32" x="809" y="-213"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>x</mi><mn>2</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-2"> y = x_1 + x_2 </script>  und <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>&amp;#x2212;</mo><msub><mi>x</mi><mn>2</mn></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.862ex" height="2.178ex" viewBox="0 -676.4 5107.3 937.7" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-79" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-3D" x="775" y="0"></use><g transform="translate(1831,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-31" x="809" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-2212" x="3080" y="0"></use><g transform="translate(4080,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-32" x="809" y="-213"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>‚àí</mo><msub><mi>x</mi><mn>2</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-3"> y = x_1 - x_2 </script>  ) unter Verwendung der Beschr√§nkungen der Gewichte mit m√∂glichen Werten von 1, 0 und -1. <br><br>  <b>Hinweis: Die NAC-Schicht enth√§lt in diesem Fall keinen freien Term (Konstante) und wendet keine nichtlinearen Transformationen auf die Daten an.</b> <br><br>  Da Standard-Neuronale Netze die L√∂sung des Problems unter √§hnlichen Einschr√§nkungen nicht bew√§ltigen k√∂nnen, bieten die Autoren des Artikels eine sehr n√ºtzliche Formel zur Berechnung solcher Parameter durch klassische (unbegrenzte) Parameter <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>W</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.424ex" height="2.057ex" viewBox="0 -780.1 2766 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-57" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>W</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-4"> \ hat {W} </script>  und <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.431ex" height="2.057ex" viewBox="0 -780.1 2769 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-4D" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-5"> \ hat {M} </script>  .  Gewichtsdaten k√∂nnen wie alle Parameter neuronaler Netze zuf√§llig initialisiert und w√§hrend des Trainings des Netzes ausgew√§hlt werden.  Formel zur Berechnung des Vektors <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.435ex" height="2.057ex" viewBox="0 -780.1 1048.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-57" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-6"> W </script>  durch <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-7-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>W</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.424ex" height="2.057ex" viewBox="0 -780.1 2766 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-57" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>W</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-7"> \ hat {W} </script>  und <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-8-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.431ex" height="2.057ex" viewBox="0 -780.1 2769 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-4D" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-8"> \ hat {M} </script>  sieht so aus: <br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-9-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>W</mi><mo>=</mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy=&quot;false&quot;>(</mo><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>W</mi></mrow><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mi>o</mi><mi>d</mi><mi>o</mi><mi>t</mi><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>a</mi><mo stretchy=&quot;false&quot;>(</mo><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi></mrow><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="38.562ex" height="2.66ex" viewBox="0 -832 16603.1 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-57" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-3D" x="1326" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="2382" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="2744" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6E" x="3273" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-68" x="3874" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-28" x="4450" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-68" x="5090" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="5666" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="6196" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-57" x="6557" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-29" x="7606" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6F" x="8245" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-64" x="8731" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6F" x="9254" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="9740" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-73" x="10351" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-69" x="10821" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-67" x="11166" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6D" x="11647" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="12525" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-28" x="13055" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-68" x="13694" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="14271" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="14800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-4D" x="15162" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-29" x="16213" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>W</mi><mo>=</mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>W</mi></mrow><mo stretchy="false">)</mo><mtext>&nbsp;</mtext><mi>o</mi><mi>d</mi><mi>o</mi><mi>t</mi><mtext>&nbsp;</mtext><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>a</mi><mo stretchy="false">(</mo><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-9"> W = tanh (\ hat {W}) \ odot \ sigma (\ hat {M}) </script></p>  <i>Die <a href="">Formel</a> verwendet das elementweise Matrixprodukt</i> <br><br>  Die Verwendung dieser Formel <b>stellt sicher,</b> dass der Bereich der W-Werte auf das Intervall [-1, 1] begrenzt ist, das n√§her an der Menge -1, 0, 1 liegt. Au√üerdem k√∂nnen Funktionen aus dieser Gleichung durch Gewichtsparameter unterschieden werden.  Somit wird es f√ºr unsere <abbr title="Neuronale Batterie">NAC-</abbr> Schicht einfacher sein, Werte zu lernen <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-10-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.435ex" height="2.057ex" viewBox="0 -780.1 1048.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-57" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-10"> W </script>  unter Verwendung von <b>Gradientenabstieg und R√ºckausbreitung von Fehlern</b> .  Das Folgende ist ein Diagramm der Architektur der <abbr title="Neuronale Batterie">NAC-</abbr> Schicht. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/eea/018/509/eea018509b6580a9364a7e7e91bff537.png" alt="Neuronale Batteriearchitektur"><br>  <i>Die Architektur einer neuronalen Batterie f√ºr das Training elementarer (linearer) arithmetischer Funktionen</i> <br><br><h2>  Python NAC-Implementierung mit Tensorflow </h2><br>  Wie wir bereits verstanden haben, ist <abbr title="Neuronale Batterie">NAC</abbr> ein ziemlich einfaches neuronales Netzwerk (Netzwerkschicht) mit kleinen Merkmalen.  Das Folgende ist eine Implementierung einer <abbr title="Neuronale Batterie">NAC-</abbr> Schicht in Python unter Verwendung der Bibliotheken Tensoflow und NumPy. <br><br><div class="spoiler">  <b class="spoiler_title">Python-Code</b> <div class="spoiler_text"><pre><code class="hljs haskell"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf #    (<span class="hljs-type"><span class="hljs-type">NAC</span></span>)  / # -&gt;     / def nac_simple_single_layer(<span class="hljs-title"><span class="hljs-title">x_in</span></span>, <span class="hljs-title"><span class="hljs-title">out_units</span></span>): '''  : x_in -&gt;   X out_units -&gt;     : y_out -&gt;     W -&gt;     ''' #       in_features = x_in.shape[1] #  W_hat  M_hat W_hat = tf.get_variable(<span class="hljs-title"><span class="hljs-title">shape</span></span>=[<span class="hljs-title"><span class="hljs-title">in_shape</span></span>, <span class="hljs-title"><span class="hljs-title">out_units</span></span>], <span class="hljs-title"><span class="hljs-title">initializer</span></span>=<span class="hljs-title"><span class="hljs-title">tf</span></span>.<span class="hljs-title"><span class="hljs-title">initializers</span></span>.<span class="hljs-title"><span class="hljs-title">random_uniform</span></span>(<span class="hljs-title"><span class="hljs-title">minval</span></span>=-2, <span class="hljs-title"><span class="hljs-title">maxval</span></span>=2), trainable=True, name='W_hat') M_hat = tf.get_variable(<span class="hljs-title"><span class="hljs-title">shape</span></span>=[<span class="hljs-title"><span class="hljs-title">in_shape</span></span>, <span class="hljs-title"><span class="hljs-title">out_units</span></span>], <span class="hljs-title"><span class="hljs-title">initializer</span></span>=<span class="hljs-title"><span class="hljs-title">tf</span></span>.<span class="hljs-title"><span class="hljs-title">initializers</span></span>.<span class="hljs-title"><span class="hljs-title">random_uniform</span></span>(<span class="hljs-title"><span class="hljs-title">minval</span></span>=-2, <span class="hljs-title"><span class="hljs-title">maxval</span></span>=2), trainable=True, name='M_hat') #  W   W = tf.nn.tanh(<span class="hljs-type"><span class="hljs-type">W_hat</span></span>) * tf.nn.sigmoid(<span class="hljs-type"><span class="hljs-type">M_hat</span></span>) y_out = tf.matmul(<span class="hljs-title"><span class="hljs-title">x_in</span></span>, <span class="hljs-type"><span class="hljs-type">W</span></span>) return y_out, W</code> </pre> </div></div><br>  Im obigen Code <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-11-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>W</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.424ex" height="2.057ex" viewBox="0 -780.1 2766 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-57" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>W</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-11"> \ hat {W} </script>  und <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-12-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.431ex" height="2.057ex" viewBox="0 -780.1 2769 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-4D" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-12"> \ hat {M} </script>  werden unter Verwendung einer gleichm√§√üigen Verteilung initialisiert, Sie k√∂nnen jedoch <b>jede</b> empfohlene Methode zum Generieren einer anf√§nglichen N√§herung f√ºr diese Parameter verwenden.  Sie k√∂nnen die Vollversion des Codes in meinem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GitHub-Repository sehen</a> (der Link wird am Ende des Beitrags dupliziert). <br><br><h2>  Weiter geht's: Von Addition und Subtraktion zu NAC f√ºr komplexe arithmetische Ausdr√ºcke </h2><br>  Obwohl das oben beschriebene Modell eines einfachen neuronalen Netzwerks mit den einfachsten Operationen wie Addition und Subtraktion zurechtkommt, m√ºssen wir aus den vielen Bedeutungen komplexerer Funktionen wie Multiplikation, Division und Exponentiation lernen k√∂nnen. <br><br>  Im Folgenden finden Sie die modifizierte <abbr title="Neuronale Batterie">NAC-</abbr> Architektur, die f√ºr die Auswahl <b>komplexerer arithmetischer Operationen</b> durch <b>Logarithmus und die Aufnahme des Exponenten</b> in das Modell angepasst ist.  Beachten Sie die Unterschiede zwischen dieser <abbr title="Neuronale Batterie">NAC-</abbr> Implementierung und der bereits oben diskutierten. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d75/2aa/629/d752aa629eae69110dc33e828cd98430.png" alt="Bild"><br>  <i><abbr title="Neuronale Batterie">NAC-</abbr> Architektur f√ºr komplexere arithmetische Operationen</i> <br><br>  Wie aus der Abbildung ersichtlich ist, logarithmieren wir die Eingabedaten, bevor wir sie mit der Gewichtungsmatrix multiplizieren, und berechnen dann den Exponenten des Ergebnisses.  Die Formel f√ºr die Berechnungen lautet wie folgt: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-13-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>Y</mi><mo>=</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>W</mi><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>u</mi><mi>l</mi><mi>l</mi><mi>e</mi><mi>t</mi><mo stretchy=&quot;false&quot;>(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mo>+</mo><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="38.878ex" height="2.66ex" viewBox="0 -832 16739 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-59" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-3D" x="1041" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-65" x="2097" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-78" x="2564" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-70" x="3136" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-28" x="3640" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-57" x="4029" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-62" x="5328" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-75" x="5757" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6C" x="6330" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6C" x="6628" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-65" x="6927" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="7393" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-28" x="7755" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6C" x="8144" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6F" x="8443" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-67" x="8928" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-28" x="9409" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-7C" x="9798" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-78" x="10077" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-7C" x="10649" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-2B" x="11150" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-65" x="12401" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-70" x="12867" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-73" x="13371" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-69" x="13840" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6C" x="14186" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6F" x="14484" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6E" x="14970" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-29" x="15570" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-29" x="15960" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-29" x="16349" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>Y</mi><mo>=</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi>W</mi><mtext>&nbsp;</mtext><mi>b</mi><mi>u</mi><mi>l</mi><mi>l</mi><mi>e</mi><mi>t</mi><mo stretchy="false">(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mo>+</mo><mtext>&nbsp;</mtext><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-13"> Y = exp (W \ bullet (log (| x | + \ epsilon))) </script></p>  <i><a href="">Die Ausgabeformel</a> f√ºr die zweite Version von <abbr title="Neuronale Batterie">NAC</abbr> .</i> <math> </math><i><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-14-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="7.942ex" height="2.419ex" viewBox="0 -780.1 3419.5 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-65" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-70" x="716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-73" x="1220" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-69" x="1689" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6C" x="2035" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6F" x="2333" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-6E" x="2819" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-14"> \ epsilon </script></i>   <i>Hier ist eine sehr kleine Zahl, um Situationen wie log (0) w√§hrend des Trainings zu vermeiden</i> <br><br>  Somit gilt f√ºr beide <abbr title="Neuronale Batterie">NAC-</abbr> Modelle das Funktionsprinzip, einschlie√ülich der Berechnung der Gewichtsmatrix mit Einschr√§nkungen <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-15-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.435ex" height="2.057ex" viewBox="0 -780.1 1048.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-57" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-15"> W </script>  durch <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-16-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>W</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.424ex" height="2.057ex" viewBox="0 -780.1 2766 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-57" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>W</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-16"> \ hat {W} </script>  und <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-17-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.431ex" height="2.057ex" viewBox="0 -780.1 2769 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-4D" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-17"> \ hat {M} </script>  √§ndert sich nicht.  Der einzige Unterschied ist die Verwendung logarithmischer Operationen f√ºr die Eingabe und Ausgabe im zweiten Fall. <br><br><h2>  Zweite NAC-Version in Python mit Tensorflow </h2><br>  Der Code wird sich wie die Architektur kaum √§ndern, mit Ausnahme der angegebenen Verbesserungen bei der Berechnung des Tensors der Ausgabewerte. <br><br><div class="spoiler">  <b class="spoiler_title">Python-Code</b> <div class="spoiler_text"><pre> <code class="hljs pgsql">#    (NAC)     # -&gt;      ,   ,      def nac_complex_single_layer(x_in, out_units, epsilon=<span class="hljs-number"><span class="hljs-number">0.000001</span></span>): <span class="hljs-string"><span class="hljs-string">''' :param x_in:   X :param out_units:    :param epsilon:    (,    log(0)   ) :return m:     :return W:     '''</span></span> in_features = x_in.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>] W_hat = tf.get_variable(shape=[in_shape, out_units], initializer=tf.initializers.random_uniform(minval=<span class="hljs-number"><span class="hljs-number">-2</span></span>, maxval=<span class="hljs-number"><span class="hljs-number">2</span></span>), trainable=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, <span class="hljs-type"><span class="hljs-type">name</span></span>="W_hat") M_hat = tf.get_variable(shape=[in_shape, out_units], initializer=tf.initializers.random_uniform(minval=<span class="hljs-number"><span class="hljs-number">-2</span></span>, maxval=<span class="hljs-number"><span class="hljs-number">2</span></span>), trainable=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, <span class="hljs-type"><span class="hljs-type">name</span></span>="M_hat") #  W   W = tf.nn.tanh(W_hat) * tf.nn.sigmoid(M_hat) #          x_modified = tf.log(tf.abs(x_in) + epsilon) m = tf.exp(tf.matmul(x_modified, W)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> m, W</code> </pre></div></div><br><br>  Ich erinnere Sie noch einmal daran, dass sich die Vollversion des Codes in meinem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GitHub-Repository befindet</a> (der Link wird am Ende des Beitrags dupliziert). <br><br><h2>  Alles zusammen: eine neuronale arithmetische Logikeinheit (NALU) </h2><br>  Wie viele bereits vermutet haben, k√∂nnen wir aus fast jeder arithmetischen Operation lernen, indem wir die beiden oben diskutierten Modelle kombinieren.  Dies ist die <b>Hauptidee von</b> <abbr title="Neuronale arithmetische Logikvorrichtung">NALU</abbr> , die eine <b>gewichtete Kombination aus</b> elementarem und komplexem <abbr title="Neuronale Batterie">NAC umfasst</abbr> , die √ºber ein Trainingssignal gesteuert wird.  Daher sind <abbr title="Neuronale Batterie">NACs</abbr> die Bausteine ‚Äã‚Äãf√ºr die <abbr title="Neuronale arithmetische Logikvorrichtung">Erstellung von NALUs</abbr> . Wenn Sie deren Design verstehen, ist die <abbr title="Neuronale arithmetische Logikvorrichtung">Erstellung von NALUs</abbr> einfach.  Wenn Sie noch Fragen haben, lesen Sie die Erkl√§rungen f√ºr beide <abbr title="Neuronale Batterie">NAC-</abbr> Modelle erneut.  Unten sehen Sie ein Diagramm mit der <abbr title="Neuronale arithmetische Logikvorrichtung">NALU-</abbr> Architektur. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ad9/8e3/2ae/ad98e32aea75b4bb4f020338e9b87bf8.png" alt="Bild"><br>  <i><abbr title="Neuronale arithmetische Logikvorrichtung">NALU-</abbr> Architekturdiagramm mit Erl√§uterungen</i> <br><br>  Wie aus der obigen Abbildung ersichtlich ist, werden beide <abbr title="Neuronale Batterie">NAC-</abbr> Einheiten (lila Bl√∂cke) innerhalb der <abbr title="Neuronale arithmetische Logikvorrichtung">NALU</abbr> durch das Trainingssignal-Sigmoid (orangefarbener Block) interpoliert (kombiniert).  Auf diese Weise k√∂nnen Sie die Ausgabe von jedem von ihnen abh√§ngig von der arithmetischen Funktion, deren Werte wir zu finden versuchen, (de) aktivieren. <br><br>  Wie oben erw√§hnt, ist die <abbr title="Neuronale Batterie">NAC-</abbr> Elementareinheit eine Akkumulationsfunktion, die es der <abbr title="Neuronale arithmetische Logikvorrichtung">NALU</abbr> erm√∂glicht, elementare lineare Operationen (Addition und Subtraktion) auszuf√ºhren, w√§hrend die komplexe NAC-Einheit f√ºr Multiplikation, Division und Exponentiation verantwortlich ist.  <b>Die Ausgabe</b> in <abbr title="Neuronale arithmetische Logikvorrichtung">NALU</abbr> kann als Formel dargestellt werden: <br><br><div class="spoiler">  <b class="spoiler_title">Pseudocode</b> <div class="spoiler_text"><pre> <code class="hljs perl">Simple NAC : a = WX Complex NAC: <span class="hljs-keyword"><span class="hljs-keyword">m</span></span> = <span class="hljs-keyword"><span class="hljs-keyword">exp</span></span>(W <span class="hljs-keyword"><span class="hljs-keyword">log</span></span>(|X| + e))  W = tanh(W_hat) * sigmoid(M_hat) <span class="hljs-comment"><span class="hljs-comment">#  G -       : g = sigmoid(GX) # , ,   NALU #  *      NALU: y = g * a + (1 - g) * m</span></span></code> </pre></div></div><br>  Aus der <abbr title="Neuronale arithmetische Logikvorrichtung">obigen NALU-</abbr> Formel k√∂nnen wir das mit schlie√üen <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-18-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>g</mi><mo>=</mo><mn>0</mn></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="5.377ex" height="2.298ex" viewBox="0 -728.2 2315.1 989.6" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-67" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-3D" x="758" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-30" x="1814" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mo>=</mo><mn>0</mn></math></span></span><script type="math/tex" id="MathJax-Element-18"> g = 0 </script>  Das neuronale Netzwerk w√§hlt nur Werte f√ºr komplexe arithmetische Operationen aus, nicht jedoch f√ºr elementare.  und umgekehrt - im Fall von <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-19-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>g</mi><mo>=</mo><mn>1</mn></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="5.377ex" height="2.298ex" viewBox="0 -728.2 2315.1 989.6" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-67" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-3D" x="758" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMAIN-31" x="1814" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mo>=</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-19"> g = 1 </script>  .  Somit kann <abbr title="Neuronale arithmetische Logikvorrichtung">NALU</abbr> im Allgemeinen jede arithmetische Operation lernen, die aus Addition, Subtraktion, Multiplikation, Division und Erh√∂hen zu einer Potenz besteht, und das Ergebnis erfolgreich √ºber die Grenzen der Intervalle der Werte der Quelldaten hinaus extrapolieren. <br><br><h2>  Python NALU-Implementierung mit Tensorflow </h2><br>  Bei der Implementierung von <abbr title="Neuronale arithmetische Logikvorrichtung">NALU werden</abbr> wir das elementare und komplexe <abbr title="Neuronale Batterie">NAC verwenden</abbr> , das wir bereits definiert haben. <br><br><div class="spoiler">  <b class="spoiler_title">Python-Code</b> <div class="spoiler_text"><pre> <code class="hljs python"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">nalu</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x_in, out_units, epsilon=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.000001</span></span></span></span><span class="hljs-function"><span class="hljs-params">, get_weights=False)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' :param x_in:   X :param out_units:    :param epsilon:    (,    log(0)   ) :param get_weights:   True      :return y_out:     :return G: o   :return W_simple:    NAC1 ( NAC) :return W_complex:    NAC2 ( NAC) '''</span></span> in_features = x_in.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-comment"><span class="hljs-comment">#     NAC a, W_simple = nac_simple_single_layer(x_in, out_units) #     NAC m, W_complex = nac_complex_single_layer(x_in, out_units, epsilon=epsilon) #    G = tf.get_variable(shape=[in_shape, out_units], initializer=tf.random_normal_initializer(stddev=1.0), trainable=True, name="Gate_weights") g = tf.nn.sigmoid(tf.matmul(x_in, G)) y_out = g * a + (1 - g) * m if(get_weights): return y_out, G, W_simple, W_complex else: return y_out</span></span></code> </pre></div></div><br>  Ich stelle erneut fest, dass ich im obigen Code die Parametermatrix erneut initialisiert habe <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-20-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.827ex" height="2.057ex" viewBox="0 -780.1 786.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhilNd3URntIAp0uK8BnInHd0eHnBg#MJMATHI-47" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi></math></span></span><script type="math/tex" id="MathJax-Element-20"> G </script>  Verwenden Sie eine gleichm√§√üige Verteilung, aber Sie k√∂nnen <b>jede</b> empfohlene Methode verwenden, um eine anf√§ngliche Ann√§herung zu generieren. <br><hr><br><h2>  Zusammenfassung </h2><br>  F√ºr mich pers√∂nlich ist die Idee von <abbr title="Neuronale arithmetische Logikvorrichtung">NALU</abbr> ein gro√üer Durchbruch auf dem Gebiet der KI, insbesondere in neuronalen Netzen, und sie sieht vielversprechend aus.  Dieser Ansatz kann die T√ºr zu den Anwendungsbereichen √∂ffnen, in denen Standard-Neuronale Netze nicht zurechtkommen k√∂nnten. <br><br>  Die Autoren des Artikels sprechen √ºber verschiedene Experimente mit <abbr title="Neuronale arithmetische Logikvorrichtung">NALU</abbr> : von der Auswahl der Werte elementarer arithmetischer Funktionen bis zur Z√§hlung der Anzahl handgeschriebener Ziffern in einer bestimmten Reihe von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MNIST-</a> Bildern, mit denen neuronale Netze Computerprogramme √ºberpr√ºfen k√∂nnen! <br><br>  Die Ergebnisse machen einen beeindruckenden Eindruck und beweisen, dass <abbr title="Neuronale arithmetische Logikvorrichtung">NALU</abbr> <b>fast alle Aufgaben im</b> Zusammenhang mit der numerischen Darstellung besser bew√§ltigt als Standardmodelle neuronaler Netze.  Ich ermutige die Leser, sich mit den Ergebnissen von Experimenten vertraut zu machen, um besser zu verstehen, wie und wo das <abbr title="Neuronale arithmetische Logikvorrichtung">NALU-</abbr> Modell n√ºtzlich sein kann. <br><br>  Es muss jedoch beachtet werden, dass weder <abbr title="Neuronale Batterie">NAC</abbr> noch <abbr title="Neuronale arithmetische Logikvorrichtung">NALU</abbr> die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ideale L√∂sung</a> f√ºr eine Aufgabe sind.  Sie repr√§sentieren vielmehr die allgemeine Idee, wie Modelle f√ºr eine bestimmte Klasse von arithmetischen Operationen erstellt werden k√∂nnen. <br><hr><br>  Unten finden Sie einen Link zu meinem GitHub-Repository, das die vollst√§ndige Implementierung des Codes aus dem Artikel enth√§lt. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">github.com/faizan2786/nalu_implementation</a> <br><br>  Sie k√∂nnen die Funktionsweise meines Modells f√ºr verschiedene Funktionen unabh√§ngig √ºberpr√ºfen, indem Sie Hyperparameter f√ºr ein neuronales Netzwerk ausw√§hlen.  Bitte stellen Sie Fragen und teilen Sie Ihre Gedanken in den Kommentaren unter diesem Beitrag, und ich werde mein Bestes tun, um Ihnen zu antworten. <br><br>  <b>PS (vom Autor): Dies ist mein erster schriftlicher Beitrag. Wenn Sie also Tipps, Vorschl√§ge und Empfehlungen f√ºr die Zukunft haben (sowohl technische als auch allgemeine), schreiben Sie mir bitte.</b> <br><br>  PPS (vom √úbersetzer): Wenn Sie Kommentare zur √úbersetzung oder zum Text haben, schreiben Sie mir bitte eine pers√∂nliche Nachricht.  Ich interessiere mich besonders f√ºr den Wortlaut des gelernten Gate-Signals - ich bin mir nicht sicher, ob ich diesen Begriff genau √ºbersetzen kann. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de422777/">https://habr.com/ru/post/de422777/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de422767/index.html">DEFCON-Konferenz 16. Fedor, InSecure.org Hacker. NMAP Scan Online</a></li>
<li><a href="../de422769/index.html">Gewinner des Startup Battlefield TechCrunch Disrupt San Francisco 2018</a></li>
<li><a href="../de422771/index.html">Die Regeln des Designs, das Erreichen einer neuen Ebene und das Designdenken</a></li>
<li><a href="../de422773/index.html">NVIDIA. Enth√ºllung der Geheimnisse der Turing-GPU-Architektur der n√§chsten Generation: Double Ray Tracing, GDDR6 und mehr</a></li>
<li><a href="../de422775/index.html">DEFCON-Konferenz 22. Andrew "Zoz" Brooks. Nicht vermasseln! Teil 1</a></li>
<li><a href="../de422781/index.html">Fintech Digest: SWIFT wird weiterhin in der Russischen F√∂deration arbeiten. Mit VISA k√∂nnen Sie Geld per Telefonnummer und teuren biometrischen Daten √ºberweisen</a></li>
<li><a href="../de422783/index.html">Besser, schneller, leistungsf√§higer: Styled-Components v4</a></li>
<li><a href="../de422785/index.html">Werksdigitalisierung: Ein Blick nach vorne</a></li>
<li><a href="../de422787/index.html">Wesentliche √Ñnderungen in f√ºhrenden Chip-Architekturen</a></li>
<li><a href="../de422789/index.html">@ Pythonetc Aug 2018</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>