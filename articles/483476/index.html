<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèø‚Äçü§ù‚Äçüë®üèª üë©üèø‚Äçüíª ‚òÆÔ∏è La moraleja del transporte rob√≥tico: el problema del carro, los riesgos y las consecuencias ü§∑üèº üëßüèø üóª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El lado moral del desarrollo de veh√≠culos rob√≥ticos es muy complejo. Los desarrolladores creen firmemente que las pruebas deben llevarse a cabo en las...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>La moraleja del transporte rob√≥tico: el problema del carro, los riesgos y las consecuencias</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/itelma/blog/483476/"><img src="https://habrastorage.org/getpro/habr/post_images/9da/572/3d5/9da5723d574c33ceb952a3ebf5334b57.jpg" alt="imagen"><br><br>  El lado moral del desarrollo de veh√≠culos rob√≥ticos es muy complejo.  Los desarrolladores creen firmemente que las pruebas deben llevarse a cabo en las v√≠as p√∫blicas, aunque saben que esto supondr√° un peque√±o riesgo para los usuarios de carreteras desprevenidos.  Los autos lanzados despu√©s de las pruebas ciertamente tendr√°n accidentes (incluidos los fatales), y este estado de cosas es una perspectiva aterradora y desalentadora.  Al mismo tiempo, el √©xito nos promete mejoras tremendas en la seguridad vial y salvar la vida de un gran n√∫mero de personas que morir√≠an si no hubiera oportunidad de reemplazar la conducci√≥n humana m√°s peligrosa con un avi√≥n no tripulado. <br><br>  Consideraremos aspectos tales como: <br><br><ol><li>  Comprensi√≥n de diferentes enfoques del razonamiento moral de las personas en diferentes (o las mismas) situaciones </li><li>  C√≥mo se relacionan la ley, la sociedad y los seguros con la conducci√≥n riesgosa, los accidentes y los accidentes. </li><li>  Riesgos y p√©rdidas durante la conducci√≥n (o entrenamiento de conducci√≥n) que parece que estamos dispuestos a aceptar por peque√±os beneficios. </li><li>  ¬øC√≥mo cambia nuestra opini√≥n de que "el fin justifica los medios" cambia seg√∫n lo que est√° en juego: atrocidades intencionales o peque√±os riesgos deliberados? </li><li>  Las grandes ventajas que obtendremos cuando una peque√±a flota de autom√≥viles aut√≥nomos aprenda a conducir de manera m√°s segura, despu√©s de lo cual su software se copiar√° en millones de otros autom√≥viles, una situaci√≥n que es imposible en el caso de los conductores humanos. </li><li>  Riesgos y principios de los enfoques modernos para probar y desarrollar autos aut√≥nomos, y c√≥mo Uber los viol√≥. </li><li>  Gran beneficio si podemos encontrar el enfoque correcto. </li></ol><br><a name="habracut"></a><br><br>  El hecho de que un accidente fatal que involucr√≥ un veh√≠culo Uber se descubri√≥ recientemente en Tempe, Arizona, solo aumenta la necesidad de comprender este problema.  Se han escrito muchos textos y se han expresado muchas opiniones sobre c√≥mo discutir los riesgos y principios de la moralidad en veh√≠culos no tripulados.  En este art√≠culo, espero presentar una visi√≥n clara de este problema y una gu√≠a para la discusi√≥n al respecto. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7ea/921/5cf/7ea9215cf82e51ed5037034af65299a6.jpg" alt="imagen"><br><br>  <i>La mayor√≠a de los accidentes son m√°quinas individuales y pueden prevenirse.</i>  <i>Sin embargo, llegamos a un acuerdo con este terrible riesgo sin pensar.</i> <br><br>  Resulta que una combinaci√≥n de las lecciones que nos da el <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D1%2580%25D0%25BE%25D0%25B1%25D0%25BB%25D0%25B5%25D0%25BC%25D0%25B0_%25D0%25B2%25D0%25B0%25D0%25B3%25D0%25BE%25D0%25BD%25D0%25B5%25D1%2582%25D0%25BA%25D0%25B8">problema</a> del <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D1%2580%25D0%25BE%25D0%25B1%25D0%25BB%25D0%25B5%25D0%25BC%25D0%25B0_%25D0%25B2%25D0%25B0%25D0%25B3%25D0%25BE%25D0%25BD%25D0%25B5%25D1%2582%25D0%25BA%25D0%25B8">carro</a> (es decir, del carro, no del veh√≠culo no tripulado) y medir las acciones que conducen a riesgos en lugar de tragedias puede ayudarnos a formar una mejor comprensi√≥n y un r√©gimen legal para resolver preguntas complejas sobre robots que pueden y salvar vidas humanas y amenazarlas.  Podemos salvar millones de vidas si estamos listos para aceptar los mismos riesgos que cuando ense√±amos a los adolescentes antes de que comiencen a conducir (en lugar de los adolescentes, puede imaginarse apresurando a los repartidores de pizza).  Casi todos los que trabajan en robots quieren que salven vidas y que comiencen a hacerlo lo antes posible, pero para esto el p√∫blico debe comprender e incluso adoptar m√©todos de trabajo en esta √°rea.  Para hacer esto, debemos comprender tanto nuestros instintos morales como nuestras matem√°ticas internas, y la diferencia entre riesgo y tragedia. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/57a/d2f/ccb/57ad2fccb3456ac9538215f4505d85fe.png" alt="imagen"><br><br>  Me di cuenta de la dificultad de entender estos problemas hace un a√±o cuando estaba hablando en casa con mi amigo en la cena.  Un amigo estaba muy preocupado por los riesgos que conten√≠an los primeros prototipos.  Le pregunt√©: ¬øser√≠a razonable que murieran 100 personas durante las pruebas, pero usar el producto final salvar√° millones de vidas?  Le parec√≠a obvio que esto estaba mal, y no estaba solo en esta opini√≥n. <br><br>  La moral humana es compleja y astuta, y puede funcionar de diferentes maneras dependiendo de la situaci√≥n.  Pocos de nosotros somos claros en nuestros principios, y esos principios que utilizamos en nuestras decisiones subjetivas pueden diferir de los utilizados en las decisiones colectivas en la sociedad o en los tribunales.  Para comprender este problema y c√≥mo llegar a una mejor soluci√≥n, necesitamos entender c√≥mo razona la gente sobre dicha moralidad y, posiblemente, cambiar este pensamiento para lograr un resultado que sea mejor en la opini√≥n de la mayor√≠a. <br><br>  La respuesta puede estar en el hecho de que, aunque no estamos de acuerdo en que grandes objetivos puedan justificar medios inmorales, resulta que estamos dispuestos a admitir que incluso objetivos rentables modestos pueden justificar medios con peque√±os riesgos inmorales. <br><br><h3>  Tipos de moralidad </h3><br>  En t√©rminos generales, los fil√≥sofos distinguen dos clases amplias de sistemas morales.  El primero requiere un c√≥digo de reglas y principios, y define un error como una violaci√≥n de estas mismas reglas y principios, independientemente del resultado.  El nombre complejo de tales sistemas es "deontol√≥gico", pero los llamaremos basados ‚Äã‚Äãen reglas.  Por otro lado, tenemos sistemas basados ‚Äã‚Äãen resultados que miden lo correcto y lo incorrecto con lo que terminamos.  Tambi√©n se les conoce como consecuencialistas.  Un subconjunto espec√≠fico de tales sistemas es utilitario, siguiendo la regla para lograr el mayor bien para la mayor cantidad de personas, o para hacer el menor da√±o a la menor cantidad de personas. <br><br>  A veces amamos los sistemas morales utilitarios, especialmente como sociedad en su conjunto.  Sin embargo, como individuos, tendemos a desconfiar de ellos, porque est√°n asociados con la peligrosa idea de que "el fin justifica los medios", y esta idea ha generado muchos horrores morales a lo largo de la historia.  La mayor√≠a de nosotros no pertenecemos estrictamente a una de las escuelas.  Como se se√±al√≥ anteriormente, nuestro enfoque cambia dependiendo de si nos consideramos una persona separada o la sociedad en su conjunto, y cambiamos nuestro enfoque al razonamiento basado en nuestros puntos de vista personales m√°s de lo que nos gustar√≠a admitir. <br><br>  Un adherente puro al enfoque utilitario aceptar√° f√°cilmente que los autom√≥viles matar√°n a 100 personas para ahorrar un mill√≥n; en el caso de la moral utilitaria, ni siquiera hay una pregunta.  Al mismo tiempo, no ser√° f√°cil para muchos aceptar.  Sospecho que la respuesta es comprender c√≥mo nosotros, como individuos, prestamos especial atenci√≥n a los incidentes y tragedias, mientras que en el papel de la sociedad, prestamos atenci√≥n a la evaluaci√≥n de riesgos y los bienes p√∫blicos. <br><br>  Quienes leen mis textos saben que <a href="https://ideas.4brad.com/barack-obama-wants-solve-robocar-trolley-problems-now">odio la aplicaci√≥n est√°ndar del "problema del tranv√≠a" a los veh√≠culos no tripulados</a> .  En esta aplicaci√≥n, las personas imaginan el software en el autom√≥vil, que deber√≠a decidir cu√°l de dos grupos diferentes de personas matar en un accidente.  Nos abruma dolorosamente la idea de que ahora las m√°quinas deciden qui√©n debe morir, aunque antes era un campo de actividad de los dioses.  De hecho, esta es una situaci√≥n extremadamente rara, y su resoluci√≥n no est√° incluida en ninguna lista de prioridades.  Los programadores y las empresas tampoco quieren escribir algoritmos relacionados con las elecciones morales, preferir√≠an que los pol√≠ticos respondan esas preguntas y escriban leyes que con mucho gusto seguir√°n. <br><br>  Sin embargo, el problema original del carro ten√≠a una funci√≥n real que podr√≠a ser √∫til en esta situaci√≥n.  Fue creado para ayudarnos a comprender nuestro propio pensamiento y para ayudarnos a comprender la diferencia entre los sistemas morales basados ‚Äã‚Äãen reglas y aquellos que est√°n orientados a resultados, es decir, en √∫ltima instancia, con la ayuda del problema del tranv√≠a, deber√≠amos tener una mejor comprensi√≥n de la filosof√≠a.  Y ella puede ayudarnos a comprender mejor esta situaci√≥n. <br><br>  Como recordar√°, en la tarea original, el carro se apresura a lo largo de los rieles con los frenos rotos.  Alguien (realmente inmoral en esta situaci√≥n) at√≥ a 5 personas a la carretera principal y una persona a su rama.  Puede presionar el interruptor para matar a una persona, pero ahorre cinco.  Hasta el 90% de las personas eligen un enfoque utilitario (basado en resultados) y prefieren tirar de la palanca, pero algunos se niegan.  (De hecho, es m√°s probable que estas personas se comporten de esta manera en el ejercicio en el aula. Se realiz√≥ un experimento en el programa <a href="https://www.youtube.com/watch%3Fv%3D1sl5KJ69qiA">Mind Field</a> en YouTube en el que se hizo pensar a las personas que realmente estaban en una situaci√≥n con un carrito apresurado y personas adjuntas. Atenci√≥n, spoiler : la mayor√≠a de los sujetos simplemente se congelaron de horror) <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/1sl5KJ69qiA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  Me gusta decir que la soluci√≥n de ingenier√≠a para este problema es m√°s simple: <a href="https://www.forbes.com/sites/bradtempleton/2019/02/21/robocar-engineers-prefer-to-solve-the-runaway-trolley-problem-by-fixing-the-brakes-on-the-trolley/">debe reparar los frenos del trole</a> .  Los ingenieros de veh√≠culos no tripulados trabajar√°n principalmente para garantizar que sus autom√≥viles nunca entren en tales situaciones, incluso si ocurren muy raramente. <br><br>  Las variaciones del problema del tranv√≠a que surgieron los fil√≥sofos son mucho m√°s interesantes que la redacci√≥n original.  En una de las opciones, no es necesario presionar el interruptor, sino empujar al hombre gordo sobre los rieles para detener el carro; pocos lo har√°n, ya que este es un asesinato deliberado.  En la versi√≥n m√°s extrema, no tienes un carrito, pero hay 5 pacientes que necesitan urgentemente un trasplante de √≥rgano.  Puedes agarrar a un hombre que camina por la calle, cortarlo y salvar a cinco personas, casi nadie acepta hacerlo, aunque este es el mismo problema desde el punto de vista de un enfoque utilitario. <br><br>  Casi nadie har√° esto, porque entre nosotros hay pocos que pertenezcan estrictamente a una escuela moral en particular.  Es m√°s dif√≠cil para nosotros cambiar nuestro pensamiento.  Para comprender y resolver problemas en el campo de las pruebas y la producci√≥n de veh√≠culos no tripulados, debemos trascender el instinto natural e individual, percibir varios casos como tragedias "simplemente" separadas que representan, y pensar qu√© riesgos son aceptables y posibles. permitido como sociedad. <br><br>  Adem√°s, somos mucho m√°s rechazados por las tragedias que les ocurren a los transe√∫ntes externos, aunque no nos importa tanto como exponer a estas personas a menores riesgos.  Finalmente, tenemos m√°s miedo de que m√°quinas independientes, en lugar de personas, nos hagan da√±o. <br>  La gente es divertida porque, aunque la mayor√≠a tiene miedo de morir a causa de los robots, es m√°s probable que un criminal borracho nos mate. <br><br>  B√°sicamente, la gente piensa que matar para salvar vidas est√° mal, es indiscutible.  La verdadera pregunta es qu√© es realmente probar y liberar veh√≠culos no tripulados en la carretera: matar para salvar vidas o correr riesgos con el mismo prop√≥sito.  De hecho, en la mayor√≠a de los casos, aceptamos arriesgarnos por salvar vidas, sujeto a un buen resultado. <br><br><h3>  Aspectos morales de la conducci√≥n. </h3><br>  Entonces, ¬øcu√°l es la relaci√≥n entre la filosof√≠a moral y los autom√≥viles?  Para averiguarlo, debemos pensar en el aspecto moral de los accidentes de tr√°fico.  Las personas y la sociedad en general tienen una actitud diferente a este problema.  Las sociedades en general y la ley en particular, prefieren no describir nada como inmoral o malvado, a menos que haya una intenci√≥n maliciosa, que se llama mens rea en la jurisprudencia.  Casi todos los delitos penales involucran intenciones maliciosas, y si no es as√≠, el castigo (en la mayor√≠a de los casos) se mitiga.  Debido a esto, las situaciones no son infrecuentes cuando alguien golpea a una persona en un autom√≥vil y no es penalmente responsable de esto.  El conductor enfrentar√° un castigo financiero, pero est√° cubierto por un seguro.  El culpable saldr√° de esta situaci√≥n solo con un sentimiento de culpa por lo que sucedi√≥.  Pero todo esto es cierto solo si el asesinato de una persona fue completamente involuntario y en realidad completamente contrario a las intenciones del conductor.  <a href="https://www.lawyers.com/legal-info/criminal/traffic-violations/when-a-drivers-actions-amount-to-manslaughter.html">No puede haber asesinato (incluso involuntario) sin una intenci√≥n clara o negligencia grave y deliberada al volante</a> . <br><br>  Al mismo tiempo, estamos trabajando arduamente para determinar si hay intenciones maliciosas o negligencia, porque estamos muy preocupados por el hecho de que algo tan tr√°gico como la muerte pueda quedar sin respuesta.  Pero si estamos hablando de la situaci√≥n en la que realmente ocurri√≥ el accidente (con lo que quiero decir que ocurri√≥ dentro del marco de los riesgos habituales de conducir con cuidado), no hay consecuencias legales, lo m√°s grave que puede suceder son las consecuencias financieras, que son totalmente pagadas por el seguro. <br><br>  Por otro lado, considere acelerar.  El exceso de velocidad es ilegal, incluso si a menudo se comete, pero rara vez se castiga en algunos lugares.  Cuando aceleras, sabes (o debes saber) si superas o no.  Al hacerlo, expone a otras personas a un riesgo adicional, aunque, afortunadamente, por regla general, no sucede nada terrible.  A pesar de que sin duda recibir√° una multa por exceso de velocidad, si este exceso conduce a un accidente, la mayor√≠a de las multas se emiten por excesos que no perjudicaron a nadie.  Casi todos superamos peri√≥dicamente la velocidad, y lo hacemos por la raz√≥n obvia: queremos llegar a un lugar un poco m√°s r√°pido.  Esto puede ser contrario a la intuici√≥n, pero el exceso de velocidad deliberado en nuestro sistema es m√°s inmoral e ilegal que el asesinato aleatorio, aunque nosotros, como individuos, somos mucho m√°s tolerantes con el exceso de velocidad. <br><br>  Creo que el verdadero problema moral al conducir es que intencionalmente ponemos a otras personas en riesgo.  O, para m√°s detalles, un riesgo inaceptablemente alto.  Conducir implica que otras personas estar√°n en riesgo.  De hecho, es probable que al conducir expongamos con mayor frecuencia a otras personas al mayor riesgo.  Todos conocemos un n√∫mero asombroso de muertes, lesiones y da√±os materiales por conducir: m√°s personas murieron en accidentes que en todas las guerras y como resultado de ataques terroristas en la historia de los Estados Unidos desde la Guerra Revolucionaria.  Todo esto es terriblemente arriesgado, pero para nosotros es tan importante que decidimos aceptar este riesgo relativamente alto para nosotros y las personas que nos rodean.  Hacemos esto a prop√≥sito, aunque a menudo lo olvidamos, y ciertamente no recordamos bien la esencia matem√°tica de estos fen√≥menos.  Consideramos que este nivel b√°sico de riesgo es "aceptable" en nuestras leyes y nuestras vidas. <br><br>  <b>La sociedad decidi√≥ que la injusticia y la ilegalidad no est√°n en v√≠ctimas espec√≠ficas, sino en la exposici√≥n deliberada y descuidada de otras personas a un riesgo extremo.</b> <br><br>  Hemos escrito leyes que proh√≠ben que exponga a otras personas a riesgos excepcionales, y usted recibe multas por exceso de velocidad, cambios de carril incorrectos y conducci√≥n descuidada.  Consideramos que tales actos son inmorales, ya que se cometen intencionalmente o por negligencia, aunque no consideramos inmoral una muerte que en realidad no fue involuntaria.  Esto no detiene a aquellos que ven esto como una gran tragedia, y esta es una reacci√≥n absolutamente natural.  Pero como sociedad que escribe y hace cumplir las leyes, percibimos todo de manera diferente. <br><br>  Encontramos un sorprendente n√∫mero de riesgos de conducci√≥n aceptables: <br><br><ol><li>  Conducci√≥n de servicio pesado </li><li>  Conducir bajo la lluvia, la nieve y la noche. </li><li>  Conducir en lugares con muchos peatones y ciclistas. </li><li>  Conducir un autom√≥vil con leves problemas mec√°nicos. </li><li>  Conducci√≥n sin frenado autom√°tico de emergencia y otras tecnolog√≠as avanzadas </li><li>  Conducir en un estado de sue√±o incluso cuando nos quedamos dormidos (dos estados tienen leyes que proh√≠ben conducir con sue√±o) </li><li>  Conducir adolescentes sin preocupaciones con licencia reciente </li><li>  Estudiantes adultos de manejo </li><li>  Conducir a personas mayores con percepciones reducidas y tiempos de reacci√≥n m√°s largos </li><li>  Conducir con niveles de alcohol en la sangre justo debajo de lo permitido </li><li>  Conducir en un estado enfermo (aunque sea ilegal) </li><li>  Usar dispositivos y escribir mensajes mientras conduce (aunque esto es ilegal) </li><li>  En los EE. UU., El exceso de velocidad es muy com√∫n, a menudo con un amplio margen de otros autom√≥viles </li></ol><br>  Veamos un prototipo de veh√≠culo no tripulado.  Cuando un equipo de desarrollo pone un autom√≥vil de este tipo en la carretera, deliberadamente ponen en riesgo a otros participantes del tr√°fico.  Por supuesto, no tienen la intenci√≥n de causar ning√∫n accidente, de hecho, tienen la intenci√≥n de evitarlo. <br><br>  Por el momento, los drones de prueba, con la excepci√≥n de algunas excepciones, siempre funcionan bajo el control de un conductor humano que est√° listo para ponerse a trabajar en caso de problemas.  Casi siempre hay una segunda persona que monitorea los sistemas y ocasionalmente mira el camino.  Esto se puede comparar con la situaci√≥n en la que un conductor adolescente con licencia de estudiante est√° acompa√±ado por un instructor.  El instructor tiene su propio pedal de freno y puede interceptar la rueda, como el conductor de un dron.  Los conductores adolescentes, acompa√±ados por instructores, en realidad tienen puntajes de seguridad bastante buenos, al igual que casi todos los equipos de veh√≠culos no tripulados, con la excepci√≥n de Uber, de lo que hablar√© m√°s adelante. <br><br>  Estos conductores adolescentes, despu√©s de haber obtenido una licencia despu√©s de pasar la prueba m√≠nima, se convierten en los conductores m√°s peligrosos en la carretera.  Los lanzamos en el camino para darles movilidad, tambi√©n porque esta es la √∫nica forma de hacerlos conductores adultos m√°s cautelosos, en lo que eventualmente se convertir√°n.  Asumimos los riesgos asociados con la conducci√≥n de un conductor adolescente, con la esperanza de obtener un conductor m√°s cuidadoso en el futuro.  De cada adolescente arriesgado en el camino, un adulto crece, un adulto m√°s cauteloso (en realidad un poco menos de uno, porque no todos los adolescentes realmente alcanzan esta edad m√°s segura). <br><br><img src="https://habrastorage.org/webt/vr/dq/bq/vrdqbqrlnxepkvveqq-do1xcoko.jpeg" alt="imagen"><br><br>  Como se se√±al√≥ anteriormente, el equipo de desarrollo de drones pone a las personas en riesgo, pero los beneficios que trae este riesgo son enormes.  Un breve viaje de estudio mejorar√° la seguridad de todos los autom√≥viles posteriores que se crear√°n m√°s adelante, lo que en √∫ltima instancia significa millones de autom√≥viles mejorados.  Al aceptar los riesgos principales de las pruebas y el desarrollo, nos beneficiamos de una reducci√≥n significativa en los riesgos en el futuro.  La reducci√≥n del riesgo se producir√° cuando los autom√≥viles comiencen a conducir de manera m√°s segura que las personas, y las personas dejen de poner en riesgo a los dem√°s al elegir viajar en un autom√≥vil no tripulado en lugar de conducir de manera independiente.  Esto es especialmente cierto cuando se trata de personas que bebieron o est√°n asociadas con alguno de los elementos de la lista anterior. <br><br>  Algunos sostienen que debemos considerar no solo los riesgos extraordinarios, sino tambi√©n los necesarios.  Porque puede ser un error poner a las personas en riesgo ordinario si esto no es necesario.  En particular, algunos argumentan que los equipos actuales realizan m√°s pruebas de las necesarias y que esto est√° mal.  Quiz√°s este sea el enfoque correcto (aunque, por supuesto, el n√∫mero requerido de pruebas sigue siendo motivo de controversia).  Sin embargo, si consideramos las razones por las cuales las personas toman decisiones arriesgadas en el camino, desde la entrega de alimentos hasta el regreso a casa un minuto antes, dif√≠cilmente satisfacen el nivel de necesidad, a pesar del hecho de que toleramos estos riesgos. <br><br>  Aqu√≠ debemos razonar, a partir de los resultados.  ,    ,      ,         .      - ,         .     -,                   .    ‚Äì ,   ‚Äì .   . <br><br><h3>   </h3><br>  ,   .         ,      0.1     .  ‚Äì  ,  1  .       ‚Äì  0.013   . ,      ,       .           ‚Äì ,  ,     ,   1/250   .         ,    ,       .     ,                      .     ,    .  ,        ,        <b></b> (  )    ,     .       1.5   ,     .     (    1.7    ),          . <br><br>   ,     ¬´ ¬ª,      ?       ‚Äì         ,       ,           .   ,       ,            . <br><br>       ,     .    ,      (    ),      100 000      .    1.5        .    . 100 000     ‚Äì    ,    10         2-3  . <br><br>   ,      .        ,    ,       ,   ,              ,  .      . ,      ,   ,  ¬´¬ª  ,     .   ,        -,        . , ,            . <br><br><h3>    ? </h3><br>   ?       Waymo.      7  ,    Google.     10          .       ,   ,     ,             ‚Äì     ,   ,   ,     .  ,                Waymo,   ,      ‚Äì   ,     ,      .     ,          . <br><br> Tesla    ,      Tesla      .     4.3        2.7     . Tesla      ,        ,       ,   ,         .  ,          ,   (       )       ,  ,          .   , ,     - ,      ,    ,          . , Tesla            ,       ,    .          ,      Uber. <br><br><h3>  Uber </h3><br>   2018    Uber    ,  , ,        .       ,            .    <a href="https://ideas.4brad.com/search/node/uber%2520fatality"> </a> ,        ,  ,      ,     99.9%  . Uber         ,   ,          -  ,       ,      .   ,   ,    ,  ,     ,       Uber,    ,    ,    ,  -          ,       .      ,             .    ,    .      ,     ‚Äî       ,   ,     ‚Äî      . Uber      18        ,      . <br><br>   ,            ,   ,   ,          .   ,      .  ,   Uber           ,   .    ,       ,     .     - ,    ,       ,    Uber  -  ,        .  Uber        . <br><br>            ,      ,       .  ,          .        ,       ,         .   ,   ,      ,          ,     ,             .               0,07%        ,           ,   ,    . <br><br>           . ,            .   ,  Uber      ( ),  ,          , ,      .        ,   .   ,      ‚Äì  .     ,             ,       .        ,         ,       .          3  . <br><br><h3>  Conclusi√≥n </h3><br>        ,    ‚Äì      ,      .     ,    ,   ,        ,   .     ,    ,            ,       . <br><br> <b>  ,     ,   100      ,    .    ,   ,     ,      ,        .</b> <br><br>       ,    .       ,    ,         ,       ‚Äî ,     ‚Äî     .  ,   ,     ,     ,   - ,     ,       .   ,     ,       . <br><br>      ,   ,     .    .               ,       ,       . <br><br><hr><br><img src="https://habrastorage.org/webt/4m/5z/_p/4m5z_pc9zhjja8pmtwxvaihckfe.png" alt="imagen"><br><br><div class="spoiler">  <b class="spoiler_title">Sobre ITELMA</b> <div class="spoiler_text">  Somos una gran <a href="https://en.wikipedia.org/wiki/Automotive_industry">empresa de</a> componentes <a href="https://en.wikipedia.org/wiki/Automotive_industry">automotrices</a> .  La compa√±√≠a emplea a unos 2.500 empleados, incluidos 650 ingenieros. <br><br>  Somos quiz√°s el centro de competencia m√°s poderoso de Rusia para el desarrollo de la electr√≥nica automotriz en Rusia.  Ahora estamos creciendo activamente y hemos abierto muchas vacantes (alrededor de 30, incluso en las regiones), como un ingeniero de software, ingeniero de dise√±o, ingeniero de desarrollo l√≠der (programador DSP), etc. <br><br>  Tenemos muchos desaf√≠os interesantes de los fabricantes de autom√≥viles y preocupaciones que impulsan la industria.  Si desea crecer como especialista y aprender de los mejores, estaremos encantados de verlo en nuestro equipo.  Tambi√©n estamos listos para compartir experiencia, lo m√°s importante que sucede en la industria automotriz.  H√°ganos cualquier pregunta, responderemos, discutiremos. </div></div><br>  <b>Leer m√°s art√≠culos √∫tiles:</b> <br><br><ul><li>  <a href="https://habr.com/ru/company/itelma/blog/479736/">C√°maras o l√°ser</a> </li><li>  <a href="https://habr.com/ru/company/itelma/blog/478640/">Autos aut√≥nomos en c√≥digo abierto</a> </li><li>  <a href="https://habr.com/ru/company/itelma/blog/476824/">McKinsey: repensar la arquitectura de software y electr√≥nica en automoci√≥n</a> </li><li>  <a href="https://habr.com/ru/company/itelma/blog/476054/">Otra guerra del sistema operativo ya est√° bajo el cap√≥ de los autom√≥viles</a> </li><li>  <a href="https://habr.com/ru/company/itelma/blog/475576/">C√≥digo de programa en el auto</a> </li><li>  <a href="https://habr.com/ru/company/itelma/blog/475448/">En un autom√≥vil moderno, hay m√°s l√≠neas de c√≥digo que ...</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/483476/">https://habr.com/ru/post/483476/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../483462/index.html">C√°llate y toma mi dinero</a></li>
<li><a href="../483466/index.html">Introduciendo el m√©todo de retropropagaci√≥n</a></li>
<li><a href="../483468/index.html">Pruebas de integraci√≥n de flutter: es f√°cil</a></li>
<li><a href="../483470/index.html">Coloque mosaicos de manera eficiente (Pro CSS, SVG, patr√≥n y m√°s)</a></li>
<li><a href="../483472/index.html">Eliminar todo: c√≥mo borrar datos y restaurar el SSD NVMe a la configuraci√≥n de f√°brica</a></li>
<li><a href="../483478/index.html">Sol, viento y agua ver 0.1</a></li>
<li><a href="../483480/index.html">Crucigrama "Si√©ntete como un analista de SOC"</a></li>
<li><a href="../483482/index.html">Comisi√≥n Federal de Comunicaciones de EE. UU. Pro V2V, V2I y V2X</a></li>
<li><a href="../483484/index.html">"Pretenderlo": c√≥mo los veh√≠culos no tripulados "se rinden a la derecha"</a></li>
<li><a href="../483492/index.html">Resolver problemas t√≠picos con json_encode (PHP)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>