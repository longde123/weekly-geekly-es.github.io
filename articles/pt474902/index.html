<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧗🏻 🌀 🚐 Tendências em visão computacional. Destaques ICCV 2019 🚦 🙉 🌊</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="As redes neurais na visão computacional estão se desenvolvendo ativamente, muitas tarefas ainda estão longe de serem resolvidas. Para se destacar em s...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Tendências em visão computacional. Destaques ICCV 2019</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/474902/"><img src="https://habrastorage.org/webt/in/lj/qf/inljqfmjnklszyujlyua8n_w0bo.jpeg"><br><br>  As redes neurais na visão computacional estão se desenvolvendo ativamente, muitas tarefas ainda estão longe de serem resolvidas.  Para se destacar em seu campo, basta seguir os influenciadores no Twitter e ler os artigos relevantes em arXiv.org.  Mas tivemos a oportunidade de ir à Conferência Internacional sobre Visão Computacional (ICCV) 2019. Este ano, é realizado na Coréia do Sul.  Agora queremos compartilhar com os leitores de Habr que vimos e aprendemos. <br><a name="habracut"></a><br>  Muitos de nós da Yandex: desenvolvedores de veículos não tripulados, pesquisadores e os envolvidos nas tarefas de CV nos serviços chegaram.  Mas agora queremos introduzir um ponto de vista um pouco subjetivo de nossa equipe - o laboratório de inteligência de máquinas (Yandex MILAB).  Outros caras provavelmente olharam para a conferência de seu ângulo. <br><br><div class="spoiler">  <b class="spoiler_title">O que o laboratório faz</b> <div class="spoiler_text">  Realizamos projetos experimentais relacionados à geração de imagens e músicas para fins de entretenimento.  Estamos especialmente interessados ​​em redes neurais que permitem alterar o conteúdo do usuário (para uma foto, essa tarefa é chamada manipulação de imagem).  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Um exemplo do</a> resultado do nosso trabalho da conferência YaC 2019. </div></div><br>  Existem muitas conferências científicas, mas as principais conferências A * se destacam, onde geralmente são publicados artigos sobre as tecnologias mais interessantes e importantes.  Não existe uma lista exata de conferências A *, aqui está um exemplo e incompleto: NeurIPS (anteriormente NIPS), ICML, SIGIR, WWW, WSDM, KDD, ACL, CVPR, ICCV, ECCV.  Os três últimos são especializados no tópico CV. <br><br><h2>  Visão geral do ICCV: pôsteres, tutoriais, oficinas, stands </h2><br>  1075 trabalhos foram aceitos na conferência, os participantes foram 7.500. 103 pessoas vieram da Rússia, artigos de funcionários da Yandex, Skoltech, Samsung AI Center Moscow e Samara University.  Este ano, não muitos pesquisadores de ponta visitaram o ICCV, mas aqui, por exemplo, Alexey (Alyosha) Efros, que sempre reúne muitas pessoas: <br><br><img src="https://habrastorage.org/webt/4g/ie/3w/4gie3wyqaablh0wmnxbq4ucdwbs.jpeg"><br><br><div class="spoiler">  <b class="spoiler_title">Estatísticas</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/1e/lz/am/1elzamxsr2xf9k_gqrwvvvrwey0.jpeg" width="500"><br><br><img src="https://habrastorage.org/webt/vb/yr/1i/vbyr1im56rz6ib-6sj_0fjiokjc.jpeg" width="500"><br><br><img src="https://habrastorage.org/webt/l6/wc/iy/l6wciy-qakwq9wwe65hz9-pqjl8.jpeg" width="500"><br><br><img src="https://habrastorage.org/webt/jb/dr/rq/jbdrrqkeo6mw26wx3zi5taa_zog.jpeg" width="500"><br><br><img src="https://habrastorage.org/webt/2g/rb/rt/2grbrtsd1xwbzwzxfstgdck6jis.jpeg" width="500"><br></div></div><br>  Em todas essas conferências, os artigos são apresentados na forma de pôsteres ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">mais</a> sobre o formato), e os melhores também na forma de breves relatórios. <br><br><div class="spoiler">  <b class="spoiler_title">Aqui está parte do trabalho da Rússia</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/iu/sc/po/iuscpof__g3ikvdguuy94javuvo.jpeg"><br><br><img src="https://habrastorage.org/webt/ae/e0/xj/aee0xjbluiby-b_xxoednb7yfws.jpeg"><br><br><img src="https://habrastorage.org/webt/1s/qt/la/1sqtla2h9xgccuhu2dr1iyvxuek.jpeg"><br></div></div><br>  Nos tutoriais, você pode mergulhar em alguma área, assemelha-se a uma palestra em uma universidade.  É lido por uma pessoa, geralmente sem falar sobre trabalhos específicos.  Exemplo de um tutorial legal ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Michael Brown, Noções básicas sobre cores e o pipeline de processamento de imagens na câmera para o Computer Vision</a> ): <br><br><img src="https://habrastorage.org/webt/2z/sq/e6/2zsqe6wgv1rtrl0tpvwl-eu5enw.jpeg"><br><br>  Nas oficinas, pelo contrário, eles falam sobre artigos.  Geralmente, esse é um trabalho em algum tópico restrito, histórias de líderes de laboratório sobre todo o trabalho mais recente dos alunos ou artigos que não foram aceitos na conferência principal. <br><br>  As empresas patrocinadoras vêm ao ICCV com estandes.  Este ano, Google, Facebook, Amazon e muitas outras empresas internacionais chegaram, além de um grande número de startups - coreanas e chinesas.  Havia muitas startups especializadas em marcação de dados.  Há apresentações nas bancas, você pode levar mercadorias e fazer perguntas.  As empresas patrocinadoras têm festas para a caça.  Eles conseguem se convencer dos recrutadores de que você está interessado e que você pode ser potencialmente entrevistado.  Se você publicou um artigo (ou, além disso, fez uma apresentação com ele), iniciou ou terminou o doutorado - isso é uma vantagem, mas às vezes você pode concordar com uma posição, fazendo perguntas interessantes aos engenheiros da empresa. <br><br><h2>  Tendências </h2><br>  A conferência permite que você dê uma olhada em toda a área do currículo.  Pelo número de pôsteres de um tópico específico, você pode avaliar a qualidade do tópico.  Algumas conclusões imploram pelas palavras-chave: <br><br><img src="https://habrastorage.org/webt/7u/td/1v/7utd1vf3hcbbhtrgl3xvldtjnjc.jpeg"><br><br><h4>  Tiro zero, tiro único, tiro curto, auto-supervisionado e semi-supervisionado: novas abordagens para problemas estudados há muito tempo </h4><br>  As pessoas aprendem a usar os dados com mais eficiência.  Por exemplo, no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">FUNIT,</a> você pode gerar expressões faciais de animais que não estavam no conjunto de treinamento (aplicando várias fotos de referência no aplicativo).  As idéias do Deep Image Prior foram desenvolvidas e agora as redes <abbr title="Redes adversárias generativas, redes adversárias generativas.">GAN</abbr> podem ser treinadas em uma imagem - falaremos sobre isso mais adiante <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">nos destaques</a> .  Você pode usar a auto-supervisão para pré-treinamento (resolvendo um problema para o qual você pode sintetizar dados alinhados, por exemplo, para prever o ângulo de rotação de uma imagem) ou aprender ao mesmo tempo a partir de dados marcados e não marcados.  Nesse sentido, a coroa da criação pode ser considerada um artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">S4L: Aprendizagem semi-supervisionada auto-supervisionada</a> .  Mas o pré-treinamento no ImageNet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">nem sempre</a> ajuda. <br><br><img src="https://habrastorage.org/webt/gj/yy/n4/gjyyn40ktbwpjaslfp0v7c-avhi.jpeg"><br><br><img src="https://habrastorage.org/webt/x5/6-/l8/x56-l8y26lyq9unxyt0soa8koay.jpeg"><br><br><h4>  3D e 360 ​​° </h4><br>  As tarefas, geralmente resolvidas para fotos (segmentação, detecção), exigem pesquisas adicionais para modelos 3D e vídeos panorâmicos.  Vimos muitos artigos sobre a conversão de RGB e <abbr title="Imagem com profundidade RGB. Para cada ponto, não apenas sua cor é conhecida, mas também sua “profundidade” - a distância do ponto de vista / disparo.">RGB-D</abbr> para 3D.  Algumas tarefas, como determinar a pose de uma pessoa (estimativa de pose), são resolvidas mais naturalmente se formos para modelos tridimensionais.  Mas até agora não há consenso sobre exatamente como representar modelos 3D - na forma de uma grade, uma nuvem de pontos, <abbr title="Análogos de pixels em 3D.">voxels</abbr> ou <abbr title="Campos de distância assinados - campos de distância assinados.">SDF</abbr> .  Aqui está outra opção: <br><br><img src="https://habrastorage.org/webt/n7/i-/1x/n7i-1xtwmc5xxvs4cfsf4vt4srw.jpeg"><br><br>  Nos panoramas, as convoluções na esfera estão se desenvolvendo ativamente (consulte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Segmentação semântica com reconhecimento de orientação nas esferas de icosaedro</a> ) e a busca por objetos-chave no quadro. <br><br><img src="https://habrastorage.org/webt/bk/4l/gw/bk4lgwc3dzrh_uliw83x21hskyy.png"><br><br><h4>  Definição de postura e previsão de movimentos humanos </h4><br>  Para determinar a pose em 2D, já existe sucesso - agora o foco mudou para o trabalho com várias câmeras e em 3D.  Por exemplo, você pode determinar o esqueleto através da parede, acompanhando as alterações no sinal Wi-Fi à medida que ele passa pelo corpo humano. <br><br>  Muito trabalho foi feito na área de detecção de ponto-chave manual.  Novos conjuntos de dados apareceram, incluindo aqueles baseados em vídeo com diálogos de duas pessoas - agora você pode prever gestos com as mãos por áudio ou texto de uma conversa!  O mesmo progresso foi feito nas tarefas de avaliação do olhar. <br><br><img src="https://habrastorage.org/webt/j0/-j/kv/j0-jkvftadbawqem0ccmm7qmdpa.jpeg"><br><br><img src="https://habrastorage.org/webt/u_/gj/j6/u_gjj6f2d-icebbun428-1e0ztc.jpeg"><br><br>  Você também pode destacar um grande conjunto de trabalhos relacionados à previsão de movimento humano (por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Previsão de movimento humano por meio de pintura espacial e temporal</a> ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Previsão estruturada ajuda a modelagem de movimento humano em 3D</a> ).  A tarefa é importante e, com base nas conversas com os autores, é mais frequentemente usada para analisar o comportamento dos pedestres na direção autônoma. <br><br><h4>  Manipular pessoas em fotos e vídeos, provadores virtuais </h4><br>  A principal tendência é alterar as imagens faciais em termos de parâmetros interpretados.  Idéias: <abbr title="Substituição de estranhos no vídeo.">busca profunda</abbr> em uma imagem, mudança de expressão por renderização de face ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">PuppetGAN</a> ), alteração de parâmetros de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">avanço</a> (por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">idade</a> ).  As transferências de estilo passaram do título do tópico para a aplicação do trabalho.  Outra história - provadores virtuais, eles quase sempre funcionam mal, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui está um exemplo de uma</a> demonstração. <br><br><img src="https://habrastorage.org/webt/u-/q9/rw/u-q9rwdkgxxor2snnsrkstmykbe.jpeg"><br><br><img src="https://habrastorage.org/webt/qw/f4/ys/qwf4ys-wamesjxss4gs30v9wbpq.jpeg"><br><br><h4>  Geração de esboço / gráfico </h4><br>  O desenvolvimento da idéia “Deixe a grade gerar algo com base na experiência anterior” tornou-se diferente: “Vamos mostrar à grade qual opção nos interessa”. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O SC-FEGAN</a> permite que você faça a pintura guiada: o usuário pode desenhar parte do rosto na área apagada da imagem e obter a imagem restaurada, dependendo da renderização. <br><br><img src="https://habrastorage.org/webt/kn/pv/0d/knpv0dzfajvu2hhcbqdgiw-sbek.gif"><br><br>  Em um dos 25 artigos da Adobe para ICCV, dois GANs são combinados: um desenha um esboço para o usuário, o outro gera uma imagem foto-realista a partir do esboço ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">página do projeto</a> ). <br><br><img src="https://habrastorage.org/webt/ua/ba/ap/uabaap4mv5jwm9tdc0qgsxty3ho.gif"><br><br>  No início da geração de imagens, os gráficos não eram necessários, mas agora eles foram transformados em um recipiente de conhecimento sobre a cena.  O prêmio de Menções Honrosas do ICCV de Melhor Artigo também foi concedido ao artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Especificando Atributos e Relações de Objetos na Geração de Cena Interativa</a> .  Em geral, você pode usá-los de diferentes maneiras: gerar gráficos a partir de figuras ou figuras e textos a partir de gráficos. <br><br><img src="https://habrastorage.org/webt/5h/qf/zw/5hqfzwxfjjt-1bnyqopp7ozoqi4.png"><br><br><h4>  Re-identificação de pessoas e máquinas, contando o número de multidões (!) </h4><br>  Muitos artigos são dedicados a rastrear pessoas e <abbr title="Re-identificação - pode ser traduzido gratuitamente como &quot;desanonimização&quot;.">reidentificar</abbr> pessoas e máquinas.  Mas o que nos surpreendeu foi um monte de artigos sobre como contar pessoas no meio da multidão e todos da China. <br><br><div class="spoiler">  <b class="spoiler_title">Pôsteres</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/fh/cn/ma/fhcnma3kitjamuo8yty7hjp1loa.jpeg"><br><br><img src="https://habrastorage.org/webt/ej/_p/66/ej_p66wxpnx52yzlv97osbkys1u.jpeg"><br><br><img src="https://habrastorage.org/webt/j7/7z/bv/j77zbvjegwrfp6emzv-ddcylgmm.jpeg"><br><br><img src="https://habrastorage.org/webt/q9/lw/kr/q9lwkrpkzozcvfa609k6t5krmnw.jpeg"><br><br><img src="https://habrastorage.org/webt/3x/rv/1i/3xrv1ibiocsa5cgbmdvecwxbkyu.jpeg"></div></div><br>  Mas o Facebook, pelo contrário, anonimamente a foto.  Além disso, ele faz isso de uma maneira interessante: ensina a rede neural a gerar um rosto sem detalhes únicos - semelhantes, mas não tanto, a ponto de serem detectados corretamente pelos sistemas de reconhecimento facial. <br><br><img src="https://habrastorage.org/webt/jg/az/oe/jgazoe4ptlfckqpaxdgri2kdlwc.jpeg"><br><br><h4>  Proteção Contra Ataques Adversários </h4><br>  Com o desenvolvimento de aplicativos de visão computacional no mundo real (em veículos não tripulados, no reconhecimento de rostos), a questão da confiabilidade de tais sistemas surge com mais frequência.  Para fazer pleno uso do CV, você precisa ter certeza de que o sistema é resistente a ataques adversários - portanto, não havia menos artigos sobre proteção contra eles do que sobre os próprios ataques.  Muito trabalho consistiu em explicar as previsões da rede (mapa de saliências) e medir a confiança no resultado. <br><br><h4>  Tarefas combinadas </h4><br>  Na maioria das tarefas com um objetivo, as possibilidades de melhorar a qualidade estão quase esgotadas; uma das novas áreas de maior crescimento da qualidade é ensinar as redes neurais a resolver vários problemas semelhantes ao mesmo tempo.  Exemplos: <br>  - previsão de ações + previsão de fluxo óptico, <br>  - apresentação de vídeo + representação da língua ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">VideoBERT</a> ), <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">super-resolução + HDR</a> . <br><br>  E havia artigos sobre segmentação, determinando a postura e a reidentificação dos animais! <br><br><img src="https://habrastorage.org/webt/qe/gk/fi/qegkfif0smvsnit1kjrpdbkajco.jpeg"><br><br><img src="https://habrastorage.org/webt/tz/hk/fo/tzhkfogbi5sxyvzbhu5bjaoii54.jpeg"><br><br><a name="highlights"></a><h2>  Destaques </h2><br>  Quase todos os artigos eram conhecidos com antecedência, o texto estava disponível no arXiv.org.  Portanto, a apresentação de trabalhos como Everybody Dance Now, FUNIT, Image2StyleGAN parece bastante estranha - esses são trabalhos muito úteis, mas não são novos.  Parece que o processo clássico de publicação científica está falhando aqui - a ciência está se desenvolvendo muito rápido. <br><br>  É muito difícil determinar os melhores trabalhos - existem muitos, os assuntos são diferentes.  Vários artigos receberam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">prêmios e referências</a> . <br><br>  Queremos destacar trabalhos que são interessantes em termos de manipulação de imagens, pois esse é o nosso tópico.  Eles se mostraram bastante novos e interessantes para nós (não pretendemos ser objetivos). <br><br><h4>  SinGAN (prêmio de melhor artigo) e InGAN </h4>  SinGAN: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">página do projeto</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">arXiv</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">código</a> . <br>  InGAN: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">página do projeto</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">arXiv</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">código</a> . <br><br>  O desenvolvimento da idéia do Deep Image Prior por Dmitry Ulyanov, Andrea Vedaldi e Victor Lempitsky.  Em vez de treinar a GAN em um conjunto de dados, as redes aprendem com fragmentos da mesma imagem para lembrar as estatísticas dentro dela.  A rede treinada permite editar e animar fotos (SinGAN) ou gerar novas imagens de qualquer tamanho a partir das texturas da imagem original, mantendo a estrutura local (InGAN). <br><br>  SinGAN: <br><br><img src="https://habrastorage.org/webt/oc/ba/pt/ocbaptuxkshaswnhnfhloplqmrm.png"><br><br>  InGAN: <br><br><img src="https://habrastorage.org/webt/xn/cc/i0/xncci0dgonpmajjiv0fisak0twa.gif"><br><br><h4>  Vendo o que um GAN não pode gerar </h4>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Página do projeto</a> . <br><br>  As redes neurais geradoras de imagens geralmente recebem um vetor de ruído aleatório como entrada.  Em uma rede treinada, muitos vetores de entrada formam um espaço, pequenos movimentos ao longo dos quais levam a pequenas mudanças na imagem.  Usando a otimização, você pode resolver o problema inverso: encontre um vetor de entrada adequado para uma imagem do mundo real.  O autor mostra que quase nunca é possível encontrar uma imagem completamente correspondente em uma rede neural quase nunca.  Alguns objetos na imagem não são gerados (aparentemente, devido à grande variabilidade desses objetos). <br><br><img src="https://habrastorage.org/webt/pv/pa/f2/pvpaf2havdxksu-mhmbus-naina.png"><br><br>  O autor propõe que o GAN não cubra todo o espaço das imagens, mas apenas alguns subconjuntos recheados de buracos, como queijo.  Quando tentamos encontrar fotos do mundo real, sempre falhamos, porque a GAN ainda gera fotos não muito reais.  Você pode superar as diferenças entre imagens reais e geradas apenas alterando o peso da rede, ou seja, treinando-a novamente para uma foto específica. <br><br><img src="https://habrastorage.org/webt/ci/vg/bp/civgbpxixfgs_76svkwewjksn3a.jpeg"><br><br>  Quando a rede é treinada novamente para uma foto específica, você pode tentar realizar várias manipulações com esta imagem.  No exemplo abaixo, uma janela foi adicionada à foto e a rede gerou reflexões adicionalmente no conjunto da cozinha.  Isso significa que a rede após o treinamento para a fotografia não perdeu a capacidade de ver a conexão entre os objetos da cena. <br><br><img src="https://habrastorage.org/webt/ov/gp/qd/ovgpqdyldwsdptpav699a_vl2qo.jpeg"><br><br><h4>  GANalyze: Em direção a definições visuais de propriedades da imagem cognitiva </h4>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Página do projeto</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">arXiv</a> . <br><br>  Usando a abordagem deste trabalho, você pode visualizar e analisar o que a rede neural aprendeu.  Os autores propõem o treinamento da GAN para criar imagens para as quais a rede gerará determinadas previsões.  Várias redes foram usadas como exemplos no artigo, incluindo o MemNet, que prevê a memorização de fotos.  Aconteceu que, para uma melhor memorização, o objeto na foto deve: <br><br><ul><li>  estar mais perto do centro </li><li>  ter uma forma redonda ou quadrada e estrutura simples, </li><li>  estar em um fundo uniforme, </li><li>  conter olhos expressivos (pelo menos para fotos de cães), </li><li>  seja mais brilhante, mais rico, em alguns casos - mais vermelho. </li></ul><br><img src="https://habrastorage.org/webt/9y/b2/wd/9yb2wdgndc2qvmugfdk5yctxfbg.png"><br><br><h4>  Liquid Warping GAN: Uma Estrutura Unificada para Imitação de Movimento Humano, Transferência de Aparência e Síntese de Novas Visões </h4>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Página do projeto</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">arXiv</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">código</a> . <br><br>  Pipeline para gerar fotos de pessoas a partir de uma foto.  Os autores mostram exemplos bem-sucedidos de transferir o movimento de uma pessoa para outra, transferir roupas entre as pessoas e gerar novas perspectivas de uma pessoa - tudo a partir de uma fotografia.  Diferentemente dos trabalhos anteriores, aqui, para criar condições, não são usados ​​pontos-chave em 2D (pose), mas uma malha 3D do corpo (pose + forma).  Os autores também descobriram como transferir informações da imagem original para a imagem gerada (Liquid Warping Block).  Os resultados parecem decentes, mas a resolução da imagem resultante é de apenas 256x256.  Para comparação, o vid2vid, que apareceu há um ano, é capaz de gerar uma resolução de 2048x1024, mas precisa de até 10 minutos de gravação de vídeo como um conjunto de dados. <br><br><img src="https://habrastorage.org/webt/el/m1/eh/elm1ehlgjasetl9leqejn9elvbu.png"><br><br><h4>  FSGAN: Troca de Agnóstico de Rosto e Reencenação </h4>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Página do projeto</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">arXiv</a> . <br><br>  A princípio, parece que nada de incomum: deepfake com qualidade mais ou menos normal.  Mas a principal conquista do trabalho é a substituição de rostos em uma imagem.  Ao contrário dos trabalhos anteriores, era necessário treinamento em uma variedade de fotografias de uma pessoa em particular.  O pipeline acabou por ser complicado (reconstituição e segmentação, exibir interpolação, pintura, mistura) e com muitos hacks técnicos, mas o resultado vale a pena. <br><br><img src="https://habrastorage.org/webt/43/33/zn/4333zncbuoqflhf2srkk-05e6m0.png"><br><br><h4>  Detectando o inesperado via ressíntese de imagens </h4>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">arXiv</a> . <br><br>  Como um drone pode entender que um objeto apareceu repentinamente na frente dele e não se enquadra em nenhuma classe de segmentação semântica?  Existem vários métodos, mas os autores oferecem um algoritmo novo e intuitivo que funciona melhor que seus antecessores.  A segmentação semântica é prevista a partir da imagem de entrada da estrada.  Ele é alimentado no GAN (pix2pixHD), que tenta restaurar a imagem original apenas do mapa semântico.  Anomalias que não caem em nenhum dos segmentos diferem significativamente na origem e na imagem gerada.  Em seguida, três imagens (inicial, segmentação e reconstruída) são enviadas para outra rede, que prevê anomalias.  O conjunto de dados para isso foi gerado a partir do conhecido conjunto de dados Cityscapes, alterando acidentalmente as classes na segmentação semântica.  Curiosamente, nesse cenário, um cachorro parado no meio da estrada, mas segmentado corretamente (o que significa que há uma classe para ele), não é uma anomalia, pois o sistema conseguiu reconhecê-lo. <br><br><img src="https://habrastorage.org/webt/fc/1_/ug/fc1_ugp2xbh5qxttjgskyxprji4.png"><br><br><h2>  Conclusão </h2><br>  Antes da conferência, é importante saber quais são seus interesses científicos, quais discursos gostaria de fazer e com quem conversar.  Então tudo será muito mais produtivo. <br><br>  O ICCV é principalmente de rede.  Você entende que existem instituições e cientistas de ponta, começa a entender isso, a conhecer pessoas.  E você pode ler artigos sobre o arXiv - e, a propósito, é muito legal que você não possa ir a lugar algum em busca de conhecimento. <br><br>  Além disso, na conferência, você pode mergulhar profundamente em tópicos que não estão perto de você, veja as tendências.  Bem, escreva uma lista de artigos para ler.  Se você é um estudante, esta é uma oportunidade para você se familiarizar com um cientista em potencial, se você é do setor, depois com um novo empregador e se a empresa se mostrar. <br><br>  Inscreva-se em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">@loss_function_porn</a> !  Este é um projeto pessoal: estamos juntos com a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">karfly</a> .  Todo o trabalho que gostamos durante a conferência, postamos aqui: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">@loss_function_live</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt474902/">https://habr.com/ru/post/pt474902/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt474890/index.html">Teste comparativo de câmeras de celulares antigos e um pouco de história</a></li>
<li><a href="../pt474892/index.html">Programação para crianças. Cinco dos melhores jogos HTML e JavaScript</a></li>
<li><a href="../pt474894/index.html">Resumo através dos olhos de um entrevistador</a></li>
<li><a href="../pt474896/index.html">Os cientistas descobriram um novo fator na entrega eficaz de drogas no tumor</a></li>
<li><a href="../pt474900/index.html">O chip de código aberto OpenTitan substitui as raízes de confiança proprietárias da Intel e ARM</a></li>
<li><a href="../pt474906/index.html">Xamarin.Forms - Mapeamento QRCode decorativo com SkiaSharp</a></li>
<li><a href="../pt474910/index.html">O que brincar com as crianças antes da escola</a></li>
<li><a href="../pt474912/index.html">Mensagens e alertas no Android via JSON</a></li>
<li><a href="../pt474916/index.html">Aplicar o ambiente Nix-Shell no Visual Studio Code</a></li>
<li><a href="../pt474918/index.html">Melhorando o projeto conjunto de componentes eletromecânicos</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>