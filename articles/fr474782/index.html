<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üîí ü§¥üèº üå¶Ô∏è Pr√©sentation de la technologie de synth√®se vocale üç¢ üåÜ üïã</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour √† tous! Je m'appelle Vlad et je travaille en tant que data scientist dans l'√©quipe Tinkoff des technologies vocales utilis√©es dans notre assis...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Pr√©sentation de la technologie de synth√®se vocale</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/tinkoff/blog/474782/"><p>  Bonjour √† tous!  Je m'appelle Vlad et je travaille en tant que data scientist dans l'√©quipe Tinkoff des technologies vocales utilis√©es dans notre assistant vocal Oleg. </p><br><p>  Dans cet article, je voudrais donner un bref aper√ßu des technologies de synth√®se vocale utilis√©es dans l'industrie et partager l'exp√©rience de notre √©quipe dans la construction de notre propre moteur de synth√®se. </p><br><p><img src="https://habrastorage.org/webt/fc/3j/vs/fc3jvsr59z_90ojbvjushekmmm4.png" alt="image"></p><a name="habracut"></a><br><h3 id="sintez-rechi">  Synth√®se vocale </h3><br><p>  La synth√®se vocale est la cr√©ation d'un son bas√© sur du texte.  Ce probl√®me est aujourd'hui r√©solu par deux approches: </p><br><ul><li>  S√©lection d'unit√© [1], ou approche concat√©native.  Il est bas√© sur le collage de fragments d'audio enregistr√©s.  Depuis la fin des ann√©es 90, il a longtemps √©t√© consid√©r√© comme la norme de facto pour le d√©veloppement de moteurs de synth√®se vocale.  Par exemple, une voix √©mise par la m√©thode de s√©lection d'unit√© peut √™tre trouv√©e dans Siri [2]. </li><li>  Synth√®se vocale param√©trique [3], dont l'essence est de construire un mod√®le probabiliste qui pr√©dit les propri√©t√©s acoustiques d'un signal audio pour un texte donn√©. </li></ul><br><p>  La parole des mod√®les de s√©lection d'unit√© est de haute qualit√©, √† faible variabilit√© et n√©cessite une grande quantit√© de donn√©es pour la formation.  Dans le m√™me temps, pour la formation de mod√®les param√©triques, une quantit√© de donn√©es beaucoup plus faible est n√©cessaire, ils g√©n√®rent des intonations plus diverses, mais jusqu'√† r√©cemment, ils souffraient d'une qualit√© sonore globale plut√¥t m√©diocre par rapport √† l'approche de s√©lection d'unit√©. </p><br><p>  Cependant, avec le d√©veloppement des technologies d'apprentissage en profondeur, les mod√®les de synth√®se param√©trique ont connu une croissance significative dans toutes les mesures de qualit√© et sont capables de cr√©er une parole qui est pratiquement impossible √† distinguer de la parole humaine. </p><br><h3 id="metriki-kachestva">  Mesures de qualit√© </h3><br><p> Avant de parler des meilleurs mod√®les de synth√®se vocale, vous devez d√©terminer les mesures de qualit√© par lesquelles les algorithmes seront compar√©s. </p><br><p>  √âtant donn√© que le m√™me texte peut √™tre lu d'une infinit√© de fa√ßons, a priori la bonne fa√ßon de prononcer une phrase sp√©cifique n'existe pas.  Par cons√©quent, souvent les mesures de la qualit√© de la synth√®se vocale sont subjectives et d√©pendent de la perception de l'auditeur. </p><br><p>  La m√©trique standard est le MOS (score d'opinion moyen), une √©valuation moyenne du caract√®re naturel de la parole, donn√©e par les √©valuateurs pour l'audio synth√©tis√© sur une √©chelle de 1 √† 5. Un signifie un son totalement invraisemblable et cinq signifie un discours qui ne peut √™tre distingu√© de l'homme.  Les enregistrements de personnes r√©elles obtiennent g√©n√©ralement environ 4,5, et une valeur sup√©rieure √† 4 est consid√©r√©e comme assez √©lev√©e. </p><br><h3 id="kak-rabotaet-sintez-rechi">  Fonctionnement de la synth√®se vocale </h3><br><p>  La premi√®re √©tape de la construction d'un syst√®me de synth√®se vocale consiste √† collecter des donn√©es pour la formation.  Il s'agit g√©n√©ralement d'enregistrements audio de haute qualit√© sur lesquels l'annonceur lit des phrases sp√©cialement s√©lectionn√©es.  La taille approximative de l'ensemble de donn√©es requis pour les mod√®les de s√©lection des unit√©s d'apprentissage est de 10 √† 20 heures de parole pure [2], tandis que pour les m√©thodes param√©triques de r√©seau neuronal, l'estimation sup√©rieure est d'environ 25 heures [4, 5]. </p><br><p>  Nous discutons des deux technologies de synth√®se. </p><br><h3 id="unit-selection">  S√©lection d'unit√© </h3><br><p><img src="https://habrastorage.org/webt/9-/r7/dm/9-r7dmw2tieg5ypyjbt-lddxddc.png" alt="image"></p><br><p>  En r√®gle g√©n√©rale, le discours enregistr√© du locuteur ne peut pas couvrir tous les cas possibles dans lesquels la synth√®se sera utilis√©e.  Par cons√©quent, l'essence de la m√©thode consiste √† diviser la base audio enti√®re en petits fragments appel√©s unit√©s, qui sont ensuite coll√©s ensemble en utilisant un post-traitement minimal.  Les unit√©s sont g√©n√©ralement des unit√©s de langage acoustique minimales, telles que les demi-t√©l√©phones ou les diphons [2]. <br>  L'ensemble du processus de g√©n√©ration se compose de deux √©tapes: le frontend NLP, qui est responsable de l'extraction de la repr√©sentation linguistique du texte, et le backend, qui calcule la fonction de p√©nalit√© unitaire pour les caract√©ristiques linguistiques donn√©es.  L'interface PNL comprend: </p><br><ol><li>  La t√¢che de normalisation du texte est la traduction de tous les caract√®res non alphab√©tiques (chiffres, signes de pourcentage, devises, etc.) dans leur repr√©sentation verbale.  Par exemple, ¬´5%¬ª doit √™tre converti en ¬´cinq pour cent¬ª. </li><li>  Extraire les caract√©ristiques linguistiques d'un texte normalis√©: repr√©sentation des phon√®mes, stress, parties du discours, etc. </li></ol><br><p>  En r√®gle g√©n√©rale, le frontend NLP est impl√©ment√© √† l'aide de r√®gles prescrites manuellement pour un langage sp√©cifique, mais r√©cemment, il y a eu un biais croissant vers l'utilisation de mod√®les d'apprentissage automatique [7]. </p><br><p>  La p√©nalit√© estim√©e par le sous-syst√®me backend est la somme du co√ªt cible, ou la correspondance de la repr√©sentation acoustique de l'unit√© pour un phon√®me particulier, et du co√ªt de concat√©nation, c'est-√†-dire la pertinence de connecter deux unit√©s voisines.  Pour √©valuer les fonctions fines, on peut utiliser les r√®gles ou le mod√®le acoustique d√©j√† form√© de la synth√®se param√©trique [2].  La s√©lection de la s√©quence d'unit√©s la plus optimale du point de vue des p√©nalit√©s d√©finies ci-dessus s'effectue √† l'aide de l'algorithme de Viterbi [1]. </p><br><p>  Valeurs approximatives des mod√®les de s√©lection d'unit√©s MOS pour la langue anglaise: 3.7-4.1 [2, 4, 5]. </p><br><p>  Avantages de l'approche de s√©lection des unit√©s: </p><br><ul><li>  Le son naturel. </li><li>  G√©n√©ration √† grande vitesse. </li><li>  Petite taille des mod√®les - cela vous permet d'utiliser la synth√®se directement sur votre appareil mobile. </li></ul><br><p>  Inconv√©nients: </p><br><ul><li>  Le discours synth√©tis√© est monotone, ne contient pas d'√©motions. </li><li>  Artefacts de collage caract√©ristiques. </li><li>  Il n√©cessite une base de formation audio suffisamment large pour couvrir toutes sortes de contextes. </li><li>  En principe, il ne peut pas g√©n√©rer de son qui ne se trouve pas dans l'ensemble d'entra√Ænement. </li></ul><br><h3 id="parametricheskiy-sintez-rechi">  Synth√®se vocale param√©trique </h3><br><p>  L'approche param√©trique est bas√©e sur l'id√©e de construire un mod√®le probabiliste qui estime la distribution des caract√©ristiques acoustiques d'un texte donn√©. <br>  Le processus de g√©n√©ration de la parole dans la synth√®se param√©trique peut √™tre divis√© en quatre √©tapes: </p><br><ol><li>  Le frontend NLP est le m√™me stade de pr√©traitement des donn√©es que dans l'approche de s√©lection d'unit√©, dont le r√©sultat est un grand nombre de fonctionnalit√©s linguistiques contextuelles. </li><li>  Mod√®le de dur√©e pr√©disant la dur√©e des phon√®mes. </li><li>  Un mod√®le acoustique qui r√©tablit la distribution des caract√©ristiques acoustiques sur les caract√©ristiques linguistiques.  Les caract√©ristiques acoustiques incluent les valeurs de fr√©quence fondamentales, la repr√©sentation spectrale du signal, etc. </li><li>  Un vocodeur traduisant les caract√©ristiques acoustiques en une onde sonore. </li></ol><br><p>  Pour la dur√©e de la formation et les mod√®les acoustiques, des mod√®les de Markov cach√©s [3], des r√©seaux de neurones profonds ou leurs vari√©t√©s r√©currentes [6] peuvent √™tre utilis√©s.  Un vocodeur traditionnel est un algorithme bas√© sur le mod√®le de filtre source [3], qui suppose que la parole est le r√©sultat de l'application d'un filtre de bruit lin√©aire au signal d'origine. <br>  La qualit√© globale de la parole des m√©thodes param√©triques classiques est assez faible en raison du grand nombre d'hypoth√®ses ind√©pendantes sur la structure du processus de g√©n√©ration du son. </p><br><p>  Cependant, avec l'av√®nement des technologies d'apprentissage en profondeur, il est devenu possible de former des mod√®les de bout en bout qui pr√©disent directement les signes acoustiques par lettre.  Par exemple, les r√©seaux de neurones Tacotron [4] et Tacotron 2 [5] entrent une s√©quence de lettres et renvoient le spectrogramme de craie en utilisant l'algorithme seq2seq [8].  Ainsi, les √©tapes 1 √† 3 de l'approche classique sont remplac√©es par un seul r√©seau neuronal.  Le sch√©ma ci-dessous montre l'architecture du r√©seau Tacotron 2, qui atteint une qualit√© sonore assez √©lev√©e. </p><br><p><img src="https://habrastorage.org/webt/tv/8l/pc/tv8lpchvxw75yr3msdbhjaqscwc.jpeg" alt="image"></p><br><p>  Un autre facteur d'une augmentation significative de la qualit√© de la parole synth√©tis√©e a √©t√© l'utilisation de vocodeurs de r√©seau neuronal au lieu d'algorithmes de traitement du signal num√©rique. </p><br><p>  Le premier de ces vocodeurs √©tait le r√©seau neuronal WaveNet [9], qui s√©quentiellement, √©tape par √©tape, pr√©dit l'amplitude de l'onde sonore. </p><br><p>  En raison de l'utilisation d'un grand nombre de couches convolutives avec des lacunes pour capturer plus de contexte et sauter la connexion dans l'architecture de r√©seau, il a √©t√© possible d'obtenir une am√©lioration d'environ 10% du MOS par rapport aux mod√®les de s√©lection d'unit√©.  Le sch√©ma ci-dessous montre l'architecture du r√©seau WaveNet. </p><br><p><img src="https://habrastorage.org/webt/lg/ei/df/lgeidfeylr_yu-u-ucmdsxi7xki.png" alt="image"></p><br><p>  Le principal inconv√©nient de WaveNet est la faible vitesse associ√©e √† un circuit d'√©chantillonnage de signal s√©rie.  Ce probl√®me peut √™tre r√©solu soit en utilisant l'optimisation d'ing√©nierie pour une architecture de fer sp√©cifique, soit en rempla√ßant le sch√©ma d'√©chantillonnage par un sch√©ma plus rapide. <br>  Les deux approches ont √©t√© mises en ≈ìuvre avec succ√®s dans l'industrie.  Le premier est sur Tinkoff.ru, et dans le cadre de la seconde approche, Google a introduit le r√©seau Parallel WaveNet [10] en 2017, dont les r√©alisations sont utilis√©es dans l'Assistant Google. </p><br><p>  Valeurs approximatives de MOS pour les m√©thodes de r√©seau neuronal: 4,4‚Äì4,5 [5, 11], c'est-√†-dire que la parole synth√©tis√©e n'est pratiquement pas diff√©rente de la parole humaine. </p><br><p>  Avantages de la synth√®se param√©trique: </p><br><ul><li>  Son naturel et lisse lors de l'utilisation de l'approche de bout en bout. </li><li>  Plus grande vari√©t√© d'intonation. </li><li>  Utilisez moins de donn√©es que les mod√®les de s√©lection d'unit√©. </li></ul><br><p>  Inconv√©nients: </p><br><ul><li>  Faible vitesse par rapport √† la s√©lection d'unit√©. </li><li>  Grande complexit√© de calcul. </li></ul><br><h3 id="kak-rabotaet-sintez-rechi-v-tinkoff">  Fonctionnement de la synth√®se vocale Tinkoff </h3><br><p>  Comme il ressort de la revue, les m√©thodes de synth√®se vocale param√©trique bas√©es sur les r√©seaux de neurones sont actuellement nettement sup√©rieures en qualit√© √† l'approche de s√©lection d'unit√© et sont beaucoup plus simples √† d√©velopper.  Par cons√©quent, pour construire notre propre moteur de synth√®se, nous les avons utilis√©s. <br>  Pour les mod√®les de formation, environ 25 heures de discours pur d'un locuteur professionnel ont √©t√© utilis√©es.  Les textes de lecture ont √©t√© sp√©cialement s√©lectionn√©s pour couvrir au mieux la phon√©tique de la parole famili√®re.  De plus, afin d'ajouter plus de vari√©t√© √† la synth√®se en intonation, nous avons demand√© √† l'annonceur de lire des textes avec une expression selon le contexte. </p><br><p>  L'architecture de notre solution ressemble conceptuellement √† ceci: </p><br><ul><li>  Frontend NLP, qui inclut la normalisation du texte du r√©seau neuronal et un mod√®le pour placer les pauses et les contraintes. </li><li>  Tacotron 2 accepte les lettres en entr√©e. </li><li>  WaveNet autor√©gressif, fonctionnant en temps r√©el sur le CPU. </li></ul><br><p>  Gr√¢ce √† cette architecture, notre moteur g√©n√®re une parole expressive de haute qualit√© en temps r√©el, ne n√©cessite pas la construction d'un dictionnaire phon√©mique et permet de contr√¥ler les contraintes dans les mots individuels.  Des exemples d'audio synth√©tis√©s peuvent √™tre entendus en cliquant sur le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lien</a> . </p><br><h3 id="ssylki">  R√©f√©rences: </h3><br><p>  [1] AJ Hunt, AW Black.  S√©lection d'unit√© dans un syst√®me de synth√®se vocale concat√©native √† l'aide d'une grande base de donn√©es vocales, ICASSP, 1996. <br>  [2] T. Capes, P. Coles, A. Conkie, L. Golipour, A. Hadjitarkhani, Q. Hu, N. Huddleston, M. Hunt, J. Li, M. Neeracher, K. Prahallad, T. Raitio , R. Rasipuram, G. Townsend, B. Williamson, D. Winarsky, Z. Wu, H. Zhang.  Syst√®me de synth√®se vocale Siri On-Device Deep Learning-Guided Unit Selection, Interspeech, 2017. <br>  [3] H. Zen, K. Tokuda, AW Black.  Synth√®se statistique param√©trique de la parole, Speech Communication, Vol.  51, non.  11, pp.  1039-1064, 2009. <br>  [4] Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc Le, Yannis Agiomyrgiannakis, Rob Clark, Rif A. Saurous .  Tacotron: Vers une synth√®se vocale de bout en bout. <br>  [5] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, Yannis Agiomyrgiannakis, Yonghui Wu.  Synth√®se TTS naturelle en conditionnant WaveNet sur les pr√©dictions du spectrogramme Mel. <br>  [6] Heiga Zen, Andrew Senior, Mike Schuster.  Synth√®se statistique param√©trique de la parole √† l'aide de r√©seaux de neurones profonds. <br>  [7] Hao Zhang, Richard Sproat, Axel H. Ng, Felix Stahlberg, Xiaochang Peng, Kyle Gorman, Brian Roark.  Mod√®les neuronaux de normalisation de texte pour les applications vocales. <br>  [8] Ilya Sutskever, Oriol Vinyals, Quoc V. Le.  Apprentissage de s√©quence en s√©quence avec les r√©seaux de neurones. <br>  [9] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu.  WaveNet: un mod√®le g√©n√©ratif pour l'audio brut. <br>  [10] Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George van den Driessche, Edward Lockhart, Luis C. Cobo, Florian Stimberg, Norman Casagrande, Dominik Grewe, Seb Noury, Sander Dieleman , Erich Elsen, Nal Kalchbrenner, Heiga Zen, Alex Graves, Helen King, Tom Walters, Dan Belov, Demis Hassabis.  Parallel WaveNet: Synth√®se vocale rapide haute fid√©lit√©. <br>  [11] Wei Ping Kainan Peng Jitong Chen.  ClariNet: g√©n√©ration d'ondes parall√®les dans la synth√®se vocale de bout en bout. <br>  [12] Dario Rethage, Jordi Pons, Xavier Serra.  Un Wavenet pour le d√©bruitage de la parole. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr474782/">https://habr.com/ru/post/fr474782/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr474762/index.html">S√©minaire: Solutions informatiques hybrides pour les entreprises. 14 novembre, Moscou</a></li>
<li><a href="../fr474768/index.html">Diffusion ouverte du Main Hall HighLoad ++ 2019</a></li>
<li><a href="../fr474770/index.html">Comment nous effectuons les tests de r√©gression de la paie dans SAP HCM</a></li>
<li><a href="../fr474772/index.html">Une startup qui a utilis√© l'IA pour d√©velopper un rem√®de en 21 jours</a></li>
<li><a href="../fr474776/index.html">Th√©orie g√©n√©rale et arch√©ologie de la virtualisation x86</a></li>
<li><a href="../fr474784/index.html">Arcade Stick Story</a></li>
<li><a href="../fr474788/index.html">Organisation d'itin√©raires √† Laravel</a></li>
<li><a href="../fr474790/index.html">Contes de n√©gociateur</a></li>
<li><a href="../fr474792/index.html">6-8 d√©cembre - Rosbank Tech.Madness Hackathon</a></li>
<li><a href="../fr474796/index.html">Qu'est-ce que l'Internet des objets et comment aidera-t-il les entreprises √† gagner plus?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>