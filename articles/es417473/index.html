<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>💆🏿 🙏🏿 🚴 Almacenamiento confiable con DRBD9 y Proxmox (Parte 1: NFS) ⭐️ 💪🏽 🐈</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Probablemente, todos los que al menos una vez se quedaron perplejos por la búsqueda de almacenamiento definido por software de alto rendimiento tarde ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Almacenamiento confiable con DRBD9 y Proxmox (Parte 1: NFS)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/417473/"><p><img src="https://habrastorage.org/getpro/habr/post_images/f6a/771/043/f6a7710433c8887d6bbbe4792cc178e1.jpg" alt="imagen"></p><br><p>  Probablemente, todos los que al menos una vez se quedaron perplejos por la búsqueda de <strong>almacenamiento definido por software de</strong> alto rendimiento tarde o temprano escucharon acerca de <strong>DRBD</strong> , o tal vez incluso lo trataron. </p><br><p> Es cierto que en la cima de la popularidad de <strong>Ceph</strong> y <strong>GlusterFS</strong> , que funcionan bastante bien en principio, y lo más importante desde el primer momento, todos se olvidaron un poco.  Además, la versión anterior no admitía la replicación en más de dos nodos, y debido a esto, a menudo se encontraban problemas con <strong>el cerebro dividido</strong> , lo que claramente no aumentaba su popularidad. </p><br><p>  La solución realmente no es nueva, pero sí bastante competitiva.  Con costos relativamente bajos para CPU y RAM, <strong>DRBD</strong> proporciona una sincronización realmente rápida y segura a nivel de <strong>dispositivo de bloque</strong> .  Durante todo este tiempo, los desarrolladores de LINBIT - DRBD no se detienen y lo refinan constantemente.  Comenzando con la versión <strong>DRBD9</strong> , deja de ser solo un espejo de red y se convierte en algo más. </p><br><p>  En primer lugar, la idea de crear un único <strong>dispositivo de bloque distribuido</strong> para varios servidores ha retrocedido en segundo plano, y ahora LINBIT está tratando de proporcionar herramientas para orquestar y administrar muchos dispositivos drbd en un clúster que se crean sobre <strong>particiones</strong> <strong>LVM</strong> y <strong>ZFS</strong> . </p><br><p>  Por ejemplo, DRBD9 admite hasta 32 réplicas, RDMA, nodos sin disco y nuevas herramientas de orquestación que le permiten usar instantáneas, migración en línea y mucho más. </p><br><p>  A pesar de que <strong>DRBD9</strong> tiene herramientas de integración con <strong>Proxmox</strong> , <strong>Kubernetes</strong> , <strong>OpenStack</strong> y <strong>OpenNebula</strong> , en este momento están en algún modo de transición, cuando las nuevas herramientas aún no son compatibles en todas partes, y las antiguas se anunciarán como <em>obsoletas</em> muy pronto.  Estos son <strong>DRBDmanage</strong> y <strong>Linstor</strong> . </p><br><p>  Aprovecharé este momento para no entrar en detalles de cada uno de ellos, sino para examinar con más detalle la configuración y los principios de trabajo con <strong>DRBD9</strong> . <a name="habracut"></a>  Todavía tiene que resolverlo, aunque solo sea porque la configuración tolerante a fallas del controlador Linstor implica instalarlo en uno de estos dispositivos. </p><br><p>  En este artículo, me gustaría informarle sobre <strong>DRBD9</strong> y la posibilidad de su uso en <strong>Proxmox</strong> sin complementos de terceros. </p><br><h2 id="drbdmanage-i-linstor">  DRBDmanage y Linstor </h2><br><p>  En primer lugar, vale la pena mencionar una vez más sobre <strong>DRBDmanage</strong> , que se integra muy bien en <strong>Proxmox</strong> .  LINBIT proporciona un complemento DRBDmanage listo para usar para Proxmox que le permite utilizar todas sus funciones directamente desde la interfaz de <strong>Proxmox</strong> . </p><br><p>  Se ve realmente increíble, pero desafortunadamente tiene algunas desventajas. </p><br><ul><li> Primero, los nombres de volumen etiquetados, el <strong>grupo LVM</strong> o el <strong>grupo</strong> <strong>ZFS</strong> deben llamarse <code>drbdpool</code> . </li><li>  Incapacidad para usar más de <strong>un</strong> grupo por nodo </li><li>  Debido a los detalles de la solución, el <strong>volumen</strong> del <strong>controlador</strong> solo puede estar en un LVM normal y no de otra manera </li><li>  <strong>Fallas</strong> periódicas en <strong>dbus</strong> , que <strong>DRBDmanage</strong> utiliza de <strong>cerca</strong> para interactuar con los nodos. </li></ul><br><p>  Como resultado, LINBIT decidió reemplazar toda la lógica compleja de DRBDmanage con una aplicación simple que se comunica con los nodos usando una <strong>conexión tcp</strong> regular y funciona sin ninguna magia allí.  Entonces estaba <strong>Linstor</strong> . </p><br><p>  <strong>Linstor</strong> realmente funciona muy bien.  Desafortunadamente, los desarrolladores eligieron <strong>Java</strong> como el idioma principal para escribir el servidor Linstor, pero no dejes que esto te asuste, ya que a Linstor solo le preocupa <strong>distribuir configuraciones</strong> DRBD y <strong>cortar</strong> particiones LVM / ZFS en los nodos. </p><br><blockquote>  Ambas soluciones son gratuitas y se distribuyen bajo la licencia <strong>GPL3</strong> gratuita <strong>.</strong> </blockquote><p>  Puede leer sobre cada uno de ellos y sobre cómo configurar el complemento mencionado anteriormente para <strong>Proxmox</strong> en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">wiki oficial de Proxmox</a> </p><br><h2 id="otkazoustoychivyy-nfs-server">  Servidor NFS de conmutación por error </h2><br><p>  Desafortunadamente, al momento de escribir, <strong>Linstor solo</strong> tiene integración con <strong>Kubernetes</strong> .  Pero a fin de año, se esperan controladores para el resto de los sistemas <strong>Proxmox</strong> , <strong>OpenNebula</strong> , <strong>OpenStack</strong> . </p><br><p>  Pero hasta ahora no existe una solución preparada, pero de alguna manera no nos gusta la anterior.  Intentemos usar DRBD9 de la manera tradicional para organizar el <strong>acceso NFS</strong> a una partición compartida. </p><br><p>  Sin embargo, esta solución también demostrará no estar exenta de ventajas, ya que el servidor NFS le permitirá organizar el <strong>acceso competitivo</strong> al sistema de archivos del repositorio desde varios servidores sin la necesidad de sistemas de archivos de clúster complejos con DLM, como OCFS y GFS2. </p><br><p>  En este caso, podrá cambiar las funciones del nodo <strong>primario</strong> / <strong>secundario</strong> simplemente migrando el contenedor con el servidor NFS en la interfaz Proxmox. </p><br><p>  También puede almacenar cualquier archivo dentro de este sistema de archivos, así como discos virtuales y copias de seguridad. </p><br><p>  En caso de que use <strong>Kubernetes</strong> , podrá organizar el acceso <strong>ReadWriteMany</strong> para sus <strong>PersistentVolumes</strong> . </p><br><h2 id="proxmox-i-lxc-konteynery">  Contenedores Proxmox y LXC </h2><br><p>  Ahora la pregunta es: ¿por qué Proxmox? </p><br><p>  En principio, para construir dicho esquema, podríamos usar Kubernetes, así como el esquema habitual con un administrador de clúster.  Pero <strong>Proxmox</strong> proporciona una interfaz lista para usar, muy multifuncional y al mismo tiempo simple e intuitiva para casi todo lo que necesita.  Está listo para usar, es capaz de <strong>agruparse</strong> y es compatible con el mecanismo de <strong>cercado</strong> basado en softdog.  Y cuando usa <strong>contenedores LXC,</strong> le permite lograr tiempos de espera mínimos al cambiar. <br>  La solución resultante no tendrá un solo <strong>punto de falla</strong> . </p><br><p>  De hecho, utilizaremos Proxmox principalmente como un <strong>administrador de clúster</strong> , donde podemos considerar un <strong>contenedor LXC</strong> separado como un servicio que se ejecuta en un clúster HA clásico, solo con la diferencia de que el contenedor también viene con su <strong>sistema raíz</strong> .  Es decir, no necesita instalar varias instancias de servicio en cada servidor por separado, solo puede hacerlo una vez dentro del contenedor. <br>  Si alguna vez trabajó con el <strong>software de administración de clúster</strong> y proporcionó <strong>HA</strong> para aplicaciones, comprenderá lo que quiero decir. </p><br><h2 id="obschaya-shema">  Esquema general </h2><br><p>  Nuestra solución se parecerá al esquema de replicación estándar de una base de datos. </p><br><ul><li>  Tenemos <strong>tres nodos</strong> </li><li>  Cada nodo tiene un <strong>dispositivo drbd</strong> distribuido. </li><li>  El dispositivo tiene un sistema de archivos normal ( <strong>ext4</strong> ) </li><li>  Solo un servidor puede ser <strong>maestro</strong> </li><li>  El <strong>servidor NFS</strong> en el <strong>contenedor LXC se</strong> inicia en el asistente. </li><li>  Todos los nodos acceden al dispositivo estrictamente a través de <strong>NFS.</strong> </li><li>  Si es necesario, el asistente puede moverse a otro nodo, junto con el <strong>servidor NFS</strong> </li></ul><br><p>  <strong>DRBD9</strong> tiene una característica muy interesante que simplifica enormemente todo: <br>  El dispositivo drbd se convierte automáticamente en <strong>Primario</strong> en el momento en que está montado en algún nodo.  Si el dispositivo está marcado como <strong>Primario</strong> , cualquier intento de montarlo en otro nodo generará un error de acceso.  Esto garantiza el bloqueo y la protección garantizada contra el acceso simultáneo al dispositivo. </p><br><p>  ¿Por qué todo esto se simplifica enormemente?  Porque cuando el contenedor se inicia, <strong>Proxmox</strong> monta automáticamente este dispositivo y se convierte en <strong>primario</strong> en este nodo, y cuando el contenedor se detiene, lo desmonta por el contrario y el dispositivo vuelve a ser <strong>secundario</strong> . <br>  Por lo tanto, ya no tenemos que preocuparnos por cambiar los dispositivos <strong>primarios</strong> / <strong>secundarios</strong> , Proxmox lo hará <strong>automáticamente</strong> , ¡Hurra! </p><br><h2 id="nastroyka-drbd">  Configuración DRBD </h2><br><p>  Bueno, hemos descubierto la idea. Ahora pasemos a la implementación. </p><br><p>  Por defecto <strong>, la octava versión de drbd</strong> se suministra <strong>con el kernel de Linux</strong> , desafortunadamente <strong>no</strong> nos <strong>conviene</strong> y necesitamos instalar la novena versión del módulo. </p><br><p>  Conecte el repositorio de LINBIT e instale todo lo que necesita: </p><br><pre> <code class="bash hljs">wget -O- https://packages.linbit.com/package-signing-pubkey.asc | apt-key add - <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"deb http://packages.linbit.com/proxmox/ proxmox-5 drbd-9.0"</span></span> \ &gt; /etc/apt/sources.list.d/linbit.list apt-get update &amp;&amp; apt-get -y install pve-headers drbd-dkms drbd-utils drbdtop</code> </pre> <br><ul><li>  <code>pve-headers</code> : <code>pve-headers</code> kernel necesarios para compilar el módulo </li><li>  <code>drbd-dkms</code> - módulo de kernel en formato DKMS </li><li>  <code>drbd-utils</code> - utilidades básicas de gestión de DRBD </li><li>  <code>drbdtop</code> es una herramienta interactiva como top solo para DRBD </li></ul><br><p>  Después de instalar el <strong>módulo,</strong> comprobaremos si todo está en orden: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># modprobe drbd # cat /proc/drbd version: 9.0.14-1 (api:2/proto:86-113)</span></span></code> </pre> <br><p>  Si ve la <strong>octava versión</strong> en la salida del comando, entonces algo salió mal y se carga el módulo del núcleo <strong>en el árbol</strong> .  Compruebe el <code>dkms status</code> averiguar cuál es el motivo. </p><br><p>  Cada nodo que tengamos tendrá el mismo <strong>dispositivo drbd</strong> ejecutándose sobre las particiones regulares.  Primero necesitamos preparar esta sección para drbd en cada nodo. </p><br><p>  Dicha partición puede ser cualquier <strong>dispositivo de bloque</strong> , puede ser lvm, zvol, una partición de disco o todo el disco.  En este artículo usaré un disco nvme separado con una partición en drbd: <code>/dev/nvme1n1p1</code> </p><br><p>  Vale la pena señalar que los nombres de los dispositivos tienden a cambiar a veces, por lo que es mejor tomarlo inmediatamente como un hábito para usar un enlace simbólico constante al dispositivo. </p><br><p>  Puede encontrar un enlace simbólico para <code>/dev/nvme1n1p1</code> esta manera: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># find /dev/disk/ -lname '*/nvme1n1p1' /dev/disk/by-partuuid/847b9713-8c00-48a1-8dff-f84c328b9da2 /dev/disk/by-path/pci-0000:0e:00.0-nvme-1-part1 /dev/disk/by-id/nvme-eui.0000000001000000e4d25c33da9f4d01-part1 /dev/disk/by-id/nvme-INTEL_SSDPEKKA010T7_BTPY703505FB1P0H-part1</span></span></code> </pre> <br><p>  Describimos nuestro recurso en los tres nodos: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cat /etc/drbd.d/nfs1.res resource nfs1 { meta-disk internal; device /dev/drbd100; protocol C; net { after-sb-0pri discard-zero-changes; after-sb-1pri discard-secondary; after-sb-2pri disconnect; } on pve1 { address 192.168.2.11:7000; disk /dev/disk/by-partuuid/95e7eabb-436e-4585-94ea-961ceac936f7; node-id 0; } on pve2 { address 192.168.2.12:7000; disk /dev/disk/by-partuuid/aa7490c0-fe1a-4b1f-ba3f-0ddee07dfee3; node-id 1; } on pve3 { address 192.168.2.13:7000; disk /dev/disk/by-partuuid/847b9713-8c00-48a1-8dff-f84c328b9da2; node-id 2; } connection-mesh { hosts pve1 pve2 pve3; } }</span></span></code> </pre> <br><p>  Es recomendable utilizar una <strong>red separada</strong> para la sincronización drbd. </p><br><p>  Ahora cree los metadatos para drbd y ejecútelo: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm create-md nfs1 initializing activity log initializing bitmap (320 KB) to all zero Writing meta data... New drbd meta data block successfully created. success # drbdadm up nfs1</span></span></code> </pre> <br><p>  Repita estos pasos en los tres nodos y verifique el estado: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm status nfs1 role:Secondary disk:Inconsistent pve2 role:Secondary peer-disk:Inconsistent pve3 role:Secondary peer-disk:Inconsistent</span></span></code> </pre> <br><p>  Ahora nuestro disco <strong>inconsistente está</strong> en los tres nodos, esto se debe a que drbd no sabe qué disco debe tomarse como el original.  Debemos marcar uno de ellos como <strong>Primario</strong> para que su estado se sincronice con los otros nodos: </p><br><pre> <code class="bash hljs">drbdadm primary --force nfs1 drbdadm secondary nfs1</code> </pre> <br><p>  Inmediatamente después de esto, comenzará la <strong>sincronización</strong> : </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm status nfs1 role:Secondary disk:UpToDate pve2 role:Secondary replication:SyncSource peer-disk:Inconsistent done:26.66 pve3 role:Secondary replication:SyncSource peer-disk:Inconsistent done:14.20</span></span></code> </pre><br><p>  No tenemos que esperar a que termine y podemos llevar a cabo más pasos en paralelo.  Se pueden ejecutar en <strong>cualquier nodo</strong> , independientemente de su estado actual del disco local en DRBD.  Todas las solicitudes serán redirigidas automáticamente al dispositivo con el estado <strong>UpToDate</strong> . </p><br><p>  No olvide activar la <strong>ejecución automática del</strong> servicio drbd en los nodos: </p><br><pre> <code class="hljs pgsql">systemctl <span class="hljs-keyword"><span class="hljs-keyword">enable</span></span> drbd.service</code> </pre> <br><h2 id="nastroyka-lxc-konteynera">  Configurar un contenedor LXC </h2><br><p>  Omitiremos la parte de configuración del <strong>clúster Proxmox</strong> de tres nodos, esta parte está bien descrita en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">wiki oficial</a> </p><br><p>  Como dije antes, nuestro <strong>servidor NFS</strong> funcionará en un <strong>contenedor LXC</strong> .  Mantendremos el contenedor en el dispositivo <code>/dev/drbd100</code> que acabamos de crear. </p><br><p>  Primero necesitamos crear un <strong>sistema de archivos</strong> en él: </p><br><pre> <code class="hljs powershell">mkfs <span class="hljs-literal"><span class="hljs-literal">-t</span></span> ext4 <span class="hljs-literal"><span class="hljs-literal">-O</span></span> mmp <span class="hljs-literal"><span class="hljs-literal">-E</span></span> mmp_update_interval=<span class="hljs-number"><span class="hljs-number">5</span></span> /dev/drbd100</code> </pre> <br><p>  <strong>Proxmox</strong> por defecto incluye <strong>protección multimount</strong> a nivel del sistema de archivos, en principio, podemos prescindir de él, porque  DRBD tiene su propia protección por defecto, simplemente prohíbe el segundo <strong>primario</strong> para el dispositivo, pero la precaución no nos perjudica. </p><br><p>  Ahora descargue la plantilla de Ubuntu: </p><br><pre> <code class="hljs pgsql"># wget http://download.proxmox.com/images/<span class="hljs-keyword"><span class="hljs-keyword">system</span></span>/ubuntu<span class="hljs-number"><span class="hljs-number">-16.04</span></span>-standard_16<span class="hljs-number"><span class="hljs-number">.04</span></span><span class="hljs-number"><span class="hljs-number">-1</span></span>_amd64.tar.gz -P /var/lib/vz/<span class="hljs-keyword"><span class="hljs-keyword">template</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">cache</span></span>/</code> </pre> <br><p>  Y crea nuestro contenedor a partir de él: </p><br><pre> <code class="hljs powershell">pct create <span class="hljs-number"><span class="hljs-number">101</span></span> local:vztmpl/ubuntu<span class="hljs-literal"><span class="hljs-literal">-16</span></span>.<span class="hljs-number"><span class="hljs-number">04</span></span><span class="hljs-literal"><span class="hljs-literal">-standard_16</span></span>.<span class="hljs-number"><span class="hljs-number">04</span></span><span class="hljs-literal"><span class="hljs-literal">-1_amd64</span></span>.tar.gz \ -<span class="hljs-literal"><span class="hljs-literal">-hostname</span></span>=nfs1 \ -<span class="hljs-literal"><span class="hljs-literal">-net0</span></span>=name=eth0,bridge=vmbr0,gw=<span class="hljs-number"><span class="hljs-number">192.168</span></span>.<span class="hljs-number"><span class="hljs-number">1.1</span></span>,ip=<span class="hljs-number"><span class="hljs-number">192.168</span></span>.<span class="hljs-number"><span class="hljs-number">1.11</span></span>/<span class="hljs-number"><span class="hljs-number">24</span></span> \ -<span class="hljs-literal"><span class="hljs-literal">-rootfs</span></span>=volume=/dev/drbd100,shared=<span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br><p>  En este comando, indicamos que el <strong>sistema raíz de</strong> nuestro contenedor estará en el dispositivo <code>/dev/drbd100</code> y agregaremos el parámetro <code>shared=1</code> para permitir la <strong>migración del</strong> contenedor entre nodos. </p><br><p>  Si algo salió mal, siempre puede solucionarlo a través de la interfaz <strong>Proxmox</strong> o en la <code>/etc/pve/lxc/101.conf</code> contenedor <code>/etc/pve/lxc/101.conf</code> </p><br><p>  Proxmox desempaquetará la plantilla y preparará <strong>el sistema raíz del</strong> contenedor para nosotros.  Después de eso podemos lanzar nuestro contenedor: </p><br><pre> <code class="hljs pgsql">pct <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span></code> </pre> <br><h2 id="nastroyka-nfs-servera">  Configurar un servidor NFS. </h2><br><p>  De forma predeterminada, Proxmox <strong>no permite que</strong> el <strong>servidor NFS se</strong> ejecute en el contenedor, pero hay varias formas de habilitarlo. </p><br><p>  Una de ellas es simplemente agregar <code>lxc.apparmor.profile: unconfined</code> a la <code>/etc/pve/lxc/100.conf</code> nuestro contenedor <code>/etc/pve/lxc/100.conf</code> . </p><br><p>  O podemos <strong>habilitar NFS</strong> para todos los contenedores de forma continua, para esto necesitamos actualizar la plantilla estándar para LXC en todos los nodos, agregar las siguientes líneas a <code>/etc/apparmor.d/lxc/lxc-default-cgns</code> : </p><br><pre> <code class="hljs nginx"> <span class="hljs-attribute"><span class="hljs-attribute">mount</span></span> fstype=nfs, mount fstype=nfs4, mount fstype=nfsd, mount fstype=rpc_pipefs,</code> </pre> <br><p>  Después de los cambios, reinicie el contenedor: </p><br><pre> <code class="hljs pgsql">pct shutdown <span class="hljs-number"><span class="hljs-number">101</span></span> pct <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span></code> </pre> <br><p>  Ahora iniciemos sesión: </p><br><pre> <code class="hljs perl">pct <span class="hljs-keyword"><span class="hljs-keyword">exec</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span> bash</code> </pre> <br><p>  Instalar actualizaciones y <strong>servidor NFS</strong> : </p><br><pre> <code class="hljs powershell">apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> update apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> upgrade apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install nfs<span class="hljs-literal"><span class="hljs-literal">-kernel</span></span><span class="hljs-literal"><span class="hljs-literal">-server</span></span></code> </pre> <br><p>  Crea una <strong>exportación</strong> : </p><br><pre> <code class="hljs haskell"><span class="hljs-title"><span class="hljs-title">echo</span></span> '/<span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class"> *(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">rw</span></span></span><span class="hljs-class">,</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">no_root_squash</span></span></span><span class="hljs-class">,</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">no_subtree_check</span></span></span><span class="hljs-class">)' &gt;&gt; /etc/exports mkdir /</span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class"> exportfs -a</span></span></code> </pre> <br><h2 id="nastroyka-ha">  Configuración HA </h2><br><p>  Al momento de escribir este artículo, proxmox <strong>HA-manager</strong> tiene un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">error</a> que no permite que el contenedor HA complete su trabajo con éxito, como resultado de lo cual los procesos del <strong>espacio</strong> del <strong>kernel</strong> del <strong>servidor nfs</strong> que no se eliminaron por completo impiden que el dispositivo drbd salga de <strong>Secundario</strong> .  Si ya se ha encontrado con tal situación, no debe entrar en pánico y simplemente ejecutar <code>killall -9 nfsd</code> en el nodo donde se lanzó el contenedor y luego el dispositivo drbd debería "liberarse" y se irá a <strong>Secundario</strong> . </p><br><p>  Para corregir este error, ejecute los siguientes comandos en todos los nodos: </p><br><pre> <code class="hljs powershell">sed <span class="hljs-literal"><span class="hljs-literal">-i</span></span> <span class="hljs-string"><span class="hljs-string">'s/forceStop =&gt; 1,/forceStop =&gt; 0,/'</span></span> /usr/share/perl5/PVE/HA/Resources/PVECT.pm systemctl restart pve<span class="hljs-literal"><span class="hljs-literal">-ha</span></span><span class="hljs-literal"><span class="hljs-literal">-lrm</span></span>.service</code> </pre> <br><p>  Ahora podemos pasar a la configuración de <strong>HA-manager</strong> .  Creemos un grupo HA separado para nuestro dispositivo: </p><br><pre> <code class="hljs powershell">ha<span class="hljs-literal"><span class="hljs-literal">-manager</span></span> groupadd nfs1 -<span class="hljs-literal"><span class="hljs-literal">-nodes</span></span> pve1,pve2,pve3 -<span class="hljs-literal"><span class="hljs-literal">-nofailback</span></span>=<span class="hljs-number"><span class="hljs-number">1</span></span> -<span class="hljs-literal"><span class="hljs-literal">-restricted</span></span>=<span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br><p>  Nuestro <strong>recurso</strong> solo funcionará en los nodos especificados para este grupo.  Agregue nuestro contenedor a este grupo: </p><br><pre> <code class="hljs powershell">ha<span class="hljs-literal"><span class="hljs-literal">-manager</span></span> add ct:<span class="hljs-number"><span class="hljs-number">101</span></span> -<span class="hljs-literal"><span class="hljs-literal">-group</span></span>=nfs1 -<span class="hljs-literal"><span class="hljs-literal">-max_relocate</span></span>=<span class="hljs-number"><span class="hljs-number">3</span></span> -<span class="hljs-literal"><span class="hljs-literal">-max_restart</span></span>=<span class="hljs-number"><span class="hljs-number">3</span></span></code> </pre> <br><p>  Eso es todo.  Simple, ¿verdad? </p><br><p>  La <strong>bola nfs</strong> resultante se puede conectar inmediatamente a Proxmox para almacenar y ejecutar otras máquinas virtuales y contenedores. </p><br><h2 id="rekomendacii-i-tyuning">  Recomendaciones y puesta a punto </h2><br><h5 id="drbd">  DRBD </h5><br><p>  Como señalé anteriormente, siempre es recomendable usar una red separada para la replicación.  Es muy recomendable utilizar <strong>adaptadores de red de 10 gigabits</strong> , de lo contrario, se encontrará con la velocidad del puerto. <br>  Si la replicación parece lo suficientemente lenta, pruebe algunas de las opciones para <strong>DRBD</strong> .  Aquí está la configuración, que en mi opinión es óptima para mi <strong>red 10G</strong> : </p><br><pre> <code class="hljs swift"># cat /etc/drbd.d/global_common.conf global { usage-<span class="hljs-built_in"><span class="hljs-built_in">count</span></span> yes; udev-always-use-vnr; } common { handlers { } startup { } options { } disk { <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-fill-target 10M; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-<span class="hljs-built_in"><span class="hljs-built_in">max</span></span>-rate 720M; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-plan-ahead <span class="hljs-number"><span class="hljs-number">10</span></span>; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-<span class="hljs-built_in"><span class="hljs-built_in">min</span></span>-rate 20M; } net { <span class="hljs-built_in"><span class="hljs-built_in">max</span></span>-buffers 36k; sndbuf-size 1024k; rcvbuf-size 2048k; } }</code> </pre> <br><p>  Puede obtener más información sobre cada parámetro de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentación oficial de DRBD.</a> </p><br><h5 id="nfs-server">  Servidor nfs </h5><br><p>  Para acelerar el funcionamiento del <strong>servidor NFS,</strong> puede ayudar aumentar el número total de <strong>instancias del</strong> servidor NFS en ejecución.  Por defecto, <strong>8</strong> , personalmente, me ayudó a aumentar este número a <strong>64</strong> . </p><br><p>  Para lograr esto, actualice el parámetro <code>RPCNFSDCOUNT=64</code> en <code>/etc/default/nfs-kernel-server</code> . <br>  Y reinicia los demonios: </p><br><pre> <code class="hljs pgsql">systemctl <span class="hljs-keyword"><span class="hljs-keyword">restart</span></span> nfs-utils systemctl <span class="hljs-keyword"><span class="hljs-keyword">restart</span></span> nfs-<span class="hljs-keyword"><span class="hljs-keyword">server</span></span></code> </pre> <br><h5 id="nfsv3-vs-nfsv4">  NFSv3 vs NFSv4 </h5><br><p>  ¿Conoces la diferencia entre <strong>NFSv3</strong> y <strong>NFSv4</strong> ? </p><br><ul><li>  <strong>NFSv3</strong> es un <strong>protocolo sin estado;</strong> como regla, tolera mejor las fallas y se recupera más rápido. </li><li>  <strong>NFSv4</strong> es un <strong>protocolo con estado</strong> , funciona más rápido y se puede vincular a ciertos puertos tcp, pero debido a la presencia de estado es más sensible a fallas.  También tiene la capacidad de usar autenticación usando Kerberos y muchas otras características interesantes. </li></ul><br><p>  Sin embargo, cuando ejecuta <code>showmount -e nfs_server</code> , se utiliza el protocolo NFSv3.  Proxmox también usa NFSv3.  NFSv3 también se usa comúnmente para organizar máquinas de arranque en red. </p><br><p>  En general, si no tiene una razón particular para usar NFSv4, intente usar NFSv3 ya que es menos doloroso por cualquier falla debido a la falta de un estado como tal. </p><br><p>  Puede montar la bola usando NFSv3 especificando el parámetro <code>-o vers=3</code> para el comando de <strong>montaje</strong> : </p><br><pre> <code class="bash hljs">mount -o vers=3 nfs_server:/share /mnt</code> </pre> <br><p>  Si lo desea, puede deshabilitar NFSv4 para el servidor, para hacer esto, agregue la <code>--no-nfs-version 4</code> a la variable <code>--no-nfs-version 4</code> y reinicie el servidor, por ejemplo: </p><br><pre> <code class="bash hljs">RPCNFSDCOUNT=<span class="hljs-string"><span class="hljs-string">"64 --no-nfs-version 4"</span></span></code> </pre> <br><h2 id="iscsi-i-lvm">  iSCSI y LVM </h2><br><p>  Del mismo modo, se puede configurar un <strong>demonio tgt</strong> normal dentro del contenedor, iSCSI producirá un rendimiento significativamente mayor para las operaciones de E / S, y el contenedor funcionará de manera más fluida porque el servidor tgt funciona completamente en el espacio del usuario. </p><br><p>  Por lo general, un <strong>LUN</strong> exportado se corta en muchas piezas con <strong>LVM</strong> .  Sin embargo, hay varios matices a tener en cuenta, por ejemplo: cómo <strong>se</strong> proporcionan <strong>bloqueos</strong> LVM para compartir un grupo exportado en varios hosts. </p><br><p>  Quizás estos y otros matices que describiré en el <strong><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">próximo artículo</a></strong> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es417473/">https://habr.com/ru/post/es417473/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de486150/index.html">Entwicklung der IT-Sphäre in der Slowakei. Arbeitsnutzen für junge Berufstätige</a></li>
<li><a href="../de486156/index.html">Da habe ich unterrichtet und dann ein Trainingshandbuch in Python geschrieben</a></li>
<li><a href="../de486158/index.html">Visualisierung der neuronalen maschinellen Übersetzung (seq2seq-Modelle mit Aufmerksamkeitsmechanismus)</a></li>
<li><a href="../de486164/index.html">Coronavirus 2019-nCoV. FAQ zu Atemschutz und Desinfektion</a></li>
<li><a href="../de486174/index.html">Ich habe keinen Umsatz</a></li>
<li><a href="../es417475/index.html">Codificación de borrado Glusterfs +: cuando necesita mucho, barato y confiable</a></li>
<li><a href="../es417477/index.html">Escritorio caliente</a></li>
<li><a href="../es417479/index.html">Concatenación de cadenas de bricolaje más rápida en Go</a></li>
<li><a href="../es417481/index.html">Acerca de los generadores en JavaScript ES6, y por qué es opcional estudiarlos</a></li>
<li><a href="../es417483/index.html">Comparación de marcos JS: React, Vue e Hyperapp</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>