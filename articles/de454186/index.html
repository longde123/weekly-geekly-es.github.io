<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤷🏽 🚑 🤸🏼 Erstellen Sie eine Pipeline für die Streaming-Datenverarbeitung. Teil 1 🧦 👨‍👦 🤶🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo an alle. Freunde, wir teilen Ihnen eine Übersetzung eines Artikels mit, der speziell für Studenten des Data Engineer- Kurses erstellt wurde. Las...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Erstellen Sie eine Pipeline für die Streaming-Datenverarbeitung. Teil 1</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/454186/">  Hallo an alle.  Freunde, wir teilen Ihnen eine Übersetzung eines Artikels mit, der speziell für Studenten des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Data Engineer-</a> Kurses erstellt wurde.  Lass uns gehen! <br><br><img src="https://habrastorage.org/webt/fv/mo/wz/fvmowz46naiimojhlq03dhxm38w.png"><br><br><h2>  Apache Beam und DataFlow für Echtzeit-Pipelines </h2><br>  Der heutige Beitrag basiert auf einer Aufgabe, an der ich kürzlich bei der Arbeit gearbeitet habe.  Ich war sehr glücklich, es zu implementieren und die geleistete Arbeit im Format eines Blogposts zu beschreiben, da es mir die Möglichkeit gab, an der Datenentwicklung zu arbeiten und etwas zu tun, das für mein Team sehr nützlich wäre.  Vor nicht allzu langer Zeit habe ich festgestellt, dass unsere Systeme eine relativ große Anzahl von Benutzerprotokollen speichern, die sich auf eines unserer Produkte für die Arbeit mit Daten beziehen.  Es stellte sich heraus, dass niemand diese Daten verwendet hatte, und ich interessierte mich sofort für das, was wir herausfinden konnten, wenn wir anfingen, sie regelmäßig zu analysieren.  Es gab jedoch einige Probleme auf dem Weg.  Das erste Problem war, dass die Daten in vielen verschiedenen Textdateien gespeichert wurden, die für eine sofortige Analyse nicht verfügbar waren.  Das zweite Problem war, dass sie in einem geschlossenen System gespeichert waren, sodass ich keines meiner bevorzugten Datenanalysetools verwenden konnte. <a name="habracut"></a><br><br>  Ich musste entscheiden, wie wir den Zugriff erleichtern und zumindest einen Mehrwert schaffen können, indem wir diese Datenquelle in einige unserer Benutzerinteraktionslösungen einbetten.  Nachdem ich eine Weile nachgedacht hatte, beschloss ich, eine Pipeline zu erstellen, um diese Daten in die Cloud-Datenbank zu übertragen, damit ich und das Team darauf zugreifen und Schlussfolgerungen ziehen können.  Nachdem ich vor einiger Zeit meine Spezialisierung in Data Engineering bei Coursera abgeschlossen hatte, war ich bestrebt, einige der Kurswerkzeuge im Projekt zu verwenden. <br><br>  Das Einfügen von Daten in eine Cloud-Datenbank schien also eine clevere Möglichkeit zu sein, mein erstes Problem zu lösen. Aber was könnte ich mit Problem Nummer 2 tun?  Glücklicherweise gab es eine Möglichkeit, diese Daten in eine Umgebung zu übertragen, in der ich auf Tools wie Python und die Google Cloud Platform (GCP) zugreifen konnte.  Es war jedoch ein langer Prozess, daher musste ich etwas tun, das es mir ermöglichte, die Entwicklung fortzusetzen, während ich auf das Ende der Datenübertragung wartete.  Die Lösung bestand darin, gefälschte Daten mithilfe der <b>Faker-</b> Bibliothek in Python zu erstellen.  Ich hatte diese Bibliothek noch nie benutzt, erkannte aber schnell, wie nützlich sie war.  Mit diesem Ansatz konnte ich Code schreiben und die Pipeline ohne tatsächliche Daten testen. <br><br>  Auf der Grundlage des Vorstehenden werde ich Ihnen in diesem Beitrag erläutern, wie ich die oben beschriebene Pipeline mit einigen der in GCP verfügbaren Technologien erstellt habe.  Insbesondere werde ich <b>Apache Beam (Version für Python), Dataflow, Pub / Sub und Big Query verwenden</b> , um Benutzerprotokolle zu sammeln, Daten zu konvertieren und zur weiteren Analyse in eine Datenbank zu übertragen.  In meinem Fall benötigte ich nur die Batch-Funktionalität von Beam, da meine Daten nicht in Echtzeit eintrafen und Pub / Sub nicht erforderlich war.  Ich werde mich jedoch auf die Streaming-Version konzentrieren, da dies in der Praxis der Fall sein kann. <br><br><h2>  Einführung in GCP und Apache Beam </h2><br>  Die Google Cloud Platform bietet eine Reihe wirklich nützlicher Tools für die Verarbeitung von Big Data.  Hier sind einige der Tools, die ich verwenden werde: <br><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pub / Sub</a> ist ein Messaging-Dienst, der die Publisher-Subscriber-Vorlage verwendet, mit der wir Daten in Echtzeit empfangen können. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DataFlow</a> ist ein Dienst, der die Erstellung von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Datenpipelines</a> vereinfacht und automatisch Aufgaben wie die Skalierung der Infrastruktur löst. Das bedeutet, dass wir uns nur auf das Schreiben von Code für unsere Pipeline konzentrieren können. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">BigQuery</a> ist ein Cloud-basiertes Data Warehouse.  Wenn Sie mit anderen SQL-Datenbanken vertraut sind, müssen Sie sich nicht lange mit BigQuery beschäftigen. </li><li>  Und schließlich werden wir Apache Beam verwenden, nämlich uns auf die Python-Version konzentrieren, um unsere Pipeline zu erstellen.  Mit diesem Tool können wir eine Pipeline für das Streaming oder die Stapelverarbeitung erstellen, die in GCP integriert ist.  Es ist besonders nützlich für die Parallelverarbeitung und eignet sich für Aufgaben wie Extrahieren, Transformieren und Laden (ETL). Wenn wir also Daten mit Transformationen oder Berechnungen von einem Ort an einen anderen verschieben müssen, ist Beam eine gute Wahl. </li></ul><br><br>  In GCP steht eine große Anzahl von Tools zur Verfügung, so dass es schwierig sein kann, alle Tools einschließlich ihres Zwecks abzudecken. Dennoch finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier eine</a> kurze Zusammenfassung als Referenz. <br><br><h2>  Visualisierung unseres Förderers </h2><br>  Lassen Sie uns die Komponenten unserer Pipeline in <i>Abbildung 1</i> visualisieren.  Auf hoher Ebene möchten wir Benutzerdaten in Echtzeit erfassen, verarbeiten und an BigQuery übertragen.  Protokolle werden erstellt, wenn Benutzer mit dem Produkt interagieren, indem sie Anforderungen an den Server senden, die dann protokolliert werden.  Diese Daten können besonders nützlich sein, um zu verstehen, wie Benutzer mit unserem Produkt interagieren und ob sie ordnungsgemäß funktionieren.  Im Allgemeinen enthält der Förderer die folgenden Schritte: <br><br><ol><li>  Die Protokolldaten unserer Benutzer werden im Bereich Pub / Sub veröffentlicht. </li><li>  Wir werden eine Verbindung zu Pub / Sub herstellen und die Daten mit Python und Beam in das entsprechende Format konvertieren (Schritte 3 und 4 in Abbildung 1). </li><li>  Nach der Konvertierung der Daten stellt Beam eine Verbindung zu BigQuery her und fügt sie unserer Tabelle hinzu (Schritte 4 und 5 in Abbildung 1). </li><li>  Zur Analyse können wir mit verschiedenen Tools wie Tableau und Python eine Verbindung zu BigQuery herstellen. </li></ol><br>  Beam macht diesen Vorgang sehr einfach, unabhängig davon, ob wir eine Streaming-Datenquelle oder eine CSV-Datei haben, und wir möchten die Stapelverarbeitung durchführen.  Später werden Sie sehen, dass der Code nur die minimalen Änderungen enthält, die zum Wechseln zwischen ihnen erforderlich sind.  Dies ist einer der Vorteile der Verwendung von Beam. <br><br><img src="https://habrastorage.org/webt/2s/nv/kc/2snvkcz6-jwybsxr2kyz3awipaw.png"><br>  <i>Abbildung 1: Die Hauptdatenpipeline</i> <br><br><h2>  Pseudodaten mit Faker erstellen </h2><br>  Wie bereits erwähnt, habe ich mich aufgrund des eingeschränkten Zugriffs auf Daten entschlossen, Pseudodaten im gleichen Format wie die tatsächlichen zu erstellen.  Dies war eine sehr nützliche Übung, da ich Code schreiben und die Pipeline testen konnte, während ich Daten erwartete.  Ich schlage vor, einen Blick auf die Faker- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation zu</a> werfen, wenn Sie wissen möchten, was diese Bibliothek sonst noch zu bieten hat.  Unsere Benutzerdaten ähneln im Allgemeinen dem folgenden Beispiel.  Basierend auf diesem Format können wir zeilenweise Daten generieren, um Echtzeitdaten zu simulieren.  Diese Protokolle geben uns Informationen wie Datum, Art der Anfrage, Antwort vom Server, IP-Adresse usw. <br><br> <code>192.52.197.161 - - [30/Apr/2019:21:11:42] "PUT /tag/category/tag HTTP/1.1" [401] 155 "https://harris-lopez.com/categories/about/" "Mozilla/5.0 (Macintosh; PPC Mac OS X 10_11_2) AppleWebKit/5312 (KHTML, like Gecko) Chrome/34.0.855.0 Safari/5312"</code> <br> <br>  Basierend auf der obigen Zeile möchten wir unsere <b>LINE-</b> Variable mit 7 Variablen in geschweiften Klammern erstellen.  Wir werden sie etwas später auch als Variablennamen in unserem Tabellenschema verwenden. <br><br> <code>LINE = """\ <br> {remote_addr} - - [{time_local}] "{request_type} {request_path} HTTP/1.1" [{status}] {body_bytes_sent} "{http_referer}" "{http_user_agent}"\ <br> """</code> <br> <br>  Wenn wir eine Stapelverarbeitung durchführen würden, wäre der Code sehr ähnlich, obwohl wir in einem bestimmten Zeitraum eine Reihe von Beispielen erstellen müssten.  Um einen Fälscher zu verwenden, erstellen wir einfach ein Objekt und rufen die Methoden auf, die wir benötigen.  Insbesondere war Faker nützlich, um IP-Adressen sowie Websites zu erstellen.  Ich habe die folgenden Methoden angewendet: <br><br> <code>fake.ipv4() <br> fake.uri_path() <br> fake.uri() <br> fake.user_agent() <br></code> <br><br><pre> <code class="php hljs">from faker import Faker import time import random import os import numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np from datetime import datetime, timedelta LINE = <span class="hljs-string"><span class="hljs-string">""</span></span><span class="hljs-string"><span class="hljs-string">"\ {remote_addr} - - [{time_local}] "</span></span>{request_type} {request_path} HTTP/<span class="hljs-number"><span class="hljs-number">1.1</span></span><span class="hljs-string"><span class="hljs-string">" [{status}] {body_bytes_sent} "</span></span>{http_referer}<span class="hljs-string"><span class="hljs-string">" "</span></span>{http_user_agent}<span class="hljs-string"><span class="hljs-string">"\ "</span></span><span class="hljs-string"><span class="hljs-string">""</span></span> def generate_log_line(): fake = Faker() now = datetime.now() remote_addr = fake.ipv4() time_local = now.strftime(<span class="hljs-string"><span class="hljs-string">'%d/%b/%Y:%H:%M:%S'</span></span>) request_type = random.choice([<span class="hljs-string"><span class="hljs-string">"GET"</span></span>, <span class="hljs-string"><span class="hljs-string">"POST"</span></span>, <span class="hljs-string"><span class="hljs-string">"PUT"</span></span>]) request_path = <span class="hljs-string"><span class="hljs-string">"/"</span></span> + fake.uri_path() status = np.random.choice([<span class="hljs-number"><span class="hljs-number">200</span></span>, <span class="hljs-number"><span class="hljs-number">401</span></span>, <span class="hljs-number"><span class="hljs-number">404</span></span>], p = [<span class="hljs-number"><span class="hljs-number">0.9</span></span>, <span class="hljs-number"><span class="hljs-number">0.05</span></span>, <span class="hljs-number"><span class="hljs-number">0.05</span></span>]) body_bytes_sent = random.choice(range(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) http_referer = fake.uri() http_user_agent = fake.user_agent() log_line = LINE.format( remote_addr=remote_addr, time_local=time_local, request_type=request_type, request_path=request_path, status=status, body_bytes_sent=body_bytes_sent, http_referer=http_referer, http_user_agent=http_user_agent ) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> log_line</code> </pre> <br><br>  Das Ende des ersten Teils. <br><br>  In den kommenden Tagen werden wir die Fortsetzung des Artikels mit Ihnen teilen, aber jetzt warten wir traditionell auf Kommentare ;-). <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zweiter Teil</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de454186/">https://habr.com/ru/post/de454186/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de454176/index.html">Uibook - ein visuelles Testwerkzeug für React-Komponenten mit Medienabfragen</a></li>
<li><a href="../de454178/index.html">Ein Beispiel für die Berechnung der Rente eines IT-Mitarbeiters aus Moskau</a></li>
<li><a href="../de454180/index.html">Schrödinger Cloud Backup</a></li>
<li><a href="../de454182/index.html">Ein ausführliches Interview mit dem Dekan der Python-Abteilung von GeekBrains - wie und warum Anfänger Sprache lernen</a></li>
<li><a href="../de454184/index.html">KubeCon Europe 2019: Wie wir das Kubernetes Main Event zum ersten Mal besuchten</a></li>
<li><a href="../de454188/index.html">Alternative Rekrutierungskanäle</a></li>
<li><a href="../de454190/index.html">Was Sie nicht tun müssen, wenn Ihr Telefon gestohlen wird</a></li>
<li><a href="../de454196/index.html">3D-Druck von Elektronik am Beispiel einer Drohne: Drähte und Platinen werden nicht mehr benötigt</a></li>
<li><a href="../de454198/index.html">Erstellen eines Gradle SpringBoot + Angular-Projekts mit mehreren Modulen in IDEA</a></li>
<li><a href="../de454204/index.html">Verhaltens-Crawlen ist kein Allheilmittel?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>