<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëÉüèΩ üë©‚Äçüë©‚Äçüëß‚Äçüë¶ üë®üèø‚Äçü§ù‚Äçüë®üèº Overclockers de longa dura√ß√£o: como o resfriamento l√≠quido come√ßou a dominar nos data centers ‚õ¥Ô∏è üìñ üê≥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="‚ÄúComputadores de alta velocidade n√£o podem ficar sem ar‚Äù 

 H√° um momento no filme Iron Man 2, em que Tony Stark assiste a um filme antigo de seu fale...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Overclockers de longa dura√ß√£o: como o resfriamento l√≠quido come√ßou a dominar nos data centers</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/442576/"><h3>  ‚ÄúComputadores de alta velocidade n√£o podem ficar sem ar‚Äù </h3><br><iframe width="560" height="315" src="https://www.youtube.com/embed/reQs3g5LO8E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  H√° um momento no filme Iron Man 2, em que Tony Stark assiste a um filme antigo de seu falecido pai, onde ele diz: ‚ÄúEstou limitado pela tecnologia do meu tempo, mas um dia voc√™ pode descobrir.  E ent√£o voc√™ vai mudar o mundo. "  √â fant√°stico, mas a ideia que expressa √© bastante real.  As id√©ias dos engenheiros costumam estar muito √† frente de seu tempo.  Sempre houve gadgets em Star Trek, mas o resto do mundo teve que trabalhar v√°rias d√©cadas para criar tablets e e-books. <br><br>  O conceito de refrigera√ß√£o l√≠quida se encaixa perfeitamente nessa categoria.  A id√©ia em si existe desde a d√©cada de 1960, mas permaneceu radical em compara√ß√£o com uma op√ß√£o muito mais barata e segura para o resfriamento do ar.  Levou mais de 40 anos at√© que o resfriamento l√≠quido come√ßou a se desenvolver nos anos 2000, e mesmo assim era principalmente uma prerrogativa dos entusiastas de PCs que procuravam dispersar suas CPUs muito al√©m dos limites recomendados pela Intel e AMD. <br><a name="habracut"></a><br>  Hoje, os sistemas de refrigera√ß√£o l√≠quida est√£o ganhando popularidade.  Esse sistema para PCs pode ser comprado por menos de US $ 100, e a produ√ß√£o de artesanato voltada para aplicativos industriais e data centers (como CoolIT, Asetek, Green Revolution Computing, Ebullient) oferece refrigera√ß√£o l√≠quida (() para servidores.  O ZhO √© usado principalmente em supercomputadores, computa√ß√£o de alta velocidade ou outras situa√ß√µes em que √© necess√°ria uma enorme quantidade de energia do computador, e os processadores trabalham com quase 100% de carga, mas essas op√ß√µes est√£o se tornando mais comuns. <br><br>  Existem dois tipos populares de ZhO: resfriamento direto por chip e imers√£o.  Com o resfriamento direto, o radiador √© conectado √† CPU, como um resfriador padr√£o, mas, em vez disso, dois tubos s√£o conectados a ele.  Um vem com √°gua fria, um dissipador de calor que absorve o calor da CPU e o outro deixa quente.  Em seguida, ele esfria e retorna √† CPU em um loop fechado semelhante ao fluxo sangu√≠neo. <br><br>  Durante o resfriamento por imers√£o, o equipamento √© preenchido com l√≠quido, o que, obviamente, n√£o deve conduzir eletricidade.  Essa abordagem √© mais semelhante √†s piscinas de resfriamento de reatores nucleares.  O resfriamento submerso continua sendo uma op√ß√£o mais avan√ßada e requer refrigerantes mais caros do que uma conex√£o direta, onde voc√™ pode usar √°gua comum.  Al√©m disso, sempre existe o risco de vazamento.  Portanto, de longe, a op√ß√£o mais popular √© a conex√£o direta. <br><br>  Como um dos principais exemplos, considere o alfabeto.  Quando essa empresa controladora do Google, em maio de 2018, introduziu processadores para o AI TensorFlow 3.0, o diretor Sundar Pichai disse que esses chips s√£o t√£o poderosos que "pela primeira vez tivemos que instalar um resfriamento l√≠quido em data centers".  A Alphabet teve que pagar esse pre√ßo por um aumento de oito vezes na produtividade. <br><br>  Por outro lado, os datacenters da Skybox anunciaram recentemente planos para criar um supercomputador enorme com 40.000 servidores da DownUnder GeoSolutions (DUG), projetado para explora√ß√£o de petr√≥leo e g√°s.  Este projeto produzir√° 250 petaflops de pot√™ncia computacional, mais do que qualquer um existente, e espera-se que os servidores sejam resfriados por fluido quando imersos em tanques cheios de fluido diel√©trico. <br><br>  De qualquer forma, "o resfriamento l√≠quido √© o resfriamento do futuro, e sempre ser√°", disse Craig Pennington, vice-presidente do departamento de design do operador de data center da Equinix.  "Parece √≥bvio que esta √© a abordagem correta, mas ningu√©m a aplicou." <br><br>  Como o JO passou da arte esot√©rica na virada da computa√ß√£o para um m√©todo quase universalmente aceito nos data centers modernos?  Como todas as tecnologias, isso aconteceu em parte como resultado da evolu√ß√£o, tentativa e erro e um grande n√∫mero de solu√ß√µes de engenharia.  No entanto, para ZhO, os data centers atuais devem agradecer aos primeiros overclockers que s√£o her√≥is desconhecidos desse m√©todo. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2c7/137/4b2/2c71374b2b2d2158d46d7e0ddb0fdabc.png"><br>  <i>O painel de controle do sistema de processamento de dados do IBM System 360</i> <br><br><h2>  O que queremos dizer com resfriamento l√≠quido </h2><br>  O resfriamento l√≠quido se tornou uma id√©ia popular em 1964, quando a IBM estudou a quest√£o do resfriamento submers√≠vel para o mainframe System 360. Era um dos mainframes da empresa;  As s√©ries 700 e 7000 existem h√° mais de dez anos, e o System / 360 "iniciou a era da compatibilidade com computadores - pela primeira vez permitindo que diferentes m√°quinas da linha de produtos trabalhem juntas", como escrevem na IBM.  O conceito era simples: a √°gua gelada tinha que fluir atrav√©s de um dispositivo que a resfria a uma temperatura abaixo da temperatura ambiente, e ent√£o a √°gua seria alimentada diretamente no sistema.  O circuito usado pela IBM agora √© conhecido como resfriamento quando o dissipador de calor √© montado atr√°s do mainframe.  O dispositivo aspirava ar quente do mainframe com ventiladores e, em seguida, esse ar era resfriado pela √°gua, como um radiador resfria o motor de um carro. <br><br>  Desde ent√£o, os engenheiros aperfei√ßoaram esse conceito b√°sico e surgiram duas formas dominantes de VF: imers√£o e contato direto.  Imers√£o √© o que √©;  A eletr√¥nica est√° em um banho l√≠quido, que, por raz√µes √≥bvias, n√£o pode ser √°gua.  O l√≠quido n√£o deve conduzir eletricidade, ou seja, ser um isolador (empresas como a 3M at√© desenvolvem l√≠quidos especificamente para isso). <br><br>  Mas o mergulho tem muitos problemas e desvantagens.  O servidor localizado no l√≠quido pode ser alcan√ßado apenas de cima.  Portanto, portas externas devem estar localizadas l√°.  A coloca√ß√£o do servidor de gabinetes 1U em um rack seria impratic√°vel; portanto, o servidor n√£o pode ser colocado seq√ºencialmente.  Um diel√©trico, e geralmente mineral, √© pequeno, muito caro e dif√≠cil de limpar em caso de vazamento.  Ser√£o necess√°rios discos r√≠gidos especiais e a altera√ß√£o do data center exigir√° investimentos significativos.  Portanto, como no caso do supercomputador mencionado acima, √© melhor fazer a imers√£o em um novo data center, em vez de refazer o antigo. <br><br>  O contato direto da JO, por outro lado, √© que o radiador (ou trocador de calor) est√° no chip, como um radiador comum.  Em vez de um ventilador, ele usa dois canos de √°gua - um trazendo √°gua fria para resfriamento e o segundo soprando √°gua quente aquecida pelo contato com o radiador.  Essa forma de ZhO se tornou a mais popular, foi adotada por fabricantes como HP Enterprise, Dell EMC e IBM, bem como por fabricantes de gabinetes Chatsworth Systems e Schneider Electric. <br><br>  O resfriamento direto usa √°gua, mas √© muito sens√≠vel √† sua qualidade.  √Ågua da torneira n√£o filtrada n√£o deve ser usada.  Basta olhar para a sua torneira ou chuveiro.  Ningu√©m precisa de ac√∫mulo de c√°lcio nos servidores.  Pelo menos o resfriamento direto requer √°gua destilada pura e, √†s vezes, sua mistura com anticongelante.  A fabrica√ß√£o desse refrigerante √© uma ci√™ncia em si mesma. <br><br><h2>  Liga√ß√£o da Intel </h2><br>  Como passamos dos radiadores da IBM para os modernos e extravagantes sistemas de refrigera√ß√£o?  Mais uma vez, gra√ßas aos overclockers.  Na virada do s√©culo, o resfriamento l√≠quido come√ßou a ganhar popularidade entre os overclockers de PC e amadores que montaram seus computadores que queriam aumentar sua velocidade al√©m dos limites oficiais.  No entanto, era uma arte esot√©rica sem desenhos padr√£o.  Todo mundo fez algo diferente.  A pessoa que montou tudo isso precisava ser t√£o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">talentosa</a> que a montagem dos produtos da IKEA parecia um absurdo completo.  A maioria dos sistemas de refrigera√ß√£o nem se encaixava nos gabinetes. <br><br>  No in√≠cio de 2004, a situa√ß√£o come√ßou a mudar devido a mudan√ßas internas nas pol√≠ticas da Intel.  Um engenheiro do centro de design de Hillsboro, Oregon - onde a maioria dos chips da empresa √© projetada, apesar de estar sediada em Santa Clara, Calif√≥rnia - trabalha em um projeto de refrigera√ß√£o especial h√° v√°rios anos.  O projeto custou √† empresa US $ 1 milh√£o e teve como objetivo criar um resfriador de l√≠quido para os processadores Intel.  Infelizmente, a Intel estava prestes a desativ√°-lo. <br><br>  O engenheiro esperava um resultado diferente.  Para salvar o projeto, ele teve essa ideia na Falcon Northwest, uma empresa de Portland que construiu complementos de jogos para computadores.  "O motivo foi porque a empresa pensou que o resfriamento l√≠quido incentivava as pessoas a fazer overclock, e essa atividade foi proibida na √©poca", disse Kelt Reeves, presidente da Falcon Northwest.  E nessa posi√ß√£o, a Intel tinha sua pr√≥pria l√≥gica.  Naquela √©poca, varejistas sem princ√≠pios da √Åsia estavam vendendo PCs com overclock sob o disfarce de mais poderosos, com pouca refrigera√ß√£o e aos olhos do p√∫blico, isso de alguma forma se transformou em um problema da Intel.  Portanto, a empresa se op√¥s ao overclock. <br><br>  No entanto, esse engenheiro do Oregon acreditava que, se ele conseguisse encontrar clientes e um mercado para um refrigerador desses, a Intel renderia.  (Al√©m disso, o produto Intel resultante era muito melhor em qualidade do que o dispon√≠vel no mercado, disse Reeves).  Portanto, ap√≥s algumas persuas√µes e negocia√ß√µes internas entre as empresas, a Intel permitiu √† Falcon vender sistemas de refrigera√ß√£o - em particular porque a Intel j√° os produzia em milhares.  O √∫nico problema foi que Falcon n√£o p√¥de mencionar que a Intel estava envolvida.  A Falcon concordou e logo se tornou o primeiro fabricante a fornecer sistemas de PC all-in-one totalmente selados. <br><br>  Reeves observou que essa solu√ß√£o avan√ßada de ZhO n√£o era particularmente f√°cil de usar.  Falcon teve que mudar o gabinete para caber no radiador e inventar uma placa de resfriamento para √°gua.  Mas com o tempo, fabricantes mais frios, como ThermalTake e Corsair, aprenderam o que a Intel estava fazendo e come√ßaram a fazer melhorias consistentes.  Desde ent√£o, v√°rios produtos e fabricantes apareceram, por exemplo, CoolIT e Asetek, que fizeram ZhO especificamente para data centers.  Alguns de seus produtos - por exemplo, tubos que n√£o quebram, n√£o quebram e n√£o vazam com garantia de at√© sete anos - foram finalmente concedidos sob licen√ßa aos fabricantes de sistemas de refrigera√ß√£o para o usu√°rio final, e essa troca de tecnologia em ambas as dire√ß√µes se tornou a norma. <br><br>  E, √† medida que esse mercado cresce em diferentes dire√ß√µes, at√© a Intel finalmente mudou de id√©ia.  Agora ela anuncia recursos de overclock para os processadores das s√©ries K e X, e nem se preocupa em vender coolers regulares junto com a CPU superior para os jogadores. <br><br>  "ZhO j√° √© uma tecnologia comprovada - todo mundo est√° fazendo isso no lado do consumidor", disse Reeves.  A Intel deixou de fornecer coolers regulares com as CPUs mais poderosas, porque elas precisam de JO;  isso j√° foi comprovado e uma b√™n√ß√£o da Intel foi recebida.  N√£o acho que haja algu√©m que diga que solu√ß√µes completas para isso n√£o s√£o confi√°veis ‚Äã‚Äão suficiente ". <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ad3/8c7/8a8/ad38c78a8953abdcdb563f84b7fadae7.jpg"><br>  <i>Refrigera√ß√£o por imers√£o no data center.</i>  <i>As caixas s√£o preenchidas com fluido diel√©trico que flui atrav√©s dos tubos.</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/221/cab/4fd/221cab4fd3522b8580d2e27d394f51d9.jpg"><br>  <i>Arrefecimento l√≠quido dos centros de dados Skybox com imers√£o.</i>  <i>Os trocadores de calor s√£o imersos em equipamentos de inform√°tica e o fluido diel√©trico n√£o sai do tanque.</i>  <i>Um circuito de √°gua passa pelas salas e se aproxima de cada trocador de calor.</i> <br><br><h2>  Fatos a favor da praticidade do resfriamento l√≠quido </h2><br>  Durante muito tempo, os data centers tradicionais previam um piso elevado com pequenas aberturas pelas quais o ar frio subia, sugado pelos servidores.  Isso foi chamado de CRAC, ou ar condicionado da sala de computadores.  O problema √© que agora n√£o √© suficiente soprar ar frio atrav√©s das aberturas no ch√£o. <br><br>  A principal raz√£o do recente boom no resfriamento de l√≠quidos √© a necessidade.  Os processadores de hoje ficam muito quentes e os servidores est√£o pr√≥ximos demais para que o ar os resfrie eficientemente, at√© o Google diz.  A capacidade de calor da √°gua √© 3300 vezes maior que a do ar, e um sistema de resfriamento de √°gua √© capaz de bombear 300 litros de √°gua por minuto, comparado a 20 metros c√∫bicos de ar por minuto. <br><br>  Simplificando, a √°gua pode esfriar com muito mais efici√™ncia e em um espa√ßo muito menor.  Portanto, depois de muitos anos tentando reduzir o consumo de energia, os fabricantes de processadores podem dispersar a energia e torcer a tens√£o para obter o m√°ximo desempenho - sabendo que o resfriamento l√≠quido pode lidar com isso. <br><br>  "Somos solicitados a resfriar chips cujo consumo de energia em breve ultrapassar√° 500 watts", disse Jeff Lyon, diretor da CoolIT.  - Alguns processadores que ainda n√£o entraram no mercado consumir√£o 300 watts cada.  Tudo isso est√° se desenvolvendo a pedido de IA e aprendizado de m√°quina.  A taxa de resfriamento simplesmente n√£o √© suficiente. ‚Äù <br><br>  Lyon disse que a CoolIT est√° considerando expandir o sistema de refrigera√ß√£o para chipsets, sistemas de controle de energia, chips de rede e mem√≥ria.  "N√£o haver√° nada radical em lidar com a mem√≥ria", acrescentou.  - Existem op√ß√µes de RAM com embalagem avan√ßada, consumindo 18 watts por DIMM.  Um DIMM t√≠pico consome 4-6 watts.  Entre os sistemas com uma grande quantidade de mem√≥ria, encontramos servidores com 16 ou mesmo 24 DIMMs instalados, o que significa muito calor. ‚Äù <br><br>  Um por um, os fabricantes s√£o confrontados com esses pedidos.  A Equinix observa como a densidade m√©dia cresce de 5 kW para 7-8 kW e agora para 15-16 kW, com alguns equipamentos j√° mostrando uma densidade de 40 kW.  ‚ÄúPortanto, a quantidade total de ar que precisa ser bombeada se torna muito grande.  Isso n√£o acontecer√° instantaneamente, mas nos pr√≥ximos anos haver√° uma ado√ß√£o fundamental do resfriamento de l√≠quidos ‚Äù, disse Pennington, da Equinix. <br><br><h2>  Um pouco sobre refrigera√ß√£o por imers√£o </h2><br>  O Green Revolution Cooling se concentra no resfriamento por imers√£o, e seu diretor Peter Poulin diz que, do ponto de vista da efici√™ncia energ√©tica, o resfriamento por imers√£o √© melhor do que o resfriamento direto por dois motivos.  Primeiro, os f√£s s√£o removidos de todos os servidores.  Somente isso reduz o consumo de energia em 15%, em m√©dia.  E um cliente da empresa reduziu em 30%. <br><br>  H√° outra vantagem indireta na elimina√ß√£o de f√£s: o sil√™ncio.  Apesar de os servidores geralmente usarem ventiladores muito pequenos, os servidores s√£o terrivelmente barulhentos e o fato de estar no data center √© desagrad√°vel devido ao calor e ao ru√≠do.  O resfriamento l√≠quido torna esses locais muito mais agrad√°veis ‚Äã‚Äãde se trabalhar. <br><br>  Outra vantagem √© que √© necess√°ria muito pouca energia para suportar o sistema de refrigera√ß√£o por imers√£o.  Existem apenas tr√™s partes m√≥veis: uma bomba para circular um refrigerador, uma bomba para mov√™-lo para uma torre de resfriamento e um ventilador da torre de resfriamento.  Ap√≥s a substitui√ß√£o do ar resfriado a l√≠quido, o consumo de eletricidade pode cair para 5% do gasto com ar-condicionado.  "Voc√™ obt√©m uma enorme redu√ß√£o no consumo de energia, o que permite fazer muitas outras coisas", disse Poulnin.  "Dependendo do consumidor, o data center pode ser mais eficiente em termos de energia ou reduzir as emiss√µes de carbono associadas √† constru√ß√£o dos data centers". <br><br><h2>  Fatos a favor do resfriamento l√≠quido por efici√™ncia energ√©tica </h2><br>  O consumo de energia tem sido uma preocupa√ß√£o para o setor de data centers (a Ag√™ncia de Prote√ß√£o Ambiental dos EUA acompanha esse n√∫mero h√° pelo menos dez anos).  Os data centers de hoje s√£o grandes empresas que consomem cerca de 2% de toda a eletricidade global e liberam tanto CO2 na atmosfera quanto o setor a√©reo.  Portanto, o interesse nesta quest√£o n√£o desaparece.  Felizmente, o resfriamento l√≠quido reduz as contas de eletricidade. <br><br>  As primeiras economias s√£o devido √† desconex√£o do ar condicionado no data center.  O segundo √© a elimina√ß√£o dos f√£s.  Cada rack de servidor possui muitos ventiladores que sopram ar, mas seu n√∫mero pode ser reduzido para um n√∫mero pequeno ou zero, dependendo da densidade. <br><br>  E com a tecnologia de "refrigera√ß√£o a seco", na qual n√£o h√° congelamento, voc√™ pode obter economias ainda maiores.  Inicialmente, o resfriamento conectado diretamente conduzia a √°gua atrav√©s de uma geladeira, que a resfriava de 15 a 25 graus Celsius.  Mas, no final, descobriu-se que os refrigeradores de l√≠quidos, que passavam a √°gua atrav√©s de uma longa sequ√™ncia de canos e ventiladores, canos frios aquecidos por √°gua quente e a difus√£o t√©rmica natural, tamb√©m esfriam a √°gua a uma temperatura suficiente. <br><br>  "Como esse processo √© t√£o eficiente, voc√™ n√£o precisa se preocupar em esfriar a √°gua a uma temperatura baixa", diz Pennington.  - A √°gua morna ainda efetivamente retira todo o calor dos servidores.  Voc√™ n√£o precisa de um ciclo de compress√£o, basta usar refrigeradores a seco. ‚Äù <br><br>  Refrigeradores a seco tamb√©m economizam √°gua.  Um grande data center que usa geladeiras pode consumir milh√µes de litros de √°gua por ano, mas um data center com refrigeradores a seco n√£o consome √°gua.  Isso economiza energia e √°gua, o que pode ser muito √∫til se o data center estiver localizado dentro da cidade. <br><br>  "N√£o consumimos muita √°gua", disse Pennington.  - Se voc√™ projeta tudo cuidadosamente, obt√©m um sistema fechado.  A √°gua n√£o entra e n√£o sai, voc√™ s√≥ precisa adicionar √°gua uma vez por ano para manter os tanques cheios.  Voc√™ n√£o adiciona √°gua constantemente ao seu carro, √© esse o caso conosco. " <br><br><h2>  A aceita√ß√£o segue a efic√°cia </h2><br>  Um exemplo do mundo real: a Dell, mudando para refrigera√ß√£o l√≠quida, aumentou a efici√™ncia energ√©tica ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">PUE</a> ) em 56%, de acordo com Brian Payne, vice-presidente de gerenciamento e marketing de produtos da PowerEdge Dell EMC.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PUE √© a propor√ß√£o da energia que precisa ser gasta no resfriamento do sistema e a energia necess√°ria para operar o sistema [na verdade, essa √© a propor√ß√£o da energia total usada pelo data center e a energia gasta diretamente na alimenta√ß√£o da infraestrutura de TI / aprox. perev]. PUE igual a 3 significa que 2 vezes mais energia √© gasta no resfriamento de um sistema do que na energia do sistema, e PUE = 2 significa que tanto a energia quanto o resfriamento s√£o consumidos igualmente. O PUE n√£o pode ser igual a 1, pois o resfriamento √© necess√°rio, mas os operadores do data center est√£o obcecados em tentar aproximar o valor o mais pr√≥ximo poss√≠vel de 1,0.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Al√©m de melhorar o PUE, o aumento no poder de computa√ß√£o que os clientes da Dell recebem pode chegar a 23%, e isso n√£o sobrecarrega o sistema al√©m da medida. "Com base nos investimentos em infraestrutura necess√°rios, prevemos o retorno anual do sistema", diz Payne. - Eu compararia isso com a compra de um condicionador de ar mais energeticamente eficiente para o lar. Voc√™ investe um pouco, mas, com o tempo, sente os benef√≠cios das contas de eletricidade. ‚Äù</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como um adepto completamente diferente do resfriamento de l√≠quidos, visite o centro de supercomputadores em Ohio, OSC. Este cluster emprega 1800 n√≥s. Depois de mudar para a JO, como disse Doug Johnson, arquiteto-chefe de sistemas, o centro atingiu PUE = 1,5. O OSC usa um circuito externo, de modo que a √°gua √© removida do pr√©dio e resfriada √† temperatura ambiente, que em m√©dia √© de 30 ¬∞ C no ver√£o e muito menos no inverno. Os chips atingem 70 ¬∞ C e, mesmo que a √°gua aque√ßa at√© 40 ¬∞ C, ainda permanece muito mais frio que os chips, e serve a seu objetivo.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como muitos dos primeiros a adotar a nova tecnologia, para a OSC tudo √© novo. H√° cinco anos, o centro n√£o usava ZhO e hoje ocupa 25%. O centro espera que em tr√™s anos a barra cres√ßa para 75% e, depois de alguns anos, eles mudem completamente para ZhO. Mas mesmo no estado atual, de acordo com Johnson, o resfriamento do centro requer quatro vezes menos energia do que antes da transi√ß√£o para ZhO e, em geral, essa solu√ß√£o reduziu o consumo total de energia em 2/3. "Acho que a porcentagem aumentar√° quando come√ßarmos a integrar a GPU no sistema de refrigera√ß√£o".</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Do ponto de vista do cliente, √© preciso tempo e energia para avaliar uma nova tecnologia - √© por isso que uma grande empresa como a Dell concordou em trabalhar com o CoolIT para anunciar ZhO. </font><font style="vertical-align: inherit;">N√£o √© de surpreender que, em primeiro lugar, entre as preocupa√ß√µes dos clientes permane√ßa a possibilidade de vazamento. </font><font style="vertical-align: inherit;">No entanto, apesar de todas as flutua√ß√µes, verifica-se que, no momento, eles t√™m pouca escolha se desejam obter o melhor desempenho. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Sempre houve medo de vazamentos", diz Lyon, da CoolIT. </font><font style="vertical-align: inherit;">- A situa√ß√£o mudou, e agora simplesmente n√£o h√° outras op√ß√µes. </font><font style="vertical-align: inherit;">Computadores de alta velocidade n√£o podem fazer exatamente isso. "</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt442576/">https://habr.com/ru/post/pt442576/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt442566/index.html">Igual √† lua: engenharia reversa de um m√≥dulo h√≠brido de amp op</a></li>
<li><a href="../pt442568/index.html">Semana 10 de seguran√ßa: Vulnerabilidades de driver NVIDIA</a></li>
<li><a href="../pt442570/index.html">Regras Sigma. Artesanato ou novo padr√£o para SOC</a></li>
<li><a href="../pt442572/index.html">Usando a ferramenta de configura√ß√£o do Datapath</a></li>
<li><a href="../pt442574/index.html">A base para uma teoria generalizada das redes neurais √© criada</a></li>
<li><a href="../pt442578/index.html">Lan√ßamento do Linux 5.0</a></li>
<li><a href="../pt442580/index.html">Engenharia reversa de formato bin√°rio usando arquivos .SNG da Korg como exemplo</a></li>
<li><a href="../pt442582/index.html">Como tentamos ass√©dio moral</a></li>
<li><a href="../pt442584/index.html">Documentos sobre o edif√≠cio: pequenas alegrias da automa√ß√£o no exemplo da Torre Negra</a></li>
<li><a href="../pt442586/index.html">Vulnerabilidade no Telegram permite ignorar a senha de c√≥digo local de qualquer tamanho</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>