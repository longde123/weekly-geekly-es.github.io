<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßñüèø ‚úãüèø üì© Aprendizado por Refor√ßo em Python ü§∞ üßî üõãÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° colegas! 



 Na √∫ltima publica√ß√£o do ano seguinte, quer√≠amos mencionar o Aprendizado por Refor√ßo - um t√≥pico para o qual j√° estamos traduzindo um...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Aprendizado por Refor√ßo em Python</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/434738/">  Ol√° colegas! <br><br><img src="https://habrastorage.org/webt/8s/-m/om/8s-mommciij8mkqdkjm62glthy4.jpeg"><br><br>  Na √∫ltima publica√ß√£o do ano seguinte, quer√≠amos mencionar o Aprendizado por Refor√ßo - um t√≥pico para o qual j√° estamos traduzindo um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">livro</a> . <br><br>  Julgue por si mesmo: havia um artigo elementar no Medium, que descrevia o contexto do problema, descrevia o algoritmo mais simples com a implementa√ß√£o em Python.  O artigo tem v√°rios gifs.  E motiva√ß√£o, recompensa e escolha da estrat√©gia certa no caminho para o sucesso s√£o coisas que ser√£o extremamente √∫teis para cada um de n√≥s no pr√≥ximo ano. <br><br>  Boa leitura! <br><a name="habracut"></a><br>  O aprendizado refor√ßado √© uma forma de aprendizado de m√°quina na qual o agente aprende a agir no ambiente, realizando a√ß√µes e desenvolvendo a intui√ß√£o, ap√≥s o que observa os resultados de suas a√ß√µes.  Neste artigo, mostrarei como entender e formular o problema de aprender com refor√ßo e depois resolv√™-lo em Python. <br><br><br>  Recentemente, nos acostumamos ao fato de que os computadores jogam jogos contra seres humanos - como bots em jogos multiplayer ou como rivais em jogos one-on-one: digamos, em Dota2, PUB-G, Mario.  A empresa de pesquisa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Deepmind</a> se <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">preocupou</a> com as not√≠cias quando seu programa AlphaGo de 2016 venceu o campe√£o sul-coreano em 2016.  Se voc√™ √© um jogador √°vido, pode ouvir as cinco partidas do Dota 2 OpenAI Five, onde carros lutaram contra pessoas e derrotaram os melhores jogadores do Dota2 em v√°rias partidas.  (Se voc√™ estiver interessado nos detalhes, o algoritmo √© analisado em detalhes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> e √© examinado como as m√°quinas tocaram). <br><br><img src="https://habrastorage.org/webt/da/l7/3j/dal73jd7dacspz0co83f6zuano0.png"><br><br>  A vers√£o mais recente do OpenAI Five <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">leva Roshan</a> . <br><br>  Ent√£o, vamos come√ßar com a pergunta central.  Por que precisamos de treinamento refor√ßado?  √â usado apenas em jogos ou √© aplic√°vel em cen√°rios realistas para resolver problemas aplicados?  Se esta √© sua primeira vez lendo um treinamento de refor√ßo, voc√™ simplesmente n√£o consegue imaginar a resposta para essas perguntas.  De fato, o aprendizado refor√ßado √© uma das tecnologias mais amplamente utilizadas e de r√°pido desenvolvimento no campo da intelig√™ncia artificial. <br>  Aqui est√£o algumas √°reas nas quais os sistemas de aprendizado por refor√ßo s√£o especialmente procurados: <br><br><ol><li>  Ve√≠culos n√£o tripulados </li><li>  Ind√∫stria de jogos </li><li>  Rob√≥tica </li><li>  Sistemas Recomendadores </li><li>  Publicidade e Marketing </li></ol><br>  <b>Vis√£o geral e antecedentes da aprendizagem por refor√ßo</b> <br><br>  Ent√£o, como o fen√¥meno do aprendizado com refor√ßo se formou quando temos tantos m√©todos de aprendizado profundo e √† m√°quina √† nossa disposi√ß√£o?  "Ele foi inventado por Rich Sutton e Andrew Barto, supervisor de pesquisa de Rich, que o ajudaram a preparar o doutorado".  O paradigma tomou forma nos anos 80 e foi arcaico.  Posteriormente, Rich acreditava que ela tinha um grande futuro, e ela acabaria por receber reconhecimento. <br><br>  O aprendizado refor√ßado suporta a automa√ß√£o no ambiente em que √© implantado.  Tanto o aprendizado de m√°quina quanto o aprendizado profundo operam aproximadamente da mesma maneira - eles s√£o organizados de forma diferente, mas ambos os paradigmas suportam a automa√ß√£o.  Ent√£o, por que surgiu o treinamento de refor√ßo? <br><br>  √â uma reminisc√™ncia do processo natural de aprendizagem em que o processo / modelo atua e recebe feedback sobre como ela consegue lidar com a tarefa: boa e n√£o. <br><br>  M√°quina e aprendizado profundo tamb√©m s√£o op√ß√µes de treinamento, no entanto, s√£o mais personalizados para identificar padr√µes nos dados dispon√≠veis.  No aprendizado por refor√ßo, por outro lado, essa experi√™ncia √© adquirida por tentativa e erro;  o sistema encontra gradualmente as op√ß√µes certas ou o ideal global.  Uma vantagem adicional s√©ria do aprendizado refor√ßado √© que, nesse caso, n√£o √© necess√°rio fornecer um conjunto extenso de dados de treinamento, como no ensino com um professor.  Alguns pequenos fragmentos ser√£o suficientes. <br><br>  <b>O conceito de aprendizagem por refor√ßo</b> <br><br>  Imagine ensinar seus gatos novos truques;  mas, infelizmente, os gatos n√£o entendem a linguagem humana, ent√£o voc√™ n√£o pode pegar e dizer a eles o que vai brincar com eles.  Portanto, voc√™ agir√° de maneira diferente: imitar a situa√ß√£o, e o gato tentar√° responder de uma maneira ou de outra em resposta.  Se o gato reagiu da maneira que voc√™ queria, voc√™ despeja leite nele.  Voc√™ entende o que vai acontecer a seguir?  Mais uma vez, em uma situa√ß√£o semelhante, o gato executar√° a a√ß√£o desejada novamente e com um entusiasmo ainda maior, esperando que seja alimentado ainda melhor.  √â assim que a aprendizagem ocorre em um exemplo positivo;  mas, se voc√™ tentar "educar" um gato com incentivos negativos, por exemplo, olhe estritamente para ele e franze a testa, ele geralmente n√£o aprende nessas situa√ß√µes. <br><br>  O aprendizado refor√ßado funciona da mesma forma.  Dizemos √† m√°quina algumas informa√ß√µes e a√ß√µes e, em seguida, recompensamos a m√°quina, dependendo da sa√≠da.  Nosso objetivo final √© maximizar as recompensas.  Agora vamos ver como reformular o problema acima em termos de aprendizado por refor√ßo. <br><br><ul><li>  O gato age como um "agente" exposto ao "ambiente". </li><li>  O ambiente √© uma casa ou √°rea de lazer, dependendo do que voc√™ est√° ensinando ao gato. </li><li>  As situa√ß√µes decorrentes do treinamento s√£o chamadas de "estados".  No caso de um gato, exemplos de condi√ß√µes s√£o quando o gato "corre" ou "rasteja debaixo da cama". </li><li>  Os agentes reagem executando a√ß√µes e passando de um "estado" para outro. </li><li>  Depois que o estado muda, o agente recebe uma ‚Äúrecompensa‚Äù ou uma ‚Äúmulta‚Äù, dependendo da a√ß√£o que ele tomou. </li><li>  "Estrat√©gia" √© uma metodologia para escolher uma a√ß√£o para obter os melhores resultados. </li></ul><br>  Agora que descobrimos o que √© aprendizagem por refor√ßo, vamos falar em detalhes sobre as origens e a evolu√ß√£o da aprendizagem por refor√ßo e aprendizagem profunda com refor√ßo, discutir como esse paradigma nos permite resolver problemas imposs√≠veis de aprender com ou sem um professor e tamb√©m observe o seguinte fato curioso: atualmente, o mecanismo de pesquisa do Google √© otimizado usando algoritmos de aprendizado por refor√ßo. <br><br>  <b>Entendendo a terminologia de aprendizado por refor√ßo</b> <br><br>  O agente e o ambiente desempenham pap√©is importantes no algoritmo de aprendizado por refor√ßo.  O ambiente √© o mundo em que o agente precisa sobreviver.  Al√©m disso, o agente recebe sinais de refor√ßo do ambiente (recompensa): este √© um n√∫mero que descreve o qu√£o bom ou ruim o estado atual do mundo pode ser considerado.  O objetivo do agente √© maximizar a recompensa total, o chamado "ganho".  Antes de escrever nossos primeiros algoritmos de aprendizado por refor√ßo, voc√™ precisa entender a seguinte terminologia. <br><br><img src="https://habrastorage.org/webt/6j/vx/cp/6jvxcpcpr52v252wa9eze1mehx4.gif"><br><br><ol><li>  <b>Estados</b> : Um estado √© uma descri√ß√£o completa de um mundo em que n√£o falta um √∫nico fragmento da informa√ß√£o que caracteriza esse mundo.  Pode ser uma posi√ß√£o, fixa ou din√¢mica.  Como regra, esses estados s√£o escritos na forma de matrizes, matrizes ou tensores de ordem superior. </li><li>  <b>A√ß√£o</b> : A a√ß√£o geralmente depende das condi√ß√µes ambientais e, em diferentes ambientes, o agente executa a√ß√µes diferentes.  Muitas a√ß√µes v√°lidas do agente s√£o registradas em um espa√ßo chamado "espa√ßo de a√ß√£o".  Normalmente, o n√∫mero de a√ß√µes no espa√ßo √© finito. </li><li>  <b>Ambiente</b> : este √© o local em que o agente existe e com o qual ele interage.  Diferentes tipos de recompensas, estrat√©gias etc. s√£o usados ‚Äã‚Äãpara diferentes ambientes. </li><li>  <b>Recompensas</b> e <b>ganhos</b> : Voc√™ precisa monitorar constantemente a fun√ß√£o de recompensa R ao treinar com refor√ßos.  √â essencial ao configurar um algoritmo, otimiz√°-lo e tamb√©m quando voc√™ parar de aprender.  Depende do estado atual do mundo, da a√ß√£o que acabou de ser realizada e do pr√≥ximo estado do mundo. </li><li>  <b>Estrat√©gias</b> : uma estrat√©gia √© uma regra segundo a qual um agente escolhe a pr√≥xima a√ß√£o.  O conjunto de estrat√©gias tamb√©m √© chamado de "c√©rebro" do agente. </li></ol><br><img src="https://habrastorage.org/webt/ur/lb/u-/urlbu-ifbred1iqfkhvqkv7bqds.png"><br><br>  Agora que nos familiarizamos com a terminologia de aprendizado por refor√ßo, vamos resolver o problema usando os algoritmos apropriados.  Antes disso, voc√™ precisa entender como formular esse problema e, ao resolv√™-lo, confiar na terminologia do treinamento com refor√ßo. <br><br>  <b>Solu√ß√£o de t√°xi</b> <br><br>  Ent√£o, passamos a resolver o problema com o uso de algoritmos de refor√ßo. <br>  Suponha que tenhamos uma zona de treinamento para um t√°xi n√£o tripulado, que treinamos para entregar passageiros ao estacionamento em quatro pontos diferentes ( <code>R,G,Y,B</code> ).  Antes disso, voc√™ precisa entender e definir o ambiente em que come√ßamos a programar em Python.  Se voc√™ est√° apenas come√ßando a aprender Python, recomendo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">este artigo para voc√™</a> . <br><br>  O ambiente para resolver um problema de t√°xi pode ser configurado usando o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">gin√°sio</a> da OpenAI - esta √© uma das bibliotecas mais populares para resolver problemas com treinamento de refor√ßo.  Bem, antes de usar o gin√°sio, voc√™ precisa instal√°-lo em sua m√°quina, e um gerenciador de pacotes Python chamado pip √© conveniente para isso.  A seguir est√° o comando de instala√ß√£o. <br><br> <code>pip install gym</code> <br> <br>  A seguir, vamos ver como nosso ambiente ser√° exibido.  Todos os modelos e a interface para esta tarefa j√° est√£o configurados no gin√°sio e nomeados sob <code>Taxi-V2</code> .  O trecho de c√≥digo abaixo √© usado para exibir esse ambiente. <br><br>  ‚ÄúTemos quatro locais (indicados por letras diferentes);  nossa tarefa √© pegar um passageiro em um ponto e deix√°-lo em outro.  Recebemos +20 pontos para um pouso de passageiros bem-sucedido e perdemos 1 ponto para cada etapa gasta nele.  H√° tamb√©m uma penalidade de 10 pontos para cada embarque e desembarque n√£o intencional de um passageiro. ‚Äù  (Fonte: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">gym.openai.com/envs/Taxi-v2</a> ) <br><br>  Aqui est√° a sa√≠da que veremos em nosso console: <br><br><img src="https://habrastorage.org/webt/n8/xj/lw/n8xjlwkbla6unwn2k6tworczc8o.png"><br><br>  T√°xi V2 ENV <br><br>  √ìtimo, o <code>env</code> √© o cora√ß√£o do OpenAi Gym, √© uma interface de ambiente unificado.  A seguir, s√£o apresentados m√©todos env que consideramos √∫teis: <br><br>  <code>env.reset</code> : redefine o ambiente e retorna um estado inicial aleat√≥rio. <br>  <code>env.step(action)</code> : <code>env.step(action)</code> desenvolvimento do meio ambiente um passo no tempo. <br>  <code>env.step(action)</code> : retorna as seguintes vari√°veis <br><br><ul><li>  <code>observation</code> : Observa√ß√£o do meio ambiente. </li><li>  <code>reward</code> : <code>reward</code> se sua a√ß√£o foi ben√©fica. </li><li>  <code>done</code> : indica se conseguimos pegar e deixar o passageiro adequadamente, tamb√©m conhecido como "um epis√≥dio". </li><li>  <code>info</code> : informa√ß√µes adicionais, como desempenho e lat√™ncia, necess√°rias para fins de depura√ß√£o. </li><li>  <code>env.render</code> : exibe um quadro do ambiente (√∫til para renderizar) </li></ul><br>  Ent√£o, depois de examinar o ambiente, vamos tentar entender melhor o problema.  Os t√°xis s√£o o √∫nico carro nesse estacionamento.  O estacionamento pode ser dividido em uma grade <code>5x5</code> , onde temos 25 locais de t√°xi poss√≠veis.  Esses 25 valores s√£o um dos elementos do nosso espa√ßo de estados.  Aten√ß√£o: no momento, nosso t√°xi est√° localizado no ponto com coordenadas (3, 1). <br><br>  Existem 4 pontos no ambiente em que os passageiros podem embarcar: s√£o <code>R, G, Y, B</code> ou <code>[(0,0), (0,4), (4,0), (4,3)]</code> em coordenadas ( horizontalmente; verticalmente), se fosse poss√≠vel interpretar o ambiente acima em coordenadas cartesianas.  Se voc√™ tamb√©m levar em conta outro (1) estado do passageiro: dentro do t√°xi, poder√° tomar todas as combina√ß√µes de localiza√ß√µes de passageiros e seus destinos para calcular o n√∫mero total de estados em nosso ambiente para treinamento em t√°xi: temos quatro (4) destinos e cinco (4+ 1) localiza√ß√µes de passageiros. <br><br>  Portanto, em nosso ambiente para um t√°xi, existem 5 √ó 5 √ó 5 √ó 4 = 500 estados poss√≠veis.  Um agente lida com uma das 500 condi√ß√µes e entra em a√ß√£o.  No nosso caso, as op√ß√µes s√£o as seguintes: mover-se em uma dire√ß√£o ou outra, ou a decis√£o de pegar / deixar o passageiro.  Em outras palavras, temos √† disposi√ß√£o seis a√ß√µes poss√≠veis: <br>  pickup, drop, norte, leste, sul, oeste (Os √∫ltimos quatro valores s√£o as dire√ß√µes nas quais um t√°xi pode se mover.) <br><br>  Este √© o <code>action space</code> : o conjunto de todas as a√ß√µes que nosso agente pode <code>action space</code> em um determinado estado. <br><br>  Como fica claro na ilustra√ß√£o acima, um t√°xi n√£o pode executar determinadas a√ß√µes em algumas situa√ß√µes (paredes interferem).  No c√≥digo que descreve o ambiente, simplesmente atribu√≠mos uma penalidade de -1 para cada acerto na parede e um t√°xi colidindo com a parede.  Assim, essas multas se acumular√£o, de modo que o t√°xi tentar√° n√£o bater nas paredes. <br><br>  Tabela de recompensa: Ao criar um ambiente de t√°xi, tamb√©m pode ser criada uma tabela de recompensa prim√°ria chamada P. Voc√™ pode consider√°-la uma matriz, em que o n√∫mero de estados corresponde ao n√∫mero de linhas e o n√∫mero de a√ß√µes ao n√∫mero de colunas.  Ou seja, estamos falando da matriz <code>states √ó actions</code> . <br><br>  Como absolutamente todas as condi√ß√µes s√£o registradas nessa matriz, √© poss√≠vel visualizar os valores de recompensa padr√£o atribu√≠dos ao estado que escolhemos ilustrar: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym &gt;&gt;&gt; env = gym.make(<span class="hljs-string"><span class="hljs-string">"Taxi-v2"</span></span>).env &gt;&gt;&gt; env.P[<span class="hljs-number"><span class="hljs-number">328</span></span>] {<span class="hljs-number"><span class="hljs-number">0</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">433</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">1</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">233</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">2</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">353</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">3</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">4</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">5</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)] }</code> </pre> <br>  A estrutura deste dicion√°rio √© a seguinte: <code>{action: [(probability, nextstate, reward, done)]}</code> . <br><br><ul><li>  Os valores de 0 a 5 correspondem √†s a√ß√µes (sul, norte, leste, oeste, pickup, dropoff) que um t√°xi pode executar no estado atual mostrado na ilustra√ß√£o. </li><li>  feito permite que voc√™ julgue quando deixamos o passageiro com sucesso no ponto desejado. </li></ul><br>  Para resolver esse problema sem nenhum treinamento com refor√ßo, voc√™ pode definir o estado de destino, fazer uma sele√ß√£o de espa√ßos e, se conseguir atingir o estado de destino para um determinado n√∫mero de itera√ß√µes, suponha que esse momento corresponda √† recompensa m√°xima.  Em outros estados, o valor da recompensa se aproxima do m√°ximo se o programa agir corretamente (se aproxima da meta) ou acumula multas se cometer erros.  Al√©m disso, o valor da multa n√£o pode atingir valores inferiores a -10. <br><br>  Vamos escrever um c√≥digo para resolver esse problema sem treinamento de refor√ßo. <br>  Como temos uma tabela P com valores de recompensa padr√£o para cada estado, podemos tentar organizar a navega√ß√£o de nosso t√°xi apenas com base nessa tabela. <br><br>  Criamos um loop infinito, rolando at√© o passageiro chegar ao destino (um epis√≥dio) ou, em outras palavras, at√© que a taxa de recompensa atinja 20. O m√©todo <code>env.action_space.sample()</code> seleciona automaticamente uma a√ß√£o aleat√≥ria do conjunto de todas as a√ß√µes dispon√≠veis .  Considere o que acontece: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sleep <span class="hljs-comment"><span class="hljs-comment">#  thr env env = gym.make("Taxi-v2").env env.s = 328 #     ,   , epochs = 0 penalties, reward = 0, 0 frames = [] done = False while not done: action = env.action_space.sample() state, reward, done, info = env.step(action) if reward == -10: penalties += 1 #         frames.append({ 'frame': env.render(mode='ansi'), 'state': state, 'action': action, 'reward': reward } ) epochs += 1 print("Timesteps taken: {}".format(epochs)) print("Penalties incurred: {}".format(penalties)) #    , ,  def frames(frames): for i, frame in enumerate(frames): clear_output(wait=True) print(frame['frame'].getvalue()) print(f"Timestep: {i + 1}") print(f"State: {frame['state']}") print(f"Action: {frame['action']}") print(f"Reward: {frame['reward']}") sleep(.1) frames(frames)</span></span></code> </pre><br>  Conclus√£o: <br><br><img src="https://habrastorage.org/webt/b3/kd/z_/b3kdz_kejhninocffgc2_3ytr_u.gif"><br><br>  cr√©ditos: OpenAI <br><br>  O problema foi resolvido, mas n√£o otimizado, ou esse algoritmo n√£o funcionar√° em todos os casos.  Precisamos de um agente de intera√ß√£o adequado para que o n√∫mero de itera√ß√µes gastas pela m√°quina / algoritmo para resolver o problema permane√ßa m√≠nimo.  Aqui, o algoritmo Q-learning nos ajudar√°, cuja implementa√ß√£o consideraremos na pr√≥xima se√ß√£o. <br><br>  <b>Introdu√ß√£o ao Q-Learning</b> <br><br>  Abaixo est√° o mais popular e um dos algoritmos de aprendizado por refor√ßo mais simples.  O ambiente recompensa o agente pelo treinamento gradual e pelo fato de que, em um determinado estado, ele d√° o passo mais ideal.  Na implementa√ß√£o discutida acima, tivemos uma tabela de recompensa "P", segundo a qual nosso agente aprender√°.  Com base na tabela de recompensas, ele escolhe a pr√≥xima a√ß√£o, dependendo de qu√£o √∫til seja, e atualiza outro valor, chamado Q-value.  Como resultado, √© criada uma nova tabela, chamada Q-table, exibida na combina√ß√£o (Status, A√ß√£o).  Se os valores Q forem melhores, obteremos recompensas mais otimizadas. <br><br>  Por exemplo, se um t√°xi estiver em um estado em que o passageiro esteja no mesmo ponto que o t√°xi, √© extremamente prov√°vel que o valor Q para a a√ß√£o de "retirada" seja maior do que para outras a√ß√µes, por exemplo, "deixe o passageiro" ou "v√° para o norte" " <br>  Os valores Q s√£o inicializados com valores aleat√≥rios e, √† medida que o agente interage com o ambiente e recebe v√°rias recompensas executando determinadas a√ß√µes, os valores Q s√£o atualizados de acordo com a seguinte equa√ß√£o: <br><br><img src="https://habrastorage.org/webt/ed/fv/br/edfvbr7xz2terdw8meeftimstx0.png"><br><br>  Isso levanta a quest√£o: como inicializar valores Q e como calcul√°-los.  √Ä medida que as a√ß√µes s√£o executadas, os valores Q s√£o executados nesta equa√ß√£o. <br><br>  Aqui, Alpha e Gamma s√£o os par√¢metros do algoritmo Q-learning.  Alfa √© o ritmo da aprendizagem e gama √© o fator de desconto.  Ambos os valores podem variar de 0 a 1 e √†s vezes s√£o iguais a um.  A gama pode ser igual a zero, mas a alfa n√£o, porque o valor das perdas durante a atualiza√ß√£o deve ser compensado (a taxa de aprendizado √© positiva).  O valor alfa aqui √© o mesmo de quando leciona com um professor.  A gama determina a import√¢ncia que queremos dar √†s recompensas que nos aguardam no futuro. <br><br>  Este algoritmo est√° resumido abaixo: <br><br><ul><li>  Etapa 1: inicialize a tabela Q, preenchendo-a com zeros, e para os valores Q, definimos constantes arbitr√°rias. </li><li>  Etapa 2: agora deixe o agente responder ao ambiente e tentar a√ß√µes diferentes.  Para cada altera√ß√£o de estado, selecionamos uma de todas as a√ß√µes poss√≠veis nesse estado (S). </li><li>  Etapa 3: v√° para o pr√≥ximo estado (S ') com base nos resultados da a√ß√£o anterior (a). </li><li>  Etapa 4: para todas as a√ß√µes poss√≠veis do estado (S '), selecione uma com o valor Q mais alto. </li><li>  Etapa 5: atualize os valores da tabela Q de acordo com a equa√ß√£o acima. </li><li>  Etapa 6: Transforme o pr√≥ximo estado no atual. </li><li>  Etapa 7: se o estado de destino for atingido, conclu√≠mos o processo e repetimos. </li></ul><br>  <b>Q-learning em Python</b> <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> IPython.display <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> clear_output <span class="hljs-comment"><span class="hljs-comment">#  Taxi-V2 Env env = gym.make("Taxi-v2").env #    q_table = np.zeros([env.observation_space.n, env.action_space.n]) #  alpha = 0.1 gamma = 0.6 epsilon = 0.1 all_epochs = [] all_penalties = [] for i in range(1, 100001): state = env.reset() #   epochs, penalties, reward, = 0, 0, 0 done = False while not done: if random.uniform(0, 1) &lt; epsilon: #    action = env.action_space.sample() else: #    action = np.argmax(q_table[state]) next_state, reward, done, info = env.step(action) old_value = q_table[state, action] next_max = np.max(q_table[next_state]) #    new_value = (1 - alpha) * old_value + alpha * \ (reward + gamma * next_max) q_table[state, action] = new_value if reward == -10: penalties += 1 state = next_state epochs += 1 if i % 100 == 0: clear_output(wait=True) print("Episode: {i}") print("Training finished.")</span></span></code> </pre><br>  √ìtimo, agora todos os seus valores ser√£o armazenados na vari√°vel <code>q_table</code> . <br><br>  Portanto, seu modelo √© treinado em condi√ß√µes ambientais e agora sabe como selecionar com mais precis√£o os passageiros.  E voc√™ se familiarizou com o fen√¥meno do aprendizado por refor√ßo e pode programar o algoritmo para resolver um novo problema. <br><br>  Outras t√©cnicas de aprendizado por refor√ßo: <br><br><ul><li>  Processos de tomada de decis√£o de Markov (MDP) e equa√ß√µes de Bellman </li><li>  Programa√ß√£o din√¢mica: RL baseada em modelo, itera√ß√£o de estrat√©gia e itera√ß√£o de valor </li><li>  Deep Q-Training </li><li>  M√©todos de descida de gradiente de estrat√©gia </li><li>  Sarsa </li></ul><br>  O c√≥digo para este exerc√≠cio est√° localizado em: <br><br>  vihar / python-refor√ßo-aprendizagem </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt434738/">https://habr.com/ru/post/pt434738/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt434728/index.html">Pessoas e processos: por que a udalenka n√£o √© adequada para todas as empresas?</a></li>
<li><a href="../pt434730/index.html">Bancos de dados na mem√≥ria: aplica√ß√£o, dimensionamento e adi√ß√µes importantes</a></li>
<li><a href="../pt434732/index.html">Vida em 6200 DPI. Revis√£o do HyperX Pulsefire Core</a></li>
<li><a href="../pt434734/index.html">Transformada de Fourier. Os velozes e furiosos</a></li>
<li><a href="../pt434736/index.html">Usando o banco de dados de log Mikrotik para suprimir a for√ßa bruta</a></li>
<li><a href="../pt434740/index.html">Rede neural ensinada a detectar pain√©is solares em imagens de sat√©lite e prever o n√≠vel de sua distribui√ß√£o</a></li>
<li><a href="../pt434742/index.html">Parte 2: Usando os controladores UDB PSoC da Cypress para reduzir o n√∫mero de interrup√ß√µes em uma impressora 3D</a></li>
<li><a href="../pt434744/index.html">Samsung SSD 860 QVO 1 TB e 4 TB: o primeiro consumidor SATA QLC (2 partes)</a></li>
<li><a href="../pt434746/index.html">BLE sob microsc√≥pio 4</a></li>
<li><a href="../pt434750/index.html">Como assumir o controle de sua infraestrutura de rede. Cap√≠tulo Dois Limpeza e documenta√ß√£o</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>