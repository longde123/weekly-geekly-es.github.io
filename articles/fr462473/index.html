<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëÅ‚Äçüó® üëÉ üï∫üèº Nous d√©veloppons un environnement pour travailler avec des microservices. Partie 1 Installer Kubernetes HA sur du m√©tal nu (Debian) üêæ ‚ôãÔ∏è ‚úåüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour chers lecteurs de Habr! 


 Avec cette publication, je veux commencer une s√©rie d'articles sur le d√©ploiement d'un environnement d'orchestrati...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Nous d√©veloppons un environnement pour travailler avec des microservices. Partie 1 Installer Kubernetes HA sur du m√©tal nu (Debian)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/462473/"><p><img src="https://habrastorage.org/webt/aq/dx/l3/aqdxl3a5akxfl3rh9b7bzc_kqji.jpeg"></p><br><h4>  Bonjour chers lecteurs de Habr! </h4><br><p>  Avec cette publication, je veux commencer une s√©rie d'articles sur le d√©ploiement d'un environnement d'orchestration √† part enti√®re avec des conteneurs Kubernetes, qui sera pr√™t √† fonctionner et √† lancer des applications. <br>  Je veux dire non seulement comment d√©ployer un cluster Kubernetes, mais aussi comment configurer un cluster apr√®s l'installation, comment y ajouter des outils pratiques et des modules compl√©mentaires pour utiliser l'architecture de microservice. </p><br><h2>  Ce cycle comprendra au moins quatre articles: </h2><br><ol><li>  Dans la premi√®re d'entre elles, je vous dirai comment installer un cluster kubernetes √† s√©curit√© int√©gr√©e sur du fer nu, comment installer un tableau de bord standard et configurer l'acc√®s √† celui-ci, comment installer un contr√¥leur d'entr√©e. </li><li>  Dans le deuxi√®me article, je vais vous montrer comment d√©ployer un cluster de basculement Ceph et comment commencer √† utiliser des volumes RBD dans notre cluster Kubernetes.  Je vais √©galement aborder un peu d'autres types de stockages (stockages) et examiner plus en d√©tail le stockage local.  De plus, je vais vous expliquer comment organiser le stockage S3 √† tol√©rance de pannes en fonction du cluster CEPH cr√©√© </li><li>  Dans le troisi√®me article, je d√©crirai comment d√©ployer un cluster de basculement MySql dans notre cluster Kubernetes, √† savoir, Percona XtraDB Cluster on Kubernetes.  Et je d√©crirai √©galement tous les probl√®mes que nous avons rencontr√©s lorsque nous avons d√©cid√© de transf√©rer la base de donn√©es vers kubernetes. </li><li>  Dans le quatri√®me article, j'essaierai de tout rassembler et de dire comment d√©ployer et ex√©cuter une application qui utilisera la base de donn√©es et les volumes ceph.  Je vais vous expliquer comment configurer le contr√¥leur d'entr√©e pour acc√©der √† notre application de l'ext√©rieur et au service de commande automatique de certificats de Let's Encrypt.  Une autre est de savoir comment maintenir automatiquement ces certificats √† jour.  Nous aborderons √©galement RBAC dans le cadre de l'acc√®s au panneau de contr√¥le.  Je vais vous parler en bref de Helm et de son installation. <br>  Si vous √™tes int√©ress√© par les informations contenues dans ces publications, alors bienvenue! <a name="habracut"></a></li></ol><br><h2>  Entr√©e: </h2><br><p>  √Ä qui sont destin√©s ces articles?  Tout d'abord, pour ceux qui commencent tout juste leur voyage dans l'√©tude de Kubernetes.  De plus, ce cycle sera utile aux ing√©nieurs qui envisagent de passer d'un monolithe √† des microservices.  Tout ce qui est d√©crit est mon exp√©rience, y compris celle obtenue lors de la traduction de plusieurs projets d'un monolithe vers Kubernetes.  Il est possible que certaines parties des publications int√©ressent des ing√©nieurs exp√©riment√©s. </p><br><h4>  Ce que je ne consid√©rerai pas en d√©tail dans cette s√©rie de publications: </h4><br><ul><li>  expliquer en d√©tail ce que sont les primitives kubernetes, telles que: pod, d√©ploiement, service, entr√©e, etc. </li><li>  Je consid√©rerai CNI (Container Networking Interface) tr√®s superficiellement, nous utilisons callico donc d'autres solutions, je vais seulement lister. </li><li>  processus de g√©n√©ration d'image Docker. </li><li>  Processus CI \ CD.  (Peut-√™tre une publication s√©par√©e, mais apr√®s tout le cycle) </li><li>  barre;  beaucoup a √©t√© √©crit √† son sujet, je ne parlerai que du processus d'installation de celui-ci dans le cluster et de configuration du client. </li></ul><br><h4>  Ce que je veux consid√©rer en d√©tail: </h4><br><ul><li>  Processus pas √† pas du d√©ploiement de cluster Kubernetes.  J'utiliserai kubeadm.  Mais en m√™me temps, j'examinerai pas √† pas le processus d'installation d'un cluster sur du m√©tal nu, les diff√©rents types d'installation ETCD et la configuration des fichiers pour kube admina.  Je vais essayer de clarifier toutes les options d'√©quilibrage pour le contr√¥leur Ingress et la diff√©rence dans les diff√©rents sch√©mas d'acc√®s des n≈ìuds de travail √† l'API du serveur. <br>  Je sais qu'aujourd'hui, pour le d√©ploiement de kubernetes, il existe de nombreux excellents outils, par exemple, kubespray ou le m√™me √©leveur.  Il sera peut-√™tre plus pratique pour quelqu'un de les utiliser.  Mais, je pense, il y a beaucoup d'ing√©nieurs qui veulent examiner la question plus en d√©tail. </li><li>  Terminologie CEPH et installation √©tape par √©tape du cluster CEPH, ainsi que des instructions √©tape par √©tape sur la connexion du stockage ceph au cluster cr√©√© par Kubernetes. </li><li>  les stockages locaux, la connexion au cluster kubernetes, ainsi que les diff√©rences par rapport aux connexions comme le chemin de l'h√¥te, etc. </li><li>  op√©rateurs kubernetes et le d√©ploiement de Percona XtraDB Cluster avec l'aide de l'op√©rateur, ainsi que d'essayer de parler des avantages et des inconv√©nients d'une telle solution apr√®s six mois d'exp√©rience en production.  Et je partagerai √©galement quelques plans pour finaliser l'op√©rateur de percona. </li></ul><br><h2>  Table des mati√®res: </h2><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Liste des h√¥tes, des ressources h√¥tes, des versions du syst√®me d'exploitation et des logiciels</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Diagramme HA du cluster Kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Avant de commencer ou avant de commencer</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Remplissez le fichier create-config.sh</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Mise √† jour du noyau du syst√®me d'exploitation</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Pr√©paration des n≈ìuds Installation de Kubelet, Kubectl, Kubeadm et Docker</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Installation de l'ETCD (diverses options)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lancement du premier assistant kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Installation de CNI Callico</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lancement des deuxi√®me et troisi√®me assistants kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Ajouter un n≈ìud de travail au cluster</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Installer haproxy sur les n≈ìuds de travail pour HA</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Installation d'Ingress Controller</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Installer l'interface utilisateur Web (tableau de bord)</a> </li></ol><br><a name="vm"></a><br><h2>  Liste et destination des h√¥tes </h2><br><p>  Tous les n≈ìuds de mon cluster seront situ√©s sur des machines virtuelles avec un syst√®me extensible Debian 9 pr√©install√© avec le noyau 4.19.0-0.bpo.5-amd64.  Pour la virtualisation, j'utilise Proxmox VE. </p><br><h4>  Table VM et leurs performances: </h4><br><div class="scrollable-table"><table><tbody><tr><th>  <b>Nom</b> </th><th>  <b>Adresse IP</b> </th><th>  <b>Commentaire</b> </th><th>  <b>CPU</b> </th><th>  <b>Mem</b> </th><th>  <b>DISK1</b> </th><th>  <b>DISK2</b> </th></tr><tr><td>  master01 </td><td>  10.73.71.25 </td><td>  n≈ìud ma√Ætre </td><td>  4vcpu </td><td>  4 Go </td><td>  Disque dur </td><td>  --- </td></tr><tr><td>  master02 </td><td>  10.73.71.26 </td><td>  n≈ìud ma√Ætre </td><td>  4vcpu </td><td>  4 Go </td><td>  Disque dur </td><td>  --- </td></tr><tr><td>  master03 </td><td>  10.73.71.27 </td><td>  n≈ìud ma√Ætre </td><td>  4vcpu </td><td>  4 Go </td><td>  Disque dur </td><td>  --- </td></tr><tr><td>  worknode01 </td><td>  10.73.75.241 </td><td>  n≈ìud de travail </td><td>  4vcpu </td><td>  4 Go </td><td>  Disque dur </td><td>  SSD </td></tr><tr><td>  worknode02 </td><td>  10.73.75.242 </td><td>  n≈ìud de travail </td><td>  4vcpu </td><td>  4 Go </td><td>  Disque dur </td><td>  SSD </td></tr><tr><td>  worknode03 </td><td>  10.73.75.243 </td><td>  n≈ìud de travail </td><td>  4vcpu </td><td>  4 Go </td><td>  Disque dur </td><td>  SSD </td></tr></tbody></table></div><br><p>  Il n'est pas n√©cessaire que vous ayez une telle configuration de machines, mais je vous conseille tout de m√™me de respecter les recommandations de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation officielle</a> et, pour les ma√Ætres, d'augmenter la quantit√© de RAM √† au moins 4 Go.  Pour l'avenir, je dirai qu'avec un plus petit nombre, j'ai eu des p√©pins dans le travail de CNI Callico <br>  Ceph est √©galement assez vorace en termes de performances de m√©moire et de disque. <br>  Nos installations de production fonctionnent sans virtualisation bare-metal, mais je connais de nombreux exemples o√π des machines virtuelles avec des ressources assez modestes suffisaient.  Tout d√©pend de vos besoins et de vos charges de travail. </p><br><h2>  Liste et versions des logiciels </h2><br><div class="scrollable-table"><table><tbody><tr><th>  <b>Nom</b> </th><th>  <b>La version</b> </th></tr><tr><td>  Kubernetes </td><td>  1.15.1 </td></tr><tr><td>  Docker </td><td>  19.3.1 </td></tr></tbody></table></div><br><p>  √Ä partir de la version 1.14, Kubeadm a cess√© de prendre en charge la version d'API v1alpha3 et est compl√®tement pass√© √† la version d'API v1beta1, qu'il prendra en charge dans un avenir proche, donc dans cet article, je ne parlerai que de v1beta1. <br>  Nous pensons donc que vous avez pr√©par√© les machines pour le cluster kubernetes.  Ils sont tous accessibles les uns aux autres sur le r√©seau, ont une ¬´connexion Internet¬ª √† Internet et un syst√®me d'exploitation ¬´propre¬ª y est install√©. <br>  Pour chaque √©tape d'installation, je pr√©ciserai sur quelles machines la commande ou le bloc de commandes est ex√©cut√©.  Ex√©cutez toutes les commandes en tant que root, sauf indication contraire. <br>  Tous les fichiers de configuration, ainsi qu'un script pour leur pr√©paration, sont disponibles en t√©l√©chargement dans mon <a href="">github</a> <br>  Commen√ßons donc. </p><br><a name="ha-image"></a><br><h2>  Diagramme HA du cluster Kubernetes </h2><br><p><img src="https://habrastorage.org/webt/ch/mx/pg/chmxpgzdyk0bvxwx7-6lawqfmvo.jpeg"><br>  Un diagramme approximatif du cluster HA.  L'artiste de moi est moyen, pour √™tre honn√™te, mais je vais essayer de l'expliquer en quelques mots et de mani√®re assez simpliste, sans me plonger particuli√®rement dans la th√©orie. <br>  Ainsi, notre cluster sera compos√© de trois n≈ìuds ma√Ætres et de trois n≈ìuds travailleurs.  Sur chaque n≈ìud ma√Ætre kubernetes, etcd (fl√®ches vertes sur le diagramme) et les pi√®ces de rechange kubernetes fonctionneront pour nous;  appelons-les g√©n√©riquement - kubeapi. <br>  Par le biais du cluster ma√Ætre etcd, les n≈ìuds √©changent l'√©tat du cluster kubernetes.  Je vais indiquer les m√™mes adresses que les points d'entr√©e du contr√¥leur d'entr√©e pour le trafic externe (fl√®ches rouges sur le sch√©ma) <br>  Sur les n≈ìuds de travail, kubelet fonctionne pour nous, qui communique avec le serveur api kubernetes via un haproxy install√© localement sur chaque n≈ìud de travail.  En tant qu'adresse API du serveur pour kubelet, j'utiliserai l'h√¥te local 127.0.0.1:6443, et l'haproxy sur roundrobin dispersera les demandes sur trois n≈ìuds ma√Ætres, il v√©rifiera √©galement la disponibilit√© des n≈ìuds ma√Ætres.  Ce sch√©ma nous permettra de cr√©er une haute disponibilit√© et, en cas de d√©faillance de l'un des n≈ìuds ma√Ætres, les n≈ìuds de travail enverront discr√®tement des demandes aux deux n≈ìuds ma√Ætres restants. </p><br><a name="begin"></a><br><h2>  Avant de commencer </h2><br><p>  Avant de commencer √† travailler sur chaque n≈ìud du cluster, nous vous fournirons les packages dont nous aurons besoin pour travailler: </p><br><pre><code class="plaintext hljs">apt-get update &amp;&amp; apt-get install -y curl apt-transport-https git</code> </pre> <br><p>  Sur les n≈ìuds principaux, copiez le r√©f√©rentiel avec des mod√®les de configuration </p><br><pre> <code class="plaintext hljs">sudo -i git clone https://github.com/rjeka/kubernetes-ceph-percona.git</code> </pre> <br><p>  V√©rifiez que l'adresse IP des h√¥tes sur les assistants correspond √† celle sur laquelle le serveur kubernetes √©coutera </p><br><pre> <code class="plaintext hljs">hostname &amp;&amp; hostname -i master01 10.73.71.25</code> </pre> <br><p>  et donc pour tous les n≈ìuds ma√Ætres. </p><br><p>  Assurez-vous de d√©sactiver SWAP, sinon kubeadm g√©n√©rera une erreur </p><br><pre> <code class="plaintext hljs">[ERROR Swap]: running with swap on is not supported. Please disable swap</code> </pre> <br><p>  Vous pouvez d√©sactiver la commande </p><br><pre> <code class="plaintext hljs">swapoff -a</code> </pre> <br><p>  N'oubliez pas de commenter dans / etc / fstab </p><br><a name="create-config"></a><br><h2>  Remplissez le fichier create-config.sh </h2><br><p>  Pour remplir automatiquement les configurations n√©cessaires √† l'installation du cluster kubernetes, j'ai t√©l√©charg√© un petit script create-config.sh.  Vous devez le remplir litt√©ralement 8 lignes.  Indiquez les adresses IP et le nom d'h√¥te de vos ma√Ætres.  Et sp√©cifiez √©galement etcd tocken, vous ne pouvez pas le changer.  Je vais donner ci-dessous la partie du script dans laquelle vous devez apporter des modifications. </p><br><pre> <code class="plaintext hljs">#!/bin/bash ####################################### # all masters settings below must be same ####################################### # master01 ip address export K8SHA_IP1=10.73.71.25 # master02 ip address export K8SHA_IP2=10.73.71.26 # master03 ip address export K8SHA_IP3=10.73.71.27 # master01 hostname export K8SHA_HOSTNAME1=master01 # master02 hostname export K8SHA_HOSTNAME2=master02 # master03 hostname export K8SHA_HOSTNAME3=master03 #etcd tocken: export ETCD_TOKEN=9489bf67bdfe1b3ae077d6fd9e7efefd #etcd version export ETCD_VERSION="v3.3.10"</code> </pre><br><a name="kernel"></a><br><h2>  Mise √† jour du noyau du syst√®me d'exploitation </h2><br><p>  Cette √©tape est facultative, car le noyau devra √™tre mis √† jour √† partir des ports arri√®re, et vous le faites √† vos risques et p√©rils.  Peut-√™tre ne rencontrerez-vous jamais ce probl√®me, et si vous le faites, vous pouvez mettre √† jour le noyau m√™me apr√®s avoir d√©ploy√© kubernetes.  En g√©n√©ral, vous d√©cidez. <br>  Une mise √† jour du noyau est requise pour corriger l'ancien bug de docker, qui n'√©tait corrig√© que dans la version 4.18 du noyau Linux.  Vous pouvez en savoir plus sur ce bug <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> .  Un bug a √©t√© exprim√© dans le blocage p√©riodique de l'interface r√©seau sur les n≈ìuds kubernetes avec l'erreur: </p><br><pre> <code class="plaintext hljs">waiting for eth0 to become free. Usage count = 1</code> </pre> <br><p>  Apr√®s avoir install√© le syst√®me d'exploitation, j'avais la version 4.9 du noyau </p><br><pre> <code class="bash hljs">uname -a Linux master01 4.9.0-7-amd64 <span class="hljs-comment"><span class="hljs-comment">#1 SMP Debian 4.9.110-3+deb9u2 (2018-08-13) x86_64 GNU/Linux</span></span></code> </pre> <br><p>  Sur chaque machine pour kubernetes, nous ex√©cutons <br>  √âtape # 1 <br>  Ajouter des ports de retour √† la liste des sources </p><br><pre> <code class="plaintext hljs">echo deb http://ftp.debian.org/debian stretch-backports main &gt; /etc/apt/sources.list apt-get update apt-cache policy linux-compiler-gcc-6-x86</code> </pre> <br><p>  √âtape num√©ro 2 <br>  Installation du package </p><br><pre> <code class="plaintext hljs">apt install -y -t stretch-backports linux-image-amd64 linux-headers-amd64</code> </pre> <br><p>  √âtape num√©ro 3 <br>  Red√©marrer </p><br><pre> <code class="plaintext hljs">reboot</code> </pre> <br><p>  V√©rifiez que tout va bien </p><br><pre> <code class="plaintext hljs">uname -a Linux master01 4.19.0-0.bpo.5-amd64 #1 SMP Debian 4.19.37-4~bpo9+1 (2019-06-19) x86_64 GNU/Linux</code> </pre><br><a name="kubelet"></a><br><h2>  Pr√©paration des n≈ìuds Installation de Kubelet, Kubectl, Kubeadm et Docker </h2><br><h4>  Installer Kubelet, Kubectl, Kubeadm </h4><br><p>  Nous mettons sur tous les n≈ìuds du cluster, selon la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation de kubernetes</a> </p><br><pre> <code class="plaintext hljs">apt-get update &amp;&amp; apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl apt-mark hold kubelet kubeadm kubectl</code> </pre> <br><h4>  Installer Docker </h4><br><p>  Installez docker selon les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">instructions de la documentation</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><br></a> </p><br><pre> <code class="plaintext hljs">apt-get remove docker docker-engine docker.io containerd runc apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common</code> </pre> <br><pre> <code class="plaintext hljs">curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add - apt-key fingerprint 0EBFCD88</code> </pre> <br><pre> <code class="plaintext hljs">add-apt-repository \ "deb [arch=amd64] https://download.docker.com/linux/debian \ $(lsb_release -cs) \ stable"</code> </pre> <br><pre> <code class="plaintext hljs">apt-get update apt-get install docker-ce docker-ce-cli containerd.io</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Installation Installation de Kubelet, Kubectl, Kubeadm et docker en utilisant ansible</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">git clone https://github.com/rjeka/kubernetes-ceph-percona.git cd kubernetes-ceph-percona/playbooks vim masters.ini</code> </pre> <br><p>  Dans le groupe ma√Ætres, enregistrez les ma√Ætres ip. <br>  Dans le groupe des travailleurs, √©crivez l'ip des n≈ìuds de travail. </p><br><pre> <code class="plaintext hljs"># sudo c  ansible-playbook -i hosts.ini kubelet.yaml -K ansible-playbook -i hosts.ini docker.yaml -K # sudo  ansible-playbook -i hosts.ini kubelet.yaml ansible-playbook -i hosts.ini docker.yaml</code> </pre> </div></div><br><p>  Si, pour une raison quelconque, vous ne souhaitez pas utiliser Docker, vous pouvez utiliser n'importe quel CRI.  Vous pouvez en savoir plus, par exemple <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> , mais cette rubrique d√©passe le cadre de cet article. </p><br><a name="etcd"></a><br><h2>  Installation ETCD </h2><br><p>  Je n'entrerai pas dans les d√©tails de la th√©orie, en bref: etcd est un stockage de valeurs-cl√©s distribu√© open source.  etcd est √©crit en GO et utilis√© dans kubernetes en fait, comme base de donn√©es pour stocker l'√©tat du cluster.  Pour un examen plus d√©taill√©, consultez la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation de kubernetes</a> . <br>  etcd peut √™tre install√© de plusieurs fa√ßons.  Vous pouvez l'installer localement et l'ex√©cuter en tant que d√©mon, vous pouvez l'ex√©cuter dans des conteneurs Docker, vous pouvez l'installer m√™me en tant que pod kubernetes.  Vous pouvez l'installer √† la main, ou vous pouvez l'installer en utilisant kubeadm (je n'ai pas essay√© cette m√©thode).  Peut √™tre install√© sur des machines en cluster ou des serveurs individuels. <br>  Je vais installer etcd localement sur les n≈ìuds principaux et ex√©cuter en tant que d√©mon via systemd, ainsi que d'envisager l'installation dans docker.  J'utilise etcd sans TLS, si vous avez besoin de TLS, reportez-vous √† la documentation d'etcd ou de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">kubernetes lui-m√™me</a> <br>  Aussi dans mon github sera ansible-playbook pour installer etcd avec lancement via systemd. </p><br><h4>  Num√©ro d'option 1 <br>  Installer localement, ex√©cuter via systemd </h4><br><p>  Sur tous les ma√Ætres: (sur les n≈ìuds de travail du cluster, cette √©tape n'est pas n√©cessaire) <br>  √âtape # 1 <br>  T√©l√©chargez et d√©compressez l'archive avec etcd: </p><br><pre> <code class="plaintext hljs">mkdir archives cd archives export etcdVersion=v3.3.10 wget https://github.com/coreos/etcd/releases/download/$etcdVersion/etcd-$etcdVersion-linux-amd64.tar.gz tar -xvf etcd-$etcdVersion-linux-amd64.tar.gz -C /usr/local/bin/ --strip-components=1</code> </pre> <br><p>  √âtape num√©ro 2 <br>  Cr√©er un fichier de configuration pour ETCD </p><br><pre> <code class="plaintext hljs">cd .. ./create-config.sh etcd</code> </pre> <br><p>  Le script accepte la valeur etcd en entr√©e et g√©n√®re un fichier de configuration dans le r√©pertoire etcd.  Une fois le script ex√©cut√©, le fichier de configuration termin√© sera situ√© dans le r√©pertoire etcd. <br>  Pour toutes les autres configurations, le script fonctionne sur le m√™me principe.  Il prend quelques entr√©es et cr√©e une configuration dans un r√©pertoire sp√©cifique. </p><br><p>  √âtape num√©ro 3 <br>  Nous d√©marrons le cluster etcd et v√©rifions ses performances </p><br><pre> <code class="plaintext hljs">systemctl start etcd</code> </pre> <br><p>  V√©rification des performances du d√©mon </p><br><pre> <code class="plaintext hljs">systemctl status etcd ‚óè etcd.service - etcd Loaded: loaded (/etc/systemd/system/etcd.service; disabled; vendor preset: enabled) Active: active (running) since Sun 2019-07-07 02:34:28 MSK; 4min 46s ago Docs: https://github.com/coreos/etcd Main PID: 7471 (etcd) Tasks: 14 (limit: 4915) CGroup: /system.slice/etcd.service ‚îî‚îÄ7471 /usr/local/bin/etcd --name master01 --data-dir /var/lib/etcd --listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 --advertise-client-urls http://10.73.71.25:2379,http://10.73.71. Jul 07 02:34:28 master01 etcd[7471]: b11e73358a31b109 [logterm: 1, index: 3, vote: 0] cast MsgVote for f67dd9aaa8a44ab9 [logterm: 2, index: 5] at term 554 Jul 07 02:34:28 master01 etcd[7471]: raft.node: b11e73358a31b109 elected leader f67dd9aaa8a44ab9 at term 554 Jul 07 02:34:28 master01 etcd[7471]: published {Name:master01 ClientURLs:[http://10.73.71.25:2379 http://10.73.71.25:4001]} to cluster d0979b2e7159c1e6 Jul 07 02:34:28 master01 etcd[7471]: ready to serve client requests Jul 07 02:34:28 master01 etcd[7471]: serving insecure client requests on [::]:4001, this is strongly discouraged! Jul 07 02:34:28 master01 systemd[1]: Started etcd. Jul 07 02:34:28 master01 etcd[7471]: ready to serve client requests Jul 07 02:34:28 master01 etcd[7471]: serving insecure client requests on [::]:2379, this is strongly discouraged! Jul 07 02:34:28 master01 etcd[7471]: set the initial cluster version to 3.3 Jul 07 02:34:28 master01 etcd[7471]: enabled capabilities for version 3.3 lines 1-19</code> </pre> <br><p>  Et la sant√© du cluster lui-m√™me: </p><br><pre> <code class="plaintext hljs">etcdctl cluster-health member 61db137992290fc is healthy: got healthy result from http://10.73.71.27:2379 member b11e73358a31b109 is healthy: got healthy result from http://10.73.71.25:2379 member f67dd9aaa8a44ab9 is healthy: got healthy result from http://10.73.71.26:2379 cluster is healthy etcdctl member list 61db137992290fc: name=master03 peerURLs=http://10.73.71.27:2380 clientURLs=http://10.73.71.27:2379,http://10.73.71.27:4001 isLeader=false b11e73358a31b109: name=master01 peerURLs=http://10.73.71.25:2380 clientURLs=http://10.73.71.25:2379,http://10.73.71.25:4001 isLeader=false f67dd9aaa8a44ab9: name=master02 peerURLs=http://10.73.71.26:2380 clientURLs=http://10.73.71.26:2379,http://10.73.71.26:4001 isLeader=true</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Installer etcd localement avec ansible, ex√©cuter via systemd</b> <div class="spoiler_text"><p>  Avec github, nous clonerons le d√©p√¥t avec le code sur la machine √† partir de laquelle vous ex√©cuterez le playbook.  Cette machine devrait avoir un acc√®s ssh sur une cl√© du ma√Ætre de notre futur cluster. </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/rjeka/kubernetes-ceph-percona.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> kubernetes-ceph-percona/playbooks vim masters.ini</code> </pre> <br><p>  Dans le groupe ma√Ætres, enregistrez les ma√Ætres ip. <br>  etcd_version est la version de etcd.  Vous pouvez le voir sur la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">page etcd dans github</a> .  Au moment d'√©crire ces lignes, il y avait la version v3.3.13 que j'utilise v3.3.10. <br>  etcdToken - vous pouvez laisser le m√™me, ou g√©n√©rer le v√¥tre. <br>  Dirigez l'√©quipe Playbook </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># sudo c  ansible-playbook -i hosts.ini -l masters etcd.yaml -K BECOME password: &lt;sudo &gt; # sudo  ansible-playbook -i hosts.ini -l masters etcd.yaml</span></span></code> </pre> </div></div><br><p>  Si vous voulez ex√©cuter etcd dans docker, alors il y a une instruction sous le spoiler. </p><br><div class="spoiler">  <b class="spoiler_title">Installer etcd avec docker-compose, lancer dans docker</b> <div class="spoiler_text"><p>  Ces commandes doivent √™tre ex√©cut√©es sur tous les n≈ìuds ma√Ætres du cluster. <br>  Avec github, nous clonons le r√©f√©rentiel avec du code </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/rjeka/kubernetes-ceph-percona.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> kubernetes-ceph-percona</code> </pre> <br><p>  etcd_version est la version de etcd.  Vous pouvez le voir sur la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">page etcd dans github</a> .  Au moment d'√©crire ces lignes, il y avait la version v3.3.13 que j'utilise v3.3.10. <br>  etcdToken - vous pouvez laisser le m√™me, ou g√©n√©rer le v√¥tre. </p><br><p>  Nous mettons docker-compose </p><br><pre> <code class="plaintext hljs">apt-get install -y docker-compose</code> </pre> <br><p>  Nous g√©n√©rons une config </p><br><pre> <code class="plaintext hljs">./create-config.sh docker</code> </pre> <br><p>  Ex√©cutez l'installation du cluster etcd dans docker </p><br><pre> <code class="plaintext hljs">docker-compose --file etcd-docker/docker-compose.yaml up -d</code> </pre> <br><p>  V√©rifiez que les conteneurs sont en place </p><br><pre> <code class="plaintext hljs">docker ps</code> </pre> <br><p>  Et le statut du cluster etcd </p><br><pre> <code class="plaintext hljs">root@master01:~/kubernetes-ceph-percona# docker exec -ti etcd etcdctl cluster-health member 61db137992290fc is healthy: got healthy result from http://10.73.71.27:2379 member b11e73358a31b109 is healthy: got healthy result from http://10.73.71.25:2379 member f67dd9aaa8a44ab9 is healthy: got healthy result from http://10.73.71.26:2379 cluster is healthy root@master01:~/kubernetes-ceph-percona# docker exec -ti etcd etcdctl member list 61db137992290fc: name=etcd3 peerURLs=http://10.73.71.27:2380 clientURLs=http://10.73.71.27:2379,http://10.73.71.27:4001 isLeader=false b11e73358a31b109: name=etcd1 peerURLs=http://10.73.71.25:2380 clientURLs=http://10.73.71.25:2379,http://10.73.71.25:4001 isLeader=true f67dd9aaa8a44ab9: name=etcd2 peerURLs=http://10.73.71.26:2380 clientURLs=http://10.73.71.26:2379,http://10.73.71.26:4001 isLeader=false</code> </pre> <br><p>  En cas de probl√®me </p><br><pre> <code class="plaintext hljs">docker logs etcd</code> </pre> </div></div><br><a name="master-one"></a><br><h2>  Lancement du premier assistant kubernetes </h2><br><p>  Tout d'abord, nous devons g√©n√©rer une configuration pour kubeadmin </p><br><pre> <code class="plaintext hljs">./create-config.sh kubeadm</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Nous d√©montons une config pour kubeadm</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: kubeadm.k8s.io/v1beta1 kind: InitConfiguration localAPIEndpoint: advertiseAddress: 10.73.71.25 #    API- --- apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration kubernetesVersion: stable #      apiServer: #    kubeadm   certSANs: - 127.0.0.1 - 10.73.71.25 - 10.73.71.26 - 10.73.71.27 controlPlaneEndpoint: 10.73.71.25 #     etcd: #  etc external: endpoints: - http://10.73.71.25:2379 - http://10.73.71.26:2379 - http://10.73.71.27:2379 networking: podSubnet: 192.168.0.0/16 #   ,   CNI  .</code> </pre> <br><p>  Vous pouvez <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lire</a> sur les sous-r√©seaux CNI dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation de kubernetes.</a> <br>  Il s'agit d'une configuration fonctionnant au minimum.  Pour un cluster avec trois assistants, vous pouvez le modifier pour la configuration de votre cluster.  Par exemple, si vous souhaitez utiliser 2 assistants, sp√©cifiez simplement deux adresses dans certSAN. <br>  Tous les param√®tres de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">configuration</a> se trouvent dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la description de l'API kubeadm</a> . </p></div></div><br><p>  Nous initions le premier ma√Ætre </p><br><pre> <code class="plaintext hljs">kubeadm init --config=kubeadmin/kubeadm-init.yaml</code> </pre> <br><p>  Si kubeadm fonctionne sans erreur, √† la sortie, nous obtenons approximativement la sortie suivante: </p><br><pre> <code class="plaintext hljs">You can now join any number of control-plane nodes by copying certificate authorities and service account keys on each node and then running the following as root: kubeadm join 10.73.71.25:6443 --token ivwoap.259retezqf34amx8 \ --discovery-token-ca-cert-hash sha256:b5c93e32457c8e6478782ff62e8ef77acf72738dda59cd603cdf4821abe12ca3 \ --control-plane Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10.73.71.25:6443 --token ivwoap.259retezqf34amx8 \ --discovery-token-ca-cert-hash sha256:b5c93e32457c8e6478782ff62e8ef77acf72738dda59cd603cdf4821abe12ca3</code> </pre> <br><a name="callica"></a><br><h2>  Installation de CNI Calico </h2><br><p>  Le moment est venu d'√©tablir un r√©seau dans lequel nos pods fonctionneront.  J'utilise du calicot, et nous le mettrons. <br>  Et pour commencer, configurez l'acc√®s pour kubelet.  Nous ex√©cutons toutes les commandes sur master01 <br>  Si vous ex√©cutez en tant que root </p><br><pre> <code class="plaintext hljs">export KUBECONFIG=/etc/kubernetes/admin.conf</code> </pre> <br><p>  Si sous le simple utilisateur </p><br><pre> <code class="plaintext hljs">mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config</code> </pre> <br><p>  Vous pouvez √©galement g√©rer le cluster √† partir de votre ordinateur portable ou de n'importe quelle machine locale.  Pour ce faire, copiez le fichier /etc/kubernetes/admin.conf sur votre ordinateur portable ou toute autre machine dans $ HOME / .kube / config </p><br><p>  Nous <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">mettons</a> CNI <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">selon la documentation de Kubernetes</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><br></a> </p><br><pre> <code class="plaintext hljs">kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml</code> </pre> <br><p>  On attend que tous les pods se l√®vent </p><br><pre> <code class="plaintext hljs">watch -n1 kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-59f54d6bbc-psr2z 1/1 Running 0 96s kube-system calico-node-hm49z 1/1 Running 0 96s kube-system coredns-5c98db65d4-svcx9 1/1 Running 0 77m kube-system coredns-5c98db65d4-zdlb8 1/1 Running 0 77m kube-system kube-apiserver-master01 1/1 Running 0 76m kube-system kube-controller-manager-master01 1/1 Running 0 77m kube-system kube-proxy-nkdqn 1/1 Running 0 77m kube-system kube-scheduler-master01 1/1 Running 0 77m</code> </pre> <br><a name="mastes-other"></a><br><h2>  Lancement des deuxi√®me et troisi√®me assistants kubernetes </h2><br><p>  Avant de d√©marrer master02 et master03, vous devez copier les certificats avec master01 g√©n√©r√©s par kubeadm lors de la cr√©ation du cluster.  Je vais copier via scp <br>  Sur master01 </p><br><pre> <code class="plaintext hljs">export master02=10.73.71.26 export master03=10.73.71.27 scp -r /etc/kubernetes/pki $master02:/etc/kubernetes/ scp -r /etc/kubernetes/pki $master03:/etc/kubernetes/</code> </pre> <br><p>  Sur master02 et master03 <br>  Cr√©er une configuration pour kubeadm </p><br><pre> <code class="plaintext hljs">./create-config.sh kubeadm</code> </pre> <br><p>  Et ajoutez master02 et master03 au cluster </p><br><pre> <code class="plaintext hljs">kubeadm init --config=kubeadmin/kubeadm-init.yaml</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Glitches √† plusieurs interfaces r√©seau !!!!</b> <div class="spoiler_text"><p>  En production, j'utilise kubernetes v1.13.5 et calico v3.3.  Et je n'ai pas eu de tels p√©pins. <br>  Mais lors de la pr√©paration de l'article et de l'utilisation de la version stable (au moment de la r√©daction, il s'agissait de v1.15.1 kubernetes et de la version 3.8 callico), j'ai rencontr√© un probl√®me qui s'est exprim√© dans les erreurs de d√©marrage CNI </p><br><pre> <code class="plaintext hljs">root@master01:~/kubernetes-ceph-percona# kubectl get pods -A -w NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-658558ddf8-t6gfs 0/1 ContainerCreating 0 11s kube-system calico-node-7td8g 1/1 Running 0 11s kube-system calico-node-dthg5 0/1 CrashLoopBackOff 1 11s kube-system calico-node-tvhkq 0/1 CrashLoopBackOff 1 11s</code> </pre> <br><p>  Il s'agit d'un probl√®me de calico daemon set lorsque le serveur a plusieurs interfaces r√©seau <br>  Sur githab, il y a un probl√®me sur ce probl√®me: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://github.com/projectcalico/calico/issues/2720</a> <br>  Il est r√©solu en modifiant le jeu de d√©mons calico-node et en ajoutant le param√®tre IP_AUTODETECTION_METHOD √† env </p><br><pre> <code class="plaintext hljs">kubectl edit -n kube-system ds calico-node</code> </pre> <br><p>  ajoutez le param√®tre IP_AUTODETECTION_METHOD avec le nom de votre interface sur laquelle l'assistant fonctionne;  dans mon cas c'est ens19 </p><br><pre> <code class="plaintext hljs">- name: IP_AUTODETECTION_METHOD value: ens19</code> </pre> <br><p><img src="https://habrastorage.org/webt/xe/pq/7h/xepq7hbpjwhgowfk0hkc6-ryw8a.png"><br>  V√©rifiez que tous les n≈ìuds du cluster sont op√©rationnels </p><br><pre> <code class="plaintext hljs"># kubectl get nodes NAME STATUS ROLES AGE VERSION master01 Ready master 28m v1.15.1 master02 Ready master 26m v1.15.1 master03 Ready master 18m v1.15.1</code> </pre> <br><p>  Et qu'est-ce que calica vivant </p><br><pre> <code class="plaintext hljs"># kubectl get pods -A -o wide | grep calico kube-system calico-kube-controllers-59f54d6bbc-5lxgn 1/1 Running 0 27m kube-system calico-node-fngpz 1/1 Running 1 24m kube-system calico-node-gk7rh 1/1 Running 0 8m55s kube-system calico-node-w4xtt 1/1 Running 0 25m</code> </pre> </div></div><br><a name="worknodes"></a><br><h2>  Ajouter des n≈ìuds de travail au cluster </h2><br><p>  √Ä l'heure actuelle, nous avons un cluster dans lequel trois n≈ìuds ma√Ætres fonctionnent.  Mais les n≈ìuds ma√Ætres sont des machines ex√©cutant l'api, le planificateur et d'autres services du cluster kubernetes.  Pour pouvoir ex√©cuter nos pods, nous avons besoin des soi-disant n≈ìuds de travail. <br>  Si vos ressources sont limit√©es, vous pouvez ex√©cuter des modules sur des n≈ìuds ma√Ætres, mais je ne vous conseille pas personnellement de le faire. </p><br><div class="spoiler">  <b class="spoiler_title">Faire fonctionner les foyers sur les masternodes</b> <div class="spoiler_text"><p>  Afin de permettre le lancement de foyers sur les n≈ìuds ma√Ætres, ex√©cutez la commande suivante sur l'un des assistants </p><br><pre> <code class="plaintext hljs">kubectl taint nodes --all node-role.kubernetes.io/master-</code> </pre> </div></div><br><p>  Installer les n≈ìuds kubelet, kubeadm, kubectl et docker sur le travailleur comme sur les n≈ìuds ma√Ætres </p><br><div class="spoiler">  <b class="spoiler_title">Installer kubelet, kubeadm, kubectl et docker</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apt-get update &amp;&amp; apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl apt-mark hold kubelet kubeadm kubectl</code> </pre> <br><h4>  Installer Docker </h4><br><p>  Installez docker selon les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">instructions de la documentation</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><br></a> </p><br><pre> <code class="plaintext hljs">apt-get remove docker docker-engine docker.io containerd runc apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common</code> </pre> <br><pre> <code class="plaintext hljs">curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add - apt-key fingerprint 0EBFCD88</code> </pre> <br><pre> <code class="plaintext hljs">add-apt-repository \ "deb [arch=amd64] https://download.docker.com/linux/debian \ $(lsb_release -cs) \ stable"</code> </pre> <br><pre> <code class="plaintext hljs">apt-get update apt-get install docker-ce docker-ce-cli containerd.io</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Installation Installation de Kubelet, Kubectl, Kubeadm et docker en utilisant ansible</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">git clone https://github.com/rjeka/kubernetes-ceph-percona.git cd kubernetes-ceph-percona/playbooks vim masters.ini</code> </pre> <br><p>  Dans le groupe ma√Ætres, enregistrez les ma√Ætres ip. <br>  Dans le groupe des travailleurs, √©crivez l'ip des n≈ìuds de travail. </p><br><pre> <code class="plaintext hljs"># sudo c  ansible-playbook -i hosts.ini kubelet.yaml -K ansible-playbook -i hosts.ini docker.yaml -K # sudo  ansible-playbook -i hosts.ini kubelet.yaml ansible-playbook -i hosts.ini docker.yaml</code> </pre> </div></div></div></div><br><p>  Il est maintenant temps de revenir √† la ligne g√©n√©r√©e par kubeadm lorsque nous avons install√© le n≈ìud ma√Ætre. <br>  Elle ressemble √† √ßa pour moi. </p><br><pre> <code class="plaintext hljs">kubeadm join 10.73.71.25:6443 --token ivwoap.259retezqf34amx8 \ --discovery-token-ca-cert-hash sha256:b5c93e32457c8e6478782ff62e8ef77acf72738dda59cd603cdf4821abe12ca3</code> </pre> <br><p>  Il est n√©cessaire d'ex√©cuter cette commande sur chaque n≈ìud de travail. <br>  Si vous n'avez pas √©crit de jeton, vous pouvez g√©n√©rer un nouveau </p><br><pre> <code class="plaintext hljs">kubeadm token create --print-join-command --ttl=0</code> </pre> <br><p>  Une fois que kubeadm fonctionne, votre nouveau n≈ìud est entr√© dans le cluster et pr√™t √† fonctionner </p><br><pre> <code class="plaintext hljs">This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster.</code> </pre> <br><p>  Maintenant regardons le r√©sultat </p><br><pre> <code class="plaintext hljs">root@master01:~# kubectl get nodes NAME STATUS ROLES AGE VERSION master01 Ready master 10d v1.15.1 master02 Ready master 10d v1.15.1 master03 Ready master 10d v1.15.1 worknode01 Ready &lt;none&gt; 5m44s v1.15.1 worknode02 Ready &lt;none&gt; 59s v1.15.1 worknode03 Ready &lt;none&gt; 51s v1.15.1</code> </pre> <br><a name="haproxy"></a><br><h2>  Installer haproxy sur les n≈ìuds de travail </h2><br><p>  Nous avons maintenant un cluster de travail avec trois n≈ìuds ma√Ætres et trois n≈ìuds travailleurs. <br>  Le probl√®me est que maintenant nos n≈ìuds de travail n'ont pas de mode HA. <br>  Si vous regardez le fichier de configuration du kubelet, nous verrons que nos n≈ìuds de travail n'acc√®dent qu'√† l'un des trois n≈ìuds ma√Ætres. </p><br><pre> <code class="plaintext hljs">root@worknode01:~# cat /etc/kubernetes/kubelet.conf | grep server: server: https://10.73.71.27:6443</code> </pre> <br><p>  Dans mon cas, c'est master03.  Avec cette configuration, si master03 se bloque, le n≈ìud de travail perdra la communication avec le serveur API du cluster.  Pour rendre notre cluster enti√®rement HA, nous allons installer un Load Balancer (Haproxy) sur chacun des travailleurs, qui, selon le round robin, dispersera les demandes pour trois n≈ìuds principaux, et dans la configuration du kubelet sur les n≈ìuds de travail, nous changerons l'adresse du serveur en 127.0.0.1:6443 <br>  Tout d'abord, installez HAProxy sur chaque n≈ìud de travail. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Il y a une bonne feuille de triche pour l'installation</a> </p><br><pre> <code class="plaintext hljs">curl https://haproxy.debian.net/bernat.debian.org.gpg | \ apt-key add - echo deb http://haproxy.debian.net stretch-backports-2.0 main | \ tee /etc/apt/sources.list.d/haproxy.list apt-get update apt-get install haproxy=2.0.\*</code> </pre> <br><p>  Une fois HAproxy install√©, nous devons cr√©er une configuration pour celui-ci. <br>  Si sur les n≈ìuds de travail il n'y a pas de r√©pertoire avec des fichiers de configuration, nous le clonons </p><br><pre> <code class="plaintext hljs">git clone https://github.com/rjeka/kubernetes-ceph-percona.git cd kubernetes-ceph-percona/</code> </pre> <br><p>  Et ex√©cutez le script de configuration avec le drapeau haproxy </p><br><pre> <code class="plaintext hljs">./create-config.sh haproxy</code> </pre> <br><p>  Le script configurera et red√©marrera haproxy. <br>  V√©rifiez que haproxy a commenc√© √† √©couter le port 6443. </p><br><pre> <code class="plaintext hljs">root@worknode01:~/kubernetes-ceph-percona# netstat -alpn | grep 6443 tcp 0 0 127.0.0.1:6443 0.0.0.0:* LISTEN 30675/haproxy tcp 0 0 10.73.75.241:6443 0.0.0.0:* LISTEN 30675/haproxy</code> </pre> <br><p>  Maintenant, nous devons dire √† kubelet d'acc√©der √† localhost au lieu du n≈ìud ma√Ætre.  Pour ce faire, modifiez la valeur du serveur dans les fichiers /etc/kubernetes/kubelet.conf et /etc/kubernetes/bootstrap-kubelet.conf sur tous les n≈ìuds de travail. </p><br><pre> <code class="plaintext hljs">vim /etc/kubernetes/kubelet.conf vim nano /etc/kubernetes/bootstrap-kubelet.conf</code> </pre> <br><p>  La valeur du serveur doit prendre cette forme: </p><br><pre> <code class="plaintext hljs">server: https://127.0.0.1:6443</code> </pre> <br><p>  Apr√®s avoir apport√© les modifications, red√©marrez les services kubelet et docker </p><br><pre> <code class="plaintext hljs">systemctl restart kubelet &amp;&amp; systemctl restart docker</code> </pre> <br><p>  V√©rifiez que tous les n≈ìuds fonctionnent correctement. </p><br><pre> <code class="plaintext hljs">kubectl get nodes NAME STATUS ROLES AGE VERSION master01 Ready master 29m v1.15.1 master02 Ready master 27m v1.15.1 master03 Ready master 26m v1.15.1 worknode01 Ready &lt;none&gt; 25m v1.15.1 worknode02 Ready &lt;none&gt; 3m15s v1.15.1 worknode03 Ready &lt;none&gt; 3m16s v1.15.1</code> </pre> <br><p>  Jusqu'√† pr√©sent, nous n'avons aucune application dans le cluster pour tester HA.  Mais nous pouvons arr√™ter le fonctionnement de kubelet sur le premier n≈ìud ma√Ætre et nous assurer que notre cluster est rest√© op√©rationnel. </p><br><pre> <code class="plaintext hljs">systemctl stop kubelet &amp;&amp; systemctl stop docker</code> </pre> <br><p>  V√©rifier √† partir du deuxi√®me n≈ìud ma√Ætre </p><br><pre> <code class="plaintext hljs">root@master02:~# kubectl get nodes NAME STATUS ROLES AGE VERSION master01 NotReady master 15h v1.15.1 master02 Ready master 15h v1.15.1 master03 Ready master 15h v1.15.1 worknode01 Ready &lt;none&gt; 15h v1.15.1 worknode02 Ready &lt;none&gt; 15h v1.15.1 worknode03 Ready &lt;none&gt; 15h v1.15.1</code> </pre> <br><p>  Tous les n≈ìuds fonctionnent normalement sauf celui sur lequel nous avons arr√™t√© les services. <br>  N'oubliez pas de r√©activer les services kubernetes sur le premier n≈ìud ma√Ætre </p><br><pre> <code class="plaintext hljs">systemctl start kubelet &amp;&amp; systemctl start docker</code> </pre> <br><a name="ingress"></a><br><h2>  Installation d'Ingress Controller </h2><br><p>  Le contr√¥leur d'entr√©e est un module compl√©mentaire Kubernetes, avec lequel nous pouvons acc√©der √† nos applications de l'ext√©rieur.  Une description d√©taill√©e se trouve dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation Kuberbnetes</a> .  Il y a pas mal de contr√¥leurs en entr√©e, j'utilise un contr√¥leur de Nginx.  Je vais parler de son installation.  La documentation sur le fonctionnement, la configuration et l'installation du contr√¥leur Ingress de Nginx peut √™tre consult√©e sur le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">site officiel</a> </p><br><p>  Commen√ßons l'installation, toutes les commandes peuvent √™tre ex√©cut√©es avec master01. <br>  Installez le contr√¥leur lui-m√™me </p><br><pre> <code class="plaintext hljs">kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml</code> </pre> <br><p>  Et maintenant - un service gr√¢ce auquel l'entr√©e sera disponible <br>  Pour ce faire, pr√©parez la config </p><br><pre> <code class="plaintext hljs">./create-config.sh ingress</code> </pre> <br><p>  Et envoyez-le √† notre cluster </p><br><pre> <code class="plaintext hljs">kubectl apply -f ingress/service-nodeport.yaml</code> </pre> <br><p>  V√©rifiez que notre Ingress fonctionne aux bonnes adresses et √©coute sur les bons ports. </p><br><pre> <code class="plaintext hljs"># kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 10.99.35.95 10.73.71.25,10.73.71.26,10.73.71.27 80:31669/TCP,443:31604/TCP 10m</code> </pre> <br><pre> <code class="plaintext hljs"> kubectl describe svc -n ingress-nginx ingress-nginx Name: ingress-nginx Namespace: ingress-nginx Labels: app.kubernetes.io/name=ingress-nginx app.kubernetes.io/part-of=ingress-nginx Annotations: kubectl.kubernetes.io/last-applied-configuration: {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app.kubernetes.io/name":"ingress-nginx","app.kubernetes.io/par... Selector: app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/part-of=ingress-nginx Type: NodePort IP: 10.99.35.95 External IPs: 10.73.71.25,10.73.71.26,10.73.71.27 Port: http 80/TCP TargetPort: 80/TCP NodePort: http 31669/TCP Endpoints: 192.168.142.129:80 Port: https 443/TCP TargetPort: 443/TCP NodePort: https 31604/TCP Endpoints: 192.168.142.129:443 Session Affinity: None External Traffic Policy: Cluster Events: &lt;none&gt;</code> </pre> <br><a name="dashboard"></a><br><h2>  Installer l'interface utilisateur Web (tableau de bord) </h2><br><p>  Kubernetes poss√®de une interface utilisateur Web standard, √† travers laquelle il est parfois pratique de consulter rapidement l'√©tat d'un cluster ou de ses parties individuelles.  Dans mon travail, j'utilise souvent le tableau de bord pour le diagnostic initial des d√©ploiements ou l'√©tat de parties d'un cluster. <br>  Le lien vers la documentation est sur le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">site kubernetes</a> <br>  L'installation  J'utilise la version stable, je n'ai pas encore essay√© 2.0. </p><br><pre> <code class="plaintext hljs">#  kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml # 2.0 kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta1/aio/deploy/recommended.yaml</code> </pre> <br><p>  Apr√®s avoir install√© le panneau dans notre cluster, le panneau est devenu disponible sur </p><br><pre> <code class="plaintext hljs">http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/.</code> </pre> <br><p>  Mais pour y acc√©der, nous devons transf√©rer les ports de la machine locale √† l'aide du proxy kubectl.  Pour moi, ce sch√©ma n'est pas tr√®s pratique.  Par cons√©quent, je vais modifier le service du panneau de configuration afin que le tableau de bord devienne disponible sur l'adresse de n'importe quel n≈ìud de cluster sur le port 30443. Il existe encore d'autres fa√ßons d'acc√©der au tableau de bord, par exemple, par entr√©e.  J'examinerai peut-√™tre cette m√©thode dans les publications suivantes. <br>  Pour modifier le service, ex√©cutez le d√©ploiement du service modifi√© </p><br><pre> <code class="plaintext hljs">kubectl apply -f dashboard/service-nodeport.yaml</code> </pre> <br><p>  Il reste √† cr√©er l'utilisateur admin et le token pour acc√©der au cluster via le tableau de bord </p><br><pre> <code class="plaintext hljs">kubectl apply -f dashboard/rbac.yaml kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')</code> </pre> <br><p>  Apr√®s cela, vous pouvez vous connecter au panneau de configuration √† <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://10.73.71.25:30443</a> <br><img src="https://habrastorage.org/webt/p7/zu/8q/p7zu8qv47mwdsmdydtvyo7gs_y4.png"><br>  √âcran d'accueil du tableau de bord <br><img src="https://habrastorage.org/webt/h2/ks/jq/h2ksjq_7egqatf4ulnl0zkwqvqk.png"></p><br><p>  F√©licitations!  Si vous avez atteint cette √©tape, vous disposez d'un cluster HA fonctionnel de kubernetes, pr√™t pour le d√©ploiement de vos applications. <br> Kubernetes    ,      .          . <br>       ,  GitHub,    ,    . <br> C ,  . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr462473/">https://habr.com/ru/post/fr462473/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr462461/index.html">R√©sultats de l'enqu√™te GOES-17 sur l'accident</a></li>
<li><a href="../fr462465/index.html">Utilisation des lieux natifs d'Apple</a></li>
<li><a href="../fr462467/index.html">Frontend Weekly Digest (29 juillet - 4 ao√ªt 2019)</a></li>
<li><a href="../fr462469/index.html">Quelques consid√©rations pour le calcul simultan√© en R pour les t√¢ches "Entreprise"</a></li>
<li><a href="../fr462471/index.html">R√©solution de probl√®mes avec pwnable.kr 16 - uaf. Utiliser apr√®s une vuln√©rabilit√© gratuite</a></li>
<li><a href="../fr462475/index.html">Alexey Savvateev: Comment lutter contre la corruption √† l'aide des math√©matiques (Prix Nobel d'√©conomie pour 2016)</a></li>
<li><a href="../fr462477/index.html">Des scientifiques affirment que l'IA est l'auteur d'un nouveau brevet et tentent de changer la loi sur les brevets</a></li>
<li><a href="../fr462479/index.html">Steam Windows Client Local Privilege Escalation 0day</a></li>
<li><a href="../fr462481/index.html">FAQ sur le syst√®me de type</a></li>
<li><a href="../fr462483/index.html">Programmation fonctionnelle: un jouet farfelu qui tue la productivit√© du travail. Partie 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>