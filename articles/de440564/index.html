<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🍉 🤸🏿 🧥 GPT-2 neuronales Netzwerk von OpenAI. Schnellstart 🧑🏾‍🤝‍🧑🏻 🧑🏾 🕤</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Kaum war das Rauschen über das neuronale BERT-Netzwerk von Google zu hören, das bei einer Reihe von Konversationsaufgaben (NLP) beim maschinellen Lern...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>GPT-2 neuronales Netzwerk von OpenAI. Schnellstart</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/440564/"><p><img src="https://habrastorage.org/getpro/habr/post_images/1cf/d63/ae6/1cfd63ae6b68d59325ef90cc4ea93f35.png" alt="Bild"></p><br><p>  Kaum war das Rauschen über das neuronale BERT-Netzwerk von Google zu hören, das bei einer Reihe von Konversationsaufgaben (NLP) beim maschinellen Lernen auf dem neuesten Stand der Technik war, als OpenAI eine neue Entwicklung einführte: GPT-2.  Dieses neuronale Netzwerk mit einer Rekordzahl von Parametern im Moment (1,5 Milliarden gegenüber den in solchen Fällen üblicherweise verwendeten 100 bis 300 Millionen) konnte ganze Seiten mit verbundenem Text erzeugen. </p><br><p>  Es ist so gut zu generieren, dass OpenAI sich weigerte, die Vollversion zu veröffentlichen, aus Angst, dass sie dieses neuronale Netzwerk verwenden würden, um gefälschte Nachrichten, Kommentare und Bewertungen zu erstellen, die nicht von den echten zu unterscheiden sind. </p><br><p>  In OpenAI wurde jedoch eine reduzierte Version des neuronalen GPT-2-Netzwerks mit 117 Millionen Parametern gemeinsam genutzt.  Wir werden es über den Google Colab-Dienst starten und damit experimentieren. </p><a name="habracut"></a><br><h1 id="nemnogo-predystorii">  Ein kleiner Hintergrund </h1><br><p>  Für diejenigen, die die Entwicklung des Fortschritts in der natürlichen Sprachverarbeitung (NLP) nicht verfolgt haben. </p><br><p>  Im Sommer 2018 trainierte OpenAI auf einer großen Textmenge ein neuronales <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GPT-</a> Netzwerk, das auf der Transformer-Architektur basiert.  Es stellte sich heraus, dass, wenn Sie einige der letzten Ebenen ersetzen und für eine bestimmte Aufgabe neu trainieren (dieser Ansatz wird als Feinabstimmung bezeichnet und wird häufig beim maschinellen Lernen verwendet), frühere Datensätze bei einer Vielzahl von Konversationsaufgaben sofort gebrochen werden. </p><br><p>  Basierend auf dieser Entwicklung hat Google Ende 2018 ein eigenes neuronales <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">BERT-</a> Netzwerk erstellt.  Sie haben das Ergebnis erheblich verbessert, indem sie das neuronale Netzwerk im Gegensatz zum GPT bidirektional gemacht haben. </p><br><p>  OpenAI wollte nicht aufgeben und erhöhte im Februar 2019 seine GPT sofort um das Zehnfache und trainierte sie für eine noch größere Textmenge - auf 8 Millionen Webseiten (insgesamt 40 GB Text).  Das resultierende <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GPT-2-</a> Netzwerk ist derzeit das größte neuronale Netzwerk mit einer beispiellosen Anzahl von Parametern von 1,5 Milliarden (BERT hatte 340 Millionen im größten Modell und 110 Millionen im Standard-BERT). </p><br><p>  Infolgedessen konnte GPT-2 ganze Seiten mit zusammenhängendem Text erzeugen.  Mit wiederholten Verweisen auf die Namen der Charaktere im Verlauf der Erzählung, Zitaten, Verweisen auf verwandte Ereignisse und so weiter.  Ich werde hier keine Beispiele nennen, sondern diejenigen verweisen, die auf den Originalartikel im OpenAI-Blog: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bessere Sprachmodelle und ihre Auswirkungen</a> oder die Links am Ende des Artikels verweisen möchten. </p><br><p>  Das Generieren eines zusammenhängenden Textes dieser Qualität ist an sich schon beeindruckend, aber das Interessanteste ist anders.  GPT-2 ohne zusätzliches Training zeigte sofort Ergebnisse auf dem neuesten Stand der Technik bei einer Reihe von Konversationsaufgaben.  Ich wiederhole, wer die Wichtigkeit des Augenblicks verpasst hat - ohne zusätzliches Training für eine bestimmte Aufgabe! </p><br><p>  Wie haben sie das erreicht?  Einfach den neuronalen Netzen die richtigen Fragen stellen. </p><br><h1 id="arhitektura-gpt-2">  GPT-2-Architektur </h1><br><p>  GPT-2 ist darauf trainiert, das nächste Wort in einem Satz vorherzusagen.  Dies ist ein klassischer Ansatz zum Generieren von Text.  Zunächst hatten Wiederholungsnetzwerke (RNN), insbesondere LSTM, in diesem Bereich Vorrang.  Nach der Erfindung der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Transformer-</a> Architektur im Sommer 2017 setzte sie sich jedoch allmählich in Konversationsaufgaben durch.  Obwohl der ursprüngliche Transformer Probleme beim Speichern langer Sequenzen hat (LSTMs erinnern sich an längere), haben die Trainingsgeschwindigkeit und die Tiefe des Netzwerks dies mehr als ausgeglichen.  Übrigens sind bereits einige Modifikationen des Transformators erschienen - mit der Einführung der Wiederholung ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Universal Transformers</a> ), einer Modifikation für längere Sequenzen ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Transformer-XL</a> ) und anderen, aber bisher wird in Google und OpenAI nur ein leicht abgestimmter Originaltransformator verwendet. </p><br><p>  Ich erinnere mich, dass BERT von Google etwas anders gelernt hat: nicht das nächste Wort in einem Satz vorherzusagen, sondern fehlende (maskierte) Wörter in einem Satz.  Und auch um festzustellen, ob zwei aufeinanderfolgende Sätze eine logische Fortsetzung voneinander sind oder ob sie in keiner Weise durch Bedeutung verbunden sind.  Dies ermöglichte es BERT, ein Sprachmodell zu sein, das die Bedeutung von Wörtern in Abhängigkeit von ihrer Umgebung (Kontext) versteht.  Was den Erfolg von BERT bei NPL-Aufgaben bestimmte.  Aber erst nach einer Umschulung (Feinabstimmung) für eine bestimmte Aufgabe.  Nur Wörter im Basismodell vorherzusagen, funktioniert darin nicht sehr gut.  Sie können mit BERT in Ihrem Browser (über Google Colab) spielen: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://habr.com/en/post/436878</a> . </p><br><p>  GPT-2 ist für die Umschulung nicht erforderlich.  Dies ist nicht nur ein Sprachmodell wie BERT, sondern ein Textgenerator.  Geben Sie ihr einfach den Anfang des Satzes, und dann wird sie ihr Wort für Wort ergänzen. </p><br><p>  Ein interessantes Detail: OpenAI-Untersuchungen haben gezeigt, dass Arrays von Wikipedia-Texten und literarischen Büchern (die insbesondere von BERT untersucht wurden) einen voreingenommenen Stil haben.  Daher erzeugen nur auf sie trainierte neuronale Netze keinen sehr guten Text.  Um die Eingabedaten und -stile zu diversifizieren, verwendete OpenAI GPT-2 für Schulungen auf regulären Webseiten, die von 8 Millionen Websites (insgesamt 40 GB Text) gesammelt wurden.  Und um Werbe- und Spammer-Websites zu verwerfen, haben sie in die Beispiel-Websites aufgenommen, deren Links im reddit eine gute Bewertung haben.  Das heißt, Websites, bei denen Live-Benutzer nützliche Informationen gefunden haben. </p><br><h1 id="pravilnyy-vopros-soderzhit-polovinu-otveta">  Die richtige Frage enthält die halbe Antwort. </h1><br><p>  So konnte GPT-2 dank seiner beispiellosen Größe Seiten mit zusammenhängendem Text generieren.  Aber das Erstaunlichste ist, dass sie durch das Stellen der richtigen Frage (d. H. Des richtigen Anfangs einer Phrase) verschiedene Fragen beantworten konnte!  Nur weil die Fortsetzung eines solchen Anfangs am natürlichsten ist. </p><br><p>  Um beispielsweise eine Antwort auf die Frage „Was ist die Erde?“ Zu erhalten, können Sie auf die Eingabe dieses neuronalen Netzwerks den Anfang des Satzes anwenden: „Erde ist ...“.  Und sie wird diesen Satz bis zum Ende vervollständigen.  Weil die Antwort eine natürliche Fortsetzung dieses Anfangs sein wird. </p><br><p>  Wenn Sie den Satzanfang richtig bilden, können Sie außerdem Erklärungen für verschiedene Zielgruppen erhalten, die deren Intelligenz, Alter und Bildung berücksichtigen.  Stellen Sie sich fortgesetzte Sätze vor: "Ich als Wissenschaftler glaube, dass die Erde ... ist."  Oder: "Ich als Landpflüger behaupte, die Erde sei ...".  Oder: "Ich als Lehrerin in einem Kindergarten werde Ihnen jetzt erklären, Kinder, dass die Erde ...". </p><br><p>  Wie Sie sehen können, können Sie durch Bilden der richtigen Fragen (am richtigen Anfang des Satzes) Antworten auf völlig unterschiedlichen Ebenen und mit unterschiedlichen Details erhalten.  In gewisser Weise passiert etwas Ähnliches bei Menschen.  Der Arzt muss dem Patienten den Krankheitsverlauf erklären, damit er versteht.  Auf Patientenebene.  Wenn Sie ein fünfjähriges Kind fragen, warum es das getan hat, kann es nicht sofort antworten (was natürlich bedeutet, dass Kinder mit Gefühlen und Emotionen leben).  Aber um die Antwort zu geben, die von ihm erwartet wird, beginnt das Kind, sie zu erfinden - um Text zu generieren.  Basierend auf der Tatsache, dass die Antwort zum Elternteil passt und zumindest irgendwie dem entspricht, was passiert ist.  Wie viele Eltern wissen, werden dies zunächst lächerliche Antworten sein.  Aber durch Ermutigung und Bestrafung ("Erzähl mir mehr", "finde keine Ausreden") lernt das Kind, detaillierte und vollständige Antworten zu geben. </p><br><p>  Diese Entwicklung von OpenAI und die Fähigkeit des GPT-2-Netzwerks, Antworten auf Konversationsaufgaben ohne spezielle zusätzliche Schulung für eine bestimmte Aufgabe bereitzustellen, werfen zwei interessante Fragen auf: </p><br><p>  1) Kann die Interpretierbarkeit neuronaler Netze durch einen solchen elementaren Textgenerator und den korrekten Beginn einer Phrase erreicht werden?  Wo die Antwort eine natürliche Erweiterung sein wird.  Angenommen, ein neuronales Netzwerk zeigt beispielsweise Siegel in einem Foto nicht durch die x-y-Koordinatennummern an, sondern erklärt seine Position im Klartext.  Wenn Sie ihr dann im Verlauf der Klärung die richtige Frage stellen, zum Beispiel: "Ich bin zu diesem Schluss gekommen, weil ...", können Sie theoretisch erklären, wie sie die Katze auf dem Foto gefunden hat.  Und diese Erklärung kann im Extremfall nicht schlechter sein als menschlich.  Dies löst das globale Problem der Interpretierbarkeit neuronaler Netze. </p><br><p>  2) Kann ein vorab trainiertes neuronales Netzwerk für große Textmengen universell sein, einen gesunden Menschenverstand haben und keine zusätzliche Schulung für bestimmte Aufgaben erfordern?  Dies bedeutet, dass das neuronale Netzwerk beim Versuch, die menschliche Sprache nachzuahmen (menschliche Antworten auf Fragen), zwangsläufig den gesunden Menschenverstand lernen muss, um diese den menschlichen Antworten sehr ähnlich zu geben.  Einsilbige fiktive Antworten zu geben, ist im Allgemeinen nicht typisch für Menschen.  Zum größten Teil geben die Menschen detaillierte und angemessene Antworten, was bedeutet, dass das Netzwerk lernen muss, dasselbe zu tun. </p><br><p>  Beide Fragen bleiben offen, aber der erste Schritt in ihrer Zustimmung ist definitiv getan. </p><br><h1 id="a-tochnee">  Und genauer? </h1><br><p>  Wenn Sie jetzt stehen, ist es besser, sich zu setzen.  Denn so hat OpenAI, das das neuronale GPT-2-Netzwerk verwendet, seine Ergebnisse bei Konversationsaufgaben für verschiedene Domänen erzielt: </p><br><p>  <strong>Antworten auf Fragen zum Text</strong> </p><br><p>  Das ist einfach.  Oder fütterte das Netzwerk mit ein paar Absätzen mit einer Beschreibung, die irgendwo in der Mitte stand, zum Beispiel "Der Apfel liegt auf dem Tisch", und am Ende wurde zugeschrieben: "Der Apfel ist auf ..." und das Netzwerk wurde der "Tabelle" hinzugefügt.  Weil es sich an den Kontext mehrerer Absätze erinnern kann. </p><br><p>  Oder fütterte das Netzwerk als ersten Satz mit einigen Beispielen vom Typ „Frage: eine Frage, Antwort: eine Antwort“ und am Ende nach der eigentlichen Frage fügten sie hinzu: „Antwort:“.  Und das neuronale Netzwerk hat die Antwort angehängt!  Da es die Struktur des Dokuments auf der vorherigen Frage-Antwort enthüllte.  Das ist erstaunlich. </p><br><p>  <strong>Kurzfassung (Zusammenfassung) des Textes</strong> </p><br><p>  Die Eingabe ist ein langer Text aus mehreren Absätzen oder sogar Seiten, und das neuronale Netzwerk sollte einen kurzen Inhalt schreiben.  Wie haben Sie dieses Verhalten von GPT-2 erhalten?  Kurz nach dem Text fügten sie "TL; DR" hinzu.  Und alle!  Dies erwies sich als ausreichend für das GPT-2, um nach diesen Zeichen eine Zusammenfassung des Artikels hinzuzufügen!  Weil solche Symbole im Internet häufig die Zusammenfassung des Beitrags bezeichnen. </p><br><p>  <strong>Textübersetzung</strong> </p><br><p>  Die GPT-2-Eingabe erhielt den Text in der Form: "Hallo = Hallo, Hund = Hund, Wind = Wind, Katze = ...".  Und das neuronale Netz fügte die Übersetzung des letzten Wortes hinzu: "Katze" (im Original auf Französisch).  Weil es die Struktur des Dokuments enthüllte und es einfach mit der logischsten Fortsetzung ergänzte.  Wenn dein Kiefer immer noch nicht von all dem abgefallen ist, dann habe ich zwei Neuigkeiten für dich und beide sind schlecht =). </p><br><h1 id="zapusk-gpt-2-cherez-google-colab">  GPT-2-Start über Google Colab </h1><br><p>  Leider wurde die Freigabe der Vollversion von GPT-2 in OpenAI verweigert.  Dies wird durch die Tatsache motiviert, dass es mit diesem neuronalen Netzwerk zu einfach ist, gefälschte Nachrichten und Bewertungen in Geschäften zu generieren.  Nach ihrer Aussage wird die Diskussion über die Angemessenheit der Auslegung dieses Modells für die nächsten 6 Monate fortgesetzt. Nach der OpenAI werden sie entscheiden, ob sie es hochladen oder nicht.  Für eine große Organisation ist es jedoch nicht schwierig, das Modell zu wiederholen (es scheint, dass sie es mehrere Tage lang für 256 TPU trainiert haben und nach vorläufigen Schätzungen etwa 45.000 US-Dollar gekostet haben). </p><br><p>  Sie veröffentlichten jedoch eine reduzierte Version von GPT-2 mit 117 Millionen Parametern (anstatt 1,5 Milliarden, wie im Vollmodell): <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://github.com/openai/gpt-2</a> .  Lassen Sie uns versuchen, es auszuführen und mit diesem Modell zu spielen. </p><br><p>  Update 9. November 2019: Endlich wurde die gesamte Modellreihe einschließlich 1,5 Milliarden angelegt. Die Dateien und Anweisungen für den Start wurden aktualisiert. </p><br><p>  Der einfachste Weg, dies zu tun, ist über Google Colab: </p><br><ol><li>  Öffnen Sie den Link </li></ol><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">http://colab.research.google.com/github/blade1780/bert/blob/master/Gpt-2.ipynb</a> </p><br><ol><li>  <strong>Wählen Sie im</strong> Menü <strong>Laufzeit</strong> die <strong>Option</strong> <strong>Alle ausführen aus</strong> , damit zum ersten Mal alle Zellen gestartet werden, die Modelldownloads und die erforderlichen Bibliotheken verbunden werden.  Stimmen Sie zu, bei Bedarf alle Runtime zurückzusetzen.  Geben Sie nach dem Erscheinen von "Model prompt &gt;&gt;&gt;" Text ein und drücken Sie die Eingabetaste. </li></ol><br><p>  Achten Sie ganz am Anfang auf die Zeile: </p><br><p>  model_name = '117M' </p><br><p>  Hier können Sie die Größe des zu verwendenden GPT-2-Modells angeben.  Folgende Modelle sind verfügbar (vorbehaltlich Aktualisierungen): </p><br><p>  117M <br>  124M <br>  355M <br>  774M <br>  1558M </p><br><p>  Hier ist 117M das kleinste Modell, das zum Zeitpunkt dieses Schreibens das einzige verfügbare war.  OpenAI legte später immer mehr Modelle vor, bis zum 5. November 2019 das Maximum von 1558 Millionen (mit 1,5 Milliarden Parametern). </p><br><div class="spoiler">  <b class="spoiler_title">Wenn etwas schief gelaufen ist ...</b> <div class="spoiler_text"><p>  Stellen Sie sicher, dass GPU und Python 3 im Menü Laufzeit -&gt; Laufzeittyp ändern ausgewählt sind </p><br><p>  Wenn die Schaltfläche Verbinden nicht aktiv ist, klicken Sie darauf, um eine Verbindung herzustellen. </p></div></div><br><p>  Oder erstellen Sie den gesamten Code manuell: </p><br><ol><li>  Gehen Sie zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://colab.research.google.com</a> </li><li>  Drücken Sie die blaue Taste NEW PYTHON 3 NOTEBOOK </li><li>  Wählen Sie im Menü Laufzeit -&gt; Laufzeittyp ändern die Option Python 3 und die GPU aus (letztere, um das neuronale Netzwerk auf der GPU auszuführen). </li><li>  Geben Sie in die erste Zelle Folgendes ein: </li></ol><br><pre><code class="python hljs">model_name = <span class="hljs-string"><span class="hljs-string">'117M'</span></span> !git clone https://github.com/openai/gpt<span class="hljs-number"><span class="hljs-number">-2</span></span> %cd gpt<span class="hljs-number"><span class="hljs-number">-2</span></span> !pip3 install -r requirements.txt !python3 download_model.py $model_name</code> </pre> <br><p>  Anstelle von 117M (dem kleinsten) können Sie ein beliebiges Zwischen- oder größtes Modell angeben: 1558M. </p><br><p>  Klicken Sie auf das schwarze Wiedergabesymbol links neben der Zelle.  Dadurch wird das ausgewählte neuronale GPT-2-Netzwerk heruntergeladen und die erforderlichen Abhängigkeiten installiert. </p><br><p>  In der zweiten Zelle (Sie können sie über das Menü Einfügen -&gt; Codezelle oder durch Bewegen der Maus unter der Mitte der aktuellen Zelle hinzufügen, werden die Schaltflächen zum Hinzufügen angezeigt): </p><br><pre> <code class="python hljs">!python3 src/interactive_conditional_samples.py --model_name=$model_name</code> </pre> <br><p>  Dadurch wird der interaktive Modus gestartet.  Warten Sie, bis das neuronale Netzwerk hochgefahren ist und ein Fenster zur Texteingabe mit der Aufschrift „Modellmodell &gt;&gt;&gt;“ angezeigt wird. Geben Sie den Anfang der Phrase ein und drücken Sie die Eingabetaste. Nach einer Weile wird der generierte Text unter der Überschrift BEISPIEL angezeigt. </p><br><p>  Sie können auch den Modus zum Generieren von vollständig zufälligem Text starten.  Der Text wird endlos in kleinen Teilen von BEISPIEL 1, BEISPIEL 2 usw. generiert, bis Sie auf die Schaltfläche Stopp in der Zelle klicken.  Erstellen Sie dazu eine neue Zelle mit dem Code: </p><br><pre> <code class="python hljs">!python3 src/generate_unconditional_samples.py --model_name=$model_name | tee samples.txt</code> </pre> <br><p>  Das Ergebnis wird in der Datei samples.txt gespeichert.  Es kann mit den folgenden Befehlen heruntergeladen werden (erstellen Sie erneut eine neue Zelle und führen Sie sie nach dem Generieren des Textes aus): </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> google.colab <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> files files.download(<span class="hljs-string"><span class="hljs-string">'samples.txt'</span></span>)</code> </pre> <br><p>  Sie können die Parameter zum Generieren von Text ändern (Zufallskoeffizient usw., siehe Beschreibung in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Originalarbeit</a> ): </p><br><pre> <code class="python hljs">!python3 src/generate_unconditional_samples.py --model_name=$model_name --top_k <span class="hljs-number"><span class="hljs-number">40</span></span> --temperature <span class="hljs-number"><span class="hljs-number">0.7</span></span> | tee samples.txt</code> </pre> <br><p>  Da es sich bei dem 117M um ein <strong>stark</strong> reduziertes Modell handelt, sollten Sie keine Wunder erwarten (Update: Zum Zeitpunkt dieses Schreibens war nur dieses verfügbar. Jetzt ist alles verfügbar, einschließlich des ursprünglich größten 1558M, siehe oben).  Die meisten der generierten Samples sind Unsinn.  Es gibt aber auch sinnvolle Abschnitte.  Der Text sollte in Englisch sein, während GPT-2 in anderen Sprachen noch nicht funktionieren kann. </p><br><h2 id="primery-generiruemogo-teksta">  Beispiele für generierten Text </h2><br><p>  Beispiele für den vom <strong>vollständigen</strong> Modell generierten Text: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://blog.openai.com/better-language-models/#sample1</a> (oben in der Leiste für 8 Geschichten). </p><br><p>  Es gibt auch eine riesige 2,4-MB-Textdatei mit zufällig generierten Beispielen: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://raw.githubusercontent.com/openai/gpt-2/master/gpt2-samples.txt</a> </p><br><p>  Und noch eine, 2,27 MB, mit anderen Zufallseinstellungen: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://raw.githubusercontent.com/openai/gpt-2/master/gpt2-topk40-samples.txt</a> </p><br><h1 id="ssylki">  Referenzen </h1><br><ul><li>  Original OpenAI Blog Artikel: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bessere Sprachmodelle und ihre Auswirkungen</a> </li><li>  Github mit allen vorgefertigten Versionen von GPT-2: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://github.com/openai/gpt-2</a> </li><li>  Diskussion über reddit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hauptnachrichten</a> </li><li>  Diskussion über reddit, das sich weigert, das vollständige Modell zu veröffentlichen: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Es ist Zeit für OpenAI, CloseAI umzubenennen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Google Colab-Notebook zum Ausführen von GPT-2 (alle Modelle) in einem Browser</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de440564/">https://habr.com/ru/post/de440564/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de440554/index.html">Bequeme BEM</a></li>
<li><a href="../de440556/index.html">Lernen des Entwurfs von Entitätsbeziehungsdiagrammen</a></li>
<li><a href="../de440558/index.html">Technologie, die Quantennetzwerke näher bringt</a></li>
<li><a href="../de440560/index.html">Alexander Belokrylov und Dmitry Chuyko über Liberica JDK auf jug.msk.ru</a></li>
<li><a href="../de440562/index.html">Windows Phone - ALLES, es ist immer wieder</a></li>
<li><a href="../de440566/index.html">Ungehindert beschleunigen oder SIMD kennenlernen</a></li>
<li><a href="../de440568/index.html">Wir schreiben eine Lernanwendung in Go und Javascript, um die tatsächlichen Aktienrenditen zu bewerten. Teil 2 - Testen des Backends</a></li>
<li><a href="../de440570/index.html">Reflektierende Schattenkarten: Teil 2 - Implementierung</a></li>
<li><a href="../de440574/index.html">Russischer AI Cup 2018, Geschichte 9 Plätze</a></li>
<li><a href="../de440576/index.html">Wichtige Änderungen an CTE in PostgreSQL 12</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>