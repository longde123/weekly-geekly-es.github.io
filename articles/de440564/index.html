<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üçâ ü§∏üèø üß• GPT-2 neuronales Netzwerk von OpenAI. Schnellstart üßëüèæ‚Äçü§ù‚Äçüßëüèª üßëüèæ üï§</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Kaum war das Rauschen √ºber das neuronale BERT-Netzwerk von Google zu h√∂ren, das bei einer Reihe von Konversationsaufgaben (NLP) beim maschinellen Lern...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>GPT-2 neuronales Netzwerk von OpenAI. Schnellstart</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/440564/"><p><img src="https://habrastorage.org/getpro/habr/post_images/1cf/d63/ae6/1cfd63ae6b68d59325ef90cc4ea93f35.png" alt="Bild"></p><br><p>  Kaum war das Rauschen √ºber das neuronale BERT-Netzwerk von Google zu h√∂ren, das bei einer Reihe von Konversationsaufgaben (NLP) beim maschinellen Lernen auf dem neuesten Stand der Technik war, als OpenAI eine neue Entwicklung einf√ºhrte: GPT-2.  Dieses neuronale Netzwerk mit einer Rekordzahl von Parametern im Moment (1,5 Milliarden gegen√ºber den in solchen F√§llen √ºblicherweise verwendeten 100 bis 300 Millionen) konnte ganze Seiten mit verbundenem Text erzeugen. </p><br><p>  Es ist so gut zu generieren, dass OpenAI sich weigerte, die Vollversion zu ver√∂ffentlichen, aus Angst, dass sie dieses neuronale Netzwerk verwenden w√ºrden, um gef√§lschte Nachrichten, Kommentare und Bewertungen zu erstellen, die nicht von den echten zu unterscheiden sind. </p><br><p>  In OpenAI wurde jedoch eine reduzierte Version des neuronalen GPT-2-Netzwerks mit 117 Millionen Parametern gemeinsam genutzt.  Wir werden es √ºber den Google Colab-Dienst starten und damit experimentieren. </p><a name="habracut"></a><br><h1 id="nemnogo-predystorii">  Ein kleiner Hintergrund </h1><br><p>  F√ºr diejenigen, die die Entwicklung des Fortschritts in der nat√ºrlichen Sprachverarbeitung (NLP) nicht verfolgt haben. </p><br><p>  Im Sommer 2018 trainierte OpenAI auf einer gro√üen Textmenge ein neuronales <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GPT-</a> Netzwerk, das auf der Transformer-Architektur basiert.  Es stellte sich heraus, dass, wenn Sie einige der letzten Ebenen ersetzen und f√ºr eine bestimmte Aufgabe neu trainieren (dieser Ansatz wird als Feinabstimmung bezeichnet und wird h√§ufig beim maschinellen Lernen verwendet), fr√ºhere Datens√§tze bei einer Vielzahl von Konversationsaufgaben sofort gebrochen werden. </p><br><p>  Basierend auf dieser Entwicklung hat Google Ende 2018 ein eigenes neuronales <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">BERT-</a> Netzwerk erstellt.  Sie haben das Ergebnis erheblich verbessert, indem sie das neuronale Netzwerk im Gegensatz zum GPT bidirektional gemacht haben. </p><br><p>  OpenAI wollte nicht aufgeben und erh√∂hte im Februar 2019 seine GPT sofort um das Zehnfache und trainierte sie f√ºr eine noch gr√∂√üere Textmenge - auf 8 Millionen Webseiten (insgesamt 40 GB Text).  Das resultierende <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GPT-2-</a> Netzwerk ist derzeit das gr√∂√üte neuronale Netzwerk mit einer beispiellosen Anzahl von Parametern von 1,5 Milliarden (BERT hatte 340 Millionen im gr√∂√üten Modell und 110 Millionen im Standard-BERT). </p><br><p>  Infolgedessen konnte GPT-2 ganze Seiten mit zusammenh√§ngendem Text erzeugen.  Mit wiederholten Verweisen auf die Namen der Charaktere im Verlauf der Erz√§hlung, Zitaten, Verweisen auf verwandte Ereignisse und so weiter.  Ich werde hier keine Beispiele nennen, sondern diejenigen verweisen, die auf den Originalartikel im OpenAI-Blog: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bessere Sprachmodelle und ihre Auswirkungen</a> oder die Links am Ende des Artikels verweisen m√∂chten. </p><br><p>  Das Generieren eines zusammenh√§ngenden Textes dieser Qualit√§t ist an sich schon beeindruckend, aber das Interessanteste ist anders.  GPT-2 ohne zus√§tzliches Training zeigte sofort Ergebnisse auf dem neuesten Stand der Technik bei einer Reihe von Konversationsaufgaben.  Ich wiederhole, wer die Wichtigkeit des Augenblicks verpasst hat - ohne zus√§tzliches Training f√ºr eine bestimmte Aufgabe! </p><br><p>  Wie haben sie das erreicht?  Einfach den neuronalen Netzen die richtigen Fragen stellen. </p><br><h1 id="arhitektura-gpt-2">  GPT-2-Architektur </h1><br><p>  GPT-2 ist darauf trainiert, das n√§chste Wort in einem Satz vorherzusagen.  Dies ist ein klassischer Ansatz zum Generieren von Text.  Zun√§chst hatten Wiederholungsnetzwerke (RNN), insbesondere LSTM, in diesem Bereich Vorrang.  Nach der Erfindung der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Transformer-</a> Architektur im Sommer 2017 setzte sie sich jedoch allm√§hlich in Konversationsaufgaben durch.  Obwohl der urspr√ºngliche Transformer Probleme beim Speichern langer Sequenzen hat (LSTMs erinnern sich an l√§ngere), haben die Trainingsgeschwindigkeit und die Tiefe des Netzwerks dies mehr als ausgeglichen.  √úbrigens sind bereits einige Modifikationen des Transformators erschienen - mit der Einf√ºhrung der Wiederholung ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Universal Transformers</a> ), einer Modifikation f√ºr l√§ngere Sequenzen ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Transformer-XL</a> ) und anderen, aber bisher wird in Google und OpenAI nur ein leicht abgestimmter Originaltransformator verwendet. </p><br><p>  Ich erinnere mich, dass BERT von Google etwas anders gelernt hat: nicht das n√§chste Wort in einem Satz vorherzusagen, sondern fehlende (maskierte) W√∂rter in einem Satz.  Und auch um festzustellen, ob zwei aufeinanderfolgende S√§tze eine logische Fortsetzung voneinander sind oder ob sie in keiner Weise durch Bedeutung verbunden sind.  Dies erm√∂glichte es BERT, ein Sprachmodell zu sein, das die Bedeutung von W√∂rtern in Abh√§ngigkeit von ihrer Umgebung (Kontext) versteht.  Was den Erfolg von BERT bei NPL-Aufgaben bestimmte.  Aber erst nach einer Umschulung (Feinabstimmung) f√ºr eine bestimmte Aufgabe.  Nur W√∂rter im Basismodell vorherzusagen, funktioniert darin nicht sehr gut.  Sie k√∂nnen mit BERT in Ihrem Browser (√ºber Google Colab) spielen: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://habr.com/en/post/436878</a> . </p><br><p>  GPT-2 ist f√ºr die Umschulung nicht erforderlich.  Dies ist nicht nur ein Sprachmodell wie BERT, sondern ein Textgenerator.  Geben Sie ihr einfach den Anfang des Satzes, und dann wird sie ihr Wort f√ºr Wort erg√§nzen. </p><br><p>  Ein interessantes Detail: OpenAI-Untersuchungen haben gezeigt, dass Arrays von Wikipedia-Texten und literarischen B√ºchern (die insbesondere von BERT untersucht wurden) einen voreingenommenen Stil haben.  Daher erzeugen nur auf sie trainierte neuronale Netze keinen sehr guten Text.  Um die Eingabedaten und -stile zu diversifizieren, verwendete OpenAI GPT-2 f√ºr Schulungen auf regul√§ren Webseiten, die von 8 Millionen Websites (insgesamt 40 GB Text) gesammelt wurden.  Und um Werbe- und Spammer-Websites zu verwerfen, haben sie in die Beispiel-Websites aufgenommen, deren Links im reddit eine gute Bewertung haben.  Das hei√üt, Websites, bei denen Live-Benutzer n√ºtzliche Informationen gefunden haben. </p><br><h1 id="pravilnyy-vopros-soderzhit-polovinu-otveta">  Die richtige Frage enth√§lt die halbe Antwort. </h1><br><p>  So konnte GPT-2 dank seiner beispiellosen Gr√∂√üe Seiten mit zusammenh√§ngendem Text generieren.  Aber das Erstaunlichste ist, dass sie durch das Stellen der richtigen Frage (d. H. Des richtigen Anfangs einer Phrase) verschiedene Fragen beantworten konnte!  Nur weil die Fortsetzung eines solchen Anfangs am nat√ºrlichsten ist. </p><br><p>  Um beispielsweise eine Antwort auf die Frage ‚ÄûWas ist die Erde?‚Äú Zu erhalten, k√∂nnen Sie auf die Eingabe dieses neuronalen Netzwerks den Anfang des Satzes anwenden: ‚ÄûErde ist ...‚Äú.  Und sie wird diesen Satz bis zum Ende vervollst√§ndigen.  Weil die Antwort eine nat√ºrliche Fortsetzung dieses Anfangs sein wird. </p><br><p>  Wenn Sie den Satzanfang richtig bilden, k√∂nnen Sie au√üerdem Erkl√§rungen f√ºr verschiedene Zielgruppen erhalten, die deren Intelligenz, Alter und Bildung ber√ºcksichtigen.  Stellen Sie sich fortgesetzte S√§tze vor: "Ich als Wissenschaftler glaube, dass die Erde ... ist."  Oder: "Ich als Landpfl√ºger behaupte, die Erde sei ...".  Oder: "Ich als Lehrerin in einem Kindergarten werde Ihnen jetzt erkl√§ren, Kinder, dass die Erde ...". </p><br><p>  Wie Sie sehen k√∂nnen, k√∂nnen Sie durch Bilden der richtigen Fragen (am richtigen Anfang des Satzes) Antworten auf v√∂llig unterschiedlichen Ebenen und mit unterschiedlichen Details erhalten.  In gewisser Weise passiert etwas √Ñhnliches bei Menschen.  Der Arzt muss dem Patienten den Krankheitsverlauf erkl√§ren, damit er versteht.  Auf Patientenebene.  Wenn Sie ein f√ºnfj√§hriges Kind fragen, warum es das getan hat, kann es nicht sofort antworten (was nat√ºrlich bedeutet, dass Kinder mit Gef√ºhlen und Emotionen leben).  Aber um die Antwort zu geben, die von ihm erwartet wird, beginnt das Kind, sie zu erfinden - um Text zu generieren.  Basierend auf der Tatsache, dass die Antwort zum Elternteil passt und zumindest irgendwie dem entspricht, was passiert ist.  Wie viele Eltern wissen, werden dies zun√§chst l√§cherliche Antworten sein.  Aber durch Ermutigung und Bestrafung ("Erz√§hl mir mehr", "finde keine Ausreden") lernt das Kind, detaillierte und vollst√§ndige Antworten zu geben. </p><br><p>  Diese Entwicklung von OpenAI und die F√§higkeit des GPT-2-Netzwerks, Antworten auf Konversationsaufgaben ohne spezielle zus√§tzliche Schulung f√ºr eine bestimmte Aufgabe bereitzustellen, werfen zwei interessante Fragen auf: </p><br><p>  1) Kann die Interpretierbarkeit neuronaler Netze durch einen solchen elementaren Textgenerator und den korrekten Beginn einer Phrase erreicht werden?  Wo die Antwort eine nat√ºrliche Erweiterung sein wird.  Angenommen, ein neuronales Netzwerk zeigt beispielsweise Siegel in einem Foto nicht durch die x-y-Koordinatennummern an, sondern erkl√§rt seine Position im Klartext.  Wenn Sie ihr dann im Verlauf der Kl√§rung die richtige Frage stellen, zum Beispiel: "Ich bin zu diesem Schluss gekommen, weil ...", k√∂nnen Sie theoretisch erkl√§ren, wie sie die Katze auf dem Foto gefunden hat.  Und diese Erkl√§rung kann im Extremfall nicht schlechter sein als menschlich.  Dies l√∂st das globale Problem der Interpretierbarkeit neuronaler Netze. </p><br><p>  2) Kann ein vorab trainiertes neuronales Netzwerk f√ºr gro√üe Textmengen universell sein, einen gesunden Menschenverstand haben und keine zus√§tzliche Schulung f√ºr bestimmte Aufgaben erfordern?  Dies bedeutet, dass das neuronale Netzwerk beim Versuch, die menschliche Sprache nachzuahmen (menschliche Antworten auf Fragen), zwangsl√§ufig den gesunden Menschenverstand lernen muss, um diese den menschlichen Antworten sehr √§hnlich zu geben.  Einsilbige fiktive Antworten zu geben, ist im Allgemeinen nicht typisch f√ºr Menschen.  Zum gr√∂√üten Teil geben die Menschen detaillierte und angemessene Antworten, was bedeutet, dass das Netzwerk lernen muss, dasselbe zu tun. </p><br><p>  Beide Fragen bleiben offen, aber der erste Schritt in ihrer Zustimmung ist definitiv getan. </p><br><h1 id="a-tochnee">  Und genauer? </h1><br><p>  Wenn Sie jetzt stehen, ist es besser, sich zu setzen.  Denn so hat OpenAI, das das neuronale GPT-2-Netzwerk verwendet, seine Ergebnisse bei Konversationsaufgaben f√ºr verschiedene Dom√§nen erzielt: </p><br><p>  <strong>Antworten auf Fragen zum Text</strong> </p><br><p>  Das ist einfach.  Oder f√ºtterte das Netzwerk mit ein paar Abs√§tzen mit einer Beschreibung, die irgendwo in der Mitte stand, zum Beispiel "Der Apfel liegt auf dem Tisch", und am Ende wurde zugeschrieben: "Der Apfel ist auf ..." und das Netzwerk wurde der "Tabelle" hinzugef√ºgt.  Weil es sich an den Kontext mehrerer Abs√§tze erinnern kann. </p><br><p>  Oder f√ºtterte das Netzwerk als ersten Satz mit einigen Beispielen vom Typ ‚ÄûFrage: eine Frage, Antwort: eine Antwort‚Äú und am Ende nach der eigentlichen Frage f√ºgten sie hinzu: ‚ÄûAntwort:‚Äú.  Und das neuronale Netzwerk hat die Antwort angeh√§ngt!  Da es die Struktur des Dokuments auf der vorherigen Frage-Antwort enth√ºllte.  Das ist erstaunlich. </p><br><p>  <strong>Kurzfassung (Zusammenfassung) des Textes</strong> </p><br><p>  Die Eingabe ist ein langer Text aus mehreren Abs√§tzen oder sogar Seiten, und das neuronale Netzwerk sollte einen kurzen Inhalt schreiben.  Wie haben Sie dieses Verhalten von GPT-2 erhalten?  Kurz nach dem Text f√ºgten sie "TL; DR" hinzu.  Und alle!  Dies erwies sich als ausreichend f√ºr das GPT-2, um nach diesen Zeichen eine Zusammenfassung des Artikels hinzuzuf√ºgen!  Weil solche Symbole im Internet h√§ufig die Zusammenfassung des Beitrags bezeichnen. </p><br><p>  <strong>Text√ºbersetzung</strong> </p><br><p>  Die GPT-2-Eingabe erhielt den Text in der Form: "Hallo = Hallo, Hund = Hund, Wind = Wind, Katze = ...".  Und das neuronale Netz f√ºgte die √úbersetzung des letzten Wortes hinzu: "Katze" (im Original auf Franz√∂sisch).  Weil es die Struktur des Dokuments enth√ºllte und es einfach mit der logischsten Fortsetzung erg√§nzte.  Wenn dein Kiefer immer noch nicht von all dem abgefallen ist, dann habe ich zwei Neuigkeiten f√ºr dich und beide sind schlecht =). </p><br><h1 id="zapusk-gpt-2-cherez-google-colab">  GPT-2-Start √ºber Google Colab </h1><br><p>  Leider wurde die Freigabe der Vollversion von GPT-2 in OpenAI verweigert.  Dies wird durch die Tatsache motiviert, dass es mit diesem neuronalen Netzwerk zu einfach ist, gef√§lschte Nachrichten und Bewertungen in Gesch√§ften zu generieren.  Nach ihrer Aussage wird die Diskussion √ºber die Angemessenheit der Auslegung dieses Modells f√ºr die n√§chsten 6 Monate fortgesetzt. Nach der OpenAI werden sie entscheiden, ob sie es hochladen oder nicht.  F√ºr eine gro√üe Organisation ist es jedoch nicht schwierig, das Modell zu wiederholen (es scheint, dass sie es mehrere Tage lang f√ºr 256 TPU trainiert haben und nach vorl√§ufigen Sch√§tzungen etwa 45.000 US-Dollar gekostet haben). </p><br><p>  Sie ver√∂ffentlichten jedoch eine reduzierte Version von GPT-2 mit 117 Millionen Parametern (anstatt 1,5 Milliarden, wie im Vollmodell): <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://github.com/openai/gpt-2</a> .  Lassen Sie uns versuchen, es auszuf√ºhren und mit diesem Modell zu spielen. </p><br><p>  Update 9. November 2019: Endlich wurde die gesamte Modellreihe einschlie√ülich 1,5 Milliarden angelegt. Die Dateien und Anweisungen f√ºr den Start wurden aktualisiert. </p><br><p>  Der einfachste Weg, dies zu tun, ist √ºber Google Colab: </p><br><ol><li>  √ñffnen Sie den Link </li></ol><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">http://colab.research.google.com/github/blade1780/bert/blob/master/Gpt-2.ipynb</a> </p><br><ol><li>  <strong>W√§hlen Sie im</strong> Men√º <strong>Laufzeit</strong> die <strong>Option</strong> <strong>Alle ausf√ºhren aus</strong> , damit zum ersten Mal alle Zellen gestartet werden, die Modelldownloads und die erforderlichen Bibliotheken verbunden werden.  Stimmen Sie zu, bei Bedarf alle Runtime zur√ºckzusetzen.  Geben Sie nach dem Erscheinen von "Model prompt &gt;&gt;&gt;" Text ein und dr√ºcken Sie die Eingabetaste. </li></ol><br><p>  Achten Sie ganz am Anfang auf die Zeile: </p><br><p>  model_name = '117M' </p><br><p>  Hier k√∂nnen Sie die Gr√∂√üe des zu verwendenden GPT-2-Modells angeben.  Folgende Modelle sind verf√ºgbar (vorbehaltlich Aktualisierungen): </p><br><p>  117M <br>  124M <br>  355M <br>  774M <br>  1558M </p><br><p>  Hier ist 117M das kleinste Modell, das zum Zeitpunkt dieses Schreibens das einzige verf√ºgbare war.  OpenAI legte sp√§ter immer mehr Modelle vor, bis zum 5. November 2019 das Maximum von 1558 Millionen (mit 1,5 Milliarden Parametern). </p><br><div class="spoiler">  <b class="spoiler_title">Wenn etwas schief gelaufen ist ...</b> <div class="spoiler_text"><p>  Stellen Sie sicher, dass GPU und Python 3 im Men√º Laufzeit -&gt; Laufzeittyp √§ndern ausgew√§hlt sind </p><br><p>  Wenn die Schaltfl√§che Verbinden nicht aktiv ist, klicken Sie darauf, um eine Verbindung herzustellen. </p></div></div><br><p>  Oder erstellen Sie den gesamten Code manuell: </p><br><ol><li>  Gehen Sie zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://colab.research.google.com</a> </li><li>  Dr√ºcken Sie die blaue Taste NEW PYTHON 3 NOTEBOOK </li><li>  W√§hlen Sie im Men√º Laufzeit -&gt; Laufzeittyp √§ndern die Option Python 3 und die GPU aus (letztere, um das neuronale Netzwerk auf der GPU auszuf√ºhren). </li><li>  Geben Sie in die erste Zelle Folgendes ein: </li></ol><br><pre><code class="python hljs">model_name = <span class="hljs-string"><span class="hljs-string">'117M'</span></span> !git clone https://github.com/openai/gpt<span class="hljs-number"><span class="hljs-number">-2</span></span> %cd gpt<span class="hljs-number"><span class="hljs-number">-2</span></span> !pip3 install -r requirements.txt !python3 download_model.py $model_name</code> </pre> <br><p>  Anstelle von 117M (dem kleinsten) k√∂nnen Sie ein beliebiges Zwischen- oder gr√∂√ütes Modell angeben: 1558M. </p><br><p>  Klicken Sie auf das schwarze Wiedergabesymbol links neben der Zelle.  Dadurch wird das ausgew√§hlte neuronale GPT-2-Netzwerk heruntergeladen und die erforderlichen Abh√§ngigkeiten installiert. </p><br><p>  In der zweiten Zelle (Sie k√∂nnen sie √ºber das Men√º Einf√ºgen -&gt; Codezelle oder durch Bewegen der Maus unter der Mitte der aktuellen Zelle hinzuf√ºgen, werden die Schaltfl√§chen zum Hinzuf√ºgen angezeigt): </p><br><pre> <code class="python hljs">!python3 src/interactive_conditional_samples.py --model_name=$model_name</code> </pre> <br><p>  Dadurch wird der interaktive Modus gestartet.  Warten Sie, bis das neuronale Netzwerk hochgefahren ist und ein Fenster zur Texteingabe mit der Aufschrift ‚ÄûModellmodell &gt;&gt;&gt;‚Äú angezeigt wird. Geben Sie den Anfang der Phrase ein und dr√ºcken Sie die Eingabetaste. Nach einer Weile wird der generierte Text unter der √úberschrift BEISPIEL angezeigt. </p><br><p>  Sie k√∂nnen auch den Modus zum Generieren von vollst√§ndig zuf√§lligem Text starten.  Der Text wird endlos in kleinen Teilen von BEISPIEL 1, BEISPIEL 2 usw. generiert, bis Sie auf die Schaltfl√§che Stopp in der Zelle klicken.  Erstellen Sie dazu eine neue Zelle mit dem Code: </p><br><pre> <code class="python hljs">!python3 src/generate_unconditional_samples.py --model_name=$model_name | tee samples.txt</code> </pre> <br><p>  Das Ergebnis wird in der Datei samples.txt gespeichert.  Es kann mit den folgenden Befehlen heruntergeladen werden (erstellen Sie erneut eine neue Zelle und f√ºhren Sie sie nach dem Generieren des Textes aus): </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> google.colab <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> files files.download(<span class="hljs-string"><span class="hljs-string">'samples.txt'</span></span>)</code> </pre> <br><p>  Sie k√∂nnen die Parameter zum Generieren von Text √§ndern (Zufallskoeffizient usw., siehe Beschreibung in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Originalarbeit</a> ): </p><br><pre> <code class="python hljs">!python3 src/generate_unconditional_samples.py --model_name=$model_name --top_k <span class="hljs-number"><span class="hljs-number">40</span></span> --temperature <span class="hljs-number"><span class="hljs-number">0.7</span></span> | tee samples.txt</code> </pre> <br><p>  Da es sich bei dem 117M um ein <strong>stark</strong> reduziertes Modell handelt, sollten Sie keine Wunder erwarten (Update: Zum Zeitpunkt dieses Schreibens war nur dieses verf√ºgbar. Jetzt ist alles verf√ºgbar, einschlie√ülich des urspr√ºnglich gr√∂√üten 1558M, siehe oben).  Die meisten der generierten Samples sind Unsinn.  Es gibt aber auch sinnvolle Abschnitte.  Der Text sollte in Englisch sein, w√§hrend GPT-2 in anderen Sprachen noch nicht funktionieren kann. </p><br><h2 id="primery-generiruemogo-teksta">  Beispiele f√ºr generierten Text </h2><br><p>  Beispiele f√ºr den vom <strong>vollst√§ndigen</strong> Modell generierten Text: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://blog.openai.com/better-language-models/#sample1</a> (oben in der Leiste f√ºr 8 Geschichten). </p><br><p>  Es gibt auch eine riesige 2,4-MB-Textdatei mit zuf√§llig generierten Beispielen: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://raw.githubusercontent.com/openai/gpt-2/master/gpt2-samples.txt</a> </p><br><p>  Und noch eine, 2,27 MB, mit anderen Zufallseinstellungen: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://raw.githubusercontent.com/openai/gpt-2/master/gpt2-topk40-samples.txt</a> </p><br><h1 id="ssylki">  Referenzen </h1><br><ul><li>  Original OpenAI Blog Artikel: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bessere Sprachmodelle und ihre Auswirkungen</a> </li><li>  Github mit allen vorgefertigten Versionen von GPT-2: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://github.com/openai/gpt-2</a> </li><li>  Diskussion √ºber reddit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hauptnachrichten</a> </li><li>  Diskussion √ºber reddit, das sich weigert, das vollst√§ndige Modell zu ver√∂ffentlichen: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Es ist Zeit f√ºr OpenAI, CloseAI umzubenennen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Google Colab-Notebook zum Ausf√ºhren von GPT-2 (alle Modelle) in einem Browser</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de440564/">https://habr.com/ru/post/de440564/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de440554/index.html">Bequeme BEM</a></li>
<li><a href="../de440556/index.html">Lernen des Entwurfs von Entit√§tsbeziehungsdiagrammen</a></li>
<li><a href="../de440558/index.html">Technologie, die Quantennetzwerke n√§her bringt</a></li>
<li><a href="../de440560/index.html">Alexander Belokrylov und Dmitry Chuyko √ºber Liberica JDK auf jug.msk.ru</a></li>
<li><a href="../de440562/index.html">Windows Phone - ALLES, es ist immer wieder</a></li>
<li><a href="../de440566/index.html">Ungehindert beschleunigen oder SIMD kennenlernen</a></li>
<li><a href="../de440568/index.html">Wir schreiben eine Lernanwendung in Go und Javascript, um die tats√§chlichen Aktienrenditen zu bewerten. Teil 2 - Testen des Backends</a></li>
<li><a href="../de440570/index.html">Reflektierende Schattenkarten: Teil 2 - Implementierung</a></li>
<li><a href="../de440574/index.html">Russischer AI Cup 2018, Geschichte 9 Pl√§tze</a></li>
<li><a href="../de440576/index.html">Wichtige √Ñnderungen an CTE in PostgreSQL 12</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>