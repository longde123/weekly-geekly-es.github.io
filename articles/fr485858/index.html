<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üçØ üö∏ üï¢ Comment apprendre √† un t√©l√©phone √† voir la beaut√© üó®Ô∏è üîî üíß</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="R√©cemment, j'ai lu un livre sur les math√©matiques et la beaut√© des gens et j'ai r√©fl√©chi √† ce qu'il y a dix ans, √† l'id√©e de comprendre quelle beaut√© ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment apprendre √† un t√©l√©phone √† voir la beaut√©</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/485858/"><img src="https://habrastorage.org/webt/-f/0z/on/-f0zonxrb_qtnmaxp2bt34gu-d4.png" alt="image"><br><br>  R√©cemment, j'ai lu un livre sur les math√©matiques et la beaut√© des gens et j'ai r√©fl√©chi √† ce qu'il y a dix ans, √† l'id√©e de comprendre quelle beaut√© humaine √©tait assez primitive.  Le raisonnement sur le visage consid√©r√© comme beau du point de vue des math√©matiques se r√©sume au fait qu'il doit √™tre sym√©trique.  De plus, depuis la Renaissance, il y a eu des tentatives pour d√©crire de beaux visages en utilisant les relations entre les distances √† certains points du visage et montrer, par exemple, que les beaux visages ont une sorte de relation proche du nombre d'or.  Des id√©es similaires sur l'emplacement des points sont maintenant utilis√©es comme l'une des m√©thodes d'identification des visages (recherche de rep√®res de visage).  Cependant, l'exp√©rience montre que si vous ne limitez pas l'ensemble des caract√©ristiques √† la position de points sp√©cifiques sur le visage, vous pouvez obtenir de meilleurs r√©sultats dans un certain nombre de t√¢ches, <a href="https://arxiv.org/abs/1603.01249" rel="nofollow">notamment la d√©termination de l'√¢ge, du sexe</a> ou m√™me de <a href="https://www.gsb.stanford.edu/sites/gsb/files/publication-pdf/wang_kosinski.pdf" rel="nofollow">l'orientation sexuelle</a> .  Il est d√©j√† √©vident ici que la question de l'√©thique de la publication des r√©sultats de ces √©tudes peut √™tre aigu√´. <br><a name="habracut"></a><br>  Le sujet de la beaut√© des gens et son √©valuation peuvent √©galement √™tre controvers√©s sur le plan √©thique.  Lors du d√©veloppement de l'application, beaucoup de mes amis ont refus√© d'utiliser leurs photos pour des tests, ou ne voulaient tout simplement pas conna√Ætre le r√©sultat (c'est dr√¥le que la plupart des filles aient refus√© de conna√Ætre les r√©sultats).  En outre, l'objectif d'automatiser l'√©valuation de la beaut√© peut soulever des questions philosophiques int√©ressantes.  Dans quelle mesure le concept de beaut√© est-il d√©termin√© par la culture?  Quelle est la v√©rit√© ¬´La beaut√© dans l'≈ìil du spectateur¬ª?  Est-il possible de mettre en √©vidence des signes objectifs de beaut√©? <br><br>  Pour r√©pondre √† ces questions, vous devez √©tudier les statistiques sur les √©valuations de certaines personnes par d'autres.  J'ai essay√© de concevoir et de former un mod√®le de r√©seau neuronal qui √©valuerait la beaut√©, ainsi que de l'ex√©cuter sur un t√©l√©phone Android. <br><br><h2>  Partie 0. Pipeline </h2><br>  Afin de comprendre comment les prochaines √©tapes sont li√©es les unes aux autres, j'ai dessin√© un sch√©ma du projet: <br><br><img src="https://habrastorage.org/webt/cy/jp/zh/cyjpzhhy_hiczxqefjp9qgaohf0.png" alt="image"><br><br>  Bleu - biblioth√®ques importantes et donn√©es externes.  Jaune - contr√¥le dans l'application. <br><br><h2>  Partie 1. Python </h2><br>  √âtant donn√© que l'√©valuation de la beaut√© est un sujet assez d√©licat, il n'y a pas beaucoup de jeux de donn√©es dans le domaine public contenant des photos avec une √©valuation (je suis s√ªr que les services de rencontres en ligne comme l'amadou ont des ensembles de statistiques beaucoup plus importants).  J'ai trouv√© <a href="https://github.com/HCIILAB/SCUT-FBP5500-Database-Release" rel="nofollow">une base de donn√©es</a> compil√©e dans l'une des universit√©s en Chine, contenant 5500 photographies, chacune √©valu√©e par 7 √©valuateurs parmi des √©tudiants chinois.  Sur les 5 500 photographies, 2 000 sont des hommes asiatiques (MA), 2 000 sont des femmes asiatiques (FA), et 750 hommes Europio√Ødes (CM) et femmes (CF) chacun. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fcf/1db/f9b/fcf1dbf9b67fee5543fdc9833d429676.jpg" alt="image"><br><br>  Lisons les donn√©es en utilisant le module Python pandas et jetons un coup d'≈ìil rapide aux donn√©es.  Distribution estim√©e pour diff√©rents genres et races: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt ratingDS=pd.read_excel(<span class="hljs-string"><span class="hljs-string">'../input/faces-scut/scut-fbp5500_v2/SCUT-FBP5500_v2/All_Ratings.xlsx'</span></span>) Answer=ratingDS.groupby(<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>).mean()[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>] ratingDS[<span class="hljs-string"><span class="hljs-string">'race'</span></span>]=ratingDS[<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>].apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x:x[:<span class="hljs-number"><span class="hljs-number">2</span></span>]) fig, ax = plt.subplots(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, sharex=<span class="hljs-string"><span class="hljs-string">'col'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, race <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate([<span class="hljs-string"><span class="hljs-string">'CF'</span></span>,<span class="hljs-string"><span class="hljs-string">'CM'</span></span>,<span class="hljs-string"><span class="hljs-string">'AF'</span></span>,<span class="hljs-string"><span class="hljs-string">'AM'</span></span>]): sbp=ax[i%<span class="hljs-number"><span class="hljs-number">2</span></span>,i//<span class="hljs-number"><span class="hljs-number">2</span></span>] ratingDS[ratingDS[<span class="hljs-string"><span class="hljs-string">'race'</span></span>]==race].groupby(<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>)[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>].mean().hist(alpha=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, bins=<span class="hljs-number"><span class="hljs-number">20</span></span>,label=race,grid=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>,rwidth=<span class="hljs-number"><span class="hljs-number">0.9</span></span>,ax=sbp) sbp.set_title(race)</code> </pre> <br><img src="https://habrastorage.org/webt/fz/1q/fo/fz1qfoby_-ijefbbl4ifctz3llo.png" alt="image"><br><br>  On peut voir qu'en g√©n√©ral, les hommes sont consid√©r√©s comme moins beaux que les femmes, la distribution est bimodale - il y en a.  qui sont consid√©r√©s comme beaux et "moyens".  Il n'y a presque pas de notes basses, les donn√©es pourraient donc √™tre renormalis√©es.  Mais laissons-les pour l'instant. <br><br>  Regardons l'√©cart type dans les estimations: <br><br><pre> <code class="python hljs">ratingDS.groupby(<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>)[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>].std().mean()</code> </pre><br>  Il est de 0,64, ce qui signifie que la diff√©rence dans les √©valuations des diff√©rents √©valuateurs est inf√©rieure √† 1 point sur 5, ce qui indique l'unanimit√© dans les √©valuations de la beaut√©.  On peut raisonnablement dire que ¬´la beaut√© n'est PAS dans l'≈ìil du spectateur¬ª.  Lors de la moyenne, vous pouvez utiliser les donn√©es de mani√®re fiable pour former le mod√®le et ne pas vous soucier de l'impossibilit√© fondamentale de l'√©valuation programmatique. <br><br>  Cependant, malgr√© la faible valeur de l'√©cart type de l'estimation, l'opinion de certains √©valuateurs peut √™tre tr√®s diff√©rente de celle ¬´ordinaire¬ª.  Construisons la distribution de la diff√©rence entre l'estimation et la m√©diane: <br><br><pre> <code class="python hljs">R2=ratingDS.join(ratingDS.groupby(<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>)[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>].median(), on=<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>, how=<span class="hljs-string"><span class="hljs-string">'inner'</span></span>,rsuffix =<span class="hljs-string"><span class="hljs-string">' median'</span></span>) R2[<span class="hljs-string"><span class="hljs-string">'ratingdiff'</span></span>]=(R2[<span class="hljs-string"><span class="hljs-string">'Rating median'</span></span>]-R2[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>]).astype(int) print(set(R2[<span class="hljs-string"><span class="hljs-string">'ratingdiff'</span></span>])) R2[<span class="hljs-string"><span class="hljs-string">'ratingdiff'</span></span>].hist(label=<span class="hljs-string"><span class="hljs-string">'difference of raings'</span></span>,bins=[<span class="hljs-number"><span class="hljs-number">-3.5</span></span>,<span class="hljs-number"><span class="hljs-number">-2.5</span></span>,<span class="hljs-number"><span class="hljs-number">-1.5</span></span>,<span class="hljs-number"><span class="hljs-number">-0.5</span></span>,<span class="hljs-number"><span class="hljs-number">0.5</span></span>,<span class="hljs-number"><span class="hljs-number">1.5</span></span>,<span class="hljs-number"><span class="hljs-number">2.5</span></span>,<span class="hljs-number"><span class="hljs-number">3.5</span></span>,<span class="hljs-number"><span class="hljs-number">4.5</span></span>],grid=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>,rwidth=<span class="hljs-number"><span class="hljs-number">0.5</span></span>)</code> </pre><br><img src="https://habrastorage.org/webt/ww/qb/7w/wwqb7wdyk_neg_semros1qthb_g.png" alt="image"><br><br>  Un motif int√©ressant est trouv√©.  Personnes dont le score diff√®re de la m√©diane de plus d'un point <br><br><pre> <code class="python hljs">len(R2[R2[<span class="hljs-string"><span class="hljs-string">'ratingdiff'</span></span>].abs()&gt;<span class="hljs-number"><span class="hljs-number">1</span></span>])/len(R2)</code> </pre><br>  0,02943333333333333332 <br>  Moins de 3%.  Autrement dit, l'unanimit√© frappante est √† nouveau confirm√©e en mati√®re d'√©valuation de la beaut√©. <br>  Cr√©er un tableau avec les notes moyennes n√©cessaires <br><br><pre> <code class="python hljs">Answer=ratingDS.groupby(<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>).mean()[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>]</code> </pre><br>  Notre base de donn√©es est petite;  De plus, toutes les photos contiennent principalement des images faciales, et j'aimerais un r√©sultat fiable pour n'importe quelle position du visage.  Pour r√©soudre les probl√®mes avec une petite quantit√© de donn√©es, la technique d'apprentissage par transfert est souvent utilis√©e - l'utilisation de mod√®les pr√©-form√©s pour des t√¢ches similaires et leur modification.  Pr√®s de ma t√¢che est la t√¢che de reconnaissance faciale.  Il est g√©n√©ralement r√©solu en trois √©tapes. <br><br>  1. Il y a une d√©tection de visage sur l'image et sa mise √† l'√©chelle. <br><br>  2. √Ä l'aide d'un r√©seau neuronal convolutif, l'image du visage est convertie en un vecteur caract√©ristique, et les propri√©t√©s d'une telle transformation sont telles que la transformation est invariante par rapport √† la rotation du visage, au changement de coiffure.  manifestations d'√©motions et toutes images temporaires.  La formation d'un tel r√©seau est en soi une t√¢che int√©ressante qui peut √™tre √©crite pendant longtemps.  De plus, de nouveaux d√©veloppements apparaissent constamment pour am√©liorer cette conversion afin d'am√©liorer les algorithmes de suivi et d'identification de masse.  Ils optimisent √† la fois l'architecture du r√©seau et la m√©thode d'apprentissage (exemple perte triplet-perte de face-arcface). <br><br>  3. Comparaison du vecteur d'entit√© avec ceux stock√©s dans la base de donn√©es. <br><br>  Pour notre t√¢che, j'ai utilis√© des solutions toutes faites de 1 √† 2 points.  La t√¢che de d√©tection des visages est g√©n√©ralement r√©solue de nombreuses mani√®res.En outre, presque tous les appareils mobiles disposent de d√©tecteurs de visages (sur Android, ils font partie du package de services GooglePlay standard), qui sont utilis√©s pour se concentrer sur les visages lors de la photographie.  Quant √† la traduction des personnes sous forme vectorielle, il y a un point subtil non √©vident.  Le fait est que les signes.  extraites pour r√©soudre le probl√®me de reconnaissance - sont caract√©ristiques d'une personne, mais elles peuvent ne pas du tout correspondre √† la beaut√©.  de plus.  en raison des particularit√©s des r√©seaux de neurones convolutifs, ces signes sont principalement locaux, et en g√©n√©ral cela peut causer de nombreux probl√®mes (attaque √† pixel unique).  N√©anmoins, j'ai trouv√© que les r√©sultats d√©pendent fortement de la dimension du vecteur, et si 128 signes ne suffisent pas pour d√©terminer la beaut√©, 512 suffisent.  Sur cette base, un <a href="https://github.com/shaoanlu/face_toolbox_keras" rel="nofollow">r√©seau insightFace pr√©-form√© bas√© sur Reset a</a> √©t√© choisi.  Nous utiliserons √©galement les keras comme cadre d'apprentissage automatique. <br>  Un code d√©taill√© pour t√©l√©charger des mod√®les pr√©-form√©s peut √™tre trouv√© <a href="https://www.kaggle.com/alexanderkhar/face-beauty-ranking-ported-to-android" rel="nofollow">ici.</a> <br><br><pre> <code class="python hljs">model=LResNet100E_IR()</code> </pre><br>  Le d√©tecteur de <a href="https://github.com/ipazc/mtcnn" rel="nofollow">visage mtcnn a</a> √©t√© utilis√© comme d√©tecteur de visage pour le pr√©traitement <a href="https://github.com/ipazc/mtcnn" rel="nofollow">.</a> <br><br><pre> <code class="python hljs">detector = MtcnnDetector(model_folder=mtcnn_path, ctx=ctx, num_worker=<span class="hljs-number"><span class="hljs-number">1</span></span>, accurate_landmark = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, threshold=det_threshold)</code> </pre><br>  Alignez, recadrez et vectorisez les images de l'ensemble de donn√©es: <br><br><pre> <code class="python hljs">imgpath=<span class="hljs-string"><span class="hljs-string">'../input/faces-scut/scut-fbp5500_v2/SCUT-FBP5500_v2/Images/'</span></span> <span class="hljs-comment"><span class="hljs-comment">#    facevecs=[] for name in tqdm.tqdm(Answer.index): #   img1 = cv2.imread(imgpath+name) # ,     pre1 = np.moveaxis(get_input(detector,img1),0,-1) #  vec = model.predict(np.stack([pre1])) #   facevecs.append(vec)</span></span></code> </pre><br>  Nous pr√©parerons les donn√©es en les d√©composant en vecteurs de formation (90% d‚Äôentre eux, nous les √©tudierons) et de validation (nous v√©rifierons le travail du mod√®le).  Nous normalisons les donn√©es dans une plage de 0 √† 1. <br><br><pre> <code class="python hljs">X=np.stack(facevecs)[:,<span class="hljs-number"><span class="hljs-number">0</span></span>,:] Y=(Answer[:])/<span class="hljs-number"><span class="hljs-number">5</span></span> Indicies=np.arange(len(Answer)) X,Y,Indicies=sklearn.utils.shuffle(X,Y,Indicies) Xtrain=X[:int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>)] Ytrain=Y[:int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>)] Indtrain=Indicies[:int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>)] Xval=X[int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>):] Yval=Y[int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>):] Indval=Indicies[int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>):]</code> </pre><br>  Passons maintenant au mod√®le.  d√©crivant la beaut√©. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Createheadmodel</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> inp=keras.layers.Input((<span class="hljs-number"><span class="hljs-number">512</span></span>,)) x=keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">32</span></span>,activation=<span class="hljs-string"><span class="hljs-string">'elu'</span></span>)(inp) x=keras.layers.Dropout(<span class="hljs-number"><span class="hljs-number">0.1</span></span>)(x) out=keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>,activation=<span class="hljs-string"><span class="hljs-string">'hard_sigmoid'</span></span>,use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>,kernel_initializer=keras.initializers.Ones())(x) model=keras.models.Model(input=inp,output=out) model.layers[<span class="hljs-number"><span class="hljs-number">-1</span></span>].trainable=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span> model.compile(optimizer=keras.optimizers.Adam(lr=<span class="hljs-number"><span class="hljs-number">0.0001</span></span>), loss=<span class="hljs-string"><span class="hljs-string">'mse'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model modelhead=Createheadmodel()</code> </pre><br>  Ce mod√®le est un r√©seau de neurones monocouche enti√®rement connect√© avec 32 neurones et 512 n≈ìuds d'entr√©e - l'une des architectures les plus simples, qui est n√©anmoins bien form√©e: <br><br><pre> <code class="python hljs">hist=modelhead.fit(Xtrain,Ytrain, epochs=<span class="hljs-number"><span class="hljs-number">4000</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">5000</span></span>, validation_data=(Xval,Yval) )</code> </pre><br>  4950/4950 [===============================] - 0s 3us / step - perte: 0,0069 - val_loss: 0,0071 <br>  Construisons des courbes d'apprentissage <br><br><pre> <code class="python hljs">plt.plot(hist.history[<span class="hljs-string"><span class="hljs-string">'loss'</span></span>][<span class="hljs-number"><span class="hljs-number">100</span></span>:], label=<span class="hljs-string"><span class="hljs-string">'loss'</span></span>) plt.plot(hist.history[<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>][<span class="hljs-number"><span class="hljs-number">100</span></span>:],label=<span class="hljs-string"><span class="hljs-string">'validation_loss'</span></span>) plt.legend(bbox_to_anchor=(<span class="hljs-number"><span class="hljs-number">0.95</span></span>, <span class="hljs-number"><span class="hljs-number">0.95</span></span>), loc=<span class="hljs-string"><span class="hljs-string">'upper right'</span></span>, borderaxespad=<span class="hljs-number"><span class="hljs-number">0.</span></span>)</code> </pre><br>  On voit que la perte (√©cart carr√© moyen) est de 0,0071 sur les donn√©es de validation, donc l'√©cart type = 0,084 ou 0,42 points sur une √©chelle √† cinq points, ce qui est inf√©rieur √† la dispersion des estimations donn√©es par les personnes (0,6 point).  Notre mod√®le fonctionne. <br><br>  Pour visualiser le fonctionnement du mod√®le, vous pouvez utiliser le diagramme de dispersion - pour chaque photo √† partir des donn√©es de validation, nous construisons un point o√π l'une des coordonn√©es correspond √† la cote moyenne du visage et la seconde √† la cote moyenne pr√©dite: <br><br><pre> <code class="python hljs">Answer2=Answer.to_frame()[:<span class="hljs-number"><span class="hljs-number">5500</span></span>] Answer2[<span class="hljs-string"><span class="hljs-string">'ans'</span></span>]=<span class="hljs-number"><span class="hljs-number">0</span></span> Answer2[<span class="hljs-string"><span class="hljs-string">'race'</span></span>]=Answer2.index Answer2[<span class="hljs-string"><span class="hljs-string">'race'</span></span>]=Answer2[<span class="hljs-string"><span class="hljs-string">'race'</span></span>].apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: x[:<span class="hljs-number"><span class="hljs-number">2</span></span>]) Answer2[<span class="hljs-string"><span class="hljs-string">'ans'</span></span>]=modelhead.predict(np.stack(facevecs)[:,<span class="hljs-number"><span class="hljs-number">0</span></span>,:])*<span class="hljs-number"><span class="hljs-number">5</span></span> xy=np.array(Answer2.iloc[Indval][[<span class="hljs-string"><span class="hljs-string">'ans'</span></span>,<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>]]) plt.scatter(xy[:,<span class="hljs-number"><span class="hljs-number">1</span></span>],xy[:,<span class="hljs-number"><span class="hljs-number">0</span></span>])</code> </pre><br><img src="https://habrastorage.org/webt/w2/t9/dd/w2t9ddnfyzjpx-xp7q3_wmsubzk.png" alt="image"><br><br>  Axe Y - valeurs pr√©dites par le mod√®le, axe X - valeurs moyennes des estimations des personnes.  Nous voyons une corr√©lation √©lev√©e (le diagramme est allong√© le long de la diagonale).  Vous pouvez √©galement consulter nos r√©sultats visuellement - prenez les visages de chacune des cat√©gories avec des notes pr√©vues de 1 √† 5 <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.image <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> mpimg f, axarr = plt.subplots(<span class="hljs-number"><span class="hljs-number">4</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>,figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, race <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate([<span class="hljs-string"><span class="hljs-string">'AF'</span></span>,<span class="hljs-string"><span class="hljs-string">'CF'</span></span>, <span class="hljs-string"><span class="hljs-string">"AM"</span></span>, <span class="hljs-string"><span class="hljs-string">'CM'</span></span>]): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> rating <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">6</span></span>): <span class="hljs-comment"><span class="hljs-comment">#axarr[i,rating-1].axis('off') axarr[i,rating-1].tick_params(# changes apply to the x-axis which='both', # both major and minor ticks are affected bottom=False, # ticks along the bottom edge are off top=False, # ticks along the top edge are off right=False, left=False, labelbottom=False, labelleft=False ) picname=(Answer2[Answer2['race']==race]['ans']-rating).abs().argmin() axarr[i,rating-1].set_xlabel(Answer2.loc[picname]['ans']) axarr[i,rating-1].imshow(mpimg.imread(imgpath+picname))</span></span></code> </pre><br><img src="https://habrastorage.org/webt/i4/az/pe/i4azpe-biju4pozojrgpiyxkoag.png" alt="image"><br><br>  On voit que le r√©sultat du tri par beaut√© semble raisonnable. <br><br>  Nous allons maintenant cr√©er un mod√®le complet dans lequel nous soumettons un visage √† l'entr√©e, √† la sortie, nous obtenons une note de 0 √† 1 et le convertissons au format tflite adapt√© au t√©l√©phone <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf finmodel=Model(input=model.input, output=modelhead(model.output)) finmodel.save(<span class="hljs-string"><span class="hljs-string">'finmodel.h5'</span></span>) converter = tf.lite.TFLiteConverter.from_keras_model_file(<span class="hljs-string"><span class="hljs-string">'finmodel.h5'</span></span>) converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE] tflite_quant_model = converter.convert() open (<span class="hljs-string"><span class="hljs-string">"modelquant.tflite"</span></span> , <span class="hljs-string"><span class="hljs-string">"wb"</span></span>).write(tflite_quant_model) <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> IPython.display <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> FileLink FileLink(<span class="hljs-string"><span class="hljs-string">r'modelquant.tflite'</span></span>)</code> </pre><br>  Ce mod√®le re√ßoit une image d'un visage d'une taille de 112 * 112 * 3 √† l'entr√©e et √† la sortie, il donne un num√©ro unique de 0 √† 1, ce qui signifie la beaut√© du visage (bien que nous devons nous rappeler que dans l'ensemble de donn√©es, les notes ne variaient pas de 0 √† 5, mais de 1 √† 5). <br><br><h2>  Partie 2. JAVA </h2><br>  Essayons d'√©crire une application simple pour un t√©l√©phone Android.  Le langage Java est nouveau pour moi, et je n'ai jamais √©t√© impliqu√© dans le d√©veloppement pour Android, donc le projet n'utilise pas d'optimisation du travail, n'utilise pas le contr√¥le de flux et d'autres choses qui demandent beaucoup de travail pour un d√©butant.  √âtant donn√© que le code java est plut√¥t lourd, je ne donnerai ici que les √©l√©ments les plus importants pour que le programme fonctionne.  Le code d'application complet est disponible <a href="https://github.com/Alexankharin/HowCuteAmI" rel="nofollow">ici</a> .  L'application ouvre une photo, d√©tecte et √©value un visage √† l'aide d'un r√©seau pr√©c√©demment enregistr√© et affiche le r√©sultat: <br><br><img src="https://habrastorage.org/webt/7s/si/a-/7ssia-98n-lxqpitskjaudyohpc.png" alt="image"><br><br>  Du point de vue du d√©veloppement, les fonctions suivantes y sont importantes. <br><br>  1. La fonction de chargement du r√©seau de neurones √† partir du fichier model.tflite dans le dossier d'actifs dans l'objet interpr√©teur <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.tensorflow.lite.Interpreter; Interpreter interpreter; <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> { interpreter=<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Interpreter(loadModelFile(MainActivity.<span class="hljs-keyword"><span class="hljs-keyword">this</span></span>)); Log.e(<span class="hljs-string"><span class="hljs-string">"TIME"</span></span>, <span class="hljs-string"><span class="hljs-string">"Interpreter_started "</span></span>); } <span class="hljs-keyword"><span class="hljs-keyword">catch</span></span> (IOException e) { e.printStackTrace(); Log.e(<span class="hljs-string"><span class="hljs-string">"TIME"</span></span>, <span class="hljs-string"><span class="hljs-string">"Interpreter NOT started "</span></span>); } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> MappedByteBuffer </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">loadModelFile</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(Activity activity)</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">throws</span></span></span><span class="hljs-function"> IOException </span></span>{ AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(<span class="hljs-string"><span class="hljs-string">"model.tflite"</span></span>); FileInputStream inputStream = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> FileInputStream(fileDescriptor.getFileDescriptor()); FileChannel fileChannel = inputStream.getChannel(); <span class="hljs-keyword"><span class="hljs-keyword">long</span></span> startOffset = fileDescriptor.getStartOffset(); <span class="hljs-keyword"><span class="hljs-keyword">long</span></span> declaredLength = fileDescriptor.getDeclaredLength(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength); }</code> </pre><br>  2. D√©tection des visages √† l'aide du module FaceDetector, qui fait partie du package de biblioth√®que standard de Google, √† l'aide d'un r√©seau de neurones et affiche les r√©sultats. <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.google.android.gms.vision.face.Face; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.google.android.gms.vision.face.FaceDetector; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">detectFace</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span>{ <span class="hljs-comment"><span class="hljs-comment">//Create a Paint object for drawing with Paint myRectPaint = new Paint(); myRectPaint.setStrokeWidth(5); myRectPaint.setColor(Color.GREEN); myRectPaint.setStyle(Paint.Style.STROKE); Paint fontPaint = new Paint(); fontPaint.setStrokeWidth(3); fontPaint.setTextSize(70); fontPaint.setColor(Color.BLUE); fontPaint.setStyle(Paint.Style.FILL_AND_STROKE); //Create a Canvas object for drawing on tempBitmap = Bitmap.createBitmap(myBitmap.getWidth(), myBitmap.getHeight(), Bitmap.Config.RGB_565); Canvas tempCanvas = new Canvas(tempBitmap); tempCanvas.drawBitmap(myBitmap, 0, 0, null); //Detect the Faces FaceDetector faceDetector = new FaceDetector.Builder(getApplicationContext()).build(); Frame frame = new Frame.Builder().setBitmap(myBitmap).build(); SparseArray&lt;Face&gt; faces = faceDetector.detect(frame); Face face; float[][] labelProbArray = new float[1][1]; imgData.order(ByteOrder.nativeOrder()); //Draw Rectangles on the Faces if (faces.size()&gt;0){ for (int i = 0; i &lt; faces.size(); i++) { face = faces.valueAt(i); isFaceFound=true; float x1 = Math.max(face.getPosition().x,0); float y1 = Math.max(face.getPosition().y,0); float x2 = Math.min(x1 + face.getWidth(),frame.getBitmap().getWidth()); float y2 = Math.min(y1 + face.getHeight(),frame.getBitmap().getHeight()); Bitmap tempbitmap2 = Bitmap.createBitmap(tempBitmap, (int)x1, (int)y1, (int) (x2-x1), (int) (y2-y1)); tempbitmap2 = Bitmap.createScaledBitmap(tempbitmap2, 112, 112, true); convertBitmapToByteBuffer(tempbitmap2); interpreter.run(imgData, labelProbArray); String textToShow = String.format("%.1f", (Answer[0][0]*5-1)/4 * 10); textToShow = textToShow + "/10"; int width= tempCanvas.getWidth(); //int height=tempCanvas.getHeight(); int fontsize=Math.max(width/20,imgView.getWidth()/20); fontPaint.setTextSize(fontsize); tempCanvas.drawText(textToShow, x1, y1-10, fontPaint); tempCanvas.drawRoundRect(new RectF(x1, y1, x2, y2), 2, 2, myRectPaint) } imgView.setImageDrawable(new BitmapDrawable(getResources(),tempBitmap)); } }</span></span></code> </pre><br>  Si vous souhaitez jouer avec le classement sur votre t√©l√©phone, vous pouvez t√©l√©charger l' <a href="https://play.google.com/store/apps/details%3Fid%3Dcom.beautyfromphoto.androidfacedetection" rel="nofollow">application sur le march√© GooglePlay</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr485858/">https://habr.com/ru/post/fr485858/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr485846/index.html">Webinaire conjoint Fujitsu et SUSE: ¬´Solutions ouvertes et fiables √† l'√®re num√©rique¬ª</a></li>
<li><a href="../fr485850/index.html">Comment Clickhouse a √©t√© choisi dans la galaxie solaire</a></li>
<li><a href="../fr485852/index.html">10 raisons de NE PAS commander la boutique en ligne d'audit de l'utilisabilit√©</a></li>
<li><a href="../fr485854/index.html">Aider le compilateur C ++ √† r√©soudre la surcharge de fonctions</a></li>
<li><a href="../fr485856/index.html">Comment nous avons imprim√© l'hexapode et ce qui en est sorti</a></li>
<li><a href="../fr485862/index.html">DDoS de la cafeti√®re</a></li>
<li><a href="../fr485866/index.html">Int√©gration de Zimbra Open-Source Edition avec Enterprise Portal</a></li>
<li><a href="../fr485868/index.html">√âclairage pour les salles de classe et les salles de classe</a></li>
<li><a href="../fr485870/index.html">Y a-t-il un GameDev √† Sakhaline? 2.V</a></li>
<li><a href="../fr485872/index.html">R√©gression logistique √† m√¢cher</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>