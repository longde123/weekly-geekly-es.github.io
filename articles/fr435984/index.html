<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ¤µğŸ½ ğŸ§™ğŸ¼ ğŸŸï¸ RÃ©seaux de neurones et philosophie du langage ğŸ‘©ğŸ¿â€ğŸ­ ğŸ’± ğŸˆ¶</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pourquoi les thÃ©ories de Wittgenstein restent le fondement de toute la PNL moderne 

 La reprÃ©sentation vectorielle des mots est peut-Ãªtre l'une des i...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>RÃ©seaux de neurones et philosophie du langage</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/435984/"> <font color="gray">Pourquoi les thÃ©ories de Wittgenstein restent le fondement de toute la PNL moderne</font> <br><br>  La reprÃ©sentation vectorielle des mots est peut-Ãªtre l'une des idÃ©es les plus belles et les plus romantiques de l'histoire de l'intelligence artificielle.  La philosophie du langage est une branche de la philosophie qui explore la relation entre le langage et la rÃ©alitÃ© et comment rendre la parole significative et comprÃ©hensible.  Une reprÃ©sentation vectorielle des mots est une mÃ©thode trÃ¨s spÃ©cifique dans le traitement moderne du langage naturel (PNL).  En un sens, il s'agit d'une preuve empirique des thÃ©ories de Ludwig Wittgenstein, l'un des philosophes les plus pertinents du siÃ¨cle dernier.  Pour Wittgenstein, l'utilisation des mots est un mouvement dans un <i>jeu de</i> langage social jouÃ© par des membres de la communautÃ© qui se comprennent.  Le sens d'un mot ne dÃ©pend que de son utilitÃ© dans un contexte, il ne correspond pas un Ã  un avec un objet du monde rÃ©el. <br><br><blockquote>  Pour une grande classe de cas dans lesquels nous utilisons le mot Â«sensÂ», il peut Ãªtre dÃ©fini comme le <b>sens du mot est son utilisation dans la langue</b> . </blockquote><a name="habracut"></a><br>  Bien sÃ»r, il est trÃ¨s difficile de comprendre la signification exacte d'un mot.  Il y a plusieurs aspects Ã  considÃ©rer: <br><br><ul><li>  Ã  quel objet le mot peut faire rÃ©fÃ©rence; </li><li>  quelle partie du discours est-ce; </li><li>  s'il s'agit d'une expression idiomatique; </li><li>  toutes les nuances de sens; </li><li>  et ainsi de suite. </li></ul><br>  Tous ces aspects, au final, se rÃ©sument Ã  une chose: savoir utiliser le mot. <br><br>  Le concept de <i>signification</i> et la raison pour laquelle un ensemble ordonnÃ© de caractÃ¨res a une connotation prÃ©cise dans la langue n'est pas seulement une question philosophique, mais aussi probablement le plus gros problÃ¨me auquel les spÃ©cialistes de l'IA travaillant avec la PNL doivent faire face.  Il est assez Ã©vident pour une personne russophone qu'un Â«chienÂ» est un Â«animalÂ», et il ressemble plus Ã  un Â«chatÂ» qu'Ã  un Â«dauphinÂ», mais cette tÃ¢che est loin d'Ãªtre simple pour une solution systÃ©matique. <br><br>  Ayant lÃ©gÃ¨rement corrigÃ© les thÃ©ories de Wittgenstein, nous pouvons dire que les chiens ressemblent Ã  des chats car ils apparaissent souvent dans les mÃªmes contextes: vous pouvez probablement trouver des chiens et des chats associÃ©s aux mots "maison" et "jardin" qu'aux mots "mer" et "l'ocÃ©an".  C'est cette intuition qui sous-tend <b>Word2Vec</b> , l'une des implÃ©mentations les plus cÃ©lÃ¨bres et rÃ©ussies de la reprÃ©sentation vectorielle des mots.  Aujourd'hui, les machines sont loin d'une rÃ©elle <i>comprÃ©hension des</i> textes et passages longs, mais la reprÃ©sentation vectorielle des mots est sans doute la seule mÃ©thode qui nous a permis de faire le plus grand pas dans cette direction au cours de la derniÃ¨re dÃ©cennie. <br><br><h1>  De BoW Ã  Word2Vec </h1><br>  Dans de nombreux problÃ¨mes informatiques, le premier problÃ¨me est de prÃ©senter les donnÃ©es sous forme numÃ©rique;  les mots et les phrases sont probablement les plus difficiles Ã  imaginer sous cette forme.  Dans notre configuration, les mots <b>D</b> sont sÃ©lectionnÃ©s dans le dictionnaire et chaque mot peut se voir attribuer un index numÃ©rique <b>i</b> . <br><br>  Pendant de nombreuses dÃ©cennies, une approche classique a Ã©tÃ© adoptÃ©e pour reprÃ©senter chaque mot comme un vecteur numÃ©rique de dimension D de tous les zÃ©ros sauf un en position i.  Ã€ titre d'exemple, considÃ©rons un dictionnaire de trois mots: Â«chienÂ», Â«chatÂ» et Â«dauphinÂ» (D = 3).  Chaque mot peut Ãªtre reprÃ©sentÃ© comme un vecteur tridimensionnel: Â«chienÂ» correspond Ã  [1,0,0], Â«chatÂ» Ã  [0,1,0] et Â«dauphinÂ», Ã©videmment, [0,0,1].  Le document peut Ãªtre reprÃ©sentÃ© comme un vecteur dimensionnel D, oÃ¹ chaque Ã©lÃ©ment compte les occurrences du i-Ã¨me mot dans le document.  Ce modÃ¨le est appelÃ© Bag-of-words (BoW), et il est utilisÃ© depuis des dÃ©cennies. <br><br>  MalgrÃ© son succÃ¨s dans les annÃ©es 90, BoW n'avait pas la seule fonction intÃ©ressante des mots: leur signification.  Nous savons que deux mots trÃ¨s diffÃ©rents peuvent avoir des significations similaires, mÃªme s'ils sont complÃ¨tement diffÃ©rents du point de vue de l'orthographe.  Â«ChatÂ» et Â«chienÂ» sont tous deux des animaux domestiques, Â«roiÂ» et Â«reineÂ» sont proches l'un de l'autre, Â«pommeÂ» et Â«cigaretteÂ» sont totalement indÃ©pendants.  Nous le <i>savons</i> , mais dans le modÃ¨le BoW, tous ces mots sont Ã  la mÃªme distance dans l'espace vectoriel: 1. <br><br>  Le mÃªme problÃ¨me s'applique aux documents: en utilisant BoW, nous pouvons conclure que les documents ne sont similaires que s'ils contiennent le mÃªme mot un certain nombre de fois.  Et voici Word2Vec, introduisant dans les termes de l'apprentissage automatique de nombreuses questions philosophiques dont Wittgenstein a discutÃ© dans ses <i>Ã©tudes philosophiques</i> il y a 60 ans. <br><br>  Dans un dictionnaire de taille D, oÃ¹ le mot est identifiÃ© par son indice, le but est de calculer la reprÃ©sentation vectorielle Ã  N dimensions de chaque mot pour N &lt;&lt; D.  IdÃ©alement, nous voulons que ce soit un vecteur dense reprÃ©sentant certains aspects sÃ©mantiquement spÃ©cifiques de la signification.  Par exemple, nous voulons idÃ©alement que Â«chienÂ» et Â«chatÂ» aient des reprÃ©sentations similaires, et Â«pommeÂ» et Â«cigaretteÂ» sont trÃ¨s Ã©loignÃ©es dans l'espace vectoriel. <br><br>  Nous voulons effectuer quelques opÃ©rations algÃ©briques de base sur des vecteurs, tels que <code>+âˆ’=</code> .  Je veux que la distance entre les vecteurs Â«acteurÂ» et Â«actriceÂ» coÃ¯ncide sensiblement avec la distance entre le Â«princeÂ» et la Â«princesseÂ».  Bien que ces rÃ©sultats soient assez utopiques, les expÃ©riences montrent que les vecteurs Word2Vec prÃ©sentent des propriÃ©tÃ©s trÃ¨s proches de celles-ci. <br><br>  Word2Vec n'apprend pas directement les vues de cela, mais les reÃ§oit en tant que sous-produit de la classification sans enseignant.  L'ensemble de donnÃ©es du corpus de mots NLP moyen se compose d'un ensemble de phrases;  chaque mot d'une phrase apparaÃ®t dans le contexte des mots environnants.  Le classificateur a pour but de prÃ©dire le mot cible, en considÃ©rant les mots contextuels comme entrÃ©e.  Pour la phrase Â«chien brun joue dans le jardinÂ», les mots [brun, joue, dans, jardin] sont fournis au modÃ¨le en entrÃ©e, et elle doit prÃ©dire le mot Â«chienÂ».  Cette tÃ¢che est considÃ©rÃ©e comme l'apprentissage sans professeur, car le corpus n'a pas besoin d'Ãªtre marquÃ© avec une source externe de vÃ©ritÃ©: si vous avez un ensemble de phrases, vous pouvez toujours crÃ©er automatiquement des exemples positifs et nÃ©gatifs.  En regardant Â«chien brun jouant dans le jardinÂ» comme exemple positif, nous pouvons crÃ©er de nombreux modÃ¨les nÃ©gatifs, tels que Â«plan brun jouant dans le jardinÂ» ou Â«raisin brun jouant dans le jardinÂ», en remplaÃ§ant le mot cible Â«chienÂ» par des mots alÃ©atoires de l'ensemble de donnÃ©es. <br><br>  Et maintenant, l'application des thÃ©ories de Wittgenstein est parfaitement claire: le contexte est crucial pour la reprÃ©sentation vectorielle des mots, car il est important d'attacher un sens au mot dans ses thÃ©ories.  Si deux mots ont des significations similaires, ils auront des reprÃ©sentations similaires (une petite distance dans l'espace Ã  N dimensions) uniquement parce qu'ils apparaissent souvent dans des contextes similaires.  Ainsi, le "chat" et le "chien" finiront par avoir des vecteurs proches car ils apparaissent souvent dans les mÃªmes contextes: il est utile que le modÃ¨le utilise des reprÃ©sentations vectorielles similaires pour eux, car c'est la chose la plus pratique qu'elle puisse faire, pour obtenir de meilleurs rÃ©sultats en prÃ©disant deux mots en fonction de leur contexte. <br><br>  L'article original propose deux architectures diffÃ©rentes: CBOW et Skip-gram.  Dans les deux cas, les reprÃ©sentations verbales sont enseignÃ©es avec une tÃ¢che de classification spÃ©cifique, fournissant les meilleures reprÃ©sentations vectorielles possibles des mots qui maximisent les performances du modÃ¨le. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2b9/e32/ee7/2b9e32ee767ad0c402e214a566d848a0.png"><br>  <i><font color="gray">Figure 1. Comparaison des architectures CBOW et Skip-gram</font></i> <br><br>  <b>CBOW</b> signifie Continuous Bag of Words, et sa tÃ¢che est de deviner un mot avec le contexte en tÃªte comme entrÃ©e.  Les entrÃ©es et les sorties sont reprÃ©sentÃ©es comme des vecteurs dimensionnels D projetÃ©s dans un espace dimensionnel N avec des poids communs.  Nous recherchons uniquement des poids de projection.  En fait, la reprÃ©sentation vectorielle des mots est des matrices D Ã— N, oÃ¹ chaque ligne reprÃ©sente un mot de dictionnaire.  Tous les mots de contexte sont projetÃ©s dans une position et leurs reprÃ©sentations vectorielles sont moyennÃ©es;  par consÃ©quent, l'ordre des mots n'affecte pas le rÃ©sultat. <br><br>  <b>Skip-gram</b> fait la mÃªme chose, mais vice versa: il essaie de prÃ©dire les mots de contexte <b>C</b> , en prenant le mot cible en entrÃ©e.  La tÃ¢che de prÃ©dire plusieurs mots contextuels peut Ãªtre reformulÃ©e en un ensemble de problÃ¨mes de classification binaire indÃ©pendants, et maintenant l'objectif est de prÃ©dire la prÃ©sence (ou l'absence) de mots contextuels. <br><br>  En rÃ¨gle gÃ©nÃ©rale, Skip-gram nÃ©cessite plus de temps pour la formation et donne souvent des rÃ©sultats lÃ©gÃ¨rement meilleurs, mais, comme d'habitude, diffÃ©rentes applications ont des exigences diffÃ©rentes, et il est difficile de prÃ©dire Ã  l'avance lesquelles afficheront le meilleur rÃ©sultat.  MalgrÃ© la simplicitÃ© du concept, l'apprentissage de ce type d'architecture est un vÃ©ritable cauchemar en raison de la quantitÃ© de donnÃ©es et de la puissance de traitement nÃ©cessaires pour optimiser les poids.  Heureusement, sur Internet, vous pouvez trouver des reprÃ©sentations vectorielles prÃ©-entraÃ®nÃ©es de mots, et vous pouvez Ã©tudier l'espace vectoriel - le plus intÃ©ressant - avec seulement quelques lignes de code Python. <br><br><h1>  AmÃ©liorations possibles: GloVe et fastText </h1><br>  Au cours du Word2Vec classique ces derniÃ¨res annÃ©es, de nombreuses amÃ©liorations possibles ont Ã©tÃ© proposÃ©es.  Les deux plus intÃ©ressants et les plus couramment utilisÃ©s sont GloVe (Stanford University) et fastText (dÃ©veloppÃ© par Facebook).  Ils essaient d'identifier et de surmonter les limites de l'algorithme d'origine. <br><br>  Dans un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article scientifique original</a> , les auteurs de GloVe soulignent que la formation de modÃ¨les sur un contexte local sÃ©parÃ© fait un mauvais usage des statistiques globales du corpus.  La premiÃ¨re Ã©tape pour surmonter cette limitation est de crÃ©er une matrice globale <b>X</b> , oÃ¹ chaque Ã©lÃ©ment <b>i, j</b> compte le nombre de rÃ©fÃ©rences au mot <b>j</b> dans le contexte du mot <b>i</b> .  La deuxiÃ¨me idÃ©e importante de ce document est de comprendre que seules les probabilitÃ©s seules ne suffisent pas pour une prÃ©diction fiable des valeurs, et une matrice de cooccurrence est Ã©galement nÃ©cessaire, Ã  partir de laquelle certains aspects des valeurs peuvent Ãªtre directement extraits. <br><br><blockquote>  ConsidÃ©rons deux mots i et j qui prÃ©sentent un intÃ©rÃªt particulier.  Pour le concret, supposons que nous nous intÃ©ressions au concept d'Ã©tat thermodynamique, pour lequel nous pouvons prendre <code>i = </code> et <code>j = </code> .  La relation de ces mots peut Ãªtre Ã©tudiÃ©e en Ã©tudiant le rapport de leurs probabilitÃ©s d'occurrence conjointe en utilisant diffÃ©rents mots qui sonnent, k.  Pour les mots k liÃ©s Ã  la glace mais pas Ã  la vapeur, disons <code>k = </code> [solide, Ã©tat de la matiÃ¨re], nous nous attendons Ã  ce que le rapport Pik / Pjk soit supÃ©rieur.  De mÃªme, pour les mots k associÃ©s Ã  la vapeur, mais pas Ã  la glace, disons <code>k = </code> , le rapport doit Ãªtre petit.  Pour des mots comme Â«eauÂ» ou Â«modeÂ», qui sont soit Ã©galement liÃ©s Ã  la glace et Ã  la vapeur, soit sans relation avec eux, ce rapport doit Ãªtre proche de l'unitÃ©. </blockquote><br>  Ce rapport de probabilitÃ©s devient le point de dÃ©part pour Ã©tudier la reprÃ©sentation vectorielle des mots.  Nous voulons pouvoir calculer des vecteurs qui, en combinaison avec une fonction spÃ©cifique <b>F,</b> maintiennent ce rapport constant dans l'espace de reprÃ©sentation vectorielle. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4dc/d5d/137/4dcd5d13763ca26ad997565ec2e6e513.jpg"></div><br>  <i><font color="gray">Figure 2. La formule la plus courante pour la reprÃ©sentation vectorielle des mots dans le modÃ¨le GloVe</font></i> <br><br>  La fonction F et la dÃ©pendance au mot k peuvent Ãªtre simplifiÃ©es en remplaÃ§ant les exponentielles et les dÃ©calages fixes, ce qui donne la fonction de minimiser les erreurs par la mÃ©thode des moindres carrÃ©s <b>J</b> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e48/049/738/e48049738a71657b998f5630dac792c4.jpg"></div><br>  <i><font color="gray">Figure 3. La fonction finale du calcul de la reprÃ©sentation vectorielle des mots dans le modÃ¨le GloVe</font></i> <br><br>  La fonction <b>f</b> est une fonction de comptage qui essaie de ne pas surcharger les correspondances trÃ¨s frÃ©quentes et rares, tandis que <b>bi</b> et <b>bj</b> sont des dÃ©calages pour restaurer la symÃ©trie de la fonction.  Dans les derniers paragraphes de l'article, il est montrÃ© que la formation de ce modÃ¨le n'est finalement pas trÃ¨s diffÃ©rente de la formation du modÃ¨le Skip-gram classique, bien que dans les tests empiriques GloVe soit supÃ©rieur aux deux implÃ©mentations de Word2Vec. <br><br>  D'un autre cÃ´tÃ©, <b>fastText</b> corrige un inconvÃ©nient complÃ¨tement diffÃ©rent de Word2Vec: si l'apprentissage du modÃ¨le commence par le codage direct d'un vecteur dimensionnel D, alors la structure interne des mots est ignorÃ©e.  Au lieu d'encoder directement le codage des mots apprenant des reprÃ©sentations verbales, fastText propose d'Ã©tudier N-grammes de caractÃ¨res et de reprÃ©senter les mots comme la somme des vecteurs N-grammes.  Par exemple, avec N = 3, le mot "fleur" est codÃ© comme 6 grammes diffÃ©rents [&lt;fl, flo, low, Debt, wer, er&gt;] plus une sÃ©quence spÃ©ciale &lt;flower&gt;.  Remarquez comment les parenthÃ¨ses angulaires sont utilisÃ©es pour indiquer le dÃ©but et la fin d'un mot.  Ainsi, un mot est reprÃ©sentÃ© par son index dans le dictionnaire des mots et l'ensemble des N-grammes qu'il contient, mappÃ© en entiers Ã  l'aide de la fonction de hachage.  Cette amÃ©lioration simple vous permet de diviser les reprÃ©sentations N-gramme entre les mots et de calculer les reprÃ©sentations vectorielles des mots qui n'Ã©taient pas dans le cas d'apprentissage. <br><br><h1>  ExpÃ©riences et applications possibles </h1><br>  Comme nous l'avons dÃ©jÃ  dit, pour <b>utiliser</b> ces reprÃ©sentations vectorielles, vous n'avez besoin que de quelques lignes de code Python.  J'ai menÃ© plusieurs expÃ©riences avec le <a href="">modÃ¨le GloVe Ã  50 dimensions</a> , formÃ© sur 6 milliards de mots de phrases WikipÃ©dia, ainsi qu'avec le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">modÃ¨le fastText Ã  300 dimensions, formÃ© sur Common Crawl</a> (qui a donnÃ© 600 milliards de jetons).  Cette section fournit des liens vers les rÃ©sultats des deux expÃ©riences uniquement pour prouver les concepts et donner une comprÃ©hension gÃ©nÃ©rale du sujet. <br><br>  Tout d'abord, je voulais vÃ©rifier quelques similitudes de base des mots, la caractÃ©ristique la plus simple mais la plus importante de leur reprÃ©sentation vectorielle.  Comme prÃ©vu, les mots les plus similaires avec le mot Â«chienÂ» Ã©taient Â«chatÂ» (0,92), Â«chiensÂ» (0,85), Â«chevalÂ» (0,79), Â«chiotÂ» (0,78) et Â«animal de compagnieÂ» (0,77).  Notez que le pluriel a presque la mÃªme signification que le singulier.  Encore une fois, c'est assez banal pour nous de dire cela, mais pour une voiture, ce n'est absolument pas un fait.  Maintenant nourriture: les mots les plus similaires pour Â«pizzaÂ» sont Â«sandwichÂ» (0,87), Â«sandwichsÂ» (0,86), Â«collationÂ» (0,81), Â«produits de boulangerieÂ» (0,79), Â«fritesÂ» (0,79) et Â«hamburgersÂ» ( 0,78).  C'est logique, les rÃ©sultats sont satisfaisants et le modÃ¨le se comporte plutÃ´t bien. <br><br>  L'Ã©tape suivante consiste Ã  effectuer des calculs de base dans l'espace vectoriel et Ã  vÃ©rifier dans quelle mesure le modÃ¨le a acquis certaines propriÃ©tÃ©s importantes.  En effet, Ã  la suite du calcul des vecteurs <code>+-</code> , le rÃ©sultat est Â«actriceÂ» (0,94), et Ã  la suite du calcul de l' <code>+-</code> , le mot Â«roiÂ» (0,86) est obtenu.  De maniÃ¨re gÃ©nÃ©rale, si la valeur est <code>a:b=c:d</code> , alors le mot <b>d</b> doit Ãªtre obtenu comme <code>d=b-a+c</code> .  Passant au niveau suivant, il est impossible d'imaginer comment ces opÃ©rations vectorielles dÃ©crivent mÃªme des aspects gÃ©ographiques: nous savons que Rome est la capitale de l'Italie, puisque Berlin est la capitale de l'Allemagne, en fait <code>+-= (0.88)</code> et <code>+-= (0.83)</code> . <br><br>  Et maintenant pour la partie amusante.  En suivant la mÃªme idÃ©e, nous allons essayer d'ajouter et de soustraire des concepts.  Par exemple, quel est l'Ã©quivalent amÃ©ricain de la pizza pour les Italiens?  <code>+-= (0.60)</code> , puis <code> (0.59)</code> .  Depuis mon arrivÃ©e aux Pays-Bas, je dis toujours que ce pays est un mÃ©lange de trois choses: un peu de capitalisme amÃ©ricain, le froid suÃ©dois et la qualitÃ© de vie, et, enfin, une pincÃ©e d' <i>abondance</i> napolitaine.  En modifiant lÃ©gÃ¨rement le thÃ©orÃ¨me d'origine, en supprimant un peu de prÃ©cision suisse, nous obtenons la Hollande (0,68) comme rÃ©sultat des <code>++-</code> - <code>++-</code> : assez impressionnant, pour Ãªtre honnÃªte. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fd8/f76/e41/fd8f76e41d78598394d1613652f44f26.png"><br>  <i><font color="gray">Figure 4. Pour tous les lecteurs nÃ©erlandais: prenez cela comme un compliment, d'accord?</font></i> <br><br>  De bonnes ressources pratiques peuvent Ãªtre trouvÃ©es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> pour utiliser ces reprÃ©sentations vectorielles prÃ©-entraÃ®nÃ©es.  <b>Gensim</b> est une bibliothÃ¨que Python simple et complÃ¨te avec des fonctions algÃ©briques et de similitude prÃªtes Ã  l'emploi.  Ces reprÃ©sentations vectorielles prÃ©-entraÃ®nÃ©es peuvent Ãªtre utilisÃ©es de diverses maniÃ¨res (et utiles), par exemple, pour amÃ©liorer les performances des analyseurs d'humeur ou des modÃ¨les de langage.  Quelle que soit la tÃ¢che, l'utilisation de vecteurs Ã  N dimensions amÃ©liorera considÃ©rablement l'efficacitÃ© du modÃ¨le par rapport au codage direct.  Bien sÃ»r, une formation aux reprÃ©sentations vectorielles dans une zone spÃ©cifique amÃ©liorera encore le rÃ©sultat, mais cela peut nÃ©cessiter, peut-Ãªtre, un effort et un temps excessifs. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr435984/">https://habr.com/ru/post/fr435984/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr435970/index.html">Guide du dÃ©butant pour le dÃ©veloppement de serveurs Web avec Node.js</a></li>
<li><a href="../fr435972/index.html">PrÃ©sentation de la programmation rÃ©active au printemps</a></li>
<li><a href="../fr435974/index.html">Three.js - faire des contrÃ´les pour l'espace ou le planÃ©tarium</a></li>
<li><a href="../fr435976/index.html">WebAssembly en production et le Â«champ de minesÂ» de la Smart TV: un entretien avec Andrei Nagikh</a></li>
<li><a href="../fr435978/index.html">Solutions de contournement de la protection biomÃ©trique</a></li>
<li><a href="../fr435986/index.html">Windows rÃ©servera 7 Go pour les mises Ã  jour du systÃ¨me afin d'Ã©viter de manquer d'espace sur le disque dur</a></li>
<li><a href="../fr435988/index.html">Une introduction aux annotations de type Python. Continuation</a></li>
<li><a href="../fr435990/index.html">Comment faire un changement?</a></li>
<li><a href="../fr435992/index.html">Les joueurs Fallout 76, qui seront capturÃ©s dans un endroit secret des dÃ©veloppeurs, seront interdits</a></li>
<li><a href="../fr435994/index.html">Est-ce Karma, bÃ©bÃ©, ou pourquoi l'attaque sur les rÃ©seaux sans fil qui devait sombrer dans l'oubli est toujours vivante</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>