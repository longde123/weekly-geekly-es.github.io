<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👲🏽 🛀🏾 👮 Robot Trolley ROS - Partie 3. Accélérez, changez la caméra, corrigez la démarche 🌷 👨‍🔧 🥙</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Messages dans la série: 
 8. Nous contrôlons à partir du téléphone-ROS Control, GPS-node 
 7. Localisation du robot: gmapping, AMCL, points de référen...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Robot Trolley ROS - Partie 3. Accélérez, changez la caméra, corrigez la démarche</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/463147/"> Messages dans la série: <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">8. Nous contrôlons à partir du téléphone-ROS Control, GPS-node</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">7. Localisation du robot: gmapping, AMCL, points de référence sur le plan de la salle</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">6. Odométrie avec encodeurs de roue, plan de salle, lidar</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">5. Nous travaillons en rviz et gazebo: xacro, nouveaux capteurs.</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">4. Créez une simulation de robot à l'aide des éditeurs rviz et gazebo.</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">3. Accélérez, changez la caméra, corrigez la démarche</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">2. Logiciel</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">1. Fer</a> <br><br>  La dernière <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">fois</a> , en travaillant avec OpenCV et ROS (système d'exploitation de robot), en utilisant toute la puissance de la framboise pi 3b +, nous avons réussi à rouler le long de la ligne, à voir des sourires sur le visage des gens, le museau triste des chats et même à aller à leur rencontre. <br><br>  Mais avec les premiers pas encourageants dans ce domaine de la robotique, j'ai dû faire face à une multitude de petites tâches: la framboise lente, une petite distance de la caméra, qui reconnaissait le visage, se déplaçant sur le côté lors de la conduite, et d'autres.  La manière de les résoudre, y compris l'ouverture de nouveaux petits horizons dans le développement de ROS, sera décrite plus loin. <br><a name="habracut"></a><br>  1. En exécutant tous les nœuds (scripts dans ROS) à partir d'un projet précédemment assemblé et en commençant à travailler avec les cascades Haar, il est rapidement devenu clair que les performances d'un ordinateur à carte unique pour de telles tâches sont encore plutôt faibles.  Malgré le fait que la charge du processeur n'était pas maximale et variait à moins de 80%, le robot a longtemps réfléchi lorsqu'il a vu le visage d'une personne ou le visage d'un chat, qu'il a dû suivre. <br><br>  Le plus triste, c'est qu'après que le visage ou le visage soit sorti du champ de vision de la caméra, le robot a commencé à bouger, "pensant" qu'il allait vers l'objet.  En général, comme dans M.Yu.  Lermontov: <br>  "Le banni de longue date a erré <br>  Dans le désert du monde sans abri ... ". <br>  Les premières tentatives pour rectifier la situation ont été les suivantes: <br>  - dans le nœud de lancement de la caméra (sur raspberry pi), la taille de l'image capturée a été réduite de 640x480 60fps à 320x240 15 fps. <br><br><pre><code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /home/pi/rosbots_catkin_ws/src/rosbots_driver/scripts/rosbots_driver nano pi_camera_driver.py</code> </pre> <br>  Cela a donné une certaine augmentation de la productivité, mais l'image dans son ensemble est loin d'être idéale. <br><br>  De plus, dans le script pour commencer à suivre le visage d'une personne (follow_face.py) ou d'un chat (follow_cat2.py), les conclusions sur l'écran des images ouvertes elles-mêmes ont été commentées.  C'est tout cv.imshow.  Maintenant, au début du voyage, il était impossible d'observer visuellement si le robot voit une image vers laquelle il se déplace ou non.  Eh bien, j'ai dû sacrifier la clarté. <br>  De plus, les informations-sorties de ROS lui-même dans les scripts ont été commentées: # rospy.loginfo. <br>  Cela a légèrement réduit la charge et augmenté la vitesse de travail. <br>  Mais ... il fallait trouver autre chose. <br><br>  2. De plus, le robot était terriblement myope.  Suite à notre propre expérience et à nos recommandations dans ce domaine, une caméra fish-eye a été utilisée: <br><br><img src="https://habrastorage.org/webt/qp/uj/ad/qpujadqdqnvga0tpfhlt6zvqe74.jpeg"><br><br>  Cette caméra offre un angle de vision assez large et vous permet de capturer une partie importante de la pièce. <br><br>  Cependant, il s'est avéré que ses résultats dans le domaine de la qualité de la reconnaissance faciale n'étaient pas impressionnants ... Mckayla n'a pas été impressionné ... En raison du même angle de vue large que celui utilisé par la caméra, il déforme simultanément l'objet reconnaissable lui-même.  Pour cette raison, les cascades Haar utilisées ne fonctionnent pas bien.  L'éclairage a également un effet significatif; lorsqu'il est ombragé, la qualité diminue encore plus. <br><br>  3. Et pour couronner le tout - le robot manquait de vitesse angulaire pour des torsions claires, et même sur les seuils (pas ceux qui doivent être franchis) et les tapis, il était complètement coincé.  Ici, le poivre a ajouté la différence de puissance des moteurs de «même puissance», qui ne sont presque jamais les mêmes, même après avoir quitté le même lot.  Pour cette raison, le robot a dérivé de manière significative vers la gauche. <br><br><h3>  Amélioration des «performances». </h3><br>  Eh bien, avec les performances, la situation a commencé à s'améliorer sensiblement lorsque le principal avantage de ROS en tant que système est venu à la rescousse, à savoir: la possibilité de répartir la charge sur différentes machines. <br><br>  Autrement dit, il suffit d'exécuter des nœuds de mouvement, des caméras et le nœud maître sur la framboise elle-même, et d'autres nœuds fortement chargés peuvent être posés sur les épaules de systèmes plus puissants. <br>  Par exemple, installer ROS sur un ordinateur portable ou déployer une machine virtuelle. <br><br>  Dans cette situation, la deuxième option a été choisie. <br><br>  Afin de ne pas décrire comment installer Ubuntu, ROS sur une machine virtuelle, nous utilisons le slogan de Winnie Jones « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">nous avons besoin d'un volontaire sans vie</a> ».  Nous l'avons préparé ... - une machine virtuelle avec toute la farce peut être téléchargée <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . <br><br>  Les paramètres de la machine virtuelle dont l'utilisation est proposée sont les suivants: <br><br><img src="https://habrastorage.org/webt/es/2m/1o/es2m1olydpnbjvn9ijaa9krmlhy.png"><br><br>  Il est entendu que vous le déployez (l'exécutez) sur la station de travail VMware. <br>  Mot de passe: framboise <br><br>  Vous pouvez désormais évaluer les performances lors de la digestion des cascades de Haar sur du fer, plus puissant que la framboise pi! <br><br>  Afin de démarrer le système ROS, désormais divisé en 2 parties, vous devez spécifier la machine virtuelle sur laquelle le nœud maître fonctionne (et il démarre sur framboise immédiatement après le démarrage du système). <br><br>  Pour ce faire, modifiez bashrc dans une machine virtuelle: <br><br><pre> <code class="bash hljs">sudo nano ~/.bashrc</code> </pre><br>  Dans les lignes à la fin du fichier, spécifiez les adresses IP: <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ROS_MASTER_URI=http://192.168.1.120:11311 <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ROS_HOSTNAME=192.168.1.114</code> </pre><br>  La première IP est l'adresse Raspberry Pi, la seconde IP est l'adresse de la machine virtuelle. <br>  * La machine virtuelle et la framboise doivent être sur le même réseau local. <br><br>  Sur framboise, le même bashrc sera différent: <br><br><pre> <code class="bash hljs">&lt;<span class="hljs-built_in"><span class="hljs-built_in">source</span></span> lang=<span class="hljs-string"><span class="hljs-string">"bash"</span></span>&gt;<span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ROS_MASTER_URI=http://192.168.1.120:11311 <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ROS_HOSTNAME=192.168.1.120</code> </pre><br>  Deux adresses IP sur match de framboise. <br>  * N'oubliez pas de redémarrer après avoir changé bashrc. <br><br><h3>  Comment démarrer un système ROS distribué? </h3><br>  Il y a deux terminaux sur framboise. <br><br>  Au 1er mouvement: <br><br><pre> <code class="bash hljs">rosrun rosbots_driver part2_cmr.py</code> </pre> <br>  Dans le 2ème appareil photo framboise: <br><br><pre> <code class="bash hljs">sudo modprobe bcm2835-v4l2 roslaunch usb_cam usb_cam-test.launch</code> </pre> <br>  Au total, 2 nœuds fonctionneront sur la framboise: l'un attend qu'une commande se déplace, le second envoie la vidéo de la caméra au réseau. <br><br>  Pour vous connecter au maître et contrôler le robot à partir du clavier, maintenant dans la machine virtuelle, vous devez exécuter: <br><br><pre> <code class="bash hljs">rosrun teleop_twist_keyboard teleop_twist_keyboard.py /cmd_vel:=/part2_cmr/cmd_vel</code> </pre> <br>  De la même manière, au lieu de gérer à partir d'une machine virtuelle, vous pouvez exécuter un nœud - en suivant le visage d'une personne: <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /home/pi/rosbots_setup_tools/rpi_setup python follow_face.py</code> </pre> <br>  Maintenant, des statistiques un peu sèches sur les résultats de la caméra fish-eye framboise pi à différents paramètres de l'image capturée par la caméra. <br><br>  <i>320x240, 30fps:</i> <i><br></i>  <i>Visage d'homme.</i>  <i>80-100 cm.Influences d'éclairage, ne voit pratiquement pas dans l'ombre.</i>  <i>Il est préférable de diriger la lampe vers le visage pour une amélioration.</i>  <i>Sourire, avec un sourire, reconnaît le visage plus rapidement.</i> <i><br></i>  <i>Visage de chat sur feuille A4.</i>  <i>15-30 cm.</i> <i><br><br></i>  <i>À 800x600 et 60fps - le visage voit à une distance de 2-3 m.</i> <i><br><br></i>  <i>À 1280x720 et 60fps - le visage voit à une distance de 3-4 m. Mais il y a des faux positifs - il voit une horloge murale comme le visage, etc.</i> <i><br><br></i>  <i>Un chat sur A4 reconnaît avec un retard de 1-2 secondes, une distance de 1-1,5 m.</i> <i><br></i>  <i>Charge de la machine virtuelle 77-88%.</i> <br><br>  Comme vous pouvez le voir, avec une plus grande fenêtre de l'image capturée par la caméra, la charge sur la machine virtuelle et la distance à laquelle le robot voit l'objet augmenter.  La charge sur la framboise augmente, mais pas de manière significative.  La vitesse de reconnaissance par rapport au fonctionnement de tous les nœuds uniquement sur la framboise a considérablement augmenté. <br><br><h3>  Traiter la myopie </h3><br>  Les statistiques ci-dessus ont montré que pour améliorer la vision, il est nécessaire d'augmenter la résolution et les fps de la caméra, ce qui paie les performances de la machine virtuelle.  Cependant, même à 1280x720 et 60fps, la distance à l'objet n'est pas trop grande. <br><br>  La solution à ce problème est venue de façon inattendue. <br><br>  Comme vous le savez, toute une gamme de caméras est disponible pour la framboise.  Et pour notre cas, l'un d'eux était parfait, à savoir: <br><br><img src="https://habrastorage.org/webt/9d/5l/g_/9d5lg_gj9ssagviofb-7gu3yfem.jpeg"><br><br>  Ses résultats sont les suivants: <br><br>  <i>Avec un éclairage artificiel dans la pièce, il voit son visage à une distance de 7 à 8 m!</i>  <i>Il n'est même pas nécessaire de sourire)</i> <i><br></i>  <i>800x600 et 60fps - téléchargement.</i>  <i>machine virtuelle - 70-80%, framboise - 21% (il n'y a que des nœuds de mouvement et commencez à diffuser la caméra).</i> <i><br></i>  <i>Un chat sur A4 reconnaît avec un retard de 1-2 secondes, une distance de 2-2,3 m.</i> <br>  Comme vous pouvez le voir, les chats souffrent à nouveau, qui sont déterminés à des distances plus courtes.  Mais ici, la raison est très probablement dans les paramètres des cascades Haar, et non dans la caméra. <br>  * Cette caméra est généralement livrée avec des oreilles infrarouges.  Il n'est pas nécessaire de les utiliser en plein jour: <br><br><img src="https://habrastorage.org/webt/h6/_j/vz/h6_jvzqmjq32whtx95uhcw3vkhi.png"><br><br>  Ainsi, l'appareil photo pour framboise pi, différent de fish-eye, a montré les meilleurs résultats lorsque vous travaillez avec des objets, même dans un éclairage artificiel de mauvaise qualité.  Cependant, pour augmenter la distance de la caméra, j'ai dû payer avec des angles de vue et cela doit être pris en compte. <br><br><h3>  Maintenant sur l'amélioration du châssis et la démolition du robot sur le côté </h3><br>  Comme mentionné ci-dessus, il n'y a pratiquement pas de moteurs bimoteurs et le robot est voué à rouler soit à droite, soit à gauche, selon la puissance du moteur, toutes choses égales par ailleurs. <br><br>  Habituellement, la différence de mouvement est compensée en utilisant des encodeurs, car ils sont bons dans notre projet.  Sur la façon dont cela fonctionne, il y a un bon <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">film</a> . <br><br>  Cependant, ici, vous pouvez tout simplifier et opter pour une petite astuce en corrigeant uniquement le script pour suivre le visage d'une personne (follow_face.py) ou d'un chat (follow_cat2.py) dans une machine virtuelle. <br><br>  Nous sommes intéressés par les lignes suivantes: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#-0.013 - because right wheel is faster twist_msg.linear.x = 0.1 twist_msg.angular.z = -0.013 elif self.face_found["x"] &lt; 0.0: # To the left cur_dir = "left" twist_msg.linear.x = 0.1 twist_msg.angular.z = 0.016 else: # To the right cur_dir = "right" twist_msg.linear.x = 0.1 twist_msg.angular.z = -0.016</span></span></code> </pre> <br>  Selon où et combien le robot souffle, vous devez ajuster les valeurs de z.  Dans le code ci-dessus, les valeurs sont sélectionnées lors de l'alimentation du pilote de moteur 12V. <br><br>  Il est possible d'augmenter les caractéristiques de fonctionnement du robot (vitesse et maniabilité) en appliquant au pilote du moteur, qui est utilisé dans le projet (L9110s) au lieu de 8 V - 12V.  Mais il vaut mieux ne pas le faire sans installer un stabilisateur de puissance ou au moins un condensateur, car  Les puces de pont en H L9110s brûlent bien, comme il s'est avéré.  Peut-être que cela les rend si bon marché.  Néanmoins, le robot pilote le 12 V de manière assez rapide et frivole. <br><br><h3>  Au lieu d'une conclusion </h3><br>  Eh bien, maintenant que le robot a cessé d'être emporté et que sa vision s'est améliorée, vous pouvez jouer à cache-cache avec lui, par exemple.  Ou pour forcer quelque chose à apporter de l'autre bout de la pièce.  Bien sûr, se tourner pour lui faire face, comme pour une personne. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/fdGjeOIEv6A" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  À suivre. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr463147/">https://habr.com/ru/post/fr463147/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr463127/index.html">Surveillance UPS. Deuxième partie - Automatiser l'analyse</a></li>
<li><a href="../fr463135/index.html">Dans quels pays est-il rentable d'enregistrer des sociétés informatiques en 2019?</a></li>
<li><a href="../fr463137/index.html">Même si vous voulez être un game designer, personne ne vous apprendra comment</a></li>
<li><a href="../fr463141/index.html">Habr Weekly # 13 / Sous la menace de 1,5 million d'utilisateurs d'un service de rencontres, enquête Medusa, deanon des Russes</a></li>
<li><a href="../fr463143/index.html">Premiers pas avec Google Analytics: App + Web</a></li>
<li><a href="../fr463149/index.html">Alan Kay recommande de lire des livres de programmation anciens et oubliés mais importants</a></li>
<li><a href="../fr463151/index.html">iOS 13 sous la loupe</a></li>
<li><a href="../fr463155/index.html">Le bruit blanc dessine un carré noir. Partie 2. Solution</a></li>
<li><a href="../fr463157/index.html">Serveur de diffusion vidéo ESP32-CAM connectant des écrans I2C et SPI</a></li>
<li><a href="../fr463159/index.html">À propos de la sécurité, des chiffres, des e-mails et, un peu sur la publicité</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>