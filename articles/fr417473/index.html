<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßîüèΩ üçÅ ‚òîÔ∏è Stockage s√©curis√© avec DRBD9 et Proxmox (Partie 1: NFS) üôçüèº üåÉ üéì</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Probablement tous ceux qui ont √©t√© au moins une fois perplexes face √† la recherche d'un stockage haute performance d√©fini par logiciel ont t√¥t ou tard...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Stockage s√©curis√© avec DRBD9 et Proxmox (Partie 1: NFS)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/417473/"><p><img src="https://habrastorage.org/getpro/habr/post_images/f6a/771/043/f6a7710433c8887d6bbbe4792cc178e1.jpg" alt="image"></p><br><p>  Probablement tous ceux qui ont √©t√© au moins une fois perplexes face √† la recherche d'un <strong>stockage</strong> haute performance <strong>d√©fini par logiciel ont</strong> t√¥t ou tard entendu parler de <strong>DRBD</strong> , ou peut-√™tre m√™me l'ont trait√©. </p><br><p> Certes, au sommet de la popularit√© de <strong>Ceph</strong> et <strong>GlusterFS</strong> , qui fonctionnent assez bien en principe, et surtout d√®s la sortie de la bo√Æte, tout le monde en a juste oubli√© un peu.  De plus, la version pr√©c√©dente ne prenait pas en charge la r√©plication sur plus de deux n≈ìuds, et √† cause de cela, des probl√®mes avec le <strong>split-brain</strong> √©taient souvent rencontr√©s, ce qui n'a clairement pas ajout√© √† sa popularit√©. </p><br><p>  La solution n'est vraiment pas nouvelle, mais assez comp√©titive.  Avec des co√ªts relativement faibles pour le CPU et la RAM, <strong>DRBD</strong> fournit une synchronisation tr√®s rapide et s√ªre au niveau du <strong>p√©riph√©rique de bloc</strong> .  Pendant tout ce temps, les d√©veloppeurs de LINBIT - DRBD ne s'arr√™tent pas et l'affinent constamment.  √Ä partir de la version <strong>DRBD9</strong> , <strong>il</strong> cesse d'√™tre simplement un miroir de r√©seau et devient quelque chose de plus. </p><br><p>  Tout d'abord, l'id√©e de cr√©er un seul <strong>p√©riph√©rique de bloc distribu√©</strong> pour plusieurs serveurs s'est retir√©e en arri√®re-plan, et maintenant LINBIT essaie de fournir des outils pour orchestrer et g√©rer de nombreux p√©riph√©riques drbd dans un cluster qui sont cr√©√©s au-dessus des <strong>partitions</strong> <strong>LVM</strong> et <strong>ZFS</strong> . </p><br><p>  Par exemple, DRBD9 prend en charge jusqu'√† 32 r√©pliques, RDMA, n≈ìuds sans disque et de nouveaux outils d'orchestration vous permettent d'utiliser des instantan√©s, une migration en ligne et bien plus encore. </p><br><p>  Malgr√© le fait que <strong>DRBD9</strong> dispose d'outils d'int√©gration avec <strong>Proxmox</strong> , <strong>Kubernetes</strong> , <strong>OpenStack</strong> et <strong>OpenNebula</strong> , ils sont actuellement en mode transitionnel, lorsque de nouveaux outils ne sont pas encore pris en charge partout, et les anciens seront annonc√©s comme <em>obsol√®tes</em> tr√®s bient√¥t.  Ce sont <strong>DRBDmanage</strong> et <strong>Linstor</strong> . </p><br><p>  Je profiterai de ce moment pour ne pas trop entrer dans les d√©tails de chacun, mais pour examiner plus en d√©tail la configuration et les principes de travail avec <strong>DRBD9</strong> lui-m√™me. <a name="habracut"></a>  Vous devez encore le comprendre, ne serait-ce que parce que la configuration √† tol√©rance de pannes du contr√¥leur Linstor implique son installation sur l'un de ces appareils. </p><br><p>  Dans cet article, je voudrais vous parler de <strong>DRBD9</strong> et de la possibilit√© de son utilisation dans <strong>Proxmox</strong> sans plug-ins tiers. </p><br><h2 id="drbdmanage-i-linstor">  DRBDmanage et Linstor </h2><br><p>  Tout d'abord, il convient de mentionner une fois de plus √† propos de <strong>DRBDmanage</strong> , qui s'int√®gre tr√®s bien dans <strong>Proxmox</strong> .  LINBIT fournit un plugin DRBDmanage pr√™t √† l'emploi pour Proxmox qui vous permet d'utiliser toutes ses fonctions directement √† partir de l'interface <strong>Proxmox</strong> . </p><br><p>  Cela a l'air vraiment incroyable, mais a malheureusement quelques inconv√©nients. </p><br><ul><li> Tout d'abord, les noms de volume balis√©s, le <strong>groupe LVM</strong> ou le <strong>pool ZFS</strong> doivent √™tre nomm√©s <code>drbdpool</code> . </li><li>  Incapacit√© √† utiliser plusieurs pools par n≈ìud </li><li>  En raison des sp√©cificit√©s de la solution, le <strong>volume</strong> du <strong>contr√¥leur</strong> ne peut √™tre que sur un LVM normal et pas autrement </li><li>  <strong>Probl√®mes de dbus</strong> p√©riodiques, qui sont √©troitement utilis√©s par <strong>DRBDmanage</strong> pour interagir avec les n≈ìuds. </li></ul><br><p>  En cons√©quence, LINBIT a d√©cid√© de remplacer toute la logique DRBDmanage complexe par une application simple qui communique avec les n≈ìuds √† l'aide d'une <strong>connexion TCP</strong> r√©guli√®re et fonctionne sans aucune magie.  Il y avait donc <strong>Linstor</strong> . </p><br><p>  <strong>Linstor</strong> fonctionne vraiment tr√®s bien.  Malheureusement, les d√©veloppeurs ont choisi <strong>java</strong> comme langue principale pour √©crire Linstor-server, mais ne vous inqui√©tez pas, car Linstor lui-m√™me ne s'int√©resse qu'√† la <strong>distribution des configurations</strong> DRBD et au <strong>d√©coupage des</strong> partitions LVM / ZFS sur les n≈ìuds. </p><br><blockquote>  Les deux solutions sont gratuites et distribu√©es sous la licence gratuite <strong>GPL3.</strong> </blockquote><p>  Vous pouvez lire sur chacun d'eux et sur la configuration du plug-in susmentionn√© pour <strong>Proxmox</strong> sur le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">wiki officiel de Proxmox</a> </p><br><h2 id="otkazoustoychivyy-nfs-server">  Serveur de basculement NFS </h2><br><p>  Malheureusement, au moment de la r√©daction de ce document, <strong>Linstor n'avait qu'une</strong> int√©gration avec <strong>Kubernetes</strong> .  Mais √† la fin de l'ann√©e, des pilotes sont attendus pour le reste des syst√®mes <strong>Proxmox</strong> , <strong>OpenNebula</strong> , <strong>OpenStack</strong> . </p><br><p>  Mais jusqu'√† pr√©sent, il n'y a pas de solution toute faite, mais nous n'aimons pas en quelque sorte l'ancienne.  Essayons d'utiliser DRBD9 √† l'ancienne pour organiser l' <strong>acc√®s NFS</strong> √† une partition partag√©e. </p><br><p>  N√©anmoins, cette solution s'av√©rera √©galement non sans avantages, car le serveur NFS vous permettra d'organiser un <strong>acc√®s comp√©titif</strong> au syst√®me de fichiers du r√©f√©rentiel √† partir de plusieurs serveurs sans avoir besoin de syst√®mes de fichiers de cluster complexes avec DLM, tels que OCFS et GFS2. </p><br><p>  Dans ce cas, vous pourrez changer les r√¥les du n≈ìud <strong>principal</strong> / <strong>secondaire</strong> simplement en migrant le conteneur avec le serveur NFS dans l'interface Proxmox. </p><br><p>  Vous pouvez √©galement stocker tous les fichiers √† l'int√©rieur de ce syst√®me de fichiers, ainsi que les disques virtuels et les sauvegardes. </p><br><p>  Si vous utilisez <strong>Kubernetes,</strong> vous pourrez organiser l'acc√®s <strong>ReadWriteMany</strong> pour vos <strong>volumes</strong> <strong>persistants</strong> . </p><br><h2 id="proxmox-i-lxc-konteynery">  Conteneurs Proxmox et LXC </h2><br><p>  Maintenant, la question est: pourquoi Proxmox? </p><br><p>  En principe, pour construire un tel sch√©ma, nous pourrions utiliser Kubernetes ainsi que le sch√©ma habituel avec un gestionnaire de cluster.  Mais <strong>Proxmox</strong> fournit une interface pr√™te √† l'emploi, tr√®s multifonctionnelle et √† la fois simple et intuitive pour presque tout ce dont vous avez besoin.  Il est pr√™t √† √™tre mis en <strong>cluster</strong> et prend en charge le m√©canisme d' <strong>escrime</strong> bas√© sur softdog.  Et lorsque vous utilisez des <strong>conteneurs LXC,</strong> cela vous permet de minimiser les d√©lais lors du changement. <br>  La solution obtenue n'aura pas un seul <strong>point de d√©faillance</strong> . </p><br><p>  En fait, nous utiliserons Proxmox principalement comme <strong>gestionnaire de cluster</strong> , o√π nous pouvons consid√©rer un <strong>conteneur LXC</strong> distinct comme un service s'ex√©cutant dans un cluster HA classique, √† la diff√©rence pr√®s que le conteneur est √©galement livr√© avec son <strong>syst√®me racine</strong> .  Autrement dit, vous n'avez pas besoin d'installer plusieurs instances de service sur chaque serveur s√©par√©ment, vous ne pouvez le faire qu'une seule fois √† l'int√©rieur du conteneur. <br>  Si vous avez d√©j√† travaill√© avec un <strong>logiciel de gestion de cluster</strong> et fourni une <strong>haute disponibilit√©</strong> pour les applications, vous comprendrez ce que je veux dire. </p><br><h2 id="obschaya-shema">  Sch√©ma g√©n√©ral </h2><br><p>  Notre solution ressemblera au sch√©ma de r√©plication standard d'une base de donn√©es. </p><br><ul><li>  Nous avons <strong>trois n≈ìuds</strong> </li><li>  Chaque n≈ìud poss√®de un <strong>p√©riph√©rique drbd</strong> distribu√©. </li><li>  L'appareil dispose d'un syst√®me de fichiers standard ( <strong>ext4</strong> ) </li><li>  Un seul serveur peut √™tre <strong>ma√Ætre</strong> </li><li>  Le <strong>serveur NFS</strong> dans le <strong>conteneur LXC est</strong> lanc√© sur l'assistant. </li><li>  Tous les n≈ìuds acc√®dent √† l'appareil strictement via <strong>NFS.</strong> </li><li>  Si n√©cessaire, l'assistant peut se d√©placer vers un autre n≈ìud, avec le <strong>serveur NFS</strong> </li></ul><br><p>  <strong>DRBD9</strong> a une fonctionnalit√© tr√®s cool qui simplifie consid√©rablement tout: <br>  Le p√©riph√©rique drbd devient automatiquement <strong>primaire</strong> au moment o√π il est mont√© sur un n≈ìud.  Si le p√©riph√©rique est marqu√© comme <strong>principal</strong> , toute tentative de le monter sur un autre n≈ìud entra√Ænera une erreur d'acc√®s.  Cela garantit le blocage et une protection garantie contre l'acc√®s simultan√© √† l'appareil. </p><br><p>  Pourquoi tout cela simplifie-t-il grandement?  Parce que lorsque le conteneur d√©marre, <strong>Proxmox</strong> monte automatiquement ce p√©riph√©rique et il devient <strong>principal</strong> sur ce n≈ìud, et lorsque le conteneur s'arr√™te, il le d√©monte au contraire et le p√©riph√©rique redevient <strong>secondaire</strong> . <br>  Ainsi, nous n'avons plus √† nous soucier de changer de p√©riph√©rique <strong>primaire</strong> / <strong>secondaire</strong> , Proxmox le fera <strong>automatiquement</strong> , Hourra! </p><br><h2 id="nastroyka-drbd">  Configuration DRBD </h2><br><p>  Eh bien, nous avons compris l'id√©e. Passons maintenant √† la mise en ≈ìuvre. </p><br><p>  Par d√©faut <strong>, la huiti√®me version de drbd</strong> est fournie <strong>avec le noyau Linux</strong> , malheureusement elle ne nous <strong>convient</strong> pas et nous devons installer la neuvi√®me version du module. </p><br><p>  Connectez le r√©f√©rentiel LINBIT et installez tout ce dont vous avez besoin: </p><br><pre> <code class="bash hljs">wget -O- https://packages.linbit.com/package-signing-pubkey.asc | apt-key add - <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"deb http://packages.linbit.com/proxmox/ proxmox-5 drbd-9.0"</span></span> \ &gt; /etc/apt/sources.list.d/linbit.list apt-get update &amp;&amp; apt-get -y install pve-headers drbd-dkms drbd-utils drbdtop</code> </pre> <br><ul><li>  <code>pve-headers</code> - <code>pve-headers</code> noyau n√©cessaires pour construire le module </li><li>  <code>drbd-dkms</code> - module du noyau au format DKMS </li><li>  <code>drbd-utils</code> - utilitaires de gestion de base de DRBD </li><li>  <code>drbdtop</code> est un outil interactif comme top pour DRBD uniquement </li></ul><br><p>  Apr√®s avoir install√© le <strong>module, nous</strong> v√©rifierons si tout est en ordre avec lui: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># modprobe drbd # cat /proc/drbd version: 9.0.14-1 (api:2/proto:86-113)</span></span></code> </pre> <br><p>  Si vous voyez la <strong>huiti√®me version</strong> dans la sortie de la commande, alors quelque chose s'est mal pass√© et le module du noyau <strong>dans l'arborescence</strong> est charg√©.  V√©rifiez l' <code>dkms status</code> conna√Ætre la raison. </p><br><p>  Chaque n≈ìud que nous avons aura le m√™me <strong>p√©riph√©rique drbd</strong> fonctionnant au-dessus des partitions r√©guli√®res.  Nous devons d'abord pr√©parer cette section pour drbd sur chaque n≈ìud. </p><br><p>  Une telle partition peut √™tre n'importe quel <strong>p√©riph√©rique bloc</strong> , elle peut √™tre lvm, zvol, une partition de disque ou le disque entier.  Dans cet article, j'utiliserai un disque nvme s√©par√© avec une partition sous drbd: <code>/dev/nvme1n1p1</code> </p><br><p>  Il convient de noter que les noms des appareils ont parfois tendance √† changer, il est donc pr√©f√©rable de prendre imm√©diatement l'habitude d'utiliser un lien symbolique constant vers l'appareil. </p><br><p>  Vous pouvez trouver un tel lien symbolique pour <code>/dev/nvme1n1p1</code> cette fa√ßon: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># find /dev/disk/ -lname '*/nvme1n1p1' /dev/disk/by-partuuid/847b9713-8c00-48a1-8dff-f84c328b9da2 /dev/disk/by-path/pci-0000:0e:00.0-nvme-1-part1 /dev/disk/by-id/nvme-eui.0000000001000000e4d25c33da9f4d01-part1 /dev/disk/by-id/nvme-INTEL_SSDPEKKA010T7_BTPY703505FB1P0H-part1</span></span></code> </pre> <br><p>  Nous d√©crivons notre ressource sur les trois n≈ìuds: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cat /etc/drbd.d/nfs1.res resource nfs1 { meta-disk internal; device /dev/drbd100; protocol C; net { after-sb-0pri discard-zero-changes; after-sb-1pri discard-secondary; after-sb-2pri disconnect; } on pve1 { address 192.168.2.11:7000; disk /dev/disk/by-partuuid/95e7eabb-436e-4585-94ea-961ceac936f7; node-id 0; } on pve2 { address 192.168.2.12:7000; disk /dev/disk/by-partuuid/aa7490c0-fe1a-4b1f-ba3f-0ddee07dfee3; node-id 1; } on pve3 { address 192.168.2.13:7000; disk /dev/disk/by-partuuid/847b9713-8c00-48a1-8dff-f84c328b9da2; node-id 2; } connection-mesh { hosts pve1 pve2 pve3; } }</span></span></code> </pre> <br><p>  Il est conseill√© d'utiliser un <strong>r√©seau s√©par√©</strong> pour la synchronisation drbd. </p><br><p>  Maintenant, cr√©ez les m√©tadonn√©es pour drbd et ex√©cutez-le: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm create-md nfs1 initializing activity log initializing bitmap (320 KB) to all zero Writing meta data... New drbd meta data block successfully created. success # drbdadm up nfs1</span></span></code> </pre> <br><p>  R√©p√©tez ces √©tapes sur les trois n≈ìuds et v√©rifiez l'√©tat: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm status nfs1 role:Secondary disk:Inconsistent pve2 role:Secondary peer-disk:Inconsistent pve3 role:Secondary peer-disk:Inconsistent</span></span></code> </pre> <br><p>  Maintenant, notre disque <strong>incoh√©rent est</strong> sur les trois n≈ìuds, c'est parce que drbd ne sait pas quel disque doit √™tre pris comme original.  Nous devons marquer l'un d'eux comme <strong>primaire</strong> pour que son √©tat soit synchronis√© avec les autres n≈ìuds: </p><br><pre> <code class="bash hljs">drbdadm primary --force nfs1 drbdadm secondary nfs1</code> </pre> <br><p>  Imm√©diatement apr√®s cela, la <strong>synchronisation</strong> commencera: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm status nfs1 role:Secondary disk:UpToDate pve2 role:Secondary replication:SyncSource peer-disk:Inconsistent done:26.66 pve3 role:Secondary replication:SyncSource peer-disk:Inconsistent done:14.20</span></span></code> </pre><br><p>  Nous n'avons pas √† attendre qu'il se termine et nous pouvons effectuer d'autres √©tapes en parall√®le.  Ils peuvent √™tre ex√©cut√©s sur <strong>n'importe quel n≈ìud</strong> , quel que soit son √©tat actuel du disque local dans DRBD.  Toutes les demandes seront automatiquement redirig√©es vers l'appareil avec l'√©tat <strong>UpToDate</strong> . </p><br><p>  N'oubliez pas d'activer <strong>l'</strong> ex√©cution <strong>automatique du</strong> service drbd sur les n≈ìuds: </p><br><pre> <code class="hljs pgsql">systemctl <span class="hljs-keyword"><span class="hljs-keyword">enable</span></span> drbd.service</code> </pre> <br><h2 id="nastroyka-lxc-konteynera">  Configuration d'un conteneur LXC </h2><br><p>  Nous <strong>allons</strong> omettre la partie configuration du <strong>cluster Proxmox</strong> de trois n≈ìuds, cette partie est bien d√©crite dans le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">wiki officiel</a> </p><br><p>  Comme je l'ai d√©j√† dit, notre <strong>serveur NFS</strong> fonctionnera dans un <strong>conteneur LXC</strong> .  Nous conserverons le conteneur sur le p√©riph√©rique <code>/dev/drbd100</code> que nous venons de cr√©er. </p><br><p>  Nous devons d'abord cr√©er un <strong>syst√®me de fichiers</strong> dessus: </p><br><pre> <code class="hljs powershell">mkfs <span class="hljs-literal"><span class="hljs-literal">-t</span></span> ext4 <span class="hljs-literal"><span class="hljs-literal">-O</span></span> mmp <span class="hljs-literal"><span class="hljs-literal">-E</span></span> mmp_update_interval=<span class="hljs-number"><span class="hljs-number">5</span></span> /dev/drbd100</code> </pre> <br><p>  <strong>Proxmox</strong> inclut par d√©faut une <strong>protection multimount</strong> au niveau du syst√®me de fichiers, en principe, nous pouvons nous en passer, car  DRBD a sa propre protection par d√©faut, il interdit simplement le deuxi√®me <strong>primaire</strong> pour l'appareil, mais la prudence ne nous fait pas de mal. </p><br><p>  T√©l√©chargez maintenant le mod√®le Ubuntu: </p><br><pre> <code class="hljs pgsql"># wget http://download.proxmox.com/images/<span class="hljs-keyword"><span class="hljs-keyword">system</span></span>/ubuntu<span class="hljs-number"><span class="hljs-number">-16.04</span></span>-standard_16<span class="hljs-number"><span class="hljs-number">.04</span></span><span class="hljs-number"><span class="hljs-number">-1</span></span>_amd64.tar.gz -P /var/lib/vz/<span class="hljs-keyword"><span class="hljs-keyword">template</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">cache</span></span>/</code> </pre> <br><p>  Et cr√©ez notre conteneur √† partir de celui-ci: </p><br><pre> <code class="hljs powershell">pct create <span class="hljs-number"><span class="hljs-number">101</span></span> local:vztmpl/ubuntu<span class="hljs-literal"><span class="hljs-literal">-16</span></span>.<span class="hljs-number"><span class="hljs-number">04</span></span><span class="hljs-literal"><span class="hljs-literal">-standard_16</span></span>.<span class="hljs-number"><span class="hljs-number">04</span></span><span class="hljs-literal"><span class="hljs-literal">-1_amd64</span></span>.tar.gz \ -<span class="hljs-literal"><span class="hljs-literal">-hostname</span></span>=nfs1 \ -<span class="hljs-literal"><span class="hljs-literal">-net0</span></span>=name=eth0,bridge=vmbr0,gw=<span class="hljs-number"><span class="hljs-number">192.168</span></span>.<span class="hljs-number"><span class="hljs-number">1.1</span></span>,ip=<span class="hljs-number"><span class="hljs-number">192.168</span></span>.<span class="hljs-number"><span class="hljs-number">1.11</span></span>/<span class="hljs-number"><span class="hljs-number">24</span></span> \ -<span class="hljs-literal"><span class="hljs-literal">-rootfs</span></span>=volume=/dev/drbd100,shared=<span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br><p>  Dans cette commande, nous indiquons que le <strong>syst√®me racine de</strong> notre conteneur sera sur le p√©riph√©rique <code>/dev/drbd100</code> et ajoutons le param√®tre <code>shared=1</code> pour permettre la <strong>migration du</strong> conteneur entre les n≈ìuds. </p><br><p>  En cas de probl√®me, vous pouvez toujours le corriger via l'interface <strong>Proxmox</strong> ou dans la <code>/etc/pve/lxc/101.conf</code> conteneur <code>/etc/pve/lxc/101.conf</code> </p><br><p>  Proxmox d√©ballera le mod√®le et pr√©parera <strong>le syst√®me racine du</strong> conteneur pour nous.  Apr√®s cela, nous pouvons lancer notre conteneur: </p><br><pre> <code class="hljs pgsql">pct <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span></code> </pre> <br><h2 id="nastroyka-nfs-servera">  Configurez un serveur NFS. </h2><br><p>  Par d√©faut, Proxmox <strong>n'autorise pas</strong> le <strong>serveur NFS</strong> √† s'ex√©cuter dans le conteneur, mais il existe plusieurs fa√ßons de l'activer. </p><br><p>  L'un d'eux consiste simplement √† ajouter <code>lxc.apparmor.profile: unconfined</code> √† la <code>/etc/pve/lxc/100.conf</code> notre conteneur <code>/etc/pve/lxc/100.conf</code> . </p><br><p>  Ou nous pouvons <strong>activer NFS</strong> pour tous les conteneurs sur une base continue, pour cela, nous devons mettre √† jour le mod√®le standard pour LXC sur tous les n≈ìuds, ajouter les lignes suivantes √† <code>/etc/apparmor.d/lxc/lxc-default-cgns</code> : </p><br><pre> <code class="hljs nginx"> <span class="hljs-attribute"><span class="hljs-attribute">mount</span></span> fstype=nfs, mount fstype=nfs4, mount fstype=nfsd, mount fstype=rpc_pipefs,</code> </pre> <br><p>  Apr√®s les modifications, red√©marrez le conteneur: </p><br><pre> <code class="hljs pgsql">pct shutdown <span class="hljs-number"><span class="hljs-number">101</span></span> pct <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span></code> </pre> <br><p>  Maintenant, connectons-nous: </p><br><pre> <code class="hljs perl">pct <span class="hljs-keyword"><span class="hljs-keyword">exec</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span> bash</code> </pre> <br><p>  Installez les mises √† jour et le <strong>serveur NFS</strong> : </p><br><pre> <code class="hljs powershell">apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> update apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> upgrade apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install nfs<span class="hljs-literal"><span class="hljs-literal">-kernel</span></span><span class="hljs-literal"><span class="hljs-literal">-server</span></span></code> </pre> <br><p>  Cr√©ez une <strong>exportation</strong> : </p><br><pre> <code class="hljs haskell"><span class="hljs-title"><span class="hljs-title">echo</span></span> '/<span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class"> *(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">rw</span></span></span><span class="hljs-class">,</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">no_root_squash</span></span></span><span class="hljs-class">,</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">no_subtree_check</span></span></span><span class="hljs-class">)' &gt;&gt; /etc/exports mkdir /</span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class"> exportfs -a</span></span></code> </pre> <br><h2 id="nastroyka-ha">  Configuration HA </h2><br><p>  Au moment de l'√©criture, proxmox <strong>HA-manager</strong> a un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">bogue</a> qui ne permet pas au conteneur HA de terminer son travail avec succ√®s, √† la suite de quoi les processus d' <strong>espace noyau</strong> du <strong>serveur nfs</strong> qui n'ont pas √©t√© compl√®tement tu√©s emp√™chent le p√©riph√©rique drbd de quitter le <strong>secondaire</strong> .  Si vous avez d√©j√† rencontr√© une telle situation, vous ne devez pas paniquer et simplement ex√©cuter <code>killall -9 nfsd</code> sur le n≈ìud o√π le conteneur a √©t√© lanc√©, puis le p√©riph√©rique drbd doit "lib√©rer" et il ira au <strong>secondaire</strong> . </p><br><p>  Pour corriger ce bogue, ex√©cutez les commandes suivantes sur tous les n≈ìuds: </p><br><pre> <code class="hljs powershell">sed <span class="hljs-literal"><span class="hljs-literal">-i</span></span> <span class="hljs-string"><span class="hljs-string">'s/forceStop =&gt; 1,/forceStop =&gt; 0,/'</span></span> /usr/share/perl5/PVE/HA/Resources/PVECT.pm systemctl restart pve<span class="hljs-literal"><span class="hljs-literal">-ha</span></span><span class="hljs-literal"><span class="hljs-literal">-lrm</span></span>.service</code> </pre> <br><p>  Nous pouvons maintenant passer √† la configuration du <strong>gestionnaire HA</strong> .  Cr√©ons un groupe HA distinct pour notre appareil: </p><br><pre> <code class="hljs powershell">ha<span class="hljs-literal"><span class="hljs-literal">-manager</span></span> groupadd nfs1 -<span class="hljs-literal"><span class="hljs-literal">-nodes</span></span> pve1,pve2,pve3 -<span class="hljs-literal"><span class="hljs-literal">-nofailback</span></span>=<span class="hljs-number"><span class="hljs-number">1</span></span> -<span class="hljs-literal"><span class="hljs-literal">-restricted</span></span>=<span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br><p>  Notre <strong>ressource</strong> ne fonctionnera que sur les n≈ìuds sp√©cifi√©s pour ce groupe.  Ajoutez notre conteneur √† ce groupe: </p><br><pre> <code class="hljs powershell">ha<span class="hljs-literal"><span class="hljs-literal">-manager</span></span> add ct:<span class="hljs-number"><span class="hljs-number">101</span></span> -<span class="hljs-literal"><span class="hljs-literal">-group</span></span>=nfs1 -<span class="hljs-literal"><span class="hljs-literal">-max_relocate</span></span>=<span class="hljs-number"><span class="hljs-number">3</span></span> -<span class="hljs-literal"><span class="hljs-literal">-max_restart</span></span>=<span class="hljs-number"><span class="hljs-number">3</span></span></code> </pre> <br><p>  C‚Äôest tout.  C'est simple, non? </p><br><p>  La <strong>boule nfs</strong> r√©sultante peut √™tre imm√©diatement connect√©e √† Proxmox pour stocker et ex√©cuter d'autres machines virtuelles et conteneurs. </p><br><h2 id="rekomendacii-i-tyuning">  Recommandations et r√©glages </h2><br><h5 id="drbd">  DRBD </h5><br><p>  Comme je l'ai not√© ci-dessus, il est toujours conseill√© d'utiliser un r√©seau distinct pour la r√©plication.  Il est fortement conseill√© d'utiliser des <strong>adaptateurs r√©seau 10 gigabits</strong> , sinon vous aurez une vitesse de port. <br>  Si la r√©plication semble assez lente, essayez certaines des options pour <strong>DRBD</strong> .  Voici la config, qui √† mon avis est optimale pour mon <strong>r√©seau 10G</strong> : </p><br><pre> <code class="hljs swift"># cat /etc/drbd.d/global_common.conf global { usage-<span class="hljs-built_in"><span class="hljs-built_in">count</span></span> yes; udev-always-use-vnr; } common { handlers { } startup { } options { } disk { <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-fill-target 10M; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-<span class="hljs-built_in"><span class="hljs-built_in">max</span></span>-rate 720M; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-plan-ahead <span class="hljs-number"><span class="hljs-number">10</span></span>; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-<span class="hljs-built_in"><span class="hljs-built_in">min</span></span>-rate 20M; } net { <span class="hljs-built_in"><span class="hljs-built_in">max</span></span>-buffers 36k; sndbuf-size 1024k; rcvbuf-size 2048k; } }</code> </pre> <br><p>  Vous pouvez obtenir plus d'informations sur chaque param√®tre dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation officielle DRBD.</a> </p><br><h5 id="nfs-server">  Serveur NFS </h5><br><p>  Pour acc√©l√©rer le fonctionnement du <strong>serveur NFS,</strong> il peut √™tre utile d'augmenter le nombre total d' <strong>instances en</strong> cours d'ex√©cution <strong>du</strong> serveur NFS.  Par d√©faut - <strong>8</strong> , personnellement, cela m'a aid√© √† augmenter ce nombre √† <strong>64</strong> . </p><br><p>  Pour ce faire, mettez √† jour le param√®tre <code>RPCNFSDCOUNT=64</code> dans <code>/etc/default/nfs-kernel-server</code> . <br>  Et red√©marrez les d√©mons: </p><br><pre> <code class="hljs pgsql">systemctl <span class="hljs-keyword"><span class="hljs-keyword">restart</span></span> nfs-utils systemctl <span class="hljs-keyword"><span class="hljs-keyword">restart</span></span> nfs-<span class="hljs-keyword"><span class="hljs-keyword">server</span></span></code> </pre> <br><h5 id="nfsv3-vs-nfsv4">  NFSv3 vs NFSv4 </h5><br><p>  Connaissez-vous la diff√©rence entre <strong>NFSv3</strong> et <strong>NFSv4</strong> ? </p><br><ul><li>  <strong>NFSv3</strong> est un <strong>protocole sans √©tat;</strong> en r√®gle g√©n√©rale, il tol√®re mieux les √©checs et r√©cup√®re plus rapidement. </li><li>  <strong>NFSv4</strong> est un <strong>protocole avec √©tat</strong> , il fonctionne plus rapidement et peut √™tre li√© √† certains ports TCP, mais en raison de la pr√©sence de l'√©tat, il est plus sensible aux pannes.  Il a √©galement la possibilit√© d'utiliser l'authentification √† l'aide de Kerberos et un tas d'autres fonctionnalit√©s int√©ressantes. </li></ul><br><p>  Cependant, lorsque vous ex√©cutez <code>showmount -e nfs_server</code> , le protocole NFSv3 est utilis√©.  Proxmox utilise √©galement NFSv3.  NFSv3 est √©galement couramment utilis√© pour organiser les machines de d√©marrage r√©seau. </p><br><p>  En g√©n√©ral, si vous n'avez aucune raison particuli√®re d'utiliser NFSv4, essayez d'utiliser NFSv3 car il est moins p√©nible pour les √©checs en raison de l'absence d'un √©tat en tant que tel. </p><br><p>  Vous pouvez monter la balle √† l'aide de NFSv3 en sp√©cifiant le param√®tre <code>-o vers=3</code> pour la commande de <strong>montage</strong> : </p><br><pre> <code class="bash hljs">mount -o vers=3 nfs_server:/share /mnt</code> </pre> <br><p>  Si vous le souhaitez, vous pouvez d√©sactiver NFSv4 pour le serveur, pour ce faire, ajoutez l'option <code>--no-nfs-version 4</code> √† la variable <code>--no-nfs-version 4</code> et red√©marrez le serveur, par exemple: </p><br><pre> <code class="bash hljs">RPCNFSDCOUNT=<span class="hljs-string"><span class="hljs-string">"64 --no-nfs-version 4"</span></span></code> </pre> <br><h2 id="iscsi-i-lvm">  iSCSI et LVM </h2><br><p>  De m√™me, un <strong>d√©mon tgt</strong> standard peut √™tre configur√© √† l'int√©rieur du conteneur, iSCSI produira des performances consid√©rablement plus √©lev√©es pour les op√©rations d'E / S et le conteneur fonctionnera plus facilement car le serveur tgt fonctionne compl√®tement dans l'espace utilisateur. </p><br><p>  En r√®gle g√©n√©rale, un <strong>LUN</strong> export√© est d√©coup√© en plusieurs morceaux √† l'aide de <strong>LVM</strong> .  Cependant, il y a plusieurs nuances √† consid√©rer, par exemple: comment les <strong>verrous</strong> LVM <strong>sont</strong> fournis pour partager un groupe export√© sur plusieurs h√¥tes. </p><br><p>  Peut-√™tre ces nuances et d'autres que je d√©crirai dans le <strong><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">prochain article</a></strong> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr417473/">https://habr.com/ru/post/fr417473/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es436926/index.html">H√©roes de la autenticaci√≥n de dos factores, o c√≥mo "caminar en la piel de los dem√°s"</a></li>
<li><a href="../es436928/index.html">WebRTC: a√∫n m√°s f√°cil (EasyRTC)</a></li>
<li><a href="../es436934/index.html">Matem√°ticas ingenuas: el motor de Mendocino y el teorema de Earnshaw</a></li>
<li><a href="../es436936/index.html">Dep√≥sito nuclear japon√©s en Primorye o el sitio de disposici√≥n de los submarinos nucleares de la Flota del Pac√≠fico</a></li>
<li><a href="../es436938/index.html">Guix es el sistema operativo m√°s avanzado.</a></li>
<li><a href="../fr417475/index.html">Codage d'effacement Glusterfs +: quand vous en avez besoin, pas cher et fiable</a></li>
<li><a href="../fr417477/index.html">Bureau chaud</a></li>
<li><a href="../fr417479/index.html">Concat√©nation plus rapide des cha√Ænes de bricolage dans Go</a></li>
<li><a href="../fr417481/index.html">√Ä propos des g√©n√©rateurs dans JavaScript ES6 et pourquoi il est facultatif de les √©tudier</a></li>
<li><a href="../fr417483/index.html">Comparaison des frameworks JS: React, Vue et Hyperapp</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>