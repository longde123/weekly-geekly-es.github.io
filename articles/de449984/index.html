<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🆎 🥗 👩🏽‍✈️ Google News und Leo Tolstoy: Visualisierung von Word2Vec-Worteinbettungen mit t-SNE 👨🏾‍🔬 👩‍👩‍👦 🥡</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Jeder nimmt Texte einzigartig wahr, unabhängig davon, ob diese Person Nachrichten im Internet oder weltbekannte klassische Romane liest. Dies gilt auc...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Google News und Leo Tolstoy: Visualisierung von Word2Vec-Worteinbettungen mit t-SNE</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/449984/"><img src="https://habrastorage.org/webt/6c/ux/7m/6cux7mvmp3phb8d8efjwmqrb_yc.gif"><br>  Jeder nimmt Texte einzigartig wahr, unabhängig davon, ob diese Person Nachrichten im Internet oder weltbekannte klassische Romane liest.  Dies gilt auch für eine Vielzahl von Algorithmen und Techniken des maschinellen Lernens, die Texte auf mathematischere Weise verstehen, nämlich unter Verwendung eines hochdimensionalen Vektorraums. <br><br>  Dieser Artikel befasst sich mit der Visualisierung hochdimensionaler Word2Vec-Worteinbettungen mit t-SNE.  Die Visualisierung kann hilfreich sein, um zu verstehen, wie Word2Vec funktioniert und wie Beziehungen zwischen Vektoren interpretiert werden, die aus Ihren Texten erfasst wurden, bevor sie in neuronalen Netzen oder anderen Algorithmen für maschinelles Lernen verwendet werden.  Als Trainingsdaten verwenden wir Artikel aus Google News und klassischen literarischen Werken von Leo Tolstoi, dem russischen Schriftsteller, der als einer der größten Autoren aller Zeiten gilt. <br><br>  Wir gehen die kurze Übersicht über den t-SNE-Algorithmus durch, gehen dann zur Berechnung der Worteinbettungen mit Word2Vec über und fahren schließlich mit der Visualisierung von Wortvektoren mit t-SNE im 2D- und 3D-Raum fort.  Wir werden unsere Skripte in Python mit Jupyter Notebook schreiben. <br><br><a name="habracut"></a><br><h1>  T-verteilte stochastische Nachbareinbettung </h1><br>  T-SNE ist ein Algorithmus für maschinelles Lernen zur Datenvisualisierung, der auf einer nichtlinearen Dimensionsreduktionstechnik basiert.  Die Grundidee von t-SNE besteht darin, den Dimensionsraum zu reduzieren und den relativen paarweisen Abstand zwischen Punkten einzuhalten.  Mit anderen Worten, der Algorithmus ordnet mehrdimensionale Daten zwei oder mehr Dimensionen zu, wobei Punkte, die anfangs weit voneinander entfernt waren, ebenfalls weit entfernt sind und nahe Punkte ebenfalls in nahe Punkte umgewandelt werden.  Es kann gesagt werden, dass t-SNE nach einer neuen Datendarstellung sucht, bei der die Nachbarschaftsbeziehungen erhalten bleiben.  Die detaillierte Beschreibung der gesamten t-SNE-Logik finden Sie im Originalartikel [1]. <br><br><h1>  Das Word2Vec-Modell </h1><br>  Zunächst sollten wir Vektordarstellungen von Wörtern erhalten.  Zu diesem Zweck habe ich Word2vec [2] ausgewählt, dh ein rechnerisch effizientes Vorhersagemodell zum Lernen mehrdimensionaler Worteinbettungen aus Rohtextdaten.  Das Schlüsselkonzept von Word2Vec besteht darin, Wörter, die im Trainingskorpus gemeinsame Kontexte haben, im Vergleich zu anderen in unmittelbarer Nähe im Vektorraum zu lokalisieren. <br><br>  Als Eingabedaten für die Visualisierung verwenden wir Artikel aus Google News und einige Romane von Leo Tolstoi.  Vorab trainierte Vektoren, die auf einem Teil des Google News-Datensatzes (ca. 100 Milliarden Wörter) trainiert wurden, wurden von Google auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der offiziellen Seite veröffentlicht</a> , daher werden wir sie verwenden. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gensim model = gensim.models.KeyedVectors.load_word2vec_format(<span class="hljs-string"><span class="hljs-string">'GoogleNews-vectors-negative300.bin'</span></span>, binary=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><br>  Zusätzlich zu dem vorgefertigten Modell werden wir ein weiteres Modell zu Tolstois Romanen unter Verwendung der Gensim [3] -Bibliothek trainieren.  Word2Vec verwendet Sätze als Eingabedaten und erzeugt Wortvektoren als Ausgabe.  Zunächst muss ein vorab trainierter Punkt-Satz-Tokenizer heruntergeladen werden, der einen Text in eine Liste von Sätzen unterteilt, wobei Abkürzungswörter, Kollokationen und Wörter berücksichtigt werden, die wahrscheinlich einen Anfang oder ein Ende von Sätzen angeben.  Standardmäßig enthält das NLTK-Datenpaket keinen vorab geschulten Punkt-Tokenizer für Russisch. Daher verwenden wir Modelle von Drittanbietern von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">github.com/mhq/train_punkt</a> . <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> codecs <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocess_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(text)</span></span></span><span class="hljs-function">:</span></span> text = re.sub(<span class="hljs-string"><span class="hljs-string">'[^a-zA-Z--1-9]+'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, text) text = re.sub(<span class="hljs-string"><span class="hljs-string">' +'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, text) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text.strip() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">prepare_for_w2v</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename_from, filename_to, lang)</span></span></span><span class="hljs-function">:</span></span> raw_text = codecs.open(filename_from, <span class="hljs-string"><span class="hljs-string">"r"</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'windows-1251'</span></span>).read() <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(filename_to, <span class="hljs-string"><span class="hljs-string">'w'</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> sentence <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> nltk.sent_tokenize(raw_text, lang): print(preprocess_text(sentence.lower()), file=f)</code> </pre><br><br>  In der Word2Vec-Trainingsphase wurden die folgenden Hyperparameter verwendet: <br><br><ul><li>  Die Dimensionalität des Merkmalsvektors beträgt 200. </li><li>  Der maximale Abstand zwischen analysierten Wörtern innerhalb eines Satzes beträgt 5. </li><li>  Ignoriert alle Wörter mit einer Gesamtfrequenz von weniger als 5 pro Korpus. </li></ul><br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> multiprocessing <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> gensim.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Word2Vec <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train_word2vec</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> data = gensim.models.word2vec.LineSentence(filename) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Word2Vec(data, size=<span class="hljs-number"><span class="hljs-number">200</span></span>, window=<span class="hljs-number"><span class="hljs-number">5</span></span>, min_count=<span class="hljs-number"><span class="hljs-number">5</span></span>, workers=multiprocessing.cpu_count())</code> </pre><br><br><h1>  Visualisierung von Worteinbettungen mit t-SNE </h1><br>  T-SNE ist sehr nützlich, wenn es notwendig ist, die Ähnlichkeit zwischen Objekten zu visualisieren, die sich im mehrdimensionalen Raum befinden.  Bei einem großen Datensatz wird es immer schwieriger, ein einfach zu lesendes t-SNE-Diagramm zu erstellen. Daher ist es üblich, Gruppen der ähnlichsten Wörter zu visualisieren. <br>  Lassen Sie uns einige Wörter aus dem Vokabular des vorgefertigten Google News-Modells auswählen und Wortvektoren für die Visualisierung vorbereiten. <br><br><pre> <code class="python hljs">keys = [<span class="hljs-string"><span class="hljs-string">'Paris'</span></span>, <span class="hljs-string"><span class="hljs-string">'Python'</span></span>, <span class="hljs-string"><span class="hljs-string">'Sunday'</span></span>, <span class="hljs-string"><span class="hljs-string">'Tolstoy'</span></span>, <span class="hljs-string"><span class="hljs-string">'Twitter'</span></span>, <span class="hljs-string"><span class="hljs-string">'bachelor'</span></span>, <span class="hljs-string"><span class="hljs-string">'delivery'</span></span>, <span class="hljs-string"><span class="hljs-string">'election'</span></span>, <span class="hljs-string"><span class="hljs-string">'expensive'</span></span>, <span class="hljs-string"><span class="hljs-string">'experience'</span></span>, <span class="hljs-string"><span class="hljs-string">'financial'</span></span>, <span class="hljs-string"><span class="hljs-string">'food'</span></span>, <span class="hljs-string"><span class="hljs-string">'iOS'</span></span>, <span class="hljs-string"><span class="hljs-string">'peace'</span></span>, <span class="hljs-string"><span class="hljs-string">'release'</span></span>, <span class="hljs-string"><span class="hljs-string">'war'</span></span>] embedding_clusters = [] word_clusters = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> keys: embeddings = [] words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> similar_word, _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> model.most_similar(word, topn=<span class="hljs-number"><span class="hljs-number">30</span></span>): words.append(similar_word) embeddings.append(model[similar_word]) embedding_clusters.append(embeddings) word_clusters.append(words)</code> </pre> <br><img src="https://habrastorage.org/webt/uc/1k/o6/uc1ko6efgx_d-wnbwolgdabifl8.gif"><br>  <i>Abb.</i>  <i>1. Die Auswirkung verschiedener Verwirrungswerte auf die Form von Wortclustern.</i> <br><br>  Als nächstes gehen wir zum faszinierenden Teil dieses Papiers über, der Konfiguration von t-SNE.  In diesem Abschnitt sollten wir uns auf die folgenden Hyperparameter konzentrieren. <br><br><ul><li>  <i>Die Anzahl der Komponenten</i> , dh die Dimension des Ausgaberaums. </li><li>  <i>Der Ratlosigkeitswert</i> , der im Zusammenhang mit t-SNE als glattes Maß für die effektive Anzahl von Nachbarn angesehen werden kann.  Dies hängt mit der Anzahl der nächsten Nachbarn zusammen, die bei vielen anderen vielfältigen Lernenden beschäftigt sind (siehe Abbildung oben).  Nach [1] wird empfohlen, einen Wert zwischen 5 und 50 zu wählen. </li><li>  <i>Die Art der anfänglichen Initialisierung</i> für Einbettungen. </li></ul><br><br><pre> <code class="python hljs">tsne_model_en_2d = TSNE(perplexity=<span class="hljs-number"><span class="hljs-number">15</span></span>, n_components=<span class="hljs-number"><span class="hljs-number">2</span></span>, init=<span class="hljs-string"><span class="hljs-string">'pca'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">3500</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">32</span></span>) embedding_clusters = np.array(embedding_clusters) n, m, k = embedding_clusters.shape embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, <span class="hljs-number"><span class="hljs-number">2</span></span>)</code> </pre> <br><br>  Es sollte erwähnt werden, dass t-SNE eine nicht konvexe Zielfunktion hat, die durch eine Gradientenabstiegsoptimierung mit zufälliger Initiierung minimiert wird, sodass unterschiedliche Läufe leicht unterschiedliche Ergebnisse liefern. <br><br>  Im Folgenden finden Sie ein Skript zum Erstellen eines 2D-Streudiagramms mit Matplotlib, einer der beliebtesten Bibliotheken für die Datenvisualisierung in Python. <br><br><img src="https://habrastorage.org/webt/34/9y/7h/349y7hxuanvvttqfxkpb48j4n-q.png"><br>  <i>Abb.</i>  <i>2. Cluster ähnlicher Wörter aus Google News (Präplexität = 15).</i> <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.manifold <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> TSNE <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.cm <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> cm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np % matplotlib inline <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tsne_plot_similar_words</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(labels, embedding_clusters, word_clusters, a=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.7</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">16</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) colors = cm.rainbow(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, len(labels))) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> label, embeddings, words, color <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(labels, embedding_clusters, word_clusters, colors): x = embeddings[:,<span class="hljs-number"><span class="hljs-number">0</span></span>] y = embeddings[:,<span class="hljs-number"><span class="hljs-number">1</span></span>] plt.scatter(x, y, c=color, alpha=a, label=label) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(words): plt.annotate(word, alpha=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, xy=(x[i], y[i]), xytext=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), textcoords=<span class="hljs-string"><span class="hljs-string">'offset points'</span></span>, ha=<span class="hljs-string"><span class="hljs-string">'right'</span></span>, va=<span class="hljs-string"><span class="hljs-string">'bottom'</span></span>, size=<span class="hljs-number"><span class="hljs-number">8</span></span>) plt.legend(loc=<span class="hljs-number"><span class="hljs-number">4</span></span>) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) plt.savefig(<span class="hljs-string"><span class="hljs-string">"f/.png"</span></span>, format=<span class="hljs-string"><span class="hljs-string">'png'</span></span>, dpi=<span class="hljs-number"><span class="hljs-number">150</span></span>, bbox_inches=<span class="hljs-string"><span class="hljs-string">'tight'</span></span>) plt.show() tsne_plot_similar_words(keys, embeddings_en_2d, word_clusters)</code> </pre> <br><br>  In einigen Fällen kann es nützlich sein, alle Wortvektoren gleichzeitig zu zeichnen, um das gesamte Bild zu sehen.  Lassen Sie uns nun Anna Karenina analysieren, einen epischen Roman aus Leidenschaft, Intrigen, Tragödien und Erlösung. <br><br><pre> <code class="python hljs">prepare_for_w2v(<span class="hljs-string"><span class="hljs-string">'data/Anna Karenina by Leo Tolstoy (ru).txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'train_anna_karenina_ru.txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'russian'</span></span>) model_ak = train_word2vec(<span class="hljs-string"><span class="hljs-string">'train_anna_karenina_ru.txt'</span></span>) words = [] embeddings = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> list(model_ak.wv.vocab): embeddings.append(model_ak.wv[word]) words.append(word) tsne_ak_2d = TSNE(n_components=<span class="hljs-number"><span class="hljs-number">2</span></span>, init=<span class="hljs-string"><span class="hljs-string">'pca'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">3500</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">32</span></span>) embeddings_ak_2d = tsne_ak_2d.fit_transform(embeddings)</code> </pre><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tsne_plot_2d</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(label, embeddings, words=[], a=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">16</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) colors = cm.rainbow(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) x = embeddings[:,<span class="hljs-number"><span class="hljs-number">0</span></span>] y = embeddings[:,<span class="hljs-number"><span class="hljs-number">1</span></span>] plt.scatter(x, y, c=colors, alpha=a, label=label) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(words): plt.annotate(word, alpha=<span class="hljs-number"><span class="hljs-number">0.3</span></span>, xy=(x[i], y[i]), xytext=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), textcoords=<span class="hljs-string"><span class="hljs-string">'offset points'</span></span>, ha=<span class="hljs-string"><span class="hljs-string">'right'</span></span>, va=<span class="hljs-string"><span class="hljs-string">'bottom'</span></span>, size=<span class="hljs-number"><span class="hljs-number">10</span></span>) plt.legend(loc=<span class="hljs-number"><span class="hljs-number">4</span></span>) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) plt.savefig(<span class="hljs-string"><span class="hljs-string">"hhh.png"</span></span>, format=<span class="hljs-string"><span class="hljs-string">'png'</span></span>, dpi=<span class="hljs-number"><span class="hljs-number">150</span></span>, bbox_inches=<span class="hljs-string"><span class="hljs-string">'tight'</span></span>) plt.show() tsne_plot_2d(<span class="hljs-string"><span class="hljs-string">'Anna Karenina by Leo Tolstoy'</span></span>, embeddings_ak_2d, a=<span class="hljs-number"><span class="hljs-number">0.1</span></span>)</code> </pre> <br><img src="https://habrastorage.org/webt/j5/hn/82/j5hn8285nih2kwlop_badd2lzlk.png"><br><br><img src="https://habrastorage.org/webt/x6/jc/i7/x6jci7vka7-efczouqxpiqueisq.png"><br>  <i>Abb.</i>  <i>3. Visualisierung des auf Anna Karenina trainierten Word2Vec-Modells.</i> <br><br>  Das gesamte Bild kann noch informativer sein, wenn wir anfängliche Einbettungen im 3D-Raum abbilden.  In dieser Zeit werfen wir einen Blick auf Krieg und Frieden, einen der wichtigsten Romane der Weltliteratur und eine der größten literarischen Errungenschaften Tolstois. <br><br><pre> <code class="python hljs">prepare_for_w2v(<span class="hljs-string"><span class="hljs-string">'data/War and Peace by Leo Tolstoy (ru).txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'train_war_and_peace_ru.txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'russian'</span></span>) model_wp = train_word2vec(<span class="hljs-string"><span class="hljs-string">'train_war_and_peace_ru.txt'</span></span>) words_wp = [] embeddings_wp = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> list(model_wp.wv.vocab): embeddings_wp.append(model_wp.wv[word]) words_wp.append(word) tsne_wp_3d = TSNE(perplexity=<span class="hljs-number"><span class="hljs-number">30</span></span>, n_components=<span class="hljs-number"><span class="hljs-number">3</span></span>, init=<span class="hljs-string"><span class="hljs-string">'pca'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">3500</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">12</span></span>) embeddings_wp_3d = tsne_wp_3d.fit_transform(embeddings_wp)</code> </pre><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mpl_toolkits.mplot3d <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Axes3D <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tsne_plot_3d</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(title, label, embeddings, a=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> fig = plt.figure() ax = Axes3D(fig) colors = cm.rainbow(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) plt.scatter(embeddings[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], embeddings[:, <span class="hljs-number"><span class="hljs-number">1</span></span>], embeddings[:, <span class="hljs-number"><span class="hljs-number">2</span></span>], c=colors, alpha=a, label=label) plt.legend(loc=<span class="hljs-number"><span class="hljs-number">4</span></span>) plt.title(title) plt.show() tsne_plot_3d(<span class="hljs-string"><span class="hljs-string">'Visualizing Embeddings using t-SNE'</span></span>, <span class="hljs-string"><span class="hljs-string">'War and Peace'</span></span>, embeddings_wp_3d, a=<span class="hljs-number"><span class="hljs-number">0.1</span></span>)</code> </pre> <br><img src="https://habrastorage.org/webt/ch/jm/os/chjmos082qn6ktw9afdeavbz6_a.png"><br>  <i>Abb.</i>  <i>4. Visualisierung des auf Krieg und Frieden trainierten Word2Vec-Modells.</i> <br><br><h1>  Die Ergebnisse </h1><br>  So sehen Texte aus der Perspektive von Word2Vec und t-SNE aus.  Wir haben ein recht informatives Diagramm für ähnliche Wörter aus Google News und zwei Diagramme für Tolstois Romane erstellt.  Noch eine Sache, GIFs!  GIFs sind fantastisch, aber das Plotten von GIFs entspricht fast dem Plotten normaler Diagramme.  Deshalb habe ich beschlossen, sie im Artikel nicht zu erwähnen, aber den Code für die Generierung von Animationen finden Sie in den Quellen. <br><br>  Der Quellcode ist bei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Github erhältlich</a> . <br><br>  Der Artikel wurde ursprünglich in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Towards Data Science veröffentlicht</a> . <br><br><h1>  Referenzen </h1><br><ol><li>  L. Maate und G. Hinton, "Visualisierung von Daten mit t-SNE", Journal of Machine Learning Research, vol.  9, pp.  2579-2605, 2008. </li><li>  T. Mikolov, I. Sutskever, K. Chen, G. Corrado und J. Dean, „Verteilte Darstellungen von Wörtern und Phrasen und ihre Zusammensetzung“, Advances in Neural Information Processing Systems, pp.  3111-3119, 2013. </li><li>  R. Rehurek und P. Sojka, „Software-Framework für die Themenmodellierung mit großen Korpora“, Proceedings des LREC 2010-Workshops zu neuen Herausforderungen für NLP-Frameworks, 2010. </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de449984/">https://habr.com/ru/post/de449984/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de449970/index.html">Poste unter der schwarzen Flagge oder Da ich deinen Videokurs nicht auf den Tracker gesetzt habe</a></li>
<li><a href="../de449972/index.html">Wie kann man Pools schnell in den Upstream injizieren?</a></li>
<li><a href="../de449974/index.html">Netramesh - leichte Service-Mesh-Lösung</a></li>
<li><a href="../de449976/index.html">Assoziative Multithread-Container in C ++. Yandex-Bericht</a></li>
<li><a href="../de449978/index.html">Igor Antarov vom Moskauer Tesla Club kämpft mit 20 Mythen über Tesla und Elektroautos</a></li>
<li><a href="../de449986/index.html">Blockchain: Was sollen wir einen Fall bauen?</a></li>
<li><a href="../de449990/index.html">Wie kann man Latex, Formeln und Habr finden?</a></li>
<li><a href="../de449992/index.html">NodeMCU Simple Driver Model (SDM) Schaufenster: Dynamische Benutzeroberfläche</a></li>
<li><a href="../de449996/index.html">Den FFT-Algorithmus verstehen</a></li>
<li><a href="../de449998/index.html">FAQ: Was ein reisender Geek über Impfungen wissen muss, bevor er reist</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>