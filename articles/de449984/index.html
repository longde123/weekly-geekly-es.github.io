<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üÜé ü•ó üë©üèΩ‚Äç‚úàÔ∏è Google News und Leo Tolstoy: Visualisierung von Word2Vec-Worteinbettungen mit t-SNE üë®üèæ‚Äçüî¨ üë©‚Äçüë©‚Äçüë¶ ü•°</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Jeder nimmt Texte einzigartig wahr, unabh√§ngig davon, ob diese Person Nachrichten im Internet oder weltbekannte klassische Romane liest. Dies gilt auc...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Google News und Leo Tolstoy: Visualisierung von Word2Vec-Worteinbettungen mit t-SNE</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/449984/"><img src="https://habrastorage.org/webt/6c/ux/7m/6cux7mvmp3phb8d8efjwmqrb_yc.gif"><br>  Jeder nimmt Texte einzigartig wahr, unabh√§ngig davon, ob diese Person Nachrichten im Internet oder weltbekannte klassische Romane liest.  Dies gilt auch f√ºr eine Vielzahl von Algorithmen und Techniken des maschinellen Lernens, die Texte auf mathematischere Weise verstehen, n√§mlich unter Verwendung eines hochdimensionalen Vektorraums. <br><br>  Dieser Artikel befasst sich mit der Visualisierung hochdimensionaler Word2Vec-Worteinbettungen mit t-SNE.  Die Visualisierung kann hilfreich sein, um zu verstehen, wie Word2Vec funktioniert und wie Beziehungen zwischen Vektoren interpretiert werden, die aus Ihren Texten erfasst wurden, bevor sie in neuronalen Netzen oder anderen Algorithmen f√ºr maschinelles Lernen verwendet werden.  Als Trainingsdaten verwenden wir Artikel aus Google News und klassischen literarischen Werken von Leo Tolstoi, dem russischen Schriftsteller, der als einer der gr√∂√üten Autoren aller Zeiten gilt. <br><br>  Wir gehen die kurze √úbersicht √ºber den t-SNE-Algorithmus durch, gehen dann zur Berechnung der Worteinbettungen mit Word2Vec √ºber und fahren schlie√ülich mit der Visualisierung von Wortvektoren mit t-SNE im 2D- und 3D-Raum fort.  Wir werden unsere Skripte in Python mit Jupyter Notebook schreiben. <br><br><a name="habracut"></a><br><h1>  T-verteilte stochastische Nachbareinbettung </h1><br>  T-SNE ist ein Algorithmus f√ºr maschinelles Lernen zur Datenvisualisierung, der auf einer nichtlinearen Dimensionsreduktionstechnik basiert.  Die Grundidee von t-SNE besteht darin, den Dimensionsraum zu reduzieren und den relativen paarweisen Abstand zwischen Punkten einzuhalten.  Mit anderen Worten, der Algorithmus ordnet mehrdimensionale Daten zwei oder mehr Dimensionen zu, wobei Punkte, die anfangs weit voneinander entfernt waren, ebenfalls weit entfernt sind und nahe Punkte ebenfalls in nahe Punkte umgewandelt werden.  Es kann gesagt werden, dass t-SNE nach einer neuen Datendarstellung sucht, bei der die Nachbarschaftsbeziehungen erhalten bleiben.  Die detaillierte Beschreibung der gesamten t-SNE-Logik finden Sie im Originalartikel [1]. <br><br><h1>  Das Word2Vec-Modell </h1><br>  Zun√§chst sollten wir Vektordarstellungen von W√∂rtern erhalten.  Zu diesem Zweck habe ich Word2vec [2] ausgew√§hlt, dh ein rechnerisch effizientes Vorhersagemodell zum Lernen mehrdimensionaler Worteinbettungen aus Rohtextdaten.  Das Schl√ºsselkonzept von Word2Vec besteht darin, W√∂rter, die im Trainingskorpus gemeinsame Kontexte haben, im Vergleich zu anderen in unmittelbarer N√§he im Vektorraum zu lokalisieren. <br><br>  Als Eingabedaten f√ºr die Visualisierung verwenden wir Artikel aus Google News und einige Romane von Leo Tolstoi.  Vorab trainierte Vektoren, die auf einem Teil des Google News-Datensatzes (ca. 100 Milliarden W√∂rter) trainiert wurden, wurden von Google auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der offiziellen Seite ver√∂ffentlicht</a> , daher werden wir sie verwenden. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gensim model = gensim.models.KeyedVectors.load_word2vec_format(<span class="hljs-string"><span class="hljs-string">'GoogleNews-vectors-negative300.bin'</span></span>, binary=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><br>  Zus√§tzlich zu dem vorgefertigten Modell werden wir ein weiteres Modell zu Tolstois Romanen unter Verwendung der Gensim [3] -Bibliothek trainieren.  Word2Vec verwendet S√§tze als Eingabedaten und erzeugt Wortvektoren als Ausgabe.  Zun√§chst muss ein vorab trainierter Punkt-Satz-Tokenizer heruntergeladen werden, der einen Text in eine Liste von S√§tzen unterteilt, wobei Abk√ºrzungsw√∂rter, Kollokationen und W√∂rter ber√ºcksichtigt werden, die wahrscheinlich einen Anfang oder ein Ende von S√§tzen angeben.  Standardm√§√üig enth√§lt das NLTK-Datenpaket keinen vorab geschulten Punkt-Tokenizer f√ºr Russisch. Daher verwenden wir Modelle von Drittanbietern von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">github.com/mhq/train_punkt</a> . <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> codecs <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocess_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(text)</span></span></span><span class="hljs-function">:</span></span> text = re.sub(<span class="hljs-string"><span class="hljs-string">'[^a-zA-Z--1-9]+'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, text) text = re.sub(<span class="hljs-string"><span class="hljs-string">' +'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, text) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text.strip() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">prepare_for_w2v</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename_from, filename_to, lang)</span></span></span><span class="hljs-function">:</span></span> raw_text = codecs.open(filename_from, <span class="hljs-string"><span class="hljs-string">"r"</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'windows-1251'</span></span>).read() <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(filename_to, <span class="hljs-string"><span class="hljs-string">'w'</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> sentence <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> nltk.sent_tokenize(raw_text, lang): print(preprocess_text(sentence.lower()), file=f)</code> </pre><br><br>  In der Word2Vec-Trainingsphase wurden die folgenden Hyperparameter verwendet: <br><br><ul><li>  Die Dimensionalit√§t des Merkmalsvektors betr√§gt 200. </li><li>  Der maximale Abstand zwischen analysierten W√∂rtern innerhalb eines Satzes betr√§gt 5. </li><li>  Ignoriert alle W√∂rter mit einer Gesamtfrequenz von weniger als 5 pro Korpus. </li></ul><br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> multiprocessing <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> gensim.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Word2Vec <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train_word2vec</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> data = gensim.models.word2vec.LineSentence(filename) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Word2Vec(data, size=<span class="hljs-number"><span class="hljs-number">200</span></span>, window=<span class="hljs-number"><span class="hljs-number">5</span></span>, min_count=<span class="hljs-number"><span class="hljs-number">5</span></span>, workers=multiprocessing.cpu_count())</code> </pre><br><br><h1>  Visualisierung von Worteinbettungen mit t-SNE </h1><br>  T-SNE ist sehr n√ºtzlich, wenn es notwendig ist, die √Ñhnlichkeit zwischen Objekten zu visualisieren, die sich im mehrdimensionalen Raum befinden.  Bei einem gro√üen Datensatz wird es immer schwieriger, ein einfach zu lesendes t-SNE-Diagramm zu erstellen. Daher ist es √ºblich, Gruppen der √§hnlichsten W√∂rter zu visualisieren. <br>  Lassen Sie uns einige W√∂rter aus dem Vokabular des vorgefertigten Google News-Modells ausw√§hlen und Wortvektoren f√ºr die Visualisierung vorbereiten. <br><br><pre> <code class="python hljs">keys = [<span class="hljs-string"><span class="hljs-string">'Paris'</span></span>, <span class="hljs-string"><span class="hljs-string">'Python'</span></span>, <span class="hljs-string"><span class="hljs-string">'Sunday'</span></span>, <span class="hljs-string"><span class="hljs-string">'Tolstoy'</span></span>, <span class="hljs-string"><span class="hljs-string">'Twitter'</span></span>, <span class="hljs-string"><span class="hljs-string">'bachelor'</span></span>, <span class="hljs-string"><span class="hljs-string">'delivery'</span></span>, <span class="hljs-string"><span class="hljs-string">'election'</span></span>, <span class="hljs-string"><span class="hljs-string">'expensive'</span></span>, <span class="hljs-string"><span class="hljs-string">'experience'</span></span>, <span class="hljs-string"><span class="hljs-string">'financial'</span></span>, <span class="hljs-string"><span class="hljs-string">'food'</span></span>, <span class="hljs-string"><span class="hljs-string">'iOS'</span></span>, <span class="hljs-string"><span class="hljs-string">'peace'</span></span>, <span class="hljs-string"><span class="hljs-string">'release'</span></span>, <span class="hljs-string"><span class="hljs-string">'war'</span></span>] embedding_clusters = [] word_clusters = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> keys: embeddings = [] words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> similar_word, _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> model.most_similar(word, topn=<span class="hljs-number"><span class="hljs-number">30</span></span>): words.append(similar_word) embeddings.append(model[similar_word]) embedding_clusters.append(embeddings) word_clusters.append(words)</code> </pre> <br><img src="https://habrastorage.org/webt/uc/1k/o6/uc1ko6efgx_d-wnbwolgdabifl8.gif"><br>  <i>Abb.</i>  <i>1. Die Auswirkung verschiedener Verwirrungswerte auf die Form von Wortclustern.</i> <br><br>  Als n√§chstes gehen wir zum faszinierenden Teil dieses Papiers √ºber, der Konfiguration von t-SNE.  In diesem Abschnitt sollten wir uns auf die folgenden Hyperparameter konzentrieren. <br><br><ul><li>  <i>Die Anzahl der Komponenten</i> , dh die Dimension des Ausgaberaums. </li><li>  <i>Der Ratlosigkeitswert</i> , der im Zusammenhang mit t-SNE als glattes Ma√ü f√ºr die effektive Anzahl von Nachbarn angesehen werden kann.  Dies h√§ngt mit der Anzahl der n√§chsten Nachbarn zusammen, die bei vielen anderen vielf√§ltigen Lernenden besch√§ftigt sind (siehe Abbildung oben).  Nach [1] wird empfohlen, einen Wert zwischen 5 und 50 zu w√§hlen. </li><li>  <i>Die Art der anf√§nglichen Initialisierung</i> f√ºr Einbettungen. </li></ul><br><br><pre> <code class="python hljs">tsne_model_en_2d = TSNE(perplexity=<span class="hljs-number"><span class="hljs-number">15</span></span>, n_components=<span class="hljs-number"><span class="hljs-number">2</span></span>, init=<span class="hljs-string"><span class="hljs-string">'pca'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">3500</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">32</span></span>) embedding_clusters = np.array(embedding_clusters) n, m, k = embedding_clusters.shape embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, <span class="hljs-number"><span class="hljs-number">2</span></span>)</code> </pre> <br><br>  Es sollte erw√§hnt werden, dass t-SNE eine nicht konvexe Zielfunktion hat, die durch eine Gradientenabstiegsoptimierung mit zuf√§lliger Initiierung minimiert wird, sodass unterschiedliche L√§ufe leicht unterschiedliche Ergebnisse liefern. <br><br>  Im Folgenden finden Sie ein Skript zum Erstellen eines 2D-Streudiagramms mit Matplotlib, einer der beliebtesten Bibliotheken f√ºr die Datenvisualisierung in Python. <br><br><img src="https://habrastorage.org/webt/34/9y/7h/349y7hxuanvvttqfxkpb48j4n-q.png"><br>  <i>Abb.</i>  <i>2. Cluster √§hnlicher W√∂rter aus Google News (Pr√§plexit√§t = 15).</i> <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.manifold <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> TSNE <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.cm <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> cm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np % matplotlib inline <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tsne_plot_similar_words</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(labels, embedding_clusters, word_clusters, a=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.7</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">16</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) colors = cm.rainbow(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, len(labels))) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> label, embeddings, words, color <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(labels, embedding_clusters, word_clusters, colors): x = embeddings[:,<span class="hljs-number"><span class="hljs-number">0</span></span>] y = embeddings[:,<span class="hljs-number"><span class="hljs-number">1</span></span>] plt.scatter(x, y, c=color, alpha=a, label=label) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(words): plt.annotate(word, alpha=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, xy=(x[i], y[i]), xytext=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), textcoords=<span class="hljs-string"><span class="hljs-string">'offset points'</span></span>, ha=<span class="hljs-string"><span class="hljs-string">'right'</span></span>, va=<span class="hljs-string"><span class="hljs-string">'bottom'</span></span>, size=<span class="hljs-number"><span class="hljs-number">8</span></span>) plt.legend(loc=<span class="hljs-number"><span class="hljs-number">4</span></span>) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) plt.savefig(<span class="hljs-string"><span class="hljs-string">"f/.png"</span></span>, format=<span class="hljs-string"><span class="hljs-string">'png'</span></span>, dpi=<span class="hljs-number"><span class="hljs-number">150</span></span>, bbox_inches=<span class="hljs-string"><span class="hljs-string">'tight'</span></span>) plt.show() tsne_plot_similar_words(keys, embeddings_en_2d, word_clusters)</code> </pre> <br><br>  In einigen F√§llen kann es n√ºtzlich sein, alle Wortvektoren gleichzeitig zu zeichnen, um das gesamte Bild zu sehen.  Lassen Sie uns nun Anna Karenina analysieren, einen epischen Roman aus Leidenschaft, Intrigen, Trag√∂dien und Erl√∂sung. <br><br><pre> <code class="python hljs">prepare_for_w2v(<span class="hljs-string"><span class="hljs-string">'data/Anna Karenina by Leo Tolstoy (ru).txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'train_anna_karenina_ru.txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'russian'</span></span>) model_ak = train_word2vec(<span class="hljs-string"><span class="hljs-string">'train_anna_karenina_ru.txt'</span></span>) words = [] embeddings = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> list(model_ak.wv.vocab): embeddings.append(model_ak.wv[word]) words.append(word) tsne_ak_2d = TSNE(n_components=<span class="hljs-number"><span class="hljs-number">2</span></span>, init=<span class="hljs-string"><span class="hljs-string">'pca'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">3500</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">32</span></span>) embeddings_ak_2d = tsne_ak_2d.fit_transform(embeddings)</code> </pre><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tsne_plot_2d</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(label, embeddings, words=[], a=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">16</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) colors = cm.rainbow(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) x = embeddings[:,<span class="hljs-number"><span class="hljs-number">0</span></span>] y = embeddings[:,<span class="hljs-number"><span class="hljs-number">1</span></span>] plt.scatter(x, y, c=colors, alpha=a, label=label) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(words): plt.annotate(word, alpha=<span class="hljs-number"><span class="hljs-number">0.3</span></span>, xy=(x[i], y[i]), xytext=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), textcoords=<span class="hljs-string"><span class="hljs-string">'offset points'</span></span>, ha=<span class="hljs-string"><span class="hljs-string">'right'</span></span>, va=<span class="hljs-string"><span class="hljs-string">'bottom'</span></span>, size=<span class="hljs-number"><span class="hljs-number">10</span></span>) plt.legend(loc=<span class="hljs-number"><span class="hljs-number">4</span></span>) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) plt.savefig(<span class="hljs-string"><span class="hljs-string">"hhh.png"</span></span>, format=<span class="hljs-string"><span class="hljs-string">'png'</span></span>, dpi=<span class="hljs-number"><span class="hljs-number">150</span></span>, bbox_inches=<span class="hljs-string"><span class="hljs-string">'tight'</span></span>) plt.show() tsne_plot_2d(<span class="hljs-string"><span class="hljs-string">'Anna Karenina by Leo Tolstoy'</span></span>, embeddings_ak_2d, a=<span class="hljs-number"><span class="hljs-number">0.1</span></span>)</code> </pre> <br><img src="https://habrastorage.org/webt/j5/hn/82/j5hn8285nih2kwlop_badd2lzlk.png"><br><br><img src="https://habrastorage.org/webt/x6/jc/i7/x6jci7vka7-efczouqxpiqueisq.png"><br>  <i>Abb.</i>  <i>3. Visualisierung des auf Anna Karenina trainierten Word2Vec-Modells.</i> <br><br>  Das gesamte Bild kann noch informativer sein, wenn wir anf√§ngliche Einbettungen im 3D-Raum abbilden.  In dieser Zeit werfen wir einen Blick auf Krieg und Frieden, einen der wichtigsten Romane der Weltliteratur und eine der gr√∂√üten literarischen Errungenschaften Tolstois. <br><br><pre> <code class="python hljs">prepare_for_w2v(<span class="hljs-string"><span class="hljs-string">'data/War and Peace by Leo Tolstoy (ru).txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'train_war_and_peace_ru.txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'russian'</span></span>) model_wp = train_word2vec(<span class="hljs-string"><span class="hljs-string">'train_war_and_peace_ru.txt'</span></span>) words_wp = [] embeddings_wp = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> list(model_wp.wv.vocab): embeddings_wp.append(model_wp.wv[word]) words_wp.append(word) tsne_wp_3d = TSNE(perplexity=<span class="hljs-number"><span class="hljs-number">30</span></span>, n_components=<span class="hljs-number"><span class="hljs-number">3</span></span>, init=<span class="hljs-string"><span class="hljs-string">'pca'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">3500</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">12</span></span>) embeddings_wp_3d = tsne_wp_3d.fit_transform(embeddings_wp)</code> </pre><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mpl_toolkits.mplot3d <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Axes3D <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tsne_plot_3d</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(title, label, embeddings, a=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> fig = plt.figure() ax = Axes3D(fig) colors = cm.rainbow(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) plt.scatter(embeddings[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], embeddings[:, <span class="hljs-number"><span class="hljs-number">1</span></span>], embeddings[:, <span class="hljs-number"><span class="hljs-number">2</span></span>], c=colors, alpha=a, label=label) plt.legend(loc=<span class="hljs-number"><span class="hljs-number">4</span></span>) plt.title(title) plt.show() tsne_plot_3d(<span class="hljs-string"><span class="hljs-string">'Visualizing Embeddings using t-SNE'</span></span>, <span class="hljs-string"><span class="hljs-string">'War and Peace'</span></span>, embeddings_wp_3d, a=<span class="hljs-number"><span class="hljs-number">0.1</span></span>)</code> </pre> <br><img src="https://habrastorage.org/webt/ch/jm/os/chjmos082qn6ktw9afdeavbz6_a.png"><br>  <i>Abb.</i>  <i>4. Visualisierung des auf Krieg und Frieden trainierten Word2Vec-Modells.</i> <br><br><h1>  Die Ergebnisse </h1><br>  So sehen Texte aus der Perspektive von Word2Vec und t-SNE aus.  Wir haben ein recht informatives Diagramm f√ºr √§hnliche W√∂rter aus Google News und zwei Diagramme f√ºr Tolstois Romane erstellt.  Noch eine Sache, GIFs!  GIFs sind fantastisch, aber das Plotten von GIFs entspricht fast dem Plotten normaler Diagramme.  Deshalb habe ich beschlossen, sie im Artikel nicht zu erw√§hnen, aber den Code f√ºr die Generierung von Animationen finden Sie in den Quellen. <br><br>  Der Quellcode ist bei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Github erh√§ltlich</a> . <br><br>  Der Artikel wurde urspr√ºnglich in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Towards Data Science ver√∂ffentlicht</a> . <br><br><h1>  Referenzen </h1><br><ol><li>  L. Maate und G. Hinton, "Visualisierung von Daten mit t-SNE", Journal of Machine Learning Research, vol.  9, pp.  2579-2605, 2008. </li><li>  T. Mikolov, I. Sutskever, K. Chen, G. Corrado und J. Dean, ‚ÄûVerteilte Darstellungen von W√∂rtern und Phrasen und ihre Zusammensetzung‚Äú, Advances in Neural Information Processing Systems, pp.  3111-3119, 2013. </li><li>  R. Rehurek und P. Sojka, ‚ÄûSoftware-Framework f√ºr die Themenmodellierung mit gro√üen Korpora‚Äú, Proceedings des LREC 2010-Workshops zu neuen Herausforderungen f√ºr NLP-Frameworks, 2010. </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de449984/">https://habr.com/ru/post/de449984/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de449970/index.html">Poste unter der schwarzen Flagge oder Da ich deinen Videokurs nicht auf den Tracker gesetzt habe</a></li>
<li><a href="../de449972/index.html">Wie kann man Pools schnell in den Upstream injizieren?</a></li>
<li><a href="../de449974/index.html">Netramesh - leichte Service-Mesh-L√∂sung</a></li>
<li><a href="../de449976/index.html">Assoziative Multithread-Container in C ++. Yandex-Bericht</a></li>
<li><a href="../de449978/index.html">Igor Antarov vom Moskauer Tesla Club k√§mpft mit 20 Mythen √ºber Tesla und Elektroautos</a></li>
<li><a href="../de449986/index.html">Blockchain: Was sollen wir einen Fall bauen?</a></li>
<li><a href="../de449990/index.html">Wie kann man Latex, Formeln und Habr finden?</a></li>
<li><a href="../de449992/index.html">NodeMCU Simple Driver Model (SDM) Schaufenster: Dynamische Benutzeroberfl√§che</a></li>
<li><a href="../de449996/index.html">Den FFT-Algorithmus verstehen</a></li>
<li><a href="../de449998/index.html">FAQ: Was ein reisender Geek √ºber Impfungen wissen muss, bevor er reist</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>