<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèæ‚Äçü§ù‚Äçüë®üèº üéÜ üéì Lebenszyklus ML üóΩ üî± üõ∞Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In einer echten ML-Implementierung macht das Lernen selbst ein Viertel des Aufwands aus. Die verbleibenden drei Viertel sind Datenaufbereitung durch S...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Lebenszyklus ML</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/455648/">  In einer echten ML-Implementierung macht das Lernen selbst ein Viertel des Aufwands aus.  Die verbleibenden drei Viertel sind Datenaufbereitung durch Schmerz und B√ºrokratie, eine komplexe Bereitstellung, die h√§ufig in einem geschlossenen Kreislauf ohne Internetzugang, Einrichtung der Infrastruktur, Testen und √úberwachen erfolgt.  Dokumente auf Hunderten von Bl√§ttern, manueller Modus, Modellversionskonflikte, Open Source und hartes Unternehmertum - all dies erwartet einen Datenwissenschaftler.  Aber er ist nicht an solchen ‚Äûlangweiligen‚Äú betrieblichen Problemen interessiert, er m√∂chte einen Algorithmus entwickeln, hohe Qualit√§t erreichen, etwas zur√ºckgeben und sich nicht mehr erinnern. <br><br>  Vielleicht ist ML irgendwo einfacher, einfacher, schneller und mit einem Knopf implementiert, aber wir haben solche Beispiele nicht gesehen.  Alles, was oben steht, ist die Erfahrung von Front Tier in den Bereichen Fintech und Telekommunikation.  Sergey Vinogradov, Experte f√ºr die Architektur hoch belasteter Systeme, f√ºr gro√üe Speicher und f√ºr die Analyse schwerer Datenmengen, sprach bei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">HighLoad ++</a> √ºber ihn. <br><br><img src="https://habrastorage.org/webt/ss/7n/cm/ss7ncmtdsij0wncwt-uuzxneit8.jpeg"><br><a name="habracut"></a><br><h2>  Modelllebenszyklus </h2><br>  Normalerweise besteht der Lebenszyklus in unserem Fachgebiet aus drei Teilen.  Im ersten <strong>Fall kommt eine Aufgabe aus dem Gesch√§ft</strong> .  Im zweiten <strong>Schritt bereiten</strong> ein <strong>Dateningenieur und / oder ein Datenwissenschaftler Daten vor</strong> und erstellen ein Modell.  Im dritten Teil beginnt das <strong>Chaos</strong> .  In den letzten beiden F√§llen treten unterschiedliche interessante Situationen auf. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/7GM9ac6ojtw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h3>  Allesk√∂nner </h3><br>  Die erste h√§ufige Situation ist, dass ein Datenwissenschaftler oder Dateningenieur Zugriff auf die Produkte hat, und sie sagen zu ihm: "Sie haben das alles getan, Sie wetten darauf." <br><br>  Eine Person nimmt ein <strong>Jupyter-Notizbuch</strong> oder ein B√ºndel von Notizb√ºchern, betrachtet sie ausschlie√ülich als Bereitstellungsartefakt und beginnt auf einigen Servern freudig zu replizieren. <br><br>  Alles scheint in Ordnung zu sein, aber nicht immer.  Ich werde dir sp√§ter sagen warum. <br><br><h3>  Gnadenlose Ausbeutung </h3><br>  Die zweite Geschichte ist komplizierter und findet normalerweise in Unternehmen statt, in denen die Ausbeutung einen Zustand leichten Wahnsinns erreicht hat.  Data Scientist bringt seine L√∂sung in Betrieb.  Sie √∂ffnen diese Black Box und sehen etwas Schreckliches: <br><br><ul><li>  Notizb√ºcher </li><li>  Essiggurke verschiedener Versionen; </li><li>  Haufen von Skripten: Es ist nicht klar, wo und wann sie ausgef√ºhrt werden sollen und wo die von ihnen generierten Daten gespeichert werden sollen. </li></ul><br>  In diesem Puzzle st√∂√üt die Ausnutzung auf Versionsinkompatibilit√§t.  Beispielsweise hat ein Datenwissenschaftler keine bestimmte Version der Bibliothek angegeben, und die Operation wurde sp√§testens ausgef√ºhrt.  Nach einer Weile greift der Datenwissenschaftler zur√ºck: <br><br>  <em>- Sie haben scikit-learn auf die falsche Version eingestellt, jetzt sind alle Metriken weg!</em>  <em>Sie m√ºssen auf die vorherige Version zur√ºcksetzen.</em> <br><br>  Dies bricht den Sto√ü vollst√§ndig und die Ausbeutung leidet. <br><br><h3>  B√ºrokratie </h3><br>  In Unternehmen mit gr√ºnen Logos erh√§lt der Datenwissenschaftler, wenn er in Betrieb genommen wird und das Modell mitbringt, normalerweise ein 800-Blatt-Dokument als Antwort: ‚ÄûBefolgen Sie diese Anweisung, sonst wird Ihr Produkt nie das Licht der Welt erblicken.‚Äú <br><br>  Der traurige Datenwissenschaftler geht, wirft alles auf die H√§lfte und gibt dann auf - er ist nicht daran interessiert. <br><br><h3>  Bereitstellen </h3><br>  Angenommen, ein Datenwissenschaftler hat alle Kreise durchlaufen und am Ende wurde alles bereitgestellt.  Aber er wird nicht verstehen k√∂nnen, dass alles so funktioniert, wie es sollte.  Nach meiner Erfahrung gibt es in denselben gesegneten Banken keine √úberwachung von Data-Science-Produkten. <br><br>  Es ist gut, wenn der Spezialist die Ergebnisse seiner Arbeit in die Datenbank schreibt.  Nach einer Weile wird er sie empfangen und sehen, was im Inneren passiert.  Dies ist jedoch nicht immer der Fall.  Wenn ein Unternehmen und ein Datenwissenschaftler einfach glauben, dass alles gut und gut funktioniert, f√ºhrt dies zu erfolglosen F√§llen. <br><br><h3>  MFI </h3><br>  Irgendwie haben wir eine Scoring-Engine f√ºr eine gro√üe Mikrofinanzorganisation entwickelt.  Sie lie√üen uns nicht zum Produkt gehen, sondern nahmen uns einfach eine Kaskade von Modellen ab, installierten sie und starteten sie.  Die Testergebnisse der Modelle haben sie zufriedengestellt.  Aber nach 6 Monaten kamen sie zur√ºck: <br><br>  <em>- Alles ist schlecht.</em>  <em>Das Gesch√§ft l√§uft nicht, wir werden immer schlechter.</em>  <em>Es scheint, dass die Modelle ausgezeichnet sind, aber die Ergebnisse fallen, Betrug und Ausfall immer mehr und weniger Geld.</em>  <em>Wof√ºr haben wir dich bezahlt?</em>  <em>Lass es uns richtig machen.</em> <br><br>  Gleichzeitig wird wieder kein Zugriff auf das Modell gew√§hrt.  Die Protokolle wurden vor sechs Monaten f√ºr einen Monat entladen.  Wir haben das Entladen f√ºr einen weiteren Monat untersucht und sind zu dem Schluss gekommen, dass die IT-Abteilung des MFI irgendwann die Eingabedaten ge√§ndert hat und anstelle von Dokumenten in JSON begonnen hat, Dokumente in XML zu senden.  Das Modell erwartete json, erhielt aber XML, war traurig und dachte, dass es keine Daten an der Eingabe gab. <br><br><blockquote>  Wenn keine Daten vorliegen, ist die Einsch√§tzung des Geschehens anders.  Ohne √úberwachung kann dies nicht erkannt werden. </blockquote><br><h3>  Neue Version, Kaskade und Tests </h3><br>  Oft sind wir mit der Tatsache konfrontiert, dass das Modell gut funktioniert, aber aus irgendeinem Grund wurde eine <strong>neue Version</strong> entwickelt.  Das Modell muss wieder irgendwie hereingebracht werden und wieder durch alle Kreise der H√∂lle gehen.  Es ist gut, wenn die Bibliotheksversionen mit denen des Vorg√§ngermodells identisch sind. Wenn nicht, beginnt die Bereitstellung von neuem ... <br><br>  Manchmal m√∂chten wir eine neue Version <strong>testen,</strong> bevor wir <strong>sie</strong> in den Kampf ziehen. Legen <strong>Sie</strong> sie auf den <strong>Pr√ºfstand</strong> , sehen Sie sich denselben Verkehrsstrom an und stellen Sie sicher, dass sie gut ist.  Dies ist wieder die vollst√§ndige Bereitstellungskette.  Dar√ºber hinaus haben wir die Systeme so eingerichtet, dass nach diesem Modell keine realen Ergebnisse erzielt werden, wenn es um die Bewertung geht, sondern nur die Ergebnisse zur weiteren Analyse √ºberwacht und analysiert wurden. <br><br>  Es gibt Situationen, in denen eine <strong>Modellkaskade verwendet wird.</strong>  Wenn die Ergebnisse der folgenden Modelle von den vorherigen abh√§ngen, m√ºssen Sie irgendwie eine Interaktion zwischen ihnen herstellen und dies alles irgendwo behalten. <br><br><h2>  Wie l√∂se ich solche Probleme? </h2><br>  Oft l√∂st eine Person Probleme <strong>manuell</strong> , insbesondere in kleinen Unternehmen.  Er wei√ü, wie alles funktioniert, denkt an alle Versionen von Modellen und Bibliotheken, wei√ü, wo und welche Skripte funktionieren, welche Storefronts sie erstellen.  Das ist alles wunderbar.  Besonders sch√∂n sind die Geschichten, die der manuelle Modus hinterl√§sst. <br><br>  <strong>Die Geschichte des Erbes</strong> .  Ein guter Mann arbeitete in einer kleinen Bank.  Einmal ging er in ein s√ºdliches Land und kehrte nicht zur√ºck.  Danach haben wir eine Vererbung erhalten: eine Reihe von Code, der Storefronts generiert, an denen Modellmodelle arbeiten.  Der Code ist wundersch√∂n, er funktioniert, aber wir kennen nicht die genaue Version des Skripts, das diese oder jene Storefront generiert.  In der Schlacht sind alle Schaufenster vorhanden und alle werden gestartet.  Wir haben zwei Monate lang versucht, dieses komplizierte Gewirr zu erkennen und es irgendwie zu strukturieren. <br><br>  <strong>In einem harten Unternehmen</strong> wollen sich die Leute nicht mit allen Arten von Python, Jupitern usw. besch√§ftigen. Sie sagen: <br><br>  <em>- Kaufen wir IBM SPSS, installieren und alles wird gro√üartig.</em>  <em>Probleme mit der Versionierung, mit Datenquellen, mit der Bereitstellung dort irgendwie gel√∂st.</em> <br><br>  Dieser Ansatz hat ein Existenzrecht, aber nicht jeder kann es sich leisten.  In jedem Fall ist dies eine so hochwertige gezackte Nadel.  Sie sitzen darauf, aber es klappt nicht, auszusteigen - Kerben.  Und es kostet normalerweise viel. <br><br>  <strong>Open Source ist das</strong> Gegenteil des vorherigen Ansatzes.  Die Entwickler surften im Internet und fanden viele Open Source-L√∂sungen, die ihre Aufgaben in unterschiedlichem Ma√üe l√∂sen.  Dies ist ein gro√üartiger Weg, aber f√ºr uns selbst haben wir keine L√∂sungen gefunden, die unsere Anforderungen zu 100% erf√ºllen. <br><br>  Deshalb haben wir uns f√ºr die klassische Option entschieden - <strong>unsere Entscheidung</strong> .  Seine Kr√ºcken, Fahrr√§der, alle ihre eigenen, einheimischen. <br><br><h2>  Was wollen wir von unserer Entscheidung? </h2><br><br>  <strong>Schreiben Sie nicht alles selbst</strong> .  Wir m√∂chten Komponenten, insbesondere Infrastrukturkomponenten, verwenden, die sich bew√§hrt haben und mit der Funktionsweise in den Institutionen vertraut sind, mit denen wir zusammenarbeiten.  Wir schreiben nur eine Umgebung, die die Arbeit des Datenwissenschaftlers leicht von der Arbeit von DevOps isoliert. <br><br>  <strong>Verarbeiten Sie Daten in zwei Modi: sowohl im Batch-Modus - Batch als auch in Echtzeit</strong> .  Unsere Aufgaben umfassen beide Betriebsarten. <br><br>  <strong>Einfache Bereitstellung und in einem geschlossenen Umkreis</strong> .  Bei der Arbeit mit vertraulichen privaten Daten besteht keine Internetverbindung.  Zu diesem Zeitpunkt sollte alles schnell und genau zur Produktion gelangen.  Aus diesem Grund haben wir uns mit Gitlab, der darin enthaltenen CI / CD-Pipeline und Docker befasst. <br><br><blockquote>  Ein Modell ist kein Selbstzweck.  Wir l√∂sen nicht das Problem des Modellbaus, sondern ein Gesch√§ftsproblem. </blockquote><br>  Innerhalb der Pipeline m√ºssen Regeln und ein Konglomerat von Modellen vorhanden sein, die die <strong>Versionierung aller</strong> Pipelinekomponenten unterst√ºtzen. <br><br>  Was ist mit Pipeline gemeint?  In Russland ist das Bundesgesetz 115 zur Bek√§mpfung der Geldw√§sche und der Finanzierung des Terrorismus in Kraft.  Nur das Inhaltsverzeichnis der Empfehlungen der Zentralbank belegt 16 Bildschirme.  Dies sind einfache Regeln, die eine Bank erf√ºllen kann, wenn sie √ºber solche Daten verf√ºgt, oder nicht, wenn sie keine Daten hat. <br><br>  Die Bewertung eines Kreditnehmers, einer Finanztransaktion oder eines anderen Gesch√§ftsprozesses ist ein Datenstrom, den wir verarbeiten.  Ein Stream muss diese Art von Regel durchlaufen.  Diese Regeln werden vom Analysten auf einfache Weise beschrieben.  Er ist kein Datenwissenschaftler, kennt aber das Gesetz oder andere Anweisungen gut.  Der Analyst setzt sich und beschreibt im Klartext die √úberpr√ºfung der Daten. <br><br>  <strong>Erstellen Sie Kaskaden von Modellen</strong> .  Oft entsteht eine Situation, in der das n√§chste Modell die in fr√ºheren Modellen erhaltenen Werte f√ºr seine Arbeit verwendet. <br><br>  <strong>Testen Sie Hypothesen schnell.</strong>  Ich wiederhole die vorherige These: Ein Datenwissenschaftler hat eine Art Modell erstellt, es dreht sich im Kampf und funktioniert gut.  Aus irgendeinem Grund hat der Spezialist eine bessere L√∂sung gefunden, m√∂chte aber den etablierten Workflow nicht ruinieren.  Der Datenwissenschaftler h√§ngt ein neues Modell an denselben Kampfverkehr im Kampfsystem.  Sie beteiligt sich nicht direkt an der Entscheidungsfindung, sondern bedient denselben Verkehr, ber√ºcksichtigt einige Schlussfolgerungen und diese Schlussfolgerungen werden irgendwo gespeichert. <br><br>  <strong>Einfache Wiederverwendungsfunktion.</strong>  Viele Aufgaben haben den gleichen Komponententyp, insbesondere diejenigen, die sich auf das Extrahieren von Features oder Regeln beziehen.  Wir m√∂chten diese Komponenten in andere Pipelines ziehen. <br><br><h2>  Was hast du beschlossen zu tun? </h2><br>  Zuerst wollen wir √ºberwachen.  Und zwei seiner Art. <br><br><h3>  √úberwachung </h3><br>  <strong>Technische √úberwachung.</strong>  Wenn Pipeline-Komponenten bereitgestellt werden, sollten sie im Betrieb sehen, was mit der Komponente passiert: wie sie Speicher, CPU und Festplatte verbraucht. <br><br>  <strong>Gesch√§fts√ºberwachung.</strong>  Dies ist ein Tool f√ºr Datenwissenschaftler, mit dem Sie von den technischen Nuancen der Implementierung abstrahieren k√∂nnen.  Auf der Entwurfsebene hilft die Konstruktion dabei, zu bestimmen, welche Modellmetriken f√ºr die √úberwachung verf√ºgbar sein sollen, z. B. die Verteilung von Features oder die Bewertung von Serviceergebnissen. <br><br>  Ein Datenwissenschaftler definiert Metriken und sollte sich keine Gedanken dar√ºber machen, wie sie in das √úberwachungssystem gelangen.  Das einzig Wichtige ist, dass er diese Metriken und das Erscheinungsbild des Dashboards definiert hat, auf dem die Metriken angezeigt werden.  Dann startete der Spezialist alles in der Produktion, setzte es ein und nach einer Weile flossen die Metriken in die √úberwachung ein.  Ein Datenwissenschaftler ohne Zugriff auf das Produkt kann also sehen, was im Modell geschieht. <br><br><h3>  Testen </h3><br>  Testen Sie die <strong>Pipeline auf Konsistenz</strong> .  Angesichts der Besonderheiten der Pipeline ist dies eine Art Rechengraph.  Wir wollen verstehen, dass wir ein Diagramm implementieren, wir k√∂nnen es umgehen und einen Ausweg finden. <br><br>  Das Diagramm enth√§lt Komponenten - Module.  Alle Module m√ºssen den Unit- und Integrationstest bestehen.  Der Prozess sollte f√ºr einen Datenwissenschaftler transparent und einfach sein. <br><br>  Der Entwickler beschreibt das Modell und testet es alleine oder mit Hilfe eines anderen.  F√ºgt alles in Gitlab ein, die durch Continuous Integration konfigurierte Pipeline wird ausgel√∂st, getestet und sieht Ergebnisse.  Wenn alles gut ist - es geht weiter, nein - es beginnt von neuem. <br><br>  Der Datenwissenschaftler konzentriert sich auf das Modell und wei√ü nicht, was sich unter der Haube befindet.  Daf√ºr bekommt er mehrere Dinge. <br><br><ul><li>  <strong>Eine API zur Integration in den Kern des</strong> <strong>Systems selbst</strong> √ºber den Datenbus - Nachrichtenbus.  In diesem Fall muss der Spezialist beschreiben, was in seinem Modell, dem Einstiegspunkt und der Verbindung mit verschiedenen Komponenten innerhalb der Pipeline ein- und ausgeht. </li><li>  Nach dem Training des Modells wird ein <strong>Artefakt</strong> angezeigt <strong>- eine XGBoost-</strong> oder <strong>Pickle-</strong> <strong>Datei</strong> .  Der Datenwissenschaftler hat einen Executer f√ºr die Arbeit mit Artefakten - er muss die Pipelinekomponenten in das System integrieren. </li><li>  Einfache und transparente API f√ºr Datenwissenschaftler zur √úberwachung des Betriebs von Pipelinekomponenten - technische und gesch√§ftliche √úberwachung. </li><li>  <strong>Eine einfache und transparente Infrastruktur</strong> zur Integration in Datenquellen und zur Erhaltung der Arbeitsergebnisse. </li></ul><br>  Oft arbeiten Modelle f√ºr uns, und nach einer Weile kommt ein Audit, das die gesamte Geschichte des Dienstes auffrischen m√∂chte.  Das Audit m√∂chte die Richtigkeit der Arbeit und die Abwesenheit von Betrug unsererseits √ºberpr√ºfen.  Es werden einfache Tools ben√∂tigt, damit jeder Auditor, der sich mit SQL auskennt, in ein spezielles Repository gelangen und sehen kann, wie alles funktioniert, welche Entscheidungen getroffen wurden und warum. <br><br>  Wir haben den Grundstein f√ºr zwei wichtige Geschichten f√ºr uns gelegt. <br><br>  <strong>Customer Journey.</strong>  Dies ist eine Gelegenheit, die Mechanismen zur Aufbewahrung der gesamten Kundenhistorie zu nutzen - was mit dem Kunden als Teil der Gesch√§ftsprozesse passiert ist, die auf diesem System implementiert sind. <br><br>  M√∂glicherweise verf√ºgen wir √ºber externe Datenquellen, z. B. DMP-Plattformen.  Von ihnen erhalten wir Informationen √ºber menschliches Verhalten im Netzwerk und auf mobilen Ger√§ten.  Dies kann sich auf den LTV und die Bewertungsmodelle seines Modells auswirken.  Wenn der Kreditnehmer zu sp√§t bezahlt, k√∂nnen wir vorhersagen, dass dies keine b√∂swillige Absicht ist - es gibt einfach Probleme.  In diesem Fall wenden wir weiche Expositionsmethoden gegen√ºber dem Kreditnehmer an.  Wenn die Probleme behoben sind, schlie√üt der Kunde das Darlehen.  Wenn er das n√§chste Mal kommt, werden wir seine ganze Geschichte kennen.  Datenwissenschaftler erhalten eine visuelle Historie aus dem Modell und f√ºhren die Bewertung im Lichtmodus durch. <br><br>  <strong>Identifizierung von Anomalien</strong> .  Wir sind st√§ndig mit einer sehr komplexen Welt konfrontiert.  Beispielsweise k√∂nnen Schwachstellen bei der beschleunigten Bewertung von MFIs eine Quelle f√ºr automatischen Betrug sein. <br><br>  Customer Journey ist ein Konzept f√ºr den schnellen und einfachen Zugriff auf den Datenstrom, der das Modell durchl√§uft.  Das Modell macht es einfach, Anomalien zu erkennen, die zum Zeitpunkt des Massenauftretens f√ºr Betrug charakteristisch sind. <br><br><h2>  Wie ist alles angeordnet? </h2><br>  Ohne zu z√∂gern haben wir <strong>Kafka</strong> als Message Bus Patch genommen.  Dies ist eine gute L√∂sung, die von vielen unserer Kunden verwendet wird. Der Betrieb kann damit arbeiten. <br><br><img src="https://habrastorage.org/webt/uu/5k/3r/uu5k3rnxw7lu4iulruewhzmif8w.jpeg"><br><br><blockquote>  Einige Systemkomponenten werden m√∂glicherweise bereits im Unternehmen selbst verwendet.  Wir bauen das System nicht erneut auf, sondern verwenden das, was sie bereits haben. </blockquote><br>  <strong>Datenspeicherung ist</strong> in diesem Fall der Speicher, √ºber den der Client normalerweise bereits verf√ºgt.  Dies k√∂nnen Hadoop-, relationale und nicht relationale Datenbanken sein.  Wir k√∂nnen mit HDFS, Hive, Impala, Greenplum und PostgreSQL sofort arbeiten.  Wir betrachten diese Speicher als Quelle f√ºr Schaufenster. <br><br>  Daten kommen im Lager an, durchlaufen unsere ETL oder die ETL des Kunden, falls er eine hat.  Wir bauen Schaufenster, die in Modellen weiter verwendet werden.  Die Datenspeicherung wird im schreibgesch√ºtzten Modus verwendet. <br><br><h3>  Unsere Entwicklungen </h3><br>  <strong>Tafel</strong>  Der Name stammt von einer ziemlich seltsamen Praxis von Mathematikern der 30-40er Jahre.  Dies ist der Manager von Pipelines, die im Administrationssystem leben.  Blackboard hat eine Art Meta-Speicher.  Es speichert die Pipelines selbst und die Konfigurationen, die zum Initialisieren aller Komponenten erforderlich sind. <br><br>  Alle Systemarbeiten beginnen mit der Tafel.  Wie durch ein Wunder landete die Pipeline in Meta Storage, Blackboard hat dies nach einer Weile verstanden, zieht die aktuelle Version der Pipeline heraus, initialisiert sie und sendet ein Signal in Kafka. <br><br>  Es gibt eine <strong>Laufzeitumgebung</strong> .  Es basiert auf Dockern und kann auf Server repliziert werden, auch in der privaten Cloud des Kunden. <br><br>  Aus der Box kommt der <strong>Hauptdarsteller :: Init</strong> - dies ist der Initialisierer.  Dies ist ein Geist, der nur zwei Dinge tun kann: <strong>Komponenten</strong> <strong>bauen</strong> und <strong>zerst√∂ren</strong> .  Er erh√§lt einen Befehl von Blackboard: "Hier ist die Pipeline, sie muss auf solchen und solchen Servern mit solchen und solchen Ressourcen in solchen und solchen Mengen gestartet werden - Arbeit!"  Dann beginnt der Schauspieler alles. <br><br>  Mathematisch gesehen ist ein Akteur eine Funktion, die ein oder mehrere Objekte als Eingabe verwendet, den Status von Objekten mithilfe eines darin enthaltenen Algorithmus √§ndert, am Ausgang ein neues Objekt generiert oder den Status eines vorhandenen Objekts √§ndert. <br><br>  Technisch gesehen ist ein Schauspieler ein Python-Programm.  L√§uft in einem Docker-Container mit seiner Umgebung. <br><br>  Der Schauspieler wei√ü nichts √ºber die Existenz anderer Schauspieler.  Die einzige Entit√§t, die wei√ü, dass zus√§tzlich zum Akteur die gesamte Pipeline als Ganzes existiert - dies ist Blackboard.  Es √ºberwacht den Ausf√ºhrungsstatus aller Akteure innerhalb des Systems und beh√§lt den aktuellen Status bei, der sich in der √úberwachung als Bild des gesamten Gesch√§ftsprozesses als Ganzes ausdr√ºckt. <br><br>  Actor :: Init erzeugt viele Docker-Container.  Dar√ºber hinaus k√∂nnen Akteure mit der Datenspeicherung arbeiten. <br><br>  Das System selbst verf√ºgt √ºber eine <strong>Ereignisspeicherkomponente</strong> .  Als Ereignisspeicher verwenden wir <strong>ClickHouse</strong> .  Die Aufgabe ist einfach: Alle Informationen, die zwischen dem Schauspieler √ºber Kafka ausgetauscht werden, werden in ClickHouse gespeichert.  Dies erfolgt <strong>zur weiteren Pr√ºfung</strong> .  Dies ist das Pipeline-Betriebsprotokoll. <br><br>  Schauspieler k√∂nnen auch f√ºr <strong>Customer Journey entwickelt werden</strong> .  Sie sehen √Ñnderungen im Pipeline-Protokoll und k√∂nnen im laufenden Betrieb die Fenster neu erstellen, die erforderlich sind, damit Modelle oder Komponenten mit den Regeln arbeiten k√∂nnen, die sich bereits in der Pipeline befinden.  Dies ist ein fortlaufender Prozess zum √Ñndern von Daten. <br><br>  Die √úberwachung basiert eher primitiv auf <strong>Prometheus</strong> .  Der Akteur erh√§lt eine grundlegende API und sendet in einem geschlossenen Modus, der f√ºr den Entwickler transparent genug ist, Nachrichten mit Metriken an Kafka.  Prometheus liest Metriken aus Kafka und speichert sie in seinem Repository. <br><br>  Zur Visualisierung verwenden wir <strong>Grafana</strong> . <br><br><h3>  Zwei Integrationspunkte </h3><br>  Der erste ist der Punkt der Integration mit Datenquellen, die √ºber ETLs zum Data Warehouse gelangen.  Der zweite Integrationspunkt, wenn ein Dienst bereits von einem Datenkonsumenten verwendet wird, z. B. ein Scoring-Dienst. <br><br>  Wir haben <strong>Apache ServiceMix genommen.</strong>  Erfahrungsgem√§√ü sind diese Integrationspunkte vom gleichen Typ mit demselben Protokolltyp: SOAP, RESTful, seltener Warteschlangen.  Jedes Mal m√∂chten wir keinen eigenen Konstruktor oder Service entwickeln, um den n√§chsten SOAP-Service zu generieren.  Daher nehmen wir ServiceMix und beschreiben es in der SDL, in der die Datenmodelle dieses Dienstes und die darin vorhandenen Methoden erstellt werden.  Dann schieben wir den Router in ServiceMix durch und er generiert den Service selbst. <br><br>  Von uns selbst haben wir eine schwierige Synchron-Asynchron-Konvertierung hinzugef√ºgt.  Alle Anforderungen, die im System gespeichert sind, sind asynchron und werden √ºber den Nachrichtenbus gesendet. <br><br>  Die meisten Bewertungsdienste sind synchron.  ServiceMix-Anforderungen kommen entweder √ºber REST oder SOAP.  Zu diesem Zeitpunkt durchl√§uft er unser Gateway, das das Wissen √ºber die HTTP-Sitzung beibeh√§lt.  Dann sendet er eine Nachricht an Kafka, sie l√§uft durch eine Pipeline und eine L√∂sung wird generiert. <br><br>  M√∂glicherweise gibt es jedoch noch keine L√∂sung.  Zum Beispiel ist etwas abgefallen oder es gibt eine schwierige SLA, um eine Entscheidung zu treffen, und Gateway √ºberwacht: "OK, ich habe eine Anfrage erhalten, er ist in einem anderen Kafka-Thema zu mir gekommen, oder es ist nichts zu mir gekommen, aber mein Timeout-Ausl√∂ser hat funktioniert."  Andererseits erfolgt die Konvertierung von synchron zu asynchron, und innerhalb derselben HTTP-Sitzung gibt es eine Antwort an den Verbraucher mit dem Ergebnis der Arbeit.  Dies kann ein Fehler oder eine normale Prognose sein. <br><br>  An diesem Ort haben wir √ºbrigens dank der gro√üartigen und leistungsstarken Open Source einen geschmacklosen Hund gefressen.  Wir haben ServiceMix aus einer der neuesten Versionen und Kafka aus fr√ºheren Versionen verwendet und alles hat perfekt funktioniert.  Wir haben in diesem Gateway geschrieben, basierend auf den Cubes, die bereits in ServiceMix enthalten waren.  Als die neue Version von Kafka herauskam, haben wir sie gerne aufgenommen, aber es stellte sich heraus, dass sich die Unterst√ºtzung f√ºr die √úberschriften in der zuvor existierenden Nachricht in Kafka ge√§ndert hatte.  Das Gateway in ServiceMix kann nicht mehr mit ihnen arbeiten.  Um dies zu verstehen, haben wir viel Zeit verbracht.  Aus diesem Grund haben wir unser Gateway erstellt, das mit neuen Versionen von Kafka kompatibel ist.  Wir haben den ServiceMix-Entwicklern √ºber das Problem geschrieben und die Antwort erhalten: "Danke, wir werden Ihnen in den n√§chsten Versionen definitiv helfen!" <br><br>  Daher sind wir gezwungen, Aktualisierungen zu √ºberwachen und regelm√§√üig etwas zu √§ndern. <br><br>  <strong>Infrastruktur ist Gitlab.</strong>  Wir verwenden fast alles, was darin enthalten ist. <br><br><ul><li>  Code-Repository. </li><li>  Setzt die Integration fort / setzt die Lieferpipeline fort. </li><li>  Registrierung zum Verwalten einer Registrierung von Docker-Containern. </li></ul><br><h3>  Komponenten </h3><br>  Wir haben 5 Komponenten entwickelt: <br><br><ul><li>  <strong>Blackboard</strong> - Pipeline-Lebenszyklusmanagement.  Wo, was und mit welchen Parametern soll von der Pipeline ausgef√ºhrt werden. </li><li>  <strong>Der Feature-Extraktor</strong> funktioniert einfach - wir informieren den Feature-Extraktor dar√ºber, dass wir an der Eingabe ein solches und ein solches Datenmodell erhalten, w√§hlen die erforderlichen Felder aus den Daten aus und ordnen sie bestimmten Werten zu.  Zum Beispiel erhalten wir das Geburtsdatum des Kunden, konvertieren es in das Alter und verwenden es als Funktion in unserem Modell.  Der Feature-Extraktor ist f√ºr die Datenanreicherung verantwortlich. </li><li>  <strong>Regelbasierte Engine</strong> - √úberpr√ºfung der Daten gem√§√ü den Regeln.  Dies ist eine einfache Beschreibungssprache, mit der eine Person, die mit der Konstruktion von &lt;code&gt; if, else &lt;code /&gt; -Bl√∂cken vertraut ist, die Regeln f√ºr die √úberpr√ºfung innerhalb des Systems beschreiben kann. </li><li>  <strong>Machine Learning Engine</strong> - Erm√∂glicht es Ihnen, den Executor auszuf√ºhren, das trainierte Modell zu initialisieren und es an die Eingabedaten zu senden.  Am Ausgang nimmt das Modell Daten auf. </li><li>  <strong>Entscheidungsmodul</strong> - Entscheidungsmodul, Verlassen des Diagramms.  Wenn Sie eine Kaskade von Modellen haben, zum Beispiel verschiedene Zweige der Kreditnehmerbewertung, m√ºssen Sie sich irgendwo f√ºr das Thema Geld entscheiden.  Das Regelwerk f√ºr die L√∂sung sollte einfach sein. ,     LTV- ‚Äî     ,     ,  . </li></ul><br><br><h3>   </h3><br>         .  ‚Äî  ,    .  ‚Äî      ,      . <br><br>   pipeline    . <br><img src="https://habrastorage.org/webt/o7/vy/dj/o7vydjrruyfittwhwzumh_gc_rq.jpeg"><br><ul><li>   <strong>Feature extractor</strong> :  ,        ,      . </li><li> <strong> </strong> . ,  -:  , ,       18. </li><li>  <strong> .</strong>    ,    .     ,      ,        pipeline. </li><li>  <strong>Decision engine</strong> .             . </li><li>  <strong></strong> . </li></ul><br>      yaml.         .    ,  ,        .           yaml. <br><br>  pipeline,   ,   : feature extractor, rules, models, decision engine,    .   ‚Äî <strong>      Docker-</strong> .    Registry,   Docker-. -,   ,    . ,  ,      Docker-       . <br><br><h3>  Pipeline </h3><br>     ,     <strong>Python</strong> ‚Äî         . Feature extractor, ,   decision engine   Python. <br><br> Pipeline   <strong>yaml.</strong>      meta storage     ‚Äî   <strong></strong> . <br><br>  Runtime environment   10 ,  Blackboard  ,    pipeline    10 .  ,      : , , IP-    Kafka, , .       . <br><br>     GitLab.      Ansible. ,    .           ,      50 000    Ansible  . <br><br><h2>   ? </h2><br>  GitLab  pipeline.    GitLab. CI  ,   ,  ,  .   <strong>GitLab Runner</strong> ,    Docker-  ,    pipeline.    ‚Äî    Registry. <br><br><img src="https://habrastorage.org/webt/gz/dx/hr/gzdxhruymhjm2_wejqybqizdp0i.jpeg"><br><br>  Docker  ,       .   Docker-        .   CI pipeline    pipeline  -  Meta Storage,    Blackboard. <br><br> Blackboard    Meta Storage ‚Äî   , , ,   -.   Docker-     , , . <br><br> -   Blackboard  Meta Storage      :   ,  Kafka,   .  ,    ,   Docker-    ,     . <br><br>  ,    Docker-,  ‚Äî pipeline ! <br><br>      DigitalOcean.     AWS  Scaleway,     . <br><br>    ,        .  pipeline         . ,    . <br><br><h3>    ? </h3><br>   ‚Äî   .  ,   pipeline,     real-time   . <br><br><ul><li> 2 Feature extractor  .     1 , .. json    . </li><li> 8  ‚Äî 8  ML engine.      XGBoost. </li><li> 18     RB engine (115 ).   1000     . </li><li> 1 decision engine. </li></ul><br>       200   .  2 Feature extractor, 8 , 18   1 decision engine      1,2 . <br><br><h3>  </h3><br> <strong>Discovery .</strong>  ,   -   .  ,      ,    .    .      Meta Storage. <br><br> <strong>  pipeline</strong> .    ,  <strong>BPM</strong> .        yaml      ,     ,      . <br><br> <strong>    .</strong>       Java, Scala, R.    Python,       ,     .    API   ,   pipeline      . <br><br><h2>  Was ist das Ergebnis? </h2><br>    ‚Äî     .    ‚Äî   .   <strong> </strong> ,      .  ,         .     ‚Äî     2018 . <br><br>         ,      .    ‚Äî    ,   ,    . <br><br> <strong>    ,   </strong> .     ,    ,    notebook   ,     . <br><br><blockquote> , -      ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> ,           .  ,     , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">UseData Conf</a> .  ,    ,       ,   16 . </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de455648/">https://habr.com/ru/post/de455648/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de455638/index.html">Alan Kay hat keine Objekte erfunden</a></li>
<li><a href="../de455640/index.html">Marvin Minskys ‚ÄûThe Emotion Machine‚Äú: Kapitel 4. ‚ÄûWie wir Bewusstsein erkennen‚Äú</a></li>
<li><a href="../de455642/index.html">Die Architektur des verteilten Nachrichtenwarteschlangendienstes in Yandex.Cloud</a></li>
<li><a href="../de455644/index.html">Wir verwenden Daten in der Praxis</a></li>
<li><a href="../de455646/index.html">Sicherheitswoche 24: Werks-Backdoors auf Android-Smartphones</a></li>
<li><a href="../de455650/index.html">Wie wir ein neuronales Netzwerk trainiert haben, um Schrauben zu klassifizieren</a></li>
<li><a href="../de455652/index.html">Deep Learning gegen gesunden Menschenverstand: Entwicklung eines Chat-Bots</a></li>
<li><a href="../de455658/index.html">Legend√§rer Intel Core i7-2600K: Testen von Sandy Bridge im Jahr 2019 (Teil 3)</a></li>
<li><a href="../de455662/index.html">Gro√ües mechanisches Display mit Nockenmechanismus als Decoder</a></li>
<li><a href="../de455666/index.html">Aufbau von Outbound Sales in einem IT-Service-Unternehmen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>