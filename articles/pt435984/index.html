<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üèöÔ∏è üèÑ ‚è¨ Redes neurais e a filosofia da linguagem üßîüèø üßúüèª ‚úåüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Por que as teorias de Wittgenstein continuam sendo a base de toda a PNL moderna 

 A representa√ß√£o vetorial das palavras √© talvez uma das id√©ias mais ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Redes neurais e a filosofia da linguagem</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/435984/"> <font color="gray">Por que as teorias de Wittgenstein continuam sendo a base de toda a PNL moderna</font> <br><br>  A representa√ß√£o vetorial das palavras √© talvez uma das id√©ias mais bonitas e rom√¢nticas da hist√≥ria da intelig√™ncia artificial.  A filosofia da linguagem √© um ramo da filosofia que explora a rela√ß√£o entre linguagem e realidade e como tornar a fala significativa e compreens√≠vel.  Uma representa√ß√£o vetorial de palavras √© um m√©todo muito espec√≠fico no moderno Natural Language Processing (NLP).  Em certo sentido, √© uma evid√™ncia emp√≠rica das teorias de Ludwig Wittgenstein, um dos fil√≥sofos mais relevantes do s√©culo passado.  Para Wittgenstein, o uso de palavras √© uma jogada em um <i>jogo de</i> linguagem social jogado por membros da comunidade que se entendem.  O significado de uma palavra depende apenas de sua utilidade em um contexto; n√£o corresponde um a um com um objeto do mundo real. <br><br><blockquote>  Para uma grande classe de casos em que usamos a palavra "significado", ela pode ser definida como o <b>significado da palavra √© o seu uso no idioma</b> . </blockquote><a name="habracut"></a><br>  Obviamente, √© muito dif√≠cil entender o significado exato de uma palavra.  H√° muitos aspectos a serem considerados: <br><br><ul><li>  a qual objeto a palavra pode se referir; </li><li>  que parte do discurso √© essa; </li><li>  se √© uma express√£o idiom√°tica; </li><li>  todos os tons de significados; </li><li>  e assim por diante. </li></ul><br>  Todos esses aspectos, no final, se resumem a uma coisa: saber usar a palavra. <br><br>  O conceito de <i>significado</i> e por que um conjunto ordenado de caracteres tem uma conota√ß√£o definida na linguagem n√£o √© apenas uma quest√£o filos√≥fica, mas provavelmente o maior problema com o qual os especialistas em IA que trabalham com a PNL precisam lidar.  √â bastante √≥bvio para uma pessoa de l√≠ngua russa que um "cachorro" √© um "animal", e parece mais um "gato" do que um "golfinho", mas essa tarefa est√° longe de ser simples para uma solu√ß√£o sistem√°tica. <br><br>  Tendo corrigido ligeiramente as teorias de Wittgenstein, podemos dizer que os c√£es se parecem com gatos porque geralmente aparecem nos mesmos contextos: voc√™ provavelmente pode encontrar c√£es e gatos associados √†s palavras "casa" e "jardim" do que √†s palavras "mar" e o "oceano".  √â essa intui√ß√£o subjacente ao <b>Word2Vec</b> , uma das implementa√ß√µes mais famosas e bem-sucedidas da representa√ß√£o vetorial de palavras.  Hoje, as m√°quinas est√£o longe de uma <i>compreens√£o</i> real <i>de</i> textos e passagens longas, mas a representa√ß√£o vetorial das palavras √© sem d√∫vida o √∫nico m√©todo que nos permitiu dar o maior passo nessa dire√ß√£o na √∫ltima d√©cada. <br><br><h1>  Do BoW ao Word2Vec </h1><br>  Em muitos problemas do computador, o primeiro problema √© apresentar os dados em forma num√©rica;  palavras e frases s√£o provavelmente as mais dif√≠ceis de imaginar nesta forma.  Em nossa configura√ß√£o, as palavras <b>D</b> s√£o selecionadas no dicion√°rio e cada palavra pode receber um √≠ndice num√©rico <b>i</b> . <br><br>  Por muitas d√©cadas, uma abordagem cl√°ssica foi adotada para representar cada palavra como um vetor num√©rico D-dimensional de todos os zeros, exceto um na posi√ß√£o i.  Como exemplo, considere um dicion√°rio de tr√™s palavras: "cachorro", "gato" e "golfinho" (D = 3).  Cada palavra pode ser representada como um vetor tridimensional: "cachorro" corresponde a [1,0,0], "gato" a [0,1,0] e "golfinho", obviamente, [0,0,1].  O documento pode ser representado como um vetor D-dimensional, em que cada elemento conta as ocorr√™ncias da i-√©sima palavra no documento.  Esse modelo √© chamado Bag-of-words (BoW) e √© usado h√° d√©cadas. <br><br>  Apesar de seu sucesso nos anos 90, a BoW n√£o possu√≠a a √∫nica fun√ß√£o interessante das palavras: seu significado.  Sabemos que duas palavras muito diferentes podem ter significados semelhantes, mesmo que sejam completamente diferentes do ponto de vista da ortografia.  ‚ÄúGato‚Äù e ‚Äúcachorro‚Äù s√£o animais dom√©sticos, ‚Äúrei‚Äù e ‚Äúrainha‚Äù est√£o pr√≥ximos um do outro, ‚Äúma√ß√£‚Äù e ‚Äúcigarro‚Äù n√£o t√™m rela√ß√£o alguma.  <i>Sabemos</i> disso, mas no modelo BoW, todas essas palavras est√£o √† mesma dist√¢ncia no espa√ßo vetorial: 1. <br><br>  O mesmo problema se aplica aos documentos: usando o BoW, podemos concluir que os documentos s√£o semelhantes apenas se contiverem a mesma palavra um certo n√∫mero de vezes.  E aqui vem o Word2Vec, introduzindo nos termos do aprendizado de m√°quina muitas quest√µes filos√≥ficas que Wittgenstein discutiu em seus <i>Estudos Filos√≥ficos</i> h√° 60 anos. <br><br>  Em um dicion√°rio de tamanho D, onde a palavra √© identificada por seu √≠ndice, o objetivo √© calcular a representa√ß√£o vetorial N-dimensional de cada palavra para N &lt;&lt; D.  Idealmente, queremos que seja um vetor denso representando alguns aspectos semanticamente espec√≠ficos do significado.  Por exemplo, idealmente queremos que "cachorro" e "gato" tenham representa√ß√µes semelhantes, e "ma√ß√£" e "cigarro" estejam muito distantes no espa√ßo vetorial. <br><br>  Queremos realizar algumas opera√ß√µes alg√©bricas b√°sicas em vetores, como <code>+‚àí=</code> .  Quero que a dist√¢ncia entre os vetores "ator" e "atriz" coincida substancialmente com a dist√¢ncia entre o "pr√≠ncipe" e a "princesa".  Embora esses resultados sejam bastante ut√≥picos, experimentos mostram que os vetores do Word2Vec exibem propriedades muito pr√≥ximas delas. <br><br>  O Word2Vec n√£o aprende diretamente as visualiza√ß√µes com isso, mas as recebe como subproduto da classifica√ß√£o sem um professor.  O conjunto m√©dio de dados de corpus de palavras da PNL consiste em um conjunto de frases;  cada palavra em uma frase aparece no contexto das palavras ao redor.  O objetivo do classificador √© prever a palavra de destino, considerando as palavras contextuais como entrada.  Para a frase ‚Äúcachorro marrom brinca no jardim‚Äù, as palavras [marrom, brinca, no jardim] s√£o fornecidas ao modelo como entrada, e ela deve prever a palavra ‚Äúcachorro‚Äù.  Essa tarefa √© considerada como aprendizado sem professor, pois o corpus n√£o precisa ser marcado com uma fonte externa de verdade: com um conjunto de frases, voc√™ sempre pode criar exemplos positivos e negativos automaticamente.  Olhando para ‚Äúcachorro marrom brincando no jardim‚Äù como um exemplo positivo, podemos criar muitos padr√µes negativos, como ‚Äúavi√£o marrom brincando no jardim‚Äù ou ‚Äúuva marrom brincando no jardim‚Äù, substituindo a palavra-alvo ‚Äúcachorro‚Äù por palavras aleat√≥rias do conjunto de dados. <br><br>  E agora a aplica√ß√£o das teorias de Wittgenstein √© perfeitamente clara: o contexto √© crucial para a representa√ß√£o vetorial das palavras, pois √© importante atribuir significado √† palavra em suas teorias.  Se duas palavras tiverem significados semelhantes, elas ter√£o representa√ß√µes semelhantes (uma pequena dist√¢ncia no espa√ßo N-dimensional) apenas porque elas geralmente aparecem em contextos semelhantes.  Assim, o "gato" e o "cachorro" acabar√£o tendo vetores pr√≥ximos porque geralmente aparecem nos mesmos contextos: √© √∫til que o modelo use representa√ß√µes vetoriais semelhantes para eles, porque √© a coisa mais conveniente que ela pode fazer, para obter melhores resultados na previs√£o de duas palavras com base em seus contextos. <br><br>  O artigo original oferece duas arquiteturas diferentes: CBOW e Skip-gram.  Nos dois casos, as representa√ß√µes verbais s√£o ensinadas juntamente com uma tarefa de classifica√ß√£o espec√≠fica, fornecendo as melhores representa√ß√µes vetoriais poss√≠veis de palavras que maximizam o desempenho do modelo. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2b9/e32/ee7/2b9e32ee767ad0c402e214a566d848a0.png"><br>  <i><font color="gray">Figura 1. Compara√ß√£o das arquiteturas CBOW e Skip-gram</font></i> <br><br>  <b>CBOW</b> √© a sigla para Continuous Bag of Words, e sua tarefa √© adivinhar uma palavra com o contexto em mente como entrada.  As entradas e sa√≠das s√£o representadas como vetores D-dimensionais projetados em um espa√ßo N-dimensional com pesos comuns.  Estamos apenas procurando pesos de proje√ß√£o.  De fato, a representa√ß√£o vetorial das palavras √© matrizes D √ó N, em que cada linha representa uma palavra do dicion√°rio.  Todas as palavras de contexto s√£o projetadas em uma posi√ß√£o e suas representa√ß√µes vetoriais s√£o calculadas como m√©dia;  portanto, a ordem das palavras n√£o afeta o resultado. <br><br>  <b>O pulo-grama</b> faz a mesma coisa, mas vice-versa: tenta prever as palavras de contexto <b>C</b> , tomando a palavra-alvo como entrada.  A tarefa de prever v√°rias palavras contextuais pode ser reformulada em um conjunto de problemas independentes de classifica√ß√£o bin√°ria, e agora o objetivo √© prever a presen√ßa (ou aus√™ncia) de palavras contextuais. <br><br>  Como regra geral, o Skip-gram exige mais tempo para treinamento e geralmente oferece resultados um pouco melhores, mas, como de costume, aplicativos diferentes t√™m requisitos diferentes, e √© dif√≠cil prever com anteced√™ncia quais mostrar√£o o melhor resultado.  Apesar da simplicidade do conceito, aprender esse tipo de arquitetura √© um verdadeiro pesadelo devido √† quantidade de dados e √† capacidade de processamento necess√°ria para otimizar pesos.  Felizmente, na Internet, voc√™ pode encontrar algumas representa√ß√µes vetoriais de palavras pr√©-treinadas e estudar o espa√ßo vetorial - o mais interessante - com apenas algumas linhas de c√≥digo Python. <br><br><h1>  Poss√≠veis melhorias: GloVe e fastText </h1><br>  Sobre o Word2Vec cl√°ssico nos √∫ltimos anos, muitas melhorias poss√≠veis foram propostas.  Os dois mais interessantes e mais usados ‚Äã‚Äãs√£o o GloVe (Stanford University) e o fastText (desenvolvido pelo Facebook).  Eles est√£o tentando identificar e superar as limita√ß√µes do algoritmo original. <br><br>  Em um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo cient√≠fico original</a> , os autores do GloVe enfatizam que o treinamento de modelos em um contexto local separado faz pouco uso das estat√≠sticas globais dos corpus.  O primeiro passo para superar essa limita√ß√£o √© criar uma matriz global <b>X</b> , onde cada elemento <b>i, j</b> conta o n√∫mero de refer√™ncias √† palavra <b>j</b> no contexto da palavra <b>i</b> .  A segunda ideia importante deste documento √© o entendimento de que apenas as probabilidades n√£o s√£o suficientes para a previs√£o confi√°vel de valores, e tamb√©m √© necess√°ria uma matriz de coocorr√™ncia, da qual certos aspectos dos valores podem ser extra√≠dos diretamente. <br><br><blockquote>  Considere duas palavras iej que s√£o de particular interesse.  Por concretude, suponha que estamos interessados ‚Äã‚Äãno conceito de um estado termodin√¢mico, para o qual podemos tomar <code>j = </code> <code>i = </code> e <code>j = </code> .  A rela√ß√£o dessas palavras pode ser investigada estudando a raz√£o de suas probabilidades de ocorr√™ncia conjunta usando diferentes palavras sonoras, k.  Para palavras k relacionadas ao gelo, mas n√£o ao vapor, digamos <code>k = </code> [s√≥lido, estado da mat√©ria], esperamos que a rela√ß√£o Pik / Pjk seja maior.  Da mesma forma, para as palavras k associadas ao vapor, mas n√£o ao gelo, digamos <code>k = </code> , a propor√ß√£o deve ser pequena.  Para palavras como "√°gua" ou "moda", que s√£o igualmente relacionadas ao gelo e ao vapor, ou n√£o t√™m rela√ß√£o com elas, essa propor√ß√£o deve estar pr√≥xima da unidade. </blockquote><br>  Essa raz√£o de probabilidades se torna o ponto de partida para o estudo da representa√ß√£o vetorial de palavras.  Queremos poder calcular vetores que, em combina√ß√£o com uma fun√ß√£o espec√≠fica <b>F,</b> mant√™m essa propor√ß√£o constante no espa√ßo de representa√ß√£o vetorial. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4dc/d5d/137/4dcd5d13763ca26ad997565ec2e6e513.jpg"></div><br>  <i><font color="gray">Figura 2. A f√≥rmula mais comum para a representa√ß√£o vetorial de palavras no modelo GloVe</font></i> <br><br>  A fun√ß√£o F e a depend√™ncia da palavra k podem ser simplificadas substituindo as exponenciais e as compensa√ß√µes fixas, o que fornece a fun√ß√£o de minimizar erros pelo m√©todo dos m√≠nimos quadrados <b>J</b> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e48/049/738/e48049738a71657b998f5630dac792c4.jpg"></div><br>  <i><font color="gray">Figura 3. A fun√ß√£o final de calcular a representa√ß√£o vetorial de palavras no modelo GloVe</font></i> <br><br>  A fun√ß√£o <b>f</b> √© uma fun√ß√£o de contagem que tenta n√£o sobrecarregar correspond√™ncias muito frequentes e raras, enquanto <b>bi</b> e <b>bj</b> s√£o deslocamentos para restaurar a simetria da fun√ß√£o.  Nos √∫ltimos par√°grafos do artigo, √© mostrado que o treinamento deste modelo no final n√£o √© muito diferente do treinamento do modelo cl√°ssico de Skip-gram, embora em testes emp√≠ricos o GloVe seja superior √†s duas implementa√ß√µes do Word2Vec. <br><br>  Por outro lado, o <b>fastText</b> corrige uma desvantagem completamente diferente do Word2Vec: se o treinamento do modelo come√ßar com a codifica√ß√£o direta de um vetor D-dimensional, a estrutura interna das palavras ser√° ignorada.  Em vez de codificar diretamente a codifica√ß√£o de palavras aprendendo representa√ß√µes verbais, o fastText oferece o estudo de N-gramas de caracteres e representa as palavras como a soma dos vetores de N-grama.  Por exemplo, com N = 3, a palavra "flor" √© codificada como 6 gramas diferentes [&lt;fl, flo, low, Debt, wer, er&gt;] mais uma sequ√™ncia especial &lt;flower&gt;.  Observe como colchetes angulares s√£o usados ‚Äã‚Äãpara indicar o in√≠cio e o fim de uma palavra.  Assim, uma palavra √© representada por seu √≠ndice no dicion√°rio de palavras e o conjunto de N-gramas que ela cont√©m, mapeado para n√∫meros inteiros usando a fun√ß√£o hash.  Esse aprimoramento simples permite dividir representa√ß√µes de grama N entre palavras e calcular representa√ß√µes vetoriais de palavras que n√£o estavam no caso de aprendizado. <br><br><h1>  Experi√™ncias e poss√≠veis aplica√ß√µes </h1><br>  Como j√° dissemos, para <b>usar</b> essas representa√ß√µes vetoriais, voc√™ precisa de apenas algumas linhas de c√≥digo Python.  <a href="">Conduzi</a> v√°rias experi√™ncias com o <a href="">modelo GloVe de 50 dimens√µes</a> , treinado em 6 bilh√µes de palavras das frases da Wikipedia, bem como com o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">modelo fastText de 300 dimens√µes, treinado no Common Crawl</a> (que forneceu 600 bilh√µes de tokens).  Esta se√ß√£o fornece links para os resultados de ambos os experimentos apenas para provar os conceitos e fornecer uma compreens√£o geral do t√≥pico. <br><br>  Antes de tudo, eu queria verificar algumas semelhan√ßas b√°sicas de palavras, a caracter√≠stica mais simples, mas importante, de sua representa√ß√£o vetorial.  Como esperado, as palavras mais semelhantes √†s palavras ‚Äúcachorro‚Äù foram ‚Äúgato‚Äù (0,92), ‚Äúc√£es‚Äù (0,85), ‚Äúcavalo‚Äù (0,79), ‚Äúcachorro‚Äù (0,78) e ‚Äúanimal de estima√ß√£o‚Äù (0,77).  Observe que a forma plural tem quase o mesmo significado que o singular.  Novamente, √© bastante trivial dizermos isso, mas para um carro n√£o √© completamente um fato.  Agora comida: as palavras mais semelhantes para "pizza" s√£o "sandu√≠che" (0,87), "sandu√≠ches" (0,86), "lanche" (0,81), "assados" (0,79), "batatas fritas" (0,79) e "hamb√∫rgueres" ( 0,78).  Faz sentido, os resultados s√£o satisfat√≥rios e o modelo se comporta muito bem. <br><br>  A pr√≥xima etapa √© executar alguns c√°lculos b√°sicos no espa√ßo vetorial e verificar se o modelo adquiriu corretamente algumas propriedades importantes.  De fato, como resultado do c√°lculo dos vetores <code>+-</code> , o resultado √© "atriz" (0,94), e como resultado do c√°lculo do <code>+-</code> , a palavra "rei" (0,86) √© obtida.  De um modo geral, se o valor √© <code>a:b=c:d</code> , a palavra <b>d</b> deve ser obtida como <code>d=b-a+c</code> .  Passando para o pr√≥ximo n√≠vel, √© imposs√≠vel imaginar como essas opera√ß√µes vetoriais descrevem aspectos geogr√°ficos: sabemos que Roma √© a capital da It√°lia, j√° que Berlim √© a capital da Alemanha, de fato <code>+-= (0.88)</code> e <code>+-= (0.83)</code> . <br><br>  E agora a parte divertida.  Seguindo a mesma ideia, tentaremos adicionar e subtrair conceitos.  Por exemplo, qual √© o equivalente americano de pizza para italianos?  <code>+-= (0.60)</code> , depois <code> (0.59)</code> .  Desde que me mudei para a Holanda, sempre digo que este pa√≠s √© uma mistura de tr√™s coisas: um pouco de capitalismo americano, frio sueco e qualidade de vida e, finalmente, uma pitada de <i>abund√¢ncia</i> napolitana.  Ao mudar levemente o teorema inicial, removendo um pouco da precis√£o su√≠√ßa, obtemos a Holanda (0,68) como resultado dos <code>++-</code> : bastante impressionante, para ser honesto. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fd8/f76/e41/fd8f76e41d78598394d1613652f44f26.png"><br>  <i><font color="gray">Figura 4. Para todos os leitores holandeses: tome isso como um elogio, ok?</font></i> <br><br>  Bons recursos pr√°ticos podem ser encontrados <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> para usar essas representa√ß√µes vetoriais pr√©-treinadas.  <b>Gensim</b> √© uma biblioteca Python simples e completa com algumas fun√ß√µes alg√©bricas e de similaridade prontas para uso.  Essas representa√ß√µes vetoriais pr√©-treinadas podem ser usadas de v√°rias maneiras (e √∫teis), por exemplo, para melhorar o desempenho de analisadores de humor ou modelos de linguagem.  Qualquer que seja a tarefa, o uso de vetores N-dimensionais melhorar√° significativamente a efici√™ncia do modelo em compara√ß√£o √† codifica√ß√£o direta.  Obviamente, o treinamento em representa√ß√µes vetoriais em uma √°rea espec√≠fica melhorar√° ainda mais o resultado, mas isso pode exigir, talvez, esfor√ßo e tempo excessivos. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt435984/">https://habr.com/ru/post/pt435984/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt435970/index.html">Guia do iniciante para desenvolvimento de servidores Web com Node.js</a></li>
<li><a href="../pt435972/index.html">Introdu√ß√£o √† programa√ß√£o reativa no Spring</a></li>
<li><a href="../pt435974/index.html">Three.js - controla o espa√ßo ou o planet√°rio</a></li>
<li><a href="../pt435976/index.html">WebAssembly em produ√ß√£o e o ‚Äúcampo minado‚Äù da Smart TV: uma entrevista com Andrey Nagikh</a></li>
<li><a href="../pt435978/index.html">Solu√ß√µes alternativas de prote√ß√£o biom√©trica</a></li>
<li><a href="../pt435986/index.html">O Windows reservar√° 7 GB para atualiza√ß√µes do sistema para evitar a falta de espa√ßo no disco r√≠gido</a></li>
<li><a href="../pt435988/index.html">Uma introdu√ß√£o √†s anota√ß√µes do tipo Python. Continua√ß√£o</a></li>
<li><a href="../pt435990/index.html">Como fazer uma troca?</a></li>
<li><a href="../pt435992/index.html">Jogadores Fallout 76, que ser√£o capturados em um local secreto dos desenvolvedores, ser√£o banidos</a></li>
<li><a href="../pt435994/index.html">√â Karma, querida, ou por que o ataque √†s redes sem fio que deveria afundar no esquecimento ainda est√° vivo</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>