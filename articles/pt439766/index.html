<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧖🏾 💪🏿 🧑🏾 Aumente isso! Aumento da resolução moderna 👩🏽‍🤝‍👩🏼 🤶🏽 🌾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Eu já parei de tremer e me perguntar quando o telefone toca e uma voz forte e confiante toca no receptor: "É este o capitão que está incomodando você ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Aumente isso! Aumento da resolução moderna</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/439766/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b88/1db/675/b881db675f3ef17c06a45cc67a8c1cb2.png"></div><br>  Eu já parei de tremer e me perguntar quando o telefone toca e uma voz forte e confiante toca no receptor: "É este o capitão que está incomodando você (major e tal), você pode responder a algumas perguntas?"  Por que não falar com sua própria polícia ... <br><br>  As perguntas são sempre as mesmas.  "Temos um vídeo com o suspeito, por favor, ajude a restaurar o rosto" ... "Ajude a aumentar o número do DVR" ... "Não há mãos humanas aqui, por favor, ajude a aumentar" ... E assim por diante. <br><br>  Para deixar claro o que é isso, aqui está um exemplo real de um vídeo altamente compactado enviado onde eles pedem para restaurar um rosto desfocado (cujo tamanho é equivalente a cerca de 8 pixels): <br><br><div style="text-align:center;"><img width="400" src="https://habrastorage.org/webt/k-/bw/5h/k-bw5hsws_7bijnrykr3vrvhk_i.png"></div><br>  E tudo bem, apenas os tios russos de Stepa perturbariam, escrevem os Western Pinkertones. <br><a name="habracut"></a>  Aqui, por exemplo, está uma carta da polícia da Inglaterra &lt;***** @ *****. Fsnet.co.uk&gt;: <br><blockquote>  Eu uso seus filtros em particular há algum tempo para resgatar meus vídeos ruins de férias em família, mas gostaria de usar os filtros comerciais para o meu trabalho.  Atualmente, sou policial de uma pequena força policial e estamos recebendo muitos vídeos de CFTV, que em algum momento são de péssima qualidade e posso ver como seus filtros fariam uma diferença real.  Você pode me dizer o custo e se eu poderia usá-los. <br><br>  Obrigada <br><br><div class="spoiler">  <b class="spoiler_title">Tradução</b> <div class="spoiler_text">  Eu já usei seus filtros para fins pessoais para salvar meus vídeos ruins de férias em família.  Mas eu gostaria de usar filtros comerciais no meu trabalho.  Atualmente sou policial em uma pequena unidade.  Recebemos um grande número de vídeos das câmeras de CFTV, às vezes de qualidade muito baixa, e seus filtros realmente ajudarão.  Você poderia me dizer o custo deles e posso usá-los? <br><br>  Obrigada </div></div></blockquote>  Ou aqui está um policial da Austrália: <br><blockquote>  Oi <br>  Trabalho para a polícia de Victoria, na Austrália, na unidade forense de vídeo e áudio.  Ocasionalmente, recebemos vídeos de câmeras portáteis ou montadas em veículos.  Geralmente, eles capturam imagens entrelaçadas de eventos em movimento rápido.  Em particular, a filmagem que geralmente tem mais "promessa" é filmagem das chapas de matrícula dos veículos.  Frequentemente, descobrimos que o veículo em questão se moveu significativamente entre o primeiro e o último campo capturado.  Como resultado, tentamos reconstruir todo o quadro a partir dos dois campos, com o segundo sendo traduzido, às vezes girado e, ocasionalmente, o tamanho também será diferente (conforme o veículo estiver se afastando ou em direção à câmera). Casando esses dois campos , de preferência com precisão de sub-pixel, e a reconstrução do quadro que contém a chapa de matrícula, pode ser difícil. <br>  Pelo que vi de você desentrelaçar as cenas, pode ser que seu filtro possa fazer parte, se não tudo, do que precisamos.  Para ser sincero, como nosso orçamento é bastante pequeno, é improvável que possamos comprar uma licença comercial.  Nós não vendemos o produto, é claro, nós o usamos como evidência em casos policiais.  De qualquer forma, pensei em escrever um e-mail e perguntar de qualquer maneira.  Quanto custaria uma licença?  É possível testar o produto em filmagens, para ver se é apropriado?  Faz parte do que precisamos?  Por fim, o algoritmo foi publicado?  Trabalhar com algoritmos desconhecidos é uma prática perigosa para um tribunal.  Se a evidência resultar em um homem que fica preso por 20 anos, é uma boa prática saber o porquê! <br><br>  Qualquer informação que você possa oferecer seria apreciada. <br><br>  Atenciosamente, <br>  Caseworker <br>  Audio Visual Unit <br>  Departamento de Serviços Forenses da Polícia de Victoria <br><br><div class="spoiler">  <b class="spoiler_title">Tradução</b> <div class="spoiler_text">  Oi <br>  Trabalho para a polícia de Victoria, na Austrália, no departamento de áudio e vídeo forense.  De tempos em tempos, recebemos vídeos de câmeras portáteis e DVRs.  Geralmente, esses vídeos são filmagens entrelaçadas de objetos em movimento rápido.  Em particular, o material mais importante são as placas dos veículos.  Frequentemente, descobrimos que o veículo em questão se move fortemente entre o primeiro e o último campo capturado.  Como resultado, estamos tentando restaurar um quadro inteiro de dois campos, o segundo sendo deslocado, às vezes girado e às vezes diferente em tamanho (quando o carro está viajando para ou da câmera).  Combinar esses dois campos, de preferência com precisão de meio pixel, e restaurar um quadro inteiro contendo uma placa de carro pode ser difícil. <br><br>  Vejo como você aplica o desentrelaçamento aos quadros, e talvez seus filtros possam fazer algo, se não tudo o que precisamos.  Honestamente, talvez não consigamos pagar uma licença comercial, porque nosso orçamento é bastante pequeno.  Nós não vendemos o produto, é claro, nós o usamos como evidência em casos policiais.  De qualquer forma, pensei em escrever uma carta e ainda perguntar.  Quanto custará a licença?  É possível testar o produto no material para descobrir se é adequado?  Ele faz parte do que precisamos?  Finalmente, o algoritmo foi publicado? .. Trabalhar com algoritmos desconhecidos é uma prática perigosa em tribunal.  Se a evidência leva uma pessoa a ir para a prisão por 20 anos, é útil saber o porquê. <br><br>  Seremos gratos por qualquer informação que você possa nos fornecer. <br><br>  Atenciosamente <br>  Investigador <br>  Divisão de Áudio e Vídeo <br>  Departamento Forense da Polícia de Victoria </div></div></blockquote>  Observe que a carta é muito atenciosa, uma pessoa está preocupada com o algoritmo sendo publicado e com a responsabilidade pela recuperação incorreta. <br><br>  Às vezes, eles apenas no processo de correspondência admitem que são da polícia.  Por exemplo, os carabinieri da Itália gostariam de ajuda: <br><blockquote>  Dr.  Vatolin <br>  Obrigado pela resposta. <br>  A resposta vale também para as forças policiais (investigação Carabinieri <br>  científico para a PARMA ITÁLIA)? <br>  Para qual software eles associam seus algoritmos a você. <br>  Seríamos muito. <br><br><div class="spoiler">  <b class="spoiler_title">Tradução</b> <div class="spoiler_text">  Dr.  Batolin <br>  Obrigado pela resposta. <br>  Isso é adequado para a polícia (unidade de investigação Carabinieri para o PARMA ITÁLIA)? <br>  Eles estão interessados ​​em qual software seus algoritmos usam? <br>  Seremos gratos. </div></div></blockquote>  E, claro, muitos apelos das pessoas comuns ... <br><br><h2>  Aumente isso!  Você sente muito pelo botão certo para pressionar? </h2><br>  É claro que todo esse fluxo de chamadas não aparece do zero. <br><br>  "Culpar" principalmente filmes e programas de TV. <br><br>  Por exemplo, aqui em 3 segundos o quadro do vídeo compactado é aumentado em 50 vezes e pelo reflexo nos óculos eles veem evidências: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/I_8ZH1Ggjk0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  E existem muitos desses momentos em filmes e séries modernos.  Por exemplo, neste vídeo, coletamos esses episódios de maneira épica de um pacote de programas de TV, não demore dois minutos para assistir: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/LhF_56SxrGk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  E quando você vê isso em todos os filmes, então o último porco-espinho fica claro que tudo o que você precisa é ter um gênio da computação competente, uma combinação de algoritmos modernos e resta apenas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">"PARAR!"</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">e "Aprimore!"</a>  .  E pronto!  Um milagre vai acontecer! <br><br>  No entanto, os roteiristas não param nesta recepção já hackeada e sua imaginação desenfreada vai além.  Aqui está um exemplo muito monstruoso.  Detetives corajosos para refletir no aluno da vítima receberam uma foto do agressor.  De fato, o reflexo nos óculos já estava lá.  Isso é comum.  Vamos seguir em frente!  Só que a resolução da câmera de CCTV na escada acabou sendo bastante aleatória como o telescópio do Hubble: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/3uoM5kfZIQ0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  No "Profeta" (00:38:07): <div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e82/109/09e/e8210909e53e8966a6484d35b096bf0f.gif"></div><br>  Em “Avatar” (1: 41: 04–1: 41: 05), o algoritmo de nitidez, a propósito, é um tanto incomum em comparação com outros filmes: ele primeiro afia em certos lugares e depois de uma fração de segundo puxa o restante da imagem, t .e.  primeiro a metade esquerda da boca e depois a direita: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fff/54d/603/fff54d603344fb7b80c8a5c93aaca7ab.gif"></div><br>  Em geral, em filmes muito populares que são assistidos por centenas de milhões, a nitidez da imagem é feita em um clique.  <b>Todas as pessoas (nos filmes) fazem isso!</b>  Então, por que você, especialistas tão espertos, não pode fazer isso ??? <br><br><hr>  "Eu sei que isso é fácil!"  E me disseram definitivamente que você está fazendo isso!  Você está com preguiça de pressionar este botão? <br><br>  <i>// Oh querido ... Roteiristas malditos com sua imaginação selvagem ...</i> <br><br>  - Entendo que você está ocupado, mas trata-se de sua ajuda ao estado na solução de um crime importante! <br><br>  <i>// Nós entendemos.</i> <br><br>  - Talvez seja sobre o dinheiro?  Quanto você precisa pagar? <br><br>  <i>// Bem, como explicar brevemente que não é que não precisamos de dinheiro ... E então novamente, e novamente ...</i> <hr><br>  Qualquer coincidência das citações acima com diálogos reais é completamente aleatória, mas, em particular, este texto é escrito para enviar uma pessoa a lê-lo cuidadosamente primeiro e só depois retornar a ligação. <br><blockquote>  <b>Conclusão:</b> Devido ao fato de que a cena com o aumento de imagens das câmeras de CFTV em um clique se tornou um selo do cinema moderno, um grande número de pessoas está sinceramente convencido de que é muito simples ampliar um fragmento de um quadro de uma câmera barata ou de um gravador de vídeo barato.  O principal é como perguntar (bem, ou comandar, é assim que acontece). </blockquote><h2>  De onde as pernas crescem </h2><br>  É claro que todo esse fluxo de chamadas não é obtido do zero.  Estamos realmente envolvidos na melhoria de vídeo há cerca de 20 anos, incluindo vários tipos de recuperação de vídeo (a propósito, existem vários tipos), e nossos exemplos serão mais baixos nesta seção. <br><br>  Um aumento "inteligente" na resolução de artigos científicos é geralmente chamado de Super Resolução (SR, abreviado).  O Google Scholar, mediante solicitação, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Super Resolution</a> encontra 2,9 milhões de artigos, ou seja,  o tópico foi, por assim dizer, bastante bem desenterrado, e um grande número de pessoas lidou com ele.  Se você seguir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">o link</a> , haverá um mar de resultados, um mais bonito que o outro.  No entanto, vale a pena aprofundar, o quadro, como sempre, não se torna tão pastoral.  O tema SR tem duas direções: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Super-resolução de vídeo</a> (0,4 milhão de artigos) - a restauração real usando quadros anteriores (e às vezes subsequentes), <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Super-resolução de imagem</a> (2,2 milhões de artigos) - aumento "inteligente" na resolução usando apenas um quadro.  Como no caso de uma figura para obter informações sobre o que realmente não estava em lugar nenhum, os algoritmos de uma maneira ou de outra completam a figura (ou, relativamente falando, “pensam”) a figura - o que poderia estar lá.  O principal critério para isso é que o resultado seja o mais natural possível ou o mais próximo possível do original.  E é claro que esses métodos não são adequados para restaurar o que era “realmente”, apesar de ampliar a imagem para que pareça melhor, por exemplo, ao imprimir (quando você tem uma foto exclusiva, mas não há versão em alta resolução) ) Tais métodos são muito possíveis. <br></li></ul><br>  Como você pode ver, 0,4 milhão versus 2,2 - ou seja, 5 vezes menos pessoas estão envolvidas na recuperação real.  Felizmente, o tópico “faça maior, apenas bonito” é muito procurado, inclusive no setor (o notório zoom digital de smartphones e saboneteiras digitais).  Além disso, se você se aprofundar ainda mais, fica claro rapidamente que um número significativo de artigos sobre a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">super resolução de vídeo</a> também é um aumento na resolução do vídeo sem recuperação, porque a recuperação é difícil.  Como resultado, podemos dizer que aqueles que "fazem lindamente" são cerca de 10 vezes mais do que aqueles que realmente estão tentando restaurar.  A propósito, uma situação bastante comum na vida. <br><br>  Nós vamos ainda mais fundo.  Muitas vezes, os resultados do algoritmo são muito bons, mas são necessários, por exemplo, 20 quadros para a frente e 20 quadros para trás, e a velocidade de processamento de um quadro é de cerca de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">15 minutos</a> ao usar a GPU mais avançada.  I.e.  por 1 minuto, o vídeo precisa de 450 horas (quase 19 dias).  Ops-ss ... Concordo, isso não é nada parecido com o instante “Zoom it!”  dos filmes.  Regularmente, existem algoritmos que funcionam por vários dias por quadro.  Para artigos, um resultado melhor geralmente é mais importante que o tempo de trabalho, porque a aceleração é uma tarefa difícil separada e é mais fácil comer um elefante grande em partes.  Essa é a diferença entre vida e cinema ... <br><br>  A solicitação de algoritmos em execução no vídeo a uma velocidade razoável deu origem a uma direção separada de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Super resolução rápida de vídeo</a> - 0,18 milhão de artigos, incluindo artigos "lentos" que são comparados com os "rápidos", ou seja,  o número real de artigos sobre esses métodos é exagerado.  Observe que, entre as abordagens "rápidas", a porcentagem de especulativas, ou seja,  sem recuperação real, maior.  Consequentemente, a porcentagem de recuperação honesta é menor. <br><br>  A imagem, você vê, está ficando clara.  Mas isso, é claro, está longe de tudo. <br><br>  Que outros pontos afetam significativamente a obtenção de um bom resultado? <br><br>  Em primeiro lugar, o ruído é muito influente.  Abaixo está um exemplo de uma restauração dupla da resolução em um vídeo muito barulhento: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/461/c9c/bd5/461c9cbd5b7b8d0a7b50f75471ebe941.png"></div><br>  <i>Fonte: materiais do autor</i> <br><br>  O principal problema neste fragmento não é mesmo com os ruídos habituais, mas com o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">moiré</a> colorido na camisa, que é difícil de processar.  Alguns podem dizer que grandes ruídos não são um problema hoje.  Isto não é verdade.  Observe os dados dos DVRs e câmeras de CFTV do carro no escuro (exatamente quando eles são mais procurados). <br><br>  No entanto, moiré também pode ocorrer relativamente "limpo" em termos de ruído de vídeo, como na cidade abaixo (os exemplos abaixo são <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">baseados em nosso trabalho</a> ): <br><br> <a href=""><img src="https://habrastorage.org/webt/0a/-x/0z/0a-x0zj47mmkgsa79aiojqtrfco.gif"></a> <br>  <i>Fonte: materiais do autor</i> <br><br>  Em segundo lugar, para uma recuperação ideal, é necessária uma previsão de movimento próxima ao ideal entre os quadros.  Por que isso é difícil é um grande tópico separado, mas isso explica por que as cenas com um movimento panorâmico da câmera são muitas vezes restauradas muito bem e as cenas com um movimento relativamente caótico são extremamente difíceis de recuperar, mas com elas você pode obter um bom resultado em algumas situações: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4bd/7bb/3da/4bd7bb3da22d2904311d2da762331b5e.gif"></div>  <i>Fonte: materiais do autor</i> <br><br>  E, finalmente, aqui está um exemplo de recuperação de texto: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a98/ad0/a00/a98ad0a0062a2fa1a7b4c602d26e2edd.png"></div><br>  <i>Fonte: materiais do autor</i> <br><br>  Aqui, o plano de fundo se move bastante bem e o algoritmo tem a capacidade de "vagar": <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a5e/1a4/542/a5e1a4542ea2b0a441c6160e4c9eaba0.png"></div><br><br>  Em particular, se compararmos uma inscrição muito pequena à direita da mão, incluindo a ampliação com <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">interpolação bicúbica</a> clássica, a diferença será claramente visível: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9d4/d00/dfc/9d4d00dfc6a3364dfae37e8637708993.png"></div><br>  Pode-se ver que para a interpolação bicúbica é quase impossível ler o ano, para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Lanczos4</a> , que é amado por quem muda semiprofissionalmente a resolução do vídeo para obter nitidez, as bordas são mais nítidas, é claro, mas ainda é impossível ler o ano.  Não comentamos o Topázio comercial, mas lemos claramente a inscrição e você pode ver que é provável que isso seja 1809. <br><blockquote>  <b>Conclusões:</b> <br><br><ul><li>  Milhares de pesquisadores no mundo estão empenhados em aumentar a resolução e milhões de artigos foram publicados sobre esse assunto.  Devido a isso, cada smartphone possui um “zoom digital”, que geralmente é objetivamente melhor do que os algoritmos para aumentar os programas convencionais, e cada TV FullHD pode exibir vídeo SD, geralmente mesmo sem artefatos característicos da alteração da resolução. <br></li><li>  A recuperação de uma imagem real de um vídeo é muito inferior a 10% dos envolvidos na Super Resolução. Além disso, a maioria dos algoritmos de recuperação é extremamente lenta (até vários dias de cálculos por quadro). <br></li><li>  Na maioria dos casos, a recuperação é projetada para garantir que as altas frequências do vídeo sejam mais ou menos preservadas e, portanto, não funcionem no vídeo com artefatos de compactação significativos.  E, como nas configurações das câmeras de CFTV, a taxa de compactação geralmente é escolhida com base no desejo de economizar mais horas (ou seja, o vídeo é compactado com mais força e as altas frequências são “mortas”), torna-se quase impossível restaurar esse vídeo. <br></li></ul></blockquote><br><h2>  Como é o SR no setor </h2><br>  Para ser sincero, observamos que hoje todos os algoritmos de aumento de resolução (ou pelo menos comprados) estão disponíveis para todos os fabricantes de TV (você precisa criar imagens HD a partir de imagens SD em tempo real), para todos os fabricantes de smartphones (o que é chamado de “zoom digital” em publicidade) etc. .d.  Falaremos sobre os resultados do Google (e não apenas).  Primeiro, porque o Google é muito bom e sem muito patos e marketing descreve os resultados em seu blog - e isso é extremamente bom.  Em segundo lugar, porque os fabricantes de smartphones (por exemplo, uma empresa coreana muito conhecida) não evitam usar, digamos, o Photoshop na publicidade de suas tecnologias (qual é a diferença - as pessoas ainda engolem) - e isso é desagradável.  Em geral, vamos falar sobre aqueles que descrevem sua tecnologia com bastante honestidade. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Em 2016, o Google publicou</a> resultados bastante interessantes do algoritmo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">RAISR (Rapid and Accurate Image Super Resolution)</a> usado no smartphone Pixel 2. Nas imagens de maior sucesso, o resultado foi ótimo: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/78c/785/e21/78c785e214d760cb58f8ea2e2d78b70a.png"></div><br>  <i>Fonte: <a href="">Blog do Google AI</a></i> <br><br>  O algoritmo foi um conjunto de filtros usados ​​após a classificação do ML, e comparado com a interpolação bicúbica (chicote tradicional), o resultado foi o seguinte: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/67b/1f3/a53/67b1f3a5335bbced94d53be35ba3881d.png"></div><br>  <i>Em ordem: interpolação bicúbica original, RAISR</i> <br><br>  Mas foi a interpolação de quadro único e, em exemplos "malsucedidos", como a folhagem abaixo, a imagem ficou muito desagradável - depois de ampliada, a imagem tornou-se visivelmente "sintética".  Ele mostrou exatamente o efeito pelo qual o zoom digital dos smartphones modernos não é apreciado: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/g0/y7/xw/g0y7xw8cipif-g-cw3yuljfmivg.png"></div><br>  O milagre, na verdade, não aconteceu, e o Google publicou honestamente e imediatamente um contra-exemplo, ou seja,  imediatamente delineou os limites de aplicabilidade de sua abordagem e salvou as pessoas de expectativas excessivas (típicas do marketing convencional). <br><br>  No entanto, menos de dois anos depois, a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">continuação do trabalho</a> usado no Google Pixel 3 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">foi publicada</a> e melhora drasticamente a qualidade de suas filmagens, que já é uma super resolução honesta com vários quadros, ou seja.  algoritmo de recuperação de resolução de vários quadros: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/83b/3ff/950/83b3ff950cf9959b2acb3d11a134ef50.gif"></div><br>  <i>Fonte: <a href="">Blog do Google AI</a></i> <br><br>  A imagem acima mostra uma comparação dos resultados do Pixel 2 e Pixel 3, e os resultados parecem muito bons - a imagem realmente ficou muito mais clara e é claramente visto que isso não está "pensando", mas realmente restaurando detalhes.  Além disso, um leitor profissional atencioso terá perguntas sobre dois tubos duplos verticais à esquerda.  A resolução aumentou claramente, enquanto a etapa de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">alias</a> (um sinal de resolução real) parece estranhamente próxima.  O que foi aquilo? <br><br>  Em poucas palavras, analisaremos o algoritmo.  Os colegas mudaram a interpolação do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">padrão Bayer</a> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/355/0ba/e72/3550bae72349a1d9d9705e1e8c69cdd3.png"></div><br>  O fato é que 2/3 das informações em uma imagem real são realmente interpoladas.  I.e.  sua foto JÁ está desfocada e “desfocada”, mas com um nível de ruído real, isso não é tão significativo.  A propósito, a capacidade de usar algoritmos de interpolação mais complexos criou programas populares com a conversão RAW da mais alta qualidade para fotografias (a diferença entre o algoritmo simples incorporado em cada câmera e o algoritmo complexo de um programa especializado é geralmente perceptível a olho nu quando a imagem é ampliada). <br><br>  Os colegas do Google usam o fato de que a grande maioria das fotos de smartphones é tirada com as mãos, ou seja,  a câmera treme levemente: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/864/59b/d05/86459bd058ec0edfdd5f3b1d0c9b287b.gif"></div><br>  <i>Fonte: <a href="">Blog do Google AI</a> (imagem de vários quadros alinhada no nível do pixel para mostrar a mudança de subpixels)</i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como resultado, se você tirar alguns quadros e avaliar a mudança (e o ferro, capaz de construir um mapa de estimativa de movimento com precisão de um quarto de pixel, estiver em qualquer smartphone com suporte a H.264), obteremos um mapa de mudança. Fiel à animação acima, é claramente visto que, com um nível de ruído real, a construção de um mapa de deslocamento com precisão de subpixel é uma tarefa muito não trivial, mas algoritmos muito bons apareceram nessa área nos últimos 20 anos. Claro, às vezes, e eles têm dificuldade. Por exemplo, no exemplo acima, algo pisca em um quadro na parte superior do corrimão da escada. E ainda é uma cena estática, não há objetos em movimento que às vezes não se movem, mas giram, mudam de forma, se movem rapidamente, deixando grandes áreas de abertura (cujo loop não deve ser visível após o processamento). O exemplo abaixo mostra claramenteo que acontece com objetos em movimento rápido, se você desativar o processamento especial desses casos (desativado à esquerda, ativado à direita, se você clicar em, os blocos de processamento serão claramente visíveis):</font></font><br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/2e8/3d1/bb2/2e83d1bb215c546de4eb6d55524f865f.png"></a> <br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fonte: </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Blog do Google AI (recomendado para clicar e ver em alta resolução)</font></font></a></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Exemplos difíceis são chamas, ondulações, brilho do sol na água, etc. Em geral, mesmo no problema "simples" de determinar a mudança, há muitos momentos não triviais que complicam significativamente a vida do algoritmo. No entanto, agora não se trata disso. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Curiosamente, mesmo que a câmera esteja completamente estacionária (por exemplo, montada em um tripé), você pode fazer o sensor se mover através do controle do </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">módulo de estabilização óptica</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (OIS - Optical Image Stabilization). Como resultado, obtemos as mudanças de subpixel desejadas. No Pixel 3, o suporte ao OIS é implementado, e você pode pressionar o telefone contra o vidro e observar com interesse como o OIS começa </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a mover a imagem ao longo de uma elipse (aproximadamente, como este link)</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, ou seja, mesmo neste caso de montagem em um tripé, difícil para ele, a Super Resolução poderá trabalhar e melhorar a qualidade. </font><font style="vertical-align: inherit;">No entanto, a maior parte das filmagens de smartphones é na mão. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como resultado, temos informações adicionais para criar uma foto com resolução maior:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/11d/67a/0a2/11d67a0a2127af61576cadffbfeba49b.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Como mencionado acima, a consequência direta do SR é uma diminuição significativa no nível de ruído; em alguns casos, é muito perceptível: </font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3e1/cd0/1b1/3e1cd01b1e996fc11a6309dca47b0e17.png"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fonte: </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Blog do Google AI</font></font></a></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Observe que recuperação também significa restauração pelo número de bits por componente.</font></font> I.e.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">resolvendo formalmente o problema de aumentar a resolução, o mesmo mecanismo sob certas condições pode não apenas suprimir o ruído, mas também transformar o quadro em HDR. É claro que hoje o HDR raramente é usado, mas isso, como vê, é um bom bônus. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O exemplo abaixo mostra uma comparação de imagens obtidas ao fotografar no Pixel 2 e no Pixel 3 após SR com qualidade de sensor comparável. A diferença de ruído e a diferença de clareza são claramente visíveis:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/74c/17b/e38/74c17be383c81603e91124d9b4fd04c9.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para quem gosta de ver os detalhes, há </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">um álbum</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> no qual a Super Resolution do Google (nome de marketing Super Res Zoom) pode ser apreciada em toda a sua glória no espectro da escala de zoom da imagem em um smartphone (mudança de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">FoV</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ): </font><font style="vertical-align: inherit;">como eles escrevem modestamente - eles deram um passo mais perto da qualidade de fotografia dos smartphones à qualidade das câmeras profissionais. </font><font style="vertical-align: inherit;">Para ser sincero, observamos que as câmeras profissionais também não param. Outra coisa é que, com vendas menores, as mesmas tecnologias custarão mais para o usuário. No entanto, o SR já está aparecendo em câmeras profissionais. </font><b><font style="vertical-align: inherit;">UPD:</font></b><font style="vertical-align: inherit;"> Como exemplo (o último link é uma comparação):</font></font><br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/321/e5e/40d/321e5e40d89b28612139434735c76fe0.png"></a> <br><br><font style="vertical-align: inherit;"></font><br><br><font style="vertical-align: inherit;"></font><br><br> <b><font style="vertical-align: inherit;"></font></b> <br><font style="vertical-align: inherit;"></font><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Testing Sony's New Pixel Shift Feature in the a7R III</a> ,     2     ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a>    ,               ), <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">   Olympus E-M5 Mark II</a>  16  40 , <br></li><li>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> Super Resolution   Pentax K-1</a> , <br></li><li>  : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Pixel-Shift Shootout: Olympus vs. Pentax vs. Sony vs. Panasonic</a> —    <b>Pentax K-1, Sony a7R III, Olympus OM-D E-M1 Mark II  Panasonic Lumix DC-G9.</b> ,    ,         ,    Pentax K-1. <br></li></ul><br><blockquote> <b>:</b> <br><br><ul><li>  Super Resolution    ,        ,         . <br></li><li>  SR: Image Super Resolution —    ( ),     . <br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Os principais bônus dos algoritmos de recuperação são redução de ruído, refinamento de detalhes, HDR “mais honesto”, qualidade de imagem claramente mais visível em televisores de tela grande. </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Toda essa magnificência foi possível graças a um aumento cardinal (em cerca de 3 ordens de grandeza no número de operações) na complexidade dos algoritmos de processamento de fotos ou, mais precisamente, em um quadro de vídeo. </font></font><br></li></ul></blockquote><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Resultados Yandex </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Como eles ainda perguntam nos comentários, vou dizer algumas palavras sobre o Yandex, que publicou sua versão da Super Resolução no ano passado: </font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/02a/ec9/09f/02aec909f0cfd75bf9902a25ee85acc9.png"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fonte: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://yandex.ru/blog/company/oldfilms</font></font></a></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> E aqui estão alguns exemplos de desenhos animados:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d6a/e9e/2cb/d6ae9e2cba814e1c02a4060ccb545280.png"></div><br>  <i>Fonte: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://yandex.ru/blog/company/soyuzmultfilm</a></i> <br><br>  O que foi aquilo?  Yandex repetiu a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tecnologia do Google em 2016</a> ? <br><br>  Na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">página de descrição da</a> tecnologia da Yandex (nome de marketing DeepHD), é vinculado apenas à Super Resolução de Imagem.  Isso significa que obviamente existem contra-exemplos nos quais o algoritmo estraga a imagem e eles são mais comuns do que para algoritmos honestos de recuperação.  Mas cerca de 80% dos artigos são dedicados ao tópico e o algoritmo é mais fácil de implementar. <br><br>  Essa tecnologia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">também</a> foi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">descrita</a> em um hub (é interessante que o autor do artigo tenha se formado em nosso laboratório), mas, como você pode ver nos comentários, os autores não responderam a nenhuma das minhas perguntas enquanto responderam às outras.  E esses não são os autores dos vilões, mas a política da empresa (em outros posts, se você olhar de perto, também muitas vezes não há respostas para perguntas de especialistas).  Para os blogs de empresas de tecnologia, relutam em aprofundar a discussão sobre detalhes de implementação ou tecnologia.  Especialmente se isso criar uma melhor impressão da tecnologia / produto.  Ou os concorrentes podem cortar a mesma coisa mais rapidamente.  Novamente, o marketing é responsável pelas postagens, e esse é o trabalho direto delas - criando uma impressão favorável dos produtos da empresa, independentemente da qualidade dos próprios produtos.  Daí a desconfiança frequente das informações provenientes do marketing. <br><br>  Em geral, vale a pena ser muito cético em relação às fotos das empresas da série "como fizemos tudo bem" pelas seguintes razões: <br><br><ul><li>  Os autores dos algoritmos de processamento estão cientes de que praticamente <b>não</b> existem <b>algoritmos que, em alguns casos, não gerariam artefatos.</b>  E, de fato, uma das principais tarefas do desenvolvedor é reduzir a porcentagem desses casos (ou a visibilidade dos artefatos nesses casos), mantendo a qualidade em outros casos.  E muitas vezes isso NÃO tem êxito: <br><br><ul><li>  Ou os artefatos são tão fortes e difíceis de corrigir que toda a abordagem é rejeitada.  Na verdade, esse é o caso, talvez (surpresa-surpresa!), Da maioria dos artigos.  Imagens divinas em alguns casos (que foram gravadas) e “não funciona” no restante. <br></li><li>  Ou (e essa é uma situação comum para empresas de tecnologia práticas), é necessário sacrificar em média alguma qualidade para que os artefatos nos piores casos possam ser tolerados. <br></li></ul><br></li></ul>  Assim, quando exemplos ruins não são publicados (clássicos para empresas) ou publicados de forma limitada e com padrões (clássicos para artigos) - esse é o caso mais comum de pessoas enganosas sobre as propriedades de uma tecnologia / algoritmo. <br><br><ul><li>  <b>Outro equívoco comum sobre algoritmos de processamento é o uso de parâmetros (incluindo parâmetros internos) do algoritmo.</b>  Os algoritmos, por acaso, têm parâmetros e usuários - e esta também é a norma - como ter no máximo um botão "ativar".  E mesmo se houver configurações, o usuário em massa não as utiliza.  É por isso que, ao comprar tecnologia, "para cem" vezes, eles perguntam novamente: "Isso com certeza é uma máquina completa?"  e peça muitos exemplos. <br><br><ul><li>  Assim, uma história comum é a publicação de um resultado obtido com determinados parâmetros.  Felizmente, o desenvolvedor os conhece bem, e mesmo quando existem cinquenta (a situação real!), Ele os pega muito rapidamente, para que a imagem seja mágica.  Exatamente essas fotos costumam ser anunciadas. <br></li><li>  Além disso, o desenvolvedor pode até ser contra.  O marketing vê os novos exemplos enviados e diz: "nada é visível neles, na última apresentação você teve exemplos normais!"  E então eles podem tentar explicar a eles que novos exemplos são o que as pessoas realmente veem e, na última apresentação, foram mostrados resultados potenciais que podem ser alcançados por estudos preliminares do início do projeto.  Isso não incomoda ninguém.  As pessoas terão a imagem "onde você pode ver".  Em alguns casos, até grandes empresas usam o photoshop.  Brincar é servido, senhores!  ) <br></li></ul><br></li></ul><ul><li>  Além disso, <b>quando se trata de vídeo - ele abre espaços enormes para a <s>máquina ...</s> bom marketing!</b>  Como regra geral, os quadros são dispostos e a qualidade do vídeo compactado sempre oscila e depende da massa de parâmetros.  Novamente - várias tecnologias podem ser aplicadas corretamente, o tempo de processamento pode ser diferente.  E isso não é tudo, o escopo é ótimo. <br><br><ul><li>  A publicidade da Yandex afirma que a tecnologia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">DeepHD funciona em tempo real, então hoje você pode assistir canais de televisão usando-a</a> .  Foi explicado acima que a velocidade de operação é o calcanhar de Aquiles da Super Resolução.  A vantagem das redes neurais, é claro, é que, durante muito tempo estudando, elas podem funcionar muito rapidamente em alguns casos, mas ainda assim eu procuraria (com grande interesse profissional) em que resolução e qualidade o algoritmo funciona em tempo real.  Geralmente, várias modificações do algoritmo são criadas e em altas resoluções em tempo real, <b>muitos</b> "chips" (críticos para a qualidade) precisam ser desativados.  <b>Demais.</b> <br></li><li>  Nos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">exemplos</a> em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">preto e branco</a> , uma análise mais detalhada revela que o brilho local está mudando.  Como o SR correto não altera o brilho, parece que algum outro algoritmo funcionou, possivelmente não um (os resultados mostram que esse não é o processamento de quadro único, ou melhor, parece não apenas).  Se você olhar para uma peça maior (pelo menos 100 quadros), a imagem ficará nítida.  No entanto, medir a qualidade do vídeo é um tópico muito grande e separado. <br></li></ul><br></li></ul><blockquote>  <b>Conclusões:</b> <br><br><ul><li>  Você precisa entender que os profissionais de marketing costumam usar seus truques precisamente porque funcionam (e como!).  A esmagadora maioria das pessoas <s>não lê o hub</s>  O que regularmente leva a todos os tipos de distorções.  Desejo que todos menos sejam anunciados, especialmente quando a narrativa está no seu melhor e realmente quer acreditar em um milagre! <br></li><li>  E, claro, é muito bom que o Yandex também trabalhe no tópico e crie seu próprio SR (mais precisamente, sua própria família SR). <br></li></ul></blockquote><br><h2>  Perspectivas </h2><br>  Vamos voltar para onde começamos.  O que fazer para quem deseja aumentar o vídeo compactado?  Isso é tudo ruim? <br><br>  Como descrito acima, mesmo uma ligeira mudança na imagem na região, literalmente no nível do ruído, é crítica para os algoritmos de recuperação "honesta".  Ou seja, as altas frequências na imagem e sua mudança entre os quadros são críticas. <br><br>  Nesse caso, a principal coisa pela qual a compactação de vídeo é realizada é a remoção do ruído entre os quadros.  No exemplo abaixo, a diferença entre quadros de um vídeo barulhento antes da compensação de movimento, após compensação (com compactação fraca) e após compactação perceptível - sente a diferença (o contraste é aumentado cerca de 6 vezes para que os detalhes possam ser vistos): <br><img src="https://habrastorage.org/getpro/habr/post_images/4a0/867/c18/4a0867c1839ba6acea0aa16a7a5a408c.png"><br>  <i>Fonte: palestras do autor sobre algoritmos de compressão</i> <br><br>  Pode-se ver claramente que, do ponto de vista do codec, a área ideal é a área na qual o movimento foi totalmente compensado e na qual não é necessário gastar mais bits.  Bem, um pouco pode ser gasto, algo minimamente corrigido.  E pode haver algumas dessas áreas.  Portanto, a Super Resolução perde seu “pão principal” - informações sobre o que há neste local em outros quadros, levando em consideração a mudança do subpixel. <br><br>  Se você olhar para os artigos, mesmo para um JPEG relativamente simples, a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">restauração jpeg</a> contém 26 mil resultados e a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">recuperação jpeg</a> - 52 mil, e isso juntamente com a restauração de arquivos quebrados, etc.  Para o vídeo, a situação é pior que a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">restauração mpeg</a> - 22 mil, ou seja,  Obviamente, o trabalho está em andamento, mas a escala da escala do trabalho em Super Resolução não é comparável.  Há cerca de uma ordem de magnitude menos trabalhosa do que restaurar a resolução de vídeo e duas ordens de magnitude menores que a Super-resolução de Imagem.  Duas ordens é muito.  Também fizemos uma abordagem ao projétil (já que fazemos compressão e processamento há muito tempo), há algo com o que trabalhar, especialmente se a qualidade estiver oscilando ou usando algo como M-JPEG (mais recentemente, uma imagem comum em vigilância por vídeo).  Mas todos serão casos especiais. <br><br>  Os resultados dos artigos dos links acima também mostram que os resultados às vezes são muito bonitos, mas obtidos em casos muito especiais.  I.e.  amanhã, em todos os smartphones, essa função, infelizmente, não aparecerá.  Esta é uma má notícia.  Bom - depois de amanhã e em um computador com uma boa GPU - aparecerá com certeza. <br><br>  Razões: <br><br><ul><li>  Os dispositivos de armazenamento (cartões SD para registradores, discos para câmeras de CFTV, etc.) estão gradualmente ficando mais baratos e a taxa de bits média para salvar vídeos está aumentando. <br></li><li>  Além disso, durante a compactação, eles mudam gradualmente para os padrões das próximas gerações (por exemplo, no HEVC), o que significa uma melhoria notável na qualidade com a mesma taxa de bits.  Os dois últimos pontos significam que gradualmente a qualidade do vídeo será mais alta e, a partir de algum momento, algoritmos de super-resolução de vídeo bem desenvolvidos começarão a funcionar. <br></li><li>  Finalmente, os algoritmos estão sendo aprimorados.  As realizações dos algoritmos baseados em aprendizado de máquina nos últimos 4 anos são especialmente boas.  Nesse sentido, com alta probabilidade, podemos esperar algo como isto: <br></li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/fce/82a/6a9/fce82a6a91aa6c5d654bd22c26bbf3d4.png"><br><br>  I.e.  o algoritmo usará explicitamente as informações de movimento recebidas do codec e, em seguida, esses dados serão alimentados a uma rede neural treinada para recuperar artefatos específicos para codecs específicos.  Atualmente, esse esquema parece bastante viável. <br><br>  Mas, em qualquer caso, você precisa entender claramente que a recuperação atual é, via de regra, um aumento de duas vezes na resolução.  Menos comumente, em alguns casos, quando o material de origem não foi compactado ou quase não foi compactado, podemos falar de 3 a 4 vezes.  Como você pode ver, isso não é nem perto de 100 a 1000 vezes a ampliação dos filmes, quando 1,5 pixels de uma gravação noturna silenciosa se transforma em um número de carro de excelente qualidade.  O gênero de "ficção científica" deve ser atribuído de fato a uma porcentagem maior de filmes e programas de TV. <br><br>  E, é claro, haverá tentativas de fazer algo universal, dentro da estrutura da tendência da moda "o principal é cortar mais camadas".  E aqui vale a pena advertir contra reações de "aplausos" aos materiais publicitários sobre esse tópico.  As redes neurais são a estrutura mais conveniente para demonstrar milagres e todo tipo de especulação.  O principal é escolher corretamente a amostra de treinamento e os exemplos finais.  E pronto!  Veja o milagre!  Muito conveniente em termos de investidores, a propósito.  Ou seja, é extremamente importante que a eficiência das tecnologias seja confirmada por alguém independente em um grande número de exemplos heterogêneos, o que raramente é demonstrado.  Para as empresas, mesmo dando um ou dois exemplos quando a tecnologia não funciona, hoje é equiparado a um feito civil. <br><br>  Bem, para que a vida não pareça mel, lembro que a chamada transcodificação é popular hoje em dia, quando na verdade você precisa trabalhar com um vídeo que originalmente foi encolhido por um algoritmo e depois encima por outro, enquanto outros vetores de movimento são usados, outros altos são destruídos novamente frequências etc.  E o fato de uma pessoa ver tudo bem lá não significa que o algoritmo que processa esse vídeo realmente fará milagres.  Não será possível restaurar vídeos pesados, embora em geral a Super Resolução se desenvolva rapidamente nos próximos 10 anos. <br><br><blockquote>  <b>Conclusões:</b> <br><br><ul><li>  Lembre-se de que o que você vê nos filmes e como é na vida real é muito diferente.  E não apenas em termos de recuperação de vídeo altamente compactado! <br></li><li>  Geralmente, os algoritmos modernos aumentam as resoluções em 2 vezes, com menos frequência - um pouco mais, ou seja,  não 50 vezes, familiar nos filmes, logo tem que esperar. <br></li><li>  A área de Super Resolução está crescendo e você pode esperar um desenvolvimento ativo da Restauração de Vídeo nos próximos anos, incluindo recuperação após a compactação. <br></li><li>  Mas a primeira coisa que veremos é todo o tipo de especulações sobre o tópico, quando os resultados demonstrados exageram bastante as reais capacidades dos algoritmos.  Cuidado! <br></li></ul></blockquote>  No final do ano passado, demos uma palestra “Redes neurais no processamento de vídeo - mitos e realidade”.  Talvez possamos colocá-la aqui. <br><br>  Fique atento! <br><br><h2>  Agradecimentos </h2><br>  Gostaria de agradecer cordialmente: <br><br><ul><li>  Laboratório de Computação Gráfica VMK Moscow State University  MV Lomonosov para poder de computação e não apenas <br></li><li>  nossos colegas do grupo de vídeos, graças a quem os algoritmos acima foram criados, e especialmente Karen Simonyan, autora do artigo cujos resultados foram mostrados acima e que agora trabalha no Google DeepMind, <br></li><li>  pessoalmente Konstantin Kozhemyakov, que fez muito para tornar este artigo melhor e mais visual, <br></li><li>  Google para um excelente blog e descrições relativamente corretas das tecnologias criadas, e Yandex por competir muito bem em uma ampla frente - o Google é praticamente o único exemplo de sucesso em um país onde os serviços do Google não são proibidos, <br></li><li>  Habrovchan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">denisshabr</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">JamboJet</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">iMADik</a> pela dica e links para câmeras profissionais de quadros múltiplos SR, <br></li><li>  e finalmente, muito obrigado a Vyacheslav Napadovsky, Evgeny Kuptsov, Stanislav Grokholsky, Ivan Molodetsky, Alexei Soloviev, Evgeny Lyapustin, Yegor Sklyarov, Denis Kondranin, Alexandra Anzina, Roman Kazantsev e Gleb Ishelev por esta grande quantidade de comentários úteis melhor! <br></li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt439766/">https://habr.com/ru/post/pt439766/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt439756/index.html">Por que preciso de um gerador termoacústico?</a></li>
<li><a href="../pt439758/index.html">No meio do caminho "Juno"</a></li>
<li><a href="../pt439760/index.html">Os engenheiros "distorceram" a luz da fibra - uma nova tecnologia acelerará a transferência de dados centenas de vezes</a></li>
<li><a href="../pt439762/index.html">No ensino superior, programadores e empregos de colarinho azul</a></li>
<li><a href="../pt439764/index.html">Frutas Corporativas</a></li>
<li><a href="../pt439768/index.html">Como funciona um código de barras?</a></li>
<li><a href="../pt439772/index.html">Escrevendo testes de unidade no Swift para testar tarefas assíncronas</a></li>
<li><a href="../pt439774/index.html">Automatize o teste de seletores de redux no aplicativo</a></li>
<li><a href="../pt439776/index.html">Frontend Weekly Digest (4-10 de fevereiro de 2019)</a></li>
<li><a href="../pt439778/index.html">O resumo de materiais frescos do mundo do front-end da última semana n ° 351 (4 a 10 de fevereiro de 2019)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>