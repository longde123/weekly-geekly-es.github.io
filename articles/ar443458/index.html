<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>✂️ 😗 🤸 6 أخطاء نظام مسلية في تشغيل Kubernetes [وحلها] 👩🏻‍⚕️ 👨‍⚖️ 🔵</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="على مدار سنوات من تشغيل Kubernetes في الإنتاج ، جمعنا العديد من القصص المثيرة للاهتمام ، حيث أدت الأخطاء في مكونات النظام المختلفة إلى عواقب غير سارة ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>6 أخطاء نظام مسلية في تشغيل Kubernetes [وحلها]</h1><div class="post__body post__body_full" style=";text-align:right;direction:rtl"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/443458/" style=";text-align:right;direction:rtl"><img src="https://habrastorage.org/webt/7o/mz/o2/7omzo2vcqpqijlxsewel9gyhcsq.png"><br><br>  على مدار سنوات من تشغيل Kubernetes في الإنتاج ، جمعنا العديد من القصص المثيرة للاهتمام ، حيث أدت الأخطاء في مكونات النظام المختلفة إلى عواقب غير سارة و / أو غير مفهومة تؤثر على تشغيل الحاويات والقرون.  في هذه المقالة ، قمنا باختيار بعض من أكثرها تكرارا أو مثيرة للاهتمام.  حتى لو لم تكن محظوظًا بما يكفي لمواجهة مثل هذه المواقف ، فإن القراءة عن مثل هؤلاء المحققين الموجزين - بشكل خاص - عن كثب - هي دائما مسلية ، أليس كذلك؟ <a name="habracut"></a><br><br><h2 style=";text-align:right;direction:rtl">  التاريخ 1. Supercronic وتجمد عامل ميناء </h2><br>  في إحدى المجموعات ، تلقينا بشكل دوري عامل تجميد ، والذي يتداخل مع الأداء الطبيعي للمجموعة.  في الوقت نفسه ، تم ملاحظة ما يلي في سجلات Docker <br><br><pre style=";text-align:right;direction:rtl"><code class="plaintext hljs">level=error msg="containerd: start init process" error="exit status 2: \"runtime/cgo: pthread_create failed: No space left on device SIGABRT: abort PC=0x7f31b811a428 m=0 goroutine 0 [idle]: goroutine 1 [running]: runtime.systemstack_switch() /usr/local/go/src/runtime/asm_amd64.s:252 fp=0xc420026768 sp=0xc420026760 runtime.main() /usr/local/go/src/runtime/proc.go:127 +0x6c fp=0xc4200267c0 sp=0xc420026768 runtime.goexit() /usr/local/go/src/runtime/asm_amd64.s:2086 +0x1 fp=0xc4200267c8 sp=0xc4200267c0 goroutine 17 [syscall, locked to thread]: runtime.goexit() /usr/local/go/src/runtime/asm_amd64.s:2086 +0x1 …</code> </pre> <br>  في هذا الخطأ ، نحن مهتمون جدًا بالرسالة: <code>pthread_create failed: No space left on device</code> .  أوضحت دراسة سريعة <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">للوثائق</a> أن Docker لم يتمكن من تفرع العملية ، مما تسبب في "تجميدها" بشكل دوري. <br><br>  في مراقبة ما يحدث ، تتوافق الصورة التالية: <br><br><img src="https://habrastorage.org/webt/7r/cq/gv/7rcqgvafvtlmis1kl7maxyz5jgk.png"><br><br>  ويلاحظ موقف مماثل على العقد الأخرى: <br><br><img src="https://habrastorage.org/webt/lj/mw/-h/ljmw-hrrlyukwgmigltivjwyxig.png"><br><br><img src="https://habrastorage.org/webt/ap/tw/5u/aptw5ufxl-9woo5zszegfg1nqvc.png"><br><br>  على نفس العقد نرى: <br><br><pre style=";text-align:right;direction:rtl"> <code class="bash hljs">root@kube-node-1 ~ <span class="hljs-comment"><span class="hljs-comment"># ps auxfww | grep curl -c 19782 root@kube-node-1 ~ # ps auxfww | grep curl | head root 16688 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 17398 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 16852 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 9473 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 4664 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 30571 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 24113 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 16475 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 7176 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 1090 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt;</span></span></code> </pre> <br>  اتضح أن هذا السلوك هو نتيجة لعمل <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">قرنة</a> مع <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">supercronic</a> (أداة مساعدة على الذهاب نستخدمها لتشغيل مهام cron في pods): <br><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs"> \_ docker-containerd-shim 833b60bb9ff4c669bb413b898a5fd142a57a21695e5dc42684235df907825567 /var/run/docker/libcontainerd/833b60bb9ff4c669bb413b898a5fd142a57a21695e5dc42684235df907825567 docker-runc | \_ /usr/local/bin/supercronic -json /crontabs/cron | \_ /usr/bin/newrelic-daemon --agent --pidfile /var/run/newrelic-daemon.pid --logfile /dev/stderr --port /run/newrelic.sock --tls --define utilization.detect_aws=true --define utilization.detect_azure=true --define utilization.detect_gcp=true --define utilization.detect_pcf=true --define utilization.detect_docker=true | | \_ /usr/bin/newrelic-daemon --agent --pidfile /var/run/newrelic-daemon.pid --logfile /dev/stderr --port /run/newrelic.sock --tls --define utilization.detect_aws=true --define utilization.detect_azure=true --define utilization.detect_gcp=true --define utilization.detect_pcf=true --define utilization.detect_docker=true -no-pidfile | \_ [newrelic-daemon] &lt;defunct&gt; | \_ [curl] &lt;defunct&gt; | \_ [curl] &lt;defunct&gt; | \_ [curl] &lt;defunct&gt; …</code> </pre> <br>  المشكلة هي: عندما تبدأ المهمة في supercronic ، فإن العملية الناتجة عنها <b>لا يمكن أن تنتهي بشكل صحيح</b> ، وتتحول إلى <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">غيبوبة</a> . <br><br>  <i><b>ملاحظة</b> : لكي نكون أكثر دقة ، يتم إنشاء العمليات من خلال مهام cron ، ومع ذلك ، فإن supercronic ليس نظامًا أوليًا ولا يمكنه "تبني" العمليات التي أوجدها أطفاله.</i>  <i>عند حدوث إشارات SIGHUP أو SIGTERM ، لا يتم إرسالها إلى العمليات الناتجة ، ونتيجة لذلك لا تنتهي العمليات الفرعية ، وتبقى في حالة الزومبي.</i>  <i>يمكنك قراءة المزيد حول كل هذا ، على سبيل المثال ، في <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">مثل هذه المقالة</a> .</i> <br><br>  هناك عدة طرق لحل المشكلات: <br><br><ol style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  كحل مؤقت - قم بزيادة عدد PIDs في النظام في وقت واحد: <br><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs"> /proc/sys/kernel/pid_max (since Linux 2.5.34) This file specifies the value at which PIDs wrap around (ie, the value in this file is one greater than the maximum PID). PIDs greater than this value are not allo‐ cated; thus, the value in this file also acts as a system-wide limit on the total number of processes and threads. The default value for this file, 32768, results in the same range of PIDs as on earlier kernels</code> </pre> </li><li style=";text-align:right;direction:rtl">  أو ، اجعل بدء المهام في supercronic ليس بشكل مباشر ، ولكن بمساعدة من نفس <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">tini</a> ، والتي هي قادرة على إكمال العمليات بشكل صحيح وليس إنشاء غيبوبة. </li></ol><br><h2 style=";text-align:right;direction:rtl">  التاريخ 2. "الزومبي" عند إزالة مجموعة cgroup </h2><br>  بدأت Kubelet تستهلك الكثير من وحدة المعالجة المركزية: <br><br><img src="https://habrastorage.org/webt/ns/lh/mp/nslhmpwfnmennya-btg5icbkh8e.png"><br><br>  لا أحد يحب هذا ، لذلك قمنا بتسليح أنفسنا <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">بالكمال</a> وبدأنا في التعامل مع المشكلة.  وكانت نتائج التحقيق على النحو التالي: <br><br><ul style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  يقضي Kubelet أكثر من ثلث وقت وحدة المعالجة المركزية في سحب بيانات الذاكرة من جميع المجموعات: <br><br><img src="https://habrastorage.org/webt/yf/u7/qv/yfu7qvvcnryl5iknz4zosagh0is.png"></li><li style=";text-align:right;direction:rtl">  في القائمة البريدية لمطوري kernel ، يمكنك العثور على <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">مناقشة للمشكلة</a> .  باختصار ، خلاصة القول هي أن <b>ملفات tmpfs المختلفة وأشياء أخرى مماثلة لا يتم إزالتها بالكامل من النظام</b> عند حذف مجموعة cgroup - ما يسمى <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">memcg</a> zombie</b> .  عاجلاً أم آجلاً ، سيتم إزالتها من ذاكرة التخزين المؤقت للصفحة ، ومع ذلك ، فإن الذاكرة الموجودة على الخادم كبيرة ولا ترى النواة نقطة تضييع الوقت في حذفها.  لذلك ، لا تزال تتراكم.  لماذا يحدث هذا حتى؟  هذا هو خادم مع وظائف كرون التي تخلق باستمرار وظائف جديدة ، ومعهم قرون جديدة.  وبالتالي ، يتم إنشاء مجموعات cgroups جديدة للحاويات الموجودة بها ، والتي سيتم حذفها قريبًا. </li><li style=";text-align:right;direction:rtl">  لماذا يقضي cAdvisor في kubelet الكثير من الوقت؟  يمكن رؤية ذلك بسهولة من خلال التنفيذ البسيط <code>time cat /sys/fs/cgroup/memory/memory.stat</code> .  إذا استغرقت العملية 0.01 ثانية على جهاز سليم ، ثم 1.2 ثانية على cron02 إشكالية.  الشيء هو أن cAdvisor ، الذي يقرأ البيانات من sysfs ببطء شديد ، يحاول أن يأخذ في الاعتبار الذاكرة المستخدمة في zgie cgroups كذلك. </li><li style=";text-align:right;direction:rtl">  لإزالة الزومبي بالقوة ، حاولنا مسح ذاكرات التخزين المؤقت ، كما هو موصى به في LKML: <code>sync; echo 3 &gt; /proc/sys/vm/drop_caches</code>  <code>sync; echo 3 &gt; /proc/sys/vm/drop_caches</code> ، ولكن تبين أن النواة كانت أكثر تعقيدًا وعلقت الآلة. </li></ul><br>  ماذا تفعل؟  تم إصلاح المشكلة ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">الالتزام</a> والوصف ، راجع <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">رسالة الإصدار</a> ) عن طريق تحديث Linux kernel إلى الإصدار 4.16. <br><br><h2 style=";text-align:right;direction:rtl">  التاريخ 3. SYSTEMD وجبلها </h2><br>  مرة أخرى ، تستهلك kubelet الكثير من الموارد على بعض العقد ، لكن هذه المرة هي الذاكرة: <br><br><img src="https://habrastorage.org/webt/ud/l3/vl/udl3vlr9r5c6hzxoamdmnzvvm5m.png"><br><br>  اتضح أن هناك مشكلة في systemd المستخدمة في Ubuntu 16.04 ، ويحدث ذلك عند التحكم في عمليات <code>subPath</code> التي تم إنشاؤها للاتصال بـ <code>subPath</code> من ConfigMaps أو الأسرار.  بعد اكتمال الجراب ، تظل <b>خدمة systemd وتثبيت الخدمة</b> على النظام.  مع مرور الوقت ، تتراكم كمية كبيرة.  هناك حتى مشاكل في هذا الموضوع: <br><br><ol style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">kops # 5916</a> ؛ </li><li style=";text-align:right;direction:rtl">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">kubernetes # 57345</a> . </li></ol><br>  ... في آخرها تشير إلى PR في systemd: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u="># 7811</a> (المشكلة في systemd هي <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u="># 7798</a> ). <br><br>  لم تعد هناك مشكلة في Ubuntu 18.04 ، ولكن إذا كنت ترغب في الاستمرار في استخدام Ubuntu 16.04 ، فقد يكون حلنا بشأن هذا الموضوع مفيدًا. <br><br>  لذلك ، قمنا بعمل DaemonSet التالية: <br><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">--- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: labels: app: systemd-slices-cleaner name: systemd-slices-cleaner namespace: kube-system spec: updateStrategy: type: RollingUpdate selector: matchLabels: app: systemd-slices-cleaner template: metadata: labels: app: systemd-slices-cleaner spec: containers: - command: - /usr/local/bin/supercronic - -json - /app/crontab Image: private-registry.org/systemd-slices-cleaner/systemd-slices-cleaner:v0.1.0 imagePullPolicy: Always name: systemd-slices-cleaner resources: {} securityContext: privileged: true volumeMounts: - name: systemd mountPath: /run/systemd/private - name: docker mountPath: /run/docker.sock - name: systemd-etc mountPath: /etc/systemd - name: systemd-run mountPath: /run/systemd/system/ - name: lsb-release mountPath: /etc/lsb-release-host imagePullSecrets: - name: antiopa-registry priorityClassName: cluster-low tolerations: - operator: Exists volumes: - name: systemd hostPath: path: /run/systemd/private - name: docker hostPath: path: /run/docker.sock - name: systemd-etc hostPath: path: /etc/systemd - name: systemd-run hostPath: path: /run/systemd/system/ - name: lsb-release hostPath: path: /etc/lsb-release</code> </pre> <br>  ... ويستخدم البرنامج النصي التالي: <br><br><pre style=";text-align:right;direction:rtl"> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash # we will work only on xenial hostrelease="/etc/lsb-release-host" test -f ${hostrelease} &amp;&amp; grep xenial ${hostrelease} &gt; /dev/null || exit 0 # sleeping max 30 minutes to dispense load on kube-nodes sleep $((RANDOM % 1800)) stoppedCount=0 # counting actual subpath units in systemd countBefore=$(systemctl list-units | grep subpath | grep "run-" | wc -l) # let's go check each unit for unit in $(systemctl list-units | grep subpath | grep "run-" | awk '{print $1}'); do # finding description file for unit (to find out docker container, who born this unit) DropFile=$(systemctl status ${unit} | grep Drop | awk -F': ' '{print $2}') # reading uuid for docker container from description file DockerContainerId=$(cat ${DropFile}/50-Description.conf | awk '{print $5}' | cut -d/ -f6) # checking container status (running or not) checkFlag=$(docker ps | grep -c ${DockerContainerId}) # if container not running, we will stop unit if [[ ${checkFlag} -eq 0 ]]; then echo "Stopping unit ${unit}" # stoping unit in action systemctl stop $unit # just counter for logs ((stoppedCount++)) # logging current progress echo "Stopped ${stoppedCount} systemd units out of ${countBefore}" fi done</span></span></code> </pre> <br>  ... ويبدأ كل 5 دقائق مع supercronic المذكورة بالفعل.  Dockerfile له يشبه هذا: <br><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">FROM ubuntu:16.04 COPY rootfs / WORKDIR /app RUN apt-get update &amp;&amp; \ apt-get upgrade -y &amp;&amp; \ apt-get install -y gnupg curl apt-transport-https software-properties-common wget RUN add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable" &amp;&amp; \ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - &amp;&amp; \ apt-get update &amp;&amp; \ apt-get install -y docker-ce=17.03.0* RUN wget https://github.com/aptible/supercronic/releases/download/v0.1.6/supercronic-linux-amd64 -O \ /usr/local/bin/supercronic &amp;&amp; chmod +x /usr/local/bin/supercronic ENTRYPOINT ["/bin/bash", "-c", "/usr/local/bin/supercronic -json /app/crontab"]</code> </pre> <br><h2 style=";text-align:right;direction:rtl">  التاريخ 4. المنافسة في التخطيط القرون </h2><br>  تمت الإشارة إلى أنه: إذا تم وضع جراب على العقدة الخاصة بنا وتم ضخ صورتها لفترة طويلة جدًا ، فإن الجراب الآخر الذي "وصل" إلى العقدة نفسها ببساطة <b>لا يبدأ في سحب صورة الجراب الجديد</b> .  بدلاً من ذلك ، ينتظر أن يتم سحب صورة الجراب السابق.  كنتيجة لذلك ، فإن الحافظة التي تم التخطيط لها بالفعل والتي يمكن تنزيل صورتها في غضون دقيقة واحدة فقط ستنتهي في حالة إنشاء <code>containerCreating</code> لفترة طويلة. <br><br>  في الأحداث ، سيكون هناك شيء مثل هذا: <br><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">Normal Pulling 8m kubelet, ip-10-241-44-128.ap-northeast-1.compute.internal pulling image "registry.example.com/infra/openvpn/openvpn:master"</code> </pre> <br>  اتضح أن <b>صورة واحدة من السجل البطيء يمكن أن تمنع النشر</b> إلى العقدة. <br><br>  لسوء الحظ ، لا توجد طرق كثيرة للخروج من الموقف: <br><br><ol style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  حاول استخدام Docker Registry مباشرة في الكتلة أو مباشرة مع الكتلة (على سبيل المثال ، GitLab Registry ، Nexus ، وما إلى ذلك) ؛ </li><li style=";text-align:right;direction:rtl">  استخدام المرافق مثل <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">kraken</a> . </li></ol><br><h2 style=";text-align:right;direction:rtl">  التاريخ 5. تعليق العقد مع نفاد الذاكرة </h2><br>  أثناء تشغيل التطبيقات المختلفة ، تلقينا أيضًا موقفًا حيث يتعذر الوصول إلى العقدة تمامًا: لا يستجيب SSH ، تسقط جميع شياطين المراقبة ، ثم لا شيء (أو لا شيء تقريبًا) غير طبيعي في السجلات. <br><br>  سأخبرك بالصور على مثال عقدة واحدة حيث تعمل MongoDB. <br><br>  هذه هي الطريقة التي تبدو عليها <b>قبل</b> الحادث: <br><br><img src="https://habrastorage.org/webt/l5/ef/az/l5efazbhjmzsuxdv6puz1tlvbzs.png"><br><br>  وهكذا - <b>بعد</b> الحادث: <br><br><img src="https://habrastorage.org/webt/wx/mh/q7/wxmhq71060dhvxsxk-dh8m--pas.png"><br><br>  في المراقبة أيضًا ، هناك قفزة حادة تتوقف فيها العقدة عن الوصول: <br><br><img src="https://habrastorage.org/webt/tp/fm/wf/tpfmwftsui_eoz91ojyy5rzektc.png"><br><br>  وبالتالي ، تظهر لقطات ما يلي: <br><br><ol style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  ذاكرة الوصول العشوائي على الجهاز بالقرب من النهاية. </li><li style=";text-align:right;direction:rtl">  لوحظ حدوث قفزة حادة في استهلاك ذاكرة الوصول العشوائي ، وبعد ذلك تم تعطيل الوصول إلى الجهاز بالكامل بحدة ؛ </li><li style=";text-align:right;direction:rtl">  تصل مهمة كبيرة إلى Mongo ، مما يفرض على عملية DBMS استخدام المزيد من الذاكرة والقراءة النشطة من القرص. </li></ol><br>  اتضح أنه في حالة نفاد ذاكرة Linux (حدوث ضغط في الذاكرة) وعدم وجود تبادل ، ثم <b>قبل</b> وصول قاتل OOM ، قد يحدث توازن بين إلقاء الصفحات في ذاكرة التخزين المؤقت للصفحة وإعادة كتابتها مرة أخرى على القرص.  يتم ذلك بواسطة kswapd ، الذي يحرر بشجاعة أكبر عدد ممكن من صفحات الذاكرة للتوزيع لاحقًا. <br><br>  لسوء الحظ ، مع وجود حمولة كبيرة من الإدخال / الإخراج ، إلى جانب مقدار صغير من الذاكرة الخالية ، <b>يصبح kswapd عنق الزجاجة للنظام بأكمله</b> ، لأن <b>جميع</b> أخطاء الصفحات في صفحات الذاكرة في النظام مرتبطة به.  يمكن أن يستمر هذا لفترة طويلة جدًا إذا لم تعد العمليات ترغب في استخدام الذاكرة ، ولكن تم إصلاحها على حافة الهاوية القاتلة لـ OOM. <br><br>  السؤال المنطقي هو: لماذا يأتي قاتل OOM متأخرا جدا؟  في التكرار الحالي لـ OOM ، يكون القاتل غبيًا للغاية: لن يقتل العملية إلا عندما تفشل محاولة تخصيص صفحة ذاكرة ، أي  إذا فشل خطأ الصفحة.  هذا لا يحدث لفترة طويلة ، لأن kswapd يحرر بشجاعة صفحات الذاكرة عن طريق مسح ذاكرة التخزين المؤقت للصفحة (كل القرص I / O في النظام ، في الواقع) مرة أخرى إلى القرص.  بمزيد من التفصيل ، مع وصف للخطوات اللازمة للتخلص من هذه المشكلات في النواة ، يمكنك أن تقرأ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">هنا</a> . <br><br>  <a href="">يجب تحسين</a> هذا السلوك باستخدام Linux 4.6+ kernel. <br><br><h2 style=";text-align:right;direction:rtl">  قصة 6. القرون هي في انتظار </h2><br>  في بعض المجموعات ، التي يوجد بها بالفعل العديد من القرون ، بدأنا نلاحظ أن معظمها كان معلقًا في حالة <code>Pending</code> لفترة طويلة جدًا ، على الرغم من أن حاويات Docker نفسها كانت تعمل بالفعل على العقد ويمكنك العمل معها يدويًا. <br><br>  لا حرج في <code>describe</code> : <br><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs"> Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 1m default-scheduler Successfully assigned sphinx-0 to ss-dev-kub07 Normal SuccessfulAttachVolume 1m attachdetach-controller AttachVolume.Attach succeeded for volume "pvc-6aaad34f-ad10-11e8-a44c-52540035a73b" Normal SuccessfulMountVolume 1m kubelet, ss-dev-kub07 MountVolume.SetUp succeeded for volume "sphinx-config" Normal SuccessfulMountVolume 1m kubelet, ss-dev-kub07 MountVolume.SetUp succeeded for volume "default-token-fzcsf" Normal SuccessfulMountVolume 49s (x2 over 51s) kubelet, ss-dev-kub07 MountVolume.SetUp succeeded for volume "pvc-6aaad34f-ad10-11e8-a44c-52540035a73b" Normal Pulled 43s kubelet, ss-dev-kub07 Container image "registry.example.com/infra/sphinx-exporter/sphinx-indexer:v1" already present on machine Normal Created 43s kubelet, ss-dev-kub07 Created container Normal Started 43s kubelet, ss-dev-kub07 Started container Normal Pulled 43s kubelet, ss-dev-kub07 Container image "registry.example.com/infra/sphinx/sphinx:v1" already present on machine Normal Created 42s kubelet, ss-dev-kub07 Created container Normal Started 42s kubelet, ss-dev-kub07 Started container</code> </pre> <br>  بعد التنقيب حولنا ، افترضنا أن kubelet فقط ليس لديه وقت لإرسال خادم واجهة برمجة التطبيقات (API) لجميع المعلومات المتعلقة بحالة القرون وعينات الصلاحية / الاستعداد. <br><br>  وبعد دراسة المساعدة ، وجدنا المعايير التالية: <br><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">--kube-api-qps - QPS to use while talking with kubernetes apiserver (default 5) --kube-api-burst - Burst to use while talking with kubernetes apiserver (default 10) --event-qps - If &gt; 0, limit event creations per second to this value. If 0, unlimited. (default 5) --event-burst - Maximum size of a bursty event records, temporarily allows event records to burst to this number, while still not exceeding event-qps. Only used if --event-qps &gt; 0 (default 10) --registry-qps - If &gt; 0, limit registry pull QPS to this value. --registry-burst - Maximum size of bursty pulls, temporarily allows pulls to burst to this number, while still not exceeding registry-qps. Only used if --registry-qps &gt; 0 (default 10)</code> </pre> <br>  كما ترون ، <b>القيم الافتراضية صغيرة جدًا</b> ، وفي 90٪ تغطي جميع الاحتياجات ... ومع ذلك ، في حالتنا هذه لم تكن كافية.  لذلك ، نضع هذه القيم: <br><br><pre style=";text-align:right;direction:rtl"> <code class="plaintext hljs">--event-qps=30 --event-burst=40 --kube-api-burst=40 --kube-api-qps=30 --registry-qps=30 --registry-burst=40</code> </pre> <br><br>  ... وأعد تشغيل kubelets ، وبعد ذلك رأوا الصورة التالية على الرسوم البيانية للوصول إلى خادم API: <br><br><img src="https://habrastorage.org/webt/nq/-i/oq/nq-ioqoyt6_qudmacm5dwfe8hnk.png"><br><br>  ... ونعم ، كل شيء بدأ يطير! <br><br><h2 style=";text-align:right;direction:rtl">  PS </h2><br>  للمساعدة في جمع الأخطاء وإعداد المقالة ، أعرب عن عميق امتناني للمهندسين العديدين لشركتنا ، ولا سيما أندريه كليمينيف (زميل من فريق البحث والتطوير) ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=" class="user_link">zuzzas</a> ). <br><br><h2 style=";text-align:right;direction:rtl">  PPS </h2><br>  اقرأ أيضًا في مدونتنا: <br><br><ul style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">Kubectl-debug plugin لتصحيح الأخطاء في قرون Kubernetes</a> " ؛ </li><li style=";text-align:right;direction:rtl">  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">المراقبة و Kubernetes (تقرير المراجعة والفيديو)</a> " ؛ </li><li style=";text-align:right;direction:rtl">  دورة نصائح وحيل Kubernetes: <ul style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">نقل الموارد العاملة في مجموعة لإدارة هيلم 2</a> " ؛ </li><li style=";text-align:right;direction:rtl">  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">بشأن تخصيص العقد والحمل على تطبيق الويب</a> " ؛ </li><li style=";text-align:right;direction:rtl">  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">الوصول إلى مواقع ديف</a> " ؛ </li><li style=";text-align:right;direction:rtl">  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">تسريع التمهيد لقواعد البيانات الكبيرة.</a> " </li></ul></li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/ar443458/">https://habr.com/ru/post/ar443458/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ar443438/index.html">7 ملحقات فايرفوكس مفيدة لتعلم اللغة الإنجليزية</a></li>
<li><a href="../ar443440/index.html">وحدة PHP للعمل مع البيانات التسلسلية في InterSystems IRIS</a></li>
<li><a href="../ar443450/index.html">لماذا لا يمكن للفقراء أن يكونوا أصحاء</a></li>
<li><a href="../ar443452/index.html">سوف الجيش الروسي إنشاء شبكة الإنترنت الخاصة بهم مغلقة</a></li>
<li><a href="../ar443456/index.html">نحن ندعوك إلى Yandex NLP لمدة أسبوع</a></li>
<li><a href="../ar443460/index.html">11 إجابات عن Yandex.Directory</a></li>
<li><a href="../ar443462/index.html">التقطيع للكاميرات: متجهات الهجوم وأدوات البحث عن الثغرات ومكافحة التتبع</a></li>
<li><a href="../ar443464/index.html">الدليل الكامل لتبديل التعبيرات في Java 12</a></li>
<li><a href="../ar443466/index.html">ملك التنمية</a></li>
<li><a href="../ar443468/index.html">ما أدوات مراقبة الشبكة التي أصبحت رائدة في إصدار Gartner</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>