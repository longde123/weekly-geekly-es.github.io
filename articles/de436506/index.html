<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🌥️ 👩🏼‍🚀 👨‍🎤 Wie man ohne großen Aufwand einen KI-Rassisten erschafft 🎡 📝 🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Eine warnende Lektion. 

 Machen wir einen Tonalitätsklassifikator! 

 Die Stimmungsanalyse (Stimmungsanalyse) ist eine sehr häufige Aufgabe in der Ve...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie man ohne großen Aufwand einen KI-Rassisten erschafft</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/436506/"> Eine warnende Lektion. <br><br>  <b>Machen wir einen Tonalitätsklassifikator!</b> <br><br>  Die Stimmungsanalyse (Stimmungsanalyse) ist eine sehr häufige Aufgabe in der Verarbeitung natürlicher Sprache (NLP), und dies ist nicht überraschend.  Für ein Unternehmen ist es wichtig zu verstehen, was die Leute sagen: positiv oder negativ.  Eine solche Analyse wird verwendet, um soziale Netzwerke, Kundenfeedback und sogar den algorithmischen Aktienhandel zu überwachen (daher <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kaufen</a> Bots <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Berkshire Hathaway-Aktien, nachdem sie im letzten Film positive Bewertungen über die Rolle von Anne Hathaway veröffentlicht haben</a> ). <br><br>  Die Analysemethode ist manchmal zu einfach, aber eine der einfachsten Möglichkeiten, messbare Ergebnisse zu erzielen.  Senden Sie einfach den Text - und die Ausgabe ist positiv und negativ.  Sie müssen sich nicht mit dem Analysebaum befassen, ein Diagramm oder eine andere komplexe Darstellung erstellen. <br><a name="habracut"></a><br>  Das werden wir tun.  Wir werden den Weg des geringsten Widerstands beschreiten und den einfachsten Klassifikator erstellen, der wahrscheinlich jedem bekannt vorkommt, der an relevanten Entwicklungen auf dem Gebiet der NLP beteiligt ist.  Ein solches Modell findet sich beispielsweise im Artikel <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Deep Averaging Networks</a></i> (Iyyer et al., 2015).  Wir versuchen überhaupt nicht, ihre Ergebnisse in Frage zu stellen oder das Modell zu kritisieren.  Wir geben einfach eine bekannte Methode zur Vektordarstellung von Wörtern. <br><br>  Arbeitsplan: <br><br><ul><li>  Führen Sie eine typische <b>Vektordarstellung von Wörtern ein</b> , um mit Bedeutungen (Bedeutungen) zu arbeiten. </li><li>  Führen Sie <b>Trainings- und Testdatensätze</b> mit Standardlisten positiver und negativer Wörter ein. </li><li>  <b>Trainieren Sie</b> den Gradientenabstiegsklassifikator, um andere positive und negative Wörter anhand ihrer Vektordarstellung zu erkennen. </li><li>  Verwenden Sie diesen Klassifikator, um die <b>Tonalitätsbewertungen</b> für Textsätze zu berechnen. </li><li>  <b>Um das Monster zu sehen, das</b> wir erschaffen haben. </li></ul><br>  Und dann werden wir sehen, "wie man ohne besondere Anstrengungen einen KI-Rassisten schafft".  Natürlich können Sie das System nicht in solch einer monströsen Form verlassen, also werden wir: <br><br><ul><li>  <b>Um das Problem</b> statistisch <b>zu bewerten</b> , damit es möglich wird, den Fortschritt zu messen, sobald er gelöst ist. </li><li>  <b>Verbessern Sie Daten</b> , um ein genaueres und weniger rassistisches semantisches Modell zu erhalten. </li></ul><br><h1>  Software-Abhängigkeiten </h1><br>  Dieses Tutorial ist in Python <code>numpy</code> und <code>scipy</code> auf einem typischen Python-Stapel für maschinelles Lernen: <code>numpy</code> und <code>scipy</code> für numerisches Rechnen, <code>pandas</code> für Datenverwaltung und <code>scikit-learn</code> für maschinelles Lernen.  Am Ende werden wir auch <code>matplotlib</code> und <code>seaborn</code> für die <code>seaborn</code> . <br><br>  Im Prinzip kann <code>scikit-learn</code> durch TensorFlow oder Keras oder ähnliches ersetzt werden: Sie können den Klassifikator auch auf Gradientenabstieg trainieren.  Ihre Abstraktionen brauchen wir aber nicht, denn hier findet das Training in einer Phase statt. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> statsmodels.formula.api <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.linear_model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SGDClassifier <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> accuracy_score <span class="hljs-comment"><span class="hljs-comment">#     %matplotlib inline seaborn.set_context('notebook', rc={'figure.figsize': (10, 6)}, font_scale=1.5)</span></span></code> </pre> <br><h1>  Schritt 1. Vektordarstellung von Wörtern </h1><br>  Vektordarstellungen werden häufig bei Texteingabe verwendet.  Wörter werden im mehrdimensionalen Raum zu Vektoren, wobei benachbarte Vektoren ähnliche Bedeutungen darstellen.  Mithilfe von Vektordarstellungen können Sie Wörter anhand (ungefähr) ihrer Bedeutung vergleichen und nicht nur anhand genauer Übereinstimmungen. <br><br>  Erfolgreiches Lernen erfordert Hunderte von Gigabyte Text.  Glücklicherweise haben verschiedene Forschungsteams diese Arbeit bereits durchgeführt und vorgefertigte Modelle von Vektordarstellungen zum Download bereitgestellt. <br><br>  Die beiden bekanntesten englischen Datensätze sind <b>word2vec</b> (auf Google News trainiert) und <b>GloVe</b> (auf Common Crawl-Webseiten).  Jeder von ihnen wird ein ähnliches Ergebnis liefern, aber wir werden das GloVe-Modell verwenden, da es eine transparentere Datenquelle hat. <br><br>  GloVe ist in drei Größen erhältlich: 6 Milliarden, 42 Milliarden und 840 Milliarden. Das neueste Modell ist das leistungsstärkste, erfordert jedoch erhebliche Verarbeitungsressourcen.  Die 42-Milliarden-Version ist ziemlich gut und das Wörterbuch ist ordentlich auf 1 Million Wörter zugeschnitten.  Wir sind auf dem Weg des geringsten Widerstands, also nehmen Sie die 42-Milliarden-Version. <br><br><blockquote>  <b>- Warum ist es so wichtig, ein „bekanntes“ Modell zu verwenden?</b> <br><br>  "Ich bin froh, dass du danach gefragt hast, hypothetischer Gesprächspartner!"  Bei jedem Schritt versuchen wir, etwas extrem Typisches zu tun, und das beste Modell für die Vektordarstellung von Wörtern wurde aus irgendeinem Grund noch nicht bestimmt.  Ich hoffe, dass dieser Artikel den Wunsch weckt, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">moderne, hochwertige Modelle zu verwenden</a> , insbesondere solche, die einen algorithmischen Fehler berücksichtigen und versuchen, ihn zu korrigieren.  Dazu später mehr. </blockquote><br>  Laden Sie glove.42B.300d.zip von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der GloVe-Website herunter</a> und extrahieren Sie die Datei <code>data/glove.42B.300d.txt</code> .  Als nächstes definieren wir eine Funktion zum Lesen von Vektoren in einem einfachen Format. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">load_embeddings</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""  DataFrame      ,   word2vec, GloVe, fastText  ConceptNet Numberbatch.            . """</span></span> labels = [] rows = [] <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(filename, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> infile: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(infile): items = line.rstrip().split(<span class="hljs-string"><span class="hljs-string">' '</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(items) == <span class="hljs-number"><span class="hljs-number">2</span></span>: <span class="hljs-comment"><span class="hljs-comment"># This is a header row giving the shape of the matrix continue labels.append(items[0]) values = np.array([float(x) for x in items[1:]], 'f') rows.append(values) arr = np.vstack(rows) return pd.DataFrame(arr, index=labels, dtype='f') embeddings = load_embeddings('data/glove.42B.300d.txt') embeddings.shape</span></span></code> </pre> <br> <code>(1917494, 300)</code> <br> <h1>  Schritt 2. Goldstandard-Tonalitätswörterbuch </h1><br>  Jetzt brauchen wir Informationen darüber, welche Wörter als positiv und welche als negativ angesehen werden.  Es gibt viele solcher Wörterbücher, aber wir werden ein sehr einfaches Wörterbuch (Hu und Liu, 2004) verwenden, das in dem Artikel von <i>Deep Averaging Networks verwendet wird</i> . <br><br>  Laden Sie das Wörterbuch von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Website</a> von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bing Liu herunter</a> und extrahieren Sie die Daten in <code>data/positive-words.txt</code> und <code>data/negative-words.txt</code> . <br><br>  Als Nächstes legen wir fest, wie diese Dateien gelesen und als <code>neg_words</code> <code>pos_words</code> und <code>neg_words</code> : <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">load_lexicon</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""       (https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html)      Latin-1.      ,    - .    ,    ';'   ,   . """</span></span> lexicon = [] <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(filename, encoding=<span class="hljs-string"><span class="hljs-string">'latin-1'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> infile: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> infile: line = line.rstrip() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> line.startswith(<span class="hljs-string"><span class="hljs-string">';'</span></span>): lexicon.append(line) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> lexicon pos_words = load_lexicon(<span class="hljs-string"><span class="hljs-string">'data/positive-words.txt'</span></span>) neg_words = load_lexicon(<span class="hljs-string"><span class="hljs-string">'data/negative-words.txt'</span></span>)</code> </pre> <br><h1>  Schritt 3. Wir trainieren das Modell, um die Tonalität vorherzusagen </h1><br>  Basierend auf den Vektoren positiver und negativer Wörter verwenden wir den Befehl Pandas <code>.loc[]</code> , um nach Vektordarstellungen aller Wörter zu suchen. <br><br>  Einige Wörter fehlen im GloVe-Wörterbuch.  Meistens sind dies Tippfehler wie „Fancinating“.  Hier sehen wir eine Reihe von <code>NaN</code> , die das Fehlen eines Vektors anzeigen, und löschen sie mit dem Befehl <code>.dropna()</code> . <br><br> <code>pos_vectors = embeddings.loc[pos_words].dropna() <br> neg_vectors = embeddings.loc[neg_words].dropna()</code> <br> <br>  Jetzt erstellen wir Datenfelder am Eingang (Vektordarstellungen) und am Ausgang (1 für positive Wörter und -1 für negative).  Wir überprüfen auch, ob die Vektoren an Wörter angehängt sind, damit wir die Ergebnisse interpretieren können. <br><br> <code>vectors = pd.concat([pos_vectors, neg_vectors]) <br> targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index]) <br> labels = list(pos_vectors.index) + list(neg_vectors.index)</code> <br> <br><blockquote>  <b>- Warte einen Moment.</b>  <b>Einige Wörter sind weder positiv noch negativ, sie sind neutral.</b>  <b>Sollte nicht eine dritte Klasse für neutrale Wörter geschaffen werden?</b> <br><br>  "Ich denke, er hätte sich als nützlich erwiesen."  Später werden wir sehen, welche Probleme durch die Zuordnung der Tonalität zu neutralen Wörtern entstehen.  Wenn wir neutrale Wörter zuverlässig identifizieren können, ist es durchaus möglich, die Komplexität des Klassifikators auf drei Kategorien zu erhöhen.  Aber Sie müssen ein Wörterbuch mit neutralen Wörtern finden, denn im Wörterbuch von Liu gibt es nur positive und negative. <br><br>  Also habe ich meine Version mit 800 Beispielen von Wörtern ausprobiert und das Gewicht erhöht, um neutrale Wörter vorherzusagen.  Aber die Endergebnisse unterschieden sich nicht sehr von dem, was Sie jetzt sehen werden. <br><br>  <b>- Wie unterscheidet diese Liste positive und negative Wörter?</b>  <b>Kommt das nicht auf den Kontext an?</b> <br><br>  - Gute Frage.  Die Analyse allgemeiner Schlüssel ist nicht so einfach, wie es scheint.  Die Grenze ist an einigen Stellen ziemlich willkürlich.  In dieser Liste wird das Wort "frech" als "schlecht" und "ehrgeizig" als "gut" markiert.  "Comic" ist schlecht und "lustig" ist gut.  Eine „Rückerstattung“ ist gut, obwohl sie normalerweise in einem schlechten Kontext erwähnt wird, wenn Sie jemandem Geld schulden oder jemandem etwas schulden. <br><br>  Jeder versteht, dass die Tonalität durch den Kontext bestimmt wird, aber in einem einfachen Modell muss man den Kontext ignorieren und hoffen, dass die durchschnittliche Tonalität richtig erraten wird. </blockquote><br>  Mit der Funktion <code>train_test_split</code> teilen <code>train_test_split</code> gleichzeitig die Eingabevektoren, Ausgabewerte und Beschriftungen in Trainings- und Testdaten auf, während 10% zum Testen übrig <code>train_test_split</code> . <br><br><pre> <code class="python hljs">train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \ train_test_split(vectors, targets, labels, test_size=<span class="hljs-number"><span class="hljs-number">0.1</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">0</span></span>)</code> </pre> <br>  Erstellen Sie nun einen Klassifikator und übergeben Sie Vektoren durch Iterationen.  Wir verwenden die logistische Verlustfunktion, damit der endgültige Klassifikator die Wahrscheinlichkeit ableiten kann, dass das Wort positiv oder negativ ist. <br><br><pre> <code class="python hljs">model = SGDClassifier(loss=<span class="hljs-string"><span class="hljs-string">'log'</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">0</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">100</span></span>) model.fit(train_vectors, train_targets) SGDClassifier(alpha=<span class="hljs-number"><span class="hljs-number">0.0001</span></span>, average=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, class_weight=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, epsilon=<span class="hljs-number"><span class="hljs-number">0.1</span></span>, eta0=<span class="hljs-number"><span class="hljs-number">0.0</span></span>, fit_intercept=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, l1_ratio=<span class="hljs-number"><span class="hljs-number">0.15</span></span>, learning_rate=<span class="hljs-string"><span class="hljs-string">'optimal'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'log'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">100</span></span>, n_jobs=<span class="hljs-number"><span class="hljs-number">1</span></span>, penalty=<span class="hljs-string"><span class="hljs-string">'l2'</span></span>, power_t=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">0</span></span>, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">0</span></span>, warm_start=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)</code> </pre> <br>  Wir bewerten den Klassifikator anhand von Testvektoren.  Es zeigt eine Genauigkeit von 95%.  Nicht schlecht. <br><br> <code>accuracy_score(model.predict(test_vectors), test_targets) <br> 0.95022624434389136</code> <br> <br>  Wir definieren die Tonalitätsvorhersagefunktion für bestimmte Wörter und verwenden sie dann für einige Beispiele aus Testdaten. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">vecs_to_sentiment</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(vecs)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># predict_log_proba  log-    predictions = model.predict_log_proba(vecs) #        #  log-    . return predictions[:, 1] - predictions[:, 0] def words_to_sentiment(words): vecs = embeddings.loc[words].dropna() log_odds = vecs_to_sentiment(vecs) return pd.DataFrame({'sentiment': log_odds}, index=vecs.index) #  20      words_to_sentiment(test_labels).ix[:20]</span></span></code> </pre> <br><table border="1" width="350"><thead><tr><th></th><th>  Tonalität </th></tr></thead><tbody><tr><th>  zappeln </th><td>  -9,931679 </td></tr><tr><th>  unterbrechen </th><td>  -9,634706 </td></tr><tr><th>  standhaft </th><td>  1,466919 </td></tr><tr><th>  imaginär </th><td>  -2,989215 </td></tr><tr><th>  Besteuerung </th><td>  0,468522 </td></tr><tr><th>  weltberühmt </th><td>  6.908561 </td></tr><tr><th>  preiswert </th><td>  9.237223 </td></tr><tr><th>  Enttäuschung </th><td>  -8,737182 </td></tr><tr><th>  totalitär </th><td>  -10,851580 </td></tr><tr><th>  kriegerisch </th><td>  -8,328674 </td></tr><tr><th>  friert ein </th><td>  -8,456981 </td></tr><tr><th>  Sünde </th><td>  -7,839670 </td></tr><tr><th>  zerbrechlich </th><td>  -4.018289 </td></tr><tr><th>  getäuscht </th><td>  -4.309344 </td></tr><tr><th>  ungelöst </th><td>  -2,816172 </td></tr><tr><th>  klug </th><td>  2.339609 </td></tr><tr><th>  dämonisiert </th><td>  -2.102152 </td></tr><tr><th>  sorglos </th><td>  8,747150 </td></tr><tr><th>  unbeliebt </th><td>  -7,887475 </td></tr><tr><th>  sympathisieren </th><td>  1,790899 </td></tr></tbody></table><br>  Es ist zu sehen, dass der Klassifikator funktioniert.  Er lernte, die Tonalität in Worten außerhalb der Trainingsdaten zu verallgemeinern. <br><br><h1>  Schritt 4. Holen Sie sich eine Tonalitätsbewertung für den Text. </h1><br>  Es gibt viele Möglichkeiten, einer Gesamtschätzung Vektoren hinzuzufügen.  Wieder folgen wir dem Weg des geringsten Widerstands, also nehmen Sie einfach den Durchschnittswert. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re TOKEN_RE = re.compile(<span class="hljs-string"><span class="hljs-string">r"\w.*?\b"</span></span>) <span class="hljs-comment"><span class="hljs-comment"># regex  ,     (\w)   #   (.+?)    (\b).   #       . def text_to_sentiment(text): tokens = [token.casefold() for token in TOKEN_RE.findall(text)] sentiments = words_to_sentiment(tokens) return sentiments['sentiment'].mean()</span></span></code> </pre> <br>  Es gibt viel zu optimieren: <br><br><ul><li>  Einführung einer umgekehrten Beziehung zwischen dem Wortgewicht und seiner Häufigkeit, so dass dieselben Präpositionen die Tonalität nicht stark beeinflussen. </li><li>  Einstellung, damit kurze Sätze nicht mit extremen Tonalitätswerten enden. </li><li>  Buchhaltungsphrasen. </li><li>  Ein zuverlässigerer Wortsegmentierungsalgorithmus, den Apostrophe nicht niederschlagen. </li><li>  Berücksichtigung von Negativen wie "nicht zufrieden". </li></ul><br>  Aber alles erfordert zusätzlichen Code und wird die Ergebnisse nicht grundlegend ändern.  Zumindest können Sie jetzt verschiedene Angebote grob vergleichen: <br><br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"this example is pretty cool"</span></span>) <span class="hljs-number"><span class="hljs-number">3.889968926086298</span></span></code> </pre> <br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"this example is okay"</span></span>) <span class="hljs-number"><span class="hljs-number">2.7997773492425186</span></span></code> </pre> <br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"meh, this example sucks"</span></span>) <span class="hljs-number"><span class="hljs-number">-1.1774475917460698</span></span></code> </pre> <br><h1>  Schritt 5. Siehe das Monster, das wir erschaffen haben </h1><br>  Nicht jeder Satz hat eine ausgeprägte Tonalität.  Mal sehen, was mit neutralen Sätzen passiert: <br><br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"Let's go get Italian food"</span></span>) <span class="hljs-number"><span class="hljs-number">2.0429166109408983</span></span></code> </pre> <br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"Let's go get Chinese food"</span></span>) <span class="hljs-number"><span class="hljs-number">1.4094033658140972</span></span></code> </pre> <br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"Let's go get Mexican food"</span></span>) <span class="hljs-number"><span class="hljs-number">0.38801985560121732</span></span></code> </pre> <br>  Ich bin bereits auf ein solches Phänomen gestoßen, als ich Bewertungen von Restaurants unter Berücksichtigung von Vektordarstellungen von Wörtern analysierte.  Ohne ersichtlichen Grund <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">haben alle mexikanischen Restaurants eine niedrigere Gesamtpunktzahl</a> . <br><br>  Vektordarstellungen erfassen subtile semantische Unterschiede im Kontext.  Sie spiegeln daher die Vorurteile unserer Gesellschaft wider. <br><br>  Hier sind einige andere neutrale Vorschläge: <br><br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"My name is Emily"</span></span>) <span class="hljs-number"><span class="hljs-number">2.2286179364745311</span></span></code> </pre> <br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"My name is Heather"</span></span>) <span class="hljs-number"><span class="hljs-number">1.3976291151079159</span></span></code> </pre> <br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"My name is Yvette"</span></span>) <span class="hljs-number"><span class="hljs-number">0.98463802132985556</span></span></code> </pre> <br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"My name is Shaniqua"</span></span>) <span class="hljs-number"><span class="hljs-number">-0.47048131775890656</span></span></code> </pre> <br>  Verdammt noch mal ... <br><br>  Das mit den Namen von Menschen verbundene System hat völlig unterschiedliche Gefühle.  Sie können sich diese und viele andere Beispiele ansehen und feststellen, dass die Tonalität bei stereotypen weißen Namen normalerweise höher und bei stereotypen schwarzen Namen niedriger ist. <br><br>  Dieser Test wurde von Caliscan, Bryson und Narayanan in ihrem Forschungsbericht verwendet, der im April 2017 in der Zeitschrift <i>Science veröffentlicht</i> wurde.  Es zeigt, dass die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Semantik des Sprachkorpus die Vorurteile der Gesellschaft enthält</a> .  Wir werden diese Methode verwenden. <br><br><h1>  Schritt 6. Bewertung des Problems </h1><br>  Wir wollen verstehen, wie solche Fehler vermieden werden können.  Lassen Sie uns mehr Daten durch den Klassifikator leiten und dessen „Bias“ statistisch messen. <br><br>  Hier haben wir vier Namenslisten, die unterschiedliche ethnische Hintergründe widerspiegeln, hauptsächlich in den USA.  Die ersten beiden sind Listen mit überwiegend „weißen“ und „schwarzen“ Namen, die auf der Grundlage eines Artikels von Kaliskan et al. Angepasst wurden. Ich habe auch spanische und muslimische Namen aus Arabisch und Urdu hinzugefügt. <br><br>  Diese Daten werden verwendet, um die Verzerrung des Algorithmus während des ConceptNet-Erstellungsprozesses zu überprüfen: Sie finden sie im Modul <code>conceptnet5.vectors.evaluation.bias</code> .  Es besteht die Idee, das Wörterbuch auf andere ethnische Gruppen auszudehnen, wobei nicht nur Namen, sondern auch Nachnamen berücksichtigt werden. <br><br>  Hier sind die Auflistungen: <br><br><pre> <code class="python hljs">NAMES_BY_ETHNICITY = { <span class="hljs-comment"><span class="hljs-comment">#           . 'White': [ 'Adam', 'Chip', 'Harry', 'Josh', 'Roger', 'Alan', 'Frank', 'Ian', 'Justin', 'Ryan', 'Andrew', 'Fred', 'Jack', 'Matthew', 'Stephen', 'Brad', 'Greg', 'Jed', 'Paul', 'Todd', 'Brandon', 'Hank', 'Jonathan', 'Peter', 'Wilbur', 'Amanda', 'Courtney', 'Heather', 'Melanie', 'Sara', 'Amber', 'Crystal', 'Katie', 'Meredith', 'Shannon', 'Betsy', 'Donna', 'Kristin', 'Nancy', 'Stephanie', 'Bobbie-Sue', 'Ellen', 'Lauren', 'Peggy', 'Sue-Ellen', 'Colleen', 'Emily', 'Megan', 'Rachel', 'Wendy' ], 'Black': [ 'Alonzo', 'Jamel', 'Lerone', 'Percell', 'Theo', 'Alphonse', 'Jerome', 'Leroy', 'Rasaan', 'Torrance', 'Darnell', 'Lamar', 'Lionel', 'Rashaun', 'Tyree', 'Deion', 'Lamont', 'Malik', 'Terrence', 'Tyrone', 'Everol', 'Lavon', 'Marcellus', 'Terryl', 'Wardell', 'Aiesha', 'Lashelle', 'Nichelle', 'Shereen', 'Temeka', 'Ebony', 'Latisha', 'Shaniqua', 'Tameisha', 'Teretha', 'Jasmine', 'Latonya', 'Shanise', 'Tanisha', 'Tia', 'Lakisha', 'Latoya', 'Sharise', 'Tashika', 'Yolanda', 'Lashandra', 'Malika', 'Shavonn', 'Tawanda', 'Yvette' ], #         . 'Hispanic': [ 'Juan', 'José', 'Miguel', 'Luís', 'Jorge', 'Santiago', 'Matías', 'Sebastián', 'Mateo', 'Nicolás', 'Alejandro', 'Samuel', 'Diego', 'Daniel', 'Tomás', 'Juana', 'Ana', 'Luisa', 'María', 'Elena', 'Sofía', 'Isabella', 'Valentina', 'Camila', 'Valeria', 'Ximena', 'Luciana', 'Mariana', 'Victoria', 'Martina' ], #       # ,   .     . # #          # -   .    #   ,    . # #       . 'Arab/Muslim': [ 'Mohammed', 'Omar', 'Ahmed', 'Ali', 'Youssef', 'Abdullah', 'Yasin', 'Hamza', 'Ayaan', 'Syed', 'Rishaan', 'Samar', 'Ahmad', 'Zikri', 'Rayyan', 'Mariam', 'Jana', 'Malak', 'Salma', 'Nour', 'Lian', 'Fatima', 'Ayesha', 'Zahra', 'Sana', 'Zara', 'Alya', 'Shaista', 'Zoya', 'Yasmin' ] }</span></span></code> </pre> <br>  Mit Pandas erstellen wir eine Tabelle mit Namen, deren vorherrschender ethnischer Herkunft und Tonalitätsbewertungen: <br><br><pre> <code class="plaintext hljs">def name_sentiment_table(): frames = [] for group, name_list in sorted(NAMES_BY_ETHNICITY.items()): lower_names = [name.lower() for name in name_list] sentiments = words_to_sentiment(lower_names) sentiments['group'] = group frames.append(sentiments) #           return pd.concat(frames) name_sentiments = name_sentiment_table()</code> </pre> <br>  Beispieldaten: <br><br> <code>name_sentiments.ix[::25]</code> <br> <table border="1" width="350"><thead><tr><th></th><th>  Tonalität </th><th>  die Gruppe </th></tr></thead><tbody><tr><th>  Mohammed </th><td>  0,834974 </td><td>  Araber / Muslim </td></tr><tr><th>  alya </th><td>  3,916803 </td><td>  Araber / Muslim </td></tr><tr><th>  Terryl </th><td>  -2,858010 </td><td>  Schwarz </td></tr><tr><th>  josé </th><td>  0,432956 </td><td>  Hispanic </td></tr><tr><th>  Luciana </th><td>  1,086073 </td><td>  Hispanic </td></tr><tr><th>  hank </th><td>  0,391858 </td><td>  Weiß </td></tr><tr><th>  Megan </th><td>  2.158679 </td><td>  Weiß </td></tr></tbody></table><br>  Wir werden für jeden Namen ein Diagramm der Verteilung der Tonalität erstellen. <br><br><pre> <code class="python hljs">plot = seaborn.swarmplot(x=<span class="hljs-string"><span class="hljs-string">'group'</span></span>, y=<span class="hljs-string"><span class="hljs-string">'sentiment'</span></span>, data=name_sentiments) plot.set_ylim([<span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>])</code> </pre> <br> <code>(-10, 10)</code> <br> <br><img src="https://habrastorage.org/webt/qv/y7/ge/qvy7gel8rrvm5txo-nou6g0i0re.png"><br><br>  Oder als Histogramm mit Konfidenzintervallen für durchschnittlich 95%. <br><br><pre> <code class="python hljs">plot = seaborn.barplot(x=<span class="hljs-string"><span class="hljs-string">'group'</span></span>, y=<span class="hljs-string"><span class="hljs-string">'sentiment'</span></span>, data=name_sentiments, capsize=<span class="hljs-number"><span class="hljs-number">.1</span></span>)</code> </pre> <br><img src="https://habrastorage.org/webt/uv/ib/n6/uvibn6olthaxt6cxbd96szehq94.png"><br><br>  Führen Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">abschließend das</a> Statistikpaket für <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">seriöse</a> Statistikmodelle aus.  Es wird gezeigt, wie groß die Verzerrung des Algorithmus ist (zusammen mit einer Reihe anderer Statistiken). <br><br><br>  <font color="gray">OLS-Regressionsergebnisse</font> <br><table><tbody><tr><th>  Dep.  Variable: </th><td>  Stimmung </td><th>  R-Quadrat: </th><td>  0,208 </td></tr><tr><th>  Modell: </th><td>  OLS </td><th>  Adj.  R-Quadrat: </th><td>  0,192 </td></tr><tr><th>  Methode: </th><td>  Kleinste Quadrate </td><th>  F-Statistik: </th><td>  04/13 </td></tr><tr><th>  Datum: </th><td>  Do, 13. Juli 2017 </td><th>  Prob (F-Statistik): </th><td>  1.31e-07 </td></tr><tr><th>  Zeit: </th><td>  11:31:17 </td><th>  Log-Wahrscheinlichkeit: </th><td>  -356,78 </td></tr><tr><th>  Nein, nein.  Beobachtungen: </th><td>  153 </td><th>  AIC: </th><td>  721.6 </td></tr><tr><th>  Df Rückstände: </th><td>  149 </td><th>  BIC: </th><td>  733.7 </td></tr><tr><th>  Df Modell: </th><td>  3 </td><th></th><td></td></tr><tr><th>  Kovarianztyp: </th><td>  nicht robust </td><th></th><td></td></tr></tbody></table><br>  Die F-Statistik ist das Verhältnis der Variation zwischen Gruppen zur Variation innerhalb von Gruppen, das als allgemeine Bewertung der Verzerrung angesehen werden kann. <br><br>  Unmittelbar darunter liegt die Wahrscheinlichkeit, dass wir die maximale F-Statistik mit der Nullhypothese sehen, dh wenn kein Unterschied zwischen den verglichenen Optionen besteht.  Die Wahrscheinlichkeit ist sehr, sehr gering.  In einem wissenschaftlichen Artikel würden wir das Ergebnis als "sehr statistisch signifikant" bezeichnen. <br><br>  Wir müssen den F-Wert verbessern.  Je niedriger desto besser. <br><br> <code>ols_model.fvalue <br> 13.041597745167659</code> <br> <br><h1>  Schritt 7. Andere Daten versuchen. </h1><br>  Wir haben jetzt die Möglichkeit, die schädliche Verzerrung des Modells numerisch zu messen.  Versuchen wir es anzupassen.  Dazu müssen Sie eine Reihe von Dingen wiederholen, die früher nur separate Schritte in einem Python-Notizblock waren. <br><br>  Wenn ich guten, unterstützten Code schreiben würde, würde ich keine globalen Variablen wie <code>model</code> und <code>embeddings</code> .  Mit dem aktuellen Spaghetti-Code können Sie jedoch jeden Schritt besser untersuchen und verstehen, was passiert.  Wir verwenden einen Teil des Codes wieder und definieren zumindest eine Funktion, um einige Schritte zu wiederholen: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">retrain_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(new_embs)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""      . """</span></span> <span class="hljs-keyword"><span class="hljs-keyword">global</span></span> model, embeddings, name_sentiments embeddings = new_embs pos_vectors = embeddings.loc[pos_words].dropna() neg_vectors = embeddings.loc[neg_words].dropna() vectors = pd.concat([pos_vectors, neg_vectors]) targets = np.array([<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> entry <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> pos_vectors.index] + [<span class="hljs-number"><span class="hljs-number">-1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> entry <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> neg_vectors.index]) labels = list(pos_vectors.index) + list(neg_vectors.index) train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \ train_test_split(vectors, targets, labels, test_size=<span class="hljs-number"><span class="hljs-number">0.1</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">0</span></span>) model = SGDClassifier(loss=<span class="hljs-string"><span class="hljs-string">'log'</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">0</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">100</span></span>) model.fit(train_vectors, train_targets) accuracy = accuracy_score(model.predict(test_vectors), test_targets) print(<span class="hljs-string"><span class="hljs-string">"Accuracy of sentiment: {:.2%}"</span></span>.format(accuracy)) name_sentiments = name_sentiment_table() ols_model = statsmodels.formula.api.ols(<span class="hljs-string"><span class="hljs-string">'sentiment ~ group'</span></span>, data=name_sentiments).fit() print(<span class="hljs-string"><span class="hljs-string">"F-value of bias: {:.3f}"</span></span>.format(ols_model.fvalue)) print(<span class="hljs-string"><span class="hljs-string">"Probability given null hypothesis: {:.3}"</span></span>.format(ols_model.f_pvalue)) <span class="hljs-comment"><span class="hljs-comment">#        Y plot = seaborn.swarmplot(x='group', y='sentiment', data=name_sentiments) plot.set_ylim([-10, 10])</span></span></code> </pre> <br><h3>  Wir versuchen word2vec </h3><br>  Es ist davon auszugehen, dass nur GloVe das Problem hat.  Es gibt wahrscheinlich viele zweifelhafte Websites in der Common Crawl-Datenbank und mindestens 20 Exemplare des Urban Dictionary of Street Slang.  Vielleicht ist es auf einer anderen Basis besser: Was ist mit dem guten alten word2vec, das in Google News trainiert wurde? <br><br>  Es scheint, dass die maßgeblichste Quelle für word2vec-Daten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diese Datei auf Google Drive ist</a> .  Laden Sie es herunter und speichern Sie es als <code>data/word2vec-googlenews-300.bin.gz</code> . <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   ConceptNet   word2vec   Pandas     from conceptnet5.vectors.formats import load_word2vec_bin w2v = load_word2vec_bin('data/word2vec-googlenews-300.bin.gz', nrows=2000000) #  word2vec    w2v.index = [label.casefold() for label in w2v.index] #  ,    w2v = w2v.reset_index().drop_duplicates(subset='index', keep='first').set_index('index') retrain_model(w2v)</span></span></code> </pre> <br> <code>Accuracy of sentiment: 94.30% <br> F-value of bias: 15.573 <br> Probability given null hypothesis: 7.43e-09</code> <br> <br>  Word2vec erwies sich mit einem F-Wert von mehr als 15 als noch schlechter. <br><br>  Im Prinzip war es dumm zu erwarten, dass <i>Nachrichten</i> besser vor Voreingenommenheit geschützt werden. <br><br><h3>  ConceptNet Numberbatch versuchen </h3><br>  Schließlich kann ich über mein eigenes Projekt zur Vektordarstellung von Wörtern sprechen. <br><br>  ConceptNet mit der Vektorpräsentationsfunktion ist das Wissensdiagramm, an dem ich arbeite.  Es normalisiert Vektordarstellungen in der Trainingsphase und identifiziert und entfernt einige Quellen für algorithmischen Rassismus und Sexismus.  Diese Methode zur Korrektur von Verzerrungen basiert auf einem wissenschaftlichen Artikel von Bulukbashi et al., <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">"Debiasing Word Embeddings",</a> und wird verallgemeinert, um mehrere Arten von Verzerrungen gleichzeitig zu beseitigen.  Soweit ich weiß, ist dies das einzige semantische System, in dem es so etwas gibt. <br><br>  Von Zeit zu Zeit exportieren wir vorberechnete Vektoren aus ConceptNet - diese Releases heißen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ConceptNet Numberbatch</a> .  Im April 2017 wurde die erste Version mit Bias-Korrektur veröffentlicht, daher werden wir die englischsprachigen Vektoren laden und unser Modell neu trainieren. <br><br>  <code><a href="">numberbatch-en-17.04b.txt.gz</a></code> , speichern es im <code>data/</code> Verzeichnis und trainieren das Modell neu: <br><br><pre> <code class="python hljs">retrain_model(load_embeddings(<span class="hljs-string"><span class="hljs-string">'data/numberbatch-en-17.04b.txt'</span></span>))</code> </pre> <br> <code>Accuracy of sentiment: 97.46% <br> F-value of bias: 3.805 <br> Probability given null hypothesis: 0.0118</code> <br> <br><img src="https://habrastorage.org/webt/5d/iu/uq/5diuuqrst8bca5-m7fljox--pro.png"><br><br>  Hat ConceptNet Numberbatch das Problem vollständig behoben?  Kein algorithmischer Rassismus mehr?  <b>Nein.</b> <br><br>  Ist Rassismus viel weniger geworden?  <b>Auf jeden Fall</b> . <br><br>  Schlüsselbereiche für ethnische Gruppen überlappen sich viel stärker als in GloVe- oder word2vec-Vektoren.  Im Vergleich zu GloVe nahm der Wert von F mehr als dreimal ab und im Vergleich zu word2vec mehr als viermal.  Und im Allgemeinen sehen wir beim Vergleich verschiedener Namen viel kleinere Unterschiede in der Tonalität: Dies sollte so sein, da die Namen das Ergebnis der Analyse wirklich nicht beeinflussen sollten. <br><br>  Eine leichte Korrelation blieb jedoch bestehen.  Vielleicht kann ich solche Daten und Trainingsparameter aufnehmen, dass das Problem gelöst zu sein scheint.  Dies wird jedoch eine schlechte Option sein, da <i>das</i> Problem <i>tatsächlich bestehen</i> bleibt, da wir in ConceptNet nicht alle Ursachen für algorithmischen Rassismus identifiziert und kompensiert haben.  Aber das ist ein guter Anfang. <br><br><h3>  Keine Fallstricke </h3><br>  Bitte beachten Sie, dass sich mit der Umstellung auf ConceptNet Numberbatch die Genauigkeit der Vorhersage der Tonalität verbessert hat. <br><br>  Jemand könnte vorgeschlagen haben, dass die Korrektur von algorithmischem Rassismus die Ergebnisse auf andere Weise verschlechtern würde.  Aber nein.  Möglicherweise haben Sie Daten, die besser und weniger rassistisch sind.  Die Daten verbessern sich mit dieser Korrektur wirklich.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Der von Menschen erworbene Wort2vec- und GloVe-Rassismus hat nichts mit der Genauigkeit des Algorithmus zu tun. </font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Andere Ansätze </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dies ist natürlich nur eine Möglichkeit, die Tonalität zu analysieren. Einige Details können unterschiedlich implementiert werden. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Stattdessen oder zusätzlich zum Ändern der Vektorbasis können Sie versuchen, dieses Problem direkt in der Ausgabe zu beheben. Beseitigen Sie beispielsweise im Allgemeinen die Bewertung der Tonalität für Namen und Personengruppen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Im Allgemeinen besteht die Möglichkeit, die Berechnung der Tonalität aller Wörter abzulehnen und sie nur für Wörter aus der Liste zu berechnen. Dies ist vielleicht die häufigste Form der Stimmungsanalyse - ohne maschinelles Lernen. Die Ergebnisse sind nicht voreingenommener als der Autor der Liste. Das Ablehnen von maschinellem Lernen bedeutet jedoch, den Rückruf (Rückruf) zu reduzieren. Die einzige Möglichkeit, das Modell an einen Datensatz anzupassen, besteht darin, die Liste manuell zu bearbeiten.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Als hybrider Ansatz können Sie eine große Anzahl geschätzter Tonalitätsschätzungen für Wörter erstellen und eine Person anweisen, diese geduldig zu bearbeiten und eine Liste von Ausnahmewörtern mit einer Tonalität von Null zu erstellen. </font><font style="vertical-align: inherit;">Aber das ist ein zusätzlicher Job. </font><font style="vertical-align: inherit;">Auf der anderen Seite werden Sie wirklich sehen, wie das Modell funktioniert. </font><font style="vertical-align: inherit;">Ich denke, dass dies auf jeden Fall gesucht werden sollte.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de436506/">https://habr.com/ru/post/de436506/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de436496/index.html">PVS-Studio für Java</a></li>
<li><a href="../de436498/index.html">Software AG: Nicht nur ARIS</a></li>
<li><a href="../de436500/index.html">Wie der Rahmen von Rise of the Tomb Raider gerendert wird</a></li>
<li><a href="../de436502/index.html">Abonnement verwöhnt oder wie man mehr an dieselben Kunden verkauft</a></li>
<li><a href="../de436504/index.html">System im Paket oder was ist unter Chip-Paketabdeckung?</a></li>
<li><a href="../de436508/index.html">Investitionen in Höhe von 10 Millionen US-Dollar und Wozniaks Lob - Schaffung eines Bildungscomputers für Kinder</a></li>
<li><a href="../de436510/index.html">Kerndaten im Detail</a></li>
<li><a href="../de436512/index.html">Wie wir problematische Releases mit Graphite und Moira finden. Erleben Sie Yandex.Money</a></li>
<li><a href="../de436514/index.html">Erstellen von Geschichten für Instagram aus PHP</a></li>
<li><a href="../de436518/index.html">Haiku β1 - mach / b / OS wieder großartig</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>