<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üå•Ô∏è üë©üèº‚ÄçüöÄ üë®‚Äçüé§ Wie man ohne gro√üen Aufwand einen KI-Rassisten erschafft üé° üìù üèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Eine warnende Lektion. 

 Machen wir einen Tonalit√§tsklassifikator! 

 Die Stimmungsanalyse (Stimmungsanalyse) ist eine sehr h√§ufige Aufgabe in der Ve...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie man ohne gro√üen Aufwand einen KI-Rassisten erschafft</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/436506/"> Eine warnende Lektion. <br><br>  <b>Machen wir einen Tonalit√§tsklassifikator!</b> <br><br>  Die Stimmungsanalyse (Stimmungsanalyse) ist eine sehr h√§ufige Aufgabe in der Verarbeitung nat√ºrlicher Sprache (NLP), und dies ist nicht √ºberraschend.  F√ºr ein Unternehmen ist es wichtig zu verstehen, was die Leute sagen: positiv oder negativ.  Eine solche Analyse wird verwendet, um soziale Netzwerke, Kundenfeedback und sogar den algorithmischen Aktienhandel zu √ºberwachen (daher <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kaufen</a> Bots <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Berkshire Hathaway-Aktien, nachdem sie im letzten Film positive Bewertungen √ºber die Rolle von Anne Hathaway ver√∂ffentlicht haben</a> ). <br><br>  Die Analysemethode ist manchmal zu einfach, aber eine der einfachsten M√∂glichkeiten, messbare Ergebnisse zu erzielen.  Senden Sie einfach den Text - und die Ausgabe ist positiv und negativ.  Sie m√ºssen sich nicht mit dem Analysebaum befassen, ein Diagramm oder eine andere komplexe Darstellung erstellen. <br><a name="habracut"></a><br>  Das werden wir tun.  Wir werden den Weg des geringsten Widerstands beschreiten und den einfachsten Klassifikator erstellen, der wahrscheinlich jedem bekannt vorkommt, der an relevanten Entwicklungen auf dem Gebiet der NLP beteiligt ist.  Ein solches Modell findet sich beispielsweise im Artikel <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Deep Averaging Networks</a></i> (Iyyer et al., 2015).  Wir versuchen √ºberhaupt nicht, ihre Ergebnisse in Frage zu stellen oder das Modell zu kritisieren.  Wir geben einfach eine bekannte Methode zur Vektordarstellung von W√∂rtern. <br><br>  Arbeitsplan: <br><br><ul><li>  F√ºhren Sie eine typische <b>Vektordarstellung von W√∂rtern ein</b> , um mit Bedeutungen (Bedeutungen) zu arbeiten. </li><li>  F√ºhren Sie <b>Trainings- und Testdatens√§tze</b> mit Standardlisten positiver und negativer W√∂rter ein. </li><li>  <b>Trainieren Sie</b> den Gradientenabstiegsklassifikator, um andere positive und negative W√∂rter anhand ihrer Vektordarstellung zu erkennen. </li><li>  Verwenden Sie diesen Klassifikator, um die <b>Tonalit√§tsbewertungen</b> f√ºr Texts√§tze zu berechnen. </li><li>  <b>Um das Monster zu sehen, das</b> wir erschaffen haben. </li></ul><br>  Und dann werden wir sehen, "wie man ohne besondere Anstrengungen einen KI-Rassisten schafft".  Nat√ºrlich k√∂nnen Sie das System nicht in solch einer monstr√∂sen Form verlassen, also werden wir: <br><br><ul><li>  <b>Um das Problem</b> statistisch <b>zu bewerten</b> , damit es m√∂glich wird, den Fortschritt zu messen, sobald er gel√∂st ist. </li><li>  <b>Verbessern Sie Daten</b> , um ein genaueres und weniger rassistisches semantisches Modell zu erhalten. </li></ul><br><h1>  Software-Abh√§ngigkeiten </h1><br>  Dieses Tutorial ist in Python <code>numpy</code> und <code>scipy</code> auf einem typischen Python-Stapel f√ºr maschinelles Lernen: <code>numpy</code> und <code>scipy</code> f√ºr numerisches Rechnen, <code>pandas</code> f√ºr Datenverwaltung und <code>scikit-learn</code> f√ºr maschinelles Lernen.  Am Ende werden wir auch <code>matplotlib</code> und <code>seaborn</code> f√ºr die <code>seaborn</code> . <br><br>  Im Prinzip kann <code>scikit-learn</code> durch TensorFlow oder Keras oder √§hnliches ersetzt werden: Sie k√∂nnen den Klassifikator auch auf Gradientenabstieg trainieren.  Ihre Abstraktionen brauchen wir aber nicht, denn hier findet das Training in einer Phase statt. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> statsmodels.formula.api <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.linear_model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SGDClassifier <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> accuracy_score <span class="hljs-comment"><span class="hljs-comment">#     %matplotlib inline seaborn.set_context('notebook', rc={'figure.figsize': (10, 6)}, font_scale=1.5)</span></span></code> </pre> <br><h1>  Schritt 1. Vektordarstellung von W√∂rtern </h1><br>  Vektordarstellungen werden h√§ufig bei Texteingabe verwendet.  W√∂rter werden im mehrdimensionalen Raum zu Vektoren, wobei benachbarte Vektoren √§hnliche Bedeutungen darstellen.  Mithilfe von Vektordarstellungen k√∂nnen Sie W√∂rter anhand (ungef√§hr) ihrer Bedeutung vergleichen und nicht nur anhand genauer √úbereinstimmungen. <br><br>  Erfolgreiches Lernen erfordert Hunderte von Gigabyte Text.  Gl√ºcklicherweise haben verschiedene Forschungsteams diese Arbeit bereits durchgef√ºhrt und vorgefertigte Modelle von Vektordarstellungen zum Download bereitgestellt. <br><br>  Die beiden bekanntesten englischen Datens√§tze sind <b>word2vec</b> (auf Google News trainiert) und <b>GloVe</b> (auf Common Crawl-Webseiten).  Jeder von ihnen wird ein √§hnliches Ergebnis liefern, aber wir werden das GloVe-Modell verwenden, da es eine transparentere Datenquelle hat. <br><br>  GloVe ist in drei Gr√∂√üen erh√§ltlich: 6 Milliarden, 42 Milliarden und 840 Milliarden. Das neueste Modell ist das leistungsst√§rkste, erfordert jedoch erhebliche Verarbeitungsressourcen.  Die 42-Milliarden-Version ist ziemlich gut und das W√∂rterbuch ist ordentlich auf 1 Million W√∂rter zugeschnitten.  Wir sind auf dem Weg des geringsten Widerstands, also nehmen Sie die 42-Milliarden-Version. <br><br><blockquote>  <b>- Warum ist es so wichtig, ein ‚Äûbekanntes‚Äú Modell zu verwenden?</b> <br><br>  "Ich bin froh, dass du danach gefragt hast, hypothetischer Gespr√§chspartner!"  Bei jedem Schritt versuchen wir, etwas extrem Typisches zu tun, und das beste Modell f√ºr die Vektordarstellung von W√∂rtern wurde aus irgendeinem Grund noch nicht bestimmt.  Ich hoffe, dass dieser Artikel den Wunsch weckt, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">moderne, hochwertige Modelle zu verwenden</a> , insbesondere solche, die einen algorithmischen Fehler ber√ºcksichtigen und versuchen, ihn zu korrigieren.  Dazu sp√§ter mehr. </blockquote><br>  Laden Sie glove.42B.300d.zip von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der GloVe-Website herunter</a> und extrahieren Sie die Datei <code>data/glove.42B.300d.txt</code> .  Als n√§chstes definieren wir eine Funktion zum Lesen von Vektoren in einem einfachen Format. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">load_embeddings</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""  DataFrame      ,   word2vec, GloVe, fastText  ConceptNet Numberbatch.            . """</span></span> labels = [] rows = [] <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(filename, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> infile: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(infile): items = line.rstrip().split(<span class="hljs-string"><span class="hljs-string">' '</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(items) == <span class="hljs-number"><span class="hljs-number">2</span></span>: <span class="hljs-comment"><span class="hljs-comment"># This is a header row giving the shape of the matrix continue labels.append(items[0]) values = np.array([float(x) for x in items[1:]], 'f') rows.append(values) arr = np.vstack(rows) return pd.DataFrame(arr, index=labels, dtype='f') embeddings = load_embeddings('data/glove.42B.300d.txt') embeddings.shape</span></span></code> </pre> <br> <code>(1917494, 300)</code> <br> <h1>  Schritt 2. Goldstandard-Tonalit√§tsw√∂rterbuch </h1><br>  Jetzt brauchen wir Informationen dar√ºber, welche W√∂rter als positiv und welche als negativ angesehen werden.  Es gibt viele solcher W√∂rterb√ºcher, aber wir werden ein sehr einfaches W√∂rterbuch (Hu und Liu, 2004) verwenden, das in dem Artikel von <i>Deep Averaging Networks verwendet wird</i> . <br><br>  Laden Sie das W√∂rterbuch von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Website</a> von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bing Liu herunter</a> und extrahieren Sie die Daten in <code>data/positive-words.txt</code> und <code>data/negative-words.txt</code> . <br><br>  Als N√§chstes legen wir fest, wie diese Dateien gelesen und als <code>neg_words</code> <code>pos_words</code> und <code>neg_words</code> : <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">load_lexicon</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""       (https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html)      Latin-1.      ,    - .    ,    ';'   ,   . """</span></span> lexicon = [] <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(filename, encoding=<span class="hljs-string"><span class="hljs-string">'latin-1'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> infile: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> infile: line = line.rstrip() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> line.startswith(<span class="hljs-string"><span class="hljs-string">';'</span></span>): lexicon.append(line) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> lexicon pos_words = load_lexicon(<span class="hljs-string"><span class="hljs-string">'data/positive-words.txt'</span></span>) neg_words = load_lexicon(<span class="hljs-string"><span class="hljs-string">'data/negative-words.txt'</span></span>)</code> </pre> <br><h1>  Schritt 3. Wir trainieren das Modell, um die Tonalit√§t vorherzusagen </h1><br>  Basierend auf den Vektoren positiver und negativer W√∂rter verwenden wir den Befehl Pandas <code>.loc[]</code> , um nach Vektordarstellungen aller W√∂rter zu suchen. <br><br>  Einige W√∂rter fehlen im GloVe-W√∂rterbuch.  Meistens sind dies Tippfehler wie ‚ÄûFancinating‚Äú.  Hier sehen wir eine Reihe von <code>NaN</code> , die das Fehlen eines Vektors anzeigen, und l√∂schen sie mit dem Befehl <code>.dropna()</code> . <br><br> <code>pos_vectors = embeddings.loc[pos_words].dropna() <br> neg_vectors = embeddings.loc[neg_words].dropna()</code> <br> <br>  Jetzt erstellen wir Datenfelder am Eingang (Vektordarstellungen) und am Ausgang (1 f√ºr positive W√∂rter und -1 f√ºr negative).  Wir √ºberpr√ºfen auch, ob die Vektoren an W√∂rter angeh√§ngt sind, damit wir die Ergebnisse interpretieren k√∂nnen. <br><br> <code>vectors = pd.concat([pos_vectors, neg_vectors]) <br> targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index]) <br> labels = list(pos_vectors.index) + list(neg_vectors.index)</code> <br> <br><blockquote>  <b>- Warte einen Moment.</b>  <b>Einige W√∂rter sind weder positiv noch negativ, sie sind neutral.</b>  <b>Sollte nicht eine dritte Klasse f√ºr neutrale W√∂rter geschaffen werden?</b> <br><br>  "Ich denke, er h√§tte sich als n√ºtzlich erwiesen."  Sp√§ter werden wir sehen, welche Probleme durch die Zuordnung der Tonalit√§t zu neutralen W√∂rtern entstehen.  Wenn wir neutrale W√∂rter zuverl√§ssig identifizieren k√∂nnen, ist es durchaus m√∂glich, die Komplexit√§t des Klassifikators auf drei Kategorien zu erh√∂hen.  Aber Sie m√ºssen ein W√∂rterbuch mit neutralen W√∂rtern finden, denn im W√∂rterbuch von Liu gibt es nur positive und negative. <br><br>  Also habe ich meine Version mit 800 Beispielen von W√∂rtern ausprobiert und das Gewicht erh√∂ht, um neutrale W√∂rter vorherzusagen.  Aber die Endergebnisse unterschieden sich nicht sehr von dem, was Sie jetzt sehen werden. <br><br>  <b>- Wie unterscheidet diese Liste positive und negative W√∂rter?</b>  <b>Kommt das nicht auf den Kontext an?</b> <br><br>  - Gute Frage.  Die Analyse allgemeiner Schl√ºssel ist nicht so einfach, wie es scheint.  Die Grenze ist an einigen Stellen ziemlich willk√ºrlich.  In dieser Liste wird das Wort "frech" als "schlecht" und "ehrgeizig" als "gut" markiert.  "Comic" ist schlecht und "lustig" ist gut.  Eine ‚ÄûR√ºckerstattung‚Äú ist gut, obwohl sie normalerweise in einem schlechten Kontext erw√§hnt wird, wenn Sie jemandem Geld schulden oder jemandem etwas schulden. <br><br>  Jeder versteht, dass die Tonalit√§t durch den Kontext bestimmt wird, aber in einem einfachen Modell muss man den Kontext ignorieren und hoffen, dass die durchschnittliche Tonalit√§t richtig erraten wird. </blockquote><br>  Mit der Funktion <code>train_test_split</code> teilen <code>train_test_split</code> gleichzeitig die Eingabevektoren, Ausgabewerte und Beschriftungen in Trainings- und Testdaten auf, w√§hrend 10% zum Testen √ºbrig <code>train_test_split</code> . <br><br><pre> <code class="python hljs">train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \ train_test_split(vectors, targets, labels, test_size=<span class="hljs-number"><span class="hljs-number">0.1</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">0</span></span>)</code> </pre> <br>  Erstellen Sie nun einen Klassifikator und √ºbergeben Sie Vektoren durch Iterationen.  Wir verwenden die logistische Verlustfunktion, damit der endg√ºltige Klassifikator die Wahrscheinlichkeit ableiten kann, dass das Wort positiv oder negativ ist. <br><br><pre> <code class="python hljs">model = SGDClassifier(loss=<span class="hljs-string"><span class="hljs-string">'log'</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">0</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">100</span></span>) model.fit(train_vectors, train_targets) SGDClassifier(alpha=<span class="hljs-number"><span class="hljs-number">0.0001</span></span>, average=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, class_weight=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, epsilon=<span class="hljs-number"><span class="hljs-number">0.1</span></span>, eta0=<span class="hljs-number"><span class="hljs-number">0.0</span></span>, fit_intercept=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, l1_ratio=<span class="hljs-number"><span class="hljs-number">0.15</span></span>, learning_rate=<span class="hljs-string"><span class="hljs-string">'optimal'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'log'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">100</span></span>, n_jobs=<span class="hljs-number"><span class="hljs-number">1</span></span>, penalty=<span class="hljs-string"><span class="hljs-string">'l2'</span></span>, power_t=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">0</span></span>, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">0</span></span>, warm_start=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)</code> </pre> <br>  Wir bewerten den Klassifikator anhand von Testvektoren.  Es zeigt eine Genauigkeit von 95%.  Nicht schlecht. <br><br> <code>accuracy_score(model.predict(test_vectors), test_targets) <br> 0.95022624434389136</code> <br> <br>  Wir definieren die Tonalit√§tsvorhersagefunktion f√ºr bestimmte W√∂rter und verwenden sie dann f√ºr einige Beispiele aus Testdaten. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">vecs_to_sentiment</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(vecs)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># predict_log_proba  log-    predictions = model.predict_log_proba(vecs) #        #  log-    . return predictions[:, 1] - predictions[:, 0] def words_to_sentiment(words): vecs = embeddings.loc[words].dropna() log_odds = vecs_to_sentiment(vecs) return pd.DataFrame({'sentiment': log_odds}, index=vecs.index) #  20      words_to_sentiment(test_labels).ix[:20]</span></span></code> </pre> <br><table border="1" width="350"><thead><tr><th></th><th>  Tonalit√§t </th></tr></thead><tbody><tr><th>  zappeln </th><td>  -9,931679 </td></tr><tr><th>  unterbrechen </th><td>  -9,634706 </td></tr><tr><th>  standhaft </th><td>  1,466919 </td></tr><tr><th>  imagin√§r </th><td>  -2,989215 </td></tr><tr><th>  Besteuerung </th><td>  0,468522 </td></tr><tr><th>  weltber√ºhmt </th><td>  6.908561 </td></tr><tr><th>  preiswert </th><td>  9.237223 </td></tr><tr><th>  Entt√§uschung </th><td>  -8,737182 </td></tr><tr><th>  totalit√§r </th><td>  -10,851580 </td></tr><tr><th>  kriegerisch </th><td>  -8,328674 </td></tr><tr><th>  friert ein </th><td>  -8,456981 </td></tr><tr><th>  S√ºnde </th><td>  -7,839670 </td></tr><tr><th>  zerbrechlich </th><td>  -4.018289 </td></tr><tr><th>  get√§uscht </th><td>  -4.309344 </td></tr><tr><th>  ungel√∂st </th><td>  -2,816172 </td></tr><tr><th>  klug </th><td>  2.339609 </td></tr><tr><th>  d√§monisiert </th><td>  -2.102152 </td></tr><tr><th>  sorglos </th><td>  8,747150 </td></tr><tr><th>  unbeliebt </th><td>  -7,887475 </td></tr><tr><th>  sympathisieren </th><td>  1,790899 </td></tr></tbody></table><br>  Es ist zu sehen, dass der Klassifikator funktioniert.  Er lernte, die Tonalit√§t in Worten au√üerhalb der Trainingsdaten zu verallgemeinern. <br><br><h1>  Schritt 4. Holen Sie sich eine Tonalit√§tsbewertung f√ºr den Text. </h1><br>  Es gibt viele M√∂glichkeiten, einer Gesamtsch√§tzung Vektoren hinzuzuf√ºgen.  Wieder folgen wir dem Weg des geringsten Widerstands, also nehmen Sie einfach den Durchschnittswert. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re TOKEN_RE = re.compile(<span class="hljs-string"><span class="hljs-string">r"\w.*?\b"</span></span>) <span class="hljs-comment"><span class="hljs-comment"># regex  ,     (\w)   #   (.+?)    (\b).   #       . def text_to_sentiment(text): tokens = [token.casefold() for token in TOKEN_RE.findall(text)] sentiments = words_to_sentiment(tokens) return sentiments['sentiment'].mean()</span></span></code> </pre> <br>  Es gibt viel zu optimieren: <br><br><ul><li>  Einf√ºhrung einer umgekehrten Beziehung zwischen dem Wortgewicht und seiner H√§ufigkeit, so dass dieselben Pr√§positionen die Tonalit√§t nicht stark beeinflussen. </li><li>  Einstellung, damit kurze S√§tze nicht mit extremen Tonalit√§tswerten enden. </li><li>  Buchhaltungsphrasen. </li><li>  Ein zuverl√§ssigerer Wortsegmentierungsalgorithmus, den Apostrophe nicht niederschlagen. </li><li>  Ber√ºcksichtigung von Negativen wie "nicht zufrieden". </li></ul><br>  Aber alles erfordert zus√§tzlichen Code und wird die Ergebnisse nicht grundlegend √§ndern.  Zumindest k√∂nnen Sie jetzt verschiedene Angebote grob vergleichen: <br><br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"this example is pretty cool"</span></span>) <span class="hljs-number"><span class="hljs-number">3.889968926086298</span></span></code> </pre> <br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"this example is okay"</span></span>) <span class="hljs-number"><span class="hljs-number">2.7997773492425186</span></span></code> </pre> <br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"meh, this example sucks"</span></span>) <span class="hljs-number"><span class="hljs-number">-1.1774475917460698</span></span></code> </pre> <br><h1>  Schritt 5. Siehe das Monster, das wir erschaffen haben </h1><br>  Nicht jeder Satz hat eine ausgepr√§gte Tonalit√§t.  Mal sehen, was mit neutralen S√§tzen passiert: <br><br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"Let's go get Italian food"</span></span>) <span class="hljs-number"><span class="hljs-number">2.0429166109408983</span></span></code> </pre> <br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"Let's go get Chinese food"</span></span>) <span class="hljs-number"><span class="hljs-number">1.4094033658140972</span></span></code> </pre> <br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"Let's go get Mexican food"</span></span>) <span class="hljs-number"><span class="hljs-number">0.38801985560121732</span></span></code> </pre> <br>  Ich bin bereits auf ein solches Ph√§nomen gesto√üen, als ich Bewertungen von Restaurants unter Ber√ºcksichtigung von Vektordarstellungen von W√∂rtern analysierte.  Ohne ersichtlichen Grund <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">haben alle mexikanischen Restaurants eine niedrigere Gesamtpunktzahl</a> . <br><br>  Vektordarstellungen erfassen subtile semantische Unterschiede im Kontext.  Sie spiegeln daher die Vorurteile unserer Gesellschaft wider. <br><br>  Hier sind einige andere neutrale Vorschl√§ge: <br><br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"My name is Emily"</span></span>) <span class="hljs-number"><span class="hljs-number">2.2286179364745311</span></span></code> </pre> <br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"My name is Heather"</span></span>) <span class="hljs-number"><span class="hljs-number">1.3976291151079159</span></span></code> </pre> <br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"My name is Yvette"</span></span>) <span class="hljs-number"><span class="hljs-number">0.98463802132985556</span></span></code> </pre> <br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"My name is Shaniqua"</span></span>) <span class="hljs-number"><span class="hljs-number">-0.47048131775890656</span></span></code> </pre> <br>  Verdammt noch mal ... <br><br>  Das mit den Namen von Menschen verbundene System hat v√∂llig unterschiedliche Gef√ºhle.  Sie k√∂nnen sich diese und viele andere Beispiele ansehen und feststellen, dass die Tonalit√§t bei stereotypen wei√üen Namen normalerweise h√∂her und bei stereotypen schwarzen Namen niedriger ist. <br><br>  Dieser Test wurde von Caliscan, Bryson und Narayanan in ihrem Forschungsbericht verwendet, der im April 2017 in der Zeitschrift <i>Science ver√∂ffentlicht</i> wurde.  Es zeigt, dass die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Semantik des Sprachkorpus die Vorurteile der Gesellschaft enth√§lt</a> .  Wir werden diese Methode verwenden. <br><br><h1>  Schritt 6. Bewertung des Problems </h1><br>  Wir wollen verstehen, wie solche Fehler vermieden werden k√∂nnen.  Lassen Sie uns mehr Daten durch den Klassifikator leiten und dessen ‚ÄûBias‚Äú statistisch messen. <br><br>  Hier haben wir vier Namenslisten, die unterschiedliche ethnische Hintergr√ºnde widerspiegeln, haupts√§chlich in den USA.  Die ersten beiden sind Listen mit √ºberwiegend ‚Äûwei√üen‚Äú und ‚Äûschwarzen‚Äú Namen, die auf der Grundlage eines Artikels von Kaliskan et al. Angepasst wurden. Ich habe auch spanische und muslimische Namen aus Arabisch und Urdu hinzugef√ºgt. <br><br>  Diese Daten werden verwendet, um die Verzerrung des Algorithmus w√§hrend des ConceptNet-Erstellungsprozesses zu √ºberpr√ºfen: Sie finden sie im Modul <code>conceptnet5.vectors.evaluation.bias</code> .  Es besteht die Idee, das W√∂rterbuch auf andere ethnische Gruppen auszudehnen, wobei nicht nur Namen, sondern auch Nachnamen ber√ºcksichtigt werden. <br><br>  Hier sind die Auflistungen: <br><br><pre> <code class="python hljs">NAMES_BY_ETHNICITY = { <span class="hljs-comment"><span class="hljs-comment">#           . 'White': [ 'Adam', 'Chip', 'Harry', 'Josh', 'Roger', 'Alan', 'Frank', 'Ian', 'Justin', 'Ryan', 'Andrew', 'Fred', 'Jack', 'Matthew', 'Stephen', 'Brad', 'Greg', 'Jed', 'Paul', 'Todd', 'Brandon', 'Hank', 'Jonathan', 'Peter', 'Wilbur', 'Amanda', 'Courtney', 'Heather', 'Melanie', 'Sara', 'Amber', 'Crystal', 'Katie', 'Meredith', 'Shannon', 'Betsy', 'Donna', 'Kristin', 'Nancy', 'Stephanie', 'Bobbie-Sue', 'Ellen', 'Lauren', 'Peggy', 'Sue-Ellen', 'Colleen', 'Emily', 'Megan', 'Rachel', 'Wendy' ], 'Black': [ 'Alonzo', 'Jamel', 'Lerone', 'Percell', 'Theo', 'Alphonse', 'Jerome', 'Leroy', 'Rasaan', 'Torrance', 'Darnell', 'Lamar', 'Lionel', 'Rashaun', 'Tyree', 'Deion', 'Lamont', 'Malik', 'Terrence', 'Tyrone', 'Everol', 'Lavon', 'Marcellus', 'Terryl', 'Wardell', 'Aiesha', 'Lashelle', 'Nichelle', 'Shereen', 'Temeka', 'Ebony', 'Latisha', 'Shaniqua', 'Tameisha', 'Teretha', 'Jasmine', 'Latonya', 'Shanise', 'Tanisha', 'Tia', 'Lakisha', 'Latoya', 'Sharise', 'Tashika', 'Yolanda', 'Lashandra', 'Malika', 'Shavonn', 'Tawanda', 'Yvette' ], #         . 'Hispanic': [ 'Juan', 'Jos√©', 'Miguel', 'Lu√≠s', 'Jorge', 'Santiago', 'Mat√≠as', 'Sebasti√°n', 'Mateo', 'Nicol√°s', 'Alejandro', 'Samuel', 'Diego', 'Daniel', 'Tom√°s', 'Juana', 'Ana', 'Luisa', 'Mar√≠a', 'Elena', 'Sof√≠a', 'Isabella', 'Valentina', 'Camila', 'Valeria', 'Ximena', 'Luciana', 'Mariana', 'Victoria', 'Martina' ], #       # ,   .     . # #          # -   .    #   ,    . # #       . 'Arab/Muslim': [ 'Mohammed', 'Omar', 'Ahmed', 'Ali', 'Youssef', 'Abdullah', 'Yasin', 'Hamza', 'Ayaan', 'Syed', 'Rishaan', 'Samar', 'Ahmad', 'Zikri', 'Rayyan', 'Mariam', 'Jana', 'Malak', 'Salma', 'Nour', 'Lian', 'Fatima', 'Ayesha', 'Zahra', 'Sana', 'Zara', 'Alya', 'Shaista', 'Zoya', 'Yasmin' ] }</span></span></code> </pre> <br>  Mit Pandas erstellen wir eine Tabelle mit Namen, deren vorherrschender ethnischer Herkunft und Tonalit√§tsbewertungen: <br><br><pre> <code class="plaintext hljs">def name_sentiment_table(): frames = [] for group, name_list in sorted(NAMES_BY_ETHNICITY.items()): lower_names = [name.lower() for name in name_list] sentiments = words_to_sentiment(lower_names) sentiments['group'] = group frames.append(sentiments) #           return pd.concat(frames) name_sentiments = name_sentiment_table()</code> </pre> <br>  Beispieldaten: <br><br> <code>name_sentiments.ix[::25]</code> <br> <table border="1" width="350"><thead><tr><th></th><th>  Tonalit√§t </th><th>  die Gruppe </th></tr></thead><tbody><tr><th>  Mohammed </th><td>  0,834974 </td><td>  Araber / Muslim </td></tr><tr><th>  alya </th><td>  3,916803 </td><td>  Araber / Muslim </td></tr><tr><th>  Terryl </th><td>  -2,858010 </td><td>  Schwarz </td></tr><tr><th>  jos√© </th><td>  0,432956 </td><td>  Hispanic </td></tr><tr><th>  Luciana </th><td>  1,086073 </td><td>  Hispanic </td></tr><tr><th>  hank </th><td>  0,391858 </td><td>  Wei√ü </td></tr><tr><th>  Megan </th><td>  2.158679 </td><td>  Wei√ü </td></tr></tbody></table><br>  Wir werden f√ºr jeden Namen ein Diagramm der Verteilung der Tonalit√§t erstellen. <br><br><pre> <code class="python hljs">plot = seaborn.swarmplot(x=<span class="hljs-string"><span class="hljs-string">'group'</span></span>, y=<span class="hljs-string"><span class="hljs-string">'sentiment'</span></span>, data=name_sentiments) plot.set_ylim([<span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>])</code> </pre> <br> <code>(-10, 10)</code> <br> <br><img src="https://habrastorage.org/webt/qv/y7/ge/qvy7gel8rrvm5txo-nou6g0i0re.png"><br><br>  Oder als Histogramm mit Konfidenzintervallen f√ºr durchschnittlich 95%. <br><br><pre> <code class="python hljs">plot = seaborn.barplot(x=<span class="hljs-string"><span class="hljs-string">'group'</span></span>, y=<span class="hljs-string"><span class="hljs-string">'sentiment'</span></span>, data=name_sentiments, capsize=<span class="hljs-number"><span class="hljs-number">.1</span></span>)</code> </pre> <br><img src="https://habrastorage.org/webt/uv/ib/n6/uvibn6olthaxt6cxbd96szehq94.png"><br><br>  F√ºhren Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">abschlie√üend das</a> Statistikpaket f√ºr <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">seri√∂se</a> Statistikmodelle aus.  Es wird gezeigt, wie gro√ü die Verzerrung des Algorithmus ist (zusammen mit einer Reihe anderer Statistiken). <br><br><br>  <font color="gray">OLS-Regressionsergebnisse</font> <br><table><tbody><tr><th>  Dep.  Variable: </th><td>  Stimmung </td><th>  R-Quadrat: </th><td>  0,208 </td></tr><tr><th>  Modell: </th><td>  OLS </td><th>  Adj.  R-Quadrat: </th><td>  0,192 </td></tr><tr><th>  Methode: </th><td>  Kleinste Quadrate </td><th>  F-Statistik: </th><td>  04/13 </td></tr><tr><th>  Datum: </th><td>  Do, 13. Juli 2017 </td><th>  Prob (F-Statistik): </th><td>  1.31e-07 </td></tr><tr><th>  Zeit: </th><td>  11:31:17 </td><th>  Log-Wahrscheinlichkeit: </th><td>  -356,78 </td></tr><tr><th>  Nein, nein.  Beobachtungen: </th><td>  153 </td><th>  AIC: </th><td>  721.6 </td></tr><tr><th>  Df R√ºckst√§nde: </th><td>  149 </td><th>  BIC: </th><td>  733.7 </td></tr><tr><th>  Df Modell: </th><td>  3 </td><th></th><td></td></tr><tr><th>  Kovarianztyp: </th><td>  nicht robust </td><th></th><td></td></tr></tbody></table><br>  Die F-Statistik ist das Verh√§ltnis der Variation zwischen Gruppen zur Variation innerhalb von Gruppen, das als allgemeine Bewertung der Verzerrung angesehen werden kann. <br><br>  Unmittelbar darunter liegt die Wahrscheinlichkeit, dass wir die maximale F-Statistik mit der Nullhypothese sehen, dh wenn kein Unterschied zwischen den verglichenen Optionen besteht.  Die Wahrscheinlichkeit ist sehr, sehr gering.  In einem wissenschaftlichen Artikel w√ºrden wir das Ergebnis als "sehr statistisch signifikant" bezeichnen. <br><br>  Wir m√ºssen den F-Wert verbessern.  Je niedriger desto besser. <br><br> <code>ols_model.fvalue <br> 13.041597745167659</code> <br> <br><h1>  Schritt 7. Andere Daten versuchen. </h1><br>  Wir haben jetzt die M√∂glichkeit, die sch√§dliche Verzerrung des Modells numerisch zu messen.  Versuchen wir es anzupassen.  Dazu m√ºssen Sie eine Reihe von Dingen wiederholen, die fr√ºher nur separate Schritte in einem Python-Notizblock waren. <br><br>  Wenn ich guten, unterst√ºtzten Code schreiben w√ºrde, w√ºrde ich keine globalen Variablen wie <code>model</code> und <code>embeddings</code> .  Mit dem aktuellen Spaghetti-Code k√∂nnen Sie jedoch jeden Schritt besser untersuchen und verstehen, was passiert.  Wir verwenden einen Teil des Codes wieder und definieren zumindest eine Funktion, um einige Schritte zu wiederholen: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">retrain_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(new_embs)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""      . """</span></span> <span class="hljs-keyword"><span class="hljs-keyword">global</span></span> model, embeddings, name_sentiments embeddings = new_embs pos_vectors = embeddings.loc[pos_words].dropna() neg_vectors = embeddings.loc[neg_words].dropna() vectors = pd.concat([pos_vectors, neg_vectors]) targets = np.array([<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> entry <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> pos_vectors.index] + [<span class="hljs-number"><span class="hljs-number">-1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> entry <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> neg_vectors.index]) labels = list(pos_vectors.index) + list(neg_vectors.index) train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \ train_test_split(vectors, targets, labels, test_size=<span class="hljs-number"><span class="hljs-number">0.1</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">0</span></span>) model = SGDClassifier(loss=<span class="hljs-string"><span class="hljs-string">'log'</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">0</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">100</span></span>) model.fit(train_vectors, train_targets) accuracy = accuracy_score(model.predict(test_vectors), test_targets) print(<span class="hljs-string"><span class="hljs-string">"Accuracy of sentiment: {:.2%}"</span></span>.format(accuracy)) name_sentiments = name_sentiment_table() ols_model = statsmodels.formula.api.ols(<span class="hljs-string"><span class="hljs-string">'sentiment ~ group'</span></span>, data=name_sentiments).fit() print(<span class="hljs-string"><span class="hljs-string">"F-value of bias: {:.3f}"</span></span>.format(ols_model.fvalue)) print(<span class="hljs-string"><span class="hljs-string">"Probability given null hypothesis: {:.3}"</span></span>.format(ols_model.f_pvalue)) <span class="hljs-comment"><span class="hljs-comment">#        Y plot = seaborn.swarmplot(x='group', y='sentiment', data=name_sentiments) plot.set_ylim([-10, 10])</span></span></code> </pre> <br><h3>  Wir versuchen word2vec </h3><br>  Es ist davon auszugehen, dass nur GloVe das Problem hat.  Es gibt wahrscheinlich viele zweifelhafte Websites in der Common Crawl-Datenbank und mindestens 20 Exemplare des Urban Dictionary of Street Slang.  Vielleicht ist es auf einer anderen Basis besser: Was ist mit dem guten alten word2vec, das in Google News trainiert wurde? <br><br>  Es scheint, dass die ma√ügeblichste Quelle f√ºr word2vec-Daten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diese Datei auf Google Drive ist</a> .  Laden Sie es herunter und speichern Sie es als <code>data/word2vec-googlenews-300.bin.gz</code> . <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   ConceptNet   word2vec   Pandas     from conceptnet5.vectors.formats import load_word2vec_bin w2v = load_word2vec_bin('data/word2vec-googlenews-300.bin.gz', nrows=2000000) #  word2vec    w2v.index = [label.casefold() for label in w2v.index] #  ,    w2v = w2v.reset_index().drop_duplicates(subset='index', keep='first').set_index('index') retrain_model(w2v)</span></span></code> </pre> <br> <code>Accuracy of sentiment: 94.30% <br> F-value of bias: 15.573 <br> Probability given null hypothesis: 7.43e-09</code> <br> <br>  Word2vec erwies sich mit einem F-Wert von mehr als 15 als noch schlechter. <br><br>  Im Prinzip war es dumm zu erwarten, dass <i>Nachrichten</i> besser vor Voreingenommenheit gesch√ºtzt werden. <br><br><h3>  ConceptNet Numberbatch versuchen </h3><br>  Schlie√ülich kann ich √ºber mein eigenes Projekt zur Vektordarstellung von W√∂rtern sprechen. <br><br>  ConceptNet mit der Vektorpr√§sentationsfunktion ist das Wissensdiagramm, an dem ich arbeite.  Es normalisiert Vektordarstellungen in der Trainingsphase und identifiziert und entfernt einige Quellen f√ºr algorithmischen Rassismus und Sexismus.  Diese Methode zur Korrektur von Verzerrungen basiert auf einem wissenschaftlichen Artikel von Bulukbashi et al., <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">"Debiasing Word Embeddings",</a> und wird verallgemeinert, um mehrere Arten von Verzerrungen gleichzeitig zu beseitigen.  Soweit ich wei√ü, ist dies das einzige semantische System, in dem es so etwas gibt. <br><br>  Von Zeit zu Zeit exportieren wir vorberechnete Vektoren aus ConceptNet - diese Releases hei√üen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ConceptNet Numberbatch</a> .  Im April 2017 wurde die erste Version mit Bias-Korrektur ver√∂ffentlicht, daher werden wir die englischsprachigen Vektoren laden und unser Modell neu trainieren. <br><br>  <code><a href="">numberbatch-en-17.04b.txt.gz</a></code> , speichern es im <code>data/</code> Verzeichnis und trainieren das Modell neu: <br><br><pre> <code class="python hljs">retrain_model(load_embeddings(<span class="hljs-string"><span class="hljs-string">'data/numberbatch-en-17.04b.txt'</span></span>))</code> </pre> <br> <code>Accuracy of sentiment: 97.46% <br> F-value of bias: 3.805 <br> Probability given null hypothesis: 0.0118</code> <br> <br><img src="https://habrastorage.org/webt/5d/iu/uq/5diuuqrst8bca5-m7fljox--pro.png"><br><br>  Hat ConceptNet Numberbatch das Problem vollst√§ndig behoben?  Kein algorithmischer Rassismus mehr?  <b>Nein.</b> <br><br>  Ist Rassismus viel weniger geworden?  <b>Auf jeden Fall</b> . <br><br>  Schl√ºsselbereiche f√ºr ethnische Gruppen √ºberlappen sich viel st√§rker als in GloVe- oder word2vec-Vektoren.  Im Vergleich zu GloVe nahm der Wert von F mehr als dreimal ab und im Vergleich zu word2vec mehr als viermal.  Und im Allgemeinen sehen wir beim Vergleich verschiedener Namen viel kleinere Unterschiede in der Tonalit√§t: Dies sollte so sein, da die Namen das Ergebnis der Analyse wirklich nicht beeinflussen sollten. <br><br>  Eine leichte Korrelation blieb jedoch bestehen.  Vielleicht kann ich solche Daten und Trainingsparameter aufnehmen, dass das Problem gel√∂st zu sein scheint.  Dies wird jedoch eine schlechte Option sein, da <i>das</i> Problem <i>tats√§chlich bestehen</i> bleibt, da wir in ConceptNet nicht alle Ursachen f√ºr algorithmischen Rassismus identifiziert und kompensiert haben.  Aber das ist ein guter Anfang. <br><br><h3>  Keine Fallstricke </h3><br>  Bitte beachten Sie, dass sich mit der Umstellung auf ConceptNet Numberbatch die Genauigkeit der Vorhersage der Tonalit√§t verbessert hat. <br><br>  Jemand k√∂nnte vorgeschlagen haben, dass die Korrektur von algorithmischem Rassismus die Ergebnisse auf andere Weise verschlechtern w√ºrde.  Aber nein.  M√∂glicherweise haben Sie Daten, die besser und weniger rassistisch sind.  Die Daten verbessern sich mit dieser Korrektur wirklich.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Der von Menschen erworbene Wort2vec- und GloVe-Rassismus hat nichts mit der Genauigkeit des Algorithmus zu tun. </font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Andere Ans√§tze </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dies ist nat√ºrlich nur eine M√∂glichkeit, die Tonalit√§t zu analysieren. Einige Details k√∂nnen unterschiedlich implementiert werden. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Stattdessen oder zus√§tzlich zum √Ñndern der Vektorbasis k√∂nnen Sie versuchen, dieses Problem direkt in der Ausgabe zu beheben. Beseitigen Sie beispielsweise im Allgemeinen die Bewertung der Tonalit√§t f√ºr Namen und Personengruppen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Im Allgemeinen besteht die M√∂glichkeit, die Berechnung der Tonalit√§t aller W√∂rter abzulehnen und sie nur f√ºr W√∂rter aus der Liste zu berechnen. Dies ist vielleicht die h√§ufigste Form der Stimmungsanalyse - ohne maschinelles Lernen. Die Ergebnisse sind nicht voreingenommener als der Autor der Liste. Das Ablehnen von maschinellem Lernen bedeutet jedoch, den R√ºckruf (R√ºckruf) zu reduzieren. Die einzige M√∂glichkeit, das Modell an einen Datensatz anzupassen, besteht darin, die Liste manuell zu bearbeiten.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Als hybrider Ansatz k√∂nnen Sie eine gro√üe Anzahl gesch√§tzter Tonalit√§tssch√§tzungen f√ºr W√∂rter erstellen und eine Person anweisen, diese geduldig zu bearbeiten und eine Liste von Ausnahmew√∂rtern mit einer Tonalit√§t von Null zu erstellen. </font><font style="vertical-align: inherit;">Aber das ist ein zus√§tzlicher Job. </font><font style="vertical-align: inherit;">Auf der anderen Seite werden Sie wirklich sehen, wie das Modell funktioniert. </font><font style="vertical-align: inherit;">Ich denke, dass dies auf jeden Fall gesucht werden sollte.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de436506/">https://habr.com/ru/post/de436506/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de436496/index.html">PVS-Studio f√ºr Java</a></li>
<li><a href="../de436498/index.html">Software AG: Nicht nur ARIS</a></li>
<li><a href="../de436500/index.html">Wie der Rahmen von Rise of the Tomb Raider gerendert wird</a></li>
<li><a href="../de436502/index.html">Abonnement verw√∂hnt oder wie man mehr an dieselben Kunden verkauft</a></li>
<li><a href="../de436504/index.html">System im Paket oder was ist unter Chip-Paketabdeckung?</a></li>
<li><a href="../de436508/index.html">Investitionen in H√∂he von 10 Millionen US-Dollar und Wozniaks Lob - Schaffung eines Bildungscomputers f√ºr Kinder</a></li>
<li><a href="../de436510/index.html">Kerndaten im Detail</a></li>
<li><a href="../de436512/index.html">Wie wir problematische Releases mit Graphite und Moira finden. Erleben Sie Yandex.Money</a></li>
<li><a href="../de436514/index.html">Erstellen von Geschichten f√ºr Instagram aus PHP</a></li>
<li><a href="../de436518/index.html">Haiku Œ≤1 - mach / b / OS wieder gro√üartig</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>