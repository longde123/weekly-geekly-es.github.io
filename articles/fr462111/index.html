<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧒 🔙 😉 Comment nous avons testé plusieurs bases de données de séries chronologiques 👵🏿 🧀 🕎</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Au cours des dernières années, les bases de données chronologiques sont passées d'une chose curieuse (hautement spécialisée dans les systèmes de surve...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment nous avons testé plusieurs bases de données de séries chronologiques</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/itsumma/blog/462111/"><img src="https://habrastorage.org/webt/u_/ty/r0/u_tyr0djrlkmaz-swo9flhddymo.jpeg"><br><br>  Au cours des dernières années, les bases de données chronologiques sont passées d'une chose curieuse (hautement spécialisée dans les systèmes de surveillance ouverts (et liés à des solutions spécifiques) ou aux projets Big Data) à un «bien de consommation».  Sur le territoire de la Fédération de Russie, des remerciements particuliers doivent être adressés à Yandex et ClickHouse pour cela.  Jusqu'à présent, si vous aviez besoin d'enregistrer une grande quantité de données de séries chronologiques, vous deviez soit accepter la nécessité de soulever une pile Hadoop monstrueuse et l'accompagner, soit communiquer avec des protocoles spécifiques à chaque système. <br><br>  Il peut sembler qu'en 2019, un article sur lequel le TSDB devrait être utilisé se composera d'une seule phrase: «utilisez simplement ClickHouse».  Mais ... il y a des nuances. <br><br>  En effet, ClickHouse se développe activement, la base d'utilisateurs augmente et le soutien est très actif, mais sommes-nous devenus les otages du succès public de ClickHouse, qui a éclipsé d'autres solutions, peut-être plus efficaces / fiables? <br><br>  Au début de l'année dernière, nous avons commencé à traiter notre propre système de surveillance, au cours duquel la question s'est posée de choisir la base de données appropriée pour le stockage des données.  Je veux raconter ici l'histoire de ce choix. <br><a name="habracut"></a><br><h4>  Énoncé du problème </h4><br>  Tout d'abord, la préface nécessaire.  Pourquoi avons-nous besoin de notre propre système de surveillance et comment a-t-il été organisé? <br><br>  Nous avons commencé à fournir des services de support en 2008, et en 2010, il est devenu clair qu'il était difficile d'agréger les données sur les processus se produisant dans l'infrastructure client avec les solutions qui existaient à l'époque (nous parlons, Dieu me pardonne, Cacti, Zabbix et le nouveau-né Graphite). <br><br>  Nos principales exigences étaient: <br><br><ul><li>  prise en charge (à l'époque - des dizaines, et à l'avenir - des centaines) de clients au sein d'un même système et en même temps la présence d'un système centralisé de gestion des alertes; </li><li>  flexibilité dans la gestion du système d'alerte (escalade d'alertes entre préposés, comptabilité horaire, base de connaissances); </li><li>  la possibilité de détailler en profondeur les graphiques (Zabbix à l'époque dessinait des graphiques sous forme d'images); </li><li>  stockage à long terme d'une grande quantité de données (un an ou plus) et possibilité de les sélectionner rapidement. </li></ul><br>  Dans cet article, nous nous intéressons au dernier point. <br><br>  En ce qui concerne le stockage, les exigences étaient les suivantes: <br><br><ul><li>  le système devrait fonctionner rapidement; </li><li>  il est souhaitable que le système ait une interface SQL; </li><li> le système doit être stable et avoir une base d'utilisateurs et un support actifs (une fois que nous avons été confrontés à la nécessité de prendre en charge des systèmes tels que, par exemple, MemcacheDB, que nous avons cessé de développer, ou le stockage distribué MooseFS, dont le bugtracker a été effectué en chinois: répéter cette histoire pour notre projet ne voulait pas); </li><li>  Correspondance avec le théorème de la PAC: cohérence (nécessaire) - les données doivent être pertinentes, nous ne voulons pas que le système de gestion des notifications ne reçoive pas de nouvelles données et ne crache pas d'alertes sur la non-arrivée de données pour tous les projets;  Tolérance de partition (nécessaire) - nous ne voulons pas obtenir de systèmes Split Brain;  Disponibilité (non critique, dans le cas d'une réplique active) - nous pouvons passer nous-mêmes au système de sauvegarde en cas d'accident, avec un code. </li></ul><br>  Curieusement, à cette époque, MySQL était la solution parfaite pour nous.  Notre structure de données était extrêmement simple: identifiant du serveur, identifiant du compteur, horodatage et valeur;  l'échantillonnage rapide des données chaudes a été fourni par un grand pool de tampons, et l'échantillonnage des données historiques a été fourni par SSD. <br><br><img src="https://habrastorage.org/webt/ii/cd/es/iicdesd_tmiqwygfha8r4bepjgg.png"><br><br>  Ainsi, nous avons réalisé un échantillonnage de nouvelles données de deux semaines, avec des détails jusqu'à 200 secondes avant que les données soient complètement restituées, et avons vécu dans ce système pendant un certain temps. <br><br>  Pendant ce temps, le temps a passé et la quantité de données a augmenté.  En 2016, les volumes de données ont atteint des dizaines de téraoctets, ce qui en termes de stockage SSD loué représentait une dépense importante. <br><br>  À ce stade, les bases de données en colonnes se propageaient activement, ce à quoi nous avons commencé à réfléchir activement: dans les bases de données en colonnes, les données sont stockées, comme vous pouvez le comprendre, dans des colonnes, et si vous regardez nos données, il est facile de voir un grand nombre de prises qui pourraient être Si vous utilisez une base de données de colonnes, compressez avec compression. <br><br><img src="https://habrastorage.org/webt/zm/gu/x3/zmgux307lo7r3i7s9uykpgozadm.png"><br><br>  Cependant, le système clé pour le travail de l'entreprise a continué de fonctionner de manière stable, et je ne voulais pas expérimenter la transition vers autre chose. <br><br>  En 2017, lors de la conférence Percona Live à San Jose, probablement la première fois que les développeurs de Clickhouse s'annonçaient.  À première vue, le système était prêt pour la production (enfin, Yandex.Metrica est une production sévère), le support était rapide et simple et, surtout, l'opération était simple.  Depuis 2018, nous avons entamé le processus de transition.  Mais à ce moment-là, il y avait beaucoup de systèmes TSDB «adultes» et testés dans le temps, et nous avons décidé d'allouer beaucoup de temps et de comparer les alternatives afin de nous assurer qu'il n'y avait pas de solutions Clickhouse alternatives, selon nos besoins. <br><br>  En plus des besoins de stockage déjà indiqués, de nouveaux sont apparus: <br><br><ul><li>  le nouveau système devrait fournir au moins les mêmes performances que MySQL, sur la même quantité de fer; </li><li>  le stockage du nouveau système devrait occuper beaucoup moins d'espace; </li><li>  Le SGBD devrait toujours être facile à gérer; </li><li>  Je voulais minimiser l'application lors du changement du SGBD. </li></ul><br><h4>  Quels systèmes nous avons commencé à considérer </h4><br>  <b><u>Apache Hive / Apache Impala</u></b> <br>  Ancienne pile Hadoop battue.  En fait, il s'agit d'une interface SQL construite en plus de stocker des données dans des formats natifs sur HDFS. <br><br>  Pour. <br><br><ul><li>  Avec un fonctionnement stable, il est très facile de mettre à l'échelle les données. </li><li>  Il existe des solutions de colonnes pour le stockage des données (moins d'espace). </li><li>  Exécution très rapide de tâches parallèles en présence de ressources. </li></ul><br>  Inconvénients <br><br><ul><li>  Il s'agit d'un Hadoop et il est difficile à utiliser.  Si nous ne sommes pas prêts à prendre une solution prête à l'emploi dans le cloud (et nous ne sommes pas prêts pour le coût), toute la pile devra être assemblée et prise en charge par les administrateurs, mais je ne le veux vraiment pas. </li><li>  Les données sont agrégées <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">très rapidement</a> . </li></ul><br>  Cependant: <br><br><img src="https://habrastorage.org/webt/zi/nv/qx/zinvqxvmxc43df-hd4xjqwprtmi.png"><br><br>  La vitesse est atteinte en faisant évoluer le nombre de serveurs informatiques.  En termes simples, si nous sommes une grande entreprise engagée dans l'analyse et les affaires, il est extrêmement important d'agréger les informations le plus rapidement possible (même au prix d'utiliser un grand nombre de ressources informatiques) - cela peut être notre choix.  Mais nous n'étions pas prêts à multiplier le parc de fer pour accélérer les tâches. <br><br>  <b><u>Druide / pinot</u></b> <br><br>  Déjà beaucoup plus sur TSDB en particulier, mais encore une fois - Hadoop-stack. <br><br>  Il y a un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">excellent article comparant les avantages et les inconvénients de Druid et Pinot par rapport à ClickHouse</a> . <br><br>  En quelques mots: Druid / Pinot sont plus beaux que Clickhouse dans les cas où: <br><br><ul><li>  Vous avez une nature hétérogène des données (dans notre cas, nous enregistrons uniquement la série temporelle des métriques du serveur, et, en fait, il s'agit d'un seul tableau. Mais il peut y avoir d'autres cas: séries chronologiques d'équipement, séries chronologiques économiques, etc. - chacune ayant sa propre structure, qui doivent être agrégées et traitées). </li><li>  De plus, il y a beaucoup de ces données. </li><li>  Des tableaux et des données avec des séries chronologiques apparaissent et disparaissent (c'est-à-dire qu'une sorte d'ensemble de données est entré, il a été analysé et supprimé). </li><li>  Il n'y a pas de critère clair selon lequel les données peuvent être partitionnées. </li></ul><br>  Dans les cas opposés, ClickHouse se montre mieux, et c'est notre cas. <br><br>  <b><u>Clickhouse</u></b> <br><br><ul><li>  Similaire à SQL. </li><li>  Facile à gérer. </li><li>  Les gens disent que ça marche. </li></ul><br>  Il fait partie de la liste restreinte des tests. <br><br>  <b><u>Influxdb</u></b> <br><br>  Alternative étrangère à ClickHouse.  Parmi les inconvénients: la haute disponibilité n'est présente que dans la version commerciale, mais elle doit être comparée. <br><br>  Il fait partie de la liste restreinte des tests. <br><br>  <b><u>Cassandra</u></b> <br><br>  D'une part, nous savons qu'il est utilisé pour stocker des séries temporelles métriques par des systèmes de surveillance tels que, par exemple, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SignalFX</a> ou OkMeter.  Cependant, il y a des détails. <br><br>  Cassandra n'est pas une base de données de colonnes au sens habituel.  Il ressemble plus à une minuscule, mais chaque ligne peut avoir un nombre différent de colonnes, ce qui permet d'organiser facilement une représentation de colonne.  En ce sens, il est clair qu'avec une limite de 2 milliards de colonnes, vous pouvez stocker certaines données dans les colonnes (oui, la même série temporelle).  Par exemple, dans MySQL, il y a une limite de 4096 colonnes et il est facile de tomber sur une erreur avec le code 1117 si vous essayez de faire de même. <br><br>  Le moteur Cassandra se concentre sur le stockage de grandes quantités de données dans un système distribué sans assistant, et dans le théorème CAP ci-dessus, Cassandra se concentre davantage sur AP, c'est-à-dire sur l'accessibilité des données et la résistance au partitionnement.  Ainsi, cet outil peut être formidable si vous n'avez qu'à écrire dans cette base de données et à en lire rarement.  Et ici, il est logique d’utiliser Cassandra comme stockage «froid».  C'est-à-dire, comme un endroit fiable à long terme pour stocker de grandes quantités de données historiques qui sont rarement nécessaires, mais peuvent être obtenues si nécessaire.  Néanmoins, par souci d'exhaustivité, nous allons le tester.  Mais, comme je l'ai dit plus tôt, nous ne souhaitons pas réécrire activement le code de la solution DB sélectionnée, nous allons donc le tester quelque peu limité - sans adapter la structure de la base de données aux spécificités de Cassandra. <br><br>  <b><u>Prométhée</u></b> <br><br>  Eh bien, et par intérêt, nous avons décidé de tester les performances de la boutique Prometheus - juste pour comprendre si nous sommes plus rapides que les solutions actuelles ou plus lentes et combien. <br><br><h4>  Méthodologie et résultats des tests </h4><br>  Nous avons donc testé 5 bases de données dans les 6 configurations suivantes: ClickHouse (1 nœud), ClickHouse (table distribuée de 3 nœuds), InfluxDB, Mysql 8, Cassandra (3 nœuds) et Prometheus.  Le plan de test est le suivant: <br><br><ol><li>  remplir les données historiques de la semaine (840 millions de valeurs par jour; 208 000 métriques); </li><li>  générer une charge d'enregistrement (6 modes de charge ont été considérés, voir ci-dessous); </li><li>  parallèlement à l'enregistrement, nous réalisons périodiquement des échantillons, en émulant les demandes d'un utilisateur travaillant avec des graphiques.  Afin de ne pas trop compliquer les choses, nous avons sélectionné les données par 10 mesures (tout autant sur le graphique du processeur) par semaine. </li></ol><br>  Nous chargeons en émulant le comportement de notre agent de surveillance, qui envoie des valeurs à chaque métrique toutes les 15 secondes.  Dans ce cas, nous souhaitons varier: <br><br><ul><li>  nombre total de mesures dans lesquelles les données sont écrites; </li><li>  intervalle d'envoi de valeurs dans une métrique; </li><li>  taille du lot. </li></ul><br>  À propos de la taille du lot.  Comme il n'est pas recommandé de charger presque toutes nos bases expérimentales avec des insertions uniques, nous aurons besoin d'un relais, qui collecte les métriques entrantes et les regroupe autant que possible et les écrit sur la base avec une insertion de paquet. <br><br>  De plus, afin de mieux comprendre comment interpréter les données reçues plus tard, imaginez que nous n'envoyons pas seulement un tas de métriques, mais les métriques sont organisées en serveurs - 125 métriques par serveur.  Ici, le serveur n'est qu'une entité virtuelle - juste pour comprendre que, par exemple, 10 000 mesures correspondent à environ 80 serveurs. <br><br>  Et donc, en tenant compte de tout cela, nos 6 modes de charge d'enregistrement de la base: <br><br><img src="https://habrastorage.org/webt/lr/li/oo/lrlioosoevoybfuw4--rsftdugi.jpeg"><br><br>  Il y a deux points.  Premièrement, pour cassandra, ces tailles de lots se sont avérées trop importantes, nous avons utilisé des valeurs de 50 ou 100. Et deuxièmement, puisque le prometeus fonctionne strictement en mode pull, c'est-à-dire  il marche et collecte des données à partir de sources métriques (et même pushgateway, malgré son nom, ne change pas fondamentalement la situation), les charges correspondantes ont été implémentées à l'aide d'une combinaison de configurations statiques. <br><br>  Les résultats des tests sont les suivants: <br><br><img src="https://habrastorage.org/webt/0r/sz/vc/0rszvcd_aeoiqwlrixsgw0tauti.jpeg"><br><br><img src="https://habrastorage.org/webt/3r/gy/2g/3rgy2guskcodctly7nevknbygss.jpeg"><br><br><img src="https://habrastorage.org/webt/v8/mg/tm/v8mgtm0ytjkjkneal1mjcvw5n8w.jpeg"><br><br>  <b>Ce qui mérite d'être noté</b> : des échantillons incroyablement rapides de Prometheus, des échantillons terriblement lents de Cassandra, des échantillons inacceptablement lents d'InfluxDB;  ClickHouse a gagné en termes de vitesse d'enregistrement, et Prometheus ne participe pas au concours, car il s'insère en lui-même et nous ne mesurons rien. <br><br>  <u><b>En conséquence</b></u> : ClickHouse et InfluxDB se sont montrés les meilleurs, mais un cluster d'Influx ne peut être construit que sur la base de la version Enterprise, qui coûte de l'argent, et ClickHouse ne coûte rien et est fabriqué en Russie.  Aux États-Unis, il est logique que le choix soit probablement en faveur de inInfluxDB, et dans notre cas, il est en faveur de ClickHouse. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr462111/">https://habr.com/ru/post/fr462111/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr462095/index.html">Travailler avec la lumière et l'optique: comment démarrer une carrière à l'université - l'expérience des diplômés de quatre programmes de maîtrise spécialisés</a></li>
<li><a href="../fr462097/index.html">Tour vert clair</a></li>
<li><a href="../fr462101/index.html">Rapport du Java Virtual Machine Language Summit 2019</a></li>
<li><a href="../fr462107/index.html">Marathon gratuit «Big Data et super-héros: votre première expérience d'analyse de données»</a></li>
<li><a href="../fr462109/index.html">Voir presque invisible, également en couleur: une technique pour visualiser des objets à travers un diffuseur</a></li>
<li><a href="../fr462113/index.html">Environnement irréprochable: personne ne devrait écrire de code de qualité</a></li>
<li><a href="../fr462115/index.html">Ajustez le ciel étoilé sur WebGL en 1009 octets de JavaScript</a></li>
<li><a href="../fr462117/index.html">Comment maximiser la valeur du nettoyage du backlog de produit?</a></li>
<li><a href="../fr462119/index.html">Delta Smart City Solutions: Vous êtes-vous déjà demandé à quel point une salle de cinéma pouvait être verte?</a></li>
<li><a href="../fr462121/index.html">Swift fonctionnel</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>