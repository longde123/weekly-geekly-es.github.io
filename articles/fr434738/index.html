<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🥇 👨🏻‍✈️ 👧🏼 Apprentissage par renforcement en Python ✊🏾 🏝️ 💆🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Salut les collègues! 



 Dans la dernière publication de l'année sortante, nous voulions mentionner l'apprentissage par renforcement - un sujet dans ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Apprentissage par renforcement en Python</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/434738/">  Salut les collègues! <br><br><img src="https://habrastorage.org/webt/8s/-m/om/8s-mommciij8mkqdkjm62glthy4.jpeg"><br><br>  Dans la dernière publication de l'année sortante, nous voulions mentionner l'apprentissage par renforcement - un sujet dans lequel nous traduisons déjà un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">livre</a> . <br><br>  Jugez par vous-même: il y avait un article élémentaire avec Medium, qui décrivait le contexte du problème, décrivait l'algorithme le plus simple avec une implémentation en Python.  L'article a plusieurs gifs.  Et la motivation, la récompense et le choix de la bonne stratégie sur la voie du succès sont des choses qui seront extrêmement utiles à chacun de nous au cours de l'année à venir. <br><br>  Bonne lecture! <br><a name="habracut"></a><br>  L'apprentissage renforcé est une forme d'apprentissage automatique dans laquelle l'agent apprend à agir dans l'environnement, à effectuer des actions et à développer ainsi l'intuition, après quoi il observe les résultats de ses actions.  Dans cet article, je vais vous expliquer comment comprendre et formuler le problème de l'apprentissage par renforcement, puis le résoudre en Python. <br><br><br>  Récemment, nous nous sommes habitués au fait que les ordinateurs jouent à des jeux contre les humains - soit comme des bots dans des jeux multijoueurs, soit comme des rivaux dans des jeux en un contre un: disons, dans Dota2, PUB-G, Mario.  La société de recherche <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Deepmind a</a> fait toute une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">histoire à</a> propos de cette nouvelle lorsque son programme AlphaGo 2016 a battu le champion sud-coréen en 2016.  Si vous êtes un joueur passionné, vous pourriez entendre parler des cinq matchs de Dota 2 OpenAI Five, où des voitures ont combattu contre des gens et battu les meilleurs joueurs de Dota2 en plusieurs matchs.  (Si vous êtes intéressé par les détails, l'algorithme est analysé en détail <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> et il est examiné comment les machines ont joué). <br><br><img src="https://habrastorage.org/webt/da/l7/3j/dal73jd7dacspz0co83f6zuano0.png"><br><br>  La dernière version d'OpenAI Five <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">prend Roshan</a> . <br><br>  Commençons donc par la question centrale.  Pourquoi avons-nous besoin d'une formation renforcée?  Est-il utilisé uniquement dans les jeux ou est-il applicable dans des scénarios réalistes pour résoudre des problèmes appliqués?  Si c'est la première fois que vous lisez une formation de renforcement, vous ne pouvez tout simplement pas imaginer la réponse à ces questions.  En effet, l'apprentissage renforcé est l'une des technologies les plus utilisées et en développement rapide dans le domaine de l'intelligence artificielle. <br>  Voici un certain nombre de domaines dans lesquels les systèmes d'apprentissage par renforcement sont particulièrement demandés: <br><br><ol><li>  Véhicules sans pilote </li><li>  Industrie du jeu </li><li>  Robotique </li><li>  Systèmes de recommandation </li><li>  Publicité et marketing </li></ol><br>  <b>Aperçu et contexte de l'apprentissage par renforcement</b> <br><br>  Alors, comment le phénomène d'apprentissage avec renforcement a-t-il pris forme alors que nous avons tant de machines et de méthodes d'apprentissage en profondeur à notre disposition?  "Il a été inventé par Rich Sutton et Andrew Barto, directeur de recherche de Rich, qui l'ont aidé à préparer son doctorat."  Le paradigme a d'abord pris forme dans les années 80, puis était archaïque.  Par la suite, Rich a cru qu'elle avait un grand avenir et qu'elle finirait par être reconnue. <br><br>  L'apprentissage renforcé prend en charge l'automatisation dans l'environnement où il est déployé.  La machine et l'apprentissage en profondeur fonctionnent à peu près de la même manière - ils sont stratégiquement disposés différemment, mais les deux paradigmes prennent en charge l'automatisation.  Alors, pourquoi la formation de renforcement est-elle apparue? <br><br>  Cela rappelle beaucoup le processus d'apprentissage naturel, dans lequel le processus / modèle agit et reçoit des commentaires sur la façon dont elle parvient à faire face à la tâche: bonne et non. <br><br>  La machine et l'apprentissage en profondeur sont également des options de formation, mais ils sont plus adaptés pour identifier les modèles dans les données disponibles.  En revanche, dans l'apprentissage par renforcement, cette expérience s'acquiert par essais et erreurs;  le système trouve progressivement les bonnes options ou l'optimum global.  Un avantage supplémentaire sérieux de l'apprentissage renforcé est que dans ce cas, il n'est pas nécessaire de fournir un ensemble complet de données de formation, comme dans l'enseignement avec un enseignant.  Quelques petits fragments suffiront. <br><br>  <b>Le concept d'apprentissage par renforcement</b> <br><br>  Imaginez apprendre à vos chats de nouveaux trucs;  mais, malheureusement, les chats ne comprennent pas le langage humain, vous ne pouvez donc pas prendre et leur dire ce que vous allez jouer avec eux.  Par conséquent, vous agirez différemment: imitez la situation et le chat essaiera de répondre d'une manière ou d'une autre en réponse.  Si le chat a réagi comme vous le vouliez, vous y versez du lait.  Comprenez-vous ce qui va se passer ensuite?  Encore une fois, dans une situation similaire, le chat effectuera à nouveau l'action souhaitée, et avec encore plus d'enthousiasme, en espérant qu'il sera encore mieux nourri.  C'est ainsi que l'apprentissage se déroule sur un exemple positif;  mais, si vous essayez «d'éduquer» un chat avec des incitations négatives, par exemple, regardez-le strictement et froncez les sourcils, il n'apprend généralement pas dans de telles situations. <br><br>  L'apprentissage renforcé fonctionne de manière similaire.  Nous indiquons à la machine certaines entrées et actions, puis récompensons la machine en fonction de la sortie.  Notre objectif ultime est de maximiser les récompenses.  Voyons maintenant comment reformuler le problème ci-dessus en termes d'apprentissage par renforcement. <br><br><ul><li>  Le chat agit comme un «agent» exposé à «l'environnement». </li><li>  L'environnement est une maison ou une aire de jeux, selon ce que vous enseignez au chat. </li><li>  Les situations résultant de la formation sont appelées «états».  Dans le cas d'un chat, des exemples de conditions sont lorsque le chat «court» ou «rampe sous le lit». </li><li>  Les agents réagissent en effectuant des actions et en passant d'un «état» à un autre. </li><li>  Après le changement d'état, l'agent reçoit une «récompense» ou une «amende» selon l'action qu'il a entreprise. </li><li>  La «stratégie» est une méthodologie pour choisir une action pour obtenir les meilleurs résultats. </li></ul><br>  Maintenant que nous avons compris ce qu'est l'apprentissage par renforcement, parlons en détail des origines et de l'évolution de l'apprentissage par renforcement et de l'apprentissage par renforcement profond, discutons de la façon dont ce paradigme nous permet de résoudre des problèmes impossibles à apprendre avec ou sans enseignant, et notons également ce qui suit fait curieux: à l'heure actuelle, le moteur de recherche de Google est optimisé à l'aide d'algorithmes d'apprentissage par renforcement. <br><br>  <b>Comprendre la terminologie de l'apprentissage par renforcement</b> <br><br>  L'agent et l'environnement jouent des rôles clés dans l'algorithme d'apprentissage par renforcement.  L'environnement est le monde dans lequel l'agent doit survivre.  De plus, l'agent reçoit des signaux de renforcement de l'environnement (récompense): il s'agit d'un nombre qui caractérise à quel point l'état actuel du monde peut être bon ou mauvais.  Le but de l'agent est de maximiser la récompense totale, ce que l'on appelle le «gain».  Avant d'écrire nos premiers algorithmes d'apprentissage par renforcement, vous devez comprendre la terminologie suivante. <br><br><img src="https://habrastorage.org/webt/6j/vx/cp/6jvxcpcpr52v252wa9eze1mehx4.gif"><br><br><ol><li>  <b>États</b> : Un État est une description complète d'un monde dans lequel aucun fragment des informations caractérisant ce monde ne manque.  Il peut s'agir d'une position, fixe ou dynamique.  En règle générale, ces états sont écrits sous la forme de tableaux, de matrices ou de tenseurs d'ordre supérieur. </li><li>  <b>Action</b> : l'action dépend généralement des conditions environnementales et, dans différents environnements, l'agent entreprendra différentes actions.  De nombreuses actions d'agent valides sont enregistrées dans un espace appelé «espace d'action».  En règle générale, le nombre d'actions dans l'espace est fini. </li><li>  <b>Environnement</b> : Il s'agit de l'endroit où l'agent existe et avec lequel il interagit.  Différents types de récompenses, stratégies, etc. sont utilisés pour différents environnements. </li><li>  <b>Récompenses</b> et <b>gains</b> : vous devez surveiller en permanence la fonction de récompense R lorsque vous vous entraînez avec des renforts.  Il est essentiel lors de la configuration d'un algorithme, de son optimisation et également lorsque vous arrêtez d'apprendre.  Cela dépend de l'état actuel du monde, des mesures qui viennent d'être prises et du prochain état du monde. </li><li>  <b>Stratégies</b> : une stratégie est une règle selon laquelle un agent choisit l'action suivante.  L'ensemble des stratégies est également appelé «cerveau» de l'agent. </li></ol><br><img src="https://habrastorage.org/webt/ur/lb/u-/urlbu-ifbred1iqfkhvqkv7bqds.png"><br><br>  Maintenant que nous nous sommes familiarisés avec la terminologie de l'apprentissage par renforcement, résolvons le problème en utilisant les algorithmes appropriés.  Avant cela, vous devez comprendre comment formuler un tel problème et, pour résoudre ce problème, vous fier à la terminologie de la formation avec renforcement. <br><br>  <b>Solution de taxi</b> <br><br>  Nous passons donc à la résolution du problème avec l'utilisation d'algorithmes de renforcement. <br>  Supposons que nous ayons une zone d'entraînement pour un taxi sans pilote, que nous formons pour livrer les passagers au parking à quatre points différents ( <code>R,G,Y,B</code> ).  Avant cela, vous devez comprendre et définir l'environnement dans lequel nous commençons la programmation en Python.  Si vous commencez tout juste à apprendre Python, je vous recommande <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cet article</a> . <br><br>  L'environnement pour résoudre un problème de taxi peut être configuré à l'aide d'OpenAI's <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Gym</a> - c'est l'une des bibliothèques les plus populaires pour résoudre les problèmes avec la formation de renforcement.  Eh bien, avant d'utiliser gym, vous devez l'installer sur votre machine, et un gestionnaire de packages Python appelé pip est pratique pour cela.  Voici la commande d'installation. <br><br> <code>pip install gym</code> <br> <br>  Voyons ensuite comment notre environnement sera affiché.  Tous les modèles et l'interface pour cette tâche sont déjà configurés dans le gymnase et nommés sous <code>Taxi-V2</code> .  L'extrait de code ci-dessous est utilisé pour afficher cet environnement. <br><br>  «Nous avons 4 emplacements (indiqués par des lettres différentes);  notre tâche consiste à prendre un passager à un moment donné et à le déposer à un autre.  Nous obtenons +20 points pour un atterrissage réussi et perdons 1 point pour chaque étape dépensée.  Il y a également une pénalité de 10 points pour chaque embarquement et débarquement involontaire d'un passager. »  (Source: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">gym.openai.com/envs/Taxi-v2</a> ) <br><br>  Voici la sortie que nous verrons dans notre console: <br><br><img src="https://habrastorage.org/webt/n8/xj/lw/n8xjlwkbla6unwn2k6tworczc8o.png"><br><br>  Taxi V2 ENV <br><br>  Génial, <code>env</code> est le cœur d'OpenAi Gym, c'est une interface d'environnement unifiée.  Voici les méthodes env que nous trouvons utiles: <br><br>  <code>env.reset</code> : réinitialise l'environnement et renvoie un état initial aléatoire. <br>  <code>env.step(action)</code> : fait progresser <code>env.step(action)</code> développement de l'environnement une étape dans le temps. <br>  <code>env.step(action)</code> : retourne les variables suivantes <br><br><ul><li>  <code>observation</code> : Observation de l'environnement. </li><li>  <code>reward</code> : <code>reward</code> si votre action a été bénéfique. </li><li>  <code>done</code> : indique si nous avons réussi à ramasser et à déposer correctement le passager, également appelé «un épisode». </li><li>  <code>info</code> : informations supplémentaires telles que les performances et la latence nécessaires à des fins de débogage. </li><li>  <code>env.render</code> : affiche une image de l'environnement (utile pour le rendu) </li></ul><br>  Donc, après avoir examiné l'environnement, essayons de mieux comprendre le problème.  Les taxis sont la seule voiture sur ce parking.  Le stationnement peut être divisé en une grille <code>5x5</code> , où nous obtenons 25 emplacements de taxi possibles.  Ces 25 valeurs sont l'un des éléments de notre espace d'état.  Remarque: pour le moment, notre taxi est situé au point avec les coordonnées (3, 1). <br><br>  Il y a 4 points dans l'environnement où les passagers sont autorisés à monter à bord: ce sont les coordonnées <code>R, G, Y, B</code> ou <code>[(0,0), (0,4), (4,0), (4,3)]</code> en coordonnées ( horizontalement; verticalement), s'il était possible d'interpréter l'environnement ci-dessus en coordonnées cartésiennes.  Si vous prenez également en compte un (1) autre état du passager: à l'intérieur du taxi, vous pouvez prendre toutes les combinaisons d'emplacements de passagers et de leurs destinations afin de calculer le nombre total d'états dans notre environnement pour la formation en taxi: nous avons quatre (4) destinations et cinq (4+) 1) emplacements des passagers. <br><br>  Donc, dans notre environnement pour un taxi, il y a 5 × 5 × 5 × 4 = 500 états possibles.  Un agent traite l'une des 500 conditions et prend des mesures.  Dans notre cas, les options sont les suivantes: se déplacer dans un sens ou dans l'autre, ou la décision de prendre / déposer le passager.  En d'autres termes, nous avons à notre disposition six actions possibles: <br>  ramasser, déposer, nord, est, sud, ouest (les quatre dernières valeurs sont les directions dans lesquelles un taxi peut se déplacer.) <br><br>  Il s'agit de l' <code>action space</code> : l'ensemble de toutes les actions que notre agent peut entreprendre dans un état donné. <br><br>  Comme le montre l'illustration ci-dessus, un taxi ne peut pas effectuer certaines actions dans certaines situations (les murs interfèrent).  Dans le code décrivant l'environnement, nous attribuons simplement une pénalité de -1 pour chaque coup dans le mur, et un taxi entrant en collision avec le mur.  Ainsi, de telles amendes s'accumuleront, de sorte que le taxi essaiera de ne pas heurter les murs. <br><br>  Tableau des récompenses: lors de la création d'un environnement de taxi, il est également possible de créer un tableau de récompenses principal appelé P. Vous pouvez le considérer comme une matrice, où le nombre d'états correspond au nombre de lignes et le nombre d'actions au nombre de colonnes.  Autrement dit, nous parlons de la matrice <code>states × actions</code> . <br><br>  Étant donné que toutes les conditions sont enregistrées dans cette matrice, vous pouvez afficher les valeurs de récompense par défaut affectées à l'état que nous avons choisi d'illustrer: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym &gt;&gt;&gt; env = gym.make(<span class="hljs-string"><span class="hljs-string">"Taxi-v2"</span></span>).env &gt;&gt;&gt; env.P[<span class="hljs-number"><span class="hljs-number">328</span></span>] {<span class="hljs-number"><span class="hljs-number">0</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">433</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">1</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">233</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">2</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">353</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">3</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">4</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">5</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)] }</code> </pre> <br>  La structure de ce dictionnaire est la suivante: <code>{action: [(probability, nextstate, reward, done)]}</code> . <br><br><ul><li>  Les valeurs 0 à 5 correspondent aux actions (sud, nord, est, ouest, prise en charge, dépose) qu'un taxi peut effectuer dans l'état actuel indiqué sur l'illustration. </li><li>  done vous permet de juger du moment où nous avons réussi à déposer le passager au point souhaité. </li></ul><br>  Pour résoudre ce problème sans aucune formation avec renforcement, vous pouvez définir l'état cible, faire une sélection d'espaces, puis, si vous pouvez atteindre l'état cible pendant un certain nombre d'itérations, supposer que ce moment correspond à la récompense maximale.  Dans d'autres États, la valeur de la récompense s'approche du maximum si le programme agit correctement (s'approche de l'objectif) ou accumule des amendes en cas d'erreur.  De plus, la valeur de l'amende peut atteindre au moins -10. <br><br>  Écrivons du code pour résoudre ce problème sans formation de renforcement. <br>  Étant donné que nous avons une table P avec des valeurs de récompense par défaut pour chaque état, nous pouvons essayer d'organiser la navigation de notre taxi en fonction de cette table. <br><br>  Nous créons une boucle sans fin, défilant jusqu'à ce que le passager atteigne la destination (un épisode), ou, en d'autres termes, jusqu'à ce que le taux de récompense atteigne 20. La méthode <code>env.action_space.sample()</code> sélectionne automatiquement une action aléatoire dans l'ensemble de toutes les actions disponibles .  Considérez ce qui se passe: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sleep <span class="hljs-comment"><span class="hljs-comment">#  thr env env = gym.make("Taxi-v2").env env.s = 328 #     ,   , epochs = 0 penalties, reward = 0, 0 frames = [] done = False while not done: action = env.action_space.sample() state, reward, done, info = env.step(action) if reward == -10: penalties += 1 #         frames.append({ 'frame': env.render(mode='ansi'), 'state': state, 'action': action, 'reward': reward } ) epochs += 1 print("Timesteps taken: {}".format(epochs)) print("Penalties incurred: {}".format(penalties)) #    , ,  def frames(frames): for i, frame in enumerate(frames): clear_output(wait=True) print(frame['frame'].getvalue()) print(f"Timestep: {i + 1}") print(f"State: {frame['state']}") print(f"Action: {frame['action']}") print(f"Reward: {frame['reward']}") sleep(.1) frames(frames)</span></span></code> </pre><br>  Conclusion: <br><br><img src="https://habrastorage.org/webt/b3/kd/z_/b3kdz_kejhninocffgc2_3ytr_u.gif"><br><br>  crédits: OpenAI <br><br>  Le problème est résolu, mais pas optimisé, ou cet algorithme ne fonctionnera pas dans tous les cas.  Nous avons besoin d'un agent d'interaction approprié pour que le nombre d'itérations passées par la machine / l'algorithme pour résoudre le problème reste minime.  Ici, l'algorithme Q-learning nous aidera, dont la mise en œuvre sera examinée dans la section suivante. <br><br>  <b>Présentation de Q-Learning</b> <br><br>  Vous trouverez ci-dessous les algorithmes d'apprentissage par renforcement les plus populaires et les plus simples.  L'environnement récompense l'agent pour une formation progressive et pour le fait que dans un état particulier, il fait le pas le plus optimal.  Dans la mise en œuvre discutée ci-dessus, nous avions une table de récompense «P», selon laquelle notre agent apprendra.  En fonction du tableau des récompenses, il choisit l'action suivante en fonction de son utilité, puis met à jour une autre valeur, appelée valeur Q.  En conséquence, une nouvelle table est créée, appelée Q-table, affichée sur la combinaison (Statut, Action).  Si les valeurs Q sont meilleures, alors nous obtenons des récompenses plus optimisées. <br><br>  Par exemple, si un taxi se trouve dans un état où le passager se trouve au même point que le taxi, il est extrêmement probable que la valeur Q pour l'action «ramasser» soit plus élevée que pour d'autres actions, par exemple, «déposer le passager» ou «aller vers le nord». ". <br>  Les valeurs Q sont initialisées avec des valeurs aléatoires et, lorsque l'agent interagit avec l'environnement et reçoit diverses récompenses en effectuant certaines actions, les valeurs Q sont mises à jour conformément à l'équation suivante: <br><br><img src="https://habrastorage.org/webt/ed/fv/br/edfvbr7xz2terdw8meeftimstx0.png"><br><br>  Cela soulève la question: comment initialiser les valeurs Q et comment les calculer.  Au fur et à mesure des actions, les valeurs Q sont exécutées dans cette équation. <br><br>  Ici, Alpha et Gamma sont les paramètres de l'algorithme Q-learning.  Alpha est le rythme d'apprentissage, et gamma est le facteur d'actualisation.  Les deux valeurs peuvent aller de 0 à 1 et sont parfois égales à un.  Le gamma peut être égal à zéro, mais pas l'alpha, car la valeur des pertes lors de la mise à jour doit être compensée (le taux d'apprentissage est positif).  La valeur alpha ici est la même que lorsque vous enseignez avec un professeur.  Gamma détermine l'importance que nous voulons donner aux récompenses qui nous attendent à l'avenir. <br><br>  Cet algorithme est résumé ci-dessous: <br><br><ul><li>  Étape 1: initialisez la table Q, remplissez-la de zéros et pour les valeurs Q, nous définissons des constantes arbitraires. </li><li>  Étape 2: Maintenant, laissez l'agent répondre à l'environnement et essayez différentes actions.  Pour chaque changement d'état, nous sélectionnons une de toutes les actions possibles dans cet état (S). </li><li>  Étape 3: Passez à l'état suivant (S ') en fonction des résultats de l'action précédente (a). </li><li>  Étape 4: Pour toutes les actions possibles de l'état (S '), sélectionnez-en une avec la valeur Q la plus élevée. </li><li>  Étape 5: Mettez à jour les valeurs de la table Q conformément à l'équation ci-dessus. </li><li>  Étape 6: Transformez l'état suivant en l'état actuel. </li><li>  Étape 7: Si l'état cible est atteint, nous terminons le processus, puis répétons. </li></ul><br>  <b>Q-learning en Python</b> <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> IPython.display <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> clear_output <span class="hljs-comment"><span class="hljs-comment">#  Taxi-V2 Env env = gym.make("Taxi-v2").env #    q_table = np.zeros([env.observation_space.n, env.action_space.n]) #  alpha = 0.1 gamma = 0.6 epsilon = 0.1 all_epochs = [] all_penalties = [] for i in range(1, 100001): state = env.reset() #   epochs, penalties, reward, = 0, 0, 0 done = False while not done: if random.uniform(0, 1) &lt; epsilon: #    action = env.action_space.sample() else: #    action = np.argmax(q_table[state]) next_state, reward, done, info = env.step(action) old_value = q_table[state, action] next_max = np.max(q_table[next_state]) #    new_value = (1 - alpha) * old_value + alpha * \ (reward + gamma * next_max) q_table[state, action] = new_value if reward == -10: penalties += 1 state = next_state epochs += 1 if i % 100 == 0: clear_output(wait=True) print("Episode: {i}") print("Training finished.")</span></span></code> </pre><br>  Génial, maintenant toutes vos valeurs seront stockées dans la variable <code>q_table</code> . <br><br>  Ainsi, votre modèle est formé aux conditions environnementales et sait désormais sélectionner plus précisément les passagers.  Et vous vous êtes familiarisé avec le phénomène de l'apprentissage par renforcement, et vous pouvez programmer l'algorithme pour résoudre un nouveau problème. <br><br>  Autres techniques d'apprentissage par renforcement: <br><br><ul><li>  Processus décisionnels de Markov (MDP) et équations de Bellman </li><li>  Programmation dynamique: RL basé sur un modèle, itération de stratégie et itération de valeur </li><li>  Deep Q-Training </li><li>  Méthodes de descente de gradient de stratégie </li><li>  Sarsa </li></ul><br>  Le code de cet exercice se trouve à: <br><br>  vihar / python-reinforcement-learning </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr434738/">https://habr.com/ru/post/fr434738/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr434728/index.html">Personnes et processus: pourquoi ne pas udalenka adapté à chaque entreprise?</a></li>
<li><a href="../fr434730/index.html">Bases de données en mémoire: application, mise à l'échelle et ajouts importants</a></li>
<li><a href="../fr434732/index.html">La vie à 6200 DPI. Examen du noyau HyperX Pulsefire</a></li>
<li><a href="../fr434734/index.html">Transformation de Fourier. Le rapide et le furieux</a></li>
<li><a href="../fr434736/index.html">Utilisation de la base de données des journaux Mikrotik pour supprimer la force brute</a></li>
<li><a href="../fr434740/index.html">Un réseau de neurones a appris à détecter les panneaux solaires dans les images satellites et à prédire le niveau de leur distribution</a></li>
<li><a href="../fr434742/index.html">Partie 2: Utilisation des contrôleurs UDB PSoC de Cypress pour réduire le nombre d'interruptions dans une imprimante 3D</a></li>
<li><a href="../fr434744/index.html">Samsung SSD 860 QVO 1 TB et 4 TB: le premier consommateur SATA QLC (2 parties)</a></li>
<li><a href="../fr434746/index.html">BLE sous microscope 4</a></li>
<li><a href="../fr434750/index.html">Comment prendre le contrôle de votre infrastructure réseau. Chapitre deux Nettoyage et documentation</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>