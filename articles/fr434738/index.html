<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü•á üë®üèª‚Äç‚úàÔ∏è üëßüèº Apprentissage par renforcement en Python ‚úäüèæ üèùÔ∏è üíÜüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Salut les coll√®gues! 



 Dans la derni√®re publication de l'ann√©e sortante, nous voulions mentionner l'apprentissage par renforcement - un sujet dans ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Apprentissage par renforcement en Python</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/434738/">  Salut les coll√®gues! <br><br><img src="https://habrastorage.org/webt/8s/-m/om/8s-mommciij8mkqdkjm62glthy4.jpeg"><br><br>  Dans la derni√®re publication de l'ann√©e sortante, nous voulions mentionner l'apprentissage par renforcement - un sujet dans lequel nous traduisons d√©j√† un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">livre</a> . <br><br>  Jugez par vous-m√™me: il y avait un article √©l√©mentaire avec Medium, qui d√©crivait le contexte du probl√®me, d√©crivait l'algorithme le plus simple avec une impl√©mentation en Python.  L'article a plusieurs gifs.  Et la motivation, la r√©compense et le choix de la bonne strat√©gie sur la voie du succ√®s sont des choses qui seront extr√™mement utiles √† chacun de nous au cours de l'ann√©e √† venir. <br><br>  Bonne lecture! <br><a name="habracut"></a><br>  L'apprentissage renforc√© est une forme d'apprentissage automatique dans laquelle l'agent apprend √† agir dans l'environnement, √† effectuer des actions et √† d√©velopper ainsi l'intuition, apr√®s quoi il observe les r√©sultats de ses actions.  Dans cet article, je vais vous expliquer comment comprendre et formuler le probl√®me de l'apprentissage par renforcement, puis le r√©soudre en Python. <br><br><br>  R√©cemment, nous nous sommes habitu√©s au fait que les ordinateurs jouent √† des jeux contre les humains - soit comme des bots dans des jeux multijoueurs, soit comme des rivaux dans des jeux en un contre un: disons, dans Dota2, PUB-G, Mario.  La soci√©t√© de recherche <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Deepmind a</a> fait toute une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">histoire √†</a> propos de cette nouvelle lorsque son programme AlphaGo 2016 a battu le champion sud-cor√©en en 2016.  Si vous √™tes un joueur passionn√©, vous pourriez entendre parler des cinq matchs de Dota 2 OpenAI Five, o√π des voitures ont combattu contre des gens et battu les meilleurs joueurs de Dota2 en plusieurs matchs.  (Si vous √™tes int√©ress√© par les d√©tails, l'algorithme est analys√© en d√©tail <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> et il est examin√© comment les machines ont jou√©). <br><br><img src="https://habrastorage.org/webt/da/l7/3j/dal73jd7dacspz0co83f6zuano0.png"><br><br>  La derni√®re version d'OpenAI Five <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">prend Roshan</a> . <br><br>  Commen√ßons donc par la question centrale.  Pourquoi avons-nous besoin d'une formation renforc√©e?  Est-il utilis√© uniquement dans les jeux ou est-il applicable dans des sc√©narios r√©alistes pour r√©soudre des probl√®mes appliqu√©s?  Si c'est la premi√®re fois que vous lisez une formation de renforcement, vous ne pouvez tout simplement pas imaginer la r√©ponse √† ces questions.  En effet, l'apprentissage renforc√© est l'une des technologies les plus utilis√©es et en d√©veloppement rapide dans le domaine de l'intelligence artificielle. <br>  Voici un certain nombre de domaines dans lesquels les syst√®mes d'apprentissage par renforcement sont particuli√®rement demand√©s: <br><br><ol><li>  V√©hicules sans pilote </li><li>  Industrie du jeu </li><li>  Robotique </li><li>  Syst√®mes de recommandation </li><li>  Publicit√© et marketing </li></ol><br>  <b>Aper√ßu et contexte de l'apprentissage par renforcement</b> <br><br>  Alors, comment le ph√©nom√®ne d'apprentissage avec renforcement a-t-il pris forme alors que nous avons tant de machines et de m√©thodes d'apprentissage en profondeur √† notre disposition?  "Il a √©t√© invent√© par Rich Sutton et Andrew Barto, directeur de recherche de Rich, qui l'ont aid√© √† pr√©parer son doctorat."  Le paradigme a d'abord pris forme dans les ann√©es 80, puis √©tait archa√Øque.  Par la suite, Rich a cru qu'elle avait un grand avenir et qu'elle finirait par √™tre reconnue. <br><br>  L'apprentissage renforc√© prend en charge l'automatisation dans l'environnement o√π il est d√©ploy√©.  La machine et l'apprentissage en profondeur fonctionnent √† peu pr√®s de la m√™me mani√®re - ils sont strat√©giquement dispos√©s diff√©remment, mais les deux paradigmes prennent en charge l'automatisation.  Alors, pourquoi la formation de renforcement est-elle apparue? <br><br>  Cela rappelle beaucoup le processus d'apprentissage naturel, dans lequel le processus / mod√®le agit et re√ßoit des commentaires sur la fa√ßon dont elle parvient √† faire face √† la t√¢che: bonne et non. <br><br>  La machine et l'apprentissage en profondeur sont √©galement des options de formation, mais ils sont plus adapt√©s pour identifier les mod√®les dans les donn√©es disponibles.  En revanche, dans l'apprentissage par renforcement, cette exp√©rience s'acquiert par essais et erreurs;  le syst√®me trouve progressivement les bonnes options ou l'optimum global.  Un avantage suppl√©mentaire s√©rieux de l'apprentissage renforc√© est que dans ce cas, il n'est pas n√©cessaire de fournir un ensemble complet de donn√©es de formation, comme dans l'enseignement avec un enseignant.  Quelques petits fragments suffiront. <br><br>  <b>Le concept d'apprentissage par renforcement</b> <br><br>  Imaginez apprendre √† vos chats de nouveaux trucs;  mais, malheureusement, les chats ne comprennent pas le langage humain, vous ne pouvez donc pas prendre et leur dire ce que vous allez jouer avec eux.  Par cons√©quent, vous agirez diff√©remment: imitez la situation et le chat essaiera de r√©pondre d'une mani√®re ou d'une autre en r√©ponse.  Si le chat a r√©agi comme vous le vouliez, vous y versez du lait.  Comprenez-vous ce qui va se passer ensuite?  Encore une fois, dans une situation similaire, le chat effectuera √† nouveau l'action souhait√©e, et avec encore plus d'enthousiasme, en esp√©rant qu'il sera encore mieux nourri.  C'est ainsi que l'apprentissage se d√©roule sur un exemple positif;  mais, si vous essayez ¬´d'√©duquer¬ª un chat avec des incitations n√©gatives, par exemple, regardez-le strictement et froncez les sourcils, il n'apprend g√©n√©ralement pas dans de telles situations. <br><br>  L'apprentissage renforc√© fonctionne de mani√®re similaire.  Nous indiquons √† la machine certaines entr√©es et actions, puis r√©compensons la machine en fonction de la sortie.  Notre objectif ultime est de maximiser les r√©compenses.  Voyons maintenant comment reformuler le probl√®me ci-dessus en termes d'apprentissage par renforcement. <br><br><ul><li>  Le chat agit comme un ¬´agent¬ª expos√© √† ¬´l'environnement¬ª. </li><li>  L'environnement est une maison ou une aire de jeux, selon ce que vous enseignez au chat. </li><li>  Les situations r√©sultant de la formation sont appel√©es ¬´√©tats¬ª.  Dans le cas d'un chat, des exemples de conditions sont lorsque le chat ¬´court¬ª ou ¬´rampe sous le lit¬ª. </li><li>  Les agents r√©agissent en effectuant des actions et en passant d'un ¬´√©tat¬ª √† un autre. </li><li>  Apr√®s le changement d'√©tat, l'agent re√ßoit une ¬´r√©compense¬ª ou une ¬´amende¬ª selon l'action qu'il a entreprise. </li><li>  La ¬´strat√©gie¬ª est une m√©thodologie pour choisir une action pour obtenir les meilleurs r√©sultats. </li></ul><br>  Maintenant que nous avons compris ce qu'est l'apprentissage par renforcement, parlons en d√©tail des origines et de l'√©volution de l'apprentissage par renforcement et de l'apprentissage par renforcement profond, discutons de la fa√ßon dont ce paradigme nous permet de r√©soudre des probl√®mes impossibles √† apprendre avec ou sans enseignant, et notons √©galement ce qui suit fait curieux: √† l'heure actuelle, le moteur de recherche de Google est optimis√© √† l'aide d'algorithmes d'apprentissage par renforcement. <br><br>  <b>Comprendre la terminologie de l'apprentissage par renforcement</b> <br><br>  L'agent et l'environnement jouent des r√¥les cl√©s dans l'algorithme d'apprentissage par renforcement.  L'environnement est le monde dans lequel l'agent doit survivre.  De plus, l'agent re√ßoit des signaux de renforcement de l'environnement (r√©compense): il s'agit d'un nombre qui caract√©rise √† quel point l'√©tat actuel du monde peut √™tre bon ou mauvais.  Le but de l'agent est de maximiser la r√©compense totale, ce que l'on appelle le ¬´gain¬ª.  Avant d'√©crire nos premiers algorithmes d'apprentissage par renforcement, vous devez comprendre la terminologie suivante. <br><br><img src="https://habrastorage.org/webt/6j/vx/cp/6jvxcpcpr52v252wa9eze1mehx4.gif"><br><br><ol><li>  <b>√âtats</b> : Un √âtat est une description compl√®te d'un monde dans lequel aucun fragment des informations caract√©risant ce monde ne manque.  Il peut s'agir d'une position, fixe ou dynamique.  En r√®gle g√©n√©rale, ces √©tats sont √©crits sous la forme de tableaux, de matrices ou de tenseurs d'ordre sup√©rieur. </li><li>  <b>Action</b> : l'action d√©pend g√©n√©ralement des conditions environnementales et, dans diff√©rents environnements, l'agent entreprendra diff√©rentes actions.  De nombreuses actions d'agent valides sont enregistr√©es dans un espace appel√© ¬´espace d'action¬ª.  En r√®gle g√©n√©rale, le nombre d'actions dans l'espace est fini. </li><li>  <b>Environnement</b> : Il s'agit de l'endroit o√π l'agent existe et avec lequel il interagit.  Diff√©rents types de r√©compenses, strat√©gies, etc. sont utilis√©s pour diff√©rents environnements. </li><li>  <b>R√©compenses</b> et <b>gains</b> : vous devez surveiller en permanence la fonction de r√©compense R lorsque vous vous entra√Ænez avec des renforts.  Il est essentiel lors de la configuration d'un algorithme, de son optimisation et √©galement lorsque vous arr√™tez d'apprendre.  Cela d√©pend de l'√©tat actuel du monde, des mesures qui viennent d'√™tre prises et du prochain √©tat du monde. </li><li>  <b>Strat√©gies</b> : une strat√©gie est une r√®gle selon laquelle un agent choisit l'action suivante.  L'ensemble des strat√©gies est √©galement appel√© ¬´cerveau¬ª de l'agent. </li></ol><br><img src="https://habrastorage.org/webt/ur/lb/u-/urlbu-ifbred1iqfkhvqkv7bqds.png"><br><br>  Maintenant que nous nous sommes familiaris√©s avec la terminologie de l'apprentissage par renforcement, r√©solvons le probl√®me en utilisant les algorithmes appropri√©s.  Avant cela, vous devez comprendre comment formuler un tel probl√®me et, pour r√©soudre ce probl√®me, vous fier √† la terminologie de la formation avec renforcement. <br><br>  <b>Solution de taxi</b> <br><br>  Nous passons donc √† la r√©solution du probl√®me avec l'utilisation d'algorithmes de renforcement. <br>  Supposons que nous ayons une zone d'entra√Ænement pour un taxi sans pilote, que nous formons pour livrer les passagers au parking √† quatre points diff√©rents ( <code>R,G,Y,B</code> ).  Avant cela, vous devez comprendre et d√©finir l'environnement dans lequel nous commen√ßons la programmation en Python.  Si vous commencez tout juste √† apprendre Python, je vous recommande <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cet article</a> . <br><br>  L'environnement pour r√©soudre un probl√®me de taxi peut √™tre configur√© √† l'aide d'OpenAI's <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Gym</a> - c'est l'une des biblioth√®ques les plus populaires pour r√©soudre les probl√®mes avec la formation de renforcement.  Eh bien, avant d'utiliser gym, vous devez l'installer sur votre machine, et un gestionnaire de packages Python appel√© pip est pratique pour cela.  Voici la commande d'installation. <br><br> <code>pip install gym</code> <br> <br>  Voyons ensuite comment notre environnement sera affich√©.  Tous les mod√®les et l'interface pour cette t√¢che sont d√©j√† configur√©s dans le gymnase et nomm√©s sous <code>Taxi-V2</code> .  L'extrait de code ci-dessous est utilis√© pour afficher cet environnement. <br><br>  ¬´Nous avons 4 emplacements (indiqu√©s par des lettres diff√©rentes);  notre t√¢che consiste √† prendre un passager √† un moment donn√© et √† le d√©poser √† un autre.  Nous obtenons +20 points pour un atterrissage r√©ussi et perdons 1 point pour chaque √©tape d√©pens√©e.  Il y a √©galement une p√©nalit√© de 10 points pour chaque embarquement et d√©barquement involontaire d'un passager. ¬ª  (Source: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">gym.openai.com/envs/Taxi-v2</a> ) <br><br>  Voici la sortie que nous verrons dans notre console: <br><br><img src="https://habrastorage.org/webt/n8/xj/lw/n8xjlwkbla6unwn2k6tworczc8o.png"><br><br>  Taxi V2 ENV <br><br>  G√©nial, <code>env</code> est le c≈ìur d'OpenAi Gym, c'est une interface d'environnement unifi√©e.  Voici les m√©thodes env que nous trouvons utiles: <br><br>  <code>env.reset</code> : r√©initialise l'environnement et renvoie un √©tat initial al√©atoire. <br>  <code>env.step(action)</code> : fait progresser <code>env.step(action)</code> d√©veloppement de l'environnement une √©tape dans le temps. <br>  <code>env.step(action)</code> : retourne les variables suivantes <br><br><ul><li>  <code>observation</code> : Observation de l'environnement. </li><li>  <code>reward</code> : <code>reward</code> si votre action a √©t√© b√©n√©fique. </li><li>  <code>done</code> : indique si nous avons r√©ussi √† ramasser et √† d√©poser correctement le passager, √©galement appel√© ¬´un √©pisode¬ª. </li><li>  <code>info</code> : informations suppl√©mentaires telles que les performances et la latence n√©cessaires √† des fins de d√©bogage. </li><li>  <code>env.render</code> : affiche une image de l'environnement (utile pour le rendu) </li></ul><br>  Donc, apr√®s avoir examin√© l'environnement, essayons de mieux comprendre le probl√®me.  Les taxis sont la seule voiture sur ce parking.  Le stationnement peut √™tre divis√© en une grille <code>5x5</code> , o√π nous obtenons 25 emplacements de taxi possibles.  Ces 25 valeurs sont l'un des √©l√©ments de notre espace d'√©tat.  Remarque: pour le moment, notre taxi est situ√© au point avec les coordonn√©es (3, 1). <br><br>  Il y a 4 points dans l'environnement o√π les passagers sont autoris√©s √† monter √† bord: ce sont les coordonn√©es <code>R, G, Y, B</code> ou <code>[(0,0), (0,4), (4,0), (4,3)]</code> en coordonn√©es ( horizontalement; verticalement), s'il √©tait possible d'interpr√©ter l'environnement ci-dessus en coordonn√©es cart√©siennes.  Si vous prenez √©galement en compte un (1) autre √©tat du passager: √† l'int√©rieur du taxi, vous pouvez prendre toutes les combinaisons d'emplacements de passagers et de leurs destinations afin de calculer le nombre total d'√©tats dans notre environnement pour la formation en taxi: nous avons quatre (4) destinations et cinq (4+) 1) emplacements des passagers. <br><br>  Donc, dans notre environnement pour un taxi, il y a 5 √ó 5 √ó 5 √ó 4 = 500 √©tats possibles.  Un agent traite l'une des 500 conditions et prend des mesures.  Dans notre cas, les options sont les suivantes: se d√©placer dans un sens ou dans l'autre, ou la d√©cision de prendre / d√©poser le passager.  En d'autres termes, nous avons √† notre disposition six actions possibles: <br>  ramasser, d√©poser, nord, est, sud, ouest (les quatre derni√®res valeurs sont les directions dans lesquelles un taxi peut se d√©placer.) <br><br>  Il s'agit de l' <code>action space</code> : l'ensemble de toutes les actions que notre agent peut entreprendre dans un √©tat donn√©. <br><br>  Comme le montre l'illustration ci-dessus, un taxi ne peut pas effectuer certaines actions dans certaines situations (les murs interf√®rent).  Dans le code d√©crivant l'environnement, nous attribuons simplement une p√©nalit√© de -1 pour chaque coup dans le mur, et un taxi entrant en collision avec le mur.  Ainsi, de telles amendes s'accumuleront, de sorte que le taxi essaiera de ne pas heurter les murs. <br><br>  Tableau des r√©compenses: lors de la cr√©ation d'un environnement de taxi, il est √©galement possible de cr√©er un tableau de r√©compenses principal appel√© P. Vous pouvez le consid√©rer comme une matrice, o√π le nombre d'√©tats correspond au nombre de lignes et le nombre d'actions au nombre de colonnes.  Autrement dit, nous parlons de la matrice <code>states √ó actions</code> . <br><br>  √âtant donn√© que toutes les conditions sont enregistr√©es dans cette matrice, vous pouvez afficher les valeurs de r√©compense par d√©faut affect√©es √† l'√©tat que nous avons choisi d'illustrer: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym &gt;&gt;&gt; env = gym.make(<span class="hljs-string"><span class="hljs-string">"Taxi-v2"</span></span>).env &gt;&gt;&gt; env.P[<span class="hljs-number"><span class="hljs-number">328</span></span>] {<span class="hljs-number"><span class="hljs-number">0</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">433</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">1</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">233</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">2</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">353</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">3</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">4</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">5</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)] }</code> </pre> <br>  La structure de ce dictionnaire est la suivante: <code>{action: [(probability, nextstate, reward, done)]}</code> . <br><br><ul><li>  Les valeurs 0 √† 5 correspondent aux actions (sud, nord, est, ouest, prise en charge, d√©pose) qu'un taxi peut effectuer dans l'√©tat actuel indiqu√© sur l'illustration. </li><li>  done vous permet de juger du moment o√π nous avons r√©ussi √† d√©poser le passager au point souhait√©. </li></ul><br>  Pour r√©soudre ce probl√®me sans aucune formation avec renforcement, vous pouvez d√©finir l'√©tat cible, faire une s√©lection d'espaces, puis, si vous pouvez atteindre l'√©tat cible pendant un certain nombre d'it√©rations, supposer que ce moment correspond √† la r√©compense maximale.  Dans d'autres √âtats, la valeur de la r√©compense s'approche du maximum si le programme agit correctement (s'approche de l'objectif) ou accumule des amendes en cas d'erreur.  De plus, la valeur de l'amende peut atteindre au moins -10. <br><br>  √âcrivons du code pour r√©soudre ce probl√®me sans formation de renforcement. <br>  √âtant donn√© que nous avons une table P avec des valeurs de r√©compense par d√©faut pour chaque √©tat, nous pouvons essayer d'organiser la navigation de notre taxi en fonction de cette table. <br><br>  Nous cr√©ons une boucle sans fin, d√©filant jusqu'√† ce que le passager atteigne la destination (un √©pisode), ou, en d'autres termes, jusqu'√† ce que le taux de r√©compense atteigne 20. La m√©thode <code>env.action_space.sample()</code> s√©lectionne automatiquement une action al√©atoire dans l'ensemble de toutes les actions disponibles .  Consid√©rez ce qui se passe: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sleep <span class="hljs-comment"><span class="hljs-comment">#  thr env env = gym.make("Taxi-v2").env env.s = 328 #     ,   , epochs = 0 penalties, reward = 0, 0 frames = [] done = False while not done: action = env.action_space.sample() state, reward, done, info = env.step(action) if reward == -10: penalties += 1 #         frames.append({ 'frame': env.render(mode='ansi'), 'state': state, 'action': action, 'reward': reward } ) epochs += 1 print("Timesteps taken: {}".format(epochs)) print("Penalties incurred: {}".format(penalties)) #    , ,  def frames(frames): for i, frame in enumerate(frames): clear_output(wait=True) print(frame['frame'].getvalue()) print(f"Timestep: {i + 1}") print(f"State: {frame['state']}") print(f"Action: {frame['action']}") print(f"Reward: {frame['reward']}") sleep(.1) frames(frames)</span></span></code> </pre><br>  Conclusion: <br><br><img src="https://habrastorage.org/webt/b3/kd/z_/b3kdz_kejhninocffgc2_3ytr_u.gif"><br><br>  cr√©dits: OpenAI <br><br>  Le probl√®me est r√©solu, mais pas optimis√©, ou cet algorithme ne fonctionnera pas dans tous les cas.  Nous avons besoin d'un agent d'interaction appropri√© pour que le nombre d'it√©rations pass√©es par la machine / l'algorithme pour r√©soudre le probl√®me reste minime.  Ici, l'algorithme Q-learning nous aidera, dont la mise en ≈ìuvre sera examin√©e dans la section suivante. <br><br>  <b>Pr√©sentation de Q-Learning</b> <br><br>  Vous trouverez ci-dessous les algorithmes d'apprentissage par renforcement les plus populaires et les plus simples.  L'environnement r√©compense l'agent pour une formation progressive et pour le fait que dans un √©tat particulier, il fait le pas le plus optimal.  Dans la mise en ≈ìuvre discut√©e ci-dessus, nous avions une table de r√©compense ¬´P¬ª, selon laquelle notre agent apprendra.  En fonction du tableau des r√©compenses, il choisit l'action suivante en fonction de son utilit√©, puis met √† jour une autre valeur, appel√©e valeur Q.  En cons√©quence, une nouvelle table est cr√©√©e, appel√©e Q-table, affich√©e sur la combinaison (Statut, Action).  Si les valeurs Q sont meilleures, alors nous obtenons des r√©compenses plus optimis√©es. <br><br>  Par exemple, si un taxi se trouve dans un √©tat o√π le passager se trouve au m√™me point que le taxi, il est extr√™mement probable que la valeur Q pour l'action ¬´ramasser¬ª soit plus √©lev√©e que pour d'autres actions, par exemple, ¬´d√©poser le passager¬ª ou ¬´aller vers le nord¬ª. ". <br>  Les valeurs Q sont initialis√©es avec des valeurs al√©atoires et, lorsque l'agent interagit avec l'environnement et re√ßoit diverses r√©compenses en effectuant certaines actions, les valeurs Q sont mises √† jour conform√©ment √† l'√©quation suivante: <br><br><img src="https://habrastorage.org/webt/ed/fv/br/edfvbr7xz2terdw8meeftimstx0.png"><br><br>  Cela soul√®ve la question: comment initialiser les valeurs Q et comment les calculer.  Au fur et √† mesure des actions, les valeurs Q sont ex√©cut√©es dans cette √©quation. <br><br>  Ici, Alpha et Gamma sont les param√®tres de l'algorithme Q-learning.  Alpha est le rythme d'apprentissage, et gamma est le facteur d'actualisation.  Les deux valeurs peuvent aller de 0 √† 1 et sont parfois √©gales √† un.  Le gamma peut √™tre √©gal √† z√©ro, mais pas l'alpha, car la valeur des pertes lors de la mise √† jour doit √™tre compens√©e (le taux d'apprentissage est positif).  La valeur alpha ici est la m√™me que lorsque vous enseignez avec un professeur.  Gamma d√©termine l'importance que nous voulons donner aux r√©compenses qui nous attendent √† l'avenir. <br><br>  Cet algorithme est r√©sum√© ci-dessous: <br><br><ul><li>  √âtape 1: initialisez la table Q, remplissez-la de z√©ros et pour les valeurs Q, nous d√©finissons des constantes arbitraires. </li><li>  √âtape 2: Maintenant, laissez l'agent r√©pondre √† l'environnement et essayez diff√©rentes actions.  Pour chaque changement d'√©tat, nous s√©lectionnons une de toutes les actions possibles dans cet √©tat (S). </li><li>  √âtape 3: Passez √† l'√©tat suivant (S ') en fonction des r√©sultats de l'action pr√©c√©dente (a). </li><li>  √âtape 4: Pour toutes les actions possibles de l'√©tat (S '), s√©lectionnez-en une avec la valeur Q la plus √©lev√©e. </li><li>  √âtape 5: Mettez √† jour les valeurs de la table Q conform√©ment √† l'√©quation ci-dessus. </li><li>  √âtape 6: Transformez l'√©tat suivant en l'√©tat actuel. </li><li>  √âtape 7: Si l'√©tat cible est atteint, nous terminons le processus, puis r√©p√©tons. </li></ul><br>  <b>Q-learning en Python</b> <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> IPython.display <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> clear_output <span class="hljs-comment"><span class="hljs-comment">#  Taxi-V2 Env env = gym.make("Taxi-v2").env #    q_table = np.zeros([env.observation_space.n, env.action_space.n]) #  alpha = 0.1 gamma = 0.6 epsilon = 0.1 all_epochs = [] all_penalties = [] for i in range(1, 100001): state = env.reset() #   epochs, penalties, reward, = 0, 0, 0 done = False while not done: if random.uniform(0, 1) &lt; epsilon: #    action = env.action_space.sample() else: #    action = np.argmax(q_table[state]) next_state, reward, done, info = env.step(action) old_value = q_table[state, action] next_max = np.max(q_table[next_state]) #    new_value = (1 - alpha) * old_value + alpha * \ (reward + gamma * next_max) q_table[state, action] = new_value if reward == -10: penalties += 1 state = next_state epochs += 1 if i % 100 == 0: clear_output(wait=True) print("Episode: {i}") print("Training finished.")</span></span></code> </pre><br>  G√©nial, maintenant toutes vos valeurs seront stock√©es dans la variable <code>q_table</code> . <br><br>  Ainsi, votre mod√®le est form√© aux conditions environnementales et sait d√©sormais s√©lectionner plus pr√©cis√©ment les passagers.  Et vous vous √™tes familiaris√© avec le ph√©nom√®ne de l'apprentissage par renforcement, et vous pouvez programmer l'algorithme pour r√©soudre un nouveau probl√®me. <br><br>  Autres techniques d'apprentissage par renforcement: <br><br><ul><li>  Processus d√©cisionnels de Markov (MDP) et √©quations de Bellman </li><li>  Programmation dynamique: RL bas√© sur un mod√®le, it√©ration de strat√©gie et it√©ration de valeur </li><li>  Deep Q-Training </li><li>  M√©thodes de descente de gradient de strat√©gie </li><li>  Sarsa </li></ul><br>  Le code de cet exercice se trouve √†: <br><br>  vihar / python-reinforcement-learning </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr434738/">https://habr.com/ru/post/fr434738/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr434728/index.html">Personnes et processus: pourquoi ne pas udalenka adapt√© √† chaque entreprise?</a></li>
<li><a href="../fr434730/index.html">Bases de donn√©es en m√©moire: application, mise √† l'√©chelle et ajouts importants</a></li>
<li><a href="../fr434732/index.html">La vie √† 6200 DPI. Examen du noyau HyperX Pulsefire</a></li>
<li><a href="../fr434734/index.html">Transformation de Fourier. Le rapide et le furieux</a></li>
<li><a href="../fr434736/index.html">Utilisation de la base de donn√©es des journaux Mikrotik pour supprimer la force brute</a></li>
<li><a href="../fr434740/index.html">Un r√©seau de neurones a appris √† d√©tecter les panneaux solaires dans les images satellites et √† pr√©dire le niveau de leur distribution</a></li>
<li><a href="../fr434742/index.html">Partie 2: Utilisation des contr√¥leurs UDB PSoC de Cypress pour r√©duire le nombre d'interruptions dans une imprimante 3D</a></li>
<li><a href="../fr434744/index.html">Samsung SSD 860 QVO 1 TB et 4 TB: le premier consommateur SATA QLC (2 parties)</a></li>
<li><a href="../fr434746/index.html">BLE sous microscope 4</a></li>
<li><a href="../fr434750/index.html">Comment prendre le contr√¥le de votre infrastructure r√©seau. Chapitre deux Nettoyage et documentation</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>