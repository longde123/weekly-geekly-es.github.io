<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üî∞ üíÜ üë®üèΩ‚Äçüîß Orienta√ß√£o √† m√°quina a longa dist√¢ncia usando aprendizado refor√ßado üë®üèΩ‚Äçüîß üôã üë®üèæ‚Äç‚úàÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Somente nos Estados Unidos, existem 3 milh√µes de pessoas com defici√™ncia que n√£o podem deixar suas casas. Os rob√¥s auxiliares que podem navegar automa...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Orienta√ß√£o √† m√°quina a longa dist√¢ncia usando aprendizado refor√ßado</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/444372/"> Somente nos Estados Unidos, existem 3 milh√µes de pessoas com defici√™ncia que n√£o podem deixar suas casas.  Os rob√¥s auxiliares que podem navegar automaticamente por longas dist√¢ncias podem tornar essas pessoas mais independentes, trazendo alimentos, rem√©dios e pacotes.  Estudos mostram que o aprendizado profundo com refor√ßo (OP) √© ‚Äã‚Äãbem adequado para comparar dados e a√ß√µes brutos de entrada, por exemplo, para aprender a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">capturar objetos</a> ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">mover rob√¥s</a> , mas geralmente os <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">agentes de</a> OP n√£o compreendem os grandes espa√ßos f√≠sicos necess√°rios para uma orienta√ß√£o segura a longa dist√¢ncia dist√¢ncias sem ajuda humana e adapta√ß√£o a um novo ambiente. <br><a name="habracut"></a><br>  Em tr√™s trabalhos recentes, ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Treinamento de orienta√ß√£o do zero com AOP</a> ‚Äù, ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">PRM-RL: implementando orienta√ß√£o rob√≥tica em longas dist√¢ncias usando uma combina√ß√£o de aprendizado refor√ßado e planejamento baseado em padr√µes</a> ‚Äù e ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Orienta√ß√£o de longo alcance com PRM-RL</a> ‚Äù <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">,</a> n√≥s Estudamos rob√¥s aut√¥nomos que se adaptam facilmente a um novo ambiente, combinando OP profundo com planejamento a longo prazo.  Ensinamos aos agentes planejadores locais como executar as a√ß√µes b√°sicas necess√°rias para orienta√ß√£o e como percorrer curtas dist√¢ncias sem colis√µes com objetos em movimento.  Planejadores locais realizam observa√ß√µes ambientais barulhentas usando sensores como lidares unidimensionais que fornecem dist√¢ncia a um obst√°culo e fornecem velocidades lineares e angulares para controlar o rob√¥.  Treinamos o planejador local em simula√ß√µes usando o aprendizado por refor√ßo autom√°tico (AOP), um m√©todo que automatiza a busca por recompensas para o OP e a arquitetura da rede neural.  Apesar do alcance limitado de 10 a 15 m, os planejadores locais se adaptam bem tanto ao uso em rob√¥s reais quanto a novos ambientes previamente desconhecidos.  Isso permite que voc√™ os use como blocos de constru√ß√£o para orienta√ß√£o em grandes espa√ßos.  Em seguida, constru√≠mos um roteiro, um gr√°fico em que os n√≥s s√£o se√ß√µes separadas e as bordas conectam os n√≥s somente se planejadores locais, imitando rob√¥s reais usando sensores e controles ruidosos, podem se mover entre eles. <br><br><h2>  Aprendizagem por refor√ßo autom√°tico (AOP) </h2><br>  Em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">nosso primeiro trabalho,</a> treinamos um planejador local em um pequeno ambiente est√°tico.  No entanto, ao aprender com o algoritmo OP profundo padr√£o, por exemplo, o gradiente determin√≠stico profundo ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">DDPG</a> ), existem v√°rios obst√°culos.  Por exemplo, o objetivo real dos planejadores locais √© atingir um determinado objetivo, como resultado do qual eles recebem recompensas raras.  Na pr√°tica, isso exige que os pesquisadores gastem um tempo consider√°vel na implementa√ß√£o passo a passo do algoritmo e no ajuste manual dos pr√™mios.  Os pesquisadores tamb√©m precisam tomar decis√µes sobre a arquitetura das redes neurais sem ter receitas claras e bem-sucedidas.  Finalmente, algoritmos como o DDPG aprendem de maneira inst√°vel e geralmente exibem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">esquecimentos catastr√≥ficos</a> . <br><br>  Para superar esses obst√°culos, automatizamos o aprendizado profundo com refor√ßo.  O AOP √© um inv√≥lucro autom√°tico evolutivo em torno de um OP profundo, buscando recompensas e arquitetura de rede neural atrav√©s da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">otimiza√ß√£o de hiperpar√¢metros em larga escala</a> .  Ele funciona em duas etapas, a busca por recompensas e a busca pela arquitetura.  Durante a busca por recompensas, o AOP treina simultaneamente a popula√ß√£o de agentes DDPG por v√°rias gera√ß√µes, e cada um tem sua pr√≥pria fun√ß√£o de recompensa ligeiramente modificada, otimizada para a verdadeira tarefa do planejador local: alcan√ßar o ponto final do caminho.  No final da fase de busca de recompensa, selecionamos uma que geralmente leva os agentes √† meta.  Na fase de busca da arquitetura da rede neural, repetimos esse processo, para esta corrida, utilizando o pr√™mio selecionado e ajustando as camadas da rede, otimizando o pr√™mio acumulado. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c96/017/74a/c9601774ae263fe9a2f333d2066e923d.png"><br>  <i>AOP com a busca por premia√ß√£o e arquitetura da rede neural</i> <br><br>  No entanto, esse processo passo a passo torna a AOP ineficaz em termos de n√∫mero de amostras.  O treinamento em AOP com 10 gera√ß√µes de 100 agentes requer 5 bilh√µes de amostras, o equivalente a 32 anos de estudo!  A vantagem √© que, ap√≥s a AOP, o processo de aprendizado manual √© automatizado e o DDPG n√£o apresenta esquecimentos catastr√≥ficos.  Mais importante ainda, a qualidade das pol√≠ticas finais √© mais alta - elas s√£o resistentes ao ru√≠do do sensor, unidade e localiza√ß√£o e s√£o bem generalizadas para novos ambientes.  Nossa melhor pol√≠tica √© 26% mais bem-sucedida do que outros m√©todos de orienta√ß√£o em nossos locais de teste. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/049/823/97c/04982397cb2b6b7a7d20bc9e49ee1a75.png"><br>  <i>Vermelho - AOP obt√©m sucesso em dist√¢ncias curtas (at√© 10 m) em v√°rios edif√≠cios anteriormente desconhecidos.</i>  <i>Compara√ß√£o com DDPG treinado manualmente (vermelho escuro), campos de potencial artificial (azul), janela din√¢mica (azul) e clonagem de comportamento (verde).</i> <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Kq1nQAF4xeM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>A pol√≠tica do agendador local da AOP funciona bem com rob√¥s em ambientes reais n√£o estruturados</i> <br><br>  E embora esses pol√≠ticos sejam capazes apenas de orienta√ß√£o local, s√£o resistentes a obst√°culos em movimento e s√£o bem tolerados por rob√¥s reais em ambientes n√£o estruturados.  E embora tenham sido treinados em simula√ß√µes com objetos est√°ticos, eles efetivamente lidam com objetos em movimento.  O pr√≥ximo passo √© combinar as pol√≠ticas de AOP com o planejamento baseado em amostras, a fim de expandir sua √°rea de trabalho e ensin√°-los a navegar por longas dist√¢ncias. <br><br><h2>  Orienta√ß√£o a longa dist√¢ncia com PRM-RL </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Planejadores baseados em padr√µes</a> trabalham com orienta√ß√£o de longo alcance, aproximando os movimentos do rob√¥.  Por exemplo, um rob√¥ cria <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">roteiros probabil√≠sticos</a> (PRMs) desenhando caminhos de transi√ß√£o entre se√ß√µes.  Em nosso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">segundo trabalho</a> , que ganhou o pr√™mio na confer√™ncia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ICRA 2018</a> , combinamos o PRM com agendadores de OP locais manualmente ajustados (sem AOP) para treinar rob√¥s localmente e depois adapt√°-los a outros ambientes. <br><br>  Primeiro, para cada rob√¥, treinamos a pol√≠tica do planejador local em uma simula√ß√£o generalizada.  Em seguida, criamos um PRM levando em considera√ß√£o essa pol√≠tica, a chamada PRM-RL, com base em um mapa do ambiente em que ser√° usado.  O mesmo cart√£o pode ser usado para qualquer rob√¥ que desejamos usar no pr√©dio. <br><br>  Para criar um PRM-RL, combinamos n√≥s de amostras apenas se o agendador de OP local puder se mover confi√°vel e repetidamente entre eles.  Isso √© feito em uma simula√ß√£o de Monte Carlo.  O mapa resultante se adapta √†s capacidades e geometria de um rob√¥ espec√≠fico.  Cart√µes para rob√¥s com a mesma geometria, mas com diferentes sensores e unidades, ter√£o conectividade diferente.  Como o agente pode girar na esquina, os n√≥s que n√£o est√£o na linha de vis√£o direta tamb√©m podem ser ativados.  No entanto, n√≥s adjacentes √†s paredes e obst√°culos ter√£o menos probabilidade de serem inclu√≠dos no mapa devido ao ru√≠do do sensor.  No tempo de execu√ß√£o, o agente OP move-se pelo mapa de uma se√ß√£o para outra. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/227/75f/f50/22775ff503bbaeb6113227523d06aa8a.gif"><br>  <i>Um mapa √© criado com tr√™s simula√ß√µes de Monte Carlo para cada par de n√≥s selecionado aleatoriamente</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/546/268/9f1/5462689f131cbd48339eec89f36add51.png"><br>  <i>O maior mapa tinha 288x163 m de tamanho e continha quase 700.000 arestas.</i>  <i>300 trabalhadores a coletaram por 4 dias, tendo realizado 1,1 bilh√£o de verifica√ß√µes de colis√£o.</i> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O terceiro trabalho</a> fornece v√°rias melhorias no PRM-RL original.  Em primeiro lugar, estamos substituindo o DDPG ajustado manualmente por agendadores locais de AOP, o que proporciona uma melhoria na orienta√ß√£o em longas dist√¢ncias.  Em segundo lugar, s√£o adicionados <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">mapas de localiza√ß√£o e marca√ß√£o</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">simult√¢nea</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SLAM</a> ), que os rob√¥s usam em tempo de execu√ß√£o como fonte para a constru√ß√£o de roteiros.  As placas SLAM est√£o sujeitas a ru√≠do, e isso fecha a ‚Äúlacuna entre o simulador e a realidade‚Äù, um problema conhecido em rob√≥tica, devido ao qual agentes treinados em simula√ß√µes se comportam muito pior no mundo real.  Nosso n√≠vel de sucesso na simula√ß√£o coincide com o n√≠vel de sucesso de rob√¥s reais.  E, finalmente, adicionamos mapas de constru√ß√£o distribu√≠dos, para que possamos criar mapas muito grandes contendo at√© 700.000 n√≥s. <br><br>  Avaliamos esse m√©todo com a ajuda do nosso agente de AOP, que criou mapas com base em desenhos de edif√≠cios que excederam o ambiente de treinamento em 200 vezes na √°rea, incluindo apenas costelas, que foram conclu√≠das com sucesso em 90% dos casos em 20 tentativas.  Comparamos o PRM-RL com v√°rios m√©todos a dist√¢ncias de at√© 100 m, o que excedeu significativamente o alcance do planejador local.  O PRM-RL obteve sucesso 2-3 vezes mais do que os m√©todos usuais devido √† conex√£o correta dos n√≥s, adequada √†s capacidades do rob√¥. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fa4/659/37a/fa465937a0e3a90383480a831ef4cec7.png"><br>  <i>Taxa de sucesso na movimenta√ß√£o de 100 m em diferentes edif√≠cios.</i>  <i>Azul - agendador local de AOP, primeiro trabalho;</i>  <i>vermelho - PRM original;</i>  <i>amarelo - campos potenciais artificiais;</i>  <i>verde √© o segundo emprego;</i>  <i>vermelho - o terceiro emprego, PRM com AOP.</i> <br><br>  Testamos o PRM-RL em muitos rob√¥s reais em muitos edif√≠cios.  Abaixo est√° uma das su√≠tes de teste;  o rob√¥ se move de maneira confi√°vel em quase todos os lugares, exceto nos lugares e √°reas mais confusos que v√£o al√©m da placa SLAM. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e05/773/fbd/e05773fbd5e6cd4cb41adfebf9a8d083.png"><br><br><h2>  Conclus√£o </h2><br>  A orienta√ß√£o da m√°quina pode aumentar seriamente a independ√™ncia das pessoas com problemas de mobilidade.  Isso pode ser alcan√ßado atrav√©s do desenvolvimento de rob√¥s aut√¥nomos que podem se adaptar facilmente ao ambiente e dos m√©todos dispon√≠veis para implementa√ß√£o no novo ambiente, com base nas informa√ß√µes existentes.  Isso pode ser feito automatizando o treinamento b√°sico de orienta√ß√£o para dist√¢ncias curtas com o AOP e, em seguida, usando as habilidades adquiridas juntamente com os cart√µes SLAM para criar roteiros.  Os roteiros consistem em n√≥s conectados por nervuras, nos quais os rob√¥s podem se mover com seguran√ßa.  Como resultado, √© desenvolvida uma pol√≠tica de comportamento do rob√¥ que, ap√≥s um treinamento, pode ser usada em diferentes ambientes e emitir roteiros especialmente adaptados para um rob√¥ espec√≠fico. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt444372/">https://habr.com/ru/post/pt444372/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt444362/index.html">Unity e Havok trabalham em um novo mecanismo de f√≠sica</a></li>
<li><a href="../pt444364/index.html">Jogo de ferrugem 24 horas: experi√™ncia em desenvolvimento pessoal</a></li>
<li><a href="../pt444366/index.html">Semin√°rio "Requisitos de seguran√ßa da informa√ß√£o: como as empresas podem conviver com eles"</a></li>
<li><a href="../pt444368/index.html">Acabamos de imprimir o microfone em uma impressora 3D em laborat√≥rio - e depois haver√° fic√ß√£o cient√≠fica completa em geral</a></li>
<li><a href="../pt444370/index.html">Do que o formato Mini PCI-e √© capaz?</a></li>
<li><a href="../pt444374/index.html">Efeito hipster: por que os n√£o-conformistas costumam ter a mesma apar√™ncia</a></li>
<li><a href="../pt444376/index.html">A economia da aten√ß√£o est√° quase morta</a></li>
<li><a href="../pt444378/index.html">USPACE - Espa√ßo √∫nico para aeronaves tripuladas e n√£o tripuladas</a></li>
<li><a href="../pt444382/index.html">Como visitar a Universidade da Cor√©ia com o sistema de arquivos em rede</a></li>
<li><a href="../pt444384/index.html">Livro "An√°lise de dados de texto aplicada em Python"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>