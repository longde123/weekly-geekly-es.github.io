<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🏣 👩🏾‍🎨 👩🏼‍🎤 Sprachbarriere und NLP. Warum verstehen uns Chatbots nicht? 🕝 🎟️ 🧘🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die Leute wollten schon lange einer Maschine beibringen, eine Person zu verstehen. Doch erst jetzt sind wir den Handlungen von Science-Fiction-Filmen ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Sprachbarriere und NLP. Warum verstehen uns Chatbots nicht?</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/binarydistrict/blog/422609/">  Die Leute wollten schon lange einer Maschine beibringen, eine Person zu verstehen.  Doch erst jetzt sind wir den Handlungen von Science-Fiction-Filmen etwas näher gekommen: Wir können Alice bitten, die Lautstärke zu verringern, Google Assistant - ein Taxi bestellen oder Siri - einen Alarm auslösen.  Sprachverarbeitungstechnologien sind bei Entwicklungen im Zusammenhang mit der Konstruktion künstlicher Intelligenz gefragt: In Suchmaschinen können Fakten extrahiert, die Tonalität des Textes, die maschinelle Übersetzung und der Dialog bewertet werden. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/io/mf/1e/iomf1eh6ihzqi5tjglwbtoljvvy.jpeg"></div><br>  Wir werden über die letzten beiden Bereiche sprechen: Sie haben eine reiche Geschichte und haben die Sprachverarbeitung erheblich beeinflusst.  Darüber hinaus werden wir uns gemeinsam mit der Sprecherin unseres Kurses <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AI Weekend, der</a> Computerlinguistin Anna Vlasova, mit den grundlegenden Möglichkeiten der Verarbeitung natürlicher Sprache beim Erstellen eines Chat-Bots befassen. <br><a name="habracut"></a><br><h2>  Wie hat alles angefangen? </h2><br>  Der erste Vortrag über die Verarbeitung natürlicher Sprache mit einem Computer begann in den 30er Jahren des 20. Jahrhunderts mit Ayers philosophischem Denken - er schlug vor, eine intelligente Person mithilfe eines empirischen Tests von einer dummen Maschine zu unterscheiden.  1950 schlug Alan Turing in der philosophischen Zeitschrift <i>Mind</i> einen Test vor, bei dem der Richter feststellen muss, mit wem er spricht: einer Person oder einem Computer.  Mit dem Test wurden Kriterien für die Bewertung der Arbeit der künstlichen Intelligenz festgelegt, die Möglichkeit ihrer Konstruktion wurde nicht in Frage gestellt.  Der Test hat viele Einschränkungen und Nachteile, hatte jedoch erhebliche Auswirkungen auf die Entwicklung von Chat-Bots. <br>  Der erste Bereich, in dem die Sprachverarbeitung erfolgreich angewendet wurde, war die maschinelle Übersetzung.  1954 demonstrierte die Georgetown University zusammen mit IBM ein maschinelles Übersetzungsprogramm von Russisch nach Englisch, das auf der Grundlage eines Wörterbuchs mit 250 Wörtern und eines Satzes von 6 Grammatikregeln arbeitete.  Das Programm war weit entfernt von dem, was man eigentlich als maschinelle Übersetzung bezeichnen könnte, und übersetzte 49 vorgewählte Angebote bei einer Demonstration.  Bis Mitte der 60er Jahre wurden viele Versuche unternommen, ein voll funktionsfähiges Übersetzungsprogramm zu erstellen. 1966 erklärte die Beratende Kommission für die automatische Verarbeitung der Sprache <i>(ALPAC)</i> die maschinelle Übersetzung für eine vergebliche Richtung.  Die staatlichen Subventionen wurden für einige Zeit eingestellt, das öffentliche Interesse an maschineller Übersetzung nahm ab, aber die Forschung hörte hier nicht auf. <br><br><img src="https://habrastorage.org/webt/3o/c3/ut/3oc3ut5jg4nlp6_jx_wehn_tbha.jpeg"><br><br>  Parallel zu den Versuchen, einem Computer das Übersetzen von Text beizubringen, dachten Wissenschaftler und ganze Universitäten daran, einen Roboter zu entwickeln, der das menschliche Sprachverhalten nachahmen kann.  Die erste erfolgreiche Implementierung des Chatbots war der virtuelle Gesprächspartner ELIZA, der 1966 von Joseph Weizenbaum geschrieben wurde.  Eliza parodierte das Verhalten des Psychotherapeuten, extrahierte wichtige Wörter aus dem Satz des Gesprächspartners und stellte eine Gegenfrage.  Wir können davon ausgehen, dass dies der erste Chat-Bot war, der auf Regeln basiert (regelbasierter Bot), und er hat den Grundstein für eine ganze Klasse solcher Systeme gelegt.  Interviewer wie Cleverbot, WeChat Xiaoice, Eugene Goostman - der den Turing-Test 2014 offiziell bestanden hat - und sogar Siri, Jarvis und Alexa wären ohne Eliza nicht erschienen. <br>  1968 entwickelte Terry Grapes das SHRDLU-Programm in LISP.  Sie bewegte einfache Objekte auf Befehl: Kegel, Würfel, Kugeln und konnte den Kontext unterstützen - sie verstand, welches Element bewegt werden musste, wenn es zuvor erwähnt wurde.  Der nächste Schritt bei der Entwicklung von Chat-Bots war das ALICE-Programm, für das Richard Wallace eine spezielle Auszeichnungssprache entwickelte - AIML <i>(English Artificial Intelligence Markup Language)</i> .  Dann, 1995, wurden die Erwartungen an den Chatbot überbewertet: Sie dachten, ALICE wäre noch schlauer als eine Person.  Natürlich gelang es dem Chatbot nicht, klüger zu sein, und für einige Zeit war das Geschäft mit Chatbots enttäuscht, und die Investoren gingen lange Zeit aus dem Thema der virtuellen Assistenten aus. <br><br><h2>  Sprache ist wichtig </h2><br>  Noch heute arbeiten Chatbots auf der Grundlage einer Reihe von Regeln und Verhaltensszenarien. Eine natürliche Sprache ist jedoch unscharf und mehrdeutig. Ein Gedanke kann viele Darstellungsmöglichkeiten haben. Daher hängt der kommerzielle Erfolg von Dialogsystemen von der Lösung von Sprachverarbeitungsproblemen ab.  Der Maschine muss beigebracht werden, die gesamte Vielfalt der eingehenden Fragen klar zu klassifizieren und klar zu interpretieren. <br>  Alle Sprachen sind unterschiedlich angeordnet, und dies ist sehr wichtig für das Parsen.  Unter dem Gesichtspunkt der morphologischen Zusammensetzung können sich die wesentlichen Elemente des Wortes nacheinander mit der Wurzel verbinden, wie beispielsweise in den türkischen Sprachen, oder sie können die Wurzel brechen, wie in Arabisch und Hebräisch.  Unter dem Gesichtspunkt der Syntax erlauben einige Sprachen die freie Reihenfolge von Wörtern in einer Phrase, während andere strenger organisiert sind.  In klassischen Systemen spielt die Wortreihenfolge eine wesentliche Rolle.  Für moderne statistische NLP-Methoden hat es keinen solchen Wert, da die Verarbeitung nicht auf der Ebene von Wörtern, sondern von ganzen Sätzen erfolgt. <br>  Weitere Schwierigkeiten bei der Entwicklung von Chat-Bots ergeben sich im Zusammenhang mit der Entwicklung der mehrsprachigen Kommunikation.  Heutzutage kommunizieren Menschen oft nicht in ihrer Muttersprache, sondern verwenden Wörter falsch.  Zum Beispiel sollten wir in der Formulierung „Ich habe vor zwei Tagen versendet, aber Waren kamen nicht“ aus Sicht des Wortschatzes über die Lieferung von physischen Objekten, zum Beispiel Waren, sprechen und nicht über die elektronische Geldtransaktion, die durch diese Wörter von einer Person beschrieben wird, die nicht spricht in der Muttersprache.  In der realen Kommunikation versteht eine Person den Gesprächspartner jedoch korrekt, und der Chat-Bot kann Probleme haben.  Bei bestimmten Themen wie Investitionen, Bankwesen oder IT wechseln die Benutzer häufig in andere Sprachen.  Es ist jedoch unwahrscheinlich, dass der Chatbot versteht, worum es geht, da er höchstwahrscheinlich in einer Sprache trainiert wird. <br><br><h2>  Erfolgsgeschichte: Maschinelle Übersetzer </h2><br>  Vor dem Aufkommen von Sprachassistenten und der weit verbreiteten Verbreitung von Chatbots war die maschinelle Übersetzung die am meisten nachgefragte intellektuelle Aufgabe, die die Verarbeitung einer natürlichen Sprache erforderte.  Das Gespräch über neuronale Netze und tiefes Lernen ging bis in die 90er Jahre zurück, und der erste Mark-1-Neurocomputer erschien 1958 im Allgemeinen.  Überall war es jedoch aufgrund der geringen Leistung der Computer und des Mangels an ausreichendem Sprachkorpus nicht möglich, sie zu verwenden.  Nur große Forschungsteams konnten es sich leisten, auf dem Gebiet der neuronalen Netze zu forschen. <br>  Maschinenübersetzer in der Mitte des 20. Jahrhunderts waren weit entfernt von Google Translate und Yandex.Translator, aber mit jeder neuen Methode der Übersetzung kamen Ideen auf, die auch heute noch in der einen oder anderen Form angewendet wurden. <br>  <b>1970</b> Regelbasierte maschinelle Übersetzung <i>(RBMT)</i> war der erste Versuch, einer Maschine das Übersetzen beizubringen.  Die Übersetzung wurde wie in einem Fünftklässler mit einem Wörterbuch erhalten, aber in der einen oder anderen Form werden die Regeln für einen maschinellen Übersetzer oder Chat-Bot immer noch verwendet. <br>  <b>1984</b> Beispielbasierte maschinelle Übersetzung <i>(EBMT)</i> konnte sogar völlig unterschiedliche Sprachen übersetzen, wobei es sinnlos war, Regeln festzulegen.  Alle modernen Maschinenübersetzer und Chat-Bots verwenden vorgefertigte Beispiele und Muster. <br>  <b>1990. Die</b> statistische maschinelle Übersetzung <i>(English SMT)</i> im Zeitalter der Entwicklung des Internets ermöglichte es, nicht nur fertige Sprachkorps, sondern auch Bücher und frei übersetzte Artikel zu verwenden.  Mehr verfügbare Daten erhöhten die Qualität der Übersetzung.  Statistische Methoden werden jetzt aktiv in der Sprachverarbeitung eingesetzt. <br><br><h2>  Neuronale Netze im Dienste von NLP </h2><br>  Mit der Entwicklung der Verarbeitung natürlicher Sprache wurden viele Probleme mit klassischen statistischen Methoden und vielen Regeln gelöst, aber dies löste nicht das Problem der Unschärfe und Mehrdeutigkeit in der Sprache.  Wenn wir ohne Kontext „Verbeugung“ sagen, ist es unwahrscheinlich, dass selbst ein lebender Gesprächspartner versteht, was gesagt wird.  Die Semantik des Wortes im Text wird durch die benachbarten Wörter bestimmt.  Aber wie kann man das einer Maschine erklären, wenn sie nur eine numerische Darstellung versteht?  So wurde die statistische Textanalysemethode <b>word2vec</b> <i>(englisches Wort zu Vektor) geboren</i> . <br><br><img src="https://habrastorage.org/webt/qq/mg/pi/qqmgpienoiwac9fvlk7eetfui6y.jpeg"><br>  <i>Die Vektoren bow_1 und bow_2 sind parallel, daher ist dies ein Wort und bow_3 ist ein Homonym.</i> <br><br>  Die Idee ist aus dem Namen ziemlich offensichtlich: das Wort in Form eines Vektors mit Koordinaten (x <sub>1</sub> , x <sub>2</sub> , ..., x <sub>n</sub> ) <sub>darzustellen</sub> .  Um die Homonymie zu bekämpfen, werden dieselben Wörter mit dem Tag "bow_1", "bow_2" usw. verbunden.  Wenn die Vektoren bow_n und bow_m parallel sind, können sie als ein Wort betrachtet werden.  Ansonsten sind diese Wörter Homonyme.  Am Ausgang hat jedes Wort seine eigene Vektordarstellung im mehrdimensionalen Raum (die Dimension des Vektorraums kann zwischen 50 und 1000 variieren). <br><br><img src="https://habrastorage.org/webt/jv/dz/so/jvdzsopezq1vldijw0ewlbvnreg.jpeg"><br><br>  Es bleibt die Frage, welche Art von neuronalem Netzwerk zum Trainieren eines bedingten Chat-Bots verwendet werden soll.  Konsistenz ist in der menschlichen Sprache wichtig: Wir ziehen Schlussfolgerungen und treffen Entscheidungen auf der Grundlage dessen, was im vorherigen Satz oder sogar im Absatz erwähnt wurde.  Ein wiederkehrendes neuronales Netzwerk (RNN) ist für diese Kriterien perfekt. Wenn jedoch der Abstand zwischen den verbundenen Teilen des Textes zunimmt, muss die Größe des RNN vergrößert werden, was zu einer Verringerung der Qualität der Informationsverarbeitung führt.  Dieses Problem wird durch das LSTM-Netzwerk <i>(English Long Short Term Memory)</i> gelöst.  Es hat ein wichtiges Merkmal - den Zustand der Zelle, der konstant bleiben oder sich bei Bedarf ändern kann.  Somit gehen die Informationen in der Kette nicht verloren, was für die Verarbeitung der natürlichen Sprache entscheidend ist. <br>  Heute gibt es eine Vielzahl von Bibliotheken zur Verarbeitung natürlicher Sprache.  Wenn wir über die Python-Sprache sprechen, die häufig für die Datenanalyse verwendet wird, dann sind dies <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NLTK</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Spacy</a> .  Große Unternehmen beteiligen sich auch an der Entwicklung von Bibliotheken für NLP, wie beispielsweise <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NLP Architect</a> von Intel oder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PyTorch</a> von Forschern von Facebook und Uber.  Trotz des großen Interesses großer Unternehmen an den Methoden der Sprachverarbeitung für neuronale Netze werden kohärente Dialoge hauptsächlich auf der Grundlage klassischer Methoden aufgebaut, und das neuronale Netzwerk spielt eine unterstützende Rolle bei der Lösung der Probleme der Sprachvorverarbeitung und -klassifizierung. <br><br><h2>  Wie kann NLP im Geschäftsleben eingesetzt werden? </h2><br>  Zu den offensichtlichsten Anwendungen für die Verarbeitung natürlicher Sprache gehören maschinelle Übersetzer, Chat-Bots und Sprachassistenten - etwas, dem wir jeden Tag begegnen.  Die meisten Call-Center-Mitarbeiter können durch virtuelle Assistenten ersetzt werden, da sich etwa 80% der Kundenanfragen an Banken auf recht typische Probleme beziehen.  Der Chatbot wird auch das erste Interview des Kandidaten ruhig bewältigen und es bei einem „Live“ -Treffen aufzeichnen.  Seltsamerweise ist die Rechtsprechung eine ziemlich genaue Richtung, so dass der Chat-Bot auch hier ein erfolgreicher Berater werden kann. <br><br><img src="https://habrastorage.org/webt/3o/mf/oy/3omfoy_r7md9i_tq4ftz4vfs5ze.jpeg"><br><br>  Die b2c-Richtung ist nicht die einzige, in der Chat-Bots verwendet werden können.  In großen Unternehmen ist die Mitarbeiterrotation sehr aktiv, sodass jeder bei der Anpassung an das neue Umfeld helfen muss.  Da die Fragen des neuen Mitarbeiters ziemlich typisch sind, kann der gesamte Prozess leicht automatisiert werden.  Es ist nicht erforderlich, eine Person zu suchen, die erklärt, wie der Drucker betankt wird, und an die Sie sich bei Problemen wenden können.  Der interne Chat-Bot des Unternehmens wird damit gut zurechtkommen. <br><br>  Mit NLP können Sie die Benutzerzufriedenheit mit einem neuen Produkt genau messen, indem Sie Bewertungen im Internet analysieren.  Wenn das Programm die Überprüfung als negativ identifiziert hat, wird der Bericht automatisch an die entsprechende Abteilung gesendet, in der bereits lebende Personen damit arbeiten. <br><br>  Die Möglichkeiten der Sprachverarbeitung werden sich nur erweitern und damit auch den Anwendungsbereich.  Wenn 40 Personen im Call Center Ihres Unternehmens arbeiten, sollten Sie überlegen: Vielleicht ist es besser, sie durch ein Team von Programmierern zu ersetzen, die einen Chat-Bot für Sie zusammenstellen. <br><br>  Weitere Informationen zu den Möglichkeiten der Sprachverarbeitung finden Sie in unserem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AI Weekend-</a> Kurs, in dem Anna Vlasova im Rahmen des Themas Künstliche Intelligenz ausführlich über Chat-Bots spricht. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de422609/">https://habr.com/ru/post/de422609/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de422595/index.html">Russische Wissenschaftler haben einen Motor für Cubesat mit einer 40% igen Alkohollösung entwickelt</a></li>
<li><a href="../de422597/index.html">Backend United # 2: Füllung</a></li>
<li><a href="../de422601/index.html">Chrome 69 verfügt über einen Zufallskennwortgenerator</a></li>
<li><a href="../de422603/index.html">Ein bisschen näher an der Perfektion</a></li>
<li><a href="../de422605/index.html">Schon in jungen Jahren einen Safe aufbauen - Bildungsprogramm von Rostelecom und MIPT</a></li>
<li><a href="../de422611/index.html">Standard Error Handler in RxJava2 oder warum RxJava Anwendungsabstürze verursacht, selbst wenn onError implementiert ist</a></li>
<li><a href="../de422613/index.html">PowerPool Cybergroup hat die Zero-Day-Sicherheitsanfälligkeit in Advanced Local Procedure Call gemeistert</a></li>
<li><a href="../de422615/index.html">Die ganze Wahrheit über RTOS. Artikel 9. Scheduler: Implementierung</a></li>
<li><a href="../de422617/index.html">Die ganze Wahrheit über RTOS. Artikel 8. Nucleus SE: Internes Design und Bereitstellung</a></li>
<li><a href="../de422623/index.html">So sichern Sie C.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>