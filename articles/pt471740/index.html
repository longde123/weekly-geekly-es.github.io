<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèª üè† üë®‚Äçüëß‚Äçüëß Servi√ßo de cache inteligente baseado em ZeroMQ e Tarantool üè¥‚Äç‚ò†Ô∏è üêó ‚òÉÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ruslan Aromatov, desenvolvedor chefe, CID 



 Ol√° Habr! Trabalho como desenvolvedor back-end no Moscow Credit Bank e, durante o meu trabalho, adquiri...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Servi√ßo de cache inteligente baseado em ZeroMQ e Tarantool</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mkb/blog/471740/">  <b>Ruslan Aromatov, desenvolvedor chefe, CID</b> <br><br><img src="https://habrastorage.org/webt/ld/1c/ck/ld1cckil16z47pv5vgyjgi7kwa8.png"><br><br>  Ol√° Habr!  Trabalho como desenvolvedor back-end no Moscow Credit Bank e, durante o meu trabalho, adquiri alguma experi√™ncia que gostaria de compartilhar com a comunidade.  Hoje vou contar como criamos nosso pr√≥prio servi√ßo de cache para os servidores frontais de nossos clientes usando o aplicativo m√≥vel MKB Online.  Este artigo pode ser √∫til para aqueles que est√£o envolvidos no design de servi√ßos e est√£o familiarizados com a arquitetura de microsservi√ßos, o banco de dados Tarantool na mem√≥ria e a biblioteca ZeroMQ.  No artigo, praticamente n√£o haver√° exemplos de c√≥digo e explica√ß√£o do b√°sico, mas apenas uma descri√ß√£o da l√≥gica dos servi√ßos e sua intera√ß√£o com um exemplo espec√≠fico que vem trabalhando em nossa batalha h√° mais de dois anos. <br><a name="habracut"></a><br><h4>  Como tudo come√ßou </h4><br>  Cerca de 6 anos atr√°s, o esquema era simples.  Como um legado da empresa de terceiriza√ß√£o, temos dois clientes de banco m√≥vel para iOS e Android, al√©m de um servidor de frente para atend√™-los.  O servidor em si foi escrito em java, foi para o back-end de diferentes maneiras (principalmente soap) e se comunicou com os clientes transmitindo xml via https. <br><br>  Os aplicativos clientes foram capazes de autenticar de alguma forma, mostrar uma lista de produtos e ... eles pareciam capazes de fazer algumas transfer√™ncias e pagamentos, mas na verdade eles n√£o o faziam muito bem e nem sempre.  Portanto, o servidor frontal n√£o teve um grande n√∫mero de usu√°rios ou cargas s√©rias (o que, no entanto, n√£o impediu que ele ca√≠sse uma vez a cada dois dias). <br><br>  √â claro que n√≥s (e na √©poca nossa equipe era composta por quatro pessoas), como respons√°veis ‚Äã‚Äãpelo banco m√≥vel, n√£o se encaixavam nessa situa√ß√£o e, para come√ßar, colocamos em ordem os aplicativos atuais, mas o servidor frontal acabou sendo muito ruim, por isso tinha que ser reescreva rapidamente o todo, substituindo simultaneamente xml por json e movendo-se para o servidor de aplicativos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">WildFly</a> .  Por alguns anos, a refatora√ß√£o n√£o se baseia em um post separado, pois tudo foi feito principalmente para garantir que o sistema funcionasse de maneira est√°vel. <br><br>  Gradualmente, os aplicativos e o servidor desenvolvidos come√ßaram a trabalhar mais est√°veis, e suas funcionalidades estavam em constante expans√£o, o que valeu a pena - havia cada vez mais usu√°rios. <br><br>  Ao mesmo tempo, come√ßaram a surgir quest√µes como toler√¢ncia a falhas, redund√¢ncia, replica√ß√£o e - assustador pensar - a carga alta. <br><br>  Uma solu√ß√£o r√°pida para o problema foi adicionar um segundo servidor WildFly, e os aplicativos aprenderam a alternar entre eles.  O problema do trabalho simult√¢neo com sess√µes do cliente foi resolvido pelo m√≥dulo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Infinispan</a> integrado ao WildFly. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/li/xt/wx/lixtwxkd09us1bfivvdxjv-dxrk.png" alt="Como era antes"></div><br>  Parecia que a vida estava melhorando ... <br><br><h4>  Voc√™ n√£o pode viver assim </h4><br>  No entanto, essa op√ß√£o de trabalhar com sess√µes n√£o foi isenta de desvantagens.  Vou mencionar aqueles que n√£o nos agradam. <br><br><ol><li>  Perda de sess√µes.  O menos importante.  Por exemplo, um aplicativo envia duas solicita√ß√µes para o servidor-1: a primeira solicita√ß√£o √© autentica√ß√£o e a segunda √© uma solicita√ß√£o para uma lista de contas.  A autentica√ß√£o √© bem-sucedida, uma sess√£o √© criada no servidor-1.  No momento, a segunda solicita√ß√£o do cliente √© interrompida repentinamente devido √† falta de comunica√ß√£o e o aplicativo alterna para o servidor-2, reenviando o encaminhamento da segunda solicita√ß√£o.  Mas, em uma determinada carga de trabalho, o Infinispan pode n√£o ter tempo para sincronizar dados entre os n√≥s.  Como resultado, o servidor-2 n√£o pode verificar a sess√£o do cliente, envia uma resposta irritada ao cliente, o cliente est√° triste e termina sua sess√£o.  O usu√°rio precisa fazer login novamente.  Triste </li><li>  Reiniciar o servidor tamb√©m pode causar perda de sess√µes.  Por exemplo, ap√≥s uma atualiza√ß√£o (e isso acontece com bastante frequ√™ncia).  Quando o servidor-2 √© iniciado, ele n√£o funcionar√° at√© que os dados sejam sincronizados com o servidor-1.  Parece que o servidor foi iniciado, mas na verdade n√£o deve aceitar solicita√ß√µes.  Isso √© inconveniente. </li><li>  Este √© um m√≥dulo incorporado do WildFly que nos impede de sair deste servidor de aplicativos em dire√ß√£o a microsservi√ßos. </li></ol><br>  A partir daqui, uma lista do que gostar√≠amos era de alguma forma formada por ela mesma. <br><br><ol><li>  Queremos armazenar sess√µes do cliente para que qualquer servidor (n√£o importa quantos existam) imediatamente ap√≥s o lan√ßamento tenha acesso a eles. </li><li>  Queremos armazenar quaisquer dados do cliente entre solicita√ß√µes (por exemplo, par√¢metros de pagamento e tudo mais). </li><li>  Queremos salvar todos os dados arbitr√°rios em uma chave arbitr√°ria em geral. </li><li>  E tamb√©m queremos receber dados do cliente antes da autentica√ß√£o passar.  Por exemplo, o usu√°rio √© autenticado e todos os seus produtos est√£o ali, frescos e quentes. </li><li>  E queremos escalar de acordo com a carga. </li><li>  E execute na janela de encaixe, escreva logs em uma √∫nica pilha e conte m√©tricas, etc. </li><li>  Ah, sim, e para que tudo funcione rapidamente. </li></ol><br><h4>  Farinha de escolha </h4><br>  Anteriormente, n√£o implement√°vamos a arquitetura de microsservi√ßos, ent√£o, para come√ßar, nos sentamos para ler, assistir e experimentar op√ß√µes diferentes.  Ficou claro imediatamente que precis√°vamos de um reposit√≥rio r√°pido e algum tipo de complemento que lidasse com a l√≥gica comercial e seja a interface de acesso ao reposit√≥rio.  Al√©m disso, seria bom agilizar o transporte r√°pido entre os servi√ßos. <br><br>  Eles escolheram por um longo tempo, discutiram muito e experimentaram.  Agora n√£o descreverei os pr√≥s e os contras de todos os candidatos, isso n√£o se aplica ao t√≥pico deste artigo, apenas digo que o armazenamento ser√° <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tarantool</a> , escreveremos nosso servi√ßo em java e o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ZeroMQ</a> funcionar√° como transporte.  Nem vou argumentar que a escolha √© muito amb√≠gua, mas foi amplamente influenciada pelo fato de n√£o gostarmos de estruturas grandes e pesadas (por seu peso e lentid√£o), solu√ß√µes in a box (por sua versatilidade e falta de personaliza√ß√£o), mas ao mesmo tempo Gostamos de controlar todas as partes do nosso sistema, tanto quanto poss√≠vel.  E para controlar o trabalho dos servi√ßos, escolhemos o servidor de coleta de m√©tricas do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Prometheus</a> com seus agentes convenientes que podem ser incorporados a praticamente qualquer c√≥digo.  Os logs de tudo isso v√£o para a pilha ELK. <br><br>  Bem, parece-me que j√° havia muita teoria. <br><br><h4>  Iniciar e terminar </h4><br>  O resultado do projeto foi aproximadamente esse esquema. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/he/kw/oh/hekwohpipjcqh4ufeoykog6f8cc.png" alt="Como queremos"></div><br>  <b>Armazenamento</b> <br><br>  Deve ser o mais est√∫pido poss√≠vel, apenas para armazenar dados e seus estados atuais, mas sempre funciona sem reiniciar.  Projetado para atender diferentes vers√µes de servidores frontais.  Mantemos todos os dados na mem√≥ria, recupera√ß√£o em caso de reinicializa√ß√£o atrav√©s de arquivos .snap e .xlog. <br><br>  Tabela (espa√ßo) para sess√µes do cliente: <br><br><ul><li>  ID da sess√£o </li><li>  ID do cliente; </li><li>  vers√£o (servi√ßo) </li><li>  hora da atualiza√ß√£o (timestamp); </li><li>  tempo de vida (ttl); </li><li>  dados de sess√£o serializados. </li></ul><br>  Tudo √© simples aqui: o cliente √© autenticado, o servidor frontal cria uma sess√£o e a salva no armazenamento, lembrando a hora.  A cada solicita√ß√£o de dados, o tempo √© atualizado, para que a sess√£o seja mantida ativa.  Se, mediante solicita√ß√£o, os dados estiverem desatualizados (ou n√£o haver√° nenhum), retornaremos um c√≥digo de retorno especial, ap√≥s o qual o cliente encerrar√° sua sess√£o. <br><br>  Tabela de cache simples (para qualquer dado da sess√£o): <br><br><ul><li>  chave; </li><li>  ID da sess√£o </li><li>  tipo de dados armazenados (n√∫mero arbitr√°rio); </li><li>  hora da atualiza√ß√£o (timestamp); </li><li>  tempo de vida (ttl); </li><li>  dados serializados. </li></ul><br>  Tabela de dados do cliente que precisam ser aquecidos antes do login: <br><ul><li>  ID do cliente; </li><li>  ID da sess√£o </li><li>  vers√£o (servi√ßo) </li><li>  tipo de dados armazenados (n√∫mero arbitr√°rio); </li><li>  hora da atualiza√ß√£o (timestamp); </li><li>  condi√ß√£o; </li><li>  dados serializados. </li></ul><br>  Um campo importante aqui √© condi√ß√£o.  Na verdade, existem apenas dois deles - ocioso e atualizado.  Eles s√£o definidos por um servi√ßo sobreposto que vai ao back-end para dados do cliente, para que outra inst√¢ncia desse servi√ßo n√£o fa√ßa o mesmo trabalho (j√° in√∫til) e n√£o carregue o back-end. <br><br>  Tabela de dispositivos: <br><br><ul><li>  ID do cliente; </li><li>  ID do dispositivo </li><li>  hora da atualiza√ß√£o (timestamp); </li></ul><br>  A tabela de dispositivos √© necess√°ria para que, mesmo antes de o cliente se autenticar no sistema, descubra seu ID e comece a receber seus produtos (aquecendo o cache).  A l√≥gica √© a seguinte: a primeira entrada √© sempre fria, pois antes da autentica√ß√£o n√£o sabemos que tipo de cliente √© proveniente de um dispositivo desconhecido (os clientes m√≥veis sempre transmitem IDs de dispositivo em qualquer solicita√ß√£o).  Todas as entradas subseq√ºentes deste dispositivo ser√£o acompanhadas de um cache de aquecimento para o cliente associado a ele. <br><br>  O trabalho com dados √© isolado do servi√ßo java pelos procedimentos do servidor.  Sim, tive que aprender lua, mas n√£o demorou muito tempo.  Al√©m do pr√≥prio gerenciamento de dados, os procedimentos lua tamb√©m s√£o respons√°veis ‚Äã‚Äãpelo retorno de estados atuais, sele√ß√µes de √≠ndices, limpeza de registros obsoletos em processos em segundo plano (fibras) e a opera√ß√£o do servidor da web embutido pelo qual o acesso direto aos dados √© realizado.  Aqui est√° - a beleza de escrever tudo com as m√£os - a possibilidade de controle ilimitado.  Mas o menos √© o mesmo - voc√™ precisa escrever tudo sozinho. <br><br>  O pr√≥prio Tarantool trabalha em um cont√™iner de encaixe, todos os arquivos lua necess√°rios s√£o colocados l√° no est√°gio de montagem da imagem.  Toda a montagem atrav√©s de scripts gradle. <br><br>  Replica√ß√£o mestre-escravo.  No outro host, o mesmo cont√™iner √© executado exatamente como a r√©plica do armazenamento principal.  √â necess√°rio no caso de uma falha de emerg√™ncia do mestre - os servi√ßos java passam para o escravo e se tornam o mestre.  H√° um terceiro escravo por precau√ß√£o.  No entanto, mesmo uma perda completa de dados no nosso caso √© triste, mas n√£o fatal.  De acordo com o pior cen√°rio, os usu√°rios precisar√£o efetuar login e recuperar todos os dados que voltar√£o ao cache. <br><br>  <b>Servi√ßo Java</b> <br><br>  Projetado como um microsservi√ßo sem estado t√≠pico.  Ele n√£o possui configura√ß√£o, todos os par√¢metros necess√°rios (e existem 6) s√£o passados ‚Äã‚Äãpelas vari√°veis ‚Äã‚Äãde ambiente ao criar o cont√™iner do docker.  Ele funciona com o servidor frontal atrav√©s do transporte ZeroMQ (org.zeromq.jzmq - a interface java do libzmq.so.5.1.1 nativo, que n√≥s pr√≥prios criamos) usando nosso pr√≥prio protocolo.  Ele funciona com uma tar√¢ntula atrav√©s de um conector java (org.tarantool.connector). <br><br>  A inicializa√ß√£o do servi√ßo √© bastante simples: <br><br><ul><li>  Iniciamos um logger (log4j2); </li><li>  A partir das vari√°veis ‚Äã‚Äãde ambiente (estamos na janela de encaixe), lemos os par√¢metros necess√°rios para o trabalho; </li><li>  Iniciamos o servidor de m√©tricas (jetty); </li><li>  Conecte-se √† tar√¢ntula (assincronamente); </li><li>  Iniciamos o n√∫mero necess√°rio de manipuladores de threads (trabalhadores); </li><li>  Iniciamos um broker (zmq) - um ciclo intermin√°vel de processamento de mensagens. </li></ul><br>  De todas as alternativas acima, apenas o mecanismo de processamento de mensagens √© interessante.  Abaixo est√° um diagrama do microsservi√ßo. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/my/fd/ef/myfdef3ggy0oerhyfvec3iozvwc.png" alt="L√≥gica do Message Broker"></div><br>  Vamos come√ßar com o in√≠cio do corretor.  Nosso broker √© um conjunto de zmq-sockets do tipo ROUTER, que aceita conex√µes de v√°rios clientes e √© respons√°vel pelo envio de mensagens provenientes deles. <br><br>  No nosso caso, temos um soquete de escuta na interface externa que recebe mensagens de clientes usando o protocolo tcp e o outro recebe mensagens de threads de trabalho usando o protocolo inproc (√© muito mais r√°pido que o tcp). <br><br><pre><code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">/** //   (   ,   ) ZContext zctx = new ZContext(); //    ZMQ.Socket clientServicePoint = zctx.createSocket(ZMQ.ROUTER); //    ZMQ.Socket workerServicePoint= zctx.createSocket(ZMQ.ROUTER); //     clientServicePoint.bind("tcp://*:" + Config.ZMQ_LISTEN_PORT); //     workerServicePoint.bind("inproc://worker-proc");</span></span></code> </pre> <br>  Ap√≥s inicializar os soquetes, iniciamos um loop de eventos sem fim. <br><br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">/** *      */</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">int</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">run</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> status;  <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> {   ZMQ.Poller poller = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ZMQ.Poller(<span class="hljs-number"><span class="hljs-number">2</span></span>);    poller.register(workerServicePoint, ZMQ.Poller.POLLIN);    poller.register(clientServicePoint, ZMQ.Poller.POLLIN);    <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> rc;    <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">true</span></span>) {      <span class="hljs-comment"><span class="hljs-comment">//        rc = poller.poll(POLL_INTERVAL);      if (rc == -1) {        status = -1;        logger.errorInternal("Broker run error rc = -1");        break; //  -     }    //     ()    if (poller.pollin(0)) {       processBackendMessage(ZMsg.recvMsg(workerServicePoint));    }    //        if (poller.pollin(1)) {       processFrontendMessage(ZMsg.recvMsg(clientServicePoint));    }    processQueueForBackend(); }  } catch (Exception e) {    status = -1;  } finally {    clientServicePoint.close();    workerServicePoint.close();  }  return status; }</span></span></code> </pre><br>  A l√≥gica do trabalho √© muito simples: recebemos mensagens de lugares diferentes e fazemos algo com eles.  Se algo gravemente falhou conosco, sa√≠mos do loop, que causa o travamento do processo, que ser√° reiniciado automaticamente pelo daemon do docker. <br><br>  A id√©ia principal √© que o broker n√£o lide com nenhuma l√≥gica comercial, ele apenas analisa o cabe√ßalho da mensagem e distribui tarefas para os threads de trabalho que foram lan√ßados anteriormente quando o servi√ßo foi iniciado.  Nisso, uma √∫nica fila de mensagens com prioriza√ß√£o de um comprimento fixo o ajuda. <br><br>  Vamos analisar o algoritmo usando o exemplo do esquema e c√≥digo acima. <br><br>  Ap√≥s o in√≠cio, os trabalhadores do encadeamento que iniciaram depois do intermedi√°rio s√£o inicializados e enviam uma mensagem de prontid√£o para o intermedi√°rio.  O intermedi√°rio os aceita, analisa-os e adiciona cada trabalhador √† lista. <br><br>  Um evento acontece no soquete do cliente - recebemos a mensagem1.  O broker chama o manipulador de mensagens recebidas, cuja tarefa √©: <br><br><ul><li>  an√°lise do cabe√ßalho da mensagem; </li><li>  colocar uma mensagem em um objeto titular com uma dada prioridade (com base na an√°lise do cabe√ßalho) e tempo de vida; </li><li>  colocando o titular na fila de mensagens; </li><li>  se a fila n√£o estiver cheia, a tarefa do manipulador terminou; </li><li>  se a fila estiver cheia, chamamos o m√©todo para enviar uma mensagem de erro ao cliente. </li></ul><br>  Na mesma itera√ß√£o do loop, chamamos o manipulador da fila de mensagens: <br><br><ul><li>  solicitamos a mensagem mais atual da fila (a fila decide isso sozinha, com base na prioridade e ordem de adi√ß√£o da mensagem); </li><li>  verifique a vida √∫til da mensagem (se ela expirou, chame o m√©todo para enviar uma mensagem de erro ao cliente); </li><li>  se a mensagem para processamento for relevante, tente preparar o primeiro trabalhador livre para trabalhar; </li><li>  se n√£o houver, coloque a mensagem de volta na fila (mais precisamente, apenas n√£o a exclua de l√°, ela ficar√° pendurada at√© a vida √∫til expirar); </li><li>  se temos um trabalhador pronto para o trabalho, marcamos como ocupado e enviamos uma mensagem para processamento; </li><li>  exclua a mensagem da fila. </li></ul><br>  Fazemos o mesmo com todas as mensagens subseq√ºentes.  O pr√≥prio operador de encadeamentos foi projetado da mesma maneira que um broker - ele tem o mesmo ciclo de processamento de mensagens sem fim.  Mas, como n√£o precisamos mais de processamento instant√¢neo, ele foi projetado para executar tarefas demoradas. <br><br>  Depois que o trabalhador concluiu sua tarefa (por exemplo, foi ao back-end dos produtos do cliente ou na tar√¢ntula da sess√£o), ele envia uma mensagem ao corretor, que o corretor envia de volta ao cliente.  O endere√ßo do cliente para quem a resposta deve ser enviada √© lembrado desde o momento em que a mensagem chega do cliente no objeto titular, que √© enviado ao trabalhador como uma mensagem em um formato ligeiramente diferente e depois retorna. <br><br>  O formato das mensagens que menciono constantemente √© nossa pr√≥pria produ√ß√£o.  Pronto, o ZeroMQ nos fornece as classes ZMsg - a pr√≥pria mensagem e o ZFrame - parte dessa mensagem, essencialmente apenas uma matriz de bytes, que eu posso usar livremente se houver esse desejo.  Nossa mensagem consiste em duas partes (dois ZFrames), o primeiro dos quais √© um cabe√ßalho bin√°rio e o segundo s√£o dados (o corpo da solicita√ß√£o, por exemplo, na forma de uma string json representada por uma matriz de bytes).  O cabe√ßalho da mensagem √© universal e viaja de cliente para servidor e de servidor para cliente. <br><br>  De fato, n√£o temos o conceito de "solicita√ß√£o" ou "resposta", apenas mensagens.  O cabe√ßalho cont√©m: vers√£o do protocolo, tipo de sistema (qual sistema √© endere√ßado), tipo de mensagem, c√≥digo de erro no n√≠vel de transporte (se n√£o for 0, algo aconteceu no mecanismo de transfer√™ncia de mensagens), ID da solicita√ß√£o (identificador de passagem proveniente do cliente - necess√°rio para rastreamento), ID da sess√£o do cliente (opcional), bem como um sinal de erro no n√≠vel dos dados (por exemplo, se a resposta de back-end n√£o puder ser analisada, definimos esse sinalizador para que o analisador no lado do cliente n√£o desserialize a resposta, mas receba dados de erro de outra maneira). <br><br>  Gra√ßas a um protocolo √∫nico entre todos os microsservi√ßos e esse cabe√ßalho, podemos simplesmente manipular os componentes de nossos servi√ßos.  Por exemplo, voc√™ pode levar o intermedi√°rio para um processo separado e transform√°-lo em um √∫nico intermedi√°rio de mensagens no n√≠vel de todo o sistema de microsservi√ßo.  Ou, por exemplo, execute trabalhadores n√£o na forma de encadeamentos dentro do processo, mas como processos independentes separados.  E enquanto o c√≥digo dentro deles n√£o muda.  Em geral, h√° margem para criatividade. <br><br><h4>  Um pouco sobre desempenho e recursos </h4><br>  O pr√≥prio broker √© r√°pido e a largura de banda total do servi√ßo √© limitada pela velocidade de back-end e pelo n√∫mero de trabalhadores.  Convenientemente, toda a quantidade necess√°ria de mem√≥ria √© alocada imediatamente no in√≠cio do servi√ßo e todos os threads s√£o iniciados imediatamente.  O tamanho da fila tamb√©m √© fixo.  No tempo de execu√ß√£o, apenas as mensagens est√£o sendo processadas. <br><br>  Como exemplo: al√©m do encadeamento principal, nosso servi√ßo de combate em cache atual lan√ßa outros encadeamentos de 100 trabalhadores, e o tamanho da fila √© limitado a tr√™s mil mensagens.  Em opera√ß√£o normal, cada inst√¢ncia processa at√© 200 mensagens por segundo e consome cerca de 250 MB de mem√≥ria e cerca de 2-3% da CPU.  √Äs vezes, nas cargas de pico, ele salta para 7-8%.  Tudo funciona em algum tipo de xeon virtual de n√∫cleo duplo. <br><br>  O trabalho regular do servi√ßo implica o emprego simult√¢neo de 3 a 5 trabalhadores (de 100) com o n√∫mero de mensagens na fila 0 (ou seja, eles passam para o processamento imediato).  Se o back-end come√ßar a diminuir, o n√∫mero de trabalhadores ocupados aumentar√° proporcionalmente ao tempo de sua resposta.  Nos casos em que ocorre um acidente e o back-end aumenta, todos os funcion√°rios primeiro terminam, ap√≥s o que a fila de mensagens come√ßa a entupir.  Quando entope completamente, come√ßamos a responder aos clientes com recusas de processar.  Ao mesmo tempo, n√£o come√ßamos a consumir recursos de mem√≥ria ou CPU, fornecendo m√©tricas de forma est√°vel e respondendo claramente aos clientes o que est√° acontecendo. <br><br>  A primeira captura de tela mostra o funcionamento regular do servi√ßo. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ji/ek/ap/jiekapcj1vylguqgijeheydw7nu.png" alt="O trabalho regular do servi√ßo"></div><br>  E no segundo, ocorreu um acidente - por algum motivo, o back-end n√£o respondeu em 30 segundos.  V√™-se que, a princ√≠pio, todos os trabalhadores acabaram, ap√≥s o que a fila de mensagens come√ßou a entupir. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/4v/he/h8/4vheh8hrqsinriaelyow6bgp9wu.png" alt="Acidente"></div><br><h4>  Testes de desempenho </h4><br>  Os testes sint√©ticos na minha m√°quina de trabalho (CentOS 7, Core i5, 16 GB de RAM) mostraram o seguinte. <br><br>  Trabalhe com o reposit√≥rio (gravando na tar√¢ntula e lendo imediatamente esse registro de 100 bytes de tamanho - simulando o trabalho com a sess√£o) - 12000 rps. <br><br>  Da mesma forma, apenas a velocidade foi medida n√£o entre o servi√ßo - pontos de tar√¢ntula, mas entre o cliente e o servi√ßo.  Claro, eu tive que escrever um cliente para testar o estresse.  Dentro de uma m√°quina, era poss√≠vel obter 7000 rps.  Em uma rede local (e temos muitas m√°quinas virtuais diferentes que n√£o est√£o claras quanto fisicamente conectadas), os resultados variam, mas √© poss√≠vel at√© 5000 rps para uma inst√¢ncia.  Deus sabe que tipo de desempenho, mas mais de dez vezes cobre nossos picos de carga.  E isso √© apenas se uma inst√¢ncia do servi√ßo estiver em execu√ß√£o, mas tivermos v√°rias delas, e a qualquer momento voc√™ poder√° executar quantas voc√™ precisar.  Quando os servi√ßos bloquearem a velocidade de armazenamento, ser√° poss√≠vel escalar a tar√¢ntula horizontalmente (fragmento com base no ID do cliente, por exemplo). <br><br><h4>  Intelig√™ncia de Servi√ßo </h4><br>  O leitor atento provavelmente j√° faz a pergunta - qual √© a ‚Äúesperteza‚Äù deste servi√ßo, mencionada no t√≠tulo.  J√° mencionei isso de passagem, mas agora vou lhe contar mais. <br><br>  Uma das principais tarefas do servi√ßo era reduzir o tempo necess√°rio para emitir seus produtos aos usu√°rios (listas de contas, cart√µes, dep√≥sitos, empr√©stimos, pacotes de servi√ßos etc.) enquanto reduz a carga no back-end (reduz o n√∫mero de solicita√ß√µes no Oracle grande e pesado) devido ao armazenamento em cache na tar√¢ntula. <br><br>  E ele fez isso muito bem.  A l√≥gica para aquecer o cache do cliente √© a seguinte: <br><br><ul><li>  o usu√°rio inicia o aplicativo m√≥vel; </li><li>  Uma solicita√ß√£o do AppStart contendo o ID do dispositivo √© enviada ao servidor frontal; </li><li>  o servidor frontal envia uma mensagem com esse ID para o servi√ßo de cache; </li><li>  o servi√ßo procura na tabela do dispositivo o ID do cliente para este dispositivo; </li><li>  se n√£o estiver l√°, nada acontece (a resposta nem √© enviada, o servidor n√£o espera); </li><li>  se o ID do cliente estiver localizado, o trabalhador criar√° um conjunto de mensagens para receber listas de produtos do usu√°rio que entram imediatamente em processamento pelo intermedi√°rio e s√£o distribu√≠dos aos trabalhadores no modo normal; </li><li>  cada trabalhador envia uma solicita√ß√£o para um determinado tipo de dados ao usu√°rio, colocando o status de "atualiza√ß√£o" no banco de dados (esse status protege o back-end de repetir as mesmas solicita√ß√µes se vierem de outras inst√¢ncias do servi√ßo); </li><li>  depois de receber os dados, eles s√£o registrados na tar√¢ntula; </li><li>  o usu√°rio efetua login no sistema e o aplicativo envia solicita√ß√µes para receber seus produtos, e o servidor envia essas solicita√ß√µes na forma de mensagens para o servi√ßo de cache; </li><li>  se os dados do usu√°rio j√° foram recebidos, simplesmente os enviamos do cache; </li><li>  se os dados estiverem sendo recebidos (status de "atualiza√ß√£o"), um ciclo de espera de dados ser√° iniciado dentro do trabalhador (√© igual ao tempo limite da solicita√ß√£o para o back-end); </li><li>  assim que os dados s√£o recebidos (ou seja, o status desse registro (tupla) na tabela passa para "ocioso", o servi√ßo os fornece ao cliente; </li><li>  se os dados n√£o forem recebidos dentro de um determinado intervalo de tempo, um erro ser√° retornado ao cliente. </li></ul><br>  Assim, na pr√°tica, conseguimos reduzir o tempo m√©dio de recebimento de produtos para o servidor frontal de 200 ms para 20 ms, ou seja, cerca de 10 vezes, e o n√∫mero de solicita√ß√µes para o back-end em cerca de 4 vezes. <br><br><h4>  Os problemas </h4><br>  O servi√ßo de cache trabalha em batalha h√° cerca de dois anos e atualmente satisfaz nossas necessidades. <br><br>  Obviamente, ainda existem problemas n√£o resolvidos, √†s vezes ocorrem problemas.  Os servi√ßos Java na batalha ainda n√£o ca√≠ram.  A tar√¢ntula caiu algumas vezes no SIGSEGV, mas era uma vers√£o antiga e, ap√≥s a atualiza√ß√£o, n√£o aconteceu novamente.  Durante o teste de estresse, a replica√ß√£o cai, ocorreu um cano quebrado no mestre, ap√≥s o qual o escravo caiu, embora o mestre continuasse a trabalhar.  Foi decidido reiniciando o escravo. <br><br>  Uma vez houve algum tipo de acidente no data center, e o sistema operacional (CentOS 7) parou de ver discos r√≠gidos.  O sistema de arquivos entrou no modo somente leitura.  O mais surpreendente foi que os servi√ßos continuaram funcionando, pois mantemos todos os dados na mem√≥ria.  A tar√¢ntula n√£o conseguiu gravar arquivos .xlog, ningu√©m registrou nada, mas de alguma forma tudo funcionou.  Mas a tentativa de reiniciar n√£o teve √™xito - ningu√©m poderia come√ßar. <br><br>  H√° um grande problema n√£o resolvido, e eu gostaria de ouvir a opini√£o da comunidade sobre esse assunto.  Quando a tar√¢ntula principal falha, os servi√ßos java podem mudar para o escravo, que continua a funcionar como mestre.  No entanto, isso s√≥ acontece se o mestre travar e n√£o puder funcionar. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/e6/pu/wh/e6puwh26kmnngnrxkvyelgvvite.png" alt="Problema n√£o resolvido"></div><br>  Suponha que tenhamos 3 inst√¢ncias de um servi√ßo que funcionem com dados em uma tar√¢ntula mestre.  Os servi√ßos em si n√£o caem, a replica√ß√£o do banco de dados est√° acontecendo, est√° tudo bem.  Mas, de repente, temos uma rede desmoronando entre o n√≥ 1 e o n√≥ 4, onde o assistente funciona.  O Servi√ßo 1 ap√≥s v√°rias tentativas malsucedidas decide alternar para o banco de dados de backup e come√ßa a enviar solicita√ß√µes para l√°. <br><br>  Imediatamente ap√≥s isso, o escravo da tar√¢ntula come√ßa a aceitar solicita√ß√µes de modifica√ß√£o de dados, como resultado da replica√ß√£o do mestre desmoronar e obtemos dados inconsistentes.  Ao mesmo tempo, os servi√ßos 2 e 3 funcionam perfeitamente com o mestre e o servi√ßo 1 se comunica bem com o ex-escravo.  √â claro que, neste caso, come√ßamos a perder sess√µes do cliente e outros dados, embora tudo funcione do lado t√©cnico.  Ainda n√£o resolvemos um problema t√£o potencial.  Felizmente, isso n√£o aconteceu em dois anos, mas a situa√ß√£o √© bastante real.  Agora, cada servi√ßo sabe o n√∫mero da loja para onde vai e temos um alerta para essa m√©trica, que funcionar√° ao mudar de mestre para escravo.  E voc√™ tem que reparar tudo com as m√£os.  Como voc√™ resolve esses problemas? <br><br><h4>  Planos </h4><br>  Planejamos trabalhar no problema descrito acima, limitando o n√∫mero de trabalhadores ocupados simultaneamente com um tipo de solicita√ß√£o, seguro (sem perder os pedidos atuais) para interromper o servi√ßo e aperfei√ßoar ainda mais. <br><br><h4>  Conclus√£o </h4><br>  Talvez isso seja tudo, embora eu tenha discutido o assunto superficialmente, mas a l√≥gica geral do trabalho deve ser clara.  Portanto, se poss√≠vel, estou pronto para responder nos coment√°rios.  Descrevi brevemente como um pequeno subsistema auxiliar dos servidores dianteiros do banco funciona para atender clientes m√≥veis. <br><br>  Se o t√≥pico for de interesse da comunidade, posso falar sobre v√°rias de nossas solu√ß√µes que contribuem para melhorar a qualidade do servi√ßo ao cliente do banco. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt471740/">https://habr.com/ru/post/pt471740/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt471724/index.html">Necessidades √°geis e cerebrais: gerenciamento do estresse</a></li>
<li><a href="../pt471726/index.html">M√©todo moderno para medir a resposta ao impulso e distor√ß√£o n√£o linear</a></li>
<li><a href="../pt471728/index.html">Avalonia meus pr√≥s e contras</a></li>
<li><a href="../pt471736/index.html">Sensor Ethernet sem contato</a></li>
<li><a href="../pt471738/index.html">Uma pequena hist√≥ria sobre como a conveni√™ncia √†s vezes dispara no joelho</a></li>
<li><a href="../pt471742/index.html">Sberbank AI Journey. Como ensinamos uma rede neural a fazer um exame</a></li>
<li><a href="../pt471744/index.html">Tarantool Data Grid: arquitetura e recursos</a></li>
<li><a href="../pt471746/index.html">Guia completo para a configura√ß√£o de cabe√ßalhos HTTP para seguran√ßa</a></li>
<li><a href="../pt471748/index.html">Otimiza√ß√£o de farm√°cia: o que fizemos com a matem√°tica</a></li>
<li><a href="../pt471750/index.html">Gerenciamento de acesso privilegiado como uma tarefa priorit√°ria em seguran√ßa da informa√ß√£o (por exemplo, Fudo PAM)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>