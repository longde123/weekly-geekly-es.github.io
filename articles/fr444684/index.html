<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§úüèΩ ‚úåüèæ üôå Nous travaillons avec les r√©seaux de neurones: une liste de contr√¥le pour le d√©bogage üë®üèæ‚Äçüç≥ üõê üçä</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Le code logiciel d'apprentissage automatique est souvent complexe et assez d√©routant. D√©tecter et √©liminer les bogues est une t√¢che gourmande en resso...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Nous travaillons avec les r√©seaux de neurones: une liste de contr√¥le pour le d√©bogage</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/skillbox/blog/444684/"><img src="https://habrastorage.org/getpro/habr/post_images/84a/7dc/4b8/84a7dc4b81799226681bb176f1c518b0.png" alt="image"><br><br>  Le code logiciel d'apprentissage automatique est souvent complexe et assez d√©routant.  D√©tecter et √©liminer les bogues est une t√¢che gourmande en ressources.  M√™me les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">r√©seaux de neurones √† connexion directe</a> les plus simples n√©cessitent une approche s√©rieuse de l'architecture du r√©seau, de l'initialisation des pond√©rations et de l'optimisation du r√©seau.  Une petite erreur peut entra√Æner des probl√®mes d√©sagr√©ables. <br><br>  Cet article concerne l'algorithme de d√©bogage de vos r√©seaux de neurones. <br><a name="habracut"></a><br><blockquote>  <b>Skillbox recommande:</b> Un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d√©veloppeur de</a> cours pratique <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Python √† partir de z√©ro</a> . <br><br>  <b>Nous vous rappelons:</b> <i>pour tous les lecteurs de ¬´Habr¬ª - une remise de 10 000 roubles lors de l'inscription √† un cours Skillbox en utilisant le code promo ¬´Habr¬ª.</i> </blockquote><cut></cut><br><h3>  L'algorithme comprend cinq √©tapes: </h3><br><ul><li>  d√©marrage simple; </li><li>  confirmation des pertes; </li><li>  v√©rification des r√©sultats interm√©diaires et des compos√©s; </li><li>  diagnostic des param√®tres; </li><li>  contr√¥le du travail. </li></ul><br>  Si quelque chose vous semble plus int√©ressant que les autres, vous pouvez acc√©der directement √† ces sections. <br><br><h3>  D√©marrage facile </h3><br>  Un r√©seau de neurones avec une architecture complexe, une r√©gularisation et un planificateur de vitesse d'apprentissage est plus difficile √† d√©marrer qu'un r√©seau r√©gulier.  Nous sommes un peu compliqu√©s ici, car l'√©l√©ment lui-m√™me a une relation indirecte avec le d√©bogage, mais c'est toujours une recommandation importante. <br><br>  Un d√©but simple consiste √† cr√©er un mod√®le simplifi√© et √† le former sur un ensemble de donn√©es (point). <br><br>  <b>Nous cr√©ons d'abord un mod√®le simplifi√©</b> <br><br>  Pour un d√©marrage rapide, cr√©ez un petit r√©seau avec une seule couche cach√©e et v√©rifiez que tout fonctionne correctement.  Ensuite, nous compliquons progressivement le mod√®le, v√©rifions chaque nouvel aspect de sa structure (couche suppl√©mentaire, param√®tre, etc.) et continuons. <br><br>  <b>Nous formons le mod√®le sur un seul ensemble de donn√©es (point)</b> <br><br>  Pour tester rapidement la sant√© de votre projet, vous pouvez utiliser un ou deux points de donn√©es pour la formation afin de v√©rifier si le syst√®me fonctionne correctement.  Le r√©seau neuronal doit montrer une pr√©cision de 100% de l'entra√Ænement et de la v√©rification.  Si ce n'est pas le cas, alors soit le mod√®le est trop petit soit vous avez d√©j√† un bug. <br><br>  M√™me si tout va bien, pr√©parez le mod√®le pour le passage d'une ou plusieurs √©poques avant de continuer. <br><br><h3>  Estimation des pertes </h3><br>  L'estimation des pertes est le principal moyen d'affiner les performances du mod√®le.  Vous devez vous assurer que la perte correspond √† la t√¢che et que les fonctions de perte sont √©valu√©es sur la bonne √©chelle.  Si vous utilisez plus d'un type de perte, assurez-vous qu'ils sont tous du m√™me ordre et correctement mis √† l'√©chelle. <br><br>  Il est important d'√™tre attentif aux pertes initiales.  V√©rifiez √† quel point le r√©sultat r√©el est proche de celui attendu si le mod√®le a commenc√© avec une hypoth√®se al√©atoire.  Le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">travail d'Andrei Karpati sugg√®re ce qui suit</a> : ¬´Assurez-vous d'obtenir le r√©sultat attendu lorsque vous commencez √† travailler avec un petit nombre de param√®tres.  Il est pr√©f√©rable de v√©rifier imm√©diatement la perte de donn√©es (avec un degr√© de r√©gularisation d√©fini sur z√©ro).  Par exemple, pour CIFAR-10 avec le classificateur Softmax, nous nous attendons √† ce que la perte initiale soit de 2,302, parce que la probabilit√© diffuse attendue est de 0,1 pour chaque classe (car il y a 10 classes), et la perte de Softmax est la probabilit√© logarithmique n√©gative de la classe correcte comme - ln (0,1) = 2,302. " <br><br>  Pour un exemple binaire, un calcul similaire est simplement effectu√© pour chacune des classes.  Voici, par exemple, les donn√©es: 20% 0 et 80% 1.  La perte initiale attendue atteindra ‚Äì0,2 ln (0,5) ‚Äì0,8 ln (0,5) = 0,693147.  Si le r√©sultat est sup√©rieur √† 1, cela peut indiquer que les poids du r√©seau neuronal ne sont pas correctement √©quilibr√©s ou que les donn√©es ne sont pas normalis√©es. <br><br><h3>  V√©rification des r√©sultats interm√©diaires et des connexions </h3><br>  Pour d√©boguer un r√©seau neuronal, il est n√©cessaire de comprendre la dynamique des processus au sein du r√©seau et le r√¥le des couches interm√©diaires individuelles, car elles sont connect√©es.  Voici quelques erreurs courantes que vous pourriez rencontrer: <br><br><ul><li>  Expressions incorrectes pour les mises √† jour de d√©grad√© </li><li>  les mises √† jour de poids ne s'appliquent pas; </li><li>  dispara√Ætre ou exploser des gradients. </li></ul><br>  Si les valeurs du gradient sont nulles, cela signifie que la vitesse d'apprentissage dans l'optimiseur est trop lente ou que vous avez rencontr√© une expression incorrecte pour mettre √† jour le gradient. <br><br>  De plus, il est n√©cessaire de surveiller les valeurs des fonctions d'activation, les poids et les mises √† jour de chacune des couches.  Par exemple, la valeur des mises √† jour des param√®tres (poids et d√©calages) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">doit √™tre 1-e3</a> . <br><br>  Il y a un ph√©nom√®ne appel√© ¬´Dying ReLU¬ª ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">¬´Disappearing Gradient Problem¬ª</a> lorsque les neurones ReLU sortiront z√©ro apr√®s avoir √©tudi√© la grande valeur de biais n√©gatif pour leurs poids.  Ces neurones ne sont plus jamais r√©activ√©s dans aucun emplacement de donn√©es. <br><br>  Vous pouvez utiliser des tests de gradient pour d√©tecter ces erreurs en approximant le gradient √† l'aide d'une approche num√©rique.  S'il est proche des gradients calcul√©s, alors la propagation inverse a √©t√© correctement mise en ≈ìuvre.  Pour cr√©er une v√©rification de gradient, consultez ces excellentes ressources CS231 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> , ainsi que le didacticiel d'Andrew Nga sur ce sujet. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Fayzan Sheikh</a> souligne trois m√©thodes principales pour visualiser un r√©seau neuronal: <br><br><ul><li>  Pr√©liminaire - m√©thodes simples qui nous montrent la structure g√©n√©rale du mod√®le entra√Æn√©.  Ils incluent la sortie de formes ou de filtres de couches individuelles du r√©seau neuronal et les param√®tres de chaque couche. </li><li>  Bas√© sur l'activation.  On y d√©chiffre l'activation de neurones individuels ou de groupes de neurones afin de comprendre leurs fonctions. </li><li>  Bas√© sur un d√©grad√©.  Ces m√©thodes ont tendance √† manipuler les gradients qui se forment √† partir du passage dans les deux sens lors de l'apprentissage du mod√®le (y compris les cartes de signification et les cartes d'activation de classe). </li></ul><br>  Il existe plusieurs outils utiles pour visualiser les activations et les connexions de couches individuelles, par exemple <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="noopener">ConX</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="noopener">Tensorboard</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d86/8ff/f33/d868fff33e5b7cb69563bfee29b9db08.png"><br><br><h3>  Diagnostic des param√®tres </h3><br>  Les r√©seaux de neurones ont de nombreux param√®tres qui interagissent entre eux, ce qui complique l'optimisation.  En fait, cette section fait l'objet d'une recherche active par des sp√©cialistes, donc les propositions ci-dessous ne doivent √™tre consid√©r√©es que comme des conseils, les points de d√©part sur lesquels vous pouvez vous appuyer. <br><br>  Taille du lot - Si vous souhaitez que la taille du paquet soit suffisamment grande pour fournir des estimations pr√©cises du gradient d'erreur, mais suffisamment petite pour que la descente du gradient stochastique (SGD) rationalise votre r√©seau.  La petite taille des packages entra√Ænera une convergence rapide due au bruit dans le processus d'apprentissage et √† l'avenir √† des difficult√©s d'optimisation.  Ceci est d√©crit plus en d√©tail <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . <br><br>  <b>Vitesse d'apprentissage</b> - Trop lent entra√Ænera une convergence lente ou le risque d'√™tre coinc√© dans les bas locaux.  Dans le m√™me temps, une vitesse d'apprentissage √©lev√©e entra√Ænera une divergence dans l'optimisation, car vous risquez de "sauter" √† travers la partie profonde, mais en m√™me temps √©troite de la fonction de perte.  Essayez d'utiliser la planification de la vitesse pour la r√©duire pendant l'entra√Ænement du r√©seau neuronal.  CS231n <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">a une grande section sur cette question</a> . <br><br>  <b>√âcr√™tage de gradient</b> - coupe les gradients des param√®tres pendant la propagation arri√®re √† la valeur maximale ou √† la norme limite.  Utile pour r√©soudre les probl√®mes de d√©grad√©s explosifs que vous pouvez rencontrer dans le troisi√®me paragraphe. <br><br>  <b>Normalisation par lots</b> - utilis√©e pour normaliser les donn√©es d'entr√©e de chaque couche, ce qui permet de r√©soudre le probl√®me de d√©calage covariant interne.  Si vous utilisez Dropout et Batch Norma ensemble, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">consultez cet article</a> . <br><br>  <b>Descente de gradient stochastique (SGD)</b> - Il existe plusieurs vari√©t√©s de SGD qui utilisent l'√©lan, les vitesses d'apprentissage adaptatives et la m√©thode de Nesterov.  Dans le m√™me temps, aucun d'entre eux n'a un avantage clair en termes d'efficacit√© de formation et de g√©n√©ralisation ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d√©tails ici</a> ). <br><br>  <b>R√©gularisation</b> - est cruciale pour la construction d'un mod√®le g√©n√©ralis√©, car elle ajoute une p√©nalit√© pour la complexit√© du mod√®le ou des valeurs de param√®tres extr√™mes.  C'est un moyen de r√©duire la variance du mod√®le sans augmenter significativement son d√©placement.  Plus d' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">informations ici</a> . <br><br>  Pour tout √©valuer vous-m√™me, vous devez d√©sactiver la r√©gularisation et v√©rifier vous-m√™me le gradient de perte de donn√©es. <br><br>  <b>Le d√©crochage</b> est un autre moyen de rationaliser votre r√©seau pour √©viter la congestion.  Pendant l'entra√Ænement, la perte se produit uniquement en maintenant l'activit√© du neurone avec une certaine probabilit√© p (hyperparam√®tre) ou en la mettant √† z√©ro dans le cas contraire.  Par cons√©quent, le r√©seau doit utiliser un sous-ensemble diff√©rent de param√®tres pour chaque formation, ce qui r√©duit les changements de certains param√®tres qui deviennent dominants. <br><br>  Important: si vous utilisez √† la fois l'abandon et la normalisation par lots, faites attention √† l'ordre de ces op√©rations ou m√™me √† leur utilisation conjointe.  Tout cela est encore activement discut√© et compl√©t√©.  Voici deux discussions importantes sur ce sujet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sur Stackoverflow</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Arxiv</a> . <br><br><h3>  Contr√¥le du travail </h3><br>  Il s'agit de documenter les workflows et les exp√©riences.  Si vous ne documentez rien, vous pouvez oublier, par exemple, quel type de vitesse d'entra√Ænement ou de poids de classe est utilis√©.  Gr√¢ce au contr√¥le, vous pouvez facilement visualiser et reproduire les exp√©riences pr√©c√©dentes.  Cela r√©duit le nombre d'exp√©riences en double. <br><br>  Il est vrai que la documentation manuelle peut √™tre difficile en cas de grande quantit√© de travail.  Des outils tels que Comet.ml viennent √† la rescousse ici, aidant √† enregistrer automatiquement les ensembles de donn√©es, les modifications de code, l'historique des exp√©riences et les mod√®les de production, y compris les informations cl√©s sur votre mod√®le (hyperparam√®tres, indicateurs de performance du mod√®le et informations environnementales). <br><br>  Un r√©seau de neurones peut √™tre tr√®s sensible aux petits changements, ce qui entra√Ænera une diminution des performances du mod√®le.  Le suivi et la documentation du travail est la premi√®re √©tape √† suivre pour normaliser votre environnement et votre mod√©lisation. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0e0/52c/de4/0e052cde4414da934bdde111cc0f0bf5.png"><br><br>  J'esp√®re que ce message pourra devenir le point de d√©part √† partir duquel vous commencerez √† d√©boguer votre r√©seau neuronal. <br><blockquote>  <b>Skillbox recommande:</b> <br><br><ul><li>  Cours pratique de deux ans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">"Je suis un d√©veloppeur web PRO</a> . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">"</a> </li><li>  Cours en ligne <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">"D√©veloppeur C # avec 0"</a> . </li><li>  Cours pratique annuel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">"D√©veloppeur PHP de 0 √† PRO"</a> . <br></li></ul></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr444684/">https://habr.com/ru/post/fr444684/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr444672/index.html">Roskachestvo a pr√©sent√© une √©valuation des √©couteurs filaires et sans fil disponibles en Russie</a></li>
<li><a href="../fr444674/index.html">Sony Xperia 10 et Xperia 10 Plus - smartphones √† vue large</a></li>
<li><a href="../fr444676/index.html">√âvaluation CRM, tops, avis - mentent-ils tous?</a></li>
<li><a href="../fr444678/index.html">Jour de disponibilit√©: 12 avril, vol normal</a></li>
<li><a href="../fr444682/index.html">Les stocks de Sony et Nintendo tombent en panne apr√®s le lancement du streaming vid√©o pour les joueurs de Google</a></li>
<li><a href="../fr444686/index.html">Waves Smart Assets: listes noires et blanches, trading par intervalles</a></li>
<li><a href="../fr444688/index.html">Veuillez arr√™ter de parler du mod√®le de r√©f√©rentiel avec Eloquent</a></li>
<li><a href="../fr444690/index.html">Comment les chercheurs d'Uber appliquent et mettent √† l'√©chelle les connaissances sur le comportement humain</a></li>
<li><a href="../fr444692/index.html">MOSDROID # 16 Soufre √† Redmadrobot</a></li>
<li><a href="../fr444694/index.html">Comme nous l'avions pr√©dit, la sortie comme une catastrophe naturelle</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>