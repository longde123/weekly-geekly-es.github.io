<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üèÇüèº üö∂üèø üí™ Migration de Mongo √† Postgres: l'exp√©rience du journal The Guardian ü•ù üç≠ üê∑</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The Guardian est l'un des plus grands journaux britanniques, il a √©t√© fond√© en 1821. Pendant pr√®s de 200 ans d'existence, les archives ont accumul√© un...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Migration de Mongo √† Postgres: l'exp√©rience du journal The Guardian</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/itsumma/blog/436416/"><img src="https://habrastorage.org/webt/4w/zu/h3/4wzuh3ik2zdn_awvt9l2rhd4oia.jpeg" alt="image"><br><br>  The Guardian est l'un des plus grands journaux britanniques, il a √©t√© fond√© en 1821.  Pendant pr√®s de 200 ans d'existence, les archives ont accumul√© une bonne quantit√©.  Heureusement, tout n'est pas stock√© sur le site - au cours des deux derni√®res d√©cennies seulement.  La base de donn√©es, que les Britanniques ont eux-m√™mes qualifi√©e de "source de v√©rit√©" pour tout le contenu en ligne, contient environ 2,3 millions d'√©l√©ments.  Et √† un moment donn√©, ils ont r√©alis√© la n√©cessit√© de migrer de Mongo vers Postgres SQL - apr√®s une chaude journ√©e de juillet 2015, les proc√©dures de basculement ont √©t√© s√©v√®rement test√©es.  La migration a pris pr√®s de 3 ans! .. <br><br>  Nous avons traduit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">un article</a> qui d√©crit le d√©roulement du processus de migration et les difficult√©s rencontr√©es par les administrateurs.  Le processus est long, mais le r√©sum√© est simple: passer √† la grosse t√¢che, concilier que des erreurs seront n√©cessaires.  Mais finalement, 3 ans plus tard, des coll√®gues britanniques ont r√©ussi √† c√©l√©brer la fin de la migration.  Et dors. <br><a name="habracut"></a><br><h4>  <b>Premi√®re partie: le d√©but</b> </h4><br>  Chez Guardian, la plupart du contenu, y compris les articles, les blogs, les galeries de photos et les vid√©os, est produit au sein de notre propre CMS, Composer.  Jusqu'√† r√©cemment, Composer travaillait avec Mongo DB bas√© sur AWS.  Cette base de donn√©es √©tait essentiellement une "source de v√©rit√©" pour tout le contenu en ligne du Guardian - environ 2,3 millions d'√©l√©ments.  Et nous venons de terminer la migration de Mongo vers Postgres SQL. <br><br>  Composer et ses bases de donn√©es √©taient initialement h√©berg√©s sur le Guardian Cloud, un centre de donn√©es situ√© au sous-sol de notre bureau pr√®s de Kings Cross, avec basculement ailleurs √† Londres.  Par une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">chaude journ√©e de juillet 2015,</a> nos proc√©dures de basculement ont √©t√© soumises √† un test assez s√©v√®re. <br><br><img src="https://habrastorage.org/webt/ij/nu/wx/ijnuwxcxdtlrnupaxikvcqq7ofe.jpeg" alt="image"><br>  <i>Chaleur: bon pour danser √† la fontaine, d√©sastreux pour le centre de donn√©es.</i>  <i>Photo: Sarah Lee / Guardian</i> <br><br>  Depuis lors, la migration de Guardian vers AWS est devenue une question de vie ou de mort.  Pour migrer vers le cloud, nous avons d√©cid√© d'acheter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OpsManager</a> , le logiciel de gestion Mongo DB, et avons sign√© un contrat de support technique Mongo.  Nous avons utilis√© OpsManager pour g√©rer les sauvegardes, orchestrer et surveiller notre cluster de bases de donn√©es. <br><br>  En raison d'exigences √©ditoriales, nous devions d√©marrer le cluster de base de donn√©es et OpsManager sur notre propre infrastructure dans AWS, et ne pas utiliser la solution g√©r√©e Mongo.  Nous avons d√ª transpirer, car Mongo ne fournissait aucun outil pour une configuration facile sur AWS: nous avons con√ßu manuellement toute l'infrastructure et √©crit des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">centaines de scripts Ruby</a> pour installer des agents de surveillance / automatisation et orchestrer de nouvelles instances de base de donn√©es.  En cons√©quence, nous avons d√ª organiser une √©quipe de programmes √©ducatifs sur la gestion des bases de donn√©es dans l'√©quipe - ce que nous esp√©rions qu'OpsManager assumerait. <br><br>  Depuis la transition vers AWS, nous avons eu deux pannes importantes en raison de probl√®mes de base de donn√©es, chacune n'ayant pas permis la publication sur theguardian.com pendant au moins une heure.  Dans les deux cas, ni OpsManager ni le personnel de support technique de Mongo n'ont pu nous fournir une assistance suffisante, et nous avons r√©solu le probl√®me nous-m√™mes - dans un cas, gr√¢ce √† un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">membre de notre √©quipe</a> qui a r√©ussi √† faire face √† la situation par t√©l√©phone depuis le d√©sert √† la p√©riph√©rie d'Abu Dhabi. <br><br>  Chacune des questions probl√©matiques m√©rite un article s√©par√©, mais voici les points g√©n√©raux: <br><br><ul><li>  Faites tr√®s attention au temps - ne bloquez pas l'acc√®s √† votre VPC √† un point tel que NTP cesse de fonctionner. </li><li>  La cr√©ation automatique d'index de base de donn√©es au d√©marrage de l'application est une mauvaise id√©e. </li><li>  La gestion des bases de donn√©es est extr√™mement importante et difficile - et nous ne voudrions pas le faire nous-m√™mes. </li></ul><br>  OpsManager n'a pas tenu sa promesse d'une gestion simple des bases de donn√©es.  Par exemple, la gestion r√©elle d'OpsManager elle-m√™me - en particulier, la mise √† jour d'OpsManager version 1 vers la version 2 - a n√©cessit√© beaucoup de temps et des connaissances particuli√®res sur notre configuration OpsManager.  Il n'a pas non plus tenu sa promesse de ¬´mises √† jour en un clic¬ª en raison de changements dans le sch√©ma d'authentification entre les diff√©rentes versions de Mongo DB.  Nous avons perdu au moins deux mois d'ing√©nieurs par an pour g√©rer la base de donn√©es. <br><br>  Tous ces probl√®mes, combin√©s aux frais annuels importants que nous avons pay√©s pour le contrat de support et OpsManager, nous ont oblig√©s √† rechercher une option de base de donn√©es alternative avec les caract√©ristiques suivantes: <br><br><ul><li>  Effort minimal pour g√©rer la base de donn√©es. </li><li>  Chiffrement au repos. </li><li>  Un chemin de migration acceptable avec Mongo. </li></ul><br>  √âtant donn√© que tous nos autres services ex√©cutent AWS, le choix √©vident est Dynamo, la base de donn√©es NoSQL d'Amazon.  Malheureusement, √† l'√©poque, Dynamo ne prenait pas en charge le chiffrement au repos.  Apr√®s avoir attendu environ neuf mois pour que cette fonctionnalit√© soit ajout√©e, nous avons fini par abandonner cette id√©e en d√©cidant d'utiliser Postgres sur AWS RDS. <br>  "Mais Postgres n'est pas un r√©f√©rentiel de documents!"  - vous √™tes indign√© ... Eh bien, oui, ce n'est pas un d√©p√¥t Dock, mais il a des tables similaires aux colonnes JSONB, avec un support pour les index dans les champs de l'outil JSON Blob.  Nous esp√©rions qu'avec JSONB, nous pourrions migrer de Mongo vers Postgres avec des changements minimes dans notre mod√®le de donn√©es.  De plus, si nous voulions passer √† un mod√®le plus relationnel √† l'avenir, nous aurions une telle opportunit√©.  Une autre grande chose √† propos de Postgres est son bon fonctionnement: pour chaque question que nous avions, dans la plupart des cas, la r√©ponse √©tait d√©j√† donn√©e dans Stack Overflow. <br><br>  En termes de performances, nous √©tions s√ªrs que Postgres pouvait le faire: Composer est un outil exclusivement pour l'enregistrement de contenu (il √©crit dans la base de donn√©es chaque fois qu'un journaliste arr√™te d'imprimer), et g√©n√©ralement le nombre d'utilisateurs simultan√©s ne d√©passe pas plusieurs centaines - ce qui ne n√©cessite pas de syst√®me super haute puissance! <br><br><h4>  <b>Deuxi√®me partie: la migration de contenu de deux d√©cennies s'est d√©roul√©e sans interruption</b> </h4><br>  <b>Plan</b> <br><br>  La plupart des migrations de bases de donn√©es impliquent les m√™mes actions, et la n√¥tre ne fait pas exception.  Voici ce que nous avons fait: <br><br><ul><li>  Cr√©ation d'une nouvelle base de donn√©es. </li><li>  Ils ont cr√©√© un moyen d'√©crire dans une nouvelle base de donn√©es (nouvelle API). </li><li>  Nous avons cr√©√© un serveur proxy qui envoie du trafic √† la fois √† l'ancienne et √† la nouvelle base de donn√©es, en utilisant l'ancienne comme principale. </li><li>  Enregistrements d√©plac√©s de l'ancienne base de donn√©es vers la nouvelle. </li><li>  Ils ont fait de la nouvelle base de donn√©es la principale. </li><li>  Suppression de l'ancienne base de donn√©es. </li></ul><br>  √âtant donn√© que la base de donn√©es vers laquelle nous avons migr√© assurait le fonctionnement de notre CMS, il √©tait essentiel que la migration cause le moins de probl√®mes possible √† nos journalistes.  En fin de compte, les nouvelles ne finissent jamais. <br><br>  <b>Nouvelle API</b> <br><br>  Les travaux sur la nouvelle API bas√©e sur Postgres ont commenc√© fin juillet 2017.  Ce fut le d√©but de notre voyage.  Mais pour comprendre comment c'√©tait, nous devons d'abord clarifier o√π nous avons commenc√©. <br><br>  Notre architecture CMS simplifi√©e ressemblait √† ceci: une base de donn√©es, une API et plusieurs applications qui y sont li√©es (comme une interface utilisateur).  La pile a √©t√© construite et fonctionne depuis 4 ans sur la base de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Scala</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Scalatra Framework</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Angular.js</a> . <br><br>  Apr√®s quelques analyses, nous sommes arriv√©s √† la conclusion qu'avant de pouvoir migrer le contenu existant, nous avons besoin d'un moyen de contacter la nouvelle base de donn√©es PostgreSQL, en maintenant l'ancienne API op√©rationnelle.  Apr√®s tout, Mongo DB est notre ¬´source de v√©rit√©¬ª.  Elle nous a servi de bou√©e de sauvetage pendant que nous exp√©rimentions la nouvelle API. <br><br>  C'est l'une des raisons pour lesquelles la construction au-dessus de l'ancienne API ne faisait pas partie de nos plans.  La s√©paration des fonctions dans l'API d'origine √©tait minime, et les m√©thodes sp√©cifiques n√©cessaires pour travailler sp√©cifiquement avec Mongo DB pouvaient √™tre trouv√©es m√™me au niveau du contr√¥leur.  En cons√©quence, la t√¢che d'ajouter un autre type de base de donn√©es √† une API existante √©tait trop risqu√©e. <br><br>  Nous sommes all√©s dans l'autre sens et avons dupliqu√© l'ancienne API.  Ainsi naquit APIV2.  C'√©tait une copie plus ou moins exacte de l'ancienne API li√©e √† Mongo, et comprenait les m√™mes points de terminaison et fonctionnalit√©s.  Nous avons utilis√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">doobie</a> , la couche de fonctionnalit√©s JDBC pure pour Scala, ajout√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Docker</a> pour ex√©cuter et tester localement, et am√©lior√© la journalisation des op√©rations et le partage des responsabilit√©s.  APIV2 √©tait cens√© √™tre une version rapide et moderne de l'API. <br><br>  Fin ao√ªt 2017, nous avions d√©ploy√© une nouvelle API utilisant PostgreSQL comme base de donn√©es.  Mais ce n'√©tait que le d√©but.  Il y a des articles dans Mongo DB qui ont √©t√© cr√©√©s il y a plus de deux d√©cennies, et ils ont tous d√ª migrer vers la base de donn√©es Postgres. <br><br>  <b>La migration</b> <br><br>  Nous devrions √™tre en mesure de modifier n'importe quel article sur le site, quel que soit le moment o√π il a √©t√© publi√©. Par cons√©quent, tous les articles existent dans notre base de donn√©es en tant que ¬´source de v√©rit√©¬ª unique. <br><br>  Bien que tous les articles vivent dans l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">API Content Guardian (CAPI)</a> , qui sert les applications et le site, il √©tait extr√™mement important pour nous de migrer sans aucun probl√®me, car notre base de donn√©es est notre ¬´source de v√©rit√©¬ª.  Si quelque chose arrivait au cluster Elasticsearch CAPI, nous le r√©indexions √† partir de la base de donn√©es Composer. <br>  Par cons√©quent, avant de d√©sactiver Mongo, nous devions nous assurer que la m√™me demande pour l'API ex√©cut√©e sur Postgres et l'API ex√©cut√©e sur Mongo renverrait des r√©ponses identiques. <br>  Pour ce faire, nous devions copier tout le contenu dans la nouvelle base de donn√©es Postgres.  Cela a √©t√© fait √† l'aide d'un script qui interagissait directement avec l'ancienne et la nouvelle API.  L'avantage de cette m√©thode √©tait que les deux API fournissaient d√©j√† une interface bien test√©e pour lire et √©crire des articles dans et hors des bases de donn√©es, par opposition √† √©crire quelque chose qui acc√©dait directement aux bases de donn√©es respectives. <br><br>  L'ordre de migration de base √©tait le suivant: <br><br><ul><li>  Obtenez du contenu de Mongo. </li><li>  Publiez du contenu sur Postgres. </li><li>  Obtenez du contenu de Postgres. </li><li>  Assurez-vous que leurs r√©ponses sont identiques. </li></ul><br>  La migration de base de donn√©es ne peut √™tre consid√©r√©e comme r√©ussie que si les utilisateurs finaux n'ont pas remarqu√© que cela s'est produit, et un bon script de migration sera toujours la cl√© d'un tel succ√®s.  Nous avions besoin d'un script qui pourrait: <br><br><ul><li>  Ex√©cutez les requ√™tes HTTP. </li><li>  Assurez-vous qu'apr√®s la migration d'une partie du contenu, la r√©ponse des deux API correspond. </li><li>  Arr√™tez en cas d'erreur. </li><li>  Cr√©ez un journal des op√©rations d√©taill√© pour diagnostiquer les probl√®mes. </li><li>  Red√©marrez apr√®s une erreur du point correct. </li></ul><br>  Nous avons commenc√© par utiliser de l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ammonite</a> .  Il vous permet d'√©crire des scripts dans le langage Scala, qui est au c≈ìur de notre √©quipe.  C'√©tait une bonne occasion d'exp√©rimenter avec quelque chose que nous n'avions pas utilis√© auparavant pour voir si cela nous serait utile.  Bien que l'ammonite nous permette d'utiliser un langage familier, nous avons constat√© plusieurs lacunes dans son travail.  Intellij <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">prend</a> actuellement en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">charge</a> Ammonite, mais il ne l'a pas fait lors de notre migration - et nous avons perdu l'auto-compl√©tion et l'importation automatique.  De plus, pendant une longue p√©riode, le script Ammonite n'a pas pu s'ex√©cuter. <br>  En fin de compte, Ammonite n'√©tait pas le bon outil pour ce travail, et nous avons plut√¥t utilis√© le projet sbt pour effectuer la migration.  Cela nous a permis de travailler dans une langue dans laquelle nous avions confiance, ainsi que d'effectuer plusieurs ¬´migrations de test¬ª avant de se lancer dans l'environnement de travail principal. <br><br>  L'inattendu √©tait son utilit√© pour v√©rifier la version de l'API ex√©cut√©e sur Postgres.  Nous avons trouv√© plusieurs erreurs difficiles √† trouver et des cas limites que nous n'avons pas trouv√©s plus t√¥t. <br><br>  Avance rapide jusqu'en janvier 2018, quand il est temps de tester la migration compl√®te dans notre environnement CODE pr√©-prod. <br><br>  Comme la plupart de nos syst√®mes, la seule similitude entre CODE et PROD est la version de l'application lanc√©e.  L'infrastructure AWS prenant en charge CODE √©tait beaucoup moins puissante que PROD, tout simplement parce qu'elle re√ßoit beaucoup moins de charge de travail. <br><br>  Nous esp√©rions que la migration de test dans l'environnement CODE nous aiderait √†: <br><br><ul><li>  Estimez la dur√©e de la migration dans l'environnement PROD. </li><li>  √âvaluez comment (le cas √©ch√©ant) la migration affecte la productivit√©. </li></ul><br>  Afin d'obtenir des mesures pr√©cises de ces indicateurs, nous avons d√ª mettre les deux environnements en correspondance mutuelle compl√®te.  Cela comprenait la restauration d'une sauvegarde Mongo DB de PROD vers CODE et la mise √† niveau de l'infrastructure prise en charge par AWS. <br><br>  La migration d'un peu plus de 2 millions d'√©l√©ments de donn√©es aurait d√ª prendre beaucoup plus de temps qu'une journ√©e de travail standard ne le permettrait.  Par cons√©quent, nous avons ex√©cut√© le script √† l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">√©cran</a> pour la nuit. <br><br>  Pour mesurer la progression de la migration, nous avons envoy√© des requ√™tes structur√©es (√† l'aide de jetons) √† notre pile ELK (Elasticsearch, Logstash et Kibana).  √Ä partir de l√†, nous pourrions cr√©er des tableaux de bord d√©taill√©s en suivant le nombre d'articles transf√©r√©s avec succ√®s, le nombre de plantages et la progression globale.  De plus, tous les indicateurs √©taient affich√©s sur grand √©cran afin que toute l'√©quipe puisse voir les d√©tails. <br><br><img src="https://habrastorage.org/webt/wk/zh/uj/wkzhujequqzbqpf65ewnt4zcfbg.png" alt="image"><br>  <i>Tableau de bord montrant la progression de la migration: Outils √©ditoriaux / Guardian</i> <br><br>  Une fois la migration termin√©e, nous avons v√©rifi√© une correspondance pour chaque document dans Postgres et dans Mongo. <br><br><h4>  <b>Troisi√®me partie: procurations et lancement sur Prod</b> </h4><br>  <b>Procurations</b> <br><br>  Maintenant que la nouvelle API fonctionnant sur Postgres a √©t√© lanc√©e, nous devions la tester avec des mod√®les de trafic et d'acc√®s aux donn√©es r√©els pour garantir sa fiabilit√© et sa stabilit√©.  Il y avait deux fa√ßons possibles de proc√©der: mettre √† jour chaque client qui acc√®de √† l'API Mongo pour qu'il acc√®de aux deux API;  ou ex√©cutez un proxy qui le fera pour nous.  Nous avons √©crit des procurations sur Scala en utilisant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Akka Streams</a> . <br><br>  Le proxy √©tait assez simple: <br><br><ul><li>  Recevez le trafic de l'√©quilibreur de charge. </li><li>  Redirigez le trafic vers l'API principale et vice versa. </li><li>  Transf√©rez le m√™me trafic de mani√®re asynchrone vers une API suppl√©mentaire. </li><li>  Calculez les √©carts entre les deux r√©ponses et enregistrez-les dans un journal. </li></ul><br>  Initialement, le proxy a enregistr√© de nombreuses divergences, y compris certaines diff√©rences de comportement difficiles √† trouver mais importantes dans les deux API qui devaient √™tre corrig√©es. <br><br>  <b>Journalisation structur√©e</b> <br><br>  Chez Guardian, nous nous <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">connectons √†</a> l'aide de la pile <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ELK</a> (Elasticsearch, Logstash et Kibana).  L'utilisation de Kibana nous a permis de visualiser le magazine de la mani√®re la plus pratique pour nous.  Kibana utilise <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la syntaxe de requ√™te de Lucene</a> , qui est assez facile √† apprendre.  Mais nous nous sommes vite rendu compte qu'il √©tait impossible de filtrer ou de regrouper les entr√©es de journal dans la configuration actuelle.  Par exemple, nous n'avons pas pu filtrer ceux qui ont √©t√© envoy√©s √† la suite de demandes GET. <br><br>  Nous avons d√©cid√© d'envoyer des donn√©es plus structur√©es √† Kibana, pas seulement des messages.  Une entr√©e de journal contient plusieurs champs, par exemple, l'horodatage et le nom de la pile ou de l'application qui a envoy√© la demande.  L'ajout de nouveaux champs est tr√®s simple.  Ces champs structur√©s sont appel√©s marqueurs et peuvent √™tre impl√©ment√©s √† l'aide de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">biblioth√®que logstash-logback-encoder</a> .  Pour chaque demande, nous avons extrait des informations utiles (par exemple, itin√©raire, m√©thode, code d'√©tat) et cr√©√© une carte avec les informations suppl√©mentaires n√©cessaires pour le journal.  Voici un exemple: <br><br><pre><code class="plaintext hljs">import akka.http.scaladsl.model.HttpRequest import ch.qos.logback.classic.{Logger =&gt; LogbackLogger} import net.logstash.logback.marker.Markers import org.slf4j.{LoggerFactory, Logger =&gt; SLFLogger} import scala.collection.JavaConverters._ object Logging { val rootLogger: LogbackLogger = LoggerFactory.getLogger(SLFLogger.ROOT_LOGGER_NAME).asInstanceOf[LogbackLogger] private def setMarkers(request: HttpRequest) = { val markers = Map( "path" -&gt; request.uri.path.toString(), "method" -&gt; request.method.value ) Markers.appendEntries(markers.asJava) } def infoWithMarkers(message: String, akkaRequest: HttpRequest) = rootLogger.info(setMarkers(akkaRequest), message) }</code> </pre> <br>  Des champs suppl√©mentaires dans nos journaux nous ont permis de cr√©er des tableaux de bord informatifs et d'ajouter plus de contexte aux √©carts, ce qui nous a aid√©s √† identifier quelques incoh√©rences mineures entre les deux API. <br><br>  <b>R√©plication du trafic et refactoring du proxy</b> <br><br>  Apr√®s avoir transf√©r√© le contenu dans la base de donn√©es CODE, nous avons obtenu une copie presque exacte de la base de donn√©es PROD.  La principale diff√©rence √©tait que CODE n'avait pas de trafic.  Pour r√©pliquer le trafic r√©el vers l'environnement CODE, nous avons utilis√© l'outil open source <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">GoReplay</a> (ci <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">-</a> apr√®s <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d√©nomm√©</a> gor).  Il est tr√®s facile √† installer et flexible √† personnaliser selon vos besoins. <br><br>  √âtant donn√© que tout le trafic provenant de nos API est d'abord all√© vers des proxy, il √©tait logique d'installer gor sur des conteneurs proxy.  Voir ci-dessous comment charger gor dans votre conteneur et comment commencer √† surveiller le trafic sur le port 80 et √† l'envoyer √† un autre serveur. <br><br><pre> <code class="plaintext hljs"> wget https://github.com/buger/goreplay/releases/download/v0.16.0.2/gor_0.16.0_x64.tar.gz tar -xzf gor_0.16.0_x64.tar.gz gor sudo gor --input-raw :80 --output-http http://apiv2.code.co.uk</code> </pre><br>  Pendant un moment, tout a bien fonctionn√©, mais tr√®s vite il y a eu un dysfonctionnement lorsque le proxy est devenu indisponible pendant plusieurs minutes.  Dans l'analyse, nous avons constat√© que les trois conteneurs proxy se bloquaient p√©riodiquement en m√™me temps.  Au d√©but, nous pensions que le proxy plantait parce que gor utilisait trop de ressources.  Apr√®s une analyse plus approfondie de la console AWS, nous avons constat√© que les conteneurs proxy se bloquaient r√©guli√®rement, mais pas en m√™me temps. <br><br>  Avant d'approfondir le probl√®me, nous avons essay√© de trouver un moyen d'ex√©cuter gor, mais cette fois sans charge suppl√©mentaire sur le proxy.  La solution est venue de notre pile secondaire pour Composer.  Cette pile n'est utilis√©e qu'en cas d'urgence, et notre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">outil de surveillance de travail la</a> teste en permanence.  Cette fois, la lecture du trafic de cette pile vers CODE √† double vitesse a fonctionn√© sans aucun probl√®me. <br><br>  De nouveaux r√©sultats ont soulev√© de nombreuses questions.  Le proxy a √©t√© con√ßu comme un outil temporaire, il se peut donc qu'il n'ait pas √©t√© con√ßu avec autant de soin que d'autres applications.  De plus, il a √©t√© construit en utilisant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://doc.akka.io/docs/akka-">Akka Http</a> , qu'aucune de nos √©quipes ne connaissait.  Le code √©tait d√©sordonn√© et plein de correctifs rapides.  Nous avons d√©cid√© de commencer beaucoup de refactoring pour am√©liorer la lisibilit√©.  Cette fois, nous avons utilis√© des g√©n√©rateurs pour au lieu de la logique imbriqu√©e croissante que nous utilisions auparavant.  Et ajout√© encore plus de marqueurs de journalisation. <br><br>  Nous esp√©rions pouvoir emp√™cher le gel des conteneurs proxy si nous approfondissons ce qui se passe √† l'int√©rieur du syst√®me et simplifions la logique de son fonctionnement.  Mais cela n'a pas fonctionn√©.  Apr√®s deux semaines √† essayer de rendre le proxy plus fiable, nous nous sommes sentis pi√©g√©s.  Il fallait prendre une d√©cision.  Nous avons d√©cid√© de prendre le risque et de laisser le proxy tel quel, car il vaut mieux passer du temps sur la migration elle-m√™me que d'essayer de r√©parer un logiciel qui deviendra inutile dans un mois.  Nous avons pay√© pour cette solution avec deux autres √©checs - presque deux minutes chacun - mais cela devait √™tre fait. <br><br>  Avance rapide jusqu'en mars 2018, lorsque nous avons d√©j√† termin√© la migration vers CODE sans sacrifier les performances de l'API ou l'exp√©rience client dans CMS.  Maintenant, nous pourrions commencer √† penser √† annuler les procurations de CODE. <br><br>  La premi√®re √©tape a √©t√© de modifier les priorit√©s de l'API afin que le proxy interagisse d'abord avec Postgres.  Comme nous l'avons dit plus haut, cela a √©t√© d√©cid√© par une modification des param√®tres.  Cependant, il y avait une difficult√©. <br><br>  Composer envoie des messages au flux Kinesis apr√®s la mise √† jour du document.  Une seule API n√©cessaire pour envoyer des messages pour √©viter la duplication.  Pour cela, les API ont un indicateur dans la configuration: true pour l'API prise en charge par Mongo et false pour les Postgres pris en charge.  Changer simplement le proxy pour interagir avec Postgres en premier n'√©tait pas suffisant, car le message ne serait pas envoy√© au flux Kinesis avant que la demande n'atteigne Mongo.  Cela fait trop longtemps. <br><br>  Pour r√©soudre ce probl√®me, nous avons cr√©√© des points de terminaison HTTP pour modifier instantan√©ment la configuration de toutes les instances de l'√©quilibreur de charge √† la vol√©e.  Cela nous a permis de connecter l'API principale tr√®s rapidement sans avoir √† modifier le fichier de configuration et √† le red√©ployer.  De plus, cela peut √™tre automatis√©, r√©duisant ainsi l'interaction humaine et la probabilit√© d'erreurs. <br><br>  Maintenant, toutes les demandes ont d'abord √©t√© envoy√©es √† Postgres et API2 a interagi avec Kinesis.  Les remplacements pourraient √™tre rendus permanents avec des changements de configuration et un red√©ploiement. <br><br>  L'√©tape suivante consistait √† supprimer compl√®tement le proxy et √† forcer les clients √† acc√©der exclusivement √† l'API Postgres.  Comme nous avons de nombreux clients, la mise √† jour de chacun d'eux n'√©tait pas possible.  Par cons√©quent, nous avons √©lev√© cette t√¢che au niveau du DNS.  Autrement dit, nous avons cr√©√© un CNAME dans DNS qui pointait d'abord vers le proxy ELB et changerait pour pointer vers l'API ELB.  Cela a permis une seule modification au lieu de mettre √† jour chaque client API individuel. <br><br>  Il est temps de d√©placer le PROD.  M√™me si c'√©tait un peu effrayant, eh bien, parce que c'est le principal environnement de travail.  Le processus a √©t√© relativement simple, car tout a √©t√© d√©cid√© en modifiant les param√®tres.  De plus, comme un marqueur d'√©tape a √©t√© ajout√© aux journaux, il est devenu possible de re-profiler les tableaux de bord pr√©c√©demment construits en mettant simplement √† jour le filtre Kibana. <br><br>  <b>D√©sactiver les proxys et Mongo DB</b> <br><br>  Apr√®s 10 mois et 2,4 millions d'articles migr√©s, nous avons enfin pu d√©sactiver toutes les infrastructures li√©es √† Mongo.  Mais d'abord, nous devions faire ce que nous attendions tous: tuer le proxy. <br><br><img src="https://habrastorage.org/webt/6q/g5/ck/6qg5cksqesrnoig3gxnu7kfea-i.png" alt="image"><br>  <i>Journaux montrant la d√©sactivation du proxy API flexible.</i>  <i>Photographie: Outils √©ditoriaux / Guardian</i> <br><br>  Ce petit logiciel nous a caus√© tellement de probl√®mes que nous avons eu h√¢te de le d√©connecter bient√¥t!  Tout ce que nous avions √† faire √©tait de mettre √† jour l'enregistrement CNAME pour pointer directement vers l'√©quilibreur de charge APIV2. <br>  Toute l'√©quipe s'est r√©unie autour d'un ordinateur.  Il fallait faire une seule frappe.  Tout le monde a retenu son souffle!  Silence complet ... Cliquez!  Le travail est termin√©.  Et rien n'a vol√©!  Nous avons tous exhal√© joyeusement. <br><br>  Cependant, la suppression de l'ancienne API Mongo DB √©tait lourde d'un autre test.  D√©sesp√©r√© de supprimer l'ancien code, nous avons constat√© que nos tests d'int√©gration n'ont jamais √©t√© ajust√©s pour utiliser la nouvelle API.  Tout est rapidement devenu rouge.  Heureusement, la plupart des probl√®mes √©taient li√©s √† la configuration et nous les avons facilement r√©solus.  Il y a eu plusieurs probl√®mes avec les requ√™tes PostgreSQL qui ont √©t√© d√©tect√©s par les tests.  En r√©fl√©chissant √† ce qui pourrait √™tre fait pour √©viter cette erreur, nous avons appris une le√ßon: lors du d√©marrage d'une grosse t√¢che, conciliez qu'il y aura des erreurs. <br><br>  Apr√®s cela, tout s'est bien pass√©.  Nous avons d√©connect√© toutes les instances de Mongo d'OpsManager, puis les avons d√©connect√©es.  Il ne restait plus qu'√† c√©l√©brer.  Et dors. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr436416/">https://habr.com/ru/post/fr436416/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr436400/index.html">Configurer l'environnement de d√©veloppement pour l'apprentissage du HTML, CSS, PHP dans Windows</a></li>
<li><a href="../fr436404/index.html">√âquilibrage du trafic VoIP √† tol√©rance de pannes. Commutation de charge entre les centres de donn√©es aux heures de pointe</a></li>
<li><a href="../fr436406/index.html">Comment devenir d√©veloppeur de jeux si vous √™tes un agent immobilier</a></li>
<li><a href="../fr436408/index.html">Mod√©lisation num√©rique - l'histoire d'un projet</a></li>
<li><a href="../fr436412/index.html">Visite photo du nouveau bureau Facebook de Boston</a></li>
<li><a href="../fr436420/index.html">Le plus grand d√©potoir de l'histoire: 2,7 milliards de comptes, dont 773 millions sont uniques</a></li>
<li><a href="../fr436422/index.html">L'imitation ne peut pas √™tre une strat√©gie de d√©veloppement de produit.</a></li>
<li><a href="../fr436424/index.html">Petites cr√©atures, grandes actions: le r√¥le des coupeurs de feuilles dans l'effet de serre des n√©otropiques</a></li>
<li><a href="../fr436426/index.html">Suspendre l'application si la connexion r√©seau est perdue</a></li>
<li><a href="../fr436428/index.html">Pourquoi encourageons-nous la programmation sportive</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>