<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>📪 👩‍🍳 🍅 Comment comprendre Tensorflow et ne pas mourir, et même enseigner quelque chose sur une voiture 🉐 👩🏽‍🤝‍👩🏼 ♒️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, les gardes. La publication d'aujourd'hui portera sur la façon de ne pas se perdre dans la nature des nombreuses options d'utilisation de Tens...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment comprendre Tensorflow et ne pas mourir, et même enseigner quelque chose sur une voiture</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/427449/"><p>  Bonjour, les gardes.  La publication d'aujourd'hui portera sur la façon de ne pas se perdre dans la nature des nombreuses options d'utilisation de TensorFlow pour l'apprentissage automatique et d'atteindre votre objectif.  L'article est conçu pour que le lecteur connaisse les bases des principes de l'apprentissage automatique, mais n'a pas encore essayé de le faire de ses propres mains.  En conséquence, nous obtenons une démo fonctionnelle sur Android, qui reconnaît quelque chose avec une précision assez élevée.  Mais tout d'abord. </p><br><p><img src="https://habrastorage.org/webt/rs/7r/_f/rs7r_f7v6dywnklpaok4htwntsq.jpeg"></p><a name="habracut"></a><br><p>  Après avoir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">examiné les</a> derniers documents, il a été décidé de faire <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">appel</a> à <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tensorflow</a> , qui prend maintenant de l'ampleur, et les articles en anglais et en russe semblent être suffisants pour ne pas creuser dans cela et réussir à comprendre ce qui est quoi. </p><br><p>  Passer deux semaines, étudier des articles et de nombreux ex-échantillons au bureau.  site, j'ai réalisé que je ne comprenais rien.  Trop d'informations et d'options sur la façon dont Tensorflow peut être utilisé.  Ma tête est déjà gonflée de voir à quel point ils proposent des solutions différentes et ce que j'en fais, tel qu'appliqué à ma tâche. </p><br><p><img src="https://habrastorage.org/webt/bd/2z/jy/bd2zjyct-gx0xbz9nfbwwya5aw8.png"></p><br><p>  J'ai ensuite décidé de tout essayer, des options les plus simples et les plus prêtes à l'emploi (dans lesquelles je devais enregistrer une dépendance dans gradle et ajouter quelques lignes de code) aux plus complexes (dans lesquelles je devrais créer et former nous-mêmes des modèles graphiques et apprendre à les utiliser dans un mobile application). </p><br><p>  Au final, j'ai dû utiliser une version compliquée, qui sera discutée plus en détail ci-dessous.  En attendant, j'ai compilé pour vous une liste d'options plus simples et tout aussi efficaces, chacune s'adaptant à son objectif. </p><br><h3 id="1--ml-kithttpsfirebasegooglecomdocsml-kit">  1. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ML KIT</a> </h3><br><p><img src="https://habrastorage.org/webt/al/of/8w/alof8wunrnv66f66xwv2rrlbrn0.png"></p><br><p>  La solution la plus simple à utiliser - quelques lignes de code que vous pouvez utiliser: </p><br><ul><li>  Reconnaissance de texte (texte, caractères latins) </li><li>  Détection des visages (visages, émotions) </li><li>  Numérisation de codes-barres (code-barres, code qr) </li><li>  Étiquetage d'image (un nombre limité de types d'objets dans l'image) </li><li>  Reconnaissance historique (attractions) </li></ul><br><p>  C'est un peu plus compliqué. Avec cette solution, vous pouvez également utiliser votre propre modèle TensorFlow Lite, mais la conversion à ce format a posé des problèmes, donc cet élément n'a pas été essayé. </p><br><p>  Comme l'écrivent les créateurs de cette progéniture, la plupart des tâches peuvent être résolues à l'aide de ces développements.  Mais si cela ne s'applique pas à votre tâche, vous devrez utiliser des modèles personnalisés. </p><br><h3 id="2--custom-visionhttpswwwcustomvisionai">  2. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Vision personnalisée</a> </h3><br><p><img src="https://habrastorage.org/webt/p9/c_/2u/p9c_2ujglvyu8mbffhrmoqpzav0.png"></p><br><p>  Un outil très pratique pour créer et former vos modèles personnalisés à l'aide d'images. <br>  Des pros - il existe une version gratuite qui vous permet de garder un projet. <br>  Des inconvénients - la version gratuite limite le nombre d'images «entrantes» à 3 000.  Pour essayer de créer un réseau de précision médiocre - cela suffit.  Pour des tâches plus précises, vous en avez besoin de plus. <br>  Tout ce qui est requis de l'utilisateur est d'ajouter des images avec une marque (par exemple - image1 est "racoon", image2 est "sun"), former et exporter le graphique pour une utilisation future. </p><br><p><img src="https://habrastorage.org/webt/co/lk/nw/colknw0ljunbtzcixxdrde6qwtm.png"></p><br><p>  Caring Microsoft propose même son propre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">échantillon</a> , avec lequel vous pouvez essayer votre graphique reçu. <br>  Pour ceux qui sont déjà «dans le sujet» - le graphique est déjà généré à l'état Frozen, c'est-à-dire  vous n'avez pas besoin de faire / convertir quoi que ce soit avec. <br>  Cette solution est bonne lorsque vous avez un grand échantillon et (beaucoup d'attention) de différentes classes en formation.  Parce que  sinon, il y aura de nombreuses fausses définitions dans la pratique.  Par exemple, vous vous êtes entraîné sur les ratons laveurs et les soleils, et s'il y a une personne à l'entrée, alors elle peut, avec une probabilité égale, être définie par un système comme l'un ou l'autre.  Bien qu'en fait - ni l'un ni l'autre. </p><br><h3 id="3--sozdanie-modeli-vruchnuyu">  3. Création manuelle d'un modèle </h3><br><p><img src="https://habrastorage.org/webt/m_/ku/r_/m_kur_ks0vdyiqoiw7h5pvbwoey.jpeg"></p><br><p>  Lorsque vous devez affiner vous-même le modèle pour la reconnaissance d'image, des manipulations plus complexes avec la sélection d'image d'entrée entrent en jeu. <br>  Par exemple, nous ne voulons pas avoir de restrictions sur le volume de l'échantillon d'entrée (comme dans le paragraphe précédent), ou nous voulons former le modèle plus précisément en définissant nous-mêmes le nombre d'époque et d'autres paramètres d'apprentissage. <br>  Dans cette approche, il existe plusieurs exemples de Tensorflow qui décrivent la procédure et le résultat final. <br>  Voici quelques exemples: </p><br><ul><li>  Cool codelab <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tensorflow pour les poètes</a> . <br></li></ul><br><br><p>  Il donne un exemple de la façon de créer un classificateur de types de couleurs basé sur la base de données d'images ouverte ImageNet - préparer des images, puis former le modèle.  Une petite mention est également faite de la façon dont vous pouvez travailler avec un outil plutôt intéressant - TensorBoard.  De ses fonctions les plus simples - il démontre clairement la structure de votre modèle fini, ainsi que le processus d'apprentissage à bien des égards. </p><br><ul><li><p>  Kodlab <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tensorflow for Poets 2</a> - poursuite du travail avec le classificateur de couleurs.  Il montre comment si vous avez les fichiers graphiques et leurs étiquettes (qui ont été obtenus dans le codelab précédent), vous pouvez exécuter l'application sur Android.  Un des points du codelab est la conversion du format graphique "habituel" ".pb" au format Tensorflow lite (qui implique des optimisations de fichier pour réduire la taille finale du fichier graphique, car les appareils mobiles en ont besoin). </p><br></li><li><p>  Reconnaissance de l'écriture manuscrite <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MNIST</a> . <br></p><br><img src="https://habrastorage.org/webt/bz/ah/mx/bzahmxc0xozicgssbkzfqbgi1qw.gif"></li></ul><br><br><p>  Le navet contient le modèle d'origine (qui a déjà été préparé pour cette tâche), des instructions sur la façon de le former, le convertir et comment exécuter un projet pour Android à la fin pour vérifier comment tout cela fonctionne. </p><br><p>  Sur la base de ces exemples, vous pouvez comprendre comment travailler avec des modèles personnalisés dans Tensorflow et essayer de créer le vôtre ou de prendre l'un des modèles pré-formés qui sont assemblés sur un github: <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Modèles de Tensorflow</a> </p><br><p>  Parlant de modèles "pré-formés".  Nuances intéressantes lors de leur utilisation: </p><br><ul><li>  Leur structure est déjà préparée pour une tâche spécifique. </li><li>  Ils sont déjà formés à de grands échantillons. <br>  Par conséquent, si votre échantillon n'est pas suffisamment rempli, vous pouvez prendre un modèle pré-formé qui est proche de la portée de votre tâche.  En utilisant ce modèle, en ajoutant vos propres règles de formation, vous obtiendrez un meilleur résultat que vous n'essaieriez de former le modèle à partir de zéro. </li></ul><br><h3 id="4--object-detection-api---cozdanie-modeli-vruchnuyu">  4. API de détection d'objets + création manuelle de modèle </h3><br><p>  Cependant, tous les paragraphes précédents n'ont pas donné le résultat souhaité.  Dès le début, il était difficile de comprendre ce qui devait être fait et avec quelle approche.  Ensuite, un article sympa sur l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">API de détection d'objets a</a> été trouvé, qui explique comment trouver plusieurs catégories sur une image, ainsi que plusieurs instances de la même catégorie.  Dans le processus de travail sur cet exemple, les articles sources et les didacticiels vidéo sur la reconnaissance des objets personnalisés se sont avérés plus pratiques (les liens seront à la fin). </p><br><p>  Mais le travail n'aurait pas pu être achevé sans un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article sur la reconnaissance de Pikachu</a> - car une nuance très importante y a été soulignée, qui pour une raison quelconque n'est mentionnée nulle part dans un guide ou un exemple.  Et sans cela, tout le travail accompli serait vain. </p><br><p>  Alors, maintenant enfin sur ce qui restait à faire et ce qui s'est passé à la sortie. </p><br><ol><li>  Tout d'abord, la farine de l'installation Tensorflow.  Qui ne peut pas l'installer ou utiliser les scripts standard pour créer, former un modèle - soyez patient et google.  Presque tous les problèmes ont déjà été écrits dans des problèmes sur githib ou sur stackoverflow. <br></li></ol><br>  Selon les instructions pour la reconnaissance d'objets, nous devons préparer un échantillon d'entrée avant de former le modèle.  Ces articles décrivent en détail comment procéder à l'aide d'un outil pratique - labelImg.  La seule difficulté ici est de faire un travail très long et minutieux pour mettre en évidence les limites des objets dont nous avons besoin.  Dans ce cas, tampons sur des images de documents. <br><br><img src="https://habrastorage.org/webt/ge/hh/x_/gehhx_5fqfezu1sbh5tvoofss20.png"><br>  L'étape suivante, à l'aide de scripts prêts à l'emploi, nous exportons les données de l'étape 2 d'abord vers des fichiers csv, puis vers TFRecords - le format de données d'entrée Tensorflow.  Aucune difficulté ne devrait survenir ici. <br>  Le choix d'un modèle pré-formé, sur la base duquel nous allons pré-former le graphique, ainsi que la formation elle-même.  C'est là que le plus grand nombre d'erreurs inconnues peut se produire, dont la cause est les packages désinstallés (ou installés de manière incorrecte) nécessaires au travail.  Mais vous réussirez, ne désespérez pas, le résultat en vaut la peine. <br><br><img src="https://habrastorage.org/webt/9y/qw/1b/9yqw1boyubfcrrf5jcaylkjjtyo.jpeg"><br>  Exportez le fichier reçu après la formation au format 'pb'.  Sélectionnez simplement le dernier fichier 'ckpt' et exportez-le. <br>  Exécution d'un exemple de travail sur Android. <br>  Téléchargement de l'échantillon de reconnaissance d'objet officiel à partir du github Tensorflow - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">TF Detect</a> .  Insérez-y votre modèle et votre fichier avec des étiquettes.  Mais.  Rien ne fonctionnera. <br><br><img src="https://habrastorage.org/webt/kj/3k/o4/kj3ko4d3ywoap8ff6oknuwova7c.gif"><br><br><p>  C'est là que le plus grand gag dans tout le travail vient de se produire, assez curieusement - eh bien, les échantillons Tensorflow ne voulaient en aucun cas fonctionner.  Tout est tombé.  Seul le puissant Pikachu avec son article a réussi à tout faire fonctionner. <br>  La première ligne du fichier labels.txt doit être l'inscription "???", car  par défaut dans l'API Object Detection, les numéros d'identification des objets ne commencent pas par 0 comme d'habitude, mais par 1. En raison du fait que la classe null est réservée, des questions magiques doivent être indiquées.  C'est-à-dire  votre fichier de balises ressemblera à ceci: </p><br><pre><code class="hljs">??? stamp</code> </pre> <br><p>  Et puis - exécutez l'échantillon et voyez la reconnaissance des objets et le niveau de confiance avec lequel il a été reçu. </p><br><p><img src="https://habrastorage.org/webt/ly/kr/dm/lykrdma-x9h8epuqsuah3gkr3bk.png"><img src="https://habrastorage.org/webt/ne/lm/7v/nelm7v8rpjiuhzhevlptp0dc-fa.png"><img src="https://habrastorage.org/webt/9t/ci/4r/9tci4rxzhixufdjhb5ecpdof0ik.png"></p><br><p>  Ainsi, le résultat est une application simple qui, lorsque vous survolez l'appareil photo, reconnaît les limites du tampon sur le document et les indique avec la précision de reconnaissance. <br>  Et si l'on exclut le temps passé à chercher la bonne approche et à essayer de la lancer, alors, dans l'ensemble, le travail s'est avéré assez rapide et vraiment pas compliqué.  Vous avez juste besoin de connaître les nuances avant de commencer à travailler. </p><br><p>  Déjà en tant que section supplémentaire (ici, vous pouvez déjà fermer l'article si vous êtes fatigué des informations), je voudrais écrire quelques astuces de vie qui ont aidé à travailler avec tout cela. </p><br><ul><li><p>  assez souvent, les scripts tensorflow ne fonctionnaient pas car ils étaient exécutés à partir de mauvais répertoires.  De plus, c'était différent sur différents PC: quelqu'un devait s'exécuter à partir du <code>tensroflowmodels/models/research</code> pour travailler, et quelqu'un <code>tensroflowmodels/models/research/object-detection</code> niveau plus profond à partir du <code>tensroflowmodels/models/research/object-detection</code> </p><br></li><li><p>  rappelez-vous que pour chaque terminal ouvert, vous devez réexporter le chemin à l'aide de la commande </p><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">export</span></span> PYTHONPATH=/  /tensroflowmodels/models/research/slim:<span class="hljs-variable"><span class="hljs-variable">$PYTHONPATH</span></span></code> </pre> <br></li><li><p>  si vous n'utilisez pas votre propre graphique et que vous souhaitez en savoir plus (par exemple, " <code>input_node_name</code> ", qui sera requis plus tard), exécutez deux commandes à partir du dossier racine: </p><br><pre> <code class="hljs powershell">bazel build tensorflow/tools/graph_transforms:summarize_graph bazel<span class="hljs-literal"><span class="hljs-literal">-bin</span></span>/tensorflow/tools/graph_transforms/summarize_graph -<span class="hljs-literal"><span class="hljs-literal">-in_graph</span></span>=<span class="hljs-string"><span class="hljs-string">"/  /frozen_inference_graph.pb"</span></span></code> </pre> <br><p>  où " <code>/  /frozen_inference_graph.pb</code> " est le chemin d'accès au graphique que vous souhaitez connaître </p><br></li><li><p>  Pour afficher des informations sur le graphique, vous pouvez utiliser Tensorboard. </p><br><pre> <code class="hljs powershell">python import_pb_to_tensorboard.py -<span class="hljs-literal"><span class="hljs-literal">-model_dir</span></span>=output/frozen_inference_graph.pb -<span class="hljs-literal"><span class="hljs-literal">-log_dir</span></span>=training</code> </pre> <br><p>  où vous devez spécifier le chemin d'accès au graphique ( <code>model_dir</code> ) et le chemin d'accès aux fichiers reçus pendant la formation ( <code>log_dir</code> ).  Ensuite, ouvrez simplement localhost dans le navigateur et regardez ce qui vous intéresse. </p><br></li></ul><br><p>  Et la dernière partie - sur l'utilisation des scripts python dans les instructions de l'API Object Detection - une petite feuille de triche ci-dessous avec des commandes et des conseils a été préparée pour vous. </p><br><div class="spoiler">  <b class="spoiler_title">Feuille de triche</b> <div class="spoiler_text"><p>  Exporter de labelimg vers csv (depuis le répertoire object_detection) </p><br><pre> <code class="hljs mel"><span class="hljs-keyword"><span class="hljs-keyword">python</span></span> xml_to_csv.py</code> </pre> <br><p>  De plus, toutes les étapes énumérées ci-dessous doivent être effectuées à partir du même dossier Tensorflow (" <code>tensroflowmodels/models/research/object-detection</code> " ou d'un niveau supérieur - selon la façon dont vous allez) - c'est tout les images de la sélection d'entrée, des TFRecords et d'autres fichiers doivent être copiés dans ce répertoire avant de commencer le travail. </p><br><p>  Exporter de csv vers tfrecord </p><br><pre> <code class="hljs powershell">python generate_tfrecord.py -<span class="hljs-literal"><span class="hljs-literal">-csv_input</span></span>=<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>/train_labels.csv -<span class="hljs-literal"><span class="hljs-literal">-output_path</span></span>=<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>/train.record python generate_tfrecord.py -<span class="hljs-literal"><span class="hljs-literal">-csv_input</span></span>=<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>/test_labels.csv -<span class="hljs-literal"><span class="hljs-literal">-output_path</span></span>=<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>/test.record</code> </pre> <br><p>  * N'oubliez pas de changer les lignes 'train' et 'test' dans les chemins du fichier lui-même (generate_tfrecord.py), ainsi que <br>  le nom des classes reconnues dans la fonction <code>class_text_to_int</code> (qui doit être dupliquée dans le fichier <code>pbtxt</code> que vous allez créer avant d'entraîner le graphique). </p><br><p>  La formation </p><br><pre> <code class="hljs powershell">python legacy/train.py —logtostderr -<span class="hljs-literal"><span class="hljs-literal">-train_dir</span></span>=training/ -<span class="hljs-literal"><span class="hljs-literal">-pipeline_config_path</span></span>=training/ssd_mobilenet_v1_coco.config</code> </pre> <br><p>  ** Avant l'entraînement, n'oubliez pas de vérifier le fichier " <code>training/object-detection.pbtxt</code> " - il devrait y avoir toutes les classes reconnues et le fichier " <code>training/ssd_mobilenet_v1_coco.config</code> " - là vous devez changer le paramètre " <code>num_classes</code> " au nombre de vos classes. </p><br><p>  Exporter le modèle vers pb </p><br><pre> <code class="hljs powershell">python export_inference_graph.py \ -<span class="hljs-literal"><span class="hljs-literal">-input_type</span></span>=image_tensor \ -<span class="hljs-literal"><span class="hljs-literal">-pipeline_config_path</span></span>=training/pipeline.config \ -<span class="hljs-literal"><span class="hljs-literal">-trained_checkpoint_prefix</span></span>=training/model.ckpt<span class="hljs-literal"><span class="hljs-literal">-110</span></span> \ -<span class="hljs-literal"><span class="hljs-literal">-output_directory</span></span>=output</code> </pre> </div></div><br><p>  Merci de votre intérêt pour ce sujet! </p><br><p>  Les références </p><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Article original sur la reconnaissance d'objets</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Un cycle de vidéo à l'article sur la reconnaissance des objets en anglais</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">L'ensemble des scripts utilisés dans l'article d'origine</a> </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr427449/">https://habr.com/ru/post/fr427449/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr427437/index.html">Configuration des serveurs liés: serveur sql ms et teradata</a></li>
<li><a href="../fr427439/index.html">Toute la vérité sur RTOS. Article # 16. Signaux</a></li>
<li><a href="../fr427441/index.html">Convergence avec Kubernetes</a></li>
<li><a href="../fr427443/index.html">Vivisection du succès</a></li>
<li><a href="../fr427447/index.html">PVS-Studio prend en charge la chaîne d'outils intégrée GNU Arm</a></li>
<li><a href="../fr427451/index.html">Connectez les tâches phpStorm à Bitrix24</a></li>
<li><a href="../fr427453/index.html">Comment j'ai fait la transmission du son sur le Raspberry Pi</a></li>
<li><a href="../fr427457/index.html">La troisième vague d'IA et de systèmes pour la sécurité de l'État</a></li>
<li><a href="../fr427459/index.html">Lampes LED Diall du magasin Castorama</a></li>
<li><a href="../fr427461/index.html">La beauté des fonctions NON anonymes en JavaScript</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>