<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üöú üßùüèª ü§¶üèæ Googles spezialisierter ASIC f√ºr maschinelles Lernen ist zehnmal schneller als die GPU üî§ üóëÔ∏è üîú</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Vor vier Jahren erkannte Google das wahre Potenzial der Verwendung neuronaler Netze in seinen Anwendungen. Dann begann sie, sie √ºberall einzuf√ºhren - ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Googles spezialisierter ASIC f√ºr maschinelles Lernen ist zehnmal schneller als die GPU</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/402955/"><img src="https://habrastorage.org/files/265/7b9/cb4/2657b9cb49834f6ebc249ddcb70c9136.jpg"><br><br>  Vor vier Jahren erkannte Google das wahre Potenzial der Verwendung neuronaler Netze in seinen Anwendungen.  Dann begann sie, sie √ºberall einzuf√ºhren - bei der Text√ºbersetzung, der Sprachsuche mit Spracherkennung usw. Es wurde jedoch sofort klar, dass die Verwendung neuronaler Netze die Belastung der Google-Server erheblich erh√∂ht.  Grob gesagt, wenn jeder nur drei Minuten am Tag eine Sprachsuche auf Android (oder diktiertem Text mit Spracherkennung) durchf√ºhren w√ºrde, m√ºsste Google die Anzahl der Rechenzentren (!) Verdoppeln, damit die neuronalen Netze eine solche Menge an Sprachverkehr verarbeiten. <br><br>  Es musste etwas getan werden - und Google fand eine L√∂sung.  2015 entwickelte sie eine eigene Hardwarearchitektur f√ºr maschinelles Lernen (Tensor Processing Unit, TPU), die hinsichtlich der Leistung bis zu 70-mal schneller als herk√∂mmliche GPUs und CPUs und hinsichtlich der Anzahl der Berechnungen pro Watt bis zu 196-mal schneller ist.  Herk√∂mmliche GPUs / CPUs beziehen sich auf Allzweckprozessoren Xeon E5 v3 (Haswell) und Nvidia Tesla K80-GPUs. <br><a name="habracut"></a><br>  Die TPU-Architektur wurde diese Woche erstmals in einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">wissenschaftlichen</a> Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">(pdf) beschrieben</a> , der auf dem 44. Internationalen Symposium f√ºr Computerarchitekturen (ISCA) am 26. Juni 2017 in Toronto vorgestellt wird.  Ein f√ºhrender Autor von mehr als 70 Autoren dieser wissenschaftlichen Arbeit, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ein herausragender Ingenieur Norman</a> Jouppi, der als einer der Sch√∂pfer des MIPS-Prozessors bekannt ist <i>,</i> erkl√§rte in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einem Interview</a> mit <i>The Next Platform</i> in seinen eigenen Worten die Merkmale der einzigartigen TPU-Architektur, die eigentlich ein spezialisierter ASIC ist, d. H. spezielle integrierte Schaltung. <br><br>  Im Gegensatz zu herk√∂mmlichen FPGAs oder hochspezialisierten ASICs werden TPU-Module wie eine GPU oder CPU programmiert, es handelt sich nicht um ein Ger√§t mit enger Reichweite f√ºr ein einzelnes neuronales Netzwerk.  Laut Norman Yuppy unterst√ºtzt TPU CISC-Anweisungen f√ºr verschiedene Arten von neuronalen Netzen: Faltungs-Neuronale Netze, LSTM-Modelle und gro√üe, vollst√§ndig verbundene Modelle.  Damit es weiterhin programmierbar bleibt, wird die Matrix nur als Grundelement und nicht als Vektor- oder Skalargrundelement verwendet. <br><br>  Google betont, dass andere Entwickler zwar ihre Mikrochips f√ºr Faltungs-Neuronale Netze optimieren, diese Neuronalen Netze jedoch nur 5% der Last in Google-Rechenzentren ausmachen.  Die meisten Google-Anwendungen verwenden die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">mehrschichtigen Rumelhart-Perzeptrone. Daher</a> war es so wichtig, eine universellere Architektur zu erstellen, die nicht nur f√ºr Faltungs-Neuronale Netze ‚Äûgesch√§rft‚Äú wurde. <br><br><img src="https://habrastorage.org/files/99f/df0/d93/99fdf0d93e494088a3f1fd86791e0b23.png"><br>  <i>Eines der Elemente der Architektur ist die systolische Datenstrom-Engine, ein Array von 256 √ó 256, das Aktivierung (Gewichte) von den Neuronen auf der linken Seite erh√§lt, und dann verschiebt sich alles Schritt f√ºr Schritt, multipliziert mit den Gewichten in der Zelle.</i>  <i>Es stellt sich heraus, dass die systolische Matrix pro Zyklus 65.536 Berechnungen durchf√ºhrt.</i>  <i>Diese Architektur ist ideal f√ºr neuronale Netze.</i> <br><br>  Laut Uppy √§hnelt die Architektur von TPUs eher dem FPU-Coprozessor als einer regul√§ren GPU, obwohl zahlreiche Multiplikationsmatrizen keine Programme in sich speichern, sondern lediglich vom Host empfangene Anweisungen ausf√ºhren. <br><br><img src="https://habrastorage.org/files/9ba/2a1/2d1/9ba2a12d11204b74888b70e29ecdf876.png"><br>  <i>Alle TPU-Architekturen mit Ausnahme des DDR3-Speichers.</i>  <i>Anweisungen werden vom Host (links) an die Warteschlange gesendet.</i>  <i>Dann kann die Steuerlogik abh√§ngig von der Anweisung jede von ihnen wiederholt ausf√ºhren</i> <br><br>  Es ist noch nicht bekannt, wie skalierbar diese Architektur ist.  Yuppy sagt, dass es in einem System mit dieser Art von Host immer einen Engpass geben wird. <br><br><img src="https://habrastorage.org/files/6c0/87d/3c7/6c087d3c75b744628a716e65a6f0ae5e.png"><br><br>  Im Vergleich zu herk√∂mmlichen CPUs und GPUs √ºbertrifft die Maschinenarchitektur von Google diese um das Zehnfache.  Beispielsweise f√ºhrt ein Haswell Xeon E5-2699 v3-Prozessor mit 18 Kernen bei einer Taktfrequenz von 2,3 GHz und einem 64-Bit-Gleitkomma 1,3 Tera-Operationen pro Sekunde (TOPS) aus und zeigt eine Daten√ºbertragungsrate von 51 GB / s.  In diesem Fall verbraucht der Chip selbst 145 Watt und das gesamte System mit 256 GB Speicher - 455 Watt. <br><br>  Zum Vergleich zeigt TPU bei 8-Bit-Operationen mit 256 GB externem Speicher und 32 GB eigenem Speicher eine √úbertragungsgeschwindigkeit von 34 GB / s Speicher, aber die Karte f√ºhrt 92 TOPS aus, d. H. Ungef√§hr 71-mal mehr als der Haswell-Prozessor.  Der Stromverbrauch des Servers auf der TPU betr√§gt 384 Watt. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/fbb/21b/e41/fbb21be41722273b1e79febcd8b6f9ab.jpg"><br><br>  Das folgende Diagramm vergleicht die relative Leistung pro Watt eines Servers mit einer GPU (blaue Spalte), einem Server auf TPU (rot), relativ zu einem Server auf der CPU.  Au√üerdem wird die relative Leistung pro Watt des Servers mit der TPU im Verh√§ltnis zum Server auf der GPU (orange) und der verbesserten Version der TPU im Verh√§ltnis zum Server auf der CPU (gr√ºn) und dem Server auf der GPU (lila) verglichen. <br><br><img src="https://habrastorage.org/files/d85/52a/445/d8552a4455ed436c9daca5bdba5c1c2e.png"><br><br>  Es ist zu beachten, dass Google bei Tests von Anwendungen auf TensorFlow Vergleiche mit der relativ alten Version von Haswell Xeon anstellte, w√§hrend in der neueren Version von Broadwell Xeon E5 v4 die Anzahl der Anweisungen pro Zyklus aufgrund von Architekturverbesserungen um 5% und in der Version von Skylake Xeon E5 v5 zunahm , was im Sommer erwartet wird, kann sich die Anzahl der Anweisungen pro Zyklus um weitere 9-10% erh√∂hen.  Und mit der Erh√∂hung der Anzahl der Kerne von 18 auf 28 in Skylake kann sich die Gesamtleistung von Intel-Prozessoren in Google-Tests um 80% verbessern.  Trotzdem wird es bei TPU einen gro√üen Leistungsunterschied geben.  In der Testversion mit 32-Bit-Gleitkomma wird der Unterschied zwischen TPUs und CPUs auf ungef√§hr das 3,5-fache reduziert.  Die meisten Modelle quantisieren jedoch perfekt auf 8 Bit. <br><br>  Google dachte √ºber die Verwendung von GPU, FPGA und ASIC in seinen Rechenzentren seit 2006 nach, fand sie jedoch erst beim letzten Mal, als es maschinelles Lernen f√ºr eine Reihe praktischer Aufgaben einf√ºhrte und die Belastung dieser neuronalen Netze mit Milliarden von Anfragen von Benutzern zunahm.  Jetzt hat das Unternehmen keine andere Wahl, als sich von herk√∂mmlichen CPUs zu entfernen. <br><br>  Das Unternehmen plant nicht, seine Prozessoren an Dritte zu verkaufen, hofft jedoch, dass die wissenschaftliche Arbeit mit dem ASIC 2015 es anderen erm√∂glichen wird, die Architektur zu verbessern und verbesserte Versionen des ASIC zu erstellen, die "die Messlatte noch h√∂her legen".  Google selbst arbeitet wahrscheinlich bereits an einer neuen Version von ASIC. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de402955/">https://habr.com/ru/post/de402955/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de402941/index.html">Pebble Smartwatch l√∂st Cloud-Synchronisierung aus</a></li>
<li><a href="../de402943/index.html">Eine Welt ohne Renten</a></li>
<li><a href="../de402947/index.html">Jeff Bezos gibt 1 Milliarde US-Dollar pro Jahr f√ºr die Entwicklung von Blue Origin aus</a></li>
<li><a href="../de402951/index.html">ONYX BOOX MAX Carta Bewertung: Kompromisslos A4</a></li>
<li><a href="../de402953/index.html">Erfand ein System zum Einl√∂sen von Geld von Kreditkarten √ºber Pfandh√§user</a></li>
<li><a href="../de402957/index.html">Drei-Bit-RAM wurde auf einem Tintenstrahldrucker gedruckt</a></li>
<li><a href="../de402961/index.html">"Ein und dasselbe noch einmal": Wie eine Wiedergabeliste bei Radiosendern erstellt wird</a></li>
<li><a href="../de402963/index.html">Wenn Sie in zwei Reihen auf der Rolltreppe stehen, erh√∂ht sich der Durchsatz um 31%</a></li>
<li><a href="../de402965/index.html">Geek Keykeeper, Teil II: SmartPoket f√ºr die gro√üen Schl√ºssel, die wir dank Ihnen erstellt haben</a></li>
<li><a href="../de402967/index.html">Ein bisschen mehr Levitation f√ºr einen Geek: Ger√§te, die in der Luft bleiben k√∂nnen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>