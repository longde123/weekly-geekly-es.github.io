<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ“¸ â—½ï¸ ğŸ‘¨ğŸ¼â€ğŸ’¼ Kubernetes Networks: Pods ğŸ¤¶ğŸ¾ ğŸ§˜ğŸ¼ ğŸ—½</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Das Material, dessen Ãœbersetzung wir heute verÃ¶ffentlichen, ist den Merkmalen der Netzwerkinteraktion von Kubernetes-Herden gewidmet. Es ist fÃ¼r dieje...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Kubernetes Networks: Pods</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/441576/">  Das Material, dessen Ãœbersetzung wir heute verÃ¶ffentlichen, ist den Merkmalen der Netzwerkinteraktion von Kubernetes-Herden gewidmet.  Es ist fÃ¼r diejenigen gedacht, die bereits Erfahrung mit Kubernetes haben.  Wenn Sie sich mit Kubernetes nicht sehr gut auskennen, lohnt es sich wahrscheinlich, dieses Kubernetes-Tutorial zu lesen, bevor Sie dieses Material lesen, in dem die Arbeit mit dieser Plattform fÃ¼r AnfÃ¤nger in Betracht gezogen wird. <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/webt/0_/ch/6q/0_ch6qrxl9vydilgtpyci7diugw.jpeg"></a> <br><a name="habracut"></a><br><h2>  <font color="#3AC1EF">Pods</font> </h2><br>  Was ist unter (pod) Kubernetes?  Sub ist eine EntitÃ¤t, die aus einem oder mehreren Containern besteht, die auf demselben Host gehostet und fÃ¼r die gemeinsame Nutzung von Netzwerkstapelressourcen und anderen Ressourcen wie Volumes konfiguriert sind.  Pods sind die Grundbausteine â€‹â€‹fÃ¼r Anwendungen, die auf der Kubernetes-Plattform ausgefÃ¼hrt werden.  Pods teilen sich einen Netzwerkstapel.  In der Praxis bedeutet dies, dass alle Container, aus denen der Herd besteht, Ã¼ber <code>localhost</code> miteinander kommunizieren kÃ¶nnen.  Wenn sich im Herd ein Container befindet, in dem Nginx auf Port 80 Ã¼berwacht wird, und ein anderer Container, in dem Scrapyd ausgefÃ¼hrt wird, kann dieser Container unter <code>http://localhost:80</code> auf den ersten Container zugreifen.  Es sieht nicht so schwierig aus.  Fragen wir uns nun, wie das tatsÃ¤chlich funktioniert.  Schauen wir uns eine typische Situation an, in der der Docker-Container auf dem lokalen Computer gestartet wird. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/641/7fe/52a/6417fe52a2e9de3296187860905907f7.png"></div><br>  <i><font color="#999999">Docker-Container, der auf dem lokalen Computer ausgefÃ¼hrt wird</font></i> <br><br>  Wenn Sie dieses Schema von oben nach unten betrachten, stellt sich heraus, dass es eine physische Netzwerkschnittstelle <code>eth0</code> .  Die <code>docker0</code> Bridge ist daran <code>docker0</code> , und die virtuelle Netzwerkschnittstelle von <code>docker0</code> ist <code>veth0</code> der Bridge verbunden.  Beachten Sie, dass sich die <code>veth0</code> <code>docker0</code> und <code>veth0</code> im selben Netzwerk befinden. In diesem Beispiel ist dies <code>172.17.0.0/24</code> .  In diesem Netzwerk wird der <code>docker0</code> Schnittstelle die IP-Adresse <code>172.17.0.1</code> . Diese Schnittstelle ist das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Standard-Gateway</a> fÃ¼r die <code>veth0</code> Schnittstelle, der die Adresse <code>172.17.0.2</code> zugewiesen ist.  Aufgrund der Besonderheiten beim Einrichten von Netzwerk-Namespaces beim Starten des Containers sehen die Prozesse im Container nur die <code>veth0</code> Schnittstelle und interagieren mit der AuÃŸenwelt Ã¼ber die <code>docker0</code> und <code>eth0</code> Schnittstellen.  FÃ¼hren Sie nun den zweiten Container aus. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8b4/33a/915/8b433a91572e4afa0f9652d4e729a8b3.png"></div><br>  <i><font color="#999999">Zwei Docker-Container, die auf dem lokalen Computer ausgefÃ¼hrt werden</font></i> <br><br>  Wie Sie im obigen Diagramm sehen kÃ¶nnen, wird die neue virtuelle Netzwerkschnittstelle <code>veth1</code> dem zweiten Container zugewiesen, der mit derselben Bridge wie der erste Container verbunden ist - mit <code>docker0</code> .  Dies ist eine ziemlich prÃ¤zise Beschreibung dessen, was tatsÃ¤chlich passiert.  DarÃ¼ber hinaus ist zu beachten, dass die Verbindung zwischen dem Container und der Bridge dank eines Paares verbundener virtueller Ethernet-Schnittstellen hergestellt wird, von denen sich eine im Container-Namespace und die andere im Root-Netzwerk-Namespace befindet.  Details dazu finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> . <br><br>  All dies ist gut, aber es beschreibt noch nicht, was wir in Bezug auf Kubernetes-Pods als "Shared Network Stack" bezeichnen.  GlÃ¼cklicherweise sind Namespaces sehr flexibel.  Docker kann einen Container starten und anstelle einer neuen virtuellen Netzwerkschnittstelle die vorhandene Schnittstelle zusammen mit anderen Containern verwenden.  Bei diesem Ansatz mÃ¼ssen wir das obige Schema wie unten gezeigt Ã¤ndern. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/456/4f6/5fb/4564f65fb5ed8773794f98f7655f0523.png"></div><br>  <i><font color="#999999">Container verwenden eine gemeinsame Netzwerkschnittstelle</font></i> <br><br>  Jetzt interagiert der zweite Container mit der bereits vorhandenen <code>veth0</code> Schnittstelle und nicht wie im vorherigen Beispiel mit seiner eigenen <code>veth1</code> Schnittstelle.  Die Verwendung eines solchen Schemas fÃ¼hrt zu mehreren Konsequenzen.  ZunÃ¤chst kÃ¶nnen wir nun sagen, dass beide Container extern unter derselben Adresse ( <code>172.17.0.2</code> sichtbar sind und in jedem von ihnen auf die Ports auf <code>localhost</code> zugreifen kÃ¶nnen, die von einem anderen Container geÃ¶ffnet wurden.  Dies bedeutet auÃŸerdem, dass diese Container nicht dieselben Ports Ã¶ffnen kÃ¶nnen.  Dies ist natÃ¼rlich eine EinschrÃ¤nkung, unterscheidet sich jedoch nicht von einer Ã¤hnlichen EinschrÃ¤nkung in der Situation, in der mehrere Prozesse Ports auf demselben Host Ã¶ffnen.  Mit diesem Ansatz erhalten eine Reihe von Prozessen alle Vorteile, die mit der AusfÃ¼hrung dieser Prozesse in Containern verbunden sind, z. B. schlechte KonnektivitÃ¤t und Isolation. Gleichzeitig kÃ¶nnen Prozesse die Zusammenarbeit in der einfachsten vorhandenen Netzwerkumgebung organisieren. <br><br>  Kubernetes implementiert dieses Muster, indem fÃ¼r jeden Herd ein spezieller Container erstellt wird, dessen einziger Zweck darin besteht, eine Netzwerkschnittstelle fÃ¼r andere Herdcontainer bereitzustellen.  Wenn Sie eine Verbindung zu dem Knoten des Kubernetes-Clusters herstellen, dem von <code>ssh</code> ein bestimmtes Sub zugewiesen wurde, und den <code>docker ps</code> ausfÃ¼hren, wird mindestens ein Container mit dem Befehl <code>pause</code> ausgefÃ¼hrt.  Dieser Befehl unterbricht den aktuellen Prozess, bis ein <code>SIGTERM</code> Signal eintrifft.  Solche Container tun absolut nichts, sie befinden sich in einem "Schlaf" -Zustand und warten auf dieses Signal.  Trotz der Tatsache, dass â€suspendierteâ€œ Container nichts bewirken, sind sie sozusagen das â€Herzâ€œ des Herdes und bieten anderen Containern eine virtuelle Netzwerkschnittstelle, Ã¼ber die sie miteinander oder mit der AuÃŸenwelt interagieren kÃ¶nnen.  Infolgedessen stellt sich heraus, dass in einer hypothetischen Umgebung, die unter Ã¤hnelt, unser vorheriges Schema wie das unten gezeigte aussehen wÃ¼rde. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3b0/e06/d66/3b0e06d66041a5dd0946f43c94314dae.png"></div><br>  <i><font color="#999999">Hypothetische Container</font></i> <br><br><h2>  <font color="#3AC1EF">Herd Netzwerk</font> </h2><br>  Einer unter, voller Container, ist der Baustein eines bestimmten Systems, aber bisher nicht dieses System selbst.  Die Kubernetes-Architektur basiert auf der Anforderung, dass Herde mit anderen Herden interagieren kÃ¶nnen mÃ¼ssen, unabhÃ¤ngig davon, ob sie auf demselben Computer oder auf verschiedenen Computern ausgefÃ¼hrt werden.  Um zu erfahren, wie dies alles funktioniert, mÃ¼ssen wir auf eine hÃ¶here Abstraktionsebene gehen und darÃ¼ber sprechen, wie Knoten im Kubernetes-Cluster funktionieren.  Hier werden wir das Thema Netzwerkrouting und Routen behandeln.  Dieses Thema wird in solchen Materialien hÃ¤ufig vermieden, da es zu komplex ist.  Es ist nicht einfach, eine verstÃ¤ndliche und nicht zu lange Anleitung zum IP-Routing zu finden. Wenn Sie sich jedoch einen kurzen Ãœberblick Ã¼ber dieses Problem verschaffen mÃ¶chten, kÃ¶nnen Sie sich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dieses</a> Material ansehen. <br><br>  Der Kubernetes-Cluster besteht aus einem oder mehreren Knoten.  Ein Knoten ist ein physisches oder virtuelles Hostsystem, das verschiedene Softwaretools und deren AbhÃ¤ngigkeiten (hauptsÃ¤chlich Docker) sowie mehrere Kubernetes-Systemkomponenten enthÃ¤lt.  Der Knoten ist mit dem Netzwerk verbunden, sodass er Daten mit anderen Knoten im Cluster austauschen kann.  So kÃ¶nnte ein einfacher Cluster mit zwei Knoten aussehen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/477/db1/b20/477db1b2030b13c41178a48821916fcc.png"></div><br>  <i><font color="#999999">Ein einfacher Cluster mit zwei Knoten</font></i> <br><br>  Wenn der betreffende Cluster in einer Cloud-Umgebung wie GCP oder AWS ausgefÃ¼hrt wird, vermittelt dieses Schema die Essenz der Standardnetzwerkarchitektur fÃ¼r einzelne Projekte ziemlich genau.  Zu Demonstrationszwecken <code>10.100.0.0/24</code> in diesem Beispiel das private Netzwerk <code>10.100.0.0/24</code> verwendet.  Infolgedessen <code>10.100.0.1</code> dem <code>10.100.0.1</code> die Adresse <code>10.100.0.1</code> zugewiesen, und die Adressen <code>10.100.0.2</code> und <code>10.100.0.3</code> zwei Knoten <code>10.100.0.3</code> .  Mit dieser Architektur kann jeder der Knoten Ã¼ber seine <code>eth0</code> Netzwerkschnittstelle miteinander <code>eth0</code> .  Erinnern wir uns jetzt daran, dass sich under, das auf dem Host ausgefÃ¼hrt wird, nicht in diesem privaten Netzwerk befindet.  Es ist in einem vÃ¶llig anderen Netzwerk mit der BrÃ¼cke verbunden.  Dies ist ein virtuelles Netzwerk, das nur innerhalb eines bestimmten Knotens existiert.  Um es klarer zu machen, lassen Sie uns das vorherige Schema neu zeichnen und das hinzufÃ¼gen, was wir oben als hypothetischen Herd bezeichnet haben. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ff6/721/c32/ff6721c32b5d74bc80fccfbf6164e486.png"></div><br>  <i><font color="#999999">Pods und Knoten</font></i> <br><br>  Der Host links in diesem Diagramm verfÃ¼gt Ã¼ber eine <code>eht0</code> Schnittstelle mit der Adresse <code>10.100.0.2</code> , deren Standard-Gateway der Router mit der Adresse <code>10.100.0.1</code> .  Die <code>docker0</code> BrÃ¼cke mit der Adresse <code>172.17.0.1</code> mit dieser Schnittstelle verbunden, und Ã¼ber die virtuelle Schnittstelle <code>veth0</code> mit der Adresse <code>172.17.0.2</code> ist mit ihr verbunden, was wir hier den Herd nennen.  Die <code>veth0</code> Schnittstelle wurde in einem angehaltenen Container erstellt.  Es ist in allen drei Containern Ã¼ber einen gemeinsam genutzten Netzwerkstapel sichtbar.  Aufgrund der Tatsache, dass beim Erstellen der Bridge lokale Routing-Regeln konfiguriert werden, wird jedes Paket, das bei <code>eth0</code> und die Zieladresse <code>172.17.0.2</code> hat, an die Bridge umgeleitet, die es an die virtuelle Schnittstelle <code>veth0</code> .  WÃ¤hrend das alles ziemlich anstÃ¤ndig aussieht.  Wenn bekannt ist, dass der von uns diskutierte Host die Adresse <code>172.17.0.2</code> , kÃ¶nnen wir den Router-Einstellungen eine Regel hinzufÃ¼gen, die beschreibt, dass der nÃ¤chste Ãœbergang fÃ¼r diese Adresse <code>10.100.0.2</code> ist. <code>veth0</code> sollten Pakete von dort an <code>veth0</code> umgeleitet <code>veth0</code> .  Hervorragend.  Schauen wir uns jetzt einen anderen Host an. <br><br>  Der im Diagramm rechts gezeigte Host hat eine physikalische Schnittstelle <code>eth0</code> mit der Adresse <code>10.100.0.3</code> .  Es verwendet dasselbe Standard-Gateway - <code>10.100.0.1</code> - und ist erneut mit der <code>docker0</code> Bridge mit der Adresse <code>172.17.0.1</code> .  Es besteht das GefÃ¼hl, dass nicht alles so gut lÃ¤uft.  Diese Adresse kann tatsÃ¤chlich von der auf dem Host auf der linken Seite verwendeten abweichen.  Die Adressen der Bridges hier werden gleich gemacht, um das schlimmste Szenario zu demonstrieren, das beispielsweise auftreten kann, wenn Sie Docker gerade installiert haben und es nach Ihren WÃ¼nschen arbeiten lassen.  Aber selbst wenn die fraglichen Netzwerke unterschiedlich sind, zeigt unser Beispiel ein tieferes Problem, nÃ¤mlich dass Knoten normalerweise nichts darÃ¼ber wissen, welche privaten Adressen Bridges zugewiesen sind, die sich an anderen Knoten befinden.  Und wir mÃ¼ssen darÃ¼ber Bescheid wissen - um Pakete an diese Bridges senden zu kÃ¶nnen und um sicher zu sein, dass sie dort ankommen, wo sie gebraucht werden.  Offensichtlich benÃ¶tigen wir hier eine Art EntitÃ¤t, die es uns ermÃ¶glicht, die korrekte Konfiguration von Adressen in verschiedenen Knoten sicherzustellen. <br><br>  Die Kubernetes-Plattform bietet uns eine zweistufige LÃ¶sung fÃ¼r dieses Problem.  Diese Plattform weist zunÃ¤chst einen gemeinsamen Adressraum fÃ¼r Bridges in jedem Knoten zu und weist Bridges dann die Adressen in diesem Raum zu, basierend darauf, in welchem â€‹â€‹Knoten sich die Bridge befindet.  Zweitens fÃ¼gt Kubernetes dem Gateway Routing-Regeln hinzu, das sich in unserem Fall um <code>10.100.0.1</code> .  Diese Regeln definieren die Regeln fÃ¼r das Weiterleiten von Paketen, die fÃ¼r jede der Bridges bestimmt sind.  Das heiÃŸt, sie beschreiben, Ã¼ber welche physikalische Schnittstelle <code>eth0</code> mit jeder der BrÃ¼cken kontaktiert werden kann.  Diese Kombination aus virtuellen Netzwerkschnittstellen, Bridges und Routing-Regeln wird Ã¼blicherweise als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Overlay-Netzwerk bezeichnet</a> .  Apropos Kubernetes, ich nenne dieses Netzwerk normalerweise ein "Herdnetzwerk", da es sich um ein Overlay-Netzwerk handelt, mit dem Pods an verschiedenen Knoten miteinander kommunizieren kÃ¶nnen.  So sieht das vorherige Diagramm aus, nachdem die Kubernetes-Mechanismen zur Sache gekommen sind. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b9c/f87/c96/b9cf87c96d0169dbcd0846f8bb3cd323.png"></div><br>  <i><font color="#999999">Herd Netzwerk</font></i> <br><br>  Es fÃ¤llt sofort auf, dass die BrÃ¼ckennamen von <code>docker0</code> in <code>cbr0</code> .  Kubernetes verwendet keine Standard-Docker-Bridges.  Was wir <code>cbr</code> ist eine AbkÃ¼rzung fÃ¼r "benutzerdefinierte BrÃ¼cke", das heiÃŸt, wir sprechen Ã¼ber einige spezielle BrÃ¼cken.  Ich bin nicht bereit, eine vollstÃ¤ndige Liste der Unterschiede zwischen dem Starten von Docker-Containern in Pods und dem AusfÃ¼hren auf normalen Computern zu geben, aber wir sprechen hier von einem der wichtigen Ã¤hnlichen Unterschiede.  AuÃŸerdem mÃ¼ssen Sie darauf achten, dass der in diesem Beispiel Bridges zugewiesene Adressraum <code>10.0.0.0/14</code> .  Diese Adresse stammt aus einem unserer Staging-Cluster, die auf der Google Cloud-Plattform bereitgestellt werden. Das obige ist also ein sehr reales Beispiel fÃ¼r ein Herdnetzwerk.  Ihrem Cluster kann ein vÃ¶llig anderer Adressbereich zugewiesen werden.  Leider gibt es derzeit keine MÃ¶glichkeit, Informationen Ã¼ber diese Adressen mit dem Dienstprogramm <code>kubectl</code> Wenn Sie jedoch beispielsweise GCP verwenden, kÃ¶nnen Sie einen Befehl wie <code>gcloud container clusters describe &lt;cluster&gt;</code> <code>clusterIpv4Cidr</code> Eigenschaft <code>clusterIpv4Cidr</code> . <br><br>  Im Allgemeinen kann festgestellt werden, dass Sie normalerweise nicht Ã¼ber die Funktionsweise des Herdnetzwerks nachdenken mÃ¼ssen.  Wenn ein Sub Daten mit einem anderen Herd austauscht, geschieht dies meistens Ã¼ber Kubernetes-Dienste.  Dies ist ein bisschen ein softwaredefinierter Proxy.  Die Netzwerkadressen der Herde werden jedoch in den Protokollen angezeigt.  In einigen Situationen, insbesondere wÃ¤hrend des Debuggens, mÃ¼ssen Sie mÃ¶glicherweise explizit Routing-Regeln in Herdnetzwerken festlegen.  Beispielsweise wird Datenverkehr, bei dem Kubernetes an eine Adresse im Bereich 10.0.0.0/8 gebunden bleibt, standardmÃ¤ÃŸig nicht mit NAT verarbeitet.  Wenn Sie mit Diensten interagieren, die sich in einem anderen privaten Netzwerk mit demselben Adressbereich befinden, mÃ¼ssen Sie mÃ¶glicherweise Routing-Regeln konfigurieren, mit denen Sie die korrekte Paketzustellung organisieren kÃ¶nnen. <br><br><h2>  <font color="#3AC1EF">Zusammenfassung</font> </h2><br>  Heute haben wir Ã¼ber Kubernetes Pods und die Funktionen ihrer Vernetzung gesprochen.  Wir hoffen, dass dieses Material Ihnen hilft, die richtigen Schritte zur Implementierung komplexer Herdinteraktionsszenarien in Kubernetes-Netzwerken zu unternehmen. <br><br>  <b>Liebe Leser!</b>  Dieser Artikel ist der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erste in einer</a> Reihe von Kubernetes-Netzwerken.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Der zweite</a> Teil dieses Zyklus wurde bereits <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ã¼bersetzt</a> .  Wir Ã¼berlegen, ob wir den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dritten</a> Teil Ã¼bersetzen sollen.  Wir bitten Sie, dies in den Kommentaren zu kommentieren. <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de441576/">https://habr.com/ru/post/de441576/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de441566/index.html">12 Wissenswertes zu JavaScript-Konzepten</a></li>
<li><a href="../de441568/index.html">Python-Speicherverwaltung</a></li>
<li><a href="../de441570/index.html">Die Verdauung von frischen Materialien aus der Welt des Frontends fÃ¼r die letzte Woche Nr. 353 (17. - 24. Februar 2019)</a></li>
<li><a href="../de441572/index.html">Frontend Weekly Digest (18. - 24. Februar 2019)</a></li>
<li><a href="../de441574/index.html">Docker lernen Teil 6: Arbeiten mit Daten</a></li>
<li><a href="../de441578/index.html">React Tutorial Teil 19: Methoden des Komponentenlebenszyklus</a></li>
<li><a href="../de441580/index.html">React Tutorial Teil 20: Erste Lektion zum bedingten Rendern</a></li>
<li><a href="../de441582/index.html">Optimierung des LQR-Steuerungssystems</a></li>
<li><a href="../de441584/index.html">PHP Digest Nr. 150 (11. - 25. Februar 2019)</a></li>
<li><a href="../de441586/index.html">Wie man Musik empfiehlt, die fast niemand gehÃ¶rt hat. Yandex-Bericht</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>