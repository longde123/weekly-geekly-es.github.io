<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>📸 ◽️ 👨🏼‍💼 Kubernetes Networks: Pods 🤶🏾 🧘🏼 🗽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Das Material, dessen Übersetzung wir heute veröffentlichen, ist den Merkmalen der Netzwerkinteraktion von Kubernetes-Herden gewidmet. Es ist für dieje...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Kubernetes Networks: Pods</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/441576/">  Das Material, dessen Übersetzung wir heute veröffentlichen, ist den Merkmalen der Netzwerkinteraktion von Kubernetes-Herden gewidmet.  Es ist für diejenigen gedacht, die bereits Erfahrung mit Kubernetes haben.  Wenn Sie sich mit Kubernetes nicht sehr gut auskennen, lohnt es sich wahrscheinlich, dieses Kubernetes-Tutorial zu lesen, bevor Sie dieses Material lesen, in dem die Arbeit mit dieser Plattform für Anfänger in Betracht gezogen wird. <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/webt/0_/ch/6q/0_ch6qrxl9vydilgtpyci7diugw.jpeg"></a> <br><a name="habracut"></a><br><h2>  <font color="#3AC1EF">Pods</font> </h2><br>  Was ist unter (pod) Kubernetes?  Sub ist eine Entität, die aus einem oder mehreren Containern besteht, die auf demselben Host gehostet und für die gemeinsame Nutzung von Netzwerkstapelressourcen und anderen Ressourcen wie Volumes konfiguriert sind.  Pods sind die Grundbausteine ​​für Anwendungen, die auf der Kubernetes-Plattform ausgeführt werden.  Pods teilen sich einen Netzwerkstapel.  In der Praxis bedeutet dies, dass alle Container, aus denen der Herd besteht, über <code>localhost</code> miteinander kommunizieren können.  Wenn sich im Herd ein Container befindet, in dem Nginx auf Port 80 überwacht wird, und ein anderer Container, in dem Scrapyd ausgeführt wird, kann dieser Container unter <code>http://localhost:80</code> auf den ersten Container zugreifen.  Es sieht nicht so schwierig aus.  Fragen wir uns nun, wie das tatsächlich funktioniert.  Schauen wir uns eine typische Situation an, in der der Docker-Container auf dem lokalen Computer gestartet wird. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/641/7fe/52a/6417fe52a2e9de3296187860905907f7.png"></div><br>  <i><font color="#999999">Docker-Container, der auf dem lokalen Computer ausgeführt wird</font></i> <br><br>  Wenn Sie dieses Schema von oben nach unten betrachten, stellt sich heraus, dass es eine physische Netzwerkschnittstelle <code>eth0</code> .  Die <code>docker0</code> Bridge ist daran <code>docker0</code> , und die virtuelle Netzwerkschnittstelle von <code>docker0</code> ist <code>veth0</code> der Bridge verbunden.  Beachten Sie, dass sich die <code>veth0</code> <code>docker0</code> und <code>veth0</code> im selben Netzwerk befinden. In diesem Beispiel ist dies <code>172.17.0.0/24</code> .  In diesem Netzwerk wird der <code>docker0</code> Schnittstelle die IP-Adresse <code>172.17.0.1</code> . Diese Schnittstelle ist das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Standard-Gateway</a> für die <code>veth0</code> Schnittstelle, der die Adresse <code>172.17.0.2</code> zugewiesen ist.  Aufgrund der Besonderheiten beim Einrichten von Netzwerk-Namespaces beim Starten des Containers sehen die Prozesse im Container nur die <code>veth0</code> Schnittstelle und interagieren mit der Außenwelt über die <code>docker0</code> und <code>eth0</code> Schnittstellen.  Führen Sie nun den zweiten Container aus. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8b4/33a/915/8b433a91572e4afa0f9652d4e729a8b3.png"></div><br>  <i><font color="#999999">Zwei Docker-Container, die auf dem lokalen Computer ausgeführt werden</font></i> <br><br>  Wie Sie im obigen Diagramm sehen können, wird die neue virtuelle Netzwerkschnittstelle <code>veth1</code> dem zweiten Container zugewiesen, der mit derselben Bridge wie der erste Container verbunden ist - mit <code>docker0</code> .  Dies ist eine ziemlich präzise Beschreibung dessen, was tatsächlich passiert.  Darüber hinaus ist zu beachten, dass die Verbindung zwischen dem Container und der Bridge dank eines Paares verbundener virtueller Ethernet-Schnittstellen hergestellt wird, von denen sich eine im Container-Namespace und die andere im Root-Netzwerk-Namespace befindet.  Details dazu finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> . <br><br>  All dies ist gut, aber es beschreibt noch nicht, was wir in Bezug auf Kubernetes-Pods als "Shared Network Stack" bezeichnen.  Glücklicherweise sind Namespaces sehr flexibel.  Docker kann einen Container starten und anstelle einer neuen virtuellen Netzwerkschnittstelle die vorhandene Schnittstelle zusammen mit anderen Containern verwenden.  Bei diesem Ansatz müssen wir das obige Schema wie unten gezeigt ändern. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/456/4f6/5fb/4564f65fb5ed8773794f98f7655f0523.png"></div><br>  <i><font color="#999999">Container verwenden eine gemeinsame Netzwerkschnittstelle</font></i> <br><br>  Jetzt interagiert der zweite Container mit der bereits vorhandenen <code>veth0</code> Schnittstelle und nicht wie im vorherigen Beispiel mit seiner eigenen <code>veth1</code> Schnittstelle.  Die Verwendung eines solchen Schemas führt zu mehreren Konsequenzen.  Zunächst können wir nun sagen, dass beide Container extern unter derselben Adresse ( <code>172.17.0.2</code> sichtbar sind und in jedem von ihnen auf die Ports auf <code>localhost</code> zugreifen können, die von einem anderen Container geöffnet wurden.  Dies bedeutet außerdem, dass diese Container nicht dieselben Ports öffnen können.  Dies ist natürlich eine Einschränkung, unterscheidet sich jedoch nicht von einer ähnlichen Einschränkung in der Situation, in der mehrere Prozesse Ports auf demselben Host öffnen.  Mit diesem Ansatz erhalten eine Reihe von Prozessen alle Vorteile, die mit der Ausführung dieser Prozesse in Containern verbunden sind, z. B. schlechte Konnektivität und Isolation. Gleichzeitig können Prozesse die Zusammenarbeit in der einfachsten vorhandenen Netzwerkumgebung organisieren. <br><br>  Kubernetes implementiert dieses Muster, indem für jeden Herd ein spezieller Container erstellt wird, dessen einziger Zweck darin besteht, eine Netzwerkschnittstelle für andere Herdcontainer bereitzustellen.  Wenn Sie eine Verbindung zu dem Knoten des Kubernetes-Clusters herstellen, dem von <code>ssh</code> ein bestimmtes Sub zugewiesen wurde, und den <code>docker ps</code> ausführen, wird mindestens ein Container mit dem Befehl <code>pause</code> ausgeführt.  Dieser Befehl unterbricht den aktuellen Prozess, bis ein <code>SIGTERM</code> Signal eintrifft.  Solche Container tun absolut nichts, sie befinden sich in einem "Schlaf" -Zustand und warten auf dieses Signal.  Trotz der Tatsache, dass „suspendierte“ Container nichts bewirken, sind sie sozusagen das „Herz“ des Herdes und bieten anderen Containern eine virtuelle Netzwerkschnittstelle, über die sie miteinander oder mit der Außenwelt interagieren können.  Infolgedessen stellt sich heraus, dass in einer hypothetischen Umgebung, die unter ähnelt, unser vorheriges Schema wie das unten gezeigte aussehen würde. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3b0/e06/d66/3b0e06d66041a5dd0946f43c94314dae.png"></div><br>  <i><font color="#999999">Hypothetische Container</font></i> <br><br><h2>  <font color="#3AC1EF">Herd Netzwerk</font> </h2><br>  Einer unter, voller Container, ist der Baustein eines bestimmten Systems, aber bisher nicht dieses System selbst.  Die Kubernetes-Architektur basiert auf der Anforderung, dass Herde mit anderen Herden interagieren können müssen, unabhängig davon, ob sie auf demselben Computer oder auf verschiedenen Computern ausgeführt werden.  Um zu erfahren, wie dies alles funktioniert, müssen wir auf eine höhere Abstraktionsebene gehen und darüber sprechen, wie Knoten im Kubernetes-Cluster funktionieren.  Hier werden wir das Thema Netzwerkrouting und Routen behandeln.  Dieses Thema wird in solchen Materialien häufig vermieden, da es zu komplex ist.  Es ist nicht einfach, eine verständliche und nicht zu lange Anleitung zum IP-Routing zu finden. Wenn Sie sich jedoch einen kurzen Überblick über dieses Problem verschaffen möchten, können Sie sich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dieses</a> Material ansehen. <br><br>  Der Kubernetes-Cluster besteht aus einem oder mehreren Knoten.  Ein Knoten ist ein physisches oder virtuelles Hostsystem, das verschiedene Softwaretools und deren Abhängigkeiten (hauptsächlich Docker) sowie mehrere Kubernetes-Systemkomponenten enthält.  Der Knoten ist mit dem Netzwerk verbunden, sodass er Daten mit anderen Knoten im Cluster austauschen kann.  So könnte ein einfacher Cluster mit zwei Knoten aussehen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/477/db1/b20/477db1b2030b13c41178a48821916fcc.png"></div><br>  <i><font color="#999999">Ein einfacher Cluster mit zwei Knoten</font></i> <br><br>  Wenn der betreffende Cluster in einer Cloud-Umgebung wie GCP oder AWS ausgeführt wird, vermittelt dieses Schema die Essenz der Standardnetzwerkarchitektur für einzelne Projekte ziemlich genau.  Zu Demonstrationszwecken <code>10.100.0.0/24</code> in diesem Beispiel das private Netzwerk <code>10.100.0.0/24</code> verwendet.  Infolgedessen <code>10.100.0.1</code> dem <code>10.100.0.1</code> die Adresse <code>10.100.0.1</code> zugewiesen, und die Adressen <code>10.100.0.2</code> und <code>10.100.0.3</code> zwei Knoten <code>10.100.0.3</code> .  Mit dieser Architektur kann jeder der Knoten über seine <code>eth0</code> Netzwerkschnittstelle miteinander <code>eth0</code> .  Erinnern wir uns jetzt daran, dass sich under, das auf dem Host ausgeführt wird, nicht in diesem privaten Netzwerk befindet.  Es ist in einem völlig anderen Netzwerk mit der Brücke verbunden.  Dies ist ein virtuelles Netzwerk, das nur innerhalb eines bestimmten Knotens existiert.  Um es klarer zu machen, lassen Sie uns das vorherige Schema neu zeichnen und das hinzufügen, was wir oben als hypothetischen Herd bezeichnet haben. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ff6/721/c32/ff6721c32b5d74bc80fccfbf6164e486.png"></div><br>  <i><font color="#999999">Pods und Knoten</font></i> <br><br>  Der Host links in diesem Diagramm verfügt über eine <code>eht0</code> Schnittstelle mit der Adresse <code>10.100.0.2</code> , deren Standard-Gateway der Router mit der Adresse <code>10.100.0.1</code> .  Die <code>docker0</code> Brücke mit der Adresse <code>172.17.0.1</code> mit dieser Schnittstelle verbunden, und über die virtuelle Schnittstelle <code>veth0</code> mit der Adresse <code>172.17.0.2</code> ist mit ihr verbunden, was wir hier den Herd nennen.  Die <code>veth0</code> Schnittstelle wurde in einem angehaltenen Container erstellt.  Es ist in allen drei Containern über einen gemeinsam genutzten Netzwerkstapel sichtbar.  Aufgrund der Tatsache, dass beim Erstellen der Bridge lokale Routing-Regeln konfiguriert werden, wird jedes Paket, das bei <code>eth0</code> und die Zieladresse <code>172.17.0.2</code> hat, an die Bridge umgeleitet, die es an die virtuelle Schnittstelle <code>veth0</code> .  Während das alles ziemlich anständig aussieht.  Wenn bekannt ist, dass der von uns diskutierte Host die Adresse <code>172.17.0.2</code> , können wir den Router-Einstellungen eine Regel hinzufügen, die beschreibt, dass der nächste Übergang für diese Adresse <code>10.100.0.2</code> ist. <code>veth0</code> sollten Pakete von dort an <code>veth0</code> umgeleitet <code>veth0</code> .  Hervorragend.  Schauen wir uns jetzt einen anderen Host an. <br><br>  Der im Diagramm rechts gezeigte Host hat eine physikalische Schnittstelle <code>eth0</code> mit der Adresse <code>10.100.0.3</code> .  Es verwendet dasselbe Standard-Gateway - <code>10.100.0.1</code> - und ist erneut mit der <code>docker0</code> Bridge mit der Adresse <code>172.17.0.1</code> .  Es besteht das Gefühl, dass nicht alles so gut läuft.  Diese Adresse kann tatsächlich von der auf dem Host auf der linken Seite verwendeten abweichen.  Die Adressen der Bridges hier werden gleich gemacht, um das schlimmste Szenario zu demonstrieren, das beispielsweise auftreten kann, wenn Sie Docker gerade installiert haben und es nach Ihren Wünschen arbeiten lassen.  Aber selbst wenn die fraglichen Netzwerke unterschiedlich sind, zeigt unser Beispiel ein tieferes Problem, nämlich dass Knoten normalerweise nichts darüber wissen, welche privaten Adressen Bridges zugewiesen sind, die sich an anderen Knoten befinden.  Und wir müssen darüber Bescheid wissen - um Pakete an diese Bridges senden zu können und um sicher zu sein, dass sie dort ankommen, wo sie gebraucht werden.  Offensichtlich benötigen wir hier eine Art Entität, die es uns ermöglicht, die korrekte Konfiguration von Adressen in verschiedenen Knoten sicherzustellen. <br><br>  Die Kubernetes-Plattform bietet uns eine zweistufige Lösung für dieses Problem.  Diese Plattform weist zunächst einen gemeinsamen Adressraum für Bridges in jedem Knoten zu und weist Bridges dann die Adressen in diesem Raum zu, basierend darauf, in welchem ​​Knoten sich die Bridge befindet.  Zweitens fügt Kubernetes dem Gateway Routing-Regeln hinzu, das sich in unserem Fall um <code>10.100.0.1</code> .  Diese Regeln definieren die Regeln für das Weiterleiten von Paketen, die für jede der Bridges bestimmt sind.  Das heißt, sie beschreiben, über welche physikalische Schnittstelle <code>eth0</code> mit jeder der Brücken kontaktiert werden kann.  Diese Kombination aus virtuellen Netzwerkschnittstellen, Bridges und Routing-Regeln wird üblicherweise als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Overlay-Netzwerk bezeichnet</a> .  Apropos Kubernetes, ich nenne dieses Netzwerk normalerweise ein "Herdnetzwerk", da es sich um ein Overlay-Netzwerk handelt, mit dem Pods an verschiedenen Knoten miteinander kommunizieren können.  So sieht das vorherige Diagramm aus, nachdem die Kubernetes-Mechanismen zur Sache gekommen sind. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b9c/f87/c96/b9cf87c96d0169dbcd0846f8bb3cd323.png"></div><br>  <i><font color="#999999">Herd Netzwerk</font></i> <br><br>  Es fällt sofort auf, dass die Brückennamen von <code>docker0</code> in <code>cbr0</code> .  Kubernetes verwendet keine Standard-Docker-Bridges.  Was wir <code>cbr</code> ist eine Abkürzung für "benutzerdefinierte Brücke", das heißt, wir sprechen über einige spezielle Brücken.  Ich bin nicht bereit, eine vollständige Liste der Unterschiede zwischen dem Starten von Docker-Containern in Pods und dem Ausführen auf normalen Computern zu geben, aber wir sprechen hier von einem der wichtigen ähnlichen Unterschiede.  Außerdem müssen Sie darauf achten, dass der in diesem Beispiel Bridges zugewiesene Adressraum <code>10.0.0.0/14</code> .  Diese Adresse stammt aus einem unserer Staging-Cluster, die auf der Google Cloud-Plattform bereitgestellt werden. Das obige ist also ein sehr reales Beispiel für ein Herdnetzwerk.  Ihrem Cluster kann ein völlig anderer Adressbereich zugewiesen werden.  Leider gibt es derzeit keine Möglichkeit, Informationen über diese Adressen mit dem Dienstprogramm <code>kubectl</code> Wenn Sie jedoch beispielsweise GCP verwenden, können Sie einen Befehl wie <code>gcloud container clusters describe &lt;cluster&gt;</code> <code>clusterIpv4Cidr</code> Eigenschaft <code>clusterIpv4Cidr</code> . <br><br>  Im Allgemeinen kann festgestellt werden, dass Sie normalerweise nicht über die Funktionsweise des Herdnetzwerks nachdenken müssen.  Wenn ein Sub Daten mit einem anderen Herd austauscht, geschieht dies meistens über Kubernetes-Dienste.  Dies ist ein bisschen ein softwaredefinierter Proxy.  Die Netzwerkadressen der Herde werden jedoch in den Protokollen angezeigt.  In einigen Situationen, insbesondere während des Debuggens, müssen Sie möglicherweise explizit Routing-Regeln in Herdnetzwerken festlegen.  Beispielsweise wird Datenverkehr, bei dem Kubernetes an eine Adresse im Bereich 10.0.0.0/8 gebunden bleibt, standardmäßig nicht mit NAT verarbeitet.  Wenn Sie mit Diensten interagieren, die sich in einem anderen privaten Netzwerk mit demselben Adressbereich befinden, müssen Sie möglicherweise Routing-Regeln konfigurieren, mit denen Sie die korrekte Paketzustellung organisieren können. <br><br><h2>  <font color="#3AC1EF">Zusammenfassung</font> </h2><br>  Heute haben wir über Kubernetes Pods und die Funktionen ihrer Vernetzung gesprochen.  Wir hoffen, dass dieses Material Ihnen hilft, die richtigen Schritte zur Implementierung komplexer Herdinteraktionsszenarien in Kubernetes-Netzwerken zu unternehmen. <br><br>  <b>Liebe Leser!</b>  Dieser Artikel ist der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erste in einer</a> Reihe von Kubernetes-Netzwerken.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Der zweite</a> Teil dieses Zyklus wurde bereits <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">übersetzt</a> .  Wir überlegen, ob wir den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dritten</a> Teil übersetzen sollen.  Wir bitten Sie, dies in den Kommentaren zu kommentieren. <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de441576/">https://habr.com/ru/post/de441576/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de441566/index.html">12 Wissenswertes zu JavaScript-Konzepten</a></li>
<li><a href="../de441568/index.html">Python-Speicherverwaltung</a></li>
<li><a href="../de441570/index.html">Die Verdauung von frischen Materialien aus der Welt des Frontends für die letzte Woche Nr. 353 (17. - 24. Februar 2019)</a></li>
<li><a href="../de441572/index.html">Frontend Weekly Digest (18. - 24. Februar 2019)</a></li>
<li><a href="../de441574/index.html">Docker lernen Teil 6: Arbeiten mit Daten</a></li>
<li><a href="../de441578/index.html">React Tutorial Teil 19: Methoden des Komponentenlebenszyklus</a></li>
<li><a href="../de441580/index.html">React Tutorial Teil 20: Erste Lektion zum bedingten Rendern</a></li>
<li><a href="../de441582/index.html">Optimierung des LQR-Steuerungssystems</a></li>
<li><a href="../de441584/index.html">PHP Digest Nr. 150 (11. - 25. Februar 2019)</a></li>
<li><a href="../de441586/index.html">Wie man Musik empfiehlt, die fast niemand gehört hat. Yandex-Bericht</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>