<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßòüèæ üéöÔ∏è üé© Nomad: problemas e solu√ß√µes üÜó üéÖüèª üò•</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="O primeiro servi√ßo no Nomad I foi lan√ßado em setembro de 2016. No momento, eu o uso como programador e apoio como administrador de dois clusters Nomad...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Nomad: problemas e solu√ß√µes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/435132/"><p>  O primeiro servi√ßo no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Nomad</a> I foi lan√ßado em setembro de 2016.  No momento, eu o uso como programador e apoio como administrador de dois clusters Nomad - um "lar" para meus projetos pessoais (6 m√°quinas micro-virtuais no Hetzner Cloud e ArubaCloud em 5 data centers diferentes na Europa) e o segundo em funcionamento (cerca de 40 servidores virtuais e f√≠sicos privados) em dois data centers). </p><br><p>  Nos √∫ltimos tempos, muita experi√™ncia foi acumulada com o ambiente Nomad. No artigo, descreverei os problemas encontrados pelo Nomad e como lidar com eles. </p><br><p><img src="https://habrastorage.org/webt/k5/9m/pp/k59mpp5iyvtxtj2q9nrvthzpelo.jpeg"><br>  <em>Yamal nomad cria inst√¢ncia de entrega cont√≠nua do seu software ¬© National Geographic Russia</em> </p><a name="habracut"></a><br><h2 id="1-kolichestvo-servernyh-nod-na-odin-datacentr">  1. O n√∫mero de n√≥s do servidor por data center </h2><br><p>  <strong>Solu√ß√£o: um n√≥ do servidor √© suficiente para um data center.</strong> </p><br><p>  A <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o</a> n√£o indica explicitamente quantos n√≥s do servidor s√£o necess√°rios em um datacenter.  √â indicado apenas que s√£o necess√°rios 3-5 n√≥s por regi√£o, o que √© l√≥gico para o consenso do protocolo de jangada. </p><br><p><img src="https://habrastorage.org/webt/go/yt/gu/goytgumjr0zxxqodicboxfgipze.png"></p><br><p>  No come√ßo, planejei 2-3 n√≥s de servidor em cada data center para fornecer redund√¢ncia. </p><br><p>  Ap√≥s o uso, descobriu-se: </p><br><ol><li>  Isso simplesmente n√£o √© necess√°rio, pois, no caso de uma falha no n√≥ no datacenter, a fun√ß√£o do n√≥ do servidor para os agentes nesse datacenter ser√° desempenhada por outros n√≥s na regi√£o. </li><li>  Acontece ainda pior se o problema 8 n√£o for resolvido.  Quando o assistente √© reeleito, podem ocorrer inconsist√™ncias e o Nomad reiniciar√° parte dos servi√ßos. </li></ol><br><h2 id="2-resursy-servera-dlya-servernoy-nody">  2. Recursos do servidor para o n√≥ do servidor </h2><br><p>  <strong>Solu√ß√£o: uma pequena m√°quina virtual √© suficiente para o n√≥ do servidor.</strong>  <strong>No mesmo servidor, √© permitido executar outros servi√ßos que n√£o consomem muitos recursos.</strong> </p><br><p>  O consumo de mem√≥ria do daemon Nomad depende do n√∫mero de tarefas em execu√ß√£o.  Consumo de CPU - com base no n√∫mero de tarefas e no n√∫mero de servidores / agentes na regi√£o (n√£o linear). </p><br><p>  No nosso caso: para 300 tarefas em execu√ß√£o, o consumo de mem√≥ria √© de cerca de 500 MB para o n√≥ principal atual. </p><br><p>  Em um cluster em funcionamento, uma m√°quina virtual para um n√≥ do servidor: 4 CPU, 6 GB RAM. <br>  Lan√ßamento adicional: Consul, Etcd, Vault. </p><br><h2 id="3-konsensus-pri-nehvatke-datacentrov">  3. Consenso sobre a falta de data centers </h2><br><p>  <strong>Solu√ß√£o: criamos tr√™s data centers virtuais e tr√™s n√≥s de servidor para dois data centers f√≠sicos.</strong> </p><br><p>  O trabalho do Nomad na regi√£o √© baseado no protocolo de jangada.  Para uma opera√ß√£o correta, voc√™ precisa de pelo menos tr√™s n√≥s de servidor localizados em diferentes datacenters.  Isso permitir√° a opera√ß√£o correta com uma perda completa da conectividade de rede com um dos data centers. </p><br><p>  Mas temos apenas dois data centers.  Assumimos um compromisso: selecionamos um data center em que confiamos mais e criamos um n√≥ de servidor adicional.  Fazemos isso introduzindo um data center virtual adicional, que estar√° fisicamente localizado no mesmo data center (consulte o par√°grafo 2 do problema 1). </p><br><p>  <strong>Solu√ß√£o alternativa: dividimos os data centers em regi√µes separadas.</strong> </p><br><p>  Como resultado, os data centers funcionam de forma independente e o consenso √© necess√°rio apenas dentro de um data center.  Dentro de um datacenter, nesse caso, √© melhor criar tr√™s n√≥s de servidor implementando tr√™s datacenters virtuais em um f√≠sico. </p><br><p>  Essa op√ß√£o √© menos conveniente para a distribui√ß√£o de tarefas, mas oferece uma garantia de 100% da independ√™ncia dos servi√ßos em caso de problemas de rede entre os data centers. </p><br><h2 id="4-server-i-agent-na-odnom-servere">  4. "Servidor" e "agente" no mesmo servidor </h2><br><p>  <strong>Solu√ß√£o: v√°lida se voc√™ tiver um n√∫mero limitado de servidores.</strong> </p><br><p>  A documenta√ß√£o do Nomad diz que fazer isso √© indesej√°vel.  Mas se voc√™ n√£o tiver a oportunidade de alocar m√°quinas virtuais separadas para n√≥s do servidor, poder√° colocar os n√≥s do servidor e do agente no mesmo servidor. </p><br><p>  Executar simultaneamente significa iniciar o daemon Nomad no modo cliente e no servidor. </p><br><p>  O que isso amea√ßa?  Com uma carga pesada na CPU deste servidor, o n√≥ do servidor Nomad funcionar√° de maneira inst√°vel, perda de consenso e pulsa√ß√µes, √© poss√≠vel recarregar o servi√ßo. <br>  Para evitar isso, aumentamos os limites da descri√ß√£o do problema n¬∫ 8. </p><br><h2 id="5-realizaciya-prostranstv-imyon-namespaces">  5. Implementa√ß√£o de namespaces </h2><br><p>  <strong>Solu√ß√£o: talvez atrav√©s da organiza√ß√£o de um data center virtual.</strong> </p><br><p>  √Äs vezes, voc√™ precisa executar parte dos servi√ßos em servidores separados. </p><br><p>  A solu√ß√£o √© a primeira, simples, mas mais exigente em recursos.  Dividimos todos os servi√ßos em grupos de acordo com sua finalidade: frontend, backend, ... Adicione metatributos aos servidores, prescreva os atributos a serem executados para todos os servi√ßos. </p><br><p>  A segunda solu√ß√£o √© simples.  Adicionamos novos servidores, prescrevemos meta atributos para eles, prescrevemos esses atributos de inicializa√ß√£o para os servi√ßos necess√°rios, e todos os outros servi√ßos prescrevem uma proibi√ß√£o de inicializa√ß√£o em servidores com esse atributo. </p><br><p>  A terceira solu√ß√£o √© complicada.  Criamos um datacenter virtual: inicie o Consul para um novo datacenter, inicie o n√≥ do servidor Nomad para esse datacenter, sem esquecer o n√∫mero de n√≥s do servidor para esta regi√£o.  Agora voc√™ pode executar servi√ßos individuais neste data center virtual dedicado. </p><br><h2 id="6-integraciya-s-vault">  6. Integra√ß√£o com o Vault </h2><br><p>  <strong>Solu√ß√£o: Evite as depend√™ncias circulares do Nomad &lt;-&gt; Vault.</strong> </p><br><p>  O Vault lan√ßado n√£o deve ter nenhuma depend√™ncia do Nomad.  O endere√ßo do Vault registrado no Nomad deve, de prefer√™ncia, apontar diretamente para o Vault, sem camadas de balanceadores (mas v√°lidas).  A reserva de cofre neste caso pode ser feita via DNS - Consul DNS ou externo. </p><br><p>  Se os dados do Vault forem gravados nos arquivos de configura√ß√£o do Nomad, o Nomad tentar√° acessar o Vault na inicializa√ß√£o.  Se o acesso n√£o tiver √™xito, o Nomad se recusa a iniciar. </p><br><p>  Cometi um erro com uma depend√™ncia c√≠clica h√° muito tempo, que uma vez destruiu completamente o cluster Nomad.  O Vault foi iniciado corretamente, independentemente do Nomad, mas o Nomad examinou o endere√ßo do Vault atrav√©s dos balanceadores que estavam sendo executados no pr√≥prio Nomad.  A reconfigura√ß√£o e a reinicializa√ß√£o dos n√≥s do servidor Nomad causaram uma reinicializa√ß√£o dos servi√ßos do balanceador, o que levou a uma falha ao iniciar os pr√≥prios n√≥s do servidor. </p><br><h2 id="7-zapusk-vazhnyh-statefull-servisov">  7. Lan√ßando importantes servi√ßos estaduais </h2><br><p>  <strong>Solu√ß√£o: v√°lida, mas n√£o.</strong> </p><br><p>  √â poss√≠vel executar PostgreSQL, ClickHouse, Redis Cluster, RabbitMQ, MongoDB via Nomad? </p><br><p>  Imagine que voc√™ tenha um conjunto de servi√ßos importantes, cujo trabalho est√° vinculado √† maioria dos outros servi√ßos.  Por exemplo, um banco de dados no PostgreSQL / ClickHouse.  Ou armazenamento geral de curto prazo no Redis Cluster / MongoDB.  Ou um barramento de dados no Redis Cluster / RabbitMQ. </p><br><p>  Todos esses servi√ßos de alguma forma implementam um esquema tolerante a falhas: Stolon / Patroni for PostgreSQL, sua pr√≥pria implementa√ß√£o de jangada no Redis Cluster, sua pr√≥pria implementa√ß√£o de cluster no RabbitMQ, MongoDB, ClickHouse. </p><br><p>  Sim, todos esses servi√ßos podem ser iniciados pelo Nomad com refer√™ncia a servidores espec√≠ficos, mas por qu√™? </p><br><p>  Plus - facilidade de lan√ßamento, um √∫nico formato de script, como outros servi√ßos.  N√£o precisa se preocupar com scripts ansible / qualquer outra coisa. </p><br><p>  Menos √© um ponto adicional de falha, que n√£o oferece nenhuma vantagem.  Pessoalmente, abandonei completamente o cluster Nomad duas vezes por v√°rios motivos: uma vez "em casa", uma vez trabalhando.  Isso foi nos est√°gios iniciais da introdu√ß√£o do Nomad e devido √† neglig√™ncia. <br>  Al√©m disso, o Nomad come√ßa a se comportar mal e reiniciar os servi√ßos devido ao problema n√∫mero 8.  Mas, mesmo que esse problema seja resolvido, o perigo permanece. </p><br><h2 id="8-stabilizaciya-raboty-i-restartov-servisov-v-nestabilnoy-seti">  8. A estabiliza√ß√£o do trabalho e do servi√ßo √© reiniciada em uma rede inst√°vel </h2><br><p>  <strong>Solu√ß√£o: use as op√ß√µes de ajuste de pulsa√ß√£o.</strong> </p><br><p>  Por padr√£o, o Nomad √© configurado para que qualquer problema de rede a curto prazo ou carga de CPU cause perda de consenso e reelei√ß√£o do assistente ou marque o n√≥ do agente como inacess√≠vel.  E isso leva a reinicializa√ß√µes espont√¢neas de servi√ßos e sua transfer√™ncia para outros n√≥s. </p><br><p>  Estat√≠sticas do cluster "inicial" antes de corrigir o problema: a vida √∫til m√°xima do cont√™iner antes de reiniciar √© de aproximadamente 10 dias.  Aqui, ainda √© sobrecarregado executar o agente e o servidor em um servidor e coloc√°-lo em 5 centros de dados diferentes na Europa, o que implica uma grande carga na CPU e uma rede menos est√°vel. </p><br><p>  Estat√≠sticas do cluster de trabalho antes de corrigir o problema: a vida √∫til m√°xima do cont√™iner antes de reiniciar √© mais de 2 meses.  Tudo aqui √© relativamente bom por causa dos servidores separados para os n√≥s do Nomad e da excelente rede entre os data centers. </p><br><p>  Valores padr√£o </p><br><pre><code class="plaintext hljs">heartbeat_grace = "10s" min_heartbeat_ttl = "10s" max_heartbeats_per_second = 50.0</code> </pre> <br><p>  A julgar pelo c√≥digo: nessa configura√ß√£o, as pulsa√ß√µes s√£o feitas a cada 10 segundos.  Com a perda de duas pulsa√ß√µes, come√ßa a reelei√ß√£o do mestre ou a transfer√™ncia de servi√ßos do n√≥ do agente.  Configura√ß√µes controversas, na minha opini√£o.  N√≥s os editamos dependendo do aplicativo. </p><br><p>  Se voc√™ possui todos os servi√ßos em execu√ß√£o em v√°rias inst√¢ncias e √© distribu√≠do pelos data centers, provavelmente n√£o importa para voc√™ um longo per√≠odo de determina√ß√£o da inacessibilidade do servidor (cerca de 5 minutos, no exemplo abaixo) - tornamos menos frequente o intervalo de pulsa√ß√£o e um per√≠odo mais longo de determina√ß√£o da inacessibilidade.  Este √© um exemplo de configura√ß√£o do meu cluster inicial: </p><br><pre> <code class="plaintext hljs"> heartbeat_grace = "300s" min_heartbeat_ttl = "30s" max_heartbeats_per_second = 10.0</code> </pre> <br><p>  Se voc√™ tiver boa conectividade de rede, servidores separados para n√≥s do servidor e o per√≠odo para determinar a inacessibilidade do servidor for importante (h√° algum servi√ßo em execu√ß√£o em uma inst√¢ncia e √© importante transferi-lo rapidamente), aumente o per√≠odo para determinar a inacessibilidade (heartbeat_grace).  Opcionalmente, voc√™ pode fazer mais pulsa√ß√µes (diminuindo min_heartbeat_ttl) - isso aumentar√° levemente a carga na CPU.  Exemplo de configura√ß√£o de cluster de trabalho: </p><br><pre> <code class="plaintext hljs"> heartbeat_grace = "60s" min_heartbeat_ttl = "10s" max_heartbeats_per_second = 50.0</code> </pre> <br><p>  Essas configura√ß√µes corrigem completamente o problema. </p><br><h2 id="9-zapusk-periodicheskih-zadach">  9. Iniciando Tarefas Peri√≥dicas </h2><br><p>  <strong>Solu√ß√£o: Servi√ßos peri√≥dicos n√¥mades podem ser usados, mas o cron √© mais conveniente para suporte.</strong> </p><br><p>  Nomad tem a capacidade de iniciar o servi√ßo periodicamente. </p><br><p>  A √∫nica vantagem √© a simplicidade dessa configura√ß√£o. </p><br><p>  O primeiro ponto negativo √© que, se o servi√ßo for iniciado com freq√º√™ncia, ele ir√° desarrumar a lista de tarefas.  Por exemplo, na inicializa√ß√£o a cada 5 minutos, 12 tarefas extras ser√£o adicionadas √† lista a cada hora, at√© que o GC Nomad seja acionado, o que excluir√° as tarefas antigas. </p><br><p>  O segundo menos - n√£o est√° claro como configurar corretamente o monitoramento de um servi√ßo desse tipo.  Como entender que um servi√ßo inicia, cumpre e faz seu trabalho at√© o fim? </p><br><p>  Como resultado, vim para a implementa√ß√£o "cron" de tarefas peri√≥dicas: </p><br><ol><li>  Pode ser um cron regular em um cont√™iner em execu√ß√£o constante.  Cron executa periodicamente um determinado script.  Uma verifica√ß√£o de integridade do script √© facilmente adicionada a esse cont√™iner, que verifica qualquer sinalizador que crie um script em execu√ß√£o. </li><li>  Pode ser um cont√™iner em execu√ß√£o constante, com um servi√ßo em execu√ß√£o constante.  Um lan√ßamento peri√≥dico j√° foi implementado dentro do servi√ßo.  Um script-healthcheck semelhante ou http-healthcheck pode ser facilmente adicionado a esse servi√ßo, que verifica o status imediatamente por seu "interior". </li></ol><br><p>  No momento em que escrevo a maior parte do tempo no Go, respectivamente, prefiro a segunda op√ß√£o com o http healthcheck - on Go e o lan√ßamento peri√≥dico, e o http healthcheck'i √© adicionado com algumas linhas de c√≥digo. </p><br><h2 id="10-obespechenie-rezervirovaniya-servisov">  10. Presta√ß√£o de servi√ßos redundantes </h2><br><p>  <strong>Solu√ß√£o: N√£o existe uma solu√ß√£o simples.</strong>  <strong>Existem mais duas op√ß√µes dif√≠ceis.</strong> </p><br><p>  O esquema de provisionamento fornecido pelos desenvolvedores do Nomad √© oferecer suporte ao n√∫mero de servi√ßos em execu√ß√£o.  Voc√™ diz que o n√¥made "lan√ßa-me 5 inst√¢ncias do servi√ßo" e ele as inicia em algum lugar por l√°.  N√£o h√° controle sobre a distribui√ß√£o.  Inst√¢ncias podem ser executadas no mesmo servidor. </p><br><p>  Se o servidor travar, as inst√¢ncias ser√£o transferidas para outros servidores.  Enquanto as inst√¢ncias est√£o sendo transferidas, o servi√ßo n√£o funciona.  Esta √© uma op√ß√£o ruim de provis√£o de reserva. </p><br><p>  Fazemos certo: </p><br><ol><li>  Distribu√≠mos inst√¢ncias nos servidores atrav√©s de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">distinct_hosts</a> . </li><li>  Distribu√≠mos inst√¢ncias entre data centers.  Infelizmente, apenas criando uma c√≥pia do script do formul√°rio service1, service2 com o mesmo conte√∫do, nomes diferentes e uma indica√ß√£o do lan√ßamento em diferentes data centers. </li></ol><br><p>  No Nomad 0.9, aparecer√° uma funcionalidade que resolver√° esse problema: ser√° poss√≠vel distribuir servi√ßos em uma propor√ß√£o percentual entre servidores e data centers. </p><br><h2 id="11-web-ui-nomad">  11. Web UI Nomad </h2><br><p>  <strong>Solu√ß√£o: a interface do usu√°rio embutida √© terr√≠vel, o hashi-ui √© bonito.</strong> </p><br><p>  O cliente do console executa a maioria das funcionalidades necess√°rias, mas √†s vezes voc√™ deseja ver os gr√°ficos, pressione os bot√µes ... </p><br><p>  Nomad tem uma interface de usu√°rio integrada.  N√£o √© muito conveniente (ainda pior que o console). </p><br><p><img src="https://habrastorage.org/webt/0s/fv/6y/0sfv6yrspbj5easwnweyx8yzsme.png"></p><br><p>  A √∫nica alternativa que conhe√ßo √© o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">hashi-ui</a> . </p><br><p><img src="https://habrastorage.org/webt/vd/4x/rv/vd4xrvrrnewmotnnio-yxbvriis.png"></p><br><p>  De fato, agora eu pessoalmente preciso do cliente do console apenas para "execu√ß√£o n√¥made".  E mesmo isso planeja transferir para a CI. </p><br><h2 id="12-podderzhka-oversubscription-po-pamyati">  12. Suporte para excesso de assinaturas da mem√≥ria </h2><br><p>  <strong>Solu√ß√£o: n√£o.</strong> </p><br><p>  Na vers√£o atual do Nomad, voc√™ deve especificar um limite estrito de mem√≥ria para o servi√ßo.  Se o limite for excedido, o servi√ßo ser√° eliminado pelo OOM Killer. </p><br><p>  Excesso de assinatura √© quando os limites de um servi√ßo podem ser especificados "de e para".  Alguns servi√ßos exigem mais mem√≥ria na inicializa√ß√£o do que durante a opera√ß√£o normal.  Alguns servi√ßos podem consumir mais mem√≥ria do que o normal por um curto per√≠odo de tempo. </p><br><p>  A escolha de uma restri√ß√£o estrita ou flex√≠vel √© um t√≥pico para discuss√£o, mas, por exemplo, o Kubernetes permite que o programador fa√ßa uma escolha.  Infelizmente, nas vers√µes atuais do Nomad n√£o existe essa possibilidade.  Eu admito que aparecer√° em vers√µes futuras. </p><br><h2 id="13-ochistka-servera-ot-servisov-nomad">  13. Limpando o Servidor dos Servi√ßos Nomad </h2><br><p>  <strong>Solu√ß√£o:</strong> </p><br><pre> <code class="plaintext hljs">sudo systemctl stop nomad mount | fgrep alloc | awk '{print $3}' | xargs -I QQ sudo umount QQ sudo rm -rf /var/lib/nomad sudo docker ps | grep -v '(-1|-2|...)' | fgrep -v IMAGE | awk '{print $1}' | xargs -I QQ sudo docker stop QQ sudo systemctl start nomad</code> </pre> <br><p>  √Äs vezes "algo d√° errado".  No servidor, ele mata o n√≥ do agente e se recusa a iniciar.  Ou o n√≥ do agente para de responder.  Ou o n√≥ do agente "perde" servi√ßos neste servidor. <br>  Isso √†s vezes acontecia com vers√µes mais antigas do Nomad, agora isso n√£o acontece ou muito raramente. </p><br><p>  O que, neste caso, √© o mais f√°cil de fazer, dado que o servidor de drenagem n√£o produzir√° o resultado desejado?  Limpamos o servidor manualmente: </p><br><ol><li>  Pare o agente n√¥made. </li><li>  Fa√ßa um valor na montagem que cria. </li><li>  Exclua todos os dados do agente. </li><li>  Removemos todos os cont√™ineres filtrando os cont√™ineres de servi√ßo (se houver). </li><li>  Come√ßamos o agente. </li></ol><br><h2 id="14-kak-luchshe-razvorachivat-nomad">  14. Qual √© a melhor maneira de implantar o Nomad? </h2><br><p>  <strong>Solu√ß√£o: √© claro, atrav√©s do Consul.</strong> </p><br><p>  O Consul neste caso n√£o √© de forma alguma uma camada extra, mas um servi√ßo que se encaixa organicamente na infraestrutura, que oferece mais vantagens do que desvantagens: DNS, armazenamento KV, pesquisa de servi√ßos, monitoramento da disponibilidade do servi√ßo, capacidade de trocar informa√ß√µes com seguran√ßa. </p><br><p>  Al√©m disso, ele se desenrola t√£o facilmente quanto o pr√≥prio Nomad. </p><br><h2 id="15-chto-luchshe---nomad-ili-kubernetes">  15. Qual √© o melhor - Nomad ou Kubernetes? </h2><br><p>  <strong>Solu√ß√£o: depende de ...</strong> </p><br><p>  Anteriormente, √†s vezes eu pensava em iniciar uma migra√ß√£o para o Kubernetes - fiquei muito irritado com a reinicializa√ß√£o espont√¢nea peri√≥dica dos servi√ßos (consulte o problema n√∫mero 8).  Mas, depois de uma solu√ß√£o completa para o problema, posso dizer: Nomad combina comigo no momento. </p><br><p>  Por outro lado: o Kubernetes tamb√©m possui uma recarga semi-espont√¢nea de servi√ßos - quando o agendador do Kubernetes redistribui as inst√¢ncias, dependendo da carga.  Isso n√£o √© muito legal, mas √© prov√°vel que esteja configurado. </p><br><p>  Vantagens do Nomad: a infraestrutura √© muito f√°cil de implantar, scripts simples, boa documenta√ß√£o, suporte interno ao Consul / Vault, o que, por sua vez, fornece: uma solu√ß√£o simples para o problema do armazenamento de senhas, DNS embutido, verifica√ß√µes de seguran√ßa f√°ceis de configurar. </p><br><p>  Pr√≥s de Kubernetes: Agora √© um "padr√£o de fato".  Boa documenta√ß√£o, muitas solu√ß√µes prontas, com boa descri√ß√£o e padroniza√ß√£o do lan√ßamento. </p><br><p>  Infelizmente, n√£o tenho a mesma grande experi√™ncia no Kubernetes para responder inequivocamente √† pergunta - o que usar para o novo cluster.  Depende das necessidades planejadas. <br>  Se voc√™ tem muitos namespaces planejados (problema n√∫mero 5) ou seus servi√ßos espec√≠ficos consomem muita mem√≥ria no in√≠cio, liberando-a (problema n√∫mero 12) - definitivamente o Kubernetes, porque  esses dois problemas no Nomad n√£o s√£o totalmente resolvidos ou inconvenientes. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt435132/">https://habr.com/ru/post/pt435132/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt435120/index.html">Fun√ß√µes Lambda no SQL ... vamos pensar</a></li>
<li><a href="../pt435122/index.html">Como a chama foi implementada em Doom no Playstation</a></li>
<li><a href="../pt435124/index.html">Obras-primas da constru√ß√£o mundial de colunas: monitor-transformador de est√∫dio com um n√∫mero vari√°vel de bandas</a></li>
<li><a href="../pt435126/index.html">Experi√™ncia na organiza√ß√£o e realiza√ß√£o de confer√™ncias corporativas para analistas</a></li>
<li><a href="../pt435128/index.html">Pi-Sonos: um hobby fora de controle</a></li>
<li><a href="../pt435134/index.html">Simplifique o trabalho com bancos de dados no Qt com QSqlRelationalTableModel</a></li>
<li><a href="../pt435136/index.html">Sergey e o m√©todo cient√≠fico</a></li>
<li><a href="../pt435138/index.html">Como assumir o controle de sua infraestrutura de rede. CAP√çTULO TR√äS Seguran√ßa de rede. Parte um</a></li>
<li><a href="../pt435142/index.html">Rastreio de aprendizado usando o eBPF: um guia e exemplos</a></li>
<li><a href="../pt435144/index.html">Introdu√ß√£o ao Spring Boot: Criando uma API REST Simples em Java</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>