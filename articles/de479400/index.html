<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë∑ ü§ôüèº üåñ Wie man aus einem neuronalen Netzwerk einen Journalisten macht oder "Geheimnisse der wortlosen Reduzierung des Textes auf Habr√©" ‚ôøÔ∏è üçë ü§üüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Seien Sie einfach nicht √ºberrascht, aber die zweite √úberschrift dieses Beitrags erzeugte ein neuronales Netzwerk, oder vielmehr den Algorithmus der Sa...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie man aus einem neuronalen Netzwerk einen Journalisten macht oder "Geheimnisse der wortlosen Reduzierung des Textes auf Habr√©"</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/abbyy/blog/479400/"><img src="https://habrastorage.org/webt/iq/f_/fg/iqf_fgkfyqb1rbtwvba0p4gbsiq.png" align="left">  Seien Sie einfach nicht √ºberrascht, aber die zweite √úberschrift dieses Beitrags erzeugte ein neuronales Netzwerk, oder vielmehr den Algorithmus der Sammarisierung.  Und was ist Sammarisierung? <br><br>  Dies ist eine der zentralen und klassischen <a href="https://habr.com/ru/company/abbyy/blog/437008/">Herausforderungen der Verarbeitung nat√ºrlicher Sprache (Natural Language Processing, NLP)</a> .  Es besteht darin, einen Algorithmus zu erstellen, der Text als Eingabe verwendet und eine gek√ºrzte Version davon ausgibt.  Dar√ºber hinaus bleibt die korrekte Struktur (entsprechend den Normen der Sprache) erhalten und die Hauptidee des Textes wird korrekt √ºbermittelt. <br><br>  Solche Algorithmen sind in der Industrie weit verbreitet.  Beispielsweise sind sie f√ºr Suchmaschinen n√ºtzlich: Mithilfe der Textreduzierung k√∂nnen Sie leicht nachvollziehen, ob die Hauptidee einer Site oder eines Dokuments mit einer Suchanfrage korreliert.  Sie werden verwendet, um in einem gro√üen Strom von Mediendaten nach relevanten Informationen zu suchen und Informationsm√ºll herauszufiltern.  Textreduktion hilft bei der Finanzanalyse, bei der Analyse von Rechtsvertr√§gen, der Kommentierung von wissenschaftlichen Arbeiten und vielem mehr.  √úbrigens, der Sammarisierungsalgorithmus hat alle Unter√ºberschriften f√ºr diesen Beitrag generiert. <br><br>  Zu meiner √úberraschung gab es auf Habr√© nur sehr wenige Artikel zum Thema Sammarisierung. Ich beschloss, meine Forschungen und Ergebnisse in diese Richtung zu teilen.  Dieses Jahr nahm ich an der Rennstrecke der <a href="http://www.dialog-21.ru/">Dialogkonferenz</a> teil und experimentierte mit √úberschriftengeneratoren f√ºr Nachrichten und Gedichte, die neuronale Netze verwendeten.  In diesem Beitrag werde ich zun√§chst kurz auf den theoretischen Teil der Sammarisierung eingehen und anschlie√üend Beispiele f√ºr die Generierung von √úberschriften geben. Ich werde Ihnen erl√§utern, welche Schwierigkeiten Modelle beim K√ºrzen des Texts haben und wie diese Modelle verbessert werden k√∂nnen, um bessere √úberschriften zu erzielen. <br><a name="habracut"></a><br>  Unten finden Sie ein Beispiel f√ºr eine Nachricht und ihre urspr√ºngliche Referenz√ºberschrift.  Die Modelle, √ºber die ich sprechen werde, werden anhand des folgenden Beispiels so trainiert, dass sie Header generieren: <br><br><img src="https://habrastorage.org/webt/8n/yy/gc/8nyygcx_ymy38dwrqpbqdiylvsu.png" alt="Bild"><br><br><h2>  Geheimnisse zum Schneiden von Text seq2seq Architektur </h2><br>  Es gibt zwei Arten von Textreduzierungsmethoden: <br><br><ol><li>  <b>Extraktiv</b> .  Es besteht darin, die informativsten Teile des Textes zu finden und daraus die f√ºr die jeweilige Sprache richtige Anmerkung zu erstellen.  Diese Gruppe von Methoden verwendet nur die W√∂rter, die sich im Quelltext befinden. </li><li>  <b>Auszug</b>  Es besteht darin, semantische Verkn√ºpfungen aus dem Text zu extrahieren und dabei Sprachabh√§ngigkeiten zu ber√ºcksichtigen.  Bei der abstrakten Zusammenstellung werden Anmerkungsw√∂rter nicht aus dem abgek√ºrzten Text, sondern aus dem W√∂rterbuch (der Liste der W√∂rter f√ºr eine bestimmte Sprache) ausgew√§hlt, wodurch die Hauptidee umformuliert wird. </li></ol><br>  Der zweite Ansatz impliziert, dass der Algorithmus Sprachabh√§ngigkeiten ber√ºcksichtigen, neu formulieren und verallgemeinern sollte.  Er m√∂chte auch etwas √ºber die reale Welt wissen, um sachliche Fehler zu vermeiden.  Lange galt dies als schwierige Aufgabe, und die Forscher konnten keine qualitativ hochwertige L√∂sung finden - einen grammatikalisch korrekten Text unter Beibehaltung der Grundidee.  Aus diesem Grund basierten die meisten Algorithmen in der Vergangenheit auf einem Extraktionsansatz, da Sie durch die Auswahl ganzer Textteile und deren √úbertragung auf das Ergebnis den Grad der Alphabetisierung wie bei der Quelle beibehalten k√∂nnen. <br><br>  Dies war jedoch vor dem Boom der neuronalen Netze und dem bevorstehenden Eindringen in das NLP.  2014 wurde die <b>seq2seq-</b> Architektur <b>mit einem Aufmerksamkeitsmechanismus eingef√ºhrt, mit dem</b> einige Textsequenzen gelesen und andere generiert werden k√∂nnen (abh√§ngig davon, was das Modell f√ºr die Ausgabe gelernt hat) ( <a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">Artikel</a> von Sutskever et al.).  2016 wurde eine solche Architektur direkt auf die L√∂sung des Sammarisierungsproblems angewendet, wodurch ein abstrakter Ansatz implementiert und ein Ergebnis erzielt wurde, das mit dem vergleichbar war, was eine kompetente Person schreiben konnte ( <a href="https://arxiv.org/pdf/1602.06023.pdf">Artikel</a> von Nallapati et al., 2016; <a href="https://arxiv.org/pdf/1509.00685.pdf">Artikel</a> von Rush et al., 2015; )  Wie funktioniert diese Architektur? <br><br><img src="https://habrastorage.org/webt/px/4x/wi/px4xwin577dd65z-lahsx7fvexs.png" alt="Bild"><br><br>  Seq2Seq besteht aus zwei Teilen: <br><br><ol><li>  <b>Encoder</b> (Encoder) - Ein Zwei-Wege-RNN, mit dem die Eingabesequenz gelesen wird, dh die Eingabeelemente werden nacheinander gleichzeitig von links nach rechts und von rechts nach links verarbeitet, um den Kontext besser zu ber√ºcksichtigen. </li><li>  <b>Decoder</b> (Decoder) - Einweg-RNN, der sequentiell und elementweise eine Ausgangssequenz erzeugt. </li></ol><br>  Zun√§chst wird die Eingabesequenz in eine Einbettungssequenz √ºbersetzt (kurz gesagt, die Einbettung ist eine pr√§gnante Darstellung eines Wortes als Vektor).  Die Einbettungen durchlaufen dann das rekursive Netzwerk des Encoders.  Daher erhalten wir f√ºr jedes Wort die verborgenen Zust√§nde des Encoders ( <i>angezeigt durch rote Rechtecke im Diagramm</i> ) und sie enthalten Informationen √ºber das Token selbst und seinen Kontext, sodass wir die Sprachverbindungen zwischen den W√∂rtern ber√ºcksichtigen k√∂nnen. <br><br>  Nachdem der Encoder die Eingabe verarbeitet hat, √ºbertr√§gt er seinen letzten verborgenen Zustand (der komprimierte Informationen √ºber den gesamten Text enth√§lt) an den Decoder, der ein spezielles Token empf√§ngt <img src="https://habrastorage.org/webt/qn/ud/38/qnud38u15vpvzie4zkne-doze7k.png" alt="Bild">  und erstellt das erste Wort der Ausgabesequenz ( <i>im Bild ist es ‚ÄûDeutschland‚Äú</i> ).  Dann nimmt er zyklisch seine vorherige Ausgabe, gibt sie an sich selbst weiter und zeigt wieder das n√§chste Ausgabeelement an ( <i>also kommt nach "Deutschland" "Schlag" und nach "Schlag" das n√§chste Wort usw.</i> ).  Dies wird solange wiederholt, bis ein spezieller Token ausgegeben wird <img src="https://habrastorage.org/webt/vw/kv/4g/vwkv4gs-ul7vlvnkjwxag8njicw.png" alt="Bild">  .  Dies bedeutet das Ende der Generation. <br><br>  Um das n√§chste Element anzuzeigen, konvertiert der Decoder genau wie der Encoder das Eingabe-Token in Einbettung, geht einen Schritt des rekursiven Netzwerks und empf√§ngt den n√§chsten verborgenen Zustand des Decoders ( <i>gelbe Rechtecke im Diagramm</i> ).  Dann wird unter Verwendung einer vollst√§ndig verbundenen Schicht eine Wahrscheinlichkeitsverteilung f√ºr alle W√∂rter aus einem vorkompilierten Modellw√∂rterbuch erhalten.  Die wahrscheinlichsten W√∂rter werden vom Modell abgeleitet. <br><br>  Das Hinzuf√ºgen <b>eines Aufmerksamkeitsmechanismus</b> hilft dem Decoder, die eingegebenen Informationen besser zu nutzen.  Der Mechanismus bei jedem Erzeugungsschritt bestimmt die sogenannte <b>Aufmerksamkeitsverteilung</b> (die <i>blauen Rechtecke in der Figur sind die Menge von Gewichten, die den Elementen der urspr√ºnglichen Sequenz entsprechen, die Summe der Gewichte ist 1, alle Gewichte&gt; = 0</i> ) und daraus wird die gewichtete Summe aller verborgenen Codiererzust√§nde erhalten, wodurch gebildet wird Kontextvektor ( <i>das Diagramm zeigt ein rotes Rechteck mit einem blauen Strich</i> ).  Dieser Vektor verkettet sich mit dem Einbetten des Decodierereingangsworts im Stadium der Berechnung des latenten Zustands und mit dem latenten Zustand selbst im Stadium der Bestimmung des n√§chsten Wortes.  So kann das Modell bei jedem Schritt der Ausgabe ermitteln, welche Geberzust√§nde f√ºr das Modell momentan am wichtigsten sind.  Mit anderen Worten, es entscheidet √ºber den Kontext, in dem die Eingabew√∂rter am meisten ber√ºcksichtigt werden sollten (z. B. zeigt das Bild das Wort ‚ÄûSchlag‚Äú an, der Aufmerksamkeitsmechanismus gewichtet die Sieger- und Gewinnmarker stark, und der Rest ist nahe Null). <br><br>  Da die Generierung von Headern auch eine der Aufgaben der Sammarisierung ist, nur mit der minimal m√∂glichen Ausgabe (1-12 W√∂rter), habe ich mich entschlossen, <b>seq2seq mit dem Aufmerksamkeitsmechanismus</b> f√ºr unseren Fall anzuwenden.  Wir trainieren ein solches System f√ºr Texte mit √úberschriften, zum Beispiel in den Nachrichten.  Dar√ºber hinaus ist es ratsam, dem Decoder in der Ausbildungsphase nicht seine eigene Ausgabe, sondern die Worte der tats√§chlichen √úberschrift (Lehrerzwang) vorzulegen, um sich und dem Modell das Leben zu erleichtern.  Als Fehlerfunktion verwenden wir die Standardfunktion f√ºr den Verlust durch Kreuzentropie, die zeigt, wie nahe die Wahrscheinlichkeitsverteilungen des Ausgangsworts und des Wortes aus dem realen Header sind: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/3v/co/b9/3vcob9esvdhgsydjcnjfpzv2wbi.png"></div><br>  Bei Verwendung des trainierten Modells verwenden wir die Strahlensuche, um eine wahrscheinlichere Folge von W√∂rtern zu finden als bei Verwendung des Greedy-Algorithmus.  Dazu leiten wir bei jedem Schritt der Generierung nicht das wahrscheinlichste Wort ab, sondern betrachten gleichzeitig beam_size der wahrscheinlichsten Wortfolgen.  Wenn sie enden, endet jeder am <img src="https://habrastorage.org/webt/vw/kv/4g/vwkv4gs-ul7vlvnkjwxag8njicw.png" alt="Bild">  ) leiten wir die wahrscheinlichste Sequenz ab. <br><br><img src="https://habrastorage.org/webt/t2/dt/ni/t2dtnicefjxn0hwd0elaeduekbk.png" alt="Bild"><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yx/ln/q-/yxlnq-7z2-lwuyylmz83pferopm.png"></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ey/81/9u/ey819uvi_jhx4wbwchr-wsixvgs.png"></div><div style="text-align:center;"><img src="https://habrastorage.org/webt/q1/tn/zu/q1tnzuopjtdslrltsmn8euup-kq.png"></div><br><br><h2>  Modellentwicklung </h2><br>  Eines der Probleme des Modells in seq2seq ist die Unf√§higkeit, W√∂rter zu zitieren, die nicht im W√∂rterbuch enthalten sind.  Zum Beispiel hat das Modell keine Chance, "obamacare" aus dem obigen Artikel abzuleiten.  Gleiches gilt f√ºr: <br><br><ul><li>  seltene Familiennamen und Namen </li><li>  neue Begriffe </li><li>  w√∂rter in anderen sprachen, </li><li>  verschiedene Wortpaare, die durch einen Bindestrich verbunden sind (als "Republikanischer Senator") </li><li>  und andere Designs. </li></ul><br>  Nat√ºrlich k√∂nnen Sie das W√∂rterbuch erweitern, dies erh√∂ht jedoch die Anzahl der trainierten Parameter erheblich.  Dar√ºber hinaus ist es erforderlich, eine gro√üe Anzahl von Dokumenten bereitzustellen, in denen diese seltenen W√∂rter vorkommen, damit der Generator lernt, sie qualitativ zu verwenden. <br><br>  Eine andere und elegantere L√∂sung f√ºr dieses Problem wurde 2017 in einem Artikel vorgestellt - ‚Äû <a href="https://arxiv.org/pdf/1704.04368.pdf">Auf den Punkt gebracht: Zusammenfassung mit Pointer-Generator-Netzwerken</a> ‚Äú (Abigail See et al.).  Sie f√ºgt unserem Modell einen neuen Mechanismus hinzu - <b>einen</b> Zeigermechanismus, mit dem W√∂rter aus dem Quelltext ausgew√§hlt und direkt in die generierte Sequenz eingef√ºgt werden k√∂nnen.  Wenn der Text eine OOV enth√§lt ( <i>kein Wortschatz - ein Wort, das nicht im W√∂rterbuch enthalten ist</i> ), kann das Modell die OOV isolieren und an der Ausgabe einf√ºgen, wenn es dies f√ºr erforderlich h√§lt.  Ein solches System wird als <b>"</b> Pointer-Generator" (Pointer-Generator oder pg) bezeichnet und ist eine Synthese von zwei Ans√§tzen zur Sammarisierung.  Sie kann selbst entscheiden, in welchem ‚Äã‚ÄãSchritt sie abstrakt sein soll und in welchem ‚Äã‚ÄãSchritt - extrahieren.  Wie sie es macht, werden wir jetzt herausfinden. <br><br><img src="https://habrastorage.org/webt/f-/9y/xw/f-9yxwborbgpjpalzwd5e_f74xi.png" alt="Bild"><br><br>  Der Hauptunterschied zum √ºblichen seq2seq-Modell ist die zus√§tzliche Aktion, f√ºr die p <sub>gen</sub> berechnet wird - die Erzeugungswahrscheinlichkeit.  Dies erfolgt unter Verwendung des verborgenen Zustands des Decoders und des Kontextvektors.  Die Bedeutung der zus√§tzlichen Aktion ist einfach.  Je n√§her p <sub>gen</sub> an 1 liegt, desto wahrscheinlicher ist es, dass das Modell mithilfe der abstrakten Generierung ein Wort aus seinem W√∂rterbuch ausgibt.  Je n√§her <sub>pgen</sub> an 0 liegt, desto wahrscheinlicher ist es, dass der Generator das Wort aus dem Text extrahiert, der von der zuvor erhaltenen Aufmerksamkeitsverteilung geleitet wird.  Die endg√ºltige Wahrscheinlichkeitsverteilung der Wortergebnisse ist die Summe der erzeugten Wahrscheinlichkeitsverteilung mit W√∂rtern (in denen es keine OOV gibt) multipliziert mit <sub>pgen</sub> und der Aufmerksamkeitsverteilung (in der OOV zum Beispiel "2-0" im Bild ist) multipliziert mit (1 - p <sub>gen</sub> ). <br><br><img src="https://habrastorage.org/webt/iv/8f/8q/iv8f8qvol1j3bbyx79vt5zo7-oq.png" alt="Bild"><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/if/ew/s3/ifews3gqgojf5mclbxd0lpkz2q8.png"></div><br>  Zus√§tzlich zum Zeigemechanismus wird im Artikel <b>ein Coverage-Mechanismus eingef√ºhrt</b> , mit dem sich wiederholende W√∂rter vermeiden lassen.  Ich habe auch damit experimentiert, aber keine signifikanten Verbesserungen in der Qualit√§t der √úberschriften festgestellt - es ist nicht wirklich notwendig.  Dies liegt h√∂chstwahrscheinlich an den Besonderheiten der Aufgabe: Da nur wenige W√∂rter ausgegeben werden m√ºssen, hat der Generator einfach keine Zeit, sich zu wiederholen.  F√ºr andere Aufgaben der Sammarisierung, zum Beispiel Annotation, kann es sich als n√ºtzlich erweisen.  Bei Interesse k√∂nnen Sie im Originalartikel dar√ºber nachlesen. <br><br><img src="https://habrastorage.org/webt/4q/bx/op/4qbxopqt862cphikmbezyaoadlk.png" alt="Bild"><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/--/3m/mz/--3mmzahhdk8kelwoqyh9rdjg8e.png"></div><br><h2>  Gro√üe Auswahl an russischen W√∂rtern </h2><br>  Eine andere M√∂glichkeit, die Qualit√§t der Ausgabe-Header zu verbessern, besteht darin, die Eingabesequenz ordnungsgem√§√ü vorzuverarbeiten.  Neben der offensichtlichen Beseitigung von Gro√übuchstaben habe ich auch versucht, W√∂rter aus dem Quelltext in Stil- und Beugungspaare (d. H. Grundlagen und Endungen) umzuwandeln.  Verwenden Sie zum Teilen den Porter Stemmer. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jy/j8/-y/jyj8-yonzpmvpyoye6mbygrlcjg.png"></div><div style="text-align:center;"><img src="https://habrastorage.org/webt/_k/__/vy/_k__vyhouqm_ctxpsp8ukcselwe.png"></div><br>  Wir markieren alle Beugungen am Anfang mit dem ‚Äû+‚Äú - Symbol, um sie von anderen Token zu unterscheiden.  Wir betrachten jedes Thema und jede Wendung als ein separates Wort und lernen auf dieselbe Weise wie in Worten daraus.  Das hei√üt, wir erhalten Einbettungen von ihnen und leiten eine Sequenz ab (die auch in Grundlagen und Enden unterteilt ist), die leicht in W√∂rter umgewandelt werden kann. <br><br>  Eine solche Konvertierung ist sehr n√ºtzlich, wenn Sie mit morphologisch reichen Sprachen wie Russisch arbeiten.  Anstatt gro√üe W√∂rterb√ºcher mit einer Vielzahl russischer Wortformen zu kompilieren, k√∂nnen Sie sich auf eine gro√üe Anzahl von Wortst√§mmen (die um ein Vielfaches kleiner sind als die Anzahl der Wortformen) und eine sehr kleine Anzahl von Endungen (ich habe viele 450 Beugungen) beschr√§nken.  Auf diese Weise erleichtern wir dem Modell die Arbeit mit diesem ‚ÄûReichtum‚Äú und erh√∂hen gleichzeitig nicht die Komplexit√§t der Architektur und die Anzahl der Parameter. <br><br><img src="https://habrastorage.org/webt/a4/es/s5/a4ess5qr3vaxprksv7avun3obl8.png" alt="Bild"><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/3d/rk/6h/3drk6hntzcow2i3soi-heul7ckq.png"></div><br>  Ich habe auch versucht, die Lemma + Gramm-Transformation zu verwenden.  Das hei√üt, aus jedem Wort vor der Verarbeitung k√∂nnen Sie mithilfe des Pymorphie-Pakets (z. B. "was") seine urspr√ºngliche Form und grammatische Bedeutung abrufen. <img src="https://habrastorage.org/webt/95/wb/-a/95wb-aopfalycfvrshqfynxnwd4.png" alt="Bild">  "Sein" und "VERB | impf | past | sing | femn").  So erhielt ich ein Paar paralleler Sequenzen (in der einen - den Anfangsformen, in der anderen - grammatikalischen Werten).  F√ºr jeden Sequenztyp habe ich meine Einbettungen zusammengestellt, die ich dann verkettet und an die zuvor beschriebene Pipeline gesendet habe.  Darin lernte der Decoder nicht, ein Wort, sondern ein Lemma und Grammatiken auszusprechen.  Ein solches System brachte jedoch keine sichtbaren Verbesserungen im Vergleich zu pg zum Thema.  Vielleicht war es eine zu einfache Architektur f√ºr die Arbeit mit grammatischen Werten, und es hat sich gelohnt, f√ºr jede grammatische Kategorie in der Ausgabe einen eigenen Klassifikator zu erstellen.  Aber ich habe nicht mit solchen oder komplexeren Modellen experimentiert. <br><br>  Ich habe mit einem anderen Zusatz zur urspr√ºnglichen Architektur des Zeigergenerators experimentiert, der sich jedoch nicht auf die Vorverarbeitung bezieht.  Dies ist eine Erh√∂hung der Anzahl von Schichten (bis zu 3) der rekursiven Netzwerke des Codierers und Decodierers.  Das Erh√∂hen der Tiefe des wiederkehrenden Netzwerks kann die Qualit√§t der Ausgabe verbessern, da der verborgene Zustand der letzten Schichten Informationen √ºber eine viel l√§ngere Eingabesubsequenz enthalten kann als der verborgene Zustand eines einschichtigen RNN.  Dies hilft, komplexe erweiterte semantische Verbindungen zwischen Elementen der Eingabesequenz zu ber√ºcksichtigen.  Dies kostet zwar eine erhebliche Erh√∂hung der Anzahl der Modellparameter und erschwert das Lernen. <br><br><img src="https://habrastorage.org/webt/cf/7g/ej/cf7gejf-hxd5pbscvqgnvuuviqy.png" alt="Bild"><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/4g/ww/zy/4gwwzytsrt506_xllgy5xcis6vk.png"></div><br><h2>  Header Generator Experimente </h2><br>  Alle meine Experimente mit √úberschriftengeneratoren lassen sich in zwei Typen unterteilen: Experimente mit Nachrichtenartikeln und Versen.  Ich werde Sie in der Reihenfolge erz√§hlen. <br><br><h3>  Nachrichten Experimente </h3><br>  Bei der Arbeit mit Nachrichten verwendete ich Modelle wie seq2seq, pg, pg mit Stielen und Flexionen - einschichtig und dreischichtig.  Ich habe auch Modelle in Betracht gezogen, die mit Gramm arbeiten, aber alles, was ich √ºber sie erz√§hlen wollte, habe ich bereits oben beschrieben.  Ich muss gleich sagen, dass alle in diesem Abschnitt beschriebenen pg den Beschichtungsmechanismus verwendeten, obwohl sein Einfluss auf das Ergebnis zweifelhaft ist (da es ohne ihn nicht viel schlimmer war). <br><br>  Ich habe den RIA Novosti-Datensatz, der von der Nachrichtenagentur Rossiya Segodnya zur Verf√ºgung gestellt wurde, geschult, um auf der Dialog-Konferenz einen Track zur Erstellung von Schlagzeilen zu erstellen.  Der Datensatz enth√§lt 1.003.869 Nachrichtenartikel, die von Januar 2010 bis Dezember 2014 ver√∂ffentlicht wurden. <br><br>  Alle untersuchten Modelle verwendeten dieselben Einbettungen (128), denselben Wortschatz (100.000) und dieselben latenten Zust√§nde (256) und trainierten f√ºr dieselbe Anzahl von Epochen.  Daher k√∂nnen sich nur qualitative √Ñnderungen in der Architektur oder in der Vorverarbeitung auf das Ergebnis auswirken. <br><br>  Modelle, die an vorverarbeiteten Text angepasst sind, liefern bessere Ergebnisse als Modelle, die mit W√∂rtern arbeiten.  Eine dreischichtige Seite, die Informationen zu Themen und Wendungen verwendet, funktioniert am besten.  Bei Verwendung von pg wird auch die erwartete Verbesserung der Qualit√§t der Header im Vergleich zu seq2seq angezeigt, was auf die bevorzugte Verwendung des Zeigers beim Generieren von Headern hinweist.  Hier ist ein Beispiel f√ºr die Funktionsweise aller Modelle: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6w/4u/0r/6w4u0ruudngxtt2szek31w0achq.png"></div><br>  Wenn wir uns die generierten Header ansehen, k√∂nnen wir die folgenden Probleme von den untersuchten Modellen unterscheiden: <br><br><ol><li>  Models verwenden oft unregelm√§√üige Formen von W√∂rtern.  Modelle mit Stielen (wie im obigen Beispiel) sind von diesem Nachteil mehr befreit; </li><li>  Alle Modelle mit Ausnahme derer, die mit Themen arbeiten, k√∂nnen unvollst√§ndig erscheinende Kopfzeilen oder seltsame Designs erzeugen, die nicht in der Sprache vorliegen (wie im obigen Beispiel). </li><li>  Alle untersuchten Modelle verwechseln h√§ufig die beschriebenen Personen, ersetzen falsche Daten oder verwenden nicht ganz passende W√∂rter. </li></ol><br><img src="https://habrastorage.org/webt/9c/-b/ot/9c-bote3bwomvfu_3bwaixhqnks.png" alt="Bild"><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/e-/3c/px/e-3cpxuknykthyu-xvf-cp-rdsi.png"></div><br><img src="https://habrastorage.org/webt/lf/ry/ep/lfryepksl4kmcqxttb14ikgx4wg.png" alt="Bild"><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/b3/4m/fw/b34mfwpsffud2kaa93tccjvyja0.png"></div><br><h3>  Versuche mit Versen </h3><br>  Da die dreischichtige Seite mit den Themen die geringsten Ungenauigkeiten in den generierten √úberschriften aufweist, ist dies das Modell, das ich f√ºr Versuche mit Versen gew√§hlt habe.  Ich brachte ihr den Fall bei, der aus 6 Millionen russischen Gedichten von der Site "stihi.ru" bestand.  Dazu geh√∂ren Liebe (etwa die H√§lfte der Verse widmet sich diesem Thema), b√ºrgerliche (etwa ein Viertel), Stadt- und Landschaftspoesie.  Schreibzeitraum: Januar 2014 - Mai 2019. Ich werde Beispiele f√ºr generierte √úberschriften f√ºr Verse geben: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yv/mo/da/yvmodarkck7cgra-ymxigffwhik.png"></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/3u/mb/fn/3umbfnpzo1hwhqy0_glxyhxynu4.png"></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tp/tk/5e/tptk5eoq9wa1xrnj9w0gewjeayk.png"></div><br><br>  Das Modell erwies sich als gr√∂√ütenteils extrahierend: Fast alle √úberschriften bestehen aus einer einzelnen Zeile, die h√§ufig aus der ersten oder letzten Zeilengruppe extrahiert wird.  In Ausnahmef√§llen kann das Modell W√∂rter generieren, die nicht im Gedicht enthalten sind.  Dies ist darauf zur√ºckzuf√ºhren, dass eine sehr gro√üe Anzahl von Texten in dem Fall tats√§chlich eine der Zeilen als Namen hat. <br><br>  Abschlie√üend m√∂chte ich sagen, dass der Indexgenerator, der an den Stielen arbeitete und einen einschichtigen Decoder und Encoder verwendete, auf der <a href="https://vk.com/wall-177402111_31">Wettbewerbsstrecke</a> f√ºr die Generierung von √úberschriften f√ºr Nachrichtenartikel auf der wissenschaftlichen Dialogkonferenz zur Computerlinguistik "Dialogue" den zweiten Platz belegte.  Hauptorganisator dieser Konferenz ist ABBYY, das Unternehmen forscht in nahezu allen modernen Bereichen der Verarbeitung nat√ºrlicher Sprache. <br><br>  Abschlie√üend schlage ich Ihnen ein wenig interaktiv vor: Senden Sie Nachrichten in den Kommentaren und sehen Sie, welche Header das neuronale Netzwerk f√ºr sie generiert. <br><br>  <i>Matvey, Entwickler bei NLP Group bei ABBYY</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de479400/">https://habr.com/ru/post/de479400/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de479384/index.html">Was macht Big Data in MegaFon und wie kommt man dorthin?</a></li>
<li><a href="../de479388/index.html">Merkmale des Aufbaus von nationalen Rechenzentren, Mikhalych</a></li>
<li><a href="../de479392/index.html">Pinebook Pro: Kein Chromebook mehr</a></li>
<li><a href="../de479394/index.html">Wie ich unter 15 L√∂sungen nach Helpdesk gesucht habe und ... nicht gefunden habe</a></li>
<li><a href="../de479398/index.html">Wir bringen die lineare Regressionsgleichung in Matrixform</a></li>
<li><a href="../de479402/index.html">So zahlen Sie offiziell f√ºr Freiberufler im Ausland, zahlen 0% Steuern und nicht f√ºr Feed-Payment-Systeme</a></li>
<li><a href="../de479404/index.html">Personal f√ºr den Weihnachtsmann</a></li>
<li><a href="../de479406/index.html">16 Entwicklungstipps f√ºr Android in Kotlin. Teil 1</a></li>
<li><a href="../de479414/index.html">Wege, um das Ziel zu finden. Die Rolle des Zufalls</a></li>
<li><a href="../de479416/index.html">Sehen Sie, wohin Sie gehen (periphere Sicht vs. kognitive Belastung)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>