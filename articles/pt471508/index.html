<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üöÉ üöº üí© Admin sem bra√ßos = hiperconverg√™ncia? üë¶üèø üìù üëãüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Este √© um mito bastante comum no campo do hardware do servidor. Na pr√°tica, solu√ß√µes hiperconvergentes (quando tudo em um) precisam muito para qu√™. Hi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Admin sem bra√ßos = hiperconverg√™ncia?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/croc/blog/471508/"><img src="https://habrastorage.org/webt/_a/wu/gy/_awugy9unrbypap90hlhc86-maa.png"><br><img src="https://habrastorage.org/webt/_k/kx/sa/_kkxsa4xs7rgz-fzktgy10mdeuc.png"><br><br>  Este √© um mito bastante comum no campo do hardware do servidor.  Na pr√°tica, solu√ß√µes hiperconvergentes (quando tudo em um) precisam muito para qu√™.  Historicamente, as primeiras arquiteturas foram desenvolvidas pela Amazon e Google por seus servi√ßos.  Ent√£o, a id√©ia era criar um farm de computa√ß√£o com os mesmos n√≥s, cada um com suas pr√≥prias unidades.  Tudo isso foi combinado por algum software de forma√ß√£o de sistema (hypervisor) e j√° estava dividido em m√°quinas virtuais.  A tarefa principal √© o m√≠nimo de esfor√ßo para manter um n√≥ e o m√≠nimo de problemas de dimensionamento: acabamos de comprar outros mil ou dois dos mesmos servidores e nos conectar nas proximidades.  Na pr√°tica, esses s√£o casos isolados e, com muito mais frequ√™ncia, estamos falando de um n√∫mero menor de n√≥s e de uma arquitetura ligeiramente diferente. <br><br>  Mas o plus continua o mesmo - a incr√≠vel facilidade de dimensionamento e controle.  Menos - tarefas diferentes consomem recursos de maneira diferente, e em algum lugar haver√° muitos discos locais, em algum lugar haver√° pouca RAM e assim por diante, ou seja, com tipos diferentes de tarefas, a utiliza√ß√£o de recursos ser√° reduzida. <br><br>  Descobriu-se que voc√™ paga de 10 a 15% a mais pela facilidade de configura√ß√£o.  Isso causou o mito das manchetes.  Pesquisamos por um longo tempo em que a tecnologia seria aplicada da melhor maneira poss√≠vel e a encontramos.  O fato √© que Tsiska n√£o tinha seus pr√≥prios sistemas de armazenamento, mas eles queriam um mercado completo de servidores.  E eles criaram o Cisco Hyperflex, uma solu√ß√£o de armazenamento local em n√≥s. <br><br>  E isso de repente se tornou uma solu√ß√£o muito boa para data centers de backup (recupera√ß√£o de desastre).  Por que e como - agora vou contar.  E eu vou mostrar testes de cluster. <a name="habracut"></a><br><br><h3>  Para onde </h3><br>  A hiperconverg√™ncia √©: <br><br><ol><li>  Transfira discos para n√≥s de computa√ß√£o. </li><li>  Integra√ß√£o completa do subsistema de armazenamento com o subsistema de virtualiza√ß√£o. </li><li>  Transfer√™ncia / integra√ß√£o com o subsistema de rede. </li></ol><br>  Essa combina√ß√£o permite implementar muitos recursos dos sistemas de armazenamento no n√≠vel da virtualiza√ß√£o e todos em uma janela de controle. <br><br>  Em nossa empresa, os projetos para projetar datacenters redundantes s√£o muito populares, e muitas vezes √© a solu√ß√£o hiperconvergente que √© frequentemente escolhida devido ao monte de op√ß√µes de replica√ß√£o (at√© o cluster metro) prontas para uso. <br><br>  No caso de data centers de backup, geralmente se trata de uma instala√ß√£o remota em um site do outro lado da cidade ou em outra cidade em geral.  Ele permite restaurar sistemas cr√≠ticos no caso de uma falha parcial ou completa do data center principal.  Os dados de vendas s√£o constantemente replicados l√°, e essa replica√ß√£o pode estar no n√≠vel do aplicativo ou no n√≠vel do dispositivo de bloco (SHD). <br><br>  Agora, falarei sobre o dispositivo e os testes do sistema e, em seguida, sobre alguns cen√°rios da vida real com dados de economia. <br><br><h3>  Testes </h3><br>  Nossa c√≥pia consiste em quatro servidores, cada um com 10 discos SSD por 960 GB.  H√° um disco dedicado para armazenar em cache as opera√ß√µes de grava√ß√£o e armazenamento da m√°quina virtual de servi√ßo.  A solu√ß√£o em si √© a quarta vers√£o.  O primeiro √© francamente bruto (a julgar pelas cr√≠ticas), o segundo √© √∫mido, o terceiro j√° √© bastante est√°vel e este pode ser chamado de release ap√≥s o t√©rmino do teste beta para o p√∫blico em geral.  Durante o teste dos problemas que n√£o vi, tudo funciona como um rel√≥gio. <br><br><div class="spoiler">  <b class="spoiler_title">Altera√ß√µes na v4</b> <div class="spoiler_text">  Corrigido um monte de bugs. <br><br>  Inicialmente, a plataforma s√≥ funcionava com o hypervisor VMware ESXi e suportava um pequeno n√∫mero de n√≥s.  Al√©m disso, o processo de implanta√ß√£o nem sempre terminava com √™xito, eu tinha que reiniciar algumas etapas, havia problemas ao atualizar a partir de vers√µes antigas, os dados na GUI nem sempre eram exibidos corretamente (embora ainda n√£o esteja feliz em exibir gr√°ficos de desempenho), √†s vezes havia problemas na interface com a virtualiza√ß√£o . <br><br>  Agora que todas as feridas das crian√ßas foram corrigidas, o HyperFlex pode executar o ESXi e o Hyper-V, al√©m disso, √© poss√≠vel: <br><br><ol><li>  Criando um cluster estendido. </li><li>  Criando um cluster para escrit√≥rios sem usar o Fabric Interconnect, de dois a quatro n√≥s (compramos apenas servidores). </li><li>  Capacidade de trabalhar com armazenamento externo. </li><li>  Suporte para cont√™ineres e Kubernetes. </li><li>  Cria√ß√£o de zonas de acessibilidade. </li><li>  Integra√ß√£o com o VMware SRM, se a funcionalidade incorporada n√£o for adequada. </li></ol><br></div></div><br>  A arquitetura n√£o √© muito diferente das decis√µes dos principais concorrentes, eles n√£o come√ßaram a criar uma bicicleta.  Tudo funciona na plataforma de virtualiza√ß√£o VMware ou Hyper-V.  Hardware hospedado em servidores propriet√°rios do Cisco UCS.  H√° quem odeie a plataforma pela relativa complexidade da configura√ß√£o inicial, muitos bot√µes, um sistema n√£o trivial de modelos e depend√™ncias, mas h√° quem tenha aprendido o Zen, inspirado na id√©ia e n√£o queira mais trabalhar com outros servidores. <br><br>  Consideraremos a solu√ß√£o especificamente para a VMware, porque a solu√ß√£o foi criada originalmente para ela e tem mais funcionalidade, o Hyper-V foi adicionado ao longo do caminho para acompanhar os concorrentes e atender √†s expectativas do mercado. <br><br>  H√° um cluster de servidores cheios de discos.  Existem discos para armazenamento de dados (SSD ou HDD - conforme seu gosto e necessidade), h√° um disco SSD para armazenamento em cache.  Quando os dados s√£o gravados no armazenamento de dados, os dados s√£o salvos na camada de armazenamento em cache (disco SSD dedicado e RAM da VM de servi√ßo).  Paralelamente, o bloco de dados √© enviado para os n√≥s no cluster (o n√∫mero de n√≥s depende do fator de replica√ß√£o do cluster).  Ap√≥s a confirma√ß√£o de todos os n√≥s sobre a grava√ß√£o bem-sucedida, a confirma√ß√£o da grava√ß√£o √© enviada ao hipervisor e depois √† VM.  Os dados gravados em segundo plano s√£o deduplicados, compactados e gravados em discos de armazenamento.  Ao mesmo tempo, um bloco grande √© sempre gravado nos discos de armazenamento e sequencialmente, o que reduz a carga nos discos de armazenamento. <br><br>  A desduplica√ß√£o e a compacta√ß√£o est√£o sempre ativadas e n√£o podem ser desativadas.  Os dados s√£o lidos diretamente dos discos de armazenamento ou do cache da RAM.  Se uma configura√ß√£o h√≠brida for usada, a leitura tamb√©m ser√° armazenada em cache no SSD. <br><br>  Os dados n√£o est√£o vinculados ao local atual da m√°quina virtual e s√£o distribu√≠dos igualmente entre os n√≥s.  Essa abordagem permite carregar igualmente todas as unidades e interfaces de rede.  O √≥bvio menos implora: n√£o podemos minimizar o atraso da leitura, pois n√£o h√° garantia de disponibilidade de dados localmente.  Mas acredito que este √© um sacrif√≠cio insignificante em compara√ß√£o com as vantagens recebidas.  Al√©m disso, os atrasos na rede atingiram tais valores que praticamente n√£o afetam o resultado geral. <br><br>  Por toda a l√≥gica do subsistema de disco, uma VM de servi√ßo especial do controlador Cisco HyperFlex Data Platform √© respons√°vel, criada em cada n√≥ de armazenamento.  Em nossa configura√ß√£o de VM de servi√ßo, oito vCPUs e 72 GB de RAM foram alocados, o que n√£o √© t√£o pequeno.  Deixe-me lembr√°-lo de que o host em si possui 28 n√∫cleos f√≠sicos e 512 GB de RAM. <br><br>  A VM de servi√ßo tem acesso aos discos f√≠sicos diretamente encaminhando o controlador SAS para a VM.  A comunica√ß√£o com o hypervisor ocorre por meio de um m√≥dulo IOVisor especial, que intercepta opera√ß√µes de E / S e usando um agente que permite transferir comandos para a API do hypervisor.  O agente √© respons√°vel por trabalhar com os instant√¢neos e clones do HyperFlex. <br><br>  No hypervisor, os recursos de disco s√£o montados como uma bola NFS ou SMB (dependendo do tipo de hypervisor, adivinhe qual).  Al√©m disso, esse √© um sistema de arquivos distribu√≠do que permite adicionar recursos de sistemas de armazenamento completos para adultos: aloca√ß√£o de volume fino, compacta√ß√£o e desduplica√ß√£o, instant√¢neos usando a tecnologia Redirect-on-Write, replica√ß√£o s√≠ncrona / ass√≠ncrona. <br><br>  A VM de servi√ßo fornece acesso √† interface WEB do gerenciamento do subsistema HyperFlex.  H√° integra√ß√£o com o vCenter, e a maioria das tarefas di√°rias pode ser executada a partir dele, mas os armazenamentos de dados, por exemplo, s√£o mais convenientes para cortar a partir de uma webcam separada se voc√™ j√° tiver mudado para uma interface r√°pida HTML5 ou usar um cliente Flash completo com integra√ß√£o total.  Na webcam de servi√ßo, voc√™ pode ver o desempenho e o status detalhado do sistema. <br><br><img src="https://habrastorage.org/webt/o0/bj/od/o0bjod1zrf25ubnrtis5q4e1ywc.png"><br><br>  H√° outro tipo de n√≥ em um cluster - n√≥s computacionais.  Pode ser servidores em rack ou blade sem unidades internas.  Nesses servidores, voc√™ pode executar VMs cujos dados s√£o armazenados em servidores com discos.  Do ponto de vista do acesso a dados, n√£o h√° diferen√ßa entre os tipos de n√≥s, porque a arquitetura envolve abstrair da localiza√ß√£o f√≠sica dos dados.  A propor√ß√£o m√°xima de n√≥s de computa√ß√£o e de armazenamento √© 2: 1. <br><br>  O uso de n√≥s computacionais aumenta a flexibilidade ao dimensionar recursos de cluster: n√£o precisamos comprar n√≥s com discos se precisarmos apenas de CPU / RAM.  Al√©m disso, podemos adicionar uma cesta blade e economizar espa√ßo no servidor em rack. <br><br>  Como resultado, temos uma plataforma hiperconvergente com os seguintes recursos: <br><br><ul><li>  At√© 64 n√≥s em um cluster (at√© 32 n√≥s de armazenamento). </li><li>  O n√∫mero m√≠nimo de n√≥s em um cluster √© tr√™s (dois para um cluster de borda). </li><li>  Mecanismo de redund√¢ncia de dados: espelhamento com fator de replica√ß√£o 2 e 3. </li><li>  Cluster Metro. </li><li>  Replica√ß√£o de VM ass√≠ncrona para outro cluster HyperFlex. </li><li>  Orquestra√ß√£o de alternar VMs para um data center remoto. </li><li>  Instant√¢neos nativos usando a tecnologia Redirect-on-Write. </li><li>  At√© 1 PB de espa√ßo utiliz√°vel com fator de replica√ß√£o 3 e sem deduplica√ß√£o.  N√£o levamos em considera√ß√£o o fator de replica√ß√£o 2, pois essa n√£o √© uma op√ß√£o para vendas s√©rias. </li></ul><br>  Outra grande vantagem √© a facilidade de gerenciamento e implanta√ß√£o.  Todas as complexidades da configura√ß√£o de servidores UCS s√£o tratadas por uma VM especializada preparada pelos engenheiros da Cisco. <br><br><h3>  Configura√ß√£o do Testbed: </h3><br><ul><li>  2 x Cisco UCS Fabric Interconnect 6248UP como um cluster de gerenciamento e componentes de rede (48 portas operando no modo Ethernet 10G / FC 16G). </li><li>  Quatro servidores Cisco UCS HXAF240 M4. </li></ul><br>  Recursos do servidor: <br><p></p><div class="scrollable-table"><table><tbody><tr><td><br><p>  CPU </p><br></td><td><br><p>  2 x Intel ¬Æ Xeon ¬Æ E5-2690 v4 </p><br></td></tr><tr><td><br><p>  RAM </p><br></td><td><br><p>  16 x 32 GB RDIMM DDR4-2400 MHz / PC4-19200 / classifica√ß√£o dupla / x4 / 1.2v </p><br></td></tr><tr><td><br><p>  Rede </p><br></td><td><br><p>  UCSC-MLOM-CSC-02 (VIC 1227).  2 x Ethernet 10G </p><br></td></tr><tr><td><br><p>  Armazenamento hba </p><br></td><td><br><p>  Passagem modular Cisco 12G SAS atrav√©s do controlador </p><br></td></tr><tr><td><br><p>  Discos de armazenamento </p><br></td><td><br><p>  1 x SSD Intel S3520 120 GB, 1 x SSD Samsung MZ-IES800D, 10 x SSD Samsung PM863a 960 GB </p><br></td></tr></tbody></table></div><br><br><div class="spoiler">  <b class="spoiler_title">Mais op√ß√µes de configura√ß√£o</b> <div class="spoiler_text">  Al√©m do ferro selecionado, as seguintes op√ß√µes est√£o dispon√≠veis no momento: <br><br><ul><li>  HXAF240c M5. </li><li>  Um ou dois CPUs que variam de Intel Silver 4110 a Intel Platinum I8260Y.  A segunda gera√ß√£o est√° dispon√≠vel. </li><li>  24 slots de mem√≥ria, slats de 16 GB RDIMM 2600 a 128 GB LRDIMM 2933. </li><li>  De 6 a 23 discos para dados, um disco de cache, um sistema e um disco de inicializa√ß√£o. </li></ul><br>  <b>Unidades de capacidade</b> <br><br><ul><li>  HX-SD960G61X-EV 960GB de 2,5 polegadas Enterprise Value 6G SATA SSD (resist√™ncia 1X) SAS 960 GB. </li><li>  HX-SD38T61X-EV 3,8 TB Enterprise Value de 2,5 polegadas SSD SATA 6G (resist√™ncia 1X) SAS 3,8 TB. </li><li>  Armazenando em cache drivers </li><li>  HX-NVMEXPB-I375 375GB Unidade Optane Intel de 2,5 polegadas, desempenho e resist√™ncia extremos. </li><li>  HX-NVMEHW-H1600 * 1,6 TB Ent de 2,5 polegadas  Perf  NVMe SSD (resist√™ncia 3X) NVMe 1,6 TB. </li><li>  HX-SD400G12TX-EP 400GB 2,5 polegadas Ent.  Perf  SSD 12G SAS (resist√™ncia 10X) SAS 400 GB. </li><li>  HX-SD800GBENK9 ** Ent de 800 GB de 2,5 polegadas  Perf  SSD SAS SED 12G (resist√™ncia 10X) SAS 800 GB. </li><li>  HX-SD16T123X-EP 1.6 TB SSD SAS de 12G com desempenho corporativo de 2,5 polegadas (resist√™ncia 3X). </li></ul><br>  <b>Unidades de Sistema / Log</b> <br><br><ul><li>  HX-SD240GM1X-EV 240GB 2,5 polegadas Enterprise Value 6G SATA SSD (Requer atualiza√ß√£o). </li></ul><br>  <b>Drivers de inicializa√ß√£o</b> <br><br><ul><li>  HX-M2-240GB SSD de 240GB SATA M.2 SATA de 240 GB. </li></ul><br></div></div><br>  Conex√£o a uma rede nas portas Ethernet 40G, 25G ou 10G. <br><br>  Como FI pode ser HX-FI-6332 (40G), HX-FI-6332-16UP (40G), HX-FI-6454 (40G / 100G). <br><br><h3>  Teste em si </h3><br>  Para testar o subsistema de disco, usei o HCIBench 2.2.1.  Este √© um utilit√°rio gratuito que permite automatizar a cria√ß√£o de carga a partir de v√°rias m√°quinas virtuais.  A carga em si √© gerada por fio regular. <br><br>  Nosso cluster consiste em quatro n√≥s, fator de replica√ß√£o 3, todas as unidades Flash. <br><br>  Para teste, criei quatro datastores e oito m√°quinas virtuais.  Para testes de grava√ß√£o, sup√µe-se que o disco de armazenamento em cache n√£o esteja cheio. <br><br>  Os resultados do teste s√£o os seguintes: <br><div class="scrollable-table"><table><tbody><tr><td></td><td colspan="5"><br><p>  100% Ler 100% Aleat√≥rio </p><br></td><td colspan="5"><br><p>  0% Ler 100% Aleat√≥rio </p><br></td></tr><tr><td><br><p>  Profundidade do bloco / fila </p><br></td><td><br><p>  128 </p><br></td><td><br><p>  256 </p><br></td><td><br><p>  512 </p><br></td><td><br><p>  1024 </p><br></td><td><br><p>  2048 </p><br></td><td><br><p>  128 </p><br></td><td><br><p>  256 </p><br></td><td><br><p>  512 </p><br></td><td><br><p>  1024 </p><br></td><td><br><p>  2048 </p><br></td></tr><tr><td><br><p>  4K </p><br></td><td><br><p>  0,59 ms 213804 IOPS </p><br></td><td><br><p>  0,84 ms 303540 IOPS </p><br></td><td><br><p>  1.36ms 374348 IOPS </p><br></td><td><br><p>  2,47 ms 414116 IOPS </p><br></td><td><br><p>  <b>4.86ms 420180 IOPS</b> </p><br></td><td><br><p>  2,22 ms 57408 IOPS </p><br></td><td><br><p>  3,09 ms 82744 IOPS </p><br></td><td><br><p>  5,02 ms 101824 IPOS </p><br></td><td><br><p>  8,75 ms 116912 IOPS </p><br></td><td><br><p>  <b>17,2 ms 118592 IOPS</b> </p><br></td></tr><tr><td><br><p>  8K </p><br></td><td><br><p>  0,67 ms 188416 IOPS </p><br></td><td><br><p>  0,93 ms 273280 IOPS </p><br></td><td><br><p>  1,7 ms 299932 IOPS </p><br></td><td><br><p>  2,72 ms 376,484 IOPS </p><br></td><td><br><p>  <b>5,47 ms 373,176 IOPS</b> </p><br></td><td><br><p>  3,1 ms 41148 IOPS </p><br></td><td><br><p>  4,7 ms 54396 IOPS </p><br></td><td><br><p>  7,09 ms 72192 IOPS </p><br></td><td><br><p>  <b>12,77 ms 80.132 IOPS</b> </p><br></td><td></td></tr><tr><td><br><p>  16K </p><br></td><td><br><p>  0,77 ms 164116 IOPS </p><br></td><td><br><p>  1,12 ms 228328 IOPS </p><br></td><td><br><p>  1,9 ms 268140 IOPS </p><br></td><td><br><p>  <b>3,96 ms 258480 IOPS</b> </p><br></td><td></td><td><br><p>  3,8 ms 33640 IOPS </p><br></td><td><br><p>  6,97 ms 36696 IOPS </p><br></td><td><br><p>  <b>11,35 ms 45060 IOPS</b> </p><br></td><td></td><td></td></tr><tr><td><br><p>  32K </p><br></td><td><br><p>  1,07 ms 119292 IOPS </p><br></td><td><br><p>  1,79 ms 142888 IOPS </p><br></td><td><br><p>  <b>3,56 ms 143760 IOPS</b> </p><br></td><td></td><td></td><td><br><p>  7.17 ms 17810 IOPS </p><br></td><td><br><p>  <b>11,96 ms 21396 IOPS</b> </p><br></td><td></td><td></td><td></td></tr><tr><td><br><p>  64K </p><br></td><td><br><p>  1,84 ms 69440 IOPS </p><br></td><td><br><p>  3,6 ms 71008 IOPS </p><br></td><td><br><p>  <b>7,26 ms 70404 IOPS</b> </p><br></td><td></td><td></td><td><br><p>  <b>11,37 ms 11248 IOPS</b> </p><br></td><td></td><td></td><td></td><td></td></tr></tbody></table></div><br>  <i>Os valores em negrito s√£o indicados, ap√≥s os quais n√£o h√° aumento na produtividade, √†s vezes at√© a degrada√ß√£o √© vis√≠vel.</i>  <i>Devido ao fato de nos basearmos no desempenho da rede / controladores / unidades.</i> <br><br><ul><li>  Leitura sequencial 4432 MB / s. </li><li>  Grava√ß√£o sequencial 804 MB / s. </li><li>  Se um controlador falhar (falha na m√°quina virtual ou no host), o rebaixamento do desempenho ser√° dobrado. </li><li>  Se a unidade de armazenamento falhar, o rebaixamento ser√° de 1/3.  O disco Rebild consome 5% dos recursos de cada controlador. </li></ul><br>  Em um pequeno bloco, encontramos o desempenho do controlador (m√°quina virtual), sua CPU √© 100% carregada, enquanto aumenta o bloco em que executamos a largura de banda da porta.  10 Gbps n√£o √© suficiente para desbloquear o potencial do sistema AllFlash.  Infelizmente, os par√¢metros do suporte de demonstra√ß√£o fornecido n√£o permitem a verifica√ß√£o do trabalho em 40 Gb / s. <br><br>  Na minha impress√£o dos testes e do estudo da arquitetura, devido ao algoritmo que coloca os dados entre todos os hosts, obtemos um desempenho previs√≠vel e escal√°vel, mas isso tamb√©m √© uma limita√ß√£o na leitura, pois seria poss√≠vel extrair mais dos discos locais e mais, aqui para salvar uma rede mais produtiva, por exemplo, FIs de 40 Gbps est√£o dispon√≠veis. <br><br>  Al√©m disso, um disco para armazenamento em cache e desduplica√ß√£o pode ser uma limita√ß√£o; de fato, nesse suporte, podemos escrever em quatro discos SSD.  Seria √≥timo poder aumentar o n√∫mero de discos em cache e ver a diferen√ßa. <br><br><h3>  Uso real </h3><br>  Duas abordagens podem ser usadas para organizar um data center de backup (n√£o consideramos colocar o backup em um site remoto): <br><br><ol><li>  Passivo ativo  Todos os aplicativos est√£o hospedados no data center principal.  A replica√ß√£o √© s√≠ncrona ou ass√≠ncrona.  No caso de uma queda no data center principal, precisamos ativar o backup.  Isso pode ser feito manualmente / scripts / aplicativos de orquestra√ß√£o.  Aqui, obtemos um RPO proporcional √† frequ√™ncia de replica√ß√£o, e o RTO depende da rea√ß√£o e das habilidades do administrador e da qualidade do desenvolvimento / depura√ß√£o do plano de comuta√ß√£o. </li><li>  Ativo Ativo  Nesse caso, apenas a replica√ß√£o s√≠ncrona est√° presente, a disponibilidade dos data centers √© determinada por um quorum / √°rbitro, colocado estritamente na terceira plataforma.  RPO = 0 e o RTO pode atingir 0 (se o aplicativo permitir) ou igual ao tempo de failover de um n√≥ em um cluster de virtualiza√ß√£o.  No n√≠vel da virtualiza√ß√£o, √© criado um cluster estendido (Metro) que requer armazenamento ativo-ativo. </li></ol><br>  Normalmente, vemos com os clientes uma arquitetura j√° implementada com armazenamento cl√°ssico no data center principal, portanto projetamos outra para replica√ß√£o.  Como mencionei, o Cisco HyperFlex oferece replica√ß√£o ass√≠ncrona e a cria√ß√£o de um cluster de virtualiza√ß√£o estendido.  Ao mesmo tempo, n√£o precisamos de um sistema de armazenamento de gama m√©dia ou superior dedicado com as caras fun√ß√µes de replica√ß√£o e acesso de dados ativo-ativo em dois sistemas de armazenamento. <br><br>  <b>Cen√°rio 1:</b> Temos data centers prim√°rios e de backup, uma plataforma de virtualiza√ß√£o no VMware vSphere.  Todos os sistemas produtivos est√£o localizados principalmente no data center, e a replica√ß√£o da m√°quina virtual √© realizada no n√≠vel do hipervisor, o que permitir√° n√£o manter as VMs ativadas no data center de backup.  N√≥s replicamos bancos de dados e aplicativos especiais com ferramentas internas e mantemos as VMs ativadas.  Se o data center principal falhar, iniciaremos o sistema no data center de backup.  Acreditamos que temos cerca de 100 m√°quinas virtuais.  Enquanto o data center principal estiver em opera√ß√£o, os ambientes de teste e outros sistemas poder√£o ser iniciados no data center de backup, que poder√° ser desativado se o data center principal for alternado.  Tamb√©m √© poss√≠vel usar a replica√ß√£o bidirecional.  Do ponto de vista do equipamento, nada mudar√°. <br><br>  No caso da arquitetura cl√°ssica, colocaremos um sistema de armazenamento h√≠brido em cada data center com acesso via FibreChannel, lacrimejamento, desduplica√ß√£o e compacta√ß√£o (mas n√£o on-line), 8 servidores por site, 2 switches FibreChannel e Ethernet 10G.  Para controle de replica√ß√£o e comuta√ß√£o em uma arquitetura cl√°ssica, podemos usar as ferramentas VMware (Replication + SRM) ou ferramentas de terceiros que ser√£o um pouco mais baratas e √†s vezes mais convenientes. <br><br>  A figura mostra um diagrama. <br><br><img src="https://habrastorage.org/webt/ej/an/92/ejan92jvmtt1ejtbtd1eyytngw8.png"><br><br>  Se voc√™ usa o Cisco HyperFlex, obt√©m a seguinte arquitetura: <br><br><img src="https://habrastorage.org/webt/9w/xs/hl/9wxshlz45c53uyitqulmrmwlraq.png"><br><br>  Para o HyperFlex, usei servidores com grandes recursos de CPU / RAM, como  parte dos recursos ir√° para a VM do controlador HyperFlex, at√© recarreguei um pouco a configura√ß√£o do HyperFlex na CPU e na mem√≥ria para n√£o jogar ao lado da Cisco e garantir recursos para o restante das VMs.  Mas podemos recusar os comutadores FibreChannel e n√£o precisamos de portas Ethernet para cada servidor, o tr√°fego local √© alternado dentro do FI. <br><br>  O resultado √© a seguinte configura√ß√£o para cada data center: <br><div class="scrollable-table"><table><tbody><tr><td><br><p>  Servidores </p><br></td><td><br><p>  Servidor 8 x 1U (384 GB RAM, 2 x Intel Gold 6132, FC HBA) </p><br></td><td><br><p>  8 x HX240C-M5L (512 GB de RAM, 2 x Intel Gold 6150, SSD de 3,2 GB, NL-SAS de 10 x 6 TB) </p><br></td></tr><tr><td><br><p>  SHD </p><br></td><td><br><p>  Armazenamento h√≠brido com FC front-end (SSD de 20 TB, NL-SAS de 130 TB) </p><br></td><td><br><p>  - </p><br></td></tr><tr><td><br><p>  LAN </p><br></td><td><br><p>  2 x switch Ethernet 10G 12 portas </p><br></td><td><br><p>  - </p><br></td></tr><tr><td><br><p>  San </p><br></td><td><br><p>  2 x comutador FC 32 / 16Gb 24 portas </p><br></td><td><br><p>  2 x Cisco UCS FI 6332 </p><br></td></tr><tr><td><br><p>  Licen√ßas </p><br></td><td><br><p>  VMware Ent Plus </p><br><p>  Replica√ß√£o e / ou orquestra√ß√£o de VM </p><br></td><td><br><p>  VMware Ent Plus </p><br></td></tr></tbody></table></div><br>  Para o Hyperflex, n√£o prometi licen√ßas de software de replica√ß√£o, pois elas est√£o dispon√≠veis imediatamente. <br><br>  Para a arquitetura cl√°ssica, peguei um fornecedor que se estabeleceu como um fabricante de qualidade e baixo custo.  Para as duas op√ß√µes, usei um padr√£o para um skid de solu√ß√£o espec√≠fico; na sa√≠da, obtive pre√ßos reais. <br><br>  A solu√ß√£o no Cisco HyperFlex foi 13% mais barata. <br><br>  <b>Cen√°rio 2:</b> criando dois data centers ativos.  Nesse cen√°rio, projetamos um cluster estendido no VMware. <br><br>  A arquitetura cl√°ssica consiste em servidores de virtualiza√ß√£o, SAN (protocolo FC) e dois sistemas de armazenamento que podem ler e gravar no estendido entre eles.  Em cada SHD, colocamos uma capacidade √∫til para a trava. <br><br><img src="https://habrastorage.org/webt/lm/1i/yu/lm1iyuqivylf7dl0xrmff21swlw.png"><br><br>  No HyperFlex, simplesmente criamos um Stretch Cluster com o mesmo n√∫mero de n√≥s nos dois sites.  Nesse caso, o fator de replica√ß√£o 2 + 2 √© usado. <br><br><img src="https://habrastorage.org/webt/e7/mq/sd/e7mqsd7codjtfbqefl40icsohpu.png"><br><br>  A seguinte configura√ß√£o acabou: <br><div class="scrollable-table"><table><tbody><tr><td></td><td><br><p>  Arquitetura cl√°ssica </p><br></td><td><br><p>  Hyperflex </p><br></td></tr><tr><td><br><p>  Servidores </p><br></td><td><br><p>  Servidor 16 x 1U (384 GB RAM, 2 x Intel Gold 6132, FC HBA, 2 x 10G NIC) </p><br></td><td><br><p>  16 x HX240C-M5L (512 GB RAM, 2 x Intel Gold 6132, 1,6 TB NVMe, 12 x 3,8 TB SSD, VIC 1387) </p><br></td></tr><tr><td><br><p>  SHD </p><br></td><td><br><p>  2 x Armazenamento AllFlash (SSD de 150 TB) </p><br></td><td><br><p>  - </p><br></td></tr><tr><td><br><p>  LAN </p><br></td><td><br><p>  4 x Ethernet switch 10G 24 portas </p><br></td><td><br><p>  - </p><br></td></tr><tr><td><br><p>  San </p><br></td><td><br><p>  4 x FC switch 32 / 16Gb 24 portas </p><br></td><td><br><p>  4 x Cisco UCS FI 6332 </p><br></td></tr><tr><td><br><p>  Licen√ßas </p><br></td><td><br><p>  VMware Ent Plus </p><br></td><td><br><p>  VMware Ent Plus </p><br></td></tr></tbody></table></div><br>  Em todos os c√°lculos, n√£o levei em considera√ß√£o a infraestrutura de rede, os custos do data center, etc .: eles ser√£o os mesmos para a arquitetura cl√°ssica e para a solu√ß√£o HyperFlex. <br><br>  No custo, o HyperFlex acabou por ser 5% mais caro.  Vale a pena notar aqui que, para os recursos de CPU / RAM, recebi um vi√©s para a Cisco, porque na configura√ß√£o ela preencheu os canais dos controladores de mem√≥ria uniformemente.   ,    ,   ,     ¬´  ¬ª,         .      ,      Cisco UCS     . <br><br>        SAN  , -  ,      (, ,   ‚Äî ),   (    ),  . <br><br>   ,         ‚Äî Cisco.        Cisco UCS,    ,  HyperFlex     ,    .          ,     .       : ¬´    ,  ?¬ª  ¬´  - ,     . !¬ª ‚Äî           , : ¬´    ¬ª   . <br><br><h3>  Refer√™ncias </h3><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> -</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">-   </a> </li><li>   ‚Äî StGeneralov@croc.ru </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt471508/">https://habr.com/ru/post/pt471508/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt471498/index.html">O mapa das c√¢meras de fixa√ß√£o de estradas √© divulgado: se alegrar ou chorar?</a></li>
<li><a href="../pt471500/index.html">Retorno de chamada ou "Aumentar a lealdade do cliente"</a></li>
<li><a href="../pt471502/index.html">Idea Farm</a></li>
<li><a href="../pt471504/index.html">Dueto bidimensional: cria√ß√£o de heteroestruturas de borofeno-grafeno</a></li>
<li><a href="../pt471506/index.html">Arredondamento correto de n√∫meros decimais no c√≥digo bin√°rio</a></li>
<li><a href="../pt471512/index.html">28 de outubro, Ecaterimburgo - Comunica√ß√£o de qualidade</a></li>
<li><a href="../pt471514/index.html">O cabe√ßalho "Leia artigos para voc√™". Janeiro - junho 2019</a></li>
<li><a href="../pt471516/index.html">Intel 665p - SSD com QLC NAND de 96 camadas</a></li>
<li><a href="../pt471518/index.html">Apple em 2019 √© Linux em 2000</a></li>
<li><a href="../pt471520/index.html">O livro "Tarefas cl√°ssicas de ci√™ncia da computa√ß√£o em Python"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>