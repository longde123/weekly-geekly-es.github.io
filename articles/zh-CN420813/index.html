<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>♈️ 👩🏾‍🤝‍👨🏼 😪 Kubernetes的后台网络 🚴🏾 👫 👩🏽‍🤝‍👨🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="注意事项 佩雷夫 ：原始文章的作者Nicolas Leiva是一位思科解决方案架构师，他决定与他的同行网络工程师分享Kubernetes网络在内部的工作方式。 为此，他积极利用常识，网络知识和标准Linux / Kubernetes实用程序，探索了集群中最简单的配置。 结果很多，但是很清楚。 


...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Kubernetes的后台网络</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/420813/">  <i><b>注意事项</b></i>  <i><b>佩雷夫</b></i>  <i>：原始文章的作者Nicolas Leiva是一位思科解决方案架构师，他决定与他的同行网络工程师分享Kubernetes网络在内部的工作方式。</i>  <i>为此，他积极利用常识，网络知识和标准Linux / Kubernetes实用程序，探索了集群中最简单的配置。</i>  <i>结果很多，但是很清楚。</i> <br><br><img src="https://habrastorage.org/webt/gr/qw/d4/grqwd4putslwaijltw9yojfzjes.png"><br><br> 除了Kelsey Hightower的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes The Hard Way</a>指南可以正常工作（ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">甚至在AWS上！</a> ）之外，我还喜欢网络保持干净简单。 这是了解容器网络接口（ <a href="">CNI</a> ）角色的绝佳机会。 话虽如此，我还要补充说，Kubernetes网络实际上并不是很直观，尤其是对于初学者来说……而且也不要忘记“根本<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">就没有</a>容器网络”。 <a name="habracut"></a><br><br> 尽管有关该主题的材料已经很丰富（请参阅<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">此处的</a>链接），但我找不到这样的示例，无法将所有必要的内容与网络工程师喜欢和讨厌的团队的结论相结合，以说明幕后的实际情况。 因此，我决定从许多来源收集信息-希望这对您有所帮助，并且您可以更好地了解一切之间的联系。 这些知识不仅对测试自己很重要，而且对简化诊断问题的过程也很重要。 您可以从<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes The Hard Way中</a>遵循集群中的示例：所有IP地址和设置都从那里获取（截至2018年5月的提交，使用<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Nabla容器</a>之前）。 <br><br> 我们将从头开始，当我们有三个控制器和三个工作节点时： <br><br><img src="https://habrastorage.org/webt/am/vs/6j/amvs6jnhsuxzwhyoyod6vjk8cby.png"><br><br> 您可能会注意到，这里至少还有三个专用子网！ 有点耐心，他们都会被考虑在内。 请记住，即使我们提到非常特定的IP前缀，它们也仅取自<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes The Hard Way</a> ，因此它们仅具有本地意义，并且您可以根据<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">RFC 1918</a>为您的环境自由选择任何其他地址块。 对于IPv6，将有单独的博客文章。 <br><br><h2> 主机网络（10.240.0.0/24） </h2><br> 这是一个内部网络，所有节点都是其中的一部分。 分配计算资源时，由<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">GCP中</a>的<code>--private-network-ip</code>标志或<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">AWS中</a>的<code>--private-ip-address</code>选项定义。 <br><br><h3> 在GCP中初始化控制器节点 </h3><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 0 1 2; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> gcloud compute instances create controller-<span class="hljs-variable"><span class="hljs-variable">${i}</span></span> \ <span class="hljs-comment"><span class="hljs-comment"># ... --private-network-ip 10.240.0.1${i} \ # ... done</span></span></code> </pre> <br>  （ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><code>controllers_gcp.sh</code></a> ） <br><br><h3> 在AWS中初始化控制器节点 </h3><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 0 1 2; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-built_in"><span class="hljs-built_in">declare</span></span> controller_id<span class="hljs-variable"><span class="hljs-variable">${i}</span></span>=`aws ec2 run-instances \ <span class="hljs-comment"><span class="hljs-comment"># ... --private-ip-address 10.240.0.1${i} \ # ... done</span></span></code> </pre> <br>  （ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><code>controllers_aws.sh</code></a> ） <br><br><img src="https://habrastorage.org/webt/gt/cj/p6/gtcjp6fgkqbv1nvs2ea9d9hhueo.png"><br><br> 每个实例将具有两个IP地址：主机网络的私有IP地址（控制器<code>10.240.0.1${i}/24</code> ，工作人员<code>10.240.0.2${i}/24</code> ））和由云提供商指定的公共IP地址，我们将在后面讨论。如何到达<code>NodePorts</code> 。 <br><br><h3> 控制点 </h3><br><pre> <code class="bash hljs">$ gcloud compute instances list NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS controller-0 us-west1-c n1-standard-1 10.240.0.10 35.231.XXX.XXX RUNNING worker-1 us-west1-c n1-standard-1 10.240.0.21 35.231.XX.XXX RUNNING ...</code> </pre> <br><br><h3>  ws </h3><br><pre> <code class="bash hljs">$ aws ec2 describe-instances --query <span class="hljs-string"><span class="hljs-string">'Reservations[].Instances[].[Tags[?Key==`Name`].Value[],PrivateIpAddress,PublicIpAddress]'</span></span> --output text | sed <span class="hljs-string"><span class="hljs-string">'$!N;s/\n/ /'</span></span> 10.240.0.10 34.228.XX.XXX controller-0 10.240.0.21 34.173.XXX.XX worker-1 ...</code> </pre> <br> 如果<a href="">安全策略正确</a> （并且主机上<code>ping</code>安装<code>ping</code> ，则所有节点必须能够相互ping通。 <br><br><h2> 炉床网络（10.200.0.0/16） </h2><br> 这是Pod所在的网络。 每个工作节点都使用该网络的一个子网。 在我们的示例中， <code>POD_CIDR=10.200.${i}.0/24</code>对于<code>worker-${i}</code> 。 <br><br><img src="https://habrastorage.org/webt/6i/yz/ih/6iyzihbxbzwugs4amvhfa9ysp5s.png"><br><br> 要了解所有内容的配置方式，请退后一步，看看<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes网络模型</a> ，该<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">模型</a>需要满足以下条件： <br><br><ul><li> 所有容器都可以在不使用NAT的情况下与任何其他容器通信。 </li><li> 所有节点都可以与所有容器通信（反之亦然），而无需使用NAT。 </li><li> 容器看到的IP必须与其他人看到的IP相同。 </li></ul><br> 所有这些都可以通过多种方式实现，Kubernetes将网络设置传递给<a href="">CNI插件</a> 。 <br><br><blockquote>  “ CNI插件负责将网络接口添加到容器的<b>网络名称空间</b> （例如， <b>veth对的</b>一端），并在主机上进行必要的更改（例如，将veth的另一端连接到网桥）。 然后，他必须通过调用所需的IPAM插件来分配IP接口并根据IP地址管理部分配置路由。”  <i>（来自<a href="">容器网络接口规范</a> ）</i> </blockquote><br><img src="https://habrastorage.org/webt/5q/fs/vw/5qfsvwg2iuduy0q3doco11hbf-g.png"><br><br><h3> 网络名称空间 </h3><br><blockquote>  “名称空间将全局系统资源包装成一个抽象，该名称空间中的进程可以通过这种抽象看到它们，使它们拥有自己的隔离的全局资源实例。 全局资源中的更改对于此名称空间中包含的其他进程可见，但对其他进程不可见。”  <i>（ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">来自名称空间手册页</a> ）</i> </blockquote><br>  Linux提供了七个不同的名称空间（ <code>Cgroup</code> ， <code>IPC</code> ， <code>Network</code> ， <code>Mount</code> ， <code>PID</code> ， <code>User</code> ， <code>UTS</code> ）。 网络名称空间（ <code>CLONE_NEWNET</code> ）定义了该过程可访问的网络资源：“每个网络名称空间都有自己的网络设备，IP地址，IP路由表， <code>/proc/net</code>目录，端口号等” <i>（摘自“ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">操作中的命名空间</a> ”一文</i> 。 <br><br><h3> 虚拟以太网设备（Veth） </h3><br><blockquote>  “虚拟网络对（veth）以“管道”的形式提供抽象，可用于在网络名称空间之间创建隧道或在另一个网络空间中创建到物理网络设备的桥接。 释放命名空间后，其中的所有veth设备都将被销毁。”  <i>（来自<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">网络名称空间手册页</a> ）</i> </blockquote><br> 深入地面，看看它们与集群之间的关系。 首先，Kubernetes中的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">网络插件</a>是多种多样的，而CNI插件就是其中之一（ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">为什么不是CNM？</a> ）。 每个节点上的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubelet</a>告诉容器<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">运行时使用</a>哪个<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">网络插件</a> 。 容器网络接口（ <a href="">CNI</a> ）在容器运行时和网络实现之间。  CNI插件已经建立了网络。 <br><br><blockquote>  “通过将命令行<code>--network-plugin=cni</code>传递给Kubelet，可以选择CNI插件。  Kubelet从<code>--cni-conf-dir</code> （默认值为<code>/etc/cni/net.d</code> ）中读取文件，并使用该文件中的CNI配置为每个文件配置网络。  <i>（根据<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">网络插件要求</a> ）</i> </blockquote><br>  CNI插件的真实二进制文件位于<code>-- cni-bin-dir</code> （默认值为<code>/opt/cni/bin</code> ）。 <br><br> 请注意， <a href=""><code>kubelet.service</code></a>调用<a href=""><code>kubelet.service</code></a>包括<code>--network-plugin=cni</code> ： <br><br><pre> <code class="plaintext hljs">[Service] ExecStart=/usr/local/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --network-plugin=cni \\ ...</code> </pre> <br> 首先，甚至在调用任何插件之前，Kubernetes都会为壁炉创建一个网络名称空间。 这是通过使用特殊的<code>pause</code>容器来实现的，该容器“充当所有炉膛容器的“父容器” <i>（来自“ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">全能暂停容器</a> ”一文）</i> 。 然后Kubernetes执行CNI插件以将<code>pause</code>容器连接到网络。 所有pod容器都使用此<code>pause</code>容器的<code>netns</code> 。 <br><br><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.1", "name": "bridge", "type": "bridge", "bridge": "cnio0", "isGateway": true, "ipMasq": true, "ipam": { "type": "host-local", "ranges": [ [{"subnet": "${POD_CIDR}"}] ], "routes": [{"dst": "0.0.0.0/0"}] } }</code> </pre> <br>  <a href="">用于CNI</a>的<a href="">配置</a>表示使用<code>bridge</code>插件在名为<code>cnio0</code>的根名称空间中配置Linux（L2）软件网桥（ <a href="">默认名称</a>为<code>cni0</code> ），该网关充当网关（ <code>"isGateway": true</code> ）。 <br><br><img src="https://habrastorage.org/webt/bo/to/jp/botojpqu0f7fascfrbk-gen27a8.png"><br><br>  veth对还将配置为将炉床连接到新创建的桥： <br><br><img src="https://habrastorage.org/webt/-6/tt/e7/-6tte7essirvuraiuypuln_syvm.png"><br><br> 要分配L3信息（例如IP地址），将调用<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">IPAM</a> （ <code>ipam</code> ） <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">插件</a> 。 在这种情况下，将使用<code>host-local</code>类型，“将状态本地存储在主机文件系统上，以确保一台主机上IP地址的唯一性” <i>（来自<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><code> host-local</code></a>的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><code> host-local</code></a> ）</i> 。  IPAM插件将此信息返回到先前的插件（ <code>bridge</code> ），以便可以配置配置中指定的所有路由（ <code>"routes": [{"dst": "0.0.0.0/0"}]</code> ）。 如果未指定<code>gw</code> ， <a href="">则从子网中获取</a> 。 还在炉床的网络名称空间中配置默认​​路由，指向桥（桥被配置为炉床的第一个IP子网）。 <br><br> 最后一个重要细节：我们要求对来自壁炉网络的流量进行伪装（ <code>"ipMasq": true</code> ）。 我们这里实际上并不需要NAT，但这是<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes The Hard Way中</a>的配置。 因此，为了完整起见，我必须提到<code>bridge</code>插件的<code>iptables</code>的条目是为此特定示例配置的。 来自炉床的所有数据包（其接收者不在<code>224.0.0.0/4</code>范围内） <a href="">将位于NAT后面，NAT</a>不能完全满足“所有容器都可以与任何其他容器通信而无需使用NAT”的要求。 好吧，我们将证明为什么不需要NAT ... <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><img src="https://habrastorage.org/webt/y-/hy/ub/y-hyubecllmzx9go5ehai4shl78.jpeg"></a> <br><br><h3> 炉膛路由 </h3><br> 现在我们可以自定义豆荚了。 让我们看一下其中一个工作节点名称的所有网络空间，并在<a href="">从此处</a>创建<code>nginx</code>部署之后分析其中一个。 我们将使用带有<code>-t</code>选项的<code>lsns</code>选择所需的名称空间类型（即<code>net</code> ）： <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo lsns -t net NS TYPE NPROCS PID USER COMMAND 4026532089 net 113 1 root /sbin/init 4026532280 net 2 8046 root /pause 4026532352 net 4 16455 root /pause 4026532426 net 3 27255 root /pause</code> </pre> <br> 使用<code>-i</code>选项可以找到它们的inode号： <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ls -1i /var/run/netns 4026532352 cni-1d85bb0c-7c61-fd9f-2adc-f6e98f7a58af 4026532280 cni-7cec0838-f50c-416a-3b45-628a4237c55c 4026532426 cni-912bcc63-712d-1c84-89a7-9e10510808a0</code> </pre> <br> 您还可以使用<code>ip netns</code>列出所有网络名称空间： <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip netns cni-912bcc63-712d-1c84-89a7-9e10510808a0 (id: 2) cni-1d85bb0c-7c61-fd9f-2adc-f6e98f7a58af (id: 1) cni-7cec0838-f50c-416a-3b45-628a4237c55c (id: 0)</code> </pre> <br> 要查看在网络空间<code>cni-912bcc63–712d-1c84–89a7–9e10510808a0</code> （ <code>4026532426</code> ）中运行的所有进程，您可以运行例如以下命令： <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ls -l /proc/[1-9]*/ns/net | grep 4026532426 | cut -f3 -d<span class="hljs-string"><span class="hljs-string">"/"</span></span> | xargs ps -p PID TTY STAT TIME COMMAND 27255 ? Ss 0:00 /pause 27331 ? Ss 0:00 nginx: master process nginx -g daemon off; 27355 ? S 0:00 nginx: worker process</code> </pre> <br> 可以看出，除了在此pod中<code>pause</code>之外，我们还启动了<code>nginx</code> 。  <code>pause</code>容器与所有其他容器容器共享<code>net</code>和<code>ipc</code>名称空间。 记住<code>pause</code>的PID-27255; 我们将回到它。 <br><br> 现在，让我们看看<code>kubectl</code>讲述了这个pod： <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide | grep nginx nginx-65899c769f-wxdx6 1/1 Running 0 5d 10.200.0.4 worker-0</code> </pre> <br> 更多详细信息： <br><br><pre> <code class="bash hljs">$ kubectl describe pods nginx-65899c769f-wxdx6</code> </pre> <br><pre> <code class="plaintext hljs">Name: nginx-65899c769f-wxdx6 Namespace: default Node: worker-0/10.240.0.20 Start Time: Thu, 05 Jul 2018 14:20:06 -0400 Labels: pod-template-hash=2145573259 run=nginx Annotations: &lt;none&gt; Status: Running IP: 10.200.0.4 Controlled By: ReplicaSet/nginx-65899c769f Containers: nginx: Container ID: containerd://4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 Image: nginx ...</code> </pre> <br> 我们看到了Pod的名称<code>nginx-65899c769f-wxdx6</code>及其容器之一（ <code>nginx</code> ）的ID，但是关于<code>pause</code>并没有说什么。 挖掘更深的工作节点以匹配所有数据。 请记住， <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes The Hard Way</a>不使用<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Docker</a> ，因此有关容器的详细信息，请参阅控制台实用程序<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">containerd</a> -ctr <i>（另请参见文章<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">``将容器化与Kubernetes集成，替换Docker，准备投入生产</a> ''- <b>大约Transfer</b> ）</i> ： <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr namespaces ls NAME LABELS k8s.io</code> </pre> <br> 了解容器化（ <code>k8s.io</code> ） <code>k8s.io</code> ，您可以获得<code>nginx</code>容器ID： <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers ls | grep nginx 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 docker.io/library/nginx:latest io.containerd.runtime.v1.linux</code> </pre> <br>  ...并<code>pause</code>一下： <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers ls | grep pause 0866803b612f2f55e7b6b83836bde09bd6530246239b7bde1e49c04c7038e43a k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux 21640aea0210b320fd637c22ff93b7e21473178de0073b05de83f3b116fc8834 k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux</code> </pre> <br> 以<code>…983c7</code>结尾的<code>nginx</code>容器ID与我们从<code>kubectl</code>获得的<code>kubectl</code> 。 让我们看看是否可以确定哪个<code>pause</code>容器属于<code>nginx</code> pod： <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io task ls TASK PID STATUS ... d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 27255 RUNNING 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 27331 RUNNING</code> </pre> <br> 还记得<code>cni-912bcc63–712d-1c84–89a7–9e10510808a0</code>网络名称空间中运行的PID为27331和27355的进程吗？ <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers info d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 { <span class="hljs-string"><span class="hljs-string">"ID"</span></span>: <span class="hljs-string"><span class="hljs-string">"d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6"</span></span>, <span class="hljs-string"><span class="hljs-string">"Labels"</span></span>: { <span class="hljs-string"><span class="hljs-string">"io.cri-containerd.kind"</span></span>: <span class="hljs-string"><span class="hljs-string">"sandbox"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx-65899c769f-wxdx6"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.namespace"</span></span>: <span class="hljs-string"><span class="hljs-string">"default"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.uid"</span></span>: <span class="hljs-string"><span class="hljs-string">"0b35e956-8080-11e8-8aa9-0a12b8818382"</span></span>, <span class="hljs-string"><span class="hljs-string">"pod-template-hash"</span></span>: <span class="hljs-string"><span class="hljs-string">"2145573259"</span></span>, <span class="hljs-string"><span class="hljs-string">"run"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx"</span></span> }, <span class="hljs-string"><span class="hljs-string">"Image"</span></span>: <span class="hljs-string"><span class="hljs-string">"k8s.gcr.io/pause:3.1"</span></span>, ...</code> </pre> <br>  ...和： <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers info 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 { <span class="hljs-string"><span class="hljs-string">"ID"</span></span>: <span class="hljs-string"><span class="hljs-string">"4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7"</span></span>, <span class="hljs-string"><span class="hljs-string">"Labels"</span></span>: { <span class="hljs-string"><span class="hljs-string">"io.cri-containerd.kind"</span></span>: <span class="hljs-string"><span class="hljs-string">"container"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.container.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx-65899c769f-wxdx6"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.namespace"</span></span>: <span class="hljs-string"><span class="hljs-string">"default"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.uid"</span></span>: <span class="hljs-string"><span class="hljs-string">"0b35e956-8080-11e8-8aa9-0a12b8818382"</span></span> }, <span class="hljs-string"><span class="hljs-string">"Image"</span></span>: <span class="hljs-string"><span class="hljs-string">"docker.io/library/nginx:latest"</span></span>, ...</code> </pre> <br> 现在，我们确切地知道此容器（ <code>nginx-65899c769f-wxdx6</code> ）和网络名称空间（ <code>cni-912bcc63–712d-1c84–89a7–9e10510808a0</code> ）中正在运行的容器： <br><br><ul><li>  nginx（ID： <code>4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7</code> ）; </li><li> 暂停（ID： <code>d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6</code> ）。 </li></ul><br><img src="https://habrastorage.org/webt/3h/cx/qq/3hcxqqv-mwlrm8ax9lu9jl0fixy.png"><br><br>  （ <code>nginx-65899c769f-wxdx6</code> ）下的该如何连接到网络？ 我们使用先前从<code>pause</code>接收到的PID 27255在其网络名称空间（ <code>cni-912bcc63–712d-1c84–89a7–9e10510808a0</code> ）中运行命令： <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns identify 27255 cni-912bcc63-712d-1c84-89a7-9e10510808a0</code> </pre> <br> 为此，我们将<code>nsenter</code>与<code>-t</code>选项一起使用，该选项定义了目标PID，而<code>-n</code>没有指定文件以进入目标进程的网络名称空间（27255）。 这是<code>ip link show</code>内容： <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo nsenter -t 27255 -n ip link show 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 3: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 0a:58:0a:c8:00:04 brd ff:ff:ff:ff:ff:ff link-netnsid 0</code> </pre> <br>  ...和<code>ifconfig eth0</code> ： <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo nsenter -t 27255 -n ifconfig eth0 eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 10.200.0.4 netmask 255.255.255.0 broadcast 0.0.0.0 inet6 fe80::2097:51ff:fe39:ec21 prefixlen 64 scopeid 0x20&lt;link&gt; ether 0a:58:0a:c8:00:04 txqueuelen 0 (Ethernet) RX packets 540 bytes 42247 (42.2 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 177 bytes 16530 (16.5 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0</code> </pre> <br> 这确认先前在<code>eth0</code>接口上配置了通过<code>kubectl get pod</code>获得的IP地址。 此接口是<b>veth对的</b>一部分，其一端在炉膛中，另一端在根名称空间中。 为了找到第二端的接口，我们使用<code>ethtool</code> ： <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> cni-912bcc63-712d-1c84-89a7-9e10510808a0 ethtool -S eth0 NIC statistics: peer_ifindex: 7</code> </pre> <br> 我们看到<code>ifindex</code>盛宴的<code>ifindex</code>为7。检查它是否在根名称空间中。 这可以使用<code>ip link</code>完成： <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip link | grep <span class="hljs-string"><span class="hljs-string">'^7:'</span></span> 7: veth71f7d238@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master cnio0 state UP mode DEFAULT group default</code> </pre> <br> 最后要确定这一点，让我们看看： <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo cat /sys/class/net/veth71f7d238/ifindex 7</code> </pre> <br> 太好了，虚拟链接现在一切都清晰了。 使用<code>brctl</code>让我们看看还有谁连接到Linux网桥： <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ brctl show cnio0 bridge name bridge id STP enabled interfaces cnio0 8000.0a580ac80001 no veth71f7d238 veth73f35410 vethf273b35f</code> </pre> <br> 因此，图片如下： <br><br><img src="https://habrastorage.org/webt/yu/gc/t6/yugct6efi7ztep277en4msjuzv4.png"><br><br><h3> 路由检查 </h3><br> 我们实际上如何转发流量？ 让我们看一下网络名称空间窗格中的路由表： <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> cni-912bcc63-712d-1c84-89a7-9e10510808a0 ip route show default via 10.200.0.1 dev eth0 10.200.0.0/24 dev eth0 proto kernel scope link src 10.200.0.4</code> </pre> <br> 至少我们知道如何到达根名称空间（ <code>default via 10.200.0.1</code> ）。 现在让我们看一下主机路由表： <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip route list default via 10.240.0.1 dev eth0 proto dhcp src 10.240.0.20 metric 100 10.200.0.0/24 dev cnio0 proto kernel scope link src 10.200.0.1 10.240.0.0/24 dev eth0 proto kernel scope link src 10.240.0.20 10.240.0.1 dev eth0 proto dhcp scope link src 10.240.0.20 metric 100</code> </pre> <br> 我们知道如何将数据包转发到VPC路由器（VPC <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">具有</a> “隐式”路由器，该路由器<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">通常具有</a>子网主IP地址空间中的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">第二个地址</a> ）。 现在：VPC路由器知道如何访问每个炉床的网络吗？ 不，他没有，因此，假定路由将由CNI插件配置或<a href="">手动配置</a> （如手册中所述）。 显然， <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">AWS CNI插件</a>正是在AWS上为我们完成的。 请记住，有<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">很多CNI插件</a> ，我们正在考虑一个<b>简单的网络配置</b>示例： <br><br><img src="https://habrastorage.org/webt/cn/v7/v_/cnv7v_qjfkidbtuljkbgkuzuaag.png"><br><br><h3> 深浸在NAT中 </h3><br>  <code>kubectl create -f busybox.yaml</code>使用Replication Controller创建两个相同的<code>busybox</code>容器： <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: ReplicationController metadata: name: busybox0 labels: app: busybox0 spec: replicas: 2 selector: app: busybox0 template: metadata: name: busybox0 labels: app: busybox0 spec: containers: - image: busybox command: - sleep - "3600" imagePullPolicy: IfNotPresent name: busybox restartPolicy: Always</code> </pre> <br>  （ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><code>busybox.yaml</code></a> ） <br><br> 我们得到： <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE busybox0-g6pww 1/1 Running 0 4s 10.200.1.15 worker-1 busybox0-rw89s 1/1 Running 0 4s 10.200.0.21 worker-0 ...</code> </pre> <br> 从一个容器到另一个容器的ping操作应该成功： <br><br><pre> <code class="bash hljs">$ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it busybox0-rw89s -- ping -c 2 10.200.1.15 PING 10.200.1.15 (10.200.1.15): 56 data bytes 64 bytes from 10.200.1.15: seq=0 ttl=62 time=0.528 ms 64 bytes from 10.200.1.15: seq=1 ttl=62 time=0.440 ms --- 10.200.1.15 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.440/0.484/0.528 ms</code> </pre> <br> 要了解流量的移动，可以使用<code>tcpdump</code>或<code>conntrack</code>查看数据包： <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 10.200.1.15 icmp 1 29 src=10.200.0.21 dst=10.200.1.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1280 src=10.200.1.15 dst=10.240.0.20 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1280 mark=0 use=1</code> </pre> <br> 来自Pod 10.200.0.21的源IP被转换为主机10.240.0.20的IP地址。 <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 10.200.1.15 icmp 1 28 src=10.240.0.20 dst=10.200.1.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1280 src=10.200.1.15 dst=10.240.0.20 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1280 mark=0 use=1</code> </pre> <br> 在iptables中，您可以看到计数在增加： <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo iptables -t nat -Z POSTROUTING -L -v Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> out <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> destination ... 5 324 CNI-be726a77f15ea47ff32947a3 all -- any any 10.200.0.0/24 anywhere /* name: <span class="hljs-string"><span class="hljs-string">"bridge"</span></span> id: <span class="hljs-string"><span class="hljs-string">"631cab5de5565cc432a3beca0e2aece0cef9285482b11f3eb0b46c134e457854"</span></span> */ Zeroing chain `POSTROUTING<span class="hljs-string"><span class="hljs-string">'</span></span></code> </pre> <br> 另一方面，如果从CNI插件配置中删除<code>"ipMasq": true</code> ，则可以看到以下内容（此操作仅出于教育目的而执行-我们不建议在工作集群上更改配置！）： <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE busybox0-2btxn 1/1 Running 0 16s 10.200.0.15 worker-0 busybox0-dhpx8 1/1 Running 0 16s 10.200.1.13 worker-1 ...</code> </pre> <br>  Ping应该仍然通过： <br><br><pre> <code class="bash hljs">$ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it busybox0-2btxn -- ping -c 2 10.200.1.13 PING 10.200.1.6 (10.200.1.6): 56 data bytes 64 bytes from 10.200.1.6: seq=0 ttl=62 time=0.515 ms 64 bytes from 10.200.1.6: seq=1 ttl=62 time=0.427 ms --- 10.200.1.6 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.427/0.471/0.515 ms</code> </pre> <br> 在这种情况下-不使用NAT： <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 10.200.1.13 icmp 1 29 src=10.200.0.15 dst=10.200.1.13 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1792 src=10.200.1.13 dst=10.200.0.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1792 mark=0 use=1</code> </pre> <br> 因此，我们检查了“所有容器都可以在不使用NAT的情况下与任何其他容器进行通信”。 <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 10.200.1.13 icmp 1 27 src=10.200.0.15 dst=10.200.1.13 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1792 src=10.200.1.13 dst=10.200.0.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1792 mark=0 use=1</code> </pre> <br><h2> 群集网络（10.32.0.0/24） </h2><br> 您可能已经在<code>busybox</code>示例中注意到，分配给<code>busybox</code>的IP地址在每种情况下都是不同的。 如果我们想使这些容器可用于其他炉床之间的通信呢？ 一个人可能会使用Pod的当前IP地址，但是它们会改变。 因此，您需要配置<code>Service</code>资源，该资源会将请求代理到许多短暂的炉膛。 <br><br><blockquote>  “ Kubernetes中的服务是一种抽象，定义了炉膛的逻辑集和可访问它们的策略。”  <i>（来自<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes Services</a>文档）</i> </blockquote><br> 有多种发布服务的方式。 默认类型为<code>ClusterIP</code> ，它从群集的CIDR块设置IP地址（即只能从群集访问）。 一个这样的示例是在Kubernetes The Hard Way中配置的DNS群集附加组件。 <br><br><pre> <code class="plaintext hljs"># ... apiVersion: v1 kind: Service metadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: "KubeDNS" spec: selector: k8s-app: kube-dns clusterIP: 10.32.0.10 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP # ...</code> </pre> <br>  （ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><code>kube-dns.yaml</code></a> ） <br><br>  <code>kubectl</code>表明<code>Service</code>记住端点并进行转换： <br><br><pre> <code class="bash hljs">$ kubectl -n kube-system describe services ... Selector: k8s-app=kube-dns Type: ClusterIP IP: 10.32.0.10 Port: dns 53/UDP TargetPort: 53/UDP Endpoints: 10.200.0.27:53 Port: dns-tcp 53/TCP TargetPort: 53/TCP Endpoints: 10.200.0.27:53 ...</code> </pre> <br> 究竟是什么？.. <code>iptables</code>又一次。 让我们看一下为该示例创建的规则。 可以使用<code>iptables-save</code>命令查看其完整列表。 <br><br> 一旦数据包由进程（ <code>OUTPUT</code> ）创建或到达网络接口（ <code>PREROUTING</code> ），它们便通过以下<code>iptables</code>链： <br><br><pre> <code class="bash hljs">-A PREROUTING -m comment --comment <span class="hljs-string"><span class="hljs-string">"kubernetes service portals"</span></span> -j KUBE-SERVICES -A OUTPUT -m comment --comment <span class="hljs-string"><span class="hljs-string">"kubernetes service portals"</span></span> -j KUBE-SERVICES</code> </pre> <br> 以下目标对应于发送到10.32.0.10处的第53个端口的TCP数据包，并通过第53个端口发送给接收者10.200.0.27： <br><br><pre> <code class="bash hljs">-A KUBE-SERVICES -d 10.32.0.10/32 -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp cluster IP"</span></span> -m tcp --dport 53 -j KUBE-SVC-ERIFXISQEP7F7OF4 -A KUBE-SVC-ERIFXISQEP7F7OF4 -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp"</span></span> -j KUBE-SEP-32LPCMGYG6ODGN3H -A KUBE-SEP-32LPCMGYG6ODGN3H -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp"</span></span> -m tcp -j DNAT --to-destination 10.200.0.27:53</code> </pre> <br> 对于UDP数据包也是如此（收件人10.32.0.10:53→10.200.0.27:53）： <br><br><pre> <code class="bash hljs">-A KUBE-SERVICES -d 10.32.0.10/32 -p udp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns cluster IP"</span></span> -m udp --dport 53 -j KUBE-SVC-TCOU7JCQXEZGVUNU -A KUBE-SVC-TCOU7JCQXEZGVUNU -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns"</span></span> -j KUBE-SEP-LRUTK6XRXU43VLIG -A KUBE-SEP-LRUTK6XRXU43VLIG -p udp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns"</span></span> -m udp -j DNAT --to-destination 10.200.0.27:53</code> </pre> <br>  Kubernetes中还有其他类型的<code>Services</code> 。 特别是，Kubernetes The Hard Way <code>NodePort</code>了<code>NodePort</code> ，请参阅<a href="">Smoke Test：Services</a> 。 <br><br><pre> <code class="bash hljs">kubectl expose deployment nginx --port 80 --<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> NodePort</code> </pre> <br>  <code>NodePort</code>在每个节点的IP地址上发布服务，并将其放置在静态端口（称为<code>NodePort</code> ）上。  <code>NodePort</code>可以从群集外部访问<code>NodePort</code> 。 您可以使用<code>kubectl</code>检查专用端口（在本例中为31088）： <br><br><pre> <code class="bash hljs">$ kubectl describe services nginx ... Type: NodePort IP: 10.32.0.53 Port: &lt;<span class="hljs-built_in"><span class="hljs-built_in">unset</span></span>&gt; 80/TCP TargetPort: 80/TCP NodePort: &lt;<span class="hljs-built_in"><span class="hljs-built_in">unset</span></span>&gt; 31088/TCP Endpoints: 10.200.1.18:80 ...</code> </pre> <br>  Under现在可以从Internet上获得，网址为<code>http://${EXTERNAL_IP}:31088/</code> 。 此处<code>EXTERNAL_IP</code>是<b>任何工作实例</b>的公共IP地址。 在此示例中，我使用<b>worker-0</b>的公共IP地址。 内部IP地址为10.240.0.20的主机（云提供商从事公共NAT）接收到该请求，但是，该服务实际上是在另一台主机上启动的（ <b>worker-1</b> ，可以通过端点的IP地址-10.200.1.18看到）： <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 31088 tcp 6 86397 ESTABLISHED src=173.38.XXX.XXX dst=10.240.0.20 sport=30303 dport=31088 src=10.200.1.18 dst=10.240.0.20 sport=80 dport=30303 [ASSURED] mark=0 use=1</code> </pre> <br> 数据包从<b>worker-0</b>发送到<b>worker-1</b> ，在其中找到接收者： <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 80 tcp 6 86392 ESTABLISHED src=10.240.0.20 dst=10.200.1.18 sport=14802 dport=80 src=10.200.1.18 dst=10.240.0.20 sport=80 dport=14802 [ASSURED] mark=0 use=1</code> </pre> <br> 这样的电路理想吗？ 也许不行，但是行得通。 在这种情况下，已编程的<code>iptables</code>规则如下： <br><br><pre> <code class="bash hljs">-A KUBE-NODEPORTS -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -m tcp --dport 31088 -j KUBE-SVC-4N57TFCL4MD7ZTDA -A KUBE-SVC-4N57TFCL4MD7ZTDA -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -j KUBE-SEP-UGTFMET44DQG7H7H -A KUBE-SEP-UGTFMET44DQG7H7H -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -m tcp -j DNAT --to-destination 10.200.1.18:80</code> </pre> <br> 换句话说，带有端口31088的数据包接收方的地址在10.200.1.18上广播。 该港口还在广播，从31088到80。 <br><br> 我们没有涉及另一种类型的服务<code>LoadBalancer</code> ，它使用云提供商的负载平衡器使该服务公开可用，但事实证明这篇文章很大。 <br><br><h2> 结论 </h2><br> 似乎有很多信息，但我们只触及到了冰山一角。 将来，我将讨论IPv6，IPVS，eBPF和几个当前有趣的CNI插件。 <br><br><h2> 译者的PS </h2><br> 另请参阅我们的博客： <br><br><ul><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes网络插图指南</a> ”； </li><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes的网络性能比较</a> ”； </li><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">在kubernetes中进行kube-proxy和主机不可访问性的实验</a> ”； </li><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">改善Kubernetes的可靠性：如何快速注意到一个节点已经崩溃</a> ”； </li><li> « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Play with Kubernetes —      K8s</a> »; </li><li> « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">   Kubernetes   </a> » <i>( ,        Kubernetes)</i> ; </li><li> « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Container Networking Interface (CNI) —      Linux-</a> ». </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN420813/">https://habr.com/ru/post/zh-CN420813/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN420799/index.html">MPS 2018.2：生成器测试，GitHub插件，VCS Aspect，迁移通知等</a></li>
<li><a href="../zh-CN420803/index.html">3D打印课程。 从3Dtool打印非功能性模型时节省塑料</a></li>
<li><a href="../zh-CN420805/index.html">[翻译]何时使用并行流</a></li>
<li><a href="../zh-CN420809/index.html">安全周31：五十种不安全的Android阴影</a></li>
<li><a href="../zh-CN420811/index.html">新一代分散信使和电话网络</a></li>
<li><a href="../zh-CN420815/index.html">“解码数字世界”是如何爆炸的：DotNext 2018 Piter的十大报道</a></li>
<li><a href="../zh-CN420819/index.html">用于机器学习和数据科学的十大Python工具</a></li>
<li><a href="../zh-CN420821/index.html">规则10：编程和写作为1</a></li>
<li><a href="../zh-CN420825/index.html">今天将是OpenAI和Dota 2专业人士之间的首次较量（获胜者）。 我们了解机器人的工作原理</a></li>
<li><a href="../zh-CN420827/index.html">使用Java EE + WildFly10 + JPA（Hibernate）+ Postgresql + EJB + IntelliJ IDEA创建一个简单的Maven项目</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>