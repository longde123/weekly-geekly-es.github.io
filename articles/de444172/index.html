<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíò ‚å®Ô∏è üë≤üèº Sieben Mythen in der maschinellen Lernforschung üö∏ ‚ùáÔ∏è üëºüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="F√ºr diejenigen, die zu faul sind, um alles zu lesen: Es wird eine Widerlegung von sieben popul√§ren Mythen vorgeschlagen, die im Bereich der maschinell...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Sieben Mythen in der maschinellen Lernforschung</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/444172/">  F√ºr diejenigen, die zu faul sind, um alles zu lesen: Es wird eine Widerlegung von sieben popul√§ren Mythen vorgeschlagen, die im Bereich der maschinellen Lernforschung ab Februar 2019 h√§ufig als wahr angesehen wird. Dieser Artikel ist auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der ArXiv-Website im PDF-Format</a> [in englischer Sprache] verf√ºgbar. <br><br>  Mythos 1: TensorFlow ist eine Tensorbibliothek. <br>  Mythos 2: Bilddatenbanken spiegeln echte Fotos wider, die in der Natur gefunden wurden. <br>  Mythos 3: MO-Forscher verwenden keine Testkits zum Testen. <br>  Mythos 4: Neuronales Netzwerktraining verwendet alle Eingabedaten. <br>  Mythos 5: Eine Chargennormalisierung ist erforderlich, um sehr tiefe Restnetzwerke zu trainieren. <br>  Mythos 6: Netzwerke mit Aufmerksamkeit sind besser als Faltung. <br>  Mythos 7: Signifikanzkarten sind eine zuverl√§ssige Methode zur Interpretation neuronaler Netze. <br><br>  Und jetzt zu den Details. <br><a name="habracut"></a><br><h2>  Mythos 1: TensorFlow ist eine Tensorbibliothek </h2><br>  Tats√§chlich ist dies eine Bibliothek f√ºr die Arbeit mit Matrizen, und dieser Unterschied ist sehr bedeutend. <br><br>  Bei der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Berechnung von Derivaten h√∂herer Ordnung von Matrix- und Tensorausdr√ºcken.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Laue et al.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Die</a> Autoren von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NeurIPS 2018</a> zeigen, dass ihre Bibliothek der automatischen Differenzierung, basierend auf der realen Tensorrechnung, viel kompaktere Expressionsb√§ume aufweist.  Tatsache ist, dass die Tensorrechnung die Indexnotation verwendet, mit der Sie gleicherma√üen mit dem direkten und dem umgekehrten Modus arbeiten k√∂nnen. <br><br>  Die Matrixnummerierung verbirgt Indizes zur Vereinfachung der Notation, weshalb automatische Differenzierungsausdrucksb√§ume h√§ufig zu komplex werden. <br><br>  Betrachten Sie die Matrixmultiplikation C = AB.  Wir haben <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>C</mi></mrow><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>A</mi></mrow><mi>B</mi><mo>+</mo><mi>A</mi><mtext>&amp;#xA0;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>B</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="26.011ex" height="2.178ex" viewBox="0 -780.1 11199 937.7" role="img" focusable="false" style="vertical-align: -0.366ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-64" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-6F" x="773" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-74" x="1259" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-43" x="1620" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMAIN-3D" x="2658" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-64" x="3965" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-6F" x="4488" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-74" x="4974" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-41" x="5335" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-42" x="6086" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMAIN-2B" x="7067" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-41" x="8068" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-64" x="9069" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-6F" x="9592" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-74" x="10078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-42" x="10439" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>C</mi></mrow><mo>=</mo><mtext>&nbsp;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>A</mi></mrow><mi>B</mi><mo>+</mo><mi>A</mi><mtext>&nbsp;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>B</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-1"> \ dot {C} = \ dot {A} B + A \ dot {B} </script>  f√ºr direkten Modus und <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>C</mi></mrow><msup><mi>B</mi><mi>T</mi></msup><mo>,</mo><mi>B</mi><mo>=</mo><msup><mi>A</mi><mi>T</mi></msup><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>C</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="28.27ex" height="2.78ex" viewBox="0 -935.7 12171.6 1197.1" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-41" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMAIN-3D" x="1028" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-62" x="2334" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-61" x="2764" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-72" x="3293" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-43" x="3745" y="0"></use><g transform="translate(4505,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-42" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-54" x="1074" y="513"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMAIN-2C" x="5863" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-42" x="6308" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMAIN-3D" x="7345" y="0"></use><g transform="translate(8401,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-41" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-54" x="1061" y="513"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-62" x="10000" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-61" x="10430" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-72" x="10959" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-43" x="11411" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi><mo>=</mo><mtext>&nbsp;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class="MJX-TeXAtom-ORD"><mi>C</mi></mrow><msup><mi>B</mi><mi>T</mi></msup><mo>,</mo><mi>B</mi><mo>=</mo><msup><mi>A</mi><mi>T</mi></msup><mtext>&nbsp;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class="MJX-TeXAtom-ORD"><mi>C</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-2"> A = \ bar {C} B ^ T, B = A ^ T \ bar {C} </script>  f√ºr das Gegenteil.  Um die Multiplikation korrekt durchzuf√ºhren, m√ºssen Sie die Reihenfolge und Verwendung der Silbentrennung genau beachten.  Aus Sicht der Aufzeichnung sieht dies f√ºr eine an MO beteiligte Person verwirrend aus, aus Sicht der Berechnungen ist dies jedoch eine zus√§tzliche Belastung f√ºr das Programm. <br><br>  Ein anderes Beispiel, weniger trivial: c = det (A).  Wir haben <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>c</mi></mrow><mo>=</mo><mi>t</mi><mi>r</mi><mo stretchy=&quot;false&quot;>(</mo><mi>i</mi><mi>n</mi><mi>v</mi><mo stretchy=&quot;false&quot;>(</mo><mi>A</mi><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>A</mi></mrow><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="23.951ex" height="2.66ex" viewBox="0 -832 10312.1 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-64" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-6F" x="773" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-74" x="1259" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-63" x="1620" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMAIN-3D" x="2331" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-74" x="3388" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-72" x="3749" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMAIN-28" x="4201" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-69" x="4590" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-6E" x="4936" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-76" x="5536" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMAIN-28" x="6022" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-41" x="6411" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMAIN-29" x="7162" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-64" x="7801" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-6F" x="8325" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-74" x="8810" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-41" x="9172" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMAIN-29" x="9922" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>c</mi></mrow><mo>=</mo><mi>t</mi><mi>r</mi><mo stretchy="false">(</mo><mi>i</mi><mi>n</mi><mi>v</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mtext>&nbsp;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>A</mi></mrow><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-3"> \ dot {c} = tr (inv (A) \ dot {A}) </script>  f√ºr direkten Modus und <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>A</mi></mrow><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>c</mi></mrow><mi>c</mi><mi>i</mi><mi>n</mi><mi>v</mi><mo stretchy=&quot;false&quot;>(</mo><mi>A</mi><msup><mo stretchy=&quot;false&quot;>)</mo><mi>T</mi></msup></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="22.835ex" height="2.901ex" viewBox="0 -935.7 9831.7 1249" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-62" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-61" x="679" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-72" x="1209" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-41" x="1660" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMAIN-3D" x="2688" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-62" x="3995" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-61" x="4424" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-72" x="4954" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-63" x="5405" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-63" x="5839" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-69" x="6272" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-6E" x="6618" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-76" x="7218" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMAIN-28" x="7704" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-41" x="8093" y="0"></use><g transform="translate(8844,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMAIN-29" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhjZoqHvl3e1Datgq_T2yil03_qniA#MJMATHI-54" x="550" y="513"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class="MJX-TeXAtom-ORD"><mi>A</mi></mrow><mo>=</mo><mtext>&nbsp;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class="MJX-TeXAtom-ORD"><mi>c</mi></mrow><mi>c</mi><mi>i</mi><mi>n</mi><mi>v</mi><mo stretchy="false">(</mo><mi>A</mi><msup><mo stretchy="false">)</mo><mi>T</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-4"> \ bar {A} = \ bar {c} cinv (A) ^ T </script>  f√ºr das Gegenteil.  In diesem Fall ist es offensichtlich unm√∂glich, den Ausdrucksbaum f√ºr beide Modi zu verwenden, da sie aus unterschiedlichen Operatoren bestehen. <br><br>  Im Allgemeinen hat die Art und Weise, wie TensorFlow und andere Bibliotheken (z. B. Mathematica, Maple, Sage, SimPy, ADOL-C, TAPENADE, TensorFlow, Theano, PyTorch, HIPS-Autograd) eine automatische Differenzierung implementiert, was dazu f√ºhrt, dass f√ºr direkte und umgekehrte Im Modus werden verschiedene und ineffektive Ausdrucksb√§ume erstellt.  Die Tensornummerierung umgeht diese Probleme aufgrund der Kommutativit√§t der Multiplikation aufgrund der Indexnotation.  Einzelheiten dazu finden Sie im wissenschaftlichen Artikel. <br><br>  Die Autoren testeten ihre Methode, indem sie das umgekehrte Regime, auch als R√ºckausbreitung bekannt, in drei verschiedenen Aufgaben automatisch differenzierten und die Zeit ma√üen, die zur Berechnung der Hessen ben√∂tigt wurde. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/923/263/331/923263331347f83e3bac0fe7a9477e8b.png"><br><br>  Im ersten Problem wurde die quadratische Funktion x <sup>T</sup> Ax optimiert.  In der zweiten wurde die logistische Regression berechnet, in der dritten - Matrixfaktorisierung. <br><br>  Auf der CPU erwies sich ihre Methode als zwei Gr√∂√üenordnungen schneller als beliebte Bibliotheken wie TensorFlow, Theano, PyTorch und HIPS autograd. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/184/395/7dd/1843957dde52a4722da309adcdd6ea92.png"><br><br>  Auf der GPU beobachteten sie eine noch gr√∂√üere Beschleunigung um bis zu drei Gr√∂√üenordnungen. <br><br>  <b>Die Folgen:</b> <br><br>  Das Berechnen von Ableitungen f√ºr Funktionen zweiter oder h√∂herer Ordnung unter Verwendung aktueller Deep-Learning-Bibliotheken ist aus rechnerischer Sicht zu teuer.  Dies schlie√üt die Berechnung allgemeiner Tensoren vierter Ordnung wie Hessische ein (z. B. in MAML und Newtons Optimierung zweiter Ordnung).  Gl√ºcklicherweise sind quadratische Formeln beim tiefen Lernen selten.  Sie finden sich jedoch h√§ufig im ‚Äûklassischen‚Äú maschinellen Lernen - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SVM</a> , Methode der kleinsten Quadrate, LASSO, Gau√üsche Prozesse usw. <br><br><h2>  Mythos 2: Bilddatenbanken spiegeln Fotos aus der realen Welt wider </h2><br>  Viele Menschen denken gerne, dass neuronale Netze gelernt haben, Objekte besser zu erkennen als Menschen.  Es ist nicht so.  Sie k√∂nnen aufgrund ausgew√§hlter Bilder, z. B. ImageNet, den Menschen voraus sein, aber im Falle der Erkennung von Objekten aus realen Fotos aus dem normalen Leben k√∂nnen sie einen normalen Erwachsenen definitiv nicht √ºberholen.  Dies liegt daran, dass die Auswahl der Bilder in den aktuellen Datens√§tzen nicht mit der Auswahl aller m√∂glichen Bilder √ºbereinstimmt, die in der Realit√§t nat√ºrlich vorkommen. <br><br>  In einer ziemlich alten Arbeit, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Unbiased Look at Dataset Bias.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Torralba und Efros.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CVPR 2011.</a> , Die Autoren schlugen vor, die mit einer Reihe von Bildern verbundenen Verzerrungen in zw√∂lf g√§ngigen Datenbanken zu untersuchen und herauszufinden, ob es m√∂glich ist, den Klassifikator zu trainieren, um den Datensatz zu bestimmen, aus dem dieses Bild aufgenommen wurde. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/400/31a/f1a/40031af1a6a5bef3c9314ecb2373926a.png"><br><br>  Die Wahrscheinlichkeit, versehentlich den richtigen Datensatz zu erraten, liegt bei 1/12 ‚âà 8%, w√§hrend die Wissenschaftler selbst mit einer Erfolgsquote von&gt; 75% die Aufgabe bew√§ltigten. <br><br>  Sie trainierten SVM anhand eines <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Richtungsgradienten-Histogramms</a> (HOG) und stellten fest, dass der Klassifikator die Aufgabe in 39% der F√§lle erledigte, was die zuf√§lligen Treffer deutlich √ºbersteigt.  Wenn wir dieses Experiment heute mit den fortschrittlichsten neuronalen Netzen wiederholen w√ºrden, w√ºrden wir sicherlich eine Erh√∂hung der Genauigkeit des Klassifikators sehen. <br><br>  Wenn die Bilddatenbanken die wahren Bilder der realen Welt korrekt anzeigen w√ºrden, m√ºssten wir nicht feststellen k√∂nnen, aus welchem ‚Äã‚ÄãDatensatz ein bestimmtes Bild stammt. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/879/7f7/4ad/8797f74ad14a0a4e28959ff0c67faca8.png"><br><br>  Es gibt jedoch Merkmale in den Daten, die jeden Bildsatz von den anderen unterscheiden.  ImageNet hat viele Rennwagen, die das ‚Äûtheoretische‚Äú Durchschnittsauto als Ganzes wahrscheinlich nicht beschreiben. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/614/416/401/6144164018d5608bc5c2dc55e6e981f6.png"><br><br>  Die Autoren bestimmten auch den Wert jedes Datensatzes, indem sie ma√üen, wie gut ein auf einem Satz trainierter Klassifikator mit Bildern aus anderen S√§tzen funktioniert.  Nach dieser Metrik erwiesen sich die LabelMe- und ImageNet-Datenbanken als am wenigsten voreingenommen, da sie nach der Methode des ‚ÄûW√§hrungskorbs‚Äú ein Rating von 0,58 erhalten hatten.  Es stellte sich heraus, dass alle Werte kleiner als eins sind, was bedeutet, dass das Training mit einem anderen Datensatz immer zu einer schlechten Leistung f√ºhrt.  In einer idealen Welt ohne voreingenommene Mengen sollten einige Zahlen eine √ºberschritten haben. <br><br>  Die Autoren folgerten pessimistisch: <br><blockquote>  Welchen Wert haben vorhandene Datens√§tze f√ºr Trainingsalgorithmen, die f√ºr die reale Welt entwickelt wurden?  Die resultierende Antwort kann als "besser als nichts, aber nicht viel" beschrieben werden. </blockquote><br><br><h2>  Mythos 3: MO-Forscher verwenden keine Testkits zum Testen </h2><br>  Im Lehrbuch √ºber maschinelles Lernen lernen wir, den Datensatz in Training, Bewertung und Verifikation zu unterteilen.  Die Effektivit√§t des Modells, das anhand des Trainingssatzes trainiert und anhand der Bewertung bewertet wurde, hilft der am MO beteiligten Person, das Modell zu optimieren, um die Effizienz bei seiner tats√§chlichen Verwendung zu maximieren.  Der Testsatz muss erst ber√ºhrt werden, wenn die Person die Anpassung abgeschlossen hat, um eine unvoreingenommene Bewertung der tats√§chlichen Wirksamkeit des Modells in der realen Welt zu erhalten.  Wenn eine Person in der Trainings- oder Bewertungsphase mit einem Testsatz betr√ºgt, besteht die Gefahr, dass das Modell f√ºr einen bestimmten Datensatz zu angepasst wird. <br><br>  In der hart umk√§mpften Welt der MO-Forschung werden neue Algorithmen und Modelle h√§ufig anhand der Effektivit√§t ihrer Arbeit mit Verifizierungsdaten beurteilt.  Daher ist es f√ºr Forscher nicht sinnvoll, Artikel zu schreiben oder zu ver√∂ffentlichen, in denen Methoden beschrieben werden, die mit Testdatens√§tzen schlecht funktionieren.  Dies bedeutet im Wesentlichen, dass die Gemeinde der gesamten Region Moskau einen Testsatz zur Bewertung verwendet. <br><br>  Was sind die Folgen dieses Betrugs? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2d4/f64/749/2d4f647491071cd0b77eb199ef563275.png"><br><br>  Autoren von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verallgemeinern CIFAR-10-Klassifikatoren auf CIFAR-10?</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Recht et al.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ArXiv 2018</a> untersuchte dieses Problem, indem es eine neue Testsuite f√ºr CIFAR-10 erstellte.  Zu diesem Zweck haben sie eine Auswahl von Bildern aus Tiny Images getroffen. <br><br>  Sie entschieden sich f√ºr CIFAR-10, weil es einer der am h√§ufigsten verwendeten Datens√§tze im MO ist, dem zweitbeliebtesten Satz in NeurIPS 2017 (nach MNIST).  Der Prozess zum Erstellen eines Datensatzes f√ºr CIFAR-10 ist ebenfalls gut beschrieben und transparent. In der gro√üen Tiny Images-Datenbank befinden sich viele detaillierte Beschriftungen, sodass Sie einen neuen Testsatz reproduzieren und so die Verteilungsverschiebung minimieren k√∂nnen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/71a/cd9/091/71acd909148795f73b49819b39946a5a.png"><br><br>  Sie fanden heraus, dass eine gro√üe Anzahl verschiedener Modelle neuronaler Netze im neuen Testsatz einen signifikanten R√ºckgang der Genauigkeit aufwies (4% - 15%).  Der relative Leistungsrang jedes Modells blieb jedoch ziemlich stabil. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b6b/5aa/cfa/b6b5aacfab0c2628ea2ffe28babae533.png"><br><br>  Im Allgemeinen zeigten Modelle mit besserer Leistung einen geringeren Genauigkeitsabfall als Modelle mit schlechterer Leistung.  Dies ist sch√∂n, da sich daraus ergibt, dass der Verlust der Generalisierbarkeit des Modells aufgrund von Betrug, zumindest im Fall von CIFAR-10, abnimmt, wenn die Community verbesserte MO-Methoden und -Modelle erfindet. <br><br><h2>  Mythos 4: Neuronales Netzwerktraining verwendet alle Eingaben </h2><br>  Es ist allgemein anerkannt, dass <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Daten ein neues √ñl</a> sind und dass je mehr Daten wir haben, desto besser k√∂nnen wir Deep-Learning-Modelle trainieren, die jetzt ineffizient und √ºberparametrisiert sind. <br><br>  In <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einer empirischen Studie zum Beispiel des Vergessens w√§hrend des Lernens in tiefen neuronalen Netzen.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Toneva et al.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Die</a> Autoren des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ICLR 2019</a> zeigen eine signifikante Redundanz in mehreren g√§ngigen S√§tzen kleiner Bilder.  √úberraschenderweise k√∂nnen 30% der Daten aus CIFAR-10 einfach entfernt werden, ohne die Genauigkeit der Pr√ºfung erheblich zu ver√§ndern. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/418/39b/03a/41839b03afd94771e940b60e2d7f905d.png"><br>  <i>Geschichten des Vergessens von (von links nach rechts) MNIST, permutedMNIST und CIFAR-10.</i> <br><br>  Das Vergessen geschieht, wenn ein neuronales Netzwerk ein Bild zum Zeitpunkt t + 1 falsch klassifiziert, w√§hrend es zum Zeitpunkt t ein Bild korrekt klassifizieren konnte.  Der Zeitfluss wird durch SGD-Updates gemessen.  Um das Vergessen zu verfolgen, starteten die Autoren ihr neuronales Netzwerk nach jedem SGD-Update mit einem kleinen Datensatz und nicht mit allen in der Datenbank verf√ºgbaren Beispielen.  Beispiele, die nicht vergessen werden m√ºssen, werden als unvergessliche Beispiele bezeichnet. <br><br>  Sie fanden heraus, dass 91,7% MNIST, 75,3% permutierter MNIST, 31,3% CIFAR-10 und 7,62% CIFAR-100 unvergessliche Beispiele sind.  Dies ist intuitiv verst√§ndlich, da eine Erh√∂hung der Diversit√§t und Komplexit√§t des Datensatzes dazu f√ºhren sollte, dass das neuronale Netzwerk weitere Beispiele vergisst. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fdb/16b/ff9/fdb16bff98521d9d764588a679e51a59.png"><br><br>  Unvergessliche Beispiele scheinen seltener und seltsamer zu sein als unvergessliche.  Die Autoren vergleichen sie mit Unterst√ºtzungsvektoren in SVM, da sie den Umriss von Entscheidungsgrenzen zu zeichnen scheinen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a75/c74/969/a75c74969991b75f621d340952a5cde3.png"><br><br>  Unvergessliche Beispiele verschl√ºsseln wiederum meist redundante Informationen.  Wenn wir die Beispiele nach dem Grad der Unvergesslichkeit sortieren, k√∂nnen wir den Datensatz komprimieren, indem wir die unvergesslichsten l√∂schen. <br><br>  30% der CIFAR-10-Daten k√∂nnen gel√∂scht werden, ohne die Genauigkeit der Pr√ºfungen zu beeintr√§chtigen, und das L√∂schen von 35% der Daten f√ºhrt zu einem leichten R√ºckgang der Genauigkeit der Pr√ºfungen um 0,2%.  Wenn Sie 30% der Daten zuf√§llig ausw√§hlen, f√ºhrt das L√∂schen zu einem erheblichen Verlust der Genauigkeit der √úberpr√ºfung von 1%. <br><br>  In √§hnlicher Weise k√∂nnen 8% der Daten aus dem CIFAR-100 entfernt werden, ohne dass die Validierungsgenauigkeit abnimmt. <br><br>  Diese Ergebnisse zeigen, dass die Daten f√ºr das Training neuronaler Netze erheblich redundant sind, √§hnlich wie beim SVM-Training, bei dem nicht unterst√ºtzende Vektoren entfernt werden k√∂nnen, ohne die Modellentscheidung zu beeinflussen. <br><br>  <b>Die Folgen:</b> <br><br>  Wenn wir vor Beginn des Trainings feststellen k√∂nnen, welche Daten unvergesslich sind, k√∂nnen wir Platz sparen, indem wir sie und Zeit l√∂schen, ohne sie beim Training eines neuronalen Netzwerks zu verwenden. <br><br><h2>  Mythos 5: Eine Chargennormalisierung ist erforderlich, um sehr tiefe Restnetzwerke zu trainieren. </h2><br>  Lange Zeit glaubte man, dass "das Trainieren eines tiefen neuronalen Netzwerks zur direkten Optimierung nur f√ºr einen kontrollierten Zweck (zum Beispiel die logarithmische Wahrscheinlichkeit einer korrekten Klassifizierung) unter Verwendung eines Gradientenabfalls, beginnend mit zuf√§lligen Parametern, nicht gut funktioniert". <br><br>  Der seitdem aufgetauchte Haufen genialer Methoden zur zuf√§lligen Initialisierung, Aktivierungsfunktionen, Optimierungstechniken und andere Innovationen, wie z. B. Restverbindungen, erleichterten das Training tiefer neuronaler Netze mithilfe der Gradientenabstiegsmethode. <br><br>  Nach der Einf√ºhrung der Batch-Normalisierung (und anderer sequentieller Normalisierungstechniken) gelang jedoch ein echter Durchbruch, bei dem die Aktivierungsgr√∂√üe f√ºr jede Schicht des Netzwerks begrenzt wurde, um das Problem des Verschwindens und der explosiven Gradienten zu beseitigen. <br><br>  In einer k√ºrzlich erschienenen Arbeit, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fixup-Initialisierung: Restliches Lernen ohne Normalisierung.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zhang et al.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ICLR 2019</a> hat gezeigt, dass es m√∂glich ist, ein Netzwerk mit 10.000 Schichten mit reinem SGD zu trainieren, ohne eine Normalisierung anzuwenden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d69/a29/cbb/d69a29cbb7bfce9750c9aa3ee6d7d659.png"><br><br>  Die Autoren verglichen das verbleibende neuronale Netzwerktraining f√ºr verschiedene Tiefen auf CIFAR-10 und stellten fest, dass Standardinitialisierungsmethoden f√ºr 100 Schichten nicht funktionierten, Fixup- und Batch-Normalisierungsmethoden jedoch mit 10.000 Schichten erfolgreich waren. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bc9/23f/26a/bc923f26a4c39fb67663383716bc10d2.png" alt="Bild"><br><br>  Sie f√ºhrten eine theoretische Analyse durch und zeigten, dass ‚Äûdie Gradientennormalisierung bestimmter Schichten durch die unendlich zunehmende Anzahl aus einem tiefen Netzwerk begrenzt ist‚Äú, was ein Problem explosiver Gradienten darstellt.  Um dies zu verhindern, wird Foxup verwendet, dessen Schl√ºsselidee darin besteht, die Gewichte in m Schichten f√ºr jeden der verbleibenden Zweige L um die H√§ufigkeit zu skalieren, die von m und L abh√§ngt. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/07f/a8f/39a/07fa8f39ad814861fd0cba6c936a2e18.png"><br><br>  Fixup half dabei, ein tiefes Restnetzwerk mit 110 Schichten auf dem CIFAR-10 mit einer hohen Lerngeschwindigkeit zu trainieren, die mit dem Verhalten eines Netzwerks √§hnlicher Architektur vergleichbar ist, das mithilfe der Batch-Normalisierung trainiert wurde. <br><br>  Die Autoren zeigten weiterhin √§hnliche Testergebnisse mit Fixup im Netzwerk ohne Normalisierung, wobei sie mit der ImageNet-Datenbank und mit √úbersetzungen aus dem Englischen ins Deutsche arbeiteten. <br><br><h2>  Mythos 6: Netzwerke mit Aufmerksamkeit sind besser als Faltungsnetzwerke. </h2><br>  Die Idee, dass die Mechanismen der ‚ÄûAufmerksamkeit‚Äú den Faltungs-Neuronalen Netzen √ºberlegen sind, gewinnt in der Gemeinschaft der MO-Forscher an Popularit√§t.  In der Arbeit von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://crazyoscarchang.github.io/2019/02/16/seven-myths-in-machine-learning-research/(">Vaswani und Kollegen</a> wurde festgestellt, dass ‚Äûdie Berechnungskosten abnehmbarer Windungen gleich der Kombination einer Selbstaufmerksamkeitsschicht und einer punktweisen Vorw√§rtskopplungsschicht sind‚Äú. <br><br>  Selbst fortschrittliche generativ-wettbewerbsf√§hige Netzwerke zeigen den Vorteil der Selbstaufmerksamkeit gegen√ºber der Standardfaltung bei der Modellierung von Abh√§ngigkeiten √ºber gro√üe Entfernungen. <br><br>  Mitwirkende <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">achten bei leichten und dynamischen Konvolutionen weniger auf sie.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Wu et al.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ICLR 2019</a> wirft Zweifel an der parametrischen Effizienz und Effektivit√§t der Selbstaufmerksamkeit bei der Modellierung von Abh√§ngigkeiten √ºber gro√üe Entfernungen auf und bietet neue Faltungsoptionen, die teilweise von der Selbstaufmerksamkeit inspiriert sind und hinsichtlich der Parameter effektiver sind. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b40/f9a/a15/b40f9aa151eb4bcc3ebf224483ff8028.png"><br><br>  Die "leichten" Windungen sind in der Tiefe trennbar, in der Zeitdimension softmax-normalisiert, in der Kanaldimension durch das Gewicht getrennt und verwenden bei jedem Zeitschritt die gleichen Gewichte (als wiederkehrende neuronale Netze).  Dynamische Windungen sind leichte Windungen, die bei jedem Zeitschritt unterschiedliche Gewichte verwenden. <br><br>  Solche Tricks machen leichte und dynamische Windungen um mehrere Gr√∂√üenordnungen effektiver als unteilbare Standardwindungen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/87d/d51/da0/87dd51da07637c42c4e4c2811b08b241.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/c6a/469/ae5/c6a469ae5432cf31761e7663449497fd.png"><br><br>  Die Autoren zeigen, dass diese neuen Faltungen selbstabsorbierenden Netzwerken bei maschineller √úbersetzung, Sprachmodellierung und abstrakten Summierungsproblemen entsprechen oder diese √ºbertreffen, wobei dieselben oder weniger Parameter verwendet werden. <br><br><h2>  Mythos 7: Signifikanzkarten - eine zuverl√§ssige Methode zur Interpretation neuronaler Netze </h2><br>  Obwohl die Meinung besteht, dass neuronale Netze Black Boxes sind, gab es viele Versuche, sie zu interpretieren.  Am beliebtesten sind Signifikanzkarten oder andere √§hnliche Methoden, mit denen Features oder Trainingsbeispiele Wichtigkeitsbewertungen zugewiesen werden. <br><br>  Es ist verlockend zu schlie√üen, dass ein bestimmtes Bild aufgrund bestimmter Teile des Bildes, die f√ºr das neuronale Netzwerk von Bedeutung sind, auf eine bestimmte Weise klassifiziert wurde.  Zur Berechnung von Signifikanzkarten gibt es verschiedene Methoden, bei denen h√§ufig neuronale Netze in einem bestimmten Bild und die durch das Netz verlaufenden Gradienten aktiviert werden. <br><br>  In der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Interpretation neuronaler Netze ist fragil.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ghorbani et al.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Die</a> Autoren von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AAAI 2019</a> zeigen, dass sie eine schwer fassbare √Ñnderung im Bild einf√ºhren k√∂nnen, die jedoch die Signifikanzkarte verzerren wird. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/077/b89/e92/077b89e923e2936769b2f99662171a94.png"><br><br>  Das neuronale Netzwerk bestimmt den Monarchenschmetterling nicht anhand des Musters auf seinen Fl√ºgeln, sondern aufgrund des Vorhandenseins unwichtiger gr√ºner Bl√§tter vor dem Hintergrund des Fotos. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1d2/314/9fa/1d23149fa91caab32dc2ea16a8593379.png"><br><br>  Mehrdimensionale Bilder sind oft n√§her an Entscheidungsgrenzen, die durch tiefe neuronale Netze gebildet werden, daher ihre Empfindlichkeit gegen√ºber gegnerischen Angriffen.  Und wenn Wettbewerbsangriffe Bilder √ºber die Grenzen der L√∂sung hinaus verschieben, verschieben sie durch wettbewerbsorientierte interpretative Angriffe entlang der Grenze der L√∂sung, ohne das Gebiet derselben L√∂sung zu verlassen. <br><br>  Die von den Autoren entwickelte Grundmethode ist eine Modifikation der Goodfello-Methode zur schnellen Gradientenmarkierung, die eine der ersten erfolgreichen Methoden f√ºr Wettbewerbsangriffe war.  Es kann davon ausgegangen werden, dass auch andere, neuere und komplexere Angriffe f√ºr Angriffe auf die Interpretation neuronaler Netze verwendet werden k√∂nnen. <br><br>  <b>Die Folgen:</b> <br><br>  Aufgrund der zunehmenden Verbreitung von Deep Learning in so kritischen Anwendungsbereichen wie der medizinischen Bildgebung ist es wichtig, die Interpretation von Entscheidungen, die von neuronalen Netzen getroffen werden, sorgf√§ltig anzugehen.  Obwohl es gro√üartig w√§re, wenn das Faltungsnetzwerk den Fleck auf dem MRT-Bild als b√∂sartigen Tumor erkennen k√∂nnte, sollten diese Ergebnisse nicht als vertrauensw√ºrdig eingestuft werden, wenn sie auf unzuverl√§ssigen Interpretationsmethoden beruhen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de444172/">https://habr.com/ru/post/de444172/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de444162/index.html">Snom D120 IP-Telefon Bewertung</a></li>
<li><a href="../de444164/index.html">√úbersicht √ºber das Snom PA1-Warnsystem</a></li>
<li><a href="../de444166/index.html">Pavel Finkelstein √ºber Kotlin in Produktion auf jug.msk.ru</a></li>
<li><a href="../de444168/index.html">So √ºbertragen Sie lizenziertes Windows 10 auf einen anderen Computer</a></li>
<li><a href="../de444170/index.html">Ver√∂ffentlichen Sie iOS-Apps im App Store mit GitLab und Fastlane</a></li>
<li><a href="../de444174/index.html">GeekBrains l√§dt Anf√§nger zu einem Lernspiel ein</a></li>
<li><a href="../de444176/index.html">Grundlegende Chiffren im Klartext</a></li>
<li><a href="../de444178/index.html">9 Tipps zum Erstellen von Indie-Spielen von einem einzelnen Entwickler</a></li>
<li><a href="../de444182/index.html">Gehen Sie Bedingungen und ihre Kuriosit√§ten</a></li>
<li><a href="../de444184/index.html">√úber die Aussichten vormontierter Rechenzentren</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>