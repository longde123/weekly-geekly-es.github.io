<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§ΩüèΩ ü§∏ ü§∑üèº Livre ¬´Site Reliability Engineering. Fiabilit√© et fiabilit√© comme dans Google ¬ª ü•§ üèùÔ∏è üìî</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Depuis pr√®s de 20 ans, Google propose des syst√®mes √† grande √©chelle d'une complexit√© inimaginable et sensibles aux demandes des utilisateurs. Le moteu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Livre ¬´Site Reliability Engineering. Fiabilit√© et fiabilit√© comme dans Google ¬ª</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/420139/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/webt/dx/hl/8t/dxhl8tgecl0xwqtx3pnxztosrom.jpeg" align="left" alt="image"></a>  Depuis pr√®s de 20 ans, Google propose des syst√®mes √† grande √©chelle d'une complexit√© inimaginable et sensibles aux demandes des utilisateurs.  Le moteur de recherche Google trouve la r√©ponse √† toutes vos questions en une fraction de seconde, les cartes Google avec la plus grande pr√©cision refl√®tent le paysage terrestre, et la messagerie Google est disponible en mode 365/24/7 et, en substance, est devenue le premier stockage cloud public.  Ces syst√®mes sont-ils parfaits?  Non, ils √©chouent √©galement, tombent en panne et deviennent obsol√®tes, comme tout √©quipement.  Nous ne le remarquons tout simplement pas.  Le fait est que depuis plus de dix ans, Google d√©veloppe la technologie unique d'Ing√©nierie de la fiabilit√© du site, qui garantit un fonctionnement ininterrompu et un d√©veloppement progressif de syst√®mes logiciels de toute complexit√©.  Ce livre est un magasin d'exp√©rience accumul√© par Google au fil des ans, le travail collectif de nombreux sp√©cialistes exceptionnels et une ressource indispensable pour tout ing√©nieur qui souhaite d√©velopper et entretenir des produits de la plus haute qualit√© et de la mani√®re la plus efficace. <br><a name="habracut"></a><br><h3>  SRE de Google en termes de SRE </h3><br>  Les centres de donn√©es Google (centres de donn√©es) sont tr√®s diff√©rents des centres de donn√©es traditionnels et des "fermes" de petits serveurs.  Ces diff√©rences introduisent √† la fois des probl√®mes et des opportunit√©s suppl√©mentaires.  Ce chapitre traite des d√©fis et opportunit√©s sp√©cifiques aux centres de donn√©es Google et pr√©sente la terminologie qui sera utilis√©e tout au long du livre. <br><br>  <b>√âquipement</b> <br><br>  La plupart des ressources informatiques de Google sont situ√©es dans des centres de donn√©es con√ßus par l'entreprise, qui disposent de leur propre syst√®me d'alimentation, syst√®me de refroidissement, r√©seau interne et √©quipement informatique [Barroso et al., 2013].  Contrairement aux centres de donn√©es classiques fournis par les fournisseurs √† leurs clients, tous les centres de donn√©es Google sont √©quip√©s du m√™me1.  Pour √©viter toute confusion entre le mat√©riel du serveur et le logiciel du serveur, dans ce livre, nous utilisons la terminologie suivante: <br><br><ul><li>  <i>machine (ordinateur)</i> - une unit√© d'√©quipement (ou, √©ventuellement, une machine virtuelle); </li><li>  <i>serveur</i> - une unit√© de logiciel qui impl√©mente un service. </li></ul><br>  Tout serveur peut √™tre d√©marr√© sur des machines, par cons√©quent, nous n'allouons pas d'ordinateurs sp√©cifiques √† des programmes de serveur sp√©cifiques.  Par exemple, nous n'avons pas de machine sp√©cifique sur laquelle le serveur de messagerie s'ex√©cute.  Au lieu de cela, les ressources sont allou√©es par notre syst√®me de gestion de cluster Borg. <br><br>  Nous comprenons qu'une telle utilisation du terme ¬´serveur¬ª n'est pas standard.  Il est plus habituel de d√©signer deux concepts √† la fois: un programme qui sert des connexions r√©seau et en m√™me temps une machine qui ex√©cute de tels programmes, mais lorsque nous parlons de la puissance de calcul de Google, la diff√©rence entre les deux est significative.  D√®s que vous vous habituerez √† notre interpr√©tation du mot ¬´serveur¬ª, vous comprendrez pourquoi il est important d'utiliser une terminologie sp√©cialis√©e non seulement directement chez Google, mais tout au long de ce livre. <br><br>  Dans la fig.  2.1 a d√©montr√© la configuration du centre de donn√©es Google. <br><br><ul><li>  Des dizaines de voitures sont plac√©es sur des supports. </li><li>  Les racks se tiennent en rang√©es. </li><li>  Une ou plusieurs lignes forment un cluster. </li><li>  Habituellement, dans la construction d'un centre de donn√©es (DPC) ou d'un centre de donn√©es, plusieurs clusters sont situ√©s. </li><li>  Plusieurs b√¢timents de centres de donn√©es proches les uns des autres composent le campus. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-3/no/2i/-3no2ios8vkdcddiea5ynqaaz1s.png" alt="image"></div><br>  √Ä l'int√©rieur de chaque centre de donn√©es, toutes les machines doivent pouvoir communiquer efficacement entre elles, nous avons donc cr√©√© un commutateur virtuel tr√®s rapide (commutateur) avec des dizaines de milliers de ports.  Cela a √©t√© possible en connectant des centaines de commutateurs d√©velopp√©s par Google dans une ¬´usine¬ª bas√©e sur la topologie du r√©seau Clos [Clos, 1953], appel√©e Jupiter [Singh et al., 2015].  √Ä sa configuration maximale, Jupiter prend en charge un d√©bit de 1,3 Pb / s entre les serveurs. <br><br>  Les centres de donn√©es sont connect√©s les uns aux autres √† l'aide de notre r√©seau dorsal mondial B4 [Jain et al., 2013].  Le B4 poss√®de une architecture r√©seau configurable par logiciel et utilise le protocole de communication ouvert OpenFlow.  B4 fournit une large bande passante √† un nombre limit√© de syst√®mes et utilise un contr√¥le de largeur de canal flexible pour maximiser sa valeur moyenne [Kumar et al., 2015]. <br><br><h3>  Logiciel syst√®me qui ¬´organise¬ª l'√©quipement </h3><br>  Le logiciel qui assure la gestion et l'administration de nos √©quipements doit √™tre capable de g√©rer d'√©normes syst√®mes.  Les pannes mat√©rielles sont l'un des principaux probl√®mes r√©solus √† l'aide de logiciels.  √âtant donn√© le grand nombre de composants mat√©riels dans un cluster, ils se produisent assez souvent.  Dans chaque cluster, des milliers de machines tombent g√©n√©ralement en panne en un an et des milliers de disques durs tombent en panne.  Si vous multipliez ce nombre par le nombre de clusters op√©rant dans le monde, le r√©sultat est stup√©fiant.  Par cons√©quent, nous voulons isoler les utilisateurs de ces probl√®mes, et les √©quipes impliqu√©es dans nos services ne veulent pas non plus √™tre distraites par des probl√®mes mat√©riels.  Chaque campus de centre de donn√©es dispose d'√©quipes charg√©es de prendre en charge l'√©quipement et l'infrastructure du centre de donn√©es. <br><br><h3>  Gestion des machines </h3><br>  Borg (figure 2.2) est un syst√®me de gestion de cluster distribu√© [Verma et al., 2015], similaire √† Apache Mesos.  Borg g√®re les travaux au niveau du cluster. <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tu/bv/iu/tubviu0qs-kyfob6r1kgodbttye.png" alt="image"></div>  Borg est responsable du lancement des jobs utilisateurs.  Ces t√¢ches peuvent √™tre soit des services ex√©cut√©s en permanence, soit des processus par lots comme MapReduce [Dean et Ghemawat, 2004].  Ils peuvent consister en plusieurs (parfois des milliers) de t√¢ches (t√¢ches) identiques - √† la fois pour des raisons de fiabilit√© et parce qu'un processus, en r√®gle g√©n√©rale, n'est pas en mesure de traiter tout le trafic du cluster.  Lorsque Borg d√©marre la t√¢che, il trouve les machines pour effectuer ses t√¢ches et leur commande de d√©marrer le programme serveur.  Borg surveille ensuite l'√©tat de ces t√¢ches.  Si la t√¢che ne fonctionne pas correctement, elle est d√©truite et red√©marre, √©ventuellement sur une autre machine. <br><br>  Comme les t√¢ches sont librement r√©parties entre les machines, nous ne pouvons pas utiliser les adresses IP et les num√©ros de port pour y acc√©der.  Ce probl√®me est r√©solu par un niveau d'abstraction suppl√©mentaire: lors du d√©marrage d'une t√¢che, Borg attribue un nom √† la t√¢che et un num√©ro (index) pour chaque t√¢che √† l'aide du service de nommage Borg (BNS).  Au lieu d'utiliser l'adresse IP et le num√©ro de port, d'autres processus s'associent aux t√¢ches Borg par leur nom BNS, qui convertit ensuite le BNS en adresse IP et num√©ro de port.  Par exemple, le chemin BNS peut √™tre une cha√Æne comme / bns / &lt;cluster&gt; / &lt;user&gt; / &lt;task_name&gt; / &lt;task_number&gt;, qui est ensuite traduite (il est d'usage de dire "autoris√©" sur les r√©seaux) au format &lt;adresse IP&gt;: &lt;port&gt; . <br><br>  Borg est √©galement responsable de l'allocation des ressources pour les missions.  Chaque t√¢che doit indiquer les ressources n√©cessaires pour la terminer (par exemple, trois c≈ìurs de processeur, 2 Go de RAM).  En utilisant la liste des exigences pour toutes les t√¢ches, Borg peut r√©partir de mani√®re optimale les t√¢ches entre les machines, en tenant √©galement compte des consid√©rations de tol√©rance aux pannes (par exemple, Borg ne d√©marrera pas toutes les t√¢ches d'une t√¢che sur le m√™me rack, car le basculement de ce rack sera un point critique en cas de panne) t√¢ches). <br><br>  Si une t√¢che essaie de r√©cup√©rer plus de ressources que ce qui √©tait demand√©, Borg la d√©truit puis red√©marre (car il est g√©n√©ralement pr√©f√©rable d'avoir une t√¢che qui se bloque et red√©marre parfois que qui ne red√©marre pas du tout). <br><br><h3>  Stockage </h3><br>  Pour un acc√®s plus rapide aux donn√©es, les t√¢ches peuvent utiliser le disque local des machines, mais nous avons plusieurs options pour organiser le stockage persistant dans le cluster (et m√™me les donn√©es stock√©es localement seront finalement d√©plac√©es vers le stockage du cluster).  Ils peuvent √™tre compar√©s √† Luster et aux syst√®mes de fichiers en cluster Hadoop Distributed File System (HDFS) avec une impl√©mentation open source. <br><br>  Le stockage offre aux utilisateurs la possibilit√© d'acc√©der facilement et de mani√®re fiable aux donn√©es disponibles pour le cluster.  Comme le montre la fig.  2.3, le r√©f√©rentiel comporte plusieurs couches. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/at/yf/7c/atyf7cemd0e2gczkjfgdnltw7zy.png" alt="image"></div><br>  1. La couche la plus basse est appel√©e D (√† partir du disque, bien que le niveau D utilise √† la fois des disques durs traditionnels et des lecteurs flash).  D est un serveur de fichiers qui s'ex√©cute sur pratiquement toutes les machines du cluster.  Cependant, les utilisateurs qui souhaitent acc√©der √† leurs donn√©es ne voudraient pas se souvenir sur quelle machine ils sont stock√©s, donc la couche suivante est connect√©e ici. <br><br>  2. Au-dessus de la couche D se trouve la couche Colossus, qui cr√©e un syst√®me de fichiers dans le cluster qui offre la s√©mantique habituelle du syst√®me de fichiers, ainsi que la r√©plication et le chiffrement.  Colossus est le successeur de GFS, le syst√®me de fichiers Google (Ghemawat et al., 2003). <br><br>  3. Ensuite, plusieurs services de type base de donn√©es sont cr√©√©s au-dessus du niveau Colossus. <br><br><ul><li>  Bigtable [Chang et al., 2006] est un syst√®me de base de donn√©es non relationnel (NoSQL) capable de travailler avec des bases de donn√©es d'un p√©taoctet.  Bigtable est une base de donn√©es ordonn√©e, distribu√©e, tol√©rante aux pannes, multidimensionnelle et index√©e par des cl√©s de ligne, de colonne et d'horodatage;  chaque valeur de base de donn√©es est un tableau d'octets arbitraire non interpr√©t√©.  Bigtable prend √©galement en charge la r√©plication entre les centres de donn√©es. </li><li>  Spanner [Corbett et al., 2012] propose une interface de type SQL pour les utilisateurs qui ont besoin d'int√©grit√© et de coh√©rence des donn√©es lorsqu'ils acc√®dent depuis n'importe o√π dans le monde. </li><li>  Plusieurs autres syst√®mes de base de donn√©es sont disponibles, comme Blobstore.  Ils ont tous leurs propres forces et faiblesses (voir chapitre 26). </li></ul><br><h3>  R√©seau </h3><br>  Google Networking est g√©r√© de plusieurs mani√®res.  Comme mentionn√© pr√©c√©demment, nous utilisons un r√©seau configurable par logiciel bas√© sur OpenFlow.  Au lieu de routeurs intelligents, nous utilisons des commutateurs idiots pas si chers en combinaison avec un contr√¥leur central (dupliqu√©), qui pr√©-calcule le meilleur itin√©raire sur le r√©seau.  Cela vous permet d'utiliser un √©quipement de commutation plus simple, le lib√©rant de la recherche d'itin√©raire fastidieuse. <br><br>  La bande passante r√©seau doit √™tre correctement allou√©e.  Comme Borg limite les ressources informatiques qu'une t√¢che peut utiliser, Bandwidth Enforcer (BwE) g√®re √©galement la bande passante disponible pour maximiser le d√©bit moyen.  L'optimisation de la bande passante n'est pas seulement li√©e au co√ªt: la gestion centralis√©e du trafic r√©sout un certain nombre de probl√®mes qui sont extr√™mement difficiles √† r√©soudre par une combinaison de routage distribu√© et de gestion classique du trafic (Kumar, 2015). <br><br>  Certains services ont des emplois ex√©cut√©s sur plusieurs clusters situ√©s dans diff√©rentes parties du monde.  Afin de r√©duire le temps de retard des syst√®mes distribu√©s √† l'√©chelle mondiale, nous souhaitons diriger les utilisateurs vers le centre de donn√©es le plus proche qui a la capacit√© appropri√©e pour cela.  Notre Global Software Load Balancer (GSLB) effectue un √©quilibrage de charge √† trois niveaux: <br><br><ul><li>  l'√©quilibrage de la charge g√©ographique pour les requ√™tes DNS (par exemple, sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">www.google.com</a> ), il est d√©crit au chapitre 19; </li><li>  √©quilibrage de la charge au niveau des services aux utilisateurs (par exemple, YouTube ou Google Maps); </li><li>  l'√©quilibrage de charge au niveau RPC (Remote Procedure Call), d√©crit au chapitre 20. </li></ul><br>  Les propri√©taires de services leur sp√©cifient des noms symboliques, une liste des adresses BNS du serveur et les performances disponibles sur chaque site (g√©n√©ralement, il est mesur√© en requ√™tes par seconde - requ√™tes par seconde, QPS).  Par la suite, le GSLB achemine le trafic vers les adresses BNS sp√©cifi√©es. <br><br><h3>  Autres logiciels syst√®me </h3><br><br>  Le logiciel du centre de donn√©es comporte d'autres composants importants. <br><br>  <b>Service de verrouillage</b> <br><br>  Le service de verrouillage Chubby [Burrows, 2006] fournit une API similaire au syst√®me de fichiers pour servir les verrous.  Chubby g√®re les verrous de tous les centres de donn√©es.  Il utilise le protocole Paxos pour acc√©der de mani√®re asynchrone au Consensus (voir chapitre 23). <br><br>  Chubby joue √©galement un r√¥le important dans le choix d'un assistant.  Si, pour certains services, cinq r√©pliques d'une t√¢che sont fournies dans le but d'augmenter la fiabilit√©, mais √† un moment donn√©, une seule d'entre elles fait le vrai travail, Chubby est utilis√© pour s√©lectionner cette r√©plique. <br>  Chubby est id√©al pour les donn√©es qui n√©cessitent une fiabilit√© de stockage.  Pour cette raison, BNS utilise Chubby pour stocker le rapport des chemins BNS aux adresses IP: paires de ports. <br><br>  <b>Surveillance et alertes</b> <br><br>  Nous voulons √™tre s√ªrs que tous les services fonctionnent correctement.  Par cons√©quent, nous lan√ßons de nombreuses instances du programme de surveillance Borgmon (voir chapitre 10).  Borgmon re√ßoit r√©guli√®rement des valeurs de r√©f√©rence de services surveill√©s.  Ces donn√©es peuvent √™tre utilis√©es imm√©diatement pour la notification ou stock√©es pour un traitement et une analyse ult√©rieurs, par exemple, pour cr√©er des graphiques.  Cette surveillance peut √™tre utilis√©e √† des fins telles que: <br><br><ul><li>  mettre en place des alertes pour les probl√®mes urgents; </li><li>  comparaison des comportements: la mise √† jour du logiciel a-t-elle acc√©l√©r√© le serveur; </li><li>  √©valuation de la nature des changements dans la consommation des ressources au fil du temps, ce qui est n√©cessaire pour la planification des capacit√©s. </li></ul><br><br><h3>  Notre infrastructure logicielle </h3><br>  L‚Äôarchitecture de notre logiciel est con√ßue de mani√®re √† pouvoir utiliser plus efficacement les ressources mat√©rielles du syst√®me.  Notre code entier est multi-thread, donc une t√¢che peut facilement utiliser plusieurs c≈ìurs.  Afin de prendre en charge les tableaux de bord, la surveillance et le d√©bogage, chaque serveur comprend une impl√©mentation de serveur HTTP en tant qu'interface via laquelle des informations de diagnostic et des statistiques pour une t√¢che sp√©cifique sont fournies. <br><br>  Tous les services Google ¬´communiquent¬ª √† l'aide de l'infrastructure d'appel de proc√©dure √† distance (RPC) appel√©e Stubby.  Il en existe une version open source, elle s'appelle gRPC (voir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">grpc.io</a> ).  Souvent, un appel RPC est effectu√© m√™me pour les routines du programme local.  Cela vous permet de r√©orienter le programme vers les appels d'un autre serveur pour obtenir une plus grande modularit√© ou √† mesure que le code du serveur d'origine se d√©veloppe.  GSLB peut effectuer un √©quilibrage de charge RPC de la m√™me mani√®re que pour les interfaces de service externes. <br><br>  Le serveur re√ßoit les demandes RPC de la partie frontale et envoie le RPC au backend.  En utilisant des termes traditionnels, le frontend est appel√© le client et le backend est appel√© le serveur. <br>  Les donn√©es sont transf√©r√©es vers et depuis RPC en utilisant le protocole de s√©rialisation - les soi-disant tampons de protocole, ou, bri√®vement, les protobufs.  Ce protocole est similaire √† Apache's Thrift et pr√©sente plusieurs avantages par rapport √† XML lorsqu'il s'agit de s√©rialiser des donn√©es structur√©es: il est plus simple, trois √† dix fois plus compact, 20 √† 100 fois plus rapide et plus unique. <br><br><h3>  Notre environnement de d√©veloppement </h3><br>  La vitesse de d√©veloppement des produits est tr√®s importante pour Google, nous avons donc cr√©√© un environnement sp√©cial qui tire le meilleur parti de notre infrastructure [Morgenthaler et al., 2012]. <br><br>  √Ä l'exception de quelques groupes dont les produits sont open source et utilisent donc leurs propres r√©f√©rentiels distincts (par exemple, Android et Chrome), les ing√©nieurs logiciels de Google travaillent dans un r√©f√©rentiel commun [Potvin, Levenberg, 2016].  Cette approche a plusieurs applications pratiques qui sont importantes pour notre processus de production. <br><br><ul><li>  Si un ing√©nieur rencontre un probl√®me dans un composant en dehors de son projet, il peut r√©soudre le probl√®me, envoyer les modifications propos√©es (¬´changelist¬ª - changelist, CL) au propri√©taire pour examen, puis impl√©menter les modifications apport√©es dans la branche principale du programme. </li><li>  Les modifications du code source dans le projet d'un ing√©nieur doivent √™tre prises en compte - effectuer un audit (examen).  Tous les logiciels passent cette √©tape avant leur adoption. </li></ul><br>  Lorsque le logiciel est en cours d'assemblage, la demande d'assemblage est envoy√©e √† des serveurs de centre de donn√©es sp√©cialis√©s.  M√™me la construction de grands projets est rapide car vous pouvez utiliser plusieurs serveurs pour la compilation parall√®le.  Cette infrastructure est √©galement utilis√©e pour les tests en continu.  Chaque fois qu'une nouvelle liste de modifications (CL) appara√Æt, des tests sont ex√©cut√©s sur tous les logiciels qui peuvent √™tre affect√©s directement ou indirectement par ces modifications.  Si le cadre d√©tecte que les modifications ont perturb√© le fonctionnement d'autres parties du syst√®me, il en avise le propri√©taire.  Certains projets utilisent le syst√®me push-on-green (¬´envoyer avec succ√®s¬ª), selon lequel la nouvelle version est automatiquement envoy√©e en exploitation commerciale apr√®s avoir r√©ussi les tests. <br><br><h3>  Shakespeare: exemple de service </h3><br>  Afin de d√©montrer comment Google d√©veloppe un service dans un environnement industriel, consid√©rons un exemple de service hypoth√©tique qui interagit avec les technologies de Google.  Supposons que nous voulons offrir un service qui vous permette de d√©terminer dans quelles ≈ìuvres de Shakespeare le mot que vous avez mentionn√© se produit. <br><br>  Nous pouvons diviser le syst√®me en deux parties. <br><br><ul><li>  Un composant de traitement par lots qui lit tous les textes de Shakespeare, cr√©e un index et l'√©crit dans Bigtable.  Cette t√¢che (plus pr√©cis√©ment, la t√¢che) est effectu√©e une fois ou, √©ventuellement, occasionnellement (apr√®s tout, un nouveau texte Shakespeare peut appara√Ætre!). </li><li>  Une application frontale qui traite les demandes des utilisateurs finaux.  Cette t√¢che est toujours en cours d'ex√©cution, car √† tout moment, un utilisateur de n'importe quel fuseau horaire peut vouloir rechercher dans les livres de Shakespeare. </li></ul><br>  Le composant de traitement par lots sera le service MapReduce, dont le travail est divis√© en trois phases. <br><br>  1. Dans la phase de cartographie, les textes de Shakespeare sont lus et divis√©s en mots s√©par√©s.  Cette partie du travail sera termin√©e plus rapidement si plusieurs processus de travail (t√¢ches) sont lanc√©s en parall√®le. <br><br>  2. Dans la phase de lecture al√©atoire, les entr√©es sont tri√©es par mot. <br><br>  3. Dans la phase R√©duire, des tuples du formulaire (mot, list_products) sont cr√©√©s. <br><br>  Chaque tuple est √©crit sous forme de cha√Æne dans Bigtable, la cl√© est le mot. <br><br><h3>  Cycle de vie de la demande </h3><br>  Dans la fig.  2.4 montre comment la demande de l'utilisateur est servie.  Tout d'abord, l'utilisateur clique sur le lien shakespeare.google.com dans le navigateur.  Pour obtenir l'adresse IP appropri√©e, l'appareil de l'utilisateur traduit ("r√©sout") l'adresse √† l'aide du serveur DNS (1).  La requ√™te DNS finit par se retrouver sur le serveur DNS de Google, qui interagit avec le GSLB.  En suivant la charge de trafic de tous les serveurs frontaux par r√©gion, GSLB choisit quelle adresse IP de quel serveur renvoyer √† l'utilisateur. <br><br>  Le navigateur se connecte au serveur HTTP √† l'adresse sp√©cifi√©e.  Ce serveur (appel√© Google Frontend ou GFE) est un serveur proxy ¬´invers√©¬ª situ√© √† l'autre extr√©mit√© de la connexion TCP du client (2).  GFE recherche le service requis (par exemple, il peut s'agir d'un service de recherche, de cartes ou, dans notre cas, du service Shakespeare).  En acc√©dant de mani√®re r√©p√©t√©e au GSLB, le serveur trouve un serveur frontal Shakespeare disponible et y acc√®de via un appel de proc√©dure √† distance (RPC), transmettant une requ√™te HTTP re√ßue de l'utilisateur (3). <br><br>  Le serveur Shakespeare analyse la requ√™te HTTP et cr√©e un ¬´tampon de protocole¬ª (protobuf) contenant les mots √† rechercher.  Maintenant, le serveur frontal Shakespeare doit contacter le serveur principal Shakespeare: le premier contacte le GSLB pour obtenir l'adresse BNS d'une instance appropri√©e et d√©charg√©e de la seconde (4).  Ensuite, le serveur backend de Shakespeare contacte le serveur Bigtable pour recevoir les donn√©es demand√©es (5). <br><br>  Le r√©sultat est √©crit dans le protobuf de r√©ponse et renvoy√© au serveur principal de Shakespeare.  Le backend transmet protobuf avec le r√©sultat du service au serveur frontal Shakespeare, qui cr√©e un document HTML et le renvoie en r√©ponse √† l'utilisateur. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6b/nd/yo/6bndyo9cfcxu5k5sx4lieg5bfw8.png" alt="image"></div><br>  Toute cette cha√Æne d'√©v√©nements se d√©roule en un clin d'≈ìil - en quelques centaines de millisecondes seulement!  √âtant donn√© que de nombreux composants sont impliqu√©s, il existe de nombreux endroits o√π une erreur potentielle peut se produire;  en particulier, une d√©faillance du GSLB peut d√©sorganiser tout le travail et conduire √† l'effondrement.   Google,   ,                 (   ),     ,    .   ,      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">www.google.com</a>  ,     . <br><br><h3>     </h3><br>   ,   -    100    (QPS).       ,       3470 QPS,       35 .    ,      37 ,  N + 2. <br><br><ul><li>        ,     36 . </li><li>         , -    35  ‚Äî  ,      . </li></ul><br>          : 1430 QPS    , 290 ‚Äî   , 1400 ‚Äî    350 ‚Äî    .      -   ,     :  ,  ,   .   N + 2   ,  17   , 16 ‚Äî     ‚Äî  .   , ,      ( ),    ‚Äî  N + 2  N + 1.                  :  GSLB    -       ,    20 % ,      .            2‚Äì3 . <br><br>  -      Bigtable,       .  -     Bigtable,   ,      ,    Bigtable   .        ,   Bigtable  ,        .   Bigtable           ,     ,         . <br><br> ,          .       ,         ,    . <br><br>  ¬ªPlus d'informations sur le livre sont disponibles sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le site Web de l'√©diteur</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Contenu</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Extrait</a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Coupon de 20% pour Habrozavitel - </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Site Reliability Engineering</font></font></b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr420139/">https://habr.com/ru/post/fr420139/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr420125/index.html">Automatisation de la finance: les employ√©s des banques peuvent se retrouver sans travail en raison de robots</a></li>
<li><a href="../fr420129/index.html">Mod√®les de coroutine Asyncio: l'ext√©rieur attend</a></li>
<li><a href="../fr420131/index.html">M√©thode d'exploration de Bitcoin probabiliste</a></li>
<li><a href="../fr420133/index.html">Mod√©lisation des syst√®mes dynamiques: comment se d√©place la lune?</a></li>
<li><a href="../fr420135/index.html">C'est aussi Toshiba: des produits inattendus de la soci√©t√© japonaise</a></li>
<li><a href="../fr420141/index.html">Depuis le SGBD MPP charg√© - Data Lake dynamique avec des outils d'analyse: partagez les d√©tails de la cr√©ation</a></li>
<li><a href="../fr420143/index.html">Performances de Kotlin sur Android</a></li>
<li><a href="../fr420145/index.html">Comment se passe la journ√©e de travail des membres du PC AppsConf</a></li>
<li><a href="../fr420147/index.html">OpenSource sur Clojure</a></li>
<li><a href="../fr420151/index.html">Plus simple qu'il n'y para√Æt. Chapitre 12</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>