<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤞🏽 👩🏽‍🚒 👨🏻‍🏫 KI, praktischer Kurs. Übersicht über neuronale Netze zur Bildklassifizierung 🎩 ⛴️ 🔟</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dieser Artikel bietet einen zugänglichen theoretischen Überblick über Faltungs-Neuronale Netze (CNN) und erläutert deren Anwendung auf das Bildklassif...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>KI, praktischer Kurs. Übersicht über neuronale Netze zur Bildklassifizierung</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/intel/blog/415811/">  Dieser Artikel bietet einen zugänglichen theoretischen Überblick über Faltungs-Neuronale Netze (CNN) und erläutert deren Anwendung auf das Bildklassifizierungsproblem. <br><br><img src="https://habrastorage.org/webt/_d/ve/hi/_dvehi4kbgauxndfn56s7-tmtku.jpeg"><br><a name="habracut"></a><br><h2>  <font color="#0071c5">Gemeinsamer Ansatz: Kein tiefes Lernen</font> </h2><br>  Der Begriff "Bildverarbeitung" bezieht sich auf eine breite Klasse von Aufgaben, für die die Eingabedaten Bilder sind, und die Ausgabe kann entweder Bilder oder Sätze von zugehörigen charakteristischen Merkmalen sein.  Es gibt viele Optionen: Klassifizierung, Segmentierung, Annotation, Objekterkennung usw. In diesem Artikel untersuchen wir die Klassifizierung von Bildern, nicht nur, weil dies die einfachste Aufgabe ist, sondern auch, weil sie vielen anderen Aufgaben zugrunde liegt. <br><br>  Der allgemeine Ansatz zur Klassifizierung von Bildern besteht aus den folgenden zwei Schritten: <br><br><ol><li>  Erzeugung wesentlicher Merkmale des Bildes. </li><li>  Klassifizierung eines Bildes anhand seiner Attribute. </li></ol><br>  Die übliche Abfolge von Vorgängen verwendet einfache Modelle wie MultiLayer Perceptron (MLP), Support Vector Machine (SVM), die Methode der nächsten Nachbarn und die logistische Regression zusätzlich zu manuell erstellten Funktionen.  Attribute werden unter Verwendung verschiedener Transformationen (z. B. Graustufen- und Schwellenwerterkennung) und Deskriptoren erzeugt, z. B. Histogramm orientierter Gradienten ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">HOG</a> ) oder skaleninvarianter Merkmalstransformationstransformationen ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SIFT</a> ) und usw. <br><br>  Die Hauptbeschränkung allgemein anerkannter Methoden besteht in der Teilnahme eines Experten, der einen Satz und eine Abfolge von Schritten zum Generieren von Merkmalen auswählt. <br><br>  Im Laufe der Zeit wurde festgestellt, dass die meisten Techniken zum Erzeugen von Merkmalen mithilfe von Kerneln (Filtern) verallgemeinert werden können - kleinen Matrizen (normalerweise 5 × 5 groß), die Windungen der Originalbilder sind.  Die Faltung kann als ein sequentieller zweistufiger Prozess betrachtet werden: <br><br><ol><li> Führen Sie denselben festen Kern über das gesamte Quellbild. </li><li>  Berechnen Sie bei jedem Schritt das Skalarprodukt des Kernels und das Originalbild an der aktuellen Position des Kernels. </li></ol><br>  Das Ergebnis der Faltung des Bildes und des Kernels wird als Feature-Map bezeichnet. <br>  Eine mathematisch strengere Erklärung findet sich im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">entsprechenden Kapitel des</a> kürzlich veröffentlichten Buches Deep Learning von I. Goodfellow, I. Benjio und A. Courville. <br><br><img src="https://habrastorage.org/webt/kw/lm/rd/kwlmrdg1y8wsniz94riol_8fiie.png"><br>  <i>Der Prozess der Faltung des Kerns (dunkelgrün) mit dem Originalbild (grün), wodurch eine Merkmalskarte erhalten wird (gelb).</i> <br><br>  Ein einfaches Beispiel für eine Transformation, die mit Filtern durchgeführt werden kann, ist das Verwischen eines Bildes.  Nehmen Sie einen Filter, der aus allen Einheiten besteht.  Es berechnet den vom Filter bestimmten Durchschnitt der Nachbarschaft.  In diesem Fall ist die Nachbarschaft ein quadratischer Abschnitt, aber sie kann kreuzförmig sein oder was auch immer.  Die Mittelung führt zum Verlust von Informationen über die genaue Position von Objekten, wodurch das gesamte Bild unscharf wird.  Eine ähnliche intuitive Erklärung kann für jeden manuell erstellten Filter gegeben werden. <br><br><img src="https://habrastorage.org/webt/5t/ud/g7/5tudg7ebng4ocb6jdc1-1alsqpo.png"><br>  <i>Das Ergebnis der Faltung des Bildes des Gebäudes der Harvard University mit drei verschiedenen Kernen.</i> <br><br><h2>  <font color="#0071c5">Faltungsneurale Netze</font> </h2><br>  Der Faltungsansatz zur Bildklassifizierung weist eine Reihe wesentlicher Nachteile auf: <br><br><ul><li>  Ein mehrstufiger Prozess anstelle einer End-to-End-Sequenz. </li><li>  Filter sind ein großartiges Verallgemeinerungswerkzeug, aber sie sind feste Matrizen.  Wie wählt man Gewichte in Filtern? </li></ul><br>  Glücklicherweise wurden lernbare Filter erfunden, die das Grundprinzip von CNN darstellen.  Das Prinzip ist einfach: Wir werden die Filter trainieren, die auf die Beschreibung von Bildern angewendet werden, um ihre Aufgabe bestmöglich zu erfüllen. <br><br>  CNN hat keinen Erfinder, aber einer der ersten Fälle ihrer Anwendung ist LeNet-5 * in der Arbeit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">„</a> Gradientenbasiertes Lernen für die Dokumentenerkennung“ von I. LeCun und anderen Autoren. <br><br>  CNN tötet zwei Fliegen mit einer Klappe: Es ist keine vorläufige Definition von Filtern erforderlich, und der Lernprozess wird von Ende zu Ende.  Eine typische CNN-Architektur besteht aus folgenden Teilen: <br><br><ul><li>  Faltungsschichten </li><li>  Unterabtastungsebenen </li><li>  Dichte (vollständig verbundene) Schichten </li></ul><br>  Lassen Sie uns jeden Teil genauer betrachten. <br><br><h3>  <font color="#0071c5">Faltungsschichten</font> </h3><br>  Die Faltungsschicht ist das Hauptstrukturelement von CNN.  Die Faltungsschicht weist eine Reihe von Eigenschaften auf: <br>  <i>Lokale (spärliche) Konnektivität</i> .  In dichten Schichten ist jedes Neuron mit jedem Neuron der vorherigen Schicht verbunden (daher wurden sie als dicht bezeichnet).  In der Faltungsschicht ist jedes Neuron nur mit einem kleinen Teil der Neuronen der vorherigen Schicht verbunden. <br><br><img src="https://habrastorage.org/webt/nl/1a/6t/nl1a6t8bvbzna0rv_y3qjv_gsua.png"><br>  <i>Ein Beispiel für ein eindimensionales neuronales Netzwerk.</i>  <i>(links) Verbindung von Neuronen in einem typischen dichten Netzwerk, (rechts) Charakterisierung der lokalen Konnektivität, die der Faltungsschicht innewohnt.</i>  <i>Bilder von I. Goodfellow und anderen von Deep Learning</i> <br><br>  <i>Die Größe des Bereichs, mit dem das Neuron verbunden</i> ist, wird als Größe des Filters bezeichnet (die Länge des Filters bei eindimensionalen Daten, z. B. Zeitreihen, oder die Breite / Höhe bei zweidimensionalen Daten, z. B. Bildern).  In der Abbildung rechts beträgt die Filtergröße 3. Die <i>Gewichte, mit denen die Verbindung hergestellt</i> wird, werden als Filter bezeichnet (ein Vektor bei eindimensionalen Daten und eine Matrix für zweidimensionale Daten).  <i>Der Schritt ist die Entfernung, um die sich der Filter über die Daten bewegt</i> (in der Abbildung rechts ist der Schritt 1).  Die Idee der lokalen Konnektivität ist nichts anderes als ein Kernel, der einen Schritt bewegt.  Jedes Neuron auf Faltungsebene repräsentiert und implementiert eine bestimmte Position des Kerns, der entlang des Originalbildes gleitet. <br><br><img src="https://habrastorage.org/webt/nu/we/gf/nuwegftvmoigtcdz2mlt0xwmm5g.png"><br>  <i>Zwei benachbarte eindimensionale Faltungsschichten</i> <br><br>  Eine weitere wichtige Eigenschaft ist die sogenannte <i>Suszeptibilitätszone</i> .  Es spiegelt die Anzahl der Positionen des ursprünglichen Signals wider, die das aktuelle Neuron „sehen“ kann.  Beispielsweise ist die in der Figur gezeigte Suszeptibilitätszone der ersten Netzwerkschicht gleich der Größe von Filter 3, da jedes Neuron nur mit drei Neuronen des ursprünglichen Signals verbunden ist.  Auf der zweiten Schicht beträgt die Suszeptibilitätszone jedoch bereits 5, da das Neuron der zweiten Schicht drei Neuronen der ersten Schicht aggregiert, von denen jede eine Suszeptibilitätszone 3 aufweist. Mit zunehmender Tiefe wächst die Suszeptibilitätszone linear. <br><br>  <i>Gemeinsame Parameter</i> .  Denken Sie daran, dass bei der klassischen Bildverarbeitung derselbe Kern über das gesamte Bild glitt.  Die gleiche Idee gilt hier.  Wir legen nur die Größe des Filter der Gewichte für eine Schicht fest und wenden diese Gewichte auf alle Neuronen in der Schicht an.  Dies entspricht dem Verschieben desselben Kerns im gesamten Bild.  Es kann sich aber die Frage stellen: Wie können wir mit so wenigen Parametern etwas lernen? <br><br><img src="https://habrastorage.org/webt/ue/qo/gi/ueqogixaqstaotlmnlfnjfn3dtc.png"><br>  <i>Dunkle Pfeile stehen für die gleichen Gewichte.</i>  <i>(links) Regulärer MLP, bei dem jeder Gewichtungsfaktor ein separater Parameter ist. (rechts) Ein Beispiel für die Parametertrennung, bei der mehrere Gewichtungsfaktoren denselben Trainingsparameter angeben</i> <br><br>  <i>Raumstruktur</i> .  Die Antwort auf diese Frage ist einfach: Wir werden mehrere Filter in einer Schicht trainieren!  Sie sind parallel zueinander angeordnet und bilden so eine neue Dimension. <br><br>  Wir machen eine kurze Pause und erläutern die Idee am Beispiel eines zweidimensionalen RGB-Bildes von 227 × 227. Beachten Sie, dass es sich hier um ein dreikanaliges Eingabebild handelt, was im Wesentlichen bedeutet, dass wir drei Eingabebilder oder dreidimensionale Eingabedaten haben. <br><br><img src="https://habrastorage.org/webt/ol/9r/ty/ol9rtyiz5s9btzfnldhf6bo_0wi.png"><br>  <i>Die räumliche Struktur des Eingabebildes</i> <br><br>  Wir werden die Abmessungen der Kanäle als Bildtiefe betrachten (beachten Sie, dass dies nicht der Tiefe der neuronalen Netze entspricht, die der Anzahl der Netzwerkschichten entspricht).  Die Frage ist, wie der Kernel für diesen Fall bestimmt wird. <br><br><img src="https://habrastorage.org/webt/l6/pt/be/l6ptberff4a-rbz81bq-uxa-10w.png"><br>  <i>Ein Beispiel für einen zweidimensionalen Kern, bei dem es sich im Wesentlichen um eine dreidimensionale Matrix mit einer zusätzlichen Tiefenmessung handelt.</i>  <i>Dieser Filter ergibt eine Faltung mit dem Bild;</i>  <i>Das heißt, es gleitet über das Bild im Raum und berechnet skalare Produkte</i> <br><br>  Die Antwort ist einfach, aber immer noch nicht offensichtlich: Wir werden den Kernel auch dreidimensional machen.  Die ersten beiden Dimensionen bleiben gleich (Kernbreite und -höhe), und die dritte Dimension entspricht immer der Tiefe der Eingabedaten. <br><br><img src="https://habrastorage.org/webt/yu/e3/xz/yue3xziqupgpwtmqvtit8ik5hos.png"><br>  <i>Ein Beispiel für einen räumlichen Faltungsschritt.</i>  <i>Das Ergebnis des Skalarprodukts des Filters und eines kleinen Teils des Bildes 5 × 5 × 3 (d. H. 5 × 5 × 5 + 1 = 76, die Dimension des Skalarprodukts + Verschiebung) ist eine Zahl</i> <br><br>  In diesem Fall wird der gesamte 5 × 5 × 3-Abschnitt des Originalbilds in eine Zahl umgewandelt, und das dreidimensionale Bild selbst wird in <i>eine Merkmalskarte</i> ( <i>Aktivierungskarte</i> ) umgewandelt.  Eine Feature-Map ist eine Reihe von Neuronen, von denen jede ihre eigene Funktion berechnet, wobei zwei oben diskutierte Grundprinzipien berücksichtigt werden: <i>lokale Konnektivität</i> (jedes Neuron ist nur einem kleinen Teil der Eingabedaten zugeordnet) und <i>Trennung von Parametern</i> (alle Neuronen verwenden denselben Filter).  Im Idealfall ist diese Feature-Map dieselbe wie die, die bereits im Beispiel eines allgemein akzeptierten Netzwerks angetroffen wurde. Sie speichert die Ergebnisse der Faltung des Eingabebilds und des Filters. <br><br><img src="https://habrastorage.org/webt/iw/4w/qr/iw4wqr77vslpfjqblrgtjlxyxkw.png"><br>  <i>Merkmalskarte als Ergebnis der Faltung des Kerns mit allen räumlichen Positionen</i> <br><br>  Beachten Sie, dass die Tiefe der Feature-Map 1 beträgt, da wir nur einen Filter verwendet haben.  Aber nichts hindert uns daran, mehr Filter zu verwenden.  Beispiel: 6. Alle interagieren mit denselben Eingabedaten und arbeiten unabhängig voneinander.  Gehen wir noch einen Schritt weiter und kombinieren diese Funktionskarten.  Ihre räumlichen Abmessungen sind gleich, da die Abmessungen der Filter gleich sind.  Somit können die zusammen gesammelten Merkmalskarten als eine neue dreidimensionale Matrix betrachtet werden, deren Tiefenabmessung durch Merkmalskarten aus verschiedenen Kernen dargestellt wird.  In diesem Sinne sind die RGB-Kanäle des Eingabebildes nichts anderes als die drei ursprünglichen Feature-Maps. <br><br><img src="https://habrastorage.org/webt/c8/f1/23/c8f123tp1nnbklt5oimm4gkmwj4.png"><br>  <i>Die parallele Anwendung mehrerer Filter auf das Eingabebild und den daraus resultierenden Satz von Aktivierungskarten</i> <br><br>  Ein solches Verständnis der Feature-Maps und ihrer Kombination ist sehr wichtig, da wir, nachdem wir dies erkannt haben, die Netzwerkarchitektur erweitern und Faltungsschichten übereinander installieren können, wodurch die Suszeptibilitätszone erhöht und unser Klassifikator angereichert wird. <br><br><img src="https://habrastorage.org/webt/7_/3g/hp/7_3ghputtiaz-2l-hbbs7dagmvi.png"><br>  <i>Übereinander installierte Faltungsschichten.</i>  <i>In jeder Schicht können die Größe der Filter und ihre Anzahl variieren</i> <br><br>  Jetzt verstehen wir, was ein Faltungsnetzwerk ist.  Das Hauptziel dieser Schichten ist das gleiche wie beim allgemein akzeptierten Ansatz - signifikante Zeichen des Bildes zu erkennen.  Und wenn diese Zeichen in der ersten Schicht sehr einfach sein können (das Vorhandensein vertikaler / horizontaler Linien), erhöht die Tiefe des Netzwerks den Grad ihrer Abstraktion (das Vorhandensein eines Hundes / einer Katze / einer Person). <br><br><h3>  <font color="#0071c5">Unterabtastungsebenen</font> </h3><br>  Faltungsschichten sind der Hauptbaustein von CNN.  Aber es gibt noch einen anderen wichtigen und häufig verwendeten Teil - dies sind Unterprobenschichten.  Bei der herkömmlichen Bildverarbeitung gibt es kein direktes Analogon, aber eine Unterprobe kann als eine andere Art von Kernel betrachtet werden.  Was ist das <br><br><img src="https://habrastorage.org/webt/n4/fm/df/n4fmdf4o-i1pa7qs4w4oj7wmzgy.png"><br>  <i>Beispiele für Unterabtastung.</i>  <i>(links) Wie ein Teilmuster die räumlichen (aber nicht die Kanal!) Größen von Datenarrays ändert. (rechts) Ein grundlegendes Schema für die Funktionsweise eines Teilmusters</i> <br><br>  Eine Unterabtastung filtert einen Teil der Nachbarschaft jedes Pixels der Eingabedaten mit einer bestimmten Aggregationsfunktion, beispielsweise Maximum, Durchschnitt usw. Die Unterabtastung entspricht im Wesentlichen der Faltung, aber die Pixelkombinationsfunktion ist nicht auf das Skalarprodukt beschränkt.  Ein weiterer wichtiger Unterschied besteht darin, dass die Unterabtastung nur in der räumlichen Dimension funktioniert.  Ein charakteristisches Merkmal der Unterabtastschicht ist, dass der <i>Abstand normalerweise gleich der Größe des Filters ist</i> (der typische Wert ist 2). <br><br>  Eine Teilstichprobe hat drei Hauptziele: <br><br><ul><li>  Verringerung der räumlichen Dimension oder Unterabtastung.  Dies geschieht, um die Anzahl der Parameter zu verringern. </li><li>  Das Wachstum der Suszeptibilitätszone.  Aufgrund der Teilprobenneuronen in den nachfolgenden Schichten werden mehr Schritte des Eingangssignals akkumuliert </li><li>  Translationale Invarianz zu kleinen Heterogenitäten in der Position der Muster im Eingangssignal.  Durch Berechnung der Aggregationsstatistik kleiner Nachbarschaften des Eingangssignals kann eine Teilstichprobe kleine räumliche Verschiebungen darin ignorieren. </li></ul><br><h3>  <font color="#0071c5">Dicke Schichten</font> </h3><br>  Faltungsebenen und Unterabtastebenen dienen demselben Zweck - sie erzeugen Bildattribute.  Der letzte Schritt besteht darin, das Eingabebild anhand der erkannten Merkmale zu klassifizieren.  Bei CNN tun dies dichte Schichten über dem Netzwerk.  Dieser Teil des Netzwerks wird als <i>Klassifizierung bezeichnet</i> .  Es kann mehrere Ebenen mit vollständiger Konnektivität übereinander enthalten, endet jedoch normalerweise mit einer <i>Softmax-</i> Klassenebene, die durch eine logistische Aktivierungsfunktion mit mehreren Variablen aktiviert wird, bei der die Anzahl der Blöcke der Anzahl der Klassen entspricht.  Am Ausgang dieser Schicht befindet sich die Wahrscheinlichkeitsverteilung nach Klasse für das Eingabeobjekt.  Jetzt kann das Bild klassifiziert werden, indem die wahrscheinlichste Klasse ausgewählt wird. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de415811/">https://habr.com/ru/post/de415811/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de415795/index.html">Schreiben einer Snapchat-Benutzeroberfläche auf Swift</a></li>
<li><a href="../de415797/index.html">Reguläre Ausdrücke + logische Programmierung. Was ist das Ergebnis?</a></li>
<li><a href="../de415801/index.html">Google: Unsere "Telefon" -KI ist nicht gut genug, um gefährlich zu sein</a></li>
<li><a href="../de415805/index.html">Modifikation des Doorhan GSM Barrier Moduls zur Internetsteuerung</a></li>
<li><a href="../de415809/index.html">Verwendung von Soja, Requirejs und Backbone-Js in Plugins für Atlassian Jira</a></li>
<li><a href="../de415813/index.html">Einige Hinweise zum aktuellen Status von Cloud Gaming</a></li>
<li><a href="../de415815/index.html">An der Spitze der Wissenschaft: eine Analyse der Artikel von arxiv.org</a></li>
<li><a href="../de415817/index.html">Wir übertakten das Backup. Yandex Vortrag</a></li>
<li><a href="../de415819/index.html">Bericht des Club of Rome 2018, Kapitel 3.16: Globale Regierung</a></li>
<li><a href="../de415821/index.html">Der Weg, um ein "intelligentes" Haus mit einer möglichst breiten elektrischen Steuerung zu organisieren</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>