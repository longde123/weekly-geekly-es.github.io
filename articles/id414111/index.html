<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🏴󠁧󠁢󠁳󠁣󠁴󠁿 🛏️ ⛰️ Debugging dan troubleshooting di Replikasi Streaming PostgreSQL 🤸🏻 👨‍🌾 💵</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Replikasi aliran, yang muncul pada 2010, telah menjadi salah satu fitur terobosan PostgreSQL dan saat ini, hampir tidak ada instalasi yang dapat dilak...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Debugging dan troubleshooting di Replikasi Streaming PostgreSQL</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/414111/">  Replikasi aliran, yang muncul pada 2010, telah menjadi salah satu fitur terobosan PostgreSQL dan saat ini, hampir tidak ada instalasi yang dapat dilakukan tanpa menggunakan replikasi streaming.  Ia dapat diandalkan, mudah dikonfigurasikan, tidak memerlukan sumber daya.  Namun, untuk semua kualitas positifnya, selama operasi berbagai masalah dan situasi yang tidak menyenangkan dapat muncul. <br><br>  <strong>Alexey Lesovsky</strong> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">@lesovsky</a> ) di Highload ++ 2017 memberi tahu cara <strong>mendiagnosis berbagai jenis masalah</strong> menggunakan alat internal dan pihak ketiga <strong>dan cara memperbaikinya</strong> .  Di bawah pemotongan, decoding laporan ini, dibangun berdasarkan prinsip spiral: pertama, kami membuat daftar semua alat diagnostik yang mungkin, kemudian beralih ke daftar masalah umum dan mendiagnosisnya, kemudian melihat tindakan darurat apa yang dapat diambil, dan akhirnya bagaimana menangani masalah secara radikal. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/on2yVvKejwc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <strong>Tentang pembicara</strong> : Alexei Lesovsky, administrator database di Data Egret.  Salah satu topik favorit Alexey di PostgreSQL adalah streaming replikasi dan bekerja dengan statistik, sehingga laporan di Highload ++ 2017 dikhususkan untuk bagaimana menemukan masalah menggunakan statistik dan metode apa yang digunakan untuk menyelesaikannya. <br><br><h2>  Rencanakan <br></h2><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Sedikit teori, atau bagaimana replikasi bekerja di PostgreSQL</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Alat pemecahan masalah atau apa yang PostgreSQL dan komunitas miliki</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kasus pemecahan masalah:</a> <br><ul><li>  masalah: gejala dan diagnosis mereka </li><li>  keputusan </li><li>  langkah-langkah yang harus diambil agar masalah ini tidak muncul. </li></ul></li></ol><br>  <strong>Kenapa semua ini?</strong>  Artikel ini akan membantu Anda lebih memahami replikasi streaming, mempelajari cara menemukan dan memperbaiki masalah dengan cepat untuk mengurangi waktu reaksi terhadap insiden yang tidak menyenangkan. <br><a name="habracut"></a><a name="replicationSql"></a><br><h2>  Sedikit teori <br></h2><br>  PostgreSQL memiliki entitas seperti Write-Ahead Log (XLOG), log transaksi.  <em>Hampir</em> semua perubahan yang terjadi dengan data dan metadata di dalam database dicatat dalam log ini.  Jika ada kecelakaan yang tiba-tiba terjadi, PostgreSQL mulai, membaca log transaksi dan mengembalikan perubahan yang direkam ke data.  Ini memastikan keandalan - salah satu properti terpenting dari DBMS dan PostgreSQL juga. <br><br>  Log transaksi dapat diisi dengan dua cara: <br><br><ol><li>  Secara default, ketika backend membuat beberapa perubahan dalam database (INSERT, UPDATE, DELETE, dll.), Semua perubahan dicatat dalam log transaksi secara <strong>serempak</strong> : <br><ul><li>  Klien mengirim perintah COMMIT untuk mengonfirmasi data. <br></li><li>  Data dicatat dalam log transaksi. <br></li><li>  Setelah fiksasi terjadi, kontrol diberikan ke backend, dan itu dapat terus menerima perintah dari klien. <br></li></ul></li><li>  Pilihan kedua adalah <strong>penulisan asinkron</strong> ke log transaksi, ketika proses penulis WAL khusus menulis perubahan pada log transaksi dengan interval waktu tertentu.  Karena itu, peningkatan kinerja backend tercapai, karena tidak perlu menunggu sampai perintah COMMIT selesai. <br></li></ol><br>  Yang paling penting, replikasi streaming didasarkan pada log transaksi ini.  Kami memiliki beberapa anggota replikasi streaming: <br><br><ul><li>  kuasai tempat semua perubahan terjadi; <br></li><li>  beberapa replika yang menerima log transaksi dari master dan mereproduksi semua perubahan ini pada data lokal mereka.  Ini replikasi streaming. <br></li></ul><br>  Patut diingat bahwa semua log transaksi ini disimpan dalam direktori pg_xlog dalam $ DATADIR - direktori dengan file data DBMS utama.  Dalam versi 10 PostgreSQL, direktori ini diganti namanya menjadi pg_wal /, karena ini tidak biasa bagi pg_xlog / untuk mengambil banyak ruang, dan pengembang atau administrator yang secara tidak sadar mengacaukannya dengan log, menghapusnya dengan sembarangan, dan semuanya menjadi buruk. <br><br>  PostgreSQL memiliki beberapa layanan latar belakang yang terlibat dalam replikasi streaming.  Mari kita melihatnya dari sudut pandang sistem operasi. <br><br><ul><li>  Dari sisi master - proses Pengirim WAL.  Ini adalah proses yang mengirim log transaksi ke replika, setiap replika akan memiliki Pengirim WAL sendiri. <br></li><li>  Replika, pada gilirannya, menjalankan proses Penerima WAL, yang menerima log transaksi melalui koneksi jaringan dari Pengirim WAL dan meneruskannya ke proses Startup. <br></li><li>  Proses startup membaca log dan mereproduksi di direktori data semua perubahan yang dicatat dalam log transaksi. <br></li></ul><br><img src="https://habrastorage.org/webt/ek/g2/rd/ekg2rdtwkvfjw8hlpj0rhhzhogu.jpeg"><br>  Secara skematis, tampilannya seperti ini: <br><br><ul><li>  Perubahan ditulis ke WAL Buffer, yang kemudian akan ditulis ke log transaksi; <br></li><li>  Log disimpan dalam direktori pg_wal /; <br></li><li>  WAL Pengirim membaca log transaksi dari repositori dan mengirimkannya melalui jaringan; <br></li><li>  Penerima WAL menerima dan menyimpan dalam Penyimpanannya - di pg_wal lokal; <br></li><li>  Proses Startup membaca semua yang diterima dan direproduksi. <br></li></ul><br>  Skema ini sederhana.  Replikasi aliran bekerja dengan cukup andal dan telah dieksploitasi dengan sangat baik selama bertahun-tahun. <br><a name="tools"></a><br><h2>  Alat pemecahan masalah <br></h2><br>  Mari kita lihat alat dan utilitas apa yang ditawarkan komunitas dan PostgreSQL untuk menyelidiki masalah yang dihadapi dengan replikasi streaming. <br><br><h3>  Alat pihak ketiga <br></h3><br>  Mari kita mulai dengan alat pihak ketiga.  Utilitas ini adalah <strong>rencana yang</strong> agak <strong>universal</strong> , mereka dapat digunakan tidak hanya untuk menyelidiki insiden yang terkait dengan replikasi streaming.  Ini umumnya <strong>utilitas dari administrator sistem apa pun</strong> . <br><br><ul><li>  <strong>atas</strong> dari paket procps.  Sebagai pengganti top, Anda dapat menggunakan utilitas seperti di atas, htop dan sejenisnya.  Mereka menawarkan fungsionalitas serupa. </li></ul><br>  Dengan bantuan top kita melihat: pemanfaatan prosesor (CPU), beban rata-rata (rata-rata beban) dan penggunaan memori dan ruang swap. <br><br><ul><li>  iostat dari sysstat dan iotop.  Utilitas ini menunjukkan pemanfaatan perangkat disk dan I / O dibuat oleh proses dalam sistem operasi. </li></ul><br>  Dengan bantuan iostat kita melihat: pemanfaatan penyimpanan, berapa iops saat ini, apa throughput pada perangkat, apa keterlambatan saat memproses permintaan untuk I / O (latensi).  Informasi yang agak terperinci ini diambil dari sistem file procfs dan diberikan kepada pengguna dalam bentuk visual. <br><br><ul><li>  nicstat adalah analog dari iostat, hanya untuk antarmuka jaringan.  Dalam utilitas ini Anda dapat menonton pemanfaatan antarmuka. </li></ul><br>  Menggunakan nicstat, kita melihat: sama, pemanfaatan antarmuka, beberapa kesalahan yang terjadi pada antarmuka, throughput juga merupakan utilitas yang sangat berguna. <br><br><ul><li>  pgCenter adalah utilitas untuk bekerja hanya dengan PostgreSQL.  Ini menunjukkan statistik PostgreSQL dalam antarmuka seperti teratas, dan Anda juga dapat melihat statistik yang terkait dengan replikasi streaming di dalamnya. </li></ul><br>  Dengan bantuan pgCenter, kami melihat: statistik tentang replikasi.  Anda dapat menonton lag replikasi, entah bagaimana mengevaluasinya, dan memprediksi pekerjaan di masa depan. <br><br><ul><li>  perf adalah utilitas untuk penyelidikan yang lebih dalam tentang penyebab "ketukan bawah tanah", ketika dalam operasi ada masalah aneh pada level kode PostgreSQL. </li></ul><br>  Dengan bantuan perf kami mencari: ketukan bawah tanah.  Agar perf dapat bekerja sepenuhnya dengan PostgreSQL, yang terakhir harus dikompilasi dengan karakter debug, sehingga Anda dapat melihat tumpukan fungsi dalam proses dan fungsi mana yang paling memakan waktu CPU. <br><br>  Semua utilitas ini diperlukan untuk <strong>menguji hipotesis</strong> yang muncul saat pemecahan masalah - di mana dan apa yang melambat, di mana dan apa yang perlu Anda perbaiki, periksa.  Utilitas ini membantu memastikan kita berada di jalur yang benar. <br><br><h3>  Alat Tertanam <br></h3><br>  Apa yang ditawarkan PostgreSQL sendiri? <br><br><h4>  Tampilan sistem <br></h4><br>  Secara umum, ada banyak alat untuk bekerja dengan PostgreSQL.  Setiap perusahaan vendor yang menyediakan dukungan PostgreSQL menawarkan alatnya sendiri.  Tetapi, sebagai aturan, alat-alat ini didasarkan pada statistik internal PostgreSQL.  Dalam hal ini, PostgreSQL memberikan pandangan sistem di mana Anda dapat membuat berbagai pilihan dan mendapatkan informasi yang Anda butuhkan.  Artinya, menggunakan klien biasa, biasanya psql, kita dapat membuat kueri dan melihat apa yang terjadi dalam statistik. <br><br>  Ada beberapa pandangan sistem.  Agar dapat bekerja dengan replikasi streaming dan menyelidiki masalah, kita hanya perlu: pg_stat_replication, pg_stat_wal_receiver, pg_stat_databases, pg_stat_databases_conflicts, dan pg_stat_activity dan pg_stat_archiver tambahan. <br><br>  Ada beberapa dari mereka, tetapi set ini cukup untuk memeriksa apakah ada masalah. <br><br><h4>  Fungsi Pembantu <br></h4><br>  Menggunakan fungsi bantu, Anda dapat mengambil data dari representasi sistem statistik dan mengubahnya menjadi bentuk yang lebih nyaman bagi Anda.  Fungsi bantu juga hanya beberapa buah. <br><br><ul><li>  pg_current_wal_lsn () (analog lama pg_current_xlog_location ()) adalah fungsi yang paling penting yang memungkinkan Anda untuk melihat posisi saat ini dalam log transaksi.  Log transaksi adalah urutan data yang berkelanjutan.  Dengan menggunakan fungsi ini, Anda dapat melihat titik terakhir, mendapatkan posisi di mana log transaksi telah berhenti sekarang. <br></li><li>  pg_last_wal_receive_lsn (), pg_last_xlog_receive_location () adalah fungsi serupa di atas, hanya untuk replika.  Replika menerima log transaksi, dan Anda dapat melihat posisi log transaksi yang terakhir diterima; <br></li><li>  pg_wal_lsn_diff (), pg_xlog_location_diff () adalah fungsi lain yang bermanfaat.  Kami memberinya dua posisi dari log transaksi, dan dia menunjukkan perbedaan - jarak antara dua titik ini dalam byte.  Fungsi ini selalu berguna untuk menentukan jeda antara master dan replika dalam byte. <br></li></ul><br>  Daftar lengkap fungsi dapat diperoleh dengan meta-command psql: \ df * (wal | xlog | lsn | location) *. <br><br>  Anda dapat mengetikkannya dalam psql dan melihat semua fungsi yang berisi lokasi, xlog, Isn,.  Akan ada sekitar 20-30 fungsi seperti itu, dan mereka juga menyediakan berbagai informasi pada log transaksi.  Saya sarankan Anda membiasakan diri. <br><br><h4>  Utilitas Pg_waldump <br></h4><br>  Sebelum versi 10.0, itu disebut pg_xlogdump.  Utilitas pg_waldump diperlukan ketika kita ingin melihat ke dalam segmen log transaksi, mencari tahu catatan sumber daya yang ada di sana, dan apa yang PostgreSQL tulis di sana, yaitu, untuk studi yang lebih rinci. <br><br><blockquote>  Di versi 10.0, semua tampilan sistem, fungsi, dan utilitas yang termasuk kata xlog diubah namanya.  Semua kemunculan kata xlog dan lokasi masing-masing digantikan oleh kata wal dan lsn.  Hal yang sama dilakukan dengan direktori pg_xlog yang menjadi direktori pg_wal. <br></blockquote><br>  Utilitas pg_waldump dengan mudah menerjemahkan isi segmen XLOG ke format yang dapat dibaca manusia.  Anda dapat melihat apa yang disebut catatan sumber daya masuk ke dalam log segmen selama pekerjaan PostgreSQL, yang mana indeks dan file tumpukan diubah, informasi apa yang dimaksudkan untuk stand-by sampai di sana.  Dengan demikian, banyak informasi dapat dilihat menggunakan pg_waldump. <br><br><blockquote>  Tetapi ada disclaimer yang ditulis dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi resmi</a> : pg_waldump dapat menampilkan data yang sedikit salah ketika PostgreSQL berjalan (Dapat memberikan hasil yang salah ketika server berjalan - apa pun artinya) </blockquote><br>  Anda dapat menggunakan perintah: <br><br><pre><code class="sql hljs">pg_waldump -f - /wal_10 \ $(psql -qAtX - "<span class="hljs-keyword"><span class="hljs-keyword">select</span></span> pg_walfile_name(pg_current_wal_lsn())<span class="hljs-string"><span class="hljs-string">")</span></span></code> </pre> <br>  Ini adalah analog dari perintah tail -f untuk log transaksi saja.  Perintah ini menunjukkan ekor log transaksi yang sedang terjadi sekarang.  Anda dapat menjalankan perintah ini, ia akan menemukan segmen terakhir dengan entri log transaksi terbaru, terhubung dengannya dan mulai menampilkan konten log transaksi.  Tim yang sedikit rumit, tetapi, bagaimanapun, ini berhasil.  Saya sering menggunakannya. <br><a name="cases"></a><br><h2>  Mengatasi kasus <br></h2><br>  Di sini kita melihat masalah paling umum yang muncul dalam praktik konsultan, gejala apa yang mungkin terjadi dan bagaimana mendiagnosisnya: <br><br>  <strong>Keterlambatan replikasi adalah masalah yang paling umum</strong> .  Baru-baru ini, kami memiliki korespondensi dengan pelanggan: <br><br><blockquote>  - <em>Kami telah merusak replikasi master-slave antara kedua server.</em> <br><br>  - Terdeteksi lag 2 jam, pg_dump dimulai. <br><br>  - <em>Oke saya mengerti.</em>  <em>Apa kelambatan kami yang diizinkan?</em> <br><br>  - 16 jam di max_standby_streaming_delay. <br><br>  <em>- Apa yang akan terjadi ketika keterlambatan ini terlampaui?</em>  <em>Sirene meraung?</em> <br><br>  - Tidak, transaksi akan dikalahkan, dan gulungan WAL akan dilanjutkan. <br></blockquote><br>  Kami memiliki masalah dengan keterlambatan replikasi sepanjang waktu, dan hampir setiap minggu kami menyelesaikannya. <br><br>  <strong>Pembengkakan direktori pg_wal / di</strong> mana segmen log transaksi disimpan adalah masalah yang lebih jarang terjadi.  Tetapi dalam hal ini perlu untuk mengambil tindakan segera sehingga masalahnya tidak berubah menjadi situasi darurat ketika replika jatuh. <br><br>  <strong>Permintaan panjang</strong> yang dijalankan pada replika menyebabkan <strong>konflik selama pemulihan</strong> .  Ini adalah situasi ketika kami memulai beberapa jenis beban pada replika, Anda dapat menjalankan kueri baca pada replika, dan pada saat ini kueri ini mengganggu reproduksi log transaksi.  Ada konflik, dan PostgreSQL perlu memutuskan apakah akan menunggu permintaan untuk menyelesaikan atau untuk menyelesaikannya dan terus memainkan log transaksi.  Ini adalah konflik replikasi atau konflik pemulihan. <br><br>  <strong>Proses pemulihan: Penggunaan CPU 100% -</strong> Proses memulihkan log transaksi pada replika membutuhkan 100% waktu prosesor.  Ini juga situasi yang jarang, tetapi agak tidak menyenangkan, karena  mengarah pada peningkatan keterlambatan replikasi dan umumnya sulit untuk diselidiki. <br><br><h3>  Replikasi tertinggal <br></h3><br>  Kelambatan replikasi adalah ketika permintaan yang sama, dijalankan pada master dan pada replika, mengembalikan data yang berbeda.  Ini berarti bahwa data tidak konsisten antara master dan replika, dan ada beberapa kelambatan.  Replika perlu mereproduksi bagian dari log transaksi untuk mengejar ketinggalan dengan wizard.  Gejala utama terlihat persis seperti ini: ada kueri, dan hasilnya berbeda. <br><br>  <strong>Bagaimana cara mencari masalah seperti itu?</strong> <br><br><ul><li>  Ada tampilan dasar pada wizard dan replika - <strong>pg_stat_replication</strong> .  Ini menunjukkan informasi tentang semua Pengirim WAL, yaitu, pada proses yang mengirim log transaksi.  Setiap replika akan memiliki baris terpisah yang menunjukkan statistik untuk replika khusus ini. <br></li><li>  Fungsi bantu <strong>pg_wal_lsn_diff ()</strong> memungkinkan Anda untuk membandingkan posisi yang berbeda dalam log transaksi dan menghitung jeda yang sama.  Dengan bantuannya, kita bisa mendapatkan angka tertentu dan menentukan di mana kita memiliki jeda yang besar, mana yang kecil dan entah bagaimana sudah menanggapi masalahnya. <br></li><li>  Fungsi <strong>pg_last_xact_replay_timestamp ()</strong> hanya bekerja pada replika dan memungkinkan Anda untuk melihat waktu ketika transaksi terakhir yang hilang dilakukan.  Ada fungsi now () yang terkenal yang menunjukkan waktu saat ini, kami kurangi waktu yang ditunjukkan kepada kami oleh fungsi pg_last_xact_replay_timestamp () dari fungsi now () dan mendapatkan jeda waktu. <br></li></ul><br>  Dalam versi 10 dari pg_stat_replication, bidang tambahan muncul yang menunjukkan jeda waktu yang sudah ada di wizard, oleh karena itu metode ini sudah usang, tetapi, bagaimanapun, itu dapat digunakan. <br><br>  Ada jebakan kecil.  Jika tidak ada transaksi pada wizard untuk waktu yang lama, dan itu tidak menghasilkan log transaksi, maka fungsi terakhir akan menunjukkan peningkatan jeda.  Faktanya, sistem ini hanya idle, tidak ada aktivitas di dalamnya, tetapi dalam pemantauan kita dapat melihat bahwa lag semakin berkembang.  Perangkap ini patut diingat. <br><br>  Tampilannya adalah sebagai berikut. <br><br><img src="https://habrastorage.org/webt/sb/jg/6k/sbjg6kswtdtymuecundldk180r8.jpeg"><br><br>  Ini berisi informasi tentang setiap Pengirim WAL dan beberapa bidang yang penting bagi kami.  Ini terutama <strong>client_addr</strong> - alamat jaringan dari replika yang terhubung (biasanya alamat IP) dan seperangkat bidang <strong>lsn</strong> (dalam versi yang lebih lama disebut lokasi), saya akan membicarakannya sedikit lebih jauh. <br><br>  Dalam versi ke-10, bidang <strong>lag</strong> muncul - ini adalah jeda yang dinyatakan dalam waktu, yaitu format yang lebih dapat dibaca oleh manusia.  Kelambatan dapat dinyatakan dalam byte atau dalam waktu - Anda dapat memilih apa yang paling Anda sukai. <br><br>  Sebagai aturan, saya menggunakan permintaan ini. <br><br><img src="https://habrastorage.org/webt/ff/8g/kv/ff8gkvz-kzdihuxabphcb0rfd4e.jpeg"><br><br>  Ini bukan kueri paling rumit yang dicetak pg_stat_replication dalam format yang lebih mudah dimengerti.  Di sini saya menggunakan fungsi-fungsi berikut: <br><br><ul><li>  <strong>pg_wal_lsn_diff ()</strong> untuk membaca diff.  Tetapi di antara apa menurut saya perbedaan itu?  Kami memiliki beberapa bidang - sent_lsn, write_lsn, flush_lsn, replay_lsn.  Dengan menghitung perbedaan antara bidang saat ini dan sebelumnya, kita dapat secara akurat memahami di mana kita tertinggal, di mana tepatnya lag terjadi. <br></li><li>  <strong>pg_current_wal_lsn ()</strong> , yang menunjukkan posisi saat ini dari log transaksi.  Di sini kita melihat jarak antara posisi saat ini di log dan yang dikirim - berapa banyak log transaksi yang dihasilkan tetapi tidak dikirim. <br></li><li>  <strong>sent_lsn</strong> , <strong>write_lsn</strong> - ini adalah berapa banyak yang dikirim ke replika, tetapi tidak direkam.  Artinya, sekarang terletak di suatu tempat di jaringan, atau diterima oleh replika, tetapi belum ditulis dari buffer jaringan ke penyimpanan disk. <br></li><li>  <strong>write_lsn, flush_lsn</strong> - ini ditulis, tetapi tidak dikeluarkan oleh perintah fsync - seolah-olah ditulis, tetapi dapat ditemukan di suatu tempat di RAM, dalam cache halaman sistem operasi.  Segera setelah kami melakukan fsync, data disinkronkan dengan disk, sampai ke penyimpanan persisten dan semuanya tampaknya dapat diandalkan. <br></li><li>  <strong>replay_lsn, flush_lsn</strong> - data dicampakkan, fsync dieksekusi, tetapi tidak direplikasi. <br></li><li>  <strong>current_wal_lsn</strong> dan <strong>replay_lsn</strong> adalah jenis total lag yang mencakup semua posisi sebelumnya. <br></li></ul><br><h4>  <strong>Beberapa contoh</strong> <br></h4><br><img src="https://habrastorage.org/webt/7c/9n/tt/7c9nttg6yp-uyimrxa4ji3ll1ha.jpeg"><br><br>  Replika 10.6.6.8 disorot di atas.  Dia memiliki <strong>lag yang tertunda</strong> , dia membuat beberapa log transaksi, tetapi masih belum terkirim dan terletak pada master.  Kemungkinan besar, ada beberapa jenis masalah dengan kinerja jaringan.  Kami akan memverifikasi ini menggunakan utilitas nicstat. <br><br>  Kami akan meluncurkan nicstat, lihat pemanfaatan antarmuka, jika ada masalah dan kesalahan di sana.  Jadi kita bisa menguji hipotesis ini. <br><br><img src="https://habrastorage.org/webt/co/-m/fj/co-mfjznunlrsuurikvz7xy9gh0.jpeg"><br><br>  <strong>Jeda penulisan</strong> ditandai di atas.  Sebenarnya, lag ini cukup langka, saya hampir tidak melihatnya terlalu besar.  Masalahnya mungkin dengan disk, dan kami menggunakan utilitas iostat atau iotop - kami melihat pemanfaatan penyimpanan disk, yang I / O dibuat oleh proses, dan kemudian kami mencari tahu mengapa. <br><br><img src="https://habrastorage.org/webt/to/xw/2j/toxw2jutr7yaof38fwbohph9lva.jpeg"><br><br>  <strong>Flush dan replay Lags</strong> - paling sering lag terjadi di sana ketika perangkat disk pada replika tidak punya waktu untuk hanya kehilangan semua perubahan yang tiba dari master. <br><br>  Juga dengan utilitas iostat dan iotop kita melihat apa yang terjadi dengan pemanfaatan disk dan mengapa rem. <br><br>  Dan <strong>total_lag</strong> terakhir adalah metrik yang berguna untuk sistem pemantauan.  Jika ambang total_lag kami terlampaui, kotak centang dinaikkan dalam pemantauan, dan kami mulai menyelidiki apa yang terjadi di sana. <br><br><h4>  <strong>Tes hipotesis</strong> <br></h4><br>  Sekarang Anda perlu mencari cara untuk menyelidiki lebih lanjut masalah tertentu.  Saya sudah mengatakan jika ini adalah lag jaringan, maka kita perlu memeriksa apakah semuanya sesuai dengan jaringan. <br><br>  Sekarang hampir semua hosters menyediakan 1 Gb / s atau bahkan 10 Gb / s, sehingga <strong>bandwidth yang tersumbat adalah skenario yang paling tidak mungkin</strong> .  Sebagai aturan, Anda perlu melihat kesalahan.  nicstat berisi informasi tentang kesalahan pada antarmuka, Anda dapat mengetahui bahwa ada masalah dengan driver, baik dengan kartu jaringan itu sendiri atau dengan kabel. <br><br>  Kami menyelidiki <strong>masalah penyimpanan</strong> menggunakan iostat dan iotop.  iostat diperlukan untuk melihat gambaran umum penyimpanan disk: daur ulang perangkat, lebar pita perangkat, latensi.  iotop - untuk penelitian yang lebih akurat, ketika kita perlu mengidentifikasi proses mana yang memuat subsistem disk.  Jika ini adalah semacam proses pihak ketiga, itu hanya dapat dideteksi, diselesaikan, dan mungkin masalahnya akan hilang. <br><br>  Pertama-tama, kita melihat <strong>penundaan pemulihan dan konflik replikasi</strong> melalui top atau pg_stat_activity: proses mana yang sedang berjalan, permintaan mana yang sedang berjalan, waktu pelaksanaannya, berapa lama mereka berjalan.  Jika ini adalah beberapa pertanyaan panjang, kami melihat mengapa mereka bekerja untuk waktu yang lama, menembak mereka, memahami dan <strong>mengoptimalkannya</strong> - kami akan memeriksa pertanyaan itu sendiri. <br><br>  Jika ini adalah sejumlah <strong>besar log transaksi yang</strong> dihasilkan oleh wizard, kami dapat mendeteksi ini dengan <strong>pg_stat_activity</strong> .  Mungkin beberapa proses pencadangan dimulai di sana, beberapa jenis kekosongan telah dimulai (pg_stat_progress_vacuum), atau pos pemeriksaan sedang dijalankan.  Artinya, jika terlalu banyak log transaksi dihasilkan, dan replika tidak punya waktu untuk memprosesnya, pada titik tertentu mungkin jatuh, dan ini akan menjadi masalah bagi kami. <br><br>  Dan tentu saja <strong>pg_wal_lsn_diff ()</strong> untuk menentukan lag dan menentukan di mana kita memiliki lag secara khusus - pada jaringan, pada disk, atau pada prosesor. <br><br><h4>  <strong>Opsi solusi</strong> <br></h4><br>  <strong>Masalah Jaringan / Penyimpanan</strong> <br><br>  Semuanya cukup sederhana di sini, tetapi dari sudut pandang konfigurasi, ini biasanya tidak terpecahkan.  Anda dapat mengencangkan beberapa mur, tetapi secara umum ada 2 opsi: <br><br><ul><li>  Periksa beban kerja </li></ul><br>  Periksa permintaan apa yang sedang berjalan.  Mungkin beberapa jenis migrasi diluncurkan yang menghasilkan banyak log transaksi, atau bisa juga transfer data, penghapusan atau penyisipan.  <strong>Setiap proses yang menghasilkan log transaksi dapat menyebabkan kelambatan transaksi</strong> .  Semua data pada wizard dihasilkan secepat mungkin, kami membuat perubahan pada data, mengirimkannya ke replika, dan replika itu dapat mengatasi atau gagal - ini bukan masalah wizard.  Kelambatan mungkin muncul di sini dan Anda perlu melakukan sesuatu dengannya. <br><br><ul><li>  Tingkatkan perangkat keras <br></li></ul><br>  Pilihan paling bodoh - mungkin kita telah mengalami kinerja besi, dan Anda hanya perlu mengubahnya.  Ini bisa berupa disk lama atau SSD berkualitas buruk, atau colokan kinerja pengontrol RAID.  Di sini kita tidak lagi menjelajahi basis itu sendiri, tetapi memeriksa kinerja kelenjar kita. <br><br>  <strong>Penundaan Pemulihan</strong> <br><br>  Jika kita memiliki segala jenis konflik replikasi karena permintaan panjang, yang mengakibatkan peningkatan keterlambatan ulangan, <strong>hal pertama yang</strong> kita lakukan adalah <strong>menembak permintaan panjang</strong> yang berjalan pada replika, karena mereka menunda pemutaran log transaksi. <br><br>  Jika kueri panjang terkait dengan tidak optimalnya kueri SQL itu sendiri (kami menemukan ini menggunakan EXPLAIN ANALYZE), Anda hanya perlu mendekati kueri ini secara berbeda dan menulis ulang.  Atau ada opsi untuk mengonfigurasi <strong>replika terpisah untuk kueri pelaporan</strong> .  Jika kami membuat laporan yang berfungsi untuk waktu yang lama, laporan tersebut perlu dikirim ke replika terpisah. <br><br>  Masih ada opsi <strong>menunggu saja</strong> .  Jika kita memiliki beberapa kelambatan pada tingkat beberapa kilobyte atau bahkan puluhan megabita, tetapi kami pikir ini dapat diterima, kami hanya menunggu permintaan diselesaikan dan kelambatan tersebut akan menyelesaikannya sendiri.  Ini juga merupakan pilihan, dan sering terjadi bahwa itu dapat diterima. <br><br>  <strong>WAL volume tinggi</strong> <br><br>  Jika kita menghasilkan volume besar log transaksi, kita perlu mengurangi <strong>volume</strong> ini <strong>per unit waktu</strong> , untuk membuat replika perlu mengunyah lebih sedikit log transaksi. <br><br>  Ini biasanya dilakukan <strong>melalui konfigurasi</strong> .  Solusi parsial dalam pengaturan full_page_writes = parameter off.  Opsi ini mengaktifkan / menonaktifkan rekaman gambar lengkap dari halaman yang berubah dalam log transaksi.  Ini berarti bahwa ketika kami memiliki layanan operasi menulis pos pemeriksaan (CHECKPOINT), saat berikutnya kami mengubah beberapa blok data di area buffer bersama, gambar lengkap halaman ini akan masuk ke log transaksi, dan bukan hanya perubahan itu sendiri.  Dengan semua perubahan berikutnya pada halaman yang sama, hanya perubahan yang akan dicatat dalam log transaksi.  Dan seterusnya ke pos pemeriksaan berikutnya. <br><br>  Setelah pos pemeriksaan, kami merekam gambar penuh halaman, dan ini memengaruhi volume log transaksi yang direkam.  Jika ada cukup banyak pos pemeriksaan per unit waktu, katakanlah 4 pos pemeriksaan dilakukan per jam, dan akan ada banyak gambar halaman penuh, ini akan menjadi masalah.  Anda dapat menonaktifkan perekaman gambar penuh dan ini akan mempengaruhi volume WAL.  Tetapi sekali lagi, ini adalah setengah ukuran. <br><br>  <i>Catatan: Rekomendasi untuk menonaktifkan full_page_writes harus dipertimbangkan dengan hati-hati, karena penulis lupa untuk mengklarifikasi selama laporan bahwa menonaktifkan parameter dapat, dalam beberapa keadaan, terjadi dalam situasi darurat (kerusakan pada sistem file atau log, sebagian menulis ke blok, dll.) file database yang berpotensi rusak.</i>  <i>Karena itu, berhati-hatilah, menonaktifkan parameter dapat meningkatkan risiko korupsi data dalam situasi darurat.</i> <br><br>  Separuh tindakan lainnya adalah <strong>meningkatkan interval di antara pos-pos pemeriksaan</strong> .  Secara default, pos pemeriksaan dilakukan setiap 5 menit, dan ini cukup umum.  Sebagai aturan, interval ini ditingkatkan menjadi 30-60 menit - ini adalah waktu yang cukup dapat diterima dimana semua halaman kotor dikelola untuk disinkronkan ke disk. <br><br>  Tetapi solusi utamanya adalah, tentu saja, untuk <strong>melihat beban kerja kita</strong> - jenis operasi berat apa yang terjadi di sana, terkait dengan mengubah data, dan, mungkin, mencoba melakukan perubahan ini dalam batch. <br><br>  Misalkan kita memiliki tabel, kita ingin menghapus beberapa juta catatan dari itu.  Pilihan terbaik adalah tidak menghapus jutaan ini sekaligus dengan satu permintaan, tetapi untuk memecahnya menjadi paket 100-200 ribu sehingga, pertama, volume kecil WAL dihasilkan, kedua, ruang hampa memiliki waktu untuk melewati data yang dihapus, dan karena itu lag tidak begitu besar dan kritis. <br><br><h3>  Pembengkakan pg_wal / <br></h3><br>  Sekarang, mari kita bicara tentang bagaimana Anda dapat menemukan bahwa direktori pg_wal / bengkak. <br><br>  Secara teori, PostgreSQL selalu mempertahankannya dalam keadaan optimal untuk dirinya sendiri di tingkat file konfigurasi tertentu, dan, sebagai aturan, seharusnya tidak tumbuh di atas batas-batas tertentu. <br><br>  Ada parameter max_wal_size, yang menentukan nilai maksimum.  Plus ada parameter wal_keep_segments - jumlah tambahan segmen yang disimpan master untuk replika jika replika tiba-tiba tidak tersedia untuk waktu yang lama. <br><br>  Setelah menghitung jumlah max_wal_size dan wal_keep_segments, kita dapat memperkirakan secara kasar berapa banyak ruang yang akan ditempati direktori pg_wal /.  Jika itu tumbuh dengan cepat dan membutuhkan lebih banyak ruang daripada nilai yang dihitung, ini berarti ada beberapa masalah, dan Anda perlu melakukan sesuatu untuk itu. <br><br><h4>  Bagaimana cara mendeteksi masalah seperti itu? <br></h4><br>  Pada sistem operasi Linux, ada <strong>perintah du -csh</strong> .  Kami hanya dapat memantau nilai dan memantau berapa banyak log transaksi yang kami miliki di sana;  simpan label yang dihitung, berapa banyak dia berutang dan berapa banyak yang sebenarnya dia ambil, dan entah bagaimana menanggapi perubahan angka. <br><br>  Tempat lain yang kami lihat adalah tampilan <strong>pg_replication_slots</strong> dan <strong>pg_stat_archiver</strong> .  Alasan paling umum mengapa pg_wal / memakan banyak ruang adalah slot replikasi yang terlupakan atau pengarsipan yang rusak.  Alasan lain juga ada tempatnya, tetapi dalam praktik saya itu sangat jarang. <br><br>  Dan, tentu saja, selalu ada kesalahan dalam log PostgreSQL yang terkait dengan perintah arsip.  Sayangnya, tidak akan ada alasan lain yang terkait dengan pg_wal / overflow.  Kami hanya dapat menangkap kesalahan arsip di sana. <br><br><h4>  Opsi untuk masalah: <br></h4><br>  <strong>CRUD Berat</strong> - operasi penyegaran data berat - INSERT, DELETE, UPDATE, terkait dengan perubahan beberapa juta baris.  Jika PostgreSQL perlu melakukan operasi seperti itu, jelas bahwa sejumlah besar log transaksi akan dihasilkan.  Ini akan disimpan di pg_wal /, dan ini akan menambah ruang yang ditempati.  Itu, sekali lagi, seperti yang saya katakan sebelumnya, adalah praktik yang baik untuk hanya memecahnya menjadi paket, dan untuk memperbarui bukan seluruh array, tetapi masing-masing 100, 200, 300 ribu. <br><br>  <strong>Slot replikasi yang terlupakan atau tidak digunakan</strong> adalah masalah umum lainnya.  Orang sering menggunakan replikasi logis untuk beberapa tugas mereka: mereka mengkonfigurasi bus yang mengirim data ke Kafka, mengirim data ke aplikasi pihak ketiga yang menerjemahkan replikasi logis ke format lain dan entah bagaimana memprosesnya.  <strong>Replikasi logis biasanya bekerja melalui slot</strong> .  Kebetulan kami mengatur slot replikasi, bermain dengan aplikasi, menyadari bahwa aplikasi ini tidak sesuai dengan kami, mematikan aplikasi, menghapusnya, <strong>dan slot replikasi terus hidup</strong> . <br><br>  PostgreSQL untuk setiap slot replikasi menyimpan segmen log transaksi seandainya aplikasi jarak jauh atau replika terhubung ke slot ini lagi, dan kemudian wizard dapat mengirimkan mereka log transaksi ini. <br><br>  Tetapi waktu berlalu, tidak ada yang terhubung ke slot, log transaksi diakumulasikan, dan pada titik tertentu mereka menempati 90% dari ruang.  Kita perlu mencari tahu apa itu, mengapa begitu banyak ruang diambil.  Sebagai aturan, slot yang terlupakan dan tidak terpakai ini hanya perlu dihapus, dan masalahnya akan diselesaikan.  Tetapi lebih lanjut tentang itu nanti. <br><br>  Opsi lain mungkin perintah <strong>archive_command rusak</strong> .  Ketika kita memiliki semacam repositori log transaksi eksternal yang kita simpan untuk tugas pemulihan bencana, perintah arsip biasanya disiapkan, lebih jarang pg_receivexlog diatur.  Perintah yang terdaftar di archive_command sangat sering berupa perintah terpisah atau beberapa skrip yang mengambil segmen log transaksi dari pg_wal / dan menyalinnya ke penyimpanan arsip. <br><br>  Kebetulan kami melakukan semacam peningkatan paket sistem, misalnya, di rsync versi berubah, bendera diperbarui atau diubah, atau dalam beberapa perintah lain yang digunakan dalam perintah arsip, formatnya juga berubah - dan skrip atau program itu sendiri yang ditentukan dalam arsip_perintah istirahat.  Akibatnya, arsip tidak lagi dapat disalin. <br><br>  Jika perintah arsip bekerja dengan output bukan 0, maka pesan tentang ini akan ditulis ke log, dan segmen akan tetap di direktori pg_wal /.  <strong>Sampai kami menemukan bahwa tim arsip kami telah rusak, segmen akan menumpuk</strong> , dan tempat itu juga akan berakhir pada titik tertentu. <br><br>  <strong>Set tindakan darurat (100% ruang yang digunakan):</strong> <br><br> 1. <strong>   CRUD </strong> ,        — pg_terminate_backend(). <br>    -  ,  , ,      ..       ,      pg_wal/,     . <br><br> 2. <strong>    </strong>    root — reserved space ratio (ext filesystems). <br>       ext      ext     5%.  ,         ,  5% —  . ,   ,     1%  ,     tune2fs -m 1.       PostgreSQL     ,    .      100%     . <br><br> 3. <strong>  </strong> (LVM, ZFS,...). <br>    LVM  ZFS,        LVM  ZFS,            ,   ,           .       ,   . <br><br> 4.    — <strong>, , HE    pg_wal/</strong> . <br>          ,     ,   ,   .     ! PostgreSQL     ,   .     ,  ,   ,    . <br><br> , pg_xlog/   pg_wal/     —  log    ,   , , ,  -   —  ! <br><br><h4>    <br></h4><br>  ,         100%  CPU,      . <br><br>   <strong> workload  </strong> .    ,       ?  ,     - ,      -.         :   ,   tablespace,    tablespace. <br><br>   <strong> </strong> .       ,  ,    ,    , ,  ,            . <strong>   —      .</strong> <br><br>   <strong>  checkpoints_segments/max_wal_size, wal_keep_segments</strong> .  ,    ,      — 10-20   wal_keep_segments,     max_wal_size.  ,        .  PostgreSQL       pg_wal/  . <br><br>  <strong>  </strong>   pg_replication_slots —     .   ,   <strong> </strong> ,        —      .   ,    ,    .    . <br><br>      WAL,       ,  <strong>    pg_stat_archiver</strong> ,    . ,  <strong>  </strong> , ,    ,    . <br><br>    <strong>   checkpoint</strong> .       ,        ,      . , PostgreSQL        . <strong>   ,    checkpoint</strong> . <br><br><h3>       <br></h3><br>    ,   ,   —        .   -             ,       .      ,         . <br><br> <strong> </strong> —    PostgreSQL  : <br><br><ul><li> User was holding shared bufer pin for too long. <br></li><li> User query might have needed to see row versions that must be removed. <br></li><li> User was holding a relation lock for too long. <br></li><li> User was or might have been using table space that must be dropped. <br></li><li> User transaction caused bufer deadlock with recovery. <br></li><li> User was connected to a database that must be dropped. <br></li></ul><br>  2  —    ,       ,      .   :  ,   ,      .        (  30 ),  <strong>PostgreSQL       </strong> —  . <br><br>    .  ,    ,             .    -    ,     timeout     .     —    ALTER,    ,    . <br><br>     .     ,  tablespace     ,            tablespace.   ,    ,  -   —   . <br><br><h4> <strong> ?</strong> <br></h4><br>       <strong>pg_stat_databases, pg_stat_databases_conflicts</strong> .         ,    .     ,    . <br><br>  <strong>  </strong> ,       <strong> </strong> .  ,      .   ,    . ,       ,    ,    . <br><br><h4> <strong> ?</strong> <br></h4><br>   ,    —    : <br><br><ol><li> <strong> max_standby_streaming_delay</strong> (  ).     ,       .    <strong>   </strong> . <br></li><li> <strong> hot_stadby_feedback</strong> (  /).  ,  vacuum   - ,       .     <strong> bloat  </strong> .     ,    ,  ,   hot_stadby_feedback  . <br></li><li>     DBA     —  <strong>  </strong> .   <strong> </strong> ,    .  ,    ,     ,   -    ,  . <br></li><li>  , ,  ,   ,  DBA —  <strong>     </strong> , ,  .    max_standby_streaming_delay  .       ,   .      ,  ,    ,  .  <strong>   </strong> —    ,     . <br></li></ol><br><h2> Recovery process: 100% CPU usage <br></h2><br>  ,  ,    , <strong> 100%   </strong> .  ,      ,      100%.  ,    pg_stat_replication,  ,     replay,       ,    . <br><br> <strong></strong>  : <br><br><ul><li> <strong>top</strong> —       —    100% CPU usage  recovery process; <br></li><li> <strong>pg_stat_replication</strong> —  ,   ,     . <br></li></ul><br><h4> <strong>   </strong> <br></h4><br>      ,    .  ,     : <br><br><ul><li> perf top/record/report ( debug—); <br></li><li> GDB; <br></li><li>  pg_waldump. <br></li></ul><br>  ,   ,       .     workload,   <strong>        </strong> .  ,    , PostgreSQL     shared buffers       ( ).        . <br><br><h4>  <strong>Solusi</strong> <br></h4><br>    ,      <strong>   </strong> . -   workload, - , -    : «      ,  -  ». <br><br>       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pgsql-hackers</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pgsql-bugs</a> ,   ,   .  ,    . <br><br>     — <strong>-   ,  , </strong> . <br><br><h2>  Ringkasan <br></h2><br> <strong>      </strong> .  ,             ,    ,      . <br><br> <strong>     </strong> .    , ,  ,    ,   ,   — . <br><br>  , <strong>  </strong> ,   — .   ,   ,  ,         . <br><br>  ,    ,  <strong>  </strong> —  ,      ,   . <br><br><h3>  Tautan yang bermanfaat <br></h3><br><ul><li> PostgreSQL official documentation — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">The Statistics Collector</a> </li><li> PostgreSQL <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Mailing Lists</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">general</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">performance</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">hackers</a> ) </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">PostgreSQL-Consulting company blog</a> </li></ul><br><blockquote>             , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Highload++ Siberia</a> ,   <strong>25  26   </strong> .   ,     ,  . <br><br><ul><li>      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="></a>     MySQL  ClickHouse. </li><li>      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="></a> ,            Oracle. </li><li>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="></a>    ,     ,  —  ,       . </li><li>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="></a> ,   VK  ClickHouse,   ,   . </li></ul><br></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id414111/">https://habr.com/ru/post/id414111/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id414095/index.html">[Terjemahan] Elasticsearch 6.3.0 dirilis</a></li>
<li><a href="../id414097/index.html">Komputer Apple menutup kerentanan firmware yang ditemukan oleh para pakar Teknologi Positif</a></li>
<li><a href="../id414103/index.html">Buka webinar: "Jaringan saraf untuk tugas farmasi"</a></li>
<li><a href="../id414105/index.html">Memprediksi hasil Piala Dunia 2018 menggunakan algoritma hutan acak</a></li>
<li><a href="../id414109/index.html">Menggunakan Docker untuk Membangun dan Menjalankan Proyek C ++</a></li>
<li><a href="../id414115/index.html">Apa itu Lazy FP State Restore: kerentanan baru ditemukan pada prosesor Intel</a></li>
<li><a href="../id414117/index.html">Format biner desimal campuran vs IEEE754</a></li>
<li><a href="../id414119/index.html">Peluang tertidur karena badai pasir di Mars. Tidak jelas apakah bajak akan dapat bekerja lagi</a></li>
<li><a href="../id414121/index.html">DIY drone mandiri dengan kontrol internet</a></li>
<li><a href="../id414123/index.html">Kami memutakhirkan protokol teks menjadi biner dan melawan kode lawas pada pertemuan Kelompok Pengguna C ++</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>