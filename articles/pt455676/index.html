<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®‚Äçüë©‚Äçüë¶ üíë ü§ú A bolha do aprendizado de m√°quina estourou ou o in√≠cio de um novo amanhecer üòï üçá üë®üèæ‚ÄçüöÄ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Recentemente, foi lan√ßado um artigo que mostra uma boa tend√™ncia no aprendizado de m√°quina nos √∫ltimos anos. Em resumo: o n√∫mero de startups no campo ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>A bolha do aprendizado de m√°quina estourou ou o in√≠cio de um novo amanhecer</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/recognitor/blog/455676/">  Recentemente, foi lan√ßado <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">um artigo</a> que mostra uma boa tend√™ncia no aprendizado de m√°quina nos √∫ltimos anos.  Em resumo: o n√∫mero de startups no campo de aprendizado de m√°quina caiu acentuadamente nos √∫ltimos dois anos. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1c6/466/4fc/1c64664fcaf125f2104e67547b533e41.png" alt="imagem"></div><br>  Bem o que.  Vamos analisar "se a bolha estourou", "como continuar a viver" e falar sobre de onde vem esse rabisco. <br><a name="habracut"></a><br>  Primeiro, vamos falar sobre o que foi o impulsionador dessa curva.  De onde ela veio.  Provavelmente todos se lembrar√£o da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">vit√≥ria do</a> aprendizado <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">de</a> m√°quina em 2012 no concurso ImageNet.  Afinal, este √© o primeiro evento global!  Mas, na realidade, n√£o √© assim.  E o crescimento da curva come√ßa um pouco antes.  Eu dividiria em v√°rios pontos. <br><br><ol><li>  2008 √© o surgimento do termo "big data".  Produtos reais come√ßaram <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">a aparecer</a> em 2010.  O big data est√° diretamente relacionado ao aprendizado de m√°quina.  Sem big data, a opera√ß√£o est√°vel dos algoritmos que existiam na √©poca √© imposs√≠vel.  E essas n√£o s√£o redes neurais.  At√© 2012, as redes neurais s√£o uma minoria marginal.  Mas ent√£o come√ßaram a funcionar algoritmos completamente diferentes, que existiam h√° anos e at√© d√©cadas: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SVM</a> (1963, 1993), <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Random Forest</a> (1995), <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">AdaBoost</a> (2003), ... As startups desses anos est√£o principalmente associadas ao processamento autom√°tico de dados estruturados : bilheteiras, usu√°rios, publicidade e muito mais. <br><br>  A derivada dessa primeira onda √© um conjunto de estruturas como XGBoost, CatBoost, LightGBM, etc. <br></li><li>  Em 2011-2012, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">as redes neurais convolucionais</a> venceram uma s√©rie de concursos de reconhecimento de imagens.  Seu uso real foi um pouco atrasado.  Eu diria que startups e solu√ß√µes massivamente significativas come√ßaram a aparecer em 2014.  Foram necess√°rios dois anos para digerir que os neur√¥nios ainda funcionam, para criar estruturas convenientes que pudessem ser instaladas e executadas em um per√≠odo de tempo razo√°vel, para desenvolver m√©todos que estabilizassem e acelerassem o tempo de converg√™ncia. <br><br>  As redes convolucionais possibilitaram solucionar problemas de vis√£o de m√°quina: classifica√ß√£o de imagens e objetos em uma imagem, detec√ß√£o de objetos, reconhecimento de objetos e pessoas, aprimoramento de imagens, etc., etc. </li><li>  2015-2017 anos.  O boom de algoritmos e projetos vinculados a redes de recorr√™ncia ou seus an√°logos (LSTM, GRU, TransformerNet, etc.).  Surgiram algoritmos de convers√£o de texto em texto e sistemas de tradu√ß√£o autom√°tica.  Em parte, eles s√£o baseados em redes convolucionais para destacar os recursos b√°sicos.  Parcialmente pelo fato de terem aprendido a coletar conjuntos de dados realmente grandes e bons. </li></ol><br><img src="https://habrastorage.org/webt/_c/bj/f2/_cbjf2doqjypuqwfwh1d8_vx92a.png"><br><br>  "A bolha estourou?"  O Hype est√° superaquecendo?  Eles morreram como uma blockchain? ‚Äù <br>  Bem ent√£o!  Amanh√£, a Siri deixar√° de funcionar no seu telefone e, depois de amanh√£, Tesla n√£o distinguir√° uma curva de um canguru. <br><br>  As redes neurais j√° est√£o funcionando.  Eles est√£o em dezenas de dispositivos.  Eles realmente permitem que voc√™ ganhe, mude o mercado e o mundo ao seu redor.  Hype parece um pouco diferente: <br><br><img src="https://habrastorage.org/webt/zl/7m/ph/zl7mphh3m3rzprgxpbaopha3gwy.png"><br><br>  As redes neurais deixaram de ser algo novo.  Sim, muitas pessoas t√™m grandes expectativas.  Mas um grande n√∫mero de empresas aprendeu a usar seus neur√¥nios e a fabricar produtos baseados neles.  Os neur√¥nios fornecem novas funcionalidades, podem reduzir empregos, reduzir o pre√ßo dos servi√ßos: <br><br><ul><li>  As empresas de manufatura integram algoritmos para a an√°lise de rejeitos no transportador. </li><li>  As fazendas de gado est√£o comprando sistemas para controlar vacas. </li><li>  Colheitadeiras autom√°ticas. </li><li>  Centros de atendimento automatizados. </li><li>  Filtros no Snapchat.  ( <s>bem, pelo menos algo sensato!</s> ) </li></ul><br>  Mas o principal, e n√£o o mais √≥bvio: "N√£o h√° mais id√©ias novas, ou elas n√£o trar√£o capital instant√¢neo".  As redes neurais resolveram dezenas de problemas.  E eles v√£o decidir ainda mais.  Todas as id√©ias √≥bvias que surgiram - geraram muitas startups.  Mas tudo o que estava na superf√≠cie j√° foi coletado.  Nos √∫ltimos dois anos, n√£o conheci uma √∫nica id√©ia nova para o uso de redes neurais.  Nem uma √∫nica abordagem nova (bem, ok, existem alguns problemas com os GANs). <br><br>  E toda pr√≥xima inicializa√ß√£o √© cada vez mais complicada.  N√£o s√£o mais necess√°rios dois indiv√≠duos que treinam um neur√¥nio em dados abertos.  Exige programadores, um servidor, uma equipe de escritores, suporte complexo, etc. <br><br>  Como resultado, h√° menos startups.  Mas a produ√ß√£o √© mais.  Precisa anexar o reconhecimento de placas?  Existem centenas de profissionais com experi√™ncia relevante no mercado.  Voc√™ pode contratar e em alguns meses seu funcion√°rio criar√° um sistema.  Ou compre um acabado.  Mas fazer uma nova startup? .. Loucura! <br><br>  Precisamos criar um sistema para rastrear visitantes - por que pagar por v√°rias licen√ßas, quando voc√™ pode fazer suas pr√≥prias por 3-4 meses, aprimor√°-las para seus neg√≥cios. <br><br>  Agora, as redes neurais seguem o mesmo caminho que dezenas de outras tecnologias. <br><br>  Lembra como o conceito de "desenvolvedor de sites" mudou desde 1995?  Enquanto o mercado n√£o est√° saturado com especialistas.  Existem muito poucos profissionais.  Mas posso apostar que em 5 a 10 anos n√£o haver√° muita diferen√ßa entre um programador Java e um desenvolvedor de rede neural.  E esses e esses especialistas ser√£o suficientes no mercado. <br><br>  Simplesmente haver√° uma classe de tarefas para as quais os neur√¥nios s√£o resolvidos.  Houve uma tarefa - contratar um especialista. <br><br>  <b>E ent√£o o que?</b>  <b>Onde est√° a intelig√™ncia artificial prometida?</b> <br><br>  E aqui h√° um neponyatchka pequeno, mas interessante :) <br><br>  A pilha de tecnologia que existe hoje, aparentemente, ainda n√£o nos levar√° √† intelig√™ncia artificial.  As id√©ias, sua novidade, esgotaram-se em grande parte.  Vamos falar sobre o que mant√©m o n√≠vel atual de desenvolvimento. <br><br><h3>  Limita√ß√µes </h3><br>  Vamos come√ßar com auto-drones.  Parece ser entendido que √© poss√≠vel fabricar carros totalmente aut√¥nomos com as tecnologias atuais.  Mas depois de quantos anos isso vai acontecer n√£o est√° claro.  Tesla acredita que isso acontecer√° em alguns anos - <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Ucp0TTmvqOE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Existem muitos outros <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">especialistas</a> que avaliam isso entre 5 e 10 anos. <br><br>  Muito provavelmente, em minha opini√£o, depois de 15 anos, a infraestrutura das cidades mudar√° para que o surgimento de carros aut√¥nomos se torne inevit√°vel, seja sua continua√ß√£o.  Mas isso n√£o pode ser considerado intelig√™ncia.  O Tesla moderno √© um pipeline muito complexo para filtrar dados, pesquis√°-los e reciclar.  Estas s√£o regras, regras, regras, coleta de dados e filtros acima deles ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> escrevi um pouco mais sobre isso, ou veja a partir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">deste</a> ponto). <br><br><h3>  Primeiro problema </h3><br>  E √© aqui que vemos o <b>primeiro problema fundamental</b> .  Big data.  Foi exatamente isso que gerou a atual onda de redes neurais e aprendizado de m√°quina.  Agora, para fazer algo complexo e autom√°tico, voc√™ precisa de muitos dados.  N√£o apenas muito, mas muito, muito mesmo.  Precisamos de algoritmos automatizados para coleta, marca√ß√£o e uso.  Queremos fazer o carro ver caminh√µes contra o sol - precisamos primeiro coletar um n√∫mero suficiente deles.  Queremos que o carro n√£o fique louco com uma bicicleta presa ao porta-malas - mais amostras. <br><br>  Al√©m disso, um exemplo n√£o √© suficiente.  Centenas?  Milhares? <br><br><img src="https://habrastorage.org/webt/hl/tm/ip/hltmipml3fq_m-md4ansomrxjmk.jpeg"><br><br><h3>  Segundo problema </h3><br>  <b>O segundo problema</b> √© a visualiza√ß√£o do que nossa rede neural entendeu.  Esta √© uma tarefa muito n√£o trivial.  At√© agora, poucas pessoas entendem como visualizar isso.  Esses artigos s√£o muito recentes, s√£o apenas alguns exemplos, mesmo remotos: <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Visualiza√ß√£o de</a> fixa√ß√£o em texturas.  Ele mostra bem o que o neur√¥nio tende a percorrer em ciclos + o que ele percebe como informa√ß√£o inicial. <br><br><img src="https://habrastorage.org/webt/d7/wb/5f/d7wb5fbpcbugnrcma1qrnn9vyc0.png" alt="imagem"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Visualiza√ß√£o de</a> atenua√ß√£o durante <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tradu√ß√µes</a> .  Realmente, a atenua√ß√£o geralmente pode ser usada com precis√£o para mostrar o que causou uma rea√ß√£o dessa rede.  Eu conheci essas coisas para depura√ß√£o e solu√ß√µes de produtos.  Existem muitos artigos sobre esse t√≥pico.  Por√©m, quanto mais complexos os dados, mais dif√≠cil √© entender como obter uma visualiza√ß√£o sustent√°vel. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/154/93f/988/15493f988978760639233843c9c28c91.png" alt="imagem"><br><br>  Bem, sim, o bom e velho conjunto de "veja qual √© a grade interna nos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">filtros</a> ".  Essas fotos eram populares h√° cerca de 3-4 anos atr√°s, mas todos rapidamente perceberam que as fotos s√£o lindas, mas n√£o h√° muito sentido nelas. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f68/78c/a4a/f6878ca4a732ef4890e1ad9d8a369896.jpg" alt="imagem"><br><br>  N√£o citei dezenas de outras lo√ß√µes, m√©todos, hacks, estudos sobre como exibir o interior da rede.  Essas ferramentas funcionam?  Eles ajudam voc√™ a entender rapidamente qual √© o problema e depurar a rede? .. Retire o √∫ltimo percentual?  Bem, algo como isto: <br><br><img width="600" src="https://habrastorage.org/webt/eo/p0/i6/eop0i67mupthvfm86dwn139kkha.jpeg"><br><br>  Voc√™ pode assistir a qualquer concurso no Kaggle.  E uma descri√ß√£o de como as pessoas tomam decis√µes finais.  Chegamos ao modelo 100-500-800 mulenov e funcionou! <br><br>  Claro, eu exagerei.  Mas essas abordagens n√£o d√£o respostas r√°pidas e diretas. <br><br>  Com experi√™ncia suficiente e op√ß√µes diferentes, √© poss√≠vel emitir um veredicto sobre o motivo pelo qual o sistema tomou essa decis√£o.  Mas corrigir o comportamento do sistema ser√° dif√≠cil.  Coloque uma muleta, mova o limite, adicione um conjunto de dados, pegue outra rede de back-end. <br><br><h3>  Terceiro problema </h3><br>  <b>O terceiro problema fundamental</b> √© que as grades n√£o ensinam l√≥gica, mas estat√≠stica.  Estatisticamente esta <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pessoa</a> : <br><br><img src="https://habrastorage.org/webt/wa/lg/6h/walg6hlyvy_cvd7i6oojypaa0lc.png" alt="imagem"><br><br>  Logicamente - n√£o √© muito parecido.  As redes neurais n√£o aprendem algo complicado se n√£o forem for√ßadas.  Eles sempre aprendem os sintomas mais simples.  Tem olhos, nariz, cabe√ßa?  Ent√£o essa cara!  Ou d√™ um exemplo em que os olhos n√£o signifiquem o rosto.  E, novamente, milh√µes de exemplos. <br><br><h3>  H√° muito espa√ßo na parte inferior </h3><br>  Eu diria que s√£o esses tr√™s problemas globais que hoje limitam o desenvolvimento de redes neurais e aprendizado de m√°quina.  E onde esses problemas n√£o estavam limitados j√° √© usado ativamente. <br><br>  <b>Esse √© o fim?</b>  <b>Redes neurais se levantaram?</b> <br><br>  Desconhecido  Mas, √© claro, todo mundo espera que n√£o. <br><br>  Existem muitas abordagens e orienta√ß√µes para solucionar os problemas fundamentais que abordamos acima.  Mas at√© agora, nenhuma dessas abordagens nos permitiu fazer algo fundamentalmente novo, resolver algo que ainda n√£o foi resolvido.  At√© o momento, todos os projetos fundamentais s√£o realizados com base em abordagens est√°veis ‚Äã‚Äã(Tesla) ou permanecem projetos de teste de institutos ou empresas (Google Brain, OpenAI). <br><br>  Grosso modo, a dire√ß√£o principal √© a cria√ß√£o de alguma representa√ß√£o de alto n√≠vel dos dados de entrada.  Em certo sentido, "mem√≥ria".  O exemplo mais simples de mem√≥ria s√£o as v√°rias representa√ß√µes de incorpora√ß√£o de imagens.  Bem, por exemplo, todos os sistemas de reconhecimento de rosto.  A rede aprende a tirar do rosto uma certa id√©ia est√°vel que n√£o depende de rota√ß√£o, ilumina√ß√£o, resolu√ß√£o.  De fato, a rede minimiza a m√©trica de "faces diferentes - distantes" e "id√™nticas - pr√≥ximas". <br><br><img src="https://habrastorage.org/webt/8z/re/sp/8zrespvq6y2unwlyuaeovq3fj58.png"><br><br>  Esse treinamento requer dezenas e centenas de milhares de exemplos.  Mas o resultado traz alguns rudimentos do "One-shot Learning".  Agora n√£o precisamos de centenas de rostos para lembrar de uma pessoa.  Apenas um rosto, e √© isso - vamos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">descobrir</a> ! <br>  S√≥ aqui est√° o problema ... A grade pode aprender apenas objetos bastante simples.  Ao tentar distinguir n√£o rostos, mas, por exemplo, ‚Äúpessoas de roupas‚Äù (a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tarefa de re-identifica√ß√£o</a> ), a qualidade falha em muitas ordens de magnitude.  E a rede n√£o pode mais aprender mudan√ßas de √¢ngulo √≥bvias o suficiente. <br><br>  E aprender com milh√µes de exemplos tamb√©m √©, de alguma forma, algo t√£o divertido. <br><br>  H√° trabalho para reduzir significativamente a elei√ß√£o.  Por exemplo, voc√™ pode recordar imediatamente um dos primeiros trabalhos do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Google OneShot</a> <b>Learning</b> : <br><br><img src="https://habrastorage.org/webt/dv/h5/zt/dvh5zte1ggsunwya3tm40zqwiec.png"><br><br>  Existem muitos desses trabalhos, por exemplo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">1</a> ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">2</a> ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">3</a> . <br><br>  H√° um ponto negativo - geralmente o treinamento funciona bem em alguns exemplos simples de "MNIST'ovskie".  E na transi√ß√£o para tarefas complexas - voc√™ precisa de uma grande base, um modelo de objetos ou algum tipo de m√°gica. <br>  Em geral, o trabalho no treinamento One-Shot √© um t√≥pico muito interessante.  Voc√™ encontra muitas id√©ias.  Mas, na maioria das vezes, os dois problemas que listei (pr√©-treinamento em um enorme conjunto de dados / instabilidade em dados complexos) s√£o muito prejudiciais ao aprendizado. <br><br>  Por outro lado, a GAN - redes genericamente competitivas - aborda a incorpora√ß√£o.  Voc√™ provavelmente leu v√°rios artigos sobre esse t√≥pico no Habr√©.  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">1</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">2</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">3</a> ) <br>  Um recurso do GAN √© a forma√ß√£o de algum espa√ßo de estado interno (essencialmente a mesma incorpora√ß√£o), que permite desenhar uma imagem.  Pode ser <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pessoas</a> , pode haver <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">a√ß√µes</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e25/77f/f6d/e2577ff6d435046c44f861ab5d3ce2b3.jpg" alt="imagem"><br><br>  O problema da GAN √© que, quanto mais complexo √© o objeto gerado, mais dif√≠cil √© descrev√™-lo na l√≥gica "discriminador de gerador".  Como resultado, das aplica√ß√µes reais do GAN, que s√£o ouvidas apenas pelo DeepFake, que novamente manipula as representa√ß√µes dos indiv√≠duos (para os quais existe uma base enorme). <br><br>  Encontrei muito poucas outras aplica√ß√µes √∫teis.  Geralmente algum tipo de apito falso com desenhos. <br><br>  E de novo  Ningu√©m entende como isso nos permitir√° avan√ßar em dire√ß√£o a um futuro melhor.  Representar a l√≥gica / espa√ßo em uma rede neural √© bom.  Mas precisamos de um grande n√∫mero de exemplos, n√£o entendemos como esse neur√¥nio representa em si, n√£o entendemos como fazer com que o neur√¥nio se lembre de uma ideia realmente complicada. <br><br>  <b>O aprendizado por refor√ßo</b> √© uma abordagem completamente diferente.  Voc√™ certamente se lembra de como o Google venceu todo mundo no Go.  Vit√≥rias recentes em Starcraft e Dota.  Mas aqui tudo est√° longe de ser t√£o otimista e promissor.  A melhor coisa sobre RL e sua complexidade √© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">este artigo</a> . <br><br>  Para resumir brevemente o que o autor escreveu: <br><br><ul><li>  Modelos prontos para uso n√£o se encaixam / funcionam mal na maioria dos casos </li><li>  Tarefas pr√°ticas s√£o mais f√°ceis de resolver de outras maneiras.  O Boston Dynamics n√£o usa RL devido √† sua complexidade / imprevisibilidade / complexidade computacional </li><li>  Para o RL funcionar, voc√™ precisa de uma fun√ß√£o complexa.  Muitas vezes √© dif√≠cil criar / escrever. </li><li>  √â dif√≠cil treinar modelos.  Temos que gastar muito tempo para balan√ßar e sair das √≥timas localidades </li><li>  Como resultado, √© dif√≠cil repetir o modelo, a instabilidade do modelo com a menor mudan√ßa </li><li>  Muitas vezes, √© preenchido demais em alguns padr√µes √† esquerda, at√© o gerador de n√∫meros aleat√≥rios </li></ul><br>  O ponto principal √© que a RL ainda n√£o funciona na produ√ß√£o.  O Google tem algum tipo de experimento ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">1</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">2</a> ).  Mas eu n√£o vi um √∫nico sistema de compras. <br><br>  <b>Mem√≥ria</b>  A desvantagem de tudo o que √© descrito acima n√£o √© estruturada.  Uma abordagem para tentar arrumar tudo isso √© fornecer √† rede neural acesso a uma mem√≥ria separada.  Para que ela possa gravar e reescrever os resultados de suas etapas l√°.  Ent√£o a rede neural pode ser determinada pelo estado atual da mem√≥ria.  Isso √© muito semelhante aos processadores e computadores cl√°ssicos. <br><br>  O artigo mais famoso e popular √© do DeepMind: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b66/f0d/3a9/b66f0d3a9da550ad89e677eb4453a6bf.png" alt="imagem"><br><br>  Parece que aqui est√° a chave para entender a intelig√™ncia?  Mas sim, n√£o.  O sistema ainda precisa de uma enorme quantidade de dados para treinamento.  E funciona principalmente com dados tabulares estruturados.  Ao mesmo tempo, quando o Facebook <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">resolveu um</a> problema semelhante, eles seguiram o caminho ‚Äúveem a mem√≥ria, apenas complicamos o neur√¥nio, mas h√° mais exemplos - e ele aprender√° a si pr√≥prio‚Äù. <br><br>  <b>Desembara√ßo</b> .  Outra maneira de criar mem√≥ria significativa √© realizar as mesmas incorpora√ß√µes, mas ao aprender a introduzir crit√©rios adicionais que lhes permitam destacar "significados" neles.  Por exemplo, queremos treinar uma rede neural para distinguir entre o comportamento de uma pessoa em uma loja.  Se segu√≠ssemos o caminho padr√£o, ter√≠amos que fazer uma d√∫zia de redes.  Um est√° procurando uma pessoa, o segundo determina o que est√° fazendo, o terceiro √© a idade, o quarto √© o sexo.  A l√≥gica separada examina a parte da loja onde ele faz / aprende.  O terceiro determina sua trajet√≥ria, etc. <br><br>  Ou, se houvesse infinitos dados, seria poss√≠vel treinar uma rede para todos os tipos de resultados (√© √≥bvio que esse conjunto de dados n√£o pode ser digitado). <br><br>  A abordagem de desdentamento nos diz - e vamos treinar a rede para que ela mesma possa distinguir entre conceitos.  Para que ela forme uma incorpora√ß√£o no v√≠deo, onde uma √°rea determinaria a a√ß√£o, uma - a posi√ß√£o no ch√£o no tempo, uma - a altura da pessoa e outra - seu g√™nero.  Ao mesmo tempo, durante o treinamento, quase nunca gostaria de sugerir conceitos-chave para a rede, mas para que ela pr√≥pria identifique e agrupe √°reas.  Existem poucos artigos (alguns s√£o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">1</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">2</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">3</a> ) e, em geral, s√£o bastante te√≥ricos. <br><br>  Mas essa dire√ß√£o, pelo menos teoricamente, deve cobrir os problemas listados no come√ßo. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/314/bd2/7ab/314bd27abc46e22c8c64430c6b6b9211.gif" alt="imagem"><br><br>  Decomposi√ß√£o da imagem de acordo com os par√¢metros ‚Äúcor da parede / cor do piso / forma do objeto / cor do objeto / etc.‚Äù <br><br><img src="https://habrastorage.org/webt/km/md/fb/kmmdfbtnliixqj3d5szfofcqe7q.jpeg"><br><br>  Decomposi√ß√£o da face de acordo com os par√¢metros ‚Äútamanho, sobrancelhas, orienta√ß√£o, cor da pele, etc.‚Äù <br><br><h3>  Outros </h3><br>  Existem muitas outras dire√ß√µes n√£o t√£o globais que nos permitem reduzir a base, trabalhar com dados mais heterog√™neos etc. <br><br>  <b>Aten√ß√£o</b> .  Provavelmente n√£o faz sentido isolar isso como um m√©todo separado.  Apenas uma abordagem que refor√ßa os outros.  Muitos artigos foram dedicados a ele ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">1</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">2</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">3</a> ).  O significado de Aten√ß√£o √© fortalecer a resposta da rede a objetos significativos durante o treinamento.  Geralmente, por alguma designa√ß√£o de destino externo ou por uma pequena rede externa. <br><br>  <b>Simula√ß√£o 3D</b> .  Se voc√™ cria um bom mecanismo 3D, geralmente pode fechar 90% dos dados de treinamento (at√© vi um exemplo em que quase 99% dos dados foram fechados com um bom mecanismo).  Existem muitas id√©ias e truques sobre como fazer uma rede treinada em um mecanismo 3D funcionar com dados reais (ajuste fino, transfer√™ncia de estilo etc.).  Mas muitas vezes criar um bom mecanismo √© v√°rias ordens de magnitude mais dif√≠ceis do que coletar dados.  Exemplos ao fabricar motores: <br>  Treinamento de rob√¥s ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">google</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">braingarden</a> ) <br>  Aprendendo a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">reconhecer</a> mercadorias em uma loja (mas em dois projetos que realizamos, dispensamos isso com calma). <br>  Treinamento na Tesla (novamente, o v√≠deo acima). <br><br><h2>  Conclus√µes </h2><br>  O artigo inteiro √©, em certo sentido, conclus√µes.  Provavelmente a principal mensagem que eu queria fazer era "o brinde acabou, os neur√¥nios n√£o d√£o solu√ß√µes mais simples".  Agora temos que trabalhar duro para criar solu√ß√µes complexas.  Ou trabalhe duro fazendo relat√≥rios cient√≠ficos complexos. <br><br>  Em geral, o t√≥pico √© discut√≠vel.  Talvez os leitores tenham exemplos mais interessantes? </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt455676/">https://habr.com/ru/post/pt455676/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt455658/index.html">Lend√°rio Intel Core i7-2600K: testando Sandy Bridge em 2019 (parte 3)</a></li>
<li><a href="../pt455662/index.html">Grande display mec√¢nico com mecanismo de came como decodificador</a></li>
<li><a href="../pt455666/index.html">Construindo vendas de sa√≠da em uma empresa de servi√ßos de TI</a></li>
<li><a href="../pt455668/index.html">Escrevemos em FPGA sem HDL. Compara√ß√£o de ferramentas de desenvolvimento de alto n√≠vel</a></li>
<li><a href="../pt455670/index.html">Como as impressoras 3D imprimem ossos, vasos sangu√≠neos e √≥rg√£os</a></li>
<li><a href="../pt455678/index.html">No caminho de Sergey Pavlovich Korolev. Projeto tripulado russo moderno. Parte 1. "Federa√ß√£o"</a></li>
<li><a href="../pt455682/index.html">Quanto voc√™ gasta em infraestrutura? E como economizar nisso?</a></li>
<li><a href="../pt455684/index.html">Por que realizamos um hackathon para testadores</a></li>
<li><a href="../pt455686/index.html">Como escolher a melhor ferramenta de gerenciamento de projetos se voc√™ √© milenar?</a></li>
<li><a href="../pt455692/index.html">ASZP: restyling ou teatro come√ßa com um cabide</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>