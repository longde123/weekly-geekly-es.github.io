<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë∞üèº ‚òîÔ∏è ‚öóÔ∏è Desenvolvimento de banco de dados no Dropbox. O caminho de um banco de dados MySQL global para milhares de servidores üï∑Ô∏è ‚è≠Ô∏è ü§∑üèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Quando o Dropbox come√ßou, um usu√°rio do Hacker News comentou que ele poderia ser implementado com v√°rios scripts bash usando FTP e Git. Agora, isso n√£...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Desenvolvimento de banco de dados no Dropbox. O caminho de um banco de dados MySQL global para milhares de servidores</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/417315/">  Quando o Dropbox come√ßou, um usu√°rio do Hacker News comentou que ele poderia ser implementado com v√°rios scripts bash usando FTP e Git.  Agora, isso n√£o pode ser dito de forma alguma: trata-se de um grande armazenamento de arquivos na nuvem com bilh√µes de novos arquivos todos os dias, que n√£o s√£o apenas armazenados de alguma forma no banco de dados, mas de maneira que qualquer banco de dados possa ser restaurado em qualquer ponto nos √∫ltimos seis dias. <br><br>  Sob o corte, a transcri√ß√£o do relat√≥rio de <strong>Glory Bakhmutov</strong> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">m0sth8</a> ) no Highload ++ 2017, sobre como os bancos de dados no Dropbox se desenvolveram e como est√£o organizados agora. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/hUFFsLoCRNU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <strong>Sobre o palestrante:</strong> Gl√≥ria a Bakhmutov - engenheiro de confiabilidade do site da equipe do Dropbox, ama muito o Go e √†s vezes aparece no podcast golangshow.com. <br><br><h2>  Conte√∫do <br></h2><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Brevemente sobre a arquitetura do Dropbox</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">A hist√≥ria do desenvolvimento</a> de bancos de dados e como funciona a arquitetura atual do Dropbox </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">As opera√ß√µes mais simples em bancos de dados</a> (amantes, backups, clones, promo√ß√µes) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Automa√ß√£o</a> - que gerencia todos os bancos de dados e executa opera√ß√µes </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Monitoramento</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Teste, teste e DRT</a> </li></ul><br><img src="https://habrastorage.org/webt/dh/gt/p7/dhgtp7pgu4eyl6rc3ncfnaf9s3i.jpeg"><br><a name="habracut"></a><br><a name="dropbox_architecture"></a><h2>  Arquitetura do Dropbox em linguagem simples </h2><br>  O Dropbox apareceu em 2008.  Este √© essencialmente um armazenamento de arquivos na nuvem.  Quando o Dropbox come√ßou, um usu√°rio do Hacker News comentou que ele poderia ser implementado com v√°rios scripts bash usando FTP e Git.  Mas, no entanto, o Dropbox est√° em desenvolvimento e agora √© um servi√ßo bastante grande, com mais de 1,5 bilh√£o de usu√°rios, 200 mil empresas e um grande n√∫mero (v√°rios bilh√µes!) De novos arquivos todos os dias. <br><br>  <strong>Como √© o Dropbox?</strong> <br><img src="https://habrastorage.org/webt/ed/em/km/edemkm1wqvlbv6jb0qobp5c2dgc.jpeg"><br><br>  Temos v√°rios clientes (interface da web, API para aplicativos que usam o Dropbox, aplicativos de desktop).  Todos esses clientes usam a API e se comunicam com dois grandes servi√ßos que podem ser divididos logicamente em: <br><br><ol><li>  <strong>Metaserver</strong> </li><li>  <strong>Blockserver</strong> </li></ol><br>  O Metaserver armazena meta-informa√ß√µes sobre o arquivo: tamanho, coment√°rios, links para este arquivo no Dropbox, etc.  O Blockserver armazena apenas informa√ß√µes sobre arquivos: pastas, caminhos, etc. <br><br>  <strong>Como isso funciona?</strong> <br><br>  Por exemplo, voc√™ tem um arquivo video.avi com algum tipo de v√≠deo. <br><img src="https://habrastorage.org/webt/kc/i9/op/kci9op0l7f_tecaxetg7uzpzemw.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Link do slide</a></em> <br><br><ul><li>  O cliente divide esse arquivo em v√°rios blocos (neste caso, 4 MB cada), calcula a soma de verifica√ß√£o e envia uma solicita√ß√£o ao Metaserver: "Eu tenho um arquivo * .avi, quero carreg√°-lo, os valores de hash s√£o assim e assim". </li><li>  O Metaserver retorna a resposta: "Eu n√£o tenho esses blocos, vamos fazer o download!"  Ou ele pode responder que possui todos ou alguns dos blocos e apenas os demais precisam ser carregados. </li></ul><br><img src="https://habrastorage.org/webt/qa/zr/79/qazr79svhai2ouk6v8lta1zcp-u.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Link do slide</a></em> <br><br><ul><li>  Depois disso, o cliente vai para o Blockserver, envia a quantidade de hash e o pr√≥prio bloco de dados, que √© armazenado no Blockserver. </li><li>  Blockserver confirma a opera√ß√£o. </li></ul><br><img src="https://habrastorage.org/webt/kl/dx/xw/kldxxw1ncgj4ytt1pqq5bh95iue.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Link do slide</a></em> <br><br>  Obviamente, este √© um esquema muito simplificado, o protocolo √© muito mais complicado: h√° sincroniza√ß√£o entre clientes na mesma rede, h√° drivers de kernel, a capacidade de resolver colis√µes etc.  Este √© um protocolo bastante complexo, mas funciona assim esquematicamente. <br><img src="https://habrastorage.org/webt/ev/-x/_h/ev-x_hwfhwpfixld61yqiraelc4.jpeg"><br><br>  Quando um cliente salva algo no Metaserver, todas as informa√ß√µes v√£o para o MySQL.  O Blockserver tamb√©m armazena informa√ß√µes sobre os arquivos, como eles s√£o estruturados, em que blocos eles consistem, no MySQL.  O Blockserver tamb√©m armazena os pr√≥prios blocos no Block Storage, que, por sua vez, armazena informa√ß√µes sobre onde fica o bloco, em qual servidor e como √© processado, tamb√©m no MYSQL. <br><br><blockquote>  Para armazenar exabytes de arquivos do usu√°rio, armazenamos simultaneamente informa√ß√µes adicionais em um banco de dados de v√°rias dezenas de petabytes espalhados por 6 mil servidores. </blockquote><br><a name="history_development"></a><h2>  Hist√≥rico de Desenvolvimento de Banco de Dados </h2><br>  Como os bancos de dados evolu√≠ram no Dropbox? <br><img src="https://habrastorage.org/webt/oe/w9/ok/oew9okiqdeivyhvhycznly7kvbe.jpeg"><br><br>  Em 2008, tudo come√ßou com um Metaserver e um banco de dados global.  Todas as informa√ß√µes que o Dropbox precisava ser armazenado em algum lugar, ele salvou no √∫nico MySQL global.  Isso n√£o durou muito, porque o n√∫mero de usu√°rios cresceu e os bancos de dados e tablets individuais dentro dos bancos de dados aumentaram mais rapidamente do que outros. <br><img src="https://habrastorage.org/webt/ry/lt/h9/rylth906zfbbcou6nz7ve_yqib8.jpeg"><br><br>  Portanto, em 2011, v√°rias tabelas foram enviadas para servidores separados: <br><br><ul><li>  <strong>Usu√°rio</strong> , com informa√ß√µes sobre usu√°rios, por exemplo, logins e tokens oAuth; </li><li>  <strong>Host</strong> , com informa√ß√µes de arquivo do Blockserver; </li><li>  <strong>Misc</strong> , que n√£o estava envolvido no processamento de solicita√ß√µes de produ√ß√£o, mas foi usado para fun√ß√µes de utilidade, como tarefas em lote. </li></ul><br><img src="https://habrastorage.org/webt/ja/ec/ja/jaecja2eklv8znsqhf5lt-dyez8.jpeg"><br><br>  Mas depois de 2012, o Dropbox come√ßou a crescer muito, desde ent√£o crescemos <strong>cerca de 100 milh√µes de usu√°rios por ano</strong> . <br><img src="https://habrastorage.org/webt/ie/cr/-s/iecr-syyi6qprj2zj45qx4l42k4.jpeg"><br><br>  Era necess√°rio levar em considera√ß√£o um crescimento t√£o grande e, portanto, no final de 2011, t√≠nhamos shards - uma base composta por 1.600 shards.  Inicialmente, apenas 8 servidores com 200 shards cada.  Agora s√£o 400 servidores principais com 4 shards em cada um. <br><img src="https://habrastorage.org/webt/b3/v9/vy/b3v9vyjqzwau2kgmadhgb2vg0vo.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Link do slide</a></em> <br><br>  Em 2012, percebemos que criar tabelas e atualiz√°-las no banco de dados para cada l√≥gica de neg√≥cios adicionada √© muito dif√≠cil, sombrio e problem√°tico.  Portanto, em 2012, inventamos nosso pr√≥prio armazenamento gr√°fico, que chamamos de <strong>Edgestore</strong> , e desde ent√£o toda a l√≥gica de neg√≥cios e as metainforma√ß√µes que o aplicativo gera s√£o armazenadas no Edgestore. <br><br>  O Edgestore abstrai essencialmente o MySQL dos clientes.  Os clientes t√™m certas entidades que s√£o interconectadas por links da API do gRPC ao Edgestore Core, que converte esses dados no MySQL e os armazena de alguma forma l√° (basicamente, fornece tudo isso a partir do cache). <br><img src="https://habrastorage.org/webt/bj/s7/dz/bjs7dz7-cdsvjblcgftjdeybrgk.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Link do slide</a></em> <br><br>  <strong>Em 2015, sa√≠mos do Amazon S3</strong> , desenvolvemos nosso pr√≥prio armazenamento em nuvem chamado Magic Pocket.  Ele cont√©m informa√ß√µes sobre onde um arquivo de bloco est√° localizado, em qual servidor, sobre os movimentos desses blocos entre servidores, armazenados no MySQL. <br><img src="https://habrastorage.org/webt/f_/bz/lm/f_bzlm3tk9e3lwqo64x4kfuhhok.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Link do slide</a></em> <br><br>  Mas o MySQL √© usado de uma maneira muito complicada - em ess√™ncia, como uma grande tabela de hash distribu√≠da.  Essa √© uma carga muito diferente, principalmente na leitura de registros aleat√≥rios.  90% da utiliza√ß√£o √© de E / S. <br><br><h2>  Arquitetura de banco de dados </h2><br>  Primeiro, identificamos imediatamente alguns princ√≠pios pelos quais constru√≠mos a arquitetura do nosso banco de dados: <br><br><ol><li>  <strong>Confiabilidade e durabilidade</strong> .  Este √© o princ√≠pio mais importante e o que os clientes esperam de n√≥s - os dados n√£o devem ser perdidos. </li><li>  <strong>A otimiza√ß√£o da solu√ß√£o</strong> √© um princ√≠pio igualmente importante.  Por exemplo, os backups devem ser feitos rapidamente e restaurados rapidamente tamb√©m. </li><li>  <strong>Simplicidade da solu√ß√£o</strong> - arquiteturalmente e em termos de servi√ßo e suporte adicional ao desenvolvimento. </li><li>  <strong>Custo de propriedade</strong> .  Se algo otimiza a solu√ß√£o, mas √© muito caro, isso n√£o nos conv√©m.  Por exemplo, um escravo que est√° um dia atr√°s do mestre √© muito conveniente para backups, mas voc√™ precisa adicionar mais 1.000 a 6.000 servidores - o custo de propriedade desse escravo √© muito alto. </li></ol><br>  Todos os princ√≠pios devem ser <strong>verific√°veis ‚Äã‚Äãe mensur√°veis</strong> , ou seja, eles devem ter m√©tricas.  Se estivermos falando sobre o custo de propriedade, precisamos calcular quantos servidores temos, por exemplo, para bancos de dados, quantos servidores v√£o para backups e quanto custa para o Dropbox no final.  Quando escolhemos uma nova solu√ß√£o, contamos todas as m√©tricas e focamos nelas.  Ao escolher qualquer solu√ß√£o, somos totalmente guiados por esses princ√≠pios. <br><br><h2>  Topologia de base </h2><br>  O banco de dados est√° estruturado da seguinte maneira: <br><br><ul><li>  No data center principal, temos um mestre, no qual todos os registros ocorrem. </li><li>  O servidor mestre possui dois servidores escravos nos quais ocorre a replica√ß√£o semisync.  Os servidores geralmente morrem (cerca de 10 por semana), por isso precisamos de dois servidores escravos. </li><li>  Servidores escravos est√£o em clusters separados.  Clusters s√£o salas completamente separadas no datacenter que n√£o est√£o conectadas entre si.  Se um quarto queimar, o segundo permanece completamente funcionando. </li><li>  Tamb√©m em outro data center, temos o chamado pseudo mestre (mestre intermedi√°rio), que na verdade √© apenas um escravo, que tem outro escravo. </li></ul><br><img src="https://habrastorage.org/webt/k6/6s/x6/k66sx6siyp6efjxxmfrot21ueha.jpeg"><br><br>  Essa topologia foi escolhida porque, se o primeiro data center morrer repentinamente em n√≥s, no segundo data center, teremos uma <strong>topologia quase completa</strong> .  Simplesmente alteramos todos os endere√ßos no Discovery, e os clientes podem trabalhar. <br><br><h3>  Topologias especializadas </h3><br>  Tamb√©m temos topologias especializadas. <br><br>  A topologia do <strong>Magic Pocket</strong> consiste em um servidor mestre e dois servidores escravos.  Isso √© feito porque o Magic Pocket duplica os dados entre as zonas.  Se ele perder um cluster, poder√° restaurar todos os dados de outras zonas atrav√©s do c√≥digo de apagamento. <br><img src="https://habrastorage.org/webt/gk/o7/bi/gko7bifb-4ted4cvwmzgyn5lasw.jpeg"><br><br>  A topologia <strong>ativo-ativo</strong> √© a topologia customizada usada pelo Edgestore.  Ele tem um mestre e dois escravos em cada um dos dois data centers, e eles s√£o escravos um do outro.  Esse √© um <strong>esquema</strong> muito <strong>perigoso</strong> , mas o Edgestore, em seu n√≠vel, sabe exatamente quais dados em qual mestre e em qual faixa ele pode gravar.  Portanto, essa topologia n√£o quebra. <br><img src="https://habrastorage.org/webt/xe/nv/wx/xenvwx3ls9ct8fssverq3htck10.jpeg"><br><br><h3>  Inst√¢ncia </h3><br>  Instalamos servidores bastante simples com uma configura√ß√£o de 4-5 anos atr√°s: <br><br><ul><li>  <strong>2x n√∫cleos Xeon 10;</strong> </li><li>  <strong>5 TB (8 SSD Raid 0 *);</strong> </li><li>  <strong>384 GB de mem√≥ria.</strong> </li></ul><br>  * Raid 0 - porque √© mais f√°cil e muito mais r√°pido substituir um servidor inteiro do que unidades. <br><br><h4>  Inst√¢ncia √∫nica </h4><br>  Nesse servidor, temos uma inst√¢ncia grande do MySQL na qual v√°rios shards est√£o localizados.  Essa inst√¢ncia do MySQL se aloca imediatamente quase toda a mem√≥ria.  Outros processos tamb√©m est√£o em execu√ß√£o no servidor: proxy, coleta de estat√≠sticas, logs etc. <br><br><img src="https://habrastorage.org/webt/z8/yd/vw/z8ydvwabte3v8pytwbrl1vi8yc0.jpeg"><br><br>  Esta solu√ß√£o √© boa em que: <br><br>  + √â <strong>f√°cil de gerenciar</strong> .  Se voc√™ precisar substituir a inst√¢ncia do MySQL, basta substituir o servidor. <br><br>  + <strong>Apenas fa√ßa faylovers</strong> . <br><br>  Por outro lado: <br><br>  - √â problem√°tico que qualquer opera√ß√£o ocorra em toda a inst√¢ncia do MySQL e imediatamente em todos os shards.  Por exemplo, se voc√™ precisar fazer backup, fazemos backup de todos os shards de uma s√≥ vez.  Se voc√™ precisa fazer um faylover, fazemos um faylover de todos os quatro fragmentos de uma s√≥ vez.  Consequentemente, a acessibilidade sofre 4 vezes mais. <br><br>  - Problemas com a replica√ß√£o de um shard afetam outros shards.  A replica√ß√£o do MySQL n√£o √© paralela, e todos os shards funcionam em um √∫nico thread.  Se algo acontecer com um fragmento, o resto tamb√©m ser√° v√≠tima. <br><br>  Ent√£o agora estamos mudando para uma topologia diferente. <br><br><h4>  Multi-inst√¢ncia </h4><br><img src="https://habrastorage.org/webt/lg/7x/ks/lg7xks5vbogjaf6slr7tidlc6ty.jpeg"><br><br>  Na nova vers√£o, v√°rias inst√¢ncias do MySQL s√£o iniciadas no servidor ao mesmo tempo, cada uma com um fragmento.  O que √© melhor? <br><br>  + Podemos <strong>realizar opera√ß√µes apenas em um fragmento espec√≠fico</strong> .  Ou seja, se voc√™ precisar de um faylover, troque apenas um fragmento; se precisar de um backup, fazemos backup de apenas um fragmento.  Isso significa que as opera√ß√µes s√£o muito aceleradas - 4 vezes para um servidor de quatro fragmentos. <br><br>  + Os <strong>fragmentos quase n√£o se afetam</strong> . <br><br>  + <strong>Melhoria na replica√ß√£o.</strong>  Podemos misturar diferentes categorias e classes de bancos de dados.  O Edgestore ocupa muito espa√ßo, por exemplo, todos os 4 TB, e o Magic Pocket ocupa apenas 1 TB, mas possui 90% de utiliza√ß√£o.  Ou seja, podemos combinar diferentes categorias que usam recursos de E / S e da m√°quina de maneiras diferentes e iniciar quatro fluxos de replica√ß√£o. <br><br>  Obviamente, esta solu√ß√£o tem suas desvantagens: <br><br>  - O maior ponto negativo √© que √© <strong>muito mais dif√≠cil gerenciar tudo isso</strong> .  Precisamos de um agendador inteligente que entenda para onde ele pode levar essa inst√¢ncia, onde haver√° uma carga ideal. <br><br>  - <strong>Mais dif√≠cil que os failovers</strong> . <br><br>  Portanto, apenas agora estamos nos movendo para essa decis√£o. <br><br><h3>  Descoberta </h3><br>  De alguma forma, os clientes precisam saber como se conectar ao banco de dados desejado, para que tenhamos o Discovery, que deve: <br><br><ol><li>  Notifique o cliente muito rapidamente sobre altera√ß√µes na topologia.  Se mudarmos de mestre e escravo, os clientes devem aprender quase instantaneamente. <br></li><li>  A topologia n√£o deve depender da topologia de replica√ß√£o do MySQL, porque em algumas opera√ß√µes alteramos a topologia do MySQL.  Por exemplo, quando dividimos, na etapa preparat√≥ria do mestre de destino, onde transferiremos parte dos shards, alguns servidores escravos s√£o reconfigurados para esse mestre de destino.  Os clientes n√£o precisam saber disso. <br></li><li>  √â importante que haja atomicidade das opera√ß√µes e verifica√ß√£o do estado.  √â imposs√≠vel que dois servidores diferentes do mesmo banco de dados se tornem mestre no mesmo momento. <br></li></ol><br><h4>  Como a descoberta se desenvolveu </h4><br>  No come√ßo, tudo era simples: o endere√ßo do banco de dados no c√≥digo-fonte na configura√ß√£o.  Quando precis√°vamos atualizar o endere√ßo, tudo era implantado muito rapidamente. <br><img src="https://habrastorage.org/webt/7d/yb/oo/7dyboo0h7eo4o9_9xp-n-6lzosy.jpeg"><br><br>  Infelizmente, isso n√£o funciona se houver muitos servidores. <br><img src="https://habrastorage.org/webt/3u/26/pf/3u26pfl_s796zdaoix-zdplb9du.jpeg"><br><br>  Acima est√° a primeira descoberta que temos.  Havia scripts de banco de dados que alteravam a placa de identifica√ß√£o no ConfigDB - era uma placa de identifica√ß√£o MySQL separada, e os clientes j√° ouviam esse banco de dados e periodicamente coletavam dados de l√°. <br><img src="https://habrastorage.org/webt/ml/qn/mh/mlqnmhmmteylazgl4itjokes_mu.jpeg"><br><br>  A tabela √© muito simples, existe uma categoria de banco de dados, uma chave de fragmento, um mestre / escravo de classe de banco de dados, proxy e um endere√ßo de banco de dados.  De fato, o cliente solicitou uma categoria, uma classe de banco de dados, uma chave de fragmento e o endere√ßo do MySQL foi retornado para o qual ele j√° podia estabelecer uma conex√£o. <br><img src="https://habrastorage.org/webt/vb/ht/en/vbhteniyw4x7s56a1xwckuz268e.jpeg"><br><br>  Assim que havia muitos servidores, o Memcache foi adicionado e os clientes come√ßaram a se comunicar com ele. <br><br>  Mas ent√£o n√≥s refizemos isso.  Os scripts do MySQL come√ßaram a se comunicar atrav√©s do gRPC, atrav√©s de um thin client com um servi√ßo que chamamos de RegisterService.  Quando ocorreram algumas altera√ß√µes, o RegisterService tinha uma fila e ele entendeu como aplicar essas altera√ß√µes.  RegisterService salvou dados no AFS.  O AFS √© o nosso sistema interno baseado no ZooKeeper. <br><img src="https://habrastorage.org/webt/mi/lm/yz/milmyzvyvayuv2av8pah4neh9zq.jpeg"><br><br>  A segunda solu√ß√£o, que n√£o √© mostrada aqui, usava o ZooKeeper diretamente e isso criava problemas porque cada shard era um n√≥ no ZooKeeper.  Por exemplo, 100 mil clientes se conectam ao ZooKeeper; se eles morrerem repentinamente devido a algum tipo de bug, 100 mil solicita√ß√µes ao ZooKeeper ser√£o recebidas imediatamente, o que simplesmente o eliminar√° e n√£o poder√° subir. <br><br>  Portanto, <strong>o sistema AFS</strong> foi desenvolvido <strong>, usado por todo o Dropbox</strong> .  De fato, ele abstrai o trabalho com o ZooKeeper para todos os clientes.  O daemon AFS √© executado localmente em cada servidor e fornece uma API de arquivo muito simples do formul√°rio: crie um arquivo, exclua um arquivo, solicite um arquivo, receba uma notifica√ß√£o de altera√ß√£o de arquivo e compare e troque opera√ß√µes.  Ou seja, voc√™ pode tentar substituir o arquivo por alguma vers√£o e, se essa vers√£o tiver sido alterada durante a altera√ß√£o, a opera√ß√£o ser√° cancelada. <br><br>  Essencialmente, essa abstra√ß√£o no ZooKeeper, na qual existe um algoritmo local de backoff e jitter.  O ZooKeeper n√£o trava mais sob carga.  Com o AFS, fazemos backups no S3 e no GIT, e o pr√≥prio AFS local notifica os clientes que os dados foram alterados. <br><img src="https://habrastorage.org/webt/ry/_0/wv/ry_0wvkyux23gicrlwmfceq0eoe.jpeg"><br><br>  No AFS, os dados s√£o armazenados como arquivos, ou seja, √© uma API do sistema de arquivos.  Por exemplo, o acima √© o arquivo shard.slave_proxy - o maior, leva cerca de 28 Kb, e quando alteramos a categoria da classe shard e slave_proxy, todos os clientes que assinam esse arquivo recebem uma notifica√ß√£o.  Eles releram este arquivo, que cont√©m todas as informa√ß√µes necess√°rias.  Usando a chave shard, eles obt√™m uma categoria e reconfiguram o conjunto de conex√µes com o banco de dados. <br><br><a name="perations_databases"></a><h2>  Opera√ß√µes </h2><br>  Utilizamos opera√ß√µes muito simples: promo√ß√£o, clone, backups / recupera√ß√£o. <br><img src="https://habrastorage.org/webt/q3/5c/df/q35cdfiso51bhhiitqja71qbysc.jpeg"><br><br>  <strong>Uma opera√ß√£o √© uma m√°quina de estado simples</strong> .  Quando entramos na opera√ß√£o, realizamos algumas verifica√ß√µes, por exemplo, verifica√ß√£o de rota√ß√£o, que v√°rias vezes por tempo limite, verificam se podemos executar essa opera√ß√£o.  Depois disso, realizamos algumas a√ß√µes preparat√≥rias que n√£o afetam os sistemas externos.  Em seguida, a pr√≥pria opera√ß√£o. <br><br>  Todas as etapas em uma opera√ß√£o t√™m uma <strong>etapa de revers√£o</strong> (desfazer).  Se houver um problema com a opera√ß√£o, a opera√ß√£o tentar√° restaurar o sistema para sua posi√ß√£o original.  Se tudo estiver bem, a limpeza ocorre e a opera√ß√£o √© conclu√≠da. <br><br>  Temos uma m√°quina de estado t√£o simples para qualquer opera√ß√£o. <br><br><h4>  <strong>Promo√ß√£o (mudan√ßa de mestre)</strong> </h4><br>  Esta √© uma opera√ß√£o muito comum no banco de dados.  Havia perguntas sobre como fazer altera√ß√µes em um servidor mestre quente que funcione - ele ter√° uma aposta.  S√≥ que todas essas opera√ß√µes s√£o executadas em servidores escravos e, em seguida, as mudan√ßas de escravos nos locais principais.  Portanto, a <strong>opera√ß√£o de promo√ß√£o √© muito frequente</strong> . <br><img src="https://habrastorage.org/webt/xx/79/jv/xx79jvszxb9wjffwqf4ld_euofo.jpeg"><br><br>  Precisamos atualizar o kernel - trocamos, precisamos atualizar a vers√£o do MySQL - atualizamos no slave, mudamos para master, atualizamos l√°. <br><img src="https://habrastorage.org/webt/wc/op/o9/wcopo9jlmz9aeoynpv-hbbseois.jpeg"><br><br>  Conseguimos uma promo√ß√£o muito r√°pida.  Por exemplo, <strong>para quatro fragmentos, agora temos promo√ß√£o por cerca de 10 a 15 s.</strong>  O gr√°fico acima mostra que, com a disponibilidade da promo√ß√£o, sofreu 0,0003%. <br><br>  Mas a promo√ß√£o normal n√£o √© t√£o interessante, porque essas s√£o opera√ß√µes comuns que s√£o realizadas todos os dias.  Failovers s√£o interessantes. <br><br><h4>  <strong>Failover (substitui√ß√£o de um mestre danificado)</strong> <br></h4><br>  Um failover significa que o banco de dados est√° morto. <br><br><ul><li>  Se o servidor realmente morreu, este √© apenas um caso ideal. </li><li>  De fato, acontece que os servidores est√£o parcialmente ativos. </li><li>  √Äs vezes, o servidor morre muito lentamente.  Os controladores RAID, o sistema de disco falham, alguns pedidos retornam respostas, mas alguns fluxos s√£o bloqueados e n√£o retornam respostas. </li><li>  Acontece que o mestre est√° simplesmente sobrecarregado e n√£o responde ao nosso exame de sa√∫de.  Mas se fizermos promo√ß√£o, o novo mestre tamb√©m ficar√° sobrecarregado e s√≥ piorar√°. </li></ul><br>  A substitui√ß√£o dos servidores principais falecidos ocorre cerca de <strong>2-3 vezes por dia</strong> ; esse √© um processo totalmente automatizado, sem necessidade de interven√ß√£o humana.  A se√ß√£o cr√≠tica leva cerca de 30 segundos e possui v√°rias verifica√ß√µes adicionais para verificar se o servidor est√° realmente ativo ou se ele j√° morreu. <br><br>  Abaixo est√° um exemplo de diagrama de como o faylover funciona. <br><img src="https://habrastorage.org/webt/ks/5d/6o/ks5d6oovtnchnmlr2zlgpwvmt5g.jpeg"><br><br>  Na se√ß√£o selecionada, <strong>reinicializamos o servidor principal</strong> .  Isso √© necess√°rio porque temos o MySQL 5.6 e nele a replica√ß√£o semi-sincronizada n√£o √© sem perdas.  Portanto, leituras fantasmas s√£o poss√≠veis, e precisamos desse mestre, mesmo que ele n√£o tenha morrido, mate o mais r√°pido poss√≠vel para que os clientes se desconectem.  Portanto, fazemos uma reinicializa√ß√£o total via Ipmi - esta √© a primeira opera√ß√£o mais importante que devemos realizar.  Na vers√£o MySQL 5.7, isso n√£o √© t√£o cr√≠tico. <br><br>  <strong>Sincroniza√ß√£o de cluster.</strong>  Por que precisamos de sincroniza√ß√£o de cluster? <br><img src="https://habrastorage.org/webt/gh/pa/go/ghpago_p12c1jittnig4rwhc1sg.jpeg"><br><br>  Se recordarmos a figura anterior com nossa topologia, um servidor mestre possui tr√™s servidores escravos: dois em um datacenter, um no outro.  Com a promo√ß√£o, precisamos que o mestre esteja no mesmo data center principal.  Mas, √†s vezes, quando os escravos s√£o carregados, com semisync, acontece que um escravo semisync se torna escravo em outro datacenter, porque n√£o est√° carregado.  Portanto, primeiro precisamos sincronizar o cluster inteiro e, em seguida, j√° promover o escravo no datacenter de que precisamos.  Isso √© feito de maneira muito simples: <br><br><ul><li>  Paramos todo o encadeamento de E / S em todos os servidores escravos. </li><li>  Depois disso, j√° sabemos ao certo que o mestre √© "somente leitura", pois o semisync foi desconectado e ningu√©m mais pode escrever nada l√°. </li><li>  Em seguida, selecionamos o escravo com o maior conjunto GTID recuperado / executado, ou seja, com a maior transa√ß√£o que ele baixou ou j√° aplicou. </li><li>  Reconfiguramos todos os servidores escravos para esse escravo selecionado, iniciamos o encadeamento de E / S e eles s√£o sincronizados. </li><li>  Esperamos at√© que sejam sincronizados, ap√≥s o que todo o cluster se torna sincronizado.   ,     executed GTID set       . </li></ul><br>    ‚Äî <strong> </strong> .   <strong>promotion</strong> ,    : <br><img src="https://habrastorage.org/webt/bd/s-/b8/bds-b8fbieqxhtc4dy9ookntiri.jpeg"><br><br><ul><li>    slave    -,  ,   master,     promotion. </li><li>    slave-   master,   ,  ACLs,  ,  - proxy, , - . </li><li>      read_only = 0,   ,    master  ,   .        master     . </li><li>       - .     -    ,  ,   ,    , ,  proxy  . </li><li>     . </li></ul><br>   ,       rollback   ,   .       rollback  reboot.   ,    , ,  ‚Äî change master ‚Äî    master   . <br><br><h4> <strong></strong> </h4><br>  ‚Äî      .   ,    ,   ,    ,    . <br><br> <strong> </strong> <br><br> ‚óè   slave <br><br>   ,       slave-,      .   . <br><br> ‚óè       <br><br>     ,     ,      .             . <br><br> ‚óè       <br><br>  ,     ,      .          .      3  . <br><br><blockquote>    ,   ,   ,     : <br><br><ol><li>      .       1  40 . <br></li><li>            . <br></li></ol></blockquote><br>    ,     .   1   40 ,      ,      ,     . <br><br><h4> <strong></strong> </h4><br>    ,  .           .     4  . <br><img src="https://habrastorage.org/webt/bv/-k/_z/bv-k_znrl7zi2obmotthihjogyo.jpeg"><br><br><ul><li>    <strong> 24 </strong> .         HDFS,      . </li><li> <strong> 6 </strong>     unsharded databases,        Global DB.      , ,  ,     . </li><li> <strong> 3 </strong>          S3. </li><li> <strong> 3 </strong>     S3     . </li></ul><br><img src="https://habrastorage.org/webt/e1/yx/3s/e1yx3s-1ikyhxnuzeympjsvem14.jpeg"><br><br>       . ,    3 ,   HDFS     3 ,   6   S3.     . <br><br>  ,   . <br><img src="https://habrastorage.org/webt/sn/hz/1l/snhz1lmio2naq40wziys-jggaaw.jpeg"><br><br>         ,      ,   .       ,   ,    recovery  -   .  ,      ,  -       .      100  ,   . <br><br>     ,    ,    ,    ,   ,     ,  ,     .        . <br><br><h5>   </h5><br><img src="https://habrastorage.org/webt/4m/4b/kb/4m4bkboro5zunrljkwxbybj7jsi.jpeg"><br><br>     hot-,      Percona xtrabackup.     ‚Äîstream=xbstream,        ,   .     script-splitter,        ,      . <br><br> MySQL              2x.     3 , ,   ,    1 500 .     ,      ,    HDFS   S3. <br><br>        . <br><img src="https://habrastorage.org/webt/j3/il/jm/j3iljma8c0rekqweak5ngqvaxkk.jpeg"><br><br>  ,    ,    HDFS   S3,    , splitter       xtrabackup,      .   crash-recovery. <br><br>      hot   ,  crash-recovery    .         ,    .     binlog,      master. <br><br> <strong>   binlogs?</strong> <br><br>     binlog'.    master ,    4 ,   100 ,    HDFS. <br><br>      :   Binlog Backuper,         . ,  ,   binlog       HDFS. <br><img src="https://habrastorage.org/webt/on/o3/ce/ono3cesuissuuwuzcglcfautfjo.jpeg"><br><br> ,       4   ,    5 ,    ,    ,    .    HDFS   S3    . <br><br><h5>   </h5><br>      . <br><br>   : <br><br><ol><li>        ‚Äî  10 ,  45  ‚Äî   . <br></li><li>      ,       scheduler  multi instance      slave  master    . <br></li><li>    ‚Äî      ,   .  ,     ,    ,    ,     ,  ,    .  pt-table-checksum   ,      . <br></li></ol><br> <strong></strong> ,        : <br><br><ol><li>       1  10 ,      .    crash-recovery,     . <br></li><li>            . <br></li></ol><br><img src="https://habrastorage.org/webt/2m/bd/ar/2mbdarekbku4hsxygdufshzyhnm.jpeg"><br><br>     slave   -,     .    ,      .   . <br><br><h4>  ++ </h4><br>     .       Hardware ,          (HDD)  10 ,       + crash recovery xtrabackup,      . ,         ,    . , ,     ,   ,   HDD  ,    HDFS  . <br><br><h4>  </h4><br>    ,  ‚Äî   : <br><br><ol><li>         ; <br></li><li>       . <br></li></ol><br>  ,     HDFS,       ,   ,       . <br><br><a name="automation"></a><h2>  Automa√ß√£o </h2><br>  ,  6 000      .         ,   ,     ‚Äî : <br><br><ul><li> Auto-replace; </li><li> DBManager; </li><li> Naoru, Wheelhouse </li></ul><br><h3> Auto-replace </h3><br>   ,   ,   ,    ,     ‚Äî ,     -.   ,   . <br><br> <strong>Availability ()</strong> ‚Äî         ,         .      ‚Äî   recovery  ,         . <br><img src="https://habrastorage.org/webt/-k/em/tr/-kemtrpvhgocinlvlmnsuj1eq-s.jpeg"><br><br>    MySQL  ,   heartbeat. Heartbeat ‚Äî   timestamp. <br><img src="https://habrastorage.org/webt/cn/r8/fn/cnr8fn-fpy_ddnthddjem3afr4o.jpeg"><br><br>    ,     , ,  master   read-write.          heartbeat. <br><br>    auto-replace ,    . <br><img src="https://habrastorage.org/webt/3u/r9/sx/3ur9sxfxf8dsxtgsgvjz3ublvye.jpeg"> <em>           <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> ,   91 .</em> <br><br> <strong>  ?</strong> <br><br><ul><li>   ,     heartbeat    . ,     .  heartbeat', ,    heartbeat'  30 . </li><li> A seguir, veja se o n√∫mero deles atende ao valor limite.  Caso contr√°rio, algo est√° errado com o servidor - pois ele n√£o enviou um batimento card√≠aco. </li><li>  Depois disso, fazemos uma verifica√ß√£o inversa, apenas por precau√ß√£o - de repente esses dois servi√ßos morreram, algo est√° com a rede ou o banco de dados global n√£o pode gravar o batimento card√≠aco por algum motivo.  Na verifica√ß√£o inversa, nos conectamos a um banco de dados quebrado e verificamos seu status. </li><li>  Se tudo mais falhar, verificamos se a posi√ß√£o principal est√° progredindo ou n√£o, se h√° registros nela.  Se nada acontecer, esse servidor definitivamente n√£o est√° funcionando. </li><li>  O √∫ltimo passo √© realmente substituir automaticamente. </li></ul><br>  A substitui√ß√£o autom√°tica √© muito conservadora, ele nunca quer fazer muitas opera√ß√µes autom√°ticas. <br><br><ol><li>  Primeiro, verificamos se houve alguma opera√ß√£o de topologia recentemente?  Talvez este servidor tenha sido adicionado e algo ainda n√£o esteja sendo executado. </li><li>  Verificamos se houve alguma substitui√ß√£o no mesmo cluster a qualquer momento. </li><li>  Verifique qual o limite de falhas que temos.  Se tivermos muitos problemas ao mesmo tempo - 10, 20 -, n√£o os resolveremos automaticamente automaticamente, pois podemos interromper inadvertidamente a opera√ß√£o de todos os bancos de dados. </li></ol><br>  Portanto, <strong>resolvemos apenas um problema de cada vez</strong> . <br><br>  Assim, para o servidor escravo, come√ßamos a clonar e simplesmente a removemos da topologia e, se for mestre, lan√ßamos o amante-fey, a chamada promo√ß√£o de emerg√™ncia. <br><br><h3>  DBManager </h3><br>  DBManager √© um servi√ßo para gerenciar nossos bancos de dados.  Tem: <br><br><ul><li>  agendador de tarefas inteligente que sabe exatamente quando iniciar o trabalho; </li><li>  logs e todas as informa√ß√µes: quem, quando e o que foi lan√ßado - essa √© a fonte da verdade; </li><li>  ponto de sincroniza√ß√£o. </li></ul><br><img src="https://habrastorage.org/webt/xx/g6/pi/xxg6pitu-pau9ifyqivpa9wul3e.jpeg"><br><br>  O DBManager √© bastante simples em termos de arquitetura. <br><br><ul><li>  Existem clientes, o DBA que faz algo atrav√©s da interface da Web ou scripts / servi√ßos que criaram o DBA que acessam via gRPC. </li><li>  Existem sistemas externos como Wheelhouse e Naoru, que acessam o DBManager via gRPC. </li><li>  H√° um agendador que entende qual opera√ß√£o, quando e onde ele pode come√ßar. </li><li>  H√° um trabalhador muito est√∫pido que, quando uma opera√ß√£o chega a ele, a inicia, verifica pelo PID.  O trabalhador pode reiniciar, os processos n√£o s√£o interrompidos.  Todos os trabalhadores est√£o localizados o mais pr√≥ximo poss√≠vel dos servidores nos quais as opera√ß√µes ocorrem, de modo que, por exemplo, ao atualizar o ACLS, n√£o precisamos fazer muitas viagens de ida e volta. </li><li>  Em cada host SQL, temos um DBAgent - este √© um servidor RPC.  Quando voc√™ precisar executar alguma opera√ß√£o no servidor, enviamos uma solicita√ß√£o de RPC. </li></ul><br>  Temos uma interface da web para o DBManager, na qual √© poss√≠vel ver as tarefas atualmente em execu√ß√£o, os registros dessas tarefas, quem a iniciou e quando, quais opera√ß√µes foram executadas para o servidor de um banco de dados espec√≠fico etc. <br><img src="https://habrastorage.org/webt/yj/tq/3m/yjtq3mrfimptba1kizre2bwie48.jpeg"><br><br>  Existe uma interface CLI bastante simples onde voc√™ pode executar tarefas e tamb√©m visualiz√°-las em visualiza√ß√µes convenientes. <br><img src="https://habrastorage.org/webt/qi/rc/vp/qircvpuoutswvmcpnuca4wem9cu.jpeg"><br><br><h3>  Remedia√ß√µes </h3><br>  Tamb√©m temos um sistema para responder a problemas.  Quando algo est√° quebrado, por exemplo, a unidade falha ou algum servi√ßo n√£o funciona, o <strong>Naoru</strong> funciona <strong>.</strong>  Este √© o sistema que funciona em todo o Dropbox, todo mundo usa e √© constru√≠do especificamente para tarefas t√£o pequenas.  Eu falei sobre Naoru no meu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">relat√≥rio</a> em 2016. <br><br>  <strong>O Wheelhouse √©</strong> baseado em uma m√°quina de <strong>estado</strong> e foi projetado para processos longos.  Por exemplo, precisamos atualizar o kernel em todo o MySQL em todo o nosso cluster de 6.000 m√°quinas.  A Wheelhouse faz isso claramente - atualiza no servidor escravo, lan√ßa promo√ß√£o, escravo se torna mestre, atualiza no servidor mestre.  Esta opera√ß√£o pode levar um m√™s ou at√© dois. <br><br><a name="monitoring"></a><h2>  Monitoramento </h2><br><img src="https://habrastorage.org/webt/vo/n_/ys/von_ysxtakb7m5ctiadwzewi3dw.jpeg"><br><br>  Isso √© muito importante. <br><br><blockquote>  Se voc√™ n√£o monitorar o sistema, provavelmente n√£o funcionar√°. </blockquote><br>  Monitoramos tudo no MySQL - todas as informa√ß√µes que podemos obter do MySQL s√£o armazenadas em algum lugar, podemos acess√°-las a tempo.  Armazenamos informa√ß√µes no InnoDb, estat√≠sticas sobre solicita√ß√µes, transa√ß√µes, dura√ß√£o das transa√ß√µes, percentual sobre dura√ß√£o das transa√ß√µes, replica√ß√£o, rede, tudo em um grande n√∫mero de m√©tricas. <br><br><h3>  Alerta </h3><br>  Temos 992 alertas configurados.  De fato, ningu√©m est√° olhando para m√©tricas, parece-me que n√£o h√° pessoas que v√™m trabalhar e come√ßam a olhar para o gr√°fico de m√©tricas, h√° tarefas mais interessantes. <br><img src="https://habrastorage.org/webt/q3/nj/sl/q3njslzahbxmh585uzeatrn4plm.jpeg"><br><br>  Portanto, existem alertas que funcionam quando determinados valores de limite s√£o atingidos.  <strong>Temos 992 alertas, aconte√ßa o que acontecer, descobriremos sobre isso</strong> . <br><br><h3>  Incidentes </h3><br><img src="https://habrastorage.org/webt/bi/ce/tp/bicetpzgstf2lggazas6t27ru10.jpeg"><br><br>  Temos o PagerDuty - um servi√ßo atrav√©s do qual alertas s√£o enviados para pessoas respons√°veis ‚Äã‚Äãque come√ßam a agir. <br><img src="https://habrastorage.org/webt/6n/hl/sr/6nhlsrj28tloxq4xg5ld3a-mpvy.jpeg"><br><br>  Nesse caso, ocorreu um erro na promo√ß√£o de emerg√™ncia e, imediatamente ap√≥s isso, um alerta foi registrado, que o mestre caiu.  Depois disso, o oficial de servi√ßo verificou o que impedia a promo√ß√£o de emerg√™ncia e fez as opera√ß√µes manuais necess√°rias. <br><br>  Certamente analisaremos cada incidente que ocorreu; para cada incidente, temos uma tarefa no rastreador de tarefas.  Mesmo que esse incidente seja um problema em nossos alertas, tamb√©m criamos uma tarefa, porque se o problema estiver na l√≥gica e nos limites dos alertas, eles precisar√£o ser alterados.  Os alertas n√£o devem apenas estragar a vida das pessoas.  Um alerta √© sempre doloroso, especialmente √†s 4 da manh√£. <br><br><a name="testing"></a><h2>  Teste </h2><br>  Como no monitoramento, tenho certeza de que todos est√£o testando.  Al√©m dos testes de unidade com os quais cobrimos nosso c√≥digo, temos testes de integra√ß√£o nos quais testamos: <br><br><ul><li>  todas as topologias que temos; </li><li>  todas as opera√ß√µes nessas topologias. </li></ul><br>  Se tivermos opera√ß√µes de promo√ß√£o, testamos as opera√ß√µes de promo√ß√£o no teste de integra√ß√£o.  Se temos clonagem, fazemos clonagem para todas as topologias que temos. <br><br>  <strong>Exemplo de topologia</strong> <br><img src="https://habrastorage.org/webt/dv/hh/va/dvhhvacdschtqr7s_ynecm0lyk0.jpeg"><br><br>  Temos topologias para todas as ocasi√µes: 2 data centers com v√°rias inst√¢ncias, com shards, sem shards, com clusters, um datacenter - geralmente quase qualquer topologia - mesmo aqueles que n√£o usamos, apenas para ver. <br><img src="https://habrastorage.org/webt/f-/oy/pl/f-oypluoyzosnphjl_aukp4vsks.jpeg"><br><br>  Neste arquivo, apenas temos as configura√ß√µes, quais servidores e com o que precisamos aumentar.  Por exemplo, precisamos elevar o mestre e dizemos que precisamos fazer isso com tais e tais dados de inst√¢ncia, com tais e tais bancos de dados em tais e em tais portas.  Quase tudo est√° indo junto com o Bazel, que cria uma topologia com base nesses arquivos, inicia o servidor MySQL e o teste √© iniciado. <br><img src="https://habrastorage.org/webt/bg/zd/b_/bgzdb_dwnggh3kf8uv9f7mn9jpe.jpeg"><br><br>  O teste parece muito simples: indicamos qual topologia est√° sendo usada.  Neste teste, testamos auto_replace. <br><br><ul><li>  Criamos o servi√ßo auto_replace, iniciamos. </li><li>  Matamos o mestre em nossa topologia, esperamos um pouco e vemos que o escravo-alvo se tornou mestre.  Caso contr√°rio, o teste falhou. </li></ul><br><h3>  Etapas </h3><br>  Os ambientes de palco s√£o os mesmos bancos de dados da produ√ß√£o, mas n√£o h√° tr√°fego de usu√°rios, mas h√° algum tr√°fego sint√©tico semelhante √† produ√ß√£o por meio do Percona Playback, sysbench e sistemas similares. <br><br>  No Percona Playback, registramos o tr√°fego e depois o perdemos no ambiente de palco com diferentes intensidades, podemos perder 2-3 vezes mais r√°pido.  Ou seja, √© artificial, mas muito pr√≥ximo da carga real. <br><br>  Isso √© necess√°rio porque nos testes de integra√ß√£o n√£o podemos testar nossa produ√ß√£o.  N√£o podemos testar o alerta ou o fato de as m√©tricas funcionarem.  Na fase de teste, testamos alertas, m√©tricas, opera√ß√µes, periodicamente matamos os servidores e vemos que eles s√£o coletados normalmente. <br><br>  Al√©m disso, testamos toda a automa√ß√£o juntos, porque nos testes de integra√ß√£o, muito provavelmente, uma parte do sistema √© testada e, na prepara√ß√£o, todos os sistemas automatizados funcionam simultaneamente.  √Äs vezes, voc√™ pensa que o sistema se comportar√° dessa maneira e n√£o de outra forma, mas pode se comportar de uma maneira completamente diferente. <br><br><h3>  DRT (teste de recupera√ß√£o de desastre) </h3><br>  Tamb√©m realizamos testes de produ√ß√£o - diretamente em bases reais.  Isso √© chamado de teste de recupera√ß√£o de falhas.  Por que precisamos disso? <br><br>  ‚óè Queremos testar nossas garantias. <br><br>  Isso √© feito por muitas grandes empresas.  Por exemplo, o Google tem um servi√ßo que funcionou de forma t√£o est√°vel - 100% do tempo - que todos os servi√ßos que o utilizaram decidiram que esse servi√ßo √© realmente 100% est√°vel e nunca falha.  Portanto, o Google teve que abandonar esse servi√ßo de prop√≥sito, para que os usu√°rios levassem em conta essa possibilidade. <br><br>  Ent√£o, n√≥s somos - temos uma garantia de que o MySQL funciona - e, √†s vezes, n√£o funciona!  E temos a garantia de que pode n√£o funcionar por um determinado per√≠odo de tempo; os clientes devem levar isso em considera√ß√£o.  De tempos em tempos, matamos o mestre de produ√ß√£o ou, se queremos fazer um escravo, matamos todos os escravos para ver como se comporta a replica√ß√£o semisync. <br><br>  ‚óè Os clientes est√£o preparados para esses erros (substitui√ß√£o e morte do mestre) <br><br>  Por que isso √© bom?  Tivemos um caso em que, durante a promo√ß√£o de 4 fragmentos de 1600, a disponibilidade caiu para 20%.  Parece que algo est√° errado, para 4 fragmentos de 1600 deve haver outros n√∫meros.  Os failovers para esse sistema eram raros, cerca de uma vez por m√™s, e todos decidiram: "Bem, √© um failover, acontece". <br><br>  Em algum momento, quando mudamos para um novo sistema, uma pessoa decidiu otimizar esses dois servi√ßos de grava√ß√£o de batimentos card√≠acos e combinou-os em um.  Este servi√ßo fez outra coisa e, no final, morreu e os batimentos card√≠acos pararam de gravar.  Aconteceu que, para esse cliente, t√≠nhamos 8 faylovers por dia.  Tudo estava - 20% de disponibilidade. <br><br>  Aconteceu que, neste cliente, o keep-alive √© de 6 horas.  Assim, assim que o mestre morreu, mantivemos todas as conex√µes por mais 6 horas.  O pool n√£o p√¥de continuar funcionando - suas conex√µes s√£o mantidas, s√£o limitadas e n√£o funcionam.  Foi consertado. <br><br>  Fazemos o amor de novo - n√£o mais 20%, mas ainda muito.  Algo ainda est√° errado.  Aconteceu que um erro na implementa√ß√£o do pool.  Quando solicitado, a piscina virou muitos fragmentos e, em seguida, conectou tudo isso.  Se alguns fragmentos estavam febris, alguma condi√ß√£o de corrida ocorreu no c√≥digo Go, e todo o pool estava entupido.  Todos esses fragmentos n√£o podiam mais funcionar. <br><br>  O teste de recupera√ß√£o de desastre √© muito √∫til, porque os clientes devem estar preparados para esses erros, eles devem verificar seu c√≥digo. <br><br>  ‚óè Al√©m disso, o teste de recupera√ß√£o de desastres √© bom porque ocorre durante o hor√°rio comercial e tudo est√° no lugar, menos estresse, as pessoas sabem o que acontecer√° agora.  Isso n√£o acontece √† noite e √© √≥timo. <br><br><h2>  Conclus√£o </h2><br>  1. Tudo precisa ser automatizado, nunca coloque as m√£os nele. <br>  Toda vez que algu√©m entra no sistema com nossas m√£os, tudo morre e quebra em nosso sistema - sempre!  - mesmo em opera√ß√µes simples.  Por exemplo, um escravo morreu, uma pessoa teve que adicionar um segundo, mas decidiu remover o escravo morto com as m√£os da topologia.  No entanto, em vez do falecido, ele copiou para o comando live - master foi deixado sem escravo.  Tais opera√ß√µes n√£o devem ser feitas manualmente. <br><br>  2. Os testes devem ser cont√≠nuos e automatizados (e em produ√ß√£o). <br>  Seu sistema est√° mudando, sua infraestrutura est√° mudando.  Se voc√™ verificou uma vez e pareceu funcionar, isso n√£o significa que funcionar√° amanh√£.  Portanto, voc√™ precisa fazer testes automatizados constantemente todos os dias, inclusive na produ√ß√£o. <br><br>  3. Certifique-se de possuir clientes (bibliotecas). <br>  Os usu√°rios podem n√£o saber como os bancos de dados funcionam.  Eles podem n√£o entender por que os intervalos s√£o necess√°rios, mantenha-vivos.  Portanto, √© melhor possuir esses clientes - voc√™ ficar√° mais calmo. <br><br>  4. √â necess√°rio determinar seus princ√≠pios para a constru√ß√£o do sistema e suas garantias, e sempre cumpri-los. <br><br>  Assim, voc√™ pode suportar 6 mil servidores de banco de dados. <br><br><div class="spoiler">  <b class="spoiler_title">Nas perguntas ap√≥s o relat√≥rio, e principalmente nas respostas, tamb√©m h√° muitas informa√ß√µes √∫teis.</b> <div class="spoiler_text"><h2>  Perguntas e Respostas <br></h2><br><blockquote>  - O que acontecer√° se houver um desequil√≠brio na carga dos shards - algumas meta-informa√ß√µes sobre algum arquivo acabaram sendo mais populares?  √â poss√≠vel difundir esse fragmento, ou a carga nos fragmentos n√£o difere em lugar nenhum por ordens de magnitude? </blockquote><br>  Ela n√£o difere por ordens de magnitude.  √â quase normalmente distribu√≠do.  Temos limita√ß√£o, ou seja, n√£o podemos sobrecarregar o fragmento, estamos limitando no n√≠vel do cliente.  Em geral, acontece que alguma estrela envia uma foto e o fragmento praticamente explode.  Ent√£o banimos esse link <br><br><blockquote>  - Voc√™ disse que possui 992 alertas.  Voc√™ poderia elaborar o que √© - est√° pronto ou foi criado?  Se criado, √© trabalho manual ou algo como aprendizado de m√°quina? </blockquote><br>  Tudo isso √© criado manualmente.  Temos nosso pr√≥prio sistema interno chamado Vortex, onde as m√©tricas s√£o armazenadas, os alertas s√£o suportados.  H√° um arquivo yaml que diz que h√° uma condi√ß√£o, por exemplo, de que os backups devem ser executados todos os dias e, se essa condi√ß√£o for atendida, o alerta n√£o funcionar√°.  Se n√£o for executado, um alerta ser√° recebido. <br><br>  Esse √© o nosso desenvolvimento interno, porque poucas pessoas podem armazenar quantas m√©tricas forem necess√°rias. <br><br><blockquote>  - Qu√£o fortes devem ser os nervos para fazer DRT?  Voc√™ caiu, CODERED, n√£o aumenta, com cada minuto de p√¢nico a mais. </blockquote><br>  Em geral, trabalhar em bancos de dados √© realmente uma dor.  Se o banco de dados travar, o servi√ßo n√£o funcionar√°, o Dropbox inteiro n√£o funcionar√°.  Isso √© uma verdadeira dor.  A DRT √© √∫til na medida em que √© um rel√≥gio de neg√≥cios.  Ou seja, estou pronto, estou sentado na minha mesa, tomei caf√©, sou fresco, estou pronto para fazer qualquer coisa. <br><br>  Pior quando acontece √†s 4 da manh√£, e n√£o √© DRT.  Por exemplo, a √∫ltima grande falha que tivemos recentemente.  Ao injetar um novo sistema, esquecemos de definir a pontua√ß√£o do OOM para o nosso MySQL.  Havia outro servi√ßo que lia binlog.  Em algum momento, nosso operador √© manual - novamente manualmente!  - executa o comando para excluir algumas informa√ß√µes na tabela de soma de verifica√ß√£o Percona.  Apenas uma exclus√£o simples, uma opera√ß√£o simples, mas essa opera√ß√£o gerou um enorme log de bin.  O servi√ßo leu esse binlog na mem√≥ria, o OOM Killer veio e pensa em quem matar?  E esquecemos de definir a pontua√ß√£o do OOM e isso mata o MySQL! <br><br>  Temos 40 mestres morrendo √†s 4 da manh√£.  Quando 40 mestres morrem, √© realmente muito assustador e perigoso.  DRT n√£o √© assustador nem perigoso.  Ficamos deitados por cerca de uma hora. <br><br>  A prop√≥sito, a DRT √© uma boa maneira de ensaiar esses momentos, para que saibamos exatamente qual sequ√™ncia de a√ß√µes √© necess√°ria se algo quebrar em massa. <br><br><blockquote>  - Gostaria de saber mais sobre a troca de mestre-mestre.  Primeiro, por que um cluster n√£o √© usado, por exemplo?  Um cluster de banco de dados, ou seja, n√£o um mestre-escravo com comuta√ß√£o, mas um aplicativo mestre-mestre, de modo que, se um deles cair, n√£o ser√° assustador. </blockquote><br>  Voc√™ quer dizer algo como replica√ß√£o de grupo, cluster galera, etc.?  Parece-me que a inscri√ß√£o em grupo ainda n√£o est√° pronta para a vida.  Infelizmente, ainda n√£o experimentamos o Galera.  Isso √© √≥timo quando um faylover est√° dentro do seu protocolo, mas, infelizmente, eles t√™m muitos outros problemas, e n√£o √© t√£o f√°cil mudar para esta solu√ß√£o. <br><br><blockquote>  - Parece que no MySQL 8 existe algo como um cluster InnoDb.  N√£o tentou? </blockquote><br>  Ainda temos 5,6 no valor.  N√£o sei quando mudaremos para 8. Talvez tentemos. <br><br><blockquote>  - Nesse caso, se voc√™ tiver um grande mestre, ao alternar de um para outro, a fila se acumula nos servidores escravos com uma carga alta.  Se o mestre for extinto, √© necess√°rio que a fila chegue, para que o escravo mude para o modo mestre - ou √© feito de alguma maneira diferente? </blockquote><br>  A carga no mestre √© regulada por semisync.  O Semisync limita a grava√ß√£o principal ao desempenho do servidor escravo.  Obviamente, pode ser que a transa√ß√£o tenha chegado, o semisync funcionou, mas os escravos perderam essa transa√ß√£o por um tempo muito longo.  Voc√™ deve esperar at√© que o escravo perca essa transa√ß√£o at√© o fim. <br><br><blockquote>  - Mas ent√£o novos dados vir√£o para o dom√≠nio, e ser√£o necess√°rios ... </blockquote><br>  Quando iniciamos o processo de promo√ß√£o, desabilitamos a E / S.  Depois disso, o mestre n√£o pode escrever nada porque a semisync √© replicada.  Infelizmente, a leitura fantasma pode vir, mas esse j√° √© outro problema. <br><br><blockquote>  - Estas s√£o todas m√°quinas de estado bonitas - em que scripts est√£o escritos e qu√£o dif√≠cil √© adicionar uma nova etapa?  O que precisa ser feito para a pessoa que escreve este sistema? </blockquote><br>  Todos os scripts s√£o escritos em Python, todos os servi√ßos s√£o escritos em Go.  Esta √© a nossa pol√≠tica.  Mudar a l√≥gica √© f√°cil - apenas no c√≥digo Python que gera o diagrama de estado. <br><br><blockquote>  - E voc√™ pode ler mais sobre testes.  Como os testes s√£o gravados, como eles implantam n√≥s em uma m√°quina virtual - s√£o esses cont√™ineres? </blockquote><br>  Sim  Vamos testar com a ajuda de Bazel.  Existem alguns arquivos de configura√ß√£o (json) e o Bazel seleciona um script que cria a topologia para nosso teste usando esse arquivo de configura√ß√£o.  Diferentes topologias s√£o descritas l√°. <br><br>  Tudo funciona para n√≥s em cont√™ineres de encaixe: funciona no CI ou no Devbox.  Temos um sistema Devbox.  Todos n√≥s estamos desenvolvendo em algum servidor remoto, e isso pode funcionar, por exemplo.  L√°, ele tamb√©m √© executado dentro do Bazel, dentro de um cont√™iner de encaixe ou na Sandbox do Bazel.  Bazel √© muito complicado, mas divertido. <br><br><blockquote>  - Quando voc√™ criou quatro inst√¢ncias em um servidor, perdeu em efici√™ncia de mem√≥ria? </blockquote><br>  Cada inst√¢ncia se tornou menor.  Portanto, quanto menos mem√≥ria o MySQL opera, mais f√°cil √© para ele viver.  Qualquer sistema √© mais f√°cil de operar com uma pequena quantidade de mem√≥ria.  Neste lugar, n√£o perdemos nada.  Temos os grupos C mais simples que limitam essas inst√¢ncias da mem√≥ria. <br><br><blockquote>  - Se voc√™ possui 6.000 servidores que armazenam bancos de dados, pode nomear quantos bilh√µes de petabytes est√£o armazenados em seus arquivos? </blockquote><br>  S√£o dezenas de exabytes, servimos dados da Amazon por um ano. <br><br><blockquote>  - Acontece que, inicialmente, voc√™ tinha 8 servidores, 200 shards neles, depois 400 servidores com 4 shards cada.  Voc√™ tem 1600 shards - isso √© algum tipo de valor codificado?  Voc√™ nunca pode fazer isso de novo?  Vai doer se voc√™ precisar, por exemplo, de 3.200 fragmentos? </blockquote><br>  Sim, era originalmente 1600. Isso foi feito h√° menos de 10 anos e ainda vivemos.  Mas ainda temos 4 fragmentos - 4 vezes ainda podemos aumentar o espa√ßo. <br><br><blockquote>  - Como os servidores morrem, principalmente por quais raz√µes?  O que acontece com mais frequ√™ncia, com menos frequ√™ncia e √© especialmente interessante, ocorrem os carapters de bloqueio espont√¢neos? </blockquote><br>  O mais importante √© que os discos voem.  Temos o RAID 0 - o disco travou, o mestre morreu.  Esse √© o principal problema, mas √© mais f√°cil substituir esse servidor.  Google √© mais f√°cil substituir o data center, ainda temos um servidor.  Quase nunca tivemos soma de verifica√ß√£o de corrup√ß√£o.  Para ser sincero, n√£o me lembro quando foi a √∫ltima vez.  N√≥s frequentemente atualizamos o assistente.  O tempo de vida de um mestre √© limitado a 60 dias.  N√£o pode durar mais, depois substitu√≠-lo por um novo servidor, porque, por algum motivo, algo est√° constantemente se acumulando no MySQL e, ap√≥s 60 dias, vemos que problemas come√ßam a ocorrer.  Talvez n√£o no MySQL, talvez no Linux. <br><br>   ,          .     60 ,    .      . <br><br><blockquote> ‚Äî  ,    6        . ,   JPEG   ,     JPEG,  ,      ?  , ,      -   ?    ‚Äî      ,       ? </blockquote><br>     ,  .   ‚Äî  Dropbox    . <br><br><blockquote> ‚Äî      ?         ?     , ,  - ,    , ? ,   10   . ,  7     ,    6    ,    .    ? </blockquote><br>   Dropbox  - ,       .   .  ,    ,       ,   -  . <br><br>  ,    .  ,  ,      ,      .        - ,     6 ,   ,     ,    ,    . <br></div></div><br><blockquote>       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">facebook</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">youtube-</a> ‚Äî          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Highload++ 2018</a> .      , <strong> 1 </strong>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">  </a> . <br></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt417315/">https://habr.com/ru/post/pt417315/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt417303/index.html">Hack para dar suporte aos bot√µes do fone de ouvido do Windows Android</a></li>
<li><a href="../pt417305/index.html">Ultima Online: um olhar nos bastidores</a></li>
<li><a href="../pt417307/index.html">Glaucoma - n√£o ouviu falar dela? Conhe√ßa o serial killer silencioso da vis√£o</a></li>
<li><a href="../pt417309/index.html">Felizmente, gerente de ITSM: como a profiss√£o do futuro ajuda a expandir as fronteiras do Service Desk</a></li>
<li><a href="../pt417311/index.html">Criando um bot para participar do mini cup AI 2018 com base em uma rede neural recorrente</a></li>
<li><a href="../pt417317/index.html">Em Highload ++ 2018 a toda velocidade</a></li>
<li><a href="../pt417319/index.html">Sistemas no caso ou O que realmente est√° sob a cobertura do microprocessador</a></li>
<li><a href="../pt417321/index.html">Como procuramos professores de cursos on-line entre desenvolvedores?</a></li>
<li><a href="../pt417323/index.html">Problemas para garantir 100% de acessibilidade ao projeto</a></li>
<li><a href="../pt417325/index.html">Dia Aberto da Netrologia, Tema de Ci√™ncia de Dados</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>