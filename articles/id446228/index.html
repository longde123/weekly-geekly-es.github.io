<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🐍 👨🏼‍🔬 👪 Meningkatkan kualitas klasifikasi teks dengan menghubungkan Wikipedia 😧 🖐🏼 🧗🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Kami menggunakan sumber terstruktur besar teks multibahasa - Wikipedia untuk meningkatkan klasifikasi teks. Pendekatannya bagus dengan otomatisme dan ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Meningkatkan kualitas klasifikasi teks dengan menghubungkan Wikipedia</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/446228/"> Kami menggunakan sumber terstruktur besar teks multibahasa - Wikipedia untuk meningkatkan klasifikasi teks.  Pendekatannya bagus dengan otomatisme dan kemandirian tingkat tinggi di mana masalah klasifikasi tertentu sedang dipecahkan.  Efek terbesar, bagaimanapun, diharapkan pada tugas-tugas menentukan topik. <br><a name="habracut"></a><br>  Gagasan utamanya adalah mengekstrak dari Wikipedia hanya teks-teks yang membantu kami memecahkan masalah klasifikasi kami, mengabaikan yang lain.  Jika kita mengklasifikasikan teks tentang kucing, kecil kemungkinan kita akan membutuhkan teks tentang fisika kuantum, meskipun teks pada jenis hewan lain bisa bermanfaat.  Pemisahan otomatis teks-teks tersebut dari satu sama lain adalah inti dari pendekatan yang dijelaskan. <br><br>  Wikipedia, seperti yang Anda ketahui, adalah kumpulan artikel tentang banyak bidang pengetahuan dan minat.  Pada saat yang sama, sebagian besar artikel memiliki tautan ke artikel-artikel dari subjek yang serupa, tetapi dalam bahasa lain.  Ini bukan terjemahan, yaitu artikel dari subjek umum.  Juga, sebagian besar artikel termasuk dalam satu kategori atau lebih.  Kategori, pada gilirannya, sebagian besar diatur dalam bentuk pohon hierarkis.  Artinya, tugas mengelompokkan artikel Wikipedia pada topik yang menarik bagi kami dapat diselesaikan. <br><br>  Kami menggunakan DBPedia sumber daya - versi Wikipedia pra-kabel dan terstruktur.  DBPedia memberi kita semua informasi yang diperlukan - nama artikel, penjelasannya, kategori artikel dan kategori superior untuk kategori tersebut.  Kami mulai dengan bahasa yang paling banyak diwakili di Wikipedia - Bahasa Inggris.  Jika tugas Anda tidak memiliki, atau sedikit, teks berbahasa Inggris, gunakan bahasa yang memiliki banyak dokumen. <br><br><h3>  Langkah 1. Clustering Wikipedia </h3><br>  Fokus pada kategori artikel.  Untuk saat ini, abaikan konten mereka.  Kategori membentuk grafik, kebanyakan seperti pohon, tetapi ada juga siklusnya.  Artikel adalah titik akhir grafik (dedaunan) yang terhubung ke satu atau lebih node grafik.  Kami menggunakan alat Node2Vec untuk mendapatkan representasi vektor dari setiap kategori dan setiap artikel.  Artikel dari subjek yang sama dikelompokkan bersama dalam ruang vektor. <br><br>  Kami mengelompokkan dengan metode apa pun yang mudah dari artikel menjadi sejumlah besar (ratusan) kelompok. <br><br><h3>  Langkah 2. Pelatihan classifier di Wikipedia </h3><br>  Kami mengganti nama artikel di cluster yang dihasilkan dengan anotasi mereka (Abstrak Panjang dan Abstrak Pendek - sekitar satu paragraf teks per artikel).  Sekarang kami memiliki ratusan cluster yang didefinisikan sebagai kumpulan teks.  Kami menggunakan model yang nyaman dan membangun classifier yang memecahkan masalah klasifikasi multiklas: satu kluster - satu kelas.  Kami menggunakan FastText. <br>  Pada output, kami mendapatkan model yang menggunakan teks sebagai input, dan pada output itu memberikan vektor perkiraan sejauh mana teks itu milik ratusan cluster kelas kami. <br><br>  Jika langkah pertama adalah mengelompokkan artikel Wikipedia bukan berdasarkan kategorinya, tetapi dengan kontennya, maka, pertama, kami akan kehilangan informasi berdasarkan kategorinya, tetapi penting, dan kedua, kami akan mendapatkan sistem degenerasi - yang, melalui teks, dikelompokkan dan dibuat model pengklasifikasi.  Kualitas akhir mungkin akan lebih buruk daripada dengan pendekatan terpisah.  Meskipun saya tidak memeriksa. <br><br><h3>  Langkah 3. Membangun model sendiri, bertarung, data </h3><br>  Kami menggunakan pilihan data tempur kami dan menyerahkan setiap dokumen ke input model dari langkah 2. Model mengembalikan vektor perkiraan.  Kami menggunakan vektor ini sebagai vektor fitur untuk dokumen yang dimaksud.  Sebagai hasilnya, setelah memproses semua sampel pelatihan dokumen tempur kami, kami mendapatkan tabel dalam bentuk standar untuk pembelajaran mesin - label kelas, satu set tanda numerik.  Kami menyebut tabel ini satu set pelatihan. <br><br>  Kami membuat sampel pelatihan sebuah classifier yang dapat mengevaluasi konten informasi dari atribut individual.  Pohon keputusan dan setiap variasi hutan acaknya sangat cocok.  Tanda-tanda yang paling informatif adalah kumpulan artikel Wikipedia yang tidak hanya memiliki tema serupa dengan tema dokumen pertempuran kita, tetapi, yang paling penting, topik artikel ini memungkinkan kita untuk memisahkan kelas pertempuran kita dengan baik.  Pada iterasi pertama, histogram keinformatifan tanda biasanya cukup datar - beberapa kelompok informatif dan ekor panjang hampir sama dalam hal keinformatifan dengan ratusan tanda yang tersisa. <br><br>  Setelah mempelajari histogram dari konten informasi karakter, titik belok ditentukan secara empiris setiap kali, dan sekitar 10 hingga 30% dari kluster pergi ke iterasi berikutnya.  Inti dari iterasi adalah artikel dari kluster informatif terpilih dipilih, diserahkan ke langkah 1-3, di mana mereka dikelompokkan lagi, dua pengklasifikasi dibangun lagi, dan semuanya berakhir dengan analisis histogram konten informasi.  Ini akan membutuhkan 3-4 iterasi. <br><br>  Ternyata pada data kami tanda-tanda digital, terutama angka-angka tahun, memiliki bobot yang sangat kuat dan menyeret keinformatifan seluruh gugus ke diri mereka sendiri.  Sebagai hasil logis, kluster yang dikhususkan untuk acara olahraga tahunan menjadi yang paling informatif - kumpulan angka dan tanggal, kosa kata yang sempit.  Saya harus menghapus semua angka dalam teks anotasi artikel (pada langkah kedua).  Menjadi terasa lebih baik, kelompok artikel yang benar-benar memiliki subjek yang ditargetkan mulai menonjol (seperti yang kita bayangkan).  Pada saat yang sama, kluster tak terduga muncul yang secara logis jatuh pada misi tempur kami, memiliki kosa kata yang tepat, tetapi sangat sulit untuk menebak apriori kegunaan kluster tersebut. <br><br><h3>  Langkah 4. Finalisasi model </h3><br>  Setelah beberapa kali pengulangan langkah 1-3, kami memiliki sejumlah artikel yang masuk akal dari Wikipedia, yang topiknya membantu membagikan dokumen pertempuran kami.  Kami sedang memperluas pilihan dengan artikel serupa dalam bahasa lain yang menarik bagi kami dan membangun cluster akhir, kali ini puluhan.  Cluster ini dapat digunakan dalam dua cara - baik membangun classifier mirip dengan langkah 2, dan menggunakannya untuk memperluas vektor fitur digital dalam misi pertempuran Anda, atau menggunakan set teks ini sebagai sumber kosakata tambahan dan mengintegrasikannya ke dalam classifier tempur Anda.  Kami menggunakan cara kedua. <br><br>  Classifier tempur kami adalah ansambel dua model - bayes naif terpotong dan xgboost.  Bayes Naif bekerja pada gram panjang, ini adalah gram dengan panjang dari 1 hingga 16 elemen, dan masing-masing gram menemukan totalnya menjadi salah satu kelas, tetapi Bayes tidak membuat keputusan akhir - itu hanya memberikan jumlah bobot gram yang terkait dengan masing-masing dari kelas.  Xgboost menerima output bayes, pengklasifikasi lain dan beberapa atribut digital yang dibangun secara independen sesuai dengan teks, dan xgboost sudah memberikan model akhir dan penilaian akhir.  Pendekatan ini memudahkan untuk menghubungkan set teks ke model gram bayes, termasuk set artikel Wikipedia yang dihasilkan, dan xgboost sudah mencari pola dalam bentuk reaksi khas cluster wikipedia terhadap teks pertempuran. <br><br><h3>  Hasil dan Kesimpulan </h3><br>  Hasil pertama memberi peningkatan dari akurasi 60% bersyarat menjadi 62%.  Saat mengganti anotasi artikel Wikipedia di langkah 4 dengan artikel kempis itu sendiri, akurasinya meningkat menjadi 66%.  Hasilnya wajar, karena ukuran anotasi adalah dua atau tiga frasa, dan ukuran artikel adalah urutan besarnya lebih besar.  Materi yang lebih linguistik - efek yang lebih tinggi. <br><br>  Kita harus berharap bahwa setelah menyelesaikan seluruh prosedur pada teks artikel, daripada anotasi, peningkatan kualitas akan lebih besar, tetapi sudah ada masalah nomor teknis - sulit untuk memompa keluar dan memproses seluruh Wikipedia, atau bagian yang terlihat (jika Anda memulai bukan dari iterasi pertama).  Juga, jika Anda awalnya menggunakan tidak hanya bahasa Inggris, tetapi semua bahasa yang menarik, Anda masih bisa memenangkan sesuatu yang lain.  Dalam hal ini, pertumbuhan volume yang diproses berlipat ganda, dan bukan berdasarkan urutan besarnya, seperti pada kasus pertama. <br><br><h4>  Vektor dokumen semantik </h4><br>  Untuk setiap dokumen, vektor dibangun dari hubungan dokumen dengan topik yang diberikan berdasarkan kategori Wikipedia.  Vektor dihitung biayanya dengan metode yang dijelaskan pada langkah 3 atau oleh gram bayes kami.  Dengan demikian, dokumen pertempuran dapat dikelompokkan sesuai dengan vektor-vektor ini dan mendapatkan pengelompokan dokumen pertempuran berdasarkan subjek.  Tetap hanya untuk meletakkan tagar dan setiap dokumen baru sudah bisa jatuh ke dalam database dengan tag.  Yang kemudian bisa dicari pengguna.  Ini terjadi jika Anda membubuhkan tag secara eksplisit dan terlihat oleh pengguna.  Ini terlihat modis, walaupun saya bukan pendukung. <br><br><h4>  Pencarian adaptif </h4><br>  Metode yang lebih menarik menggunakan vektor dokumen semantik adalah pencarian adaptif.  Mengamati aktivitas pengguna, pada dokumen mana ia tinggal dan yang mana ia bahkan tidak membaca, Anda dapat menguraikan bidang minat pengguna dalam arti jangka panjang (setelah semua, pengguna juga memiliki pembagian tanggung jawab dan semua orang terutama mencari sendiri) dan dalam kerangka sesi pencarian saat ini. <br><br>  Dokumen dengan topik serupa memiliki vektor semantik yang serupa dengan ukuran kosinus tinggi, dan ini memungkinkan Anda untuk mengevaluasi dokumen dalam hasil pencarian dengan cepat sesuai dengan tingkat kepatuhan yang diharapkan dengan kepentingan pengguna, sehingga Anda dapat menambah dokumen yang diperlukan dalam hasil pencarian. <br><br>  Akibatnya, bahkan dengan permintaan pencarian yang identik untuk setiap pengguna, hasil pencarian dapat dipersonalisasi untuknya dan tergantung pada dokumen mana pada langkah sebelumnya yang diminati pengguna, langkah pencarian berikutnya akan disesuaikan dengan kebutuhan pengguna, bahkan jika permintaan pencarian itu sendiri tidak berubah. <br><br>  Kami sekarang sedang mengerjakan masalah pencarian adaptif. <br><br><h4>  Pengujian Hipotesis Bisnis </h4><br>  Bisnis secara berkala hadir dengan ide-ide cemerlang yang sangat sulit diimplementasikan.  Kita harus belajar menemukan dokumen dengan deskripsi mereka, tanpa memiliki sampel mark-up untuk pelatihan, atau kemampuan untuk menyerahkan kepada penilai beberapa set dokumen untuk penandaan.  Ini biasanya terjadi ketika dokumen target jarang ditemukan sehubungan dengan aliran umum dokumen, dan sebagai hasilnya, dengan mengirimkan kumpulan 10 ribu dokumen kepada penilai tanpa penyaringan awal, Anda bisa mendapatkan 1-2 hasil yang diperlukan atau bahkan lebih sedikit. <br><br>  Pendekatan kami adalah menciptakan proses belajar berulang berdasarkan vektor semantik.  Pada langkah pertama, kami menemukan beberapa teks yang menetapkan topik target kami - ini mungkin artikel Wikipedia, atau teks dari sumber lain.  Untuk setiap teks, vektor semantiknya dihasilkan.  Jika topik target kompleks, aljabar set berfungsi - penyatuan, persimpangan, pengecualian beberapa topik dari yang lain.  Misalnya - ada artikel Wikipedia tentang "Penelitian dan Pengembangan" dan tentang "Kosmetik", persimpangan set akan memberikan "R&amp;D tentang kosmetik". <br><br>  Semua dokumen dalam database dapat diurutkan berdasarkan tingkat kepatuhannya dengan topik yang diberikan, kemudian aljabar set bekerja pada dokumen itu sendiri sebagai berikut - dokumen tersebut dianggap relevan dengan topik jika vektor semantiknya lebih dekat dengan vektor artikel Wikipedia dari topik yang diberikan daripada rata-rata untuk database.  Persimpangan - jika pada saat yang sama vektor semantik dokumen lebih dekat ke kedua topik daripada rata-rata untuk database.  Operasi lain serupa. <br><br>  Kami menemukan sekumpulan ratusan atau dua dokumen yang memiliki kedekatan paling dekat dengan semua topik positif dan, pada saat yang sama, kedekatan terdekat dengan semua topik negatif (jika kami tidak tertarik dengan masalah keuangan dalam penelitian yang kami cari, kami akan menetapkan artikel dari kategori "Keuangan" sebagai contoh negatif) )  Kami akan memberikan dokumen-dokumen ini kepada penilai, mereka akan menemukan beberapa contoh positif di dalamnya, berdasarkan contoh-contoh ini kita akan mencari dokumen lain dengan vektor semantik dekat, menandai mereka, dan pada output kita akan mendapatkan dokumen yang cukup untuk kelas positif untuk membangun classifier yang nyaman.  Mungkin diperlukan beberapa iterasi. <br><br><h4>  Ringkasan </h4><br>  Pendekatan yang dijelaskan memungkinkan secara otomatis, tanpa analisis manual, untuk memilih dari Wikipedia atau kumpulan teks sumber lain yang membantu menyelesaikan masalah klasifikasi.  Dengan hanya menghubungkan cluster dari Wikipedia ke classifier yang berfungsi, orang dapat mengharapkan peningkatan kualitas yang signifikan, tanpa memerlukan adaptasi dari classifier itu sendiri. <br><br>  Yah, pencarian adaptif itu menarik. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id446228/">https://habr.com/ru/post/id446228/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id446210/index.html">ADAM-3600 - pengontrol industri multifungsi</a></li>
<li><a href="../id446212/index.html">Kedalaman SIEM: korelasi out-of-box. Bagian 5. Metodologi untuk mengembangkan aturan korelasi</a></li>
<li><a href="../id446214/index.html">OS1: kernel primitif di Rust untuk x86. Bagian 3. Kartu memori, pengecualian kesalahan halaman, tumpukan dan alokasi</a></li>
<li><a href="../id446218/index.html">Desainer game tidak jauh berbeda dari seorang psiko. Bagaimana kami membuat game CMAN</a></li>
<li><a href="../id446222/index.html">Penggunaan potensi termal untuk analisis wilayah</a></li>
<li><a href="../id446230/index.html">Pemantauan jarak jauh dan pengelolaan perangkat berbasis Linux / OpenWrt / Lede melalui port 80, lanjutan</a></li>
<li><a href="../id446234/index.html">Bagaimana para sukarelawan dari seluruh dunia membuat siaran langsung ICPC-2019</a></li>
<li><a href="../id446236/index.html">Yandex akan meningkatkan algoritme pengenalan suara</a></li>
<li><a href="../id446238/index.html">Memanfaatkan bootloader yang telah ditandatangani untuk menghindari UEFI Secure Boot</a></li>
<li><a href="../id446242/index.html">Penundaan sebagai alat untuk perjalanan waktu</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>