<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üñêüèø üëõ üë©üèæ‚Äçü§ù‚Äçüë®üèª Transfer√™ncia de Conhecimento e Tradu√ß√£o Autom√°tica Neural na Pr√°tica üë©‚Äçüíª ü§´ üéá</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A Tradu√ß√£o Autom√°tica Neural (NMT) est√° se desenvolvendo muito rapidamente. Hoje, para montar seu tradutor, voc√™ n√£o precisa ter dois estudos superior...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Transfer√™ncia de Conhecimento e Tradu√ß√£o Autom√°tica Neural na Pr√°tica</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/475750/"> A Tradu√ß√£o Autom√°tica Neural (NMT) est√° se desenvolvendo muito rapidamente.  Hoje, para montar seu tradutor, voc√™ n√£o precisa ter dois estudos superiores.  Mas, para treinar o modelo, voc√™ precisa de um corpus paralelo grande (um corpus no qual a tradu√ß√£o no idioma de origem est√° associada √† senten√ßa).  Na pr√°tica, estamos falando de pelo menos um milh√£o de pares de frases.  Existe at√© uma grande √°rea separada do FMI que explora m√©todos para o ensino de pares de idiomas com uma pequena quantidade de dados eletr√¥nicos (English Low Resource NMT). <br><br>  Estamos coletando o corpo russo-chuvash e, ao mesmo tempo, analisando o que pode ser feito com o volume de dados dispon√≠vel.  Neste exemplo, um caso de 90.000 pares de frases foi usado.  O melhor resultado no momento foi dado pelo m√©todo de transfer√™ncia de conhecimento (Eng. Transfer Learning), e ser√° discutido no artigo.  O objetivo do artigo √© dar um exemplo pr√°tico de implementa√ß√£o que pode ser facilmente reproduzido. <a name="habracut"></a><br><br>  O plano de treinamento √© o seguinte.  Precisamos pegar um pr√©dio grande (pai), treinar um modelo neural nele e depois treinar nosso modelo filha.  Al√©m disso, o idioma alvo da tradu√ß√£o ser√° o mesmo: russo.  Intuitivamente, isso pode ser comparado ao aprendizado de um segundo idioma.  √â mais f√°cil aprender, conhecendo uma l√≠ngua estrangeira.  Tamb√©m parece estudar uma √°rea estreita de uma l√≠ngua estrangeira, por exemplo, a terminologia m√©dica da l√≠ngua inglesa: primeiro voc√™ precisa aprender ingl√™s em geral. <br><br>  Como um corpo parental, tentamos tomar 1 milh√£o de pares de senten√ßas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow">do corpo paralelo ingl√™s-russo</a> e 1 milh√£o <a href="" rel="nofollow">do corpo cazaque-russo</a> .  Existem 5 milh√µes de frases nos dados do Cazaque.  Desses, apenas aqueles com um coeficiente de conformidade (terceira coluna) superior a 2. A vers√£o cazaque apresentou resultados um pouco melhores.  Parece intuitivamente que isso seja compreens√≠vel, pois as l√≠nguas chuvash e cazaque s√£o mais parecidas entre si.  Mas, na verdade, isso n√£o est√° comprovado e tamb√©m depende muito da qualidade do caso.  Mais detalhes sobre a sele√ß√£o do corpo dos pais podem ser encontrados <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow">neste artigo</a> .  Sobre o corpo subsidi√°rio de 90.000 pares de ofertas, voc√™ pode <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow">descobrir e solicitar dados de amostra aqui.</a> <br><br>  Agora para o c√≥digo.  Se voc√™ n√£o possui sua pr√≥pria placa gr√°fica r√°pida, voc√™ pode treinar o modelo no site da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow">Colab</a> .  Para o treinamento, usamos a biblioteca <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow">Sockeye</a> .  Sup√µe-se que o Python3 j√° esteja instalado. <br><br><pre><code class="bash hljs">pip install sockeye</code> </pre> <br>  Voc√™ tamb√©m pode precisar mexer em separado com o <a href="" rel="nofollow">MXNet</a> , respons√°vel por trabalhar com a placa de v√≠deo.  O Colab precisa de instala√ß√£o adicional da biblioteca <br><br><pre> <code class="bash hljs">pip install mxnet-cu100mkl</code> </pre> <br>  Sobre as redes neurais, geralmente √© aceito que basta alimentar os dados como est√£o, e eles descobrir√£o isso.  Mas, na realidade, esse nem sempre √© o caso.  Portanto, no nosso caso, o corpo precisa ser pr√©-processado.  Primeiro, a tokenizamos para que seja mais f√°cil para os modelos entenderem que "gato!" E "gato" s√£o praticamente a mesma coisa.  Por exemplo, apenas um tokenizer python serve. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.tokenize <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> WordPunctTokenizer <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tokenize</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(src_filename, new_filename)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(src_filename, encoding=<span class="hljs-string"><span class="hljs-string">"utf-8"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> src_file: <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(new_filename, <span class="hljs-string"><span class="hljs-string">"w"</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">"utf-8"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> new_file: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> src_file: new_file.write(<span class="hljs-string"><span class="hljs-string">"%s"</span></span> % <span class="hljs-string"><span class="hljs-string">' '</span></span>.join(WordPunctTokenizer().tokenize(line))) new_file.write(<span class="hljs-string"><span class="hljs-string">"\n"</span></span>)</code> </pre> <br>  Como resultado, alimentamos pares de frases no formul√°rio <br><br><pre> <code class="xml hljs">  ”ó  “´ ”≥ ”ë”ë. ”ë ”ë ”ë ”ë”ó, ”ë ”ë”ë”ó,   ”ë  ”ó”ó -”ë ”ó”ó“´,  “´”ó ”ó ”ó“´ ”ë”ë ”ë”ë, “´ ”ó ”ó   ”ë ”ë ”ë”ë ”ë .</code> </pre> <br>  e <br><br><pre> <code class="xml hljs">     .  , ,       , ,    ,        .</code> </pre> <br>  A sa√≠da s√£o as seguintes ofertas tokenizadas: <br><br><pre> <code class="xml hljs">  ”ó  “´ ”≥ ”ë”ë . ”ë ”ë ”ë ”ë”ó , ”ë ”ë”ë”ó ,   ”ë  ”ó”ó  - ”ë ”ó”ó“´ ,  “´”ó ”ó ”ó“´ ”ë”ë ”ë”ë , “´ ”ó ”ó   ”ë ”ë ”ë”ë ”ë  .</code> </pre> <br>  e em russo <br><br><pre> <code class="xml hljs">      .   ,  ,        ,  ,     ,         .</code> </pre> <br>  No nosso caso, precisaremos dos dicion√°rios combinados dos casos pai e filho, para criar arquivos comuns: <br><br><pre> <code class="bash hljs">cp kk.parent.train.tok kkchv.all.train.tok cat chv.child.train.tok &gt;&gt; kk.parent.train.tok cp ru.parent.train.tok ru.all.train.tok cat ru.child.train.tok &gt;&gt; ru.all.train.tok</code> </pre> <br>  uma vez que o treinamento adicional do modelo filho ocorre no mesmo dicion√°rio. <br><br>  Agora, uma digress√£o pequena, mas importante.  No MP, as frases s√£o divididas em √°tomos na forma de palavras e depois operam nas frases como sequ√™ncias de palavras.  Mas isso geralmente n√£o √© suficiente, porque uma cauda enorme √© formada a partir das palavras que ocorrem no corpus uma vez.  Construir um modelo probabil√≠stico para eles √© dif√≠cil.  Isto √© especialmente verdade para idiomas com morfologia desenvolvida (caso, sexo, n√∫mero).  O russo e o chuvash s√£o exatamente esses idiomas.  Mas existe uma solu√ß√£o.  Voc√™ pode dividir a frase em um n√≠vel inferior, em subpalavras.  Usamos a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow">codifica√ß√£o de pares de bytes.</a> <br><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/rsennrich/subword-nmt.git</code> </pre> <br>  Temos aproximadamente essas seq√º√™ncias de subpalavras <br><br><pre> <code class="xml hljs">@@   ”ó  “´ ”≥@@  ”ë”ë . @@ ”ë ”ë ”ë @@ ”ë”ó , ”ë ”ë”ë@@ ”ó ,   ”ë@@  @@  ”ó”ó  - ”ë@@  ”ó@@ ”ó“´ ,  “´”ó@@  ”ó ”ó“´@@ @@  ”ë”ë ”ë”ë , “´@@ @@ @@  ”ó ”ó @@ @@  @@  ”ë ”ë ”ë”ë ”ë  .</code> </pre> <br>  e <br><br><pre> <code class="xml hljs">@@    @@  @@   . @@  @@ @@ @@ @@  , @@  , @@ @@  @@    @@  @@ @@  @@ @@ @@ @@  ,  ,  @@  @@ @@ @@  @@ @@ @@  ,       @@ @@  @@ @@ @@  .</code> </pre> <br>  Pode-se ver que os afixos s√£o bem diferenciados das palavras: @@ por um longo tempo e bom @@. <br>  Para fazer isso, prepare dicion√°rios bpe <br><br><pre> <code class="bash hljs">python subword-nmt/subword_nmt/learn_joint_bpe_and_vocab.py --input kkchv.all.train.tok ru.all.train.tok -s 10000 -o bpe.codes --write-vocabulary bpe.vocab.kkchv bpe.vocab.ru</code> </pre> <br>  E aplique-os aos tokens, por exemplo: <br><br><pre> <code class="bash hljs">python subword-nmt/subword_nmt/apply_bpe.py -c bpe.codes --vocabulary bpe.vocab.kkchv --vocabulary-threshold 50 &lt; kkchv.all.train.tok &gt; kkchv.all.train.bpe !python subword-nmt/subword_nmt/apply_bpe.py -c bpe.codes --vocabulary bpe.vocab.ru --vocabulary-threshold 50 &lt; ru.all.train.tok &gt; ru.all.train.bpe</code> </pre> <br>  Por analogia, voc√™ precisa fazer para todos os arquivos: treinamento, valida√ß√£o e teste de modelos pai e filho. <br><br>  Agora nos voltamos diretamente para o treinamento do modelo neural.  Primeiro, voc√™ precisa preparar dicion√°rios gerais de modelo: <br><br><pre> <code class="bash hljs">python -m sockeye.prepare_data -s kk.all.train.bpe -t ru.all.train.bpe -o kkru_all_data</code> </pre> <br>  Em seguida, treine o modelo pai.  Um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow">exemplo simples √© descrito em</a> mais detalhes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow">na p√°gina Sockeye.</a>  Tecnicamente, o processo consiste em duas etapas: prepara√ß√£o de dados usando dicion√°rios de modelo criados anteriormente <br><br><pre> <code class="bash hljs">python -m sockeye.prepare_data -s kk.parent.train.bpe -t ru.parent.train.bpe -o kkru_parent_data --<span class="hljs-built_in"><span class="hljs-built_in">source</span></span>-vocab kkru_all_data/vocab.src.0.json --target-vocab kkru_all_data/vocab.trg.0.json</code> </pre> <br>  e o pr√≥prio aprendizado <br><br><pre> <code class="bash hljs">python -m sockeye.train -d kkru_parent_data -vs kk.parent.dev.bpe -vt ru.parent.dev.bpe --encoder transformer --decoder transformer --transformer-model-size 512 --transformer-feed-forward-num-hidden 256 --transformer-dropout-prepost 0.1 --num-embed 512 --max-seq-len 100 --decode-and-evaluate 500 -o kkru_parent_model --num-layers 6 --<span class="hljs-built_in"><span class="hljs-built_in">disable</span></span>-device-locking --batch-size 1024 --optimized-metric bleu --max-num-checkpoint-not-improved 10</code> </pre> <br>  O treinamento nas instala√ß√µes da Colab leva cerca de um dia.  Quando o treinamento do modelo estiver conclu√≠do, voc√™ poder√° traduzi-lo para <br><br><pre> <code class="bash hljs">python -m sockeye.translate --input kk.parent.test.bpe -m kkru_parent_model --output ru.parent.test_kkru_parent.bpe</code> </pre> <br>  Para treinar o modelo filho <br><pre> <code class="bash hljs">python -m sockeye.prepare_data -s chv.child.train.bpe -t ru.child.train.bpe -o chvru_child_data --<span class="hljs-built_in"><span class="hljs-built_in">source</span></span>-vocab kkru_all_data/vocab.src.0.json --target-vocab kkru_all_data/vocab.trg.0.json</code> </pre> <br>  O c√≥digo de in√≠cio do treinamento √© semelhante a este <br><br><pre> <code class="bash hljs">python -m sockeye.train -d chvru_child_data -vs chv.child.dev.bpe -vt ru.child.dev.bpe --encoder transformer --decoder transformer --transformer-model-size 512 --transformer-feed-forward-num-hidden 256 --transformer-dropout-prepost 0.1 --num-embed 512 --max-seq-len 100 --decode-and-evaluate 500 -o ruchv_150K_skv_dev19_model --num-layers 6 --<span class="hljs-built_in"><span class="hljs-built_in">disable</span></span>-device-locking --batch-size 1024 --optimized-metric bleu --max-num-checkpoint-not-improved 10 --config kkru_parent_model/args.yaml --params kkru_parent_model/params.best</code> </pre> <br>  S√£o adicionados par√¢metros que indicam que a configura√ß√£o e os pesos do modelo pai devem ser usados ‚Äã‚Äãcomo ponto de partida.  Detalhes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow">no exemplo com a reciclagem do Sockeye</a> .  Aprender um modelo infantil converge em cerca de 12 horas. <br><br>  Para resumir, compare os resultados.  O modelo usual de tradu√ß√£o autom√°tica produziu 24,96 BLEU de qualidade, enquanto o modelo de transfer√™ncia de conhecimento foi de 32,38 BLEU.  A diferen√ßa √© vis√≠vel tamb√©m visualmente em exemplos de tradu√ß√µes.  Portanto, enquanto continuamos a montar o gabinete, usaremos esse modelo. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt475750/">https://habr.com/ru/post/pt475750/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt475740/index.html">O livro "Programa√ß√£o das Olimp√≠adas" foi lan√ßado</a></li>
<li><a href="../pt475742/index.html">Caso do RetouchMe: o que obtivemos ao localizar o aplicativo em 35 idiomas</a></li>
<li><a href="../pt475744/index.html">Modelo de Administrador do Sistema em Quatro N√≠veis</a></li>
<li><a href="../pt475746/index.html">Anatomia de Sistemas Ac√∫sticos: Cermetos e Comp√≥sitos - Sobre os Difusores de √Åudio do Monitor</a></li>
<li><a href="../pt475748/index.html">22 de novembro de Moscou - AnalyzeIT MeetUp No. 3</a></li>
<li><a href="../pt475754/index.html">Shorts sobre Scrum</a></li>
<li><a href="../pt475758/index.html">N√≠veis de assinatura renov√°veis ‚Äã‚Äãautomaticamente no aplicativo iOS</a></li>
<li><a href="../pt475760/index.html">Desenvolvedores juniores - por que os contratamos e como trabalhamos com eles</a></li>
<li><a href="../pt475762/index.html">Um curr√≠culo com uma fotografia voa para uma urna. Caracter√≠sticas da procura de emprego nos EUA</a></li>
<li><a href="../pt475764/index.html">Cadeias de Markov para gera√ß√£o processual de edif√≠cios</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>