<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🖐🏿 👛 👩🏾‍🤝‍👨🏻 Transferência de Conhecimento e Tradução Automática Neural na Prática 👩‍💻 🤫 🎇</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A Tradução Automática Neural (NMT) está se desenvolvendo muito rapidamente. Hoje, para montar seu tradutor, você não precisa ter dois estudos superior...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Transferência de Conhecimento e Tradução Automática Neural na Prática</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/475750/"> A Tradução Automática Neural (NMT) está se desenvolvendo muito rapidamente.  Hoje, para montar seu tradutor, você não precisa ter dois estudos superiores.  Mas, para treinar o modelo, você precisa de um corpus paralelo grande (um corpus no qual a tradução no idioma de origem está associada à sentença).  Na prática, estamos falando de pelo menos um milhão de pares de frases.  Existe até uma grande área separada do FMI que explora métodos para o ensino de pares de idiomas com uma pequena quantidade de dados eletrônicos (English Low Resource NMT). <br><br>  Estamos coletando o corpo russo-chuvash e, ao mesmo tempo, analisando o que pode ser feito com o volume de dados disponível.  Neste exemplo, um caso de 90.000 pares de frases foi usado.  O melhor resultado no momento foi dado pelo método de transferência de conhecimento (Eng. Transfer Learning), e será discutido no artigo.  O objetivo do artigo é dar um exemplo prático de implementação que pode ser facilmente reproduzido. <a name="habracut"></a><br><br>  O plano de treinamento é o seguinte.  Precisamos pegar um prédio grande (pai), treinar um modelo neural nele e depois treinar nosso modelo filha.  Além disso, o idioma alvo da tradução será o mesmo: russo.  Intuitivamente, isso pode ser comparado ao aprendizado de um segundo idioma.  É mais fácil aprender, conhecendo uma língua estrangeira.  Também parece estudar uma área estreita de uma língua estrangeira, por exemplo, a terminologia médica da língua inglesa: primeiro você precisa aprender inglês em geral. <br><br>  Como um corpo parental, tentamos tomar 1 milhão de pares de sentenças <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow">do corpo paralelo inglês-russo</a> e 1 milhão <a href="" rel="nofollow">do corpo cazaque-russo</a> .  Existem 5 milhões de frases nos dados do Cazaque.  Desses, apenas aqueles com um coeficiente de conformidade (terceira coluna) superior a 2. A versão cazaque apresentou resultados um pouco melhores.  Parece intuitivamente que isso seja compreensível, pois as línguas chuvash e cazaque são mais parecidas entre si.  Mas, na verdade, isso não está comprovado e também depende muito da qualidade do caso.  Mais detalhes sobre a seleção do corpo dos pais podem ser encontrados <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow">neste artigo</a> .  Sobre o corpo subsidiário de 90.000 pares de ofertas, você pode <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow">descobrir e solicitar dados de amostra aqui.</a> <br><br>  Agora para o código.  Se você não possui sua própria placa gráfica rápida, você pode treinar o modelo no site da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow">Colab</a> .  Para o treinamento, usamos a biblioteca <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow">Sockeye</a> .  Supõe-se que o Python3 já esteja instalado. <br><br><pre><code class="bash hljs">pip install sockeye</code> </pre> <br>  Você também pode precisar mexer em separado com o <a href="" rel="nofollow">MXNet</a> , responsável por trabalhar com a placa de vídeo.  O Colab precisa de instalação adicional da biblioteca <br><br><pre> <code class="bash hljs">pip install mxnet-cu100mkl</code> </pre> <br>  Sobre as redes neurais, geralmente é aceito que basta alimentar os dados como estão, e eles descobrirão isso.  Mas, na realidade, esse nem sempre é o caso.  Portanto, no nosso caso, o corpo precisa ser pré-processado.  Primeiro, a tokenizamos para que seja mais fácil para os modelos entenderem que "gato!" E "gato" são praticamente a mesma coisa.  Por exemplo, apenas um tokenizer python serve. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.tokenize <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> WordPunctTokenizer <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tokenize</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(src_filename, new_filename)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(src_filename, encoding=<span class="hljs-string"><span class="hljs-string">"utf-8"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> src_file: <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(new_filename, <span class="hljs-string"><span class="hljs-string">"w"</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">"utf-8"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> new_file: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> src_file: new_file.write(<span class="hljs-string"><span class="hljs-string">"%s"</span></span> % <span class="hljs-string"><span class="hljs-string">' '</span></span>.join(WordPunctTokenizer().tokenize(line))) new_file.write(<span class="hljs-string"><span class="hljs-string">"\n"</span></span>)</code> </pre> <br>  Como resultado, alimentamos pares de frases no formulário <br><br><pre> <code class="xml hljs">  ӗ  ҫ ӳ ӑӑ. ӑ ӑ ӑ ӑӗ, ӑ ӑӑӗ,   ӑ  ӗӗ -ӑ ӗӗҫ,  ҫӗ ӗ ӗҫ ӑӑ ӑӑ, ҫ ӗ ӗ   ӑ ӑ ӑӑ ӑ .</code> </pre> <br>  e <br><br><pre> <code class="xml hljs">     .  , ,       , ,    ,        .</code> </pre> <br>  A saída são as seguintes ofertas tokenizadas: <br><br><pre> <code class="xml hljs">  ӗ  ҫ ӳ ӑӑ . ӑ ӑ ӑ ӑӗ , ӑ ӑӑӗ ,   ӑ  ӗӗ  - ӑ ӗӗҫ ,  ҫӗ ӗ ӗҫ ӑӑ ӑӑ , ҫ ӗ ӗ   ӑ ӑ ӑӑ ӑ  .</code> </pre> <br>  e em russo <br><br><pre> <code class="xml hljs">      .   ,  ,        ,  ,     ,         .</code> </pre> <br>  No nosso caso, precisaremos dos dicionários combinados dos casos pai e filho, para criar arquivos comuns: <br><br><pre> <code class="bash hljs">cp kk.parent.train.tok kkchv.all.train.tok cat chv.child.train.tok &gt;&gt; kk.parent.train.tok cp ru.parent.train.tok ru.all.train.tok cat ru.child.train.tok &gt;&gt; ru.all.train.tok</code> </pre> <br>  uma vez que o treinamento adicional do modelo filho ocorre no mesmo dicionário. <br><br>  Agora, uma digressão pequena, mas importante.  No MP, as frases são divididas em átomos na forma de palavras e depois operam nas frases como sequências de palavras.  Mas isso geralmente não é suficiente, porque uma cauda enorme é formada a partir das palavras que ocorrem no corpus uma vez.  Construir um modelo probabilístico para eles é difícil.  Isto é especialmente verdade para idiomas com morfologia desenvolvida (caso, sexo, número).  O russo e o chuvash são exatamente esses idiomas.  Mas existe uma solução.  Você pode dividir a frase em um nível inferior, em subpalavras.  Usamos a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow">codificação de pares de bytes.</a> <br><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/rsennrich/subword-nmt.git</code> </pre> <br>  Temos aproximadamente essas seqüências de subpalavras <br><br><pre> <code class="xml hljs">@@   ӗ  ҫ ӳ@@  ӑӑ . @@ ӑ ӑ ӑ @@ ӑӗ , ӑ ӑӑ@@ ӗ ,   ӑ@@  @@  ӗӗ  - ӑ@@  ӗ@@ ӗҫ ,  ҫӗ@@  ӗ ӗҫ@@ @@  ӑӑ ӑӑ , ҫ@@ @@ @@  ӗ ӗ @@ @@  @@  ӑ ӑ ӑӑ ӑ  .</code> </pre> <br>  e <br><br><pre> <code class="xml hljs">@@    @@  @@   . @@  @@ @@ @@ @@  , @@  , @@ @@  @@    @@  @@ @@  @@ @@ @@ @@  ,  ,  @@  @@ @@ @@  @@ @@ @@  ,       @@ @@  @@ @@ @@  .</code> </pre> <br>  Pode-se ver que os afixos são bem diferenciados das palavras: @@ por um longo tempo e bom @@. <br>  Para fazer isso, prepare dicionários bpe <br><br><pre> <code class="bash hljs">python subword-nmt/subword_nmt/learn_joint_bpe_and_vocab.py --input kkchv.all.train.tok ru.all.train.tok -s 10000 -o bpe.codes --write-vocabulary bpe.vocab.kkchv bpe.vocab.ru</code> </pre> <br>  E aplique-os aos tokens, por exemplo: <br><br><pre> <code class="bash hljs">python subword-nmt/subword_nmt/apply_bpe.py -c bpe.codes --vocabulary bpe.vocab.kkchv --vocabulary-threshold 50 &lt; kkchv.all.train.tok &gt; kkchv.all.train.bpe !python subword-nmt/subword_nmt/apply_bpe.py -c bpe.codes --vocabulary bpe.vocab.ru --vocabulary-threshold 50 &lt; ru.all.train.tok &gt; ru.all.train.bpe</code> </pre> <br>  Por analogia, você precisa fazer para todos os arquivos: treinamento, validação e teste de modelos pai e filho. <br><br>  Agora nos voltamos diretamente para o treinamento do modelo neural.  Primeiro, você precisa preparar dicionários gerais de modelo: <br><br><pre> <code class="bash hljs">python -m sockeye.prepare_data -s kk.all.train.bpe -t ru.all.train.bpe -o kkru_all_data</code> </pre> <br>  Em seguida, treine o modelo pai.  Um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow">exemplo simples é descrito em</a> mais detalhes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow">na página Sockeye.</a>  Tecnicamente, o processo consiste em duas etapas: preparação de dados usando dicionários de modelo criados anteriormente <br><br><pre> <code class="bash hljs">python -m sockeye.prepare_data -s kk.parent.train.bpe -t ru.parent.train.bpe -o kkru_parent_data --<span class="hljs-built_in"><span class="hljs-built_in">source</span></span>-vocab kkru_all_data/vocab.src.0.json --target-vocab kkru_all_data/vocab.trg.0.json</code> </pre> <br>  e o próprio aprendizado <br><br><pre> <code class="bash hljs">python -m sockeye.train -d kkru_parent_data -vs kk.parent.dev.bpe -vt ru.parent.dev.bpe --encoder transformer --decoder transformer --transformer-model-size 512 --transformer-feed-forward-num-hidden 256 --transformer-dropout-prepost 0.1 --num-embed 512 --max-seq-len 100 --decode-and-evaluate 500 -o kkru_parent_model --num-layers 6 --<span class="hljs-built_in"><span class="hljs-built_in">disable</span></span>-device-locking --batch-size 1024 --optimized-metric bleu --max-num-checkpoint-not-improved 10</code> </pre> <br>  O treinamento nas instalações da Colab leva cerca de um dia.  Quando o treinamento do modelo estiver concluído, você poderá traduzi-lo para <br><br><pre> <code class="bash hljs">python -m sockeye.translate --input kk.parent.test.bpe -m kkru_parent_model --output ru.parent.test_kkru_parent.bpe</code> </pre> <br>  Para treinar o modelo filho <br><pre> <code class="bash hljs">python -m sockeye.prepare_data -s chv.child.train.bpe -t ru.child.train.bpe -o chvru_child_data --<span class="hljs-built_in"><span class="hljs-built_in">source</span></span>-vocab kkru_all_data/vocab.src.0.json --target-vocab kkru_all_data/vocab.trg.0.json</code> </pre> <br>  O código de início do treinamento é semelhante a este <br><br><pre> <code class="bash hljs">python -m sockeye.train -d chvru_child_data -vs chv.child.dev.bpe -vt ru.child.dev.bpe --encoder transformer --decoder transformer --transformer-model-size 512 --transformer-feed-forward-num-hidden 256 --transformer-dropout-prepost 0.1 --num-embed 512 --max-seq-len 100 --decode-and-evaluate 500 -o ruchv_150K_skv_dev19_model --num-layers 6 --<span class="hljs-built_in"><span class="hljs-built_in">disable</span></span>-device-locking --batch-size 1024 --optimized-metric bleu --max-num-checkpoint-not-improved 10 --config kkru_parent_model/args.yaml --params kkru_parent_model/params.best</code> </pre> <br>  São adicionados parâmetros que indicam que a configuração e os pesos do modelo pai devem ser usados ​​como ponto de partida.  Detalhes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow">no exemplo com a reciclagem do Sockeye</a> .  Aprender um modelo infantil converge em cerca de 12 horas. <br><br>  Para resumir, compare os resultados.  O modelo usual de tradução automática produziu 24,96 BLEU de qualidade, enquanto o modelo de transferência de conhecimento foi de 32,38 BLEU.  A diferença é visível também visualmente em exemplos de traduções.  Portanto, enquanto continuamos a montar o gabinete, usaremos esse modelo. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt475750/">https://habr.com/ru/post/pt475750/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt475740/index.html">O livro "Programação das Olimpíadas" foi lançado</a></li>
<li><a href="../pt475742/index.html">Caso do RetouchMe: o que obtivemos ao localizar o aplicativo em 35 idiomas</a></li>
<li><a href="../pt475744/index.html">Modelo de Administrador do Sistema em Quatro Níveis</a></li>
<li><a href="../pt475746/index.html">Anatomia de Sistemas Acústicos: Cermetos e Compósitos - Sobre os Difusores de Áudio do Monitor</a></li>
<li><a href="../pt475748/index.html">22 de novembro de Moscou - AnalyzeIT MeetUp No. 3</a></li>
<li><a href="../pt475754/index.html">Shorts sobre Scrum</a></li>
<li><a href="../pt475758/index.html">Níveis de assinatura renováveis ​​automaticamente no aplicativo iOS</a></li>
<li><a href="../pt475760/index.html">Desenvolvedores juniores - por que os contratamos e como trabalhamos com eles</a></li>
<li><a href="../pt475762/index.html">Um currículo com uma fotografia voa para uma urna. Características da procura de emprego nos EUA</a></li>
<li><a href="../pt475764/index.html">Cadeias de Markov para geração processual de edifícios</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>