<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ûó üóùÔ∏è üé´ So k√∂nnen Sie einen Monolithen in Dienste zerlegen und die Leistung von In-Memory-Caches aufrechterhalten, ohne die Konsistenz zu verlieren üõê üî≠ ü§¥üèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo allerseits. Mein Name ist Alexander, ich bin Java-Entwickler in der Tinkoff-Unternehmensgruppe. 

 In diesem Artikel m√∂chte ich meine Erfahrunge...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>So k√∂nnen Sie einen Monolithen in Dienste zerlegen und die Leistung von In-Memory-Caches aufrechterhalten, ohne die Konsistenz zu verlieren</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/tinkoff/blog/474994/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/io/g8/yg/iog8ygcpquu5zvi7xmlsecbbwmu.png"></div><br>  Hallo allerseits.  Mein Name ist Alexander, ich bin Java-Entwickler in der Tinkoff-Unternehmensgruppe. <br><br>  In diesem Artikel m√∂chte ich meine Erfahrungen bei der L√∂sung von Problemen im Zusammenhang mit der Synchronisierung des Cachezustands in verteilten Systemen mitteilen.  Wir sind ihnen begegnet und haben unsere monolithische Anwendung in <s>Mikrodienste aufgeteilt</s> .  Offensichtlich werden wir √ºber das Zwischenspeichern von Daten auf JVM-Ebene sprechen, da bei externen Caches Synchronisationsprobleme au√üerhalb des Anwendungskontexts gel√∂st werden. <br><br>  In diesem Artikel werde ich √ºber unsere Erfahrungen mit der Umstellung auf eine serviceorientierte Architektur, begleitet von einem Wechsel zu Kubernetes, und √ºber die L√∂sung damit zusammenh√§ngender Probleme sprechen.  Wir werden den Ansatz zur Organisation des verteilten Cachingsystems In-Memory Data Grid (IMDG) mit seinen Vor- und Nachteilen betrachten, weshalb wir beschlossen haben, eine eigene L√∂sung zu schreiben. <br><br>  Dieser Artikel beschreibt ein Projekt, dessen Backend in Java geschrieben ist.  Daher werden wir auch √ºber Standards im Bereich des tempor√§ren In-Memory-Cachings sprechen.  Wir diskutieren die JSR-107-Spezifikation, die fehlgeschlagene JSR-347-Spezifikation und die Caching-Funktionen im Fr√ºhjahr.  Willkommen bei Katze! <br><a name="habracut"></a><br><br><h1>  Und lassen Sie uns die Anwendung in Dienste schneiden ... </h1><br>  Wir werden uns der serviceorientierten Architektur und Kubernetes zuwenden - das haben wir vor etwas mehr als 6 Monaten entschieden.  Lange Zeit war unser Projekt ein Monolith, viele Probleme im Zusammenhang mit der technischen Verschuldung haben sich angesammelt, und wir haben neue Anwendungsmodule als separate Dienste geschrieben.  Infolgedessen war der √úbergang zu einer serviceorientierten Architektur und einem Monolithschnitt unvermeidlich. <br><br>  Unsere Anwendung ist geladen, im Durchschnitt kommen 500 U / min zu Webdiensten (in der Spitze erreichen sie 900 U / min).  Um das gesamte Datenmodell als Antwort auf jede Anforderung zu erfassen, m√ºssen Sie mehrere hundert Mal zu den verschiedenen Caches wechseln. <br><br>  Wir versuchen, nicht √∂fter als dreimal pro Anforderung zum Remote-Cache zu wechseln, je nach dem erforderlichen Datensatz. Bei internen JVM-Caches erreicht die Last 90.000 Rps pro Cache.  Wir haben ungef√§hr 30 solcher Caches f√ºr eine Vielzahl von Entit√§ten und DTO-shki.  Bei einigen geladenen Caches k√∂nnen wir es uns nicht einmal leisten, den Wert zu l√∂schen, da dies zu einer Verl√§ngerung der Antwortzeit von Webdiensten und zu einem Absturz der Anwendung f√ºhren kann. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mn/db/2v/mndb2vp6ot9byy_fqysaooom6j0.png"></div><br>  So sieht die Last√ºberwachung aus, die w√§hrend des Tages von den internen Caches auf jedem Knoten entfernt wird.  Anhand des Lastprofils ist leicht zu erkennen, dass es sich bei den meisten Anforderungen um gelesene Daten handelt.  Eine gleichm√§√üige Schreiblast ist darauf zur√ºckzuf√ºhren, dass die Werte in den Caches mit einer bestimmten H√§ufigkeit aktualisiert werden. <br><br>  Ausfallzeiten gelten nicht f√ºr unsere Anwendung.  Aus diesem Grund haben wir zum Zweck einer nahtlosen Bereitstellung den gesamten eingehenden Datenverkehr auf zwei Knoten verteilt und die Anwendung mithilfe der Methode "Rolling Update" bereitgestellt.  Kubernetes wurde zu unserer idealen Infrastrukturl√∂sung beim Wechsel zu Diensten.  Somit haben wir mehrere Probleme auf einmal gel√∂st. <br><br><h3>  Das Problem, st√§ndig Infrastruktur f√ºr neue Dienste zu bestellen und einzurichten </h3><br>  Wir erhielten einen Namespace im Cluster f√ºr jede Schaltung, f√ºr die wir drei haben: dev - f√ºr Entwickler, qa - f√ºr Tester, prod - f√ºr Kunden. <br><br>  Wenn der Namespace markiert ist, m√ºssen beim Hinzuf√ºgen eines neuen Dienstes oder einer neuen Anwendung vier Manifeste erstellt werden: Deployment, Service, Ingress und ConfigMap. <br><br><h3>  Hohe Belastungstoleranz </h3><br>  Das Gesch√§ft w√§chst und w√§chst stetig - vor einem Jahr war die durchschnittliche Auslastung doppelt so hoch wie die derzeitige. <br><br>  Die horizontale Skalierung in Kubernetes erm√∂glicht es Ihnen, die Skaleneffekte mit zunehmender Arbeitsbelastung des entwickelten Projekts auszugleichen. <br><br><h3>  Wartung, Protokollierung und √úberwachung </h3><br>  Das Leben wird viel einfacher, wenn Sie keine Protokolle zum Protokollierungssystem hinzuf√ºgen m√ºssen, wenn Sie jeden Knoten hinzuf√ºgen, den Messbereich konfigurieren (es sei denn, Sie haben ein Push-√úberwachungssystem), Netzwerkeinstellungen vornehmen und einfach die f√ºr den Betrieb erforderliche Software installieren. <br><br>  Nat√ºrlich kann dies alles mit Ansible oder Terraform automatisiert werden, aber letztendlich ist es viel einfacher, mehrere Manifeste f√ºr jeden Service zu schreiben. <br><br><h3>  Hohe Zuverl√§ssigkeit </h3><br>  Dank des integrierten k8s-Mechanismus f√ºr Liveness- und Readiness-Samples k√∂nnen Sie sich keine Sorgen machen, dass die Anwendung langsamer wird oder gar nicht mehr reagiert. <br><br>  Kubernetes kontrolliert jetzt den Lebenszyklus von Herd-Pods, die Anwendungscontainer enthalten, und den Verkehr, der zu ihnen geleitet wird. <br><br>  Zusammen mit den beschriebenen Annehmlichkeiten mussten wir eine Reihe von Problemen l√∂sen, um die Dienste f√ºr die horizontale Skalierung und die Verwendung eines gemeinsamen Datenmodells f√ºr viele Dienste geeignet zu machen.  Es mussten zwei Probleme gel√∂st werden: <br><br><ol><li>  <b>Der Status der Anwendung.</b>  Wenn das Projekt im k8s-Cluster bereitgestellt wird, werden Pods mit Containern der neuen Version der Anwendung erstellt, die sich nicht auf den Status der Pods der vorherigen Version beziehen.  Neue Anwendungs-Pods k√∂nnen auf beliebigen Clusterservern erstellt werden, die die angegebenen Einschr√§nkungen erf√ºllen.  Au√üerdem kann jetzt jeder Anwendungscontainer, der im Kubernetes-Pod ausgef√ºhrt wird, jederzeit zerst√∂rt werden, wenn der Liveness-Test einen Neustart anzeigt. </li><li>  <b>Datenkonsistenz.</b>  Auf allen Knoten m√ºssen Konsistenz und Datenintegrit√§t gew√§hrleistet sein.  Dies gilt insbesondere dann, wenn mehrere Knoten in einem einzigen Datenmodell arbeiten.  Es ist inakzeptabel, dass inkonsistente Daten beim Client eingehen, wenn Anforderungen an verschiedene Knoten der Anwendung in der Antwort gestellt werden. </li></ol><br>  In der modernen Entwicklung skalierbarer Systeme ist die zustandslose Architektur die L√∂sung f√ºr die oben genannten Probleme.  Das erste Problem wurde behoben, indem alle statischen Daten in den S3-Cloud-Speicher verschoben wurden. <br><br>  Aufgrund der Notwendigkeit, ein komplexes Datenmodell zu aggregieren und die Antwortzeit unserer Webservices zu verk√ºrzen, konnten wir das Speichern von Daten in In-Memory-Caches jedoch nicht ablehnen.  Um das zweite Problem zu l√∂sen, haben sie eine Bibliothek geschrieben, um den Zustand der internen Caches einzelner Knoten zu synchronisieren. <br><br><h1>  Wir synchronisieren Caches auf separaten Knoten </h1><br>  Als Ausgangsdaten haben wir ein verteiltes System bestehend aus N Knoten.  Jeder Knoten verf√ºgt √ºber ungef√§hr 20 In-Memory-Caches, deren Daten mehrmals pro Stunde aktualisiert werden. <br><br>  Die meisten Caches verf√ºgen √ºber eine TTL-Datenaktualisierungsrichtlinie (Time-to-Live). Einige Daten werden aufgrund der hohen Auslastung alle 20 Minuten mit einer CRON-Operation aktualisiert.  Die Arbeitsbelastung der Caches variiert zwischen mehreren Tausend U / min in der Nacht und mehreren Zehntausend im Laufe des Tages.  Die Spitzenlast √ºberschreitet in der Regel 100.000 U / min nicht.  Die Anzahl der Datens√§tze im tempor√§ren Speicher √ºberschreitet nicht mehrere Hunderttausend und wird auf dem Heap eines Knotens abgelegt. <br><br>  Unsere Aufgabe ist es, die Datenkonsistenz zwischen demselben Cache auf verschiedenen Knoten sowie die k√ºrzestm√∂gliche Antwortzeit zu erreichen.  √úberlegen Sie, wie Sie dieses Problem im Allgemeinen l√∂sen k√∂nnen. <br><br>  Die erste und einfachste L√∂sung besteht darin, alle Informationen in einem Remote-Cache abzulegen.  In diesem Fall k√∂nnen Sie den Status der Anwendung vollst√§ndig aufheben, sich keine Gedanken √ºber die Probleme beim Erreichen der Konsistenz machen und einen einzigen Zugriffspunkt auf ein tempor√§res Data Warehouse haben. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ez/zm/bg/ezzmbg3cuvhczrwpwmh5hg5a0-o.png"></div><br>  Diese Methode der tempor√§ren Datenspeicherung ist recht einfach und wird von uns verwendet.  Wir speichern einen Teil der Daten in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Redis</a> , einem NoSQL-Datenspeicher im RAM.  In Redis zeichnen wir normalerweise ein Web-Service-Antwort-Framework auf und m√ºssen diese Daten f√ºr jede Anforderung mit relevanten Informationen anreichern, f√ºr die wir mehrere hundert Anforderungen an den lokalen Cache senden m√ºssen. <br><br>  Offensichtlich k√∂nnen wir die Daten interner Caches nicht f√ºr die Remotespeicherung entfernen, da die Kosten f√ºr die √úbertragung eines solchen Verkehrsvolumens √ºber das Netzwerk es uns nicht erm√∂glichen, die erforderliche Antwortzeit einzuhalten. <br><br>  Die zweite Option ist die Verwendung eines <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">In-Memory Data Grid</a> (IMDG), bei dem es sich um einen verteilten In-Memory-Cache handelt.  Das Schema einer solchen L√∂sung ist wie folgt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/l7/ku/k9/l7kuk9uaiuydck1hs1boeykflys.png"></div><br>  Die IMDG-Architektur basiert auf dem Prinzip der Datenpartitionierung interner Caches einzelner Knoten.  Tats√§chlich kann dies eine Hash-Tabelle genannt werden, die auf einem Cluster von Knoten verteilt ist.  IMDG gilt als eine der schnellsten Implementierungen von tempor√§rem verteiltem Speicher. <br><br>  Es gibt viele IMDG-Implementierungen, die beliebtesten sind <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hazelcast</a> .  Mit dem verteilten Cache k√∂nnen Sie Daten im RAM auf mehreren Anwendungsknoten mit einem akzeptablen Ma√ü an Zuverl√§ssigkeit und Wahrung der Konsistenz speichern, was durch die Datenreplikation erreicht wird. <br><br>  Die Aufgabe, einen solchen verteilten Cache zu erstellen, ist nicht einfach. Die Verwendung einer vorgefertigten IMDG-L√∂sung k√∂nnte jedoch ein guter Ersatz f√ºr JVM-Caches sein und die Probleme der Replikation, Konsistenz und Datenverteilung zwischen allen Anwendungsknoten beseitigen. <br><br>  Die meisten IMDG-Anbieter f√ºr Java-Anwendungen implementieren <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">JSR-107</a> , die Standard-Java-API f√ºr die Arbeit mit internen Caches.  Im Allgemeinen hat dieser Standard eine ziemlich gro√üe Geschichte, auf die ich im Folgenden n√§her eingehen werde. <br><br>  Es waren einmal Ideen, Ihre Schnittstelle f√ºr die Interaktion mit IMDG - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">JSR 347</a> zu implementieren.  Die Implementierung einer solchen API wurde jedoch von der Java-Community nicht ausreichend unterst√ºtzt, und jetzt verf√ºgen wir √ºber eine einzige Schnittstelle f√ºr die Interaktion mit In-Memory-Caches, unabh√§ngig von der Architektur unserer Anwendung.  Gut oder schlecht ist eine andere Frage, aber es erm√∂glicht uns, alle Schwierigkeiten bei der Implementierung eines verteilten In-Memory-Caches zu ignorieren und damit als Cache einer monolithischen Anwendung zu arbeiten. <br><br>  Trotz der offensichtlichen Vorteile der Verwendung von IMDG ist diese L√∂sung immer noch langsamer als der Standard-JVM-Cache, da die fortlaufende Replikation von Daten, die auf mehrere JVM-Knoten verteilt sind, sowie die Sicherung dieser Daten √ºberfl√ºssig wird.  In unserem Fall war die Datenmenge f√ºr die tempor√§re Speicherung nicht so gro√ü, und die Daten mit einem Spielraum passen in den Speicher einer Anwendung. Daher schien die Zuordnung zu mehreren JVMs eine √ºberm√§√üige L√∂sung zu sein.  Zus√§tzlicher Netzwerkverkehr zwischen Anwendungsknoten unter hoher Last kann die Leistung erheblich beeintr√§chtigen und die Antwortzeit von Webdiensten verl√§ngern.  Am Ende haben wir beschlossen, eine eigene L√∂sung f√ºr dieses Problem zu schreiben. <br><br>  Wir haben In-Memory-Caches als tempor√§ren Datenspeicher belassen und zur Wahrung der Konsistenz den RabbitMQ-Warteschlangenmanager verwendet.  Wir haben das Designmuster <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Publisher-Subscriber √ºbernommen</a> und die Relevanz der Daten beibehalten, indem wir den ge√§nderten Eintrag aus dem Cache jedes Knotens gel√∂scht haben.  Das L√∂sungsschema lautet wie folgt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/05/y9/di/05y9dihavtlltrsgxxnkvuj5bhs.png"></div><br>  Das Diagramm zeigt einen Cluster von N Knoten, von denen jeder √ºber einen Standard-In-Memory-Cache verf√ºgt.  Alle Knoten verwenden ein gemeinsames Datenmodell und m√ºssen konsistent sein.  Beim ersten Zugriff auf den Cache √ºber einen beliebigen Schl√ºssel fehlt der Wert im Cache, und wir geben den tats√§chlichen Wert aus der Datenbank in den Cache ein.  Bei jeder √Ñnderung - l√∂schen Sie den Datensatz. <br><br>  Tats√§chliche Informationen in der Cache-Antwort werden hier durch Synchronisieren des L√∂schens eines Eintrags bereitgestellt, wenn dieser an einem der Knoten ge√§ndert wird.  Jeder Knoten im System hat eine Warteschlange im RabbitMQ-Warteschlangenmanager.  Die Aufzeichnung in allen Warteschlangen erfolgt √ºber einen gemeinsamen Zugriffspunkt vom Typ Thema.  Dies bedeutet, dass an Topic gesendete Nachrichten in alle damit verbundenen Warteschlangen fallen.  Wenn Sie also den Wert auf einem beliebigen Knoten des Systems √§ndern, wird dieser Wert aus dem tempor√§ren Speicher jedes Knotens gel√∂scht, und durch nachfolgenden Zugriff wird der aktuelle Wert aus der Datenbank in den Cache geschrieben. <br><br>  √úbrigens gibt es in Redis einen √§hnlichen PUB / SUB-Mechanismus.  Aber meiner Meinung nach ist es immer noch besser, den Warteschlangenmanager f√ºr die Arbeit mit Warteschlangen zu verwenden, und RabbitMQ war perfekt f√ºr unsere Aufgabe. <br><br><h1>  JSR 107 Standard und dessen Implementierung </h1><br>  Die Standard-Java-Cache-API f√ºr die tempor√§re Speicherung von Daten im Speicher (Spezifikation <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">JSR-107</a> ) hat eine ziemlich lange Geschichte und wurde seit 12 Jahren entwickelt. <br><br>  In so langer Zeit haben sich die Ans√§tze zur Softwareentwicklung ge√§ndert, die Monolithen wurden durch Mikroservice-Architekturen ersetzt.  Aufgrund eines so langen Mangels an Spezifikationen f√ºr die Cache-API gab es sogar Anfragen, API-Caches f√ºr verteilte Systeme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">JSR-347</a> (Data Grids f√ºr die Java-Plattform) zu entwickeln.  Nach der lang erwarteten Ver√∂ffentlichung von JSR-107 und der Ver√∂ffentlichung von JCache wurde die Aufforderung zur Erstellung einer separaten Spezifikation f√ºr verteilte Systeme zur√ºckgezogen. <br><br>  Im Laufe der langen 12 Jahre auf dem Markt hat sich der Ort f√ºr die tempor√§re Datenspeicherung mit der Ver√∂ffentlichung von Java 1.5 von HashMap zu ConcurrentHashMap ge√§ndert, und sp√§ter wurden viele fertige Open-Source-Implementierungen von In-Memory-Caching ver√∂ffentlicht. <br><br>  Nach der Ver√∂ffentlichung von JSR-107 begannen Anbieterl√∂sungen, die neue Spezifikation schrittweise umzusetzen.  F√ºr JCache gibt es sogar Anbieter, die sich auf verteiltes Caching spezialisiert haben - die eigentlichen Data Grids, deren Spezifikation noch nie implementiert wurde. <br><br>  √úberlegen Sie, woraus das Paket <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">javax.cache besteht</a> und wie Sie eine Cache-Instanz f√ºr unsere Anwendung erhalten: <br><pre><code class="java hljs">CachingProvider provider = Caching.getCachingProvider(<span class="hljs-string"><span class="hljs-string">"org.cache2k.jcache.provider.JCacheProvider"</span></span>); CacheManager cacheManager = provider.getCacheManager(); CacheConfiguration&lt;Integer, String&gt; config = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> MutableConfiguration&lt;Integer, String&gt;() .setTypes(Integer.class, String.class) .setReadThrough(<span class="hljs-keyword"><span class="hljs-keyword">true</span></span>) . . .; Cache&lt;Integer, String&gt; cache = cacheManager.createCache(cacheName, config);</code> </pre> <br>  Hier ist Caching ein Bootloader f√ºr CachingProvider. <br><br>  In unserem Fall wird JCacheProvider, die Cache2k-Implementierung des JSR-107-Anbieters <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SPI</a> , von ClassLoader geladen.  F√ºr den Loader m√ºssen Sie m√∂glicherweise nicht die Provider-Implementierung angeben, aber dann wird versucht, die Implementierung zu laden, in der sich der Loader befindet <br><blockquote>  META-INF / services / javax.cache.spi.CachingProvider </blockquote><br>  In jedem Fall sollte es in ClassLoader eine einzige Implementierung des CachingProviders geben. <br><br>  Wenn Sie die Bibliothek javax.cache ohne Implementierung verwenden, wird beim Versuch, JCache zu erstellen, eine Ausnahme ausgel√∂st.  Der Anbieter erstellt und verwaltet den Lebenszyklus von CacheManager, der seinerseits f√ºr die Verwaltung und Konfiguration der Caches verantwortlich ist.  Um einen Cache zu erstellen, m√ºssen Sie folgenderma√üen vorgehen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fx/lp/fs/fxlpfsdfmlifqegpwtaynhiakt8.png"></div><br>  Die mit CacheManager erstellten Standard-Caches m√ºssen implementierungskompatibel konfiguriert sein.  Die von javax.cache bereitgestellte standardm√§√üige parametrisierte CacheConfiguration kann auf eine bestimmte CacheProvider-Implementierung erweitert werden. <br><br>  Heutzutage gibt es Dutzende verschiedener Implementierungen der JSR-107-Spezifikation: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ehcache</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Guava</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Koffein</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Cache2k</a> .  Bei vielen Implementierungen handelt es sich um In-Memory- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Datenraster</a> in verteilten Systemen - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hazelcast</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Oracle Coherence</a> . <br><br>  Es gibt auch viele tempor√§re Speicherimplementierungen, die die Standard-API nicht unterst√ºtzen.  In unserem Projekt haben wir lange Zeit Ehcache 2 verwendet, das nicht mit JCache kompatibel ist (Implementierung der Spezifikation erschien mit Ehcache 3).  Die Notwendigkeit eines √úbergangs zu einer JCache-kompatiblen Implementierung ergab sich mit der Notwendigkeit, den Status von In-Memory-Caches zu √ºberwachen.  Mit der Standard-MetricRegistry konnte die √úberwachung nur mit Hilfe der JCacheGaugeSet-Implementierung beschleunigt werden, die Metriken aus dem Standard-JCache sammelt. <br><br>  Wie w√§hle ich die passende In-Memory-Cache-Implementierung f√ºr mein Projekt aus?  Vielleicht sollten Sie folgendes beachten: <br><br><ol><li>  Ben√∂tigen Sie Unterst√ºtzung f√ºr die JSR-107-Spezifikation? </li><li>  Es lohnt sich auch, auf die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Geschwindigkeit der</a> ausgew√§hlten Implementierung zu achten.  Unter hoher Last kann sich die Leistung interner Caches erheblich auf die Reaktionszeit Ihres Systems auswirken. </li><li>  Unterst√ºtzung im Fr√ºhling.  Wenn Sie das bekannte Framework in Ihrem Projekt verwenden, sollten Sie ber√ºcksichtigen, dass nicht jede JVM-Cache-Implementierung im Fr√ºhjahr √ºber einen kompatiblen CacheManager verf√ºgt. </li></ol><br>  Wenn Sie Spring genau wie wir in Ihrem Projekt aktiv einsetzen, folgen Sie beim Zwischenspeichern von Daten h√∂chstwahrscheinlich dem aspektorientierten Ansatz (AOP) und verwenden die Annotation @Cacheable.  Spring verwendet ein eigenes CacheManager-SPI, damit Aspekte funktionieren.  Die folgende Bean ist erforderlich, damit Spring Caches funktionieren: <br><pre> <code class="java hljs"><span class="hljs-meta"><span class="hljs-meta">@Bean</span></span> <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> org.springframework.cache.<span class="hljs-function"><span class="hljs-function">CacheManager </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">cacheManager</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ CachingProvider provider = Caching.getCachingProvider(); CacheManager cacheManager = provider.getCacheManager(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> JCacheCacheManager(cacheManager); }</code> </pre><br>  Um mit Caches im AOP-Paradigma arbeiten zu k√∂nnen, m√ºssen auch Transaktionsaspekte ber√ºcksichtigt werden.  Der Spring-Cache muss unbedingt das Transaktionsmanagement unterst√ºtzen.  Zu diesem Zweck erbt spring CacheManager die AbstractTransactionSupportingCacheManager-Eigenschaften, mit denen Put- / Evict-Vorg√§nge innerhalb einer Transaktion synchronisiert und erst ausgef√ºhrt werden k√∂nnen, nachdem eine erfolgreiche Transaktion festgeschrieben wurde. <br><br>  Das obige Beispiel zeigt die Verwendung des JCacheCacheManager-Wrappers f√ºr den Cache-Spezifikationsmanager.  Dies bedeutet, dass jede JSR-107-Implementierung auch mit Spring CacheManager kompatibel ist.  Dies ist ein weiterer Grund, sich f√ºr einen In-Memory-Cache mit Unterst√ºtzung der JSR-Spezifikation f√ºr Ihr Projekt zu entscheiden.  Wenn Sie diese Unterst√ºtzung jedoch immer noch nicht ben√∂tigen, aber wirklich @Cacheable verwenden m√∂chten, stehen Ihnen zwei weitere interne Cache-L√∂sungen zur Verf√ºgung: EhCacheCacheManager und CaffeineCacheManager. <br><br>  Bei der Auswahl der Implementierung des speicherinternen Caches haben wir, wie bereits erw√§hnt, die Unterst√ºtzung von IMDG f√ºr verteilte Systeme nicht ber√ºcksichtigt.  Um die Leistung von JVM-Caches auf unserem System aufrechtzuerhalten, haben wir unsere eigene L√∂sung geschrieben. <br><br><h1>  L√∂schen von Caches in einem verteilten System </h1><br>  Mit modernen IMDGs, die in Projekten mit Mikroservice-Architektur verwendet werden, k√∂nnen Sie Daten im Arbeitsspeicher √ºber eine skalierbare Datenpartitionierung mit der erforderlichen Redundanzstufe auf alle Arbeitsknoten des Systems verteilen. <br><br>  In diesem Fall treten viele Probleme im Zusammenhang mit der Synchronisation, der Datenkonsistenz usw. auf, ganz zu schweigen von der Zunahme der Zugriffszeit auf den tempor√§ren Speicher.  Ein solches Schema ist redundant, wenn die Menge der verwendeten Daten in den RAM eines Knotens passt. Um die Konsistenz der Daten zu gew√§hrleisten, reicht es aus, diesen Eintrag auf allen Knoten zu l√∂schen, wenn sich der Cache-Wert √§ndert. <br><br>  Bei der Implementierung einer solchen L√∂sung f√§llt zun√§chst die Idee ein, einen EventListener zu verwenden. In JCache gibt es einen CacheEntryRemovedListener f√ºr den Fall, dass ein Eintrag aus dem Cache gel√∂scht wird.  Es ist anscheinend ausreichend, eine eigene Listener-Implementierung hinzuzuf√ºgen, die beim L√∂schen des Datensatzes Nachrichten an das Thema sendet, und der Eutect-Cache auf allen Knoten ist bereit - vorausgesetzt, jeder Knoten wartet auf Ereignisse aus der Warteschlange, die dem allgemeinen Thema zugeordnet sind (siehe Abbildung) oben. <br><br>  Bei Verwendung dieser L√∂sung sind die Daten auf verschiedenen Knoten inkonsistent, da EventLists in jedem JCache-Implementierungsprozess nach einem Ereignis auftreten.  Wenn sich f√ºr den angegebenen Schl√ºssel kein Datensatz im lokalen Cache befindet und f√ºr denselben Schl√ºssel ein Datensatz auf einem anderen Knoten vorhanden ist, wird das Ereignis nicht an das Thema gesendet. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/_a/ev/5c/_aev5cw333ni_cnstj5elxbowhw.png"></div><br>  √úberlegen Sie, wie Sie das Ereignis abfangen k√∂nnen, wenn ein Wert aus dem lokalen Cache gel√∂scht wird. <br><br>  Im Paket javax.cache.event befindet sich neben EventListeners auch ein CacheEntryEventFilter, mit dem gem√§√ü JavaDoc vor dem Senden dieses Ereignisses an den CacheEntryListener gepr√ºft wird, ob es sich um einen Datensatz, eine L√∂schung, eine Aktualisierung oder ein Ereignis im Zusammenhang mit dem Ablauf des Datensatzes handelt im Cache.  Bei Verwendung des Filters bleibt unser Problem bestehen, da die Logik ausgef√ºhrt wird, nachdem das CacheEntryEvent-Ereignis protokolliert wurde und nachdem die CRUD-Operation im Cache ausgef√ºhrt wurde. <br><br>  Trotzdem ist es m√∂glich, die Ausl√∂sung eines Ereignisses abzufangen, um einen Datensatz aus dem Cache zu l√∂schen.  Verwenden Sie dazu das in JCache integrierte Tool, mit dem Sie API-Spezifikationen zum Schreiben und Laden von Daten aus einer externen Quelle verwenden k√∂nnen, wenn diese sich nicht im Cache befinden.  Daf√ºr gibt es im Paket javax.cache.integration zwei Schnittstellen: <br><br><ul><li>  CacheLoader - um die vom Schl√ºssel angeforderten Daten zu laden, wenn sich keine Eintr√§ge im Cache befinden. </li><li>  CacheWriter - zum Schreiben, L√∂schen und Aktualisieren von Daten auf einer externen Ressource beim Aufrufen der entsprechenden Cache-Vorg√§nge. </li></ul><br>  Um die Konsistenz zu gew√§hrleisten, sind die CacheWriter-Methoden in Bezug auf die entsprechende Cache-Operation atomar.  Wir scheinen eine L√∂sung f√ºr unser Problem gefunden zu haben. <br><br>  Jetzt k√∂nnen wir die Konsistenz der Reaktion von In-Memory-Caches auf Knoten beibehalten, wenn Sie unsere Implementierung von CacheWriter verwenden, die Ereignisse an das RabbitMQ-Thema sendet, wenn √Ñnderungen im Datensatz im lokalen Cache vorgenommen werden. <br><br><h1>  Fazit </h1><br>  Bei der Entwicklung eines Projekts muss bei der Suche nach einer geeigneten L√∂sung f√ºr aufkommende Probleme dessen Spezifit√§t ber√ºcksichtigt werden.  In unserem Fall war es aufgrund der charakteristischen Merkmale des Projektdatenmodells, des vererbten Legacy-Codes und der Art der Auslastung nicht m√∂glich, eine der vorhandenen L√∂sungen f√ºr das verteilte Caching-Problem zu verwenden. <br><br>  Es ist sehr schwierig, eine universelle Implementierung auf ein entwickeltes System anzuwenden.  F√ºr jede solche Implementierung gibt es optimale Einsatzbedingungen.  In unserem Fall f√ºhrten die Besonderheiten des Projekts zu der in diesem Artikel beschriebenen L√∂sung.  Wenn jemand ein √§hnliches Problem hat, teilen wir Ihnen gerne unsere L√∂sung mit und ver√∂ffentlichen sie auf GitHub. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de474994/">https://habr.com/ru/post/de474994/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de474982/index.html">JavaFX Tutorial: FXML und SceneBuilder</a></li>
<li><a href="../de474984/index.html">RabbitMQ vs. Kafka: Failover und Hochverf√ºgbarkeit</a></li>
<li><a href="../de474988/index.html">Willkommen bei Mitap: Karriere bei Data Science f√ºr Einsteiger</a></li>
<li><a href="../de474990/index.html">Harte √úbung: Wie baue ich ein WLAN-Netzwerk in einem Stadtpark?</a></li>
<li><a href="../de474992/index.html">Analyse fehlerhafter Laptop-Batterien. Elektrische Radfahrernotizen</a></li>
<li><a href="../de474996/index.html">Die Zusammenfassung der IT-Ereignisse im November (zweiter Teil)</a></li>
<li><a href="../de475000/index.html">√ñffentliches Testen von Ethereum Cloud- und Cloud-L√∂sungen f√ºr Datenschutz und Skalierbarkeit</a></li>
<li><a href="../de475002/index.html">Arbeit ist kein Wolf, Teil 2. √úbergeben Sie den Chef und √ºberleben Sie auf Bew√§hrung</a></li>
<li><a href="../de475004/index.html">Wie viel haben Entwickler mit unterschiedlichen Qualifikationen im ersten Halbjahr 2019 verdient?</a></li>
<li><a href="../de475006/index.html">So erstellen Sie in 28 Stunden einen Prototyp-Dokumentenvergleich und gewinnen einen Hackathon</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>