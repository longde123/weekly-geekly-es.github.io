<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßö ü§öüèº üë©‚Äçüë¶ Audio AI: extraire le chant de la musique √† l'aide de r√©seaux de neurones convolutifs üéÖüèΩ ü§∂ ‚ñ™Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pirater la musique pour d√©mocratiser le contenu d√©riv√© 

 Avertissement: Toutes les propri√©t√©s intellectuelles, conceptions et m√©thodes d√©crites dans ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Audio AI: extraire le chant de la musique √† l'aide de r√©seaux de neurones convolutifs</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/441090/">  <i>Pirater la musique pour d√©mocratiser le contenu d√©riv√©</i> <br><br><blockquote>  <b>Avertissement:</b> Toutes les propri√©t√©s intellectuelles, conceptions et m√©thodes d√©crites dans cet article sont divulgu√©es dans US10014002B2 et US9842609B2. </blockquote><br>  J'aimerais pouvoir revenir en 1965, frapper √† la porte d'entr√©e du studio Abby Road avec un laissez-passer, entrer et entendre les vraies voix de Lennon et McCartney ... Eh bien, essayons.  Entr√©e: MP3 de qualit√© moyenne des Beatles.  La piste sup√©rieure est le mixage d'entr√©e, la piste inf√©rieure est le chant isol√© que notre r√©seau de neurones a mis en √©vidence. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://player.vimeo.com/video/305275806" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><a name="habracut"></a><br>  Formellement, ce probl√®me est connu sous le nom de <i>s√©paration des sources sonores</i> ou <i>s√©paration du signal</i> (s√©paration des sources audio).  Il consiste √† restaurer ou reconstruire un ou plusieurs des signaux d'origine qui, √† la suite d'un processus <i>lin√©aire ou convolutionnel</i> , sont m√©lang√©s √† d'autres signaux.  Ce domaine de recherche a de nombreuses applications pratiques, notamment l'am√©lioration de la qualit√© du son (parole) et l'√©limination du bruit, les remixages musicaux, la distribution spatiale du son, la remasterisation, etc. Les ing√©nieurs du son appellent parfois cette technique le d√©mixage.  Il existe de nombreuses ressources sur ce sujet, de la s√©paration aveugle des signaux avec l'analyse des composants ind√©pendants (ICA) √† la factorisation semi-contr√¥l√©e des matrices non n√©gatives et se terminant par des approches ult√©rieures bas√©es sur des r√©seaux de neurones.  Vous pouvez trouver de bonnes informations sur les deux premiers points dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ces mini-guides</a> du CCRMA, qui √† un moment donn√© m'ont √©t√© tr√®s utiles. <br><br>  <b>Mais avant de plonger dans le d√©veloppement ... un peu de philosophie d'apprentissage automatique appliqu√©e ...</b> <br><br>  J'√©tais engag√© dans le traitement du signal et de l'image avant m√™me que le slogan ¬´l'apprentissage profond ne r√©sout tout¬ª se soit propag√©, je peux donc vous pr√©senter une solution comme un voyage d' <i>ing√©nierie des fonctionnalit√©s</i> et montrer <b>pourquoi le r√©seau neuronal est la meilleure approche pour ce probl√®me particulier</b> .  Pourquoi?  Tr√®s souvent, je vois des gens √©crire quelque chose comme √ßa: <br><br>  <i>¬´Avec l'apprentissage en profondeur, vous n'avez plus √† vous soucier du choix des fonctionnalit√©s;</i>  <i>il le fera pour vous. "</i> <br><br>  ou pire ... <br><br>  <i>"La diff√©rence entre l'apprentissage automatique et l'apprentissage en profondeur</i> [h√© ... l'apprentissage en profondeur est toujours l'apprentissage en machine!] Est <i>que, dans ML, vous extrayez vous-m√™me les attributs, et dans l'apprentissage en profondeur, cela se produit automatiquement au sein du r√©seau."</i> <br><br>  Ces g√©n√©ralisations proviennent probablement du fait que les DNN peuvent √™tre tr√®s efficaces pour explorer de bons espaces cach√©s.  Mais il est donc impossible de g√©n√©raliser.  Je suis tr√®s boulevers√© lorsque de r√©cents dipl√¥m√©s et praticiens succombent aux id√©es fausses ci-dessus et adoptent l'approche du ¬´deep-learning-it-all¬ª.  Par exemple, il suffit de jeter un tas de donn√©es brutes (m√™me apr√®s un petit traitement pr√©liminaire) - et tout fonctionnera comme il se doit.  Dans le monde r√©el, vous devez vous occuper de choses comme les performances, l'ex√©cution en temps r√©el, etc. √Ä cause de telles id√©es fausses, vous serez coinc√© en mode exp√©rience pendant tr√®s longtemps ... <br><br>  <b>L'ing√©nierie des fonctionnalit√©s reste une discipline tr√®s importante dans la conception de r√©seaux de neurones artificiels.</b>  <b>Comme dans toute autre technique de ML, dans la plupart des cas, c'est elle qui distingue les solutions efficaces du niveau de production des exp√©riences infructueuses ou inefficaces.</b>  <b>Une compr√©hension approfondie de vos donn√©es et de leur nature signifie encore beaucoup ...</b> <br><br><h1>  De A √† Z </h1><br>  Ok, j'ai fini le sermon.  Voyons maintenant pourquoi nous sommes ici!  Comme pour tout probl√®me de traitement des donn√©es, voyons d'abord √† quoi il ressemble.  Jetez un coup d'≈ìil √† la prochaine partie de la voix de l'enregistrement studio original. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://player.vimeo.com/video/305288385" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Voix de studio 'One Last Time', Ariana Grande</font></i> <br><br>  Pas trop int√©ressant, non?  Eh bien, c'est parce que nous visualisons le signal <i>dans le temps</i> .  Ici, nous ne voyons que les changements d'amplitude au fil du temps.  Mais vous pouvez extraire toutes sortes d'autres choses, telles que les enveloppes d'amplitude (enveloppe), les valeurs quadratiques moyennes (RMS), le taux de changement des valeurs positives d'amplitude au n√©gatif (taux de passage par z√©ro), etc., mais ces <i>signes sont</i> trop <i>primitifs</i> et pas suffisamment distinctifs, pour aider dans notre probl√®me.  Si nous voulons extraire la voix d'un signal audio, nous devons d'abord d√©terminer d'une mani√®re ou d'une autre la structure de la parole humaine.  Heureusement, la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">transform√©e de Fourier de</a> fen√™tre (STFT) vient √† la rescousse. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://player.vimeo.com/video/305391461" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Spectre d'amplitude STFT - taille de la fen√™tre = 2048, chevauchement = 75%, √©chelle de fr√©quence logarithmique [Sonic Visualizer]</font></i> <br><br>  Bien que j'aime le traitement de la parole et que j'adore jouer avec <i>les simulations de filtres d'entr√©e, les cepstres, les sottotamis, les LPC, les MFCC</i> et ainsi de suite <i>, nous allons</i> ignorer toutes ces absurdit√©s et nous concentrer sur les principaux √©l√©ments li√©s √† notre probl√®me afin que l'article puisse √™tre compris par le plus de personnes possible, pas seulement des sp√©cialistes du traitement du signal. <br><br>  Alors, que nous dit la structure de la parole humaine? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/541/f8d/c74/541f8dc74fecdb1e994d560b44da112d.png"><br><br>  Eh bien, nous pouvons d√©finir trois √©l√©ments principaux ici: <br><br><ul><li>  <b>La fr√©quence fondamentale</b> (f0), qui est d√©termin√©e par la fr√©quence de vibration de nos cordes vocales.  Dans ce cas, Ariana chante dans la plage de 300 √† 500 Hz. <br></li><li>  Une s√©rie d' <b>harmoniques</b> au-dessus de f0 qui suivent une forme ou un motif similaire.  Ces harmoniques apparaissent √† des fr√©quences qui sont des multiples de f0. <br></li><li>  Discours <b>non vois√©</b> , qui comprend des consonnes telles que `` t '', `` p '', `` k '', `` s '' (qui n'est pas produit par la vibration des cordes vocales), la respiration, etc. Tout cela se manifeste sous la forme de courtes salves dans la r√©gion des hautes fr√©quences. </li></ul><br><h1>  Premi√®re tentative avec des r√®gles </h1><br>  Oublions une seconde ce qu'on appelle le machine learning.  Peut-on d√©velopper une m√©thode d'extraction vocale bas√©e sur notre connaissance du signal?  Laisse moi essayer ... <br><br>  <b>Isolement vocal <i>na√Øf</i> V1.0:</b> <br><br><ol><li>  Identifiez les zones vocales.  Il y a beaucoup de choses dans le signal d'origine.  Nous voulons nous concentrer sur les domaines qui contiennent vraiment du contenu vocal et ignorer tout le reste. <br></li><li>  Faites la distinction entre la parole vocale et non vocale.  Comme nous l'avons vu, ils sont tr√®s diff√©rents.  Ils doivent probablement √™tre trait√©s diff√©remment. <br></li><li>  √âvaluez le changement de fr√©quence fondamentale au fil du temps. <br></li><li>  Sur la base de la broche 3, appliquez une sorte de masque pour capturer les harmoniques. <br></li><li>  Faites quelque chose avec des fragments de discours sans voix ... </li></ol><br><img src="https://habrastorage.org/getpro/habr/post_images/ecf/bb4/82e/ecfbb482ea6c1b29b96a13ce5cf8a5a2.gif"><br><br>  Si nous travaillons dignement, le r√©sultat devrait √™tre un <i>masque</i> <i>doux</i> ou <i>binaire</i> , dont l'application √† l'amplitude du STFT (multiplication par √©l√©ments) donne une reconstruction approximative de l'amplitude du chant STFT.  Ensuite, nous combinons ce STFT vocal avec des informations sur la phase du signal d'origine, calculons le STFT inverse et obtenons le signal temporel du vocal reconstruit. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/56d/fb7/012/56dfb70125de59f0e1ee03b58e6c79e6.png"><br><br>  Le faire √† partir de z√©ro est d√©j√† un gros travail.  Mais √† des fins de d√©monstration, l'impl√©mentation de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">l'algorithme pYIN est applicable</a> .  Bien qu'il soit destin√© √† r√©soudre l'√©tape 3, mais avec les param√®tres corrects, il ex√©cute d√©cemment les √©tapes 1 et 2, en suivant la base vocale m√™me en pr√©sence de musique.  L'exemple ci-dessous contient la sortie apr√®s traitement de cet algorithme, sans traitement de la parole non vois√©e. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://player.vimeo.com/video/305636014" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Et quoi ...?  Il semble avoir fait tout le travail, mais il n'y a pas de bonne qualit√© et proche.  Peut-√™tre qu'en d√©pensant plus de temps, d'√©nergie et d'argent, nous am√©liorerons cette m√©thode ... <br><br>  Mais laissez-moi vous demander ... <br><br>  Que se passe-t-il si <b>quelques voix</b> apparaissent sur la piste, et pourtant on la retrouve souvent dans au moins 50% des pistes professionnelles modernes? <br><br>  Que se passe-t-il si les voix sont trait√©es par <b>r√©verb√©ration, retards</b> et autres effets?  Jetons un coup d'≈ìil au dernier refrain d'Ariana Grande de cette chanson. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://player.vimeo.com/video/306589126" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Ressentez-vous d√©j√† de la douleur ...?  Je le suis. <br><br>  De telles m√©thodes sur des r√®gles strictes se transforment tr√®s vite en ch√¢teau de cartes.  Le probl√®me est trop compliqu√©.  Trop de r√®gles, trop d'exceptions et trop de conditions diff√©rentes (effets et r√©glages de mixage).  Une approche en plusieurs √©tapes implique √©galement que les erreurs dans une √©tape prolongent les probl√®mes √† l'√©tape suivante.  L'am√©lioration de chaque √©tape deviendra tr√®s co√ªteuse: il faudra un grand nombre d'it√©rations pour bien faire les choses.  Et enfin, mais non des moindres, il est probable qu'√† la fin, nous aurons un convoyeur tr√®s gourmand en ressources, ce qui en soi peut annuler tous les efforts. <br><br>  <b>Dans une telle situation, il est temps de commencer √† r√©fl√©chir √† une approche plus <i>globale</i> et de laisser ML d√©couvrir une partie des processus et op√©rations de base n√©cessaires pour r√©soudre le probl√®me.</b>  <b>Mais nous devons encore montrer nos comp√©tences et nous engager dans l'ing√©nierie des fonctionnalit√©s, et vous comprendrez pourquoi.</b> <br><br><h1>  Hypoth√®se: utiliser le r√©seau neuronal comme une fonction de transfert qui traduit les mixages en voix </h1><br>  En regardant les r√©alisations des r√©seaux de neurones convolutifs dans le traitement des photos, pourquoi ne pas appliquer la m√™me approche ici? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1f0/356/00a/1f035600aadc5f6bb50d7478984aa1d1.png"><br>  <i><font color="gray">Les r√©seaux de neurones r√©solvent avec succ√®s des probl√®mes tels que la colorisation des images, la nettet√© et la r√©solution.</font></i> <br><br>  En fin de compte, vous pouvez imaginer le signal sonore "comme une image" en utilisant la transform√©e de Fourier √† court terme, non?  Bien que ces <i>images sonores</i> ne correspondent pas √† la distribution statistique des images naturelles, elles ont encore des mod√®les spatiaux (dans l'espace temps et fr√©quence) sur lesquels former le r√©seau. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/54c/b43/7fa/54cb437fa908fbe8b4d36bd120e2d009.png"><br>  <i><font color="gray">√Ä gauche: rythme de batterie et ligne de base ci-dessous, plusieurs sons de synth√©tiseur au milieu, tous m√©lang√©s avec des voix.</font></i>  <i><font color="gray">Droite: seulement le chant</font></i> <br><br>  La r√©alisation d'une telle exp√©rience serait une entreprise co√ªteuse car il est difficile d'obtenir ou de g√©n√©rer les donn√©es de formation n√©cessaires.  Mais en recherche appliqu√©e, j'essaie toujours d'utiliser cette approche: d'abord, <b>pour identifier un probl√®me plus simple qui confirme les m√™mes principes</b> , mais qui ne n√©cessite pas beaucoup de travail.  Cela vous permet d'√©valuer l'hypoth√®se, d'it√©rer plus rapidement et de corriger le mod√®le avec des pertes minimales s'il ne fonctionne pas comme il se doit. <br><br>  La condition implicite est que le <b>r√©seau neuronal doit comprendre la structure de la parole humaine</b> .  Un probl√®me plus simple peut √™tre le suivant: <i>un r√©seau de neurones peut-il d√©terminer la pr√©sence de la parole sur un fragment arbitraire d'un enregistrement sonore</i> .  Nous parlons d'un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d√©tecteur d'activit√© vocale</a> fiable <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">(VAD)</a> , impl√©ment√© sous la forme d'un classificateur binaire. <br><br><h3>  Nous concevons l'espace des signes </h3><br>  Nous savons que les signaux sonores, tels que la musique et la parole humaine, sont bas√©s sur des d√©pendances temporelles.  Autrement dit, rien ne se passe isol√©ment √† un moment donn√©.  Si je veux savoir s'il y a une voix sur un morceau particulier d'enregistrement sonore, alors je dois regarder les r√©gions voisines.  Un tel <i>contexte temporel</i> fournit de bonnes informations sur ce qui se passe dans la zone d'int√©r√™t.  Dans le m√™me temps, il est souhaitable d'effectuer une classification avec de tr√®s petits incr√©ments de temps afin de reconna√Ætre une voix humaine avec la r√©solution temporelle la plus √©lev√©e possible. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/471/d58/2d4/471d582d4d8dad39249584940137d4e3.gif"><br><br>  Comptons un peu ... <br><br><ul><li>  Fr√©quence d'√©chantillonnage (fs): 22050 Hz (nous sous-√©chantillonnons de 44100 √† 22050) <br></li><li>  Conception STFT: taille de fen√™tre = 1024, taille de houblon = 256, interpolation de l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">√©chelle de craie</a> pour le filtre de pond√©ration, en tenant compte de la perception.  Puisque notre entr√©e est <i>r√©elle</i> , vous pouvez travailler avec la moiti√© du STFT (une explication d√©passe le cadre de cet article ...) tout en conservant le composant DC (facultatif), ce qui nous donne 513 bins de fr√©quence. <br></li><li>  R√©solution de classification cible: une trame STFT (~ 11,6 ms = 256/22050) <br></li><li>  Contexte temporel cible: ~ 300 millisecondes = 25 trames STFT. <br></li><li>  Le nombre cible d'exemples de formation: 500 000. <br></li><li>  En supposant que nous utilisons une fen√™tre coulissante par incr√©ments de 1 intervalle de temps STFT pour g√©n√©rer des donn√©es d'entra√Ænement, nous avons besoin d'environ 1,6 heures de son balis√© pour g√©n√©rer 500 000 √©chantillons de donn√©es </li></ul><br>  Avec les exigences ci-dessus, les entr√©es et sorties de notre classificateur binaire sont les suivantes: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f57/01c/bd9/f5701cbd91fe9ba38f89b541b9d4492e.png"><br><br><h3>  Mod√®le </h3><br>  En utilisant Keras, nous allons construire un petit mod√®le de r√©seau neuronal pour tester notre hypoth√®se. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense, Dropout, Flatten, Conv2D, MaxPooling2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.optimizers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SGD <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.advanced_activations <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LeakyReLU model = Sequential() model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">513</span></span>, <span class="hljs-number"><span class="hljs-number">25</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(LeakyReLU()) model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(LeakyReLU()) model.add(MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>))) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.25</span></span>)) model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(LeakyReLU()) model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(LeakyReLU()) model.add(MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>))) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.25</span></span>)) model.add(Flatten()) model.add(Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>)) model.add(LeakyReLU()) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)) sgd = SGD(lr=<span class="hljs-number"><span class="hljs-number">0.001</span></span>, decay=<span class="hljs-number"><span class="hljs-number">1e-6</span></span>, momentum=<span class="hljs-number"><span class="hljs-number">0.9</span></span>, nesterov=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) model.compile(loss=keras.losses.binary_crossentropy, optimizer=sgd, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/447/ebe/19f/447ebe19f6ba953b4af0b4bed1d9e7af.png"></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/7d1/002/083/7d1002083c78486e36dd92959ec5afbd.png"><br><br>  En divisant les donn√©es 80/20 en formation et test apr√®s ~ 50 √©poques, nous obtenons la <b>pr√©cision lors du test ~ 97%</b> .  C'est une preuve suffisante que notre mod√®le est capable de distinguer les voix dans les fragments sonores musicaux (et les fragments sans voix).  Si nous v√©rifions certaines cartes d'entit√©s de la 4√®me couche convolutionnelle, nous pouvons conclure que le r√©seau neuronal semble avoir optimis√© ses noyaux pour effectuer deux t√¢ches: filtrer la musique et filtrer les voix ... <br><br><img src="https://habrastorage.org/getpro/habr/post_images/438/bce/536/438bce536c3aa746a3120e2364b512c8.png"><br>  <i><font color="gray">Un exemple de carte d'objets √† la sortie de la 4√®me couche convolutionnelle.</font></i>  <i><font color="gray">Apparemment, la sortie de gauche est le r√©sultat des op√©rations du noyau dans une tentative de pr√©server le contenu vocal tout en ignorant la musique.</font></i>  <i><font color="gray">Les valeurs √©lev√©es ressemblent √† la structure harmonieuse de la parole humaine.</font></i>  <i><font color="gray">La carte des objets √† droite semble √™tre le r√©sultat de la t√¢che oppos√©e.</font></i> <br><br><h1>  Du d√©tecteur vocal √† la d√©connexion du signal </h1><br>  Apr√®s avoir r√©solu le probl√®me de classification plus simple, comment pouvons-nous passer √† la v√©ritable s√©paration du chant et de la musique?  Eh bien, en regardant la premi√®re m√©thode <i>na√Øve</i> , nous voulons toujours obtenir en quelque sorte un spectrogramme d'amplitude pour le chant.  Maintenant, cela devient une t√¢che de r√©gression.  Ce que nous voulons faire est de calculer le spectre d'amplitude correspondant pour les voix dans cette p√©riode √† partir de la STFT du signal d'origine, c'est-√†-dire du mixage (avec un contexte temporel suffisant). <br><br>  <b>Qu'en est-il de l'ensemble de donn√©es de formation?</b>  <b>(vous pouvez me demander en ce moment)</b> <br><br>  Merde ... pourquoi donc.  J'allais consid√©rer cela √† la fin de l'article afin de ne pas √™tre distrait du sujet! <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9ac/67f/91d/9ac67f91d85022f6bbc75f296ce3f04a.png"><br><br>  Si notre mod√®le est bien form√©, alors pour une conclusion logique, il vous suffit d'impl√©menter une fen√™tre coulissante simple pour le m√©lange STFT.  Apr√®s chaque pr√©vision, d√©placez la fen√™tre vers la droite d'un intervalle de temps, pr√©disez la prochaine trame avec voix et associez-la √† la pr√©diction pr√©c√©dente.  Quant au mod√®le, prenons le m√™me mod√®le que celui utilis√© pour le d√©tecteur de voix et apportons de petits changements: la forme d'onde de sortie est d√©sormais (513.1), activation lin√©aire en sortie, MSE en fonction des pertes.  Nous commen√ßons maintenant l'entra√Ænement. <br><br>  <b>Ne vous r√©jouissez pas encore ...</b> <br><br>  Bien que cette repr√©sentation des E / S soit logique, apr√®s avoir entra√Æn√© notre mod√®le plusieurs fois, avec divers param√®tres et normalisations de donn√©es, il n'y a aucun r√©sultat.  Il semble que nous en demandions trop ... <br><br>  Nous sommes pass√©s d'un classifieur binaire √† une <i>r√©gression</i> sur un vecteur √† 513 dimensions.  Bien que le r√©seau √©tudie le probl√®me dans une certaine mesure, les voix restaur√©es ont toujours des artefacts √©vidents et des interf√©rences provenant d'autres sources.  M√™me apr√®s avoir ajout√© des couches suppl√©mentaires et augment√© le nombre de param√®tres du mod√®le, les r√©sultats ne changent pas beaucoup.  Et puis la question se pose: <b>comment ¬´simplifier¬ª la t√¢che du r√©seau par la tromperie, et en m√™me temps obtenir les r√©sultats souhait√©s?</b> <br><br>  Et si, au lieu d'estimer l'amplitude des voix STFT, nous formions le r√©seau pour obtenir un masque binaire, qui, appliqu√© au mixage STFT, nous donne un spectrogramme d'amplitude des voix simplifi√© mais <b>perceptuellement acceptable</b> ? <br><br>  En exp√©rimentant diverses heuristiques, nous avons trouv√© un moyen tr√®s simple (et, bien s√ªr, peu orthodoxe en termes de traitement du signal ...) d'extraire la voix de mixages √† l'aide de masques binaires.  Sans entrer dans les d√©tails, l'essentiel est le suivant.  Imaginez la sortie comme une image binaire, o√π la valeur ¬´1¬ª indique la <b>pr√©sence dominante de contenu vocal</b> √† une fr√©quence et une p√©riode de temps donn√©es, et la valeur ¬´0¬ª indique la pr√©sence dominante de musique √† un endroit donn√©.  Nous pouvons appeler cela la <i>binarisation de la perception</i> , juste pour trouver un nom.  Visuellement, √ßa a l'air assez moche, pour √™tre honn√™te, mais les r√©sultats sont √©tonnamment bons. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/16f/857/721/16f85772187f89a73baf7fe0158aba2c.png"><br><br>  Maintenant, notre probl√®me devient une sorte de classification-r√©gression hybride (tr√®s grossi√®rement ...).  Nous demandons au mod√®le de ¬´classer les pixels¬ª en sortie comme vocaux ou non vocaux, bien que conceptuellement (ainsi que du point de vue de la fonction de perte MSE utilis√©e) la t√¢che reste r√©gressive. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e21/6a0/654/e216a065488c058c37e2758563ff4052.png"><br><br>  Bien que cette distinction puisse sembler inappropri√©e pour certains, elle est en fait d'une grande importance dans la capacit√© du mod√®le √† √©tudier la t√¢che, la seconde √©tant plus simple et plus limit√©e.  En m√™me temps, cela nous permet de garder notre mod√®le relativement petit en termes de nombre de param√®tres, √©tant donn√© la complexit√© de la t√¢che, ce qui est tr√®s souhaitable pour travailler en temps r√©el, ce qui dans ce cas √©tait une exigence de conception.  Apr√®s quelques ajustements mineurs, le mod√®le final ressemble √† ceci. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/416/d25/856/416d2585671e97a1f39c9584a30d4bbf.png"></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/21e/04e/4d5/21e04e4d5642a3282aa445846c64c576.png"><br><br><h3>  Comment r√©cup√©rer un signal temporel? </h3><br>  En fait, comme dans la <i>m√©thode na√Øve</i> .  Dans ce cas, pour chaque passage, nous pr√©disons une p√©riode du masque de voix binaire.  Encore une fois, r√©alisant une simple fen√™tre coulissante avec un pas d'une p√©riode, nous continuons √† √©valuer et √† combiner des p√©riodes successives, qui constituent finalement le masque binaire vocal entier. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a34/2a5/ae1/a342a5ae1b0ca37825978f7b92d574cb.gif"><br><br><h3>  Cr√©er un ensemble d'entra√Ænement </h3><br>  Comme vous le savez, l'un des principaux probl√®mes lors de l'enseignement avec un enseignant (laissez ces exemples de jouets avec des jeux de donn√©es pr√™ts √† l'emploi) est les donn√©es correctes (en quantit√© et en qualit√©) pour le probl√®me sp√©cifique que vous essayez de r√©soudre.  Sur la base des repr√©sentations d√©crites des entr√©es et des sorties, pour former notre mod√®le, vous aurez d'abord besoin d'un nombre important de mixages et de leurs pistes vocales correspondantes, parfaitement align√©es et normalis√©es.  Cet ensemble peut √™tre cr√©√© de plusieurs fa√ßons, et nous avons utilis√© une combinaison de strat√©gies, de la cr√©ation manuelle de paires [mix &lt;-&gt; voix] bas√©es sur plusieurs a cappels trouv√©s sur Internet, √† la recherche de mat√©riel musical de groupe de rock et de scrapbooking Youtube.  Juste pour vous donner une id√©e de la lourdeur et de la p√©nibilit√© de ce processus, une partie du projet consistait √† d√©velopper un tel outil pour cr√©er automatiquement des paires [mix &lt;-&gt; voix]: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/617/b71/5ac/617b715acc8d8c913752054f84214c8b.png"><br><br>  Une tr√®s grande quantit√© de donn√©es est n√©cessaire pour que le r√©seau neuronal apprenne la fonction de transfert pour la diffusion de mixages vocaux.  Notre ensemble final √©tait compos√© d'environ 15 millions d'√©chantillons de m√©langes de 300 ms et de leurs masques binaires vocaux correspondants. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/21f/01a/02d/21f01a02d22dfc4f2d615e511cf470a6.png"><br><br><h3>  Architecture de pipeline </h3><br>  Comme vous le savez probablement, la cr√©ation d'un mod√®le ML pour une t√¢che sp√©cifique ne repr√©sente que la moiti√© de la bataille.  Dans le monde r√©el, vous devez penser √† l'architecture logicielle, surtout si vous avez besoin de travailler en temps r√©el ou √† proximit√©. <br><br>  Dans cette impl√©mentation particuli√®re, la reconstruction dans le domaine temporel peut se produire imm√©diatement apr√®s la pr√©diction du masque de voix binaire complet (mode autonome) ou, plus int√©ressant, en mode multithread, o√π nous recevons et traitons des donn√©es, restaurons la voix et reproduisons le son - le tout en petits segments, pr√®s de streaming et m√™me presque en temps r√©el, le traitement de la musique qui est enregistr√©e √† la vol√©e avec un minimum de retard.  En fait, c'est un sujet distinct, et je vais le laisser pour un autre article <b>sur les pipelines ML en temps r√©el</b> ... <br><br><h1>  Je suppose que j'en ai assez dit, alors pourquoi ne pas √©couter quelques exemples!? </h1><br><h3>  Daft Punk - Get Lucky (enregistrement studio) </h3><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://player.vimeo.com/video/315172280" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Ici, vous pouvez entendre une interf√©rence minimale des tambours ...</font></i> <br><br><h3>  Adele - Set Fire to the Rain (enregistrement en direct!) </h3><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://player.vimeo.com/video/315172388" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Remarquez comment au tout d√©but notre mod√®le extrait les cris de la foule sous forme de contenu vocal :).</font></i>  <i><font color="gray">Dans ce cas, il y a des interf√©rences provenant d'autres sources.</font></i>  <i><font color="gray">Comme il s'agit d'un enregistrement live, il semble acceptable que les voix extraites soient de moins bonne qualit√© que les pr√©c√©dentes.</font></i> <br><br><h1>  Oui, et ¬´autre chose¬ª ... </h1><br><h1>  Si le syst√®me fonctionne pour le chant, pourquoi ne pas l'appliquer √† d'autres instruments ...? </h1><br>  L'article est d√©j√† assez volumineux, mais compte tenu du travail accompli, vous m√©ritez d'entendre la derni√®re d√©mo.  Avec la m√™me logique exacte que lors de l'extraction de voix, nous pouvons essayer de diviser la musique st√©r√©o en composants (batterie, basse, voix, autres), en apportant quelques changements √† notre mod√®le et, bien s√ªr, en ayant le jeu d'entra√Ænement appropri√© :). <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://player.vimeo.com/video/315173879" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Merci d'avoir lu.  Enfin, comme vous pouvez le voir, le mod√®le actuel de notre r√©seau de neurones convolutionnels n'est pas si sp√©cial.  Le succ√®s de ce travail a √©t√© d√©termin√© par l' <b>ing√©nierie des fonctionnalit√©s</b> et le processus de test d'hypoth√®se soign√©, dont je parlerai dans les prochains articles! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr441090/">https://habr.com/ru/post/fr441090/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr441076/index.html">Optimisation de script avec Webpack SplitChunksPlugin</a></li>
<li><a href="../fr441078/index.html">LG pr√©sentera un smartphone avec un √©cran haut-parleur OLED: quelques mots sur le nouvel appareil et la nouvelle technologie</a></li>
<li><a href="../fr441082/index.html">Les billets pour Mars co√ªteront moins de 500 000 $</a></li>
<li><a href="../fr441084/index.html">O√π sont pass√©s les premiers adoptants?</a></li>
<li><a href="../fr441088/index.html">D√©veloppeur, souvenez-vous - le trafic de votre application est surveill√©</a></li>
<li><a href="../fr441092/index.html">Embedded World 2019 - la plus grande exposition d'√©lectronique embarqu√©e</a></li>
<li><a href="../fr441096/index.html">Articles de lecture sur simulateur</a></li>
<li><a href="../fr441098/index.html">Profondeurs SIEM: corr√©lations pr√™tes √† l'emploi. Partie 4. Mod√®le de syst√®me comme contexte de r√®gles de corr√©lation</a></li>
<li><a href="../fr441102/index.html">Kaspersky Mobile Talks - une r√©union pour les d√©veloppeurs avanc√©s</a></li>
<li><a href="../fr441104/index.html">Obtention d'informations et contournement de l'authentification √† deux facteurs sur les cartes bancaires du TOP-10 (Ukraine)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>