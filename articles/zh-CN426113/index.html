<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🌜 👊 👁‍🗨 Google新闻和列夫·托尔斯泰：使用t-SNE可视化单词的矢量表示 🙌🏼 🤟 🎬</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="无论是互联网新闻，诗歌还是古典小说，我们每个人都以自己的方式感知文本。 这同样适用于机器学习的算法和方法，这些算法和方法通常以多维矢量空间的形式感知数学形式的文本。 

 本文致力于使用Word-Vec单词的多维矢量表示计算出的t-SNE进行可视化。 可视化将有助于更好地理解Word2Vec的原理以...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Google新闻和列夫·托尔斯泰：使用t-SNE可视化单词的矢量表示</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/426113/"><img src="https://habrastorage.org/webt/6c/ux/7m/6cux7mvmp3phb8d8efjwmqrb_yc.gif"><br><br> 无论是互联网新闻，诗歌还是古典小说，我们每个人都以自己的方式感知文本。 这同样适用于机器学习的算法和方法，这些算法和方法通常以多维矢量空间的形式感知数学形式的文本。 <br><br> 本文致力于使用Word-Vec单词的多维矢量表示计算出的t-SNE进行可视化。 可视化将有助于更好地理解Word2Vec的原理以及在进一步用于神经网络和其他机器学习算法之前如何解释单词向量之间的关系。 本文着重于可视化，未考虑进一步的研究和数据分析。 作为数据源，我们使用Google新闻的文章和L.N.的经典著作。 托尔斯泰。 我们将在Jupyter Notebook中使用Python编写代码。 <br><a name="habracut"></a><br><h1>  T分布随机邻居嵌入 </h1><br>  T-SNE是一种基于非线性降维方法的用于数据可视化的机器学习算法，在原始文章[1]和<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Habré</a>上进行了详细描述。  t-SNE操作的基本原理是在保持相对位置的同时减小点之间的成对距离。 换句话说，该算法将多维数据映射到较低维的空间，同时保持点邻域的结构。 <br><br><h1> 单词和Word2Vec的矢量表示 </h1><br> 首先，我们需要以向量形式呈现单词。 对于此任务，我选择了Word2Vec分布语义实用程序，该实用程序旨在显示矢量空间中单词的语义含义。  Word2Vec通过假设在相似的上下文中发现了语义相关的单词来查找单词之间的关系。 您可以在原始文章[2]中以及<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">此处</a>和<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">此处</a>阅读有关Word2Vec的更多信息。 <br><br> 作为输入，我们采用Google新闻和L.N.的小说中的文章。 托尔斯泰。 在第一种情况下，我们将使用Google <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">在项目页面上</a>发布的Google新闻数据集（约1000亿个单词）中预先训练的向量。 <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gensim model = gensim.models.KeyedVectors.load_word2vec_format(<span class="hljs-string"><span class="hljs-string">'GoogleNews-vectors-negative300.bin'</span></span>, binary=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br> 除了使用Gensim库[3]预先训练的向量外，我们还将在L.N.的文本中训练另一个模型。 托尔斯泰。 由于Word2Vec接受一组句子作为输入，因此我们使用NLTK软件包中经过预先训练的Punkt句子标记器模型来自动将文本拆分为句子。 可以<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">从此处</a>下载俄语模型。 <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> codecs <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocess_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(text)</span></span></span><span class="hljs-function">:</span></span> text = re.sub(<span class="hljs-string"><span class="hljs-string">'[^a-zA-Z--1-9]+'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, text) text = re.sub(<span class="hljs-string"><span class="hljs-string">' +'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, text) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text.strip() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">prepare_for_w2v</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename_from, filename_to, lang)</span></span></span><span class="hljs-function">:</span></span> raw_text = codecs.open(filename_from, <span class="hljs-string"><span class="hljs-string">"r"</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'windows-1251'</span></span>).read() <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(filename_to, <span class="hljs-string"><span class="hljs-string">'w'</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> sentence <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> nltk.sent_tokenize(raw_text, lang): print(preprocess_text(sentence.lower()), file=f)</code> </pre><br> 接下来，使用Gensim库，我们将使用以下参数训练Word2Vec模型： <br><br><ul><li>  <i>size = 200-</i>属性空间的尺寸； </li><li>  <i>window = 5-</i>算法分析的上下文中的单词数； </li><li>  <i>min_count = 5-</i>该单词必须至少出现五次，以便模型<i>将</i>其考虑在内。 </li></ul><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> multiprocessing <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> gensim.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Word2Vec <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train_word2vec</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> data = gensim.models.word2vec.LineSentence(filename) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Word2Vec(data, size=<span class="hljs-number"><span class="hljs-number">200</span></span>, window=<span class="hljs-number"><span class="hljs-number">5</span></span>, min_count=<span class="hljs-number"><span class="hljs-number">5</span></span>, workers=multiprocessing.cpu_count())</code> </pre><br><h1> 使用t-SNE可视化单词的矢量表示 </h1><br>  T-SNE对于可视化多维空间中对象之间的相似性非常有用。 随着数据量的增加，建立可视化图表变得越来越困难，因此在实践中，将相关单词组合成组以进一步可视化。 例如，从以前在Google新闻中训练过的Word2Vec模型的词典中拿几个单词。 <br><br><pre> <code class="python hljs">keys = [<span class="hljs-string"><span class="hljs-string">'Paris'</span></span>, <span class="hljs-string"><span class="hljs-string">'Python'</span></span>, <span class="hljs-string"><span class="hljs-string">'Sunday'</span></span>, <span class="hljs-string"><span class="hljs-string">'Tolstoy'</span></span>, <span class="hljs-string"><span class="hljs-string">'Twitter'</span></span>, <span class="hljs-string"><span class="hljs-string">'bachelor'</span></span>, <span class="hljs-string"><span class="hljs-string">'delivery'</span></span>, <span class="hljs-string"><span class="hljs-string">'election'</span></span>, <span class="hljs-string"><span class="hljs-string">'expensive'</span></span>, <span class="hljs-string"><span class="hljs-string">'experience'</span></span>, <span class="hljs-string"><span class="hljs-string">'financial'</span></span>, <span class="hljs-string"><span class="hljs-string">'food'</span></span>, <span class="hljs-string"><span class="hljs-string">'iOS'</span></span>, <span class="hljs-string"><span class="hljs-string">'peace'</span></span>, <span class="hljs-string"><span class="hljs-string">'release'</span></span>, <span class="hljs-string"><span class="hljs-string">'war'</span></span>] embedding_clusters = [] word_clusters = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> keys: embeddings = [] words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> similar_word, _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> model.most_similar(word, topn=<span class="hljs-number"><span class="hljs-number">30</span></span>): words.append(similar_word) embeddings.append(model[similar_word]) embedding_clusters.append(embeddings) word_clusters.append(words)</code> </pre> <br><img src="https://habrastorage.org/webt/uc/1k/o6/uc1ko6efgx_d-wnbwolgdabifl8.gif"><br>  <i>图1.来自Google新闻的相似词组，具有不同的复杂性值。</i> <br><br> 接下来，我们转到本文最引人注目的片段，即t-SNE配置。 在这里，首先，您应注意以下超参数： <br><br><ul><li>  <i>n_components-</i>组件数，即值空间的维数； </li><li>  <i>困惑</i> -困惑，在t-SNE中的值可以等于邻居的有效数量。 它与最近邻居的数量有关，在其他基于品种的学习模型中使用了该邻居（请参见上图）。 建议将其值[1]设置在5-50的范围内； </li><li>  <i>init-</i>向量的初始初始化类型。 </li></ul><br><pre> <code class="python hljs">tsne_model_en_2d = TSNE(perplexity=<span class="hljs-number"><span class="hljs-number">15</span></span>, n_components=<span class="hljs-number"><span class="hljs-number">2</span></span>, init=<span class="hljs-string"><span class="hljs-string">'pca'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">3500</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">32</span></span>) embedding_clusters = np.array(embedding_clusters) n, m, k = embedding_clusters.shape embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, <span class="hljs-number"><span class="hljs-number">2</span></span>)</code> </pre> <br> 以下是使用Matplotlib构建二维图形的脚本，Matplotlib是用于在Python中可视化数据的最受欢迎的库之一。 <br><br><img src="https://habrastorage.org/webt/34/9y/7h/349y7hxuanvvttqfxkpb48j4n-q.png"><br>  <i>图2.来自Google新闻的相似词组（复杂度= 15）。</i> <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.manifold <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> TSNE <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.cm <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> cm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np % matplotlib inline <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tsne_plot_similar_words</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(labels, embedding_clusters, word_clusters, a=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.7</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">16</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) colors = cm.rainbow(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, len(labels))) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> label, embeddings, words, color <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(labels, embedding_clusters, word_clusters, colors): x = embeddings[:,<span class="hljs-number"><span class="hljs-number">0</span></span>] y = embeddings[:,<span class="hljs-number"><span class="hljs-number">1</span></span>] plt.scatter(x, y, c=color, alpha=a, label=label) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(words): plt.annotate(word, alpha=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, xy=(x[i], y[i]), xytext=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), textcoords=<span class="hljs-string"><span class="hljs-string">'offset points'</span></span>, ha=<span class="hljs-string"><span class="hljs-string">'right'</span></span>, va=<span class="hljs-string"><span class="hljs-string">'bottom'</span></span>, size=<span class="hljs-number"><span class="hljs-number">8</span></span>) plt.legend(loc=<span class="hljs-number"><span class="hljs-number">4</span></span>) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) plt.savefig(<span class="hljs-string"><span class="hljs-string">"f/.png"</span></span>, format=<span class="hljs-string"><span class="hljs-string">'png'</span></span>, dpi=<span class="hljs-number"><span class="hljs-number">150</span></span>, bbox_inches=<span class="hljs-string"><span class="hljs-string">'tight'</span></span>) plt.show() tsne_plot_similar_words(keys, embeddings_en_2d, word_clusters)</code> </pre> <br> 有时有必要建立不是单独的单词簇，而是整个字典。 为此，让我们分析一下安娜·卡列尼娜（Anna Karenina），这是关于激情，背叛，悲剧和赎罪的伟大故事。 <br><br><pre> <code class="python hljs">prepare_for_w2v(<span class="hljs-string"><span class="hljs-string">'data/Anna Karenina by Leo Tolstoy (ru).txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'train_anna_karenina_ru.txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'russian'</span></span>) model_ak = train_word2vec(<span class="hljs-string"><span class="hljs-string">'train_anna_karenina_ru.txt'</span></span>) words = [] embeddings = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> list(model_ak.wv.vocab): embeddings.append(model_ak.wv[word]) words.append(word) tsne_ak_2d = TSNE(n_components=<span class="hljs-number"><span class="hljs-number">2</span></span>, init=<span class="hljs-string"><span class="hljs-string">'pca'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">3500</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">32</span></span>) embeddings_ak_2d = tsne_ak_2d.fit_transform(embeddings)</code> </pre><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tsne_plot_2d</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(label, embeddings, words=[], a=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">16</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) colors = cm.rainbow(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) x = embeddings[:,<span class="hljs-number"><span class="hljs-number">0</span></span>] y = embeddings[:,<span class="hljs-number"><span class="hljs-number">1</span></span>] plt.scatter(x, y, c=colors, alpha=a, label=label) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(words): plt.annotate(word, alpha=<span class="hljs-number"><span class="hljs-number">0.3</span></span>, xy=(x[i], y[i]), xytext=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), textcoords=<span class="hljs-string"><span class="hljs-string">'offset points'</span></span>, ha=<span class="hljs-string"><span class="hljs-string">'right'</span></span>, va=<span class="hljs-string"><span class="hljs-string">'bottom'</span></span>, size=<span class="hljs-number"><span class="hljs-number">10</span></span>) plt.legend(loc=<span class="hljs-number"><span class="hljs-number">4</span></span>) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) plt.savefig(<span class="hljs-string"><span class="hljs-string">"hhh.png"</span></span>, format=<span class="hljs-string"><span class="hljs-string">'png'</span></span>, dpi=<span class="hljs-number"><span class="hljs-number">150</span></span>, bbox_inches=<span class="hljs-string"><span class="hljs-string">'tight'</span></span>) plt.show() tsne_plot_2d(<span class="hljs-string"><span class="hljs-string">'Anna Karenina by Leo Tolstoy'</span></span>, embeddings_ak_2d, a=<span class="hljs-number"><span class="hljs-number">0.1</span></span>)</code> </pre> <br><img src="https://habrastorage.org/webt/j5/hn/82/j5hn8285nih2kwlop_badd2lzlk.png"><br><br><img src="https://habrastorage.org/webt/x6/jc/i7/x6jci7vka7-efczouqxpiqueisq.png"><br>  <i>图3.以小说《安娜·卡列尼娜》为训练对象的Word2Vec模型字典的可视化。</i> <br><br> 如果我们使用三维空间，图片将变得更加有用。 看一下《战争与和平》，这是世界文学的主要小说之一。 <br><br><pre> <code class="python hljs">prepare_for_w2v(<span class="hljs-string"><span class="hljs-string">'data/War and Peace by Leo Tolstoy (ru).txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'train_war_and_peace_ru.txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'russian'</span></span>) model_wp = train_word2vec(<span class="hljs-string"><span class="hljs-string">'train_war_and_peace_ru.txt'</span></span>) words_wp = [] embeddings_wp = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> list(model_wp.wv.vocab): embeddings_wp.append(model_wp.wv[word]) words_wp.append(word) tsne_wp_3d = TSNE(perplexity=<span class="hljs-number"><span class="hljs-number">30</span></span>, n_components=<span class="hljs-number"><span class="hljs-number">3</span></span>, init=<span class="hljs-string"><span class="hljs-string">'pca'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">3500</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">12</span></span>) embeddings_wp_3d = tsne_wp_3d.fit_transform(embeddings_wp)</code> </pre><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mpl_toolkits.mplot3d <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Axes3D <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tsne_plot_3d</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(title, label, embeddings, a=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> fig = plt.figure() ax = Axes3D(fig) colors = cm.rainbow(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) plt.scatter(embeddings[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], embeddings[:, <span class="hljs-number"><span class="hljs-number">1</span></span>], embeddings[:, <span class="hljs-number"><span class="hljs-number">2</span></span>], c=colors, alpha=a, label=label) plt.legend(loc=<span class="hljs-number"><span class="hljs-number">4</span></span>) plt.title(title) plt.show() tsne_plot_3d(<span class="hljs-string"><span class="hljs-string">'Visualizing Embeddings using t-SNE'</span></span>, <span class="hljs-string"><span class="hljs-string">'War and Peace'</span></span>, embeddings_wp_3d, a=<span class="hljs-number"><span class="hljs-number">0.1</span></span>)</code> </pre> <br><img src="https://habrastorage.org/webt/ch/jm/os/chjmos082qn6ktw9afdeavbz6_a.png"><br>  <i>图4. Word2Vec模型字典的可视化，该字典经过小说《战争与和平》的训练。</i> <br><br><h1> 源代码 </h1><br> 该代码可在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">GitHub上获得</a> 。 在这里您可以找到用于渲染动画的代码。 <br><br><h1> 资料来源 </h1><br><ol><li>  Maaten L.，Hinton G.使用t-SNE可视化数据//机器学习研究杂志。  -2008年-T. 9-S. 2579-2605。 </li><li> 单词和短语的分布式表示及其组成性// <i>神经信息处理系统的进展</i> 。  -2013 .-- S.3111-3119。 </li><li>  Rehurek R.，Sojka P.用于大型语料库主题建模的软件框架//在NREC框架新挑战LREC 2010研讨会的论文集中。  -2010年。 </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN426113/">https://habr.com/ru/post/zh-CN426113/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN426099/index.html">Typegram新闻</a></li>
<li><a href="../zh-CN426101/index.html">微服务架构中的数据完整性-如何确保没有分布式事务和紧密连接</a></li>
<li><a href="../zh-CN426103/index.html">网络研讨会“您需要Kubernetes吗”，10月15日19:00</a></li>
<li><a href="../zh-CN426105/index.html">我一周的收获</a></li>
<li><a href="../zh-CN426111/index.html">Google仍将在中国推出受审查的搜索服务</a></li>
<li><a href="../zh-CN426115/index.html">在Quake Champions后端平台中使用actor模型的实践</a></li>
<li><a href="../zh-CN426117/index.html">申诉专员建议封锁含有潜在危险婴儿用品的广告</a></li>
<li><a href="../zh-CN426119/index.html">上古：Cryptonomicon铁</a></li>
<li><a href="../zh-CN426121/index.html">MC.exe（消息编译器），rc.exe，link.exe为EventMessageFile生成.dll</a></li>
<li><a href="../zh-CN426123/index.html">学习OpenGL。 第6.1课。 PBR或物理正确的渲染。 理论</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>