<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚úçÔ∏è üîò ü§Ωüèº Sequenz-zu-Sequenz-Teil-2-Modelle üôåüèº ü§≥üèª üë©üèª‚Äçüéì</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo allerseits! 

 Der zweite Teil der √úbersetzung, den wir vor einigen Wochen ver√∂ffentlicht haben, bereitet den Start des zweiten Streams des Kurs...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Sequenz-zu-Sequenz-Teil-2-Modelle</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/432302/"> Hallo allerseits! <br><br>  Der zweite Teil der √úbersetzung, den wir vor einigen Wochen ver√∂ffentlicht haben, bereitet den Start des zweiten Streams des Kurses <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚ÄûData Scientist‚Äú vor</a> .  Ahead ist ein weiteres interessantes Material und eine offene Lektion. <br><br>  In der Zwischenzeit gingen wir weiter in den Dschungel der Models. <br><br>  <b>Neuronales √úbersetzungsmodell</b> <br><br>  W√§hrend der Kern des Sequenz-zu-Sequenz-Modells durch Funktionen aus <code>tensorflow/tensorflow/python/ops/seq2seq.py</code> , gibt es in unserem √úbersetzungsmodell in <code>models/tutorials/rnn/translate/seq2seq_model.py</code> noch einige Tricks erw√§hnenswert. <br><br><img src="https://habrastorage.org/webt/3z/s9/fy/3zs9fym7zcpeqweylxlqhknznko.png"><a name="habracut"></a><br><br>  <b>Sampled Softmax und Ausgangsprojektion</b> <br><br>  Wie oben erw√§hnt, m√∂chten wir den abgetasteten Softmax verwenden, um mit einem gro√üen Ausgabew√∂rterbuch zu arbeiten.  Um daraus zu dekodieren, m√ºssen Sie die Projektion der Ausgabe verfolgen.  Sowohl der abgetastete Softmax-Verlust als auch die Ausgabeprojektion werden durch den folgenden Code in <code>seq2seq_model.py</code> . <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> num_samples &gt; <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> num_samples &lt; self.target_vocab_size: w_t = tf.get_variable(<span class="hljs-string"><span class="hljs-string">"proj_w"</span></span>, [self.target_vocab_size, size], dtype=dtype) w = tf.transpose(w_t) b = tf.get_variable(<span class="hljs-string"><span class="hljs-string">"proj_b"</span></span>, [self.target_vocab_size], dtype=dtype) output_projection = (w, b) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sampled_loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(labels, inputs)</span></span></span><span class="hljs-function">:</span></span> labels = tf.reshape(labels, [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>]) <span class="hljs-comment"><span class="hljs-comment"># We need to compute the sampled_softmax_loss using 32bit floats to # avoid numerical instabilities. local_w_t = tf.cast(w_t, tf.float32) local_b = tf.cast(b, tf.float32) local_inputs = tf.cast(inputs, tf.float32) return tf.cast( tf.nn.sampled_softmax_loss( weights=local_w_t, biases=local_b, labels=labels, inputs=local_inputs, num_sampled=num_samples, num_classes=self.target_vocab_size), dtype)</span></span></code> </pre><br>  Beachten Sie zun√§chst, dass wir nur dann gesampelten Softmax erstellen, wenn die Anzahl der Samples (standardm√§√üig 512) unter der Zielw√∂rterbuchgr√∂√üe liegt.  F√ºr W√∂rterb√ºcher kleiner als 512 ist es besser, den Standard-Softmax-Verlust zu verwenden. <br><br>  Erstellen Sie dann eine Projektion der Ausgabe.  Dies ist ein Paar, das aus einer Matrix von Gewichten und einem Verschiebungsvektor besteht.  Bei Verwendung gibt die rnn-Zelle die Formvektoren der Anzahl der Trainingsmuster nach <code>size</code> und nicht die Anzahl der Trainingsmuster nach <code>target_vocab_size</code> .  Um Protokolle wiederherzustellen, m√ºssen Sie sie mit der Gewichtungsmatrix multiplizieren und einen Versatz hinzuf√ºgen, der in den Zeilen 124-126 in <code>seq2seq_model.py</code> . <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> output_projection <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> b <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(len(buckets)): self.outputs[b] = [tf.matmul(output, output_projection[<span class="hljs-number"><span class="hljs-number">0</span></span>]) + output_projection[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> ...]</code> </pre> <br>  <b>Schaufeln und Polstern</b> <br><br>  Zus√§tzlich zum abgetasteten Softmax verwendet unser √úbersetzungsmodell auch <i>Bucketing</i> , eine Methode, mit der Sie S√§tze unterschiedlicher L√§nge effizient verwalten k√∂nnen.  Erkl√§ren Sie zun√§chst das Problem.  Bei der √úbersetzung vom Englischen ins Franz√∂sische haben wir englische S√§tze unterschiedlicher L√§nge L1 am Eingang und franz√∂sische S√§tze unterschiedlicher L√§nge L1 am Ausgang.  Da der englische Satz als <code>encoder_inputs</code> wird und der franz√∂sische Satz als <code>decoder_inputs</code> (mit dem GO-Symbolpr√§fix) angezeigt wird, muss f√ºr jedes Paar (L1, L2 + 1) von L√§ngen englischer und franz√∂sischer S√§tze ein seq2seq-Modell erstellt werden.  Als Ergebnis erhalten wir ein riesiges Diagramm, das aus vielen √§hnlichen Untergraphen besteht.  Auf der anderen Seite k√∂nnen wir jeden Satz mit speziellen PAD-Zeichen ‚Äûauff√ºllen‚Äú.  Und dann brauchen wir nur ein seq2seq-Modell f√ºr "gepackte" L√§ngen.  Ein solches Modell ist jedoch in kurzen S√§tzen unwirksam - Sie m√ºssen viele nutzlose PAD-Zeichen codieren und decodieren. <br><br>  Als Kompromiss zwischen dem Erstellen eines Diagramms f√ºr jedes L√§ngenpaar und dem F√ºllen auf eine einzelne L√§nge verwenden wir eine bestimmte Anzahl von Eimern und f√ºllen jeden Satz auf die L√§nge der obigen Gruppe.  In <code>translate.py</code> wir standardm√§√üig die folgenden Gruppen. <br><br><pre> <code class="python hljs">buckets = [(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>), (<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">15</span></span>), (<span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">25</span></span>), (<span class="hljs-number"><span class="hljs-number">40</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>)]</code> </pre> <br><br>  Wenn also ein englischer Satz mit 3 Token am Eingang ankommt und der entsprechende franz√∂sische Satz 6 Token am Ausgang enth√§lt, gehen sie in die erste Gruppe und werden am Eingang des Codierers auf L√§nge 5 und am Eingang des Decoders auf L√§nge 10 gef√ºllt.  Und wenn das englische Angebot 8 Token enth√§lt und die entsprechenden franz√∂sischen 18, fallen sie nicht in die Gruppe (10, 15) und werden an die Gruppe (20, 25) √ºbertragen, dh das englische Angebot erh√∂ht sich auf 20 Token und das franz√∂sische auf 25. <br><br>  Denken Sie daran, dass wir beim Erstellen des Decoder-Eingangs dem Eingang das Sonderzeichen <code>GO</code> hinzuf√ºgen.  Dies geschieht in der Funktion <code>get_batch()</code> in <code>seq2seq_model.py</code> , die auch den englischen Satz <code>seq2seq_model.py</code> .  Das Umdrehen der Eingabe trug dazu bei, die Ergebnisse des neuronalen √úbersetzungsmodells von <a href="">Sutskever et al., 2014 (pdf)</a> zu verbessern <a href="">.</a>  Um es endlich herauszufinden, stellen Sie sich vor, dass es einen Satz "Ich gehe" gibt. An der Eingabe, aufgeteilt in Token <code>["I", "go", "."]</code> Und an der Ausgabe gibt es einen Satz "Je vais.", Gebrochen in Token <code>["Je", "vais", "."]</code> .  Sie werden der Gruppe (5, 10) mit einer Darstellung des Eingangscodierers <code>[PAD PAD "." "go" "I"]</code> hinzugef√ºgt <code>[PAD PAD "." "go" "I"]</code>  <code>[PAD PAD "." "go" "I"]</code> und Decodereingang <code>[GO "Je" "vais" "." EOS PAD PAD PAD PAD PAD]</code>  <code>[GO "Je" "vais" "." EOS PAD PAD PAD PAD PAD]</code> . <br><br>  <b>F√ºhren Sie es aus</b> <br><br>  Um das oben beschriebene Modell zu trainieren, ben√∂tigen Sie ein gro√ües anglo-franz√∂sisches Korps.  F√ºr das Training werden wir das 10 ^ 9 franz√∂sisch-englische Korps von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der WMT'15-Website verwenden</a> und Nachrichten von derselben Site als Arbeitsprobe testen.  Beide Datens√§tze werden beim n√§chsten Befehl in <code>train_dir</code> geladen. <br><br><pre> <code class="plaintext hljs">python translate.py --data_dir [your_data_directory] --train_dir [checkpoints_directory] --en_vocab_size=40000 --fr_vocab_size=40000</code> </pre> <br>  Sie ben√∂tigen 18 GB Festplattenspeicher und mehrere Stunden, um das Schulungsgeb√§ude vorzubereiten.  Der Fall wird entpackt, W√∂rterbuchdateien werden in <code>data_dir,</code> und danach wird der Fall tokenisiert und in ganzzahlige Bezeichner konvertiert.  Achten Sie auf die Parameter, die f√ºr die Gr√∂√üe des W√∂rterbuchs verantwortlich sind.  Im obigen Beispiel werden alle W√∂rter au√üerhalb der 40.000 am h√§ufigsten verwendeten W√∂rter in ein UNK-Token konvertiert, das ein unbekanntes Wort darstellt.  Wenn Sie also die Gr√∂√üe des W√∂rterbuchs √§ndern, reformiert die Bin√§rdatei das Geh√§use durch die Token-ID neu.  Nach der Datenvorbereitung beginnt das Training. <br><br>  Die in <code>translate</code> angegebenen Werte sind standardm√§√üig sehr hoch.  Gro√üe Modelle, die lange lernen, zeigen gute Ergebnisse, aber es kann zu viel Zeit oder zu viel GPU-Speicher dauern.  Sie k√∂nnen ein kleineres Modelltraining angeben, wie im folgenden Beispiel. <br><br><pre> <code class="plaintext hljs">python translate.py --data_dir [your_data_directory] --train_dir [checkpoints_directory] --size=256 --num_layers=2 --steps_per_checkpoint=50</code> </pre> <br>  Der obige Befehl trainiert das Modell mit zwei Ebenen (standardm√§√üig gibt es 3), von denen jede 256 Einheiten hat (Standard - 1024), mit einem Pr√ºfpunkt alle 50 Schritte (Standard - 200).  Experimentieren Sie mit diesen Optionen, um festzustellen, welches Modell f√ºr Ihre GPU geeignet ist. <br><br>  W√§hrend des Trainings gibt jeder Schritt der Bin√§rdatei <code>steps_per_checkpoin</code> t Statistiken √ºber vergangene Schritte aus.  Bei Standardparametern (3 Schichten der Gr√∂√üe 1024) lautet die erste Meldung wie folgt: <br><br><pre> <code class="plaintext hljs">global step 200 learning rate 0.5000 step-time 1.39 perplexity 1720.62 eval: bucket 0 perplexity 184.97 eval: bucket 1 perplexity 248.81 eval: bucket 2 perplexity 341.64 eval: bucket 3 perplexity 469.04 global step 400 learning rate 0.5000 step-time 1.38 perplexity 379.89 eval: bucket 0 perplexity 151.32 eval: bucket 1 perplexity 190.36 eval: bucket 2 perplexity 227.46 eval: bucket 3 perplexity 238.66</code> </pre> <br>  Beachten Sie, dass jeder Schritt etwas weniger als 1,4 Sekunden dauert, was die Trainingsprobe und die Arbeitsprobe in jeder Gruppe verwirrt.  Nach ungef√§hr 30.000 Schritten sehen wir, wie die Verwirrungen kurzer S√§tze (Gruppen 0 und 1) eindeutig werden.  Das Trainingsgeb√§ude enth√§lt ungef√§hr 22 Millionen S√§tze. Eine Iteration (ein Lauf von Trainingsdaten) dauert ungef√§hr 340.000 Schritte mit einer Anzahl von Trainingsmustern in H√∂he von 64. In diesem Stadium kann das Modell verwendet werden, um englische S√§tze mit der Option <code>--decode</code> ins Franz√∂sische zu √ºbersetzen. <br><br><pre> <code class="plaintext hljs">python translate.py --decode --data_dir [your_data_directory] --train_dir [checkpoints_directory] Reading model parameters from /tmp/translate.ckpt-340000 &gt; Who is the president of the United States? Qui est le pr√©sident des √âtats-Unis ?</code> </pre> <br>  <b>Was weiter?</b> <br><br>  Das obige Beispiel zeigt, wie Sie Ihren eigenen Englisch-Franz√∂sisch-√úbersetzer Ende-zu-Ende erstellen.  F√ºhren Sie es aus und sehen Sie, wie das Modell funktioniert.  Die Qualit√§t ist akzeptabel, aber mit Standardparametern kann kein ideales √úbersetzungsmodell erhalten werden.  Hier sind einige Dinge, die Sie verbessern k√∂nnen. <br><br>  Zun√§chst verwenden wir die primitive Tokenisierung, die Grundfunktion von <code>basic_tokenizer</code> in <code>data_utils</code> .  Einen besseren Tokenizer finden Sie auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der WMT'15-Website</a> .  Die Verwendung und eines gro√üen W√∂rterbuchs kann die √úbersetzungen verbessern. <br><br>  Au√üerdem sind die Standardparameter des √úbersetzungsmodells nicht perfekt konfiguriert.  Sie k√∂nnen versuchen, die Lerngeschwindigkeit, D√§mpfung und Initialisierung der Modellgewichte zu √§ndern.  Sie k√∂nnen auch den Standard- <code>GradientDescentOptimizer</code> in <code>seq2seq_model.py</code> durch etwas Fortgeschritteneres wie <code>AdagradOptimizer</code> .  Versuchen Sie, auf bessere Ergebnisse zu achten! <br><br>  Schlie√ülich kann das oben dargestellte Modell nicht nur f√ºr die √úbersetzung verwendet werden, sondern auch f√ºr jede andere Sequenz-zu-Sequenz-Aufgabe.  Selbst wenn Sie eine Sequenz in einen Baum verwandeln m√∂chten, z. B. einen Analysebaum generieren m√∂chten, kann dieses Modell Ergebnisse auf dem <a href="">neuesten</a> Stand der Technik liefern, wie von <a href="">Vinyals &amp; Kaiser et al., 2014 (pdf) gezeigt</a> .  So k√∂nnen Sie nicht nur einen √úbersetzer, sondern auch einen Parser, einen Chat-Bot oder ein beliebiges anderes Programm erstellen.  Experimentieren Sie! <br><br>  Das ist alles! <br><br>  Wir warten hier auf Ihre Kommentare und Fragen oder laden Sie ein, ihren <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Lehrer</a> in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einer offenen Lektion</a> zu fragen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de432302/">https://habr.com/ru/post/de432302/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de432292/index.html">Variable und parametrische Schriftarten - Win-Win f√ºr Designer</a></li>
<li><a href="../de432294/index.html">Ansible Tower: Workflow-Jobvorlagen</a></li>
<li><a href="../de432296/index.html">Google h√§lt Sie in einer pers√∂nlichen "Suchblase", selbst wenn Sie Ihr Konto verlassen</a></li>
<li><a href="../de432298/index.html">Timeweb hat die TOP-10-Domain-Registrare in der Zone .RU eingegeben</a></li>
<li><a href="../de432300/index.html">Support, Service, Kopfschmerzen und alles in allem</a></li>
<li><a href="../de432304/index.html">Ein brillanter Neurowissenschaftler, der m√∂glicherweise einen Schl√ºssel zur Schaffung echter k√ºnstlicher Intelligenz hat</a></li>
<li><a href="../de432306/index.html">Speicherklasse Speicher im Speicher - wenn Sie ihn noch schneller ben√∂tigen</a></li>
<li><a href="../de432308/index.html">Modulares Sci-Fi-Level UE4: inspiriert von Nostromo und Serenity</a></li>
<li><a href="../de432310/index.html">Ktor als HTTP-Client f√ºr Android</a></li>
<li><a href="../de432312/index.html">Erstellen Sie eine Formkarte der RF-Karte in Power BI</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>