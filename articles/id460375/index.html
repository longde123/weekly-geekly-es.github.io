<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ¦ƒ ğŸ™ğŸ¿ âºï¸ Buku "Pembelajaran Mesin untuk Bisnis dan Pemasaran" ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ â˜¯ï¸ ğŸ§•ğŸ¼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ilmu data menjadi bagian integral dari setiap kegiatan pemasaran, dan buku ini adalah potret hidup dari transformasi digital dalam pemasaran. Analisis...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Buku "Pembelajaran Mesin untuk Bisnis dan Pemasaran"</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/460375/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><img src="https://habrastorage.org/webt/gq/td/mc/gqtdmc8joactk6gu7xrzcdr0f4i.jpeg" align="left" alt="gambar"></a>  Ilmu data menjadi bagian integral dari setiap kegiatan pemasaran, dan buku ini adalah potret hidup dari transformasi digital dalam pemasaran.  Analisis data dan algoritma pintar mengotomatiskan tugas pemasaran yang menghabiskan waktu.  Proses pengambilan keputusan menjadi tidak hanya lebih sempurna, tetapi juga lebih cepat, yang sangat penting dalam lingkungan kompetitif yang terus meningkat. <br><br>  â€œBuku ini adalah potret hidup dari transformasi digital dalam pemasaran.  Ini menunjukkan bagaimana ilmu data menjadi bagian integral dari setiap kegiatan pemasaran.  Ini menjelaskan secara terperinci bagaimana pendekatan yang didasarkan pada analisis data dan algoritma cerdas berkontribusi pada otomatisasi mendalam dari tugas pemasaran padat karya tradisional.  Proses pengambilan keputusan menjadi tidak hanya lebih sempurna, tetapi juga lebih cepat, yang penting dalam lingkungan kompetitif kita yang terus meningkat.  Buku ini harus dibaca oleh spesialis pemrosesan data dan spesialis pemasaran, dan lebih baik jika mereka membacanya bersama. "  Andrey Sebrant, Direktur Pemasaran Strategis, Yandex. <br><a name="habracut"></a><br><h3>  Kutipan.  5.8.3.  Model Faktor Tersembunyi </h3><br>  Dalam algoritme penyaringan bersama yang dibahas sejauh ini, sebagian besar perhitungan didasarkan pada elemen individual dari matriks peringkat.  Metode berbasis kedekatan mengevaluasi peringkat yang hilang langsung dari nilai yang diketahui dalam matriks peringkat.  Metode berbasis model menambahkan lapisan abstraksi di atas matriks peringkat, menciptakan model prediksi yang menangkap pola hubungan tertentu antara pengguna dan elemen, tetapi pelatihan model masih sangat tergantung pada sifat-sifat matriks peringkat.  Akibatnya, teknik penyaringan kolaboratif ini biasanya menghadapi masalah berikut: <br><br>  Matriks peringkat dapat berisi jutaan pengguna, jutaan elemen, dan miliaran peringkat yang diketahui, yang menciptakan masalah serius pada kompleksitas komputasi dan skalabilitas. <br><br>  Matriks peringkat biasanya sangat jarang (dalam praktiknya, sekitar 99% peringkat mungkin hilang).  Ini memengaruhi stabilitas komputasi dari algoritme rekomendasi dan menyebabkan perkiraan yang tidak dapat diandalkan ketika pengguna atau elemen tidak memiliki tetangga yang benar-benar mirip.  Masalah ini sering diperburuk oleh fakta bahwa sebagian besar algoritma dasar berorientasi pada pengguna atau elemen, yang membatasi kemampuan mereka untuk merekam semua jenis persamaan dan hubungan yang tersedia dalam matriks peringkat. <br><br>  Data dalam matriks peringkat biasanya sangat berkorelasi karena kesamaan antara pengguna dan elemen.  Ini berarti bahwa sinyal yang tersedia dalam matriks peringkat tidak hanya jarang, tetapi juga redundan, yang berkontribusi pada eksaserbasi masalah skalabilitas. <br><br>  Pertimbangan di atas menunjukkan bahwa matriks peringkat asli mungkin bukan representasi sinyal yang paling optimal, dan representasi alternatif lain yang lebih cocok untuk pemfilteran bersama harus dipertimbangkan.  Untuk menjelajahi ide ini, mari kita kembali ke titik awal dan berpikir sedikit tentang sifat dari layanan rekomendasi.  Bahkan, layanan rekomendasi dapat dianggap sebagai algoritme yang memprediksi peringkat berdasarkan pada beberapa ukuran kesamaan antara pengguna dan elemen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/at/bc/6n/atbc6no-aj2vssp1mrgyctgu_oy.png" alt="gambar"></div><br>  Salah satu cara untuk menentukan ukuran kesamaan ini adalah dengan menggunakan pendekatan faktor tersembunyi dan memetakan pengguna dan elemen ke titik dalam beberapa ruang dimensi k sehingga setiap pengguna dan setiap elemen diwakili oleh vektor dimensi k: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9e/ck/no/9eckno4ntrf2yl0hw1q2tr2irbk.png" alt="gambar"></div><br>  Vektor harus dikonstruksi sedemikian sehingga dimensi yang sesuai p dan q dapat dibandingkan satu sama lain.  Dengan kata lain, setiap dimensi dapat dianggap sebagai tanda atau konsep, yaitu, puj adalah ukuran kedekatan pengguna u dan konsep j, dan qij, masing-masing, adalah ukuran elemen i dan konsep j.  Dalam praktiknya, dimensi ini sering ditafsirkan sebagai genre, gaya, dan atribut lainnya yang berlaku secara simultan untuk pengguna dan elemen.  Kesamaan antara pengguna dan elemen dan, dengan demikian, peringkat dapat didefinisikan sebagai produk dari vektor yang sesuai: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/k3/1w/9w/k31w9wqmwas5pfvhr8_mopo7ibg.png" alt="gambar"></div><br>  Karena setiap peringkat dapat didekomposisi menjadi produk dari dua vektor yang termasuk dalam ruang konsep yang tidak secara langsung diamati dalam matriks peringkat asli, p dan q disebut faktor tersembunyi.  Keberhasilan pendekatan abstrak ini, tentu saja, sepenuhnya tergantung pada bagaimana faktor-faktor tersembunyi ditentukan dan dibangun.  Untuk menjawab pertanyaan ini, kami mencatat bahwa ekspresi 5.92 dapat ditulis ulang dalam bentuk matriks sebagai berikut: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2i/bp/j9/2ibpj9pmptpun2amylvxf41okjw.png" alt="gambar"></div><br>  di mana P adalah matriks n Ã— k yang dirakit dari vektor p, dan Q adalah matriks mxk yang dirakit dari vektor q, seperti yang ditunjukkan pada Gambar.  5.13.  Tujuan utama dari sistem pemfilteran gabungan biasanya untuk meminimalkan kesalahan prediksi peringkat, yang memungkinkan Anda untuk secara langsung menentukan masalah pengoptimalan sehubungan dengan matriks faktor tersembunyi: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9h/nm/mx/9hnmmxregnvn9snp9empxqf91qk.png" alt="gambar"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/z-/vl/nu/z-vlnuz-v9git5dmm0nj79etl5g.png" alt="gambar"></div><br>  Dengan asumsi bahwa jumlah dimensi tersembunyi k adalah tetap dan k â‰¤ n dan k â‰¤ m, masalah optimisasi 5.94 berkurang menjadi masalah perkiraan peringkat rendah, yang kami bahas pada Bab 2. Untuk mendemonstrasikan pendekatan terhadap solusi, mari kita asumsikan sejenak bahwa matriks peringkat telah selesai.  Dalam hal ini, masalah optimisasi memiliki solusi analitis dalam hal Singular Value Decomposition (SVD) dari matriks peringkat.  Secara khusus, menggunakan algoritma SVD standar, matriks dapat didekomposisi menjadi produk dari tiga matriks: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7z/ic/x0/7zicx0iwp156hctp6t7axd5bzu4.png" alt="gambar"></div><br>  di mana U adalah matriks n Ã— n yang dinormalisasi oleh kolom, Î£ adalah matriks diagonal n Ã— m, dan V adalah matriks mx m yang di-orthonormalisasi oleh kolom.  Solusi optimal untuk masalah 5.94 dapat diperoleh dalam hal faktor-faktor ini, terpotong ke k dimensi paling signifikan: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mw/i-/os/mwi-oskhgcs-ehf_bhepytyo_os.png" alt="gambar"></div><br>  Akibatnya, faktor tersembunyi yang optimal dalam hal akurasi prediksi dapat diperoleh dengan dekomposisi singular, seperti yang ditunjukkan di bawah ini: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uc/ih/yl/ucihyltxuqye29vkocknsbzyguw.png" alt="gambar"></div><br>  Model faktor tersembunyi berbasis SVD ini membantu memecahkan masalah pemfilteran bersama yang dijelaskan di awal bagian ini.  Pertama, ia menggantikan matriks rating n Ã— m yang besar dengan matriks faktor n Ã— k dan m Ã— k, yang biasanya jauh lebih kecil, karena dalam praktiknya jumlah optimal dimensi tersembunyi k sering kecil.  Sebagai contoh, ada kasus di mana matriks penilaian dengan 500.000 pengguna dan 17.000 elemen mampu didekati dengan cukup baik menggunakan 40 pengukuran [Funk, 2016].  Lebih lanjut, SVD menghilangkan korelasi dalam matriks peringkat: matriks faktor laten yang didefinisikan oleh 5.97 adalah ortonormal dalam kolom, yaitu dimensi tersembunyi tidak berkorelasi.  Jika, yang biasanya benar dalam praktiknya, SVD juga memecahkan masalah kerapuhan, karena sinyal yang ada dalam matriks peringkat awal terkonsentrasi secara efektif (ingat bahwa kami memilih dimensi k dengan energi sinyal tertinggi), dan matriks faktor laten tidak jarang.  Gambar 5.14 menggambarkan properti ini.  Algoritme kedekatan berbasis pengguna (5.14, a) runtuh vektor peringkat jarang untuk elemen yang diberikan dan pengguna yang diberikan untuk mendapatkan skor peringkat.  Model faktor tersembunyi (5.14, b), sebaliknya, memperkirakan peringkat dengan konvolusi dua vektor dimensi tereduksi dan dengan kepadatan energi yang lebih tinggi. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/v3/te/fm/v3tefmto-k2yf54og0xn4rpbxlq.png" alt="gambar"></div><br>  Pendekatan yang baru saja dideskripsikan terlihat seperti solusi yang koheren untuk masalah faktor tersembunyi, tetapi pada kenyataannya memiliki kelemahan serius karena asumsi bahwa matriks peringkat telah selesai.  Jika matriks peringkat jarang, yang hampir selalu demikian, algoritma SVD standar tidak dapat diterapkan secara langsung, karena tidak dapat memproses elemen yang hilang (tidak ditentukan).  Solusi paling sederhana dalam hal ini adalah mengisi peringkat yang hilang dengan beberapa nilai default, tetapi ini dapat menyebabkan bias serius dalam perkiraan.  Selain itu, ini tidak efisien secara komputasi karena kompleksitas komputasi dari solusi seperti itu sama dengan kompleksitas SVD untuk matriks n Ã— m penuh, sementara itu diinginkan untuk memiliki metode dengan kompleksitas yang proporsional dengan jumlah peringkat yang diketahui.  Masalah-masalah ini dapat diatasi dengan menggunakan metode dekomposisi alternatif yang dijelaskan pada bagian berikut. <br><br><h3>  5.8.3.1.  Dekomposisi tidak terbatas </h3><br>  Algoritma SVD standar adalah solusi analitis untuk masalah pendekatan peringkat rendah.  Namun, masalah ini dapat dianggap sebagai masalah optimisasi, dan metode optimisasi universal juga dapat diterapkan.  Salah satu pendekatan yang paling sederhana adalah dengan menggunakan metode gradient descent untuk memperbaiki nilai faktor tersembunyi secara iteratif.  Titik awal adalah definisi fungsi biaya J sebagai kesalahan perkiraan residual: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/g8/d_/gk/g8d_gkpksz8-d3wyg0xs9ca60a8.png" alt="gambar"></div><br>  Harap perhatikan bahwa saat ini kami tidak memaksakan pembatasan apa pun, seperti ortogonalitas, pada matriks faktor tersembunyi.  Menghitung gradien fungsi biaya sehubungan dengan faktor-faktor tersembunyi, kami memperoleh hasil sebagai berikut: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jn/_l/cu/jn_lcurj48vk0kuluh8mojadgcy.png" alt="gambar"></div><br>  di mana E adalah matriks kesalahan residual: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6z/wt/at/6zwtat7l4h9qoo0fq7ejzatlobi.png" alt="gambar"></div><br>  Algoritma gradient descent meminimalkan fungsi biaya dengan bergerak pada setiap langkah ke arah negatif gradien.  Oleh karena itu, Anda dapat menemukan faktor tersembunyi yang meminimalkan kesalahan kuadrat dari prediksi peringkat dengan secara iteratif mengubah matriks P dan Q untuk bertemu, sesuai dengan ekspresi berikut: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/f9/uw/gu/f9uwguspa118fw__nuchwxu1ycy.png" alt="gambar"></div><br>  di mana Î± adalah kecepatan belajar.  Kerugian dari metode gradient descent adalah kebutuhan untuk menghitung seluruh matriks kesalahan residual dan secara bersamaan mengubah semua nilai faktor-faktor tersembunyi di setiap iterasi.  Pendekatan alternatif, yang mungkin lebih cocok untuk matriks besar, adalah penurunan gradien stokastik [Funk, 2016].  Algoritma gradient descent stochastic menggunakan fakta bahwa kesalahan perkiraan total J adalah jumlah kesalahan untuk elemen individu dari matriks peringkat, oleh karena itu, gradien umum J dapat didekati dengan gradien pada satu titik data dan faktor-faktor tersembunyi dapat diubah berdasarkan elemen.  Implementasi penuh dari ide ini ditunjukkan dalam algoritma 5.1. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/le/c5/ct/lec5ctumggjq0twtb30wnamvj0g.png" alt="gambar"></div><br>  Tahap pertama dari algoritma adalah inisialisasi matriks faktor tersembunyi.  Pilihan nilai-nilai awal ini tidak terlalu penting, tetapi dalam kasus ini, distribusi yang seragam dari energi dari peringkat yang diketahui di antara faktor-faktor tersembunyi yang dihasilkan secara acak dipilih.  Kemudian algoritma secara berurutan mengoptimalkan dimensi konsep.  Untuk setiap pengukuran, ini berulang kali mengelilingi semua peringkat dalam set pelatihan, memprediksi setiap peringkat menggunakan nilai saat ini dari faktor-faktor tersembunyi, memperkirakan kesalahan dan mengoreksi nilai-nilai faktor sesuai dengan ekspresi 5.101.  Optimalisasi pengukuran selesai ketika kondisi konvergensi terpenuhi, setelah itu algoritma melanjutkan ke pengukuran berikutnya. <br><br>  Algoritma 5.1 membantu mengatasi keterbatasan metode SVD standar.  Ini mengoptimalkan faktor tersembunyi dengan perulangan melalui titik data individual, dan dengan demikian menghindari masalah dengan peringkat yang hilang dan operasi aljabar dengan matriks raksasa.  Pendekatan iteratif juga membuat penurunan gradien stokastik lebih nyaman untuk aplikasi praktis daripada penurunan gradien, yang memodifikasi seluruh matriks menggunakan ekspresi 5.101. <br><br><h3>  CONTOH 5.6 </h3><br>  Faktanya, pendekatan yang didasarkan pada faktor-faktor tersembunyi adalah seluruh kelompok metode representasi pengajaran yang dapat mengidentifikasi pola-pola yang tersirat dalam matriks peringkat dan mewakilinya secara eksplisit dalam bentuk konsep.  Terkadang konsep memiliki interpretasi yang sepenuhnya bermakna, terutama yang berenergi tinggi, meskipun ini tidak berarti bahwa semua konsep selalu memiliki makna yang bermakna.  Misalnya, menerapkan algoritma dekomposisi matriks ke database peringkat film dapat membuat faktor yang kira-kira sesuai dengan dimensi psikografis, seperti melodrama, komedi, horor, dll. Mari kita ilustrasikan fenomena ini dengan contoh numerik kecil yang menggunakan matriks peringkat dari Tabel.  5.3: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xw/6a/vu/xw6avuizjd2x8aku8k-0r89ieem.png" alt="gambar"></div><br>  Pertama, kurangi rata-rata global Î¼ = 2.82 dari semua elemen untuk memusatkan matriks, dan kemudian jalankan algoritma 5.1 dengan k = 3 pengukuran tersembunyi dan tingkat pembelajaran Î± = 0,01 untuk mendapatkan dua matriks faktor berikut: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/k-/rr/vd/k-rrvde1sezar3qo0e2vqfb7jh8.png" alt="gambar"></div><br>  Setiap baris dalam matriks ini sesuai dengan pengguna atau film, dan semua vektor 12 baris ditunjukkan pada Gambar.  5.15.  Harap dicatat bahwa elemen-elemen di kolom pertama (vektor konsep pertama) memiliki nilai terbesar, dan nilai-nilai di kolom berikutnya secara bertahap berkurang.  Ini dijelaskan oleh fakta bahwa vektor konsep pertama menangkap energi sinyal sebanyak mungkin untuk menangkap menggunakan satu pengukuran, vektor konsep kedua hanya menangkap sebagian dari energi residual, dll. Lebih lanjut, perhatikan bahwa konsep pertama dapat secara semantik diartikan sebagai sumbu drama - film aksi, di mana arah positif sesuai dengan genre film aksi, dan negatif - ke genre drama.  Peringkat dalam contoh ini sangat berkorelasi, sehingga dapat dilihat dengan jelas bahwa tiga pengguna pertama dan tiga film pertama memiliki nilai negatif besar dalam vektor konsep pertama (film drama dan pengguna yang menyukai film tersebut), sedangkan tiga pengguna terakhir dan tiga terakhir film memiliki makna positif yang luar biasa di kolom yang sama (film aksi dan pengguna yang menyukai genre ini).  Dimensi kedua dalam kasus khusus ini berhubungan terutama dengan bias pengguna atau elemen, yang dapat diartikan sebagai atribut psikografis (kekritisan penilaian pengguna? Popularitas film?).  Konsep lain dapat dianggap sebagai kebisingan. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/z0/s6/t9/z0s6t9vbnwdtgonx0p6rswo_ufe.png" alt="gambar"></div><br>  Matriks faktor yang dihasilkan tidak sepenuhnya ortogonal dalam kolom, tetapi cenderung ortogonal, karena ini mengikuti dari optimalitas solusi SVD.  Ini dapat dilihat dengan melihat produk-produk PTP dan QTQ, yang dekat dengan matriks diagonal: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pb/zr/sf/pbzrsfbqm0hjvmoqgh1xlurslnm.png" alt="gambar"></div><br>  Matriks 5.103 pada dasarnya adalah model prediksi yang dapat digunakan untuk mengevaluasi peringkat yang diketahui dan yang hilang.  Perkiraan dapat diperoleh dengan mengalikan dua faktor dan menambahkan kembali rata-rata global: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qg/du/d2/qgdud2i8fqm_ut6doi-8jnpllq8.png" alt="gambar"></div><br>  Hasilnya secara akurat mereproduksi yang diketahui dan memperkirakan peringkat yang hilang sesuai dengan harapan intuitif.  Keakuratan estimasi dapat ditingkatkan atau dikurangi dengan mengubah jumlah pengukuran, dan jumlah optimal pengukuran dapat ditentukan dalam praktik dengan memeriksa silang dan memilih kompromi yang masuk akal antara kompleksitas dan akurasi komputasi. <br><br>  Â»Informasi lebih lanjut tentang buku ini dapat ditemukan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">situs web penerbit</a> <br>  Â» <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Isi</a> <br>  Â» <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kutipan</a> <br><br>  Kupon diskon 25% untuk penjaja - <b>Pembelajaran Mesin</b> <br><br>  Setelah pembayaran versi kertas buku, sebuah buku elektronik dikirim melalui email. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id460375/">https://habr.com/ru/post/id460375/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id460361/index.html">FAQ Hebat tentang keamanan siber sistem informasi medis</a></li>
<li><a href="../id460363/index.html">7 faktor yang hilang dalam pendekatan 12 Factor App</a></li>
<li><a href="../id460365/index.html">Jejak terdistribusi: kami melakukan semua yang salah</a></li>
<li><a href="../id460367/index.html">Chaos Engineering: seni kehancuran yang disengaja. Bagian 1</a></li>
<li><a href="../id460373/index.html">Di Balik Terpal Halaman Turbo: Arsitektur Teknologi Unduhan Cepat Halaman Web</a></li>
<li><a href="../id460377/index.html">Menggunakan Liquibase untuk mengelola struktur database dalam aplikasi Spring Boot. Bagian 1</a></li>
<li><a href="../id460381/index.html">Apa itu ketegasan dan mengapa itu dibutuhkan</a></li>
<li><a href="../id460383/index.html">Transisi layar di Legend of Zelda menggunakan fitur NES yang tidak berdokumen</a></li>
<li><a href="../id460387/index.html">Panduan SELinux Beginner</a></li>
<li><a href="../id460393/index.html">Latar belakang: apa yang diharapkan dari Fedora Silverblue</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>