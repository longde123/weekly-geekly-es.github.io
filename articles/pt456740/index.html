<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§±üèª üë®üèæ‚Äç‚úàÔ∏è üàµ Imers√£o em redes neurais convolucionais. Parte 5/1 - 9 üî´ üë∫ üåª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="O curso completo em russo pode ser encontrado neste link . 
 O curso de ingl√™s original est√° dispon√≠vel neste link . 



 Novas palestras s√£o agendada...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Imers√£o em redes neurais convolucionais. Parte 5/1 - 9</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/456740/"><p>  O curso completo em russo pode ser encontrado <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">neste link</a> . <br>  O curso de ingl√™s original est√° dispon√≠vel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">neste link</a> . </p><br><p><img src="https://habrastorage.org/webt/1m/hz/qn/1mhzqnwa288uxjmptqhexriiahc.png"><br>  <i>Novas palestras s√£o agendadas a cada 2-3 dias.</i> </p><a name="habracut"></a><br><h1>  Conte√∫do </h1><br><ol><li>  Entrevista com Sebastian Trun </li><li>  1. Introdu√ß√£o </li><li>  Conjunto de dados C√£es e Gatos </li><li>  Imagens de v√°rios tamanhos </li><li>  Imagens coloridas.  Parte 1 </li><li>  Imagens coloridas.  Parte 2 </li><li>  Opera√ß√£o de convolu√ß√£o em imagens coloridas </li><li>  A opera√ß√£o de subamostragem pelo valor m√°ximo em imagens coloridas </li><li>  CoLab: gatos e c√£es </li><li>  Softmax e sigmoid </li><li>  Verifique </li><li>  Extens√£o de imagem </li><li>  Exce√ß√£o </li><li>  CoLab: c√£es e gatos.  Repeti√ß√£o </li><li>  Outras t√©cnicas para evitar a reciclagem </li><li>  Exerc√≠cios: classifica√ß√£o de imagens coloridas </li><li>  Solu√ß√£o: classifica√ß√£o da imagem colorida </li><li>  Sum√°rio </li></ol><br><h1>  Entrevista com Sebastian Trun </h1><br><p>  - Ent√£o, hoje estamos aqui de novo, junto com Sebastian e falaremos sobre reciclagem.  Este t√≥pico √© muito interessante para n√≥s, especialmente nas partes pr√°ticas do curso atual sobre o trabalho com o TensorFlow. <br>  - Sebastian, voc√™ j√° encontrou overfitting - over, fit?  Se voc√™ diz que n√£o encontrou, eu definitivamente direi que n√£o posso acreditar em voc√™! <br>  - Portanto, o motivo da reciclagem √© o chamado <strong>trade-off de desvio de desvio</strong> (um compromisso entre os valores do par√¢metro de desvio e seu spread).  Uma rede neural na qual um pequeno n√∫mero de pesos n√£o √© capaz de aprender um n√∫mero suficiente de exemplos, uma situa√ß√£o semelhante no aprendizado de m√°quina √© chamada distor√ß√£o. <br>  Sim. <br>  - Uma rede neural com tantos par√¢metros pode escolher arbitrariamente uma solu√ß√£o que voc√™ n√£o gosta, apenas por causa de um n√∫mero t√£o grande desses par√¢metros.  O resultado da escolha de uma solu√ß√£o de rede neural depende da variabilidade dos dados de origem.  Assim, uma regra simples pode ser formulada: quanto mais par√¢metros houver na rede em rela√ß√£o ao tamanho (quantidade) dos dados, maior a probabilidade de obter uma solu√ß√£o aleat√≥ria em vez da correta.  Por exemplo, voc√™ se pergunta: "Quem √© o homem nesta sala e quem √© a mulher?"  Uma rede neural complexa pode dizer que, por exemplo, todos aqueles cujos nomes come√ßam com T s√£o homens e nunca se treinam novamente.  Existem duas solu√ß√µes.  O primeiro deles usa um conjunto de dados de valida√ß√£o (uma pequena quantidade do conjunto de treinamento para validar a precis√£o do modelo).  Voc√™ pode pegar os dados, dividi-los em duas partes - 90% para treinamento e 10% para testar e realizar a chamada valida√ß√£o cruzada, onde voc√™ verifica a precis√£o do modelo nos dados que a rede neural n√£o viu - assim que o valor do erro come√ßa crescer ap√≥s um certo ciclo de treinamento - √© hora de parar de aprender.  A segunda solu√ß√£o √© introduzir restri√ß√µes na rede neural.  Por exemplo, para limitar os valores dos par√¢metros de deslocamentos e pesos, aproximando-os cada vez mais de zero.  Quanto mais limitados os pesos, menos treinado o modelo ser√°. <br>  - Entendo corretamente que podemos ter conjuntos de dados para treinamento e teste e valida√ß√£o, certo? <br>  Isso mesmo.  Se voc√™ possui um conjunto de dados para valida√ß√£o, deve ter um conjunto de dados que nunca tocou ou mostrou √† sua rede neural.  Se voc√™ mostrou ao modelo um determinado conjunto de dados v√°rias vezes, √© claro que o processo de reciclagem ser√° iniciado, o que √© muito ruim para n√≥s. <br>  - Talvez voc√™ se lembre dos casos mais interessantes quando seu modelo foi treinado novamente? <br>  - Ah, sim ... houve um incidente desse tipo quando eu estava desenvolvendo uma rede neural para jogar xadrez.  Foi em 1993. O interessante foi que, a partir dos dados do xadrez nos quais a rede neural foi treinada, a rede rapidamente determinou que, se um especialista move a rainha para o centro do tabuleiro de xadrez, h√° 60% de chance de ganhar.  O que ela come√ßou a fazer foi abrir a "passagem" com um pe√£o e mover a rainha para o centro do tabuleiro de xadrez.  Foi uma decis√£o t√£o est√∫pida para qualquer jogador de xadrez, que testemunhou claramente a reciclagem do modelo. <br>  √ìtimo!  Ent√£o, discutimos v√°rias t√©cnicas sobre como melhorar nossos modelos.  O que voc√™ acha que √© o lado mais subestimado do aprendizado profundo? <br>  - 90% do seu trabalho est√° subestimado, porque 90% do seu trabalho consistir√° em limpeza de dados. <br>  - Aqui eu concordo plenamente com voc√™! <br>  - Como mostra a pr√°tica, qualquer conjunto de dados cont√©m algum tipo de lixo.  √â muito dif√≠cil trazer os dados para o tipo certo, para torn√°-los consistentes; √© um processo muito trabalhoso. <br>  - Sim, mesmo se voc√™ trabalhar com conjuntos de dados como imagens ou v√≠deos, onde, ao que parece, todas as informa√ß√µes j√° est√£o l√°, dentro, ainda √© necess√°rio pr√©-processar as imagens. <br>  - As √∫nicas pessoas para quem os dados s√£o ideais s√£o os professores, porque eles t√™m a oportunidade de fingir em uma apresenta√ß√£o no PowerPoint que tudo est√° exatamente como deveria e tudo est√° perfeito!  Na realidade, 90% do seu tempo ser√° ocupado pela limpeza de dados. <br>  √ìtimo.  Ent√£o, vamos descobrir mais sobre reciclagem e t√©cnicas que nos permitir√£o melhorar nossos modelos de aprendizado profundo. </p><br><h1>  1. Introdu√ß√£o </h1><br><p>  oi!  E novamente, bem-vindo ao curso! <br>  ‚ÄúNa √∫ltima li√ß√£o, desenvolvemos uma pequena rede neural convolucional para classificar imagens de itens de vestu√°rio em tons de cinza a partir do conjunto de dados FASHION MNIST.  Vimos na pr√°tica que nossa pequena rede neural pode classificar imagens recebidas com precis√£o bastante alta.  No entanto, no mundo real, temos que trabalhar com imagens de alta resolu√ß√£o e v√°rios tamanhos.  Uma das grandes vantagens do SNA √© o fato de poderem funcionar igualmente bem com imagens coloridas.  Portanto, come√ßaremos nossa li√ß√£o atual, explorando como o SNA funciona com imagens coloridas. <br>  - Mais tarde, na mesma frequ√™ncia, voc√™ criar√° uma rede neural convolucional que pode classificar imagens de c√£es e gatos.  No caminho para a implementa√ß√£o de uma rede neural convolucional capaz de classificar imagens de c√£es e gatos, tamb√©m aprenderemos como usar v√°rias t√©cnicas para resolver um dos problemas mais comuns das redes neurais - a reciclagem.  E no final desta li√ß√£o, na parte pr√°tica, voc√™ desenvolver√° sua pr√≥pria rede neural convolucional para classificar imagens coloridas.  Vamos come√ßar! </p><br><h1>  Conjunto de dados de c√£es e gatos </h1><br><p>  At√© aquele momento, trabalhamos apenas com imagens em escala de cinza e tamanhos 28x28 do conjunto de dados FASHION MNIST. </p><br><p><img src="https://habrastorage.org/webt/e4/1o/cb/e41ocb39ngplbr8osyfccvji0mm.png"></p><br><p>  Em aplicativos reais, somos for√ßados a encontrar imagens de v√°rios tamanhos, por exemplo, as mostradas abaixo: </p><br><p><img src="https://habrastorage.org/webt/xs/oc/u2/xsocu2t1m1ywlkqk5_qdfslglow.png"></p><br><p>  Como mencionamos no in√≠cio desta li√ß√£o, nesta li√ß√£o, desenvolveremos uma rede neural convolucional que pode classificar imagens coloridas de c√£es e gatos. </p><br><p>  Para implementar nossos planos, usaremos imagens de c√£es e gatos do conjunto de dados Microsoft Asirra.  Cada imagem neste conjunto de dados √© rotulada como 1 ou 0 se houver um cachorro ou gato na imagem, respectivamente. </p><br><p><img src="https://habrastorage.org/webt/pn/4e/uf/pn4euf-gaaxlv8_2eakpvntjtkw.png"></p><br><p>  Apesar do conjunto de dados Microsoft Asirra conter mais de 3 milh√µes de imagens marcadas de c√£es e gatos, apenas 25.000 est√£o dispon√≠veis ao p√∫blico.  Treinar nossa rede neural convolucional nessas 25.000 imagens levar√° muito tempo.  √â por isso que usaremos um pequeno n√∫mero de imagens para treinar nossa rede neural convolucional a partir dos 25.000 dispon√≠veis. </p><br><p>  Nosso subconjunto de imagens de treinamento consiste em 2.000 e 1.000 pe√ßas de imagens para valida√ß√£o do modelo.  No conjunto de dados de treinamento, 1.000 imagens cont√™m gatos e as outras 1.000 imagens cont√™m c√£es.  Falaremos sobre o conjunto de dados para valida√ß√£o um pouco mais adiante nesta parte da li√ß√£o. </p><br><p><img src="https://habrastorage.org/webt/oa/wh/dg/oawhdglv3_dtccwky_e1pjwygog.png"></p><br><p>  Trabalhando com esse conjunto de dados, encontraremos duas dificuldades principais - trabalhar com imagens de tamanhos diferentes e trabalhar com imagens coloridas. </p><br><p>  Vamos come√ßar a explorar como trabalhar com imagens de v√°rios tamanhos. </p><br><h1>  Imagens de v√°rios tamanhos </h1><br><p>  Nosso primeiro teste ser√° resolver o problema de processar imagens de v√°rios tamanhos.  Isso ocorre porque uma rede neural na entrada precisa de dados de tamanho fixo. </p><br><p> Como exemplo, voc√™ pode se lembrar de nossas partes anteriores usando o par√¢metro <code>input_shape</code> ao criar uma camada <code>Flatten</code> : </p><br><p><img src="https://habrastorage.org/webt/v5/tt/hk/v5tthkilik-9reer8owxjpv-x3m.png"></p><br><p>  Antes de transmitir a imagem de um elemento de vestu√°rio para uma rede neural, n√≥s a convertemos em uma matriz 1D de tamanho fixo - 28x28 = 784 elementos (pixels).  Como as imagens no conjunto de dados Fashion MNIST eram do mesmo tamanho, a matriz unidimensional resultante era do mesmo tamanho e consistia em 784 elementos. </p><br><p>  No entanto, trabalhando com imagens de v√°rios tamanhos (altura e largura) e transformando-as em matrizes unidimensionais, obtemos matrizes de tamanhos diferentes. </p><br><p>  Como as redes neurais na entrada exigem dados do mesmo tamanho, n√£o basta simplesmente se safar da convers√£o em uma matriz unidimensional de valores de pixel. </p><br><p>  Resolvendo os problemas de classifica√ß√£o de imagens, sempre recorremos a uma das op√ß√µes para unificar os dados de entrada - reduzindo o tamanho das imagens a valores comuns (redimensionamento). </p><br><p><img src="https://habrastorage.org/webt/dt/rt/7f/dtrt7frns4fdinexvf0e3bow5gi.png"></p><br><p>  Neste tutorial, recorreremos ao redimensionamento de todas as imagens para tamanhos de 150 pixels de altura e 150 pixels de largura.  Convertendo imagens para um tamanho √∫nico, garantimos que a imagem do tamanho certo chegue √† entrada da rede neural e, quando transferida para uma camada <code>flatten</code> , obtemos uma matriz unidimensional do mesmo tamanho. </p><br><pre> <code class="python hljs">tf.keras.layers.Flatten(input_shape(<span class="hljs-number"><span class="hljs-number">150</span></span>,<span class="hljs-number"><span class="hljs-number">150</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))</code> </pre> <br><p>  Como resultado, obtivemos uma matriz unidimensional que consiste em 150x150 = 22.500 valores (pixels). </p><br><p>  O pr√≥ximo problema que enfrentaremos ser√° o problema das imagens coloridas.  Falaremos sobre eles na pr√≥xima parte. </p><br><h1>  Imagens coloridas.  Parte 1 </h1><br><p>  Para entender e entender como as redes neurais convolucionais funcionam com imagens coloridas, devemos investigar como exatamente o SNA funciona em geral.  Vamos atualizar o que j√° sabemos. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/3b5/c0c/687/3b5c0c68738975962dc882b9ab1522b2.png" alt="imagem"></p><br><p>  Um exemplo acima √© uma imagem em escala de cinza e como o computador a interpreta como uma matriz bidimensional de valores de pixel. </p><br><p>  Um exemplo abaixo √© uma imagem, desta vez uma <strong>cor,</strong> e como o computador a interpreta como uma matriz tridimensional de valores de pixel. </p><br><p><img src="https://habrastorage.org/webt/n-/2u/ic/n-2uicbpmraz7i2sggtwqckexbc.png"></p><br><p>  A altura e a largura da matriz 3D ser√£o determinadas pela altura e largura da imagem, e a profundidade (profundidade) determina o n√∫mero de canais de cores da imagem. </p><br><p>  A maioria das imagens coloridas pode ser representada por tr√™s canais de cores - vermelho (vermelho), verde (verde) e azul (azul). </p><br><p><img src="https://habrastorage.org/webt/kn/14/zm/kn14zmbazrhrnbhtweoil9m-ipk.png"></p><br><p>  As imagens que consistem em canais vermelho, verde e azul s√£o chamadas de imagens RGB.  A combina√ß√£o desses tr√™s canais resulta em uma imagem colorida.  Em cada uma das imagens RGB, cada canal √© representado por uma matriz bidimensional separada de pixels. </p><br><p><img src="https://habrastorage.org/webt/56/h5/g6/56h5g6loe_bu4_oiuu0-vy_unoc.png"></p><br><p>  Como o n√∫mero de canais que temos √© tr√™s, como resultado, teremos tr√™s matrizes bidimensionais.  Assim, uma imagem colorida composta por 3 canais de cores ter√° a seguinte representa√ß√£o: </p><br><p><img src="https://habrastorage.org/webt/nn/2r/q6/nn2rq6itb9gz5suamhcl5kvjwtu.png"></p><br><h1>  Imagens coloridas.  Parte 2 </h1><br><p>  Portanto, como nossa imagem agora consiste em 3 cores, o que significa que ser√° uma matriz tridimensional de valores de pixels, nosso c√≥digo precisar√° ser alterado de acordo. </p><br><p>  Se voc√™ observar o c√≥digo que usamos em nossa √∫ltima li√ß√£o, quando resolvemos o problema de classificar elementos de roupas em imagens, podemos ver que indicamos a dimens√£o dos dados de entrada: </p><br><pre> <code class="python hljs">model = Sequential() model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">28</span></span>,<span class="hljs-number"><span class="hljs-number">28</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Os dois primeiros par√¢metros da tupla <code>(28,28,1)</code> s√£o os valores da altura e largura da imagem.  As imagens no conjunto de dados Fashion MNIST tinham tamanho de 28x28 pixels.  O √∫ltimo par√¢metro na tupla <code>(28,28,1)</code> indica o n√∫mero de canais de cores.  No conjunto de dados Fashion MNIST, as imagens eram apenas em tons de cinza - 1 canal de cores. </p><br><p>  Agora que a tarefa se tornou um pouco mais complicada, e nossas imagens de c√£es e gatos se tornaram de tamanhos diferentes (mas convertidas em uma √∫nica - 150x150 pixels) e cont√™m 3 canais de cores, a tupla de valores tamb√©m deve ser diferente: </p><br><pre> <code class="python hljs">model = Sequential() model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">150</span></span>,<span class="hljs-number"><span class="hljs-number">150</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>)))</code> </pre> <br><p>  Na pr√≥xima parte, veremos como a convolu√ß√£o √© calculada na presen√ßa de tr√™s canais de cores na imagem. </p><br><h1>  Opera√ß√£o de convolu√ß√£o em imagens coloridas </h1><br><p>  Nas li√ß√µes anteriores, aprendemos como executar uma opera√ß√£o de convolu√ß√£o em imagens em escala de cinza.  Mas como executar uma opera√ß√£o de convolu√ß√£o em imagens coloridas?  Vamos come√ßar repetindo como a opera√ß√£o de convolu√ß√£o √© executada em imagens em escala de cinza. </p><br><p>  Tudo come√ßa com um filtro (n√∫cleo) de um determinado tamanho. </p><br><p><img src="https://habrastorage.org/webt/hv/jo/1h/hvjo1h8-b8xxaxnd5yzoj_6cr78.png"></p><br><p>  O filtro est√° localizado em um pixel de imagem espec√≠fico a ser convertido, ent√£o cada valor de filtro √© multiplicado pelo valor de pixel correspondente na imagem e todos esses valores s√£o somados.  O valor final do pixel √© definido na nova imagem no local em que o pixel original convertido estava localizado.  A opera√ß√£o √© repetida para cada pixel da imagem original. </p><br><p>  Tamb√©m √© importante lembrar que, durante a opera√ß√£o de convolu√ß√£o, para n√£o perder informa√ß√µes nas bordas da imagem, podemos aplicar o alinhamento e preencher as bordas da imagem com zeros: </p><br><p><img src="https://habrastorage.org/webt/fc/wa/1s/fcwa1sx3ekaufxf0zzstuzzk7bo.png"></p><br><p>  Agora vamos descobrir como podemos executar a opera√ß√£o de convolu√ß√£o em imagens coloridas. </p><br><p>  Assim como ao converter uma imagem em tons de cinza, come√ßamos escolhendo o tamanho do filtro (n√∫cleo) de um determinado tamanho. </p><br><p><img src="https://habrastorage.org/webt/ho/s2/8d/hos28dmxwzluclof1hpjtwa8eq0.png"></p><br><p>  A √∫nica diferen√ßa agora ser√° que agora o filtro em si ser√° tridimensional e o valor do par√¢metro de profundidade ser√° igual ao valor do n√∫mero de canais de cores na imagem - 3 (no nosso caso, RGB).  Para cada "camada" do canal de cores, tamb√©m aplicaremos a opera√ß√£o de convolu√ß√£o com um filtro do tamanho selecionado.  Vamos ver como ser√° um exemplo. </p><br><p><img src="https://habrastorage.org/webt/77/yz/ms/77yzmskwfveucvryfrqnfqk9ti0.png"></p><br><p>  Imagine que temos uma imagem RGB e queremos aplicar a opera√ß√£o de convolu√ß√£o com o pr√≥ximo filtro 3D.  Vale a pena prestar aten√ß√£o ao fato de que nosso filtro √© composto por 3 filtros bidimensionais.  Para simplificar, vamos imaginar que nossa imagem RGB tenha tamanho de 5x5 pixels. </p><br><p><img src="https://habrastorage.org/webt/eg/46/54/eg4654ymcuktxfo-sybo7c2jw9q.png"></p><br><p>  Lembre-se tamb√©m de que cada canal de cores √© uma matriz bidimensional de valores de cores de pixels. </p><br><p><img src="https://habrastorage.org/webt/th/sk/9y/thsk9yyqd91koooaqcftklhyq-i.png"></p><br><p>  Assim como na opera√ß√£o de convolu√ß√£o sobre imagens em tons de cinza, bem como com imagens coloridas - faremos o alinhamento e suplementaremos a imagem nas bordas com zeros para evitar a perda de informa√ß√µes nas bordas. </p><br><p><img src="https://habrastorage.org/webt/us/vl/h7/usvlh7czpbmfn2iwbibynpr_aem.png"></p><br><p>  Agora estamos prontos para a opera√ß√£o de convolu√ß√£o! </p><br><p>  O mecanismo de convolu√ß√£o para imagens coloridas ser√° semelhante ao processo que realizamos com imagens em escala de cinza.  A √∫nica diferen√ßa entre as opera√ß√µes executadas em imagens em escala de cinza e em cores √© que a opera√ß√£o de convolu√ß√£o agora precisa ser executada 3 vezes para cada canal de cores. </p><br><p><img src="https://habrastorage.org/webt/-8/mq/x5/-8mqx5ehxcqfg_4jttweanhhrhc.png"></p><br><p>  Depois de realizar a opera√ß√£o de convolu√ß√£o em cada canal de cor, adicione os tr√™s valores obtidos e adicione 1 a eles (o valor padr√£o usado ao executar opera√ß√µes desse tipo).  O novo valor resultante √© fixado na mesma posi√ß√£o na nova imagem, na qual o pixel convertido atual estava. </p><br><p>  Realizamos uma opera√ß√£o de convers√£o semelhante (uma opera√ß√£o de convolu√ß√£o) para cada pixel em nossa imagem original e para cada canal de cores. </p><br><p>  Neste exemplo em particular, a imagem resultante tem o mesmo tamanho em altura e largura da imagem RGB original. </p><br><p>  Como voc√™ pode ver, a aplica√ß√£o da opera√ß√£o de convolu√ß√£o com um √∫nico filtro 3D resulta em um √∫nico valor de sa√≠da. </p><br><p><img src="https://habrastorage.org/webt/y_/kq/c7/y_kqc7d4p07hq7v-qkfxwakwuwg.png"></p><br><p>  No entanto, ao trabalhar com redes neurais convolucionais, √© pr√°tica comum usar mais de um filtro 3D.  Se usarmos mais de um filtro 3D, o resultado ser√° v√°rios valores de sa√≠da - cada valor √© o resultado de um filtro. </p><br><p><img src="https://habrastorage.org/webt/36/ci/01/36ci013hfcfdcdthe23etkcg-_g.png"></p><br><p>  No exemplo acima, como usamos 3 filtros, a representa√ß√£o 3D resultante ter√° uma profundidade de 3 - cada camada corresponder√° ao valor de sa√≠da da convers√£o de um filtro acima da imagem com todos os seus canais de cores. </p><br><p>  Se, por exemplo, em vez de tr√™s filtros, decidirmos usar 16, a representa√ß√£o 3D de sa√≠da conter√° 16 camadas de profundidade. </p><br><p>  No c√≥digo, podemos controlar o n√∫mero de filtros criados, passando o valor apropriado para o par√¢metro de <code>filters</code> : </p><br><pre> <code class="python hljs">tf.keras.layers.Conv2D(filters, kernel_size, ...)</code> </pre> <br><p>  Tamb√©m podemos especificar o tamanho do filtro atrav√©s do par√¢metro <code>kernel_size</code> .  Por exemplo, para criar tr√™s filtros de tamanho 3x3, como foi o caso no exemplo acima, podemos escrever o c√≥digo da seguinte maneira: </p><br><pre> <code class="python hljs">tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">3</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), ...)</code> </pre> <br><p>  Lembre-se de que durante o treinamento da rede neural convolucional, os valores nos filtros 3D ser√£o atualizados para minimizar o valor da fun√ß√£o de perda. </p><br><p>  Agora que sabemos como executar a opera√ß√£o de convolu√ß√£o em imagens coloridas, √© hora de descobrir como aplicar a opera√ß√£o de subamostragem ao resultado m√°ximo pelo valor m√°ximo (o mesmo pool m√°ximo). </p><br><h1>  A opera√ß√£o de subamostragem pelo valor m√°ximo em imagens coloridas </h1><br><p>  Vamos agora aprender como executar a opera√ß√£o de subamostragem no valor m√°ximo em imagens coloridas.  De fato, a opera√ß√£o de subamostragem pelo valor m√°ximo funciona da mesma maneira que trabalha com imagens em tons de cinza com uma pequena diferen√ßa - a opera√ß√£o de subamostragem agora precisa ser aplicada a cada representa√ß√£o de sa√≠da que recebemos como resultado da aplica√ß√£o de filtros.  Vejamos um exemplo. </p><br><p>  Para simplificar, vamos imaginar que nossa visualiza√ß√£o de sa√≠da seja assim: </p><br><p><img src="https://habrastorage.org/webt/b0/-k/u1/b0-ku1roxi7hyplaadecnribfvw.png"></p><br><p>  Como antes, usaremos um kernel 2x2 e a etapa 2 para executar a opera√ß√£o de subamostragem no valor m√°ximo.  A opera√ß√£o de subamostragem pelo valor m√°ximo come√ßa com a ‚Äúinstala√ß√£o‚Äù de um kernel 2x2 no canto superior esquerdo de cada representa√ß√£o de sa√≠da (a representa√ß√£o que foi obtida ap√≥s a aplica√ß√£o da opera√ß√£o de convolu√ß√£o). </p><br><p><img src="https://habrastorage.org/webt/sc/hv/68/schv68ab1pzdg-lcazzelhmmvr8.png"></p><br><p>  Agora podemos iniciar a opera√ß√£o de subamostragem no valor m√°ximo.  Por exemplo, em nossa primeira representa√ß√£o de sa√≠da, os seguintes valores ca√≠ram no kernel 2x2 - 1, 9, 5, 4. Como o valor m√°ximo nesse kernel √© 9, √© ele que √© enviado para a nova representa√ß√£o de sa√≠da.  Uma opera√ß√£o semelhante √© repetida para cada representa√ß√£o de entrada. </p><br><p>  Como resultado, devemos obter o seguinte resultado: </p><br><p><img src="https://habrastorage.org/webt/9v/um/_0/9vum_0x2p4a78inha_xv3rnce8y.png"></p><br><p>  Depois de executar a opera√ß√£o de subamostragem pelo valor m√°ximo, o resultado √© 3 matrizes bidimensionais, cada uma das quais √© 2 vezes menor que a representa√ß√£o de entrada original. </p><br><p>  Assim, nesse caso em particular, ao executar a opera√ß√£o de subamostragem pelo valor m√°ximo sobre a representa√ß√£o tridimensional de entrada, obtemos uma representa√ß√£o tridimensional de sa√≠da da mesma profundidade, mas com os valores de altura e largura com metade dos valores iniciais. </p><br><p>  Portanto, essa √© toda a teoria de que precisamos para mais trabalho.  Agora vamos ver como isso funciona no c√≥digo! </p><br><h1>  CoLab: gatos e c√£es </h1><br><p>  O CoLab original em ingl√™s est√° dispon√≠vel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">neste link</a> . <br>  O CoLab em russo est√° dispon√≠vel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">neste link</a> . </p><br><p>  Neste tutorial, discutiremos como categorizar imagens de c√£es e gatos.  Vamos desenvolver um classificador de imagens usando o modelo <code>tf.keras.Sequential</code> e usar <code>tf.keras.Sequential</code> para carregar os dados. </p><br><h3 id="idei-kotorye-budut-zatronuty-v-etoy-chasti">  Ideias a serem abordadas nesta parte: </h3><br><p>  Adquiriremos experi√™ncia pr√°tica no desenvolvimento de um classificador e desenvolveremos uma compreens√£o intuitiva dos seguintes conceitos: </p><br><ol><li>  Construindo um modelo de fluxo de dados ( <em>pipelines de entrada de dados</em> ) usando a classe <code>tf.keras.preprocessing.image.ImageDataGenerator</code> (Como trabalhar com efici√™ncia com dados em disco interagindo com o modelo?) </li><li>  Reciclagem - o que √© e como determin√°-la? </li></ol><br><p>  <strong>Antes de come√ßarmos ...</strong> </p><br><p>  Antes de iniciar o c√≥digo no editor, recomendamos que voc√™ redefina todas as configura√ß√µes em <strong>Tempo de execu√ß√£o -&gt; Redefinir tudo</strong> no menu superior.  Essa a√ß√£o ajudar√° a evitar problemas com falta de mem√≥ria, se voc√™ trabalhou em paralelo ou est√° trabalhando com v√°rios editores. </p><br><h1 id="importirovanie-paketov">  Pacotes de importa√ß√£o </h1><br><p>  Vamos come√ßar importando os pacotes que voc√™ precisa: </p><br><ul><li>  <code>os</code> - leia arquivos e estruturas de diret√≥rios; </li><li>  <code>numpy</code> - para algumas opera√ß√µes de matriz fora do TensorFlow; </li><li>  <code>matplotlib.pyplot</code> - plotando e exibindo imagens de um conjunto de dados de teste e valida√ß√£o. </li></ul><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> __future__ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> absolute_import, division, print_function, unicode_literals <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np</code> </pre> <br><p>  <code>TensorFlow</code> importa√ß√£o </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.keras.preprocessing.image <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ImageDataGenerator</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> logging logger = tf.get_logger() logger.setLevel(logging.ERROR)</code> </pre> <br><h1 id="zagruzka-dannyh">  Carregamento de dados </h1><br><p>  Iniciamos o desenvolvimento do nosso classificador carregando um conjunto de dados.  O conjunto de dados que usamos √© uma vers√£o filtrada do conjunto de dados <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Dogs vs Cats</a> do servi√ßo Kaggle (no final, esse conjunto de dados √© fornecido pela Microsoft Research). </p><br><p>  No passado, o CoLab e eu usamos um conjunto de dados do pr√≥prio m√≥dulo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">TensorFlow Dataset</a> , o que √© extremamente conveniente para trabalho e teste.  Neste CoLab, no entanto, usaremos a classe <code>tf.keras.preprocessing.image.ImageDataGenerator</code> para ler dados do disco.  Portanto, primeiro precisamos fazer o download do conjunto de dados Dog VS Cats e descompact√°-lo. </p><br><pre> <code class="python hljs">_URL = <span class="hljs-string"><span class="hljs-string">'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'</span></span> zip_dir = tf.keras.utils.get_file(<span class="hljs-string"><span class="hljs-string">'cats_and_dogs_filterted.zip'</span></span>, origin=_URL, extract=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><p>  O conjunto de dados que baixamos tem a seguinte estrutura: </p><br><pre> <code class="plaintext hljs">cats_and_dogs_filtered |__ train |______ cats: [cat.0.jpg, cat.1.jpg, cat.2.jpg ...] |______ dogs: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...] |__ validation |______ cats: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ...] |______ dogs: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...]</code> </pre> <br><p>  Para obter uma lista completa de diret√≥rios, voc√™ pode usar o seguinte comando: </p><br><pre> <code class="python hljs">zip_dir_base = os.path.dirname(zip_dir) !find $zip_dir_base -type d -<span class="hljs-keyword"><span class="hljs-keyword">print</span></span></code> </pre> <br><p>  Como resultado, obtemos algo semelhante: </p><br><pre> <code class="plaintext hljs">/root/.keras/datasets /root/.keras/datasets/cats_and_dogs_filtered /root/.keras/datasets/cats_and_dogs_filtered/train /root/.keras/datasets/cats_and_dogs_filtered/train/dogs /root/.keras/datasets/cats_and_dogs_filtered/train/cats /root/.keras/datasets/cats_and_dogs_filtered/validation /root/.keras/datasets/cats_and_dogs_filtered/validation/dogs /root/.keras/datasets/cats_and_dogs_filtered/validation/cats</code> </pre> <br><p>  Agora atribua os caminhos corretos aos diret√≥rios com os conjuntos de dados para treinamento e valida√ß√£o para as vari√°veis: </p><br><pre> <code class="python hljs">base_dir = os.path.join(os.path.dirname(zip_dir), <span class="hljs-string"><span class="hljs-string">'cats_and_dogs_filtered'</span></span>) train_dir = os.path.join(base_dir, <span class="hljs-string"><span class="hljs-string">'train'</span></span>) validation_dir = os.path.join(base_dir, <span class="hljs-string"><span class="hljs-string">'validation'</span></span>) train_cats_dir = os.path.join(train_dir, <span class="hljs-string"><span class="hljs-string">'cats'</span></span>) train_dogs_dir = os.path.join(train_dir, <span class="hljs-string"><span class="hljs-string">'dogs'</span></span>) validation_cats_dir = os.path.join(validation_dir, <span class="hljs-string"><span class="hljs-string">'cats'</span></span>) validation_dogs_dir = os.path.join(validation_dir, <span class="hljs-string"><span class="hljs-string">'dogs'</span></span>)</code> </pre> <br><h4 id="razbiraemsya-s-dannymi-i-ih-strukturoy">  Compreendendo dados e sua estrutura </h4><br><p>  Vamos ver quantas imagens de c√£es e gatos temos nos conjuntos de dados de teste e valida√ß√£o (diret√≥rios). </p><br><pre> <code class="python hljs">num_cats_tr = len(os.listdir(train_cats_dir)) num_dogs_tr = len(os.listdir(train_dogs_dir)) num_cats_val = len(os.listdir(validation_cats_dir)) num_dogs_val = len(os.listdir(validation_dogs_dir)) total_train = num_cats_tr + num_dogs_tr total_val = num_cats_val + num_dogs_val</code> </pre> <br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">'    : '</span></span>, num_cats_tr) print(<span class="hljs-string"><span class="hljs-string">'    : '</span></span>, num_dogs_tr) print(<span class="hljs-string"><span class="hljs-string">'    : '</span></span>, num_cats_val) print(<span class="hljs-string"><span class="hljs-string">'    : '</span></span>, num_dogs_val) print(<span class="hljs-string"><span class="hljs-string">'--'</span></span>) print(<span class="hljs-string"><span class="hljs-string">'     : '</span></span>, total_train) print(<span class="hljs-string"><span class="hljs-string">'     : '</span></span>, total_val)</code> </pre> <br><p>  A sa√≠da do √∫ltimo bloco ser√° a seguinte: </p><br><pre> <code class="plaintext hljs">    : 1000     : 1000     : 500     : 500 --      : 2000      : 1000</code> </pre> <br><h1 id="ustanovka-parametrov-modeli">  Configurando par√¢metros do modelo </h1><br><p>  Por conveni√™ncia, colocaremos a instala√ß√£o das vari√°veis ‚Äã‚Äãnecess√°rias para mais processamento de dados e treinamento de modelos em um an√∫ncio separado: </p><br><pre> <code class="python hljs">BATCH_SIZE = <span class="hljs-number"><span class="hljs-number">100</span></span> <span class="hljs-comment"><span class="hljs-comment">#          IMG_SHAPE = 150 #  150x150      </span></span></code> </pre> <br><h1 id="podgotovka-dannyh">  Prepara√ß√£o de dados </h1><br><p>  Antes que as imagens possam ser usadas como entrada para nossa rede, elas devem ser convertidas em tensores com valores de ponto flutuante.  Lista de etapas a serem seguidas para fazer isso: </p><br><ol><li>  Ler imagens do disco </li><li>  Decodifique o conte√∫do da imagem e converta para o formato desejado, levando em considera√ß√£o o perfil RGB </li><li>  Converter em tensores com valores de ponto flutuante </li><li>  Para normalizar os valores do tensor do intervalo de 0 a 255 ao intervalo de 0 a 1, uma vez que as redes neurais funcionam melhor com pequenos valores de entrada. </li></ol><br><p>  Felizmente, todas essas opera√ß√µes podem ser executadas usando a classe <code>tf.keras.preprocessing.image.ImageDataGenerator</code> . </p><br><p>  Podemos fazer tudo isso usando v√°rias linhas de c√≥digo: </p><br><pre> <code class="python hljs">train_image_generator = ImageDataGenerator(rescale=<span class="hljs-number"><span class="hljs-number">1.</span></span>/<span class="hljs-number"><span class="hljs-number">255</span></span>) validation_image_generator = ImageDataGenerator(rescale=<span class="hljs-number"><span class="hljs-number">1.</span></span>/<span class="hljs-number"><span class="hljs-number">255</span></span>)</code> </pre> <br><p>  Depois de definir geradores para um conjunto de dados de teste e valida√ß√£o, o m√©todo <strong>flow_from_directory</strong> carregar√° imagens do disco, normalizar√° os dados e redimensionar√° as imagens com apenas uma linha de c√≥digo: </p><br><pre> <code class="python hljs">train_data_gen = train_image_generator.flow_from_directory(batch_size=BATCH_SIZE, directory=train_dir, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, target_size=(IMG_SHAPE,IMG_SHAPE), class_mode=<span class="hljs-string"><span class="hljs-string">'binary'</span></span>)</code> </pre> <br><p>  Conclus√£o: </p><br><pre> <code class="plaintext hljs">Found 2000 images belonging to 2 classes.</code> </pre> <br><p>  Gerador de dados de valida√ß√£o: </p><br><pre> <code class="python hljs">val_data_gen = validation_image_generator.flow_from_directory(batch_size=BATCH_SIZE, directory=validation_dir, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, target_size=(IMG_SHAPE,IMG_SHAPE), class_mode=<span class="hljs-string"><span class="hljs-string">'binary'</span></span>)</code> </pre> <br><p>  Conclus√£o: </p><br><pre> <code class="plaintext hljs">Found 1000 images belonging to 2 classes.</code> </pre> <br><h4 id="vizualiziruem-izobrazheniya-iz-trenirovochnogo-nabora">  Visualize as imagens do conjunto de treinamento. </h4><br><p>  Podemos visualizar imagens de um conjunto de dados de treinamento usando <code>matplotlib</code> : </p><br><pre> <code class="python hljs">sample_training_images, _ = next(train_data_gen)</code> </pre> <br><p>  A <code>next</code> fun√ß√£o retorna um bloco de imagens do conjunto de dados.  Um bloco √© uma tupla de <em>(muitas imagens, muitos r√≥tulos)</em> .  No momento, vamos largar os r√≥tulos, j√° que n√£o precisamos deles - estamos interessados ‚Äã‚Äãnas pr√≥prias imagens. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#        15 def plotImages(images_arr): fig, axes = plt.subplots(1, 5, figsize=(20, 20)) axes = axes.flatten() for img, ax in zip(images_arr, axes): ax.imshow(img) plt.tight_layout() plt.show()</span></span></code> </pre> <br><pre> <code class="python hljs">plotImages(sample_training_images[:<span class="hljs-number"><span class="hljs-number">5</span></span>]) <span class="hljs-comment"><span class="hljs-comment">#   0-4</span></span></code> </pre> <br><p>  Exemplo de sa√≠da (2 imagens em vez de todas as 5): </p><br><p><img src="https://habrastorage.org/webt/ag/sd/hr/agsdhrbpj3jqtgd-xwbnzoh-7go.png"></p><br><h1 id="sozdanie-modeli">  Cria√ß√£o de modelo </h1><br><h3 id="opisyvaem-model">  N√≥s descrevemos o modelo </h3><br><p>  O modelo consiste em 4 blocos de convolu√ß√£o, ap√≥s cada um dos quais existe um bloco com uma camada de subamostra.  Em seguida, temos uma camada totalmente conectada com 512 neur√¥nios e uma <code>relu</code> ativa√ß√£o <code>relu</code> .  O modelo fornecer√° uma distribui√ß√£o de probabilidade para duas classes - c√£es e gatos - usando o <code>softmax</code> . </p><br><pre> <code class="python hljs">model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">32</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, input_shape=(IMG_SHAPE, IMG_SHAPE, <span class="hljs-number"><span class="hljs-number">3</span></span>)), tf.keras.layers.MaxPooling2D(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">64</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), tf.keras.layers.MaxPooling2D(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">128</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), tf.keras.layers.MaxPooling2D(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">128</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), tf.keras.layers.MaxPooling2D(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), tf.keras.layers.Flatten(), tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">512</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">2</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>) ])</code> </pre> <br><h4 id="kompilirovanie-modeli">  Compila√ß√£o do modelo </h4><br><p>  Como antes, usaremos o otimizador do <code>adam</code> .  Usamos <code>sparse_categorical_crossentropy</code> como uma fun√ß√£o de perda.  Tamb√©m queremos monitorar a precis√£o do modelo em cada itera√ß√£o de treinamento, para que passemos o valor da <code>accuracy</code> ao par√¢metro de <code>metrics</code> : </p><br><pre> <code class="python hljs">model.compile(optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre> <br><h4 id="predstavlenie-modeli">  Vista modelo </h4><br><p>  Vamos dar uma olhada na estrutura do nosso modelo por n√≠veis usando o m√©todo de <strong>resumo</strong> : </p><br><pre> <code class="python hljs">model.summary()</code> </pre> <br><p>  Conclus√£o: </p><br><pre> <code class="plaintext hljs">Model: "sequential" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 148, 148, 32) 896 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 74, 74, 32) 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 72, 72, 64) 18496 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 34, 34, 128) 73856 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 15, 15, 128) 147584 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128) 0 _________________________________________________________________ flatten (Flatten) (None, 6272) 0 _________________________________________________________________ dense (Dense) (None, 512) 3211776 _________________________________________________________________ dense_1 (Dense) (None, 2) 1026 ================================================================= Total params: 3,453,634 Trainable params: 3,453,634 Non-trainable params: 0</code> </pre> <br><h4 id="trenirovka-modeli">   </h4><br><p>    ! </p><br><p>         ( <code>ImageDataGenerator</code> )    <code>fit_generator</code>     <code>fit</code> : </p><br><pre> <code class="python hljs">EPOCHS = <span class="hljs-number"><span class="hljs-number">100</span></span> history = model.fit_generator( train_data_gen, steps_per_epoch=int(np.ceil(total_train / float(BATCH_SIZE))), epochs=EPOCHS, validation_data=val_data_gen, validation_steps=int(np.ceil(total_val / float(BATCH_SIZE))) )</code> </pre> <br><h4 id="vizualizaciya-rezultatov-trenirovki">    </h4><br><p>       : </p><br><pre> <code class="python hljs">acc = history.history[<span class="hljs-string"><span class="hljs-string">'acc'</span></span>] val_acc = history.history[<span class="hljs-string"><span class="hljs-string">'val_acc'</span></span>] loss = history.history[<span class="hljs-string"><span class="hljs-string">'loss'</span></span>] val_loss = history.history[<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>] epochs_range = range(EPOCHS) plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">8</span></span>,<span class="hljs-number"><span class="hljs-number">8</span></span>)) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(epochs_range, acc, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.plot(epochs_range, val_acc, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'lower right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'     '</span></span>) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) plt.plot(epochs_range, loss, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.plot(epochs_range, val_loss, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'upper right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'     '</span></span>) plt.savefig(<span class="hljs-string"><span class="hljs-string">'./foo.png'</span></span>) plt.show()</code> </pre> <br><p>  Conclus√£o: </p><br><p><img src="https://habrastorage.org/webt/z5/wn/we/z5wnwe2v8nmxrkwlrpgwgdr38qg.png"></p><br><p>     ,                   70%      (    ). </p><br><p>     .          ,             . </p><br><p> <em> ‚Ä¶   .</em> </p><br><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ... e call to action padr√£o - inscreva-se, coloque um plus e compartilhe :) </font></font></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">YouTube</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Telegram</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">VKontakte</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt456740/">https://habr.com/ru/post/pt456740/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt456724/index.html">5 maneiras extremamente simples de acelerar significativamente seu aplicativo VueJS</a></li>
<li><a href="../pt456730/index.html">Livro "{Voc√™ n√£o conhece JS} Tipos e constru√ß√µes gramaticais"</a></li>
<li><a href="../pt456732/index.html">Ser um mentor</a></li>
<li><a href="../pt456736/index.html">Receitas do PostgreSQL: cURL: get, post e ... email</a></li>
<li><a href="../pt456738/index.html">Redes neurais e aprendizagem profunda, cap√≠tulo 1: usando redes neurais para reconhecer n√∫meros manuscritos</a></li>
<li><a href="../pt456744/index.html">10 problemas que resolvi com lembretes no meu smartphone</a></li>
<li><a href="../pt456746/index.html">Big data - grande responsabilidade, muito estresse e muito dinheiro</a></li>
<li><a href="../pt456748/index.html">Impressora t√©rmica 2003 de um mercado de pulgas: o que pode fazer em 2019?</a></li>
<li><a href="../pt456754/index.html">GitOps: comparando m√©todos Pull e Push</a></li>
<li><a href="../pt456756/index.html">Por que o CockroachDB altera a licen√ßa de c√≥digo-fonte aberto</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>