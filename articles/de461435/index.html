<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧑🏼‍🤝‍🧑🏼 👩🏾‍🏭 👨🏽‍🏫 Emotionserkennung unter Verwendung eines Faltungs-Neuronalen Netzwerks 💌 🤛🏿 👧</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Das Erkennen von Emotionen war für Wissenschaftler schon immer eine spannende Herausforderung. Kürzlich arbeite ich an einem experimentellen SER-Proje...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Emotionserkennung unter Verwendung eines Faltungs-Neuronalen Netzwerks</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/Voximplant/blog/461435/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/4f/gh/qr/4fghqrzh79wxguvk28uxvxg1ice.png"></div><br>  Das Erkennen von Emotionen war für Wissenschaftler schon immer eine spannende Herausforderung.  Kürzlich arbeite ich an einem experimentellen SER-Projekt (Speech Emotion Recognition), um das Potenzial dieser Technologie zu verstehen. Dazu habe ich die beliebtesten Repositories auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Github ausgewählt</a> und sie zur Grundlage meines Projekts gemacht. <br><br>  Bevor wir anfangen, das Projekt zu verstehen, wird es schön sein, sich daran zu erinnern, welche Engpässe SER hat. <br><a name="habracut"></a><br><h2>  Haupthindernisse </h2><br><ul><li>  Emotionen sind subjektiv, sogar Menschen interpretieren sie anders.  Es ist schwierig, das Konzept der „Emotion“ zu definieren. </li><li>  Audio zu kommentieren ist schwierig.  Sollten wir irgendwie jedes einzelne Wort, jeden Satz oder die gesamte Kommunikation als Ganzes markieren?  Eine Reihe von Emotionen, die zur Erkennung verwendet werden sollen? </li><li>  Das Sammeln von Daten ist ebenfalls nicht einfach.  Viele Audiodaten können aus Filmen und Nachrichten gesammelt werden.  Beide Quellen sind jedoch „voreingenommen“, da die Nachrichten neutral sein müssen und die Emotionen der Schauspieler gespielt werden.  Es ist schwierig, eine „objektive“ Quelle für Audiodaten zu finden. </li><li>  Markup-Daten erfordern große personelle und zeitliche Ressourcen.  Im Gegensatz zum Zeichnen von Rahmen auf Bildern muss speziell geschultes Personal ganze Audioaufnahmen anhören, analysieren und Kommentare abgeben.  Und dann müssen diese Kommentare von <b>vielen</b> anderen Menschen geschätzt werden, da die Bewertungen subjektiv sind. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6a/kn/wc/6aknwc9ko-dj-nzw2dmlvfb31ry.png"></div><br><h2>  Projektbeschreibung </h2><br>  Verwenden eines neuronalen Faltungsnetzwerks zum Erkennen von Emotionen in Audioaufnahmen.  Und ja, der Eigentümer des Repositorys hat keine Quellen angegeben. <br><br><h2>  Datenbeschreibung </h2><br>  Es gibt zwei Datensätze, die in den RAVDESS- und SAVEE-Repositorys verwendet wurden. Ich habe RAVDESS gerade in meinem Modell angepasst.  Im RAVDESS-Kontext gibt es zwei Arten von Daten: Sprache und Lied. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Datensatz RAVDESS (Die audiovisuelle Ryerson-Datenbank für emotionale Sprache und Gesang)</a> : <br><br><ul><li>  12 Schauspieler und 12 Schauspielerinnen nahmen ihre Rede und Lieder in ihrer Aufführung auf; </li><li>  Schauspieler Nr. 18 hat keine aufgenommenen Songs; </li><li>  Emotionen Ekel (Ekel), Neutral (Neutral) und Überraschungen (Überraschung) fehlen in den "Lied" -Daten. </li></ul><br>  Aufschlüsselung der Emotionen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ed/c1/zk/edc1zkhvwub39whbvy-v61kt9vk.png"></div><br>  Emotionsverteilungsdiagramm: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gq/un/1a/gqun1a1oud8gnesmrvkt6jtnwmu.png"></div><br><h3>  Merkmalsextraktion </h3><br>  Wenn wir mit Spracherkennungsaufgaben arbeiten, sind die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Cepstral-Koeffizienten (MFCCs)</a> eine fortschrittliche Technologie, obwohl sie in den 80er Jahren aufgetaucht sind. <br><br>  Zitat aus dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MFCC Tutorial</a> : <br><blockquote>  Diese Form bestimmt den Ausgangston.  Wenn wir die Form genau bestimmen können, erhalten wir eine genaue Darstellung des ertönten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Phonems</a> .  Die Form des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vokaltrakts</a> manifestiert sich in einer Hüllkurve eines kurzen Spektrums, und die Aufgabe des MFCC besteht darin, diese Hüllkurve genau anzuzeigen. </blockquote><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nv/id/_7/nvid_7_cvd_qg9puppfec9riswk.png"></div><br>  <font color="grey">Wellenform</font> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wz/5d/rl/wz5drlsibxxjtuu4azisa7grico.png"></div><br>  <font color="grey">Spektrogramm</font> <br><br>  Wir verwenden MFCC als Eingabefunktion.  Wenn Sie mehr über MFCC erfahren möchten, ist dieses <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tutorial</a> genau das Richtige für Sie.  Das Herunterladen und Konvertieren von Daten in das MFCC-Format kann mit dem librosa Python-Paket problemlos durchgeführt werden. <br><br><h3>  Standardmodellarchitektur </h3><br>  Der Autor entwickelte ein CNN-Modell unter Verwendung des Keras-Pakets, wobei 7 Schichten erstellt wurden - sechs Con1D-Schichten und eine Dichteschicht (Dense). <br><br><pre><code class="python hljs">model = Sequential() model.add(Conv1D(<span class="hljs-number"><span class="hljs-number">256</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>,padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">216</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) <span class="hljs-comment"><span class="hljs-comment">#1 model.add(Activation('relu')) model.add(Conv1D(128, 5,padding='same')) #2 model.add(Activation('relu')) model.add(Dropout(0.1)) model.add(MaxPooling1D(pool_size=(8))) model.add(Conv1D(128, 5,padding='same')) #3 model.add(Activation('relu')) #model.add(Conv1D(128, 5,padding='same')) #4 #model.add(Activation('relu')) #model.add(Conv1D(128, 5,padding='same')) #5 #model.add(Activation('relu')) #model.add(Dropout(0.2)) model.add(Conv1D(128, 5,padding='same')) #6 model.add(Activation('relu')) model.add(Flatten()) model.add(Dense(10)) #7 model.add(Activation('softmax')) opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)</span></span></code> </pre> <br><blockquote>  Der Autor hat die Ebenen 4 und 5 in der neuesten Version (18. September 2018) kommentiert, und die endgültige Dateigröße dieses Modells passt nicht zum bereitgestellten Netzwerk, sodass ich nicht das gleiche Ergebnis bei der Genauigkeit erzielen kann - 72%. </blockquote><br>  Das Modell wird einfach mit den Parametern <code>batch_size=16</code> und <code>epochs=700</code> ohne Trainingsplan usw. trainiert. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Compile Model model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy']) # Fit Model cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=700, validation_data=(x_testcnn, y_test))</span></span></code> </pre> <br>  Hier ist die <code>categorical_crossentropy</code> Kreuzentropie eine Funktion der Verluste, und das Maß für die Bewertung ist die Genauigkeit. <br><br><h2>  Mein Experiment </h2><br><h3>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Explorative Datenanalyse</a> </h3><br>  Im RAVDESS-Datensatz zeigt jeder Schauspieler 8 Emotionen, indem er 2 Sätze ausspricht und singt, jeweils 2 Mal.  Infolgedessen werden von jedem Schauspieler 4 Beispiele für jede Emotion erhalten, mit Ausnahme der oben genannten neutralen Emotionen, des Ekels und der Überraschung.  Jedes Audio dauert ungefähr 4 Sekunden, in der ersten und letzten Sekunde ist es meistens Stille. <br><br>  <b>Typische Angebote</b> : <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/es/87/u_/es87u_qdtsjmiv-slst1vzmzyay.png"></div><br><h3>  Beobachtung </h3><br>  Nachdem ich einen Datensatz aus 1 Schauspieler und 1 Schauspielerin ausgewählt und dann alle ihre Aufzeichnungen angehört hatte, stellte ich fest, dass Männer und Frauen ihre Gefühle auf unterschiedliche Weise ausdrücken.  Zum Beispiel: <br><br><ul><li>  männlicher Zorn (wütend) ist nur lauter; </li><li>  Männerfreude (glücklich) und Frustration (traurig) - ein Merkmal beim Lachen und Weinen während der "Stille"; </li><li>  weibliche Freude (glücklich), Wut (wütend) und Frustration (traurig) sind lauter; </li><li>  weiblicher Ekel (Ekel) enthält das Geräusch von Erbrechen. </li></ul><br><h3>  Wiederholung des Experiments </h3><br>  Der Autor entfernte die neutralen, angewiderten und überraschten Klassen, um die RAVDESS 10-Klassen-Erkennung des Datensatzes zu ermöglichen.  Beim Versuch, die Erfahrung des Autors zu wiederholen, habe ich folgendes Ergebnis erhalten: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-l/yz/vm/-lyzvmb6xx5vwqtkmjrdsawgjtc.png"></div><br><br>  Ich habe jedoch festgestellt, dass ein Datenleck vorliegt, wenn der zu validierende Datensatz mit dem Testdatensatz identisch ist.  Daher wiederholte ich die Trennung der Daten und isolierte die Datensätze von zwei Akteuren und zwei Schauspielerinnen so, dass sie während des Tests nicht sichtbar waren: <br><br><ul><li>  Die Akteure 1 bis 20 werden für Train / Valid-Sets im Verhältnis 8: 2 verwendet. </li><li>  Die Akteure 21 bis 24 sind von den Tests isoliert. </li><li>  Train Set-Parameter: (1248, 216, 1); </li><li>  Gültige Set-Parameter: (312, 216, 1); </li><li>  Test Set-Parameter: (320, 216, 1) - (isoliert). </li></ul><br>  Ich habe das Modell neu trainiert und hier ist das Ergebnis: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/57/gj/jh/57gjjho-dwtlz1p7j4zv-eawhu0.png"></div><br><h3>  Leistungstest </h3><br>  Aus dem Diagramm Zuggültiges Brutto geht hervor, dass für die ausgewählten 10 Klassen keine Konvergenz besteht.  Daher habe ich beschlossen, die Komplexität des Modells zu reduzieren und nur männliche Emotionen zu belassen.  Ich isolierte zwei Schauspieler im Test-Set und legte den Rest in das Zug / gültige Set im Verhältnis 8: 2.  Dadurch wird sichergestellt, dass der Datensatz kein Ungleichgewicht aufweist.  Dann habe ich die männlichen und weiblichen Daten getrennt trainiert, um den Test durchzuführen. <br><br>  <b>Männlicher Datensatz</b> <br><br><ul><li>  Zugset - 640 Proben von Schauspielern 1-10; </li><li>  Gültiges Set - 160 Proben von Schauspielern 1-10; </li><li>  Test Set - 160 Proben von Schauspielern 11-12. </li></ul><br>  <b>Referenzzeile: Männer</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/we/xy/hd/wexyhdyxjn9_i_wph4caesawaua.png"></div><br>  <b>Weiblicher Datensatz</b> <br><br><ul><li>  Train Set - 608 Proben von Schauspielerinnen 1-10; </li><li>  Gültiges Set - 152 Proben von Schauspielerinnen 1-10; </li><li>  Test Set - 160 Proben von Schauspielerinnen 11-12. </li></ul><br>  <b>Referenzzeile: Frauen</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ja/jc/np/jajcnp6xzp7oncqgl4-2zkshigm.png"></div><br>  Wie Sie sehen können, sind Fehlermatrizen unterschiedlich. <br><br>  Männer: Wütend und glücklich sind die wichtigsten vorhergesagten Klassen im Modell, aber sie sind nicht gleich. <br><br>  Frauen: Unordnung (traurig) und Freude (glücklich) - im Grunde vorhergesagte Klassen im Modell;  Wut und Freude sind leicht zu verwechseln. <br><br>  Ich erinnere mich an die Beobachtungen aus der <b>Intelligence Data Analysis</b> und vermute, dass weibliche Angry und Happy dem Punkt der Verwirrung ähnlich sind, weil ihre Ausdrucksweise einfach darin besteht, ihre Stimmen zu erheben. <br><br>  Darüber hinaus bin ich gespannt, ob ich, wenn ich das Modell noch weiter vereinfache, nur die Klassen Positiv, Neutral und Negativ belasse.  Oder nur positiv und negativ.  Kurz gesagt, ich habe Emotionen in 2 bzw. 3 Klassen eingeteilt. <br><br>  <b>2 Klassen:</b> <br><br><ul><li>  Positiv: Freude (glücklich), ruhig (ruhig); </li><li>  Negativ: Wut, Angst (ängstlich), Frustration (traurig). </li></ul><br>  <b>3 Klassen:</b> <br><br><ul><li>  Positiv: Freude (glücklich); </li><li>  Neutral: ruhig (ruhig), neutral (neutral); </li><li>  Negativ: Wut, Angst (ängstlich), Frustration (traurig). </li></ul><br>  Bevor ich mit dem Experiment begann, habe ich die Modellarchitektur unter Verwendung männlicher Daten eingerichtet und eine 5-Klassen-Erkennung durchgeführt. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   -  target_class = 5 #  model = Sequential() model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1))) #1 model.add(Activation('relu')) model.add(Conv1D(256, 8, padding='same')) #2 model.add(BatchNormalization()) model.add(Activation('relu')) model.add(Dropout(0.25)) model.add(MaxPooling1D(pool_size=(8))) model.add(Conv1D(128, 8, padding='same')) #3 model.add(Activation('relu')) model.add(Conv1D(128, 8, padding='same')) #4 model.add(Activation('relu')) model.add(Conv1D(128, 8, padding='same')) #5 model.add(Activation('relu')) model.add(Conv1D(128, 8, padding='same')) #6 model.add(BatchNormalization()) model.add(Activation('relu')) model.add(Dropout(0.25)) model.add(MaxPooling1D(pool_size=(8))) model.add(Conv1D(64, 8, padding='same')) #7 model.add(Activation('relu')) model.add(Conv1D(64, 8, padding='same')) #8 model.add(Activation('relu')) model.add(Flatten()) model.add(Dense(target_class)) #9 model.add(Activation('softmax')) opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)</span></span></code> </pre> <br>  Ich habe 2 Schichten Conv1D, eine Schicht MaxPooling1D und 2 Schichten BarchNormalization hinzugefügt.  Ich habe auch den Dropout-Wert auf 0,25 geändert.  Schließlich habe ich den Optimierer auf SGD mit einer Lerngeschwindigkeit von 0,0001 geändert. <br><br><pre> <code class="python hljs">lr_reduce = ReduceLROnPlateau(monitor=<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>, factor=<span class="hljs-number"><span class="hljs-number">0.9</span></span>, patience=<span class="hljs-number"><span class="hljs-number">20</span></span>, min_lr=<span class="hljs-number"><span class="hljs-number">0.000001</span></span>) mcp_save = ModelCheckpoint(<span class="hljs-string"><span class="hljs-string">'model/baseline_2class_np.h5'</span></span>, save_best_only=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, monitor=<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'min'</span></span>) cnnhistory=model.fit(x_traincnn, y_train, batch_size=<span class="hljs-number"><span class="hljs-number">16</span></span>, epochs=<span class="hljs-number"><span class="hljs-number">700</span></span>, validation_data=(x_testcnn, y_test), callbacks=[mcp_save, lr_reduce])</code> </pre> <br>  Um das Modell zu trainieren, habe ich eine Reduzierung des „Trainingsplateaus“ angewendet und nur das beste Modell mit einem Mindestwert von <code>val_loss</code> .  Und hier sind die Ergebnisse für die verschiedenen Zielklassen. <br><br><h2>  Neue Modellleistung </h2><br>  <b>Männer, 5 Klassen</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tw/jm/ay/twjmaytexfauezocygymudu6m_8.png"></div><br><br>  <b>Frauen, Klasse 5</b> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uz/mc/7t/uzmc7t4mtmwlf_mfohv3qzzc4hq.png"></div><br>  <b>Männer, Klasse 2</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/dw/y5/rf/dwy5rf0osgtrxfhb6kb-kitxyu8.png"></div><br>  <b>Männer, 3 Klassen</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/15/sy/2w/15sy2wzciyl66tapqxfwpguhzze.png"></div><br><h2>  Erhöhen (Augmentation) </h2><br>  Als ich die Architektur des Modells, den Optimierer und die Trainingsgeschwindigkeit stärkte, stellte sich heraus, dass das Modell im Trainingsmodus immer noch nicht konvergiert.  Ich schlug vor, dass dies ein Datenmengenproblem ist, da wir nur 800 Proben haben.  Dies führte mich zu Methoden zur Erhöhung der Audioqualität. Am Ende verdoppelte ich die Datensätze.  Werfen wir einen Blick auf diese Methoden. <br><br><h3>  Männer, 5 Klassen </h3><br>  <b>Dynamisches Inkrement</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">dyn_change</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> dyn_change = np.random.uniform(low=<span class="hljs-number"><span class="hljs-number">1.5</span></span>,high=<span class="hljs-number"><span class="hljs-number">3</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (data * dyn_change)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/kc/-l/iz/kc-lizrw-sintgd-ko0cdd3htlc.png"></div><br><br>  <b>Tonhöheneinstellung</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">pitch</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data, sample_rate)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> bins_per_octave = <span class="hljs-number"><span class="hljs-number">12</span></span> pitch_pm = <span class="hljs-number"><span class="hljs-number">2</span></span> pitch_change = pitch_pm * <span class="hljs-number"><span class="hljs-number">2</span></span>*(np.random.uniform()) data = librosa.effects.pitch_shift(data.astype(<span class="hljs-string"><span class="hljs-string">'float64'</span></span>), sample_rate, n_steps=pitch_change, bins_per_octave=bins_per_octave)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pk/rq/2x/pkrq2xfcdfsyph3eipzuwpvtu8a.png"></div><br>  <b>Offset</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">shift</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""   """</span></span> s_range = int(np.random.uniform(low=<span class="hljs-number"><span class="hljs-number">-5</span></span>, high = <span class="hljs-number"><span class="hljs-number">5</span></span>)*<span class="hljs-number"><span class="hljs-number">500</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.roll(data, s_range)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wc/3_/ik/wc3_ikjjnyejxxbmw23pcuanr9c.png"></div><br>  <b>Hinzufügen von weißem Rauschen</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">noise</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> <span class="hljs-comment"><span class="hljs-comment">#     : https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.random.html noise_amp = 0.005*np.random.uniform()*np.amax(data) data = data.astype('float64') + noise_amp * np.random.normal(size=data.shape[0]) return data</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uh/qh/aw/uhqhawd_gpp5sampbam9yez3lre.png"></div><br>  Es fällt auf, dass die Augmentation die Genauigkeit erheblich erhöht, im allgemeinen Fall um bis zu 70 +%.  Insbesondere bei Zugabe von Weiß, wodurch die Genauigkeit auf 87,19% erhöht wird, sinken jedoch die Testgenauigkeit und das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">F1-Maß</a> um mehr als 5%.  Und dann kam mir die Idee, mehrere Augmentationsmethoden zu kombinieren, um ein besseres Ergebnis zu erzielen. <br><br><h3>  Mehrere Methoden kombinieren </h3><br>  <b>Weißes Rauschen + Voreingenommenheit</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qv/fn/tg/qvfntgup-tjnrytyxoj2egxy1ys.png"></div><br><h2>  Testen der Augmentation bei Männern </h2><br><h3>  Männer, Klasse 2 </h3><br>  <b>Weißes Rauschen + Voreingenommenheit</b> <br><br>  Für alle Proben <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lm/8v/h8/lm8vh88-i4moor2edj9j2rtksoi.png"></div><br>  <b>Weißes Rauschen + Voreingenommenheit</b> <br><br>  Nur für positive Proben, da der 2-Klassen-Satz nicht ausgeglichen ist (gegenüber negativen Proben). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1c/0u/us/1c0uus8vlv-reh0hpd-jnoherm8.png"></div><br>  <b>Tonhöhe + weißes Rauschen</b> <br>  Für alle Proben <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gb/qb/oz/gbqbozph3uampaicmgzewiitacy.png"></div><br>  <b>Tonhöhe + weißes Rauschen</b> <br><br>  Nur für positive Proben <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ip/jk/4p/ipjk4phb0cqc56qfbudr-_vwuww.png"></div><br><h2>  Fazit </h2><br>  Am Ende konnte ich nur mit einem männlichen Datensatz experimentieren.  Ich habe die Daten neu aufgeteilt, um Ungleichgewichte und infolgedessen Datenlecks zu vermeiden.  Ich habe das Modell so eingerichtet, dass es mit Männerstimmen experimentiert, da ich das Modell so weit wie möglich vereinfachen wollte, um loszulegen.  Ich habe auch Tests mit verschiedenen Augmentationsmethoden durchgeführt.  Das Hinzufügen von weißem Rauschen und Vorspannung hat bei unausgeglichenen Daten gut funktioniert. <br><br><h2>  Schlussfolgerungen </h2><br><ul><li>  Emotionen sind subjektiv und schwer zu beheben; </li><li>  Es ist notwendig, im Voraus zu bestimmen, welche Emotionen für das Projekt geeignet sind. </li><li>  Vertraue Github nicht immer Inhalten, auch wenn es viele Sterne hat. </li><li>  Datenaustausch - denken Sie daran; </li><li>  Die explorative Datenanalyse liefert immer eine gute Idee, aber Sie müssen geduldig sein, wenn Sie mit Audiodaten arbeiten. </li><li>  Bestimmen Sie, was Sie für die Eingabe Ihres Modells geben: einen Satz, einen gesamten Datensatz oder einen Ausruf? </li><li>  Datenmangel ist ein wichtiger Erfolgsfaktor bei SER. Die Erstellung eines guten Datensatzes mit Emotionen ist jedoch eine komplexe und teure Aufgabe. </li><li>  Vereinfachen Sie Ihr Modell bei Datenmangel. </li></ul><br><h2>  Weitere Verbesserung </h2><br><ul><li>  Ich habe nur die ersten 3 Sekunden als Eingabe verwendet, um die Gesamtdatengröße zu reduzieren - das ursprüngliche Projekt verwendete 2,5 Sekunden.  Ich würde gerne mit Aufnahmen in voller Größe experimentieren. </li><li>  Sie können die Daten vorverarbeiten: Trimmen Sie die Stille, normalisieren Sie die Länge durch Auffüllen mit Nullen usw.; </li><li>  Versuchen Sie es mit wiederkehrenden neuronalen Netzen für diese Aufgabe. </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de461435/">https://habr.com/ru/post/de461435/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de461421/index.html">So versichern Sie sich gegen mögliche Verluste bei einer Investition an der Börse: Strukturprodukte</a></li>
<li><a href="../de461423/index.html">11 Tipps: Präsentieren von UI / UX-Arbeiten für „Nicht-Designer“</a></li>
<li><a href="../de461425/index.html">Wie man ein Produktmanager wird und weiter wächst</a></li>
<li><a href="../de461431/index.html">"Liebt und mag nicht": DNS über HTTPS</a></li>
<li><a href="../de461433/index.html">Verwenden von Identity Server 4 in Net Core 3.0</a></li>
<li><a href="../de461437/index.html">370 Glühbirnen</a></li>
<li><a href="../de461439/index.html">Starten der React- und TypeScript-Komponentenbibliothek</a></li>
<li><a href="../de461441/index.html">Berichte über den Speicherstatus mithilfe von R. Parallel Computing, Grafiken, XLSX, E-Mail und all dies</a></li>
<li><a href="../de461443/index.html">Nachanalyse: Was ist über den jüngsten Angriff auf das SKS Keyserver-Kryptoschlüsselservernetzwerk bekannt?</a></li>
<li><a href="../de461447/index.html">Das Epos über Systemadministratoren als gefährdete Spezies</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>