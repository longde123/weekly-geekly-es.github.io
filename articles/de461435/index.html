<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßëüèº‚Äçü§ù‚Äçüßëüèº üë©üèæ‚Äçüè≠ üë®üèΩ‚Äçüè´ Emotionserkennung unter Verwendung eines Faltungs-Neuronalen Netzwerks üíå ü§õüèø üëß</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Das Erkennen von Emotionen war f√ºr Wissenschaftler schon immer eine spannende Herausforderung. K√ºrzlich arbeite ich an einem experimentellen SER-Proje...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Emotionserkennung unter Verwendung eines Faltungs-Neuronalen Netzwerks</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/Voximplant/blog/461435/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/4f/gh/qr/4fghqrzh79wxguvk28uxvxg1ice.png"></div><br>  Das Erkennen von Emotionen war f√ºr Wissenschaftler schon immer eine spannende Herausforderung.  K√ºrzlich arbeite ich an einem experimentellen SER-Projekt (Speech Emotion Recognition), um das Potenzial dieser Technologie zu verstehen. Dazu habe ich die beliebtesten Repositories auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Github ausgew√§hlt</a> und sie zur Grundlage meines Projekts gemacht. <br><br>  Bevor wir anfangen, das Projekt zu verstehen, wird es sch√∂n sein, sich daran zu erinnern, welche Engp√§sse SER hat. <br><a name="habracut"></a><br><h2>  Haupthindernisse </h2><br><ul><li>  Emotionen sind subjektiv, sogar Menschen interpretieren sie anders.  Es ist schwierig, das Konzept der ‚ÄûEmotion‚Äú zu definieren. </li><li>  Audio zu kommentieren ist schwierig.  Sollten wir irgendwie jedes einzelne Wort, jeden Satz oder die gesamte Kommunikation als Ganzes markieren?  Eine Reihe von Emotionen, die zur Erkennung verwendet werden sollen? </li><li>  Das Sammeln von Daten ist ebenfalls nicht einfach.  Viele Audiodaten k√∂nnen aus Filmen und Nachrichten gesammelt werden.  Beide Quellen sind jedoch ‚Äûvoreingenommen‚Äú, da die Nachrichten neutral sein m√ºssen und die Emotionen der Schauspieler gespielt werden.  Es ist schwierig, eine ‚Äûobjektive‚Äú Quelle f√ºr Audiodaten zu finden. </li><li>  Markup-Daten erfordern gro√üe personelle und zeitliche Ressourcen.  Im Gegensatz zum Zeichnen von Rahmen auf Bildern muss speziell geschultes Personal ganze Audioaufnahmen anh√∂ren, analysieren und Kommentare abgeben.  Und dann m√ºssen diese Kommentare von <b>vielen</b> anderen Menschen gesch√§tzt werden, da die Bewertungen subjektiv sind. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6a/kn/wc/6aknwc9ko-dj-nzw2dmlvfb31ry.png"></div><br><h2>  Projektbeschreibung </h2><br>  Verwenden eines neuronalen Faltungsnetzwerks zum Erkennen von Emotionen in Audioaufnahmen.  Und ja, der Eigent√ºmer des Repositorys hat keine Quellen angegeben. <br><br><h2>  Datenbeschreibung </h2><br>  Es gibt zwei Datens√§tze, die in den RAVDESS- und SAVEE-Repositorys verwendet wurden. Ich habe RAVDESS gerade in meinem Modell angepasst.  Im RAVDESS-Kontext gibt es zwei Arten von Daten: Sprache und Lied. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Datensatz RAVDESS (Die audiovisuelle Ryerson-Datenbank f√ºr emotionale Sprache und Gesang)</a> : <br><br><ul><li>  12 Schauspieler und 12 Schauspielerinnen nahmen ihre Rede und Lieder in ihrer Auff√ºhrung auf; </li><li>  Schauspieler Nr. 18 hat keine aufgenommenen Songs; </li><li>  Emotionen Ekel (Ekel), Neutral (Neutral) und √úberraschungen (√úberraschung) fehlen in den "Lied" -Daten. </li></ul><br>  Aufschl√ºsselung der Emotionen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ed/c1/zk/edc1zkhvwub39whbvy-v61kt9vk.png"></div><br>  Emotionsverteilungsdiagramm: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gq/un/1a/gqun1a1oud8gnesmrvkt6jtnwmu.png"></div><br><h3>  Merkmalsextraktion </h3><br>  Wenn wir mit Spracherkennungsaufgaben arbeiten, sind die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Cepstral-Koeffizienten (MFCCs)</a> eine fortschrittliche Technologie, obwohl sie in den 80er Jahren aufgetaucht sind. <br><br>  Zitat aus dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MFCC Tutorial</a> : <br><blockquote>  Diese Form bestimmt den Ausgangston.  Wenn wir die Form genau bestimmen k√∂nnen, erhalten wir eine genaue Darstellung des ert√∂nten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Phonems</a> .  Die Form des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vokaltrakts</a> manifestiert sich in einer H√ºllkurve eines kurzen Spektrums, und die Aufgabe des MFCC besteht darin, diese H√ºllkurve genau anzuzeigen. </blockquote><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nv/id/_7/nvid_7_cvd_qg9puppfec9riswk.png"></div><br>  <font color="grey">Wellenform</font> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wz/5d/rl/wz5drlsibxxjtuu4azisa7grico.png"></div><br>  <font color="grey">Spektrogramm</font> <br><br>  Wir verwenden MFCC als Eingabefunktion.  Wenn Sie mehr √ºber MFCC erfahren m√∂chten, ist dieses <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tutorial</a> genau das Richtige f√ºr Sie.  Das Herunterladen und Konvertieren von Daten in das MFCC-Format kann mit dem librosa Python-Paket problemlos durchgef√ºhrt werden. <br><br><h3>  Standardmodellarchitektur </h3><br>  Der Autor entwickelte ein CNN-Modell unter Verwendung des Keras-Pakets, wobei 7 Schichten erstellt wurden - sechs Con1D-Schichten und eine Dichteschicht (Dense). <br><br><pre><code class="python hljs">model = Sequential() model.add(Conv1D(<span class="hljs-number"><span class="hljs-number">256</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>,padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">216</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) <span class="hljs-comment"><span class="hljs-comment">#1 model.add(Activation('relu')) model.add(Conv1D(128, 5,padding='same')) #2 model.add(Activation('relu')) model.add(Dropout(0.1)) model.add(MaxPooling1D(pool_size=(8))) model.add(Conv1D(128, 5,padding='same')) #3 model.add(Activation('relu')) #model.add(Conv1D(128, 5,padding='same')) #4 #model.add(Activation('relu')) #model.add(Conv1D(128, 5,padding='same')) #5 #model.add(Activation('relu')) #model.add(Dropout(0.2)) model.add(Conv1D(128, 5,padding='same')) #6 model.add(Activation('relu')) model.add(Flatten()) model.add(Dense(10)) #7 model.add(Activation('softmax')) opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)</span></span></code> </pre> <br><blockquote>  Der Autor hat die Ebenen 4 und 5 in der neuesten Version (18. September 2018) kommentiert, und die endg√ºltige Dateigr√∂√üe dieses Modells passt nicht zum bereitgestellten Netzwerk, sodass ich nicht das gleiche Ergebnis bei der Genauigkeit erzielen kann - 72%. </blockquote><br>  Das Modell wird einfach mit den Parametern <code>batch_size=16</code> und <code>epochs=700</code> ohne Trainingsplan usw. trainiert. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Compile Model model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy']) # Fit Model cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=700, validation_data=(x_testcnn, y_test))</span></span></code> </pre> <br>  Hier ist die <code>categorical_crossentropy</code> Kreuzentropie eine Funktion der Verluste, und das Ma√ü f√ºr die Bewertung ist die Genauigkeit. <br><br><h2>  Mein Experiment </h2><br><h3>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Explorative Datenanalyse</a> </h3><br>  Im RAVDESS-Datensatz zeigt jeder Schauspieler 8 Emotionen, indem er 2 S√§tze ausspricht und singt, jeweils 2 Mal.  Infolgedessen werden von jedem Schauspieler 4 Beispiele f√ºr jede Emotion erhalten, mit Ausnahme der oben genannten neutralen Emotionen, des Ekels und der √úberraschung.  Jedes Audio dauert ungef√§hr 4 Sekunden, in der ersten und letzten Sekunde ist es meistens Stille. <br><br>  <b>Typische Angebote</b> : <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/es/87/u_/es87u_qdtsjmiv-slst1vzmzyay.png"></div><br><h3>  Beobachtung </h3><br>  Nachdem ich einen Datensatz aus 1 Schauspieler und 1 Schauspielerin ausgew√§hlt und dann alle ihre Aufzeichnungen angeh√∂rt hatte, stellte ich fest, dass M√§nner und Frauen ihre Gef√ºhle auf unterschiedliche Weise ausdr√ºcken.  Zum Beispiel: <br><br><ul><li>  m√§nnlicher Zorn (w√ºtend) ist nur lauter; </li><li>  M√§nnerfreude (gl√ºcklich) und Frustration (traurig) - ein Merkmal beim Lachen und Weinen w√§hrend der "Stille"; </li><li>  weibliche Freude (gl√ºcklich), Wut (w√ºtend) und Frustration (traurig) sind lauter; </li><li>  weiblicher Ekel (Ekel) enth√§lt das Ger√§usch von Erbrechen. </li></ul><br><h3>  Wiederholung des Experiments </h3><br>  Der Autor entfernte die neutralen, angewiderten und √ºberraschten Klassen, um die RAVDESS 10-Klassen-Erkennung des Datensatzes zu erm√∂glichen.  Beim Versuch, die Erfahrung des Autors zu wiederholen, habe ich folgendes Ergebnis erhalten: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-l/yz/vm/-lyzvmb6xx5vwqtkmjrdsawgjtc.png"></div><br><br>  Ich habe jedoch festgestellt, dass ein Datenleck vorliegt, wenn der zu validierende Datensatz mit dem Testdatensatz identisch ist.  Daher wiederholte ich die Trennung der Daten und isolierte die Datens√§tze von zwei Akteuren und zwei Schauspielerinnen so, dass sie w√§hrend des Tests nicht sichtbar waren: <br><br><ul><li>  Die Akteure 1 bis 20 werden f√ºr Train / Valid-Sets im Verh√§ltnis 8: 2 verwendet. </li><li>  Die Akteure 21 bis 24 sind von den Tests isoliert. </li><li>  Train Set-Parameter: (1248, 216, 1); </li><li>  G√ºltige Set-Parameter: (312, 216, 1); </li><li>  Test Set-Parameter: (320, 216, 1) - (isoliert). </li></ul><br>  Ich habe das Modell neu trainiert und hier ist das Ergebnis: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/57/gj/jh/57gjjho-dwtlz1p7j4zv-eawhu0.png"></div><br><h3>  Leistungstest </h3><br>  Aus dem Diagramm Zugg√ºltiges Brutto geht hervor, dass f√ºr die ausgew√§hlten 10 Klassen keine Konvergenz besteht.  Daher habe ich beschlossen, die Komplexit√§t des Modells zu reduzieren und nur m√§nnliche Emotionen zu belassen.  Ich isolierte zwei Schauspieler im Test-Set und legte den Rest in das Zug / g√ºltige Set im Verh√§ltnis 8: 2.  Dadurch wird sichergestellt, dass der Datensatz kein Ungleichgewicht aufweist.  Dann habe ich die m√§nnlichen und weiblichen Daten getrennt trainiert, um den Test durchzuf√ºhren. <br><br>  <b>M√§nnlicher Datensatz</b> <br><br><ul><li>  Zugset - 640 Proben von Schauspielern 1-10; </li><li>  G√ºltiges Set - 160 Proben von Schauspielern 1-10; </li><li>  Test Set - 160 Proben von Schauspielern 11-12. </li></ul><br>  <b>Referenzzeile: M√§nner</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/we/xy/hd/wexyhdyxjn9_i_wph4caesawaua.png"></div><br>  <b>Weiblicher Datensatz</b> <br><br><ul><li>  Train Set - 608 Proben von Schauspielerinnen 1-10; </li><li>  G√ºltiges Set - 152 Proben von Schauspielerinnen 1-10; </li><li>  Test Set - 160 Proben von Schauspielerinnen 11-12. </li></ul><br>  <b>Referenzzeile: Frauen</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ja/jc/np/jajcnp6xzp7oncqgl4-2zkshigm.png"></div><br>  Wie Sie sehen k√∂nnen, sind Fehlermatrizen unterschiedlich. <br><br>  M√§nner: W√ºtend und gl√ºcklich sind die wichtigsten vorhergesagten Klassen im Modell, aber sie sind nicht gleich. <br><br>  Frauen: Unordnung (traurig) und Freude (gl√ºcklich) - im Grunde vorhergesagte Klassen im Modell;  Wut und Freude sind leicht zu verwechseln. <br><br>  Ich erinnere mich an die Beobachtungen aus der <b>Intelligence Data Analysis</b> und vermute, dass weibliche Angry und Happy dem Punkt der Verwirrung √§hnlich sind, weil ihre Ausdrucksweise einfach darin besteht, ihre Stimmen zu erheben. <br><br>  Dar√ºber hinaus bin ich gespannt, ob ich, wenn ich das Modell noch weiter vereinfache, nur die Klassen Positiv, Neutral und Negativ belasse.  Oder nur positiv und negativ.  Kurz gesagt, ich habe Emotionen in 2 bzw. 3 Klassen eingeteilt. <br><br>  <b>2 Klassen:</b> <br><br><ul><li>  Positiv: Freude (gl√ºcklich), ruhig (ruhig); </li><li>  Negativ: Wut, Angst (√§ngstlich), Frustration (traurig). </li></ul><br>  <b>3 Klassen:</b> <br><br><ul><li>  Positiv: Freude (gl√ºcklich); </li><li>  Neutral: ruhig (ruhig), neutral (neutral); </li><li>  Negativ: Wut, Angst (√§ngstlich), Frustration (traurig). </li></ul><br>  Bevor ich mit dem Experiment begann, habe ich die Modellarchitektur unter Verwendung m√§nnlicher Daten eingerichtet und eine 5-Klassen-Erkennung durchgef√ºhrt. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   -  target_class = 5 #  model = Sequential() model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1))) #1 model.add(Activation('relu')) model.add(Conv1D(256, 8, padding='same')) #2 model.add(BatchNormalization()) model.add(Activation('relu')) model.add(Dropout(0.25)) model.add(MaxPooling1D(pool_size=(8))) model.add(Conv1D(128, 8, padding='same')) #3 model.add(Activation('relu')) model.add(Conv1D(128, 8, padding='same')) #4 model.add(Activation('relu')) model.add(Conv1D(128, 8, padding='same')) #5 model.add(Activation('relu')) model.add(Conv1D(128, 8, padding='same')) #6 model.add(BatchNormalization()) model.add(Activation('relu')) model.add(Dropout(0.25)) model.add(MaxPooling1D(pool_size=(8))) model.add(Conv1D(64, 8, padding='same')) #7 model.add(Activation('relu')) model.add(Conv1D(64, 8, padding='same')) #8 model.add(Activation('relu')) model.add(Flatten()) model.add(Dense(target_class)) #9 model.add(Activation('softmax')) opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)</span></span></code> </pre> <br>  Ich habe 2 Schichten Conv1D, eine Schicht MaxPooling1D und 2 Schichten BarchNormalization hinzugef√ºgt.  Ich habe auch den Dropout-Wert auf 0,25 ge√§ndert.  Schlie√ülich habe ich den Optimierer auf SGD mit einer Lerngeschwindigkeit von 0,0001 ge√§ndert. <br><br><pre> <code class="python hljs">lr_reduce = ReduceLROnPlateau(monitor=<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>, factor=<span class="hljs-number"><span class="hljs-number">0.9</span></span>, patience=<span class="hljs-number"><span class="hljs-number">20</span></span>, min_lr=<span class="hljs-number"><span class="hljs-number">0.000001</span></span>) mcp_save = ModelCheckpoint(<span class="hljs-string"><span class="hljs-string">'model/baseline_2class_np.h5'</span></span>, save_best_only=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, monitor=<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'min'</span></span>) cnnhistory=model.fit(x_traincnn, y_train, batch_size=<span class="hljs-number"><span class="hljs-number">16</span></span>, epochs=<span class="hljs-number"><span class="hljs-number">700</span></span>, validation_data=(x_testcnn, y_test), callbacks=[mcp_save, lr_reduce])</code> </pre> <br>  Um das Modell zu trainieren, habe ich eine Reduzierung des ‚ÄûTrainingsplateaus‚Äú angewendet und nur das beste Modell mit einem Mindestwert von <code>val_loss</code> .  Und hier sind die Ergebnisse f√ºr die verschiedenen Zielklassen. <br><br><h2>  Neue Modellleistung </h2><br>  <b>M√§nner, 5 Klassen</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tw/jm/ay/twjmaytexfauezocygymudu6m_8.png"></div><br><br>  <b>Frauen, Klasse 5</b> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uz/mc/7t/uzmc7t4mtmwlf_mfohv3qzzc4hq.png"></div><br>  <b>M√§nner, Klasse 2</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/dw/y5/rf/dwy5rf0osgtrxfhb6kb-kitxyu8.png"></div><br>  <b>M√§nner, 3 Klassen</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/15/sy/2w/15sy2wzciyl66tapqxfwpguhzze.png"></div><br><h2>  Erh√∂hen (Augmentation) </h2><br>  Als ich die Architektur des Modells, den Optimierer und die Trainingsgeschwindigkeit st√§rkte, stellte sich heraus, dass das Modell im Trainingsmodus immer noch nicht konvergiert.  Ich schlug vor, dass dies ein Datenmengenproblem ist, da wir nur 800 Proben haben.  Dies f√ºhrte mich zu Methoden zur Erh√∂hung der Audioqualit√§t. Am Ende verdoppelte ich die Datens√§tze.  Werfen wir einen Blick auf diese Methoden. <br><br><h3>  M√§nner, 5 Klassen </h3><br>  <b>Dynamisches Inkrement</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">dyn_change</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> dyn_change = np.random.uniform(low=<span class="hljs-number"><span class="hljs-number">1.5</span></span>,high=<span class="hljs-number"><span class="hljs-number">3</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (data * dyn_change)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/kc/-l/iz/kc-lizrw-sintgd-ko0cdd3htlc.png"></div><br><br>  <b>Tonh√∂heneinstellung</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">pitch</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data, sample_rate)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> bins_per_octave = <span class="hljs-number"><span class="hljs-number">12</span></span> pitch_pm = <span class="hljs-number"><span class="hljs-number">2</span></span> pitch_change = pitch_pm * <span class="hljs-number"><span class="hljs-number">2</span></span>*(np.random.uniform()) data = librosa.effects.pitch_shift(data.astype(<span class="hljs-string"><span class="hljs-string">'float64'</span></span>), sample_rate, n_steps=pitch_change, bins_per_octave=bins_per_octave)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pk/rq/2x/pkrq2xfcdfsyph3eipzuwpvtu8a.png"></div><br>  <b>Offset</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">shift</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""   """</span></span> s_range = int(np.random.uniform(low=<span class="hljs-number"><span class="hljs-number">-5</span></span>, high = <span class="hljs-number"><span class="hljs-number">5</span></span>)*<span class="hljs-number"><span class="hljs-number">500</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.roll(data, s_range)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wc/3_/ik/wc3_ikjjnyejxxbmw23pcuanr9c.png"></div><br>  <b>Hinzuf√ºgen von wei√üem Rauschen</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">noise</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> <span class="hljs-comment"><span class="hljs-comment">#     : https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.random.html noise_amp = 0.005*np.random.uniform()*np.amax(data) data = data.astype('float64') + noise_amp * np.random.normal(size=data.shape[0]) return data</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uh/qh/aw/uhqhawd_gpp5sampbam9yez3lre.png"></div><br>  Es f√§llt auf, dass die Augmentation die Genauigkeit erheblich erh√∂ht, im allgemeinen Fall um bis zu 70 +%.  Insbesondere bei Zugabe von Wei√ü, wodurch die Genauigkeit auf 87,19% erh√∂ht wird, sinken jedoch die Testgenauigkeit und das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">F1-Ma√ü</a> um mehr als 5%.  Und dann kam mir die Idee, mehrere Augmentationsmethoden zu kombinieren, um ein besseres Ergebnis zu erzielen. <br><br><h3>  Mehrere Methoden kombinieren </h3><br>  <b>Wei√ües Rauschen + Voreingenommenheit</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qv/fn/tg/qvfntgup-tjnrytyxoj2egxy1ys.png"></div><br><h2>  Testen der Augmentation bei M√§nnern </h2><br><h3>  M√§nner, Klasse 2 </h3><br>  <b>Wei√ües Rauschen + Voreingenommenheit</b> <br><br>  F√ºr alle Proben <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lm/8v/h8/lm8vh88-i4moor2edj9j2rtksoi.png"></div><br>  <b>Wei√ües Rauschen + Voreingenommenheit</b> <br><br>  Nur f√ºr positive Proben, da der 2-Klassen-Satz nicht ausgeglichen ist (gegen√ºber negativen Proben). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1c/0u/us/1c0uus8vlv-reh0hpd-jnoherm8.png"></div><br>  <b>Tonh√∂he + wei√ües Rauschen</b> <br>  F√ºr alle Proben <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gb/qb/oz/gbqbozph3uampaicmgzewiitacy.png"></div><br>  <b>Tonh√∂he + wei√ües Rauschen</b> <br><br>  Nur f√ºr positive Proben <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ip/jk/4p/ipjk4phb0cqc56qfbudr-_vwuww.png"></div><br><h2>  Fazit </h2><br>  Am Ende konnte ich nur mit einem m√§nnlichen Datensatz experimentieren.  Ich habe die Daten neu aufgeteilt, um Ungleichgewichte und infolgedessen Datenlecks zu vermeiden.  Ich habe das Modell so eingerichtet, dass es mit M√§nnerstimmen experimentiert, da ich das Modell so weit wie m√∂glich vereinfachen wollte, um loszulegen.  Ich habe auch Tests mit verschiedenen Augmentationsmethoden durchgef√ºhrt.  Das Hinzuf√ºgen von wei√üem Rauschen und Vorspannung hat bei unausgeglichenen Daten gut funktioniert. <br><br><h2>  Schlussfolgerungen </h2><br><ul><li>  Emotionen sind subjektiv und schwer zu beheben; </li><li>  Es ist notwendig, im Voraus zu bestimmen, welche Emotionen f√ºr das Projekt geeignet sind. </li><li>  Vertraue Github nicht immer Inhalten, auch wenn es viele Sterne hat. </li><li>  Datenaustausch - denken Sie daran; </li><li>  Die explorative Datenanalyse liefert immer eine gute Idee, aber Sie m√ºssen geduldig sein, wenn Sie mit Audiodaten arbeiten. </li><li>  Bestimmen Sie, was Sie f√ºr die Eingabe Ihres Modells geben: einen Satz, einen gesamten Datensatz oder einen Ausruf? </li><li>  Datenmangel ist ein wichtiger Erfolgsfaktor bei SER. Die Erstellung eines guten Datensatzes mit Emotionen ist jedoch eine komplexe und teure Aufgabe. </li><li>  Vereinfachen Sie Ihr Modell bei Datenmangel. </li></ul><br><h2>  Weitere Verbesserung </h2><br><ul><li>  Ich habe nur die ersten 3 Sekunden als Eingabe verwendet, um die Gesamtdatengr√∂√üe zu reduzieren - das urspr√ºngliche Projekt verwendete 2,5 Sekunden.  Ich w√ºrde gerne mit Aufnahmen in voller Gr√∂√üe experimentieren. </li><li>  Sie k√∂nnen die Daten vorverarbeiten: Trimmen Sie die Stille, normalisieren Sie die L√§nge durch Auff√ºllen mit Nullen usw.; </li><li>  Versuchen Sie es mit wiederkehrenden neuronalen Netzen f√ºr diese Aufgabe. </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de461435/">https://habr.com/ru/post/de461435/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de461421/index.html">So versichern Sie sich gegen m√∂gliche Verluste bei einer Investition an der B√∂rse: Strukturprodukte</a></li>
<li><a href="../de461423/index.html">11 Tipps: Pr√§sentieren von UI / UX-Arbeiten f√ºr ‚ÄûNicht-Designer‚Äú</a></li>
<li><a href="../de461425/index.html">Wie man ein Produktmanager wird und weiter w√§chst</a></li>
<li><a href="../de461431/index.html">"Liebt und mag nicht": DNS √ºber HTTPS</a></li>
<li><a href="../de461433/index.html">Verwenden von Identity Server 4 in Net Core 3.0</a></li>
<li><a href="../de461437/index.html">370 Gl√ºhbirnen</a></li>
<li><a href="../de461439/index.html">Starten der React- und TypeScript-Komponentenbibliothek</a></li>
<li><a href="../de461441/index.html">Berichte √ºber den Speicherstatus mithilfe von R. Parallel Computing, Grafiken, XLSX, E-Mail und all dies</a></li>
<li><a href="../de461443/index.html">Nachanalyse: Was ist √ºber den j√ºngsten Angriff auf das SKS Keyserver-Kryptoschl√ºsselservernetzwerk bekannt?</a></li>
<li><a href="../de461447/index.html">Das Epos √ºber Systemadministratoren als gef√§hrdete Spezies</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>