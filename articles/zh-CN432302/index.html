<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩‍⚕️ 🤟🏾 🦑 序列到序列第2部分模型 🧑🏼‍🤝‍🧑🏻 🎐 ⚙️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="大家好！ 

 我们在几周前发布了翻译的第二部分，以准备启动“数据科学家”课程的第二部分。 未来是另一种有趣的材料，也是一个公开的课程。 

 同时，我们进一步深入了模型的丛林。 

 神经翻译模型 

 虽然序列到序列模型的核心是由tensorflow/tensorflow/python/ops/...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>序列到序列第2部分模型</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/432302/">大家好！ <br><br> 我们在几周前发布了翻译的第二部分，以准备启动<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">“数据科学家”</a>课程的第二部分。 未来是另一种有趣的材料，也是一个公开的课程。 <br><br> 同时，我们进一步深入了模型的丛林。 <br><br>  <b>神经翻译模型</b> <br><br> 虽然序列到序列模型的核心是由<code>tensorflow/tensorflow/python/ops/seq2seq.py</code>的函数创建的，但在<code>models/tutorials/rnn/translate/seq2seq_model.py</code> ，我们的翻译模型中仍使用了一些技巧值得一提。 <br><br><img src="https://habrastorage.org/webt/3z/s9/fy/3zs9fym7zcpeqweylxlqhknznko.png"><a name="habracut"></a><br><br>  <b>采样的softmax和输出投影</b> <br><br> 如上所述，我们希望使用采样的softmax处理大型输出字典。 要从中解码，您必须跟踪输出的投影。 采样的softmax损耗和输出投影都是由<code>seq2seq_model.py</code>的以下代码生成的。 <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> num_samples &gt; <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> num_samples &lt; self.target_vocab_size: w_t = tf.get_variable(<span class="hljs-string"><span class="hljs-string">"proj_w"</span></span>, [self.target_vocab_size, size], dtype=dtype) w = tf.transpose(w_t) b = tf.get_variable(<span class="hljs-string"><span class="hljs-string">"proj_b"</span></span>, [self.target_vocab_size], dtype=dtype) output_projection = (w, b) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sampled_loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(labels, inputs)</span></span></span><span class="hljs-function">:</span></span> labels = tf.reshape(labels, [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>]) <span class="hljs-comment"><span class="hljs-comment"># We need to compute the sampled_softmax_loss using 32bit floats to # avoid numerical instabilities. local_w_t = tf.cast(w_t, tf.float32) local_b = tf.cast(b, tf.float32) local_inputs = tf.cast(inputs, tf.float32) return tf.cast( tf.nn.sampled_softmax_loss( weights=local_w_t, biases=local_b, labels=labels, inputs=local_inputs, num_sampled=num_samples, num_classes=self.target_vocab_size), dtype)</span></span></code> </pre><br> 首先，请注意，只有在样本数（默认为512）小于目标字典大小的情况下，我们才创建样本softmax。 对于小于512的字典，最好使用标准softmax损失。 <br><br> 然后，创建输出的投影。 这是一对由权重矩阵和位移向量组成的对。 当使用rnn单元格时，返回按<code>size</code>排列的训练样本数的形状矢量，而不是按<code>target_vocab_size</code>返回训练样本数的形状向量。 要恢复logit，您需要将其乘以权重矩阵并添加一个偏移量，这发生在<code>seq2seq_model.py</code>中的第124-126行中。 <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> output_projection <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> b <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(len(buckets)): self.outputs[b] = [tf.matmul(output, output_projection[<span class="hljs-number"><span class="hljs-number">0</span></span>]) + output_projection[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> ...]</code> </pre> <br>  <b>桶装和填充</b> <br><br> 除了采样的softmax之外，我们的翻译模型还使用了<i>bucketing（存储桶）功能</i> ，该方法可让您有效地管理不同长度的句子。 首先，说明问题。 从英语翻译成法语时，我们在入口处使用长度为L1的英语句子，在出口处使用长度为L1的法语句子。 由于英语句子是作为<code>encoder_inputs</code>传输的，而法语句子则是作为<code>decoder_inputs</code> （带有GO符号前缀）显示的，因此有必要为英语和法语句子的每对长度（L1，L2 + 1）创建一个seq2seq模型。 结果，我们得到了一个包含许多相似子图的巨大图。 另一方面，我们可以用特殊的PAD字符“填充”每个句子。 然后，对于“打包”长度，我们只需要一个seq2seq模型。 但是，这样的模型在短句子中将无效-您必须编码和解码许多无用的PAD字符。 <br><br> 在为每对长度创建一个图和填充为单个长度之间的折衷方案中，我们使用一定数量的存储桶，并将每个句子填充至上述组的长度。 默认情况下，在<code>translate.py</code>我们使用以下组。 <br><br><pre> <code class="python hljs">buckets = [(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>), (<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">15</span></span>), (<span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">25</span></span>), (<span class="hljs-number"><span class="hljs-number">40</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>)]</code> </pre> <br><br> 因此，如果一个带有3个标记的英语句子到达输入，而对应的法语句子在输出处包含6个标记，则它们将进入第一组，并在编码器的输入处填充为长度5，在解码器的输入处填充为长度10。 如果英语报价中有8个令牌，而对应的法语18中，则不会落入组（10、15）中，而是将其转移到组（20、25）中，即英语报价将增加到20个令牌，而法语报价将增加到25个令牌。 <br><br> 请记住，在创建解码器输入时，我们会在输入中添加特殊的<code>GO</code>字符。 这发生在<code>get_batch()</code>中的<code>get_batch()</code>函数中，该函数还会翻转英语句子。 输入的翻转有助于改善<a href="">Sutskever等人（2014）（pdf）</a>的神经翻译模型的结果<a href="">。</a> 为了弄清楚，想像一下有一个句子“ I go”。在输入处，将其分解为标记<code>["I", "go", "."]</code> ，在输出处则是一个句子“ Je vais。”，将其分解为标记<code>["Je", "vais", "."]</code> 。 它们将以输入编码器<code>[PAD PAD "." "go" "I"]</code>形式添加到组（5，10）中<code>[PAD PAD "." "go" "I"]</code>  <code>[PAD PAD "." "go" "I"]</code>和解码器输入<code>[GO "Je" "vais" "." EOS PAD PAD PAD PAD PAD]</code>  <code>[GO "Je" "vais" "." EOS PAD PAD PAD PAD PAD]</code> 。 <br><br>  <b>运行它</b> <br><br> 要训​​练上述模型，您将需要一个大型的英法军团。 为了进行培训，我们将使用来自<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">WMT'15网站</a>的10 ^ 9法语-英语军，并将来自同一站点的新闻作为工作样本进行测试。 当运行下一个命令时，两个数据集都将被加载到<code>train_dir</code> 。 <br><br><pre> <code class="plaintext hljs">python translate.py --data_dir [your_data_directory] --train_dir [checkpoints_directory] --en_vocab_size=40000 --fr_vocab_size=40000</code> </pre> <br> 您将需要18GB的硬盘空间和几个小时来准备培训大楼。 对该案例进行解压缩，在<code>data_dir,</code>中创建字典文件<code>data_dir,</code>然后对该案例进行标记并转换为整数标识符。 请注意负责字典大小的参数。 在上面的示例中，40,000个最常用词之外的所有词都将转换为代表未知词的UNK令牌。 因此，当更改字典的大小时，二进制文件将通过令牌ID重新更改外壳。 数据准备培训开始之后。 <br><br> 默认情况下， <code>translate</code>中指定的值非常高。 长时间学习的大型模型显示出良好的效果，但是可能会花费太多时间或GPU内存过多。 您可以指定一个较小的模型锻炼，如下例所示。 <br><br><pre> <code class="plaintext hljs">python translate.py --data_dir [your_data_directory] --train_dir [checkpoints_directory] --size=256 --num_layers=2 --steps_per_checkpoint=50</code> </pre> <br> 上面的命令将训练模型分为两层（默认为3层），每层有256个单位（默认为1024个），每50步带有一个检查点（默认为200个）。 尝试使用这些选项，以查看哪种尺寸模型适合您的GPU。 <br><br> 在训练过程中， <code>steps_per_checkpoin</code> t二进制文件的每个步骤都会提供有关过去步骤的统计信息。 使用默认参数（3层，大小为1024），第一条消息如下： <br><br><pre> <code class="plaintext hljs">global step 200 learning rate 0.5000 step-time 1.39 perplexity 1720.62 eval: bucket 0 perplexity 184.97 eval: bucket 1 perplexity 248.81 eval: bucket 2 perplexity 341.64 eval: bucket 3 perplexity 469.04 global step 400 learning rate 0.5000 step-time 1.38 perplexity 379.89 eval: bucket 0 perplexity 151.32 eval: bucket 1 perplexity 190.36 eval: bucket 2 perplexity 227.46 eval: bucket 3 perplexity 238.66</code> </pre> <br> 请注意，每个步骤花费的时间不到1.4秒，这使训练样本感到困惑，并使每个组的工作样本感到困惑。 经过大约3万步，我们看到了简短句子（第0组和第1组）的困惑如何变得清晰。 培训大楼包含约2200万个句子，一次迭代（一次运行的训练数据）大约需要34万步，训练样本的数量为64个。在此阶段，该模型可用于使用<code>--decode</code>选项将英语句子翻译为法语。 <br><br><pre> <code class="plaintext hljs">python translate.py --decode --data_dir [your_data_directory] --train_dir [checkpoints_directory] Reading model parameters from /tmp/translate.ckpt-340000 &gt; Who is the president of the United States? Qui est le président des États-Unis ?</code> </pre> <br>  <b>接下来是什么？</b> <br><br> 上面的示例显示了如何端到端创建自己的英语-法语翻译器。 运行它，看看模型如何工作。 质量是可以接受的，但是无法使用默认参数获得理想的翻译模型。 这是您可以改进的几件事。 <br><br> 首先，我们使用原始标记化，即<code>basic_tokenizer</code>中<code>data_utils</code>的基本功能。  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">在WMT'15网站</a>上可以找到更好的令牌生成器。 如果使用它和大型词典，则可以实现更好的翻译。 <br><br> 此外，转换模型的默认参数配置不完善。 您可以尝试更改学习速度，衰减，模型权重的初始化。 您还可以使用更高级的功能（例如<code>AdagradOptimizer</code>替换<code>seq2seq_model.py</code>的标准<code>GradientDescentOptimizer</code> 。 尝试观察更好的结果！ <br><br> 最后，以上介绍的模型不仅可以用于翻译，还可以用于任何其他序列到序列的任务。 例如，即使您要将序列变成树，生成分析树，该模型也可以产生最新的结果，如<a href="">Vinyals＆Kaiser等人，2014（pdf）所示</a> 。 因此，您不仅可以创建翻译器，还可以创建解析器，聊天机器人或您想要的任何其他程序。 实验！ <br><br> 仅此而已！ <br><br> 我们在这里等待您的评论和问题，或者我们邀请您在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">公开课中</a>向他们的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">老师</a>提问。 </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN432302/">https://habr.com/ru/post/zh-CN432302/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN432292/index.html">可变和参数字体-设计师双赢</a></li>
<li><a href="../zh-CN432294/index.html">Ansible Tower：工作流作业模板</a></li>
<li><a href="../zh-CN432296/index.html">即使您退出帐户，Google也会使您陷入个人“搜索泡沫”</a></li>
<li><a href="../zh-CN432298/index.html">Timeweb在.RU区域进入了TOP-10域注册商。</a></li>
<li><a href="../zh-CN432300/index.html">支持，服务，头痛和所有一切</a></li>
<li><a href="../zh-CN432304/index.html">一位杰出的神经科学家，可能对创造真正的人工智能有关键作用</a></li>
<li><a href="../zh-CN432306/index.html">存储中的存储类内存-如果您需要更快的话</a></li>
<li><a href="../zh-CN432308/index.html">UE4模块化科幻级别：受Nostromo和Serenity启发</a></li>
<li><a href="../zh-CN432310/index.html">Ktor作为Android的HTTP客户端</a></li>
<li><a href="../zh-CN432312/index.html">在Power BI中创建RF映射的形状映射</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>