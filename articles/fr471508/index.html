<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèæ‚ÄçüöÄ üé≥ üßìüèº Administrateur sans bras = hyperconvergence? ‚õìÔ∏è üà∫ ‚§¥Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Il s'agit d'un mythe assez courant dans le domaine du mat√©riel serveur. Dans la pratique, les solutions hyperconverg√©es (quand elles sont toutes r√©uni...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Administrateur sans bras = hyperconvergence?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/croc/blog/471508/"><img src="https://habrastorage.org/webt/_a/wu/gy/_awugy9unrbypap90hlhc86-maa.png"><br><img src="https://habrastorage.org/webt/_k/kx/sa/_kkxsa4xs7rgz-fzktgy10mdeuc.png"><br><br>  Il s'agit d'un mythe assez courant dans le domaine du mat√©riel serveur.  Dans la pratique, les solutions hyperconverg√©es (quand elles sont toutes r√©unies) ont besoin de beaucoup pour quoi.  Historiquement, les premi√®res architectures ont √©t√© d√©velopp√©es par Amazon et Google pour leurs services.  L'id√©e √©tait alors de faire une batterie de calcul des m√™mes n≈ìuds, chacun ayant ses propres disques.  Tout cela a √©t√© combin√© par un logiciel de formation de syst√®me (hyperviseur) et √©tait d√©j√† divis√© en machines virtuelles.  La t√¢che principale est un minimum d'effort pour maintenir un n≈ìud et un minimum de probl√®mes de mise √† l'√©chelle: nous venons d'acheter encore mille ou deux des m√™mes serveurs et de nous connecter √† proximit√©.  En pratique, ce sont des cas isol√©s, et bien plus souvent nous parlons d'un plus petit nombre de n≈ìuds et d'une architecture l√©g√®rement diff√©rente. <br><br>  Mais le plus reste le m√™me: l'incroyable facilit√© de mise √† l'√©chelle et de contr√¥le.  Moins - diff√©rentes t√¢ches consomment les ressources diff√©remment, et quelque part il y aura beaucoup de disques locaux, quelque part il y aura peu de RAM, et ainsi de suite, c'est-√†-dire qu'avec diff√©rents types de t√¢ches, l'utilisation des ressources diminuera. <br><br>  Il s'est av√©r√© que vous payez 10 √† 15% de plus pour la facilit√© d'installation.  Cela a provoqu√© le mythe titre.  Nous avons longtemps cherch√© o√π la technologie sera appliqu√©e de mani√®re optimale et l'avons trouv√©e.  Le fait est que Tsiska n'avait pas ses propres syst√®mes de stockage, mais ils voulaient un march√© de serveurs complet.  Et ils ont fait Cisco Hyperflex, une solution de stockage local sur les n≈ìuds. <br><br>  Et cela s'est av√©r√© soudain √™tre une tr√®s bonne solution pour les centres de donn√©es de sauvegarde (Disaster Recovery).  Pourquoi et comment - maintenant je vais le dire.  Et je vais montrer des tests de cluster. <a name="habracut"></a><br><br><h3>  O√π aller </h3><br>  L'hyper convergence est: <br><br><ol><li>  Transf√©rez des disques vers des n≈ìuds de calcul. </li><li>  Int√©gration compl√®te du sous-syst√®me de stockage avec le sous-syst√®me de virtualisation. </li><li>  Transfert / int√©gration avec le sous-syst√®me r√©seau. </li></ol><br>  Une telle combinaison vous permet de mettre en ≈ìuvre de nombreuses fonctionnalit√©s des syst√®mes de stockage au niveau de la virtualisation et le tout √† partir d'une seule fen√™tre de contr√¥le. <br><br>  Dans notre entreprise, les projets de conception de centres de donn√©es de sauvegarde sont tr√®s demand√©s, et c'est souvent la solution hyperconverg√©e qui est choisie en raison de la multitude d'options de r√©plication (jusqu'au cluster de m√©tro) pr√™tes √† l'emploi. <br><br>  Dans le cas des datacenters de sauvegarde, il s'agit g√©n√©ralement d'une installation distante sur un site de l'autre c√¥t√© de la ville ou dans une autre ville en g√©n√©ral.  Il vous permet de restaurer des syst√®mes critiques en cas de panne partielle ou totale du datacenter principal.  Les donn√©es de vente y sont constamment r√©pliqu√©es, et cette r√©plication peut se faire au niveau de l'application ou au niveau du p√©riph√©rique de bloc (SHD). <br><br>  Alors maintenant, je vais parler de l'appareil et des tests du syst√®me, puis de quelques sc√©narios r√©els avec des donn√©es sur les √©conomies. <br><br><h3>  Les tests </h3><br>  Notre copie se compose de quatre serveurs, chacun ayant 10 disques SSD par 960 Go.  Il existe un disque d√©di√© pour la mise en cache des op√©rations d'√©criture et le stockage de la machine virtuelle de service.  La solution elle-m√™me est la quatri√®me version.  Le premier est franchement brut (√† en juger par les critiques), le second est humide, le troisi√®me est d√©j√† assez stable, et celui-ci peut √™tre appel√© une version apr√®s la fin des tests b√™ta pour le grand public.  Lors du test des probl√®mes que je n'ai pas vus, tout fonctionne comme une horloge. <br><br><div class="spoiler">  <b class="spoiler_title">Changements dans la v4</b> <div class="spoiler_text">  Correction d'un tas de bugs. <br><br>  Initialement, la plate-forme ne pouvait fonctionner qu'avec l'hyperviseur VMware ESXi et supportait un petit nombre de n≈ìuds.  De plus, le processus de d√©ploiement ne s'est pas toujours termin√© avec succ√®s, j'ai d√ª red√©marrer certaines √©tapes, il y avait des probl√®mes de mise √† jour √† partir des anciennes versions, les donn√©es dans l'interface graphique n'√©taient pas toujours affich√©es correctement (m√™me si je ne suis toujours pas satisfait de l'affichage des graphiques de performances), parfois il y avait des probl√®mes √† l'interface avec la virtualisation . <br><br>  Maintenant, toutes les plaies des enfants ont √©t√© corrig√©es, HyperFlex peut faire √† la fois ESXi et Hyper-V, plus c'est possible: <br><br><ol><li>  Cr√©ation d'un cluster √©tir√©. </li><li>  Cr√©ation d'un cluster pour les bureaux sans utiliser Fabric Interconnect, de deux √† quatre n≈ìuds (nous n'achetons que des serveurs). </li><li>  Capacit√© √† travailler avec un stockage externe. </li><li>  Prise en charge des conteneurs et Kubernetes. </li><li>  Cr√©ation de zones d'accessibilit√©. </li><li>  Int√©gration avec VMware SRM, si la fonctionnalit√© int√©gr√©e ne convient pas. </li></ol><br></div></div><br>  L'architecture n'est pas tr√®s diff√©rente des d√©cisions des principaux concurrents, ils n'ont pas commenc√© √† cr√©er un v√©lo.  Tout fonctionne sur la plate-forme de virtualisation VMware ou Hyper-V.  Mat√©riel h√©berg√© sur des serveurs propri√©taires Cisco UCS.  Il y a ceux qui d√©testent la plate-forme pour la complexit√© relative de la configuration initiale, de nombreux boutons, un syst√®me non trivial de mod√®les et de d√©pendances, mais il y a aussi ceux qui ont appris le Zen, ont √©t√© inspir√©s par l'id√©e et ne veulent plus travailler avec d'autres serveurs. <br><br>  Nous consid√©rerons la solution sp√©cifiquement pour VMware, car la solution a √©t√© cr√©√©e √† l'origine pour elle et a plus de fonctionnalit√©s, Hyper-V a √©t√© ajout√© en cours de route pour rester en phase avec les concurrents et r√©pondre aux attentes du march√©. <br><br>  Il y a un cluster de serveurs plein de disques.  Il existe des disques pour le stockage des donn√©es (SSD ou HDD - √† votre go√ªt et vos besoins), il y a un disque SSD pour la mise en cache.  Lorsque des donn√©es sont √©crites dans le magasin de donn√©es, les donn√©es sont enregistr√©es sur la couche de mise en cache (disque SSD d√©di√© et RAM de la VM de service).  En parall√®le, le bloc de donn√©es est envoy√© aux n≈ìuds du cluster (le nombre de n≈ìuds d√©pend du facteur de r√©plication du cluster).  Apr√®s confirmation de la r√©ussite de l'enregistrement par tous les n≈ìuds, la confirmation de l'enregistrement est envoy√©e √† l'hyperviseur puis √† la machine virtuelle.  Les donn√©es enregistr√©es en arri√®re-plan sont d√©dupliqu√©es, compress√©es et √©crites sur des disques de stockage.  Dans le m√™me temps, un gros bloc est toujours √©crit sur les disques de stockage et s√©quentiellement, ce qui r√©duit la charge sur les disques de stockage. <br><br>  La d√©duplication et la compression sont toujours activ√©es et ne peuvent pas √™tre d√©sactiv√©es.  Les donn√©es sont lues directement √† partir des disques de stockage ou du cache RAM.  Si une configuration hybride est utilis√©e, la lecture est √©galement mise en cache sur le SSD. <br><br>  Les donn√©es ne sont pas li√©es √† l'emplacement actuel de la machine virtuelle et sont r√©parties uniform√©ment entre les n≈ìuds.  Cette approche vous permet de charger √©galement tous les lecteurs et interfaces r√©seau.  L'inconv√©nient √©vident: nous ne pouvons pas minimiser le d√©lai de lecture, car il n'y a aucune garantie de disponibilit√© des donn√©es localement.  Mais je crois que c'est un sacrifice insignifiant par rapport aux avantages re√ßus.  De plus, les retards du r√©seau ont atteint des valeurs telles qu'ils n'affectent pratiquement pas le r√©sultat global. <br><br>  Pour toute la logique du sous-syst√®me de disque, une machine virtuelle de service sp√©ciale du contr√¥leur Cisco HyperFlex Data Platform est responsable, qui est cr√©√©e sur chaque n≈ìud de stockage.  Dans notre configuration de VM de service, huit vCPU et 72 Go de RAM ont √©t√© allou√©s, ce qui n'est pas si petit.  Permettez-moi de vous rappeler que l'h√¥te lui-m√™me poss√®de 28 c≈ìurs physiques et 512 Go de RAM. <br><br>  La machine virtuelle de service a directement acc√®s aux disques physiques en transmettant le contr√¥leur SAS √† la machine virtuelle.  La communication avec l'hyperviseur se fait via un module IOVisor sp√©cial, qui intercepte les op√©rations d'E / S et utilise un agent qui vous permet de transf√©rer des commandes vers l'API de l'hyperviseur.  L'agent est charg√© de travailler avec les instantan√©s et les clones HyperFlex. <br><br>  Dans l'hyperviseur, les ressources disque sont mont√©es comme une boule NFS ou SMB (selon le type d'hyperviseur, devinez lequel).  Et sous le capot, il s'agit d'un syst√®me de fichiers distribu√© qui vous permet d'ajouter des fonctionnalit√©s des syst√®mes de stockage √† part enti√®re pour adultes: allocation de volume l√©ger, compression et d√©duplication, instantan√©s utilisant la technologie de redirection sur √©criture, r√©plication synchrone / asynchrone. <br><br>  Service VM donne acc√®s √† l'interface WEB de la gestion du sous-syst√®me HyperFlex.  Il existe une int√©gration avec vCenter, et la plupart des t√¢ches quotidiennes peuvent √™tre effectu√©es √† partir de celui-ci, mais les banques de donn√©es, par exemple, sont plus pratiques √† couper √† partir d'une webcam distincte si vous √™tes d√©j√† pass√© √† une interface HTML5 rapide, ou utilisez un client Flash complet avec une int√©gration compl√®te.  Dans la webcam de service, vous pouvez voir les performances et l'√©tat d√©taill√© du syst√®me. <br><br><img src="https://habrastorage.org/webt/o0/bj/od/o0bjod1zrf25ubnrtis5q4e1ywc.png"><br><br>  Il existe un autre type de n≈ìud dans un cluster: les n≈ìuds de calcul.  Il peut s'agir de serveurs en rack ou en lame sans disques int√©gr√©s.  Sur ces serveurs, vous pouvez ex√©cuter des machines virtuelles dont les donn√©es sont stock√©es sur des serveurs avec des disques.  Du point de vue de l'acc√®s aux donn√©es, il n'y a pas de diff√©rence entre les types de n≈ìuds, car l'architecture implique l'abstraction de l'emplacement physique des donn√©es.  Le rapport maximal entre les n≈ìuds de calcul et les n≈ìuds de stockage est de 2: 1. <br><br>  L'utilisation de n≈ìuds de calcul augmente la flexibilit√© lors de la mise √† l'√©chelle des ressources de cluster: nous n'avons pas besoin d'acheter des n≈ìuds avec des disques si nous n'avons besoin que de CPU / RAM.  De plus, nous pouvons ajouter un panier de lames et √©conomiser de l'espace sur le serveur rack. <br><br>  En cons√©quence, nous avons une plateforme hyperconverg√©e avec les fonctionnalit√©s suivantes: <br><br><ul><li>  Jusqu'√† 64 n≈ìuds dans un cluster (jusqu'√† 32 n≈ìuds de stockage). </li><li>  Le nombre minimum de n≈ìuds dans un cluster est de trois (deux pour un cluster Edge). </li><li>  M√©canisme de redondance des donn√©es: mise en miroir avec les facteurs de r√©plication 2 et 3. </li><li>  Cluster Metro. </li><li>  R√©plication de machine virtuelle asynchrone vers un autre cluster HyperFlex. </li><li>  Orchestration de la commutation des machines virtuelles vers un centre de donn√©es distant. </li><li>  Instantan√©s natifs utilisant la technologie de redirection sur √©criture. </li><li>  Jusqu'√† 1 PB d'espace utilisable avec facteur de r√©plication 3 et sans d√©duplication.  Nous ne prenons pas en compte le facteur de r√©plication 2, car ce n'est pas une option pour une vente s√©rieuse. </li></ul><br>  Un autre √©norme avantage est la facilit√© de gestion et de d√©ploiement.  Toutes les complexit√©s de la configuration des serveurs UCS sont g√©r√©es par une machine virtuelle sp√©cialis√©e pr√©par√©e par les ing√©nieurs Cisco. <br><br><h3>  Configuration du banc d'essai: </h3><br><ul><li>  2 x Cisco UCS Fabric Interconnect 6248UP en tant que cluster de gestion et composants r√©seau (48 ports fonctionnant en mode Ethernet 10G / FC 16G). </li><li>  Quatre serveurs Cisco UCS HXAF240 M4. </li></ul><br>  Caract√©ristiques du serveur: <br><p></p><div class="scrollable-table"><table><tbody><tr><td><br><p>  CPU </p><br></td><td><br><p>  2 x Intel ¬Æ Xeon ¬Æ E5-2690 v4 </p><br></td></tr><tr><td><br><p>  RAM </p><br></td><td><br><p>  16 x 32 Go DDR4-2400 MHz RDIMM / PC4-19200 / double rang / x4 / 1,2 v </p><br></td></tr><tr><td><br><p>  R√©seau </p><br></td><td><br><p>  UCSC-MLOM-CSC-02 (VIC 1227).  2 x Ethernet 10G </p><br></td></tr><tr><td><br><p>  Stockage HBA </p><br></td><td><br><p>  Cisco 12G Modular SAS Pass through Controller </p><br></td></tr><tr><td><br><p>  Disques de stockage </p><br></td><td><br><p>  1 x SSD Intel S3520 120 Go, 1 x SSD Samsung MZ-IES800D, 10 x SSD Samsung PM863a 960 Go </p><br></td></tr></tbody></table></div><br><br><div class="spoiler">  <b class="spoiler_title">Plus d'options de configuration</b> <div class="spoiler_text">  En plus du fer s√©lectionn√©, les options suivantes sont actuellement disponibles: <br><br><ul><li>  HXAF240c M5. </li><li>  Un ou deux processeurs allant d'Intel Silver 4110 √† Intel Platinum I8260Y.  La deuxi√®me g√©n√©ration est disponible. </li><li>  24 emplacements m√©moire, lattes de 16 Go RDIMM 2600 √† 128 Go LRDIMM 2933. </li><li>  De 6 √† 23 disques pour les donn√©es, un disque de mise en cache, un syst√®me et un disque de d√©marrage. </li></ul><br>  <b>Lecteurs de capacit√©</b> <br><br><ul><li>  HX-SD960G61X-EV 960 Go 2,5 pouces SSD Enterprise Value 6G SATA (1X endurance) SAS 960 Go. </li><li>  HX-SD38T61X-EV 3,8 To 2,5 pouces Enterprise Value 6G SATA SSD (1X endurance) SAS 3.8 TB. </li><li>  Mise en cache des pilotes </li><li>  HX-NVMEXPB-I375 Disque Intel Optane 2,5 pouces 375 Go 2,5 pouces, performances et endurance extr√™mes. </li><li>  HX-NVMEHW-H1600 * 1,6 To 2,5 pouces Ent.  Perf  NVMe SSD (3X endurance) NVMe 1,6 To. </li><li>  HX-SD400G12TX-EP 400 Go 2,5 pouces Ent.  Perf  SSD SAS 12G (endurance 10X) SAS 400 Go. </li><li>  HX-SD800GBENK9 ** 800 Go 2,5 pouces Ent.  Perf  SSD SAS SED 12G (endurance 10X) SAS 800 Go. </li><li>  HX-SD16T123X-EP 1,6 To 2,5 pouces SSD Enterprise Performance 12G SAS (3X endurance). </li></ul><br>  <b>Lecteurs syst√®me / journaux</b> <br><br><ul><li>  SSD HX-SD240GM1X-EV 240 Go 2,5 pouces Enterprise Value 6G SATA (N√©cessite une mise √† niveau). </li></ul><br>  <b>Pilotes de d√©marrage</b> <br><br><ul><li>  HX-M2-240 Go 240 Go SATA M.2 SSD SATA 240 Go. </li></ul><br></div></div><br>  Connexion √† un r√©seau sur des ports Ethernet 40G, 25G ou 10G. <br><br>  Comme FI peut √™tre HX-FI-6332 (40G), HX-FI-6332-16UP (40G), HX-FI-6454 (40G / 100G). <br><br><h3>  Se tester </h3><br>  Pour tester le sous-syst√®me de disque, j'ai utilis√© HCIBench 2.2.1.  Il s'agit d'un utilitaire gratuit qui vous permet d'automatiser la cr√©ation de charge √† partir de plusieurs machines virtuelles.  La charge elle-m√™me est g√©n√©r√©e par fio ordinaire. <br><br>  Notre cluster se compose de quatre n≈ìuds, facteur de r√©plication 3, tous les lecteurs Flash. <br><br>  Pour les tests, j'ai cr√©√© quatre banques de donn√©es et huit machines virtuelles.  Pour les tests d'√©criture, il est suppos√© que le disque de mise en cache n'est pas plein. <br><br>  Les r√©sultats des tests sont les suivants: <br><div class="scrollable-table"><table><tbody><tr><td></td><td colspan="5"><br><p>  100% lu 100% al√©atoire </p><br></td><td colspan="5"><br><p>  0% lu 100% al√©atoire </p><br></td></tr><tr><td><br><p>  Profondeur de bloc / file d'attente </p><br></td><td><br><p>  128 </p><br></td><td><br><p>  256 </p><br></td><td><br><p>  512 </p><br></td><td><br><p>  1024 </p><br></td><td><br><p>  2048 </p><br></td><td><br><p>  128 </p><br></td><td><br><p>  256 </p><br></td><td><br><p>  512 </p><br></td><td><br><p>  1024 </p><br></td><td><br><p>  2048 </p><br></td></tr><tr><td><br><p>  4K </p><br></td><td><br><p>  0,59 ms 213804 IOPS </p><br></td><td><br><p>  0,84 ms 303540 IOPS </p><br></td><td><br><p>  1.36ms 374348 IOPS </p><br></td><td><br><p>  2,47 ms 414116 IOPS </p><br></td><td><br><p>  <b>4.86ms 420180 IOPS</b> </p><br></td><td><br><p>  2,22 ms 57408 IOPS </p><br></td><td><br><p>  3,09 ms 82744 IOPS </p><br></td><td><br><p>  5,02 ms 101824 IPOS </p><br></td><td><br><p>  8,75 ms 116912 IOPS </p><br></td><td><br><p>  <b>17,2 ms 118592 IOPS</b> </p><br></td></tr><tr><td><br><p>  8K </p><br></td><td><br><p>  0,67 ms 188416 IOPS </p><br></td><td><br><p>  0,93 ms 273280 IOPS </p><br></td><td><br><p>  1,7 ms 299932 IOPS </p><br></td><td><br><p>  2,72 ms 376,484 IOPS </p><br></td><td><br><p>  <b>5,47 ms 373,176 IOPS</b> </p><br></td><td><br><p>  3,1 ms 41148 IOPS </p><br></td><td><br><p>  4,7 ms 54396 IOPS </p><br></td><td><br><p>  7,09 ms 72192 IOPS </p><br></td><td><br><p>  <b>12,77 ms 80,132 IOPS</b> </p><br></td><td></td></tr><tr><td><br><p>  16K </p><br></td><td><br><p>  0,77 ms 164116 IOPS </p><br></td><td><br><p>  1,12 ms 228328 IOPS </p><br></td><td><br><p>  1,9 ms 268140 IOPS </p><br></td><td><br><p>  <b>3,96 ms 258480 IOPS</b> </p><br></td><td></td><td><br><p>  3,8 ms 33640 IOPS </p><br></td><td><br><p>  6,97 ms 36696 IOPS </p><br></td><td><br><p>  <b>11,35 ms 45060 IOPS</b> </p><br></td><td></td><td></td></tr><tr><td><br><p>  32K </p><br></td><td><br><p>  1,07 ms 119292 IOPS </p><br></td><td><br><p>  1,79 ms 142888 IOPS </p><br></td><td><br><p>  <b>3,56 ms 143760 IOPS</b> </p><br></td><td></td><td></td><td><br><p>  7,17 ms 17810 IOPS </p><br></td><td><br><p>  <b>11,96 ms 21396 IOPS</b> </p><br></td><td></td><td></td><td></td></tr><tr><td><br><p>  64K </p><br></td><td><br><p>  1,84 ms 69440 IOPS </p><br></td><td><br><p>  3,6 ms 71008 IOPS </p><br></td><td><br><p>  <b>7,26 ms 70404 IOPS</b> </p><br></td><td></td><td></td><td><br><p>  <b>11,37 ms 11248 IOPS</b> </p><br></td><td></td><td></td><td></td><td></td></tr></tbody></table></div><br>  <i>Des valeurs en gras sont indiqu√©es, apr√®s quoi il n'y a pas d'augmentation de la productivit√©, parfois m√™me une d√©gradation est visible.</i>  <i>En raison du fait que nous nous reposons sur les performances du r√©seau / contr√¥leurs / lecteurs.</i> <br><br><ul><li>  Lecture s√©quentielle 4432 Mo / s. </li><li>  √âcriture s√©quentielle 804 Mo / s. </li><li>  Si un contr√¥leur tombe en panne (√©chec de la machine virtuelle ou de l'h√¥te), le rabattement des performances est doubl√©. </li><li>  Si le lecteur de stockage tombe en panne, le rabattement est de 1/3.  Le disque de reconstruction n√©cessite 5% des ressources de chaque contr√¥leur. </li></ul><br>  Sur un petit bloc, nous rencontrons les performances du contr√¥leur (machine virtuelle), son CPU est charg√© √† 100%, tout en augmentant le bloc que nous ex√©cutons sur la bande passante du port.  10 Gbps ne suffisent pas pour lib√©rer le potentiel du syst√®me AllFlash.  Malheureusement, les param√®tres du support de d√©monstration fourni ne permettent pas de v√©rifier le travail √† 40 Gb / s. <br><br>  Dans mon impression des tests et de l'√©tude de l'architecture, en raison de l'algorithme qui place les donn√©es entre tous les h√¥tes, nous obtenons des performances pr√©visibles √©volutives, mais c'est aussi une limitation lors de la lecture, car il serait possible de presser davantage sur les disques locaux et plus, ici pour √©conomiser un r√©seau plus productif, par exemple, des IF √† 40 Gbit / s sont disponibles. <br><br>  En outre, un disque pour la mise en cache et la d√©duplication peut √™tre une limitation; en fait, dans ce stand, nous pouvons √©crire sur quatre disques SSD.  Ce serait formidable de pouvoir augmenter le nombre de disques mis en cache et voir la diff√©rence. <br><br><h3>  Utilisation r√©elle </h3><br>  Deux approches peuvent √™tre utilis√©es pour organiser un centre de donn√©es de sauvegarde (nous n'envisageons pas de placer la sauvegarde sur un site distant): <br><br><ol><li>  Passif actif  Toutes les applications sont h√©berg√©es dans le centre de donn√©es principal.  La r√©plication est synchrone ou asynchrone.  En cas de chute du centre de donn√©es principal, nous devons activer celui de sauvegarde.  Cela peut √™tre fait manuellement / applications de scripts / orchestration.  Ici, nous obtenons un RPO proportionnel √† la fr√©quence de r√©plication, et le RTO d√©pend de la r√©action et des comp√©tences de l'administrateur et de la qualit√© du d√©veloppement / d√©bogage du plan de commutation. </li><li>  Actif Actif  Dans ce cas, seule la r√©plication synchrone est pr√©sente, la disponibilit√© des centres de donn√©es est d√©termin√©e par un quorum / arbitre, plac√© strictement sur la troisi√®me plateforme.  RPO = 0 et RTO peut atteindre 0 (si l'application le permet) ou √©gal au temps de basculement sur un n≈ìud dans un cluster de virtualisation.  Au niveau de la virtualisation, un cluster √©tendu (Metro) est cr√©√© qui n√©cessite un stockage actif-actif. </li></ol><br>  Habituellement, nous voyons avec les clients une architecture d√©j√† mise en ≈ìuvre avec un stockage classique dans le centre de donn√©es principal, nous en concevons donc une autre pour la r√©plication.  Comme je l'ai mentionn√©, Cisco HyperFlex offre une r√©plication asynchrone et la cr√©ation d'un cluster de virtualisation √©tendu.  Dans le m√™me temps, nous n'avons pas besoin d'un syst√®me de stockage de milieu de gamme ou sup√©rieur d√©di√© avec les fonctions co√ªteuses de r√©plication et d'acc√®s aux donn√©es Active-Active sur deux syst√®mes de stockage. <br><br>  <b>Sc√©nario 1:</b> Nous avons des centres de donn√©es principaux et de sauvegarde, une plate-forme de virtualisation sur VMware vSphere.  Tous les syst√®mes productifs sont situ√©s principalement dans le centre de donn√©es et la r√©plication des machines virtuelles est effectu√©e au niveau de l'hyperviseur, ce qui permettra de ne pas garder les machines virtuelles sous tension dans le centre de donn√©es de sauvegarde.  Nous r√©pliquons les bases de donn√©es et les applications sp√©ciales avec des outils int√©gr√©s et gardons les VM sous tension.  Si le centre de donn√©es principal tombe en panne, nous d√©marrons le syst√®me dans le centre de donn√©es de sauvegarde.  Nous pensons que nous avons environ 100 machines virtuelles.  Tant que le centre de donn√©es principal fonctionne, des environnements de test et d'autres syst√®mes peuvent √™tre lanc√©s dans le centre de donn√©es de sauvegarde, qui peut √™tre d√©sactiv√© si le centre de donn√©es principal est commut√©.  Il est √©galement possible que nous utilisions la r√©plication bidirectionnelle.  Du point de vue des √©quipements, rien ne changera. <br><br>  Dans le cas d'une architecture classique, nous mettrons un syst√®me de stockage hybride dans chaque centre de donn√©es avec acc√®s via FibreChannel, d√©chirement, d√©duplication et compression (mais pas en ligne), 8 serveurs par site, 2 commutateurs FibreChannel et Ethernet 10G.  Pour la r√©plication et le contr√¥le de commutation dans une architecture classique, nous pouvons utiliser des outils VMware (R√©plication + SRM) ou des outils tiers qui seront l√©g√®rement moins chers et parfois plus pratiques. <br><br>  La figure montre un diagramme. <br><br><img src="https://habrastorage.org/webt/ej/an/92/ejan92jvmtt1ejtbtd1eyytngw8.png"><br><br>  Si vous utilisez Cisco HyperFlex, vous obtenez l'architecture suivante: <br><br><img src="https://habrastorage.org/webt/9w/xs/hl/9wxshlz45c53uyitqulmrmwlraq.png"><br><br>  Pour HyperFlex, j'ai utilis√© des serveurs avec de grandes ressources CPU / RAM, comme  une partie des ressources ira √† la machine virtuelle du contr√¥leur HyperFlex, j'ai m√™me recharg√© un peu la configuration HyperFlex sur le processeur et la m√©moire afin de ne pas jouer avec Cisco et garantir des ressources pour le reste des machines virtuelles.  Mais nous pouvons refuser des commutateurs FibreChannel, et nous n'avons pas besoin de ports Ethernet pour chaque serveur, le trafic local est commut√© √† l'int√©rieur de FI. <br><br>  Le r√©sultat est la configuration suivante pour chaque centre de donn√©es: <br><div class="scrollable-table"><table><tbody><tr><td><br><p>  Serveurs </p><br></td><td><br><p>  8 serveurs 1U (384 Go de RAM, 2 x Intel Gold 6132, FC HBA) </p><br></td><td><br><p>  8 x HX240C-M5L (512 Go de RAM, 2 x Intel Gold 6150, SSD de 3,2 Go, 10 x 6 To NL-SAS) </p><br></td></tr><tr><td><br><p>  SHD </p><br></td><td><br><p>  Stockage hybride avec frontal FC (SSD 20 To, 130 To NL-SAS) </p><br></td><td><br><p>  - </p><br></td></tr><tr><td><br><p>  LAN </p><br></td><td><br><p>  2 x commutateurs Ethernet 10G 12 ports </p><br></td><td><br><p>  - </p><br></td></tr><tr><td><br><p>  San </p><br></td><td><br><p>  2 x commutateur FC 32 / 16Gb 24 ports </p><br></td><td><br><p>  2 x Cisco UCS FI 6332 </p><br></td></tr><tr><td><br><p>  Licences </p><br></td><td><br><p>  VMware Ent Plus </p><br><p>  R√©plication et / ou orchestration de VM </p><br></td><td><br><p>  VMware Ent Plus </p><br></td></tr></tbody></table></div><br>  Pour Hyperflex, je n'ai pas promis de licences de logiciel de r√©plication, car cela est disponible d√®s le d√©part avec nous. <br><br>  Pour l'architecture classique, j'ai pris un vendeur qui s'est impos√© comme un fabricant de qualit√© et peu co√ªteux.  Pour les deux options, j'ai utilis√© un standard pour un skid de solution sp√©cifique, √† la sortie, j'ai obtenu des prix r√©els. <br><br>  La solution sur Cisco HyperFlex √©tait 13% moins ch√®re. <br><br>  <b>Sc√©nario 2:</b> cr√©ation de deux centres de donn√©es actifs.  Dans ce sc√©nario, nous concevons un cluster √©tendu sur VMware. <br><br>  L'architecture classique se compose de serveurs de virtualisation, d'un SAN (protocole FC) et de deux syst√®mes de stockage qui peuvent lire et √©crire sur celui qui les s√©pare.  Sur chaque SHD, nous fixons une capacit√© utile pour la serrure. <br><br><img src="https://habrastorage.org/webt/lm/1i/yu/lm1iyuqivylf7dl0xrmff21swlw.png"><br><br>  Chez HyperFlex, nous cr√©ons simplement un Stretch Cluster avec le m√™me nombre de n≈ìuds sur les deux sites.  Dans ce cas, le facteur de r√©plication 2 + 2 est utilis√©. <br><br><img src="https://habrastorage.org/webt/e7/mq/sd/e7mqsd7codjtfbqefl40icsohpu.png"><br><br>  La configuration suivante s'est av√©r√©e: <br><div class="scrollable-table"><table><tbody><tr><td></td><td><br><p>  Architecture classique </p><br></td><td><br><p>  Hyperflex </p><br></td></tr><tr><td><br><p>  Serveurs </p><br></td><td><br><p>  Serveur 16 x 1U (384 Go de RAM, 2 x Intel Gold 6132, FC HBA, 2 x 10G NIC) </p><br></td><td><br><p>  16 x HX240C-M5L (512 Go de RAM, 2 x Intel Gold 6132, 1,6 To NVMe, 12 x 3,8 To SSD, VIC 1387) </p><br></td></tr><tr><td><br><p>  SHD </p><br></td><td><br><p>  2 x AllFlash Storage (SSD 150 To) </p><br></td><td><br><p>  - </p><br></td></tr><tr><td><br><p>  LAN </p><br></td><td><br><p>  4 x commutateurs Ethernet 10G 24 ports </p><br></td><td><br><p>  - </p><br></td></tr><tr><td><br><p>  San </p><br></td><td><br><p>  4 x commutateur FC 32 / 16Gb 24 ports </p><br></td><td><br><p>  4 x Cisco UCS FI 6332 </p><br></td></tr><tr><td><br><p>  Licences </p><br></td><td><br><p>  VMware Ent Plus </p><br></td><td><br><p>  VMware Ent Plus </p><br></td></tr></tbody></table></div><br>  Dans tous les calculs, je n'ai pas pris en compte l'infrastructure r√©seau, les co√ªts du datacenter, etc.: ils seront les m√™mes pour l'architecture classique et pour la solution HyperFlex. <br><br>  Au prix co√ªtant, HyperFlex s'est av√©r√© 5% plus cher.  Il convient de noter ici que pour les ressources CPU / RAM, j'ai eu un biais pour Cisco, car dans la configuration, il remplissait les canaux des contr√¥leurs de m√©moire de mani√®re √©gale.   ,    ,   ,     ¬´  ¬ª,         .      ,      Cisco UCS     . <br><br>        SAN  , -  ,      (, ,   ‚Äî ),   (    ),  . <br><br>   ,         ‚Äî Cisco.        Cisco UCS,    ,  HyperFlex     ,    .          ,     .       : ¬´    ,  ?¬ª  ¬´  - ,     . !¬ª ‚Äî           , : ¬´    ¬ª   . <br><br><h3>  Les r√©f√©rences </h3><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> -</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">-   </a> </li><li>   ‚Äî StGeneralov@croc.ru </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr471508/">https://habr.com/ru/post/fr471508/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr471498/index.html">La carte des cam√©ras de fixation des routes est rendue publique: r√©jouissez-vous ou pleurez-vous?</a></li>
<li><a href="../fr471500/index.html">Rappel ou ¬´Fid√©lisation de la client√®le¬ª</a></li>
<li><a href="../fr471502/index.html">Ferme d'id√©es</a></li>
<li><a href="../fr471504/index.html">Duo bidimensionnel: cr√©ation d'h√©t√©rostructures borof√®ne-graph√®ne</a></li>
<li><a href="../fr471506/index.html">Arrondi correct des nombres d√©cimaux dans le code binaire</a></li>
<li><a href="../fr471512/index.html">28 octobre, Iekaterinbourg - Communication de qualit√©</a></li>
<li><a href="../fr471514/index.html">La rubrique ¬´Lisez des articles pour vous¬ª. Janvier - juin 2019</a></li>
<li><a href="../fr471516/index.html">Intel 665p - SSD avec NAND QLC √† 96 couches</a></li>
<li><a href="../fr471518/index.html">Apple en 2019 est Linux en 2000</a></li>
<li><a href="../fr471520/index.html">Le livre "T√¢ches classiques en informatique en Python"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>