<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòé üï¥üèº üê¥ Como aumentamos a produtividade do servi√ßo em Tensorflow em 70% üëàüèø üåô üè≥Ô∏è‚Äçüåà</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="O Tensorflow se tornou a plataforma padr√£o para aprendizado de m√°quina (ML), popular tanto no setor quanto na pesquisa. Muitas bibliotecas, ferramenta...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Como aumentamos a produtividade do servi√ßo em Tensorflow em 70%</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/445928/"> O Tensorflow se tornou a plataforma padr√£o para aprendizado de m√°quina (ML), popular tanto no setor quanto na pesquisa.  Muitas bibliotecas, ferramentas e estruturas gratuitas foram criadas para treinamento e manuten√ß√£o de modelos de ML.  O projeto Tensorflow Serving ajuda a manter modelos de ML em um ambiente de produ√ß√£o distribu√≠do. <br><br>  Nosso servi√ßo Mux usa o Tensorflow Serving em v√°rias partes da infraestrutura. J√° discutimos o uso do Tensorflow Serving na codifica√ß√£o de t√≠tulos de v√≠deo.  Hoje, focaremos nos m√©todos que melhoram a lat√™ncia, otimizando o servidor de previs√£o e o cliente.  As previs√µes de modelo s√£o geralmente opera√ß√µes "online" (no caminho cr√≠tico da solicita√ß√£o de um aplicativo); portanto, os principais objetivos da otimiza√ß√£o s√£o processar grandes volumes de solicita√ß√µes com o menor atraso poss√≠vel. <br><a name="habracut"></a><br><h1>  O que √© o Tensorflow Serving? </h1><br>  O Tensorflow Serving fornece uma arquitetura de servidor flex√≠vel para implantar e manter modelos de ML.  Depois que o modelo √© treinado e pronto para uso para previs√£o, o Tensorflow Serving exige export√°-lo para um formato compat√≠vel (que pode ser utilizado). <br><br>  <i>Servable</i> √© uma abstra√ß√£o central que envolve objetos do Tensorflow.  Por exemplo, um modelo pode ser representado como um ou mais objetos Serv√≠veis.  Assim, Servable s√£o os objetos b√°sicos que o cliente usa para executar c√°lculos.  O tamanho do servi√ßo √© importante: modelos menores ocupam menos espa√ßo, usam menos mem√≥ria e carregam mais rapidamente.  Para baixar e manter usando a API Predict, os modelos devem estar no formato SavedModel. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/20b/9f9/47e/20b9f947ea0f318dc7c2eba619b3901f.png"><br><br>  O Tensorflow Serving combina os componentes b√°sicos para criar um servidor gRPC / HTTP que atende a v√°rios modelos de ML (ou v√°rias vers√µes), fornece componentes de monitoramento e uma arquitetura customizada. <br><br><h1>  Servi√ßo Tensorflow com Docker </h1><br>  Vamos dar uma olhada nas m√©tricas b√°sicas de lat√™ncia na previs√£o de desempenho com as configura√ß√µes padr√£o do Tensorflow Serving (sem otimiza√ß√£o da CPU). <br><br>  Primeiro, baixe a imagem mais recente do hub TensorFlow Docker: <br><br><pre><code class="bash hljs">docker pull tensorflow/serving:latest</code> </pre> <br>  Neste artigo, todos os cont√™ineres s√£o executados em um host com quatro n√∫cleos, 15 GB, Ubuntu 16.04. <br><br><h3>  Exportar modelo de fluxo de tens√£o para SavedModel </h3><br>  Quando um modelo √© treinado usando o Tensorflow, a sa√≠da pode ser salva como pontos de controle vari√°veis ‚Äã‚Äã(arquivos em disco).  A sa√≠da √© realizada diretamente, restaurando os pontos de controle do modelo ou em um formato de gr√°fico congelado congelado (arquivo bin√°rio). <br><br>  Para a Tensorflow Serving, esse gr√°fico congelado precisa ser exportado para o formato SavedModel.  A <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o do Tensorflow</a> cont√©m exemplos de exporta√ß√£o de modelos treinados para o formato SavedModel. <br><br>  O Tensorflow tamb√©m fornece muitos modelos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">oficiais e de pesquisa</a> como ponto de partida para experimenta√ß√£o, pesquisa ou produ√ß√£o. <br><br>  Como exemplo, usaremos o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">modelo de rede neural residual profunda (ResNet)</a> para classificar um conjunto de dados ImageNet de 1000 classes.  Fa√ßa o download do modelo <code>ResNet-50 v2</code> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pr√©</a> - <code>ResNet-50 v2</code> , especificamente a op√ß√£o Channels_last (NHWC) no <i>SavedModel</i> : como regra geral, ele funciona melhor na CPU. <br><br>  Copie o diret√≥rio do modelo RestNet na seguinte estrutura: <br><br><pre> <code class="plaintext hljs">models/ 1/ saved_model.pb variables/ variables.data-00000-of-00001 variables.index</code> </pre> <br>  O Tensorflow Serving espera uma estrutura de diret√≥rios ordenada numericamente para controle de vers√£o.  No nosso caso, o diret√≥rio <code>1/</code> corresponde ao modelo da vers√£o 1, que cont√©m a arquitetura do modelo <code>saved_model.pb</code> com uma captura instant√¢nea dos pesos do modelo (vari√°veis). <br><br><h3>  Carregando e processando SavedModel </h3><br>  O comando a seguir inicia o servidor do modelo Tensorflow Serving em um cont√™iner do Docker.  Para carregar SavedModel, voc√™ deve montar o diret√≥rio de modelo no diret√≥rio de cont√™iner esperado. <br><br><pre> <code class="plaintext hljs">docker run -d -p 9000:8500 \ -v $(pwd)/models:/models/resnet -e MODEL_NAME=resnet \ -t tensorflow/serving:latest</code> </pre> <br>  A verifica√ß√£o dos logs do cont√™iner mostra que o ModelServer est√° ativo e em execu√ß√£o para manipular solicita√ß√µes de sa√≠da para o modelo de <code>resnet</code> - <code>resnet</code> nos pontos de extremidade gRPC e HTTP: <br><br><pre> <code class="plaintext hljs">... I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: resnet version: 1} I tensorflow_serving/model_servers/server.cc:286] Running gRPC ModelServer at 0.0.0.0:8500 ... I tensorflow_serving/model_servers/server.cc:302] Exporting HTTP/REST API at:localhost:8501 ...</code> </pre> <br><h3>  Cliente de Previs√£o </h3><br>  O Tensorflow Serving define um esquema de API no formato de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">buffers de protocolo</a> (protobufs).  As implementa√ß√µes do cliente GRPC para a API de previs√£o s√£o empacotadas como um pacote Python <code>tensorflow_serving.apis</code> .  N√≥s precisaremos de outro pacote Python <code>tensorflow</code> para fun√ß√µes utilit√°rias. <br><br>  Instale as depend√™ncias para criar um cliente simples: <br><br><pre> <code class="plaintext hljs">virtualenv .env &amp;&amp; source .env/bin/activate &amp;&amp; \ pip install numpy grpcio opencv-python tensorflow tensorflow-serving-api</code> </pre> <br>  O modelo <code>ResNet-50 v2</code> espera a entrada de tensores de ponto flutuante em uma estrutura de dados formatados channels_last (NHWC).  Portanto, a imagem de entrada √© lida usando opencv-python e carregada na matriz numpy (altura √ó largura √ó canais) como um tipo de dados float32.  O script abaixo cria um stub do cliente de previs√£o e carrega os dados JPEG em uma matriz numpy, converte-os em tensor_proto para fazer uma solicita√ß√£o de previs√£o para o gRPC: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/env python from __future__ import print_function import argparse import numpy as np import time tt = time.time() import cv2 import tensorflow as tf from grpc.beta import implementations from tensorflow_serving.apis import predict_pb2 from tensorflow_serving.apis import prediction_service_pb2 parser = argparse.ArgumentParser(description='incetion grpc client flags.') parser.add_argument('--host', default='0.0.0.0', help='inception serving host') parser.add_argument('--port', default='9000', help='inception serving port') parser.add_argument('--image', default='', help='path to JPEG image file') FLAGS = parser.parse_args() def main(): # create prediction service client stub channel = implementations.insecure_channel(FLAGS.host, int(FLAGS.port)) stub = prediction_service_pb2.beta_create_PredictionService_stub(channel) # create request request = predict_pb2.PredictRequest() request.model_spec.name = 'resnet' request.model_spec.signature_name = 'serving_default' # read image into numpy array img = cv2.imread(FLAGS.image).astype(np.float32) # convert to tensor proto and make request # shape is in NHWC (num_samples x height x width x channels) format tensor = tf.contrib.util.make_tensor_proto(img, shape=[1]+list(img.shape)) request.inputs['input'].CopyFrom(tensor) resp = stub.Predict(request, 30.0) print('total time: {}s'.format(time.time() - tt)) if __name__ == '__main__': main()</span></span></code> </pre> <br>  Ap√≥s receber uma entrada JPEG, um cliente ativo produzir√° o seguinte resultado: <br><br><pre> <code class="bash hljs">python tf_serving_client.py --image=images/pupper.jpg total time: 2.56152906418s</code> </pre> <br>  O tensor resultante cont√©m uma previs√£o na forma de um valor inteiro e probabilidade de sinais. <br><br><pre> <code class="python hljs">outputs { key: <span class="hljs-string"><span class="hljs-string">"classes"</span></span> value { dtype: DT_INT64 tensor_shape { dim { size: <span class="hljs-number"><span class="hljs-number">1</span></span> } } int64_val: <span class="hljs-number"><span class="hljs-number">238</span></span> } } outputs { key: <span class="hljs-string"><span class="hljs-string">"probabilities"</span></span> ...</code> </pre> <br>  Para uma √∫nica solicita√ß√£o, esse atraso n√£o √© aceit√°vel.  Mas nada de surpreendente: o bin√°rio Tensorflow Serving √© projetado por padr√£o para a mais ampla gama de equipamentos para a maioria dos casos de uso.  Voc√™ provavelmente notou as seguintes linhas nos logs do cont√™iner Tensorflow Serving padr√£o: <br><br><pre> <code class="plaintext hljs">I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</code> </pre> <br>  Isso indica um bin√°rio do TensorFlow Serving em execu√ß√£o em uma plataforma de CPU para a qual n√£o foi otimizado. <br><br><h3>  Construa um bin√°rio otimizado </h3><br>  De acordo com a <a href="">documenta√ß√£o</a> do Tensorflow, √© recomend√°vel compilar o Tensorflow a partir da fonte com todas as otimiza√ß√µes dispon√≠veis para a CPU no host em que o bin√°rio funcionar√°.  Ao montar, sinalizadores especiais permitem a ativa√ß√£o de conjuntos de instru√ß√µes da CPU para uma plataforma espec√≠fica: <br><br><div class="scrollable-table"><table><tbody><tr><th>  Conjunto de instru√ß√µes </th><th>  Bandeiras </th></tr><tr><td>  AVX </td><td>  --copt = -mavx </td></tr><tr><td>  AVX2 </td><td>  --copt = -mavx2 </td></tr><tr><td>  Fma </td><td>  --copt = -mfma </td></tr><tr><td>  SSE 4.1 </td><td>  --copt = -msse4.1 </td></tr><tr><td>  SSE 4.2 </td><td>  --copt = -msse4.2 </td></tr><tr><td>  Tudo suportado pelo processador </td><td>  --copt = -march = nativo </td></tr></tbody></table></div><br>  Clone uma veicula√ß√£o de fluxo tensor de uma vers√£o espec√≠fica.  No nosso caso, √© 1,13 (o √∫ltimo no momento da publica√ß√£o deste artigo): <br><br><pre> <code class="bash hljs">USER=<span class="hljs-variable"><span class="hljs-variable">$1</span></span> TAG=<span class="hljs-variable"><span class="hljs-variable">$2</span></span> TF_SERVING_VERSION_GIT_BRANCH=<span class="hljs-string"><span class="hljs-string">"r1.13"</span></span> git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> --branch=<span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$TF_SERVING_VERSION_GIT_BRANCH</span></span></span><span class="hljs-string">"</span></span> https://github.com/tensorflow/serving</code> </pre> <br>  A imagem do desenvolvedor do Tensorflow Serving usa a ferramenta Basel para criar.  N√≥s o configuramos para conjuntos espec√≠ficos de instru√ß√µes da CPU: <br><br><pre> <code class="bash hljs">TF_SERVING_BUILD_OPTIONS=<span class="hljs-string"><span class="hljs-string">"--copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2"</span></span></code> </pre> <br>  Se n√£o houver mem√≥ria suficiente, limite o consumo de mem√≥ria durante o processo de constru√ß√£o com o sinalizador <code>--local_resources=2048,.5,1.0</code> .  Para obter informa√ß√µes sobre sinalizadores, consulte a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ajuda</a> do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tensorflow Serving and Docker</a> , bem como a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o</a> do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Bazel</a> . <br><br>  Crie uma imagem de trabalho com base na existente: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash USER=$1 TAG=$2 TF_SERVING_VERSION_GIT_BRANCH="r1.13" git clone --branch="${TF_SERVING_VERSION_GIT_BRANCH}" https://github.com/tensorflow/serving TF_SERVING_BUILD_OPTIONS="--copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2" cd serving &amp;&amp; \ docker build --pull -t $USER/tensorflow-serving-devel:$TAG \ --build-arg TF_SERVING_VERSION_GIT_BRANCH="${TF_SERVING_VERSION_GIT_BRANCH}" \ --build-arg TF_SERVING_BUILD_OPTIONS="${TF_SERVING_BUILD_OPTIONS}" \ -f tensorflow_serving/tools/docker/Dockerfile.devel . cd serving &amp;&amp; \ docker build -t $USER/tensorflow-serving:$TAG \ --build-arg TF_SERVING_BUILD_IMAGE=$USER/tensorflow-serving-devel:$TAG \ -f tensorflow_serving/tools/docker/Dockerfile .</span></span></code> </pre> <br>  O ModelServer √© configurado usando <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">sinalizadores TensorFlow</a> para oferecer suporte √† simultaneidade.  As seguintes op√ß√µes configuram dois conjuntos de encadeamentos para opera√ß√£o paralela: <br><br><pre> <code class="plaintext hljs">intra_op_parallelism_threads</code> </pre> <br><ul><li>  controla o n√∫mero m√°ximo de threads para execu√ß√£o paralela de uma opera√ß√£o; <br></li><li>  usado para paralelizar opera√ß√µes que possuem subopera√ß√µes de natureza independente. </li></ul><br><pre> <code class="plaintext hljs">inter_op_parallelism_threads</code> </pre> <br><ul><li>  controla o n√∫mero m√°ximo de threads para execu√ß√£o paralela de opera√ß√µes independentes; <br></li><li>  As opera√ß√µes de gr√°fico de tens√£o tensor, que s√£o independentes uma da outra e, portanto, podem ser executadas em threads diferentes. </li></ul><br>  Por padr√£o, os dois par√¢metros s√£o definidos como <code>0</code> .  Isso significa que o pr√≥prio sistema seleciona o n√∫mero apropriado, o que geralmente significa um encadeamento por n√∫cleo.  No entanto, o par√¢metro pode ser alterado manualmente para simultaneidade de v√°rios n√∫cleos. <br><br>  Em seguida, execute o cont√™iner Serving da mesma maneira que o anterior, desta vez com uma imagem do Docker compilada a partir das fontes e com os sinalizadores de otimiza√ß√£o do Tensorflow para um processador espec√≠fico: <br><br><pre> <code class="bash hljs">docker run -d -p 9000:8500 \ -v $(<span class="hljs-built_in"><span class="hljs-built_in">pwd</span></span>)/models:/models/resnet -e MODEL_NAME=resnet \ -t <span class="hljs-variable"><span class="hljs-variable">$USER</span></span>/tensorflow-serving:<span class="hljs-variable"><span class="hljs-variable">$TAG</span></span> \ --tensorflow_intra_op_parallelism=4 \ --tensorflow_inter_op_parallelism=4</code> </pre> <br>  Os logs do cont√™iner n√£o devem mais mostrar avisos sobre uma CPU indefinida.  Sem alterar o c√≥digo na mesma solicita√ß√£o de previs√£o, o atraso √© reduzido em cerca de 35,8%: <br><br><pre> <code class="bash hljs">python tf_serving_client.py --image=images/pupper.jpg total time: 1.64234706879s</code> </pre> <br><h3>  Aumentar a velocidade na previs√£o do cliente </h3><br>  Ainda √© poss√≠vel acelerar?  Otimizamos o lado do servidor para nossa CPU, mas um atraso de mais de 1 segundo ainda parece muito grande. <br><br>  Aconteceu que o carregamento das bibliotecas <code>tensorflow</code> e <code>tensorflow</code> contribui significativamente para o atraso.  Cada chamada desnecess√°ria para <code>tf.contrib.util.make_tensor_proto</code> tamb√©m adiciona uma fra√ß√£o de segundo. <br><br>  Voc√™ pode perguntar: "N√£o precisamos dos pacotes TensorFlow Python para realmente fazer solicita√ß√µes de previs√£o ao servidor Tensorflow?"  De fato, n√£o h√° <i>necessidade</i> real de pacotes <code>tensorflow</code> e <code>tensorflow</code> . <br><br>  Conforme observado anteriormente, as APIs de previs√£o do Tensorflow s√£o definidas como proto-buffers.  Portanto, duas depend√™ncias externas podem ser substitu√≠das <code>tensorflow_serving</code> correspondentes <code>tensorflow</code> e <code>tensorflow_serving</code> - e voc√™ n√£o precisa extrair toda a biblioteca (pesada) do Tensorflow no cliente. <br><br>  Primeiro, livre-se das <code>tensorflow_serving</code> e <code>tensorflow_serving</code> e adicione o pacote <code>grpcio-tools</code> . <br><br><pre> <code class="bash hljs">pip uninstall tensorflow tensorflow-serving-api &amp;&amp; \ pip install grpcio-tools==1.0.0</code> </pre> <br>  Clone os <code>tensorflow/tensorflow</code> e <code>tensorflow/serving</code> e copie os seguintes arquivos protobuf no projeto do cliente: <br><br><pre> <code class="plaintext hljs">tensorflow/serving/ tensorflow_serving/apis/model.proto tensorflow_serving/apis/predict.proto tensorflow_serving/apis/prediction_service.proto tensorflow/tensorflow/ tensorflow/core/framework/resource_handle.proto tensorflow/core/framework/tensor_shape.proto tensorflow/core/framework/tensor.proto tensorflow/core/framework/types.proto</code> </pre> <br>  Copie esses arquivos protobuf para o diret√≥rio <code>protos/</code> com os caminhos originais preservados: <br><br><pre> <code class="plaintext hljs">protos/ tensorflow_serving/ apis/ *.proto tensorflow/ core/ framework/ *.proto</code> </pre> <br>  Por uma quest√£o de simplicidade, o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">prediction_service.proto</a> pode ser simplificado para implementar apenas o Predict RPC, para n√£o baixar as depend√™ncias aninhadas de outros RPCs especificados no servi√ßo.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aqui est√°</a> um exemplo de um <code>prediction_service.</code> simplificado. <br><br>  Crie implementa√ß√µes de gRPC do Python usando <code>grpcio.tools.protoc</code> : <br><br><pre> <code class="plaintext hljs">PROTOC_OUT=protos/ PROTOS=$(find . | grep "\.proto$") for p in $PROTOS; do python -m grpc.tools.protoc -I . --python_out=$PROTOC_OUT --grpc_python_out=$PROTOC_OUT $p done</code> </pre> <br>  Agora, todo o m√≥dulo <code>tensorflow_serving</code> pode ser removido: <br><br><pre> <code class="plaintext hljs">from tensorflow_serving.apis import predict_pb2 from tensorflow_serving.apis import prediction_service_pb2</code> </pre> <br>  ... e substitua pelos protobuffers gerados a partir de <code>protos/tensorflow_serving/apis</code> : <br><br><pre> <code class="plaintext hljs">from protos.tensorflow_serving.apis import predict_pb2 from protos.tensorflow_serving.apis import prediction_service_pb2</code> </pre> <br>  A biblioteca Tensorflow √© importada para usar a fun√ß√£o auxiliar <code>make_tensor_proto</code> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">necess√°ria para</a> agrupar um objeto python / numpy como um objeto TensorProto. <br><br>  Assim, podemos substituir a seguinte depend√™ncia e fragmento de c√≥digo: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf ... tensor = tf.contrib.util.make_tensor_proto(features) request.inputs[<span class="hljs-string"><span class="hljs-string">'inputs'</span></span>].CopyFrom(tensor)</code> </pre> <br>  importar protobuffers e criar um objeto TensorProto: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> protos.tensorflow.core.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensor_pb2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> protos.tensorflow.core.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensor_shape_pb2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> protos.tensorflow.core.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> types_pb2 ... <span class="hljs-comment"><span class="hljs-comment"># ensure NHWC shape and build tensor proto tensor_shape = [1]+list(img.shape) dims = [tensor_shape_pb2.TensorShapeProto.Dim(size=dim) for dim in tensor_shape] tensor_shape = tensor_shape_pb2.TensorShapeProto(dim=dims) tensor = tensor_pb2.TensorProto( dtype=types_pb2.DT_FLOAT, tensor_shape=tensor_shape, float_val=list(img.reshape(-1))) request.inputs['inputs'].CopyFrom(tensor)</span></span></code> </pre> <br>  O script Python completo est√° <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> .  Execute um cliente inicial atualizado que fa√ßa uma solicita√ß√£o de previs√£o para a exibi√ß√£o otimizada do Tensorflow: <br><br><pre> <code class="bash hljs">python tf_inception_grpc_client.py --image=images/pupper.jpg total time: 0.58314920859s</code> </pre> <br>  O diagrama a seguir mostra o tempo de execu√ß√£o da previs√£o na vers√£o otimizada do Tensorflow Serving em compara√ß√£o com o padr√£o, em 10 execu√ß√µes: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/48d/990/b83/48d990b83dc54762fec809a580c450c8.png"><br><br>  O atraso m√©dio diminuiu cerca de 3,38 vezes. <br><br><h1>  Otimiza√ß√£o da largura de banda </h1><br>  O Tensorflow Serving pode ser configurado para lidar com grandes quantidades de dados.  A otimiza√ß√£o da largura de banda geralmente √© executada para processamento em lote "independente", onde limites de lat√™ncia r√≠gidos n√£o s√£o um requisito estrito. <br><br><h3>  Processamento em lote do lado do servidor </h3><br>  Conforme declarado na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o</a> , o processamento em lote do lado do servidor √© suportado nativamente no Tensorflow Serving. <br><br>  As compensa√ß√µes entre lat√™ncia e taxa de transfer√™ncia s√£o determinadas pelos par√¢metros de processamento em lote.  Eles permitem que voc√™ alcance a taxa de transfer√™ncia m√°xima que os aceleradores de hardware s√£o capazes. <br><br>  Para ativar o empacotamento, defina os <code>--batching_parameters_file</code> <code>--enable_batching</code> e <code>--batching_parameters_file</code> .  Os par√¢metros s√£o definidos de acordo com <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SessionBundleConfig</a> .  Para sistemas na CPU, configure <code>num_batch_threads</code> para o n√∫mero de n√∫cleos dispon√≠veis.  Para a GPU, consulte os par√¢metros apropriados <a href="">aqui</a> . <br><br>  Ap√≥s preencher o pacote inteiro no lado do servidor, as solicita√ß√µes de emiss√£o s√£o combinadas em uma solicita√ß√£o grande (tensor) e enviadas √† sess√£o do Tensorflow com uma solicita√ß√£o combinada.  Nessa situa√ß√£o, o paralelismo CPU / GPU est√° realmente envolvido. <br><br>  Alguns usos comuns para o processamento em lote do Tensorflow: <br><br><ul><li>  Usando solicita√ß√µes de cliente ass√≠ncronas para preencher pacotes do lado do servidor <br></li><li>  Processamento em lote mais r√°pido, transferindo os componentes do gr√°fico do modelo para a CPU / GPU <br></li><li>  Atendendo solicita√ß√µes de v√°rios modelos em um √∫nico servidor <br></li><li>  O processamento em lote √© altamente recomendado para o processamento "offline" de um grande n√∫mero de solicita√ß√µes </li></ul><br><h3>  Processamento em lote do lado do cliente </h3><br>  O processamento em lote do lado do cliente agrupa v√°rias solicita√ß√µes recebidas em uma. <br><br>  Como o modelo ResNet est√° aguardando entrada no formato NHWC (a primeira dimens√£o √© o n√∫mero de entradas), podemos combinar v√°rias imagens de entrada em uma solicita√ß√£o RPC: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">... </span></span>batch = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> jpeg <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> os.listdir(FLAGS.images_path): path = os.path.join(FLAGS.images_path, jpeg) img = cv2.imread(path).astype(np.float32) batch.append(img) ... batch_np = np.array(batch).astype(np.float32) dims = [tensor_shape_pb2.TensorShapeProto.Dim(size=dim) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> dim <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> batch_np.shape] t_shape = tensor_shape_pb2.TensorShapeProto(dim=dims) tensor = tensor_pb2.TensorProto( dtype=types_pb2.DT_FLOAT, tensor_shape=t_shape, float_val=list(batched_np.reshape(<span class="hljs-number"><span class="hljs-number">-1</span></span>))) request.inputs[<span class="hljs-string"><span class="hljs-string">'inputs'</span></span>].CopyFrom(tensor)</code> </pre> <br>  Para um pacote de N imagens, o tensor de sa√≠da na resposta conter√° os resultados da previs√£o para o mesmo n√∫mero de entradas.  No nosso caso, N = 2: <br><br><pre> <code class="python hljs">outputs { key: <span class="hljs-string"><span class="hljs-string">"classes"</span></span> value { dtype: DT_INT64 tensor_shape { dim { size: <span class="hljs-number"><span class="hljs-number">2</span></span> } } int64_val: <span class="hljs-number"><span class="hljs-number">238</span></span> int64_val: <span class="hljs-number"><span class="hljs-number">121</span></span> } } ...</code> </pre> <br><h1>  Acelera√ß√£o de hardware </h1><br>  Algumas palavras sobre GPUs. <br><br>  O processo de aprendizado naturalmente usa paraleliza√ß√£o na GPU, pois a constru√ß√£o de redes neurais profundas exige c√°lculos massivos para alcan√ßar a solu√ß√£o ideal. <br><br>  Mas, para gerar resultados, a paraleliza√ß√£o n√£o √© t√£o √≥bvia.  Freq√ºentemente, voc√™ pode acelerar a sa√≠da de uma rede neural para uma GPU, mas precisa selecionar e testar cuidadosamente o equipamento, al√©m de realizar an√°lises t√©cnicas e econ√¥micas detalhadas.  A paraleliza√ß√£o de hardware √© mais valiosa para o processamento em lote de conclus√µes "aut√¥nomas" (grandes volumes). <br><br>  Antes de mudar para uma GPU, considere os requisitos de neg√≥cios com uma an√°lise cuidadosa dos custos (monet√°rio, operacional, t√©cnico) para obter o maior benef√≠cio (lat√™ncia reduzida, alto rendimento). </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt445928/">https://habr.com/ru/post/pt445928/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt445918/index.html">20 anos de RollerCoaster Tycoon: uma entrevista com o criador do jogo</a></li>
<li><a href="../pt445920/index.html">Live: como restringir o desenvolvimento do iOS em grandes equipes</a></li>
<li><a href="../pt445922/index.html">Por que assistir transmiss√µes online, se voc√™ pode ler Habr</a></li>
<li><a href="../pt445924/index.html">TESOUROS: quando os rel√≥gios inteligentes ficam estranhos</a></li>
<li><a href="../pt445926/index.html">O Programa Secreto de OVNIs dos EUA tamb√©m pesquisou buracos de minhoca e dimens√µes extras.</a></li>
<li><a href="../pt445932/index.html">Seguran√ßa de aplicativos clientes: dicas pr√°ticas para um desenvolvedor front-end</a></li>
<li><a href="../pt445936/index.html">Desenvolvimento de eletr√¥nicos. Sobre microcontroladores nos dedos</a></li>
<li><a href="../pt445940/index.html">AMA com Habr, v 7.0. Lim√£o, Rosquinhas e Not√≠cias</a></li>
<li><a href="../pt445946/index.html">MWC: instru√ß√µes de uso</a></li>
<li><a href="../pt445948/index.html">Heran√ßa em C ++: iniciante, intermedi√°rio, avan√ßado</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>