<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©‚Äç‚ù§Ô∏è‚Äçüë® üö† ü§ú Juega a Mortal Kombat con TensorFlow.js üïò üö™ üë§</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Experimentando con mejoras para el modelo de pron√≥stico de Guess.js , comenc√© a mirar de cerca el aprendizaje profundo: redes neuronales recurrentes (...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Juega a Mortal Kombat con TensorFlow.js</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/428019/">  Experimentando con mejoras para el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">modelo de</a> pron√≥stico de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Guess.js</a> , comenc√© a mirar de cerca el aprendizaje profundo: redes neuronales recurrentes (RNN), en particular LSTM, debido a su <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"efectividad irracional"</a> en el √°rea donde trabaja Guess.js.  Al mismo tiempo, comenc√© a jugar con redes neuronales convolucionales (CNN), que tambi√©n se usan a menudo para series de tiempo.  Las CNN se usan com√∫nmente para clasificar, reconocer y detectar im√°genes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1fb/9be/edc/1fb9beedcad00d1c0dcdc7bbef67e6d9.png"><br>  <i><font color="gray">Administrar <a href="">MK.js</a> con TensorFlow.js</font></i> <br><br><blockquote>  El c√≥digo fuente de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este art√≠culo</a> y <a href="">MK.js</a> est√°n en mi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">GitHub</a> .  ¬°No he publicado un conjunto de datos de entrenamiento, pero puedes construir el tuyo y entrenar el modelo como se describe a continuaci√≥n! </blockquote><a name="habracut"></a><br>  Despu√©s de jugar con CNN, record√© un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">experimento que</a> realic√© hace varios a√±os cuando los desarrolladores de navegadores lanzaron la API <code>getUserMedia</code> .  En √©l, la c√°mara del usuario sirvi√≥ como controlador para jugar al peque√±o clon JavaScript de Mortal Kombat 3. Puedes encontrar ese juego en <a href="">el repositorio de GitHub</a> .  Como parte del experimento, implement√© un algoritmo de posicionamiento b√°sico que clasifica la imagen en las siguientes clases: <br><br><ul><li>  Golpe izquierdo o derecho </li><li>  Patada izquierda o derecha </li><li>  Pasos a izquierda y derecha </li><li>  Ponerse en cuclillas </li><li>  Ninguna de las anteriores </li></ul><br>  El algoritmo es tan simple que puedo explicarlo en unas pocas oraciones: <br><br><blockquote>  El algoritmo fotograf√≠a el fondo.  Tan pronto como el usuario aparece en el cuadro, el algoritmo calcula la diferencia entre el fondo y el cuadro actual con el usuario.  Entonces determina la posici√≥n de la figura del usuario.  El siguiente paso es mostrar el cuerpo del usuario en blanco sobre negro.  Despu√©s de eso, se construyen histogramas verticales y horizontales, sumando los valores para cada p√≠xel.  Basado en este c√°lculo, el algoritmo determina la posici√≥n actual del cuerpo. </blockquote><br>  El video muestra c√≥mo funciona el programa.  C√≥digo fuente de <a href="">GitHub</a> . <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/0_yfU_iNUYo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Aunque el peque√±o clon MK funcion√≥ con √©xito, el algoritmo est√° lejos de ser perfecto.  Se requiere un marco con un fondo.  Para una operaci√≥n adecuada, el fondo debe ser del mismo color durante la ejecuci√≥n del programa.  Tal limitaci√≥n significa que los cambios en la luz, la sombra y otras cosas interferir√°n y dar√°n un resultado inexacto.  Finalmente, el algoritmo no reconoce la acci√≥n;  solo clasifica el nuevo marco como la posici√≥n del cuerpo de un conjunto predefinido. <br><br>  Ahora, gracias al progreso en la API web, es decir, WebGL, decid√≠ volver a esta tarea aplicando TensorFlow.js. <br><br><h1>  Introduccion </h1><br>  En este art√≠culo, compartir√© mi experiencia en la creaci√≥n de un algoritmo para clasificar las posiciones del cuerpo usando TensorFlow.js y MobileNet.  Considere los siguientes temas: <br><br><ul><li>  Recolecci√≥n de datos de capacitaci√≥n para la clasificaci√≥n de im√°genes. </li><li>  Aumento de datos con <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">imgaug</a> </li><li>  Transferir aprendizaje con MobileNet </li><li>  Clasificaci√≥n binaria y clasificaci√≥n N-primaria </li><li>  Entrenamiento del modelo de clasificaci√≥n de im√°genes TensorFlow.js en Node.js y uso en un navegador </li><li>  Algunas palabras sobre la clasificaci√≥n de acciones con LSTM </li></ul><br>  En este art√≠culo, reduciremos el problema para determinar la posici√≥n del cuerpo sobre la base de un cuadro, en contraste con el reconocimiento de acciones por una secuencia de cuadros.  Desarrollaremos un modelo de aprendizaje profundo con un maestro, que, seg√∫n la imagen de la c√°mara web del usuario, determina los movimientos de una persona: patada, pierna o nada de esto. <br><br>  Al final del art√≠culo, podremos construir un modelo para jugar <a href="">MK.js</a> : <br><br><img src="https://habrastorage.org/webt/2u/0e/g6/2u0eg6ng2p4kwxosmut1koa751g.gif"><br><br>  Para una mejor comprensi√≥n del art√≠culo, el lector debe estar familiarizado con los conceptos fundamentales de programaci√≥n y JavaScript.  Una comprensi√≥n b√°sica del aprendizaje profundo tambi√©n es √∫til, pero no necesaria. <br><br><h1>  Recogida de datos </h1><br>  La precisi√≥n del modelo de aprendizaje profundo depende en gran medida de la calidad de los datos.  Necesitamos esforzarnos por recopilar un amplio conjunto de datos, como en la producci√≥n. <br><br>  Nuestro modelo deber√≠a ser capaz de reconocer golpes y patadas.  Esto significa que debemos recopilar im√°genes de tres categor√≠as: <br><br><ul><li>  Patadas </li><li>  Patadas </li><li>  Otros </li></ul><br>  En este experimento, dos voluntarios ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">@lili_vs</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">@gsamokovarov</a> ) me ayudaron a recolectar fotos.  Grabamos 5 videos QuickTime en mi MacBook Pro, cada uno con 2-4 patadas y 2-4 patadas. <br><br>  Luego usamos ffmpeg para extraer cuadros individuales de los videos y guardarlos como im√°genes <code>jpg</code> : <br><br> <code>ffmpeg -i video.mov $filename%03d.jpg</code> <br> <br>  Para ejecutar el comando anterior, primero debe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">instalar</a> <code>ffmpeg</code> en la computadora. <br><br>  Si queremos entrenar el modelo, debemos proporcionar los datos de entrada y los datos de salida correspondientes, pero en esta etapa solo tenemos un grupo de im√°genes de tres personas en diferentes poses.  Para estructurar los datos, debe clasificar los marcos en tres categor√≠as: golpes, patadas y otros.  Para cada categor√≠a, se crea un directorio separado donde se mueven todas las im√°genes correspondientes. <br><br>  Por lo tanto, en cada directorio debe haber aproximadamente 200 im√°genes similares a las siguientes: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/798/e9a/908/798e9a9083a1f5dfa5811fbb7de3bcc9.jpg"><br><br>  Tenga en cuenta que habr√° muchas m√°s im√°genes en el directorio Otros, porque relativamente pocos cuadros contienen fotos de golpes y patadas, y en los cuadros restantes la gente camina, se da vuelta o controla el video.  Si tenemos demasiadas im√°genes de una clase, corremos el riesgo de ense√±ar el modelo sesgado hacia esta clase en particular.  En este caso, al clasificar una imagen con un impacto, la red neuronal a√∫n puede determinar la clase "Otro".  Para reducir este sesgo, puede eliminar algunas fotos del directorio Otros y entrenar al modelo en un n√∫mero igual de im√°genes de cada categor√≠a. <br><br>  Para mayor comodidad, asignamos los n√∫meros en los n√∫meros de cat√°logos del <code>1</code> al <code>190</code> , por lo que la primera imagen ser√° <code>1.jpg</code> , la segunda <code>2.jpg</code> , etc. <br><br>  Si entrenamos el modelo en solo 600 fotograf√≠as tomadas en el mismo entorno con las mismas personas, no alcanzaremos un nivel de precisi√≥n muy alto.  Para aprovechar al m√°ximo nuestros datos, es mejor generar algunas muestras adicionales utilizando el aumento de datos. <br><br><h1>  Aumento de datos </h1><br>  El aumento de datos es una t√©cnica que aumenta el n√∫mero de puntos de datos al sintetizar nuevos puntos de un conjunto existente.  Por lo general, el aumento se usa para aumentar el tama√±o y la diversidad del conjunto de entrenamiento.  Transferimos las im√°genes originales a la tuber√≠a de transformaciones que crean nuevas im√°genes.  No puede abordar las transformaciones con demasiada agresividad: solo se deber√≠an generar otros golpes de mano a partir de un golpe. <br><br>  Las transformaciones aceptables son rotaci√≥n, inversi√≥n de color, desenfoque, etc. Existen excelentes herramientas de c√≥digo abierto para el aumento de datos.  Al momento de escribir este art√≠culo en JavaScript, no hab√≠a demasiadas opciones, as√≠ que utilic√© la biblioteca implementada en Python - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">imgaug</a> .  Tiene un conjunto de aumentadores que se pueden aplicar probabil√≠sticamente. <br><br>  Aqu√≠ est√° la l√≥gica de aumento de datos para este experimento: <br><br><pre> <code class="python hljs">np.random.seed(<span class="hljs-number"><span class="hljs-number">44</span></span>) ia.seed(<span class="hljs-number"><span class="hljs-number">44</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">main</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">191</span></span>): draw_single_sequential_images(str(i), <span class="hljs-string"><span class="hljs-string">"others"</span></span>, <span class="hljs-string"><span class="hljs-string">"others-aug"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">191</span></span>): draw_single_sequential_images(str(i), <span class="hljs-string"><span class="hljs-string">"hits"</span></span>, <span class="hljs-string"><span class="hljs-string">"hits-aug"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">191</span></span>): draw_single_sequential_images(str(i), <span class="hljs-string"><span class="hljs-string">"kicks"</span></span>, <span class="hljs-string"><span class="hljs-string">"kicks-aug"</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">draw_single_sequential_images</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename, path, aug_path)</span></span></span><span class="hljs-function">:</span></span> image = misc.imresize(ndimage.imread(path + <span class="hljs-string"><span class="hljs-string">"/"</span></span> + filename + <span class="hljs-string"><span class="hljs-string">".jpg"</span></span>), (<span class="hljs-number"><span class="hljs-number">56</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>)) sometimes = <span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> aug: iaa.Sometimes(<span class="hljs-number"><span class="hljs-number">0.5</span></span>, aug) seq = iaa.Sequential( [ iaa.Fliplr(<span class="hljs-number"><span class="hljs-number">0.5</span></span>), <span class="hljs-comment"><span class="hljs-comment"># horizontally flip 50% of all images # crop images by -5% to 10% of their height/width sometimes(iaa.CropAndPad( percent=(-0.05, 0.1), pad_mode=ia.ALL, pad_cval=(0, 255) )), sometimes(iaa.Affine( scale={"x": (0.8, 1.2), "y": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis translate_percent={"x": (-0.1, 0.1), "y": (-0.1, 0.1)}, # translate by -10 to +10 percent (per axis) rotate=(-5, 5), shear=(-5, 5), # shear by -5 to +5 degrees order=[0, 1], # use nearest neighbour or bilinear interpolation (fast) cval=(0, 255), # if mode is constant, use a cval between 0 and 255 mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples) )), iaa.Grayscale(alpha=(0.0, 1.0)), iaa.Invert(0.05, per_channel=False), # invert color channels # execute 0 to 5 of the following (less important) augmenters per image # don't execute all of them, as that would often be way too strong iaa.SomeOf((0, 5), [ iaa.OneOf([ iaa.GaussianBlur((0, 2.0)), # blur images with a sigma between 0 and 2.0 iaa.AverageBlur(k=(2, 5)), # blur image using local means with kernel sizes between 2 and 5 iaa.MedianBlur(k=(3, 5)), # blur image using local medians with kernel sizes between 3 and 5 ]), iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.01*255), per_channel=0.5), # add gaussian noise to images iaa.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value) iaa.AddToHueAndSaturation((-20, 20)), # change hue and saturation # either change the brightness of the whole image (sometimes # per channel) or change the brightness of subareas iaa.OneOf([ iaa.Multiply((0.9, 1.1), per_channel=0.5), iaa.FrequencyNoiseAlpha( exponent=(-2, 0), first=iaa.Multiply((0.9, 1.1), per_channel=True), second=iaa.ContrastNormalization((0.9, 1.1)) ) ]), iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5), # improve or worsen the contrast ], random_order=True ) ], random_order=True ) im = np.zeros((16, 56, 100, 3), dtype=np.uint8) for c in range(0, 16): im[c] = image for im in range(len(grid)): misc.imsave(aug_path + "/" + filename + "_" + str(im) + ".jpg", grid[im])</span></span></code> </pre> <br>  Este script utiliza el m√©todo <code>main</code> con tres bucles <code>for</code> , uno para cada categor√≠a de imagen.  En cada iteraci√≥n, en cada uno de los bucles, llamamos al m√©todo <code>draw_single_sequential_images</code> : el primer argumento es el nombre del archivo, el segundo es la ruta, el tercero es el directorio donde guardar el resultado. <br><br>  Despu√©s de eso, leemos la imagen del disco y le aplicamos una serie de transformaciones.  He documentado la mayor√≠a de las transformaciones en el fragmento de c√≥digo anterior, por lo que no lo repetiremos. <br><br>  Para cada imagen, se crean otras 16 im√°genes.  Aqu√≠ hay un ejemplo de c√≥mo se ven: <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/759/ad9/43d/759ad943d7aa07dbccee4a6f26a1d920.jpg"></a> <br><br>  Tenga en cuenta que en el script anterior <code>100x56</code> im√°genes a <code>100x56</code> p√≠xeles.  Hacemos esto para reducir la cantidad de datos y, en consecuencia, la cantidad de c√°lculos que nuestro modelo realiza durante el entrenamiento y la evaluaci√≥n. <br><br><h1>  Edificio modelo </h1><br>  ¬°Ahora construya un modelo para la clasificaci√≥n! <br><br>  Como estamos tratando con im√°genes, utilizamos una red neuronal convolucional (CNN).  Se sabe que esta arquitectura de red es adecuada para el reconocimiento de im√°genes, detecci√≥n de objetos y clasificaci√≥n. <br><br><h3>  Transferencia de aprendizaje </h3><br>  La imagen a continuaci√≥n muestra el popular CNN VGG-16, utilizado para clasificar im√°genes. <br><br><img src="https://habrastorage.org/webt/7t/0u/zk/7t0uzk4kdf4pbesgvlojn5nal18.png"><br><br>  La red neuronal VGG-16 reconoce 1000 clases de im√°genes.  Tiene 16 capas (sin contar las capas de agrupaci√≥n y salida).  Tal red multicapa es dif√≠cil de entrenar en la pr√°ctica.  Esto requerir√° un gran conjunto de datos y muchas horas de capacitaci√≥n. <br><br>  Las capas ocultas de CNN entrenado reconocen varios elementos de im√°genes del conjunto de entrenamiento, comenzando desde los bordes, pasando a elementos m√°s complejos, como formas, objetos individuales, etc.  Una CNN entrenada al estilo de VGG-16 para reconocer un gran conjunto de im√°genes debe tener capas ocultas que hayan aprendido muchas caracter√≠sticas del conjunto de capacitaci√≥n.  Dichas caracter√≠sticas ser√°n comunes a la mayor√≠a de las im√°genes y, en consecuencia, se reutilizar√°n en diferentes tareas. <br><br>  La transferencia de aprendizaje le permite reutilizar una red existente y capacitada.  Podemos tomar la salida de cualquiera de las capas de la red existente y transferirla como entrada a la nueva red neuronal.  Por lo tanto, al ense√±ar la red neuronal reci√©n creada, con el tiempo se puede ense√±ar a reconocer nuevas caracter√≠sticas de un nivel superior y clasificar correctamente las im√°genes de clases que el modelo original nunca hab√≠a visto antes. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7n/cc/a7/7ncca7e5ne2ammearn2sqnk4by0.png"></div><br><br>  Para nuestros prop√≥sitos, tome la red neuronal <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MobileNet del paquete @ tensorflow-models / mobilenet</a> .  MobileNet es tan poderoso como VGG-16, pero es mucho m√°s peque√±o, lo que acelera la distribuci√≥n directa, es decir, la propagaci√≥n de red (propagaci√≥n directa) y reduce el tiempo de descarga en el navegador.  MobileNet se capacit√≥ en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">conjunto de datos de</a> clasificaci√≥n de im√°genes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ILSVRC-2012-CLS</a> . <br><br>  Al desarrollar un modelo con una transferencia de aprendizaje, tenemos dos opciones: <br><br><ol><li>  La salida de qu√© capa del modelo de origen usar como entrada para el modelo de destino. </li><li>  ¬øCu√°ntas capas del modelo de destino vamos a entrenar, si hay alguna? </li></ol><br>  El primer punto es muy significativo.  Dependiendo de la capa seleccionada, obtendremos caracter√≠sticas en un nivel de abstracci√≥n m√°s bajo o m√°s alto como entrada a nuestra red neuronal. <br><br>  No vamos a entrenar ninguna capa de MobileNet.  <code>global_average_pooling2d_1</code> salida de <code>global_average_pooling2d_1</code> y la pasamos como entrada a nuestro peque√±o modelo.  ¬øPor qu√© eleg√≠ esta capa en particular?  Emp√≠ricamente  Hice algunas pruebas, y esta capa funciona bastante bien. <br><br><h3>  Definici√≥n del modelo </h3><br>  La tarea inicial era clasificar la imagen en tres clases: mano, pie y otros movimientos.  Primero, solucionemos el problema m√°s peque√±o: determinaremos si hay un golpe de mano en el marco o no.  Este es un problema t√≠pico de clasificaci√≥n binaria.  Para este prop√≥sito, podemos definir el siguiente modelo: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> <span class="hljs-string"><span class="hljs-string">'@tensorflow/tfjs'</span></span>; const model = tf.sequential(); model.add(tf.layers.inputLayer({ inputShape: [<span class="hljs-number"><span class="hljs-number">1024</span></span>] })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">1024</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'relu'</span></span> })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">1</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span> })); model.compile({ optimizer: tf.train.adam(<span class="hljs-number"><span class="hljs-number">1e-6</span></span>), loss: tf.losses.sigmoidCrossEntropy, metrics: [<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>] });</code> </pre> <br>  Dicho c√≥digo define un modelo simple, una capa con <code>1024</code> unidades y activaci√≥n <code>ReLU</code> , as√≠ como una unidad de salida que pasa a trav√©s de la <code>sigmoid</code> activaci√≥n <code>sigmoid</code> .  Este √∫ltimo da un n√∫mero de <code>0</code> a <code>1</code> , dependiendo de la probabilidad de un golpe de mano en este cuadro. <br><br>  ¬øPor qu√© eleg√≠ <code>1024</code> unidades para el segundo nivel y una velocidad de entrenamiento de <code>1e-6</code> ?  Bueno, prob√© varias opciones diferentes y vi que esas opciones funcionan mejor.  El M√©todo Spear no parece ser el mejor enfoque, pero en gran medida as√≠ es c√≥mo funcionan los ajustes de hiperpar√°metros en el aprendizaje profundo: en funci√≥n de nuestra comprensi√≥n del modelo, utilizamos la intuici√≥n para actualizar los par√°metros ortogonales y verificar emp√≠ricamente c√≥mo funciona el modelo. <br><br>  El m√©todo de <code>compile</code> compila las capas juntas, preparando el modelo para capacitaci√≥n y evaluaci√≥n.  Aqu√≠ anunciamos que queremos usar el algoritmo de optimizaci√≥n de <code>adam</code> .  Tambi√©n declaramos que calcularemos la p√©rdida (p√©rdida) a partir de la entrop√≠a cruzada, e indicamos que queremos evaluar la precisi√≥n del modelo.  TensorFlow.js luego calcula la precisi√≥n utilizando la f√≥rmula: <br><br> <code>Accuracy = (True Positives + True Negatives) / (Positives + Negatives)</code> <br> <br>  Si transfiere el entrenamiento desde el modelo original de MobileNet, primero debe descargarlo.  Como no es pr√°ctico entrenar nuestro modelo en m√°s de 3,000 im√°genes en un navegador, usaremos Node.js y cargaremos la red neuronal desde el archivo. <br><br>  Descargue MobileNet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> .  El cat√°logo contiene el archivo <code>model.json</code> , que contiene la arquitectura del modelo: capas, activaciones, etc.  Los archivos restantes contienen par√°metros del modelo.  Puede cargar el modelo desde un archivo usando este c√≥digo: <br><br><pre> <code class="python hljs">export const loadModel = <span class="hljs-keyword"><span class="hljs-keyword">async</span></span> () =&gt; { const mn = new mobilenet.MobileNet(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>); mn.path = `file://PATH/TO/model.json`; <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> mn.load(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (input): tf.Tensor1D =&gt; mn.infer(input, <span class="hljs-string"><span class="hljs-string">'global_average_pooling2d_1'</span></span>) .reshape([<span class="hljs-number"><span class="hljs-number">1024</span></span>]); };</code> </pre> <br>  Tenga en cuenta que en el m√©todo <code>loadModel</code> devolvemos una funci√≥n que acepta un tensor unidimensional como entrada y devuelve <code>mn.infer(input, Layer)</code> .  El m√©todo <code>infer</code> toma un tensor y una capa como argumentos.  La capa determina de qu√© capa oculta queremos la salida.  Si abre <a href="">model.json</a> y <code>global_average_pooling2d_1</code> , encontrar√° dicho nombre en una de las capas. <br><br>  Ahora necesita crear un conjunto de datos para entrenar el modelo.  Para hacer esto, debemos pasar todas las im√°genes a trav√©s del m√©todo <code>infer</code> en MobileNet y asignarles etiquetas: <code>1</code> para im√°genes con trazos y <code>0</code> para im√°genes sin trazos: <br><br><pre> <code class="python hljs">const punches = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Punches) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Punches}/${f}`); const others = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Others) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Others}/${f}`); const ys = tf.tensor1d( new Array(punches.length).fill(<span class="hljs-number"><span class="hljs-number">1</span></span>) .concat(new Array(others.length).fill(<span class="hljs-number"><span class="hljs-number">0</span></span>))); const xs: tf.Tensor2D = tf.stack( punches .map((path: string) =&gt; mobileNet(readInput(path))) .concat(others.map((path: string) =&gt; mobileNet(readInput(path)))) ) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor2D;</code> </pre> <br>  En el c√≥digo anterior, primero leemos los archivos en directorios con y sin aciertos.  Luego determinamos el tensor unidimensional que contiene las etiquetas de salida.  Si tenemos <code>n</code> im√°genes con trazos <code>m</code> otras im√°genes, el tensor tendr√° <code>n</code> elementos con un valor de 1 <code>m</code> elementos con un valor de 0. <br><br>  En <code>xs</code> <code>infer</code> resultados de llamar al m√©todo <code>infer</code> para im√°genes individuales.  Tenga en cuenta que para cada imagen, llamamos al m√©todo <code>readInput</code> .  Aqu√≠ est√° su implementaci√≥n: <br><br><pre> <code class="python hljs">export const readInput = img =&gt; imageToInput(readImage(img), TotalChannels); const readImage = path =&gt; jpeg.decode(fs.readFileSync(path), true); const imageToInput = image =&gt; { const values = serializeImage(image); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> tf.tensor3d(values, [image.height, image.width, <span class="hljs-number"><span class="hljs-number">3</span></span>], <span class="hljs-string"><span class="hljs-string">'int32'</span></span>); }; const serializeImage = image =&gt; { const totalPixels = image.width * image.height; const result = new Int32Array(totalPixels * <span class="hljs-number"><span class="hljs-number">3</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (let i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; totalPixels; i++) { result[i * <span class="hljs-number"><span class="hljs-number">3</span></span> + <span class="hljs-number"><span class="hljs-number">0</span></span>] = image.data[i * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">0</span></span>]; result[i * <span class="hljs-number"><span class="hljs-number">3</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>] = image.data[i * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>]; result[i * <span class="hljs-number"><span class="hljs-number">3</span></span> + <span class="hljs-number"><span class="hljs-number">2</span></span>] = image.data[i * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">2</span></span>]; } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> result; };</code> </pre> <br>  <code>readInput</code> primero llama a la funci√≥n <code>readImage</code> , y luego delega su llamada a <code>imageToInput</code> .  La funci√≥n <code>readImage</code> lee una imagen del disco y luego decodifica jpg del b√∫fer utilizando el paquete <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">jpeg-js</a> .  En <code>imageToInput</code> convertimos la imagen en un tensor tridimensional. <br><br>  Como resultado, para cada <code>i</code> de <code>0</code> a <code>TotalImages</code> deber√≠a ser <code>ys[i]</code> igual a <code>1</code> si <code>xs[i]</code> corresponde a la imagen con un hit, y <code>0</code> caso contrario. <br><br><h1>  Entrenamiento modelo </h1><br>  ¬°Ahora el modelo est√° listo para entrenar!  Llame al m√©todo de <code>fit</code> : <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">await</span></span> model.fit(xs, ys, { epochs: Epochs, batchSize: parseInt(((punches.length + others.length) * BatchSize).toFixed(<span class="hljs-number"><span class="hljs-number">0</span></span>)), callbacks: { onBatchEnd: <span class="hljs-keyword"><span class="hljs-keyword">async</span></span> (_, logs) =&gt; { console.log(<span class="hljs-string"><span class="hljs-string">'Cost: %s, accuracy: %s'</span></span>, logs.loss.toFixed(<span class="hljs-number"><span class="hljs-number">5</span></span>), logs.acc.toFixed(<span class="hljs-number"><span class="hljs-number">5</span></span>)); <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> tf.nextFrame(); } } });</code> </pre> <br>  Las llamadas de c√≥digo anteriores se <code>fit</code> a tres argumentos: <code>xs</code> , ys y el objeto de configuraci√≥n.  En el objeto de configuraci√≥n, establecemos cu√°ntas eras se entrenar√° el modelo, el tama√±o del paquete y la devoluci√≥n de llamada que generar√° TensorFlow.js despu√©s de procesar cada paquete. <br><br>  El tama√±o del paquete determina <code>xs</code> e <code>ys</code> para entrenar al modelo en una era.  Para cada era, TensorFlow.js seleccionar√° un subconjunto de <code>xs</code> y los elementos correspondientes de <code>ys</code> , realizar√° una distribuci√≥n directa, recibir√° la salida de la capa con activaci√≥n <code>sigmoid</code> y luego, bas√°ndose en la p√©rdida, realizar√° la optimizaci√≥n utilizando el algoritmo de <code>adam</code> . <br><br>  Despu√©s de comenzar la secuencia de comandos de entrenamiento, ver√° un resultado similar al siguiente: <br><br><pre>  Costo: 0.84212, precisi√≥n: 1.00000
 eta = 0.3&gt; ---------- acc = 1.00 p√©rdida = 0.84 Costo: 0.79740, precisi√≥n: 1.00000
 eta = 0.2 =&gt; --------- acc = 1.00 p√©rdida = 0.80 Costo: 0.81533, precisi√≥n: 1.00000
 eta = 0.2 ==&gt; -------- acc = 1.00 p√©rdida = 0.82 Costo: 0.64303, precisi√≥n: 0.50000
 eta = 0.2 ===&gt; ------- acc = 0.50 p√©rdida = 0.64 Costo: 0.51377, precisi√≥n: 0.00000
 eta = 0.2 ====&gt; ------ acc = 0.00 p√©rdida = 0.51 Costo: 0.46473, precisi√≥n: 0.50000
 eta = 0.1 =====&gt; ----- acc = 0.50 p√©rdida = 0.46 Costo: 0.50872, precisi√≥n: 0.00000
 eta = 0.1 ======&gt; ---- acc = 0.00 p√©rdida = 0.51 Costo: 0.62556, precisi√≥n: 1.00000
 eta = 0.1 =======&gt; --- acc = 1.00 p√©rdida = 0.63 Costo: 0.65133, precisi√≥n: 0.50000
 eta = 0.1 ========&gt; - acc = 0.50 p√©rdida = 0.65 Costo: 0.63824, precisi√≥n: 0.50000
 eta = 0.0 ===========&gt;
 293 ms 14675us / paso - acc = 0,60 p√©rdida = 0,65
 √âpoca 3/50
 Costo: 0.44661, precisi√≥n: 1.00000
 eta = 0.3&gt; ---------- acc = 1.00 p√©rdida = 0.45 Costo: 0.78060, precisi√≥n: 1.00000
 eta = 0.3 =&gt; --------- acc = 1.00 p√©rdida = 0.78 Costo: 0.79208, precisi√≥n: 1.00000
 eta = 0.3 ==&gt; -------- acc = 1.00 p√©rdida = 0.79 Costo: 0.49072, precisi√≥n: 0.50000
 eta = 0.2 ===&gt; ------- acc = 0.50 p√©rdida = 0.49 Costo: 0.62232, precisi√≥n: 1.00000
 eta = 0.2 ====&gt; ------ acc = 1.00 p√©rdida = 0.62 Costo: 0.82899, precisi√≥n: 1.00000
 eta = 0.2 =====&gt; ----- acc = 1.00 p√©rdida = 0.83 Costo: 0.67629, precisi√≥n: 0.50000
 eta = 0.1 ======&gt; ---- acc = 0.50 p√©rdida = 0.68 Costo: 0.62621, precisi√≥n: 0.50000
 eta = 0.1 =======&gt; --- acc = 0.50 p√©rdida = 0.63 Costo: 0.46077, precisi√≥n: 1.00000
 eta = 0.1 ========&gt; - acc = 1.00 p√©rdida = 0.46 Costo: 0.62076, precisi√≥n: 1.00000
 eta = 0.0 ===========&gt;
 304ms 15221us / paso - acc = 0.85 p√©rdida = 0.63 </pre><br>  Observe c√≥mo la precisi√≥n aumenta con el tiempo y la p√©rdida disminuye. <br><br>  En mi conjunto de datos, el modelo despu√©s del entrenamiento mostr√≥ una precisi√≥n del 92%.  Tenga en cuenta que la precisi√≥n puede no ser muy alta debido al peque√±o conjunto de datos de entrenamiento. <br><br><h1>  Ejecutando el modelo en un navegador </h1><br>  En la secci√≥n anterior, entrenamos el modelo de clasificaci√≥n binaria.  ¬°Ahora ejec√∫telo en un navegador y con√©ctese a <a href="">MK.js</a> ! <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> video = <span class="hljs-built_in"><span class="hljs-built_in">document</span></span>.getElementById(<span class="hljs-string"><span class="hljs-string">'cam'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> Layer = <span class="hljs-string"><span class="hljs-string">'global_average_pooling2d_1'</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> mobilenetInfer = <span class="hljs-function"><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">m</span></span></span><span class="hljs-function"> =&gt;</span></span> (p): tf.Tensor&lt;tf.Rank&gt; =&gt; m.infer(p, Layer); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> canvas = <span class="hljs-built_in"><span class="hljs-built_in">document</span></span>.getElementById(<span class="hljs-string"><span class="hljs-string">'canvas'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> scale = <span class="hljs-built_in"><span class="hljs-built_in">document</span></span>.getElementById(<span class="hljs-string"><span class="hljs-string">'crop'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> ImageSize = { <span class="hljs-attr"><span class="hljs-attr">Width</span></span>: <span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-attr"><span class="hljs-attr">Height</span></span>: <span class="hljs-number"><span class="hljs-number">56</span></span> }; navigator.mediaDevices .getUserMedia({ <span class="hljs-attr"><span class="hljs-attr">video</span></span>: <span class="hljs-literal"><span class="hljs-literal">true</span></span>, <span class="hljs-attr"><span class="hljs-attr">audio</span></span>: <span class="hljs-literal"><span class="hljs-literal">false</span></span> }) .then(<span class="hljs-function"><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">stream</span></span></span><span class="hljs-function"> =&gt;</span></span> { video.srcObject = stream; });</code> </pre> <br>  Hay varias declaraciones en el c√≥digo anterior: <br><br><ul><li>  <code>video</code> contiene un enlace al elemento de <code>HTML5 video</code> en la p√°gina </li><li> <code>Layer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> contiene el nombre de la capa de MobileNet de la que queremos obtener la salida y pasarla como entrada para nuestro modelo </font></font></li><li> <code>mobilenetInfer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- una funci√≥n que toma una instancia de MobileNet y devuelve otra funci√≥n. </font><font style="vertical-align: inherit;">La funci√≥n devuelta acepta la entrada y devuelve la salida correspondiente de la capa MobileNet especificada.</font></font></li><li> <code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">indica el elemento </font></font><code>HTML5 canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">que usaremos para extraer fotogramas del video</font></font></li><li> <code>scale</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- otro </font></font><code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">que se usa para escalar cuadros individuales</font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Despu√©s de eso, obtenemos la transmisi√≥n de video de la c√°mara del usuario y la configuramos como la fuente del elemento </font></font><code>video</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El siguiente paso es implementar un filtro de escala de grises que acepte </font></font><code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y convierta su contenido:</font></font><br><br><pre> <code class="python hljs">const grayscale = (canvas: HTMLCanvasElement) =&gt; { const imageData = canvas.getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>).getImageData(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, canvas.width, canvas.height); const data = imageData.data; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (let i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; data.length; i += <span class="hljs-number"><span class="hljs-number">4</span></span>) { const avg = (data[i] + data[i + <span class="hljs-number"><span class="hljs-number">1</span></span>] + data[i + <span class="hljs-number"><span class="hljs-number">2</span></span>]) / <span class="hljs-number"><span class="hljs-number">3</span></span>; data[i] = avg; data[i + <span class="hljs-number"><span class="hljs-number">1</span></span>] = avg; data[i + <span class="hljs-number"><span class="hljs-number">2</span></span>] = avg; } canvas.getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>).putImageData(imageData, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>); };</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Como siguiente paso, conectaremos el modelo con MK.js: </font></font><br><br><pre> <code class="python hljs">let mobilenet: (p: any) =&gt; tf.Tensor&lt;tf.Rank&gt;; tf.loadModel(<span class="hljs-string"><span class="hljs-string">'http://localhost:5000/model.json'</span></span>).then(model =&gt; { mobileNet .load() .then((mn: any) =&gt; mobilenet = mobilenetInfer(mn)) .then(startInterval(mobilenet, model)); });</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En el c√≥digo anterior, primero cargamos el modelo que hemos entrenado anteriormente y luego descargamos MobileNet. Pasamos MobileNet al m√©todo </font></font><code>mobilenetInfer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">para calcular la salida de la capa de red oculta. Despu√©s de eso, llamamos al m√©todo </font></font><code>startInterval</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">con dos redes como argumentos.</font></font><br><br><pre> <code class="python hljs">const startInterval = (mobilenet, model) =&gt; () =&gt; { setInterval(() =&gt; { canvas.getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>).drawImage(video, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>); grayscale(scale .getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>) .drawImage( canvas, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, canvas.width, canvas.width / (ImageSize.Width / ImageSize.Height), <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, ImageSize.Width, ImageSize.Height )); const [punching] = Array.<span class="hljs-keyword"><span class="hljs-keyword">from</span></span>(( model.predict(mobilenet(tf.fromPixels(scale))) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor1D) .dataSync() <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> Float32Array); const detect = (window <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> any).Detect; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (punching &gt;= <span class="hljs-number"><span class="hljs-number">0.4</span></span>) detect &amp;&amp; detect.onPunch(); }, <span class="hljs-number"><span class="hljs-number">100</span></span>); };</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¬°La parte m√°s interesante comienza en el m√©todo </font></font><code>startInterval</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">! Primero, ejecutamos un intervalo donde todos </font></font><code>100ms</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">llaman a una funci√≥n an√≥nima. En √©l, el </font></font><code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">video con el cuadro actual se muestra </font><font style="vertical-align: inherit;">primero encima </font><font style="vertical-align: inherit;">. Luego reducimos el tama√±o del marco </font></font><code>100x56</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y le aplicamos un filtro de escala de grises. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El siguiente paso es transferir el marco a MobileNet, obtener el resultado de la capa oculta deseada y transferirlo como entrada al m√©todo de </font></font><code>predict</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">nuestro modelo. Eso devuelve un tensor con un elemento. Usando, </font></font><code>dataSync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">obtenemos el valor del tensor y lo asignamos a una constante </font></font><code>punching</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Finalmente, verificamos: si la probabilidad de un golpe de mano excede </font></font><code>0.4</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, entonces llamamos al m√©todo de </font></font><code>onPunch</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">objeto global </font></font><code>Detect</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. MK.js proporciona un objeto global con tres m√©todos:</font></font><code>onKick</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>onPunch</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y </font></font><code>onStand</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">que podemos usar para controlar uno de los personajes.</font></font><br><br>  Hecho<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Aqu√≠ est√° el resultado! </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/83e/05c/e0e/83e05ce0e9304865bb6aee072204902b.gif"><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Reconocimiento de patadas y brazos con clasificaci√≥n N </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En la siguiente secci√≥n, crearemos un modelo m√°s inteligente: una red neuronal que reconoce golpes, patadas y otras im√°genes. </font><font style="vertical-align: inherit;">Esta vez, comencemos preparando el conjunto de entrenamiento:</font></font><br><br><pre> <code class="python hljs">const punches = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Punches) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Punches}/${f}`); const kicks = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Kicks) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Kicks}/${f}`); const others = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Others) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Others}/${f}`); const ys = tf.tensor2d( new Array(punches.length) .fill([<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>]) .concat(new Array(kicks.length).fill([<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>])) .concat(new Array(others.length).fill([<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>])), [punches.length + kicks.length + others.length, <span class="hljs-number"><span class="hljs-number">3</span></span>] ); const xs: tf.Tensor2D = tf.stack( punches .map((path: string) =&gt; mobileNet(readInput(path))) .concat(kicks.map((path: string) =&gt; mobileNet(readInput(path)))) .concat(others.map((path: string) =&gt; mobileNet(readInput(path)))) ) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor2D;</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como antes, primero leemos los cat√°logos con im√°genes de golpes a mano, pies y otras im√°genes. Despu√©s de esto, a diferencia de la √∫ltima vez, formamos el resultado esperado en forma de un tensor bidimensional, y no unidimensional. Si tenemos </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">n</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> im√°genes con una patada, </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> im√°genes con una patada </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">yk</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> otras im√°genes, entonces el tensor </font></font><code>ys</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tendr√° </font></font><code>n</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">elementos con un valor </font></font><code>[1, 0, 0]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>m</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">elementos con un valor </font></font><code>[0, 1, 0]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y </font></font><code>k</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">elementos con un valor </font></font><code>[0, 0, 1]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un vector de </font></font><code>n</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">elementos en el que hay </font></font><code>n - 1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">elementos con un valor </font></font><code>0</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y un elemento con un valor </font></font><code>1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, lo llamamos un vector unitario (vector de un solo hot). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Despu√©s de eso, formamos el tensor de entrada</font></font><code>xs</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">apilando la salida de cada imagen de MobileNet. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aqu√≠ debe actualizar la definici√≥n del modelo:</font></font><br><br><pre> <code class="python hljs">const model = tf.sequential(); model.add(tf.layers.inputLayer({ inputShape: [<span class="hljs-number"><span class="hljs-number">1024</span></span>] })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">1024</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'relu'</span></span> })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">3</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'softmax'</span></span> })); <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> model.compile({ optimizer: tf.train.adam(<span class="hljs-number"><span class="hljs-number">1e-6</span></span>), loss: tf.losses.sigmoidCrossEntropy, metrics: [<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>] });</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Las √∫nicas dos diferencias con respecto al modelo anterior son: </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> El n√∫mero de unidades en la capa de salida. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Activaciones en la capa de salida </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Hay tres unidades en la capa de salida, porque tenemos tres categor√≠as diferentes de im√°genes: </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Golpe de mano </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Patada </font></font></li><li>  Otros </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La activaci√≥n se activa en estas tres unidades </font></font><code>softmax</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, que convierte sus par√°metros en un tensor con tres valores. ¬øPor qu√© tres unidades para la capa de salida? Cada uno de los tres valores para tres clases puede ser representado por dos bits: </font></font><code>00</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>01</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>10</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. La suma de los valores del tensor creado </font></font><code>softmax</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">es 1, es decir, nunca obtendremos 00, por lo que no podremos clasificar im√°genes de una de las clases. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Despu√©s de entrenar al modelo a lo largo de los </font></font><code>500</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a√±os, ¬°logr√© una precisi√≥n de aproximadamente el 92%! Esto no est√° mal, pero no olvide que la capacitaci√≥n se realiz√≥ en un peque√±o conjunto de datos. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¬°El siguiente paso es ejecutar el modelo en un navegador! Dado que la l√≥gica es muy similar a ejecutar el modelo para clasificaci√≥n binaria, eche un vistazo al √∫ltimo paso, donde la acci√≥n se selecciona en funci√≥n de la salida del modelo:</font></font><br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> [punch, kick, nothing] = <span class="hljs-built_in"><span class="hljs-built_in">Array</span></span>.from((model.predict( mobilenet(tf.fromPixels(scaled)) ) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor1D).dataSync() <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> <span class="hljs-built_in"><span class="hljs-built_in">Float32Array</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> detect = (<span class="hljs-built_in"><span class="hljs-built_in">window</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> any).Detect; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (nothing &gt;= <span class="hljs-number"><span class="hljs-number">0.4</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (kick &gt; punch &amp;&amp; kick &gt;= <span class="hljs-number"><span class="hljs-number">0.35</span></span>) { detect.onKick(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span>; } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (punch &gt; kick &amp;&amp; punch &gt;= <span class="hljs-number"><span class="hljs-number">0.35</span></span>) detect.onPunch();</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Primero llamamos a MobileNet con un marco reducido en tonos de gris, luego transferimos el resultado de nuestro modelo entrenado. </font><font style="vertical-align: inherit;">El modelo devuelve un tensor unidimensional, que convertimos a </font></font><code>Float32Array</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">c </font></font><code>dataSync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">En el siguiente paso, usamos </font></font><code>Array.from</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">para convertir una matriz escrita en una matriz de JavaScript. </font><font style="vertical-align: inherit;">Luego extraemos las probabilidades de que un disparo con una mano, una patada o nada est√© presente en el cuadro. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Si la probabilidad del tercer resultado excede </font></font><code>0.4</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, regresamos. </font><font style="vertical-align: inherit;">De lo contrario, si la probabilidad de una patada es mayor </font></font><code>0.32</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, enviamos un comando de patada a MK.js. </font><font style="vertical-align: inherit;">Si la probabilidad de una patada es mayor </font></font><code>0.32</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y mayor que la probabilidad de una patada, entonces enviamos la acci√≥n de una patada. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En general, eso es todo! </font><font style="vertical-align: inherit;">El resultado se muestra a continuaci√≥n:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/168/f71/f3d/168f71f3df8d267bec3e0791d5857c64.gif"><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Reconocimiento de la acci√≥n </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Si recopila un conjunto de datos amplio y variado sobre personas que golpean con las manos y los pies, puede construir un modelo que funcione muy bien en cuadros individuales. ¬øPero es eso suficiente? ¬øQu√© pasa si queremos ir a√∫n m√°s lejos y distinguir dos tipos diferentes de patadas: desde un giro y desde un retroceso (retroceso). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como se puede ver en los cuadros a continuaci√≥n, en un cierto punto en el tiempo desde un √°ngulo determinado, ambos trazos se ven iguales: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/6c1/567/5bf/6c15675bf7b8c238e7ce9d5aaefeea80.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/a60/e3c/dba/a60e3cdba0eb3ecbc8730c39bc6c95b2.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pero si observa el rendimiento, los movimientos son completamente diferentes: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/e72/28b/fe8/e7228bfe8cfe9bbe73f9011d94778a7a.gif"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¬øc√≥mo puede entrenar una red neuronal para analizar la secuencia de cuadros, y no solo un cuadro? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para este prop√≥sito, podemos explorar otra clase de redes neuronales, llamadas redes neuronales recurrentes (RNN). Por ejemplo, los RNN son excelentes para trabajar con series de tiempo:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Procesamiento del lenguaje natural (PNL), donde cada palabra depende de anteriores y posteriores </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Predecir la p√°gina siguiente en funci√≥n de su historial de navegaci√≥n </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Reconocimiento de cuadros </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> La implementaci√≥n de dicho modelo est√° m√°s all√° del alcance de este art√≠culo, pero veamos una arquitectura de ejemplo para tener una idea de c√≥mo funcionar√° todo esto en conjunto. </font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> El poder de RNN </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El siguiente diagrama muestra el modelo de reconocimiento de acciones: </font></font><br><br><img src="https://habrastorage.org/webt/kz/oq/ie/kzoqieod8t9nhs_taapnhpr_y0c.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tomamos los √∫ltimos </font></font><code>n</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cuadros del video y los transferimos a CNN. </font><font style="vertical-align: inherit;">La salida CNN para cada trama se transmite como entrada RNN. </font><font style="vertical-align: inherit;">Una red neuronal recurrente determinar√° las relaciones entre cuadros individuales y reconocer√° a qu√© acci√≥n corresponden.</font></font><br><br><h1>  Conclusi√≥n </h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En este art√≠culo, desarrollamos un modelo de clasificaci√≥n de im√°genes. Para este prop√≥sito, recopilamos un conjunto de datos: extrajimos cuadros de video y los dividimos manualmente en tres categor√≠as. Luego, los datos se </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">aumentaron</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> agregando im√°genes usando </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;">imgaug</font></a><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Despu√©s de eso, explicamos qu√© es la transferencia de aprendizaje y utilizamos el modelo capacitado de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MobileNet del</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> paquete </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;">@ tensorflow-models / mobilenet</font></a><font style="vertical-align: inherit;"> para nuestros propios fines </font><font style="vertical-align: inherit;">. Descargamos MobileNet de un archivo en el proceso Node.js y capacitamos a una capa densa adicional donde los datos se alimentaron desde la capa oculta de MobileNet. ¬°Despu√©s del entrenamiento, logramos una precisi√≥n de m√°s del 90%! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para usar este modelo en un navegador, lo descargamos junto con MobileNet y comenzamos a categorizar marcos de la c√°mara web del usuario cada 100 ms. Conectamos el modelo con el juego.</font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MK.js</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y utiliz√≥ la salida del modelo para controlar uno de los caracteres. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Finalmente, vimos c√≥mo mejorar el modelo combin√°ndolo con una red neuronal recurrente para reconocer acciones. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¬°Espero que hayas disfrutado este peque√±o proyecto no menos que yo! </font><font style="vertical-align: inherit;">‚Äç</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es428019/">https://habr.com/ru/post/es428019/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es428003/index.html">Dise√±o receptivo: mantener la forma de los elementos de marcado</a></li>
<li><a href="../es428005/index.html">Tres formas efectivas de exacerbar un desastre de relaciones p√∫blicas</a></li>
<li><a href="../es428007/index.html">Ya no es una PC con capacidad de carga, a√∫n no es una computadora port√°til: Laptop TOSHIBA T3100 / 20</a></li>
<li><a href="../es428009/index.html">Equifax: un a√±o despu√©s de la mayor fuga de datos</a></li>
<li><a href="../es428011/index.html">Canciones de zombies espaciales</a></li>
<li><a href="../es428021/index.html">Sellos contra la red neuronal. O seleccione y ejecute una red neuronal para reconocer objetos en Raspberry Zero</a></li>
<li><a href="../es428023/index.html">Conceptos b√°sicos de seguridad el√©ctrica en el dise√±o de dispositivos electr√≥nicos.</a></li>
<li><a href="../es428025/index.html">Conexi√≥n de un archivo de intercambio (SWAP) en MAC OS X cuando se utiliza un SSD externo como sistema</a></li>
<li><a href="../es428027/index.html">C√≥mo intent√© hacer un analizador est√°tico GLSL (y qu√© sali√≥ mal)</a></li>
<li><a href="../es428029/index.html">Eventos digitales en Mosc√∫ del 29 de octubre al 4 de noviembre.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>