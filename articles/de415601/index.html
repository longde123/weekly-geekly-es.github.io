<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôçüèø ‚úäüèº ü¶à Kubernetes HA-Cluster mit Containerd. Oder gibt es ein Leben ohne Hafenarbeiter? üëï üö≠ üëù</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Stellen Sie Kubernetes HA mit Containerd bereit 



 Guten Tag liebe Leser von Habr! Am 24. Mai 2018 wurde im offiziellen Kubernetes-Blog ein Artikel ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Kubernetes HA-Cluster mit Containerd. Oder gibt es ein Leben ohne Hafenarbeiter?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/415601/"><h1 id="razvertyvaenie-kubernetes-ha-s-containerd">  Stellen Sie Kubernetes HA mit Containerd bereit </h1><br><p><img src="https://habrastorage.org/webt/0p/w3/7g/0pw37gyankmz9a8s2gcshvto_ek.png"><br>  Guten Tag liebe Leser von Habr!  Am 24. Mai 2018 wurde im offiziellen Kubernetes-Blog ein Artikel mit dem Titel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes Containerd Integration Goes GA</a> ver√∂ffentlicht, in dem angegeben wird, dass die Integration von Containerd in Kubernetes produktionsbereit ist.  Au√üerdem haben Jungs von der Firma Flant eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">√úbersetzung des Artikels ins Russische in</a> ihrem Blog ver√∂ffentlicht, um ein wenig Klarheit von sich selbst zu schaffen.  Nachdem ich die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Projektdokumentation auf Github gelesen hatte</a> , beschloss ich, Containerd auf "meiner eigenen Haut" <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zu testen</a> . </p><br><p>  Unser Unternehmen hat mehrere Projekte in der Phase "noch sehr weit von der Produktion entfernt".  So werden sie unser Experiment;  F√ºr sie haben wir uns entschlossen, einen Failover-Cluster von Kubernetes mithilfe von Containerd bereitzustellen und zu pr√ºfen, ob es ein Leben ohne Docker gibt. </p><br><p>  Wenn Sie interessiert sind, wie wir es gemacht haben und was daraus wurde, sind Sie bei cat willkommen. </p><a name="habracut"></a><br><p>  <b>Schema und Einsatzbeschreibung</b> <br><img src="https://habrastorage.org/webt/db/xm/pn/dbxmpnpsth-psiiyn_ittkfkc4a.png"></p><br><p>  Wie √ºblich beim Bereitstellen eines Clusters (dar√ºber habe ich in einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">fr√ºheren Artikel geschrieben</a> </p><div class="spoiler">  <b class="spoiler_title">keepalived - Implementierungen von VRRP (Virtual Router Redundancy Protocol) f√ºr Linux</b> <div class="spoiler_text">  Keepalived erstellt eine virtuelle IP (VIRTIP), die auf die IP eines der drei Master "verweist" (eine Subschnittstelle erstellt).  Der Keepalived-Daemon √ºberwacht den Zustand der Computer und schlie√üt im Falle eines Fehlers den ausgefallenen Server aus der Liste der aktiven Server aus, indem er VIRTIP auf die IP eines anderen Servers umschaltet. Dies entspricht der "Gewichtung", die bei der Konfiguration von Keepalived auf jedem Server angegeben wurde. </div></div><br><p>  Keepalived-Daemons kommunizieren √ºber VRRP und senden sich gegenseitig Nachrichten an die Adresse 224.0.0.18. </p><br><p>  Wenn der Nachbar seine Nachricht nicht gesendet hat, gilt er nach Ablauf der Frist als tot.  Sobald der abgest√ºrzte Server seine Nachrichten an das Netzwerk sendet, kehrt alles an seinen Platz zur√ºck </p><br><p>  Wir konfigurieren die Arbeit mit dem API-Server auf Kubernetes-Knoten wie folgt. </p><br><p>  Konfigurieren Sie nach der Installation des Clusters den Kube-Proxy und √§ndern Sie den Port von 6443 auf 16443 (Details unten).  Auf jedem der Master wird Nginx bereitgestellt, das als Loadbalancer fungiert, Port 16443 √ºberwacht und alle drei Master auf Port 6443 vorverlegt (Details siehe unten). </p><br><p>  Durch dieses Schema wurde eine erh√∂hte Fehlertoleranz sowohl bei Verwendung von Keepalived als auch bei Verwendung von Nginx erzielt. Es wurde ein Ausgleich zwischen den API-Servern auf den Assistenten erreicht. <br></p><br><p>  In einem fr√ºheren Artikel habe ich die Bereitstellung von nginx und etcd im Docker beschrieben.  In diesem Fall haben wir jedoch kein Docker, sodass Nginx und etcd lokal auf Masterknoten funktionieren. </p><br><p>  Theoretisch w√§re es m√∂glich, nginx und etcd mithilfe von Containerd bereitzustellen. Bei Problemen w√ºrde dieser Ansatz die Diagnose jedoch erschweren. Daher haben wir beschlossen, nicht zu experimentieren und lokal auszuf√ºhren. </p><br><p>  <b>Beschreibung der Server f√ºr die Bereitstellung:</b> </p><br><table><thead><tr><th>  Name </th><th>  IP </th><th>  Dienstleistungen </th></tr></thead><tbody><tr><td>  VIRTIP </td><td>  172.26.133.160 </td><td>  ------ ------. </td></tr><tr><td>  kube-master01 </td><td>  172.26.133.161 </td><td>  kubeadm, kubelet, kubectl, etcd, Containerd, Nginx, Keepalived </td></tr><tr><td>  kube-master02 </td><td>  172.26.133.162 </td><td>  kubeadm, kubelet, kubectl, etcd, Containerd, Nginx, Keepalived </td></tr><tr><td>  kube-master03 </td><td>  172.26.133.163 </td><td>  kubeadm, kubelet, kubectl, etcd, Containerd, Nginx, Keepalived </td></tr><tr><td>  kube-node01 </td><td>  172.26.133.164 </td><td>  Kubeadm, Kubelet, Kubectl, Containerd </td></tr><tr><td>  kube-node02 </td><td>  172.26.133.165 </td><td>  Kubeadm, Kubelet, Kubectl, Containerd </td></tr><tr><td>  kube-node03 </td><td>  172.26.133.166 </td><td>  Kubeadm, Kubelet, Kubectl, Containerd </td></tr></tbody></table><br><p>  <b>Installieren Sie kubeadm, kubelet, kubectl und verwandte Pakete</b> </p><br><p>  Alle Befehle werden von root ausgef√ºhrt </p><br><pre><code class="plaintext hljs">sudo -i</code> </pre> <br><pre> <code class="bash hljs">apt-get update &amp;&amp; apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl unzip tar apt-transport-https btrfs-tools libseccomp2 socat util-linux mc vim keepalived</code> </pre> <br><p>  <b>Installieren Sie conteinerd</b> <br><img src="https://habrastorage.org/webt/ul/nw/vg/ulnwvgeblmt74ivcsnsz2tuxcfa.png"></p><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> / wget https://storage.googleapis.com/cri-containerd-release/cri-containerd-1.1.0-rc.0.linux-amd64.tar.gz tar -xvf cri-containerd-1.1.0-rc.0.linux-amd64.tar.gz</code> </pre> <br><p>  <b>Containerd-Konfigurationen konfigurieren</b> </p><br><pre> <code class="bash hljs">mkdir -p /etc/containerd nano /etc/containerd/config.toml</code> </pre> <br><p>  Zur Datei hinzuf√ºgen: </p><br><pre> <code class="bash hljs">[plugins.cri] enable_tls_streaming = <span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br><p>  Wir fangen an zu pr√ºfen, ob alles in Ordnung ist </p><br><pre> <code class="bash hljs">systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> containerd systemctl start containerd systemctl status containerd ‚óè containerd.service - containerd container runtime Loaded: loaded (/etc/systemd/system/containerd.service; disabled; vendor preset: enabled) Active: active (running) since Mon 2018-06-25 12:32:01 MSK; 7s ago Docs: https://containerd.io Process: 10725 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS) Main PID: 10730 (containerd) Tasks: 15 (<span class="hljs-built_in"><span class="hljs-built_in">limit</span></span>: 4915) Memory: 14.9M CPU: 375ms CGroup: /system.slice/containerd.service ‚îî‚îÄ10730 /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/bin/containerd Jun 25 12:32:01 hb-master02 containerd[10730]: time=<span class="hljs-string"><span class="hljs-string">"2018-06-25T12:32:01+03:00"</span></span> level=info msg=<span class="hljs-string"><span class="hljs-string">"Get image filesystem path "</span></span>/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs<span class="hljs-string"><span class="hljs-string">""</span></span> Jun 25 12:32:01 hb-master02 containerd[10730]: time=<span class="hljs-string"><span class="hljs-string">"2018-06-25T12:32:01+03:00"</span></span> level=error msg=<span class="hljs-string"><span class="hljs-string">"Failed to load cni during init, please check CRI plugin status before setting up network for pods"</span></span> error=<span class="hljs-string"><span class="hljs-string">"cni con Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>loading plugin <span class="hljs-string"><span class="hljs-string">"io.containerd.grpc.v1.introspection"</span></span>...<span class="hljs-string"><span class="hljs-string">" type=io.containerd.grpc.v1 Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start subscribing containerd event<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start recovering state<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg=serving... address="</span></span>/run/containerd/containerd.sock<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>containerd successfully booted <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 0.308755s<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start event monitor<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start snapshots syncer<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start streaming server<span class="hljs-string"><span class="hljs-string">"</span></span></code> </pre> <br><p>  <b>Installieren und ausf√ºhren Sie etcd</b> </p><br><p>  Wichtiger Hinweis, ich habe den kubernetes Cluster Version 1.10 installiert.  Nur ein paar Tage sp√§ter, zum Zeitpunkt des Schreibens des Artikels, wurde Version 1.11 ver√∂ffentlicht. Wenn Sie Version 1.11 installieren, setzen Sie die Variable ETCD_VERSION = "v3.2.17", wenn 1.10, dann ETCD_VERSION = "v3.1.12". </p><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCD_VERSION=<span class="hljs-string"><span class="hljs-string">"v3.1.12"</span></span> curl -sSL https://github.com/coreos/etcd/releases/download/<span class="hljs-variable"><span class="hljs-variable">${ETCD_VERSION}</span></span>/etcd-<span class="hljs-variable"><span class="hljs-variable">${ETCD_VERSION}</span></span>-linux-amd64.tar.gz | tar -xzv --strip-components=1 -C /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/bin/</code> </pre> <br><p>  Kopieren Sie Konfigurationen aus Gitahab. </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/rjeka/k8s-containerd.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> k8s-containerd</code> </pre> <br><p>  Konfigurieren Sie die Variablen in der Konfigurationsdatei. </p><br><pre> <code class="bash hljs">vim create-config.sh</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Beschreibung der Dateivariablen create-config.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash # local machine ip address export K8SHA_IPLOCAL=172.26.133.161 # local machine etcd name, options: etcd1, etcd2, etcd3 export K8SHA_ETCDNAME=kube-master01 # local machine keepalived state config, options: MASTER, BACKUP. One keepalived cluster only one MASTER, other's are BACKUP export K8SHA_KA_STATE=MASTER # local machine keepalived priority config, options: 102, 101,100 MASTER must 102 export K8SHA_KA_PRIO=102 # local machine keepalived network interface name config, for example: eth0 export K8SHA_KA_INTF=ens18 ####################################### # all masters settings below must be same ####################################### # master keepalived virtual ip address export K8SHA_IPVIRTUAL=172.26.133.160 # master01 ip address export K8SHA_IP1=172.26.133.161 # master02 ip address export K8SHA_IP2=172.26.133.162 # master03 ip address export K8SHA_IP3=172.26.133.163 # master01 hostname export K8SHA_HOSTNAME1=kube-master01 # master02 hostname export K8SHA_HOSTNAME2=kube-master02 # master03 hostname export K8SHA_HOSTNAME3=kube-master03 # keepalived auth_pass config, all masters must be same export K8SHA_KA_AUTH=56cf8dd754c90194d1600c483e10abfr #etcd tocken: export ETCD_TOKEN=9489bf67bdfe1b3ae077d6fd9e7efefd # kubernetes cluster token, you can use 'kubeadm token generate' to get a new one export K8SHA_TOKEN=535tdi.utzk5hf75b04ht8l # kubernetes CIDR pod subnet, if CIDR pod subnet is "10.244.0.0/16" please set to "10.244.0.0\\/16" export K8SHA_CIDR=10.244.0.0\\/16</span></span></code> </pre> <br><p>  Einstellungen auf dem lokalen Computer jedes Knotens (jeder Knoten hat seinen eigenen) <br>  <b>K8SHA_IPLOCAL</b> - IP-Adresse des Knotens, auf dem das Skript konfiguriert ist <br>  <b>K8SHA_ETCDNAME</b> - lokaler Computername im ETCD-Cluster <br>  <b>K8SHA_KA_STATE</b> - Rolle in Keepalived.  Ein MASTER-Knoten, alle anderen BACKUP. <br>  <b>K8SHA_KA_PRIO</b> - Keepalived-Priorit√§t, der Master hat 102 f√ºr die verbleibenden 101, 100. Wenn der Master mit der Nummer 102 f√§llt, nimmt der Knoten mit der Nummer 101 seinen Platz ein und so weiter. <br>  <b>K8SHA_KA_INTF</b> - Keepalived-Netzwerkschnittstelle.  Der Name der Schnittstelle, die beibehalten wird, wird abgeh√∂rt. </p><br><p>  Die allgemeinen Einstellungen f√ºr alle Masterknoten sind gleich: </p><br><p>  <b>K8SHA_IPVIRTUAL</b> = 172.26.133.160 - virtuelle IP des Clusters. <br>  <b>K8SHA_IP1 ... K8SHA_IP3 - IP-</b> Adressen von Mastern <br>  <b>K8SHA_HOSTNAME1 ... K8SHA_HOSTNAME3</b> - Hostnamen f√ºr Masterknoten.  Ein wichtiger Punkt, unter diesen Namen kubeadm generiert Zertifikate. <br>  <b>K8SHA_KA_AUTH</b> - Passwort f√ºr Keepalived.  Sie k√∂nnen beliebige angeben <br>  <b>K8SHA_TOKEN</b> - Cluster-Token.  Kann mit dem Befehl <b>kubeadm token generate generiert werden</b> <br>  <b>K8SHA_CIDR</b> - <b>Subnetzadresse</b> f√ºr Herde.  Ich benutze Flanell also CIDR 0.244.0.0/16.  Achten Sie auf den Bildschirm - in der Konfiguration sollte K8SHA_CIDR = 10.244.0.0 \ / 16 sein </p></div></div><br><p>  F√ºhren Sie das Skript aus, mit dem nginx, keepalived, etcd und kubeadmin konfiguriert werden </p><br><pre> <code class="bash hljs">./create-config.sh</code> </pre> <br><p>  Wir starten etcd. </p><br>  etcd ich hob ohne tls.  Wenn Sie tls ben√∂tigen, wird in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen kubernetes-Dokumentation</a> ausf√ºhrlich beschrieben, wie Zertifikate f√ºr etcd generiert werden. <br><br><br><pre> <code class="bash hljs">systemctl daemon-reload &amp;&amp; systemctl start etcd &amp;&amp; systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> etcd</code> </pre> <br><p>  Statuspr√ºfung </p><br><pre> <code class="bash hljs">etcdctl cluster-health member ad059013ec46f37 is healthy: got healthy result from http://192.168.5.49:2379 member 4d63136c9a3226a1 is healthy: got healthy result from http://192.168.4.169:2379 member d61978cb3555071e is healthy: got healthy result from http://192.168.4.170:2379 cluster is healthy etcdctl member list ad059013ec46f37: name=hb-master03 peerURLs=http://192.168.5.48:2380 clientURLs=http://192.168.5.49:2379,http://192.168.5.49:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">false</span></span> 4d63136c9a3226a1: name=hb-master01 peerURLs=http://192.168.4.169:2380 clientURLs=http://192.168.4.169:2379,http://192.168.4.169:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">true</span></span> d61978cb3555071e: name=hb-master02 peerURLs=http://192.168.4.170:2380 clientURLs=http://192.168.4.170:2379,http://192.168.4.170:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">false</span></span></code> </pre> <br><p>  Wenn alles in Ordnung ist, fahren Sie mit dem n√§chsten Schritt fort. </p><br><p>  <b>Konfigurieren Sie kubeadmin</b> <br>  Wenn Sie kubeadm Version 1.11 verwenden, k√∂nnen Sie diesen Schritt √ºberspringen <br>  Konfigurieren Sie die kubeadmin-Konfiguration, damit kybernetes nicht mit Docker, sondern mit Containerd funktioniert </p><br><pre> <code class="bash hljs">vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> </pre> <br><p>  F√ºgen Sie nach [Service] eine Zeile zum Block hinzu </p><br><pre> <code class="bash hljs">Environment=<span class="hljs-string"><span class="hljs-string">"KUBELET_EXTRA_ARGS=--runtime-cgroups=/system.slice/containerd.service --container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock"</span></span></code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Die gesamte Konfiguration sollte folgenderma√üen aussehen:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Service] Environment="KUBELET_EXTRA_ARGS=--runtime-cgroups=/system.slice/containerd.service --container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock" Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf" Environment="KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true" Environment="KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin" Environment="KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local" Environment="KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt" Environment="KUBELET_CADVISOR_ARGS=--cadvisor-port=0" Environment="KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki" ExecStart= ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_SYSTEM_PODS_ARGS $KUBELET_NETWORK_ARGS $KUBELET_DNS_ARGS $KUBELET_AUTHZ_ARGS $KUBELET_CADVISOR_ARGS $KUBELET_CERTIFICATE_ARGS $KUBELET_EXTRA_ARGS</code> </pre> </div></div><br><p>  Wenn Sie Version 1.11 installieren und mit CoreDNS anstelle von kube-dns experimentieren und die dynamische Konfiguration testen m√∂chten, kommentieren Sie den folgenden Block in der Konfigurationsdatei kubeadm-init.yaml aus: </p><br><pre> <code class="bash hljs">feature-gates: DynamicKubeletConfig: <span class="hljs-literal"><span class="hljs-literal">true</span></span> CoreDNS: <span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br><p>  Kubelet neu starten </p><br><pre> <code class="plaintext hljs">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</code> </pre> <br><p>  <b>Initialisierung des ersten Assistenten</b> </p><br><p>  Bevor Sie kubeadm starten, m√ºssen Sie keepalived neu starten und seinen Status √ºberpr√ºfen </p><br><pre> <code class="bash hljs">systemctl restart keepalived.service systemctl status keepalived.service ‚óè keepalived.service - Keepalive Daemon (LVS and VRRP) Loaded: loaded (/lib/systemd/system/keepalived.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2018-06-27 10:40:03 MSK; 1min 44s ago Process: 4589 ExecStart=/usr/sbin/keepalived <span class="hljs-variable"><span class="hljs-variable">$DAEMON_ARGS</span></span> (code=exited, status=0/SUCCESS) Main PID: 4590 (keepalived) Tasks: 7 (<span class="hljs-built_in"><span class="hljs-built_in">limit</span></span>: 4915) Memory: 15.3M CPU: 968ms CGroup: /system.slice/keepalived.service ‚îú‚îÄ4590 /usr/sbin/keepalived ‚îú‚îÄ4591 /usr/sbin/keepalived ‚îú‚îÄ4593 /usr/sbin/keepalived ‚îú‚îÄ5222 /usr/sbin/keepalived ‚îú‚îÄ5223 sh -c /etc/keepalived/check_apiserver.sh ‚îú‚îÄ5224 /bin/bash /etc/keepalived/check_apiserver.sh ‚îî‚îÄ5231 sleep 5</code> </pre> <br><p>  √úberpr√ºfen Sie, ob VIRTIP pingt </p><br><pre> <code class="bash hljs">ping -c 4 172.26.133.160 PING 172.26.133.160 (172.26.133.160) 56(84) bytes of data. 64 bytes from 172.26.133.160: icmp_seq=1 ttl=64 time=0.030 ms 64 bytes from 172.26.133.160: icmp_seq=2 ttl=64 time=0.050 ms 64 bytes from 172.26.133.160: icmp_seq=3 ttl=64 time=0.050 ms 64 bytes from 172.26.133.160: icmp_seq=4 ttl=64 time=0.056 ms --- 172.26.133.160 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3069ms rtt min/avg/max/mdev = 0.030/0.046/0.056/0.012 ms</code> </pre> <br><p>  F√ºhren Sie danach kubeadmin aus.  Stellen Sie sicher, dass Sie die Zeile --skip-preflight-pr√ºfungen einf√ºgen.  Kubeadmin sucht standardm√§√üig nach Docker und schl√§gt ohne √úberspringen mit einem Fehler fehl. </p><br><pre> <code class="bash hljs">kubeadm init --config=kubeadm-init.yaml --skip-preflight-checks</code> </pre> <br><p>  Speichern Sie die generierte Zeile, nachdem kubeadm funktioniert hat.  Es wird ben√∂tigt, um Arbeitsknoten in den Cluster einzugeben. </p><br><pre> <code class="bash hljs">kubeadm join 172.26.133.160:6443 --token XXXXXXXXXXXXXXXXXXXXXXXXX --discovery-token-ca-cert-hash sha256:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX</code> </pre> <br><p>  Geben Sie als N√§chstes an, wo die Datei admin.conf gespeichert ist <br>  Wenn wir als root arbeiten, dann: </p><br><pre> <code class="bash hljs">vim ~/.bashrc <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> KUBECONFIG=/etc/kubernetes/admin.conf <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> ~/.bashrc</code> </pre> <br><p>  Befolgen Sie f√ºr einen einfachen Benutzer die Anweisungen auf dem Bildschirm. </p><br><pre> <code class="bash hljs">mkdir -p <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube sudo cp -i /etc/kubernetes/admin.conf <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube/config sudo chown $(id -u):$(id -g) <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube/config</code> </pre> <br><p>  F√ºgen Sie dem Cluster zwei weitere Assistenten hinzu.  Kopieren Sie dazu die Zertifikate von kube-master01 nach kube-master02 und kube-master03 in das Verzeichnis / etc / kubernetes /.  Zu diesem Zweck habe ich den SSH-Zugriff f√ºr root konfiguriert und nach dem Kopieren der Dateien die Einstellungen zur√ºckgegeben. </p><br><pre> <code class="bash hljs">scp -r /etc/kubernetes/pki 172.26.133.162:/etc/kubernetes/ scp -r /etc/kubernetes/pki 172.26.133.163:/etc/kubernetes/</code> </pre> <br><p>  F√ºhren Sie nach dem Kopieren nach kube-master02 und kube-master03 aus. </p><br><pre> <code class="bash hljs">kubeadm init --config=kubeadm-init.yaml --skip-preflight-checks</code> </pre> <br><p>  <b>Installieren Sie das CIDR-Flanell</b> </p><br><p>  auf kube-master01 ausf√ºhren </p><br><pre> <code class="bash hljs">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml</code> </pre> <br><p>  Die aktuelle Version von Flanel finden Sie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation zu Kubernetes</a> . </p><br><p>  Wir warten, bis alle Container erstellt sind. </p><br><pre> <code class="bash hljs">watch -n1 kubectl get pods --all-namespaces -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE kube-system kube-apiserver-kube-master01 1/1 Running 0 17m 172.26.133.161 kube-master01 kube-system kube-apiserver-kube-master02 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-apiserver-kube-master03 1/1 Running 0 6m 172.26.133.163 kube-master03 kube-system kube-controller-manager-kube-master01 1/1 Running 0 17m 172.26.133.161 kube-master01 kube-system kube-controller-manager-kube-master02 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-controller-manager-kube-master03 1/1 Running 0 6m 172.26.133.163 kube-master03 kube-system kube-dns-86f4d74b45-8c24s 3/3 Running 0 17m 10.244.2.2 kube-master03 kube-system kube-flannel-ds-4h4w7 1/1 Running 0 2m 172.26.133.163 kube-master03 kube-system kube-flannel-ds-kf5mj 1/1 Running 0 2m 172.26.133.162 kube-master02 kube-system kube-flannel-ds-q6k4z 1/1 Running 0 2m 172.26.133.161 kube-master01 kube-system kube-proxy-9cjtp 1/1 Running 0 6m 172.26.133.163 kube-master03 kube-system kube-proxy-9sqk2 1/1 Running 0 17m 172.26.133.161 kube-master01 kube-system kube-proxy-jg2pt 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-scheduler-kube-master01 1/1 Running 0 18m 172.26.133.161 kube-master01 kube-system kube-scheduler-kube-master02 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-scheduler-kube-master03 1/1 Running 0 6m 172.26.133.163 kube-master03</code> </pre> <br><p>  <b>Wir replizieren kube-dns an alle drei Master</b> </p><br><p>  Auf kube-master01 ausf√ºhren </p><br><pre> <code class="bash hljs">kubectl scale --replicas=3 -n kube-system deployment/kube-dns</code> </pre> <br><p>  <b>Installieren und konfigurieren Sie nginx</b> </p><br><p>  Installieren Sie auf jedem Masterknoten nginx als Balancer f√ºr die Kubernetes-API <br>  Ich habe alle Cluster-Maschinen auf Debian.  Von den Nginx-Paketen wird das Stream-Modul nicht unterst√ºtzt. F√ºgen Sie daher die Nginx-Repositorys hinzu und installieren Sie sie aus den Nginx-Repositorys.  Wenn Sie ein anderes Betriebssystem haben, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">lesen</a> Sie die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Nginx-Dokumentation</a> . </p><br><pre> <code class="bash hljs">wget https://nginx.org/keys/nginx_signing.key sudo apt-key add nginx_signing.key <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> -e <span class="hljs-string"><span class="hljs-string">"\n#nginx\n\ deb http://nginx.org/packages/debian/ stretch nginx\n\ deb-src http://nginx.org/packages/debian/ stretch nginx"</span></span> &gt;&gt; /etc/apt/sources.list apt-get update &amp;&amp; apt-get install nginx -y</code> </pre> <br><p>  Nginx-Konfiguration erstellen (falls noch nicht erstellt) </p><br><pre> <code class="bash hljs">./create-config.sh</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">nginx.conf</b> <div class="spoiler_text"><p>  Benutzer nginx; <br>  worker_processes auto; </p><br><p>  error_log /var/log/nginx/error.log warn; <br>  pid /var/run/nginx.pid; </p><br><p>  Ereignisse { <br>  worker_connections 1024; <br>  }} </p><br><p>  http { <br>  include /etc/nginx/mime.types; <br>  default_type application / octet-stream; </p><br><pre> <code class="plaintext hljs">log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf;</code> </pre> <br><p>  }} </p><br><p>  stream { <br>  Upstream-Apiserver { <br>  Server 172.26.133.161:6443 weight = 5 max_fails = 3 fail_timeout = 30s; <br>  Server 172.26.133.162:6443 weight = 5 max_fails = 3 fail_timeout = 30s; <br>  Server 172.26.133.163:6443 weight = 5 max_fails = 3 fail_timeout = 30s; </p><br><pre> <code class="plaintext hljs">} server { listen 16443; proxy_connect_timeout 1s; proxy_timeout 3s; proxy_pass apiserver; }</code> </pre> <br><p>  }} </p></div></div><br><p>  Wir pr√ºfen, ob alles in Ordnung ist und wenden die Konfiguration an </p><br><pre> <code class="bash hljs">nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf <span class="hljs-built_in"><span class="hljs-built_in">test</span></span> is successful systemctl restart nginx systemctl status nginx ‚óè nginx.service - nginx - high performance web server Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2018-06-28 08:48:09 MSK; 22s ago Docs: http://nginx.org/en/docs/ Process: 22132 ExecStart=/usr/sbin/nginx -c /etc/nginx/nginx.conf (code=exited, status=0/SUCCESS) Main PID: 22133 (nginx) Tasks: 2 (<span class="hljs-built_in"><span class="hljs-built_in">limit</span></span>: 4915) Memory: 1.6M CPU: 7ms CGroup: /system.slice/nginx.service ‚îú‚îÄ22133 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.conf ‚îî‚îÄ22134 nginx: worker process</code> </pre> <br><p>  Testen Sie den Balancer </p><br><pre> <code class="bash hljs">curl -k https://172.26.133.161:16443 | wc -l % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 233 100 233 0 0 12348 0 --:--:-- --:--:-- --:--:-- 12944</code> </pre> <br><p>  <b>Konfigurieren Sie kube-proxy f√ºr die Arbeit mit dem Balancer</b> </p><br><p>  Bearbeiten Sie nach der Konfiguration des Balancers den Port in den Kubernetes-Einstellungen. </p><br><pre> <code class="bash hljs">kubectl edit -n kube-system configmap/kube-proxy</code> </pre> <br><p>  √Ñndern Sie die Servereinstellungen in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://172.26.133.160:16443</a> <br>  Als N√§chstes m√ºssen Sie kube-proxy so konfigurieren, dass es mit dem neuen Port funktioniert </p><br><pre> <code class="bash hljs">kubectl get pods --all-namespaces -o wide | grep proxy kube-system kube-proxy-9cjtp 1/1 Running 1 22h 172.26.133.163 kube-master03 kube-system kube-proxy-9sqk2 1/1 Running 1 22h 172.26.133.161 kube-master01 kube-system kube-proxy-jg2pt 1/1 Running 4 22h 172.26.133.162 kube-</code> </pre> <br><p>  Wir l√∂schen alle Pods, nach dem Entfernen werden sie automatisch mit den neuen Einstellungen neu erstellt </p><br><pre> <code class="bash hljs">kubectl delete pod -n kube-system kube-proxy-XXX ```bash    .      ```bash kubectl get pods --all-namespaces -o wide | grep proxy kube-system kube-proxy-hqrsw 1/1 Running 0 33s 172.26.133.161 kube-master01 kube-system kube-proxy-kzvw5 1/1 Running 0 47s 172.26.133.163 kube-master03 kube-system kube-proxy-zzkz5 1/1 Running 0 7s 172.26.133.162 kube-master02</code> </pre> <br><p>  <b>Hinzuf√ºgen von Arbeitsknoten zum Cluster</b> </p><br><p>  F√ºhren Sie f√ºr jeden Grundton den von kubeadm generierten Befehl aus </p><br><pre> <code class="bash hljs">kubeadm join 172.26.133.160:6443 --token XXXXXXXXXXXXXXXXXXXXXXXXX --discovery-token-ca-cert-hash sha256:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX --cri-socket /run/containerd/containerd.sock --skip-preflight-checks</code> </pre> <br><p>  Wenn die Zeile "verloren" ist, m√ºssen Sie eine neue generieren </p><br><pre> <code class="bash hljs">kubeadm token generate kubeadm token create &lt;generated-token&gt; --<span class="hljs-built_in"><span class="hljs-built_in">print</span></span>-join-command --ttl=0</code> </pre> <br><p>  Auf Arbeitsknoten in den Dateien /etc/kubernetes/bootstrap-kubelet.conf und /etc/kubernetes/kubelet.conf <br>  Servervariablenwert zu unserer Virtip </p><br><pre> <code class="bash hljs">vim /etc/kubernetes/bootstrap-kubelet.conf server: https://172.26.133.60:16443 vim /etc/kubernetes/kubelet.conf server: https://172.26.133.60:16443</code> </pre> <br><p>  Und starten Sie Containerd und Kubernetes neu </p><br><pre> <code class="bash hljs">systemctl restart containerd kubelet</code> </pre> <br><p>  <b>Dashboard-Installation</b> </p><br><pre> <code class="bash hljs">kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml</code> </pre> <br><p>  Erstellen Sie einen Benutzer mit Administratorrechten: </p><br><pre> <code class="bash hljs">kubectl apply -f kube-dashboard/dashboard-adminUser.yaml</code> </pre> <br><p>  Wir bekommen den Token f√ºr den Eintritt: </p><br><pre> <code class="bash hljs">kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk <span class="hljs-string"><span class="hljs-string">'{print $1}'</span></span>)</code> </pre> <br><p>  Konfigurieren des Dashboard-Zugriffs √ºber NodePort in VIRTIP </p><br><pre> <code class="bash hljs">kubectl -n kube-system edit service kubernetes-dashboard</code> </pre> <br><p>  Wir ersetzen den Wert von Typ: ClusterIP durch Typ: NodePort und f√ºgen im Abschnitt Port den Wert von nodePort: 30000 hinzu (oder den Port im Bereich von 30000 bis 32000, auf den das Panel zugreifen soll): </p><br><p><img src="https://habrastorage.org/webt/fn/ql/kr/fnqlkren3ltk88xzi8fqwi4vbxa.png"></p><br><p>  Das Panel ist jetzt unter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https: // VIRTIP: 30000</a> verf√ºgbar </p><br><p>  <b>Heapster</b> </p><br><p>  Installieren Sie als N√§chstes Heapster, ein Tool zum Abrufen von Metriken f√ºr Clusterkomponenten. </p><br><p>  Installation: </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/kubernetes/heapster.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> heapster kubectl create -f deploy/kube-config/influxdb/ kubectl create -f deploy/kube-config/rbac/heapster-rbac.yaml</code> </pre> <br><p>  <b>Schlussfolgerungen</b> </p><br><p>  Ich habe keine besonderen Probleme bei der Arbeit mit Containerd bemerkt.  Einmal gab es eine unverst√§ndliche Panne mit einem Herd, nachdem der Einsatz entfernt wurde.  Kubernetes glaubte, dass unter gel√∂scht wurde, aber unter wurde so ein eigenartiger "Zombie". Es blieb auf dem Knoten existieren, aber im erweiterten Status. </p><br><p>  Ich glaube, dass Containerd eher als Container-Laufzeit f√ºr Kubernetes ausgerichtet ist.  H√∂chstwahrscheinlich wird es in Zukunft als Umgebung f√ºr den Start von Microservices in Kubernetes m√∂glich und notwendig sein, unterschiedliche Umgebungen zu verwenden, die sich an unterschiedlichen Aufgaben, Projekten usw. orientieren. </p><br><p>  Das Projekt entwickelt sich sehr schnell.  Alibaba Cloud hat begonnen, conatinerd aktiv zu nutzen, und betont, dass es die ideale Umgebung f√ºr den Betrieb von Containern ist. </p><br><p>  Laut den Entwicklern entspricht die Integration von Containerd in die Google Cloud-Plattform Kubernetes nun der Docker-Integration. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ein gutes Beispiel f√ºr das Dienstprogramm crictl console</a> .  Ich werde auch einige Beispiele aus dem erstellten Cluster geben: </p><br><pre> <code class="plaintext hljs">kubectl describe nodes | grep "Container Runtime Version:"</code> </pre> <br><p><img src="https://habrastorage.org/webt/gr/_l/of/gr_lofuou-20jmqzb800qdi5yny.png"></p><br><p>  Der Docker-CLI fehlen die grundlegenden Konzepte von Kubernetes, z. B. Pod und Namespace, w√§hrend crictl diese Konzepte unterst√ºtzt </p><br><pre> <code class="plaintext hljs">crictl pods</code> </pre> <br><p><img src="https://habrastorage.org/webt/kr/im/hl/krimhlcrmwcaxysx3wgd9mjtvuo.png"></p><br><p>  Und wenn n√∂tig, k√∂nnen wir uns die Container im √ºblichen Format wie Docker ansehen </p><br><pre> <code class="plaintext hljs">crictl ps</code> </pre> <br><p><img src="https://habrastorage.org/webt/zo/2m/ez/zo2mezfxjgohjpu8f35n-nazf74.png"></p><br><p>  Wir k√∂nnen die Bilder sehen, die sich auf dem Knoten befinden </p><br><pre> <code class="plaintext hljs">crictl images</code> </pre> <br><p><img src="https://habrastorage.org/webt/mw/u6/tu/mwu6tunxz4re5yrr0h-ajq-_3xs.png"></p><br><p>  Wie sich herausstellte, ist das Leben ohne Hafenarbeiter :) </p><br><p>  Es ist zu fr√ºh, um √ºber Fehler und St√∂rungen zu sprechen. Der Cluster arbeitet seit etwa einer Woche mit uns zusammen.  In naher Zukunft wird der Test darauf √ºbertragen, und bei Erfolg h√∂chstwahrscheinlich der Entwicklerstand eines der Projekte.  Es besteht die Idee, eine Reihe von Artikeln zu DevOps-Prozessen zu verfassen, z. B.: Erstellen eines Clusters, Einrichten eines Ingress-Controllers und Verschieben auf separate Clusterknoten, Automatisieren der Image-Assemblierung, √úberpr√ºfen von Images auf Schwachstellen, Bereitstellung usw.  In der Zwischenzeit werden wir die Stabilit√§t der Arbeit untersuchen, nach Fehlern suchen und neue Produkte entwickeln. </p><br><p>  Dieses Handbuch eignet sich auch zum Bereitstellen eines Failoverclusters mit Docker. Sie m√ºssen Docker nur gem√§√ü den Anweisungen in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen Kubernetes-Dokumentation</a> installieren und die Schritte zum Installieren von Containerd und zum Konfigurieren der Kubeadm-Konfiguration √ºberspringen. </p><br><p>  Oder Sie k√∂nnen Containerd und Docker gleichzeitig auf demselben Host platzieren und, wie die Entwickler versichern, perfekt zusammenarbeiten.  Containerd ist die Konbernetes-Containerer-Startumgebung, und Docker ist genau wie Docker))) </p><br><p><img src="https://habrastorage.org/webt/qj/f2/r2/qjf2r2vn_j4odysnyxytokycz9u.png"><br></p><br>  Das Containerd-Repository verf√ºgt √ºber ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ansible Playbook</a> zum Einrichten eines einzelnen Assistentenclusters.  F√ºr mich war es jedoch interessanter, das System mit meinen H√§nden zu ‚Äûheben‚Äú, um die Konfiguration jeder Komponente genauer zu verstehen und zu verstehen, wie es in der Praxis funktioniert. <br><p>  Vielleicht werden meine H√§nde eines Tages reichen und ich werde mein Playbook f√ºr die Bereitstellung eines Clusters mit HA schreiben, da ich in den letzten sechs Monaten mehr als ein Dutzend davon bereitgestellt habe und es wahrscheinlich an der Zeit w√§re, den Prozess zu automatisieren. </p><br><p>  W√§hrend des Schreibens dieses Artikels wurde auch die Version kubernetes 1.11 ver√∂ffentlicht.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Informationen zu</a> den wichtigsten √Ñnderungen finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">im Flant-Blog</a> oder im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen Kubernetes-Blog</a> .  Wir haben die Testcluster auf Version 1.11 aktualisiert und kube-dns durch CoreDNS ersetzt.  Dar√ºber hinaus haben wir die DynamicKubeletConfig-Funktion zum Testen der Funktionen der dynamischen Aktualisierung von Konfigurationen hinzugef√ºgt. </p><br><p>  Verwendete Materialien: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes Containerd Integration geht GA</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes Containerd-Integration ersetzt Docker f√ºr die Produktion</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Containerd Github</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes-Dokumentation</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NGINX-Dokumentation</a> </li></ul><br><p>  Vielen Dank f√ºr das Lesen bis zum Ende. </p><br><p>  Da Informationen zu Kubernetes, insbesondere zu Clustern, die unter realen Bedingungen betrieben werden, in RuNet sehr knapp sind, sind Hinweise auf Ungenauigkeiten ebenso willkommen wie Kommentare zum allgemeinen Clusterbereitstellungsschema.  Ich werde versuchen, sie zu ber√ºcksichtigen und entsprechende Korrekturen vorzunehmen.  Und ich bin immer bereit, Fragen in den Kommentaren, auf Githab und in allen sozialen Netzwerken zu beantworten, die in meinem Profil angegeben sind. </p><br><p>  Mit freundlichen Gr√º√üen Eugene. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de415601/">https://habr.com/ru/post/de415601/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de415591/index.html">Apple Engineers Trap MacBook Pro Tastaturfalle</a></li>
<li><a href="../de415593/index.html">6 Jahre sp√§ter eine neue Version der legend√§ren Crash-Distribution Hiren's BootCD</a></li>
<li><a href="../de415595/index.html">Methoden zur Erh√∂hung der Spielerbindung am Beispiel von SLOT-Spielen: Teil 1</a></li>
<li><a href="../de415597/index.html">Postfix - amavisd-new ohne localhost oder Mailserver auf neue Weise</a></li>
<li><a href="../de415599/index.html">Indien will auch Helium-3 bekommen</a></li>
<li><a href="../de415605/index.html">Wie wir die Kartenverarbeitung mit Exadata gespeichert haben</a></li>
<li><a href="../de415611/index.html">PKI: GCrypt- und KSBA-Bibliotheken als Alternative zu OpenSSL mit Unterst√ºtzung f√ºr die russische Kryptographie. Fortsetzung</a></li>
<li><a href="../de415613/index.html">Warum sollten Sie keine LED-Kronleuchter kaufen</a></li>
<li><a href="../de415615/index.html">Interaktion mit dem Server √ºber die API in iOS unter Swift 3. Teil 2</a></li>
<li><a href="../de415617/index.html">Verwenden der InternetPCools-FPC-Bibliothek in Delphi</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>