<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🙍🏿 ✊🏼 🦈 Kubernetes HA-Cluster mit Containerd. Oder gibt es ein Leben ohne Hafenarbeiter? 👕 🚭 👝</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Stellen Sie Kubernetes HA mit Containerd bereit 



 Guten Tag liebe Leser von Habr! Am 24. Mai 2018 wurde im offiziellen Kubernetes-Blog ein Artikel ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Kubernetes HA-Cluster mit Containerd. Oder gibt es ein Leben ohne Hafenarbeiter?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/415601/"><h1 id="razvertyvaenie-kubernetes-ha-s-containerd">  Stellen Sie Kubernetes HA mit Containerd bereit </h1><br><p><img src="https://habrastorage.org/webt/0p/w3/7g/0pw37gyankmz9a8s2gcshvto_ek.png"><br>  Guten Tag liebe Leser von Habr!  Am 24. Mai 2018 wurde im offiziellen Kubernetes-Blog ein Artikel mit dem Titel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes Containerd Integration Goes GA</a> veröffentlicht, in dem angegeben wird, dass die Integration von Containerd in Kubernetes produktionsbereit ist.  Außerdem haben Jungs von der Firma Flant eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Übersetzung des Artikels ins Russische in</a> ihrem Blog veröffentlicht, um ein wenig Klarheit von sich selbst zu schaffen.  Nachdem ich die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Projektdokumentation auf Github gelesen hatte</a> , beschloss ich, Containerd auf "meiner eigenen Haut" <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zu testen</a> . </p><br><p>  Unser Unternehmen hat mehrere Projekte in der Phase "noch sehr weit von der Produktion entfernt".  So werden sie unser Experiment;  Für sie haben wir uns entschlossen, einen Failover-Cluster von Kubernetes mithilfe von Containerd bereitzustellen und zu prüfen, ob es ein Leben ohne Docker gibt. </p><br><p>  Wenn Sie interessiert sind, wie wir es gemacht haben und was daraus wurde, sind Sie bei cat willkommen. </p><a name="habracut"></a><br><p>  <b>Schema und Einsatzbeschreibung</b> <br><img src="https://habrastorage.org/webt/db/xm/pn/dbxmpnpsth-psiiyn_ittkfkc4a.png"></p><br><p>  Wie üblich beim Bereitstellen eines Clusters (darüber habe ich in einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">früheren Artikel geschrieben</a> </p><div class="spoiler">  <b class="spoiler_title">keepalived - Implementierungen von VRRP (Virtual Router Redundancy Protocol) für Linux</b> <div class="spoiler_text">  Keepalived erstellt eine virtuelle IP (VIRTIP), die auf die IP eines der drei Master "verweist" (eine Subschnittstelle erstellt).  Der Keepalived-Daemon überwacht den Zustand der Computer und schließt im Falle eines Fehlers den ausgefallenen Server aus der Liste der aktiven Server aus, indem er VIRTIP auf die IP eines anderen Servers umschaltet. Dies entspricht der "Gewichtung", die bei der Konfiguration von Keepalived auf jedem Server angegeben wurde. </div></div><br><p>  Keepalived-Daemons kommunizieren über VRRP und senden sich gegenseitig Nachrichten an die Adresse 224.0.0.18. </p><br><p>  Wenn der Nachbar seine Nachricht nicht gesendet hat, gilt er nach Ablauf der Frist als tot.  Sobald der abgestürzte Server seine Nachrichten an das Netzwerk sendet, kehrt alles an seinen Platz zurück </p><br><p>  Wir konfigurieren die Arbeit mit dem API-Server auf Kubernetes-Knoten wie folgt. </p><br><p>  Konfigurieren Sie nach der Installation des Clusters den Kube-Proxy und ändern Sie den Port von 6443 auf 16443 (Details unten).  Auf jedem der Master wird Nginx bereitgestellt, das als Loadbalancer fungiert, Port 16443 überwacht und alle drei Master auf Port 6443 vorverlegt (Details siehe unten). </p><br><p>  Durch dieses Schema wurde eine erhöhte Fehlertoleranz sowohl bei Verwendung von Keepalived als auch bei Verwendung von Nginx erzielt. Es wurde ein Ausgleich zwischen den API-Servern auf den Assistenten erreicht. <br></p><br><p>  In einem früheren Artikel habe ich die Bereitstellung von nginx und etcd im Docker beschrieben.  In diesem Fall haben wir jedoch kein Docker, sodass Nginx und etcd lokal auf Masterknoten funktionieren. </p><br><p>  Theoretisch wäre es möglich, nginx und etcd mithilfe von Containerd bereitzustellen. Bei Problemen würde dieser Ansatz die Diagnose jedoch erschweren. Daher haben wir beschlossen, nicht zu experimentieren und lokal auszuführen. </p><br><p>  <b>Beschreibung der Server für die Bereitstellung:</b> </p><br><table><thead><tr><th>  Name </th><th>  IP </th><th>  Dienstleistungen </th></tr></thead><tbody><tr><td>  VIRTIP </td><td>  172.26.133.160 </td><td>  ------ ------. </td></tr><tr><td>  kube-master01 </td><td>  172.26.133.161 </td><td>  kubeadm, kubelet, kubectl, etcd, Containerd, Nginx, Keepalived </td></tr><tr><td>  kube-master02 </td><td>  172.26.133.162 </td><td>  kubeadm, kubelet, kubectl, etcd, Containerd, Nginx, Keepalived </td></tr><tr><td>  kube-master03 </td><td>  172.26.133.163 </td><td>  kubeadm, kubelet, kubectl, etcd, Containerd, Nginx, Keepalived </td></tr><tr><td>  kube-node01 </td><td>  172.26.133.164 </td><td>  Kubeadm, Kubelet, Kubectl, Containerd </td></tr><tr><td>  kube-node02 </td><td>  172.26.133.165 </td><td>  Kubeadm, Kubelet, Kubectl, Containerd </td></tr><tr><td>  kube-node03 </td><td>  172.26.133.166 </td><td>  Kubeadm, Kubelet, Kubectl, Containerd </td></tr></tbody></table><br><p>  <b>Installieren Sie kubeadm, kubelet, kubectl und verwandte Pakete</b> </p><br><p>  Alle Befehle werden von root ausgeführt </p><br><pre><code class="plaintext hljs">sudo -i</code> </pre> <br><pre> <code class="bash hljs">apt-get update &amp;&amp; apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl unzip tar apt-transport-https btrfs-tools libseccomp2 socat util-linux mc vim keepalived</code> </pre> <br><p>  <b>Installieren Sie conteinerd</b> <br><img src="https://habrastorage.org/webt/ul/nw/vg/ulnwvgeblmt74ivcsnsz2tuxcfa.png"></p><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> / wget https://storage.googleapis.com/cri-containerd-release/cri-containerd-1.1.0-rc.0.linux-amd64.tar.gz tar -xvf cri-containerd-1.1.0-rc.0.linux-amd64.tar.gz</code> </pre> <br><p>  <b>Containerd-Konfigurationen konfigurieren</b> </p><br><pre> <code class="bash hljs">mkdir -p /etc/containerd nano /etc/containerd/config.toml</code> </pre> <br><p>  Zur Datei hinzufügen: </p><br><pre> <code class="bash hljs">[plugins.cri] enable_tls_streaming = <span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br><p>  Wir fangen an zu prüfen, ob alles in Ordnung ist </p><br><pre> <code class="bash hljs">systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> containerd systemctl start containerd systemctl status containerd ● containerd.service - containerd container runtime Loaded: loaded (/etc/systemd/system/containerd.service; disabled; vendor preset: enabled) Active: active (running) since Mon 2018-06-25 12:32:01 MSK; 7s ago Docs: https://containerd.io Process: 10725 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS) Main PID: 10730 (containerd) Tasks: 15 (<span class="hljs-built_in"><span class="hljs-built_in">limit</span></span>: 4915) Memory: 14.9M CPU: 375ms CGroup: /system.slice/containerd.service └─10730 /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/bin/containerd Jun 25 12:32:01 hb-master02 containerd[10730]: time=<span class="hljs-string"><span class="hljs-string">"2018-06-25T12:32:01+03:00"</span></span> level=info msg=<span class="hljs-string"><span class="hljs-string">"Get image filesystem path "</span></span>/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs<span class="hljs-string"><span class="hljs-string">""</span></span> Jun 25 12:32:01 hb-master02 containerd[10730]: time=<span class="hljs-string"><span class="hljs-string">"2018-06-25T12:32:01+03:00"</span></span> level=error msg=<span class="hljs-string"><span class="hljs-string">"Failed to load cni during init, please check CRI plugin status before setting up network for pods"</span></span> error=<span class="hljs-string"><span class="hljs-string">"cni con Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>loading plugin <span class="hljs-string"><span class="hljs-string">"io.containerd.grpc.v1.introspection"</span></span>...<span class="hljs-string"><span class="hljs-string">" type=io.containerd.grpc.v1 Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start subscribing containerd event<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start recovering state<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg=serving... address="</span></span>/run/containerd/containerd.sock<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>containerd successfully booted <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 0.308755s<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start event monitor<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start snapshots syncer<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start streaming server<span class="hljs-string"><span class="hljs-string">"</span></span></code> </pre> <br><p>  <b>Installieren und ausführen Sie etcd</b> </p><br><p>  Wichtiger Hinweis, ich habe den kubernetes Cluster Version 1.10 installiert.  Nur ein paar Tage später, zum Zeitpunkt des Schreibens des Artikels, wurde Version 1.11 veröffentlicht. Wenn Sie Version 1.11 installieren, setzen Sie die Variable ETCD_VERSION = "v3.2.17", wenn 1.10, dann ETCD_VERSION = "v3.1.12". </p><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCD_VERSION=<span class="hljs-string"><span class="hljs-string">"v3.1.12"</span></span> curl -sSL https://github.com/coreos/etcd/releases/download/<span class="hljs-variable"><span class="hljs-variable">${ETCD_VERSION}</span></span>/etcd-<span class="hljs-variable"><span class="hljs-variable">${ETCD_VERSION}</span></span>-linux-amd64.tar.gz | tar -xzv --strip-components=1 -C /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/bin/</code> </pre> <br><p>  Kopieren Sie Konfigurationen aus Gitahab. </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/rjeka/k8s-containerd.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> k8s-containerd</code> </pre> <br><p>  Konfigurieren Sie die Variablen in der Konfigurationsdatei. </p><br><pre> <code class="bash hljs">vim create-config.sh</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Beschreibung der Dateivariablen create-config.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash # local machine ip address export K8SHA_IPLOCAL=172.26.133.161 # local machine etcd name, options: etcd1, etcd2, etcd3 export K8SHA_ETCDNAME=kube-master01 # local machine keepalived state config, options: MASTER, BACKUP. One keepalived cluster only one MASTER, other's are BACKUP export K8SHA_KA_STATE=MASTER # local machine keepalived priority config, options: 102, 101,100 MASTER must 102 export K8SHA_KA_PRIO=102 # local machine keepalived network interface name config, for example: eth0 export K8SHA_KA_INTF=ens18 ####################################### # all masters settings below must be same ####################################### # master keepalived virtual ip address export K8SHA_IPVIRTUAL=172.26.133.160 # master01 ip address export K8SHA_IP1=172.26.133.161 # master02 ip address export K8SHA_IP2=172.26.133.162 # master03 ip address export K8SHA_IP3=172.26.133.163 # master01 hostname export K8SHA_HOSTNAME1=kube-master01 # master02 hostname export K8SHA_HOSTNAME2=kube-master02 # master03 hostname export K8SHA_HOSTNAME3=kube-master03 # keepalived auth_pass config, all masters must be same export K8SHA_KA_AUTH=56cf8dd754c90194d1600c483e10abfr #etcd tocken: export ETCD_TOKEN=9489bf67bdfe1b3ae077d6fd9e7efefd # kubernetes cluster token, you can use 'kubeadm token generate' to get a new one export K8SHA_TOKEN=535tdi.utzk5hf75b04ht8l # kubernetes CIDR pod subnet, if CIDR pod subnet is "10.244.0.0/16" please set to "10.244.0.0\\/16" export K8SHA_CIDR=10.244.0.0\\/16</span></span></code> </pre> <br><p>  Einstellungen auf dem lokalen Computer jedes Knotens (jeder Knoten hat seinen eigenen) <br>  <b>K8SHA_IPLOCAL</b> - IP-Adresse des Knotens, auf dem das Skript konfiguriert ist <br>  <b>K8SHA_ETCDNAME</b> - lokaler Computername im ETCD-Cluster <br>  <b>K8SHA_KA_STATE</b> - Rolle in Keepalived.  Ein MASTER-Knoten, alle anderen BACKUP. <br>  <b>K8SHA_KA_PRIO</b> - Keepalived-Priorität, der Master hat 102 für die verbleibenden 101, 100. Wenn der Master mit der Nummer 102 fällt, nimmt der Knoten mit der Nummer 101 seinen Platz ein und so weiter. <br>  <b>K8SHA_KA_INTF</b> - Keepalived-Netzwerkschnittstelle.  Der Name der Schnittstelle, die beibehalten wird, wird abgehört. </p><br><p>  Die allgemeinen Einstellungen für alle Masterknoten sind gleich: </p><br><p>  <b>K8SHA_IPVIRTUAL</b> = 172.26.133.160 - virtuelle IP des Clusters. <br>  <b>K8SHA_IP1 ... K8SHA_IP3 - IP-</b> Adressen von Mastern <br>  <b>K8SHA_HOSTNAME1 ... K8SHA_HOSTNAME3</b> - Hostnamen für Masterknoten.  Ein wichtiger Punkt, unter diesen Namen kubeadm generiert Zertifikate. <br>  <b>K8SHA_KA_AUTH</b> - Passwort für Keepalived.  Sie können beliebige angeben <br>  <b>K8SHA_TOKEN</b> - Cluster-Token.  Kann mit dem Befehl <b>kubeadm token generate generiert werden</b> <br>  <b>K8SHA_CIDR</b> - <b>Subnetzadresse</b> für Herde.  Ich benutze Flanell also CIDR 0.244.0.0/16.  Achten Sie auf den Bildschirm - in der Konfiguration sollte K8SHA_CIDR = 10.244.0.0 \ / 16 sein </p></div></div><br><p>  Führen Sie das Skript aus, mit dem nginx, keepalived, etcd und kubeadmin konfiguriert werden </p><br><pre> <code class="bash hljs">./create-config.sh</code> </pre> <br><p>  Wir starten etcd. </p><br>  etcd ich hob ohne tls.  Wenn Sie tls benötigen, wird in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen kubernetes-Dokumentation</a> ausführlich beschrieben, wie Zertifikate für etcd generiert werden. <br><br><br><pre> <code class="bash hljs">systemctl daemon-reload &amp;&amp; systemctl start etcd &amp;&amp; systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> etcd</code> </pre> <br><p>  Statusprüfung </p><br><pre> <code class="bash hljs">etcdctl cluster-health member ad059013ec46f37 is healthy: got healthy result from http://192.168.5.49:2379 member 4d63136c9a3226a1 is healthy: got healthy result from http://192.168.4.169:2379 member d61978cb3555071e is healthy: got healthy result from http://192.168.4.170:2379 cluster is healthy etcdctl member list ad059013ec46f37: name=hb-master03 peerURLs=http://192.168.5.48:2380 clientURLs=http://192.168.5.49:2379,http://192.168.5.49:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">false</span></span> 4d63136c9a3226a1: name=hb-master01 peerURLs=http://192.168.4.169:2380 clientURLs=http://192.168.4.169:2379,http://192.168.4.169:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">true</span></span> d61978cb3555071e: name=hb-master02 peerURLs=http://192.168.4.170:2380 clientURLs=http://192.168.4.170:2379,http://192.168.4.170:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">false</span></span></code> </pre> <br><p>  Wenn alles in Ordnung ist, fahren Sie mit dem nächsten Schritt fort. </p><br><p>  <b>Konfigurieren Sie kubeadmin</b> <br>  Wenn Sie kubeadm Version 1.11 verwenden, können Sie diesen Schritt überspringen <br>  Konfigurieren Sie die kubeadmin-Konfiguration, damit kybernetes nicht mit Docker, sondern mit Containerd funktioniert </p><br><pre> <code class="bash hljs">vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> </pre> <br><p>  Fügen Sie nach [Service] eine Zeile zum Block hinzu </p><br><pre> <code class="bash hljs">Environment=<span class="hljs-string"><span class="hljs-string">"KUBELET_EXTRA_ARGS=--runtime-cgroups=/system.slice/containerd.service --container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock"</span></span></code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Die gesamte Konfiguration sollte folgendermaßen aussehen:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Service] Environment="KUBELET_EXTRA_ARGS=--runtime-cgroups=/system.slice/containerd.service --container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock" Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf" Environment="KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true" Environment="KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin" Environment="KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local" Environment="KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt" Environment="KUBELET_CADVISOR_ARGS=--cadvisor-port=0" Environment="KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki" ExecStart= ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_SYSTEM_PODS_ARGS $KUBELET_NETWORK_ARGS $KUBELET_DNS_ARGS $KUBELET_AUTHZ_ARGS $KUBELET_CADVISOR_ARGS $KUBELET_CERTIFICATE_ARGS $KUBELET_EXTRA_ARGS</code> </pre> </div></div><br><p>  Wenn Sie Version 1.11 installieren und mit CoreDNS anstelle von kube-dns experimentieren und die dynamische Konfiguration testen möchten, kommentieren Sie den folgenden Block in der Konfigurationsdatei kubeadm-init.yaml aus: </p><br><pre> <code class="bash hljs">feature-gates: DynamicKubeletConfig: <span class="hljs-literal"><span class="hljs-literal">true</span></span> CoreDNS: <span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br><p>  Kubelet neu starten </p><br><pre> <code class="plaintext hljs">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</code> </pre> <br><p>  <b>Initialisierung des ersten Assistenten</b> </p><br><p>  Bevor Sie kubeadm starten, müssen Sie keepalived neu starten und seinen Status überprüfen </p><br><pre> <code class="bash hljs">systemctl restart keepalived.service systemctl status keepalived.service ● keepalived.service - Keepalive Daemon (LVS and VRRP) Loaded: loaded (/lib/systemd/system/keepalived.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2018-06-27 10:40:03 MSK; 1min 44s ago Process: 4589 ExecStart=/usr/sbin/keepalived <span class="hljs-variable"><span class="hljs-variable">$DAEMON_ARGS</span></span> (code=exited, status=0/SUCCESS) Main PID: 4590 (keepalived) Tasks: 7 (<span class="hljs-built_in"><span class="hljs-built_in">limit</span></span>: 4915) Memory: 15.3M CPU: 968ms CGroup: /system.slice/keepalived.service ├─4590 /usr/sbin/keepalived ├─4591 /usr/sbin/keepalived ├─4593 /usr/sbin/keepalived ├─5222 /usr/sbin/keepalived ├─5223 sh -c /etc/keepalived/check_apiserver.sh ├─5224 /bin/bash /etc/keepalived/check_apiserver.sh └─5231 sleep 5</code> </pre> <br><p>  Überprüfen Sie, ob VIRTIP pingt </p><br><pre> <code class="bash hljs">ping -c 4 172.26.133.160 PING 172.26.133.160 (172.26.133.160) 56(84) bytes of data. 64 bytes from 172.26.133.160: icmp_seq=1 ttl=64 time=0.030 ms 64 bytes from 172.26.133.160: icmp_seq=2 ttl=64 time=0.050 ms 64 bytes from 172.26.133.160: icmp_seq=3 ttl=64 time=0.050 ms 64 bytes from 172.26.133.160: icmp_seq=4 ttl=64 time=0.056 ms --- 172.26.133.160 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3069ms rtt min/avg/max/mdev = 0.030/0.046/0.056/0.012 ms</code> </pre> <br><p>  Führen Sie danach kubeadmin aus.  Stellen Sie sicher, dass Sie die Zeile --skip-preflight-prüfungen einfügen.  Kubeadmin sucht standardmäßig nach Docker und schlägt ohne Überspringen mit einem Fehler fehl. </p><br><pre> <code class="bash hljs">kubeadm init --config=kubeadm-init.yaml --skip-preflight-checks</code> </pre> <br><p>  Speichern Sie die generierte Zeile, nachdem kubeadm funktioniert hat.  Es wird benötigt, um Arbeitsknoten in den Cluster einzugeben. </p><br><pre> <code class="bash hljs">kubeadm join 172.26.133.160:6443 --token XXXXXXXXXXXXXXXXXXXXXXXXX --discovery-token-ca-cert-hash sha256:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX</code> </pre> <br><p>  Geben Sie als Nächstes an, wo die Datei admin.conf gespeichert ist <br>  Wenn wir als root arbeiten, dann: </p><br><pre> <code class="bash hljs">vim ~/.bashrc <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> KUBECONFIG=/etc/kubernetes/admin.conf <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> ~/.bashrc</code> </pre> <br><p>  Befolgen Sie für einen einfachen Benutzer die Anweisungen auf dem Bildschirm. </p><br><pre> <code class="bash hljs">mkdir -p <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube sudo cp -i /etc/kubernetes/admin.conf <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube/config sudo chown $(id -u):$(id -g) <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube/config</code> </pre> <br><p>  Fügen Sie dem Cluster zwei weitere Assistenten hinzu.  Kopieren Sie dazu die Zertifikate von kube-master01 nach kube-master02 und kube-master03 in das Verzeichnis / etc / kubernetes /.  Zu diesem Zweck habe ich den SSH-Zugriff für root konfiguriert und nach dem Kopieren der Dateien die Einstellungen zurückgegeben. </p><br><pre> <code class="bash hljs">scp -r /etc/kubernetes/pki 172.26.133.162:/etc/kubernetes/ scp -r /etc/kubernetes/pki 172.26.133.163:/etc/kubernetes/</code> </pre> <br><p>  Führen Sie nach dem Kopieren nach kube-master02 und kube-master03 aus. </p><br><pre> <code class="bash hljs">kubeadm init --config=kubeadm-init.yaml --skip-preflight-checks</code> </pre> <br><p>  <b>Installieren Sie das CIDR-Flanell</b> </p><br><p>  auf kube-master01 ausführen </p><br><pre> <code class="bash hljs">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml</code> </pre> <br><p>  Die aktuelle Version von Flanel finden Sie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation zu Kubernetes</a> . </p><br><p>  Wir warten, bis alle Container erstellt sind. </p><br><pre> <code class="bash hljs">watch -n1 kubectl get pods --all-namespaces -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE kube-system kube-apiserver-kube-master01 1/1 Running 0 17m 172.26.133.161 kube-master01 kube-system kube-apiserver-kube-master02 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-apiserver-kube-master03 1/1 Running 0 6m 172.26.133.163 kube-master03 kube-system kube-controller-manager-kube-master01 1/1 Running 0 17m 172.26.133.161 kube-master01 kube-system kube-controller-manager-kube-master02 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-controller-manager-kube-master03 1/1 Running 0 6m 172.26.133.163 kube-master03 kube-system kube-dns-86f4d74b45-8c24s 3/3 Running 0 17m 10.244.2.2 kube-master03 kube-system kube-flannel-ds-4h4w7 1/1 Running 0 2m 172.26.133.163 kube-master03 kube-system kube-flannel-ds-kf5mj 1/1 Running 0 2m 172.26.133.162 kube-master02 kube-system kube-flannel-ds-q6k4z 1/1 Running 0 2m 172.26.133.161 kube-master01 kube-system kube-proxy-9cjtp 1/1 Running 0 6m 172.26.133.163 kube-master03 kube-system kube-proxy-9sqk2 1/1 Running 0 17m 172.26.133.161 kube-master01 kube-system kube-proxy-jg2pt 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-scheduler-kube-master01 1/1 Running 0 18m 172.26.133.161 kube-master01 kube-system kube-scheduler-kube-master02 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-scheduler-kube-master03 1/1 Running 0 6m 172.26.133.163 kube-master03</code> </pre> <br><p>  <b>Wir replizieren kube-dns an alle drei Master</b> </p><br><p>  Auf kube-master01 ausführen </p><br><pre> <code class="bash hljs">kubectl scale --replicas=3 -n kube-system deployment/kube-dns</code> </pre> <br><p>  <b>Installieren und konfigurieren Sie nginx</b> </p><br><p>  Installieren Sie auf jedem Masterknoten nginx als Balancer für die Kubernetes-API <br>  Ich habe alle Cluster-Maschinen auf Debian.  Von den Nginx-Paketen wird das Stream-Modul nicht unterstützt. Fügen Sie daher die Nginx-Repositorys hinzu und installieren Sie sie aus den Nginx-Repositorys.  Wenn Sie ein anderes Betriebssystem haben, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">lesen</a> Sie die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Nginx-Dokumentation</a> . </p><br><pre> <code class="bash hljs">wget https://nginx.org/keys/nginx_signing.key sudo apt-key add nginx_signing.key <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> -e <span class="hljs-string"><span class="hljs-string">"\n#nginx\n\ deb http://nginx.org/packages/debian/ stretch nginx\n\ deb-src http://nginx.org/packages/debian/ stretch nginx"</span></span> &gt;&gt; /etc/apt/sources.list apt-get update &amp;&amp; apt-get install nginx -y</code> </pre> <br><p>  Nginx-Konfiguration erstellen (falls noch nicht erstellt) </p><br><pre> <code class="bash hljs">./create-config.sh</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">nginx.conf</b> <div class="spoiler_text"><p>  Benutzer nginx; <br>  worker_processes auto; </p><br><p>  error_log /var/log/nginx/error.log warn; <br>  pid /var/run/nginx.pid; </p><br><p>  Ereignisse { <br>  worker_connections 1024; <br>  }} </p><br><p>  http { <br>  include /etc/nginx/mime.types; <br>  default_type application / octet-stream; </p><br><pre> <code class="plaintext hljs">log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf;</code> </pre> <br><p>  }} </p><br><p>  stream { <br>  Upstream-Apiserver { <br>  Server 172.26.133.161:6443 weight = 5 max_fails = 3 fail_timeout = 30s; <br>  Server 172.26.133.162:6443 weight = 5 max_fails = 3 fail_timeout = 30s; <br>  Server 172.26.133.163:6443 weight = 5 max_fails = 3 fail_timeout = 30s; </p><br><pre> <code class="plaintext hljs">} server { listen 16443; proxy_connect_timeout 1s; proxy_timeout 3s; proxy_pass apiserver; }</code> </pre> <br><p>  }} </p></div></div><br><p>  Wir prüfen, ob alles in Ordnung ist und wenden die Konfiguration an </p><br><pre> <code class="bash hljs">nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf <span class="hljs-built_in"><span class="hljs-built_in">test</span></span> is successful systemctl restart nginx systemctl status nginx ● nginx.service - nginx - high performance web server Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2018-06-28 08:48:09 MSK; 22s ago Docs: http://nginx.org/en/docs/ Process: 22132 ExecStart=/usr/sbin/nginx -c /etc/nginx/nginx.conf (code=exited, status=0/SUCCESS) Main PID: 22133 (nginx) Tasks: 2 (<span class="hljs-built_in"><span class="hljs-built_in">limit</span></span>: 4915) Memory: 1.6M CPU: 7ms CGroup: /system.slice/nginx.service ├─22133 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.conf └─22134 nginx: worker process</code> </pre> <br><p>  Testen Sie den Balancer </p><br><pre> <code class="bash hljs">curl -k https://172.26.133.161:16443 | wc -l % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 233 100 233 0 0 12348 0 --:--:-- --:--:-- --:--:-- 12944</code> </pre> <br><p>  <b>Konfigurieren Sie kube-proxy für die Arbeit mit dem Balancer</b> </p><br><p>  Bearbeiten Sie nach der Konfiguration des Balancers den Port in den Kubernetes-Einstellungen. </p><br><pre> <code class="bash hljs">kubectl edit -n kube-system configmap/kube-proxy</code> </pre> <br><p>  Ändern Sie die Servereinstellungen in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://172.26.133.160:16443</a> <br>  Als Nächstes müssen Sie kube-proxy so konfigurieren, dass es mit dem neuen Port funktioniert </p><br><pre> <code class="bash hljs">kubectl get pods --all-namespaces -o wide | grep proxy kube-system kube-proxy-9cjtp 1/1 Running 1 22h 172.26.133.163 kube-master03 kube-system kube-proxy-9sqk2 1/1 Running 1 22h 172.26.133.161 kube-master01 kube-system kube-proxy-jg2pt 1/1 Running 4 22h 172.26.133.162 kube-</code> </pre> <br><p>  Wir löschen alle Pods, nach dem Entfernen werden sie automatisch mit den neuen Einstellungen neu erstellt </p><br><pre> <code class="bash hljs">kubectl delete pod -n kube-system kube-proxy-XXX ```bash    .      ```bash kubectl get pods --all-namespaces -o wide | grep proxy kube-system kube-proxy-hqrsw 1/1 Running 0 33s 172.26.133.161 kube-master01 kube-system kube-proxy-kzvw5 1/1 Running 0 47s 172.26.133.163 kube-master03 kube-system kube-proxy-zzkz5 1/1 Running 0 7s 172.26.133.162 kube-master02</code> </pre> <br><p>  <b>Hinzufügen von Arbeitsknoten zum Cluster</b> </p><br><p>  Führen Sie für jeden Grundton den von kubeadm generierten Befehl aus </p><br><pre> <code class="bash hljs">kubeadm join 172.26.133.160:6443 --token XXXXXXXXXXXXXXXXXXXXXXXXX --discovery-token-ca-cert-hash sha256:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX --cri-socket /run/containerd/containerd.sock --skip-preflight-checks</code> </pre> <br><p>  Wenn die Zeile "verloren" ist, müssen Sie eine neue generieren </p><br><pre> <code class="bash hljs">kubeadm token generate kubeadm token create &lt;generated-token&gt; --<span class="hljs-built_in"><span class="hljs-built_in">print</span></span>-join-command --ttl=0</code> </pre> <br><p>  Auf Arbeitsknoten in den Dateien /etc/kubernetes/bootstrap-kubelet.conf und /etc/kubernetes/kubelet.conf <br>  Servervariablenwert zu unserer Virtip </p><br><pre> <code class="bash hljs">vim /etc/kubernetes/bootstrap-kubelet.conf server: https://172.26.133.60:16443 vim /etc/kubernetes/kubelet.conf server: https://172.26.133.60:16443</code> </pre> <br><p>  Und starten Sie Containerd und Kubernetes neu </p><br><pre> <code class="bash hljs">systemctl restart containerd kubelet</code> </pre> <br><p>  <b>Dashboard-Installation</b> </p><br><pre> <code class="bash hljs">kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml</code> </pre> <br><p>  Erstellen Sie einen Benutzer mit Administratorrechten: </p><br><pre> <code class="bash hljs">kubectl apply -f kube-dashboard/dashboard-adminUser.yaml</code> </pre> <br><p>  Wir bekommen den Token für den Eintritt: </p><br><pre> <code class="bash hljs">kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk <span class="hljs-string"><span class="hljs-string">'{print $1}'</span></span>)</code> </pre> <br><p>  Konfigurieren des Dashboard-Zugriffs über NodePort in VIRTIP </p><br><pre> <code class="bash hljs">kubectl -n kube-system edit service kubernetes-dashboard</code> </pre> <br><p>  Wir ersetzen den Wert von Typ: ClusterIP durch Typ: NodePort und fügen im Abschnitt Port den Wert von nodePort: 30000 hinzu (oder den Port im Bereich von 30000 bis 32000, auf den das Panel zugreifen soll): </p><br><p><img src="https://habrastorage.org/webt/fn/ql/kr/fnqlkren3ltk88xzi8fqwi4vbxa.png"></p><br><p>  Das Panel ist jetzt unter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https: // VIRTIP: 30000</a> verfügbar </p><br><p>  <b>Heapster</b> </p><br><p>  Installieren Sie als Nächstes Heapster, ein Tool zum Abrufen von Metriken für Clusterkomponenten. </p><br><p>  Installation: </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/kubernetes/heapster.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> heapster kubectl create -f deploy/kube-config/influxdb/ kubectl create -f deploy/kube-config/rbac/heapster-rbac.yaml</code> </pre> <br><p>  <b>Schlussfolgerungen</b> </p><br><p>  Ich habe keine besonderen Probleme bei der Arbeit mit Containerd bemerkt.  Einmal gab es eine unverständliche Panne mit einem Herd, nachdem der Einsatz entfernt wurde.  Kubernetes glaubte, dass unter gelöscht wurde, aber unter wurde so ein eigenartiger "Zombie". Es blieb auf dem Knoten existieren, aber im erweiterten Status. </p><br><p>  Ich glaube, dass Containerd eher als Container-Laufzeit für Kubernetes ausgerichtet ist.  Höchstwahrscheinlich wird es in Zukunft als Umgebung für den Start von Microservices in Kubernetes möglich und notwendig sein, unterschiedliche Umgebungen zu verwenden, die sich an unterschiedlichen Aufgaben, Projekten usw. orientieren. </p><br><p>  Das Projekt entwickelt sich sehr schnell.  Alibaba Cloud hat begonnen, conatinerd aktiv zu nutzen, und betont, dass es die ideale Umgebung für den Betrieb von Containern ist. </p><br><p>  Laut den Entwicklern entspricht die Integration von Containerd in die Google Cloud-Plattform Kubernetes nun der Docker-Integration. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ein gutes Beispiel für das Dienstprogramm crictl console</a> .  Ich werde auch einige Beispiele aus dem erstellten Cluster geben: </p><br><pre> <code class="plaintext hljs">kubectl describe nodes | grep "Container Runtime Version:"</code> </pre> <br><p><img src="https://habrastorage.org/webt/gr/_l/of/gr_lofuou-20jmqzb800qdi5yny.png"></p><br><p>  Der Docker-CLI fehlen die grundlegenden Konzepte von Kubernetes, z. B. Pod und Namespace, während crictl diese Konzepte unterstützt </p><br><pre> <code class="plaintext hljs">crictl pods</code> </pre> <br><p><img src="https://habrastorage.org/webt/kr/im/hl/krimhlcrmwcaxysx3wgd9mjtvuo.png"></p><br><p>  Und wenn nötig, können wir uns die Container im üblichen Format wie Docker ansehen </p><br><pre> <code class="plaintext hljs">crictl ps</code> </pre> <br><p><img src="https://habrastorage.org/webt/zo/2m/ez/zo2mezfxjgohjpu8f35n-nazf74.png"></p><br><p>  Wir können die Bilder sehen, die sich auf dem Knoten befinden </p><br><pre> <code class="plaintext hljs">crictl images</code> </pre> <br><p><img src="https://habrastorage.org/webt/mw/u6/tu/mwu6tunxz4re5yrr0h-ajq-_3xs.png"></p><br><p>  Wie sich herausstellte, ist das Leben ohne Hafenarbeiter :) </p><br><p>  Es ist zu früh, um über Fehler und Störungen zu sprechen. Der Cluster arbeitet seit etwa einer Woche mit uns zusammen.  In naher Zukunft wird der Test darauf übertragen, und bei Erfolg höchstwahrscheinlich der Entwicklerstand eines der Projekte.  Es besteht die Idee, eine Reihe von Artikeln zu DevOps-Prozessen zu verfassen, z. B.: Erstellen eines Clusters, Einrichten eines Ingress-Controllers und Verschieben auf separate Clusterknoten, Automatisieren der Image-Assemblierung, Überprüfen von Images auf Schwachstellen, Bereitstellung usw.  In der Zwischenzeit werden wir die Stabilität der Arbeit untersuchen, nach Fehlern suchen und neue Produkte entwickeln. </p><br><p>  Dieses Handbuch eignet sich auch zum Bereitstellen eines Failoverclusters mit Docker. Sie müssen Docker nur gemäß den Anweisungen in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen Kubernetes-Dokumentation</a> installieren und die Schritte zum Installieren von Containerd und zum Konfigurieren der Kubeadm-Konfiguration überspringen. </p><br><p>  Oder Sie können Containerd und Docker gleichzeitig auf demselben Host platzieren und, wie die Entwickler versichern, perfekt zusammenarbeiten.  Containerd ist die Konbernetes-Containerer-Startumgebung, und Docker ist genau wie Docker))) </p><br><p><img src="https://habrastorage.org/webt/qj/f2/r2/qjf2r2vn_j4odysnyxytokycz9u.png"><br></p><br>  Das Containerd-Repository verfügt über ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ansible Playbook</a> zum Einrichten eines einzelnen Assistentenclusters.  Für mich war es jedoch interessanter, das System mit meinen Händen zu „heben“, um die Konfiguration jeder Komponente genauer zu verstehen und zu verstehen, wie es in der Praxis funktioniert. <br><p>  Vielleicht werden meine Hände eines Tages reichen und ich werde mein Playbook für die Bereitstellung eines Clusters mit HA schreiben, da ich in den letzten sechs Monaten mehr als ein Dutzend davon bereitgestellt habe und es wahrscheinlich an der Zeit wäre, den Prozess zu automatisieren. </p><br><p>  Während des Schreibens dieses Artikels wurde auch die Version kubernetes 1.11 veröffentlicht.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Informationen zu</a> den wichtigsten Änderungen finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">im Flant-Blog</a> oder im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen Kubernetes-Blog</a> .  Wir haben die Testcluster auf Version 1.11 aktualisiert und kube-dns durch CoreDNS ersetzt.  Darüber hinaus haben wir die DynamicKubeletConfig-Funktion zum Testen der Funktionen der dynamischen Aktualisierung von Konfigurationen hinzugefügt. </p><br><p>  Verwendete Materialien: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes Containerd Integration geht GA</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes Containerd-Integration ersetzt Docker für die Produktion</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Containerd Github</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes-Dokumentation</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NGINX-Dokumentation</a> </li></ul><br><p>  Vielen Dank für das Lesen bis zum Ende. </p><br><p>  Da Informationen zu Kubernetes, insbesondere zu Clustern, die unter realen Bedingungen betrieben werden, in RuNet sehr knapp sind, sind Hinweise auf Ungenauigkeiten ebenso willkommen wie Kommentare zum allgemeinen Clusterbereitstellungsschema.  Ich werde versuchen, sie zu berücksichtigen und entsprechende Korrekturen vorzunehmen.  Und ich bin immer bereit, Fragen in den Kommentaren, auf Githab und in allen sozialen Netzwerken zu beantworten, die in meinem Profil angegeben sind. </p><br><p>  Mit freundlichen Grüßen Eugene. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de415601/">https://habr.com/ru/post/de415601/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de415591/index.html">Apple Engineers Trap MacBook Pro Tastaturfalle</a></li>
<li><a href="../de415593/index.html">6 Jahre später eine neue Version der legendären Crash-Distribution Hiren's BootCD</a></li>
<li><a href="../de415595/index.html">Methoden zur Erhöhung der Spielerbindung am Beispiel von SLOT-Spielen: Teil 1</a></li>
<li><a href="../de415597/index.html">Postfix - amavisd-new ohne localhost oder Mailserver auf neue Weise</a></li>
<li><a href="../de415599/index.html">Indien will auch Helium-3 bekommen</a></li>
<li><a href="../de415605/index.html">Wie wir die Kartenverarbeitung mit Exadata gespeichert haben</a></li>
<li><a href="../de415611/index.html">PKI: GCrypt- und KSBA-Bibliotheken als Alternative zu OpenSSL mit Unterstützung für die russische Kryptographie. Fortsetzung</a></li>
<li><a href="../de415613/index.html">Warum sollten Sie keine LED-Kronleuchter kaufen</a></li>
<li><a href="../de415615/index.html">Interaktion mit dem Server über die API in iOS unter Swift 3. Teil 2</a></li>
<li><a href="../de415617/index.html">Verwenden der InternetPCools-FPC-Bibliothek in Delphi</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>