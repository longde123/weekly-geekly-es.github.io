<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üêé üå¨Ô∏è üë®üèæ‚Äçüé® Deep Learning gegen gesunden Menschenverstand: Entwicklung eines Chat-Bots üë®üèΩ‚Äçüöí ü•õ ‚è¨</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Je mehr Benutzer Ihres Dienstes sind, desto h√∂her ist die Wahrscheinlichkeit, dass sie Hilfe ben√∂tigen. Der Chat des technischen Supports ist eine off...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Deep Learning gegen gesunden Menschenverstand: Entwicklung eines Chat-Bots</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/455652/">  Je mehr Benutzer Ihres Dienstes sind, desto h√∂her ist die Wahrscheinlichkeit, dass sie Hilfe ben√∂tigen.  Der Chat des technischen Supports ist eine offensichtliche, aber recht teure L√∂sung.  Wenn Sie jedoch Technologie f√ºr maschinelles Lernen verwenden, k√∂nnen Sie etwas Geld sparen. <br><br>  Der Bot kann nun einfache Fragen beantworten.  Dar√ºber hinaus kann dem Chatbot beigebracht werden, die Absichten des Benutzers zu bestimmen und den Kontext zu erfassen, damit er die meisten Probleme der Benutzer ohne menschliches Eingreifen l√∂sen kann.  Wie das geht, werden Vladislav Blinov und Valery Baranova, Entwickler des beliebten Assistenten Oleg, helfen, es herauszufinden. <br><br><img src="https://habrastorage.org/webt/rr/do/kw/rrdokweqzrtdqiogus2vahosksg.png"><br><br>  Beim √úbergang von einfachen zu komplizierteren Methoden bei der Entwicklung eines Chat-Bots werden wir praktische Implementierungsprobleme analysieren und herausfinden, welcher Qualit√§tsgewinn erzielt werden kann und wie viel er kosten wird. <br><a name="habracut"></a><br><iframe width="560" height="315" src="https://www.youtube.com/embed/eL3dkh-WaSU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <strong>Vladislav Blinov</strong> ist ein leitender Entwickler von Dialogsystemen in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tinkoff</a> und verwendet h√§ufig Abk√ºrzungen: ML, NLP, DL usw.  Dar√ºber hinaus untersucht die Graduiertenschule die Modellierung von Humor durch maschinelles Lernen und neuronale Netze. <br><br>  <strong>Valeria Baranova</strong> schreibt seit √ºber 5 Jahren coole Dinge im NLP-Bereich in Python.  Jetzt erstellt Tinkoff im Team interaktiver Systeme Chat-Bots und unterrichtet einen Kurs zum maschinellen Lernen f√ºr Studenten.  Er forscht auch auf dem Gebiet des rechnerischen Humors, dh er lehrt die KI, Witze zu verstehen und neue zu erfinden - Valeria und Vladislav <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">werden dar√ºber</a> bei UseData Conf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sprechen</a> . <br><br>  Die Dienste der Tinkoff Bank werden von Millionen von Menschen genutzt.  Um eine solche Anzahl von Benutzern rund um die Uhr zu unterst√ºtzen, wird ein gro√ües Personal ben√∂tigt, was zu hohen Servicekosten f√ºhrt.  Es erscheint logisch, dass die beliebten Fragen der Benutzer mithilfe des Chat-Bots automatisch beantwortet werden k√∂nnen. <br><br><h2>  Benutzerabsicht oder Absicht </h2><br>  Das erste, was ein Chatbot braucht, ist zu verstehen, <strong>was der Benutzer will</strong> .  Diese Aufgabe wird als Klassifizierung von Absichten oder Absichten bezeichnet.  Dar√ºber hinaus werden alle Modelle und Ans√§tze im Rahmen dieser Aufgabe ber√ºcksichtigt. <br><br>  Schauen wir uns ein Beispiel f√ºr die Klassifizierung von Absichten an.  Wenn Sie schreiben: "√úberweisen Sie hundert Lera", wird der Chat-Bot Oleg verstehen, dass dies die Absicht einer Geld√ºberweisung ist, dh die Absicht des Benutzers, Geld zu √ºberweisen.  Oder besser gesagt, dass Lera die Menge von 100 Rubel √ºbertragen muss. <br><br>  Wir werden Methoden vergleichen und die Qualit√§t ihrer Arbeit an einem Testmuster testen, das aus echten Dialogen mit Benutzern besteht.  Unsere Stichprobe enth√§lt mehr als 30.000 markierte Beispiele und 170 Absichten, zum Beispiel: ins Kino gehen, nach Restaurants suchen, eine Kaution er√∂ffnen oder schlie√üen usw.  Oleg hat auch seine eigene Meinung zu vielem und er kann einfach mit dir chatten. <br><br><h2>  W√∂rterbuchklassifikation </h2><br>  Das Einfachste, was bei der Klassifizierung von Absichten getan werden kann, ist die <strong>Verwendung eines W√∂rterbuchs</strong> .  Wenn beispielsweise das Wort "√ºbersetzen" in der Phrase eines Benutzers vorkommt, sollten Sie eine Geld√ºberweisung in Betracht ziehen. <br><br>  Schauen wir uns die Qualit√§t eines so einfachen Ansatzes an. <br><div class="scrollable-table"><table><tbody><tr><td></td><td>  Pr√§zision </td><td>  R√ºckruf </td><td>  F1-Score </td></tr><tr><td>  Geld√ºberweisung </td><td>  0,88 </td><td>  0,23 </td><td>  0,36 </td></tr><tr><td>  Der Rest </td><td>  0,97 </td><td>  0,99 </td><td>  0,98 </td></tr></tbody></table></div>  Wenn der Klassifizierer die Absicht des Benutzers einfach als "Geldtransfer" mit dem Wort "√ºbersetzen" definiert, ist die Qualit√§t bereits recht hoch.  Pr√§zision - 88%, w√§hrend die Vollst√§ndigkeit gering ist, entspricht nur 23%.  Das ist verst√§ndlich: Das Wort ‚Äû√ºbersetzen‚Äú beschreibt nicht alle M√∂glichkeiten, ‚ÄûGeld an jemanden √ºberweisen‚Äú zu sagen. <br><br>  Dieser Ansatz hat jedoch Vorteile: <br><br><ul><li>  Es ist keine gekennzeichnete Stichprobe erforderlich (wenn Sie das Modell nicht untersuchen, ist keine Stichprobe erforderlich). </li><li>  Sie k√∂nnen eine hohe Genauigkeit erzielen, wenn Sie W√∂rterb√ºcher gut kompilieren (dies erfordert jedoch Zeit und Ressourcen). </li></ul><br>  Die Vollst√§ndigkeit einer solchen L√∂sung d√ºrfte jedoch gering sein, da alle Variationen einer Klasse schwer zu beschreiben sind. <br><br>  Betrachten Sie ein Gegenbeispiel.  Wenn zus√§tzlich zur Geldtransferabsicht "√úberweisung" auch die zweite Absicht enthalten kann - "√úberweisung an den Betreiber".  Wenn wir dem Operator eine neue √úbersetzungsabsicht hinzuf√ºgen, erhalten wir unterschiedliche Ergebnisse. <br><div class="scrollable-table"><table><tbody><tr><td></td><td>  Pr√§zision </td><td>  R√ºckruf </td><td>  F1-Score </td></tr><tr><td>  Geld√ºberweisung </td><td>  0,70 </td><td>  0,23 </td><td>  0,34 </td></tr><tr><td>  Der Rest </td><td>  0,97 </td><td>  0,99 </td><td>  0,98 </td></tr></tbody></table></div>  Die Genauigkeit sinkt um 18 Punkte, w√§hrend die Vollst√§ndigkeit nat√ºrlich nicht zunimmt.  Dies zeigt, dass ein fortgeschrittener Ansatz erforderlich ist. <br><br><h2>  Textanalyse </h2><br>  Bevor Sie maschinelles Lernen verwenden, m√ºssen Sie wissen, wie Sie Text als Vektor darstellen.  Einer der einfachsten Ans√§tze ist die <strong>Verwendung eines tf-idf-Vektors</strong> . <br><br>  Der tf-idf-Vektor ber√ºcksichtigt das Auftreten jedes Wortes in der Phrase des Benutzers und das gesamte Auftreten von W√∂rtern in der Sammlung.  W√∂rter, die h√§ufig in verschiedenen Texten vorkommen, haben in dieser Vektordarstellung weniger Gewicht. <br><br>  Betrachten wir die Qualit√§t des linearen Modells f√ºr tf-idf-Darstellungen (in unserem Fall logistische Regression). <br><div class="scrollable-table"><table><tbody><tr><td></td><td>  Pr√§zision </td><td>  R√ºckruf </td><td>  F1-Score </td></tr><tr><td>  Geld√ºberweisung </td><td>  0,74 </td><td>  0,86 </td><td>  0,80 </td></tr><tr><td>  Der Rest </td><td>  0,99 </td><td>  0,99 </td><td>  0,99 </td></tr></tbody></table></div>  Infolgedessen nahm <strong>die Vollst√§ndigkeit</strong> stark <strong>zu</strong> , und die Genauigkeit blieb mit der Verwendung des W√∂rterbuchs vergleichbar. Das f1-Ma√ü (gewichtetes harmonisches Mittel zwischen Genauigkeit und Vollst√§ndigkeit) nahm ebenfalls zu.  Das hei√üt, das Modell selbst versteht bereits, welche W√∂rter f√ºr welche Absicht wichtig sind - Sie m√ºssen selbst nichts erfinden. <br><br><h3>  Datenvisualisierung </h3><br>  Die Datenvisualisierung hilft zu verstehen, wie Absichten aussehen und wie gut sie im Raum gruppiert sind.  Aufgrund der gro√üen Dimension k√∂nnen wir tf-idf-Darstellungen jedoch nicht direkt visualisieren. Daher verwenden wir <strong>die Dimensionskomprimierungsmethode - t-SNE</strong> . <br><br><img src="https://habrastorage.org/webt/zx/gh/dc/zxghdc_rwrmjjqojzp9b5lao6tq.png"><br><br>  Der Hauptunterschied zwischen dieser Methode und PCA besteht darin, dass bei der √úbertragung in den zweidimensionalen Raum der <strong>relative Abstand zwischen den Objekten erhalten bleibt</strong> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1p/8w/xr/1p8wxr2sc5guuqktq4u7aqe88jk.png" width="400"></div><br>  <em>t-SNE auf tf-idf (Top 10 Absichten), F1-Punktzahl 0,92</em> <br><br>  Die Top 10 Absichten nach Vorkommen in unserer Sammlung sind oben dargestellt.  Es gibt gr√ºne Punkte, die keiner Absicht angeh√∂ren, und 10 Cluster, die mit unterschiedlichen Farben markiert sind, sind unterschiedliche Absichten.  Es ist zu sehen, dass einige von ihnen sehr gut gruppiert sind.  Das gewichtete <strong>f1-Ma√ü betr√§gt 0,92</strong> - das ist ziemlich viel, man kann schon damit arbeiten. <br><br>  Also mit einem linearen Klassifikator √ºber tf-idf: <br><br><ul><li>  viel h√∂here Vollst√§ndigkeit als die Verwendung eines W√∂rterbuchs mit vergleichbarer Genauigkeit; </li><li>  Sie m√ºssen sich nicht √ºberlegen, welche W√∂rter welcher Absicht entsprechen. </li></ul><br>  Es gibt aber auch Nachteile: <br><br><ul><li>  Mit begrenztem Wortschatz k√∂nnen Sie nur f√ºr die W√∂rter Gewicht bekommen, die im Trainingsbeispiel vorhanden sind. </li><li>  Umformulierung wird nicht ber√ºcksichtigt; </li><li>  Die Reihenfolge, in der die W√∂rter im Text vorkommen, wird nicht ber√ºcksichtigt. </li></ul><br><h2>  Umformulieren </h2><br>  Betrachten wir das Problem der Neuformulierung genauer. <br><br>  Tf-idf-Vektoren k√∂nnen nur f√ºr Texte nahe sein, die sich in W√∂rtern schneiden.  Die N√§he zwischen den Vektoren kann durch den Kosinus des Winkels zwischen ihnen berechnet werden.  Die Kosinusn√§he in der Vektordarstellung tf-idf wird f√ºr bestimmte Beispiele berechnet. <br><br><img src="https://habrastorage.org/webt/gf/7c/eb/gf7ceblapupg3xysxiyxgcjwo3o.jpeg"><br><br>  Dies sind keine sehr engen S√§tze f√ºr die tf-idf-Vektordarstellung, obwohl es f√ºr uns dieselbe Absicht und dieselbe Klasse ist. <br><br>  Was kann man dagegen tun?  Anstelle einer Zahl k√∂nnen Sie beispielsweise ein Wort als ganzen Vektor darstellen - dies wird als "Worteinbettung" bezeichnet. <br><br><img src="https://habrastorage.org/webt/rl/9n/xm/rl9nxmdp8sr_q7fwo7iokyj8hwy.png"><br><br>  Eines der beliebtesten Modelle zur L√∂sung dieses Problems wurde 2013 vorgeschlagen.  Es hei√üt <strong>word2vec</strong> und ist seitdem weit verbreitet. <br><br>  Eine der M√∂glichkeiten, Word2vec zu lernen, funktioniert ungef√§hr wie folgt: Wir nehmen den Text, nehmen ein Wort aus dem Kontext und werfen es weg, dann nehmen wir ein anderes zuf√§lliges Wort aus dem Kontext und pr√§sentieren beide W√∂rter als One-Hot-Vektoren.  Ein hei√üer Vektor ist ein Vektor gem√§√ü der W√∂rterbuchdimension, wobei nur die Koordinate, die dem Index des Wortes im W√∂rterbuch entspricht, den Wert 1 hat, die verbleibende 0. <br><br><img src="https://habrastorage.org/webt/ow/ta/vc/owtavcymm3e4a1k28xltaxdguds.png"><br><br>  Als n√§chstes trainieren wir ein einfaches einschichtiges neuronales Netzwerk ohne Aktivierung auf der inneren Schicht, um das n√§chste Wort im Kontext vorherzusagen, dh das Wort "am Abend" unter Verwendung des Wortes "Rocketman" vorherzusagen.  Am Ausgang erhalten wir die Wahrscheinlichkeitsverteilung f√ºr alle W√∂rter aus dem W√∂rterbuch wie folgt.  Da wir wissen, was das Wort wirklich war, k√∂nnen wir den Fehler berechnen, die Gewichte aktualisieren usw. <br><br><img src="https://habrastorage.org/webt/x6/m-/_e/x6m-_exb0v8m5mmge-q-mr9ez4a.png"><br><br>  Die aktualisierten Gewichte, die als Ergebnis des Trainings an unserer Stichprobe erhalten wurden, sind die Worteinbettung. <br><br>  Der Vorteil der Verwendung der Einbettung anstelle der Zahl besteht zum einen darin, <strong>dass der Kontext ber√ºcksichtigt wird</strong> .  Ein beliebtes Beispiel: Trump und Putin stehen sich in word2vec nahe, weil sie beide Pr√§sidenten sind und h√§ufig zusammen in Texten verwendet werden. <br><br>  F√ºr die W√∂rter, die im Trainingsbeispiel gefunden wurden, nehmen Sie einfach die Einbettungsmatrix, nehmen ihren Vektor anhand des Index des Wortes und erhalten die Einbettung. <br><br>  Es scheint, dass alles in Ordnung ist, au√üer dass einige W√∂rter in Ihrer Matrix m√∂glicherweise nicht in Ordnung sind, da das Modell sie w√§hrend des Trainings nicht gesehen hat.  Um das Problem unbekannter W√∂rter (au√üerhalb des Wortschatzes) zu l√∂sen, haben sie 2014 eine Modifikation von word2vec - <strong>Fasttext entwickelt</strong> . <br><br>  Fasttext funktioniert wie folgt: Wenn das Wort nicht im W√∂rterbuch enthalten ist, wird es in symbolische n-Gramm unterteilt. F√ºr jede n-Gramm-Einbettung wird die Einbettungsmatrix von n-Gramm (die wie word2vec trainiert werden) entnommen, Einbettungen werden gemittelt und ein Vektor wird erhalten. <br><br><img src="https://habrastorage.org/webt/sz/ws/mx/szwsmx8vblcqrbumxrjbzz7nzvo.png"><br><br>  Insgesamt erhalten wir Vektoren f√ºr W√∂rter, die nicht in unserem W√∂rterbuch enthalten sind.  Jetzt k√∂nnen wir die <strong>√Ñhnlichkeit auch f√ºr unbekannte W√∂rter berechnen</strong> .  Und vor allem gibt es geschulte Modelle f√ºr Russisch, Englisch und Chinesisch, zum Beispiel Facebook und das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DeepPavlov-</a> Projekt, sodass Sie dies schnell in Ihre Pipeline aufnehmen k√∂nnen. <br><br>  <strong>Die Nachteile bleiben jedoch bestehen:</strong> <br><br><ul><li>  Das Modell wird nicht f√ºr den gesamten Textvektor verwendet.  Um einen gemeinsamen Textvektor zu erhalten, m√ºssen Sie sich etwas √ºberlegen: Durchschnitt oder Durchschnitt mit Multiplikation mit IDF-Gewichten, und bei verschiedenen Aufgaben kann dies auf unterschiedliche Weise funktionieren. </li><li>  Der Vektor f√ºr ein Wort ist unabh√§ngig vom Kontext immer noch einer.  Word2vec trainiert einen Wortvektor f√ºr jeden Kontext, in dem das Wort vorkommt.  F√ºr mehrwertige W√∂rter (wie zum Beispiel Sprache) gibt es ein und denselben Vektor. </li></ul><br><img src="https://habrastorage.org/webt/ob/i3/ap/obi3apyrva_kjktfpfjh7farkx8.png"><br><br>  In der Tat ist die Kosinusn√§he in unserem Beispiel im Fasttext h√∂her als die Kosinusn√§he in tf-idf, obwohl diese Phrasen nur ‚Äûin‚Äú gemeinsam haben. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2p/lz/hq/2plzhqw0n1-8m4vrc71_zwokjla.png" width="400"></div><br>  <em>t-SNE auf Fasttext (Top 10 Absichten), F1-Punktzahl: 0,86</em> <br><br>  Bei der Visualisierung von Fasttextergebnissen bei der t-SNE-Zerlegung fallen Intent-Cluster jedoch viel schlechter auf als bei tf-idf.  Das F1-Ma√ü betr√§gt hier 0,86 statt 0,92. <br><br>  Wir haben ein Experiment durchgef√ºhrt: kombinierte tf-idf- und Fasttext-Vektoren.  Die Qualit√§t ist absolut die gleiche wie bei Verwendung von nur tf-idf.  Dies gilt nicht f√ºr alle Aufgaben. Es gibt Probleme, bei denen die Kombination aus tf-idf und Fasttext besser als nur tf-idf funktioniert oder bei denen Fasttext besser als tf-idf funktioniert.  Sie m√ºssen experimentieren und versuchen. <br><br>  Versuchen wir, die Anzahl der Absichten zu erh√∂hen (denken Sie daran, dass wir 170 davon haben).  Unten finden Sie Cluster f√ºr die 30 wichtigsten Absichten f√ºr tf-idf-Vektoren. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2k/a3/q9/2ka3q9iuylym9_cqdizf4dbtbfu.png" width="400"></div><br>  <em>t-SNE bei tf-idf (Top 30 Absichten), F1 Punktzahl 0, 85 (bei 10 war es 0,92)</em> <br><br>  Die Qualit√§t sinkt um 7 Punkte, und jetzt sehen wir keine ausgepr√§gte Clusterstruktur. <br><br>  Schauen wir uns Beispiele f√ºr Texte an, die verwirrt wurden, weil mehr Absichten hinzugef√ºgt wurden, die sich semantisch und in Worten √ºberschneiden. <br><br>  Zum Beispiel: "Und wenn Sie eine Einzahlung er√∂ffnen, wie hoch sind die Zinsen daf√ºr?"  und "Und ich m√∂chte einen Beitrag bei 7 Prozent er√∂ffnen."  Sehr √§hnliche S√§tze, aber das sind unterschiedliche Absichten.  Im ersten Fall m√∂chte eine Person die Bedingungen f√ºr Einzahlungen kennen und im zweiten Fall eine Einzahlung er√∂ffnen.  Um solche Texte in verschiedene Klassen zu unterteilen, brauchen wir etwas Komplexeres - <strong>tiefes Lernen</strong> . <br><br><h2>  Sprachmodell </h2><br>  Wir wollen einen Textvektor und insbesondere einen Wortvektor erhalten, der vom Verwendungskontext abh√§ngt.  Der Standardweg, um einen solchen Vektor zu erhalten, besteht darin <strong>, Einbettungen aus dem Sprachmodell zu verwenden</strong> . <br><br>  Das Sprachmodell l√∂st das Problem der Sprachmodellierung.  Und was ist diese Aufgabe?  Lassen Sie es eine Folge von W√∂rtern geben, zum Beispiel: "Ich werde nur in Gegenwart meiner eigenen sprechen ...", und wir versuchen, das n√§chste Wort in der Folge vorherzusagen.  Das Sprachmodell bietet Kontext f√ºr Einbettungen.  Nachdem man f√ºr jedes Wort kontextbezogene Einbettungen und Vektoren erhalten hat, kann man die Wahrscheinlichkeit des n√§chsten Wortes vorhersagen. <br><br>  Es gibt einen W√∂rterbuch-Dimensionsvektor, und jedem Wort wird die Wahrscheinlichkeit zugewiesen, dass es das n√§chste ist.  Wir wissen wieder, welches Wort in Wirklichkeit war, betrachten einen Fehler und trainieren das Modell. <br><br><img src="https://habrastorage.org/webt/1i/yd/f1/1iydf1xm3j97nwwuys6jmrq1ate.jpeg"><br><br>  Es gibt einige Sprachmodelle. Gab es letztes Jahr einen Boom?  und viele verschiedene Architekturen wurden vorgeschlagen.  Einer von ihnen ist <strong>ELMo</strong> . <br><br><h3>  ELMo </h3><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Die Idee des ELMo-Modells besteht darin</a> , zun√§chst f√ºr jedes Wort im Text eine symbolische <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Worteinbettung</a> zu erstellen und dann ein <strong>LSTM-Netzwerk</strong> f√ºr diese W√∂rter so anzuwenden, dass Einbettungen ber√ºcksichtigt werden, die den Kontext ber√ºcksichtigen, in dem das Wort vorkommt. <br><br>  Lassen Sie uns untersuchen, wie eine symbolische Einbettung erhalten wird: Wir teilen das Wort in Symbole auf, wenden f√ºr jedes Symbol eine Einbettungsebene an und erhalten eine Einbettungsmatrix.  Wenn es nur um Symbole geht, ist die Dimension einer solchen Matrix klein.  Dann wird eine eindimensionale Faltung auf die Einbettungsmatrix angewendet, wie dies normalerweise in NLP durchgef√ºhrt wird, wobei am Ende eine maximale Poolbildung erfolgt und ein Vektor erhalten wird.  Auf diesen Vektor wird ein zweischichtiges sogenanntes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Autobahnnetz</a> angewendet, das den <strong>allgemeinen Vektor eines Wortes</strong> berechnet. <br><br><img src="https://habrastorage.org/webt/in/zp/zw/inzpzwr-5-in2jiuszhjrkimczs.png"><br><br>  Dar√ºber hinaus wird das Modell eine Art Hypothese der Einbettung selbst f√ºr ein Wort erstellen, das nicht im Trainingssatz gefunden wurde. <br><br>  Nachdem wir f√ºr jedes Wort symbolische Einbettungen erhalten haben, wenden wir ein zweischichtiges BiLSTM-Netzwerk auf sie an. <br><br><img src="https://habrastorage.org/webt/vi/em/mb/viemmbyrjg0lboyuiq8bfbrgewy.png"><br><br>  Nach dem Anwenden eines zweischichtigen BiLSTM-Netzwerks werden normalerweise die verborgenen Zust√§nde der letzten Schicht genommen, und es wird angenommen, dass dies eine kontextbezogene Einbettung ist.  ELMo bietet jedoch zwei Funktionen: <br><br><ul><li>  <strong>Restverbindung</strong> zwischen dem Eingang der ersten LSTM-Schicht und ihrem Ausgang.  Der LSTM-Eingang wird dem Ausgang hinzugef√ºgt, um das Problem von Fading-Verl√§ufen zu vermeiden. </li><li>  Die Autoren von ELMo schlagen vor, die symbolische Einbettung f√ºr jedes Wort, die Ausgabe der ersten LSTM-Schicht und die Ausgabe der zweiten LSTM-Schicht mit einigen Gewichten zu kombinieren, die f√ºr jede Aufgabe ausgew√§hlt werden.  Dies ist erforderlich, um sowohl Funktionen auf niedriger Ebene als auch Funktionen auf h√∂herer Ebene zu ber√ºcksichtigen, die die erste und zweite Schicht von LSTM ergeben. </li></ul><br>  In unserem Problem haben wir eine einfache Mittelung dieser drei Einbettungen verwendet und so f√ºr jedes Wort eine kontextbezogene Einbettung erhalten. <br><br><img src="https://habrastorage.org/webt/8y/tl/pa/8ytlpa3lia0oxj461muadegcdyq.png"><br><br>  Das Sprachmodell bietet folgende Vorteile: <br><br><ul><li>  Der Vektor eines Wortes h√§ngt vom Kontext ab, in dem das Wort verwendet wird.  Das hei√üt, f√ºr das Wort "Sprache" im Sinne des K√∂rperteils und des sprachlichen Begriffs erhalten wir unterschiedliche Vektoren. </li><li>  Wie bei word2vec und fasttext gibt es viele trainierte Modelle, zum Beispiel aus dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DeepPavlov-</a> Projekt.  Sie k√∂nnen das fertige Modell nehmen und versuchen, es in Ihrer Aufgabe anzuwenden. </li><li>  Sie m√ºssen nicht mehr dar√ºber nachdenken, wie die Wortvektoren gemittelt werden.  Das ELMo-Modell erzeugt sofort einen Vektor des gesamten Textes. </li><li>  Sie k√∂nnen das Sprachmodell f√ºr Ihre Aufgabe neu trainieren. Hierf√ºr gibt es verschiedene M√∂glichkeiten, z. B. ULMFiT. </li></ul><br>  Das einzige Minus bleibt - das <strong>Sprachmodell garantiert nicht,</strong> dass Texte, die zur selben Klasse geh√∂ren, dh zu einer Absicht, im Vektorraum nahe beieinander liegen. <br><br><img src="https://habrastorage.org/webt/hv/09/qf/hv09qfkirx8mbcfl8rvbsd11aiu.png"><br><br>  In unserem Restaurantbeispiel sind die Kosinuswerte nach dem ELMo-Modell wirklich h√∂her geworden. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/m8/d0/ax/m8d0axjr1fysz33kaoi7ydg4kga.png" width="400"></div><br>  <em>t-SNE auf ELMo (Top 10 Absichten), F1-Punktzahl 0,93 (0,92 von tf-idf)</em> <br><br>  Cluster mit Top-10-Absichten sind ebenfalls ausgepr√§gter.  In der obigen Abbildung sind alle 10 Cluster deutlich sichtbar, w√§hrend die Genauigkeit leicht zugenommen hat. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ux/fx/ws/uxfxws0f9y1bf-zdpjynmle3xqk.png" width="400"></div><br>  <em>t-SNE auf ELMo (Top 30 Absichten) F1-Punktzahl 0,86 (0,85 von tf-idf)</em> <br><br>  Bei den Top-30-Absichten bleibt die Clusterstruktur erhalten, und die Qualit√§t steigt um einen Punkt. <br><br>  Aber in einem solchen Modell gibt es keine Garantie daf√ºr, dass die Vorschl√§ge "Und wenn Sie eine Einzahlung er√∂ffnen, was sind die Zinsen auf sie?"  und "Und ich m√∂chte einen Beitrag bei 7 Prozent er√∂ffnen" wird weit voneinander entfernt sein, obwohl sie in verschiedenen Klassen liegen.  Mit ELMo lernen wir einfach das Sprachmodell, und wenn die semantisch √§hnlichen Texte, dann sind sie nah.  <strong>ELMo wei√ü nichts √ºber unsere Klassen</strong> , aber Sie k√∂nnen <strong>Textvektoren mit</strong> derselben Absicht im Raum mithilfe von Klassenbeschriftungen zusammenf√ºhren. <br><br><h3>  Siamesisches Netzwerk </h3><br>  Nehmen Sie Ihre bevorzugte neuronale Netzwerkarchitektur f√ºr die Textvektorisierung und zwei Beispiele f√ºr Absichten.  F√ºr jedes der Beispiele erhalten wir Einbettungen und berechnen dann den Kosinusabstand zwischen ihnen. <br><br><img src="https://habrastorage.org/webt/ah/sw/ha/ahswhaivfdofbehikcsi7qvhntk.jpeg"><br><br>  Der Kosinusabstand ist gleich eins minus der Kosinusn√§he, die wir zuvor getroffen haben. <br><br>  Dieser Ansatz wird als <strong>siamesisches Netzwerk bezeichnet</strong> . <br><br>  Wir m√∂chten, dass Texte aus derselben Klasse, zum Beispiel ‚Äû√úberweisung machen‚Äú und ‚ÄûGeld werfen‚Äú, dicht im Raum liegen.  Das hei√üt, der Kosinusabstand zwischen ihren Vektoren sollte so klein wie m√∂glich sein, idealerweise Null.  Und Texte, die sich auf unterschiedliche Absichten beziehen, sollten so weit wie m√∂glich voneinander entfernt sein. <br><br>  In der Praxis funktioniert diese Trainingsmethode jedoch nicht so gut, da Objekte verschiedener Klassen nicht ausreichend voneinander entfernt sind.  Die Verlustfunktion <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">"Triplettverlust"</a> funktioniert viel besser.  Es werden Tripel von Objekten verwendet, die als Drillinge bezeichnet werden. <br><br>  Die Abbildung zeigt ein Triplett: ein Ankerobjekt in einem blauen Kreis, ein positives Objekt in Gr√ºn und ein negatives Objekt in einem roten Kreis.  Das negative Objekt und der Anker befinden sich in verschiedenen Klassen, und das positive und der Anker befinden sich in einer. <br><br><img src="https://habrastorage.org/webt/qw/38/lz/qw38lz9wpgcphm8w2ic55aqbhea.png"><br><br>  Wir wollen sicherstellen, dass das positive Objekt nach dem Training n√§her am Anker liegt als das negative.  Dazu betrachten wir den Kosinusabstand zwischen den Objektpaaren und geben den Hyperparameter - "Rand" - den Abstand ein, den wir zwischen den positiven und negativen Objekten erwarten. <br><br><img src="https://habrastorage.org/webt/lv/ib/b8/lvibb8qcivpp0evrqxgnyezlo0a.png"><br><br>  Die Verlustfunktion sieht folgenderma√üen aus: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>t</mi><mi>r</mi><mi>i</mi><mi>p</mi><mi>l</mi><mi>e</mi><mi>t</mi><msub><mtext>&amp;#xA0;</mtext><mi>l</mi></msub><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy=&quot;false&quot;>[</mo><mn>0</mn><mo>,</mo><mi>R</mi><mi>a</mi><mi>n</mi><mi>d</mi><mo>+</mo><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy=&quot;false&quot;>(</mo><mi>A</mi><mo>,</mo><mi>P</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy=&quot;false&quot;>(</mo><mi>A</mi><mo>,</mo><mi>N</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>]</mo><mo>.</mo></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="55.668ex" height="2.66ex" viewBox="0 -832 23968 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-74" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-72" x="361" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-69" x="813" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-70" x="1158" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-6C" x="1662" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-65" x="1960" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-74" x="2427" y="0"></use><g transform="translate(2788,0)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-6C" x="353" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-6F" x="3349" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-73" x="3835" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-73" x="4304" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-3D" x="5051" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-6D" x="6358" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-61" x="7236" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-78" x="7766" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-5B" x="8338" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-30" x="8617" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-2C" x="9117" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-52" x="9562" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-61" x="10322" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-6E" x="10851" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-64" x="11452" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-2B" x="12198" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-64" x="13198" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-69" x="13722" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-73" x="14067" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-74" x="14537" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-28" x="14898" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-41" x="15288" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-2C" x="16038" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-50" x="16483" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-29" x="17235" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-2212" x="17847" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-64" x="18847" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-69" x="19371" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-73" x="19716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-74" x="20186" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-28" x="20547" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-41" x="20937" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-2C" x="21687" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-4E" x="22133" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-29" x="23021" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-5D" x="23411" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-2E" x="23689" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>t</mi><mi>r</mi><mi>i</mi><mi>p</mi><mi>l</mi><mi>e</mi><mi>t</mi><msub><mtext>&nbsp;</mtext><mi>l</mi></msub><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mtext>&nbsp;</mtext><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi>R</mi><mi>a</mi><mi>n</mi><mi>d</mi><mo>+</mo><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false">(</mo><mi>A</mi><mo>,</mo><mi>P</mi><mo stretchy="false">)</mo><mo>‚àí</mo><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false">(</mo><mi>A</mi><mo>,</mo><mi>N</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>.</mo></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> triplet \ _loss = \ max [0, Rand + dist (A, P) - dist (A, N)]. </script></p><br>  Mit anderen Worten, w√§hrend des Trainings erreichen wir, dass das positive Objekt n√§her am Anker liegt als das negative, zumindest am Rand.  Wenn die Verlustfunktion Null ist, funktioniert sie und wir beenden das Training, andernfalls minimieren wir weiterhin die Zielfunktion. <br><br>  Nachdem wir das Modell trainiert haben, erhalten wir immer noch keinen Klassifikator. Es ist nur eine Methode, um solche Einbettungen zu erhalten, dass Objekte, die in einer Absicht liegen, h√∂chstwahrscheinlich enge Vektoren haben. <br><br>  Wenn wir das Modell erhalten haben, k√∂nnen wir zus√§tzlich zu den Einbettungen eine andere Klassifizierungsmethode verwenden.  <strong>KNN</strong> passt gut, da wir bereits festgestellt haben, dass Einbettungen eine unterschiedliche Clusterstruktur aufweisen. <br><br>  Erinnern Sie sich daran, wie kNN f√ºr Texte funktioniert: Nehmen Sie ein Element des Textes, binden Sie es ein, √ºbersetzen Sie es in den Vektorraum und sehen Sie dann, wer sein Nachbar ist.  Unter den Nachbarn betrachten wir die h√§ufigste Klasse und schlie√üen daraus, dass das neue Objekt zu dieser Klasse geh√∂rt. <br><br>  Die Dimension der von uns verwendeten Einbettungen betr√§gt 300, und im Trainingsbeispiel befinden sich etwa 500.000 Objekte.  Standardmethoden zur Suche nach den n√§chsten Nachbarn passen in Bezug auf die Leistung nicht zu uns.  Wir haben die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">HNSW-</a> Methode verwendet - <strong>Hierarchical Navigable Small World</strong> . <br><br>  Navigable Small World ist ein zusammenh√§ngendes Diagramm, in dem nur wenige Kanten zwischen Scheitelpunkten in gro√üer Entfernung und viele Kanten zwischen Scheitelpunkten in der N√§he vorhanden sind.  In unserem Fall wird die Kantenl√§nge durch den Kosinusabstand bestimmt, d.h.  F√ºr ein Trainingsbeispiel berechnen wir den Abstand zwischen allen Beispielen von Absichten und werfen dann zuf√§llig sehr gro√üe Entfernungen aus, damit der Graph weiterhin verbunden bleibt. <br><br>      ,    Hierarchical.        ,  ,       ,    .            . <br><br>       , ,         ,     ,      . <br><br>     ,     ,       , ,  ,       .   ,    ,         ,     <strong>  ‚Äî  0,95-0,99</strong> ,    . <br><br>  ,       ,     ,          , <strong>   </strong> .              . <br><br>  ,      .    ,      .          . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/db/2v/yx/db2vyx8eoubfaytjge3imauvgra.png" width="400"></div><br> <em>t-SNE  siamese (-10 ), F1 score 0,95 (0,93  ELMo)</em> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/rz/ib/op/rzibopijmd5mqginmihafxfiivy.png" width="400"></div><br> <em>t-SNE  siamese (-30 ), F1 score 0,87 (0,86  ELMo)</em> <br><br>  10             ELMo,  30 ‚Äî  ,       . <br><br><h2>  </h2><br> <b>      ,     </b> , , 2-5,          .    ,     ,        ,     20-30  .      ,   . <br><br> <b>  ,      ,     ,       tf-idf</b> .    ,      ,    ,       . <br><br> <b>   ,    word2vec  fasttext.</b>   ,   ,         .        ,      ,       ,     . <br><br>   ,  ,   ELMo.       , , ,      ,       ,      . <b>   ELMo,        </b> ,           . <br><br>             ,    -  .       .      ,              .  ,   ,           .  ,     ,     ..    ,      . <br><div class="scrollable-table"><table><tbody><tr><td> F1-score </td><td> ~2-5  <br>   </td><td> ~10  <br>   </td><td> ~30  <br>   </td></tr><tr><td> ,  </td><td>  MVP </td><td>    </td><td>    </td></tr><tr><td> ML + tf-idf </td><td>  </td><td> 0,92 </td><td> 0,85 </td></tr><tr><td> ML + fasttext </td><td>  warum? </td><td>  0,86 </td><td>  0,82 </td></tr><tr><td> ELMo </td><td> ?? </td><td> 0,93 </td><td>  0,86 </td></tr><tr><td> siamese </td><td> ??? </td><td> 0,95 </td><td> 0,87 </td></tr></tbody></table></div>  <b>N√ºtzliche Links:</b> <br><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">rusvectores.org/ru/models</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">docs.deeppavlov.ai/en/master/intro/pretrained_vectors.html</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">www.mihaileric.com/posts/deep-contextualized-word-representations-elmo</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">omoindrot.github.io/triplet-loss</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">towardsdatascience.com/review-highway-networks-gating-function-to-highway-image-classification-5a33833797b5</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">habr.com/ru/company/mailru/blog/338360</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">http://jalammar.github.io/illustrated-bert</a> </li></ul><br><blockquote>    ‚Äî ¬´Deep Learning vs common sense¬ª ‚Äî       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">UseData Conf</a> .  ,    -   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a>  18        ,      ,          . <br><br>        ,        ,    ,         ,   16   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">UseData Conf</a> . </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de455652/">https://habr.com/ru/post/de455652/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de455642/index.html">Die Architektur des verteilten Nachrichtenwarteschlangendienstes in Yandex.Cloud</a></li>
<li><a href="../de455644/index.html">Wir verwenden Daten in der Praxis</a></li>
<li><a href="../de455646/index.html">Sicherheitswoche 24: Werks-Backdoors auf Android-Smartphones</a></li>
<li><a href="../de455648/index.html">Lebenszyklus ML</a></li>
<li><a href="../de455650/index.html">Wie wir ein neuronales Netzwerk trainiert haben, um Schrauben zu klassifizieren</a></li>
<li><a href="../de455658/index.html">Legend√§rer Intel Core i7-2600K: Testen von Sandy Bridge im Jahr 2019 (Teil 3)</a></li>
<li><a href="../de455662/index.html">Gro√ües mechanisches Display mit Nockenmechanismus als Decoder</a></li>
<li><a href="../de455666/index.html">Aufbau von Outbound Sales in einem IT-Service-Unternehmen</a></li>
<li><a href="../de455668/index.html">Wir schreiben unter FPGA ohne HDL. Vergleich von Entwicklungswerkzeugen auf hoher Ebene</a></li>
<li><a href="../de455670/index.html">Wie 3D-Drucker Knochen, Blutgef√§√üe und Organe drucken</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>