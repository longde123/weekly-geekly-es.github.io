<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🐎 🌬️ 👨🏾‍🎨 Deep Learning gegen gesunden Menschenverstand: Entwicklung eines Chat-Bots 👨🏽‍🚒 🥛 ⏬</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Je mehr Benutzer Ihres Dienstes sind, desto höher ist die Wahrscheinlichkeit, dass sie Hilfe benötigen. Der Chat des technischen Supports ist eine off...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Deep Learning gegen gesunden Menschenverstand: Entwicklung eines Chat-Bots</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/455652/">  Je mehr Benutzer Ihres Dienstes sind, desto höher ist die Wahrscheinlichkeit, dass sie Hilfe benötigen.  Der Chat des technischen Supports ist eine offensichtliche, aber recht teure Lösung.  Wenn Sie jedoch Technologie für maschinelles Lernen verwenden, können Sie etwas Geld sparen. <br><br>  Der Bot kann nun einfache Fragen beantworten.  Darüber hinaus kann dem Chatbot beigebracht werden, die Absichten des Benutzers zu bestimmen und den Kontext zu erfassen, damit er die meisten Probleme der Benutzer ohne menschliches Eingreifen lösen kann.  Wie das geht, werden Vladislav Blinov und Valery Baranova, Entwickler des beliebten Assistenten Oleg, helfen, es herauszufinden. <br><br><img src="https://habrastorage.org/webt/rr/do/kw/rrdokweqzrtdqiogus2vahosksg.png"><br><br>  Beim Übergang von einfachen zu komplizierteren Methoden bei der Entwicklung eines Chat-Bots werden wir praktische Implementierungsprobleme analysieren und herausfinden, welcher Qualitätsgewinn erzielt werden kann und wie viel er kosten wird. <br><a name="habracut"></a><br><iframe width="560" height="315" src="https://www.youtube.com/embed/eL3dkh-WaSU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <strong>Vladislav Blinov</strong> ist ein leitender Entwickler von Dialogsystemen in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tinkoff</a> und verwendet häufig Abkürzungen: ML, NLP, DL usw.  Darüber hinaus untersucht die Graduiertenschule die Modellierung von Humor durch maschinelles Lernen und neuronale Netze. <br><br>  <strong>Valeria Baranova</strong> schreibt seit über 5 Jahren coole Dinge im NLP-Bereich in Python.  Jetzt erstellt Tinkoff im Team interaktiver Systeme Chat-Bots und unterrichtet einen Kurs zum maschinellen Lernen für Studenten.  Er forscht auch auf dem Gebiet des rechnerischen Humors, dh er lehrt die KI, Witze zu verstehen und neue zu erfinden - Valeria und Vladislav <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">werden darüber</a> bei UseData Conf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sprechen</a> . <br><br>  Die Dienste der Tinkoff Bank werden von Millionen von Menschen genutzt.  Um eine solche Anzahl von Benutzern rund um die Uhr zu unterstützen, wird ein großes Personal benötigt, was zu hohen Servicekosten führt.  Es erscheint logisch, dass die beliebten Fragen der Benutzer mithilfe des Chat-Bots automatisch beantwortet werden können. <br><br><h2>  Benutzerabsicht oder Absicht </h2><br>  Das erste, was ein Chatbot braucht, ist zu verstehen, <strong>was der Benutzer will</strong> .  Diese Aufgabe wird als Klassifizierung von Absichten oder Absichten bezeichnet.  Darüber hinaus werden alle Modelle und Ansätze im Rahmen dieser Aufgabe berücksichtigt. <br><br>  Schauen wir uns ein Beispiel für die Klassifizierung von Absichten an.  Wenn Sie schreiben: "Überweisen Sie hundert Lera", wird der Chat-Bot Oleg verstehen, dass dies die Absicht einer Geldüberweisung ist, dh die Absicht des Benutzers, Geld zu überweisen.  Oder besser gesagt, dass Lera die Menge von 100 Rubel übertragen muss. <br><br>  Wir werden Methoden vergleichen und die Qualität ihrer Arbeit an einem Testmuster testen, das aus echten Dialogen mit Benutzern besteht.  Unsere Stichprobe enthält mehr als 30.000 markierte Beispiele und 170 Absichten, zum Beispiel: ins Kino gehen, nach Restaurants suchen, eine Kaution eröffnen oder schließen usw.  Oleg hat auch seine eigene Meinung zu vielem und er kann einfach mit dir chatten. <br><br><h2>  Wörterbuchklassifikation </h2><br>  Das Einfachste, was bei der Klassifizierung von Absichten getan werden kann, ist die <strong>Verwendung eines Wörterbuchs</strong> .  Wenn beispielsweise das Wort "übersetzen" in der Phrase eines Benutzers vorkommt, sollten Sie eine Geldüberweisung in Betracht ziehen. <br><br>  Schauen wir uns die Qualität eines so einfachen Ansatzes an. <br><div class="scrollable-table"><table><tbody><tr><td></td><td>  Präzision </td><td>  Rückruf </td><td>  F1-Score </td></tr><tr><td>  Geldüberweisung </td><td>  0,88 </td><td>  0,23 </td><td>  0,36 </td></tr><tr><td>  Der Rest </td><td>  0,97 </td><td>  0,99 </td><td>  0,98 </td></tr></tbody></table></div>  Wenn der Klassifizierer die Absicht des Benutzers einfach als "Geldtransfer" mit dem Wort "übersetzen" definiert, ist die Qualität bereits recht hoch.  Präzision - 88%, während die Vollständigkeit gering ist, entspricht nur 23%.  Das ist verständlich: Das Wort „übersetzen“ beschreibt nicht alle Möglichkeiten, „Geld an jemanden überweisen“ zu sagen. <br><br>  Dieser Ansatz hat jedoch Vorteile: <br><br><ul><li>  Es ist keine gekennzeichnete Stichprobe erforderlich (wenn Sie das Modell nicht untersuchen, ist keine Stichprobe erforderlich). </li><li>  Sie können eine hohe Genauigkeit erzielen, wenn Sie Wörterbücher gut kompilieren (dies erfordert jedoch Zeit und Ressourcen). </li></ul><br>  Die Vollständigkeit einer solchen Lösung dürfte jedoch gering sein, da alle Variationen einer Klasse schwer zu beschreiben sind. <br><br>  Betrachten Sie ein Gegenbeispiel.  Wenn zusätzlich zur Geldtransferabsicht "Überweisung" auch die zweite Absicht enthalten kann - "Überweisung an den Betreiber".  Wenn wir dem Operator eine neue Übersetzungsabsicht hinzufügen, erhalten wir unterschiedliche Ergebnisse. <br><div class="scrollable-table"><table><tbody><tr><td></td><td>  Präzision </td><td>  Rückruf </td><td>  F1-Score </td></tr><tr><td>  Geldüberweisung </td><td>  0,70 </td><td>  0,23 </td><td>  0,34 </td></tr><tr><td>  Der Rest </td><td>  0,97 </td><td>  0,99 </td><td>  0,98 </td></tr></tbody></table></div>  Die Genauigkeit sinkt um 18 Punkte, während die Vollständigkeit natürlich nicht zunimmt.  Dies zeigt, dass ein fortgeschrittener Ansatz erforderlich ist. <br><br><h2>  Textanalyse </h2><br>  Bevor Sie maschinelles Lernen verwenden, müssen Sie wissen, wie Sie Text als Vektor darstellen.  Einer der einfachsten Ansätze ist die <strong>Verwendung eines tf-idf-Vektors</strong> . <br><br>  Der tf-idf-Vektor berücksichtigt das Auftreten jedes Wortes in der Phrase des Benutzers und das gesamte Auftreten von Wörtern in der Sammlung.  Wörter, die häufig in verschiedenen Texten vorkommen, haben in dieser Vektordarstellung weniger Gewicht. <br><br>  Betrachten wir die Qualität des linearen Modells für tf-idf-Darstellungen (in unserem Fall logistische Regression). <br><div class="scrollable-table"><table><tbody><tr><td></td><td>  Präzision </td><td>  Rückruf </td><td>  F1-Score </td></tr><tr><td>  Geldüberweisung </td><td>  0,74 </td><td>  0,86 </td><td>  0,80 </td></tr><tr><td>  Der Rest </td><td>  0,99 </td><td>  0,99 </td><td>  0,99 </td></tr></tbody></table></div>  Infolgedessen nahm <strong>die Vollständigkeit</strong> stark <strong>zu</strong> , und die Genauigkeit blieb mit der Verwendung des Wörterbuchs vergleichbar. Das f1-Maß (gewichtetes harmonisches Mittel zwischen Genauigkeit und Vollständigkeit) nahm ebenfalls zu.  Das heißt, das Modell selbst versteht bereits, welche Wörter für welche Absicht wichtig sind - Sie müssen selbst nichts erfinden. <br><br><h3>  Datenvisualisierung </h3><br>  Die Datenvisualisierung hilft zu verstehen, wie Absichten aussehen und wie gut sie im Raum gruppiert sind.  Aufgrund der großen Dimension können wir tf-idf-Darstellungen jedoch nicht direkt visualisieren. Daher verwenden wir <strong>die Dimensionskomprimierungsmethode - t-SNE</strong> . <br><br><img src="https://habrastorage.org/webt/zx/gh/dc/zxghdc_rwrmjjqojzp9b5lao6tq.png"><br><br>  Der Hauptunterschied zwischen dieser Methode und PCA besteht darin, dass bei der Übertragung in den zweidimensionalen Raum der <strong>relative Abstand zwischen den Objekten erhalten bleibt</strong> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1p/8w/xr/1p8wxr2sc5guuqktq4u7aqe88jk.png" width="400"></div><br>  <em>t-SNE auf tf-idf (Top 10 Absichten), F1-Punktzahl 0,92</em> <br><br>  Die Top 10 Absichten nach Vorkommen in unserer Sammlung sind oben dargestellt.  Es gibt grüne Punkte, die keiner Absicht angehören, und 10 Cluster, die mit unterschiedlichen Farben markiert sind, sind unterschiedliche Absichten.  Es ist zu sehen, dass einige von ihnen sehr gut gruppiert sind.  Das gewichtete <strong>f1-Maß beträgt 0,92</strong> - das ist ziemlich viel, man kann schon damit arbeiten. <br><br>  Also mit einem linearen Klassifikator über tf-idf: <br><br><ul><li>  viel höhere Vollständigkeit als die Verwendung eines Wörterbuchs mit vergleichbarer Genauigkeit; </li><li>  Sie müssen sich nicht überlegen, welche Wörter welcher Absicht entsprechen. </li></ul><br>  Es gibt aber auch Nachteile: <br><br><ul><li>  Mit begrenztem Wortschatz können Sie nur für die Wörter Gewicht bekommen, die im Trainingsbeispiel vorhanden sind. </li><li>  Umformulierung wird nicht berücksichtigt; </li><li>  Die Reihenfolge, in der die Wörter im Text vorkommen, wird nicht berücksichtigt. </li></ul><br><h2>  Umformulieren </h2><br>  Betrachten wir das Problem der Neuformulierung genauer. <br><br>  Tf-idf-Vektoren können nur für Texte nahe sein, die sich in Wörtern schneiden.  Die Nähe zwischen den Vektoren kann durch den Kosinus des Winkels zwischen ihnen berechnet werden.  Die Kosinusnähe in der Vektordarstellung tf-idf wird für bestimmte Beispiele berechnet. <br><br><img src="https://habrastorage.org/webt/gf/7c/eb/gf7ceblapupg3xysxiyxgcjwo3o.jpeg"><br><br>  Dies sind keine sehr engen Sätze für die tf-idf-Vektordarstellung, obwohl es für uns dieselbe Absicht und dieselbe Klasse ist. <br><br>  Was kann man dagegen tun?  Anstelle einer Zahl können Sie beispielsweise ein Wort als ganzen Vektor darstellen - dies wird als "Worteinbettung" bezeichnet. <br><br><img src="https://habrastorage.org/webt/rl/9n/xm/rl9nxmdp8sr_q7fwo7iokyj8hwy.png"><br><br>  Eines der beliebtesten Modelle zur Lösung dieses Problems wurde 2013 vorgeschlagen.  Es heißt <strong>word2vec</strong> und ist seitdem weit verbreitet. <br><br>  Eine der Möglichkeiten, Word2vec zu lernen, funktioniert ungefähr wie folgt: Wir nehmen den Text, nehmen ein Wort aus dem Kontext und werfen es weg, dann nehmen wir ein anderes zufälliges Wort aus dem Kontext und präsentieren beide Wörter als One-Hot-Vektoren.  Ein heißer Vektor ist ein Vektor gemäß der Wörterbuchdimension, wobei nur die Koordinate, die dem Index des Wortes im Wörterbuch entspricht, den Wert 1 hat, die verbleibende 0. <br><br><img src="https://habrastorage.org/webt/ow/ta/vc/owtavcymm3e4a1k28xltaxdguds.png"><br><br>  Als nächstes trainieren wir ein einfaches einschichtiges neuronales Netzwerk ohne Aktivierung auf der inneren Schicht, um das nächste Wort im Kontext vorherzusagen, dh das Wort "am Abend" unter Verwendung des Wortes "Rocketman" vorherzusagen.  Am Ausgang erhalten wir die Wahrscheinlichkeitsverteilung für alle Wörter aus dem Wörterbuch wie folgt.  Da wir wissen, was das Wort wirklich war, können wir den Fehler berechnen, die Gewichte aktualisieren usw. <br><br><img src="https://habrastorage.org/webt/x6/m-/_e/x6m-_exb0v8m5mmge-q-mr9ez4a.png"><br><br>  Die aktualisierten Gewichte, die als Ergebnis des Trainings an unserer Stichprobe erhalten wurden, sind die Worteinbettung. <br><br>  Der Vorteil der Verwendung der Einbettung anstelle der Zahl besteht zum einen darin, <strong>dass der Kontext berücksichtigt wird</strong> .  Ein beliebtes Beispiel: Trump und Putin stehen sich in word2vec nahe, weil sie beide Präsidenten sind und häufig zusammen in Texten verwendet werden. <br><br>  Für die Wörter, die im Trainingsbeispiel gefunden wurden, nehmen Sie einfach die Einbettungsmatrix, nehmen ihren Vektor anhand des Index des Wortes und erhalten die Einbettung. <br><br>  Es scheint, dass alles in Ordnung ist, außer dass einige Wörter in Ihrer Matrix möglicherweise nicht in Ordnung sind, da das Modell sie während des Trainings nicht gesehen hat.  Um das Problem unbekannter Wörter (außerhalb des Wortschatzes) zu lösen, haben sie 2014 eine Modifikation von word2vec - <strong>Fasttext entwickelt</strong> . <br><br>  Fasttext funktioniert wie folgt: Wenn das Wort nicht im Wörterbuch enthalten ist, wird es in symbolische n-Gramm unterteilt. Für jede n-Gramm-Einbettung wird die Einbettungsmatrix von n-Gramm (die wie word2vec trainiert werden) entnommen, Einbettungen werden gemittelt und ein Vektor wird erhalten. <br><br><img src="https://habrastorage.org/webt/sz/ws/mx/szwsmx8vblcqrbumxrjbzz7nzvo.png"><br><br>  Insgesamt erhalten wir Vektoren für Wörter, die nicht in unserem Wörterbuch enthalten sind.  Jetzt können wir die <strong>Ähnlichkeit auch für unbekannte Wörter berechnen</strong> .  Und vor allem gibt es geschulte Modelle für Russisch, Englisch und Chinesisch, zum Beispiel Facebook und das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DeepPavlov-</a> Projekt, sodass Sie dies schnell in Ihre Pipeline aufnehmen können. <br><br>  <strong>Die Nachteile bleiben jedoch bestehen:</strong> <br><br><ul><li>  Das Modell wird nicht für den gesamten Textvektor verwendet.  Um einen gemeinsamen Textvektor zu erhalten, müssen Sie sich etwas überlegen: Durchschnitt oder Durchschnitt mit Multiplikation mit IDF-Gewichten, und bei verschiedenen Aufgaben kann dies auf unterschiedliche Weise funktionieren. </li><li>  Der Vektor für ein Wort ist unabhängig vom Kontext immer noch einer.  Word2vec trainiert einen Wortvektor für jeden Kontext, in dem das Wort vorkommt.  Für mehrwertige Wörter (wie zum Beispiel Sprache) gibt es ein und denselben Vektor. </li></ul><br><img src="https://habrastorage.org/webt/ob/i3/ap/obi3apyrva_kjktfpfjh7farkx8.png"><br><br>  In der Tat ist die Kosinusnähe in unserem Beispiel im Fasttext höher als die Kosinusnähe in tf-idf, obwohl diese Phrasen nur „in“ gemeinsam haben. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2p/lz/hq/2plzhqw0n1-8m4vrc71_zwokjla.png" width="400"></div><br>  <em>t-SNE auf Fasttext (Top 10 Absichten), F1-Punktzahl: 0,86</em> <br><br>  Bei der Visualisierung von Fasttextergebnissen bei der t-SNE-Zerlegung fallen Intent-Cluster jedoch viel schlechter auf als bei tf-idf.  Das F1-Maß beträgt hier 0,86 statt 0,92. <br><br>  Wir haben ein Experiment durchgeführt: kombinierte tf-idf- und Fasttext-Vektoren.  Die Qualität ist absolut die gleiche wie bei Verwendung von nur tf-idf.  Dies gilt nicht für alle Aufgaben. Es gibt Probleme, bei denen die Kombination aus tf-idf und Fasttext besser als nur tf-idf funktioniert oder bei denen Fasttext besser als tf-idf funktioniert.  Sie müssen experimentieren und versuchen. <br><br>  Versuchen wir, die Anzahl der Absichten zu erhöhen (denken Sie daran, dass wir 170 davon haben).  Unten finden Sie Cluster für die 30 wichtigsten Absichten für tf-idf-Vektoren. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2k/a3/q9/2ka3q9iuylym9_cqdizf4dbtbfu.png" width="400"></div><br>  <em>t-SNE bei tf-idf (Top 30 Absichten), F1 Punktzahl 0, 85 (bei 10 war es 0,92)</em> <br><br>  Die Qualität sinkt um 7 Punkte, und jetzt sehen wir keine ausgeprägte Clusterstruktur. <br><br>  Schauen wir uns Beispiele für Texte an, die verwirrt wurden, weil mehr Absichten hinzugefügt wurden, die sich semantisch und in Worten überschneiden. <br><br>  Zum Beispiel: "Und wenn Sie eine Einzahlung eröffnen, wie hoch sind die Zinsen dafür?"  und "Und ich möchte einen Beitrag bei 7 Prozent eröffnen."  Sehr ähnliche Sätze, aber das sind unterschiedliche Absichten.  Im ersten Fall möchte eine Person die Bedingungen für Einzahlungen kennen und im zweiten Fall eine Einzahlung eröffnen.  Um solche Texte in verschiedene Klassen zu unterteilen, brauchen wir etwas Komplexeres - <strong>tiefes Lernen</strong> . <br><br><h2>  Sprachmodell </h2><br>  Wir wollen einen Textvektor und insbesondere einen Wortvektor erhalten, der vom Verwendungskontext abhängt.  Der Standardweg, um einen solchen Vektor zu erhalten, besteht darin <strong>, Einbettungen aus dem Sprachmodell zu verwenden</strong> . <br><br>  Das Sprachmodell löst das Problem der Sprachmodellierung.  Und was ist diese Aufgabe?  Lassen Sie es eine Folge von Wörtern geben, zum Beispiel: "Ich werde nur in Gegenwart meiner eigenen sprechen ...", und wir versuchen, das nächste Wort in der Folge vorherzusagen.  Das Sprachmodell bietet Kontext für Einbettungen.  Nachdem man für jedes Wort kontextbezogene Einbettungen und Vektoren erhalten hat, kann man die Wahrscheinlichkeit des nächsten Wortes vorhersagen. <br><br>  Es gibt einen Wörterbuch-Dimensionsvektor, und jedem Wort wird die Wahrscheinlichkeit zugewiesen, dass es das nächste ist.  Wir wissen wieder, welches Wort in Wirklichkeit war, betrachten einen Fehler und trainieren das Modell. <br><br><img src="https://habrastorage.org/webt/1i/yd/f1/1iydf1xm3j97nwwuys6jmrq1ate.jpeg"><br><br>  Es gibt einige Sprachmodelle. Gab es letztes Jahr einen Boom?  und viele verschiedene Architekturen wurden vorgeschlagen.  Einer von ihnen ist <strong>ELMo</strong> . <br><br><h3>  ELMo </h3><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Die Idee des ELMo-Modells besteht darin</a> , zunächst für jedes Wort im Text eine symbolische <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Worteinbettung</a> zu erstellen und dann ein <strong>LSTM-Netzwerk</strong> für diese Wörter so anzuwenden, dass Einbettungen berücksichtigt werden, die den Kontext berücksichtigen, in dem das Wort vorkommt. <br><br>  Lassen Sie uns untersuchen, wie eine symbolische Einbettung erhalten wird: Wir teilen das Wort in Symbole auf, wenden für jedes Symbol eine Einbettungsebene an und erhalten eine Einbettungsmatrix.  Wenn es nur um Symbole geht, ist die Dimension einer solchen Matrix klein.  Dann wird eine eindimensionale Faltung auf die Einbettungsmatrix angewendet, wie dies normalerweise in NLP durchgeführt wird, wobei am Ende eine maximale Poolbildung erfolgt und ein Vektor erhalten wird.  Auf diesen Vektor wird ein zweischichtiges sogenanntes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Autobahnnetz</a> angewendet, das den <strong>allgemeinen Vektor eines Wortes</strong> berechnet. <br><br><img src="https://habrastorage.org/webt/in/zp/zw/inzpzwr-5-in2jiuszhjrkimczs.png"><br><br>  Darüber hinaus wird das Modell eine Art Hypothese der Einbettung selbst für ein Wort erstellen, das nicht im Trainingssatz gefunden wurde. <br><br>  Nachdem wir für jedes Wort symbolische Einbettungen erhalten haben, wenden wir ein zweischichtiges BiLSTM-Netzwerk auf sie an. <br><br><img src="https://habrastorage.org/webt/vi/em/mb/viemmbyrjg0lboyuiq8bfbrgewy.png"><br><br>  Nach dem Anwenden eines zweischichtigen BiLSTM-Netzwerks werden normalerweise die verborgenen Zustände der letzten Schicht genommen, und es wird angenommen, dass dies eine kontextbezogene Einbettung ist.  ELMo bietet jedoch zwei Funktionen: <br><br><ul><li>  <strong>Restverbindung</strong> zwischen dem Eingang der ersten LSTM-Schicht und ihrem Ausgang.  Der LSTM-Eingang wird dem Ausgang hinzugefügt, um das Problem von Fading-Verläufen zu vermeiden. </li><li>  Die Autoren von ELMo schlagen vor, die symbolische Einbettung für jedes Wort, die Ausgabe der ersten LSTM-Schicht und die Ausgabe der zweiten LSTM-Schicht mit einigen Gewichten zu kombinieren, die für jede Aufgabe ausgewählt werden.  Dies ist erforderlich, um sowohl Funktionen auf niedriger Ebene als auch Funktionen auf höherer Ebene zu berücksichtigen, die die erste und zweite Schicht von LSTM ergeben. </li></ul><br>  In unserem Problem haben wir eine einfache Mittelung dieser drei Einbettungen verwendet und so für jedes Wort eine kontextbezogene Einbettung erhalten. <br><br><img src="https://habrastorage.org/webt/8y/tl/pa/8ytlpa3lia0oxj461muadegcdyq.png"><br><br>  Das Sprachmodell bietet folgende Vorteile: <br><br><ul><li>  Der Vektor eines Wortes hängt vom Kontext ab, in dem das Wort verwendet wird.  Das heißt, für das Wort "Sprache" im Sinne des Körperteils und des sprachlichen Begriffs erhalten wir unterschiedliche Vektoren. </li><li>  Wie bei word2vec und fasttext gibt es viele trainierte Modelle, zum Beispiel aus dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DeepPavlov-</a> Projekt.  Sie können das fertige Modell nehmen und versuchen, es in Ihrer Aufgabe anzuwenden. </li><li>  Sie müssen nicht mehr darüber nachdenken, wie die Wortvektoren gemittelt werden.  Das ELMo-Modell erzeugt sofort einen Vektor des gesamten Textes. </li><li>  Sie können das Sprachmodell für Ihre Aufgabe neu trainieren. Hierfür gibt es verschiedene Möglichkeiten, z. B. ULMFiT. </li></ul><br>  Das einzige Minus bleibt - das <strong>Sprachmodell garantiert nicht,</strong> dass Texte, die zur selben Klasse gehören, dh zu einer Absicht, im Vektorraum nahe beieinander liegen. <br><br><img src="https://habrastorage.org/webt/hv/09/qf/hv09qfkirx8mbcfl8rvbsd11aiu.png"><br><br>  In unserem Restaurantbeispiel sind die Kosinuswerte nach dem ELMo-Modell wirklich höher geworden. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/m8/d0/ax/m8d0axjr1fysz33kaoi7ydg4kga.png" width="400"></div><br>  <em>t-SNE auf ELMo (Top 10 Absichten), F1-Punktzahl 0,93 (0,92 von tf-idf)</em> <br><br>  Cluster mit Top-10-Absichten sind ebenfalls ausgeprägter.  In der obigen Abbildung sind alle 10 Cluster deutlich sichtbar, während die Genauigkeit leicht zugenommen hat. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ux/fx/ws/uxfxws0f9y1bf-zdpjynmle3xqk.png" width="400"></div><br>  <em>t-SNE auf ELMo (Top 30 Absichten) F1-Punktzahl 0,86 (0,85 von tf-idf)</em> <br><br>  Bei den Top-30-Absichten bleibt die Clusterstruktur erhalten, und die Qualität steigt um einen Punkt. <br><br>  Aber in einem solchen Modell gibt es keine Garantie dafür, dass die Vorschläge "Und wenn Sie eine Einzahlung eröffnen, was sind die Zinsen auf sie?"  und "Und ich möchte einen Beitrag bei 7 Prozent eröffnen" wird weit voneinander entfernt sein, obwohl sie in verschiedenen Klassen liegen.  Mit ELMo lernen wir einfach das Sprachmodell, und wenn die semantisch ähnlichen Texte, dann sind sie nah.  <strong>ELMo weiß nichts über unsere Klassen</strong> , aber Sie können <strong>Textvektoren mit</strong> derselben Absicht im Raum mithilfe von Klassenbeschriftungen zusammenführen. <br><br><h3>  Siamesisches Netzwerk </h3><br>  Nehmen Sie Ihre bevorzugte neuronale Netzwerkarchitektur für die Textvektorisierung und zwei Beispiele für Absichten.  Für jedes der Beispiele erhalten wir Einbettungen und berechnen dann den Kosinusabstand zwischen ihnen. <br><br><img src="https://habrastorage.org/webt/ah/sw/ha/ahswhaivfdofbehikcsi7qvhntk.jpeg"><br><br>  Der Kosinusabstand ist gleich eins minus der Kosinusnähe, die wir zuvor getroffen haben. <br><br>  Dieser Ansatz wird als <strong>siamesisches Netzwerk bezeichnet</strong> . <br><br>  Wir möchten, dass Texte aus derselben Klasse, zum Beispiel „Überweisung machen“ und „Geld werfen“, dicht im Raum liegen.  Das heißt, der Kosinusabstand zwischen ihren Vektoren sollte so klein wie möglich sein, idealerweise Null.  Und Texte, die sich auf unterschiedliche Absichten beziehen, sollten so weit wie möglich voneinander entfernt sein. <br><br>  In der Praxis funktioniert diese Trainingsmethode jedoch nicht so gut, da Objekte verschiedener Klassen nicht ausreichend voneinander entfernt sind.  Die Verlustfunktion <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">"Triplettverlust"</a> funktioniert viel besser.  Es werden Tripel von Objekten verwendet, die als Drillinge bezeichnet werden. <br><br>  Die Abbildung zeigt ein Triplett: ein Ankerobjekt in einem blauen Kreis, ein positives Objekt in Grün und ein negatives Objekt in einem roten Kreis.  Das negative Objekt und der Anker befinden sich in verschiedenen Klassen, und das positive und der Anker befinden sich in einer. <br><br><img src="https://habrastorage.org/webt/qw/38/lz/qw38lz9wpgcphm8w2ic55aqbhea.png"><br><br>  Wir wollen sicherstellen, dass das positive Objekt nach dem Training näher am Anker liegt als das negative.  Dazu betrachten wir den Kosinusabstand zwischen den Objektpaaren und geben den Hyperparameter - "Rand" - den Abstand ein, den wir zwischen den positiven und negativen Objekten erwarten. <br><br><img src="https://habrastorage.org/webt/lv/ib/b8/lvibb8qcivpp0evrqxgnyezlo0a.png"><br><br>  Die Verlustfunktion sieht folgendermaßen aus: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>t</mi><mi>r</mi><mi>i</mi><mi>p</mi><mi>l</mi><mi>e</mi><mi>t</mi><msub><mtext>&amp;#xA0;</mtext><mi>l</mi></msub><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy=&quot;false&quot;>[</mo><mn>0</mn><mo>,</mo><mi>R</mi><mi>a</mi><mi>n</mi><mi>d</mi><mo>+</mo><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy=&quot;false&quot;>(</mo><mi>A</mi><mo>,</mo><mi>P</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy=&quot;false&quot;>(</mo><mi>A</mi><mo>,</mo><mi>N</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>]</mo><mo>.</mo></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="55.668ex" height="2.66ex" viewBox="0 -832 23968 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-74" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-72" x="361" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-69" x="813" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-70" x="1158" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-6C" x="1662" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-65" x="1960" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-74" x="2427" y="0"></use><g transform="translate(2788,0)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-6C" x="353" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-6F" x="3349" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-73" x="3835" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-73" x="4304" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-3D" x="5051" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-6D" x="6358" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-61" x="7236" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-78" x="7766" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-5B" x="8338" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-30" x="8617" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-2C" x="9117" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-52" x="9562" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-61" x="10322" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-6E" x="10851" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-64" x="11452" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-2B" x="12198" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-64" x="13198" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-69" x="13722" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-73" x="14067" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-74" x="14537" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-28" x="14898" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-41" x="15288" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-2C" x="16038" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-50" x="16483" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-29" x="17235" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-2212" x="17847" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-64" x="18847" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-69" x="19371" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-73" x="19716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-74" x="20186" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-28" x="20547" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-41" x="20937" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-2C" x="21687" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMATHI-4E" x="22133" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-29" x="23021" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-5D" x="23411" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhhhkkYHEy1G-b2iSjoJeds-RboOEw#MJMAIN-2E" x="23689" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>t</mi><mi>r</mi><mi>i</mi><mi>p</mi><mi>l</mi><mi>e</mi><mi>t</mi><msub><mtext>&nbsp;</mtext><mi>l</mi></msub><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mtext>&nbsp;</mtext><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi>R</mi><mi>a</mi><mi>n</mi><mi>d</mi><mo>+</mo><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false">(</mo><mi>A</mi><mo>,</mo><mi>P</mi><mo stretchy="false">)</mo><mo>−</mo><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false">(</mo><mi>A</mi><mo>,</mo><mi>N</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>.</mo></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> triplet \ _loss = \ max [0, Rand + dist (A, P) - dist (A, N)]. </script></p><br>  Mit anderen Worten, während des Trainings erreichen wir, dass das positive Objekt näher am Anker liegt als das negative, zumindest am Rand.  Wenn die Verlustfunktion Null ist, funktioniert sie und wir beenden das Training, andernfalls minimieren wir weiterhin die Zielfunktion. <br><br>  Nachdem wir das Modell trainiert haben, erhalten wir immer noch keinen Klassifikator. Es ist nur eine Methode, um solche Einbettungen zu erhalten, dass Objekte, die in einer Absicht liegen, höchstwahrscheinlich enge Vektoren haben. <br><br>  Wenn wir das Modell erhalten haben, können wir zusätzlich zu den Einbettungen eine andere Klassifizierungsmethode verwenden.  <strong>KNN</strong> passt gut, da wir bereits festgestellt haben, dass Einbettungen eine unterschiedliche Clusterstruktur aufweisen. <br><br>  Erinnern Sie sich daran, wie kNN für Texte funktioniert: Nehmen Sie ein Element des Textes, binden Sie es ein, übersetzen Sie es in den Vektorraum und sehen Sie dann, wer sein Nachbar ist.  Unter den Nachbarn betrachten wir die häufigste Klasse und schließen daraus, dass das neue Objekt zu dieser Klasse gehört. <br><br>  Die Dimension der von uns verwendeten Einbettungen beträgt 300, und im Trainingsbeispiel befinden sich etwa 500.000 Objekte.  Standardmethoden zur Suche nach den nächsten Nachbarn passen in Bezug auf die Leistung nicht zu uns.  Wir haben die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">HNSW-</a> Methode verwendet - <strong>Hierarchical Navigable Small World</strong> . <br><br>  Navigable Small World ist ein zusammenhängendes Diagramm, in dem nur wenige Kanten zwischen Scheitelpunkten in großer Entfernung und viele Kanten zwischen Scheitelpunkten in der Nähe vorhanden sind.  In unserem Fall wird die Kantenlänge durch den Kosinusabstand bestimmt, d.h.  Für ein Trainingsbeispiel berechnen wir den Abstand zwischen allen Beispielen von Absichten und werfen dann zufällig sehr große Entfernungen aus, damit der Graph weiterhin verbunden bleibt. <br><br>      ,    Hierarchical.        ,  ,       ,    .            . <br><br>       , ,         ,     ,      . <br><br>     ,     ,       , ,  ,       .   ,    ,         ,     <strong>  —  0,95-0,99</strong> ,    . <br><br>  ,       ,     ,          , <strong>   </strong> .              . <br><br>  ,      .    ,      .          . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/db/2v/yx/db2vyx8eoubfaytjge3imauvgra.png" width="400"></div><br> <em>t-SNE  siamese (-10 ), F1 score 0,95 (0,93  ELMo)</em> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/rz/ib/op/rzibopijmd5mqginmihafxfiivy.png" width="400"></div><br> <em>t-SNE  siamese (-30 ), F1 score 0,87 (0,86  ELMo)</em> <br><br>  10             ELMo,  30 —  ,       . <br><br><h2>  </h2><br> <b>      ,     </b> , , 2-5,          .    ,     ,        ,     20-30  .      ,   . <br><br> <b>  ,      ,     ,       tf-idf</b> .    ,      ,    ,       . <br><br> <b>   ,    word2vec  fasttext.</b>   ,   ,         .        ,      ,       ,     . <br><br>   ,  ,   ELMo.       , , ,      ,       ,      . <b>   ELMo,        </b> ,           . <br><br>             ,    -  .       .      ,              .  ,   ,           .  ,     ,     ..    ,      . <br><div class="scrollable-table"><table><tbody><tr><td> F1-score </td><td> ~2-5  <br>   </td><td> ~10  <br>   </td><td> ~30  <br>   </td></tr><tr><td> ,  </td><td>  MVP </td><td>    </td><td>    </td></tr><tr><td> ML + tf-idf </td><td>  </td><td> 0,92 </td><td> 0,85 </td></tr><tr><td> ML + fasttext </td><td>  warum? </td><td>  0,86 </td><td>  0,82 </td></tr><tr><td> ELMo </td><td> ?? </td><td> 0,93 </td><td>  0,86 </td></tr><tr><td> siamese </td><td> ??? </td><td> 0,95 </td><td> 0,87 </td></tr></tbody></table></div>  <b>Nützliche Links:</b> <br><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">rusvectores.org/ru/models</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">docs.deeppavlov.ai/en/master/intro/pretrained_vectors.html</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">www.mihaileric.com/posts/deep-contextualized-word-representations-elmo</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">omoindrot.github.io/triplet-loss</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">towardsdatascience.com/review-highway-networks-gating-function-to-highway-image-classification-5a33833797b5</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">habr.com/ru/company/mailru/blog/338360</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">http://jalammar.github.io/illustrated-bert</a> </li></ul><br><blockquote>    — «Deep Learning vs common sense» —       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">UseData Conf</a> .  ,    -   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a>  18        ,      ,          . <br><br>        ,        ,    ,         ,   16   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">UseData Conf</a> . </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de455652/">https://habr.com/ru/post/de455652/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de455642/index.html">Die Architektur des verteilten Nachrichtenwarteschlangendienstes in Yandex.Cloud</a></li>
<li><a href="../de455644/index.html">Wir verwenden Daten in der Praxis</a></li>
<li><a href="../de455646/index.html">Sicherheitswoche 24: Werks-Backdoors auf Android-Smartphones</a></li>
<li><a href="../de455648/index.html">Lebenszyklus ML</a></li>
<li><a href="../de455650/index.html">Wie wir ein neuronales Netzwerk trainiert haben, um Schrauben zu klassifizieren</a></li>
<li><a href="../de455658/index.html">Legendärer Intel Core i7-2600K: Testen von Sandy Bridge im Jahr 2019 (Teil 3)</a></li>
<li><a href="../de455662/index.html">Großes mechanisches Display mit Nockenmechanismus als Decoder</a></li>
<li><a href="../de455666/index.html">Aufbau von Outbound Sales in einem IT-Service-Unternehmen</a></li>
<li><a href="../de455668/index.html">Wir schreiben unter FPGA ohne HDL. Vergleich von Entwicklungswerkzeugen auf hoher Ebene</a></li>
<li><a href="../de455670/index.html">Wie 3D-Drucker Knochen, Blutgefäße und Organe drucken</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>