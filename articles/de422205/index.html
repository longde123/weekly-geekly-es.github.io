<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®‚Äçüë¶‚Äçüë¶ ü•ã üåä Richard Hamming: Kapitel 13. Informationstheorie üë®üèæ‚Äçüíª üéÖüèª üòÇ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wir haben es geschafft! 

 "Ziel dieses Kurses ist es, Sie auf Ihre technische Zukunft vorzubereiten." 
 Hallo Habr. Erinnern Sie sich an den gro√üarti...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Richard Hamming: Kapitel 13. Informationstheorie</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/422205/">  Wir haben es geschafft! <br><br><blockquote>  "Ziel dieses Kurses ist es, Sie auf Ihre technische Zukunft vorzubereiten." </blockquote><br><img src="https://habrastorage.org/getpro/habr/post_images/d67/6ff/9ea/d676ff9eadd2a38b0948de76bbf27fd4.jpg" alt="Bild" align="right">  Hallo Habr.  Erinnern Sie sich an den gro√üartigen Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚ÄûSie und Ihre Arbeit‚Äú</a> (+219, 2588 Lesezeichen, 429.000 Lesungen)? <br><br>  Hamming (ja, ja, selbst√ºberpr√ºfende und selbstkorrigierende <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hamming-Codes</a> ) hat ein ganzes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Buch</a> geschrieben, das auf seinen Vorlesungen basiert.  Wir √ºbersetzen es, weil der Mann gesch√§ftlich spricht. <br><br>  In diesem Buch geht es nicht nur um IT, sondern auch um den Denkstil unglaublich cooler Leute.  <i>‚ÄûDies ist nicht nur eine Anklage f√ºr positives Denken.</i>  <i>Es beschreibt Bedingungen, die die Chancen erh√∂hen, gute Arbeit zu leisten. ‚Äú</i> <br><br>  <i>Vielen Dank f√ºr die √úbersetzung an Andrei Pakhomov.</i> <br><br>  Die Informationstheorie wurde Ende der 1940er Jahre von C. E. Shannon entwickelt.  Das Management von Bell Labs bestand darauf, dass er es "Kommunikationstheorie" nannte, weil  Dies ist ein viel genauerer Name.  Aus offensichtlichen Gr√ºnden hat der Name "Informationstheorie" einen wesentlich gr√∂√üeren Einfluss auf die √ñffentlichkeit, daher hat Shannon ihn gew√§hlt, und das wissen wir bis heute.  Der Name selbst legt nahe, dass sich die Theorie mit Informationen befasst, was sie wichtig macht, da wir tiefer in das Informationszeitalter eindringen.  In diesem Kapitel werde ich einige grundlegende Schlussfolgerungen aus dieser Theorie ansprechen. Ich werde einige der einzelnen Bestimmungen dieser Theorie nicht streng, sondern intuitiv belegen, damit Sie verstehen, was die "Informationstheorie" tats√§chlich ist, wo Sie sie anwenden k√∂nnen und wo nicht . <br><a name="habracut"></a><br>  Was ist ‚ÄûInformation‚Äú?  Shannon identifiziert Informationen mit Unsicherheit.  Er w√§hlte den negativen Logarithmus der Wahrscheinlichkeit eines Ereignisses als quantitatives Ma√ü f√ºr die Informationen, die Sie erhalten, wenn ein Ereignis mit der Wahrscheinlichkeit p eintritt.  Wenn ich Ihnen zum Beispiel sage, dass das Wetter in Los Angeles neblig ist, liegt p nahe bei 1, was uns im Gro√üen und Ganzen nicht viele Informationen gibt.  Aber wenn ich sage, dass es im Juni in Monterey regnet, dann wird diese Nachricht unsicher sein und mehr Informationen enthalten.  Ein zuverl√§ssiges Ereignis enth√§lt keine Informationen, da log 1 = 0 ist. <br><br>  Lassen Sie uns n√§her darauf eingehen.  Shannon war der Ansicht, dass ein quantitatives Informationsma√ü eine kontinuierliche Funktion der Wahrscheinlichkeit eines Ereignisses p sein sollte und f√ºr unabh√§ngige Ereignisse additiv sein sollte - die Informationsmenge, die als Ergebnis zweier unabh√§ngiger Ereignisse erhalten wird, sollte gleich der Informationsmenge sein, die als Ergebnis eines gemeinsamen Ereignisses erhalten wird.  Beispielsweise wird das Ergebnis eines W√ºrfel- und M√ºnzwurfs normalerweise als eigenst√§ndiges Ereignis angesehen.  Lassen Sie uns das Obige in die Sprache der Mathematik √ºbersetzen.  Wenn I (p) die Informationsmenge ist, die in dem Ereignis mit der Wahrscheinlichkeit p enthalten ist, dann erhalten wir f√ºr ein gemeinsames Ereignis, das aus zwei unabh√§ngigen Ereignissen x mit der Wahrscheinlichkeit p <sub>1</sub> und y mit der Wahrscheinlichkeit p <sub>2 besteht</sub> <br><br><img src="https://habrastorage.org/webt/yi/hc/2d/yihc2dbw1d5_rlv2rptbyqs1mww.jpeg" alt="Bild"><br>  <i>(x und y unabh√§ngige Ereignisse)</i> <br><br>  Dies ist die funktionale Cauchy-Gleichung, die f√ºr alle p <sub>1</sub> und p <sub>2 gilt</sub> .  Nehmen wir an, um diese Funktionsgleichung zu l√∂sen <br><br>  p <sub>1</sub> = p <sub>2</sub> = p, <br><br>  es gibt <br><br><img src="https://habrastorage.org/webt/dk/rj/gf/dkrjgfn-xucnz-twcyhq3l6xefw.jpeg" alt="Bild"><br><br>  Wenn p <sub>1</sub> = p <sup>2</sup> und p <sub>2</sub> = p, dann <br><br><img src="https://habrastorage.org/webt/ux/-s/cj/ux-scjh2wymlpjj-hpnsnzrgsly.jpeg" alt="Bild"><br><br>  usw.  Wenn Sie diesen Prozess mit der Standardmethode f√ºr Exponentiale f√ºr alle rationalen Zahlen m / n erweitern, gilt Folgendes <br><br><img src="https://habrastorage.org/webt/gt/_t/ro/gt_trop25xscgj5m3c1-k0nxiwe.jpeg" alt="Bild"><br><br>  Aus der angenommenen Kontinuit√§t des Informationsma√ües folgt, dass die logarithmische Funktion die einzige kontinuierliche L√∂sung f√ºr die funktionale Cauchy-Gleichung ist. <br><br>  In der Informationstheorie ist es √ºblich, die Basis des Logarithmus von 2 zu verwenden, sodass die bin√§re Auswahl genau 1 Bit Information enth√§lt.  Daher werden Informationen durch die Formel gemessen <br><br><img src="https://habrastorage.org/webt/wx/ix/ow/wxixowgi6tookkpcslvwgmu6bpg.jpeg" alt="Bild"><br><br>  Lassen Sie uns innehalten und sehen, was oben passiert ist.  Zun√§chst haben wir das Konzept der ‚ÄûInformation‚Äú nicht definiert, sondern lediglich eine Formel f√ºr das quantitative Ma√ü definiert. <br><br>  Zweitens h√§ngt diese Ma√ünahme von der Unsicherheit ab, und obwohl sie f√ºr Maschinen - beispielsweise Telefonsysteme, Radio, Fernsehen, Computer usw. - ausreichend geeignet ist, spiegelt sie keine normale menschliche Einstellung zu Informationen wider. <br><br>  Drittens ist dies ein relatives Ma√ü, das vom aktuellen Wissensstand abh√§ngt.  Wenn Sie sich den Strom von ‚ÄûZufallszahlen‚Äú aus dem Zufallszahlengenerator ansehen, nehmen Sie an, dass jede n√§chste Zahl unbestimmt ist. Wenn Sie jedoch die Formel zur Berechnung von ‚ÄûZufallszahlen‚Äú kennen, ist die n√§chste Zahl bekannt und enth√§lt dementsprechend keine Informationen. <br><br>  Daher ist die von Shannon f√ºr Informationen gegebene Definition in vielen F√§llen f√ºr Maschinen geeignet, scheint jedoch nicht dem menschlichen Verst√§ndnis des Wortes zu entsprechen.  Aus diesem Grund sollte die ‚ÄûInformationstheorie‚Äú als ‚ÄûKommunikationstheorie‚Äú bezeichnet werden.  Es ist jedoch zu sp√§t, um die Definitionen zu √§ndern (dank derer die Theorie ihre anf√§ngliche Popularit√§t erlangt hat und die Leute immer noch denken lassen, dass diese Theorie sich mit ‚ÄûInformationen‚Äú befasst), also m√ºssen wir uns mit ihnen abfinden, aber Sie m√ºssen klar verstehen, inwieweit die von Shannon gegebene Definition von Informationen weit vom gesunden Menschenverstand entfernt ist.  In Shannons Informationen geht es um etwas v√∂llig anderes, n√§mlich um Unsicherheit. <br><br>  Dies ist, woran Sie denken m√ºssen, wenn Sie eine Terminologie anbieten.  Wie konsistent ist die vorgeschlagene Definition, zum Beispiel die von Shannon gegebene Definition, mit Ihrer urspr√ºnglichen Idee und wie unterschiedlich ist sie?  Es gibt fast keinen Begriff, der Ihre fr√ºhere Vision des Konzepts genau widerspiegeln w√ºrde, aber letztendlich ist es die verwendete Terminologie, die die Bedeutung des Konzepts widerspiegelt, sodass die Formalisierung von etwas durch klare Definitionen immer etwas L√§rm macht. <br><br>  Stellen Sie sich ein System vor, dessen Alphabet aus Symbolen q mit den Wahrscheinlichkeiten pi besteht.  In diesem Fall <i>betr√§gt</i> die <i>durchschnittliche Informationsmenge</i> im System (der erwartete Wert): <br><br><img src="https://habrastorage.org/webt/wj/ss/83/wjss83z5fnynnyurbcftgokak6e.jpeg" alt="Bild"><br><br>  Dies nennt man die Entropie des Wahrscheinlichkeitsverteilungssystems {pi}.  Wir verwenden den Begriff ‚ÄûEntropie‚Äú, weil in der Thermodynamik und der statistischen Mechanik dieselbe mathematische Form auftritt.  Deshalb schafft der Begriff "Entropie" um sich herum eine Aura von Bedeutung, die letztendlich nicht gerechtfertigt ist.  Dieselbe mathematische Form der Notation impliziert nicht dieselbe Interpretation von Zeichen! <br><br>  Die Entropie der Wahrscheinlichkeitsverteilung spielt eine wichtige Rolle in der Codierungstheorie.  Die Gibbs-Ungleichung f√ºr zwei verschiedene Wahrscheinlichkeitsverteilungen pi und qi ist eine der wichtigen Konsequenzen dieser Theorie.  Das m√ºssen wir also beweisen <br><br><img src="https://habrastorage.org/webt/jx/1v/c-/jx1vc-ac5t4kzyxhpplqfb20bmu.jpeg" alt="Bild"><br><br>  Der Beweis basiert auf einer offensichtlichen Grafik, Abb.  13.I, was das zeigt <br><br><img src="https://habrastorage.org/webt/gn/_o/i1/gn_oi1vqrmafw6k5v0g043jpz30.jpeg" alt="Bild"><br><br>  und Gleichheit wird nur f√ºr x = 1 erreicht. Wir wenden die Ungleichung auf jeden Summanden der Summe von der linken Seite an: <br><br><img src="https://habrastorage.org/webt/rg/wl/5o/rgwl5owcdte29pj1ycvixfgdogo.jpeg" alt="Bild"><br><br>  Wenn das Alphabet des Kommunikationssystems aus q Zeichen besteht, dann erhalten wir aus der Gibbs-Ungleichung die Wahrscheinlichkeit der √úbertragung jedes Zeichens qi = 1 / q und ersetzen q <br><br><img src="https://habrastorage.org/webt/qo/dz/gv/qodzgviyhmwnovomxokegxao54e.jpeg" alt="Bild"><br><br><img src="https://habrastorage.org/webt/8n/2m/y9/8n2my9t_5vd7vktnvdezxi0p1q4.jpeg" alt="Bild"><br><br>  <i>Abbildung 13.I.</i> <br><br>  Dies legt nahe, dass, wenn die Wahrscheinlichkeit der √úbertragung aller q Zeichen gleich und gleich - 1 / q ist, die maximale Entropie ln q ist, andernfalls gilt die Ungleichung. <br><br>  Im Fall eines eindeutig decodierten Codes haben wir die Kraft-Ungleichung <br><br><img src="https://habrastorage.org/webt/0-/8k/5l/0-8k5lsd16wlmzgwmmzvxgvlbr4.jpeg" alt="Bild"><br><br>  Nun definieren wir Pseudowahrscheinlichkeiten <br><br><img src="https://habrastorage.org/webt/s9/qc/fv/s9qcfv1ywfj7_zkfvwm4jna8cqw.jpeg" alt="Bild"><br><br>  wo nat√ºrlich <img src="https://habrastorage.org/webt/is/_u/vw/is_uvwjifihdmybm68w_ayyfk-q.jpeg" alt="Bild">  = 1, was sich aus der Gibbs-Ungleichung ergibt, <br><br><img src="https://habrastorage.org/webt/my/hy/s0/myhys0cvxd6kgzu7f8r10vtnlym.jpeg" alt="Bild"><br><br>  und wenden Sie etwas Algebra an (denken Sie daran, dass K ‚â§ 1 ist, damit wir den logarithmischen Term weglassen und m√∂glicherweise die Ungleichung sp√§ter verst√§rken k√∂nnen) <br><br><img src="https://habrastorage.org/webt/v0/i4/yo/v0i4yoxhwj8grndqupxwbo4zppw.jpeg" alt="Bild"><br><br>  Dabei ist L die durchschnittliche L√§nge des Codes. <br><br>  Somit ist Entropie die minimale Grenze f√ºr jeden Zeichencode mit einem durchschnittlichen Codewort L. Dies ist Shannons Theorem f√ºr einen Kanal ohne Interferenz. <br><br>  Wir betrachten nun den Hauptsatz zu den Einschr√§nkungen von Kommunikationssystemen, in denen Informationen in Form eines Stroms unabh√§ngiger Bits √ºbertragen werden und Rauschen vorhanden ist.  Es wird impliziert, dass die Wahrscheinlichkeit der korrekten √úbertragung eines Bits P&gt; 1/2 ist und die Wahrscheinlichkeit, dass der Bitwert w√§hrend der √úbertragung invertiert wird (ein Fehler tritt auf), Q = 1 - P ist. Der Einfachheit halber nehmen wir an, dass die Fehler unabh√§ngig sind und die Fehlerwahrscheinlichkeit f√ºr jedes gesendete gleich ist Bits - das hei√üt, es gibt "wei√ües Rauschen" im Kommunikationskanal. <br><br>  Die Art und Weise, wie ein langer Strom von n Bits in einer einzelnen Nachricht codiert ist, ist eine n-dimensionale Erweiterung eines Ein-Bit-Codes.  Wir werden den Wert von n sp√§ter bestimmen.  Betrachten Sie eine Nachricht, die aus n-Bits besteht, als einen Punkt im n-dimensionalen Raum.  Da wir einen n-dimensionalen Raum haben - und der Einfachheit halber nehmen wir an, dass jede Nachricht die gleiche Wahrscheinlichkeit des Auftretens hat - gibt es M m√∂gliche Nachrichten (M wird auch sp√§ter bestimmt), daher ist die Wahrscheinlichkeit einer gesendeten Nachricht gleich <br><br><img src="https://habrastorage.org/webt/vs/gj/e5/vsgje5adnr3222jgs5qqlii1hga.jpeg" alt="Bild"><br><br><img src="https://habrastorage.org/webt/s8/mu/br/s8mubr4blaju8evwtbsgqxjvqye.jpeg" alt="Bild"><br>  (Absender) <br>  <i>Grafik 13.II.</i> <br><br>  Betrachten Sie als n√§chstes die Idee der Kanalbandbreite.  Ohne auf Details einzugehen, wird die Kanalkapazit√§t als die maximale Informationsmenge definiert, die unter Ber√ºcksichtigung der Verwendung der effizientesten Codierung zuverl√§ssig √ºber den Kommunikationskanal √ºbertragen werden kann.  Es gibt kein Argument daf√ºr, dass mehr Informationen √ºber den Kommunikationskanal √ºbertragen werden k√∂nnen als seine Kapazit√§t.  Dies kann f√ºr einen bin√§ren symmetrischen Kanal (den wir in unserem Fall verwenden) bewiesen werden.  Die Kanalkapazit√§t f√ºr das bitweise Senden ist auf eingestellt <br><br><img src="https://habrastorage.org/webt/pi/ek/76/piek76b-zyqfhs7k2qxzhleky9y.jpeg" alt="Bild"><br><br>  Dabei ist P wie zuvor die Wahrscheinlichkeit, dass in keinem gesendeten Bit ein Fehler auftritt.  Beim Senden von n unabh√§ngigen Bits wird die Kanalkapazit√§t als bestimmt <br><br><img src="https://habrastorage.org/webt/nv/7x/67/nv7x67ybwwcisk8pxntmligj3ky.jpeg" alt="Bild"><br><br>  Wenn wir nahe an der Bandbreite des Kanals sind, sollten wir f√ºr jedes der Zeichen ai, i = 1, ..., M fast ein solches Informationsvolumen senden. Wenn die Wahrscheinlichkeit des Auftretens jedes Zeichens ai 1 / M betr√§gt, erhalten wir <br><br><img src="https://habrastorage.org/webt/ne/mf/om/nemfom_mjugxea6infrm7asm4la.jpeg" alt="Bild"><br><br>  Wenn wir eine von M gleichwahrscheinlichen Nachrichten ai senden, haben wir <br><br><img src="https://habrastorage.org/webt/mt/c-/si/mtc-siba4teyqt0flsyzqs3-eja.jpeg" alt="Bild"><br><br>  Beim Senden von n Bits erwarten wir, dass nQ-Fehler auftreten.  In der Praxis haben wir f√ºr eine Nachricht, die aus n Bits besteht, ungef√§hr nQ Fehler in der empfangenen Nachricht.  F√ºr gro√ües n relative Variation (Variation = Verteilungsbreite,) <br>  Die Verteilung der Anzahl der Fehler wird mit zunehmendem n enger. <br><br>  Von der Seite des Senders nehme ich die ai-Nachricht, um eine Kugel mit einem Radius um ihn herum zu senden und zu zeichnen <br><br><img src="https://habrastorage.org/webt/de/j8/h9/dej8h9wxzm0ktckfoksyksxdcm0.jpeg" alt="Bild"><br><br>  Dies ist um einen Betrag gleich e2 etwas gr√∂√üer als die erwartete Anzahl von Fehlern Q (Abbildung 13.II).  Wenn n gro√ü genug ist, besteht eine beliebig kleine Wahrscheinlichkeit f√ºr das Auftreten des Nachrichtenpunkts bj auf der Empf√§ngerseite, die √ºber diese Sph√§re hinausgeht.  Wir werden die Situation, wie ich sie aus Sicht des Senders sehe, zeichnen: Wir haben beliebige Radien von der gesendeten Nachricht ai zur empfangenen Nachricht bj mit einer Fehlerwahrscheinlichkeit, die gleich (oder fast gleich) der Normalverteilung ist und ein Maximum in nQ erreicht.  F√ºr jedes gegebene e2 ist n so gro√ü, dass die Wahrscheinlichkeit, dass der resultierende Punkt bj, der √ºber meine Sph√§re hinausgeht, so klein ist, wie Sie m√∂chten. <br><br>  Betrachten Sie nun die gleiche Situation von Ihrer Seite (Abb. 13.III).  Auf der Empf√§ngerseite befindet sich eine Kugel S (r) mit dem gleichen Radius r um den empfangenen Punkt bj im n-dimensionalen Raum. Wenn sich die empfangene Nachricht bj in meiner Kugel befindet, befindet sich die von mir gesendete Nachricht ai in Ihrer Kugel. <br><br>  Wie kann ein Fehler auftreten?  In den in der folgenden Tabelle beschriebenen F√§llen kann ein Fehler auftreten: <br><br><img src="https://habrastorage.org/webt/ap/jj/ce/apjjcepa80evnhjxgc_swmf62z4.jpeg" alt="Bild"><br><br>  <i>Abbildung 13.III</i> <br><br><img src="https://habrastorage.org/webt/qt/m3/v8/qtm3v8viaw9cw291jxn11t9ya0i.jpeg" alt="Bild"><br><br>  Hier sehen wir, dass, wenn in der Kugel, die um den empfangenen Punkt herum aufgebaut ist, mindestens ein weiterer Punkt vorhanden ist, der einer m√∂glicherweise gesendeten nicht codierten Nachricht entspricht, w√§hrend der √úbertragung ein Fehler aufgetreten ist, da Sie nicht bestimmen k√∂nnen, welche dieser Nachrichten gesendet wurde.  Die gesendete Nachricht enth√§lt nur dann keine Fehler, wenn sich der entsprechende Punkt in der Kugel befindet und in diesem Code keine anderen Punkte m√∂glich sind, die sich in derselben Kugel befinden. <br><br>  Wir haben eine mathematische Gleichung f√ºr die Wahrscheinlichkeit eines Re-Fehlers, wenn ai gesendet wurde <br><br><img src="https://habrastorage.org/webt/uu/fr/fo/uufrfozpmxs0asmf4sjlinmk9rk.jpeg" alt="Bild"><br><br>  Wir k√∂nnen den ersten Faktor im zweiten Term wegwerfen und ihn als 1 annehmen. So erhalten wir die Ungleichung <br><br><img src="https://habrastorage.org/webt/oh/qt/_9/ohqt_97ig6afppntwqmz7ifd-fs.jpeg" alt="Bild"><br><br>  Offensichtlich <br><br><img src="https://habrastorage.org/webt/ac/hx/ty/achxtyvmg-aq2yirdswhnsm4olk.jpeg" alt="Bild"><br><br>  deshalb <br><br><img src="https://habrastorage.org/webt/ym/27/5p/ym275poqvhr2e8jyuajtui9a4ky.jpeg" alt="Bild"><br><br>  Wenden Sie sich erneut an das letzte Mitglied auf der rechten Seite <br><br><img src="https://habrastorage.org/webt/hd/4v/9g/hd4v9gnlnbiyjcjv-grtzvsslsg.jpeg" alt="Bild"><br><br>  Wenn n gro√ü genug genommen wird, kann der erste Term beliebig klein genommen werden, beispielsweise kleiner als eine bestimmte Zahl d.  Deshalb haben wir <br><br><img src="https://habrastorage.org/webt/lt/ey/hf/lteyhffjmrrzy8pjblxrsigpkzu.jpeg" alt="Bild"><br><br>  Nun wollen wir sehen, wie Sie einen einfachen Ersatzcode f√ºr die Codierung von M Nachrichten erstellen k√∂nnen, die aus n Bits bestehen.  Da Shannon keine Ahnung hatte, wie der Code erstellt werden soll (Fehlerkorrekturcodes wurden noch nicht erfunden), entschied er sich f√ºr eine zuf√§llige Codierung.  Wirf eine M√ºnze f√ºr jedes der n Bits in der Nachricht und wiederhole den Vorgang f√ºr M Nachrichten.  Alles, was Sie tun m√ºssen, ist daher nM M√ºnzwurf <br><br><img src="https://habrastorage.org/webt/ii/ke/ss/iikessacb5q-xgdhr5btks6otss.jpeg" alt="Bild"><br><br>  Code-W√∂rterb√ºcher mit der gleichen Wahrscheinlichkeit von ¬ΩnM.  Nat√ºrlich bedeutet der zuf√§llige Prozess des Erstellens eines Codebuchs, dass die Wahrscheinlichkeit von Duplikaten sowie von Codepunkten besteht, die nahe beieinander liegen und daher eine Quelle f√ºr wahrscheinliche Fehler darstellen.  Es muss bewiesen werden, dass das gegebene n gro√ü genug ist, wenn dies nicht mit einer Wahrscheinlichkeit geschieht, die h√∂her als eine kleine ausgew√§hlte Fehlerstufe ist. <br>  Der entscheidende Punkt ist, dass Shannon alle m√∂glichen Codeb√ºcher gemittelt hat, um den durchschnittlichen Fehler zu finden!  Wir werden das Av [.] Symbol verwenden, um den Durchschnitt √ºber die Menge aller m√∂glichen Zufallscode-W√∂rterb√ºcher zu bezeichnen.  Die Mittelung √ºber die Konstante d ergibt nat√ºrlich eine Konstante, da zur Mittelung jeder Term mit jedem anderen Term in der Summe zusammenf√§llt <br><br><img src="https://habrastorage.org/webt/-j/gh/_z/-jgh_zkjbnuzjdj39bz03euss_g.jpeg" alt="Bild"><br><br>  was erh√∂ht werden kann (M - 1 geht an M) <br><br><img src="https://habrastorage.org/webt/ma/3u/wz/ma3uwzaee8iaohanrz5qsgvspnq.jpeg" alt="Bild"><br><br>  Bei einer bestimmten Nachricht durchl√§uft die Codierung bei der Mittelung aller Codeb√ºcher alle m√∂glichen Werte. Die durchschnittliche Wahrscheinlichkeit, dass sich ein Punkt in einer Kugel befindet, ist also das Verh√§ltnis des Volumens der Kugel zum Gesamtvolumen des Raums.  Der Umfang der Kugel <br><br><img src="https://habrastorage.org/webt/ry/pu/qb/rypuqbtnwvzsd4ei3_-6sxsot3c.jpeg" alt="Bild"><br><br>  Dabei ist s = Q + e2 &lt;1/2 und ns muss eine ganze Zahl sein. <br><br>  Der letzte Term rechts ist der gr√∂√üte in dieser Summe.  Zun√§chst sch√§tzen wir seinen Wert nach der Stirling-Formel f√ºr Fakult√§ten.  Dann betrachten wir den Reduktionskoeffizienten des Termes davor, stellen fest, dass dieser Koeffizient zunimmt, wenn wir uns nach links bewegen, und daher k√∂nnen wir: (1) den Wert der Summe auf die Summe der geometrischen Progression mit diesem Anfangskoeffizienten begrenzen, (2) die geometrische Progression von ns Mitgliedern auf erweitern unendlich viele Terme, (3) berechne die Summe der unendlichen geometrischen Progression (Standardalgebra, nichts signifikantes) und erhalte schlie√ülich den Grenzwert (f√ºr ein ausreichend gro√ües n): <br><br><img src="https://habrastorage.org/webt/6f/jo/lr/6fjolr9tmeljs-eglmaxup0yrli.jpeg" alt="Bild"><br><br>  Beachten Sie, wie die Entropie H (s) in der Binomialidentit√§t erschien.  Es ist zu beachten, dass die Erweiterung in der Taylor-Reihe H (s) = H (Q + e2) eine Sch√§tzung ergibt, die erhalten wird, indem nur die erste Ableitung ber√ºcksichtigt wird und alle anderen ignoriert werden.  Sammeln wir nun den letzten Ausdruck: <br><br><img src="https://habrastorage.org/webt/9h/-u/wd/9h-uwd1ukrxttavrdy91ifezgpw.jpeg" alt="Bild"><br><br>  wo <br><br><img src="https://habrastorage.org/webt/bm/h4/bb/bmh4bbgfnl2h9g0-omxft9b8lbe.jpeg" alt="Bild"><br><br>  Alles was wir tun m√ºssen, ist e2 so zu w√§hlen, dass e3 &lt;e1 ist, und dann wird der letzte Term beliebig klein sein, f√ºr n ausreichend gro√ü.  Daher kann der durchschnittliche PE-Fehler beliebig klein mit einer Kanalkapazit√§t erhalten werden, die beliebig nahe bei C liegt. <br>  Wenn der Durchschnitt aller Codes einen ausreichend kleinen Fehler aufweist, muss mindestens ein Code geeignet sein, daher existiert mindestens ein geeignetes Codierungssystem.  Dies ist ein wichtiges Ergebnis von Shannon - ‚ÄûShannons Theorem f√ºr einen Kanal mit Interferenz‚Äú, obwohl anzumerken ist, dass er es f√ºr einen viel allgemeineren Fall bewiesen hat als f√ºr einen einfachen bin√§ren symmetrischen Kanal, den ich verwendet habe.  F√ºr den allgemeinen Fall sind mathematische Berechnungen viel komplizierter, aber die Ideen sind nicht so unterschiedlich, so dass man sehr oft am Beispiel eines Sonderfalls die wahre Bedeutung des Satzes offenbaren kann. <br><br>  Lassen Sie uns das Ergebnis kritisieren.  Wir wiederholten wiederholt: "F√ºr ausreichend gro√üe n."  Aber wie gro√ü ist n?  Sehr, sehr gro√ü, wenn Sie wirklich gleichzeitig nahe an der Bandbreite des Kanals sein und auf die richtige Daten√ºbertragung achten m√∂chten!  So gro√ü, dass Sie tats√§chlich sehr lange warten m√ºssen, um eine Nachricht aus so vielen Bits zu akkumulieren, um sie sp√§ter zu codieren.  In diesem Fall ist die Gr√∂√üe des Zufallscode-W√∂rterbuchs sehr gro√ü (schlie√ülich kann ein solches W√∂rterbuch nicht in einer k√ºrzeren Form als die vollst√§ndige Liste aller Mn-Bits dargestellt werden, w√§hrend n und M sehr gro√ü sind)! <br><br>  Fehlerkorrekturcodes vermeiden das Warten auf eine sehr lange Nachricht mit anschlie√üender Codierung und Decodierung durch sehr gro√üe Codeb√ºcher, da sie Codeb√ºcher an sich vermeiden und stattdessen herk√∂mmliche Berechnungen verwenden.  In einer einfachen Theorie verlieren solche Codes in der Regel ihre F√§higkeit, sich der Kanalkapazit√§t anzun√§hern und gleichzeitig eine relativ niedrige Fehlerrate beizubehalten. Wenn der Code jedoch eine gro√üe Anzahl von Fehlern korrigiert, zeigen sie gute Ergebnisse.  Mit anderen Worten, wenn Sie eine Art Kanalkapazit√§t f√ºr die Fehlerkorrektur festlegen, sollten Sie die Fehlerkorrekturoption die meiste Zeit verwenden, dh in jeder gesendeten Nachricht muss eine gro√üe Anzahl von Fehlern behoben werden, andernfalls verlieren Sie diese Kapazit√§t vergeblich. <br><br>  Dar√ºber hinaus ist der oben bewiesene Satz immer noch nicht bedeutungslos!  Es zeigt, dass effiziente √úbertragungssysteme ausgefeilte Codierungsschemata f√ºr sehr lange Bitfolgen verwenden sollten.  Ein Beispiel sind Satelliten, die au√üerhalb des √§u√üeren Planeten geflogen sind.  Wenn sie sich von der Erde und der Sonne entfernen, m√ºssen sie immer mehr Fehler im Datenblock korrigieren: Einige Satelliten verwenden Solarbatterien, die ungef√§hr 5 Watt liefern, andere verwenden Atomstromquellen, die ungef√§hr die gleiche Leistung liefern.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die schwache Leistung der Stromquelle, die geringe Gr√∂√üe der Senderplatten und die begrenzte Gr√∂√üe der Empf√§ngerplatten auf der Erde, die gro√üe Entfernung, die das Signal zur√ºcklegen muss - all dies erfordert die Verwendung von Codes mit einem hohen Ma√ü an Fehlerkorrektur, um ein effektives Kommunikationssystem aufzubauen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir kehren zu dem n-dimensionalen Raum zur√ºck, den wir im obigen Beweis verwendet haben. Als wir dar√ºber diskutierten, zeigten wir, dass fast das gesamte Volumen der Kugel in der N√§he der Au√üenfl√§che konzentriert ist - daher befindet sich das gesendete Signal mit ziemlicher Sicherheit an der Oberfl√§che der Kugel, die um das empfangene Signal herum aufgebaut ist, selbst bei einem relativ kleinen Radius einer solchen Kugel. Daher ist es nicht √ºberraschend, dass sich herausstellt, dass das empfangene Signal nach Korrektur einer beliebig gro√üen Anzahl von Fehlern nQ dem Signal ohne Fehler beliebig nahe kommt. Die Kapazit√§t des Kommunikationskanals, die wir zuvor untersucht haben, ist der Schl√ºssel zum Verst√§ndnis dieses Ph√§nomens. Bitte beachten Sie, dass sich solche Kugeln, die f√ºr Hamming-Fehlerkorrekturcodes konstruiert wurden, nicht √ºberlappen. Eine gro√üe Anzahl von praktisch orthogonalen Messungen im n-dimensionalen Raum zeigenWarum k√∂nnen wir M Kugeln in einen Raum mit einer leichten √úberlappung einpassen? Wenn Sie eine kleine, willk√ºrlich kleine √úberlappung zulassen, die beim Decodieren nur zu einer geringen Anzahl von Fehlern f√ºhren kann, k√∂nnen Sie eine dichte Anordnung von Kugeln im Raum erhalten. Hamming garantierte ein gewisses Ma√ü an Fehlerkorrektur, Shannon - eine geringe Fehlerwahrscheinlichkeit, hielt aber gleichzeitig die tats√§chliche Bandbreite willk√ºrlich nahe an der Kapazit√§t des Kommunikationskanals, was Hamming-Codes nicht k√∂nnen.Gleichzeitig liegt die Aufrechterhaltung des tats√§chlichen Durchsatzes willk√ºrlich nahe an der Kapazit√§t des Kommunikationskanals, was Hamming-Codes nicht k√∂nnen.Gleichzeitig liegt die Aufrechterhaltung des tats√§chlichen Durchsatzes willk√ºrlich nahe an der Kapazit√§t des Kommunikationskanals, was Hamming-Codes nicht k√∂nnen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Informationstheorie spricht nicht dar√ºber, wie ein effektives System entworfen werden kann, sondern gibt die Bewegungsrichtung zu effektiven Kommunikationssystemen an. Dies ist ein wertvolles Werkzeug zum Aufbau von Kommunikationssystemen zwischen Maschinen, hat jedoch, wie bereits erw√§hnt, nicht viel damit zu tun, wie Menschen Informationen miteinander austauschen. Inwieweit die biologische Vererbung technischen Kommunikationssystemen √§hnelt, ist einfach unbekannt. Daher ist derzeit nicht klar, wie die Informationstheorie auf Gene angewendet wird. Wir haben keine andere Wahl, als es zu versuchen, und wenn der Erfolg uns die maschinen√§hnliche Natur dieses Ph√§nomens zeigt, wird das Scheitern auf andere wichtige Aspekte der Natur von Informationen hinweisen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lassen wir uns nicht viel ablenken. Wir haben gesehen, dass alle anf√§nglichen Definitionen mehr oder weniger das Wesen unserer anf√§nglichen √úberzeugungen ausdr√ºcken sollten, aber sie sind durch ein gewisses Ma√ü an Verzerrung gekennzeichnet und daher nicht anwendbar. Es wird traditionell akzeptiert, dass letztendlich die Definition, die wir verwenden, tats√§chlich das Wesen definiert; Aber es sagt uns nur, wie wir Dinge verarbeiten sollen und macht in keiner Weise Sinn. Der in mathematischen Kreisen so hoch gelobte postulative Ansatz l√§sst in der Praxis zu w√ºnschen √ºbrig.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jetzt sehen wir uns ein Beispiel f√ºr IQ-Tests an, bei denen die Definition so zyklisch ist, wie Sie m√∂chten, und Sie dadurch in die Irre f√ºhrt. Es wird ein Test erstellt, der die Intelligenz messen soll. Danach wird √ºberpr√ºft, ob es so konsistent wie m√∂glich ist, und dann wird es auf einfache Weise ver√∂ffentlicht und kalibriert, so dass die gemessene "Intelligenz" normal verteilt ist (nat√ºrlich entlang der Kalibrierungskurve). Alle Definitionen sollten √ºberpr√ºft werden, nicht nur wenn sie zum ersten Mal vorgeschlagen werden, sondern viel sp√§ter, wenn sie in den Schlussfolgerungen verwendet werden. Inwieweit sind die Definitionsgrenzen f√ºr die jeweilige Aufgabe geeignet? Wie oft werden die unter denselben Bedingungen angegebenen Definitionen unter ganz unterschiedlichen Bedingungen angewendet? Das ist h√§ufig genug!In den Geisteswissenschaften, denen Sie in Ihrem Leben unweigerlich begegnen werden, geschieht dies h√§ufiger.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Daher bestand eines der Ziele dieser Pr√§sentation der Informationstheorie neben dem Nachweis ihrer N√ºtzlichkeit darin, Sie vor dieser Gefahr zu warnen oder zu demonstrieren, wie Sie sie verwenden k√∂nnen, um das gew√ºnschte Ergebnis zu erzielen. Es ist seit langem bekannt, dass die anf√§nglichen Definitionen in viel gr√∂√üerem Ma√üe bestimmen, was Sie am Ende finden, als es scheint. Erste Definitionen erfordern, dass Sie nicht nur in jeder neuen Situation, sondern auch in Bereichen, mit denen Sie lange gearbeitet haben, gro√üe Aufmerksamkeit schenken. Auf diese Weise k√∂nnen Sie nachvollziehen, inwieweit es sich bei den erzielten Ergebnissen um eine Tautologie handelt und nicht um etwas N√ºtzliches.</font></font><br><br>      ,       .   ,   ,     ,    !      ,   . <br><br> <i> ...</i> <br><br> <i>    ,     ‚Äî       magisterludi2016@yandex.ru</i> <br><br> ,         ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">¬´The Dream Machine:   ¬ª</a> ) <br><br> <b> </b> ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> ,     </a> . ( <i>  10 ,  20  </i> ) <br><br><div class="spoiler"> <b class="spoiler_title">    </b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vorwort</a> <br><ol><li>  Einf√ºhrung in die Kunst, Wissenschaft und Technik zu betreiben: Lernen lernen (28. M√§rz 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">√úbersetzung: Kapitel 1</a> </li><li>  "Grundlagen der digitalen (diskreten) Revolution" (30. M√§rz 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 2. Grundlagen der digitalen (diskreten) Revolution</a> </li><li>  "Geschichte der Computer - Hardware" (31. M√§rz 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 3.</a> Computergeschichte - Hardware </li><li>  "Geschichte der Computer - Software" (4. April 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 4. Geschichte der Computer - Software</a> </li><li>  Geschichte der Computer - Anwendungen (6. April 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 5. Computergeschichte - Praktische Anwendung</a> </li><li>  "K√ºnstliche Intelligenz - Teil I" (7. April 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 6. K√ºnstliche Intelligenz - 1</a> </li><li>  "K√ºnstliche Intelligenz - Teil II" (11. April 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 7. K√ºnstliche Intelligenz - II</a> </li><li>  "K√ºnstliche Intelligenz III" (13. April 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 8. K√ºnstliche Intelligenz-III</a> </li><li>  "N-dimensionaler Raum" (14. April 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 9. N-dimensionaler Raum</a> </li><li>  "Codierungstheorie - Die Darstellung von Informationen, Teil I" (18. April 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 10. Codierungstheorie - I.</a> </li><li>  "Codierungstheorie - Die Darstellung von Informationen, Teil II" (20. April 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 11. Codierungstheorie - II</a> </li><li>  "Fehlerkorrekturcodes" (21. April 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 12. Fehlerkorrekturcodes</a> </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Informationstheorie (25. April 1995) </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kapitel 13. Informationstheorie</font></font></a> </li><li>  Digitale Filter, Teil I (27. April 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 14. Digitale Filter - 1</a> </li><li>  Digitale Filter, Teil II (28. April 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 15. Digitale Filter - 2</a> </li><li>  Digitale Filter, Teil III (2. Mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 16. Digitale Filter - 3</a> </li><li>  Digitale Filter, Teil IV (4. Mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 17. Digitale Filter - IV</a> </li><li>  "Simulation, Teil I" (5. Mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 18. Modellierung - I.</a> </li><li>  "Simulation, Teil II" (9. Mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 19. Modellierung - II</a> </li><li>  "Simulation, Teil III" (11. Mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 20. Modellierung - III</a> </li><li>  Fiber Optics (12. Mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 21. Fiber Optics</a> </li><li>  Computer Aided Instruction (16. Mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 22.</a> Computer Aided <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Learning (CAI)</a> </li><li>  Mathematik (18. Mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 23. Mathematik</a> </li><li>  Quantenmechanik (19. Mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 24. Quantenmechanik</a> </li><li>  Kreativit√§t (23. Mai 1995).  √úbersetzung: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 25. Kreativit√§t</a> </li><li>  "Experten" (25. Mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 26. Experten</a> </li><li>  "Unzuverl√§ssige Daten" (26. Mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 27. Ung√ºltige Daten</a> </li><li>  Systems Engineering (30. Mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 28. Systems Engineering</a> </li><li>  "Sie bekommen, was Sie messen" (1. Juni 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 29.</a> Sie bekommen, was Sie messen </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">"Woher wissen wir, was wir wissen?"</a> (2. Juni 1995) wird <i>in 10-Minuten-Scheiben √ºbersetzt</i> </li><li>  Hamming, "Sie und Ihre Forschung" (6. Juni 1995).  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">√úbersetzung: Sie und Ihre Arbeit</a> </li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wer bei der √úbersetzung, dem Layout und der Ver√∂ffentlichung des Buches helfen m√∂chte, schreibt eine pers√∂nliche E-Mail oder eine E-Mail an magisterludi2016@yandex.ru </font></font><br><br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de422205/">https://habr.com/ru/post/de422205/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de422195/index.html">Die Verwendung von ACS im Bergbau</a></li>
<li><a href="../de422197/index.html">Wir sagen ein Wort √ºber die Staffel</a></li>
<li><a href="../de422199/index.html">Sicherheitswoche 33: Von wem schwingt der Monitor?</a></li>
<li><a href="../de422201/index.html">China, lass mich abschreiben?</a></li>
<li><a href="../de422203/index.html">DIY Clicker</a></li>
<li><a href="../de422207/index.html">Monster nach den Ferien: AMD Threadripper 2990WX 32-Core und 2950X 16-Core (Teil 4)</a></li>
<li><a href="../de422209/index.html">Monster nach den Ferien: AMD Threadripper 2990WX 32-Core und 2950X 16-Core (Teil 5)</a></li>
<li><a href="../de422211/index.html">Sch√∂ne Komponentenstruktur in der Microsoft Azure Cloud</a></li>
<li><a href="../de422213/index.html">Eines Tages ohne JavaScript: Was k√∂nnte schief gehen?</a></li>
<li><a href="../de422217/index.html">Nicht wirklich ernst mit Hosting oder wie man die Angemessenheit des Hosts √ºberpr√ºft</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>