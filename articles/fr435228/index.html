<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ùï üë©üèª‚Äçüåæ üë©üèΩ‚Äçüî¨ Cluster Kubernetes pour 20 $ par mois üìá üéÄ üëØ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="TL DR 


 Nous √©levons le cluster pour servir les applications Web sans √©tat avec entr√©e , permet de chiffrer , sans utiliser d'outils d'automatisatio...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cluster Kubernetes pour 20 $ par mois</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/iponweb/blog/435228/"><h1 id="tl-dr">  TL  DR </h1><br><p>  Nous √©levons le cluster pour servir les applications Web <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sans √©tat</a> avec <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">entr√©e</a> , permet de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">chiffrer</a> , sans utiliser d'outils d'automatisation comme kubespray, kubeadm et tout autre. <br>  Temps de lecture: ~ 45-60 minutes, temps de lecture: √† partir de 3 heures. </p><br><h1 id="preambula">  Pr√©ambule </h1><br><p>  J'ai √©t√© invit√© √† √©crire un article par la n√©cessit√© de mon propre cluster de kubernetes pour l'exp√©rimentation.  Les solutions d'installation et de configuration automatis√©es open source n'ont pas fonctionn√© dans mon cas, car j'ai utilis√© des distributions Linux non traditionnelles.  Un travail intensif avec kubernetes dans IPONWEB vous encourage √† avoir une telle plate-forme, r√©solvant vos t√¢ches de mani√®re confortable, y compris pour des projets √† domicile. </p><br><h1 id="komponenty">  Composants </h1><br><p>  Les composants suivants appara√Ætront dans l'article: </p><br><p>  - <em>Votre</em> Linux <em>pr√©f√©r√©</em> - J'ai utilis√© Gentoo (node-1: systemd / node-2: openrc), Ubuntu 18.04.1. <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Serveur Kubernetes</a> - kube-apiserver, kube-controller-manager, kube-scheduler, kubelet, kube-proxy. <br>  - <a href="">Plugins</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Containerd</a> + <a href="">CNI (0.7.4)</a> - pour la conteneurisation, nous prendrons containerd + CNI au lieu de docker (bien qu'au d√©part, la configuration enti√®re ait √©t√© t√©l√©charg√©e sur docker, donc rien ne l'emp√™chera d'√™tre utilis√©e si n√©cessaire). <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">CoreDNS</a> - pour organiser la d√©couverte de services des composants travaillant √† l'int√©rieur du cluster kubernetes.  Une version non inf√©rieure √† 1.2.5 est recommand√©e, car avec cette version, il existe une prise en charge saine des coredns pour fonctionner comme un processus ex√©cut√© en dehors du cluster. <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Flanelle</a> - pour organiser une pile de r√©seau, communiquer des foyers et des conteneurs entre eux. <br>  - <em>Votre</em> db <em>pr√©f√©r√©</em> . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/423/a71/5fc/423a715fc68fed1101c86d3335b0a8a8.jpg" alt="Pour tous"></p><a name="habracut"></a><br><h1 id="ogranicheniya-i-dopuscheniya">  Limitations et hypoth√®ses </h1><br><ul><li>  L'article ne prend pas en compte le co√ªt des solutions vps / vds sur le march√©, ainsi que la possibilit√© de d√©ployer des machines sur ces services.  On suppose que vous avez d√©j√† d√©velopp√© quelque chose ou que vous pouvez le faire vous-m√™me.  De plus, l'installation / configuration de votre base de donn√©es pr√©f√©r√©e et de votre d√©p√¥t Docker priv√©, si vous en avez besoin, ne sont pas couverts. </li><li>  Nous pouvons utiliser les plugins containerd + cni et docker.  Cet article ne consid√®re pas l'utilisation de Docker comme outil de conteneurisation.  Si vous souhaitez utiliser docker, vous pourrez vous-m√™me configurer la <a href="">flanelle en cons√©quence</a> , en plus vous devrez configurer kubelet, √† savoir supprimer toutes les options li√©es √† containerd.  Comme mes exp√©riences l'ont montr√©, docker et containerd sur diff√©rents n≈ìuds en tant que conteneurs fonctionneront correctement. </li><li> Nous ne pouvons pas utiliser le backend <code>host-gw</code> pour la flanelle, lisez la section <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Configuration de</a> la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">flanelle</a> pour plus de d√©tails </li><li>  Nous n'utiliserons rien pour la surveillance, les sauvegardes, la sauvegarde des fichiers utilisateur (statut), le stockage des fichiers de configuration et du code d'application (git / hg / svn / etc) </li></ul><br><h1 id="vvedenie">  Pr√©sentation </h1><br><p>  Au cours du travail, j'ai utilis√© un grand nombre de sources, mais je veux mentionner s√©par√©ment un guide <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubernetes le plus difficile</a> , qui couvre environ 90% de la configuration de base de son propre cluster.  Si vous avez d√©j√† lu ce manuel, vous pouvez passer en toute s√©curit√© directement √† la section <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Configuration</a> de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">flanelle</a> . </p><br><div class="spoiler">  <b class="spoiler_title">D√©signations</b> <div class="spoiler_text"><h2 id="spisok-terminov--glossariy">  Liste des termes / glossaire </h2><br><ul><li>  api-server - une machine physique ou virtuelle sur laquelle se trouve un ensemble d'applications pour l'ex√©cution et le bon fonctionnement de kubernetes kube-apiserver.  Pour les besoins de cet article, il s'agit de etcd, kube-apiserver, kube-controller-manager, kube-scheduler. </li><li>  master - une station de travail d√©di√©e ou une installation VPS, synonyme d'api-server. </li><li>  node-X - une station de travail d√©di√©e ou une installation VPS, <code>X</code> indique le num√©ro de s√©rie de la station.  Dans cet article, tous les chiffres sont uniques et sont essentiels √† la compr√©hension: <br><ul><li>  node-1 - machine num√©ro 1 </li><li>  node-2 - machine num√©ro 2 </li></ul></li><li>  vCPU - CPU virtuel, c≈ìur de processeur.  Le nombre correspond au nombre de c≈ìurs: 1vCPU - un c≈ìur, 2vCPU - deux, etc. </li><li>  utilisateur - espace utilisateur ou utilisateur.  Lorsque vous utilisez l' <code>user$</code> dans les instructions de ligne de commande, le terme fait r√©f√©rence √† n'importe quel ordinateur client. </li><li>  travailleur - le n≈ìud de travail sur lequel les calculs directs seront effectu√©s, synonyme de <code>node-X</code> </li><li>  ressource est l'entit√© sur laquelle le cluster Kubernetes fonctionne.  Les ressources Kubernetes comprennent un grand nombre d' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">entit√©s li√©es</a> . </li></ul></div></div><br><h1 id="setevaya-arhitektura-resheniya">  Solutions d'architecture r√©seau </h1><br><p>  Dans le processus de rel√®vement de la grappe, je n'ai pas fix√© la t√¢che d'optimiser les ressources en fer de mani√®re √† s'ins√©rer dans le budget de 20 $ par mois.  Il √©tait juste n√©cessaire d'assembler un cluster de travail avec au moins deux n≈ìuds de travail (n≈ìuds).  Par cons√©quent, au d√©part, le cluster ressemblait √† ceci: </p><br><ul><li>  machine avec 2 vCPU / 4G RAM: api-server + node-1 [20 $] </li><li>  machine avec 2 vCPU / 4G RAM: node-2 [20 $] </li></ul><br><p>  Apr√®s que la premi√®re version du cluster ait fonctionn√©, j'ai d√©cid√© de le reconstruire afin de faire la distinction entre les n≈ìuds responsables de l'ex√©cution des applications au sein du cluster (les n≈ìuds de travail, ils sont √©galement des travailleurs) et l'API du serveur ma√Ætre. </p><br><p>  En cons√©quence, j'ai obtenu la r√©ponse √† la question: "Comment obtenir un cluster plus ou moins bon march√©, mais fonctionnel, si je ne souhaite pas y placer les applications les plus √©paisses." </p><br><div class="spoiler">  <b class="spoiler_title">D√©cision de 20 $</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/getpro/habr/post_images/216/89c/a88/21689ca889156d11108e5f5327c606cc.png" alt="La conception"><br>  (Pr√©vu pour √™tre comme √ßa) </p></div></div><br><div class="spoiler">  <b class="spoiler_title">Informations g√©n√©rales sur Kubernetes Architecture</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/getpro/habr/post_images/820/462/882/820462882e7cc92479190c067ac4a4f8.jpg" alt="La conception"><br>  (Vol√© sur Internet si quelqu'un soudain ne sait toujours pas ou n'a pas vu) </p></div></div><br><h2 id="komponenty-i-ih-proizvoditelnost">  Composants et leurs performances </h2><br><p>  La premi√®re √©tape consistait √† comprendre le nombre de ressources dont j'avais besoin pour ex√©cuter des packages logiciels directement li√©s au cluster.  La recherche de ¬´configuration mat√©rielle requise¬ª n'a pas donn√© de r√©sultats sp√©cifiques, j'ai donc d√ª aborder la t√¢che d'un point de vue pratique.  En tant que mesure de MEM et CPU, j'ai pris des statistiques de systemd - nous pouvons supposer que les mesures ont √©t√© effectu√©es de mani√®re tr√®s amateur, mais il n'y avait pas de t√¢che d'obtenir des valeurs pr√©cises, car je ne pouvais toujours pas trouver d'options moins ch√®res que 5 $ par instance. </p><br><div class="spoiler">  <b class="spoiler_title">Pourquoi exactement 5 $?</b> <div class="spoiler_text"><p>  Il √©tait possible de trouver des VPS / VDS moins chers lors de l'h√©bergement de serveurs en Russie ou dans la CEI, mais les tristes histoires associ√©es √† ILV et √† ses actions cr√©ent certains risques et suscitent un d√©sir naturel de les √©viter. </p></div></div><br><p>  Donc: </p><br><ul><li>  Serveur ma√Ætre / configuration du serveur (n≈ìuds ma√Ætres): <br><ul><li>  etcd (3.2.17): 80 - 100M, les mesures ont √©t√© prises √† un moment choisi au hasard.  La consommation moyenne de m√©moire Etcd n'a pas d√©pass√© 300M; </li><li>  kube-apiserver (1.12.x - 1.13.0): 237,6 M ~ 300 M; </li><li>  kube-controller-manager (1.12.x - 1.13.0): environ 90M, ne d√©passe pas 100M; </li><li>  kube-scheduler (1.12.x - 1.13.0): environ 20M, la consommation au-dessus de 30-50M n'est pas fixe. </li></ul></li><li>  Configuration du serveur de travail (n≈ìuds de travail): <br><ul><li>  kubelet (1.12.3 - 1.13.1): environ 35 Mb, la consommation au-dessus de 50M n'est pas fixe; </li><li>  kube-proxy (1.12.3 - 1.13.1): environ 7,5 - 10M; </li><li>  flanelle (0,10,0): environ 15-20 M; </li><li>  coredns (1.3.0): environ 25M; </li><li>  containerd (1.2.1): la consommation de containerd est faible, mais les statistiques montrent √©galement les processus de conteneur lanc√©s par le d√©mon. </li></ul></li></ul><br><div class="spoiler">  <b class="spoiler_title">Le conteneur / docker est-il n√©cessaire sur les n≈ìuds ma√Ætres?</b> <div class="spoiler_text"><p>  <strong>Non, pas n√©cessaire</strong> .  Le n≈ìud ma√Ætre ne n√©cessite pas de docker ou de containerd en soi, bien qu'il existe un grand nombre de manuels sur Internet qui, dans un but ou un autre, incluent l'utilisation de l'environnement pour la conteneurisation.  Dans la configuration en question, containerd a √©t√© d√©sactiv√© intentionnellement de la liste des d√©pendances, cependant, je ne souligne aucun avantage √©vident de cette approche. </p><br><p>  La configuration fournie ci-dessus est minimale et suffisante pour d√©marrer le cluster.  Aucune action / composante suppl√©mentaire n'est requise, sauf si vous souhaitez ajouter quelque chose comme vous le souhaitez. </p></div></div><br><p>  Pour cr√©er un cluster de test ou un cluster pour des projets domestiques, 1vCPU / 1G RAM sera suffisant pour que le n≈ìud ma√Ætre fonctionne.  Bien s√ªr, la charge sur le n≈ìud ma√Ætre variera en fonction du nombre de travailleurs impliqu√©s, ainsi que de la disponibilit√© et du volume des demandes tierces au serveur api. </p><br><p>  J'ai explos√© les configurations ma√Ætre et travailleur comme suit: </p><br><ul><li>  1x ma√Ætre avec les composants install√©s: etcd, kube-apiserver, kube-controller-manager, kube-scheduler </li><li>  2x Travailleurs avec des composants install√©s: containerd, coredns, flannel, kubelet, kube-proxy </li></ul><br><h1 id="konfiguraciya">  La configuration </h1><br><p>  Pour configurer l'assistant, les composants suivants sont requis: </p><br><ul><li><p>  etcd - pour stocker des donn√©es pour api-server, ainsi que pour la flanelle; </p><br></li><li><p>  kube-apiserver - en fait, api-server; </p><br></li><li><p>  kube-controller-manager - pour g√©n√©rer et traiter des √©v√©nements; </p><br></li><li><p>  kube-scheduler - pour la distribution des ressources enregistr√©es via api-server - par exemple, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">foyer</a> . <br>  Pour la configuration des chevaux de bataille, les composants suivants sont requis: </p><br></li><li><p>  kubelet - pour faire fonctionner les foyers, pour configurer les param√®tres r√©seau; </p><br></li><li><p>  kube-proxy - pour organiser le routage / √©quilibrage des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">services</a> kubernetes; </p><br></li><li><p>  coredns - pour la d√©couverte de services √† l'int√©rieur de conteneurs en cours d'ex√©cution; </p><br></li><li><p>  flanelle - pour organiser l'acc√®s au r√©seau des conteneurs op√©rant sur diff√©rents n≈ìuds, ainsi que pour la distribution dynamique des r√©seaux entre les n≈ìuds de cluster (n≈ìud kubernetes). </p><br></li></ul><br><div class="spoiler">  <b class="spoiler_title">Coredns</b> <div class="spoiler_text"><p>  Une petite digression doit √™tre faite ici: les coredns peuvent √©galement √™tre lanc√©s sur le serveur ma√Ætre.  Aucune restriction n'obligerait les coredns √† s'ex√©cuter sur les n≈ìuds de travail, √† l'exception de la nuance de configuration coredns.service, qui ne d√©marre tout simplement pas sur un serveur Ubuntu standard / non modifi√© en raison d'un conflit avec le service r√©solu par systemd.  Je n'ai pas essay√© de r√©soudre ce probl√®me, car les serveurs 2 ns situ√©s sur les n≈ìuds de travail √©taient assez satisfaits de moi. </p></div></div><br><p>  Afin de ne pas perdre de temps √† vous familiariser avec tous les d√©tails du processus de configuration des composants, je vous sugg√®re de vous familiariser avec eux dans le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">guide Kubernetes √† la dure</a> .  Je vais me concentrer sur les caract√©ristiques distinctives de mon option de configuration. </p><br><h2 id="fayly">  Fichiers </h2><br><p>  Tous les fichiers pour le fonctionnement des composants du cluster pour l'assistant et les n≈ìuds de travail sont plac√©s dans <strong>/ var / lib / kubernetes /</strong> pour plus de commodit√©.  Si n√©cessaire, vous pouvez les placer d'une autre mani√®re. </p><br><h2 id="sertifikaty">  Certifications </h2><br><p>  La base pour la g√©n√©ration de certificats est toujours la m√™me <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubernetes √† la dure</a> , il n'y a pratiquement aucune diff√©rence significative.  Pour r√©g√©n√©rer les certificats subordonn√©s, de simples scripts bash ont √©t√© √©crits autour des applications <a href="">cfssl</a> - ce qui √©tait tr√®s utile dans le processus de d√©bogage. </p><br><p>  Vous pouvez g√©n√©rer des certificats pour vos besoins √† l'aide des scripts ci-dessous, des recettes de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubernetes √† la dure</a> ou d'autres outils appropri√©s. </p><br><div class="spoiler">  <b class="spoiler_title">G√©n√©ration de certificats √† l'aide de scripts bash</b> <div class="spoiler_text"><p>  Vous pouvez <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">obtenir des</a> scripts ici: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">kubernetes bootstrap</a> .  Avant de commencer, modifiez le fichier <a href="">certs / env.sh</a> , en sp√©cifiant vos param√®tres.  Un exemple: </p><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> certs <span class="hljs-comment"><span class="hljs-comment">#:   certs$ ./generate-keys.sh # ... certificate generate output #:  kubeconfig     certs$ ./generate-configkube.sh</span></span></code> </pre> <br><p>  Si vous avez utilis√© <code>env.sh</code> et sp√©cifi√© correctement tous les param√®tres, il n'est pas n√©cessaire de toucher les certificats g√©n√©r√©s.  Si vous avez fait une erreur √† un moment donn√©, les certificats peuvent √™tre r√©g√©n√©r√©s par parties.  Les scripts bash ci-dessus sont triviaux, les trier n'est pas difficile. </p><br><p>  Remarque importante - vous ne devez pas souvent recr√©er les <code>ca.pem</code> et <code>ca-key.pem</code> , car ce sont les certificats racine pour tous les certificats ult√©rieurs, en d'autres termes, vous devrez recr√©er tous les certificats d'accompagnement et les livrer √† toutes les machines et √† tous les r√©pertoires n√©cessaires. </p></div></div><br><h3 id="master">  Le ma√Ætre </h3><br><p>  Les certificats n√©cessaires pour d√©marrer les services sur le n≈ìud ma√Ætre doivent √™tre plac√©s dans <code>/var/lib/kubernetes/</code> : </p><br><ul><li>  ca.pem - ce certificat est utilis√© partout, il ne peut √™tre g√©n√©r√© qu'une seule fois puis utilis√© sans modifications, alors soyez prudent.  Lorsque vous le r√©g√©n√©rez, vous devrez le copier sur tous les n≈ìuds, ainsi que mettre √† jour les fichiers kubeconfig l'utilisant (√©galement sur toutes les machines). </li><li>  ca-key.pem √©quivaut √† copier sur des n≈ìuds. </li><li>  kube-controller-manager.pem - n√©cessaire uniquement pour kube-controller-manager. </li><li>  kube-controller-manager-key.pem - n√©cessaire uniquement pour kube-controller-manager. </li><li><p>  kubernetes.pem - requis pour flanelle, coredns lors de la connexion √† etcd, kube-apiserver. </p><br><div class="spoiler">  <b class="spoiler_title">Retraite th√©orique</b> <div class="spoiler_text"><p>  Cette fonctionnalit√© est bas√©e sur la logique de configuration <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubernetes √† la dure</a> . <br>  Sur cette base, ce fichier sera n√©cessaire partout - √† la fois sur l'assistant et sur les n≈ìuds de travail.  Je n'ai pas chang√© l'approche fournie par le manuel d'origine, car avec son aide, il est possible d'organiser le fonctionnement du cluster plus rapidement et plus clairement et de comprendre l'ensemble des d√©pendances. </p><br><p>  Mon opinion personnelle est que pour etcd, vous avez besoin de certificats s√©par√©s qui ne chevauchent pas les certificats utilis√©s pour kubernetes. </p><br></div></div><br></li></ul><br><ul><li>  kubernetes-key.pem - reste sur les serveurs ma√Ætres. </li><li>  service-account.pem - n√©cessaire uniquement pour les d√©mons kube-controller-manager. </li><li>  service-account-key.pem - de m√™me. </li></ul><br><h3 id="rabochie-uzly">  Unit√©s de travail </h3><br><ul><li>  ca.pem - n√©cessaire pour tous les services impliqu√©s sur les n≈ìuds de travail (kubelet, kube-proxy), ainsi que pour la flanelle, les coredns.  Entre autres choses, son contenu est inclus dans les fichiers kubeconfig lorsqu'ils sont g√©n√©r√©s √† l'aide de kubectl. </li><li>  kubernetes-key.pem - n√©cessaire uniquement pour que la flanelle et les c≈ìurs se connectent √† etcd, qui est situ√© sur le n≈ìud ma√Ætre api. </li><li>  kubernetes.pem - similaire √† la pr√©c√©dente, n√©cessaire uniquement pour la flanelle et les cors. </li><li>  kubelet / node-1.pem - cl√© pour l'autorisation node-1. </li><li>  kubelet / node-1-key.pem - cl√© pour l'autorisation node-1. </li></ul><br><p>  <strong>Important!</strong>  Si vous avez plusieurs n≈ìuds, chaque n≈ìud comprendra les fichiers <code>node-X-key.pem</code> , <code>node-X.pem</code> et <code>node-X.kubeconfig</code> √† l'int√©rieur de kubelet. </p><br><div class="spoiler">  <b class="spoiler_title">D√©bogage de certificat</b> <div class="spoiler_text"><h4 id="otladka-sertifikatov">  D√©bogage de certificat </h4><br><p>  Parfois, vous devrez peut-√™tre regarder comment le certificat est configur√© pour savoir quels h√¥tes IP / DNS ont √©t√© utilis√©s pour le g√©n√©rer.  La commande <code>cfssl-certinfo -cert &lt;cert&gt;</code> nous y aidera.  Par exemple, nous apprenons ces informations pour <code>node-1.pem</code> : </p><br><pre> <code class="bash hljs">$ cfssl-certinfo -cert node-1.pem</code> </pre> <br><pre> <code class="json hljs">{ <span class="hljs-attr"><span class="hljs-attr">"subject"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"common_name"</span></span>: <span class="hljs-string"><span class="hljs-string">"system:node:node-1"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"country"</span></span>: <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organization"</span></span>: <span class="hljs-string"><span class="hljs-string">"system:nodes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organizational_unit"</span></span>: <span class="hljs-string"><span class="hljs-string">"Infrastructure Unit"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"locality"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"province"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"names"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"system:nodes"</span></span>, <span class="hljs-string"><span class="hljs-string">"Infrastructure Unit"</span></span>, <span class="hljs-string"><span class="hljs-string">"system:node:node-1"</span></span> ] }, <span class="hljs-attr"><span class="hljs-attr">"issuer"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"common_name"</span></span>: <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"country"</span></span>: <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organization"</span></span>: <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organizational_unit"</span></span>: <span class="hljs-string"><span class="hljs-string">"Infrastructure"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"locality"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"province"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"names"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-string"><span class="hljs-string">"Infrastructure"</span></span>, <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span> ] }, <span class="hljs-attr"><span class="hljs-attr">"serial_number"</span></span>: <span class="hljs-string"><span class="hljs-string">"161113741562559533299282037709313751074033027073"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"sans"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"w40k.net"</span></span>, <span class="hljs-string"><span class="hljs-string">"node-1"</span></span>, <span class="hljs-string"><span class="hljs-string">"178.79.168.130"</span></span>, <span class="hljs-string"><span class="hljs-string">"192.168.164.230"</span></span> ], <span class="hljs-attr"><span class="hljs-attr">"not_before"</span></span>: <span class="hljs-string"><span class="hljs-string">"2019-01-04T14:24:00Z"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"not_after"</span></span>: <span class="hljs-string"><span class="hljs-string">"2029-01-01T14:24:00Z"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"sigalg"</span></span>: <span class="hljs-string"><span class="hljs-string">"SHA256WithRSA"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"authority_key_id"</span></span>: <span class="hljs-string"><span class="hljs-string">"6:C8:94:67:59:55:19:82:AD:ED:6D:50:F1:89:B:8D:46:78:FD:9A"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"subject_key_id"</span></span>: <span class="hljs-string"><span class="hljs-string">"A1:5E:B3:3C:45:14:3D:C6:C:A:97:82:1:D5:2B:75:1A:A6:9D:B0"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"pem"</span></span>: <span class="hljs-string"><span class="hljs-string">"&lt;pem content&gt;"</span></span> }</code> </pre> </div></div><br><p>  Tous les autres certificats pour kubelet et kube-proxy sont int√©gr√©s directement dans le kubeconfig correspondant. </p><br><h2 id="kubeconfig">  kubeconfig </h2><br><p>  Tout le kubeconfig n√©cessaire peut √™tre fait en utilisant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubernetes √† la dure</a> , cependant, ici quelques diff√©rences commencent.  Le manuel utilise des <code>kubedns</code> et de <code>cni bridge</code> , il couvre √©galement les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">coredns</a> et la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">flanelle</a> .  Ces deux services utilisent √† leur tour <code>kubeconfig</code> pour se <code>kubeconfig</code> au cluster. </p><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> certs <span class="hljs-comment"><span class="hljs-comment">#:  kubeconfig     certs$ ./generate-configkube.sh</span></span></code> </pre> <br><h3 id="master-1">  Le ma√Ætre </h3><br><p>  Pour l'assistant, les fichiers kubeconfig suivants sont n√©cessaires (comme mentionn√© ci-dessus, apr√®s la g√©n√©ration, ils peuvent √™tre pris dans <code>certs/kubeconfig</code> ): </p><br><pre> <code class="plaintext hljs">master /var/lib/kubernetes/$ tree -L 2 . +-- kube-controller-manager.kubeconfig L-- kube-scheduler  L-- kube-scheduler.kubeconfig</code> </pre> <br><p>  Ces fichiers seront n√©cessaires pour ex√©cuter chacun des composants de service. </p><br><h3 id="rabochie-uzly-1">  Unit√©s de travail </h3><br><p>  Pour les n≈ìuds de travail, les fichiers kubeconfig suivants sont requis: </p><br><pre> <code class="plaintext hljs">node-1 /var/lib/kubernetes/$ tree -L 2 . +-- coredns ¬¶  L-- coredns.kubeconfig +-- flanneld ¬¶  L-- flanneld.kubeconfig +-- kubelet ¬¶  L-- node-1.kubeconfig L-- kube-proxy  L-- kube-proxy.kubeconfig</code> </pre> <br><h2 id="zapusk-servisov">  Lancement du service </h2><br><div class="spoiler">  <b class="spoiler_title">Les services</b> <div class="spoiler_text"><p>  Malgr√© le fait que mes n≈ìuds de travail utilisent diff√©rents syst√®mes d'initialisation, les exemples et le r√©f√©rentiel donnent des options en utilisant systemd.  Avec leur aide, il est plus facile de comprendre quel processus et avec quels param√®tres vous devez d√©marrer.En outre, ils ne devraient pas causer de gros probl√®mes lors de l'√©tude des services avec des indicateurs de destination. </p></div></div><br><p>  Pour d√©marrer les services, vous devez copier <code>service-name.service</code> dans <code>/lib/systemd/system/</code> ou tout autre r√©pertoire o√π se trouvent les services pour systemd, puis allumer et d√©marrer le service.  Exemple pour kube-apiserver: </p><br><pre> <code class="bash hljs">$ systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> kube-apiserver.service $ systemctl start kube-apiserver.service</code> </pre> <br><p>  Bien s√ªr, tous les services doivent √™tre <em>verts</em> (c'est-√†-dire qu'ils fonctionnent et fonctionnent).  Si vous rencontrez une erreur, les commandes <code>journalct -xe</code> ou <code>journal -f -t kube-apiserver</code> vous aideront √† comprendre ce qui s'est exactement pass√©. </p><br><p>  Ne vous pr√©cipitez pas pour d√©marrer tous les serveurs √† la fois, pour commencer, il suffira d'activer etcd et kube-apiserver.  Si tout s'est bien pass√© et que vous avez imm√©diatement gagn√© les quatre services de l'assistant, le lancement de l'assistant peut √™tre consid√©r√© comme r√©ussi. </p><br><h3 id="master-2">  Le ma√Ætre </h3><br><p>  Vous pouvez utiliser les param√®tres <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">systemd</a> ou g√©n√©rer des scripts init pour la configuration que vous utilisez.  Comme d√©j√† mentionn√©, pour le ma√Ætre, vous avez besoin de: </p><br><p>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">systemd / etcd</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">systemd / kube-apiserver</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">systemd / kube-controller-manager</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ordonnanceur systemd / kube</a> </p><br><h3 id="rabochie-uzly-2">  Unit√©s de travail </h3><br><p>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">systemd / containerd</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">systemd / kubelet</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">systemd / kube-proxy</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">systemd / coredns</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">systemd / flanelle</a> </p><br><h3 id="klient">  Client </h3><br><p>  Pour que le client fonctionne, copiez simplement <code>certs/kubeconfig/admin.kubeconfig</code> (apr√®s l'avoir g√©n√©r√© ou √©crit vous-m√™me) dans <code>${HOME}/.kube/config</code> </p><br><p>  T√©l√©chargez <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">kubectl</a> et v√©rifiez le fonctionnement de kube-apiserver.  Permettez-moi de vous rappeler une fois de plus qu'√† ce stade, pour que kube-apiserver fonctionne, seul etcd devrait fonctionner.  Les composants restants seront n√©cessaires pour le fonctionnement complet du cluster un peu plus tard. </p><br><p>  V√©rifiez que kube-apiserver et kubectl fonctionnent: </p><br><pre> <code class="bash hljs">$ kubectl version Client Version: version.Info{Major:<span class="hljs-string"><span class="hljs-string">"1"</span></span>, Minor:<span class="hljs-string"><span class="hljs-string">"13"</span></span>, GitVersion:<span class="hljs-string"><span class="hljs-string">"v1.13.0"</span></span>, <span class="hljs-string"><span class="hljs-string">"extra info"</span></span>: <span class="hljs-string"><span class="hljs-string">"..."</span></span>} Server Version: version.Info{Major:<span class="hljs-string"><span class="hljs-string">"1"</span></span>, Minor:<span class="hljs-string"><span class="hljs-string">"13"</span></span>, GitVersion:<span class="hljs-string"><span class="hljs-string">"v1.13.0"</span></span>, <span class="hljs-string"><span class="hljs-string">"extra info"</span></span>: <span class="hljs-string"><span class="hljs-string">"..."</span></span>}</code> </pre> <br><h1 id="konfiguraciya-flannel">  Configuration de la flanelle </h1><br><p>  En tant que configuration de flanelle, je me suis install√© sur le backend <code>vxlan</code> .  En savoir plus sur les backends <a href="">ici</a> . </p><br><div class="spoiler">  <b class="spoiler_title">host-gw et pourquoi cela ne fonctionnera pas</b> <div class="spoiler_text"><p>  Je dois dire tout de suite que l'ex√©cution d'un cluster kubernetes sur un VPS est susceptible de vous limiter √† utiliser le backend <code>host-gw</code> .  N'√©tant pas un ing√©nieur r√©seau exp√©riment√©, j'ai pass√© environ deux jours √† d√©boguer pour comprendre quel √©tait le probl√®me avec son utilisation sur les fournisseurs VDS / VPS populaires. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Linode.com</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">digitalocean</a> ont √©t√© test√©s.  L'essence du probl√®me est que les fournisseurs ne fournissent pas de L2 honn√™te pour un r√©seau priv√©.  Cela, √† son tour, rend impossible le d√©placement du trafic r√©seau entre les n≈ìuds dans cette configuration: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/e0c/c7e/add/e0cc7eadd6253cf4921df17ca6fe5d47.png" alt="Trafic"></p><br><p>  Pour que le trafic r√©seau fonctionne entre les n≈ìuds, un routage normal suffit.  N'oubliez pas que net.ipv4.ip_forward doit √™tre d√©fini sur 1 et la cha√Æne FORWARD dans la table de filtrage ne doit pas contenir de r√®gles d'interdiction pour les n≈ìuds. </p><br><pre> <code class="bash hljs">node1$ ip route add 10.200.12.0/24 via 192.168.1.2 node2$ ip route add 10.200.8.0/24 via 192.168.1.1</code> </pre> <br><pre> <code class="plaintext hljs">[10.200.80.23 container-1]-&gt;[192.168.1.1 node-1]-&gt;[192.168.1.2 node-2]-&gt;[10.200.12.5 container-2]</code> </pre> <br><p>  C'est exactement ce qui ne fonctionne pas sur les VPS / VDS indiqu√©s (et, tr√®s probablement, g√©n√©ralement sur tous). </p><br><p>  Par cons√©quent, si la configuration d'une solution avec des performances r√©seau √©lev√©es entre les n≈ìuds <strong>est importante pour</strong> vous, vous devez toujours d√©penser plus de 20 $ pour organiser le cluster. </p></div></div><br><p>  Vous pouvez utiliser <code>set-flannel-config.sh</code> depuis <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">etc /</a> <code>set-flannel-config.sh</code> pour d√©finir la configuration de flanelle souhait√©e.  <strong>Il est important de se rappeler</strong> : si vous d√©cidez de changer le backend, vous devrez supprimer la configuration dans etcd et red√©marrer tous les d√©mons de flanelle sur tous les n≈ìuds, alors choisissez-le judicieusement.  La valeur par d√©faut est vxlan. </p><br><pre> <code class="bash hljs">master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_CA_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/ca.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_CERT_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/kubernetes.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_KEY_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/kubernetes-key.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_ENDPOINTS=<span class="hljs-string"><span class="hljs-string">'https://127.0.0.1:2379'</span></span> master$ etcdctl ls /coreos.com/network/subnets/ /coreos.com/network/subnets/10.200.8.0-24 /coreos.com/network/subnets/10.200.12.0-24 master$ etcdctl get /coreos.com/network/subnets/10.200.8.0-24 {<span class="hljs-string"><span class="hljs-string">"PublicIP"</span></span>:<span class="hljs-string"><span class="hljs-string">"178.79.168.130"</span></span>,<span class="hljs-string"><span class="hljs-string">"BackendType"</span></span>:<span class="hljs-string"><span class="hljs-string">"vxlan"</span></span>,<span class="hljs-string"><span class="hljs-string">"BackendData"</span></span>:{<span class="hljs-string"><span class="hljs-string">"VtepMAC"</span></span>:<span class="hljs-string"><span class="hljs-string">"22:ca:ac:15:71:59"</span></span>}}</code> </pre> <br><p>  Apr√®s avoir enregistr√© la configuration souhait√©e dans etcd, vous devez configurer le service pour l'ex√©cuter sur chacun des n≈ìuds de travail. </p><br><h2 id="flannelservice">  flannel.service </h2><br><p>  Un exemple pour le service peut √™tre pris ici: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">systemd / flannel</a> </p><br><div class="spoiler">  <b class="spoiler_title">flannel.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Flanneld overlay address etcd agent After=network.target [Service] Type=notify #: current host ip. don't change if ip have not changed Environment=PUBLIC_IP=178.79.168.130 Environment=FLANNEL_ETCD=https://192.168.153.60:2379 ExecStart=/usr/bin/flanneld \ -etcd-endpoints=${FLANNEL_ETCD} -etcd-prefix=${FLANNEL_ETCD_KEY} \ -etcd-cafile=/var/lib/kubernetes/ca.pem \ -etcd-certfile=/var/lib/kubernetes/kubernetes.pem \ -etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \ -etcd-prefix=/coreos.com/network \ -healthz-ip=127.0.0.1 \ -subnet-file=/run/flannel/subnet.env \ -public-ip=${PUBLIC_IP} \ -kubeconfig-file=/var/lib/kubernetes/config/kubeconfig/flanneld.kubeconfig \ $FLANNEL_OPTIONS ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker Restart=on-failure RestartSec=5 [Install] RequiredBy=docker.service</code> </pre> </div></div><br><h2 id="nastroyka">  Personnalisation </h2><br><p>  Comme d√©crit pr√©c√©demment, nous avons besoin des fichiers ca.pem, kubernetes.pem et kubernetes-key.pem pour l'autorisation dans etcd.  Tous les autres param√®tres n'ont aucune signification sacr√©e.  La seule chose qui est vraiment importante est de configurer l'adresse IP globale par laquelle les paquets r√©seau passeront entre les r√©seaux: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/037/e54/803/037e5480319cedd1e662c925bce23b3e.png" alt="R√©seautage en flanelle"><br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Superposition de mise en r√©seau multi-h√¥tes avec flanelle</a> ) </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable flanneld.service #:  node-1$ systemctl start flanneld</span></span></code> </pre> <br><p>  Une fois la flanelle d√©marr√©e avec succ√®s, vous devriez trouver l'interface r√©seau flannel.N sur votre syst√®me: </p><br><pre> <code class="plaintext hljs">node-1$ ifconfig flannel.100: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1450 inet 10.200.8.0 netmask 255.255.255.255 broadcast 0.0.0.0 inet6 fe80::20ca:acff:fe15:7159 prefixlen 64 scopeid 0x20&lt;link&gt; ether 22:ca:ac:15:71:59 txqueuelen 0 (Ethernet) RX packets 18853 bytes 1077085 (1.0 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 11856 bytes 264331154 (252.0 MiB) TX errors 0 dropped 47 overruns 0 carrier 0 collisions 0</code> </pre> <br><p>  V√©rifier que vos interfaces fonctionnent correctement sur tous les n≈ìuds est assez simple.  Dans mon cas, le n≈ìud 1 et le n≈ìud 2 ont respectivement les r√©seaux 10.200.8.0/24 et 10.200.12.0/24, donc avec une demande r√©guli√®re d'icmp, nous v√©rifions leur disponibilit√©: </p><br><pre> <code class="plaintext hljs">#:  node-2  node-1 node-1 $ ping -c 1 10.200.12.0 PING 10.200.12.0 (10.200.12.0) 56(84) bytes of data. 64 bytes from 10.200.12.0: icmp_seq=1 ttl=64 time=4.58 ms #:  node-1  node-2 node-2 $ ping -c 1 10.200.8.0 PING 10.200.8.0 (10.200.8.0) 56(84) bytes of data. 64 bytes from 10.200.8.0: icmp_seq=1 ttl=64 time=1.44 ms</code> </pre> <br><p>  En cas de probl√®me, il est recommand√© de v√©rifier s'il existe des r√®gles de coupe dans iptables sur UDP entre les h√¥tes. </p><br><h1 id="konfiguraciya-containerd">  Configuration de Containerd </h1><br><p>  Placez <a href="">etc / containerd / config.toml</a> dans <code>/etc/containerd/config.toml</code> ou l√† o√π cela vous convient, l'essentiel est de vous rappeler de changer le chemin d'acc√®s au fichier de configuration dans le service (containerd.service, d√©crit ci-dessous). </p><br><p>  Configuration avec quelques modifications de la norme.  <strong>Il est important de ne pas d√©finir</strong> <code>enable_tls_streaming = true</code> si vous ne comprenez pas pourquoi vous faites cela.    <code>kubectl exec</code>        ,      . </p><br><h2 id="containerdservice"> containerd.service </h2><br><div class="spoiler"> <b class="spoiler_title">containerd.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target [Service] ; uncomment this if your overlay module are built as module ; ExecStartPre=/sbin/modprobe overlay ExecStart=/usr/bin/containerd \ -c /etc/containerd/config.toml Restart=always RestartSec=5 Delegate=yes KillMode=process OOMScoreAdjust=-999 LimitNOFILE=1048576 LimitNPROC=infinity LimitCORE=infinity [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-1">  Personnalisation </h2><br><p>  ,   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cri-tools</a> . <br>  <a href="">etc/crictl.yaml</a>  <code>/etc/crictl.yaml</code> .      : </p><br><pre> <code class="bash hljs">node-1$ CONTAINERD_NAMESPACE=k8s.io crictl ps CONTAINER ID IMAGE CREATED STATE NAME ATTEMPT POD ID</code> </pre> <br><p>  ,    -    kubernetes , crictl       , ,    . </p><br><h1 id="konfiguraciya-cni-plugins">  CNI Plugins </h1><br><p>  CNI    ,      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a>   ,    ,   . </p><br><h1 id="nastroyka-2">  Personnalisation </h1><br><p>       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cni plugins</a>        <code>/opt/cni/bin/</code> </p><br><p>  <a href="">/etc/cni/net.d</a>      : </p><br><div class="spoiler"> <b class="spoiler_title">/etc/cni/net.d/10-flannel.conflist</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.0", "name": "cbr0", "plugins": [ { "type": "flannel", "name": "kubenet", "delegate": { "hairpinMode": true, "isDefaultGateway": true } }, { "type": "portmap", "capabilities": { "portMappings": true }, "externalSetMarkChain": "KUBE-MARK-MASQ" } ] }</code> </pre> </div></div><br><div class="spoiler"> <b class="spoiler_title">/etc/cni/net.d/99-loopback.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.0", "type": "loopback" }</code> </pre> </div></div><br><p>       ,    .  ,       ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Red Hat  Docker  Podman</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Intro to Podman</a> </p><br><h1 id="konfiguraciya-kubelet">  Kubelet </h1><br><p>     kubelet  (     cni) ‚Äî    .   kubelet    hostname.         ,      ""   <code>kubectl logs</code> , <code>kubectl exec</code> , <code>kubectl port-forward</code> . </p><br><div class="spoiler"> <b class="spoiler_title"> kubelet-config.yaml</b> <div class="spoiler_text"><p>  ,   <a href="">etc/kubelet-config.yaml</a>   ,        ,     .     : </p><br><pre> <code class="plaintext hljs">systemReserved: cpu: 200m memory: 600Mi</code> </pre> <br><p>  ,        GO  kubernetes,  ,       .        .           0.2 vCPU  600 MB     . </p><br><p>   ,  , kubelet, kube-proxy, coredns, flannel    . ,               ‚Äî     2 vCPU / 4G ram,           ,     kubernetes + postgresql . </p><br><p>    - (micro nodes)        . </p></div></div><br><h2 id="kubeletservice"> kubelet.service </h2><br><p>  service    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">systemd/kubelet</a> </p><br><div class="spoiler"> <b class="spoiler_title">kubelet.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes Requires=containerd.service [Service] #Environment=NODE_IP=192.168.164.230 Environment=NODE_IP=178.79.168.130 #: node name given by env Environment=NODE_NAME=w40k.net ExecStart=kubelet \ --allow-privileged \ --root-dir=/var/lib/kubernetes/kubelet \ --config=/var/lib/kubernetes/kubelet/kubelet-config.yaml \ --kubeconfig=/var/lib/kubernetes/kubelet/node-1.kubeconfig \ --cni-bin-dir=/opt/cni/bin \ --cni-conf-dir=/etc/cni/net.d/ \ --network-plugin=cni \ --container-runtime=remote \ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \ --image-pull-progress-deadline=10m \ --node-ip=${NODE_IP} \ --hostname-override=${NODE_NAME} \ --v=1 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-3">  Personnalisation </h2><br><p>      ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">RBAC</a> ,                kubelet. </p><br><p>  <a href="">etc/kubelet-default-rbac.yaml</a>  ,  kubelet        : </p><br><pre> <code class="bash hljs">user$ kubectl apply -f etc/kubelet-default-rbac.yaml</code> </pre> <br><p>  ,    ,        . </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable kubelet.service #:  node-1$ systemctl start kubelet</span></span></code> </pre> <br><p>    ,           api : </p><br><pre> <code class="plaintext hljs">$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME w40k.net Ready &lt;none&gt; 5m v1.13.1 178.79.168.130 &lt;none&gt; Gentoo/Linux 4.18.16-x86_64-linode118 containerd://1.2.1</code> </pre> <br><h1 id="konfiguraciya-kube-proxy">  Kube Proxy </h1><br><p> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">systemd/kubelet</a> .   ,   , <code>kube-proxy-config.yaml</code>     : <a href="">etc/kube-proxy</a> </p><br><h2 id="kube-proxyservice"> kube-proxy.service </h2><br><div class="spoiler"> <b class="spoiler_title">kube-proxy.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Kubernetes Proxy Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=kube-proxy \ --config=/var/lib/kubernetes/kube-proxy/kube-proxy-config.yaml Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-4">  Personnalisation </h2><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable kube-proxy.service #:  node-1$ systemctl start kube-proxy</span></span></code> </pre> <br><p>   kube-proxy   ""   iptables,         ,   -   kubernetes  (- ).   . </p><br><h1 id="konfiguraciya-coredns">  CoreDNS </h1><br><p> Corefile   : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">etc/coredns/Corefile</a> ,    : </p><br><div class="spoiler"> <b class="spoiler_title">/etc/coredns/Corefile</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">.:53 { errors log stdout health :8081 kubernetes cluster.local 10.200.0.0/16 { endpoint https://178.79.148.185:6443 tls /var/lib/kubernetes/kubernetes.pem /var/lib/kubernetes/kubernetes-key.pem /var/lib/kubernetes/ca.pem pods verified upstream /etc/resolv.conf kubeconfig /var/lib/kubernetes/config/kubeconfig/coredns.kubeconfig default } proxy . /etc/resolv.conf cache 30 }</code> </pre> </div></div><br><p>     coredns.kubeconfig  pem- (    )   worker . , coredns      systemd-resolved. ,         Ubuntu ,  ,  ,  ,  .        . </p><br><h2 id="corednsservice"> coredns.service </h2><br><div class="spoiler"> <b class="spoiler_title">coredns.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=CoreDNS Documentation=https://coredns.io/ After=network.target [Service] ExecStart=/usr/bin/coredns -conf /etc/coredns/Corefile Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-5">  Personnalisation </h2><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable coredns.service #:  node-1$ systemctl start coredns</span></span></code> </pre> <br><p> ,   ,   : </p><br><pre> <code class="plaintext hljs">node-1$ dig kubernetes.default.svc.cluster.local @127.0.0.1 #:    ;kubernetes.default.svc.cluster.local. IN A ;; ANSWER SECTION: kubernetes.default.svc.cluster.local. 5 IN A 10.32.0.1</code> </pre> <br><p>   , coredns   ip   kubernetes . <br> <strong></strong> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">kubernetes.default </a>   <strong></strong> kube-controller-manager,      : </p><br><pre> <code class="plaintext hljs">$ kubectl get svc -n default NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.32.0.1 &lt;none&gt; 443/TCP 26h</code> </pre> <br><h1 id="nginx-ingress--cert-manager"> nginx-ingress &amp; cert-manager </h1><br><p>   ,    .        nginx-ingress  cert-manager. </p><br><p> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">nginx kubernetes ingress</a> (master),  : </p><br><pre> <code class="bash hljs"> user$ git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/nginxinc/kubernetes-ingress.git user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> kubernetes-ingress/deployments user$ kubectl apply -f common/ns-and-sa.yaml user$ kubectl apply -f common/nginx-config.yaml user$ kubectl apply -f common/default-server-secret.yaml user$ kubectl apply -f daemon-set/nginx-ingress.yaml user$ kubectl apply -f rbac/rbac.yaml</code> </pre> <br><p> ‚Äî <a href="">cert manager</a> (v0.5.2) </p><br><pre> <code class="bash hljs"> user$ git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/jetstack/cert-manager.git user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> cert-manager &amp;&amp; git co v0.5.2 user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> contrib/manifests/cert-manager user$ kubectl apply -f with-rbac.yaml</code> </pre> <br><p>  ,    ,  ,    : </p><br><pre> <code class="plaintext hljs">NAMESPACE NAME READY STATUS RESTARTS AGE cert-manager cert-manager-554c76fbb7-t9762 1/1 Running 0 3h38m nginx-ingress nginx-ingress-sdztf 1/1 Running 0 10h nginx-ingress nginx-ingress-vrf85 1/1 Running 0 10h</code> </pre> <br><p>  cert-manager  nginx-ingress    running state,   ,    .          ,         <code>Running</code> .            . </p><br><h1 id="zapuskaem-prilozhenie">   </h1><br><p>   ,     .      ,   kubernetes resource : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">app/k8s</a> </p><br><pre> <code class="bash hljs">user$ kube apply -f ns-and-sa.yaml user$ kube apply -f configmap.yaml <span class="hljs-comment"><span class="hljs-comment">#:  secret-example.yaml       #: secret.yaml user$ kube apply -f secret.yaml user$ kube apply -f tls-production.yaml user$ kube apply -f deployment.yaml user$ kube apply -f service.yaml user$ kube apply -f ingress-production.yaml</span></span></code> </pre> <br><p>   ,     - .  ,    (      kubernetes-example.w40k.net),     ,    ,  cert-manager    nginx-ingress              .   ,    ingress  tls/ssl. </p><br><p>      : </p><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=http://no-">http://no-https.kubernetes-example.w40k.net/</a> ‚Äî  ssl;  ,  -   ,     . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://kubernetes-example.w40k.net/</a> ‚Äî   (,   ,   ),  ,     ,       kubernetes     . </li></ul><br><p>       ,      -   .    -       ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a> ,        . </p><br><h1 id="ssylki">  Les r√©f√©rences </h1><br><p> ,     ,   : </p><br><p> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubernetes the hard way</a> <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Multi-Host Networking Overlay with Flannel</a> <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Intro to Podman</a> <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Stateless Applications</a> <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">What is ingress</a> </p><br><p>   : </p><br><p> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubernetes Networking: Behind the scenes</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> ) <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">A Guide to the Kubernetes Networking Model</a> <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Understanding kubernetes networking: services</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> ) </p><br><h1 id="qa"> Q&amp;A </h1><br><p> &lt;tbd&gt;,           . </p><br><h1 id="otladochnaya-informaciya">   </h1><br><p>     , ,     .    ,       ,  -  ,    ,  . </p><br><div class="spoiler"> <b class="spoiler_title"> </b> <div class="spoiler_text"><h2 id="api-server"> Api Server </h2><br><p>   <code>kube-apiserver.service</code>    ,       api-server'   curl    http .            - . <br>     admin.kubeconfig  ${HOME}/.kube/config,   kubectl      api-server (kube-apiserver). </p><br><p>    (   )  HTTP 200 OK + ,  api-server  : </p><br><pre> <code class="plaintext hljs">curl -H "Authorization: Bearer e5qXNAtwwCHUUwyLilZmAoFPozrQwUpw" -k -L https://&lt;api-server-address&gt;:6443/api/v1/</code> </pre> <br><h2 id="kube-controller-manager"> Kube Controller Manager </h2><br><p>  ,  controller manager   api    ,      .        ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">service account'</a> : </p><br><pre> <code class="plaintext hljs">$ kubectl get sa NAME SECRETS AGE default 1 19h</code> </pre> <br><p>    ,   ,  kube-controller-manager  . </p><br><h2 id="kube-scheduler"> Kube Scheduler </h2><br><p>       .  ,    ,    <a href="">debug/job.yaml</a>        <code>kubectl describe &lt;type/resource&gt;</code> . <br>    <strong> </strong>  ,  kube controller manager . </p><br><pre> <code class="plaintext hljs">#:   job user$ kubectl apply -f debug/job.yaml job.batch/app created #:  ,   job user$ kubectl get pods -l job-name=app NAME READY STATUS RESTARTS AGE app-9kr9z 0/1 Completed 0 54s #: ,        #:   user$ kubectl describe pods app-9kr9z # ...   ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 12s default-scheduler Successfully assigned example/app-9kr9z to w40k.net</code> </pre> <br><p>   , default-scheduler   pod   w40k.net.    -  ,            ‚Äî    . </p><br><p>              . , ,   , ‚Äî      "".       systemd        . </p><br><p>   kube scheduler  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> </p><br><h2 id="kubelet"> Kubelet </h2><br><p> Kubelet    kubernetes     .  kubelet       .     kubernetes event ( <code>kubectl get events -o wide</code> )         . </p><br><p>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> (  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> ) </p><br><h2 id="kube-proxy-i-servisy"> Kube Proxy   </h2><br><p>     kube-proxy    : </p><br><ul><li>      (     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> Flannel</a> ,      ); </li><li>  iptables,   filter  nat . </li></ul><br><p> <strong></strong> , 10.32.0.0/24   "".  ,        .     iptables,     ,   ,     -    +.  <strong> </strong>  icmp    ,      ping'  .        ,     . </p><br><p>  ,     kube-proxy,               : </p><br><pre> <code class="plaintext hljs">#:    user$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend ClusterIP 10.32.0.195 &lt;none&gt; 80/TCP 5m #:     user$ kubectl get pods -o wide #:     ' NAME READY STATUS RESTARTS AGE IP NODE backend-896584448-4r94s 1/1 Running 0 11h 10.200.8.105 w40k.net backend-896584448-np992 1/1 Running 0 11h 10.200.12.68 docker.grart.net #:  10   /status/ endpoint ,       #:       node-1$ for i in `seq 10`; do curl -L http://10.32.0.195/status/; done okokokokokokokokokok node-1$ conntrack -L -d 10.32.0.195 tcp 6 62 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62158 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62158 [ASSURED] mark=0 use=1 tcp 6 60 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62144 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62144 [ASSURED] mark=0 use=1 tcp 6 58 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62122 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62122 [ASSURED] mark=0 use=1 tcp 6 59 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62142 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62142 [ASSURED] mark=0 use=1 tcp 6 58 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62130 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62130 [ASSURED] mark=0 use=1 tcp 6 61 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62150 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62150 [ASSURED] mark=0 use=1 tcp 6 56 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62116 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62116 [ASSURED] mark=0 use=1 tcp 6 57 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62118 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62118 [ASSURED] mark=0 use=1 tcp 6 59 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62132 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62132 [ASSURED] mark=0 use=1 tcp 6 56 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62114 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62114 [ASSURED] mark=0 use=1</code> </pre> <br><p>      src/dst (9  10 ).   ,  src      : </p><br><ul><li> 10.200.8.105 </li><li> 10.200.12.68 </li></ul><br><p>  ,    .      ,  -  ( ,    )  .         . </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#: node-1   10.200.8.105, node-2 10.200.12.68, #:      8000  #:  node-1 node-1$ curl -L http://10.200.8.105:8000/status/ ok node-1$ curl -L http://10.200.12.68:8000/status/ ok #:  node-2 node-2$ curl -L http://10.200.8.105:8000/status/ ok node-2$ curl -L http://10.200.12.68:8000/status/ ok</span></span></code> </pre> <br><p>    ,    ,    conntrack        ,  ,      kube-proxy.   ,       nat : </p><br><p> <code>node-1$ iptables -t nat -vnL</code> </p> <br><p>          . </p><br><p>                  .  ,    ,      .   ,       .  -       , ,   . </p><br><p>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> </p></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr435228/">https://habr.com/ru/post/fr435228/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr435214/index.html">Linux 4.20 est sorti - ce qui a chang√© dans la nouvelle version du noyau</a></li>
<li><a href="../fr435216/index.html">Comment faire 200 √† partir de deux lignes de code et pourquoi vous devez le faire</a></li>
<li><a href="../fr435220/index.html">Kotlin Native: garder une trace des fichiers</a></li>
<li><a href="../fr435224/index.html">Comment communiquer dans un bureau anglophone: 14 idiomes utiles</a></li>
<li><a href="../fr435226/index.html">Restaurer les donn√©es √† partir de z√©ro</a></li>
<li><a href="../fr435234/index.html">Plus intelligent, plus loin et plus pr√©cis√©ment: comment l'IA transforme les vols dans l'espace</a></li>
<li><a href="../fr435236/index.html">Octet-machine pour le fort (et pas seulement) en am√©rindien (partie 3)</a></li>
<li><a href="../fr435240/index.html">Unreal Engine4 - Effet de scan PostProcess</a></li>
<li><a href="../fr435242/index.html">Pourquoi ai-je peur de devenir un "homme pomp√©"</a></li>
<li><a href="../fr435244/index.html">Projet ITER en 2018</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>