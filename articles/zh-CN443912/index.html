<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>◀️ 🧑🏽‍🤝‍🧑🏻 ❗️ 自制BigData。 第1部分。AWS集群上的Spark流实践 👨🏼‍🏫 🛄 🙎🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="你好 

 在本文中，我们将在家中的EC2 AWS（Amazon Web Services）平台上安装Apache Kafka，Apache Spark，Zookeeper，Spark-shell，并学习如何使用它们。 

 介绍Amazon Web Services 
 1.1。 您必须在aws....">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>自制BigData。 第1部分。AWS集群上的Spark流实践</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/443912/">你好 <br><br> 在本文中，我们将在家中的EC2 AWS（Amazon Web Services）平台上安装Apache Kafka，Apache Spark，Zookeeper，Spark-shell，并学习如何使用它们。 <br><a name="habracut"></a><br><h3> 介绍Amazon Web Services </h3><br>  1.1。 您必须在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">aws.amazon.com/console</a>进行注册。 输入名称并记住密码。 <br><br>  1.2。 为Zookeeper和Kafka服务配置节点实例。 <br><br><ul><li> 从菜单中选择“服务-&gt; EC2”。 接下来，选择虚拟机映像的操作系统版本，选择Ubuntu Server 16.04 LTS（HVM），SSD卷类型，单击“选择”。我们继续配置服务器实例：键入“ t3.medium”，其参数为2vCPU，4 GB内存，通用单击“下一步：配置实例详细信息”。 </li><li> 添加实例数1，单击“下一步：添加存储” </li><li> 我们接受8 GB磁盘大小的默认值，并将类型更改为Magnetic（在生产设置中，基于数据量和High Performance SSD） </li><li> 在“名称”的“标签实例”部分中，输入节点“ Home1”的实例的名称（其中1只是序列号），然后单击“下一步：...”。 </li><li> 在“配置安全组”部分中，通过选择安全组的名称（“ Spark_Kafka_Zoo_Project”）来选择“使用现有安全组”选项，并设置传入流量的规则。 点击“下一步：...” </li><li> 滚动查看页面，以验证您的输入并启动启动。 </li><li> 要连接到群集节点，必须创建（在我们的示例中，使用现有的）一对公共密钥以进行标识和授权。 为此，请在列表中选择操作类型“使用现有对”。 </li></ul><br><h3> 密钥创建 </h3><br><ul><li> 为客户端<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">下载Putty</a>或从终端使用SSH连接。 </li><li> 为了方便起见，.pem密钥文件使用旧格式，我们将其转换为Putty使用的ppk格式。 为此，请运行PuTTYgen实用程序，将旧的.pem格式的密钥加载到该实用程序中。 我们转换密钥并将其保存（保存私钥），以备以后在扩展名为.ppk的主文件夹中使用。 </li></ul><br><h3> 集群启动 </h3><br>  1.3。 为方便起见，请使用Node01-04表示法重命名群集节点。 要通过SSH从本地计算机连接到群集节点，您需要确定节点的IP地址及其公用/专用DNS名称，一个接一个地选择每个群集节点，并为所选实例写下其公用/专用DNS名称，以通过SSH连接并进行安装将软件添加到文本文件HadoopAdm01.txt。 <br><br> 示例：ec2-35-162-169-76.us-west-2.compute.amazonaws.com <br><br><h3> 在AWS集群节点上以单节点模式安装Apache Kafka </h3><br>  2.1。 要安装软件，请选择我们的节点（复制其公共DNS）以通过SSH连接。 我们通过SSH配置连接。 我们使用第一个节点的保存名称，使用条款1.3中创建的私钥/公钥对“ HadoopUser01.ppk”，通过SSH配置连接。 我们通过“浏览”按钮转到“连接/身份验证”部分，并找到我们先前保存文件“ HadoopUserXX.ppk”的文件夹。 <br><br> 我们将连接配置保存在设置中。 <br><br>  2.2。 我们已连接到该节点并使用登录名：ubuntu。 <br><br>  •使用root特权，我们更新软件包并安装进一步安装和配置集群所需的其他软件包。 <br><br><pre><code class="plaintext hljs">sudo apt-get update sudo apt-get -y install wget net-tools netcat tar</code> </pre> <br>  •安装Java 8 jdk并检查Java版本。 <br><br><pre> <code class="plaintext hljs">sudo apt-get -y install openjdk-8-jdk</code> </pre><br>  •为了获得正常的群集节点性能，您需要调整内存交换设置。  VM swappines默认设置为60％，这意味着当以60％的比例使用内存时，系统会主动将数据从RAM交换到磁盘。 根据Linux的版本，VM swappines参数可以设置为0或1： <br><br><pre> <code class="plaintext hljs">sudo sysctl vm.swappiness=1</code> </pre><br>  •要在重新引导期间保存设置，请在配置文件中添加一行。 <br><br><pre> <code class="plaintext hljs">echo 'vm.swappiness=1' | sudo tee --append /etc/sysctl.conf</code> </pre><br>  •编辑/ etc / hosts文件中的条目，以方便地将群集节点名称kafka和zookeeper解析为已分配群集节点的专用IP地址。 <br><br><pre> <code class="plaintext hljs">echo "172.31.26.162 host01" | sudo tee --append /etc/hosts</code> </pre><br> 我们使用ping任何条目检查名称的正确识别。 <br><br>  •下载kafka和scala发行版的最新最新版本（http://kafka.apache.org/downloads），并准备一个包含安装文件的目录。 <br><br><pre> <code class="plaintext hljs">wget http://mirror.linux-ia64.org/apache/kafka/2.1.0/kafka_2.12-2.1.0.tgz tar -xvzf kafka_2.12-2.1.0.tgz ln -s kafka_2.12-2.1.0 kafka</code> </pre><br>  •删除tgz存档文件，我们将不再需要它 <br><br>  •为此，我们尝试启动Zookeeper服务： <br><br><pre> <code class="plaintext hljs">~/kafka/bin/zookeeper-server-start.sh -daemon ~/kafka/config/zookeeper.properties</code> </pre><br>  Zookeeper从默认启动选项开始。 您可以检查日志： <br><br><pre> <code class="plaintext hljs"> tail -n 5 ~/kafka/logs/zookeeper.out</code> </pre><br> 为了确保Zookeeper守护程序在重新启动后启动，我们需要将Zookeper作为后台服务启动： <br><br><pre> <code class="plaintext hljs">bin/zookeeper-server-start.sh -daemon config/zookeeper.properties</code> </pre><br> 要检查Zookepper的发射，请检查 <br><br><pre> <code class="plaintext hljs">netcat -vz localhost 2181</code> </pre><br>  2.3。 我们将Zookeeper和Kafka服务配置为工作。 最初，编辑/创建文件/etc/systemd/system/zookeeper.service（下面的文件内容）。 <br><br><pre> <code class="plaintext hljs">[Unit] Description=Apache Zookeeper server Documentation=http://zookeeper.apache.org Requires=network.target remote-fs.target After=network.target remote-fs.target [Service] Type=simple ExecStart=/home/ubuntu/kafka/bin/zookeeper-server-start.sh /home/ubuntu/kafka/config/zookeeper.properties ExecStop=/home/ubuntu/kafka/bin/zookeeper-server-stop.sh [Install] WantedBy=multi-user.target</code> </pre><br> 接下来，对于Kafka，编辑/创建文件/etc/systemd/system/kafka.service（下面的文件内容）。 <br><br><pre> <code class="plaintext hljs">[Unit] Description=Apache Kafka server (broker) Documentation=http://kafka.apache.org/documentation.html Requires=zookeeper.service [Service] Type=simple ExecStart=/home/ubuntu/kafka/bin/kafka-server-start.sh /home/ubuntu/kafka/config/server.properties ExecStop=/home/ubuntu/kafka/bin/kafka-server-stop.sh [Install] WantedBy=multi-user.target</code> </pre><br>  •激活Kafka和Zookeeper服务的系统脚本。 <br><br><pre> <code class="plaintext hljs">sudo systemctl enable zookeeper sudo systemctl enable kafka</code> </pre><br>  •检查systemd脚本的操作。 <br><br><pre> <code class="plaintext hljs">sudo systemctl start zookeeper sudo systemctl start kafka sudo systemctl status zookeeper sudo systemctl status kafka sudo systemctl stop zookeeper sudo systemctl stop kafka</code> </pre><br>  •检查Kafka和Zookeeper服务的可维护性。 <br><br><pre> <code class="plaintext hljs">netcat -vz localhost 2181 netcat -vz localhost 9092</code> </pre><br>  •检查Zookeeper日志文件。 <br><br><pre> <code class="plaintext hljs">cat logs/zookeeper.out</code> </pre><br><h3> 第一喜 </h3><br>  2.4。 我们在组装的kafka服务器上创建第一个主题。 <br><ul><li> 如在server.properties配置文件中指出的，使用到“ host01：2181”的连接很重要。 </li><li> 我们在主题中编写了一些数据。 </li></ul><br><pre> <code class="plaintext hljs">kafka-console-producer.sh --broker-list host01:9092 --topic first_topic    </code> </pre><br>  Ctrl-C-退出主题控制台。 <br><br>  •现在尝试从主题中读取数据。 <br><br><pre> <code class="plaintext hljs">kafka-console-consumer.sh --bootstrap-server host01:9092 --topic last_topic --from-beginning</code> </pre><br>  •查看kafka主题列表。 <br><br><pre> <code class="plaintext hljs">bin/kafka-topics.sh --zookeeper spark01:2181 --list</code> </pre><br>  •编辑kafka服务器参数以进行单群集设置调整 <br>  ＃您需要将ISR参数更改为1。 <br><br><pre> <code class="plaintext hljs">bin/kafka-topics.sh --zookeeper spark01:2181 --config min.insync.replicas=1 --topic __consumer_offsets --alter</code> </pre><br>  •重新启动Kafka服务器，然后尝试再次连接用户欧姆 <br><br>  •让我们看一看主题列表。 <br><br><pre> <code class="plaintext hljs">bin/kafka-topics.sh --zookeeper host01:2181 --list</code> </pre><br><h3> 在单节点集群上配置Apache Spark </h3><br> 我们准备了在AWS上安装了Zookeeper和Kafka服务的节点实例，现在您需要为此安装Apache Spark： <br><br>  3.1。 下载最新的Apache Spark发行版。 <br><br><pre> <code class="plaintext hljs">wget https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.6.tgz</code> </pre><br>  •解压缩发行版，并为spark创建符号链接，并删除不必要的存档文件。 <br><br><pre> <code class="plaintext hljs">tar -xvf spark-2.4.0-bin-hadoop2.6.tgz ln -s spark-2.4.0-bin-hadoop2.6 spark rm spark*.tgz</code> </pre><br>  •转到sbin目录并运行spark向导。 <br><br><pre> <code class="plaintext hljs">./start-master.sh</code> </pre><br>  •使用Web浏览器连接到端口8080上的Spark服务器。 <br><br>  •在同一节点上运行Spark-slaves <br><br><pre> <code class="plaintext hljs">./start-slave.sh spark://host01:7077</code> </pre><br>  •在host01上与主模块一起运行spark shell。 <br><br><pre> <code class="plaintext hljs">./spark-shell --master spark://host01:7077</code> </pre><br>  •如果启动不起作用，请在bash中将路径添加到Spark。 <br><br><pre> <code class="plaintext hljs">vi ~/.bashrc #      SPARK_HOME=/home/ubuntu/spark export PATH=$SPARK_HOME/bin:$PATH</code> </pre><br><pre> <code class="plaintext hljs">source ~/.bashrc</code> </pre><br>  •使用host01上的主模块再次运行spark shell。 <br><br><pre> <code class="plaintext hljs">./spark-shell --master spark://host01:7077</code> </pre><br>  3.2。 具有Kafka，Zookeeper和Spark的单节点群集可以工作。 万岁！ <br><br><h3> 一点创意 </h3><br>  4.1。 下载Scala-IDE编辑器（位于<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">scala-ide.org</a> ）。 我们开始并开始编写代码。 在此不再赘述，因为<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">在哈布雷（Habré）上</a>有一篇<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">不错的文章</a> 。 <br><br>  4.2。 有用的文献和课程可帮助： <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">courses.hadoopinrealworld.com/courses/enrolled/319237</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">data-flair.training/博客/ kafka消费者</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">www.udemy.com/apache-spark-with-scala-hands-on-with-big-data</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN443912/">https://habr.com/ru/post/zh-CN443912/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN443902/index.html">我们如何通过学习skibidi，牙线和javascript赢得内部黑客马拉松</a></li>
<li><a href="../zh-CN443904/index.html">如果没有足够的立方体来构建机器人，该怎么办？</a></li>
<li><a href="../zh-CN443906/index.html">视频分析采集器：大脑和机器如何处理我们的脸部</a></li>
<li><a href="../zh-CN443908/index.html">我们控制Android设备</a></li>
<li><a href="../zh-CN443910/index.html">临时编程概念</a></li>
<li><a href="../zh-CN443914/index.html">我们如何做SCRUM</a></li>
<li><a href="../zh-CN443916/index.html">文化部和电影制片人部正提议建立一种在互联网上记录观众的状态系统</a></li>
<li><a href="../zh-CN443918/index.html">两个新的简约口袋游戏</a></li>
<li><a href="../zh-CN443920/index.html">30年后发现未发行的NES游戏</a></li>
<li><a href="../zh-CN443924/index.html">GDPR可以很好地保护您的个人数据，但前提是您在欧洲</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>