<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚è≠Ô∏è üïØÔ∏è üï∫ Prediksi dari ahli matematika. Kami menganalisis metode utama untuk mendeteksi anomali üçè üëâ üõÉ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Penggunaan kecerdasan buatan dalam industri untuk pemeliharaan prediktif berbagai sistem semakin populer di luar negeri. Tujuan metodologi ini adalah ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Prediksi dari ahli matematika. Kami menganalisis metode utama untuk mendeteksi anomali</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/lanit/blog/447190/">  Penggunaan kecerdasan buatan dalam industri untuk <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pemeliharaan prediktif</a> berbagai sistem semakin populer di luar negeri.  Tujuan metodologi ini adalah untuk mengidentifikasi malfungsi dalam pengoperasian sistem selama fase operasi hingga kegagalannya untuk respons yang tepat waktu. <br><br>  Seberapa relevankah pendekatan ini di negara kita dan di Barat?  Kesimpulannya dapat dibuat, misalnya, pada artikel tentang Habr√© dan dalam Medium.  Hampir tidak ada artikel tentang Habr√© tentang pemecahan masalah pemeliharaan prediktif.  Pada Medium ada satu set lengkap.  Di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> , di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> dan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> dijelaskan dengan baik apa tujuan dan keuntungan dari pendekatan ini. <br><br>  Dari artikel ini Anda akan belajar: <br><br><ul><li>  mengapa teknik ini dibutuhkan </li><li>  pendekatan pembelajaran mesin yang lebih umum digunakan untuk pemeliharaan prediktif, </li><li>  bagaimana saya mencoba salah satu trik dengan contoh sederhana. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/845/2a4/888/8452a4888db8d633dd14d426f6b80cbe.jpg"></div>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><i>Sumber</i></a> <br><a name="habracut"></a><br>  Fitur apa saja yang disediakan layanan prediksi? <br><br><ul><li>  proses pekerjaan perbaikan yang terkontrol, yang dilakukan seperlunya, sehingga menghemat uang, dan tanpa terburu-buru, yang meningkatkan kualitas pekerjaan ini; </li><li>  identifikasi kerusakan tertentu dalam pengoperasian peralatan (kemampuan untuk membeli bagian tertentu untuk penggantian ketika peralatan beroperasi memberikan keuntungan besar); </li><li>  optimalisasi operasi peralatan, beban, dll.; </li><li>  pengurangan biaya untuk pemadaman peralatan secara rutin. </li></ul><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Artikel berikutnya tentang Media</a> menjelaskan dengan baik pertanyaan yang perlu dijawab untuk memahami bagaimana cara mendekati masalah ini dalam kasus tertentu. <br><br>  Saat mengumpulkan data atau ketika memilih data untuk membangun model, penting untuk menjawab tiga kelompok pertanyaan: <br><br><ol><li>  Bisakah semua masalah sistem diprediksi?  Prediksi mana yang sangat penting? </li><li>  Apa itu proses kegagalan?  Apakah seluruh sistem berhenti bekerja atau apakah mode operasi hanya berubah?  Apakah ini proses cepat, degradasi instan atau bertahap? </li><li>  Apakah kinerja sistem cukup mencerminkan kinerjanya?  Apakah mereka berhubungan dengan bagian-bagian individual dari sistem atau ke sistem secara keseluruhan? </li></ol><br>  Penting juga untuk memahami terlebih dahulu apa yang ingin Anda prediksi, dan apa yang mungkin untuk diprediksi dan apa yang tidak. <br><br>  Artikel pada Medium juga mencantumkan pertanyaan yang akan membantu menentukan tujuan spesifik Anda: <br><br><ul><li>  Apa yang perlu diprediksi?  Sisa waktu hidup, perilaku abnormal atau tidak, probabilitas kegagalan dalam N jam / hari / minggu berikutnya? </li><li>  Apakah ada cukup data historis? </li><li>  Apakah diketahui kapan sistem melakukan pembacaan yang tidak normal, dan kapan tidak.  Apakah mungkin untuk menandai indikasi seperti itu? </li><li>  Seberapa jauh model harus melihat?  Seberapa independen pembacaan yang mencerminkan operasi sistem dalam interval satu jam / hari / minggu </li><li>  Apa yang Anda butuhkan untuk mengoptimalkan?  Haruskah model menangkap pelanggaran sebanyak mungkin, sambil memberikan alarm palsu, atau cukup menangkap beberapa peristiwa tanpa positif palsu. </li></ul><br>  Diharapkan bahwa situasi akan membaik di masa depan.  Sejauh ini, ada kesulitan di bidang pemeliharaan prediktif: ada beberapa contoh kerusakan sistem, atau saat-saat kegagalan fungsi sistem sudah cukup, tetapi tidak ditandai;  proses kegagalan tidak diketahui. <br><br>  Cara utama untuk mengatasi kesulitan dalam pemeliharaan prediktif adalah dengan menggunakan <b>metode pencarian anomali</b> .  Algoritma semacam itu tidak memerlukan markup untuk pelatihan.  Untuk algoritma pengujian dan debugging, markup dalam satu bentuk atau lainnya diperlukan.  Metode tersebut terbatas karena mereka tidak akan memprediksi kegagalan tertentu, tetapi hanya menandakan kelainan indikator. <br><br>  Tapi ini sudah tidak buruk. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/847/46c/6d7/84746c6d7414722b0a1b7b322000f201.jpg"></div>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><i>Sumber</i></a> <br><br><h2>  Metode </h2><br>  Sekarang saya ingin berbicara tentang beberapa fitur dari pendekatan deteksi anomali, dan kemudian bersama-sama kita akan menguji kemampuan beberapa algoritma sederhana dalam praktiknya. <br><br>  Meskipun situasi tertentu akan memerlukan pengujian beberapa algoritma untuk mencari anomali dan memilih yang terbaik, adalah mungkin untuk menentukan beberapa kelebihan dan kekurangan dari teknik utama yang digunakan di area ini. <br><br>  Pertama-tama, penting untuk memahami terlebih dahulu berapa persentase anomali dalam data. <br><br>  Jika kita berbicara tentang variasi dari pendekatan semi-diawasi (kita belajar hanya pada data "normal", dan kita bekerja (menguji) kemudian pada data dengan anomali), maka pilihan yang paling optimal adalah <b>metode vektor dukungan dengan satu kelas ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">SVM Satu Kelas</a> )</b> .  Saat menggunakan fungsi basis radial sebagai kernel, algoritma ini membangun permukaan nonlinear di sekitar titik asal.  Semakin bersih data pelatihan, semakin baik kerjanya. <br><br>  Dalam kasus lain, kebutuhan untuk mengetahui rasio poin abnormal dan "normal" juga tetap - untuk menentukan ambang batas cutoff. <br><br>  Jika jumlah anomali dalam data lebih dari 5%, dan mereka cukup baik dipisahkan dari sampel utama, metode pencarian anomali standar dapat digunakan. <br><br>  Dalam hal ini, <b>metode <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">hutan isolasi</a></b> adalah yang paling stabil dalam hal kualitas: <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">hutan isolasi adalah</a></b> data acak.  Indikasi yang lebih khas cenderung lebih dalam, sementara indikator yang tidak biasa akan terpisah dari sisa sampel di iterasi pertama. <br><br>  Algoritme lain berfungsi lebih baik jika "sesuai" dengan spesifikasi data tertentu. <br><br>  Ketika data memiliki distribusi normal, <b>metode amplop Elliptic</b> cocok, mendekati data dengan distribusi normal multidimensi.  Semakin kecil kemungkinan bahwa titik tersebut termasuk dalam distribusi, semakin besar probabilitas bahwa anomali. <br><br>  Jika data disajikan sedemikian rupa sehingga posisi relatif dari titik yang berbeda mencerminkan perbedaan mereka dengan baik, maka metode metrik tampaknya menjadi pilihan yang baik: misalnya, <b>k tetangga terdekat, tetangga terdekat k-th, ABOD (deteksi berbasis berbasis sudut) atau LOF (faktor pencilan lokal) )</b> <br><br>  Semua metode ini menunjukkan bahwa indikator "benar" terkonsentrasi di satu area ruang multidimensi.  Jika di antara k (atau k-th) tetangga terdekat semuanya jauh dari target, maka intinya adalah anomali.  Untuk ABOD, alasannya serupa: jika semua titik terdekat k berada di sektor ruang yang sama relatif terhadap yang dianggap, maka titik tersebut adalah anomali.  Untuk LOF: jika kepadatan lokal (ditentukan sebelumnya untuk setiap titik oleh k tetangga terdekat) lebih rendah dari k tetangga terdekat, maka titik tersebut adalah anomali. <br><br>  Jika data terkelompok dengan baik, <b>metode berdasarkan analisis kelompok</b> adalah pilihan yang baik.  Jika titik tersebut sama jauhnya dari pusat beberapa cluster, maka itu adalah anomali. <br><br>  Jika arah variasi terbesar dalam varian dibedakan dengan baik dalam data, maka itu tampaknya menjadi pilihan yang baik untuk <b>mencari anomali berdasarkan metode komponen utama</b> .  Dalam hal ini, penyimpangan dari nilai rata-rata untuk n1 (komponen paling "utama") dan n2 (paling sedikit "utama") dianggap sebagai ukuran anomali. <br><br>  Sebagai contoh, disarankan untuk melihat kumpulan data dari <b>The Prognostics and Health Management Society (PHM Society)</b> .  Organisasi nirlaba ini mengatur kompetisi setiap tahun.  Pada tahun 2018, misalnya, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">diperlukan untuk memprediksi kesalahan dalam operasi dan waktu sebelum kegagalan pabrik etsa balok ion</a> .  Kami akan mengambil <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">kumpulan data untuk tahun 2015</a> .  Ini berisi pembacaan beberapa sensor selama 30 instalasi (sampel pelatihan), dan diperlukan untuk memprediksi kapan dan kesalahan apa yang akan terjadi. <br><br>  Saya tidak menemukan jawaban untuk sampel uji di jaringan, jadi kami hanya akan bermain dengan yang pelatihan. <br><br>  Secara umum, semua pengaturan serupa, tetapi berbeda, misalnya, dalam jumlah komponen, jumlah anomali, dll.  Oleh karena itu, belajar di 20 pertama, dan menguji pada orang lain tidak masuk akal. <br><br>  Jadi, kita akan memilih salah satu instalasi, memuatnya dan melihat data ini.  Artikel ini tidak akan membahas tentang <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">rekayasa fitur</a> , jadi kami tidak akan mengintip banyak hal. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt %matplotlib inline <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.covariance <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> EllipticEnvelope <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.neighbors <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LocalOutlierFactor <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.ensemble <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> IsolationForest <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.svm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> OneClassSVM dfa=pd.read_csv(<span class="hljs-string"><span class="hljs-string">'plant_12a.csv'</span></span>,names=[<span class="hljs-string"><span class="hljs-string">'Component number'</span></span>,<span class="hljs-string"><span class="hljs-string">'Time'</span></span>,<span class="hljs-string"><span class="hljs-string">'S1'</span></span>,<span class="hljs-string"><span class="hljs-string">'S2'</span></span>,<span class="hljs-string"><span class="hljs-string">'S3'</span></span>,<span class="hljs-string"><span class="hljs-string">'S4'</span></span>,<span class="hljs-string"><span class="hljs-string">'S1ref'</span></span>,<span class="hljs-string"><span class="hljs-string">'S2ref'</span></span>,<span class="hljs-string"><span class="hljs-string">'S3ref'</span></span>,<span class="hljs-string"><span class="hljs-string">'S4ref'</span></span>]) dfa.head(<span class="hljs-number"><span class="hljs-number">10</span></span>)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/301/77e/2bb/30177e2bb970e82495b18008b44d7ea4.jpg"></div><br>  Seperti yang Anda lihat, ada tujuh komponen untuk masing-masing yang ada pembacaan empat sensor yang diambil setiap 15 menit.  S1ref-S4ref dalam deskripsi kompetisi terdaftar sebagai nilai referensi, tetapi nilainya sangat berbeda dari pembacaan sensor.  Agar tidak membuang waktu memikirkan apa artinya, kami menghapusnya.  Jika Anda melihat distribusi nilai untuk setiap karakteristik (S1-S4), ternyata distribusi tersebut berkelanjutan untuk S1, S2 dan S4, dan diskrit untuk S3.  Selain itu, jika Anda melihat distribusi gabungan S2 dan S4, ternyata keduanya berbanding terbalik. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/69b/82b/cde/69b82bcdecdfca69128d88673b0518b4.jpg"></div><br>  Meskipun penyimpangan dari ketergantungan langsung dapat mengindikasikan kesalahan, kami tidak akan memeriksa ini, tetapi cukup menghapus S4. <br><br>  Sekali lagi, kami memproses kumpulan data.  Tinggalkan S1, S2 dan S3.  Skala S1 dan S2 dengan StandardScaler (kita kurangi rata-rata dan bagi dengan deviasi standar), terjemahkan S3 ke OHE (One Hot Encoding).  Kami menjahit bacaan dari semua komponen instalasi dalam satu baris.  Total 89 fitur.  2 * 7 = 14 - bacaan S1 dan S2 untuk 7 komponen dan 75 nilai unik R3.  Hanya 56 ribu garis seperti itu. <br><br>  Unggah file dengan kesalahan. <br><br><pre> <code class="python hljs">dfc=pd.read_csv(<span class="hljs-string"><span class="hljs-string">'plant_12c.csv'</span></span>,names=[<span class="hljs-string"><span class="hljs-string">'Start Time'</span></span>, <span class="hljs-string"><span class="hljs-string">'End Time'</span></span>,<span class="hljs-string"><span class="hljs-string">'Type'</span></span>]) dfc.head()</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3ad/419/e50/3ad419e50ac870a71c153b523a22bfed.jpg"></div><br>  Sebelum mencoba algoritme ini pada kumpulan data kami, saya akan membiarkan saya menyimpang sedikit.  Anda perlu diuji.  Untuk ini, diusulkan untuk mengambil waktu mulai dari kesalahan dan waktu akhir.  Dan semua indikasi di dalam interval ini dianggap abnormal, dan di luar normal.  Pendekatan ini memiliki banyak kelemahan.  Tetapi terutama satu - perilaku abnormal kemungkinan besar terjadi sebelum kesalahan diperbaiki.  Untuk kesetiaan, marilah kita menggeser jendela anomali setengah jam yang lalu dalam waktu.  Kami akan mengevaluasi ukuran-F1, presisi, dan penarikan kembali. <br><br>  Kode untuk membedakan fitur dan menentukan kualitas model: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">load_and_preprocess</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(plant_num)</span></span></span><span class="hljs-function">:</span></span>   <span class="hljs-comment"><span class="hljs-comment">#      ,       dfa=pd.read_csv('plant_{}a.csv'.format(plant_num),names=['Component number','Time','S1','S2','S3','S4','S1ref','S2ref','S3ref','S4ref'])   dfc=pd.read_csv('plant_{}c.csv'.format(plant_num),names=['Start Time','End Time','Type']).drop(0,axis=0)   N_comp=len(dfa['Component number'].unique())   #  15    dfa['Time']=pd.to_datetime(dfa['Time']).dt.round('15min')   #  6    (  ,    )   dfc=dfc[dfc['Type']!=6]   dfc['Start Time']=pd.to_datetime(dfc['Start Time'])   dfc['End Time']=pd.to_datetime(dfc['End Time'])   #      ,       OHE  3-    dfa=pd.concat([dfa.groupby('Time').nth(i)[['S1','S2','S3']].rename(columns={"S1":"S1_{}".format(i),"S2":"S2_{}".format(i),"S3":"S3_{}".format(i)}) for i in range(N_comp)],axis=1).dropna().reset_index()   for k in range(N_comp):       dfa=pd.concat([dfa.drop('S3_'+str(k),axis=1),pd.get_dummies(dfa['S3_'+str(k)],prefix='S3_'+str(k))],axis=1).reset_index(drop=True)   #          df_train,df_test=train_test_split(dfa,test_size=0.25,shuffle=False)   cols_to_scale=df_train.filter(regex='S[1,2]').columns   scaler=preprocessing.StandardScaler().fit(df_train[cols_to_scale])   df_train[cols_to_scale]=scaler.transform(df_train[cols_to_scale])   df_test[cols_to_scale]=scaler.transform(df_test[cols_to_scale])   return df_train,df_test,dfc #       def get_true_labels(measure_times,dfc,shift_delta):   idxSet=set()   dfc['Start Time']-=pd.Timedelta(minutes=shift_delta)   dfc['End Time']-=pd.Timedelta(minutes=shift_delta)   for idx,mes_time in tqdm_notebook(enumerate(measure_times),total=measure_times.shape[0]):       intersect=np.array(dfc['Start Time']&lt;mes_time).astype(int)*np.array(dfc['End Time']&gt;mes_time).astype(int)       idxs=np.where(intersect)[0]       if idxs.shape[0]:           idxSet.add(idx)   dfc['Start Time']+=pd.Timedelta(minutes=shift_delta)   dfc['End Time']+=pd.Timedelta(minutes=shift_delta)   true_labels=pd.Series(index=measure_times.index)   true_labels.iloc[list(idxSet)]=1   true_labels.fillna(0,inplace=True)   return true_labels #          def check_model(model,df_train,df_test,filt='S[123]'):   model.fit(df_train.drop('Time',axis=1).filter(regex=(filt)))   y_preds = pd.Series(model.predict(df_test.drop(['Time','Label'],axis=1).filter(regex=(filt)))).map({-1:1,1:0})   print('F1 score: {:.3f}'.format(f1_score(df_test['Label'],y_preds)))   print('Precision score: {:.3f}'.format(precision_score(df_test['Label'],y_preds)))   print('Recall score: {:.3f}'.format(recall_score(df_test['Label'],y_preds)))   score = model.decision_function(df_test.drop(['Time','Label'],axis=1).filter(regex=(filt)))   sns.distplot(score[df_test['Label']==0])   sns.distplot(score[df_test['Label']==1]) df_train,df_test,anomaly_times=load_and_preprocess(12) df_test['Label']=get_true_labels(df_test['Time'],dfc,30)</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/50b/6f2/be9/50b6f2be9c3220853e92461805030e0b.jpg"></div>  <i>Hasil tes untuk algoritma pencarian anomali sederhana pada dataset Tantangan Data PHM 2015</i> <br><br>  Kembali ke algoritme.  Mari kita coba One Class SVM (OCSVM), IsolationForest (IF), EllipticEnvelope (EE) dan LocalOutlierFactor (LOF) pada data kami.  Untuk memulainya, kami tidak akan menetapkan parameter apa pun.  Saya perhatikan bahwa LOF dapat bekerja dalam dua mode.  Jika novelty = False dapat mencari anomali hanya di set pelatihan (hanya ada fit_predict), jika Benar, maka itu ditujukan untuk mencari anomali di luar set pelatihan (dapat cocok dan memprediksi secara terpisah).  JIKA memiliki mode perilaku lama dan baru.  Kami menggunakan yang baru.  Dia memberikan hasil yang lebih baik. <br><br>  OCSVM mendeteksi anomali dengan baik, tetapi ada terlalu banyak kesalahan positif.  Untuk metode lain, hasilnya bahkan lebih buruk. <br><br>  Tapi anggaplah kita tahu persentase anomali dalam data.  Dalam kasus kami, 27%.  OCSVM memiliki nu - estimasi atas untuk persentase kesalahan dan yang lebih rendah untuk persentase vektor dukungan.  Metode kontaminasi lainnya memiliki persentase kesalahan data.  Dalam metode IF dan LOF, ditentukan secara otomatis, sedangkan untuk OCSVM dan EE diatur ke 0,1 secara default.  Mari kita coba atur kontaminasi (nu) menjadi 0,27.  Sekarang hasil teratas untuk EE. <br><br>  Kode untuk memeriksa model: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">check_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model,df_train,df_test,filt=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'S[123]'</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span>   model_type,model = model   model.fit(df_train.drop(<span class="hljs-string"><span class="hljs-string">'Time'</span></span>,axis=<span class="hljs-number"><span class="hljs-number">1</span></span>).filter(regex=(filt)))   y_preds = pd.Series(model.predict(df_test.drop([<span class="hljs-string"><span class="hljs-string">'Time'</span></span>,<span class="hljs-string"><span class="hljs-string">'Label'</span></span>],axis=<span class="hljs-number"><span class="hljs-number">1</span></span>).filter(regex=(filt)))).map({<span class="hljs-number"><span class="hljs-number">-1</span></span>:<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">0</span></span>})   print(<span class="hljs-string"><span class="hljs-string">'F1 score for {}: {:.3f}'</span></span>.format(model_type,f1_score(df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>],y_preds)))   print(<span class="hljs-string"><span class="hljs-string">'Precision score for {}: {:.3f}'</span></span>.format(model_type,precision_score(df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>],y_preds)))   print(<span class="hljs-string"><span class="hljs-string">'Recall score for {}: {:.3f}'</span></span>.format(model_type,recall_score(df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>],y_preds)))   score = model.decision_function(df_test.drop([<span class="hljs-string"><span class="hljs-string">'Time'</span></span>,<span class="hljs-string"><span class="hljs-string">'Label'</span></span>],axis=<span class="hljs-number"><span class="hljs-number">1</span></span>).filter(regex=(filt)))   sns.distplot(score[df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>]==<span class="hljs-number"><span class="hljs-number">0</span></span>])   sns.distplot(score[df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>]==<span class="hljs-number"><span class="hljs-number">1</span></span>])   plt.title(<span class="hljs-string"><span class="hljs-string">'Decision score distribution for {}'</span></span>.format(model_type))   plt.show()</code> </pre> <br>  Sangat menarik untuk melihat distribusi indikator anomali untuk berbagai metode.  Dapat dilihat bahwa LOF tidak berfungsi dengan baik untuk data ini.  EE memiliki poin yang algoritma anggap sangat abnormal.  Namun, poin normal jatuh di sana.  IsoFor dan OCSVM menunjukkan bahwa pilihan ambang batas (kontaminasi / nu) penting, yang akan mengubah trade-off antara akurasi dan kelengkapan. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a9d/0c8/80f/a9d0c880f36fe37f1de6c9290b8cccc7.png"></div><br>  Adalah logis bahwa pembacaan sensor memiliki distribusi mendekati normal, dekat dengan nilai stasioner.  Jika kita benar-benar memiliki sampel uji berlabel, dan lebih disukai sampel validasi, maka nilai kontaminasi dapat diwarnai.  Pertanyaan selanjutnya adalah, kesalahan mana yang lebih berorientasi: false positive atau false negative? <br><br>  Hasil LOF sangat rendah.  Tidak terlalu mengesankan.  Tapi ingat bahwa variabel OHE pergi ke input bersama dengan variabel yang diubah oleh StandardScaler.  Dan jarak standar adalah Euclidean.  Tetapi jika Anda hanya menghitung variabel sesuai dengan S1 dan S2, maka situasinya diperbaiki dan hasilnya sebanding dengan metode lain.  Namun, penting untuk dipahami bahwa salah satu parameter utama dari pengklasifikasi metrik yang terdaftar adalah jumlah tetangga.  Ini secara signifikan mempengaruhi kualitas, dan itu harus disetel.  Metrik jarak itu sendiri juga akan bagus untuk diambil. <br><br>  Sekarang coba kombinasikan kedua model.  Di awal satu, kami menghapus anomali dari set pelatihan.  Dan kemudian kita akan melatih OCSVM pada set pelatihan "bersih".  Menurut hasil sebelumnya, kami mengamati kelengkapan terbesar dalam EE.  Kami menghapus sampel pelatihan melalui EE, melatih OCSVM di atasnya dan mendapatkan F1 = 0,50, Akurasi = 0,34, kelengkapan = 0,95.  Tidak mengesankan.  Tapi kami hanya bertanya nu = 0,27.  Dan data yang kami miliki kurang lebih "bersih."  Jika kita mengasumsikan bahwa kepenuhan EE pada sampel pelatihan adalah sama, maka 5% kesalahan akan tetap ada.  Kami mengatur sendiri nu tersebut dan mendapatkan F1 = 0,69, Akurasi = 0,59, kelengkapan = 0,82.  Bagus  Penting untuk dicatat bahwa dalam metode lain kombinasi seperti itu tidak akan bekerja, karena mereka menyiratkan bahwa jumlah anomali dalam set pelatihan dan jumlah tes adalah sama.  Saat melatih metode ini pada kumpulan data pelatihan murni, Anda harus menetapkan lebih sedikit kontaminasi daripada dalam data nyata dan tidak mendekati nol, tetapi lebih baik memilihnya untuk validasi silang. <br><br>  Sangat menarik untuk melihat hasil pencarian pada urutan indikasi: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2a2/0c5/26d/2a20c526d944b55116f3fcd6af064eab.png"></div><br>  Gambar tersebut menunjukkan segmen pembacaan sensor pertama dan kedua untuk 7 komponen.  Dalam legenda, warna kesalahan yang sesuai (awal dan akhir ditunjukkan oleh garis-garis vertikal dengan warna yang sama).  Titik menunjukkan prediksi: prediksi hijau - benar, merah - positif palsu, ungu - negatif negatif.  Dapat dilihat dari gambar bahwa sulit untuk menentukan waktu kesalahan secara visual, dan algoritma mengatasi tugas ini dengan cukup baik.  Meskipun penting untuk dipahami bahwa pembacaan sensor ketiga tidak diberikan di sini.  Selain itu, ada pembacaan positif palsu setelah akhir kesalahan.  Yaitu  Algoritma melihat bahwa ada juga nilai yang salah, dan kami menandai area ini sebagai bebas kesalahan.  Sisi kanan gambar menunjukkan area sebelum kesalahan, yang kami tandai sebagai salah (setengah jam sebelum kesalahan), yang dikenali sebagai bebas kesalahan, yang mengarah ke kesalahan model negatif palsu.  Di tengah-tengah gambar, sepotong yang koheren diakui, diakui sebagai kesalahan.  Kesimpulannya dapat ditarik sebagai berikut: ketika memecahkan masalah mencari anomali, Anda perlu berinteraksi secara dekat dengan insinyur yang memahami esensi sistem yang outputnya perlu Anda prediksi, karena memeriksa algoritma yang digunakan pada markup tidak sepenuhnya mencerminkan kenyataan dan tidak mensimulasikan kondisi di mana algoritma tersebut bisa digunakan. <br><br>  Kode untuk merencanakan grafik: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">plot_time_course</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(df_test,dfc,y_preds,start,end,vert_shift=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">4</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span>   plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">15</span></span>,<span class="hljs-number"><span class="hljs-number">10</span></span>))   cols=df_train.filter(regex=(<span class="hljs-string"><span class="hljs-string">'S[12]'</span></span>)).columns   add=<span class="hljs-number"><span class="hljs-number">0</span></span>   preds_idx=y_preds.iloc[start:end][y_preds[<span class="hljs-number"><span class="hljs-number">0</span></span>]==<span class="hljs-number"><span class="hljs-number">1</span></span>].index   true_idx=df_test.iloc[start:end,:][df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>]==<span class="hljs-number"><span class="hljs-number">1</span></span>].index   tp_idx=set(true_idx.values).intersection(set(preds_idx.values))   fn_idx=set(true_idx.values).difference(set(preds_idx.values))   fp_idx=set(preds_idx.values).difference(set(true_idx.values))   xtime=df_test[<span class="hljs-string"><span class="hljs-string">'Time'</span></span>].iloc[start:end]   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> col <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> cols:       plt.plot(xtime,df_test[col].iloc[start:end]+add)       plt.scatter(xtime.loc[tp_idx].values,df_test.loc[tp_idx,col]+add,color=<span class="hljs-string"><span class="hljs-string">'green'</span></span>)       plt.scatter(xtime.loc[fn_idx].values,df_test.loc[fn_idx,col]+add,color=<span class="hljs-string"><span class="hljs-string">'violet'</span></span>)       plt.scatter(xtime.loc[fp_idx].values,df_test.loc[fp_idx,col]+add,color=<span class="hljs-string"><span class="hljs-string">'red'</span></span>)       add+=vert_shift   failures=dfc[(dfc[<span class="hljs-string"><span class="hljs-string">'Start Time'</span></span>]&gt;xtime.iloc[<span class="hljs-number"><span class="hljs-number">0</span></span>])&amp;(dfc[<span class="hljs-string"><span class="hljs-string">'Start Time'</span></span>]&lt;xtime.iloc[<span class="hljs-number"><span class="hljs-number">-1</span></span>])]   unique_fails=np.sort(failures[<span class="hljs-string"><span class="hljs-string">'Type'</span></span>].unique())   colors=np.array([np.random.rand(<span class="hljs-number"><span class="hljs-number">3</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> fail <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> unique_fails])   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> fail_idx <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> failures.index:       c=colors[np.where(unique_fails==failures.loc[fail_idx,<span class="hljs-string"><span class="hljs-string">'Type'</span></span>])[<span class="hljs-number"><span class="hljs-number">0</span></span>]][<span class="hljs-number"><span class="hljs-number">0</span></span>]       plt.axvline(failures.loc[fail_idx,<span class="hljs-string"><span class="hljs-string">'Start Time'</span></span>],color=c)       plt.axvline(failures.loc[fail_idx,<span class="hljs-string"><span class="hljs-string">'End Time'</span></span>],color=c)   leg=plt.legend(unique_fails)   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(unique_fails)):       leg.legendHandles[i].set_color(colors[i])</code> </pre> <br>  Jika persentase anomali di bawah 5% dan / atau mereka dipisahkan dengan buruk dari indikator "normal", metode di atas bekerja dengan buruk dan layak menggunakan algoritma berdasarkan jaringan saraf.  Dalam kasus paling sederhana, ini adalah: <br><br><ul><li>  auto encoders (kesalahan tinggi dari auto encoder terlatih akan menandakan kelainan dalam pembacaan); </li><li>  jaringan berulang (belajar dengan urutan untuk memprediksi bacaan terakhir. Jika perbedaannya besar - intinya tidak normal). </li></ul><br>  Secara terpisah, perlu dicatat spesifikasi bekerja dengan deret waktu.  Penting untuk dipahami bahwa sebagian besar algoritma di atas (kecuali autoencoder dan mengisolasi hutan) kemungkinan besar akan memberikan kualitas yang lebih buruk ketika menambahkan fitur jeda (pembacaan dari titik sebelumnya dalam waktu). <br><br>  Mari kita coba tambahkan fitur lag pada contoh kita.  Deskripsi kompetisi mengatakan bahwa nilai 3 jam sebelum kesalahan tidak terhubung dengan kesalahan dengan cara apa pun.  Kemudian tambahkan tanda-tanda dalam 3 jam.  Total 259 tanda. <br><br>  Akibatnya, hasil untuk OCSVM dan IsolationForest tetap hampir tidak berubah, sedangkan untuk Elliptic Envelope dan LOF turun. <br><br>  Untuk menggunakan informasi tentang dinamika sistem, auto-encoders dengan jaringan saraf berulang atau convolutional harus digunakan.  Atau, misalnya, kombinasi auto-encoders, mengompresi informasi, dan pendekatan konvensional untuk mencari anomali berdasarkan informasi terkompresi.  Pendekatan sebaliknya juga tampak menjanjikan.  Penapisan primer dari poin yang paling tidak biasa dengan algoritma standar, dan kemudian melatih autode-encoder pada data yang lebih bersih. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/340/9e8/dbc/3409e8dbcc5e9bcac2a1acb08bc7d146.jpg"></div>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><i>Sumber</i></a> <br><br>  Ada serangkaian teknik untuk bekerja dengan deret waktu satu dimensi.  Semuanya ditujukan untuk memprediksi pembacaan di masa depan, dan poin yang menyimpang dari prediksi dianggap anomali. <br><br><h2>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Model Holt-Winters</a> </h2><br>  Triple smoothing eksponensial, membagi seri menjadi 3 komponen: level, tren, dan musiman.  Dengan demikian, jika seri disajikan dalam bentuk ini, metode ini berfungsi dengan baik.  Facebook Prophet beroperasi dengan prinsip yang sama, tetapi mengevaluasi komponen itu sendiri dengan cara yang berbeda.  Lebih detail dapat dibaca, misalnya, di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . <br><br><h2>  S (ARIMA) </h2><br>  Dalam metode ini, model prediktif didasarkan pada autoregresi dan moving average.  Jika kita berbicara tentang perluasan S (ARIMA), maka itu memungkinkan kita untuk mengevaluasi musiman.  Baca lebih lanjut tentang pendekatan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> , di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> dan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . <br><br><h2>  Pendekatan layanan prediktif lainnya </h2><br>  Ketika datang ke seri waktu dan ada informasi tentang waktu terjadinya kesalahan, Anda dapat menerapkan metode pengajaran dengan seorang guru.  Selain perlunya data yang ditandai, dalam hal ini penting untuk memahami bahwa prediksi kesalahan akan tergantung pada sifat kesalahan.  Jika ada banyak kesalahan dan sifat yang berbeda, kemungkinan besar akan perlu untuk memprediksi masing-masing secara terpisah, yang akan membutuhkan lebih banyak data yang ditandai, tetapi prospeknya akan lebih menarik. <br><br>  Ada cara alternatif untuk menggunakan pembelajaran mesin dalam pemeliharaan prediktif.  Misalnya, memprediksi kegagalan sistem dalam N hari berikutnya (tugas klasifikasi).  Penting untuk dipahami bahwa pendekatan semacam itu mensyaratkan terjadinya kesalahan dalam pengoperasian sistem didahului oleh periode degradasi (tidak harus bertahap).  Dalam hal ini, pendekatan yang paling berhasil tampaknya adalah penggunaan jaringan saraf dengan lapisan konvolusional dan / atau berulang.  Secara terpisah, perlu dicatat metode untuk menambah deret waktu.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Menurut saya dua pendekatan</a> yang paling menarik dan sekaligus sederhana: <br><br><ul><li>  bagian kontinu dari baris dipilih (misalnya, 70%, dan sisanya dihilangkan) dan direntangkan ke ukuran aslinya </li><li>  sebagian baris terus menerus (mis., 20%) dipilih dan diregangkan atau dikompresi.  Setelah itu, seluruh baris dikompresi atau direntangkan sesuai dengan ukuran aslinya. </li></ul><br>  Ada juga pilihan untuk memprediksi masa pakai sistem yang tersisa (tugas regresi).  Di sini kita dapat membedakan pendekatan yang terpisah: prediksi bukanlah masa pakai, tetapi dari parameter distribusi Weibull. <br><br>  Anda dapat membaca tentang distribusi itu sendiri di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> , dan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> tentang penggunaannya dalam hubungannya dengan jerat berulang.  Distribusi ini memiliki dua parameter Œ± dan Œ≤.  Œ± menunjukkan kapan acara akan terjadi, dan Œ≤ menunjukkan seberapa percaya diri algoritma tersebut.  Meskipun penerapan pendekatan ini menjanjikan, kesulitan muncul dalam melatih jaringan saraf dalam kasus ini, karena lebih mudah untuk algoritma menjadi tidak aman pada awalnya daripada memprediksi masa hidup yang memadai. <br><br>  Secara terpisah, perlu dicatat <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">regresi Cox</a> .  Ini memungkinkan Anda untuk memprediksi toleransi kesalahan sistem untuk setiap titik waktu setelah diagnosis, menyajikannya sebagai produk dari dua fungsi.  Salah satu fungsinya adalah degradasi sistem, terlepas dari parameternya, mis.  umum untuk sistem semacam itu.  Dan yang kedua adalah ketergantungan eksponensial pada parameter sistem tertentu.  Jadi bagi seseorang ada fungsi umum yang terkait dengan penuaan, kurang lebih sama untuk semua orang.  Tetapi penurunan kesehatan juga terkait dengan keadaan organ dalam, yang berbeda untuk semua orang. <br><br>  Saya harap Anda sekarang tahu lebih banyak tentang pemeliharaan prediktif.  Saya yakin Anda akan memiliki pertanyaan mengenai metode pembelajaran mesin yang paling sering digunakan untuk teknologi ini.  Saya akan dengan senang hati menjawab masing-masing dalam komentar.  Jika Anda tertarik untuk tidak hanya bertanya tentang apa yang ditulis, tetapi ingin melakukan hal serupa, tim <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">CleverDATA</a> kami selalu senang dengan profesional yang berbakat dan antusias. <br><br><div class="spoiler">  <b class="spoiler_title">Apakah ada lowongan?</b>  <b class="spoiler_title">Tentu saja!</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pengembang Java (Big Data)</a> </li></ul></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id447190/">https://habr.com/ru/post/id447190/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id447178/index.html">5 peluang efektif untuk menggunakan teknologi proses penambangan</a></li>
<li><a href="../id447180/index.html">Ikhtisar dan Perbandingan Pengontrol Ingress untuk Kubernetes</a></li>
<li><a href="../id447182/index.html">Sistem Operasi: Tiga Potongan Mudah. Bagian 3: API Proses (terjemahan)</a></li>
<li><a href="../id447184/index.html">Apa itu Initial Exchange Offering (IEO) dan bagaimana perbedaannya dengan ICO?</a></li>
<li><a href="../id447186/index.html">Cara meluncurkan prototipe ML dalam satu hari. Laporkan Yandex.Taxi</a></li>
<li><a href="../id447192/index.html">Apa peran yang bisa dimainkan oleh teknologi dalam seni kuno pencampuran rempah?</a></li>
<li><a href="../id447194/index.html">Fitur Rendering di Metro: Exodus c raytracing</a></li>
<li><a href="../id447196/index.html">7. Periksa Titik Memulai R80.20. Kontrol akses</a></li>
<li><a href="../id447198/index.html">Misi Lunar "Bereshit": pendaratan-kecelakaan-jatuh di bulan</a></li>
<li><a href="../id447204/index.html">17 April: Kuliah terbuka "Lintasan pengembang game: dari ide ke peluncuran" dan perpustakaan game di Sekolah Tinggi Hukum</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>