<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üèÜ üéå üôã Mask-R CNN du d√©butant au professionnel üõÄüèæ üßîüèº üïµüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Une fois que j'avais besoin d'analyser les informations de l'image et de la sortie pour avoir le type de l'objet, son type, et aussi, analyser la tota...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Mask-R CNN du d√©butant au professionnel</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/483018/"><p><img src="https://lh6.googleusercontent.com/2ZHVaccuXIFgG1Z5qnHROW6cKyxjEe1RLo0SwRAaOxxyQI7Q_Ymbs7ZuZ6bOs56v7oMaCInsarFjOPZRDWL5hNdKpVhlVtHINo9u3gae4utQB-FARZPFxEV5UmkYY8LVJdTjZ0PK"></p><br><p> Une fois que j'avais besoin d'analyser les informations de l'image et de la sortie pour avoir le type de l'objet, son type, et aussi, analyser la totalit√© des images, je devais donner l'identifiant de l'objet et le temps pass√© dans l'image, il fallait d√©terminer comment l'objet se d√©pla√ßait et quelles cam√©ras √©taient visibles.  Commen√ßons peut-√™tre par les deux premiers, l'analyse du personnel dans son ensemble sera discut√©e dans la partie suivante. </p><a name="habracut"></a><br><p>  Eh bien, nous d√©crirons plus en d√©tail nos t√¢ches: </p><br><ul><li>  R√©parez les personnes et les voitures - s√©lectionnez-les dans l'image et g√©n√©rez les instances de classe correspondantes avec les champs n√©cessaires. </li><li>  D√©terminer le num√©ro de la voiture, si elle est tomb√©e dans le cadre d'une cam√©ra sp√©cifique </li><li>  Comparez le cadre actuel avec le pr√©c√©dent pour l'√©galit√© des objets, afin que nous puissions d√©couvrir </li></ul><br><p>  Ok, je pensais, et j'ai ramass√© un serpent √©pais, du python, √ßa veut dire.  Il a √©t√© d√©cid√© d'utiliser le r√©seau neuronal <a href="https://github.com/matterport/Mask_RCNN">Mask R-Cnn</a> en lien avec sa simplicit√© et <a href="https://habr.com/ru/post/421299/">ses fonctionnalit√©s modernes</a> .  Bien entendu, nous utiliserons √©galement OpenCV pour la manipulation d'images. </p><br><h2 id="ustanovka-sredy">  Configuration de l'environnement </h2><br><p>  Nous utiliserons Windows 10, car vous √™tes le plus susceptible de l'utiliser. <br>  Il est entendu que vous disposez d√©j√† de Python 64 bits.  Sinon, vous pouvez t√©l√©charger le package, par exemple, √† <a href="https://www.python.org/downloads/release/python-374/">partir d'ici</a> </p><br><h3 id="ustanovka-paketov">  Installation du package </h3><br><pre><code class="plaintext hljs">git clone https://github.com/matterport/Mask_RCNN cd Mask_RCNN pip3 install -r requirements.txt python3 setup.py install</code> </pre> <br><p>  Si pour une raison quelconque, il n'est pas possible de compiler √† partir des sources, il existe une version de pip: </p><br><pre> <code class="plaintext hljs">pip3 install mrcnn --user</code> </pre> <br><p>  Le paquet, bien s√ªr, est livr√© avec toutes les <a href="https://github.com/matterport/Mask_RCNN/blob/master/requirements.txt">d√©pendances</a> . </p><br><h2 id="etap-1-sozdanie-prosteyshey-programmy-raspoznavatelya">  √âtape 1. Cr√©ation d'un simple reconnaisseur. </h2><br><p>  Nous ferons les importations n√©cessaires </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mrcnn.config <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mrcnn <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mrcnn.model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MaskRCNN</code> </pre> <br><p>  Le r√©seau de neurones n√©cessite la cr√©ation d'une configuration avec des champs remplac√©s </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">MaskRCNNConfig</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(mrcnn.config.Config)</span></span></span><span class="hljs-class">:</span></span> NAME = <span class="hljs-string"><span class="hljs-string">"coco_pretrained_model_config"</span></span> GPU_COUNT = <span class="hljs-number"><span class="hljs-number">1</span></span> IMAGES_PER_GPU = <span class="hljs-number"><span class="hljs-number">1</span></span> DETECTION_MIN_CONFIDENCE = <span class="hljs-number"><span class="hljs-number">0.8</span></span> <span class="hljs-comment"><span class="hljs-comment">#     NUM_CLASSES = 81</span></span></code> </pre> <br><p>  Indiquez l'emplacement du fichier avec les √©chelles.  Laissez dans cet exemple, il sera dans le dossier avec ce fichier.  Si ce n'est pas le cas, il sera t√©l√©charg√©. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mrcnn.utils DATASET_FILE = <span class="hljs-string"><span class="hljs-string">"mask_rcnn_coco.h5"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> os.path.exists(DATASET_FILE): mrcnn.utils.download_trained_weights(DATASET_FILE)</code> </pre> <br><p>  Cr√©ons notre mod√®le avec les param√®tres ci-dessus </p><br><pre> <code class="python hljs">model = MaskRCNN(mode=<span class="hljs-string"><span class="hljs-string">"inference"</span></span>, model_dir=<span class="hljs-string"><span class="hljs-string">"logs"</span></span>, config=MaskRCNNConfig()) model.load_weights(DATASET_FILE, by_name=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><p>  Et peut-√™tre que nous allons commencer √† traiter toutes les images dans le r√©pertoire <code>images</code> r√©pertoire actuel. </p><br><pre> <code class="python hljs">IMAGE_DIR = os.path.join(os.getcwd(), <span class="hljs-string"><span class="hljs-string">"images"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> filename <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> os.listdir(IMAGE_DIR): image = cv2.imread(os.path.join(IMAGE_DIR, filename)) rgb_image = image[:, :, ::<span class="hljs-number"><span class="hljs-number">-1</span></span>] detections = model.detect([rgb_image], verbose=<span class="hljs-number"><span class="hljs-number">1</span></span>)[<span class="hljs-number"><span class="hljs-number">0</span></span>]</code> </pre> <br><p>  Que verrons-nous dans les d√©tections? </p><br><pre> <code class="python hljs"> print(detections)</code> </pre> <br><p>  Par exemple, quelque chose de similaire: </p><br><pre> <code class="plaintext hljs">{'rois': array([[ 303, 649, 542, 1176],[ 405, 2, 701, 319]]), 'class_ids': array([3, 3]), 'scores': array([0.99896, 0.99770015], dtype=float32), 'masks': array()}</code> </pre> <br><p>  Dans ce cas, 2 objets ont √©t√© trouv√©s. <br>  <code>rois</code> - tableaux de coordonn√©es du coin inf√©rieur gauche et sup√©rieur droit <br>  <code>class_ids</code> sont les identifiants num√©riques des objets trouv√©s, alors que nous devons savoir que 1 est une personne, 3 est une voiture, 8 est un camion. <br>  <code>scores</code> - dans la mesure o√π le mod√®le est confiant dans la solution, ce param√®tre peut √™tre <code>DETECTION_MIN_CONFIDENCE</code> via <code>DETECTION_MIN_CONFIDENCE</code> dans la configuration, coupant toutes les options inappropri√©es. <br>  <code>masks</code> - le contour de l'objet.  Les donn√©es sont utilis√©es pour dessiner un masque d'objet.  Parce que  ils sont assez volumineux et ne sont pas destin√©s √† la compr√©hension humaine, je ne les citerai pas dans l'article. </p><br><p>  Ok, nous pourrions nous arr√™ter l√†, mais nous voulons regarder l'image que les guides sur l'utilisation des r√©seaux de neurones avec des objets magnifiquement s√©lectionn√©s donnent habituellement? </p><br><p>  Il serait plus simple d'appeler la fonction <code>mrcnn.visualize.display_instances</code> , mais nous ne le ferons pas, nous √©crirons la n√¥tre. </p><br><p>  La fonction prendra une image, et les principaux param√®tres obtenus √† partir du dictionnaire d√®s les premi√®res √©tapes. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">visualize_detections</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(image, masks, boxes, class_ids, scores)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np bgr_image = image[:, :, ::<span class="hljs-number"><span class="hljs-number">-1</span></span>] CLASS_NAMES = [<span class="hljs-string"><span class="hljs-string">'BG'</span></span>,<span class="hljs-string"><span class="hljs-string">"person"</span></span>, <span class="hljs-string"><span class="hljs-string">"bicycle"</span></span>, <span class="hljs-string"><span class="hljs-string">"car"</span></span>, <span class="hljs-string"><span class="hljs-string">"motorcycle"</span></span>, <span class="hljs-string"><span class="hljs-string">"bus"</span></span>, <span class="hljs-string"><span class="hljs-string">"truck"</span></span>] COLORS = mrcnn.visualize.random_colors(len(CLASS_NAMES)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(boxes.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]): y1, x1, y2, x2 = boxes[i] classID = class_ids[i] label = CLASS_NAMES[classID] font = cv2.FONT_HERSHEY_DUPLEX color = [int(c) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> c <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> np.array(COLORS[classID]) * <span class="hljs-number"><span class="hljs-number">255</span></span>] text = <span class="hljs-string"><span class="hljs-string">"{}: {:.3f}"</span></span>.format(label, scores[i]) size = <span class="hljs-number"><span class="hljs-number">0.8</span></span> width = <span class="hljs-number"><span class="hljs-number">2</span></span> cv2.rectangle(bgr_image, (x1, y1), (x2, y2), color, width) cv2.putText(bgr_image, text, (x1, y1<span class="hljs-number"><span class="hljs-number">-20</span></span>), font, size, color, width)</code> </pre><br><p><img src="https://lh3.googleusercontent.com/dnSAiGZW32zMK92_T8yyk2nXCFCKECQ_eSdNiVv5Bzpz9TOsBZT6_jyAY-LfUT4c0jCzfdgFOCpy6_0HzT54CAPo3vvkZ6VgR1U5cdmSTb0zLLpAxJjX-_pTNUnpIExXsao_u29b"></p><br><div class="spoiler">  <b class="spoiler_title">Image source</b> <div class="spoiler_text"><p><img src="https://sun9-10.userapi.com/c854324/v854324789/e2f73/nJHcTGLnWI4.jpg"></p></div></div><br><p>  Bien que l'un des principaux avantages de ce r√©seau de neurones soit la solution aux probl√®mes de segmentation des instances - obtention des contours d'objets, nous ne l'avons pas encore utilis√©, nous allons l'analyser. </p><br><p>  Pour impl√©menter des masques, ajoutez quelques lignes avant de dessiner un rectangle pour chaque objet trouv√©. </p><br><pre> <code class="python hljs">mask = masks[:, :, i] <span class="hljs-comment"><span class="hljs-comment">#   image = mrcnn.visualize.apply_mask(image, mask, color, alpha=0.6) #  </span></span></code> </pre> <br><p>  R√©sultat: <br><img src="https://lh6.googleusercontent.com/2ZHVaccuXIFgG1Z5qnHROW6cKyxjEe1RLo0SwRAaOxxyQI7Q_Ymbs7ZuZ6bOs56v7oMaCInsarFjOPZRDWL5hNdKpVhlVtHINo9u3gae4utQB-FARZPFxEV5UmkYY8LVJdTjZ0PK"></p><br><div class="spoiler">  <b class="spoiler_title">Version avec masques blancs</b> <div class="spoiler_text"><p><img src="https://lh5.googleusercontent.com/JUUlUQfOCc9jeeuzMDiSc2Hd06P2aMVli2UPSRkUoxlNVwdlwEfi-BHmuyzOsx-nlvm19lPmYlyxFGYV9xGzpppXRORmDBLtLYH6UcYRh1zO47ROLb04HgUggDoz14Zk2AWZ3ta3"></p></div></div><br><h2 id="etap-ii-pervye-uspehi-raspoznavanie-nomerov-mashin">  √âtape II.  Premiers succ√®s.  Reconnaissance du nombre de voitures. </h2><br><p>  Pour la reconnaissance, nous avons besoin d'un cadre clair de la voiture pr√®s, il a donc √©t√© d√©cid√© de ne prendre que des cadres du point de contr√¥le, puis de les comparer √† la similitude (plus √† ce sujet dans le chapitre suivant).  Cette m√©thode donne cependant trop d'inexactitude, car  les machines peuvent √™tre tr√®s similaires visuellement et mon algorithme ne peut pas encore √©viter de telles situations. </p><br><p>  Il a √©t√© d√©cid√© d'utiliser une <a href="https://github.com/ria-com/nomeroff-net">biblioth√®que</a> pr√™te √† l'emploi du fabricant ukrainien <a href="https://github.com/ria-com/nomeroff-net">nomeroff-net</a> (pas de publicit√©).  Parce que  presque tout le code peut √™tre trouv√© dans les exemples du mod√®le, alors je ne donnerai pas une description compl√®te. </p><br><p>  Je peux seulement dire que cette fonction peut √™tre d√©marr√©e avec l'image d'origine ou que la machine reconnue peut √™tre d√©coup√©e du cadre et pass√©e √† cette fonction. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.image <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> mpimg <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os sys.path.append(cfg.NOMEROFF_NET_DIR) <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> NomeroffNet <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> filters, RectDetector, TextDetector, OptionsDetector, Detector, textPostprocessing nnet = Detector(cfg.MASK_RCNN_DIR, cfg.MASK_RCNN_LOG_DIR) nnet.loadModel(<span class="hljs-string"><span class="hljs-string">"latest"</span></span>) rectDetector = RectDetector() optionsDetector = OptionsDetector() optionsDetector.load(<span class="hljs-string"><span class="hljs-string">"latest"</span></span>) textDetector = TextDetector.get_static_module(<span class="hljs-string"><span class="hljs-string">"ru"</span></span>)() textDetector.load(<span class="hljs-string"><span class="hljs-string">"latest"</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">detectCarNumber</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(imgPath: str)</span></span></span><span class="hljs-function"> -&gt; str:</span></span> img = mpimg.imread(imgPath) NP = nnet.detect([img]) cvImgMasks = filters.cv_img_mask(NP) arrPoints = rectDetector.detect(cvImgMasks) zones = rectDetector.get_cv_zonesBGR(img, arrPoints) regionIds, stateIds, _c = optionsDetector.predict(zones) regionNames = optionsDetector.getRegionLabels(regionIds) <span class="hljs-comment"><span class="hljs-comment"># find text with postprocessing by standart textArr = textDetector.predict(zones) textArr = textPostprocessing(textArr, regionNames) return textArr</span></span></code> </pre> <br><p>  la sortie textArr repr√©sentera un tableau de cha√Ænes avec le nombre de machines trouv√©es sur le cadre, par exemple: <br>  <code>["293163"]</code> , ou <code>[""]</code> , <code>[]</code> - si aucun num√©ro correspondant n'a √©t√© trouv√©. </p><br><h2 id="etap-iii-opoznaem-obekty-na-shozhest">  √âtape III.  Identifiez les objets par similitude. </h2><br><p>  Maintenant, nous devons comprendre comment r√©parer un objet une fois, pour comprendre que c'est lui dans le cadre suivant.  √Ä ce stade, nous supposerons que nous n'avons qu'une seule cam√©ra et nous ne distinguerons que les diff√©rentes images de celle-ci. </p><br><p>  Pour ce faire, vous devez d√©couvrir comment nous comparerons les deux objets. </p><br><p>  Je proposerai un algorithme de <a href="https://en.wikipedia.org/wiki/Scale-invariant_feature_transform">criblage</a> √† ces fins.  Nous faisons une r√©servation qu'il ne fait pas partie de la partie principale d'OpenCV, nous devons donc fournir des modules contrib en plus.  Malheureusement, l'algorithme est brevet√© et son utilisation dans les programmes commerciaux est limit√©e.  Mais nous nous concentrons sur les activit√©s de recherche, non? </p><br><pre> <code class="plaintext hljs">pip3 install opencv-contrib-python --user</code> </pre> <br><p>  ~~ Surcharge de l'op√©rateur == ~~ Nous √©crivons une fonction qui prend 2 objets compar√©s sous forme de matrices.  Par exemple, nous les obtenons apr√®s avoir appel√© la fonction <code>cv2.open(path)</code> </p><br><p>  Nous allons √©crire une impl√©mentation de notre algorithme. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">compareImages</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(img1, img2)</span></span></span><span class="hljs-function"> -&gt; bool:</span></span> sift = cv2.xfeatures2d.SIFT_create()</code> </pre> <br><p>  Trouvez des points cl√©s et des descripteurs √† l'aide de SIFT.  Je ne fournirai peut-√™tre pas d'aide pour ces fonctions, car vous pouvez toujours l'appeler dans le shell interactif comme <code>help(somefunc)</code> </p><br><pre> <code class="python hljs"> kp1, des1 = sift.detectAndCompute(img1, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>) kp2, des2 = sift.detectAndCompute(img2, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>)</code> </pre> <br><p>  Configurez notre algorithme. </p><br><pre> <code class="python hljs"> FLANN_INDEX_KDTREE = <span class="hljs-number"><span class="hljs-number">0</span></span> indexParams = dict(algorithm=FLANN_INDEX_KDTREE, trees=<span class="hljs-number"><span class="hljs-number">5</span></span>) searchParams = dict(checks=<span class="hljs-number"><span class="hljs-number">50</span></span>) flann = cv2.FlannBasedMatcher(indexParams, searchParams)</code> </pre> <br><p>  Maintenant, lancez-le. </p><br><pre> <code class="python hljs"> matches = flann.knnMatch(des1, des2, k=<span class="hljs-number"><span class="hljs-number">2</span></span>)</code> </pre> <br><p>  Comptez les similitudes entre les images. </p><br><pre> <code class="python hljs"> matchesCount = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> m, n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> matches: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> m.distance &lt; cfg.cencitivity*n.distance: matchesCount += <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> matchesCount &gt; cfg.MIN_MATCH_COUNT</code> </pre> <br><p>  Maintenant, essayez de l'utiliser <br>  Pour ce faire, apr√®s avoir d√©tect√© des objets, nous devons les couper de l'image d'origine </p><br><p>  Je ne pouvais rien √©crire de mieux que de le sauvegarder pour une m√©moire lente, puis de lire √† partir de l√†. </p><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">extractObjects</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(objects, binaryImage, outputImageDirectory, filename=None)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> item <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> objects: y1, x1, y2, x2 = item.coordinates <span class="hljs-comment"><span class="hljs-comment">#       cropped = binaryImage[y1:y2, x1:x2] beforePoint, afterPoint = filename.split(".") outputDirPath = os.path.join(os.path.split(outputImageDirectory)[0], "objectsOn" + beforePoint) if not os.path.exists(outputDirPath): os.mkdir(outputDirPath) coordinates = str(item).replace(" ", ",") pathToObjectImage = "{}{}.jpg".format(item.type, coordinates) cv2.imwrite(os.path.join(outputDirPath, str(pathToObjectImage)), cropped)</span></span></code> </pre> <br><p>  Nous avons maintenant les objets dans le <code>&lt;outputImageDirectory&gt;/objectsOn&lt;imageFilename&gt;</code> </p><br><p>  Maintenant, si nous avons au moins 2 de ces r√©pertoires, nous pouvons comparer les objets qu'ils contiennent.  Ex√©cutez la fonction √©crite pr√©c√©demment </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> compareImages(previousObjects, currentObjects): print(‚Äú  !‚Äù)</code> </pre> <br><p>  Ou nous pouvons faire une autre action, comme marquer ces objets avec le m√™me identifiant. </p><br><p>  Bien s√ªr, comme tous les r√©seaux de neurones, celui-ci a tendance √† donner des r√©sultats parfois erron√©s. </p><br><p>  En g√©n√©ral, nous avons termin√© les 3 t√¢ches d√©finies au d√©but, nous allons donc terminer.  Je doute que cet article ait ouvert les yeux des personnes qui ont √©crit au moins un programme qui r√©sout les probl√®mes de reconnaissance d'image / segmentation d'image, mais j'esp√®re avoir aid√© au moins un d√©veloppeur novice). </p><br><p>  Le code source complet du projet peut √™tre trouv√© <a href="https://github.com/Sapfir0/premier-eye">ici</a> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr483018/">https://habr.com/ru/post/fr483018/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr483004/index.html">Effet Kuleshov dans Disco Elysium: comment le contexte cr√©e du sens</a></li>
<li><a href="../fr483008/index.html">Un autre avenir - une scission de l'humanit√©</a></li>
<li><a href="../fr483012/index.html">Antiquit√©s: Roland MT-32, un son alternatif pour les jeux DOS</a></li>
<li><a href="../fr483014/index.html">Files d'attente de messages PostgreSQL utilisant PgQ</a></li>
<li><a href="../fr483016/index.html">Une br√®ve histoire des microprocesseurs spatiaux, deuxi√®me partie</a></li>
<li><a href="../fr483024/index.html">¬´Qu'est-ce que les entreprises ont fait avec votre vie priv√©e?¬ª, Arthur Khachuyan (Tazeros Global)</a></li>
<li><a href="../fr483026/index.html">Java / Spring: comment g√©n√©rer compl√®tement une API CRUD REST √† l'aide de Speedment</a></li>
<li><a href="../fr483030/index.html">API qui vous fait pleurer</a></li>
<li><a href="../fr483032/index.html">Passer de la CEI √† la R√©publique tch√®que, sa propre exp√©rience (partie 2)</a></li>
<li><a href="../fr483036/index.html">n-Queens Completion Problem - algorithme de solution lin√©aire</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>