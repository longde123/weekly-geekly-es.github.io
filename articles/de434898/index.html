<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ’†ğŸ½ ğŸ˜´ ğŸ‘¨â€ğŸ‘¦â€ğŸ‘¦ Grasp2Vec: Lernen, Objekte durch selbstlernende Erfassung darzustellen ğŸ‘ ğŸ‘¨ğŸ¾â€ğŸš’ ğŸŒ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Menschen ab einem Ã¼berraschend jungen Alter kÃ¶nnen ihre Lieblingsobjekte bereits erkennen und aufheben, obwohl ihnen dies nicht speziell beigebracht w...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Grasp2Vec: Lernen, Objekte durch selbstlernende Erfassung darzustellen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/434898/"><img src="https://habrastorage.org/getpro/habr/post_images/220/c80/5fb/220c805fb8ffb53d2b33413fa2e9eeda.png"><br><br>  Menschen ab einem Ã¼berraschend jungen Alter kÃ¶nnen ihre Lieblingsobjekte bereits erkennen und aufheben, obwohl ihnen dies nicht speziell beigebracht wird.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Studien zur</a> Entwicklung kognitiver FÃ¤higkeiten zufolge spielt die MÃ¶glichkeit der Interaktion mit Objekten der Welt um uns herum eine entscheidende Rolle bei der Entwicklung von FÃ¤higkeiten wie dem Erfassen und Manipulieren von Objekten - beispielsweise der gezielten Erfassung.  Im Umgang mit der AuÃŸenwelt kÃ¶nnen Menschen lernen, indem sie ihre eigenen Fehler korrigieren: Wir wissen, was wir getan haben, und lernen aus den Ergebnissen.  In der Robotik wird diese Art des Trainings mit Selbstkorrektur von Fehlern aktiv untersucht, da Robotersysteme ohne groÃŸe Menge an Trainingsdaten oder manueller Anpassung lernen kÃ¶nnen. <br><br>  Wir bei Google bieten, inspiriert vom <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Konzept der Persistenz von Objekten</a> , das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grasp2Vec-</a> System an - einen einfachen, aber effektiven Algorithmus zum <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erstellen</a> der Darstellung von Objekten.  Grasp2Vec basiert auf einem intuitiven VerstÃ¤ndnis, dass ein Versuch, ein Objekt anzuheben, einige Informationen liefert. Wenn der Roboter das Objekt ergreift und es aufnimmt, muss sich das Objekt an dieser Stelle befinden, bevor es erfasst wird.  DarÃ¼ber hinaus weiÃŸ der Roboter, dass sich das erfasste Objekt nicht mehr an dem Ort befindet, an dem es sich befand, wenn es erfasst wird.  Mit dieser Form des Selbstlernens kann der Roboter lernen, ein Objekt aufgrund der visuellen VerÃ¤nderung in der Szene zu erkennen, nachdem es erfasst wurde. <br><a name="habracut"></a><br>  Basierend auf unserer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zusammenarbeit mit X Robotics</a> , bei der mehrere Roboter gleichzeitig geschult wurden, um Haushaltsobjekte mit nur einer Kamera als Eingabedatenquelle zu erfassen, verwenden wir die Robotererfassung, um Objekte â€versehentlichâ€œ zu erfassen. Diese Erfahrung ermÃ¶glicht es uns, eine umfassende Vorstellung vom Objekt zu erhalten.  Diese Idee kann bereits verwendet werden, um die FÃ¤higkeit der "absichtlichen Erfassung" zu erwerben, wenn der Roboterarm Objekte nach Bedarf anheben kann. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/QzlI_ny4l8s" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h2>  Erstellen einer Wahrnehmungsbelohnungsfunktion </h2><br>  Auf einer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">VerstÃ¤rkungslernplattform wird der</a> Erfolg einer Aufgabe durch eine Belohnungsfunktion gemessen.  Durch die Maximierung der Belohnungen lernen Roboter verschiedene ErfassungsfÃ¤higkeiten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">von Grund auf neu</a> .  Das Erstellen einer Belohnungsfunktion ist einfach, wenn der Erfolg mit einfachen Sensorablesungen gemessen werden kann.  Ein einfaches Beispiel ist eine SchaltflÃ¤che, die eine Belohnung <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">direkt auf die Eingabe eines Roboters</a> Ã¼bertrÃ¤gt, indem Sie darauf klicken. <br><br>  Das Erstellen einer Belohnungsfunktion ist jedoch viel komplizierter, wenn das Erfolgskriterium von einem WahrnehmungsverstÃ¤ndnis der Aufgabe abhÃ¤ngt.  Betrachten Sie das Erfassungsproblem in einem Beispiel, in dem der Roboter ein Bild des gewÃ¼nschten Objekts erhÃ¤lt, das in der Erfassung enthalten ist.  Nachdem der Roboter versucht hat, das Objekt zu erfassen, untersucht er den Inhalt der Erfassung.  Die Belohnungsfunktion fÃ¼r diese Aufgabe hÃ¤ngt von der Antwort auf die Frage der Mustererkennung ab: Stimmen die Objekte Ã¼berein? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f23/a41/c24/f23a41c24b4cc60b062d055bbb5b9347.png"><br>  <i>Links hÃ¤lt der Griff den Pinsel und im Hintergrund sind mehrere Objekte sichtbar (eine gelbe Tasse, ein blauer Plastikblock).</i>  <i>Rechts hÃ¤lt der Griff die Tasse und der Pinsel befindet sich im Hintergrund.</i>  <i>Wenn das linke Bild das gewÃ¼nschte Ergebnis darstellt, besteht eine gute Belohnungsfunktion darin, zu â€verstehenâ€œ, dass diese beiden Fotos zwei verschiedenen Objekten entsprechen.</i> <br><br>  Um das Erkennungsproblem zu lÃ¶sen, benÃ¶tigen wir ein Wahrnehmungssystem, das aussagekrÃ¤ftige Konzepte von Objekten aus unstrukturierten Bildern (nicht von Personen signiert) extrahiert und lernt, Objekte ohne Lehrer zu visualisieren.  Im Wesentlichen funktionieren lehrerlose Lernalgorithmen, indem sie strukturelle Annahmen Ã¼ber Daten erstellen.  Es wird hÃ¤ufig angenommen, dass Bilder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">auf einen Raum mit weniger Abmessungen komprimiert</a> werden kÃ¶nnen und Videobilder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">aus frÃ¼heren vorhergesagt werden kÃ¶nnen</a> .  Ohne zusÃ¤tzliche Annahmen Ã¼ber den Inhalt der Daten reicht dies jedoch normalerweise nicht aus, um aus nicht verwandten Darstellungen von Objekten zu lernen. <br><br>  Was wÃ¤re, wenn wir einen Roboter verwenden wÃ¼rden, um Objekte wÃ¤hrend der Datenerfassung physisch zu trennen?  Die Robotik bietet eine hervorragende Gelegenheit, das Darstellen von Objekten zu lernen, da Roboter sie manipulieren kÃ¶nnen, wodurch die erforderlichen Variationsfaktoren erhalten werden.  Unsere Methode basiert auf der Idee, dass das Erfassen eines Objekts es aus der Szene entfernt.  Das Ergebnis ist 1) ein Bild der Szene vor der Aufnahme, 2) ein Bild der Szene nach der Aufnahme und 3) eine separate Ansicht des aufgenommenen Objekts. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8da/43f/0d7/8da43f0d74077b6c65a80026ce7041f0.png"><br>  <i>Links - Objekte, die erfasst werden sollen.</i>  <i>In der Mitte - nach der Aufnahme.</i>  <i>Rechts ist das erfasste Objekt.</i> <br><br>  Wenn wir eine integrierte Funktion betrachten, die eine â€Menge von Objektenâ€œ aus Bildern extrahiert, sollte die folgende Subtraktionsbeziehung beibehalten werden: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1c8/aa9/723/1c8aa97238f20d832c90f02335c0367c.png"><br>  <i>Objekte vor der Erfassung - Objekte nach der Erfassung = erfasstes Objekt</i> <br><br>  Diese Gleichheit erreichen wir mit einer Faltungsarchitektur und einem einfachen metrischen Lernalgorithmus.  WÃ¤hrend des Trainings bettet die unten gezeigte Architektur Bilder vor und nach der Erfassung in eine dichte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Karte mit rÃ¤umlichen Eigenschaften ein</a> .  Diese Karten werden durch eine gemittelte Vereinigung zu Vektoren, und der Unterschied zwischen den Vektoren "vor der Erfassung" und "nach der Erfassung" reprÃ¤sentiert eine Reihe von Objekten.  Dieser Vektor und die entsprechende Darstellung des Vektors dieses wahrgenommenen Objekts werden durch die Funktion von N-Paaren gleichgesetzt. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/32f/e51/c0d/32fe51c0d4915374be646fc0bb2ba76c.png"><br><br>  Nach dem Training hat unser Modell natÃ¼rlich zwei nÃ¼tzliche Eigenschaften. <br><br><h2>  1. Ã„hnlichkeit von Objekten </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Der Kosinuskoeffizient des</a> Abstands zwischen den Vektoreinbettungen ermÃ¶glicht es uns, Objekte zu vergleichen und festzustellen, ob sie identisch sind.  Dies kann verwendet werden, um die Belohnungsfunktion fÃ¼r verstÃ¤rktes Lernen zu implementieren, und ermÃ¶glicht es Robotern zu lernen, wie man mit Beispielen erfasst, ohne Daten von Menschen zu markieren. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c47/fb7/dd4/c47fb7dd4c79a4e16564f9d6ab669f5e.png"><br><br><h2>  2. Ziele finden </h2><br>  Wir kÃ¶nnen rÃ¤umliche Karten der Szene und die Einbettung von Objekten kombinieren, um das â€gewÃ¼nschte Objektâ€œ im Bildraum zu lokalisieren.  Durch elementweise Multiplikation von rÃ¤umlichen Merkmalskarten und Vektorkorrespondenz des gewÃ¼nschten Objekts kÃ¶nnen wir alle Pixel auf der rÃ¤umlichen Karte finden, die dem Zielobjekt entsprechen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c42/271/47b/c4227147baeb69c052549544b9065961.png"><br>  <i>Verwenden von Grasp2Vec-Inlays zum Lokalisieren von Objekten in der Szene.</i>  <i>Oben links befinden sich Objekte im Korb.</i>  <i>Unten links - das gewÃ¼nschte Objekt, das erfasst werden soll.</i>  <i>Das Skalarprodukt des Vektors des Zielobjekts und die rÃ¤umlichen Merkmale des Bildes geben uns eine â€Aktivierungskarteâ€œ pro Pixel (oben rechts) der Ã„hnlichkeit eines bestimmten Bildabschnitts mit dem Ziel.</i>  <i>Diese Karte kann verwendet werden, um nÃ¤her an das Ziel heranzukommen.</i> <br><br>  Unsere Methode funktioniert auch, wenn mehrere Objekte dem Ziel entsprechen oder wenn das Ziel aus mehreren Objekten besteht (der Durchschnitt von zwei Vektoren).  In diesem Szenario identifiziert der Roboter beispielsweise mehrere orangefarbene BlÃ¶cke in der Szene. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4f3/f9e/b15/4f3f9eb159738345f5a8da4c1dfb83fc.png"><br>  <i>Die resultierende â€WÃ¤rmekarteâ€œ kann verwendet werden, um die AnnÃ¤herung des Roboters an das / die Zielobjekt (e) zu planen.</i>  <i>Wir kombinieren die Lokalisierung von Grasp2Vec und die Mustererkennung mit unserer Richtlinie "Alles erfassen" und erzielen in 80% der FÃ¤lle Erfolg bei der Datenerfassung und in 59% bei neuen Objekten, auf die der Roboter zuvor noch nicht gestoÃŸen ist.</i> <br><br><h2>  Fazit </h2><br>  In unserer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Arbeit haben</a> wir gezeigt, wie Roboter-GreiferfÃ¤higkeiten Daten erstellen kÃ¶nnen, die zum Unterrichten von Objektdarstellungen verwendet werden.  AnschlieÃŸend kÃ¶nnen wir mithilfe von PrÃ¤sentationstrainings schnell komplexere FÃ¤higkeiten erwerben, z. B. das Erfassen anhand eines Beispiels, und gleichzeitig alle Eigenschaften des Unterrichts ohne Lehrer in unserem autonomen Erfassungssystem beibehalten. <br><br>  ZusÃ¤tzlich zu unserer Arbeit haben mehrere andere neuere Arbeiten auch untersucht, wie Interaktion ohne Lehrer verwendet werden kann, um Darstellungen von Objekten zu erhalten, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">indem</a> Objekte in der Umgebung <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erfasst</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">verschoben</a> und auf andere Weise <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">interagiert werden</a> .  Wir erwarten freudig nicht nur, was maschinelles Lernen Robotik in Bezug auf bessere Wahrnehmung und Kontrolle geben kann, sondern auch, was Robotik maschinelles Lernen in Bezug auf neue selbstlernende Paradigmen geben kann. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de434898/">https://habr.com/ru/post/de434898/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de434888/index.html">Neujahrsgeschenk von Binary District</a></li>
<li><a href="../de434890/index.html">Die Qiwi Bank (JSC) weist den Benutzern Geld zu</a></li>
<li><a href="../de434892/index.html">Zeichnungscode in Swift, PaintCode</a></li>
<li><a href="../de434894/index.html">Die Kunst des Schamanismus oder der benutzerdefinierten Firmware fÃ¼r Olinuxino. Teil 1</a></li>
<li><a href="../de434896/index.html">Unterhaltungselektronik Hall of Fame: Die Geschichten der besten GerÃ¤te der letzten 50 Jahre, Teil 1</a></li>
<li><a href="../de434902/index.html">Erstellen eines benutzerdefinierten Abfragegenerators in Spring Data Neo4j (Teil 1)</a></li>
<li><a href="../de434906/index.html">Tests in C ++ ohne Makros und dynamischen Speicher</a></li>
<li><a href="../de434908/index.html">Programmiererausbildung - Was? Wo? Wann?</a></li>
<li><a href="../de434912/index.html">Der jÃ¤hrliche Bestand von Porsche Taycan ist bereits reserviert, hauptsÃ¤chlich von Tesla-Besitzern</a></li>
<li><a href="../de434924/index.html">Was Sie Ã¼ber die Organisation von ArbeitsplÃ¤tzen, Coworking und die Gestaltung von RÃ¤umen fÃ¼r Fernarbeit lesen sollten</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>