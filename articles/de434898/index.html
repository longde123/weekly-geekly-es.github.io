<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>💆🏽 😴 👨‍👦‍👦 Grasp2Vec: Lernen, Objekte durch selbstlernende Erfassung darzustellen 👏 👨🏾‍🚒 🐌</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Menschen ab einem überraschend jungen Alter können ihre Lieblingsobjekte bereits erkennen und aufheben, obwohl ihnen dies nicht speziell beigebracht w...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Grasp2Vec: Lernen, Objekte durch selbstlernende Erfassung darzustellen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/434898/"><img src="https://habrastorage.org/getpro/habr/post_images/220/c80/5fb/220c805fb8ffb53d2b33413fa2e9eeda.png"><br><br>  Menschen ab einem überraschend jungen Alter können ihre Lieblingsobjekte bereits erkennen und aufheben, obwohl ihnen dies nicht speziell beigebracht wird.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Studien zur</a> Entwicklung kognitiver Fähigkeiten zufolge spielt die Möglichkeit der Interaktion mit Objekten der Welt um uns herum eine entscheidende Rolle bei der Entwicklung von Fähigkeiten wie dem Erfassen und Manipulieren von Objekten - beispielsweise der gezielten Erfassung.  Im Umgang mit der Außenwelt können Menschen lernen, indem sie ihre eigenen Fehler korrigieren: Wir wissen, was wir getan haben, und lernen aus den Ergebnissen.  In der Robotik wird diese Art des Trainings mit Selbstkorrektur von Fehlern aktiv untersucht, da Robotersysteme ohne große Menge an Trainingsdaten oder manueller Anpassung lernen können. <br><br>  Wir bei Google bieten, inspiriert vom <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Konzept der Persistenz von Objekten</a> , das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grasp2Vec-</a> System an - einen einfachen, aber effektiven Algorithmus zum <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erstellen</a> der Darstellung von Objekten.  Grasp2Vec basiert auf einem intuitiven Verständnis, dass ein Versuch, ein Objekt anzuheben, einige Informationen liefert. Wenn der Roboter das Objekt ergreift und es aufnimmt, muss sich das Objekt an dieser Stelle befinden, bevor es erfasst wird.  Darüber hinaus weiß der Roboter, dass sich das erfasste Objekt nicht mehr an dem Ort befindet, an dem es sich befand, wenn es erfasst wird.  Mit dieser Form des Selbstlernens kann der Roboter lernen, ein Objekt aufgrund der visuellen Veränderung in der Szene zu erkennen, nachdem es erfasst wurde. <br><a name="habracut"></a><br>  Basierend auf unserer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zusammenarbeit mit X Robotics</a> , bei der mehrere Roboter gleichzeitig geschult wurden, um Haushaltsobjekte mit nur einer Kamera als Eingabedatenquelle zu erfassen, verwenden wir die Robotererfassung, um Objekte „versehentlich“ zu erfassen. Diese Erfahrung ermöglicht es uns, eine umfassende Vorstellung vom Objekt zu erhalten.  Diese Idee kann bereits verwendet werden, um die Fähigkeit der "absichtlichen Erfassung" zu erwerben, wenn der Roboterarm Objekte nach Bedarf anheben kann. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/QzlI_ny4l8s" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h2>  Erstellen einer Wahrnehmungsbelohnungsfunktion </h2><br>  Auf einer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verstärkungslernplattform wird der</a> Erfolg einer Aufgabe durch eine Belohnungsfunktion gemessen.  Durch die Maximierung der Belohnungen lernen Roboter verschiedene Erfassungsfähigkeiten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">von Grund auf neu</a> .  Das Erstellen einer Belohnungsfunktion ist einfach, wenn der Erfolg mit einfachen Sensorablesungen gemessen werden kann.  Ein einfaches Beispiel ist eine Schaltfläche, die eine Belohnung <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">direkt auf die Eingabe eines Roboters</a> überträgt, indem Sie darauf klicken. <br><br>  Das Erstellen einer Belohnungsfunktion ist jedoch viel komplizierter, wenn das Erfolgskriterium von einem Wahrnehmungsverständnis der Aufgabe abhängt.  Betrachten Sie das Erfassungsproblem in einem Beispiel, in dem der Roboter ein Bild des gewünschten Objekts erhält, das in der Erfassung enthalten ist.  Nachdem der Roboter versucht hat, das Objekt zu erfassen, untersucht er den Inhalt der Erfassung.  Die Belohnungsfunktion für diese Aufgabe hängt von der Antwort auf die Frage der Mustererkennung ab: Stimmen die Objekte überein? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f23/a41/c24/f23a41c24b4cc60b062d055bbb5b9347.png"><br>  <i>Links hält der Griff den Pinsel und im Hintergrund sind mehrere Objekte sichtbar (eine gelbe Tasse, ein blauer Plastikblock).</i>  <i>Rechts hält der Griff die Tasse und der Pinsel befindet sich im Hintergrund.</i>  <i>Wenn das linke Bild das gewünschte Ergebnis darstellt, besteht eine gute Belohnungsfunktion darin, zu „verstehen“, dass diese beiden Fotos zwei verschiedenen Objekten entsprechen.</i> <br><br>  Um das Erkennungsproblem zu lösen, benötigen wir ein Wahrnehmungssystem, das aussagekräftige Konzepte von Objekten aus unstrukturierten Bildern (nicht von Personen signiert) extrahiert und lernt, Objekte ohne Lehrer zu visualisieren.  Im Wesentlichen funktionieren lehrerlose Lernalgorithmen, indem sie strukturelle Annahmen über Daten erstellen.  Es wird häufig angenommen, dass Bilder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">auf einen Raum mit weniger Abmessungen komprimiert</a> werden können und Videobilder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">aus früheren vorhergesagt werden können</a> .  Ohne zusätzliche Annahmen über den Inhalt der Daten reicht dies jedoch normalerweise nicht aus, um aus nicht verwandten Darstellungen von Objekten zu lernen. <br><br>  Was wäre, wenn wir einen Roboter verwenden würden, um Objekte während der Datenerfassung physisch zu trennen?  Die Robotik bietet eine hervorragende Gelegenheit, das Darstellen von Objekten zu lernen, da Roboter sie manipulieren können, wodurch die erforderlichen Variationsfaktoren erhalten werden.  Unsere Methode basiert auf der Idee, dass das Erfassen eines Objekts es aus der Szene entfernt.  Das Ergebnis ist 1) ein Bild der Szene vor der Aufnahme, 2) ein Bild der Szene nach der Aufnahme und 3) eine separate Ansicht des aufgenommenen Objekts. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8da/43f/0d7/8da43f0d74077b6c65a80026ce7041f0.png"><br>  <i>Links - Objekte, die erfasst werden sollen.</i>  <i>In der Mitte - nach der Aufnahme.</i>  <i>Rechts ist das erfasste Objekt.</i> <br><br>  Wenn wir eine integrierte Funktion betrachten, die eine „Menge von Objekten“ aus Bildern extrahiert, sollte die folgende Subtraktionsbeziehung beibehalten werden: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1c8/aa9/723/1c8aa97238f20d832c90f02335c0367c.png"><br>  <i>Objekte vor der Erfassung - Objekte nach der Erfassung = erfasstes Objekt</i> <br><br>  Diese Gleichheit erreichen wir mit einer Faltungsarchitektur und einem einfachen metrischen Lernalgorithmus.  Während des Trainings bettet die unten gezeigte Architektur Bilder vor und nach der Erfassung in eine dichte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Karte mit räumlichen Eigenschaften ein</a> .  Diese Karten werden durch eine gemittelte Vereinigung zu Vektoren, und der Unterschied zwischen den Vektoren "vor der Erfassung" und "nach der Erfassung" repräsentiert eine Reihe von Objekten.  Dieser Vektor und die entsprechende Darstellung des Vektors dieses wahrgenommenen Objekts werden durch die Funktion von N-Paaren gleichgesetzt. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/32f/e51/c0d/32fe51c0d4915374be646fc0bb2ba76c.png"><br><br>  Nach dem Training hat unser Modell natürlich zwei nützliche Eigenschaften. <br><br><h2>  1. Ähnlichkeit von Objekten </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Der Kosinuskoeffizient des</a> Abstands zwischen den Vektoreinbettungen ermöglicht es uns, Objekte zu vergleichen und festzustellen, ob sie identisch sind.  Dies kann verwendet werden, um die Belohnungsfunktion für verstärktes Lernen zu implementieren, und ermöglicht es Robotern zu lernen, wie man mit Beispielen erfasst, ohne Daten von Menschen zu markieren. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c47/fb7/dd4/c47fb7dd4c79a4e16564f9d6ab669f5e.png"><br><br><h2>  2. Ziele finden </h2><br>  Wir können räumliche Karten der Szene und die Einbettung von Objekten kombinieren, um das „gewünschte Objekt“ im Bildraum zu lokalisieren.  Durch elementweise Multiplikation von räumlichen Merkmalskarten und Vektorkorrespondenz des gewünschten Objekts können wir alle Pixel auf der räumlichen Karte finden, die dem Zielobjekt entsprechen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c42/271/47b/c4227147baeb69c052549544b9065961.png"><br>  <i>Verwenden von Grasp2Vec-Inlays zum Lokalisieren von Objekten in der Szene.</i>  <i>Oben links befinden sich Objekte im Korb.</i>  <i>Unten links - das gewünschte Objekt, das erfasst werden soll.</i>  <i>Das Skalarprodukt des Vektors des Zielobjekts und die räumlichen Merkmale des Bildes geben uns eine „Aktivierungskarte“ pro Pixel (oben rechts) der Ähnlichkeit eines bestimmten Bildabschnitts mit dem Ziel.</i>  <i>Diese Karte kann verwendet werden, um näher an das Ziel heranzukommen.</i> <br><br>  Unsere Methode funktioniert auch, wenn mehrere Objekte dem Ziel entsprechen oder wenn das Ziel aus mehreren Objekten besteht (der Durchschnitt von zwei Vektoren).  In diesem Szenario identifiziert der Roboter beispielsweise mehrere orangefarbene Blöcke in der Szene. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4f3/f9e/b15/4f3f9eb159738345f5a8da4c1dfb83fc.png"><br>  <i>Die resultierende „Wärmekarte“ kann verwendet werden, um die Annäherung des Roboters an das / die Zielobjekt (e) zu planen.</i>  <i>Wir kombinieren die Lokalisierung von Grasp2Vec und die Mustererkennung mit unserer Richtlinie "Alles erfassen" und erzielen in 80% der Fälle Erfolg bei der Datenerfassung und in 59% bei neuen Objekten, auf die der Roboter zuvor noch nicht gestoßen ist.</i> <br><br><h2>  Fazit </h2><br>  In unserer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Arbeit haben</a> wir gezeigt, wie Roboter-Greiferfähigkeiten Daten erstellen können, die zum Unterrichten von Objektdarstellungen verwendet werden.  Anschließend können wir mithilfe von Präsentationstrainings schnell komplexere Fähigkeiten erwerben, z. B. das Erfassen anhand eines Beispiels, und gleichzeitig alle Eigenschaften des Unterrichts ohne Lehrer in unserem autonomen Erfassungssystem beibehalten. <br><br>  Zusätzlich zu unserer Arbeit haben mehrere andere neuere Arbeiten auch untersucht, wie Interaktion ohne Lehrer verwendet werden kann, um Darstellungen von Objekten zu erhalten, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">indem</a> Objekte in der Umgebung <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erfasst</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">verschoben</a> und auf andere Weise <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">interagiert werden</a> .  Wir erwarten freudig nicht nur, was maschinelles Lernen Robotik in Bezug auf bessere Wahrnehmung und Kontrolle geben kann, sondern auch, was Robotik maschinelles Lernen in Bezug auf neue selbstlernende Paradigmen geben kann. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de434898/">https://habr.com/ru/post/de434898/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de434888/index.html">Neujahrsgeschenk von Binary District</a></li>
<li><a href="../de434890/index.html">Die Qiwi Bank (JSC) weist den Benutzern Geld zu</a></li>
<li><a href="../de434892/index.html">Zeichnungscode in Swift, PaintCode</a></li>
<li><a href="../de434894/index.html">Die Kunst des Schamanismus oder der benutzerdefinierten Firmware für Olinuxino. Teil 1</a></li>
<li><a href="../de434896/index.html">Unterhaltungselektronik Hall of Fame: Die Geschichten der besten Geräte der letzten 50 Jahre, Teil 1</a></li>
<li><a href="../de434902/index.html">Erstellen eines benutzerdefinierten Abfragegenerators in Spring Data Neo4j (Teil 1)</a></li>
<li><a href="../de434906/index.html">Tests in C ++ ohne Makros und dynamischen Speicher</a></li>
<li><a href="../de434908/index.html">Programmiererausbildung - Was? Wo? Wann?</a></li>
<li><a href="../de434912/index.html">Der jährliche Bestand von Porsche Taycan ist bereits reserviert, hauptsächlich von Tesla-Besitzern</a></li>
<li><a href="../de434924/index.html">Was Sie über die Organisation von Arbeitsplätzen, Coworking und die Gestaltung von Räumen für Fernarbeit lesen sollten</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>