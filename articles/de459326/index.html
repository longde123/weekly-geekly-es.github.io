<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🕯️ 👩🏿‍🎤 🙍 Automatische Skalierung und Ressourcenverwaltung in Kubernetes (Überprüfung und Videobericht) 👏🏽 🍛 👨🏻‍⚖️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Am 27. April wurde auf der Strike-2019- Konferenz im Rahmen der DevOps-Sektion ein Bericht zum Thema „Automatische Skalierung und Ressourcenverwaltung...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Automatische Skalierung und Ressourcenverwaltung in Kubernetes (Überprüfung und Videobericht)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/459326/">  Am 27. April wurde auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Strike-2019-</a> Konferenz im Rahmen der DevOps-Sektion ein Bericht zum Thema „Automatische Skalierung und Ressourcenverwaltung in Kubernetes“ erstellt.  Es wird erläutert, wie K8s verwendet werden, um eine hohe Verfügbarkeit von Anwendungen und deren maximale Leistung sicherzustellen. <br><br><img src="https://habrastorage.org/webt/ol/sv/vf/olsvvfwmfrorzavctm_ipdufuxo.jpeg"><br><br>  Aus Tradition freuen wir uns, ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><b>Video mit einem Bericht</b></a> (44 Minuten, viel informativer als der Artikel) und dem Hauptdruck in Textform zu präsentieren.  Lass uns gehen! <a name="habracut"></a><br><br>  Wir werden das Thema des Berichts anhand der Wörter analysieren und am Ende beginnen. <br><br><h2>  Kubernetes </h2><br>  Lassen Sie uns Docker-Container auf dem Host haben.  Warum?  Um Wiederholbarkeit und Isolation zu gewährleisten, was wiederum eine einfache und gute Bereitstellung ermöglicht, CI / CD.  Wir haben viele solcher Maschinen mit Containern. <br><br>  Was gibt Kubernetes in diesem Fall? <br><br><ol><li>  Wir hören auf, an diese Maschinen zu denken, und beginnen mit der „Cloud“ zu arbeiten, einer <b>Gruppe von Containern</b> oder Pods (Gruppen von Containern). </li><li>  Darüber hinaus denken wir nicht einmal an einzelne Pods, sondern verwalten größere Gruppen.  Mit solchen <b>übergeordneten Grundelementen</b> können wir sagen, dass es eine Vorlage zum Starten einer bestimmten Arbeitslast gibt, jedoch die erforderliche Anzahl von Instanzen für deren Start.  Wenn wir die Vorlage anschließend ändern, ändern sich auch alle Instanzen. </li><li>  Mithilfe der <b>deklarativen API</b> beschreiben wir das von Kubernetes erstellte „Weltgerät“ (in YAML) <b>,</b> anstatt eine Folge bestimmter Befehle auszuführen.  Und noch einmal: Wenn sich die Beschreibung ändert, ändert sich auch die tatsächliche Anzeige. </li></ol><br><h2>  Ressourcenmanagement </h2><br><h3>  CPU </h3><br>  Lassen Sie uns nginx, php-fpm und mysql auf dem Server ausführen.  Diese Dienste werden sogar noch mehr laufende Prozesse haben, für die jeweils Rechenressourcen erforderlich sind: <br><br><img src="https://habrastorage.org/webt/yu/v6/t8/yuv6t8a5q6txbhi25fe5mwv8f4k.png"><br>  <i>(Die Zahlen auf der Folie sind "Papageien", das abstrakte Bedürfnis jedes Prozesses nach Rechenleistung)</i> <br><br>  Um die Arbeit damit zu vereinfachen, ist es logisch, Prozesse zu Gruppen zusammenzufassen (z. B. alle Nginx-Prozesse zu einer „Nginx“ -Gruppe).  Eine einfache und offensichtliche Möglichkeit, dies zu tun, besteht darin, jede Gruppe in einen Container zu legen: <br><br><img src="https://habrastorage.org/webt/yu/en/fr/yuenfrw6vzvexx1xrkvy3dfgw3o.png"><br><br>  Um fortzufahren, müssen Sie sich merken, was ein Container ist (unter Linux).  Ihr Erscheinungsbild wurde durch drei wichtige Funktionen im Kernel ermöglicht, die seit langem implementiert sind: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Funktionen</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Namespaces</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">cgroups</a> .  Andere Technologien (einschließlich praktischer "Shells" wie Docker) trugen zur Weiterentwicklung bei: <br><br><img src="https://habrastorage.org/webt/f-/nf/ua/f-nfuaos1_9xdwyblt2em4yp-gs.png"><br><br>  Im Kontext des Berichts sind wir nur an <b>cgroups</b> interessiert, da Kontrollgruppen Teil der Funktionalität von Containern (Docker usw.) sind, die das Ressourcenmanagement implementieren.  Die Prozesse, die, wie wir wollten, in Gruppen zusammengefasst sind, sind die Kontrollgruppen. <br><br>  Kehren wir zu den CPU-Anforderungen für diese Prozesse und jetzt für die Prozessgruppen zurück: <br><br><img src="https://habrastorage.org/webt/_s/cl/7v/_scl7v6nsak1ieo-dipaz9sgb_a.png"><br>  <i>(Ich wiederhole, dass alle Zahlen ein abstrakter Ausdruck der Ressourcenanforderungen sind.)</i> <br><br>  Gleichzeitig verfügt die CPU selbst über eine bestimmte endgültige Ressource <i>(im Beispiel 1000)</i> , die möglicherweise nicht für alle ausreicht (die Summe der Anforderungen aller Gruppen beträgt 150 + 850 + 460 = 1460).  Was wird in diesem Fall passieren? <br><br>  Der Kernel beginnt, Ressourcen zu verteilen und tut dies „ehrlich“, indem er jeder Gruppe die gleiche Menge an Ressourcen zur Verfügung stellt.  Im ersten Fall gibt es jedoch mehr als nötig (333&gt; 150), sodass der Überschuss (333-150 = 183) in der Reserve verbleibt, die ebenfalls gleichmäßig auf zwei andere Container verteilt ist: <br><br><img src="https://habrastorage.org/webt/nm/qv/te/nmqvteopou65lsc_ku2b0-qmbco.gif"><br><br>  Als Ergebnis: Der erste Container hatte genug Ressourcen, der zweite - war nicht genug, der dritte - war nicht genug.  Dies ist das Ergebnis des <b>"ehrlichen" Schedulers in Linux</b> - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CFS</a> .  Seine Arbeit kann reguliert werden, indem jedem Behälter ein <b>Gewicht</b> zugewiesen wird.  Zum Beispiel so: <br><br><img src="https://habrastorage.org/webt/z1/c_/_b/z1c__bumb8hy1k5zdrkcwo_aak0.gif"><br><br>  Schauen wir uns den Fall eines Ressourcenmangels im zweiten Container (php-fpm) an.  Alle Containerressourcen werden gleichmäßig auf die Prozesse verteilt.  Infolgedessen funktioniert der Master-Prozess gut, und alle Mitarbeiter werden langsamer, da sie weniger als die Hälfte des benötigten Bedarfs erhalten haben: <br><br><img src="https://habrastorage.org/webt/9z/wi/d2/9zwid2g1znmdoslkl9bf9cogrnq.gif"><br><br>  So funktioniert der CFS-Scheduler.  Die Gewichte, die wir den Containern zuweisen, werden in Zukunft als <b>Anfragen bezeichnet</b> .  Warum so - siehe unten. <br><br>  Schauen wir uns die ganze Situation von der anderen Seite an.  Wie Sie wissen, führen alle Wege nach Rom und im Falle eines Computers zur CPU.  Eine CPU, viele Aufgaben - Sie brauchen eine Ampel.  Der einfachste Weg, Ressourcen zu verwalten, ist die Ampel: Sie geben einem Prozess eine feste Zugriffszeit auf die CPU, dann dem nächsten usw. <br><br><img src="https://habrastorage.org/webt/vf/af/qe/vfafqespiii6mnts87i4amfdyku.gif"><br><br>  Dieser Ansatz wird als <i>harte Begrenzung bezeichnet</i> .  Denken Sie daran, nur als <b>Grenzen</b> .  Wenn Sie jedoch Grenzwerte auf alle Container verteilen, tritt ein Problem auf: MySQL fuhr die Straße entlang und irgendwann endete sein Bedarf an einer CPU, aber alle anderen Prozesse mussten warten, während die CPU im <b>Leerlauf war</b> . <br><br><img src="https://habrastorage.org/webt/7o/1m/ps/7o1mps5khnoqkfywizrusxtug1q.png"><br><br>  Kehren wir zum Linux-Kernel und seiner Interaktion mit der CPU zurück - das Gesamtbild sieht wie folgt aus: <br><br><img src="https://habrastorage.org/webt/m7/xz/x8/m7xzx8brcdgbihu8qchb63rwjwc.png"><br><br>  Cgroup verfügt über zwei Einstellungen. Dies sind zwei einfache „Wendungen“, mit denen Sie Folgendes bestimmen können: <br><br><ol><li>  Gewicht für den Container (Anfrage) ist <b>Aktien</b> ; </li><li>  Ein Prozentsatz der gesamten CPU-Zeit für die Bearbeitung von Containeraufgaben (Limits) ist das <b>Kontingent</b> . </li></ol><br><h3>  Wie messe ich die CPU? </h3><br>  Es gibt verschiedene Möglichkeiten: <br><br><ol><li>  Was <i>Papageien sind</i> , weiß niemand - jedes Mal, wenn Sie zustimmen müssen. </li><li>  <i>Das Interesse ist</i> klarer, aber relativ: 50% eines Servers mit 4 Kernen und 20 Kernen sind völlig verschiedene Dinge. </li><li>  Sie können die bereits erwähnten <i>Gewichte verwenden</i> , die Linux kennt, aber sie sind auch relativ. </li><li>  Die am besten geeignete Option besteht darin, die Rechenressourcen in <i>Sekunden</i> zu messen.  Das heißt,  in Sekunden Prozessorzeit im Verhältnis zu Sekunden Echtzeit: Sie gaben 1 Sekunde Prozessorzeit in 1 realen Sekunde aus - dies ist ein ganzer CPU-Kern. </li></ol><br>  Um es noch einfacher zu sagen, begannen sie direkt in den <i>Kernen</i> zu messen, dh die CPU-Zeit relativ zur realen.  Da Linux eher Gewichte als Prozessorzeit / -kerne versteht, war ein Übersetzungsmechanismus von einem zum anderen erforderlich. <br><br>  Stellen Sie sich ein einfaches Beispiel mit einem Server mit 3 CPU-Kernen vor, bei dem drei Pods Gewichte (500, 1000 und 1500) auswählen, die leicht in die entsprechenden Teile der ihnen zugewiesenen Kerne (0,5, 1 und 1,5) konvertiert werden können. <br><br><img src="https://habrastorage.org/webt/mz/vl/1x/mzvl1xmzbtvlwgsqtf1-_n_rhns.png"><br><br>  Wenn Sie einen zweiten Server mit doppelt so vielen Kernen (6) verwenden und dort dieselben Pods platzieren, kann die Verteilung der Kerne einfach durch einfaches Multiplizieren mit 2 (1, 2 bzw. 3) berechnet werden.  Der wichtige Punkt tritt jedoch auf, wenn der vierte Pod auf diesem Server angezeigt wird, dessen Gewicht der Einfachheit halber 3000 beträgt. Dadurch werden einige CPU-Ressourcen (die Hälfte der Kerne) weggenommen, und der Rest der Pods gibt sie wieder (Hälfte): <br><br><img src="https://habrastorage.org/webt/p3/1t/nd/p31tndrejrchgwgbnpk53q5qnig.gif"><br><br><h3>  Kubernetes und CPU-Ressourcen </h3><br>  In Kubernetes werden CPU-Ressourcen normalerweise in <b>Millikernen</b> gemessen, d.h.  Als Grundgewicht werden 0,001 Kerne verwendet.  <i>(Dasselbe gilt in der Linux / cgroups-Terminologie als CPU-Freigabe, genauer gesagt 1000 CPU = 1024 CPU-Freigaben.)</i> K8s stellt sicher, dass nicht mehr Pods auf dem Server platziert werden, als CPU-Ressourcen für die Summe der Gewichte vorhanden sind alle Hülsen. <br><br>  Wie läuft das  Wenn ein Server zu einem Kubernetes-Cluster hinzugefügt wird, wird gemeldet, wie viele CPU-Kerne für ihn verfügbar sind.  Beim Erstellen eines neuen Pods weiß der Kubernetes-Scheduler, wie viele Kerne dieser Pod benötigt.  Somit wird der Pod auf dem Server definiert, auf dem genügend Kerne vorhanden sind. <br><br>  Was passiert, wenn <b>keine</b> Anforderung angegeben wird (d. H. Der Pod bestimmt nicht die Anzahl der benötigten Kernel)?  Mal sehen, wie Kubernetes im Allgemeinen Ressourcen zählt. <br><br>  Der Pod kann sowohl Anforderungen (CFS-Scheduler) als auch Grenzwerte angeben (erinnern Sie sich an die Ampel?): <br><br><ul><li>  Wenn sie gleich sind, wird die garantierte QoS-Klasse dem Pod zugewiesen.  Eine solche Menge an Kerneln, die ihm immer zur Verfügung stehen, ist garantiert. </li><li>  Wenn die Anforderung unter dem Grenzwert liegt, kann die QoS-Klasse <b>geplatzt werden</b> .  Das heißt,  Wir erwarten, dass Pod beispielsweise immer 1 Kern verwendet, aber dieser Wert ist keine Einschränkung dafür: <i>Manchmal</i> kann Pod mehr verwenden (wenn dafür freie Ressourcen auf dem Server vorhanden sind). </li><li>  Es gibt auch die QoS-Klasse mit dem <b>besten Aufwand</b> - diejenigen Pods, für die keine Anforderung angegeben ist, gehören dazu.  Ressourcen werden ihnen zuletzt gegeben. </li></ul><br><h3>  Die Erinnerung </h3><br>  Ähnlich verhält es sich mit dem Gedächtnis, aber ein wenig anders - schließlich ist die Art dieser Ressourcen anders.  Im Allgemeinen lautet die Analogie wie folgt: <br><br><img src="https://habrastorage.org/webt/hw/zu/ja/hwzuja_vf0ojiz8uai-hhtn23ys.png"><br><br>  Mal sehen, wie Anfragen im Speicher implementiert werden.  Lassen Sie Pods auf dem Server leben und ändern Sie den verbrauchten Speicher, bis einer von ihnen so groß wird, dass der Speicher knapp wird.  In diesem Fall erscheint der OOM-Killer und beendet den größten Prozess: <br><br><img src="https://habrastorage.org/webt/mg/i0/at/mgi0atkc5o5m0xo-crxt3augbnm.gif"><br><br>  Dies passt nicht immer zu uns, daher ist es möglich zu regeln, welche Prozesse für uns wichtig sind und nicht getötet werden sollten.  Verwenden Sie dazu den Parameter <b>oom_score_adj</b> . <br><br>  Kehren wir zu den CPU-QoS-Klassen zurück und ziehen eine Analogie zu den oom_score_adj-Werten, die die Speicherverbrauchsprioritäten für Pods bestimmen: <br><br><ul><li>  Der niedrigste oom_score_adj-Wert eines Pods ist -998, was bedeutet, dass ein solcher Pod an letzter Stelle getötet werden sollte. Dies ist <b>garantiert</b> . </li><li>  Die höchste - 1000 - ist die <b>beste Anstrengung</b> , solche Schoten werden vor allen anderen getötet. </li><li>  Um den Rest der Werte ( <b>Burstable</b> ) zu berechnen, gibt es eine Formel, deren Essenz darauf <b>hinausläuft</b> , dass je mehr Pod Ressourcen angefordert hat, desto geringer ist die Wahrscheinlichkeit, dass sie getötet werden. </li></ul><br><img src="https://habrastorage.org/webt/sc/yo/rm/scyorm9zwn_lltxminknyv-cbhy.png"><br><br>  Die zweite "Drehung" - <b>limit_in_bytes</b> - für Limits.  Damit ist alles einfacher: Wir weisen einfach die maximale Speichermenge zu, die ausgegeben werden soll, und hier (im Gegensatz zur CPU) steht außer Frage, worin sie (Speicher) gemessen wird. <br><br><h3>  Insgesamt </h3><br>  Anforderungen und <code>limits</code> werden für jeden Pod in Kubernetes festgelegt - sowohl Parameter für die CPU als auch für den Speicher: <br><br><ol><li>  Basierend auf Anforderungen funktioniert der Kubernetes-Scheduler, der Pods auf mehrere Server verteilt. </li><li>  Basierend auf allen Parametern wird die QodS-Klasse des Pods bestimmt. </li><li>  Relative Gewichte werden basierend auf CPU-Anforderungen berechnet. </li><li>  Basierend auf CPU-Anforderungen wird ein CFS-Scheduler konfiguriert. </li><li>  Basierend auf Speicheranforderungen wird OOM Killer konfiguriert. </li><li>  Basierend auf den CPU-Grenzwerten wird eine „Ampel“ konfiguriert. </li><li>  Basierend auf Speicherlimits wird ein Limit für cgroup festgelegt. </li></ol><br><img src="https://habrastorage.org/webt/dr/1i/0_/dr1i0_troiqlcg4q_ki2fytefs0.png"><br><br>  Im Allgemeinen beantwortet dieses Bild alle Fragen, wie der Hauptteil des Ressourcenmanagements in Kubernetes stattfindet. <br><br><h2>  Autoskalierung </h2><br><h3>  K8s Cluster-Autoscaler </h3><br>  Stellen Sie sich vor, der gesamte Cluster ist bereits belegt und es muss ein neuer Pod erstellt werden.  Der Pod kann zwar nicht angezeigt werden, hängt jedoch im Status " <i>Ausstehend</i> ".  Damit es so aussieht, können wir einen neuen Server mit dem Cluster verbinden oder ... Cluster-Autoscaler einsetzen, der dies für uns erledigt: Bestellen Sie eine virtuelle Maschine beim Cloud-Anbieter (auf API-Anfrage) und verbinden Sie sie mit dem Cluster. Anschließend wird der Pod hinzugefügt . <br><br><img src="https://habrastorage.org/webt/zu/va/dq/zuvadqhlpycxkqaw5q35bmr08_e.gif"><br><br>  Dies ist die automatische Skalierung des Kubernetes-Clusters, die (nach unserer Erfahrung) hervorragend funktioniert.  Wie auch hier gibt es hier einige Nuancen ... <br><br>  Während wir die Clustergröße vergrößerten, war alles in Ordnung, aber was passiert, wenn der Cluster <b>freigegeben wird</b> ?  Das Problem ist, dass die Migration von Pods (auf freie Hosts) technisch sehr schwierig und ressourcenintensiv ist.  Kubernetes verfolgt einen völlig anderen Ansatz. <br><br>  Stellen Sie sich einen Cluster von 3 Servern vor, auf denen eine Bereitstellung stattfindet.  Er hat 6 Pods: Jetzt sind es 2 für jeden Server.  Aus irgendeinem Grund wollten wir einen der Server ausschalten.  Verwenden Sie dazu den Befehl <code>kubectl drain</code> , der: <br><br><ul><li>  verbietet das Senden neuer Pods an diesen Server; </li><li>  Entfernen Sie vorhandene Pods auf dem Server. </li></ul><br>  Da Kubernetes die Wartung der Anzahl der Pods (6) überwacht, werden sie einfach auf anderen Knoten neu erstellt, jedoch nicht auf dem nicht verbundenen Knoten, da sie bereits als nicht zugänglich für das Platzieren neuer Pods markiert sind.  Dies ist die grundlegende Mechanik für Kubernetes. <br><br><img src="https://habrastorage.org/webt/l8/dw/jf/l8dwjfv1yyszva-knkk6p0hls_s.gif"><br><br>  Hier gibt es jedoch eine Nuance.  In einer ähnlichen Situation für StatefulSet (anstelle von Deployment) sind die Aktionen unterschiedlich.  Jetzt haben wir bereits eine Stateful-Anwendung - zum Beispiel drei Pods mit MongoDB, von denen einer ein Problem hatte (die Daten wurden beschädigt oder ein anderer Fehler verhinderte, dass der Pod richtig gestartet wurde).  Und wieder beschließen wir, einen Server zu trennen.  Was wird passieren? <br><br><img src="https://habrastorage.org/webt/a1/dx/ck/a1dxckkad2wpckrygftdzcjbuyq.gif"><br><br>  MongoDB <i>könnte</i> sterben, weil es ein Quorum benötigt: Für einen Cluster von drei Installationen müssen mindestens zwei funktionieren.  Dies <i>geschieht jedoch nicht</i> - dank des <b>PodDisruptionBudget</b> .  Dieser Parameter bestimmt die minimal erforderliche Anzahl von Arbeitskapseln.  Wenn Sie wissen, dass einer der Pods mit MongoDB nicht mehr funktioniert, und wenn minAvailable in <code>minAvailable: 2</code> für MongoDB festgelegt ist, können Sie den Pod mit Kubernetes nicht entfernen. <br><br>  Fazit: Um Pods bei der Freigabe des Clusters korrekt zu verschieben (und tatsächlich neu zu erstellen), müssen Sie PodDisruptionBudget konfigurieren. <br><br><h3>  Horizontale Skalierung </h3><br>  Betrachten Sie eine andere Situation.  In Kubernetes wird eine Anwendung als Bereitstellung ausgeführt.  Der Benutzerverkehr kommt zu seinen Pods (zum Beispiel gibt es drei davon), und wir messen einen bestimmten Indikator in ihnen (z. B. CPU-Auslastung).  Wenn die Last zunimmt, korrigieren wir sie im Zeitplan und erhöhen die Anzahl der Pods, um Anforderungen zu verteilen. <br><br>  In Kubernetes müssen Sie dies heute nicht mehr manuell tun: Sie können die Anzahl der Pods abhängig von den Werten der gemessenen Lastindikatoren automatisch erhöhen / verringern. <br><br><img src="https://habrastorage.org/webt/kj/fm/_t/kjfm_tu0u83c4mthfjayisabme0.gif"><br><br>  Die Hauptfragen hier sind, <b>was genau zu messen ist</b> und <b>wie die</b> erhaltenen Werte <b>zu interpretieren</b> sind (um eine Entscheidung über die Änderung der Anzahl der Pods zu treffen).  Sie können viel messen: <br><br><img src="https://habrastorage.org/webt/h-/tw/a8/h-twa8kqe49av8gwxwxeoadyalc.png"><br><br>  Wie es technisch geht - Metriken sammeln usw.  - Ich habe im Bericht über <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Überwachung und Kubernetes</a> ausführlich gesprochen.  Und der wichtigste Rat für die Auswahl der optimalen Parameter ist das <b>Experimentieren</b> ! <br><br>  Es gibt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">eine USE-Methode</a> <i>(Utilization Saturation and Errors</i> ), deren Bedeutung wie folgt lautet.  Auf welcher Basis ist es sinnvoll, beispielsweise php-fpm zu skalieren?  Basierend auf der Tatsache, dass die Arbeiter enden, ist es die <i>Nutzung</i> .  Und wenn die Arbeiter vorbei sind und neue Verbindungen nicht akzeptiert werden, ist das <i>Sättigung</i> .  Beide Parameter müssen gemessen werden, und abhängig von den Werten sollte eine Skalierung durchgeführt werden. <br><br><h2>  Anstelle einer Schlussfolgerung </h2><br>  Der Bericht enthält eine Fortsetzung: Informationen zur vertikalen Skalierung und zur Auswahl der richtigen Ressourcen.  Ich werde in zukünftigen Videos auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">unserem YouTube darüber</a> sprechen - abonnieren, um es nicht zu verpassen! <br><br><h2>  Videos und Folien </h2><br>  Video von der Aufführung (44 Minuten): <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/10ZR-fbyuSY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  Präsentation des Berichts: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h2>  PS </h2><br>  Andere Kubernetes-Berichte in unserem Blog: <br><br><ul><li>  „ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes erweitern und ergänzen</a> “ <i>(Andrey Polov; 8. April 2019 bei Saint HighLoad ++)</i> ; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Datenbanken und Kubernetes</a> " <i>(Dmitry Stolyarov; 8. November 2018 auf HighLoad ++)</i> ; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Monitoring and Kubernetes</a> " <i>(Dmitry Stolyarov; 28. Mai 2018 bei RootConf)</i> ; </li><li>  „ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beste CI / CD-Praktiken mit Kubernetes und GitLab</a> “ <i>(Dmitry Stolyarov; 7. November 2017 bei HighLoad ++)</i> ; </li><li>  „ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Unsere Erfahrung mit Kubernetes in kleinen Projekten</a> “ <i>(Dmitry Stolyarov; 6. Juni 2017 bei RootConf)</i> . </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de459326/">https://habr.com/ru/post/de459326/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de459314/index.html">Visualisieren und behandeln Sie Hash Match Join</a></li>
<li><a href="../de459316/index.html">Hydra 2019: kostenlose Übertragung der ersten Halle und ein wenig darüber, was auf der Konferenz sein wird</a></li>
<li><a href="../de459318/index.html">TypeScript und kurze Sprints. Wie wir das Variationstool für das Front-End-Interview erstellt haben</a></li>
<li><a href="../de459320/index.html">Kubernetes Operator in Python ohne Frameworks und SDKs</a></li>
<li><a href="../de459322/index.html">Herausgeber Peter. Sommerschlussverkauf</a></li>
<li><a href="../de459328/index.html">Best-in-Class-Preis-Leistungsverhältnis - Mpow A5 (059)</a></li>
<li><a href="../de459330/index.html">Bitrix für Programmierer und Manager: Liebe und Hass</a></li>
<li><a href="../de459334/index.html">YouTrack 2019.2: ein systemweites Banner, Verbesserungen der Aufgabenlistenseite, neue Suchoptionen und mehr</a></li>
<li><a href="../de459336/index.html">Lebe und lerne. Teil 1. Schul- und Berufsberatung</a></li>
<li><a href="../de459338/index.html">Verwendung des Verifizierers als Mittel zur schnellen Modellierung von RTL-Projekten. Einführung in UVM</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>