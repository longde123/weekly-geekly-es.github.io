<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü•õ ü§ú ü§Ω Ein Zeichenbot zur Verwirklichung allt√§glicher Szenen und sogar Geschichten üíÜ üë®üèº‚Äçüéì ‚òòÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wenn Sie gebeten w√ºrden, ein Bild von mehreren Personen in Skiausr√ºstung zu zeichnen, die im Schnee stehen, w√ºrden Sie wahrscheinlich mit einem Umriss...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ein Zeichenbot zur Verwirklichung allt√§glicher Szenen und sogar Geschichten</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/microsoft/blog/457200/"><p> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/657/93f/e0f/65793fe0f0e42b382c46426806424e36.png" alt="Bot zeichnen" width="1024" height="414"></a> </p><br><p>  Wenn Sie gebeten w√ºrden, ein Bild von mehreren Personen in Skiausr√ºstung zu zeichnen, die im Schnee stehen, w√ºrden Sie wahrscheinlich mit einem Umriss von drei oder vier Personen beginnen, die in der Mitte der Leinwand positioniert sind, und dann die Skier unter ihren skizzieren F√º√üe.  Obwohl dies nicht angegeben wurde, k√∂nnen Sie jedem Skifahrer einen Rucksack hinzuf√ºgen, um die Erwartungen an die Sportarten der Skifahrer zu erf√ºllen.  Schlie√ülich w√ºrden Sie die Details sorgf√§ltig ausf√ºllen und vielleicht ihre Kleidung blau und ihre Schals rosa vor einem wei√üen Hintergrund streichen, um diese Personen realistischer zu machen und sicherzustellen, dass ihre Umgebung der Beschreibung entspricht.  Um die Szene lebendiger zu gestalten, k√∂nnen Sie sogar einige braune Steine ‚Äã‚Äãskizzieren, die durch den Schnee ragen, um darauf hinzuweisen, dass sich diese Skifahrer in den Bergen befinden. </p><br><p>  Jetzt gibt es einen Bot, der das alles kann. </p><a name="habracut"></a><br><p>  Die neue KI-Technologie, die bei Microsoft Research AI entwickelt wird, kann eine Beschreibung in nat√ºrlicher Sprache verstehen, ein Layout des Bildes skizzieren, das Bild synthetisieren und dann Details basierend auf dem Layout und den einzelnen bereitgestellten W√∂rtern verfeinern.  Mit anderen Worten, dieser Bot kann Bilder aus beschriftungs√§hnlichen Textbeschreibungen allt√§glicher Szenen erzeugen.  Dieser absichtliche Mechanismus f√ºhrte zu einer signifikanten Verbesserung der erzeugten Bildqualit√§t im Vergleich zu der fr√ºheren hochmodernen Technik zur Text-zu-Bild-Erzeugung f√ºr komplizierte allt√§gliche Szenen. Dies geht aus Ergebnissen von Industriestandardtests hervor, die in ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Objektgesteuerter Text- to-Image-Synthese durch kontradiktorisches Training</a> ‚Äú, das diesen Monat in Long Beach, Kalifornien, auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">IEEE-Konferenz 2019 √ºber Computer Vision und Mustererkennung</a> (CVPR 2019) ver√∂ffentlicht wird.  Dies ist ein Kooperationsprojekt zwischen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pengchuan Zhang</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Qiuyuan Huang</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Jianfeng Gao</a> von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Microsoft Research AI</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Lei Zhang</a> von Microsoft, Xiaodong He von JD AI Research und Wenbo Li und Siwei Lyu von der Universit√§t in Albany, SUNY (w√§hrend Wenbo Li als arbeitete) ein Praktikant bei Microsoft Research AI). </p><br><p>  Es gibt zwei Hauptherausforderungen, die mit dem beschreibungsbasierten Zeichenbotproblem verbunden sind.  Das erste ist, dass viele Arten von Objekten in allt√§glichen Szenen erscheinen k√∂nnen und der Bot in der Lage sein sollte, alle zu verstehen und zu zeichnen.  Fr√ºhere Verfahren zur Erzeugung von Text zu Bild verwenden Bildunterschriftenpaare, die nur ein sehr grobk√∂rniges √úberwachungssignal zum Erzeugen einzelner Objekte liefern, wodurch deren Qualit√§t bei der Objekterzeugung eingeschr√§nkt wird.  Bei dieser neuen Technologie verwenden die Forscher den COCO-Datensatz, der Beschriftungen und Segmentierungskarten f√ºr 1,5 Millionen Objektinstanzen in 80 g√§ngigen Objektklassen enth√§lt, sodass der Bot sowohl das Konzept als auch das Erscheinungsbild dieser Objekte lernen kann.  Dieses feink√∂rnige √ºberwachte Signal f√ºr die Objekterzeugung verbessert die Erzeugungsqualit√§t f√ºr diese allgemeinen Objektklassen erheblich. </p><br><p>  Die zweite Herausforderung besteht im Verst√§ndnis und der Erzeugung der Beziehungen zwischen mehreren Objekten in einer Szene.  Es wurden gro√üe Erfolge bei der Erzeugung von Bildern erzielt, die nur ein Hauptobjekt f√ºr mehrere bestimmte Dom√§nen enthalten, z. B. Gesichter, V√∂gel und gemeinsame Objekte.  Das Erzeugen komplexerer Szenen mit mehreren Objekten mit semantisch bedeutsamen Beziehungen zwischen diesen Objekten bleibt jedoch eine bedeutende Herausforderung in der Text-zu-Bild-Generierungstechnologie.  Dieser neue Zeichnungsbot hat gelernt, das Layout von Objekten aus Mustern des gemeinsamen Auftretens im COCO-Datensatz zu generieren, um dann ein Bild zu generieren, das vom vorgenerierten Layout abh√§ngig ist. </p><br><h3>  Objektgesteuerte, aufmerksame Bilderzeugung </h3><br><p>  Das Herzst√ºck des Zeichenbot von Microsoft Research AI ist eine Technologie, die als Generative Adversarial Network (GAN) bekannt ist.  Das GAN besteht aus zwei Modellen f√ºr maschinelles Lernen - einem Generator, der Bilder aus Textbeschreibungen generiert, und einem Diskriminator, der Textbeschreibungen verwendet, um die Authentizit√§t der generierten Bilder zu beurteilen.  Der Generator versucht, gef√§lschte Bilder am Diskriminator vorbei zu bringen.  Der Diskriminator hingegen will sich niemals t√§uschen lassen.  Zusammen arbeitet der Diskriminator den Generator in Richtung Perfektion. </p><br><p>  Der Zeichnungsbot wurde an einem Datensatz von 100.000 Bildern mit jeweils hervorstechenden Objektbeschriftungen und Segmentierungskarten sowie f√ºnf verschiedenen Beschriftungen trainiert, sodass die Modelle einzelne Objekte und semantische Beziehungen zwischen Objekten konzipieren konnten.  Das GAN lernt zum Beispiel, wie ein Hund aussehen soll, wenn Bilder mit und ohne Hundebeschreibungen verglichen werden. </p><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/27e/b5f/e0a/27eb5fe0ab38f35180ba6558f15f0456.png" alt="Abbildung 1: Eine komplexe Szene mit mehreren Objekten und Beziehungen." width="273" height="272"></a> <br><p>  Abbildung 1: Eine komplexe Szene mit mehreren Objekten und Beziehungen. </p><br><p>  GANs funktionieren gut, wenn Bilder erzeugt werden, die nur ein hervorstechendes Objekt enthalten, z. B. ein menschliches Gesicht, V√∂gel oder Hunde. Die Qualit√§t stagniert jedoch bei komplexeren Alltagsszenen, wie z. B. ‚ÄûEine Frau mit Helm reitet auf einem Pferd‚Äú (siehe Abbildung) 1.) Dies liegt daran, dass solche Szenen mehrere Objekte (Frau, Helm, Pferd) und reichhaltige semantische Beziehungen zwischen ihnen enthalten (Frau tr√§gt Helm, Frau reitet Pferd).  Der Bot muss zuerst diese Konzepte verstehen und sie mit einem aussagekr√§ftigen Layout im Bild platzieren.  Danach ist ein besser √ºberwachtes Signal erforderlich, das die Objekterzeugung und die Layouterzeugung lehren kann, um diese Aufgabe des Sprachverst√§ndnisses und der Bilderzeugung zu erf√ºllen. </p><br><p>  Wenn Menschen diese komplizierten Szenen zeichnen, entscheiden wir uns zun√§chst f√ºr die zu zeichnenden Hauptobjekte und erstellen ein Layout, indem wir Begrenzungsrahmen f√ºr diese Objekte auf der Leinwand platzieren.  Dann konzentrieren wir uns auf jedes Objekt, indem wir wiederholt die entsprechenden W√∂rter √ºberpr√ºfen, die dieses Objekt beschreiben.  Um dieses menschliche Merkmal zu erfassen, haben die Forscher ein sogenanntes objektgesteuertes aufmerksames GAN (ObjGAN) erstellt, um das menschliche Verhalten objektzentrierter Aufmerksamkeit mathematisch zu modellieren.  ObjGAN zerlegt den Eingabetext in einzelne W√∂rter und ordnet diese W√∂rtern bestimmten Objekten im Bild zu. </p><br><p>  Menschen √ºberpr√ºfen normalerweise zwei Aspekte, um die Zeichnung zu verfeinern: den Realismus einzelner Objekte und die Qualit√§t von Bildfeldern.  ObjGAN ahmt dieses Verhalten ebenfalls nach, indem es zwei Diskriminatoren einf√ºhrt - einen objektbezogenen Diskriminator und einen patchweisen Diskriminator.  Der objektbezogene Diskriminator versucht festzustellen, ob das erzeugte Objekt realistisch ist oder nicht und ob das Objekt mit der Satzbeschreibung √ºbereinstimmt.  Der Patch-weise Diskriminator versucht festzustellen, ob dieser Patch realistisch ist oder nicht und ob dieser Patch mit der Satzbeschreibung √ºbereinstimmt. </p><br><h3>  Verwandte Arbeiten: Visualisierung von Geschichten </h3><br><p>  Hochmoderne Modelle zur Erzeugung von Text zu Bild k√∂nnen realistische Vogelbilder basierend auf einer Beschreibung mit einem Satz erzeugen.  Die Erzeugung von Text zu Bild kann jedoch weit √ºber die Synthese eines einzelnen Bildes auf der Grundlage eines Satzes hinausgehen.  In ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">StoryGAN: Ein sequentielles bedingtes GAN f√ºr die Visualisierung von Geschichten</a> ‚Äú haben <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Jianfeng Gao</a> von Microsoft Research zusammen mit Zhe Gan, Jingjing Liu und Yu Cheng von Microsoft Dynamics 365 AI Research, Yitong Li, David Carlson und Lawrence Carin von der Duke University, Yelong Shen of Tencent AI Research und Yuexin Wu von der Carnegie Mellon University gehen noch einen Schritt weiter und schlagen eine neue Aufgabe mit dem Namen Story Visualization vor.  Bei einem Absatz mit mehreren S√§tzen kann eine vollst√§ndige Geschichte visualisiert werden, wobei eine Folge von Bildern erzeugt wird, eines f√ºr jeden Satz.  Dies ist eine herausfordernde Aufgabe, da der Zeichenbot nicht nur ein Szenario vorstellen muss, das zur Geschichte passt, die Interaktionen zwischen verschiedenen Charakteren in der Geschichte modellieren muss, sondern auch in der Lage sein muss, die globale Konsistenz √ºber dynamische Szenen und Charaktere hinweg aufrechtzuerhalten.  Diese Herausforderung wurde durch keine einzelnen Bild- oder Videoerzeugungsmethoden angegangen. </p><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/04c/5c0/39c/04c5c039ca8dc39e2f47ffabb9437db7.png" alt="Abbildung 2: Story-Visualisierung vs. einfache Bilderzeugung." width="1024" height="454"></a> <br><p>  Abbildung 2: Story-Visualisierung vs.  einfache Bilderzeugung. </p><br><p>  Die Forscher entwickelten ein neues Modell zur Generierung von Story-to-Image-Sequenzen, StoryGAN, das auf dem sequentiellen bedingten GAN-Framework basiert.  Dieses Modell ist insofern einzigartig, als es aus einem Deep Context Encoder besteht, der den Story-Fluss dynamisch verfolgt, und zwei Diskriminatoren auf Story- und Bildebene, um die Bildqualit√§t und die Konsistenz der generierten Sequenzen zu verbessern.  StoryGAN kann nat√ºrlich auch f√ºr die interaktive Bildbearbeitung erweitert werden, wobei ein Eingabebild basierend auf den Textanweisungen nacheinander bearbeitet werden kann.  In diesem Fall dient eine Folge von Benutzeranweisungen als "Story" -Eingabe.  Dementsprechend modifizierten die Forscher vorhandene Datens√§tze, um die CLEVR-SV- und Pororo-SV-Datens√§tze zu erstellen, wie in Abbildung 2 dargestellt. </p><br><h3>  Praktische Anwendungen - eine echte Geschichte </h3><br><p>  Die Technologie zur Erzeugung von Text zu Bild k√∂nnte praktische Anwendungen finden, die als eine Art Skizzenassistent f√ºr Maler und Innenarchitekten oder als Werkzeug f√ºr die sprachaktivierte Fotobearbeitung dienen.  Mit mehr Rechenleistung stellen sich die Forscher die Technologie vor, mit der Animationsfilme auf der Grundlage von Drehb√ºchern erstellt werden. Dies erweitert die Arbeit der Animationsfilmer, indem ein Teil der manuellen Arbeit entf√§llt. </p><br><p>  Die generierten Bilder sind vorerst noch weit von fotorealistisch entfernt.  Einzelne Objekte weisen fast immer Fehler auf, z. B. verschwommene Gesichter und / oder Busse mit verzerrten Formen.  Diese M√§ngel sind ein klarer Hinweis darauf, dass ein Computer, kein Mensch, die Bilder erstellt hat.  Trotzdem ist die Qualit√§t der ObjGAN-Bilder deutlich besser als die der bisher besten GAN-Bilder ihrer Klasse und dient als Meilenstein auf dem Weg zu einer generischen, menschen√§hnlichen Intelligenz, die die menschlichen F√§higkeiten erweitert. </p><br><p>  Damit KIs und Menschen dieselbe Welt teilen k√∂nnen, muss jeder eine M√∂glichkeit haben, miteinander zu interagieren.  Sprache und Vision sind die beiden wichtigsten Modalit√§ten f√ºr die Interaktion von Mensch und Maschine.  Die Text-zu-Bild-Generierung ist eine wichtige Aufgabe, die die multimodale Intelligenzforschung f√ºr Sprachvision vorantreibt. </p><br><p>  Die Forscher, die diese aufregende Arbeit geschaffen haben, freuen sich darauf, diese Ergebnisse mit den Teilnehmern des CVPR in Long Beach zu teilen und zu h√∂ren, was Sie denken.  In der Zwischenzeit k√∂nnen Sie sich den Open-Source-Code f√ºr <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ObjGAN</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">StoryGAN</a> auf GitHub <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ansehen</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de457200/">https://habr.com/ru/post/de457200/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de457190/index.html">Wir behandeln das Gesch√§ft durch die Implementierung von CRM-Systemen</a></li>
<li><a href="../de457192/index.html">Airbus erreicht mit der gemischten Realit√§t von Microsoft neue H√∂hen</a></li>
<li><a href="../de457194/index.html">Airbus erreicht mit Hilfe der Microsoft Mixed Reality-Technologie neue H√∂hen</a></li>
<li><a href="../de457196/index.html">Kleine Freude # 5: Dynaconf - Verwalten der Einstellungen im Projekt</a></li>
<li><a href="../de457198/index.html">Das neuronale Netzwerk hat gelernt, komplexe Szenen aus einer Textbeschreibung zu zeichnen</a></li>
<li><a href="../de457202/index.html">Wie wir Ideen f√ºr die Entwicklung unserer Produkte ausw√§hlen: Der Anbieter muss h√∂ren k√∂nnen ...</a></li>
<li><a href="../de457204/index.html">Windows PowerShell und lange Pfade</a></li>
<li><a href="../de457206/index.html">SQL Index Manager - eine lange Geschichte √ºber SQL Server, Grabgraben und Indexpflege</a></li>
<li><a href="../de457208/index.html">Dynamisches Generieren von robots.txt f√ºr ASP.NET Core-Sites basierend auf der Umgebung</a></li>
<li><a href="../de457210/index.html">Speichern Sie statische Ressourcen auf Ihrem Hosting</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>