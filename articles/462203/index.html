<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üïØÔ∏è üë®üèæ‚Äçü§ù‚Äçüë®üèº üë©üèø KVM (bajo) VDI con m√°quinas virtuales √∫nicas que usan bash ‚òùüèª üÜó üèúÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="¬øPara qui√©n es este art√≠culo? 
 Este art√≠culo puede ser de inter√©s para los administradores de sistemas que se enfrentaron con la tarea de crear un se...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>KVM (bajo) VDI con m√°quinas virtuales √∫nicas que usan bash</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/462203/"><h4>  ¬øPara qui√©n es este art√≠culo? </h4><br>  Este art√≠culo puede ser de inter√©s para los administradores de sistemas que se enfrentaron con la tarea de crear un servicio de trabajos "√∫nicos". <br><br><h4>  Prologo </h4><br>  Se le pidi√≥ al departamento de soporte de TI de una empresa joven en desarrollo din√°mico con una peque√±a red regional que organizara "estaciones de autoservicio" para el uso de sus clientes externos.  Se supon√≠a que los datos de la estaci√≥n se utilizar√≠an para registrarse en los portales externos de la compa√±√≠a, descargar datos de dispositivos externos y trabajar con portales gubernamentales. <br><br>  Un aspecto importante fue el hecho de que la mayor√≠a del software est√° "mejorado" en MS Windows (por ejemplo, "Declaraci√≥n") y, a pesar del movimiento hacia formatos abiertos, MS Office sigue siendo el est√°ndar dominante en el intercambio de documentos electr√≥nicos.  Por lo tanto, no podr√≠amos rechazar MS Windows al resolver este problema. <br><a name="habracut"></a><br>  El principal problema era la posibilidad de acumular diversos datos de las sesiones de los usuarios, lo que podr√≠a llevar a su filtraci√≥n a terceros.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Esta situaci√≥n ya ha decepcionado al MFC</a> .  Pero a diferencia del MFC cuasijudicial (instituci√≥n aut√≥noma estatal), las organizaciones no estatales ser√°n castigadas mucho m√°s por tales deficiencias.  El siguiente problema cr√≠tico fue el requisito de trabajar con medios de almacenamiento externos, en los cuales, por supuesto, habr√° un mont√≥n de malware malicioso.  La probabilidad de entrada de malware desde Internet se consider√≥ menos probable debido a la restricci√≥n del acceso a Internet a trav√©s de una lista blanca de direcciones.Los empleados de otros departamentos se unieron para resolver los requisitos, hacer sus requisitos y deseos, los requisitos finales fueron los siguientes: <br><br>  <b>Requisitos de IS</b> <br><br><ul><li>  Despu√©s del uso, se deben eliminar todos los datos del usuario (incluidos los archivos temporales y las claves de registro). </li><li>  Todos los procesos iniciados por el usuario deben completarse al final del trabajo. </li><li>  Acceso a Internet a trav√©s de una lista blanca de direcciones. </li><li>  Restricciones sobre la capacidad de ejecutar c√≥digo de terceros. </li><li>  Si la sesi√≥n est√° inactiva durante m√°s de 5 minutos, la sesi√≥n debe finalizar autom√°ticamente, la estaci√≥n debe realizar una limpieza. </li></ul><br>  <b>Requisitos del cliente</b> <br><br><ul><li>  El n√∫mero de estaciones cliente por sucursal no es m√°s de 4. </li><li>  El tiempo m√≠nimo de espera para la preparaci√≥n del sistema, desde el momento en que "me sent√© en una silla" hasta el comienzo de trabajar con el software del cliente. </li><li>  La capacidad de conectar dispositivos perif√©ricos (esc√°neres, unidades flash) directamente desde el sitio de instalaci√≥n de la "estaci√≥n de autoservicio". </li><li>  Deseos del cliente </li><li>  Demostraci√≥n de material publicitario (fotos) en el momento del cierre del complejo. </li></ul><cut></cut><br><h4>  Harina de creatividad </h4><br>  Despu√©s de haber jugado lo suficiente con Windows LiveCD, llegamos a la conclusi√≥n un√°nime de que la soluci√≥n resultante no satisface al menos 3 puntos cr√≠ticos.  Est√°n cargados durante mucho tiempo, o no est√°n realmente vivos, o su personalizaci√≥n se asoci√≥ con un dolor salvaje.  Tal vez buscamos mal, y puede aconsejar un conjunto de algunas herramientas, se lo agradecer√©. <br><br>  Adem√°s, comenzamos a mirar hacia VDI, pero para esta tarea, la mayor√≠a de las soluciones son demasiado caras o requieren mucha atenci√≥n.  Y quer√≠a una herramienta simple con una cantidad m√≠nima de magia, la mayor√≠a de los problemas podr√≠an resolverse simplemente reiniciando / reiniciando el servicio.  Afortunadamente, ten√≠amos equipos de servidor, de clase baja en las sucursales, del servicio fuera de servicio, que podr√≠amos utilizar para la base tecnol√≥gica. <br><br>  Cual es el resultado?  Pero no podr√© decirte lo que sucedi√≥ al final, porque el NDA, pero en el proceso de b√∫squeda, desarrollamos un esquema interesante que se mostr√≥ bien en las pruebas de laboratorio, aunque no entr√≥ en serie. <br><br>  Algunos descargos de responsabilidad: el autor no afirma que la soluci√≥n propuesta resuelva por completo todas las tareas y lo hace de forma voluntaria y con la canci√≥n.  El autor est√° de acuerdo de antemano con la afirmaci√≥n de que Sein Englishe sprache es zehr schlecht.  Como la soluci√≥n ya no se desarrolla, no puede contar con una correcci√≥n de errores o un cambio en la funcionalidad, todo est√° en sus manos.  El autor asume que est√° al menos un poco familiarizado con KVM y lee un art√≠culo de revisi√≥n sobre el protocolo Spice, y trabaj√≥ un poco con Centos u otra distribuci√≥n GNU Linux. <br><br>  En este art√≠culo, me gustar√≠a analizar la columna vertebral de la soluci√≥n resultante, es decir, la interacci√≥n del cliente y el servidor y la esencia de los procesos en el ciclo de vida de las m√°quinas virtuales en el marco de la soluci√≥n en cuesti√≥n.  Si el art√≠culo ser√° interesante para el p√∫blico, describir√© los detalles de la implementaci√≥n de im√°genes en vivo para crear clientes ligeros basados ‚Äã‚Äãen Fedora y hablar√© sobre los detalles de ajuste de m√°quinas virtuales y servidores KVM para optimizar el rendimiento y la seguridad. <br><br>  Si tomas papel de color, <br>  Pinturas, pinceles y pegamentos. <br>  Y un poco m√°s de destreza ... <br>  ¬°Puedes hacer cien rublos! <br><br><h4>  Esquema y descripci√≥n del banco de pruebas. </h4><br><img src="https://habrastorage.org/webt/pu/tu/rk/puturkaiwqcpbp4wld7ezdk_lcw.png"><br><br>  Todo el equipo se encuentra dentro de la red de sucursales, solo se apaga el canal de Internet.  Hist√≥ricamente, ya ha habido un servidor proxy, no es nada extraordinario.  Pero es en √©l, entre otras cosas, que se filtrar√° el tr√°fico de las m√°quinas virtuales (abreviatura VM m√°s adelante en el texto).  Nada impide colocar este servicio en el servidor KVM, lo √∫nico que debe observar es c√≥mo cambia la carga del mismo en el subsistema de disco. <br><br>  Estaci√≥n del cliente: de hecho, "estaciones de autoservicio", "front-end" de nuestro servicio.  Son nettops de Lenovo IdeaCentre.  ¬øPara qu√© sirve esta unidad?  S√≠, casi todos, especialmente satisfechos con la gran cantidad de conectores USB y lectores de tarjetas en el panel frontal.  En nuestro esquema, se inserta una tarjeta SD con protecci√≥n de escritura de hardware en el lector de tarjetas, en la cual se graba la imagen en vivo modificada de Fedora 28. Por supuesto, un monitor, teclado y mouse est√°n conectados a la red. <br><br>  Conmutador: un conmutador de hardware poco notable del segundo nivel, est√° en la sala de servidores y parpadea con luces.  No est√° conectado a ninguna red, excepto a la red de "estaciones de autoservicio". <br><br>  KVM_Server es el n√∫cleo del circuito; en las pruebas de banco del Core 2 Quad Q9650 con 8 GB de RAM, con confianza sac√≥ 3 m√°quinas virtuales de Windows 10 sobre s√≠ mismo.  Subsistema de disco: adaptatec 3405 2 unidades Raid 1 + SSD.  En las pruebas de campo del Xeon 1220, el SSD LSI 9260 + m√°s serio extrajo f√°cilmente 5-6 m√°quinas virtuales.  Obtendr√≠amos el servidor del servicio retirado, no habr√≠a muchos costos de capital.  El sistema de virtualizaci√≥n KVM con pool_Vm virtual machine pool se implementa en este servidor (es). <br><br>  Vm es una m√°quina virtual, el backend de nuestro servicio.  Es el trabajo del usuario. <br><br>  Enp5s0 es una interfaz de red que mira hacia la red de "estaciones de autoservicio", dhcpd, ntpd, httpd live on it, y xinetd escucha el puerto de "se√±al". <br><br>  Lo0 es la pseudointerfaz de bucle invertido.  Est√°ndar. <br><br>  Spice_console: una cosa muy interesante, el hecho es que, a diferencia del RDP cl√°sico, cuando activa el paquete de protocolo KVM + Spice, aparece una entidad adicional: el puerto de la consola de la m√°quina virtual.  De hecho, al conectarnos a este puerto TCP, obtenemos la consola Vm, sin la necesidad de conectarse a Vm a trav√©s de su interfaz de red.  Toda la interacci√≥n con Vm para la transmisi√≥n de se√±al, el servidor se hace cargo.  El an√°logo m√°s cercano en funci√≥n es IPKVM.  Es decir  La imagen del monitor VM se transfiere a este puerto, se le transmiten datos sobre el movimiento del mouse y (lo m√°s importante) la interacci√≥n a trav√©s del protocolo Spice le permite redirigir sin problemas los dispositivos USB a la m√°quina virtual, como si este dispositivo estuviera conectado al Vm.  Probado para unidades flash, esc√°neres, c√°maras web. <br><br>  Vnet0, virbr0 y tarjetas de red virtuales Vm forman una red de m√°quinas virtuales. <br><br><h4>  Como funciona </h4><br>  Desde la estaci√≥n del cliente <br><br>  La estaci√≥n cliente arranca en modo gr√°fico desde la imagen en vivo modificada de Fedora 28, recibe la direcci√≥n IP por dhcp desde el espacio de direcciones de red 169.254.24.0/24.  Durante el proceso de arranque, se crean reglas de firewall que permiten conexiones a los puertos del servidor "se√±al" y "spice".  Una vez completada la descarga, la estaci√≥n espera la autorizaci√≥n del usuario del Cliente.  Despu√©s de la autorizaci√≥n del usuario, se inicia el administrador de escritorio "openbox" y el script de inicio autom√°tico se ejecuta en nombre del usuario autorizado.  Entre otras cosas, el script de ejecuci√≥n autom√°tica ejecuta el script remote.sh. <br><br><div class="spoiler">  <b class="spoiler_title">$ HOME / .config / openbox / scripts / remote.sh</b> <div class="spoiler_text"><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh server_ip=$(/usr/bin/cat /etc/client.conf |/usr/bin/grep "server_ip" \ |/usr/bin/cut -d "=" -f2) vdi_signal_port=$(/usr/bin/cat /etc/client.conf |/usr/bin/grep "vdi_signal_port" \ |/usr/bin/cut -d "=" -f2) vdi_spice_port=$(/usr/bin/cat /etc/client.conf |/usr/bin/grep "vdi_spice_port" \ |/usr/bin/cut -d "=" -f2) animation_folder=$(/usr/bin/cat /etc/client.conf |/usr/bin/grep "animation_folder" \ |/usr/bin/cut -d "=" -f2) process=/usr/bin/remote-viewer while true do if [ -z `/usr/bin/pidof feh` ] then /usr/bin/echo $animation_folder /usr/bin/feh -N -x -D1 $animation_folder &amp; else /usr/bin/echo fi /usr/bin/nc -i 1 $server_ip $vdi_signal_port |while read line do if /usr/bin/echo "$line" |/usr/bin/grep "RULE ADDED, CONNECT NOW!" then /usr/bin/killall feh pid_process=$($process "spice://$server_ip:$vdi_spice_port" \ "--spice-disable-audio" "--spice-disable-effects=animation" \ "--spice-preferred-compression=auto-glz" "-k" \ "--kiosk-quit=on-disconnect" | /bin/echo $!) /usr/bin/wait $pid_process /usr/bin/killall -u $USER exit else /usr/bin/echo $line &gt;&gt; /var/log/remote.log fi done done</span></span></code> </pre> <br></div></div><br><div class="spoiler">  <b class="spoiler_title">/etc/client.conf</b> <div class="spoiler_text"><pre> <code class="bash hljs">server_ip=169.254.24.1 vdi_signal_port=5905 vdi_spice_port=5906 animation_folder=/usr/share/backgrounds/animation background_folder=/usr/share/backgrounds2/fedora-workstation</code> </pre><br></div></div><br>  Descripci√≥n de las variables del archivo client.conf <br>  server_ip - direcci√≥n KVM_Server <br>  vdi_signal_port - puerto KVM_Server en el que xinetd "se sienta" <br>  vdi_spice_port: puerto de red KVM_Server, desde el cual la solicitud de conexi√≥n se redirigir√° desde el cliente de visor remoto al puerto de especias del Vm seleccionado (detalles a continuaci√≥n) <br>  animation_folder - carpeta de donde se toman las im√°genes para la animaci√≥n de demostraci√≥n de mierda <br>  background_folder: la carpeta de la que se toman las im√°genes para presentaciones en espera.  M√°s sobre animaci√≥n en la siguiente parte del art√≠culo. <br><br>  El script remote.sh toma la configuraci√≥n del archivo de configuraci√≥n /etc/client.conf y usa nc para conectarse al puerto "vdi_signal_port" del servidor KVM y recibe una secuencia de datos del servidor, entre los cuales espera la cadena "REGLA AGREGADA, CONECTE AHORA".  Cuando se recibe la l√≠nea deseada, el proceso del visor remoto comienza en modo quiosco, estableciendo una conexi√≥n con el puerto del servidor "vdi_spice_port".  La ejecuci√≥n del script se suspende hasta el final de la ejecuci√≥n del visor remoto. <br><br>  El visor remoto que se conecta al puerto "vdi_spice_port", debido a una redirecci√≥n en el lado del servidor, llega al puerto "spice_console" de la interfaz lo0, es decir  a la consola de la m√°quina virtual y el trabajo del usuario se realiza directamente.  Mientras espera la conexi√≥n, al usuario se le muestra animaci√≥n de mierda, en forma de presentaci√≥n de diapositivas de archivos jpeg, la ruta al directorio con im√°genes est√° determinada por el valor de la variable animation_folder del archivo de configuraci√≥n. <br><br>  Si se pierde la conexi√≥n al puerto "spice_console" de la m√°quina virtual, lo que indica el apagado / reinicio de la m√°quina virtual (es decir, el final real de la sesi√≥n del usuario), todos los procesos que se ejecutan en nombre del usuario autorizado finalizan, lo que conduce al reinicio de lightdm y vuelve a la pantalla de autorizaci√≥n . <br><br><h4>  Del lado del servidor KVM </h4><br>  En el puerto de "se√±al" de la tarjeta de red, enp5s0 est√° esperando la conexi√≥n xinetd.  Despu√©s de conectarse al puerto de "se√±al", xinetd ejecuta el script vm_manager.sh sin pasarle ning√∫n par√°metro de entrada y redirige el resultado del script a la sesi√≥n nc de Client Station. <br><br><div class="spoiler">  <b class="spoiler_title">/etc/xinetd.d/test-server</b> <div class="spoiler_text"><pre> <code class="bash hljs">service vdi_signal { port = 5905 socket_type = stream protocol = tcp <span class="hljs-built_in"><span class="hljs-built_in">wait</span></span> = no user = root server = /home/admin/scripts_vdi_new/vm_manager.sh }</code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_manager.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/sh #&lt;SET LOCAL VARIABLES FOR SCRIPT&gt;# SRV_SCRIPTS_DIR=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_scripts_dir" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_SCRIPTS_DIR=$SRV_SCRIPTS_DIR" export SRV_SCRIPTS_DIR=$SRV_SCRIPTS_DIR SRV_POOL_SIZE=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_pool_size" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_POOL_SIZE=$SRV_POOL_SIZE" export "SRV_POOL_SIZE=$SRV_POOL_SIZE" SRV_START_PORT_POOL=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_start_port_pool" |/usr/bin/cut -d "=" -f2) /usr/bin/echo SRV_START_PORT_POOL=$SRV_START_PORT_POOL export SRV_START_PORT_POOL=$SRV_START_PORT_POOL SRV_TMP_DIR=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_tmp_dir" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_TMP_DIR=$SRV_TMP_DIR" export SRV_TMP_DIR=$SRV_TMP_DIR date=$(/usr/bin/date) #&lt;/SET LOCAL VARIABLES FOR SCRIPT&gt;# /usr/bin/echo "# $date START EXECUTE VM_MANAGER.SH #" make_connect_to_vm() { #&lt;READING CLEAR.LIST AND CHECK PORT FOR NETWORK STATE&gt;# /usr/bin/echo "READING CLEAN.LIST AND CHECK PORT STATE" #&lt;CHECK FOR NO ONE PORT IN CLEAR.LIST&gt;# if [ -z `/usr/bin/cat $SRV_TMP_DIR/clear.list` ] then /usr/bin/echo "NO AVALIBLE PORTS IN CLEAN.LIST FOUND" /usr/bin/echo "Will try to make housekeeper, and create new vm" make_housekeeper else #&lt;MINIMUN ONE PORT IN CLEAR.LIST FOUND&gt;# /usr/bin/cat $SRV_TMP_DIR/clear.list |while read line do clear_vm_port=$(($line)) /bin/echo "FOUND PORT $clear_vm_port IN CLEAN.LIST. TRY NETSTAT" \ "CHECK FOR PORT=$clear_vm_port" #&lt;NETSTAT LISTEN CHECK FOR PORT FROM CLEAN.LIST&gt;# if /usr/bin/netstat -lnt |/usr/bin/grep ":$clear_vm_port" &gt; /dev/null then /bin/echo "$clear_vm_port IS LISTEN" #&lt;PORT IS LISTEN. CHECK FOR IS CONNECTED NOW&gt;# if /usr/bin/netstat -nt |/usr/bin/grep ":$clear_vm_port" \ |/usr/bin/grep "ESTABLISHED" &gt; /dev/null then #&lt;PORT LISTEN AND ALREADY CONNECTED! MOVE PORT FROM CLEAR.LIST # TO WASTE.LIST&gt;# /bin/echo "$clear_vm_port IS ALREADY CONNECTED, MOVE PORT TO WASTE.LIST" /usr/bin/sed -i "/$clear_vm_port/d" $SRV_TMP_DIR/clear.list /usr/bin/echo $clear_vm_port &gt;&gt; $SRV_TMP_DIR/waste.list else #&lt;PORT LISTEN AND NO ONE CONNECT NOW. MOVE PORT FROM CLEAR.LIST TO # CONN_WAIT.LIST AND CREATE IPTABLES RULES&gt;## /usr/bin/echo "OK, $clear_vm_port IS NOT ALREADY CONNECTED" /usr/bin/sed -i "/$clear_vm_port/d" $SRV_TMP_DIR/clear.list /usr/bin/echo $clear_vm_port &gt;&gt; $SRV_TMP_DIR/conn_wait.list $SRV_SCRIPTS_DIR/vm_connect.sh $clear_vm_port #&lt;TRY TO CLEAN VM IN WASTE.LIST AND CREATE NEW WM&gt;# /bin/echo "TRY TO CLEAN VM IN WASTE.LIST AND CREATE NEW VM" make_housekeeper /usr/bin/echo "# $date STOP EXECUTE VM_MANAGER.SH#" exit fi else #&lt;PORT IS NOT A LISTEN. MOVE PORT FROM CLEAR.LIST TO WASTE.LIST&gt;# /bin/echo " "$clear_vm_port" is NOT LISTEN. REMOVE PORT FROM CLEAR.LIST" /usr/bin/sed -i "/$clear_vm_port/d" $SRV_TMP_DIR/clear.list /usr/bin/echo $clear_vm_port &gt;&gt; $SRV_TMP_DIR/waste.list make_housekeeper fi done fi } make_housekeeper() { /usr/bin/echo "=Execute housekeeper=" /usr/bin/cat $SRV_TMP_DIR/waste.list |while read line do /usr/bin/echo "$line" if /usr/bin/netstat -lnt |/usr/bin/grep ":$line" &gt; /dev/null then /bin/echo "port_alive, vm is running" if /usr/bin/netstat -nt |/usr/bin/grep ":$line" \ |/usr/bin/grep "ESTABLISHED" &gt; /dev/null then /bin/echo "port_in_use can't delete vm!!!" else /bin/echo "port_not in use. Deleting vm" /usr/bin/sed -i "/$line/d" $SRV_TMP_DIR/waste.list /usr/bin/echo $line &gt;&gt; $SRV_TMP_DIR/recycle.list $SRV_SCRIPTS_DIR/vm_delete.sh $line fi else /usr/bin/echo "posible vm is already off. Deleting vm" /usr/bin/echo "MOVE VM IN OFF STATE $line FROM WASTE.LIST TO" \ "RECYCLE.LIST AND DELETE VM" /usr/bin/sed -i "/$line/d" $SRV_TMP_DIR/waste.list /usr/bin/echo $line &gt;&gt; $SRV_TMP_DIR/recycle.list $SRV_SCRIPTS_DIR/vm_delete.sh "$line" fi done create_clear_vm } create_clear_vm() { /usr/bin/echo "=Create new VM=" while [ $SRV_POOL_SIZE -gt 0 ] do new_vm_port=$(($SRV_START_PORT_POOL+$SRV_POOL_SIZE)) /usr/bin/echo "new_vm_port=$new_vm_port" if /usr/bin/grep "$new_vm_port" $SRV_TMP_DIR/clear.list &gt; /dev/null then /usr/bin/echo "$new_vm_port port is already defined in clear.list" else if /usr/bin/grep "$new_vm_port" $SRV_TMP_DIR/waste.list &gt; /dev/null then /usr/bin/echo "$new_vm_port port is already defined in waste.list" else if /usr/bin/grep "$new_vm_port" $SRV_TMP_DIR/recycle.list &gt; /dev/null then /usr/bin/echo "$new_vm_port PORT IS ALREADY DEFINED IN RECYCLE LIST" else if /usr/bin/grep "$new_vm_port" $SRV_TMP_DIR/conn_wait.list &gt; /dev/null then /usr/bin/echo "$new_vm_port PORT IS ALREADY DEFINED IN CONN_WAIT LIST" else /usr/bin/echo "PORT IN NOT DEFINED IN NO ONE LIST WILL CREATE" \ "VM ON PORT $new_vm_port" /usr/bin/echo $new_vm_port &gt;&gt; $SRV_TMP_DIR/recycle.list $SRV_SCRIPTS_DIR/vm_create.sh $new_vm_port fi fi fi fi SRV_POOL_SIZE=$(($SRV_POOL_SIZE-1)) done /usr/bin/echo "# $date STOP EXECUTE VM_MANAGER.SH #" } make_connect_to_vm |/usr/bin/tee -a /var/log/vm_manager.log</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">/etc/vm_manager.conf</b> <div class="spoiler_text">  srv_scripts_dir = / home / admin / scripts_vdi_new <br>  srv_pool_size = 4 <br>  srv_start_port_pool = 5920 <br>  srv_tmp_dir = / tmp / vm_state <br>  base_host = win10_2 <br>  input_iface = enp5s0 <br>  vdi_spice_port = 5906 <br>  count_conn_tryes = 10 <br></div></div><br><br>  Descripci√≥n de las variables del archivo de configuraci√≥n vm_manager.conf <br>  srv_scripts_dir: carpeta de ubicaci√≥n de script vm_manager.sh, vm_connect.sh, vm_delete.sh, vm_create.sh, vm_clear.sh <br>  srv_pool_size - Tama√±o del grupo de Vm <br>  srv_start_port_pool: el puerto inicial, despu√©s del cual comenzar√°n los puertos de especias de las consolas de m√°quinas virtuales <br>  srv_tmp_dir - carpeta para archivos temporales <br>  base_host - base Vm (imagen dorada) desde la cual los clones Vm se convertir√°n en el grupo <br>  input_iface - la interfaz de red del servidor, mirando hacia las estaciones de clientes <br>  vdi_spice_port: el puerto de red del servidor desde el cual se redirigir√° la solicitud de conexi√≥n desde el cliente de visor remoto al puerto de especias del Vm seleccionado <br>  count_conn_tryes: un temporizador de espera, despu√©s del cual se considera que no se ha producido una conexi√≥n a Vm (para obtener m√°s informaci√≥n, consulte vm_connect.sh) <br><br>  El script vm_manager.sh lee el archivo de configuraci√≥n del archivo vm_manager.conf, eval√∫a el estado de las m√°quinas virtuales en el grupo de acuerdo con varios par√°metros, a saber: cu√°ntas m√°quinas virtuales se implementan, si hay m√°quinas virtuales libres y limpias.  Para hacer esto, lee el archivo clear.list que contiene los n√∫meros de puertos "spice_console" de las m√°quinas virtuales "reci√©n creadas" (vea el ciclo de creaci√≥n de VM a continuaci√≥n) y comprueba una conexi√≥n establecida con ellas.  Si se detecta un puerto con una conexi√≥n de red establecida (que no deber√≠a serlo), se muestra una advertencia y el puerto se transfiere a waste.list Cuando se encuentra el primer puerto del archivo clear.list con el que actualmente no hay conexi√≥n, vm_manager.sh llama al script vm_connect.sh y pasa √©l como par√°metro el n√∫mero de este puerto. <br><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_connect.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh date=$(/usr/bin/date) /usr/bin/echo "#" "$date" "START EXECUTE VM_CONNECT.SH#" #&lt;SET LOCAL VARIABLES FOR SCRIPT&gt;# free_port="$1" input_iface=$(/usr/bin/cat /etc/vm_manager.conf |/usr/bin/grep "input_iface" \ |/usr/bin/cut -d "=" -f2) /usr/bin/echo "input_iface=$input_iface" vdi_spice_port=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "vdi_spice_port" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "vdi_spice_port=$vdi_spice_port" count_conn_tryes=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "count_conn_tryes" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "count_conn_tryes=$count_conn_tryes" #&lt;/SET LOCAL VARIABLES FOR SCRIPT&gt;# #&lt;CREATE IPTABLES RULES AND SEND SIGNAL TO CONNECT&gt;# /usr/bin/echo "create rule for port" $free_port /usr/sbin/iptables -I INPUT -i $input_iface -p tcp -m tcp --dport \ $free_port -j ACCEPT /usr/sbin/iptables -I OUTPUT -o $input_iface -p tcp -m tcp --sport \ $free_port -j ACCEPT /usr/sbin/iptables -t nat -I PREROUTING -p tcp -i $input_iface --dport \ $vdi_spice_port -j DNAT --to-destination 127.0.0.1:$free_port /usr/bin/echo "RULE ADDED, CONNECT NOW!" #&lt;/CREATE IPTABLES RULES AND SEND SIGNAL TO CONNECT&gt;# #&lt;WAIT CONNECT ESTABLISHED AND ACTIVATE CONNECT TIMER&gt;# while [ $count_conn_tryes -gt 0 ] do if /usr/bin/netstat -nt |/usr/bin/grep ":$free_port" \ |/usr/bin/grep "ESTABLISHED" &gt; /dev/null then /bin/echo "$free_port NOW in use!!!" /usr/bin/sleep 1s /usr/sbin/iptables -t nat -D PREROUTING -p tcp -i $input_iface --dport \ $vdi_spice_port -j DNAT --to-destination 127.0.0.1:$free_port /usr/sbin/iptables -D INPUT -i $input_iface -p tcp -m tcp --dport \ $free_port -j ACCEPT /usr/sbin/iptables -D OUTPUT -o $input_iface -p tcp -m tcp --sport \ $free_port -j ACCEPT /usr/bin/sed -i "/$free_port/d" $SRV_TMP_DIR/conn_wait.list /usr/bin/echo $free_port &gt;&gt; $SRV_TMP_DIR/waste.list return else /usr/bin/echo "$free_port NOT IN USE" /usr/bin/echo "RULE ADDED, CONNECT NOW!" /usr/bin/sleep 1s fi count_conn_tryes=$((count_conn_tryes-1)) done #&lt;/WAIT CONNECT ESTABLISED AND ACTIVATE CONNECT TIMER&gt;# #&lt;IF COUNT HAS EXPIRED. REMOVE IPTABLES RULE AND REVERT \ # VM TO CLEAR.LIST&gt;# /usr/bin/echo "REVERT IPTABLES RULE AND REVERT VM TO CLEAN \ LIST $free_port" /usr/sbin/iptables -t nat -D PREROUTING -p tcp -i $input_iface --dport \ $vdi_spice_port -j DNAT --to-destination 127.0.0.1:$free_port /usr/sbin/iptables -D INPUT -i $input_iface -p tcp -m tcp --dport $free_port \ -j ACCEPT /usr/sbin/iptables -D OUTPUT -o $input_iface -p tcp -m tcp --sport \ $free_port -j ACCEPT /usr/bin/sed -i "/$free_port/d" $SRV_TMP_DIR/conn_wait.list /usr/bin/echo $free_port &gt;&gt; $SRV_TMP_DIR/clear.list #&lt;/COUNT HAS EXPIRED. REMOVE IPTABLES RULE AND REVERT VM \ #TO CLEAR.LIST&gt;# /usr/bin/echo "#" "$date" "END EXECUTE VM_CONNECT.SH#" # Attention! Must Be! sysctl net.ipv4.conf.all.route_localnet=1</span></span></code> </pre><br></div></div><br>  El script vm_connect.sh introduce reglas de firewall que crean un "vdi_spice_port" de redireccionamiento del puerto del servidor de la interfaz enp5s0 al "puerto de consola de especias" de la VM ubicada en la interfaz del servidor lo0, que se pasa como par√°metro de inicio.  El puerto se transfiere a conn_wait.list, se considera que la VM est√° pendiente de conexi√≥n.  La l√≠nea REGLA AGREGADA, CONECTAR AHORA se env√≠a a la sesi√≥n de Client Station en el puerto de "se√±al" del servidor, que es lo que espera el script remote.sh que se ejecuta en √©l.  Un ciclo de espera de conexi√≥n comienza con el n√∫mero de intentos determinados por el valor de la variable "count_conn_tryes" del archivo de configuraci√≥n.  Cada segundo en la sesi√≥n nc, se le dar√° la cadena "REGLA AGREGADA, CONECTAR AHORA" y se comprobar√° la conexi√≥n establecida al puerto "spice_console". <br><br>  Si la conexi√≥n fall√≥ durante el n√∫mero establecido de intentos, el puerto spice_console se transfiere de nuevo a clear.list La ejecuci√≥n de vm_connect.sh se completa, se reanuda la ejecuci√≥n de vm_manager.sh, lo que inicia el ciclo de limpieza. <br><br>  Si Client Station se conecta al puerto spice_console en la interfaz lo0, las reglas del firewall que crean una redirecci√≥n entre el puerto del servidor spice y el puerto spice_console se eliminan y la conexi√≥n se mantiene mediante un mecanismo para determinar el estado del firewall.  En el caso de una conexi√≥n desconectada, la reconexi√≥n al puerto spice_console fallar√°.  El puerto spice_console se transfiere a waste.list, la VM se considera sucia y no puede volver al grupo de m√°quinas virtuales limpias sin pasar por la limpieza.  Se completa la ejecuci√≥n de vm_connect.sh, se reanuda la ejecuci√≥n de vm_manager.sh, lo que inicia el ciclo de limpieza. <br><br>  El ciclo de limpieza comienza observando el archivo waste.list, al que se transfieren los n√∫meros de spice_console de los puertos de la m√°quina virtual a los que se establece la conexi√≥n.  La presencia de una conexi√≥n activa se determina en cada puerto spice_console de la lista.  Si no hay conexi√≥n, se considera que la m√°quina virtual ya no est√° en uso y el puerto se transfiere a recycle.list y se inicia el proceso de eliminaci√≥n de la m√°quina virtual (ver m√°s abajo) a la que pertenec√≠a este puerto.  Si se detecta una conexi√≥n de red activa en el puerto, se supone que se est√° utilizando la m√°quina virtual, no se toman medidas al respecto.  Si no se toca el puerto, se supone que la VM est√° apagada y ya no es necesaria.  El puerto se transfiere a recycle.list y se inicia el proceso de eliminaci√≥n de la m√°quina virtual.  Para hacer esto, se llama al script vm_delete.sh, al que se transfiere el n√∫mero "spice_console" al puerto VM como par√°metro, que debe eliminarse. <br><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_delete.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh #&lt;Set local VARIABLES&gt;# port_to_delete="$1" date=$(/usr/bin/date) #&lt;/Set local VARIABLES&gt;# /usr/bin/echo "# $date START EXECUTE VM_DELETE.SH#" /usr/bin/echo "TRY DELETE VM ON PORT: $vm_port" #&lt;VM NAME SETUP&gt;# vm_name_part1=$(/usr/bin/cat /etc/vm_manager.conf |/usr/bin/grep 'base_host' \ |/usr/bin/cut -d'=' -f2) vm_name=$(/usr/bin/echo "$vm_name_part1""-""$port_to_delete") #&lt;/VM NAME SETUP&gt;# #&lt;SHUTDOWN AND DELETE VM&gt;# /usr/bin/virsh destroy $vm_name /usr/bin/virsh undefine $vm_name /usr/bin/rm -f /var/lib/libvirt/images_write/$vm_name.qcow2 /usr/bin/sed -i "/$port_to_delete/d" $SRV_TMP_DIR/recycle.list #&lt;/SHUTDOWN AND DELETE VM&gt;# /usr/bin/echo "VM ON PORT $vm_port HAS BEEN DELETE AND REMOVE" \ "FROM RECYCLE.LIST. EXIT FROM VM_DELETE.SH" /usr/bin/echo "# $date STOP EXECUTE VM_DELETE.SH#" exit</span></span></code> </pre><br></div></div><br>  Eliminar una m√°quina virtual es una operaci√≥n bastante trivial, el script vm_delete.sh determina el nombre de la m√°quina virtual que posee el puerto pasado como par√°metro de inicio.  La VM se ve obligada a detenerse, la VM se elimina del hipervisor y el disco duro virtual de esta VM se elimina.  El puerto spice_console se elimina de recycle.list.  Finaliza la ejecuci√≥n de vm_delete.sh, se reanuda la ejecuci√≥n de vm_manager.sh <br><br>  El script vm_manager.sh, al final de las operaciones para limpiar m√°quinas virtuales innecesarias de la lista waste.list, inicia el ciclo de creaci√≥n de m√°quinas virtuales en el grupo. <br><br>  El proceso comienza con la determinaci√≥n de los puertos spice_console disponibles para el alojamiento.  Para hacer esto, seg√∫n el par√°metro del archivo de configuraci√≥n "srv_start_port_pool" que establece el puerto de inicio para el grupo "spice_console" de m√°quinas virtuales y el par√°metro "srv_pool_size", que determina el l√≠mite en el n√∫mero de m√°quinas virtuales, todas las variantes de puerto posibles se ordenan secuencialmente.  Para cada puerto espec√≠fico, se busca en clear.list, waste.list, conn_wait.list, recycle.list.  Si se encuentra un puerto en cualquiera de estos archivos, el puerto se considera ocupado y se omite.  Si el puerto no se encuentra en los archivos especificados, se ingresa en el archivo recycle.list y comienza el proceso de creaci√≥n de una nueva m√°quina virtual.  Para hacer esto, se llama al script vm_create.sh al que se pasa como par√°metro el n√∫mero spice_console del puerto para el que desea crear una VM. <br><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_create.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh /usr/bin/echo "#" "$date" "START RUNNING VM_CREATE.SH#" new_vm_port=$1 date=$(/usr/bin/date) a=0 /usr/bin/echo SRV_TMP_DIR=$SRV_TMP_DIR #&lt;SET LOCAL VARIABLES FOR SCRIPT&gt;# base_host=$(/usr/bin/cat /etc/vm_manager.conf |/usr/bin/grep "base_host" \ |/usr/bin/cut -d "=" -f2) /usr/bin/echo "base_host=$base_host" #&lt;/SET LOCAL VARIABLES FOR SCRIPT&gt;# hdd_image_locate() { /bin/echo "Run STEP 1 - hdd_image_locate" hdd_base_image=$(/usr/bin/virsh dumpxml $base_host \ |/usr/bin/grep "source file" |/usr/bin/grep "qcow2" |/usr/bin/head -n 1 \ |/usr/bin/cut -d "'" -f2) if [ -z "$hdd_base_image" ] then /bin/echo "base hdd image not found!" else /usr/bin/echo "hdd_base_image found is a $hdd_base_image. Run next step 2" #&lt; CHECK FOR SNAPSHOT ON BASE HDD &gt;# if [ 0 -eq `/usr/bin/qemu-img info "$hdd_base_image" | /usr/bin/grep -c "Snapshot"` ] then /usr/bin/echo "base image haven't snapshot, run NEXT STEP 3" else /usr/bin/echo "base hdd image have a snapshot, can't use this image" exit fi #&lt;/ CHECK FOR SNAPSHOT ON BASE HDD &gt;# #&lt; CHECK FOR HDD IMAGE IS LINK CLONE &gt;# if [ 0 -eq `/usr/bin/qemu-img info "$hdd_base_image" |/usr/bin/grep -c "backing file" then /usr/bin/echo "base image is not a linked clone, NEXT STEP 4" /usr/bin/echo "Base image check complete!" else /usr/bin/echo "base hdd image is a linked clone, can't use this image" exit fi fi #&lt;/ CHECK FOR HDD IMAGE IS LINK CLONE &gt;# cloning } cloning() { # &lt;Step_1 turn the base VM off &gt;# /usr/bin/virsh shutdown $base_host &gt; /dev/null 2&gt;&amp;1 # &lt;/Step_1 turn the base VM off &gt;# #&lt;Create_vm_config&gt;# /usr/bin/echo "Free port for Spice VM is $new_vm_port" #&lt;Setup_name_for_new_VM&gt;# new_vm_name=$(/bin/echo $base_host"-"$new_vm_port) #&lt;/Setup_name_for_new_VM&gt;# #&lt;Make_base_config_as_clone_base_VM&gt;# /usr/bin/virsh dumpxml $base_host &gt; $SRV_TMP_DIR/$new_vm_name.xml #&lt;Make_base_config_as_clone_base_VM&gt;# ##&lt;Setup_New_VM_Name_in_config&gt;## /usr/bin/sed -i "s%&lt;name&gt;$base_host&lt;/name&gt;%&lt;name&gt;$new_vm_name&lt;/name&gt;%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;/Setup_New_VM_Name_in_config&gt;# #&lt;UUID Changing&gt;# old_uuid=$(/usr/bin/cat $SRV_TMP_DIR/$new_vm_name.xml |/usr/bin/grep "&lt;uuid&gt;") /usr/bin/echo old UUID $old_uuid new_uuid_part1=$(/usr/bin/echo "$old_uuid" |/usr/bin/cut -d "-" -f 1,2) new_uuid_part2=$(/usr/bin/echo "$old_uuid" |/usr/bin/cut -d "-" -f 4,5) new_uuid=$(/bin/echo $new_uuid_part1"-"$new_vm_port"-"$new_uuid_part2) /usr/bin/echo $new_uuid /usr/bin/sed -i "s%$old_uuid%$new_uuid%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;/UUID Changing&gt;# #&lt;Spice port replace&gt;# old_spice_port=$(/usr/bin/cat $SRV_TMP_DIR/$new_vm_name.xml \ |/usr/bin/grep "graphics type='spice' port=") /bin/echo old spice port $old_spice_port new_spice_port=$(/usr/bin/echo "&lt;graphics type='spice' port='$new_vm_port' autoport='no' listen='127.0.0.1'&gt;") /bin/echo $new_spice_port /usr/bin/sed -i "s%$old_spice_port%$new_spice_port%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;/Spice port replace&gt;# #&lt;MAC_ADDR_GENERATE&gt;# mac_new=$(/usr/bin/hexdump -n6 -e '/1 ":%02X"' /dev/random|/usr/bin/sed s/^://g) /usr/bin/echo New Mac is $mac_new #&lt;/MAC_ADDR_GENERATE&gt;# #&lt;GET OLD MAC AND REPLACE&gt;# mac_old=$(/usr/bin/cat $SRV_TMP_DIR/$new_vm_name.xml |/usr/bin/grep "mac address=") /usr/bin/echo old mac is $mac_old /usr/bin/sed -i "s%$mac_old%$mac_new%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;GET OLD MAC AND REPLACE&gt;# #&lt;new_disk_create&gt;# /usr/bin/qemu-img create -f qcow2 -b $hdd_base_image /var/lib/libvirt/images_write/$new_vm_name.qcow2 #&lt;/new_disk_create&gt;# #&lt;attach_new_disk_in_confiig&gt;# /usr/bin/echo hdd base image is $hdd_base_image /usr/bin/sed -i "s%&lt;source file='$hdd_base_image'/&gt;%&lt;source file='/var/lib/libvirt/images_write/$new_vm_name.qcow2'/&gt;%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;/attach_new_disk_in_confiig&gt;# starting_vm #&lt;/Create_vm config&gt;# } starting_vm() { /usr/bin/virsh define $SRV_TMP_DIR/$new_vm_name.xml /usr/bin/virsh start $new_vm_name while [ $a -ne 1 ] do if /usr/bin/virsh list --all |/usr/bin/grep "$new_vm_name" |/usr/bin/grep "running" &gt; /dev/null 2&gt;&amp;1 then a=1 /usr/bin/sed -i "/$new_vm_port/d" $SRV_TMP_DIR/recycle.list /usr/bin/echo $new_vm_port &gt;&gt; $SRV_TMP_DIR/clear.list /usr/bin/echo "#" "$date" "VM $new_vm_name IS STARTED #" else /usr/bin/echo "#VM $new_vm_name is not ready#" a=0 /usr/bin/sleep 2s fi done /usr/bin/echo "#$date EXIT FROM VM_CREATE.SH#" exit } hdd_image_locate</span></span></code> </pre><br></div></div><br>  El proceso de crear una nueva m√°quina virtual <br><br>  El script vm_create.sh lee del archivo de configuraci√≥n el valor de la variable "base_host" que determina la m√°quina virtual de muestra en funci√≥n de la cual se realizar√° el clon.  Descarga la configuraci√≥n xml de la VM de la base de datos del hipervisor, realiza una serie de comprobaciones qcow de la imagen de disco de la VM y, una vez finalizado con √©xito, crea el archivo de configuraci√≥n xml para la nueva VM y la imagen de disco de "clon vinculado" de la nueva VM.  Despu√©s de eso, la configuraci√≥n xml de la nueva VM se carga en la base de datos del hipervisor y se inicia la VM.  El puerto spice_console se transfiere de recycle.list a clear.list.  La ejecuci√≥n de vm_create.sh finaliza y la ejecuci√≥n de vm_manager.sh finaliza. <br>  La pr√≥xima vez que te conectes, comienza desde el principio. <br><br>  Para casos de emergencia, el kit incluye un script vm_clear.sh que ejecuta por la fuerza todas las m√°quinas virtuales del grupo y las elimina al poner a cero los valores de las listas.  Llamarlo en la etapa de carga le permite iniciar (bajo) VDI desde cero. <br><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_clear.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/sh #set VARIABLES# SRV_SCRIPTS_DIR=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_scripts_dir" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_SCRIPTS_DIR=$SRV_SCRIPTS_DIR" export SRV_SCRIPTS_DIR=$SRV_SCRIPTS_DIR SRV_TMP_DIR=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_tmp_dir" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_TMP_DIR=$SRV_TMP_DIR" export SRV_TMP_DIR=$SRV_TMP_DIR SRV_POOL_SIZE=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_pool_size" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_POOL_SIZE=$SRV_POOL_SIZE" SRV_START_PORT_POOL=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_start_port_pool" |/usr/bin/cut -d "=" -f2) /usr/bin/echo SRV_START_PORT_POOL=$SRV_START_PORT_POOL #Set VARIABLES# /usr/bin/echo "= Cleanup ALL VM=" /usr/bin/mkdir $SRV_TMP_DIR /usr/sbin/service iptables restart /usr/bin/cat /dev/null &gt; $SRV_TMP_DIR/clear.list /usr/bin/cat /dev/null &gt; $SRV_TMP_DIR/waste.list /usr/bin/cat /dev/null &gt; $SRV_TMP_DIR/recycle.list /usr/bin/cat /dev/null &gt; $SRV_TMP_DIR/conn_wait.list port_to_delete=$(($SRV_START_PORT_POOL+$SRV_POOL_SIZE)) while [ "$port_to_delete" -gt "$SRV_START_PORT_POOL" ] do $SRV_SCRIPTS_DIR/vm_delete.sh $port_to_delete port_to_delete=$(($port_to_delete-1)) done /usr/bin/echo "= EXIT FROM VM_CLEAR.SH="</span></span></code> </pre><br></div></div><br>  Sobre esto me gustar√≠a terminar la primera parte de mi historia.  Lo anterior deber√≠a ser suficiente para que los administradores de sistemas prueben bajo VDI en los negocios.  Si la comunidad encuentra este tema interesante, en la segunda parte hablar√© sobre la modificaci√≥n de livecd Fedora y su transformaci√≥n en un quiosco. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/462203/">https://habr.com/ru/post/462203/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../462181/index.html">Reglas para una comunicaci√≥n efectiva en chats grupales</a></li>
<li><a href="../462185/index.html">La revoluci√≥n ha terminado. ¬øExiste una alternativa a una bater√≠a de iones de litio?</a></li>
<li><a href="../462189/index.html">Grabado de datos con travajs</a></li>
<li><a href="../462191/index.html">DataArt Museum: un recorrido por el norte de Italia</a></li>
<li><a href="../462197/index.html">Consejos sobre c√≥mo liberar tu mente y aumentar tu creatividad</a></li>
<li><a href="../462205/index.html">PHDays 9 ganadores The Standoff: la cr√≥nica del equipo True0xA3</a></li>
<li><a href="../462209/index.html">Soluciones de videoconferencia de Polycom. Recuerdos 6 a√±os despu√©s ... Etapa 2. Parte 1. RMX1500</a></li>
<li><a href="../462213/index.html">Aprender y trabajar: la experiencia de los estudiantes universitarios en la facultad de tecnolog√≠a de la informaci√≥n y programaci√≥n.</a></li>
<li><a href="../462221/index.html">Qu√© decepcionado estoy en Google Play</a></li>
<li><a href="../462227/index.html">Mosc√∫, 9 de agosto - Backend Stories 4.0</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>