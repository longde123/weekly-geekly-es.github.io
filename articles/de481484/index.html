<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👉🏿 👡 👩🏼‍🤝‍👨🏻 Die KI, die versucht, Probleme zu vermeiden, lernte komplexes Verhalten 👪 🛌🏼 👨🏽‍🤝‍👨🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Reinforcement Learning nutzt häufig Neugier als Motivation für KI. Ihn zwingen, neue Empfindungen zu suchen und die Welt zu erforschen. Aber das Leben...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Die KI, die versucht, Probleme zu vermeiden, lernte komplexes Verhalten</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/481484/"><p><img src="https://habrastorage.org/webt/id/qs/te/idqste6ktiyfnkpzihpvgcnfhmq.png"></p><br><p>  Reinforcement Learning nutzt häufig Neugier als Motivation für KI.  Ihn zwingen, neue Empfindungen zu suchen und die Welt zu erforschen.  Aber das Leben ist voller unangenehmer Überraschungen.  Sie können von einer Klippe fallen und aus der Sicht der Neugier wird es immer sehr neue und interessante Empfindungen sein.  Aber offensichtlich nicht das, wonach man streben soll. </p><br><p>  Die Entwickler aus Berkeley stellten die Aufgabe für den virtuellen Agenten auf den Kopf: Nicht die Neugier machte die Hauptmotivation aus, sondern der Wunsch, Neuheiten auf alle Fälle zu vermeiden.  Aber "nichts tun" war schwieriger als es sich anhört.  In einer sich ständig verändernden Umgebung musste die KI komplexes Verhalten lernen, um neue Empfindungen zu vermeiden. </p><a name="habracut"></a><br><p>  Reinforcement Learning unternimmt zaghafte Schritte, um eine starke KI aufzubauen.  Und während alles auf sehr geringe Dimensionen beschränkt ist, erscheinen buchstäblich die Einheiten, in denen der virtuelle Agent (vorzugsweise vernünftigerweise) agieren muss, von Zeit zu Zeit neue Ideen, wie das Training der künstlichen Intelligenz verbessert werden kann. </p><br><p>  Aber nicht nur Lernalgorithmen sind kompliziert.  Die Umwelt wird auch schwieriger.  Die meisten Bestärkungslernumgebungen sind sehr einfach und motivieren den Agenten, die Welt zu erkunden.  Es kann ein Labyrinth sein, das vollständig umgangen werden muss, um einen Ausweg zu finden, oder ein Computerspiel, das bis zum Ende abgeschlossen sein muss. </p><br><p>  Aber auf lange Sicht streben Lebewesen (vernünftig und nicht so) nicht nur danach, die Welt um sie herum zu erkunden.  Aber auch, um all das Gute zu bewahren, das in ihrem kurzen (oder nicht so kurzen) Leben steckt. </p><br><p>  Dies wird Homöostase genannt - der Wunsch des Körpers, einen konstanten Zustand aufrechtzuerhalten.  In der einen oder anderen Form ist dies allen Lebewesen gemeinsam.  <a href="https://bair.berkeley.edu/blog/2019/12/18/smirl/" rel="nofollow">Entwickler aus Berkeley</a> geben ein so merkwürdiges Beispiel: Alle Errungenschaften der Menschheit sollen im Großen und Ganzen vor unangenehmen Überraschungen schützen.  Zum Schutz vor einer immer größer werdenden Entropie der Umwelt.  Wir bauen Häuser, in denen wir eine konstante Temperatur aufrechterhalten, die vor Wetteränderungen geschützt ist.  Wir benutzen Medizin, um ständig gesund zu sein und so weiter. </p><br><p>  Man kann damit streiten, aber in dieser Analogie steckt wirklich etwas. </p><br><p>  Die Jungs stellten die Frage: Was passiert, wenn die Hauptmotivation für die KI darin besteht, Neuheiten zu vermeiden?  Minimieren Sie mit anderen Worten das Chaos als objektive Lernfunktion. </p><br><p>  Und sie haben den Agenten in eine sich ständig verändernde gefährliche Welt gebracht. </p><br><p><img src="https://habrastorage.org/webt/ya/iy/d3/yaiyd3jxw0gnbaywrqyh4deyi8u.gif"></p><br><p><img src="https://habrastorage.org/webt/qc/ss/am/qcssam7pbjxhrwx4uhpzlqm6xq4.gif"></p><br><p>  Die Ergebnisse waren interessant.  In vielen Fällen hat ein solches Lernen das lehrplanbasierte Lernen übertroffen und kommt dem Lernen mit einem Lehrer in Bezug auf die Qualität meistens nahe.  Das heißt, zu spezialisiertem Training, um ein bestimmtes Ziel zu erreichen - um das Spiel zu gewinnen, gehe durch das Labyrinth. </p><br><p>  Das ist natürlich logisch, denn wenn Sie auf einer einstürzenden Brücke stehen, müssen Sie sich ständig von der Kante entfernen, um weiterhin auf ihr zu stehen (um die Konstanz aufrechtzuerhalten und neue Sturzgefühle zu vermeiden).  Lauf mit aller Kraft davon, um still zu stehen, wie Alice sagte. </p><br><p><img src="https://habrastorage.org/webt/vl/p2/h1/vlp2h1png42kowbspxvoxald2gc.gif"></p><br><p>  Tatsächlich gibt es in jedem Algorithmus zum Lernen der Verstärkung einen solchen Moment.  Weil der Tod im Spiel und das schnelle Ende der Episode mit einer negativen Belohnung bestraft werden.  Oder, abhängig vom Algorithmus, indem Sie die maximale Belohnung reduzieren, die ein Agent erhalten könnte, wenn er nicht kontinuierlich von der Klippe fällt. </p><br><p><img src="https://habrastorage.org/webt/7h/le/ni/7hleniwgnqd_wu6deeihk078ptk.gif"></p><br><p>  Aber in einem solchen Umfeld, in dem die KI keine anderen Ziele verfolgt als den Wunsch, Neuheiten zu vermeiden, scheint es, als würde sie zum ersten Mal beim verstärkten Lernen eingesetzt. </p><br><p>  Interessanterweise lernte der virtuelle Agent mit dieser Motivation, viele Spiele zu spielen, die ein Ziel haben, um zu gewinnen.  Zum Beispiel Tetris. </p><br><p><img src="https://habrastorage.org/webt/a0/7w/at/a07watp9unf5zcgbcojewuv_7wy.gif"></p><br><p>  Oder die Umgebung von Doom, in der Sie fliegenden Feuerbällen ausweichen und auf sich nähernde Gegner schießen müssen.  Weil viele Aufgaben als Aufgaben zur Aufrechterhaltung der Konstanz formuliert werden können.  Für Tetris ist dies der Wunsch, das Feld leer zu halten.  Füllt sich der Bildschirm ständig?  Oh je, was wird passieren, wenn es bis zum Ende gefüllt ist?  Nein, nein, wir brauchen kein solches Glück.  Zu viel Schock. </p><br><p>  Von der technischen Seite ist es ganz einfach angeordnet.  Wenn ein Agent einen neuen Status erhält, bewertet er, wie vertraut dieser Status ist.  Das heißt, wie viel der neue Staat in der Verteilung des Staates enthalten ist, den er zuvor besucht hat.  Der Agent wird umso vertrauter, je höher die Belohnung ist.  Und die Aufgabe der Lernpolitik (all das sind die Begriffe aus dem Reinforcement Learning, wenn jemand sie nicht kennt) besteht darin, Maßnahmen zu wählen, die zum Übergang in den vertrautesten Zustand führen würden.  Außerdem wird jeder neue Status verwendet, um die Statistiken bekannter Status zu aktualisieren, mit denen neue Status verglichen werden. </p><br><p>  Interessanterweise lernte ich im Verlauf der KI spontan zu verstehen, dass neue Zustände das beeinflussen, was als Neuheit angesehen wird.  Und dass Sie vertraute Zustände auf zwei Arten erreichen können: entweder in einen bereits bekannten Zustand.  Oder gehen Sie in einen Zustand, der <em>das</em> Konzept der Persistenz / Vertrautheit der Umgebung <em>aktualisiert</em> , und der Agent wird in einem neuen, durch seine Handlungen gebildeten, vertrauten Zustand sein. </p><br><p>  Dies zwingt den Agenten, komplexe koordinierte Aktionen durchzuführen, wenn auch nur, um nichts im Leben zu tun. </p><br><p>  Paradoxerweise führt dies zu einem Analogon der Neugierde des gewöhnlichen Lernens und zwingt den Agenten, die Welt um ihn herum zu erkunden.  Plötzlich gibt es irgendwo einen Ort, der noch sicherer ist als hier und jetzt?  Dort können Sie sich vollkommen der Faulheit hingeben und absolut nichts tun, um Probleme und neue Empfindungen zu vermeiden.  Es wäre keine Übertreibung zu sagen, dass solche Gedanken wahrscheinlich jedem von uns einfielen.  Und für viele ist dies eine echte treibende Kraft im Leben.  Obwohl im wirklichen Leben keiner von uns damit zu tun hatte, dass Tetris bis zum Rand aufgefüllt wurde, war dies natürlich nicht der Fall. </p><br><p>  Um ehrlich zu sein, ist dies eine komplizierte Geschichte.  Aber die Praxis zeigt, dass es funktioniert.  Die Forscher verglichen diesen Algorithmus mit den neugierigsten Vertretern: <a href="https://pathak22.github.io/noreward-rl/" rel="nofollow">ICM</a> und <a href="https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/" rel="nofollow">RND</a> .  Der erste ist ein wirksamer Mechanismus der Neugier, der beim Lernen mit Verstärkung bereits zum Klassiker geworden ist.  Der Agent strebt nicht einfach nach neuen ungewohnten und damit interessanten Zuständen.  Die Unbekanntheit der Situation in solchen Algorithmen wird dadurch abgeschätzt, ob der Agent sie vorhersagen kann (in den früheren gab es buchstäblich Zähler für besuchte Zustände, aber jetzt ist alles auf die integrale Schätzung hinausgegangen, die das neuronale Netzwerk liefert).  Aber in diesem Fall hätten die sich bewegenden Blätter auf den Bäumen oder das weiße Rauschen im Fernsehen für einen solchen Agenten eine unendliche Neuheit und ein unendliches Gefühl der Neugierde hervorgerufen.  Weil er niemals alle möglichen neuen Zustände in einer völlig zufälligen Umgebung vorhersagen kann. </p><br><p>  Daher sucht ein Agent in ICM nur nach den neuen Zuständen, die er mit seinen Aktionen beeinflussen kann.  Kann AI weißes Rauschen im Fernsehen beeinflussen?  Nein.  So uninteressant.  Und kann es den Ball beeinflussen, wenn Sie ihn bewegen?  Ja  Es ist also interessant, mit dem Ball zu spielen.  Dazu verwendet ICM eine sehr coole Idee mit dem Inverse Model, mit dem das Forward Model verglichen wird.  Weitere Details in der <a href="https://pathak22.github.io/noreward-rl/" rel="nofollow">Originalarbeit</a> . </p><br><p>  RND ist eine neuere Entwicklung des Neugiermechanismus.  Was in der Praxis ICM übertroffen hat.  Kurz gesagt, das neuronale Netzwerk versucht, die Ausgaben eines anderen neuronalen Netzwerks vorherzusagen, das durch zufällige Gewichte initiiert wird und sich nie ändert.  Es wird angenommen, dass das aktuelle neuronale Netzwerk umso häufiger in der Lage ist, zufällig initiierte Ausgaben vorherzusagen, je vertrauter die Situation ist (die dem Eingang beider neuronaler Netze zugeführt wird, aktuell und zufällig initiiert).  Ich weiß nicht, wer das alles erfindet.  Einerseits möchte ich einer solchen Person die Hand geben und andererseits einen Kick für solche Verzerrungen geben. </p><br><p>  Aber auf die eine oder andere Art und Weise und mit dem Gedanken der Aufrechterhaltung der Homöostase und dem Versuch, Neuheiten zu vermeiden, wurde in der Praxis in vielen Fällen ein besseres Endergebnis erzielt als mit einem Lehrplan, der auf ICN oder RND basiert.  Was spiegelt sich in den Grafiken wider? </p><br><p><img src="https://habrastorage.org/webt/p0/uw/uc/p0uwucutpngtfjp5zunetzqbf0k.png"></p><br><p>  Hier muss jedoch klargestellt werden, dass dies nur für die Umgebungen gilt, die die Forscher in ihrer Arbeit verwendet haben.  Sie sind gefährlich, zufällig, laut und mit zunehmender Entropie.  Es kann wirklich rentabler sein, nichts in ihnen zu tun.  Und nur gelegentlich bewegt es sich aktiv, wenn ein Feuerball in Ihnen fliegt oder die Brücke hinter Ihnen zusammenbricht.  Forscher aus Berkeley bestehen jedoch offenbar aufgrund ihrer schwierigen Lebenserfahrung darauf, dass solche Umgebungen dem komplexen wirklichen Leben viel näher kommen als bisher im Verstärkungstraining.  Nun, ich weiß es nicht, ich weiß es nicht.  In meinem Leben werden Feuerbälle von Monstern, die in mich hineinfliegen, und unbewohnte Labyrinthe mit einem einzigen Ausgang mit ungefähr der gleichen Häufigkeit gefunden.  Es ist jedoch nicht zu leugnen, dass der vorgeschlagene Ansatz trotz seiner Einfachheit erstaunliche Ergebnisse erbracht hat.  Vielleicht sollten in Zukunft beide Ansätze sinnvoll kombiniert werden - Homöostase mit Erhalt der langfristigen positiven Konstanz und Neugier für aktuelle Umweltstudien. </p><br><p>  <a href="https://bair.berkeley.edu/blog/2019/12/18/smirl/" rel="nofollow">Link zur Originalarbeit</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de481484/">https://habr.com/ru/post/de481484/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de481474/index.html">Wir sind von einem anderen Test - wir testen die Datenbank auf MSTest</a></li>
<li><a href="../de481476/index.html">Wie ich anfing auf Konferenzen zu sprechen und nicht aufhören kann</a></li>
<li><a href="../de481478/index.html">STM32 + CMSIS + STM32CubeIDE</a></li>
<li><a href="../de481480/index.html">Dies ist die Norm: Was sind normale Karten und wie funktionieren sie?</a></li>
<li><a href="../de481482/index.html">Cross-Posting auf eine Facebook-Seite mit dem PHP-SDK</a></li>
<li><a href="../de481486/index.html">„Am Leben bleiben, am Leben bleiben“: Das neue Protokoll wird den Radius der möglichen Nutzung von WLAN um 60 Meter vergrößern</a></li>
<li><a href="../de481488/index.html">Wie die Beamten der Region Moskau die Luftverschmutzung messen</a></li>
<li><a href="../de481490/index.html">Vitamin D. Ein kurzer Ausflug</a></li>
<li><a href="../de481492/index.html">Wir sammeln Farbmusik für das neue Jahr</a></li>
<li><a href="../de481496/index.html">DDoS-Angriff durch Social Engineering</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>