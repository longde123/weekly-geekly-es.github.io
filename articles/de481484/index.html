<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘‰ğŸ¿ ğŸ‘¡ ğŸ‘©ğŸ¼â€ğŸ¤â€ğŸ‘¨ğŸ» Die KI, die versucht, Probleme zu vermeiden, lernte komplexes Verhalten ğŸ‘ª ğŸ›ŒğŸ¼ ğŸ‘¨ğŸ½â€ğŸ¤â€ğŸ‘¨ğŸ¼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Reinforcement Learning nutzt hÃ¤ufig Neugier als Motivation fÃ¼r KI. Ihn zwingen, neue Empfindungen zu suchen und die Welt zu erforschen. Aber das Leben...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Die KI, die versucht, Probleme zu vermeiden, lernte komplexes Verhalten</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/481484/"><p><img src="https://habrastorage.org/webt/id/qs/te/idqste6ktiyfnkpzihpvgcnfhmq.png"></p><br><p>  Reinforcement Learning nutzt hÃ¤ufig Neugier als Motivation fÃ¼r KI.  Ihn zwingen, neue Empfindungen zu suchen und die Welt zu erforschen.  Aber das Leben ist voller unangenehmer Ãœberraschungen.  Sie kÃ¶nnen von einer Klippe fallen und aus der Sicht der Neugier wird es immer sehr neue und interessante Empfindungen sein.  Aber offensichtlich nicht das, wonach man streben soll. </p><br><p>  Die Entwickler aus Berkeley stellten die Aufgabe fÃ¼r den virtuellen Agenten auf den Kopf: Nicht die Neugier machte die Hauptmotivation aus, sondern der Wunsch, Neuheiten auf alle FÃ¤lle zu vermeiden.  Aber "nichts tun" war schwieriger als es sich anhÃ¶rt.  In einer sich stÃ¤ndig verÃ¤ndernden Umgebung musste die KI komplexes Verhalten lernen, um neue Empfindungen zu vermeiden. </p><a name="habracut"></a><br><p>  Reinforcement Learning unternimmt zaghafte Schritte, um eine starke KI aufzubauen.  Und wÃ¤hrend alles auf sehr geringe Dimensionen beschrÃ¤nkt ist, erscheinen buchstÃ¤blich die Einheiten, in denen der virtuelle Agent (vorzugsweise vernÃ¼nftigerweise) agieren muss, von Zeit zu Zeit neue Ideen, wie das Training der kÃ¼nstlichen Intelligenz verbessert werden kann. </p><br><p>  Aber nicht nur Lernalgorithmen sind kompliziert.  Die Umwelt wird auch schwieriger.  Die meisten BestÃ¤rkungslernumgebungen sind sehr einfach und motivieren den Agenten, die Welt zu erkunden.  Es kann ein Labyrinth sein, das vollstÃ¤ndig umgangen werden muss, um einen Ausweg zu finden, oder ein Computerspiel, das bis zum Ende abgeschlossen sein muss. </p><br><p>  Aber auf lange Sicht streben Lebewesen (vernÃ¼nftig und nicht so) nicht nur danach, die Welt um sie herum zu erkunden.  Aber auch, um all das Gute zu bewahren, das in ihrem kurzen (oder nicht so kurzen) Leben steckt. </p><br><p>  Dies wird HomÃ¶ostase genannt - der Wunsch des KÃ¶rpers, einen konstanten Zustand aufrechtzuerhalten.  In der einen oder anderen Form ist dies allen Lebewesen gemeinsam.  <a href="https://bair.berkeley.edu/blog/2019/12/18/smirl/" rel="nofollow">Entwickler aus Berkeley</a> geben ein so merkwÃ¼rdiges Beispiel: Alle Errungenschaften der Menschheit sollen im GroÃŸen und Ganzen vor unangenehmen Ãœberraschungen schÃ¼tzen.  Zum Schutz vor einer immer grÃ¶ÃŸer werdenden Entropie der Umwelt.  Wir bauen HÃ¤user, in denen wir eine konstante Temperatur aufrechterhalten, die vor WetterÃ¤nderungen geschÃ¼tzt ist.  Wir benutzen Medizin, um stÃ¤ndig gesund zu sein und so weiter. </p><br><p>  Man kann damit streiten, aber in dieser Analogie steckt wirklich etwas. </p><br><p>  Die Jungs stellten die Frage: Was passiert, wenn die Hauptmotivation fÃ¼r die KI darin besteht, Neuheiten zu vermeiden?  Minimieren Sie mit anderen Worten das Chaos als objektive Lernfunktion. </p><br><p>  Und sie haben den Agenten in eine sich stÃ¤ndig verÃ¤ndernde gefÃ¤hrliche Welt gebracht. </p><br><p><img src="https://habrastorage.org/webt/ya/iy/d3/yaiyd3jxw0gnbaywrqyh4deyi8u.gif"></p><br><p><img src="https://habrastorage.org/webt/qc/ss/am/qcssam7pbjxhrwx4uhpzlqm6xq4.gif"></p><br><p>  Die Ergebnisse waren interessant.  In vielen FÃ¤llen hat ein solches Lernen das lehrplanbasierte Lernen Ã¼bertroffen und kommt dem Lernen mit einem Lehrer in Bezug auf die QualitÃ¤t meistens nahe.  Das heiÃŸt, zu spezialisiertem Training, um ein bestimmtes Ziel zu erreichen - um das Spiel zu gewinnen, gehe durch das Labyrinth. </p><br><p>  Das ist natÃ¼rlich logisch, denn wenn Sie auf einer einstÃ¼rzenden BrÃ¼cke stehen, mÃ¼ssen Sie sich stÃ¤ndig von der Kante entfernen, um weiterhin auf ihr zu stehen (um die Konstanz aufrechtzuerhalten und neue SturzgefÃ¼hle zu vermeiden).  Lauf mit aller Kraft davon, um still zu stehen, wie Alice sagte. </p><br><p><img src="https://habrastorage.org/webt/vl/p2/h1/vlp2h1png42kowbspxvoxald2gc.gif"></p><br><p>  TatsÃ¤chlich gibt es in jedem Algorithmus zum Lernen der VerstÃ¤rkung einen solchen Moment.  Weil der Tod im Spiel und das schnelle Ende der Episode mit einer negativen Belohnung bestraft werden.  Oder, abhÃ¤ngig vom Algorithmus, indem Sie die maximale Belohnung reduzieren, die ein Agent erhalten kÃ¶nnte, wenn er nicht kontinuierlich von der Klippe fÃ¤llt. </p><br><p><img src="https://habrastorage.org/webt/7h/le/ni/7hleniwgnqd_wu6deeihk078ptk.gif"></p><br><p>  Aber in einem solchen Umfeld, in dem die KI keine anderen Ziele verfolgt als den Wunsch, Neuheiten zu vermeiden, scheint es, als wÃ¼rde sie zum ersten Mal beim verstÃ¤rkten Lernen eingesetzt. </p><br><p>  Interessanterweise lernte der virtuelle Agent mit dieser Motivation, viele Spiele zu spielen, die ein Ziel haben, um zu gewinnen.  Zum Beispiel Tetris. </p><br><p><img src="https://habrastorage.org/webt/a0/7w/at/a07watp9unf5zcgbcojewuv_7wy.gif"></p><br><p>  Oder die Umgebung von Doom, in der Sie fliegenden FeuerbÃ¤llen ausweichen und auf sich nÃ¤hernde Gegner schieÃŸen mÃ¼ssen.  Weil viele Aufgaben als Aufgaben zur Aufrechterhaltung der Konstanz formuliert werden kÃ¶nnen.  FÃ¼r Tetris ist dies der Wunsch, das Feld leer zu halten.  FÃ¼llt sich der Bildschirm stÃ¤ndig?  Oh je, was wird passieren, wenn es bis zum Ende gefÃ¼llt ist?  Nein, nein, wir brauchen kein solches GlÃ¼ck.  Zu viel Schock. </p><br><p>  Von der technischen Seite ist es ganz einfach angeordnet.  Wenn ein Agent einen neuen Status erhÃ¤lt, bewertet er, wie vertraut dieser Status ist.  Das heiÃŸt, wie viel der neue Staat in der Verteilung des Staates enthalten ist, den er zuvor besucht hat.  Der Agent wird umso vertrauter, je hÃ¶her die Belohnung ist.  Und die Aufgabe der Lernpolitik (all das sind die Begriffe aus dem Reinforcement Learning, wenn jemand sie nicht kennt) besteht darin, MaÃŸnahmen zu wÃ¤hlen, die zum Ãœbergang in den vertrautesten Zustand fÃ¼hren wÃ¼rden.  AuÃŸerdem wird jeder neue Status verwendet, um die Statistiken bekannter Status zu aktualisieren, mit denen neue Status verglichen werden. </p><br><p>  Interessanterweise lernte ich im Verlauf der KI spontan zu verstehen, dass neue ZustÃ¤nde das beeinflussen, was als Neuheit angesehen wird.  Und dass Sie vertraute ZustÃ¤nde auf zwei Arten erreichen kÃ¶nnen: entweder in einen bereits bekannten Zustand.  Oder gehen Sie in einen Zustand, der <em>das</em> Konzept der Persistenz / Vertrautheit der Umgebung <em>aktualisiert</em> , und der Agent wird in einem neuen, durch seine Handlungen gebildeten, vertrauten Zustand sein. </p><br><p>  Dies zwingt den Agenten, komplexe koordinierte Aktionen durchzufÃ¼hren, wenn auch nur, um nichts im Leben zu tun. </p><br><p>  Paradoxerweise fÃ¼hrt dies zu einem Analogon der Neugierde des gewÃ¶hnlichen Lernens und zwingt den Agenten, die Welt um ihn herum zu erkunden.  PlÃ¶tzlich gibt es irgendwo einen Ort, der noch sicherer ist als hier und jetzt?  Dort kÃ¶nnen Sie sich vollkommen der Faulheit hingeben und absolut nichts tun, um Probleme und neue Empfindungen zu vermeiden.  Es wÃ¤re keine Ãœbertreibung zu sagen, dass solche Gedanken wahrscheinlich jedem von uns einfielen.  Und fÃ¼r viele ist dies eine echte treibende Kraft im Leben.  Obwohl im wirklichen Leben keiner von uns damit zu tun hatte, dass Tetris bis zum Rand aufgefÃ¼llt wurde, war dies natÃ¼rlich nicht der Fall. </p><br><p>  Um ehrlich zu sein, ist dies eine komplizierte Geschichte.  Aber die Praxis zeigt, dass es funktioniert.  Die Forscher verglichen diesen Algorithmus mit den neugierigsten Vertretern: <a href="https://pathak22.github.io/noreward-rl/" rel="nofollow">ICM</a> und <a href="https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/" rel="nofollow">RND</a> .  Der erste ist ein wirksamer Mechanismus der Neugier, der beim Lernen mit VerstÃ¤rkung bereits zum Klassiker geworden ist.  Der Agent strebt nicht einfach nach neuen ungewohnten und damit interessanten ZustÃ¤nden.  Die Unbekanntheit der Situation in solchen Algorithmen wird dadurch abgeschÃ¤tzt, ob der Agent sie vorhersagen kann (in den frÃ¼heren gab es buchstÃ¤blich ZÃ¤hler fÃ¼r besuchte ZustÃ¤nde, aber jetzt ist alles auf die integrale SchÃ¤tzung hinausgegangen, die das neuronale Netzwerk liefert).  Aber in diesem Fall hÃ¤tten die sich bewegenden BlÃ¤tter auf den BÃ¤umen oder das weiÃŸe Rauschen im Fernsehen fÃ¼r einen solchen Agenten eine unendliche Neuheit und ein unendliches GefÃ¼hl der Neugierde hervorgerufen.  Weil er niemals alle mÃ¶glichen neuen ZustÃ¤nde in einer vÃ¶llig zufÃ¤lligen Umgebung vorhersagen kann. </p><br><p>  Daher sucht ein Agent in ICM nur nach den neuen ZustÃ¤nden, die er mit seinen Aktionen beeinflussen kann.  Kann AI weiÃŸes Rauschen im Fernsehen beeinflussen?  Nein.  So uninteressant.  Und kann es den Ball beeinflussen, wenn Sie ihn bewegen?  Ja  Es ist also interessant, mit dem Ball zu spielen.  Dazu verwendet ICM eine sehr coole Idee mit dem Inverse Model, mit dem das Forward Model verglichen wird.  Weitere Details in der <a href="https://pathak22.github.io/noreward-rl/" rel="nofollow">Originalarbeit</a> . </p><br><p>  RND ist eine neuere Entwicklung des Neugiermechanismus.  Was in der Praxis ICM Ã¼bertroffen hat.  Kurz gesagt, das neuronale Netzwerk versucht, die Ausgaben eines anderen neuronalen Netzwerks vorherzusagen, das durch zufÃ¤llige Gewichte initiiert wird und sich nie Ã¤ndert.  Es wird angenommen, dass das aktuelle neuronale Netzwerk umso hÃ¤ufiger in der Lage ist, zufÃ¤llig initiierte Ausgaben vorherzusagen, je vertrauter die Situation ist (die dem Eingang beider neuronaler Netze zugefÃ¼hrt wird, aktuell und zufÃ¤llig initiiert).  Ich weiÃŸ nicht, wer das alles erfindet.  Einerseits mÃ¶chte ich einer solchen Person die Hand geben und andererseits einen Kick fÃ¼r solche Verzerrungen geben. </p><br><p>  Aber auf die eine oder andere Art und Weise und mit dem Gedanken der Aufrechterhaltung der HomÃ¶ostase und dem Versuch, Neuheiten zu vermeiden, wurde in der Praxis in vielen FÃ¤llen ein besseres Endergebnis erzielt als mit einem Lehrplan, der auf ICN oder RND basiert.  Was spiegelt sich in den Grafiken wider? </p><br><p><img src="https://habrastorage.org/webt/p0/uw/uc/p0uwucutpngtfjp5zunetzqbf0k.png"></p><br><p>  Hier muss jedoch klargestellt werden, dass dies nur fÃ¼r die Umgebungen gilt, die die Forscher in ihrer Arbeit verwendet haben.  Sie sind gefÃ¤hrlich, zufÃ¤llig, laut und mit zunehmender Entropie.  Es kann wirklich rentabler sein, nichts in ihnen zu tun.  Und nur gelegentlich bewegt es sich aktiv, wenn ein Feuerball in Ihnen fliegt oder die BrÃ¼cke hinter Ihnen zusammenbricht.  Forscher aus Berkeley bestehen jedoch offenbar aufgrund ihrer schwierigen Lebenserfahrung darauf, dass solche Umgebungen dem komplexen wirklichen Leben viel nÃ¤her kommen als bisher im VerstÃ¤rkungstraining.  Nun, ich weiÃŸ es nicht, ich weiÃŸ es nicht.  In meinem Leben werden FeuerbÃ¤lle von Monstern, die in mich hineinfliegen, und unbewohnte Labyrinthe mit einem einzigen Ausgang mit ungefÃ¤hr der gleichen HÃ¤ufigkeit gefunden.  Es ist jedoch nicht zu leugnen, dass der vorgeschlagene Ansatz trotz seiner Einfachheit erstaunliche Ergebnisse erbracht hat.  Vielleicht sollten in Zukunft beide AnsÃ¤tze sinnvoll kombiniert werden - HomÃ¶ostase mit Erhalt der langfristigen positiven Konstanz und Neugier fÃ¼r aktuelle Umweltstudien. </p><br><p>  <a href="https://bair.berkeley.edu/blog/2019/12/18/smirl/" rel="nofollow">Link zur Originalarbeit</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de481484/">https://habr.com/ru/post/de481484/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de481474/index.html">Wir sind von einem anderen Test - wir testen die Datenbank auf MSTest</a></li>
<li><a href="../de481476/index.html">Wie ich anfing auf Konferenzen zu sprechen und nicht aufhÃ¶ren kann</a></li>
<li><a href="../de481478/index.html">STM32 + CMSIS + STM32CubeIDE</a></li>
<li><a href="../de481480/index.html">Dies ist die Norm: Was sind normale Karten und wie funktionieren sie?</a></li>
<li><a href="../de481482/index.html">Cross-Posting auf eine Facebook-Seite mit dem PHP-SDK</a></li>
<li><a href="../de481486/index.html">â€Am Leben bleiben, am Leben bleibenâ€œ: Das neue Protokoll wird den Radius der mÃ¶glichen Nutzung von WLAN um 60 Meter vergrÃ¶ÃŸern</a></li>
<li><a href="../de481488/index.html">Wie die Beamten der Region Moskau die Luftverschmutzung messen</a></li>
<li><a href="../de481490/index.html">Vitamin D. Ein kurzer Ausflug</a></li>
<li><a href="../de481492/index.html">Wir sammeln Farbmusik fÃ¼r das neue Jahr</a></li>
<li><a href="../de481496/index.html">DDoS-Angriff durch Social Engineering</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>