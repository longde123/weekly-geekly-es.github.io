<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§ûüèø ü§öüèæ üìö Wie man eine gute Reparatur von einer schlechten unterscheidet oder wie wir in SRG eine Multithread-Java-Bibliothek aus dem Tomit-Parser erstellt haben üï∫ ü§≤üèø üö∞</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In diesem Artikel wird erl√§utert, wie wir den von Yandex entwickelten Tomita-Parser in unser System integriert, in eine dynamische Bibliothek umgewand...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie man eine gute Reparatur von einer schlechten unterscheidet oder wie wir in SRG eine Multithread-Java-Bibliothek aus dem Tomit-Parser erstellt haben</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/srg/blog/439614/">  In diesem Artikel wird erl√§utert, wie wir den von Yandex entwickelten Tomita-Parser in unser System integriert, in eine dynamische Bibliothek umgewandelt, uns mit Java angefreundet, Multithreading durchgef√ºhrt und damit das Problem der Textklassifizierung f√ºr die Immobilienbewertung gel√∂st haben. <br><br><img src="https://habrastorage.org/webt/vi/om/ub/viomubb37jryhlbwqzqucsl583s.jpeg"><br><a name="habracut"></a><br><h3>  Erkl√§rung des Problems </h3><br>  Bei der Immobilienbewertung ist die Analyse von Verkaufsank√ºndigungen von gro√üer Bedeutung.  Aus der Ank√ºndigung k√∂nnen Sie alle notwendigen Informationen √ºber die Immobilie erhalten, einschlie√ülich Informationen √ºber den Zustand der Reparatur in der Wohnung.  Normalerweise sind diese Informationen im Anzeigentext enthalten.  Dies ist bei der Beurteilung sehr wichtig, da eine gute Reparatur den Quadratmeterpreis um mehrere Tausend erh√∂hen kann. <br><br>  Wir haben also einen Anzeigentext, der je nach Reparaturzustand in der Wohnung in eine der Kategorien eingeteilt werden muss (unvollendet, fair, durchschnittlich, gut, ausgezeichnet, exklusiv).  Bei Reparaturen kann eine Anzeige ein oder zwei S√§tze, ein paar W√∂rter oder nichts enthalten. Daher ist es nicht sinnvoll, den Text vollst√§ndig zu klassifizieren.  Aufgrund der Spezifit√§t des Textes und der begrenzten Anzahl von W√∂rtern im Zusammenhang mit dem Reparaturkontext bestand die einzig sinnvolle L√∂sung darin, alle erforderlichen Informationen aus dem Text zu extrahieren und zu klassifizieren. <br><br><img src="https://habrastorage.org/webt/p9/jr/id/p9jrid1dluodkjquebvvr7yc5dq.png" alt="Bild"><br><br>  Jetzt m√ºssen wir lernen, wie wir aus dem Text alle Fakten √ºber den Zustand der Dekoration extrahieren k√∂nnen.  Insbesondere, was direkt mit der Reparatur zusammenh√§ngt, sowie alles, was indirekt √ºber den Zustand der Wohnung sprechen kann - das Vorhandensein von abgeh√§ngten Decken, Einbauger√§ten, Kunststofffenstern, einem Whirlpool, die Verwendung teurer Veredelungsmaterialien usw. <br><br>  In diesem Fall m√ºssen wir nur Informationen √ºber Reparaturen in der Wohnung selbst extrahieren, da uns der Zustand der Eing√§nge, Keller und Dachb√∂den nicht interessiert.  Es ist auch notwendig zu ber√ºcksichtigen, dass der Text in nat√ºrlicher Sprache mit all seinen inh√§renten Fehlern, Tippfehlern, Abk√ºrzungen und anderen Merkmalen geschrieben ist. Ich pers√∂nlich habe drei Schreibweisen der W√∂rter ‚ÄûLinoleum‚Äú und ‚ÄûLaminat‚Äú und f√ºnf Schreibweisen des Wortes ‚Äûendg√ºltig‚Äú gefunden.  Einige Leute verstehen nicht, warum Leerzeichen zwischen W√∂rtern ben√∂tigt werden, w√§hrend andere nichts von Kommas geh√∂rt haben.  Daher wurde der Parser mit kontextfrei freien Grammatiken zur einfachsten und vern√ºnftigsten L√∂sung. <br><br>  Als die Entscheidung getroffen wurde, wurde eine zweite gro√üe und interessante Aufgabe gebildet - zu lernen, wie alle ausreichenden und notwendigen Informationen √ºber die Reparatur aus der Ank√ºndigung extrahiert werden k√∂nnen, n√§mlich eine schnelle syntaktische und morphologische Analyse des Textes bereitzustellen, die im Bibliotheksmodus unter Last parallel arbeiten kann. <br><br><h3>  Fahren Sie mit der L√∂sung fort </h3><br>  Von den verf√ºgbaren Mitteln zum Extrahieren von Fakten aus Text basierend auf kontextfreien Grammatiken, die mit der russischen Sprache arbeiten k√∂nnen, wurde unsere Aufmerksamkeit auf Tomita-Parser und die Yagry-Bibliothek in Python gelenkt.  Yagry wurde sofort abgelehnt, da es vollst√§ndig in Python geschrieben und kaum gut optimiert ist.  Und Tomita sah anfangs sehr attraktiv aus: Sie hatte eine detaillierte Dokumentation f√ºr den Entwickler und viele Beispiele, C ++ versprach eine akzeptable Geschwindigkeit.  Es war nicht schwer, die Regeln f√ºr das Schreiben von Grammatiken zu verstehen, und die erste Version des Klassifikators mit seiner Verwendung war bereits am n√§chsten Tag fertig. <br><br>  Beispiele f√ºr Regeln aus unserer Grammatik, die Adjektive und Verben im Zusammenhang mit dem Reparaturkontext extrahieren: <br><br><pre><code class="javascript hljs">RepairW -&gt; <span class="hljs-string"><span class="hljs-string">""</span></span> | <span class="hljs-string"><span class="hljs-string">""</span></span> | <span class="hljs-string"><span class="hljs-string">""</span></span>; StopWords -&gt; <span class="hljs-string"><span class="hljs-string">""</span></span> | <span class="hljs-string"><span class="hljs-string">""</span></span> | <span class="hljs-string"><span class="hljs-string">""</span></span> | <span class="hljs-string"><span class="hljs-string">""</span></span>; Repair -&gt; RepairW&lt;gnc-agr[<span class="hljs-number"><span class="hljs-number">1</span></span>]&gt; Adj&lt;gnc-agr[<span class="hljs-number"><span class="hljs-number">1</span></span>]&gt;+ interp (Repair.AdjGroup {weight = <span class="hljs-number"><span class="hljs-number">0.5</span></span>}); Repair -&gt; Verb&lt;gnc-agr[<span class="hljs-number"><span class="hljs-number">1</span></span>]&gt; Adj&lt;gnc-agr[<span class="hljs-number"><span class="hljs-number">1</span></span>]&gt;* interp (Repair.Verb) RepairW&lt;gnc-agr[<span class="hljs-number"><span class="hljs-number">1</span></span>]&gt; {weight = <span class="hljs-number"><span class="hljs-number">0.5</span></span>};</code> </pre> <br>  Regeln, mit denen sichergestellt wird, dass keine Informationen zum Status √∂ffentlicher R√§ume abgerufen werden: <br><br><pre> <code class="javascript hljs">Repair -&gt; StopWords Verb* Prep* Adj* RepairW; Repair -&gt; Adj+ RepairW Prep* StopWords;</code> </pre><br>  Standardm√§√üig ist das Gewicht der Regel 1, wobei der Regel ein kleineres Gewicht zugewiesen wird. Wir legen die Reihenfolge ihrer Ausf√ºhrung fest. <br><br>  Es war ein wenig peinlich, dass nur die Konsolenanwendung und eine Menge C ++ - Code in die √ñffentlichkeit hochgeladen wurden.  Der zweifelsfreie Vorteil war jedoch die einfache Bedienung und die schnellen Ergebnisse bei Experimenten.  Daher wurde beschlossen, √ºber die m√∂glichen Schwierigkeiten nachzudenken, es n√§her an der Implementierung selbst in unser System einzuf√ºhren. <br><br>  Fast sofort war es m√∂glich, nahezu alle notwendigen Informationen √ºber die Reparatur in hoher Qualit√§t zu extrahieren.  "Fast", weil anfangs einige W√∂rter unter keinen Bedingungen und Grammatiken extrahiert wurden.  Es war jedoch schwierig, das Ausma√ü dieses Problems sofort einzusch√§tzen, inwieweit es die Qualit√§t der L√∂sung des gesamten Klassifizierungsproblems beeinflussen kann. <br><br>  Nachdem wir sichergestellt hatten, dass Tomita uns in erster N√§herung die erforderlichen Funktionen zur Verf√ºgung stellt, stellten wir fest, dass es keine Option ist, sie als Konsolenanwendung zu verwenden: Erstens erwies sich die Konsolenanwendung aus unbekannten Gr√ºnden als instabil und st√ºrzte von Zeit zu Zeit ab, und zweitens bot sie keine die erforderliche Parsing-Last von mehreren Millionen Anzeigen pro Tag.  So wurde definitiv klar, woraus man eine Bibliothek machen sollte. <br><br><h3>  Wie wir Tomitha zu einer Multithread-Bibliothek gemacht und uns mit Java angefreundet haben </h3><br>  Unser System ist in Java geschrieben, Tomita-Parser in C ++.  Wir mussten in der Lage sein, Parsing-Anzeigentext von Java aus aufzurufen. <br><br>  Die Entwicklung von Java-Bindungen f√ºr Tomita-Parser kann bedingt in zwei Komponenten unterteilt werden - die Implementierung der M√∂glichkeit, Tomita als gemeinsam genutzte Bibliothek zu verwenden und tats√§chlich eine Integrationsschicht mit jvm zu schreiben.  Die Hauptschwierigkeit betraf den ersten Teil.  Tomita selbst war urspr√ºnglich f√ºr die Ausf√ºhrung in einem separaten Prozess konzipiert.  Daraus folgte, dass die Haupthindernisse f√ºr die Verwendung des Parsers im Bewerbungsprozess zwei Faktoren waren. <br><br><ol><li>  Der Datenaustausch wurde √ºber verschiedene Arten von E / A durchgef√ºhrt.  Es war erforderlich, die F√§higkeit zum Datenaustausch mit dem Parser √ºber den Speicher zu implementieren.  Dar√ºber hinaus war es notwendig, dies so zu tun, dass der Code des Parsers selbst minimal beeinflusst wurde.  Die Architektur von Tomita schlug eine M√∂glichkeit vor, das Lesen von Eingabedokumenten aus dem Speicher als Implementierung der Schnittstellen CDocStreamBase und CDocListRetrieverBase zu implementieren.  Bei der Ausgabe war es schwieriger - ich musste den Code des XML-Generators ber√ºhren. </li><li>  Der zweite Faktor, der sich aus dem Prinzip ‚Äûein Parser - ein Prozess‚Äú ergibt, ist der globale Status, der aus verschiedenen Instanzen des Parsers modifiziert wurde.  Wenn Sie sich die Datei <a href="">src / util / generic / singleton.h ansehen</a> , sehen Sie den Mechanismus f√ºr die Verwendung des freigegebenen Status.  Es ist leicht vorstellbar, dass bei Verwendung von zwei Parser-Instanzen im selben Adressraum eine Race-Bedingung auftritt.  Um nicht den gesamten Parser neu zu schreiben, wurde beschlossen, diese Klasse zu √§ndern und den globalen Status durch einen lokalen Status relativ zum Thread (thread_local) zu ersetzen.  Dementsprechend setzen wir diese thread_local-Variablen vor jedem Parser-Aufruf im JTextMiner-Wrapper auf die aktuelle Parser-Instanz. Danach arbeitet der Parser-Code mit den Adressen der aktuellen Parser-Instanz. </li></ol><br>  Nachdem diese beiden Faktoren beseitigt wurden, konnte der Parser in jeder Umgebung als gemeinsam genutzte Bibliothek verwendet werden.  Das Schreiben von JNI-Bindemitteln und eines Java-Wrappers war nicht l√§nger schwierig. <br><br>  Der Tomita-Parser muss vor der Verwendung konfiguriert werden.  Die Konfigurationsparameter √§hneln denen, die beim Aufrufen des Konsolendienstprogramms verwendet werden.  Das Parsen selbst besteht darin, die parse () -Methode aufzurufen, die Dokumente zum Parsen empf√§ngt und XML als Zeichenfolge mit den Ergebnissen des Parsers zur√ºckgibt. <br><br>  Die Multithread-Version von Tomita - TomitaPooledParser verwendet zum Parsen eines Pools von TomitaParser-Objekten, die auf die gleiche Weise konfiguriert sind.  Zum Parsen wird der erste freie Parser verwendet.  Da die Anzahl der erstellten Parser der Anzahl der Threads im Pool entspricht, ist immer mindestens ein Parser f√ºr die Aufgabe verf√ºgbar.  Die Analysemethode analysiert asynchron die bereitgestellten Dokumente im ersten freien Parser. <br><br>  Ein Beispiel f√ºr den Aufruf der Tomita-Bibliothek von Java aus: <br><br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">/** * </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">@param</span></span></span><span class="hljs-comment"> threadAmount number of threads in the pool * </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">@param</span></span></span><span class="hljs-comment"> tomitaConfigFilename tomita config.proto * </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">@param</span></span></span><span class="hljs-comment"> configDirname dir with configs: grammars, gazetteer, facttypes.proto */</span></span> tomitaPooledParser = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> TomitaPooledParser(threadAmount, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> File(configDirname), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> String[]{tomitaConfigFilename}); Future&lt;String&gt; result = tomitaPooledParser.parse(documents); String response = result.get();</code> </pre><br>  Als Antwort eine XML-Zeichenfolge mit dem Ergebnis der Analyse. <br><br><h3>  Probleme, auf die wir gesto√üen sind und wie wir sie gel√∂st haben </h3><br>  Wenn die Bibliothek fertig ist, starten wir den Dienst mit seiner Verwendung f√ºr eine gro√üe Datenmenge und erinnern uns an das Problem, einige W√∂rter nicht zu extrahieren, und erkennen, dass dies f√ºr unsere Aufgabe sehr wichtig ist. <br><br>  Unter diesen W√∂rtern befanden sich "vorgefiltert" sowie "erledigt", "produziert" und andere gek√ºrzte Partizipien.  Das hei√üt, die W√∂rter, die sehr oft in der Anzeige gefunden werden, und manchmal ist dies die einzige oder sehr wichtige Information √ºber die Reparatur.  Der Grund f√ºr dieses Verhalten - das Wort ‚Äûvorgefiltert‚Äú stellte sich als Wort mit unbekannter Morphologie heraus, dh Tomita kann einfach nicht bestimmen, welcher Teil der Sprache es ist, und kann es dementsprechend nicht extrahieren.  Und f√ºr gek√ºrzte Partizipien musste ich eine separate Regel schreiben, und das Problem wurde gel√∂st, aber es dauerte einige Zeit, um herauszufinden, dass es sich um abgek√ºrzte Partizipien handelt, f√ºr deren Extraktion eine spezielle Regel erforderlich ist.  Und f√ºr das langm√ºtige ‚Äûendg√ºltige‚Äú Ende musste ich eine separate Regel f√ºr ein Wort mit einer unbekannten Morphologie schreiben. <br><br>  Um Analyseprobleme mithilfe von Grammatiken zu l√∂sen, f√ºgen wir dem Ortsverzeichnis ein Wort mit unbekannter Morphologie hinzu: <br><br><pre> <code class="javascript hljs">TAuxDicArticle <span class="hljs-string"><span class="hljs-string">"adjNonExtracted"</span></span> { key = <span class="hljs-string"><span class="hljs-string">""</span></span> | <span class="hljs-string"><span class="hljs-string">"-"</span></span> }</code> </pre><br>  F√ºr abgek√ºrzte Partizipien verwenden wir die grammatikalischen Eigenschaften von partcp, brev. <br><br>  Und jetzt k√∂nnen wir die Regeln f√ºr diese F√§lle schreiben: <br><br><pre> <code class="javascript hljs">Repair -&gt; RepairW&lt;gnc-agr[<span class="hljs-number"><span class="hljs-number">1</span></span>]&gt; Word&lt;gram=<span class="hljs-string"><span class="hljs-string">"partcp,brev"</span></span>,gnc-agr[<span class="hljs-number"><span class="hljs-number">1</span></span>]&gt; interp (Repair.AdjGroup) {weight = <span class="hljs-number"><span class="hljs-number">0.5</span></span>}; Repair -&gt; Word&lt;kwtype=<span class="hljs-string"><span class="hljs-string">"adjNonExtracted"</span></span>,gnc-agr[<span class="hljs-number"><span class="hljs-number">1</span></span>]&gt; interp (Repair.AdjGroup) RepairW&lt;gnc-agr[<span class="hljs-number"><span class="hljs-number">1</span></span>]&gt; Prep* Adj&lt;gnc-agr[<span class="hljs-number"><span class="hljs-number">1</span></span>]&gt;+;</code> </pre><br>  Und das letzte der Probleme, die wir entdeckt haben, ist ein Dienst mit Multithread-Nutzung der Tomita-Bibliothek, der myStem-Prozesse erzeugt, die nicht zerst√∂rt werden und nach einiger Zeit den gesamten Speicher f√ºllen.  Die einfachste L√∂sung bestand darin, die maximale und minimale Anzahl von Threads in Tomcat zu begrenzen. <br><br><h3>  Ein paar Worte zur Klassifizierung </h3><br>  Jetzt haben wir die Reparaturinformationen aus dem Text extrahiert.  Es war nicht schwierig, es mit einem der Gradientenverst√§rkungsalgorithmen zu klassifizieren.  Wir werden hier nicht lange auf dieses Thema verzichten, es wurde viel dar√ºber gesagt und geschrieben, und wir haben in diesem Bereich nichts radikal Neues getan.  Ich werde nur die Qualit√§tsindikatoren der Klassifizierung angeben, die wir bei den Tests erhalten haben: <br><br><ul><li>  Genauigkeit = 95% </li><li>  F1-Punktzahl = 93% </li></ul><br><h3>  Fazit </h3><br>  Der implementierte Dienst, der Tomita-Parser im Bibliotheksmodus verwendet, arbeitet derzeit kontinuierlich und analysiert und klassifiziert mehrere Millionen Anzeigen pro Tag. <br><br><h3>  PS </h3><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Der gesamte Tomita-Code</a> , den wir im Rahmen dieses Projekts geschrieben haben, wird auf den Github hochgeladen.  Ich hoffe, dass dies f√ºr jemanden n√ºtzlich ist, und diese Person wird ein wenig Zeit f√ºr etwas noch N√ºtzlicheres sparen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de439614/">https://habr.com/ru/post/de439614/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de439604/index.html">Wie ist der Barcode angeordnet?</a></li>
<li><a href="../de439606/index.html">Pilotproduktion von Elektronik zum Mindestpreis</a></li>
<li><a href="../de439608/index.html">Moderne Religion: Was geben Google, Facebook, Amazon und Apple den Menschen?</a></li>
<li><a href="../de439610/index.html">Wie bei meinem Elektrofahrzeug Chevrolet Bolt wurden im Rahmen der Garantie zwei Batteriemodule ausgetauscht und das dritte √ºberwacht</a></li>
<li><a href="../de439612/index.html">Robustes JavaScript: Jagd nach einem Mythos</a></li>
<li><a href="../de439616/index.html">Die Zusammenfassung interessanter IT-Projekte auf Kickstarter Nr. 7</a></li>
<li><a href="../de439618/index.html">PHP f√ºr Anf√§nger. Dateiverbindung</a></li>
<li><a href="../de439620/index.html">Asynchronit√§t in JavaScript verstehen [√úbersetzung von Sukhjinder Arora]</a></li>
<li><a href="../de439624/index.html">Warum Daten im Orbit speichern?</a></li>
<li><a href="../de439626/index.html">Erleben Sie die Entwicklung einer kostenlosen Anwendung f√ºr OpenNumismat-Sammler</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>