<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üî∫ üë®üèΩ‚Äçüíº üìÄ Desarrollo de bases de datos en Dropbox. La ruta desde una base de datos MySQL global a miles de servidores üë®üèº‚Äçüîß üë®‚Äçüè´ ü§õüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Cuando Dropbox acaba de comenzar, un usuario de Hacker News coment√≥ que podr√≠a implementarse con varios scripts de bash usando FTP y Git. Ahora, esto ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Desarrollo de bases de datos en Dropbox. La ruta desde una base de datos MySQL global a miles de servidores</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/417315/">  Cuando Dropbox acaba de comenzar, un usuario de Hacker News coment√≥ que podr√≠a implementarse con varios scripts de bash usando FTP y Git.  Ahora, esto no se puede decir de ninguna manera, este es un gran almacenamiento de archivos en la nube con miles de millones de archivos nuevos todos los d√≠as, que no solo se almacenan de alguna manera en la base de datos, sino de tal manera que cualquier base de datos se puede restaurar en cualquier punto en los √∫ltimos seis d√≠as. <br><br>  Debajo del corte, la transcripci√≥n del informe de <strong>Glory Bakhmutov</strong> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" class="user_link">m0sth8</a> ) en Highload ++ 2017, sobre c√≥mo se desarrollaron las bases de datos en Dropbox y c√≥mo se organizan ahora. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/hUFFsLoCRNU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <strong>Sobre el orador:</strong> Gloria a Bakhmutov: ingeniero de confiabilidad del sitio en el equipo de Dropbox, ama mucho Go y a veces aparece en el podcast golangshow.com. <br><br><h2>  Contenido <br></h2><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Brevemente sobre la arquitectura de Dropbox</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">La historia del desarrollo</a> de bases de datos y c√≥mo funciona la arquitectura actual de Dropbox </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Las operaciones m√°s simples en bases de datos</a> (feylovers, copias de seguridad, clones, promociones) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Automatizaci√≥n</a> : que administra todas las bases de datos y ejecuta operaciones </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Monitoreo</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pruebas, estadificaci√≥n y DRT</a> </li></ul><br><img src="https://habrastorage.org/webt/dh/gt/p7/dhgtp7pgu4eyl6rc3ncfnaf9s3i.jpeg"><br><a name="habracut"></a><br><a name="dropbox_architecture"></a><h2>  Arquitectura de Dropbox en lenguaje sencillo </h2><br>  Dropbox apareci√≥ en 2008.  Esto es esencialmente un almacenamiento de archivos en la nube.  Cuando Dropbox acaba de comenzar, un usuario de Hacker News coment√≥ que podr√≠a implementarse con varios scripts de bash usando FTP y Git.  Pero, sin embargo, Dropbox se est√° desarrollando, y ahora es un servicio bastante grande con m√°s de 1.500 millones de usuarios, 200.000 empresas y un gran n√∫mero (¬°varios miles de millones!) De archivos nuevos todos los d√≠as. <br><br>  <strong>¬øC√≥mo se ve Dropbox?</strong> <br><img src="https://habrastorage.org/webt/ed/em/km/edemkm1wqvlbv6jb0qobp5c2dgc.jpeg"><br><br>  Tenemos varios clientes (interfaz web, API para aplicaciones que usan Dropbox, aplicaciones de escritorio).  Todos estos clientes usan la API y se comunican con dos grandes servicios que l√≥gicamente se pueden dividir en: <br><br><ol><li>  <strong>Metaservidor</strong> </li><li>  <strong>Servidor de bloques</strong> </li></ol><br>  Metaserver almacena metainformaci√≥n sobre el archivo: tama√±o, comentarios sobre √©l, enlaces a este archivo en Dropbox, etc.  Blockserver solo almacena informaci√≥n sobre archivos: carpetas, rutas, etc. <br><br>  <strong>Como funciona</strong> <br><br>  Por ejemplo, tiene un archivo video.avi con alg√∫n tipo de video. <br><img src="https://habrastorage.org/webt/kc/i9/op/kci9op0l7f_tecaxetg7uzpzemw.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Enlace desde la diapositiva</a></em> <br><br><ul><li>  El cliente divide este archivo en varios fragmentos (en este caso, 4 MB cada uno), calcula la suma de verificaci√≥n y env√≠a una solicitud a Metaserver: "Tengo un archivo * .avi, quiero cargarlo, las cantidades de hash son tal y tal". </li><li>  Metaserver devuelve la respuesta: "No tengo estos bloques, ¬°descarguemos!"  O puede responder que tiene todos o algunos de los bloques, y solo los restantes deben cargarse. </li></ul><br><img src="https://habrastorage.org/webt/qa/zr/79/qazr79svhai2ouk6v8lta1zcp-u.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Enlace desde la diapositiva</a></em> <br><br><ul><li>  Despu√©s de eso, el cliente va al Blockserver, env√≠a la cantidad de hash y el bloque de datos en s√≠, que se almacena en el Blockserver. </li><li>  Blockserver confirma la operaci√≥n. </li></ul><br><img src="https://habrastorage.org/webt/kl/dx/xw/kldxxw1ncgj4ytt1pqq5bh95iue.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Enlace desde la diapositiva</a></em> <br><br>  Por supuesto, este es un esquema muy simplificado, el protocolo es mucho m√°s complicado: hay sincronizaci√≥n entre clientes dentro de la misma red, hay controladores de kernel, la capacidad de resolver colisiones, etc.  Este es un protocolo bastante complejo, pero funciona as√≠ esquem√°ticamente. <br><img src="https://habrastorage.org/webt/ev/-x/_h/ev-x_hwfhwpfixld61yqiraelc4.jpeg"><br><br>  Cuando un cliente guarda algo en Metaserver, toda la informaci√≥n va a MySQL.  Blockserver tambi√©n almacena informaci√≥n sobre archivos, c√≥mo est√°n estructurados, en qu√© bloques consisten, en MySQL.  Blockserver tambi√©n almacena los bloques en Block Storage, que, a su vez, almacena informaci√≥n sobre d√≥nde se encuentra ese bloque, en qu√© servidor y c√≥mo se procesa, tambi√©n en MYSQL. <br><br><blockquote>  Para almacenar exabytes de archivos de usuario, almacenamos simult√°neamente informaci√≥n adicional en una base de datos de varias docenas de petabytes dispersos en 6 mil servidores. </blockquote><br><a name="history_development"></a><h2>  Historial de desarrollo de bases de datos </h2><br>  ¬øC√≥mo evolucionaron las bases de datos en Dropbox? <br><img src="https://habrastorage.org/webt/oe/w9/ok/oew9okiqdeivyhvhycznly7kvbe.jpeg"><br><br>  En 2008, todo comenz√≥ con un Metaserver y una base de datos global.  Toda la informaci√≥n que Dropbox necesitaba ser almacenada en alg√∫n lugar, la guard√≥ en el √∫nico MySQL global.  Esto no dur√≥ mucho, porque el n√∫mero de usuarios creci√≥ y las bases de datos y tabletas individuales dentro de las bases de datos aumentaron m√°s r√°pido que otras. <br><img src="https://habrastorage.org/webt/ry/lt/h9/rylth906zfbbcou6nz7ve_yqib8.jpeg"><br><br>  Por lo tanto, en 2011 se enviaron varias tablas a servidores separados: <br><br><ul><li>  <strong>Usuario</strong> , con informaci√≥n sobre los usuarios, por ejemplo, inicios de sesi√≥n y tokens oAuth; </li><li>  <strong>Host</strong> , con informaci√≥n de archivo de Blockserver; </li><li>  <strong>Varios</strong> , que no particip√≥ en el procesamiento de solicitudes de producci√≥n, pero se utiliz√≥ para funciones de utilidad, como trabajos por lotes. </li></ul><br><img src="https://habrastorage.org/webt/ja/ec/ja/jaecja2eklv8znsqhf5lt-dyez8.jpeg"><br><br>  Pero despu√©s de 2012, Dropbox comenz√≥ a crecer mucho, desde entonces <strong>hemos</strong> crecido en <strong>unos 100 millones de usuarios al a√±o</strong> . <br><img src="https://habrastorage.org/webt/ie/cr/-s/iecr-syyi6qprj2zj45qx4l42k4.jpeg"><br><br>  Era necesario tener en cuenta un crecimiento tan enorme y, por lo tanto, a finales de 2011 ten√≠amos fragmentos, una base que constaba de 1.600 fragmentos.  Inicialmente, solo 8 servidores con 200 fragmentos cada uno.  Ahora son 400 servidores maestros con 4 fragmentos en cada uno. <br><img src="https://habrastorage.org/webt/b3/v9/vy/b3v9vyjqzwau2kgmadhgb2vg0vo.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Enlace desde la diapositiva</a></em> <br><br>  En 2012, nos dimos cuenta de que crear tablas y actualizarlas en la base de datos para cada l√≥gica comercial agregada es muy dif√≠cil, triste y problem√°tico.  Por lo tanto, en 2012, inventamos nuestro propio almacenamiento de gr√°ficos, que llamamos <strong>Edgestore</strong> , y desde entonces toda la l√≥gica empresarial y la metainformaci√≥n que genera la aplicaci√≥n se almacenan en Edgestore. <br><br>  Edgestore esencialmente abstrae MySQL de los clientes.  Los clientes tienen ciertas entidades que est√°n interconectadas por enlaces desde la API de gRPC a Edgestore Core, que convierte estos datos en MySQL y de alguna manera los almacena all√≠ (b√°sicamente, proporciona todo esto desde el cach√©). <br><img src="https://habrastorage.org/webt/bj/s7/dz/bjs7dz7-cdsvjblcgftjdeybrgk.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Enlace desde la diapositiva</a></em> <br><br>  <strong>En 2015, dejamos Amazon S3</strong> , desarrollamos nuestro propio almacenamiento en la nube llamado Magic Pocket.  Contiene informaci√≥n sobre d√≥nde se encuentra un archivo de bloque, en qu√© servidor, sobre los movimientos de estos bloques entre servidores, almacenados en MySQL. <br><img src="https://habrastorage.org/webt/f_/bz/lm/f_bzlm3tk9e3lwqo64x4kfuhhok.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Enlace desde la diapositiva</a></em> <br><br>  Pero MySQL se usa de una manera muy complicada, en esencia, como una gran tabla hash distribuida.  Esta es una carga muy diferente, principalmente en la lectura de registros aleatorios.  El 90% de la utilizaci√≥n es E / S. <br><br><h2>  Arquitectura de base de datos </h2><br>  Primero, identificamos de inmediato algunos principios por los cuales construimos la arquitectura de nuestra base de datos: <br><br><ol><li>  <strong>Fiabilidad y durabilidad</strong> .  Este es el principio m√°s importante y lo que los clientes esperan de nosotros: los datos no deben perderse. </li><li>  <strong>La optimizaci√≥n de la soluci√≥n</strong> es un principio igualmente importante.  Por ejemplo, las copias de seguridad deben hacerse r√°pidamente y restaurarse r√°pidamente tambi√©n. </li><li>  <strong>Simplicidad de soluci√≥n</strong> , tanto arquitect√≥nicamente como en t√©rminos de servicio y soporte de desarrollo adicional. </li><li>  <strong>Costo de propiedad</strong> .  Si algo optimiza la soluci√≥n, pero es muy costoso, esto no nos conviene.  Por ejemplo, un esclavo que est√° un d√≠a detr√°s del maestro es muy conveniente para las copias de seguridad, pero luego debe agregar 1,000 m√°s a 6,000 servidores: el costo de propiedad de dicho esclavo es muy alto. </li></ol><br>  Todos los principios deben ser <strong>verificables y medibles</strong> , es decir, deben tener m√©tricas.  Si hablamos del costo de propiedad, debemos calcular cu√°ntos servidores tenemos, por ejemplo, va a bases de datos, cu√°ntos servidores van a copias de seguridad y cu√°nto cuesta al final Dropbox.  Cuando elegimos una nueva soluci√≥n, contamos todas las m√©tricas y nos enfocamos en ellas.  Al elegir cualquier soluci√≥n, nos guiamos completamente por estos principios. <br><br><h2>  Topolog√≠a base </h2><br>  La base de datos est√° estructurada de la siguiente manera: <br><br><ul><li>  En el centro de datos principal, tenemos un maestro, en el que se producen todos los registros. </li><li>  El servidor maestro tiene dos servidores esclavos en los que se produce la replicaci√≥n semisincr√≥nica.  Los servidores a menudo mueren (aproximadamente 10 por semana), por lo que necesitamos dos servidores esclavos. </li><li>  Los servidores esclavos est√°n en grupos separados.  Los cl√∫steres son salas completamente separadas en el centro de datos que no est√°n conectadas entre s√≠.  Si una habitaci√≥n se quema, la segunda sigue funcionando por completo. </li><li>  Tambi√©n en otro centro de datos tenemos el llamado pseudo maestro (maestro intermedio), que en realidad es solo un esclavo, que tiene otro esclavo. </li></ul><br><img src="https://habrastorage.org/webt/k6/6s/x6/k66sx6siyp6efjxxmfrot21ueha.jpeg"><br><br>  Se eligi√≥ dicha topolog√≠a porque si el primer centro de datos muere repentinamente en nosotros, entonces en el segundo centro de datos tenemos una <strong>topolog√≠a casi completa</strong> .  Simplemente cambiamos todas las direcciones en Discovery y los clientes pueden trabajar. <br><br><h3>  Topolog√≠as Especializadas </h3><br>  Tambi√©n contamos con topolog√≠as especializadas. <br><br>  La topolog√≠a de <strong>Magic Pocket</strong> consta de un servidor maestro y dos servidores esclavos.  Esto se hace porque Magic Pocket mismo duplica datos entre zonas.  Si pierde un cl√∫ster, puede restaurar todos los datos de otras zonas a trav√©s del c√≥digo de borrado. <br><img src="https://habrastorage.org/webt/gk/o7/bi/gko7bifb-4ted4cvwmzgyn5lasw.jpeg"><br><br>  La topolog√≠a <strong>activo-activo</strong> es la topolog√≠a personalizada utilizada por Edgestore.  Tiene un maestro y dos esclavos en cada uno de los dos centros de datos, y son esclavos el uno del otro.  Este es un <strong>esquema</strong> muy <strong>peligroso</strong> , pero Edgestore a su nivel sabe exactamente qu√© datos en qu√© maestro puede escribir sobre qu√© rango.  Por lo tanto, esta topolog√≠a no se rompe. <br><img src="https://habrastorage.org/webt/xe/nv/wx/xenvwx3ls9ct8fssverq3htck10.jpeg"><br><br><h3>  Instancia </h3><br>  Hemos instalado servidores bastante simples con una configuraci√≥n de hace 4-5 a√±os: <br><br><ul><li>  <strong>2x Xeon 10 n√∫cleos;</strong> </li><li>  <strong>5 TB (8 SSD Raid 0 *);</strong> </li><li>  <strong>384 GB de memoria.</strong> </li></ul><br>  * Raid 0: porque es m√°s f√°cil y mucho m√°s r√°pido reemplazar un servidor completo que las unidades. <br><br><h4>  Instancia √∫nica </h4><br>  En este servidor, tenemos una instancia grande de MySQL en la que se encuentran varios fragmentos.  Esta instancia de MySQL se asigna inmediatamente casi toda la memoria.  Tambi√©n se ejecutan otros procesos en el servidor: proxy, recopilaci√≥n de estad√≠sticas, registros, etc. <br><br><img src="https://habrastorage.org/webt/z8/yd/vw/z8ydvwabte3v8pytwbrl1vi8yc0.jpeg"><br><br>  Esta soluci√≥n es buena en eso: <br><br>  + Es <strong>f√°cil de administrar</strong> .  Si necesita reemplazar la instancia de MySQL, simplemente reemplace el servidor. <br><br>  + <strong>Solo haz faylovers</strong> . <br><br>  Por otro lado: <br><br>  - Es problem√°tico que cualquier operaci√≥n ocurra en toda la instancia de MySQL e inmediatamente en todos los fragmentos.  Por ejemplo, si necesita hacer una copia de seguridad, respaldamos todos los fragmentos a la vez.  Si necesita hacer un faylover, hacemos el faylover los cuatro fragmentos a la vez.  En consecuencia, la accesibilidad sufre 4 veces m√°s. <br><br>  - Los problemas con la replicaci√≥n de un fragmento afectan a otros fragmentos.  La replicaci√≥n de MySQL no es paralela, y todos los fragmentos funcionan en un solo hilo.  Si algo le sucede a un fragmento, el resto tambi√©n se convierte en v√≠ctima. <br><br>  As√≠ que ahora nos estamos moviendo a una topolog√≠a diferente. <br><br><h4>  Instancia m√∫ltiple </h4><br><img src="https://habrastorage.org/webt/lg/7x/ks/lg7xks5vbogjaf6slr7tidlc6ty.jpeg"><br><br>  En la nueva versi√≥n, se lanzan varias instancias de MySQL en el servidor a la vez, cada una con un fragmento.  Que es mejor <br><br>  + Podemos <strong>realizar operaciones solo en un fragmento espec√≠fico</strong> .  Es decir, si necesita un faylover, cambie solo un fragmento, si necesita una copia de seguridad, respaldamos solo un fragmento.  Esto significa que las operaciones se aceleran enormemente: 4 veces para un servidor de cuatro fragmentos. <br><br>  + Los <strong>fragmentos apenas se afectan entre s√≠</strong> . <br><br>  + <strong>Mejora en la replicaci√≥n.</strong>  Podemos mezclar diferentes categor√≠as y clases de bases de datos.  Edgestore ocupa mucho espacio, por ejemplo, los 4 TB, y Magic Pocket ocupa solo 1 TB, pero tiene una utilizaci√≥n del 90%.  Es decir, podemos combinar diferentes categor√≠as que usan E / S y recursos de m√°quina de diferentes maneras, y comenzar 4 secuencias de replicaci√≥n. <br><br>  Por supuesto, esta soluci√≥n tiene sus inconvenientes: <br><br>  - El mayor inconveniente es que es <strong>mucho m√°s dif√≠cil manejar todo esto</strong> .  Necesitamos un planificador inteligente que entienda a d√≥nde puede llevar esta instancia, donde habr√° una carga √≥ptima. <br><br>  - <strong>M√°s duro que los failovers</strong> . <br><br>  Por lo tanto, solo ahora nos estamos moviendo a esta decisi√≥n. <br><br><h3>  Descubrimiento </h3><br>  Los clientes deben saber de alguna manera c√≥mo conectarse a la base de datos deseada, por lo que tenemos Discovery, que deber√≠a: <br><br><ol><li>  Notifique al cliente muy r√°pidamente sobre los cambios de topolog√≠a.  Si cambiamos maestro y esclavo, los clientes deber√≠an aprenderlo casi al instante. <br></li><li>  La topolog√≠a no debe depender de la topolog√≠a de replicaci√≥n de MySQL, porque con algunas operaciones cambiamos la topolog√≠a de MySQL.  Por ejemplo, cuando dividimos, en el paso preparatorio del maestro de destino, donde transferiremos parte de los fragmentos, algunos de los servidores esclavos se reconfiguran a este maestro de destino.  Los clientes no necesitan saber sobre esto. <br></li><li>  Es importante que haya atomicidad de las operaciones y verificaci√≥n del estado.  Es imposible que dos servidores diferentes de la misma base de datos se conviertan en maestros en el mismo momento. <br></li></ol><br><h4>  C√≥mo se desarroll√≥ el descubrimiento </h4><br>  Al principio todo era simple: la direcci√≥n de la base de datos en el c√≥digo fuente en la configuraci√≥n.  Cuando necesit√°bamos actualizar la direcci√≥n, todo se despleg√≥ muy r√°pidamente. <br><img src="https://habrastorage.org/webt/7d/yb/oo/7dyboo0h7eo4o9_9xp-n-6lzosy.jpeg"><br><br>  Desafortunadamente, esto no funciona si hay muchos servidores. <br><img src="https://habrastorage.org/webt/3u/26/pf/3u26pfl_s796zdaoix-zdplb9du.jpeg"><br><br>  Arriba est√° el primer Discovery que tenemos.  Hubo scripts de base de datos que cambiaron la placa de identificaci√≥n en ConfigDB; era una placa de identificaci√≥n de MySQL separada, y los clientes ya escuchaban esta base de datos y tomaban datos peri√≥dicamente de all√≠. <br><img src="https://habrastorage.org/webt/ml/qn/mh/mlqnmhmmteylazgl4itjokes_mu.jpeg"><br><br>  La tabla es muy simple, hay una categor√≠a de base de datos, una clave de fragmento, una clase de base de datos maestro / esclavo, proxy y una direcci√≥n de base de datos.  De hecho, el cliente solicit√≥ una categor√≠a, una clase de base de datos, una clave de fragmento y se devolvi√≥ la direcci√≥n MySQL a la que ya pod√≠a establecer una conexi√≥n. <br><img src="https://habrastorage.org/webt/vb/ht/en/vbhteniyw4x7s56a1xwckuz268e.jpeg"><br><br>  Tan pronto como hubo muchos servidores, se agreg√≥ Memcache y los clientes comenzaron a comunicarse con √©l. <br><br>  Pero luego lo reelaboramos.  Los scripts de MySQL comenzaron a comunicarse a trav√©s de gRPC, a trav√©s de un cliente ligero con un servicio que llamamos RegisterService.  Cuando ocurrieron algunos cambios, RegisterService ten√≠a una cola y entendi√≥ c√≥mo aplicar estos cambios.  RegisterService guard√≥ datos en AFS.  AFS es nuestro sistema interno basado en ZooKeeper. <br><img src="https://habrastorage.org/webt/mi/lm/yz/milmyzvyvayuv2av8pah4neh9zq.jpeg"><br><br>  La segunda soluci√≥n, que no se muestra aqu√≠, us√≥ ZooKeeper directamente, y esto cre√≥ problemas porque cada fragmento era un nodo en ZooKeeper.  Por ejemplo, 100 mil clientes se conectan a ZooKeeper, si murieron repentinamente debido a alg√∫n tipo de error, 100 mil solicitudes a ZooKeeper llegar√°n de inmediato, lo que simplemente lo dejar√° y no podr√° aumentar. <br><br>  Por lo tanto, se desarroll√≥ el <strong>sistema AFS, que es utilizado por todo Dropbox</strong> .  De hecho, abstrae el trabajo con ZooKeeper para todos los clientes.  El demonio AFS se ejecuta localmente en cada servidor y proporciona una API de archivo muy simple de la forma: crear un archivo, eliminar un archivo, solicitar un archivo, recibir notificaci√≥n de un cambio de archivo y comparar e intercambiar operaciones.  Es decir, puede intentar reemplazar el archivo con alguna versi√≥n, y si esta versi√≥n ha cambiado durante el cambio, la operaci√≥n se cancela. <br><br>  Esencialmente, tal abstracci√≥n sobre ZooKeeper, en la que hay un algoritmo local de retroceso y jitter.  ZooKeeper ya no se bloquea bajo carga.  Con AFS, tomamos copias de seguridad en S3 y en GIT, luego el AFS local mismo notifica a los clientes que los datos han cambiado. <br><img src="https://habrastorage.org/webt/ry/_0/wv/ry_0wvkyux23gicrlwmfceq0eoe.jpeg"><br><br>  En AFS, los datos se almacenan como archivos, es decir, es una API del sistema de archivos.  Por ejemplo, lo anterior es el archivo shard.slave_proxy: el m√°s grande, toma aproximadamente 28 Kb, y cuando cambiamos la categor√≠a de la clase shard y slave_proxy, todos los clientes que se suscriben a este archivo reciben una notificaci√≥n.  Vuelven a leer este archivo, que contiene toda la informaci√≥n necesaria.  Utilizando la clave de fragmento, obtienen una categor√≠a y reconfiguran el grupo de conexiones a la base de datos. <br><br><a name="perations_databases"></a><h2>  Operaciones </h2><br>  Utilizamos operaciones muy simples: promoci√≥n, clonaci√≥n, copias de seguridad / recuperaci√≥n. <br><img src="https://habrastorage.org/webt/q3/5c/df/q35cdfiso51bhhiitqja71qbysc.jpeg"><br><br>  <strong>Una operaci√≥n es una m√°quina de estado simple</strong> .  Cuando entramos en la operaci√≥n, realizamos algunas verificaciones, por ejemplo, spin-check, que varias veces por tiempo de espera verifica si podemos realizar esta operaci√≥n.  Despu√©s de eso, hacemos algunas acciones preparatorias que no afectan a los sistemas externos.  A continuaci√≥n, la operaci√≥n en s√≠. <br><br>  Todos los pasos dentro de una operaci√≥n tienen un <strong>retroceso</strong> (deshacer).  Si hay un problema con la operaci√≥n, la operaci√≥n intenta restaurar el sistema a su posici√≥n original.  Si todo est√° bien, se realiza la limpieza y se completa la operaci√≥n. <br><br>  Tenemos una m√°quina de estado tan simple para cualquier operaci√≥n. <br><br><h4>  <strong>Promoci√≥n (cambio de master)</strong> </h4><br>  Esta es una operaci√≥n muy com√∫n en la base de datos.  Hubo preguntas sobre c√≥mo hacer un alter en un servidor maestro caliente que funciona: obtendr√° una apuesta.  Es solo que todas estas operaciones se realizan en servidores esclavos, y luego los cambios esclavos con lugares maestros.  Por lo tanto, la <strong>operaci√≥n de promoci√≥n es muy frecuente</strong> . <br><img src="https://habrastorage.org/webt/xx/79/jv/xx79jvszxb9wjffwqf4ld_euofo.jpeg"><br><br>  Necesitamos actualizar el kernel - intercambiamos, necesitamos actualizar la versi√≥n de MySQL - actualizamos en esclavo, cambiamos a maestro, actualizamos all√≠. <br><img src="https://habrastorage.org/webt/wc/op/o9/wcopo9jlmz9aeoynpv-hbbseois.jpeg"><br><br>  Hemos logrado una promoci√≥n muy r√°pida.  Por ejemplo, <strong>para cuatro fragmentos, ahora tenemos promoci√≥n durante unos 10-15 s.</strong>  El gr√°fico anterior muestra que con la disponibilidad de la promoci√≥n sufri√≥ un 0,0003%. <br><br>  Pero la promoci√≥n normal no es tan interesante, porque estas son operaciones ordinarias que se realizan todos los d√≠as.  Las failovers son interesantes. <br><br><h4>  <strong>Conmutaci√≥n por error (reemplazo de un maestro roto)</strong> <br></h4><br>  Una conmutaci√≥n por error significa que la base de datos est√° muerta. <br><br><ul><li>  Si el servidor realmente muri√≥, este es solo un caso ideal. </li><li>  De hecho, sucede que los servidores est√°n parcialmente vivos. </li><li>  A veces el servidor muere muy lentamente.  Los controladores de incursi√≥n, el sistema de disco fallan, algunas solicitudes devuelven respuestas, pero algunos flujos est√°n bloqueados y no devuelven respuestas. </li><li>  Sucede que el maestro simplemente est√° sobrecargado y no responde a nuestro chequeo de salud.  Pero si hacemos promoci√≥n, el nuevo maestro tambi√©n se sobrecargar√°, y solo empeorar√°. </li></ul><br>  El reemplazo de los servidores maestros fallecidos se realiza aproximadamente <strong>2-3 veces al d√≠a</strong> , este es un proceso completamente automatizado, no se necesita intervenci√≥n humana.  La secci√≥n cr√≠tica tarda unos 30 segundos, y tiene un mont√≥n de comprobaciones adicionales para ver si el servidor est√° realmente vivo, o tal vez ya haya muerto. <br><br>  A continuaci√≥n se muestra un diagrama de ejemplo de c√≥mo funciona el faylover. <br><img src="https://habrastorage.org/webt/ks/5d/6o/ks5d6oovtnchnmlr2zlgpwvmt5g.jpeg"><br><br>  En la secci√≥n seleccionada, <strong>reiniciamos el servidor maestro</strong> .  Esto es necesario porque tenemos MySQL 5.6, y en √©l la replicaci√≥n semisincr√≥nica no es sin p√©rdidas.  Por lo tanto, las lecturas fantasmas son posibles, y necesitamos este maestro, incluso si no ha muerto, matar lo m√°s r√°pido posible para que los clientes se desconecten de √©l.  Por lo tanto, hacemos un restablecimiento completo a trav√©s de Ipmi; esta es la primera operaci√≥n m√°s importante que debemos hacer.  En la versi√≥n MySQL 5.7, esto no es tan cr√≠tico. <br><br>  <strong>Sincronizaci√≥n de cl√∫ster.</strong>  ¬øPor qu√© necesitamos sincronizaci√≥n de cl√∫ster? <br><img src="https://habrastorage.org/webt/gh/pa/go/ghpago_p12c1jittnig4rwhc1sg.jpeg"><br><br>  Si recordamos la imagen anterior con nuestra topolog√≠a, un servidor maestro tiene tres servidores esclavos: dos en un centro de datos, uno en el otro.  Con la promoci√≥n, necesitamos que el maestro est√© en el mismo centro de datos principal.  Pero a veces, cuando se cargan esclavos, con semisync sucede que un esclavo semisync se convierte en esclavo en otro centro de datos, porque no est√° cargado.  Por lo tanto, primero debemos sincronizar todo el cl√∫ster, y luego hacer promoci√≥n en esclavo en el centro de datos que necesitamos.  Esto se hace de manera muy simple: <br><br><ul><li>  Paramos todos los hilos de E / S en todos los servidores esclavos. </li><li>  Despu√©s de eso, ya sabemos con certeza que el maestro es de "solo lectura", ya que la semisincronizaci√≥n se ha desconectado y nadie m√°s puede escribir nada all√≠. </li><li>  A continuaci√≥n, seleccionamos el esclavo con el mayor conjunto GTID recuperado / ejecutado, es decir, con la transacci√≥n m√°s grande que descarg√≥ o ya aplic√≥. </li><li>  Reconfiguramos todos los servidores esclavos para este esclavo seleccionado, iniciamos el hilo de E / S y est√°n sincronizados. </li><li>  Esperamos hasta que est√©n sincronizados, despu√©s de lo cual tenemos todo el cl√∫ster se sincroniza.   ,     executed GTID set       . </li></ul><br>    ‚Äî <strong> </strong> .   <strong>promotion</strong> ,    : <br><img src="https://habrastorage.org/webt/bd/s-/b8/bds-b8fbieqxhtc4dy9ookntiri.jpeg"><br><br><ul><li>    slave    -,  ,   master,     promotion. </li><li>    slave-   master,   ,  ACLs,  ,  - proxy, , - . </li><li>      read_only = 0,   ,    master  ,   .        master     . </li><li>       - .     -    ,  ,   ,    , ,  proxy  . </li><li>     . </li></ul><br>   ,       rollback   ,   .       rollback  reboot.   ,    , ,  ‚Äî change master ‚Äî    master   . <br><br><h4> <strong></strong> </h4><br>  ‚Äî      .   ,    ,   ,    ,    . <br><br> <strong> </strong> <br><br> ‚óè   slave <br><br>   ,       slave-,      .   . <br><br> ‚óè       <br><br>     ,     ,      .             . <br><br> ‚óè       <br><br>  ,     ,      .          .      3  . <br><br><blockquote>    ,   ,   ,     : <br><br><ol><li>      .       1  40 . <br></li><li>            . <br></li></ol></blockquote><br>    ,     .   1   40 ,      ,      ,     . <br><br><h4> <strong></strong> </h4><br>    ,  .           .     4  . <br><img src="https://habrastorage.org/webt/bv/-k/_z/bv-k_znrl7zi2obmotthihjogyo.jpeg"><br><br><ul><li>    <strong> 24 </strong> .         HDFS,      . </li><li> <strong> 6 </strong>     unsharded databases,        Global DB.      , ,  ,     . </li><li> <strong> 3 </strong>          S3. </li><li> <strong> 3 </strong>     S3     . </li></ul><br><img src="https://habrastorage.org/webt/e1/yx/3s/e1yx3s-1ikyhxnuzeympjsvem14.jpeg"><br><br>       . ,    3 ,   HDFS     3 ,   6   S3.     . <br><br>  ,   . <br><img src="https://habrastorage.org/webt/sn/hz/1l/snhz1lmio2naq40wziys-jggaaw.jpeg"><br><br>         ,      ,   .       ,   ,    recovery  -   .  ,      ,  -       .      100  ,   . <br><br>     ,    ,    ,    ,   ,     ,  ,     .        . <br><br><h5>   </h5><br><img src="https://habrastorage.org/webt/4m/4b/kb/4m4bkboro5zunrljkwxbybj7jsi.jpeg"><br><br>     hot-,      Percona xtrabackup.     ‚Äîstream=xbstream,        ,   .     script-splitter,        ,      . <br><br> MySQL              2x.     3 , ,   ,    1 500 .     ,      ,    HDFS   S3. <br><br>        . <br><img src="https://habrastorage.org/webt/j3/il/jm/j3iljma8c0rekqweak5ngqvaxkk.jpeg"><br><br>  ,    ,    HDFS   S3,    , splitter       xtrabackup,      .   crash-recovery. <br><br>      hot   ,  crash-recovery    .         ,    .     binlog,      master. <br><br> <strong>   binlogs?</strong> <br><br>     binlog'.    master ,    4 ,   100 ,    HDFS. <br><br>      :   Binlog Backuper,         . ,  ,   binlog       HDFS. <br><img src="https://habrastorage.org/webt/on/o3/ce/ono3cesuissuuwuzcglcfautfjo.jpeg"><br><br> ,       4   ,    5 ,    ,    ,    .    HDFS   S3    . <br><br><h5>   </h5><br>      . <br><br>   : <br><br><ol><li>        ‚Äî  10 ,  45  ‚Äî   . <br></li><li>      ,       scheduler  multi instance      slave  master    . <br></li><li>    ‚Äî      ,   .  ,     ,    ,    ,     ,  ,    .  pt-table-checksum   ,      . <br></li></ol><br> <strong></strong> ,        : <br><br><ol><li>       1  10 ,      .    crash-recovery,     . <br></li><li>            . <br></li></ol><br><img src="https://habrastorage.org/webt/2m/bd/ar/2mbdarekbku4hsxygdufshzyhnm.jpeg"><br><br>     slave   -,     .    ,      .   . <br><br><h4>  ++ </h4><br>     .       Hardware ,          (HDD)  10 ,       + crash recovery xtrabackup,      . ,         ,    . , ,     ,   ,   HDD  ,    HDFS  . <br><br><h4>  </h4><br>    ,  ‚Äî   : <br><br><ol><li>         ; <br></li><li>       . <br></li></ol><br>  ,     HDFS,       ,   ,       . <br><br><a name="automation"></a><h2>  Automatizaci√≥n </h2><br>  ,  6 000      .         ,   ,     ‚Äî : <br><br><ul><li> Auto-replace; </li><li> DBManager; </li><li> Naoru, Wheelhouse </li></ul><br><h3> Auto-replace </h3><br>   ,   ,   ,    ,     ‚Äî ,     -.   ,   . <br><br> <strong>Availability ()</strong> ‚Äî         ,         .      ‚Äî   recovery  ,         . <br><img src="https://habrastorage.org/webt/-k/em/tr/-kemtrpvhgocinlvlmnsuj1eq-s.jpeg"><br><br>    MySQL  ,   heartbeat. Heartbeat ‚Äî   timestamp. <br><img src="https://habrastorage.org/webt/cn/r8/fn/cnr8fn-fpy_ddnthddjem3afr4o.jpeg"><br><br>    ,     , ,  master   read-write.          heartbeat. <br><br>    auto-replace ,    . <br><img src="https://habrastorage.org/webt/3u/r9/sx/3ur9sxfxf8dsxtgsgvjz3ublvye.jpeg"> <em>           <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> ,   91 .</em> <br><br> <strong>  ?</strong> <br><br><ul><li>   ,     heartbeat    . ,     .  heartbeat', ,    heartbeat'  30 . </li><li> Luego, vea si su n√∫mero satisface el valor umbral.  Si no, entonces algo est√° mal con el servidor, ya que no envi√≥ un latido. </li><li>  Despu√©s de eso, hacemos una verificaci√≥n inversa por si acaso: de repente, estos dos servicios han muerto, algo est√° en la red o la base de datos global no puede escribir el latido del coraz√≥n por alguna raz√≥n.  En la verificaci√≥n inversa, nos conectamos a una base de datos rota y verificamos su estado. </li><li>  Si todo lo dem√°s falla, observamos si la posici√≥n maestra est√° progresando o no, si hay registros en ella.  Si no sucede nada, entonces este servidor definitivamente no funciona. </li><li>  El √∫ltimo paso es en realidad el reemplazo autom√°tico. </li></ul><br>  El reemplazo autom√°tico es muy conservador, nunca quiere hacer muchas operaciones autom√°ticas. <br><br><ol><li>  Primero, verificamos si ha habido alguna operaci√≥n de topolog√≠a recientemente.  Tal vez este servidor acaba de ser agregado y algo en √©l a√∫n no se est√° ejecutando. </li><li>  Verificamos si hubo reemplazos en el mismo cl√∫ster en cualquier momento. </li><li>  Verifique qu√© l√≠mite de falla tenemos.  Si tenemos muchos problemas al mismo tiempo, 10, 20, entonces no los resolveremos autom√°ticamente, porque inadvertidamente podemos interrumpir el funcionamiento de todas las bases de datos. </li></ol><br>  Por lo tanto, <strong>resolvemos solo un problema a la vez</strong> . <br><br>  En consecuencia, para el servidor esclavo, comenzamos a clonar y simplemente lo eliminamos de la topolog√≠a, y si es maestro, entonces lanzamos el feylover, la llamada promoci√≥n de emergencia. <br><br><h3>  DBManager </h3><br>  DBManager es un servicio para administrar nuestras bases de datos.  Tiene: <br><br><ul><li>  planificador de tareas inteligente que sabe exactamente cu√°ndo comenzar el trabajo; </li><li>  registros y toda la informaci√≥n: qui√©n, cu√°ndo y qu√© se lanz√≥: esta es la fuente de la verdad; </li><li>  punto de sincronizaci√≥n </li></ul><br><img src="https://habrastorage.org/webt/xx/g6/pi/xxg6pitu-pau9ifyqivpa9wul3e.jpeg"><br><br>  DBManager es bastante simple arquitect√≥nicamente. <br><br><ul><li>  Hay clientes, ya sea DBA que hacen algo a trav√©s de la interfaz web, o scripts / servicios que escribieron DBA que acceden a trav√©s de gRPC. </li><li>  Hay sistemas externos como Wheelhouse y Naoru, que van a DBManager a trav√©s de gRPC. </li><li>  Hay un programador que entiende qu√© operaci√≥n, cu√°ndo y d√≥nde puede comenzar. </li><li>  Hay un trabajador muy est√∫pido que, cuando se trata de una operaci√≥n, lo inicia y lo verifica por PID.  El trabajador puede reiniciar, los procesos no se interrumpen.  Todos los trabajadores est√°n ubicados lo m√°s cerca posible de los servidores en los que tienen lugar las operaciones, de modo que, por ejemplo, al actualizar ACLS, no necesitamos hacer muchos viajes de ida y vuelta. </li><li>  En cada servidor SQL tenemos un DBAgent: este es un servidor RPC.  Cuando necesite realizar alguna operaci√≥n en el servidor, le enviaremos una solicitud RPC. </li></ul><br>  Tenemos una interfaz web para DBManager, donde puede ver las tareas que se ejecutan actualmente, los registros de estas tareas, qui√©n la inici√≥ y cu√°ndo, qu√© operaciones se realizaron para el servidor de una base de datos espec√≠fica, etc. <br><img src="https://habrastorage.org/webt/yj/tq/3m/yjtq3mrfimptba1kizre2bwie48.jpeg"><br><br>  Hay una interfaz CLI bastante simple donde puede ejecutar tareas y tambi√©n verlas en vistas convenientes. <br><img src="https://habrastorage.org/webt/qi/rc/vp/qircvpuoutswvmcpnuca4wem9cu.jpeg"><br><br><h3>  Remediaciones </h3><br>  Tambi√©n tenemos un sistema para responder a los problemas.  Cuando algo est√° roto, por ejemplo, la unidad falla, o alg√∫n servicio no funciona, <strong>Naoru</strong> funciona <strong>.</strong>  Este es el sistema que funciona en Dropbox, todos lo usan y est√° dise√±ado espec√≠ficamente para tareas tan peque√±as.  Habl√© sobre Naoru en mi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">informe</a> en 2016. <br><br>  <strong>Wheelhouse se</strong> basa en una m√°quina de <strong>estado</strong> y est√° dise√±ado para procesos largos.  Por ejemplo, necesitamos actualizar el kernel en todo MySQL en todo nuestro cl√∫ster de 6,000 m√°quinas.  Wheelhouse hace esto claramente: actualizaciones en el servidor esclavo, inicia la promoci√≥n, el esclavo se convierte en maestro, actualizaciones en el servidor maestro.  Esta operaci√≥n puede tomar un mes o incluso dos. <br><br><a name="monitoring"></a><h2>  Monitoreo </h2><br><img src="https://habrastorage.org/webt/vo/n_/ys/von_ysxtakb7m5ctiadwzewi3dw.jpeg"><br><br>  Esto es muy importante <br><br><blockquote>  Si no supervisa el sistema, lo m√°s probable es que no funcione. </blockquote><br>  Monitoreamos todo en MySQL: toda la informaci√≥n que podemos obtener de MySQL se almacena en alg√∫n lugar, podemos acceder a ella a tiempo.  Almacenamos informaci√≥n en InnoDb, estad√≠sticas sobre solicitudes, sobre transacciones, sobre la duraci√≥n de las transacciones, percentil sobre la duraci√≥n de las transacciones, sobre la replicaci√≥n, en la red, en general, una gran cantidad de m√©tricas. <br><br><h3>  Alerta </h3><br>  Tenemos 992 alertas configuradas.  De hecho, nadie est√° mirando las m√©tricas, me parece que no hay personas que vienen a trabajar y comienzan a mirar la tabla de m√©tricas, hay tareas m√°s interesantes. <br><img src="https://habrastorage.org/webt/q3/nj/sl/q3njslzahbxmh585uzeatrn4plm.jpeg"><br><br>  Por lo tanto, hay alertas que funcionan cuando se alcanzan ciertos valores de umbral.  <strong>Tenemos 992 alertas, pase lo que pase, lo descubriremos</strong> . <br><br><h3>  Incidentes </h3><br><img src="https://habrastorage.org/webt/bi/ce/tp/bicetpzgstf2lggazas6t27ru10.jpeg"><br><br>  Tenemos PagerDuty, un servicio a trav√©s del cual se env√≠an alertas a las personas responsables que comienzan a tomar medidas. <br><img src="https://habrastorage.org/webt/6n/hl/sr/6nhlsrj28tloxq4xg5ld3a-mpvy.jpeg"><br><br>  En este caso, se produjo un error en la promoci√≥n de emergencia, e inmediatamente despu√©s de que se registr√≥ una alerta, el maestro cay√≥.  Despu√©s de eso, el oficial de servicio verific√≥ qu√© imped√≠a la promoci√≥n de emergencia e hizo las operaciones manuales necesarias. <br><br>  Ciertamente analizaremos cada incidente que ha ocurrido, para cada incidente tenemos una tarea en el rastreador de tareas.  Incluso si este incidente es un problema en nuestras alertas, tambi√©n creamos una tarea, porque si el problema est√° en la l√≥gica y los umbrales de las alertas, entonces deben cambiarse.  Las alertas no solo deben estropear la vida de las personas.  Una alerta siempre es dolorosa, especialmente a las 4 a.m. <br><br><a name="testing"></a><h2>  Prueba </h2><br>  Al igual que con el monitoreo, estoy seguro de que todos lo est√°n probando.  Adem√°s de las pruebas unitarias con las que cubrimos nuestro c√≥digo, tenemos pruebas de integraci√≥n en las que probamos: <br><br><ul><li>  todas las topolog√≠as que tenemos; </li><li>  todas las operaciones en estas topolog√≠as. </li></ul><br>  Si tenemos operaciones de promoci√≥n, probamos las operaciones de promoci√≥n en la prueba de integraci√≥n.  Si tenemos clonaci√≥n, hacemos clonaci√≥n para todas las topolog√≠as que tenemos. <br><br>  <strong>Ejemplo de topolog√≠a</strong> <br><img src="https://habrastorage.org/webt/dv/hh/va/dvhhvacdschtqr7s_ynecm0lyk0.jpeg"><br><br>  Tenemos topolog√≠as para todas las ocasiones: 2 centros de datos con m√∫ltiples instancias, con fragmentos, sin fragmentos, con cl√∫steres, un centro de datos, generalmente casi cualquier topolog√≠a, incluso aquellos que no usamos, solo para ver. <br><img src="https://habrastorage.org/webt/f-/oy/pl/f-oypluoyzosnphjl_aukp4vsks.jpeg"><br><br>  En este archivo, solo tenemos la configuraci√≥n, qu√© servidores y con lo que necesitamos generar.  Por ejemplo, necesitamos elevar master, y decimos que necesitamos hacer esto con tal y tal informaci√≥n de instancia, con tal y tal base de datos en tal y tal puerto.  Casi todo va junto con Bazel, que crea una topolog√≠a sobre la base de estos archivos, inicia el servidor MySQL y luego comienza la prueba. <br><img src="https://habrastorage.org/webt/bg/zd/b_/bgzdb_dwnggh3kf8uv9f7mn9jpe.jpeg"><br><br>  La prueba parece muy simple: indicamos qu√© topolog√≠a se est√° utilizando.  En esta prueba, probamos auto_replace. <br><br><ul><li>  Creamos el servicio auto_replace, lo iniciamos. </li><li>  Matamos al maestro en nuestra topolog√≠a, esperamos un momento y vemos que el esclavo objetivo se ha convertido en maestro.  Si no, la prueba fall√≥. </li></ul><br><h3>  Etapas </h3><br>  Los entornos de escenario son las mismas bases de datos que en producci√≥n, pero no hay tr√°fico de usuarios en ellos, pero hay algo de tr√°fico sint√©tico que es similar a la producci√≥n a trav√©s de Percona Playback, sysbench y sistemas similares. <br><br>  En Percona Playback, registramos el tr√°fico, luego lo perdemos en el entorno del escenario con diferentes intensidades, podemos perder 2-3 veces m√°s r√°pido.  Es decir, es artificial, pero muy cercano a la carga real. <br><br>  Esto es necesario porque en las pruebas de integraci√≥n no podemos probar nuestra producci√≥n.  No podemos probar la alerta o el hecho de que las m√©tricas funcionen.  En la etapa de prueba, probamos alertas, m√©tricas, operaciones, peri√≥dicamente matamos los servidores y vemos que se recopilan normalmente. <br><br>  Adem√°s, probamos toda la automatizaci√≥n juntos, porque en las pruebas de integraci√≥n, lo m√°s probable es que se pruebe una parte del sistema, y ‚Äã‚Äãen la puesta en escena, todos los sistemas automatizados funcionan simult√°neamente.  A veces piensas que el sistema se comportar√° de esta manera y no de otra manera, pero puede comportarse de una manera completamente diferente. <br><br><h3>  DRT (prueba de recuperaci√≥n de desastres) </h3><br>  Tambi√©n realizamos pruebas en producci√≥n, directamente sobre bases reales.  Esto se llama prueba de recuperaci√≥n de desastres.  ¬øPor qu√© necesitamos esto? <br><br>  ‚óè Queremos probar nuestras garant√≠as. <br><br>  Esto lo hacen muchas grandes empresas.  Por ejemplo, Google tiene un servicio que funcion√≥ de manera tan estable, el 100% del tiempo, que todos los servicios que lo utilizaron decidieron que este servicio es realmente 100% estable y nunca falla.  Por lo tanto, Google tuvo que abandonar este servicio a prop√≥sito, para que los usuarios tengan en cuenta esta posibilidad. <br><br>  As√≠ que estamos, tenemos la garant√≠a de que MySQL funciona, ¬°y a veces no funciona!  Y tenemos la garant√≠a de que puede no funcionar durante un cierto per√≠odo de tiempo, los clientes deben tener esto en cuenta.  De vez en cuando, matamos al maestro de producci√≥n, o si queremos hacer un faylover, matamos a todos los esclavos para ver c√≥mo se comporta la replicaci√≥n semisincr√≥nica. <br><br>  ‚óè Los clientes est√°n preparados para estos errores (reemplazo y muerte del maestro) <br><br>  ¬øPor qu√© es eso bueno?  Tuvimos un caso cuando durante la promoci√≥n 4 fragmentos de 1600, la disponibilidad cay√≥ al 20%.  Parece que algo est√° mal, para 4 fragmentos de 1600 deber√≠a haber otros n√∫meros.  Las failovers para este sistema eran raras, aproximadamente una vez al mes, y todos decidieron: "Bueno, es una failover, sucede". <br><br>  En alg√∫n momento, cuando cambiamos a un nuevo sistema, una persona decidi√≥ optimizar esos dos servicios de grabaci√≥n de latidos y los combin√≥ en uno.  Este servicio hizo algo m√°s y, al final, muri√≥ y los latidos del coraz√≥n dejaron de grabar.  Dio la casualidad de que para este cliente ten√≠amos 8 faylovers al d√≠a.  Todo yac√≠a - 20% de disponibilidad. <br><br>  Result√≥ que en este cliente mantener vivo es de 6 horas.  En consecuencia, tan pronto como el maestro muri√≥, mantuvimos todas las conexiones durante otras 6 horas.  El grupo no pudo continuar funcionando: sus conexiones se mantienen, es limitado y no funciona.  Fue reparado. <br><br>  Hacemos el feylover nuevamente, ya no es del 20%, pero a√∫n es mucho.  Algo sigue mal.  Result√≥ que era un error en la implementaci√≥n del grupo.  Cuando se le solicit√≥, el grupo se convirti√≥ en muchos fragmentos y luego conect√≥ todo esto.  Si algunos fragmentos eran febriles, se produjo alguna condici√≥n de carrera en el c√≥digo Go y todo el grupo se obstruy√≥.  Todos estos fragmentos ya no pod√≠an funcionar. <br><br>  Las pruebas de recuperaci√≥n ante desastres son muy √∫tiles, ya que los clientes deben estar preparados para estos errores, deben verificar su c√≥digo. <br><br>  ‚óè Adem√°s, las pruebas de recuperaci√≥n ante desastres son buenas porque se realizan durante el horario comercial y todo est√° en su lugar, menos estr√©s, la gente sabe lo que suceder√° ahora.  Esto no sucede de noche, y es genial. <br><br><h2>  Conclusi√≥n </h2><br>  1. Todo necesita ser automatizado, nunca lo tengas en tus manos. <br>  Cada vez que alguien sube al sistema con nuestras manos, todo muere y se rompe en nuestro sistema, ¬°cada vez!  - Incluso en operaciones simples.  Por ejemplo, un esclavo muri√≥, una persona tuvo que agregar un segundo, pero decidi√≥ eliminar al esclavo muerto con sus manos de la topolog√≠a.  Sin embargo, en lugar del difunto, copi√≥ al comando en vivo: el maestro se qued√≥ sin esclavo.  Dichas operaciones no deben hacerse manualmente. <br><br>  2. Las pruebas deben ser continuas y automatizadas (y en producci√≥n). <br>  Su sistema est√° cambiando, su infraestructura est√° cambiando.  Si marc√≥ una vez, y pareci√≥ funcionar, esto no significa que funcionar√° ma√±ana.  Por lo tanto, debe realizar pruebas autom√°ticas constantemente todos los d√≠as, incluso en producci√≥n. <br><br>  3. Aseg√∫rese de tener clientes (bibliotecas). <br>  Los usuarios pueden no saber c√≥mo funcionan las bases de datos.  Es posible que no entiendan por qu√© se necesitan tiempos de espera, mantener vivo.  Por lo tanto, es mejor tener estos clientes: estar√° m√°s tranquilo. <br><br>  4. Es necesario determinar sus principios para construir el sistema y sus garant√≠as, y cumplir siempre con ellos. <br><br>  Por lo tanto, puede soportar 6 mil servidores de bases de datos. <br><br><div class="spoiler">  <b class="spoiler_title">En las preguntas posteriores al informe, y especialmente las respuestas a ellas, tambi√©n hay mucha informaci√≥n √∫til.</b> <div class="spoiler_text"><h2>  Preguntas y respuestas <br></h2><br><blockquote>  - ¬øQu√© suceder√° si hay un desequilibrio en la carga de fragmentos? ¬øAlguna metainformaci√≥n sobre alg√∫n archivo result√≥ ser m√°s popular?  ¬øEs posible difundir este fragmento, o la carga en los fragmentos no difiere en ning√∫n lugar por orden de magnitud? </blockquote><br>  Ella no difiere en √≥rdenes de magnitud.  Se distribuye casi normalmente.  Tenemos aceleraci√≥n, es decir, no podemos sobrecargar el fragmento, de hecho, estamos acelerando a nivel del cliente.  En general, sucede que alguna estrella sube una foto y el fragmento pr√°cticamente explota.  Entonces prohibimos este enlace <br><br><blockquote>  - Dijiste que tienes 992 alertas.  ¬øPodr√≠a dar m√°s detalles sobre lo que es? ¬øEst√° listo para usar o se ha creado?  Si se crea, ¬øes trabajo manual o algo as√≠ como el aprendizaje autom√°tico? </blockquote><br>  Todo esto se crea manualmente.  Tenemos nuestro propio sistema interno llamado Vortex, donde se almacenan las m√©tricas, se admiten alertas.  Hay un archivo yaml que dice que existe una condici√≥n, por ejemplo, que las copias de seguridad deben ejecutarse todos los d√≠as, y si se cumple esta condici√≥n, la alerta no funciona.  Si no se ejecuta, entonces llega una alerta. <br><br>  Este es nuestro desarrollo interno, porque pocas personas pueden almacenar tantas m√©tricas como necesitemos. <br><br><blockquote>  - ¬øQu√© tan fuertes deben ser los nervios para hacer DRT?  Ca√≠ste, CODIFICASTE, no sube, con cada minuto de p√°nico m√°s. </blockquote><br>  En general, trabajar en bases de datos es realmente un dolor.  Si la base de datos falla, el servicio no funciona, todo el Dropbox no funciona.  Este es un verdadero dolor.  DRT es √∫til porque es un reloj de negocios.  Es decir, estoy listo, estoy sentado en mi escritorio, tom√© caf√©, estoy fresco, estoy listo para hacer cualquier cosa. <br><br>  Peor cuando sucede a las 4 a.m., y no es DRT.  Por ejemplo, la √∫ltima falla importante que tuvimos recientemente.  Al inyectar un nuevo sistema, olvidamos establecer la puntuaci√≥n OOM para nuestro MySQL.  Hab√≠a otro servicio que le√≠a binlog.  En alg√∫n momento, nuestro operador es manual, ¬°de nuevo manualmente!  - ejecuta el comando para eliminar cierta informaci√≥n en la tabla de suma de comprobaci√≥n de Percona.  Solo una simple eliminaci√≥n, una operaci√≥n simple, pero esta operaci√≥n gener√≥ un enorme binlog.  El servicio ley√≥ este binlog en la memoria, ¬øOOM Killer vino y pens√≥ a qui√©n matar?  ¬°Y olvidamos establecer el puntaje OOM, y mata a MySQL! <br><br>  Tenemos 40 maestros muriendo a las 4 a.m.  Cuando mueren 40 maestros, es realmente muy aterrador y peligroso.  DRT no da miedo y no es peligroso.  Nos quedamos por alrededor de una hora. <br><br>  Por cierto, DRT es una buena manera de ensayar esos momentos para que sepamos exactamente qu√© secuencia de acciones se necesita si algo se rompe en masa. <br><br><blockquote>  - Me gustar√≠a aprender m√°s sobre c√≥mo cambiar maestro-maestro.  Primero, ¬øpor qu√© no se usa un cl√∫ster, por ejemplo?  Un cl√∫ster de base de datos, es decir, no un maestro-esclavo con conmutaci√≥n, sino una aplicaci√≥n maestro-maestro, de modo que si uno cae, no da miedo. </blockquote><br>  ¬øTe refieres a algo como replicaci√≥n grupal, grupo de galera, etc.?  Me parece que la aplicaci√≥n grupal a√∫n no est√° lista para la vida.  Desafortunadamente, a√∫n no hemos probado Galera.  Esto es genial cuando un faylover est√° dentro de su protocolo, pero, desafortunadamente, tienen muchos otros problemas, y no es tan f√°cil cambiar a esta soluci√≥n. <br><br><blockquote>  - Parece que en MySQL 8 hay algo as√≠ como un cl√∫ster InnoDb.  ¬øNo lo intentaste? </blockquote><br>  Todav√≠a tenemos un valor de 5.6.  No s√© cu√°ndo cambiaremos a 8. Quiz√°s lo intentemos. <br><br><blockquote>  - En este caso, si tiene un gran maestro, al cambiar de uno a otro, resulta que la cola se acumula en los servidores esclavos con una carga alta.  Si el maestro se extingue, ¬øes necesario que llegue la cola para que el esclavo cambie al modo maestro, o se hace de alguna manera diferente? </blockquote><br>  La carga en el maestro est√° regulada por semisync.  Semisync limita la grabaci√≥n maestra al rendimiento del servidor esclavo.  Por supuesto, puede ser que la transacci√≥n haya llegado, la sincronizaci√≥n funcion√≥, pero los esclavos perdieron esta transacci√≥n durante mucho tiempo.  Luego debe esperar hasta que el esclavo pierda esta transacci√≥n hasta el final. <br><br><blockquote>  - Pero entonces nuevos datos vendr√°n a dominar, y ser√° necesario ... </blockquote><br>  Cuando comenzamos el proceso de promoci√≥n, deshabilitamos las E / S.  Despu√©s de eso, el maestro no puede escribir nada porque la semisincronizaci√≥n se replica.  La lectura fantasma puede venir, desafortunadamente, pero este ya es otro problema. <br><br><blockquote>  - Estas son todas m√°quinas de estado hermosas: ¬øen qu√© est√°n escritas las secuencias de comandos y qu√© tan dif√≠cil es agregar un nuevo paso?  ¬øQu√© debe hacerse a la persona que escribe este sistema? </blockquote><br>  Todos los scripts est√°n escritos en Python, todos los servicios est√°n escritos en Go.  Esta es nuestra pol√≠tica.  Cambiar la l√≥gica es f√°cil, solo en el c√≥digo de Python que genera el diagrama de estado. <br><br><blockquote>  - Y puedes leer m√°s sobre las pruebas.  ¬øC√≥mo se escriben las pruebas, c√≥mo implementan los nodos en una m√°quina virtual? ¬øSon estos contenedores? </blockquote><br>  Si  Vamos a probar con la ayuda de Bazel.  Hay algunos archivos de configuraci√≥n (json) y Bazel recoge un script que crea la topolog√≠a para nuestra prueba usando este archivo de configuraci√≥n.  Se describen diferentes topolog√≠as all√≠. <br><br>  Todo funciona para nosotros en contenedores acoplables: funciona en CI o en Devbox.  Tenemos un sistema Devbox.  Todos estamos desarrollando en alg√∫n servidor remoto, y esto puede funcionar en √©l, por ejemplo.  All√≠ tambi√©n se ejecuta dentro de Bazel, dentro de un contenedor acoplable o en el Sandbox de Bazel.  Bazel es muy complicado pero divertido. <br><br><blockquote>  - Cuando realiz√≥ 4 instancias en un servidor, ¬øperdi√≥ la eficiencia de la memoria? </blockquote><br>  Cada instancia se ha vuelto m√°s peque√±a.  En consecuencia, cuanto menos memoria opere MySQL, m√°s f√°cil ser√° para √©l vivir.  Cualquier sistema es m√°s f√°cil de operar con una peque√±a cantidad de memoria.  En este lugar, no hemos perdido nada.  Tenemos los grupos C m√°s simples que limitan estas instancias de la memoria. <br><br><blockquote>  - Si tiene 6,000 servidores que almacenan bases de datos, ¬øpuede nombrar cu√°ntos miles de millones de petabytes est√°n almacenados en sus archivos? </blockquote><br>  Estas son docenas de exabytes, hemos vertido datos de Amazon durante un a√±o. <br><br><blockquote>  - Resulta que al principio ten√≠as 8 servidores, 200 fragmentos en ellos, luego 400 servidores con 4 fragmentos cada uno.  Tienes 1600 fragmentos, ¬øes alg√∫n tipo de valor codificado?  ¬øNunca puedes volver a hacerlo?  ¬øTe doler√° si necesitas, por ejemplo, 3.200 fragmentos? </blockquote><br>  S√≠, originalmente era 1600. Esto se hizo hace menos de 10 a√±os, y todav√≠a vivimos.  Pero todav√≠a tenemos 4 fragmentos, 4 veces a√∫n podemos aumentar el espacio. <br><br><blockquote>  - ¬øC√≥mo mueren los servidores, principalmente por qu√© razones?  ¬øQu√© sucede con m√°s frecuencia, con menos frecuencia, y es especialmente interesante, se producen los cap√≠tulos espont√°neos? </blockquote><br>  Lo m√°s importante es que los discos salgan volando.  Tenemos RAID 0: el disco se bloque√≥, el maestro muri√≥.  Este es el problema principal, pero es m√°s f√°cil para nosotros reemplazar este servidor.  Google es m√°s f√°cil de reemplazar el centro de datos, todav√≠a tenemos un servidor.  Casi nunca tuvimos la suma de verificaci√≥n de la corrupci√≥n.  Para ser sincero, no recuerdo cu√°ndo fue la √∫ltima vez.  A menudo actualizamos el asistente.  Nuestro tiempo de vida para un maestro est√° limitado a 60 d√≠as.  No puede vivir m√°s tiempo, despu√©s de eso lo reemplazamos con un nuevo servidor, porque por alguna raz√≥n algo se est√° acumulando constantemente en MySQL, y despu√©s de 60 d√≠as vemos que los problemas comienzan a ocurrir.  Quiz√°s no en MySQL, quiz√°s en Linux. <br><br>   ,          .     60 ,    .      . <br><br><blockquote> ‚Äî  ,    6        . ,   JPEG   ,     JPEG,  ,      ?  , ,      -   ?    ‚Äî      ,       ? </blockquote><br>     ,  .   ‚Äî  Dropbox    . <br><br><blockquote> ‚Äî      ?         ?     , ,  - ,    , ? ,   10   . ,  7     ,    6    ,    .    ? </blockquote><br>   Dropbox  - ,       .   .  ,    ,       ,   -  . <br><br>  ,    .  ,  ,      ,      .        - ,     6 ,   ,     ,    ,    . <br></div></div><br><blockquote>       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">facebook</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">youtube-</a> ‚Äî          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Highload++ 2018</a> .      , <strong> 1 </strong>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  </a> . <br></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es417315/">https://habr.com/ru/post/es417315/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es417303/index.html">Hack para admitir botones de auriculares con Windows Android</a></li>
<li><a href="../es417305/index.html">Ultima Online: una mirada entre bastidores</a></li>
<li><a href="../es417307/index.html">Glaucoma: ¬øno has o√≠do hablar de ella? Conoce al asesino en serie de visi√≥n silenciosa</a></li>
<li><a href="../es417309/index.html">Afortunadamente, el gerente de ITSM: c√≥mo la profesi√≥n del futuro ayuda a ampliar las fronteras de la mesa de servicio</a></li>
<li><a href="../es417311/index.html">Crear un bot para participar en la mini copa AI 2018 basada en una red neuronal recurrente</a></li>
<li><a href="../es417317/index.html">En Highload ++ 2018 a toda velocidad</a></li>
<li><a href="../es417319/index.html">Sistemas en el caso o lo que realmente est√° bajo la cubierta del microprocesador</a></li>
<li><a href="../es417321/index.html">¬øC√≥mo buscamos maestros de cursos en l√≠nea entre desarrolladores?</a></li>
<li><a href="../es417323/index.html">Problemas para garantizar el 100% de accesibilidad del proyecto</a></li>
<li><a href="../es417325/index.html">Jornada de puertas abiertas de netrolog√≠a, tema de ciencia de datos</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>