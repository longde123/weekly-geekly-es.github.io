<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👽 🔺 🌂 Changements majeurs dans les principales architectures de puces ⏳ 🈯️ 👨🏽‍🔬</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="L'introduction de l'IA au niveau de la puce vous permet de traiter plus de données localement, car une augmentation du nombre d'appareils ne donne plu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Changements majeurs dans les principales architectures de puces</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/422787/"><h4>  <font color="gray">L'introduction de l'IA au niveau de la puce vous permet de traiter plus de données localement, car une augmentation du nombre d'appareils ne donne plus le même effet</font> </h4><br>  Les fabricants de puces travaillent sur de nouvelles architectures qui augmentent considérablement la quantité de données traitées par watt et par cycle.  Le terrain est préparé pour l'une des plus grandes révolutions de l'architecture de puces de ces dernières décennies. <br><br>  Tous les principaux fabricants de puces et de systèmes changent de direction de développement.  Ils sont entrés dans la course aux architectures, qui prévoit un changement de paradigme en tout: des méthodes de lecture et d'écriture à la mémoire, à leur traitement et, finalement, à la disposition de divers éléments sur une puce.  Bien que la miniaturisation se poursuive, personne ne parie sur la mise à l'échelle pour faire face à la croissance explosive des données des capteurs et à l'augmentation du volume de trafic entre les machines. <br><a name="habracut"></a><br>  Parmi les changements dans les nouvelles architectures: <br><br><ul><li>  De nouvelles méthodes pour traiter une plus grande quantité de données en 1 cycle d'horloge, parfois avec moins de précision ou par priorité de certaines opérations, selon l'application. </li><li>  De nouvelles architectures de mémoire qui changent la façon dont nous stockons, lisons, écrivons et accédons aux données. </li><li>  Modules de traitement plus spécialisés situés dans tout le système près de la mémoire.  Au lieu d'un processeur central, les accélérateurs sont sélectionnés en fonction du type de données et de l'application. </li><li>  Dans le domaine de l'IA, des travaux sont en cours pour combiner différents types de données sous forme de modèles, ce qui augmente efficacement la densité des données tout en minimisant les écarts entre les différents types. </li><li>  Maintenant, la disposition du boîtier est le composant principal de l'architecture, avec une attention de plus en plus grande accordée à la facilité de changer ces conceptions. </li></ul><br>  «Il existe plusieurs tendances qui affectent les avancées technologiques», a déclaré Stephen Wu, un ingénieur distingué de Rambus.  - Dans les centres de données, vous tirez le meilleur parti du matériel et des logiciels.  Sous cet angle, les propriétaires de centres de données envisagent l'économie.  Introduire quelque chose de nouveau coûte cher.  Mais les goulots d'étranglement changent, des puces spécialisées sont donc introduites pour un calcul plus efficace.  Et si vous réduisez les flux de données dans les deux sens vers les E / S et la mémoire, cela peut avoir un impact important. » <br><br>  Les changements sont plus évidents au bord de l'infrastructure informatique, c'est-à-dire parmi les capteurs d'extrémité.  Les fabricants ont soudain réalisé que des dizaines de milliards d'appareils généreraient trop de données: un tel volume ne pouvait pas être envoyé vers le cloud pour traitement.  Mais le traitement de toutes ces données en périphérie pose d'autres problèmes: il nécessite des améliorations de performances majeures sans augmentation significative de la consommation électrique. <br><br>  «Il y a une nouvelle tendance vers une précision moindre», a déclaré Robert Ober, architecte principal de la plate-forme Tesla chez Nvidia.  - Ce ne sont pas seulement des cycles de calcul.  Il s'agit d'un conditionnement de données plus intensif en mémoire, où le format d'instructions 16 bits est utilisé. » <br><br>  Aubert estime que grâce à une série d'optimisations architecturales dans un avenir prévisible, vous pouvez doubler la vitesse de traitement tous les deux ans.  "Nous verrons une augmentation spectaculaire de la productivité", a-t-il déclaré.  - Pour cela, vous devez faire trois choses.  Le premier est l'informatique.  Le second est la mémoire.  La troisième zone est la bande passante hôte et la bande passante d'E / S.  Beaucoup de travail doit être fait pour optimiser le stockage et la pile réseau. » <br><br>  Quelque chose est déjà en cours d'implémentation.  Dans une présentation à la conférence Hot Chips 2018, Jeff Rupley, architecte en chef du Samsung Research Center de Samsung, a souligné plusieurs changements architecturaux majeurs du processeur M3.  Un inclut plus d'instructions par temps - six au lieu de quatre dans la dernière puce M2.  De plus, la prédiction de branchement sur les réseaux de neurones a été implémentée et la file d'attente d'instructions a été doublée. <br><br>  De tels changements déplacent le point d'innovation de la fabrication directe de microcircuits à l'architecture et au design d'une part et à la disposition des éléments de l'autre côté de la chaîne de production.  Bien que les innovations continuent de se poursuivre dans les processus technologiques, ce n'est qu'aux dépens de celui-ci qu'il est incroyablement difficile d'obtenir une augmentation de 15 à 20% de la productivité et de la puissance dans chaque nouveau modèle de puce - et cela ne suffit pas pour faire face à la croissance rapide du volume de données. <br><br>  «Les changements ont lieu à un rythme exponentiel», a déclaré Victor Pan, président et chef de la direction de Xilinx, dans un discours à la conférence Hot Chips, «10 zettaoctets [10 <sup>21</sup> octets] de données seront générés chaque année, et la plupart d'entre eux ne sont pas structurés.» <br><br><h1>  De nouvelles approches de la mémoire </h1><br>  Travailler avec autant de données nécessite de repenser chaque composant du système, des méthodes de traitement des données à leur stockage. <br><br>  «Il y a eu de nombreuses tentatives pour créer de nouvelles architectures de mémoire», a déclaré Carlos Machin, directeur principal de l'innovation chez eSilicon EMEA.  - Le problème est que vous devez lire toutes les lignes et sélectionner un bit dans chacune.  Une option consiste à créer une mémoire qui peut être lue de gauche à droite, ainsi que de haut en bas.  Vous pouvez aller encore plus loin et ajouter du calcul à la mémoire. » <br><br>  Ces changements comprennent la modification des méthodes de lecture de la mémoire, l'emplacement et le type des éléments de traitement, ainsi que l'introduction de l'IA pour prioriser le stockage, le traitement et le mouvement des données dans tout le système. <br><br>  «Et si, dans le cas de données éparses, nous ne pouvons lire qu'un octet de ce tableau à la fois - ou peut-être huit octets consécutifs du même chemin d'octets sans gaspiller d'énergie sur d'autres octets ou chemins d'octets qui ne nous intéressent pas ?  "Demande Mark Greenberg, directeur du marketing produit Cadence."  - À l'avenir, cela est possible.  Si vous regardez l'architecture de HBM2, par exemple, la pile est organisée en 16 canaux virtuels de 64 bits chacun, et vous n'avez besoin que de 4 mots 64 bits consécutifs pour accéder à n'importe quel canal virtuel.  Ainsi, il est possible de créer des tableaux de données d'une largeur de 1024 bits, d'écrire horizontalement, mais de lire verticalement quatre mots de 64 bits à la fois. " <br><br>  La mémoire est l'un des principaux composants de l'architecture de von Neumann, mais maintenant elle est également devenue l'un des principaux domaines d'expérimentation.  «L'ennemi principal est les systèmes de mémoire virtuelle, où les données sont déplacées de manière plus artificielle», a déclaré Dan Bouvier, architecte en chef des produits clients chez AMD.  - Ceci est une émission diffusée.  Nous sommes habitués à cela dans le domaine du graphisme.  Mais si nous résolvons les conflits dans la banque de mémoire DRAM, nous obtenons un streaming beaucoup plus efficace.  Un GPU séparé peut alors utiliser la DRAM dans une plage d'efficacité de 90%, ce qui est très bon.  Mais si vous configurez le streaming sans interruption, le CPU et l'APU tomberont également dans la plage d'efficacité de 80% à 85%. » <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6af/814/e8c/6af814e8c5d0a2f4b9274d165aa8f622.png"><br>  <i><font color="gray">Fig.</font></i>  <i><font color="gray">1. Architecture von Neumann.</font></i>  <i><font color="gray">Source: Ingénierie des semi-conducteurs</font></i> <br><br>  IBM développe un type différent d'architecture de mémoire, qui est essentiellement une version mise à niveau de l'agrégation de disques.  L'objectif est qu'au lieu d'utiliser un seul lecteur, le système puisse utiliser arbitrairement toute la mémoire disponible via un connecteur, que Jeff Stucheli, architecte matériel IBM, appelle le "Swiss Army Knife" pour connecter les éléments.  L'avantage de l'approche est qu'elle vous permet de mélanger et de faire correspondre différents types de données. <br><br>  «Le processeur devient le centre d'une interface de signalisation haute performance», explique Stucelli.  «Si vous modifiez la microarchitecture, le cœur effectue plus d'opérations par cycle à la même fréquence.» <br><br>  La connectivité et le débit doivent garantir le traitement d'un volume radicalement accru de données générées.  "Les principaux goulots d'étranglement se trouvent maintenant dans les emplacements de déplacement des données", a déclaré Wu de Rambus.  "L'industrie a fait un excellent travail en augmentant la vitesse de l'informatique."  Mais si vous attendez des données ou des modèles de données spécialisés, vous devez exécuter la mémoire plus rapidement.  Ainsi, si vous regardez la DRAM et la NVM, les performances dépendent du modèle de trafic.  Si les données sont en streaming, la mémoire offrira de très bonnes performances.  Mais si les données arrivent par gouttes aléatoires, elles sont moins efficaces.  Et peu importe ce que vous faites, avec une augmentation de volume, vous devez toujours le faire plus rapidement. » <br><br><h1>  Plus d'informatique, moins de trafic. </h1><br>  Le problème est aggravé par le fait qu'il existe plusieurs types différents de données générées à différentes fréquences et vitesses par des dispositifs en périphérie.  Pour que ces données puissent circuler librement entre différents modules de traitement, la gestion doit devenir beaucoup plus efficace que par le passé. <br><br>  «Il existe quatre configurations principales: plusieurs-à-plusieurs, des sous-systèmes de mémoire, des E / S basse consommation et des grilles et topologies en anneau», explique Charlie Janak, président-directeur général d'Arteris IP.  - Vous pouvez placer les quatre sur une puce, ce qui se produit avec les puces IoT clés.  Ou vous pouvez ajouter des sous-systèmes HBM à haut débit.  Mais la complexité est énorme, car certaines de ces charges de travail sont très spécifiques et la puce a plusieurs tâches différentes.  Si vous regardez certaines de ces micropuces, elles obtiennent d'énormes quantités de données.  C'est dans des systèmes tels que les radars et les lidars de voiture.  Ils ne peuvent exister sans certaines interconnexions avancées. » <br><br>  La tâche est de savoir comment minimiser le mouvement des données, mais en même temps maximiser le flux de données lorsque cela est nécessaire - et trouver en quelque sorte un équilibre entre le traitement local et centralisé sans augmenter inutilement la consommation d'énergie. <br><br>  «D'une part, il s'agit d'un problème de bande passante», a déclaré Rajesh Ramanujam, responsable du marketing produit pour NetSpeed ​​Systems.  - Vous voulez réduire le trafic autant que possible, alors transférez les données plus près du processeur.  Mais si vous avez encore besoin de déplacer les données, il est conseillé de les compacter autant que possible.  Mais rien n'existe par lui-même.  Tout doit être planifié au niveau du système.  A chaque étape, plusieurs axes interdépendants doivent être considérés.  Ils déterminent si vous utilisez la mémoire de la manière traditionnelle de lecture et d'écriture, ou si vous utilisez de nouvelles technologies.  Dans certains cas, vous devrez peut-être modifier la façon dont vous stockez les données elles-mêmes.  Si vous avez besoin de performances supérieures, cela signifie généralement une augmentation de la surface de la puce, ce qui affecte la dissipation thermique.  Et maintenant, compte tenu de la sécurité fonctionnelle, la surcharge de données ne peut pas être autorisée. » <br><br>  C'est pourquoi tant d'attention est accordée au traitement des données à la périphérie et à la bande passante du canal par divers modules de traitement des données.  Mais au fur et à mesure que vous développez différentes architectures, la manière et le lieu de mise en œuvre de ce traitement de données sont très différents. <br><br>  Par exemple, Marvell a introduit un contrôleur SSD avec IA intégrée pour gérer la lourde charge de calcul en périphérie.  Le moteur AI peut être utilisé pour l'analyse directement à l'intérieur du disque SSD. <br><br>  «Vous pouvez charger des modèles directement dans le matériel et effectuer le traitement matériel sur le contrôleur SSD», a déclaré Ned Varnitsa, ingénieur en chef de Marvell.  - Aujourd'hui, il rend le serveur dans le cloud (hôte).  Mais si chaque disque envoie des données vers le cloud, cela créera une énorme quantité de trafic réseau.  Il est préférable de faire le traitement en périphérie, et l'hôte émet uniquement une commande, qui n'est que des métadonnées.  Plus vous avez de disques, plus la puissance de traitement est importante.  Il s'agit d'un énorme avantage de la réduction du trafic. " <br><br>  Cette approche est particulièrement intéressante car elle s'adapte à différentes données selon l'application.  Ainsi, l'hôte peut générer une tâche et l'envoyer au périphérique de stockage pour traitement, après quoi seuls les métadonnées ou les résultats des calculs sont renvoyés.  Dans un autre scénario, un périphérique de stockage peut stocker des données, les prétraiter et générer des métadonnées, des balises et des index, qui sont ensuite récupérés par l'hôte au besoin pour une analyse plus approfondie. <br><br>  C'est l'une des options possibles.  Il y en a d'autres.  Rupli de Samsung a souligné l'importance du traitement et de la fusion des idiomes qui peuvent décoder deux instructions et les combiner en une seule opération. <br><br><h1>  L'IA s'occupe du contrôle et de l'optimisation </h1><br>  À tous les niveaux d'optimisation, l'intelligence artificielle est utilisée - c'est l'un des éléments vraiment nouveaux de l'architecture de la puce.  Au lieu de permettre au système d'exploitation et au middleware de gérer les fonctions, cette fonction de surveillance est répartie sur la puce, entre les puces et au niveau du système.  Dans certains cas, des réseaux de neurones matériels sont introduits. <br><br>  «Il ne s'agit pas tant de rassembler davantage, mais de changer l'architecture traditionnelle», explique Mike Gianfanya, vice-président du marketing, eSilicon.  - Avec l'aide de l'IA et de l'apprentissage automatique, vous pouvez distribuer des éléments à travers le système, obtenant un traitement plus efficace avec les prévisions.  Ou vous pouvez utiliser des puces distinctes qui fonctionnent indépendamment dans le système ou dans le module. » <br><br>  ARM a développé sa première puce d'apprentissage automatique, qu'elle prévoit de publier plus tard cette année pour plusieurs marchés.  «Il s'agit d'un nouveau type de processeur», a déclaré Ian Bratt, ingénieur émérite d'ARM.  - Il comprend un bloc fondamental - c'est un moteur informatique, ainsi qu'un moteur MAC, un moteur DMA avec un module de contrôle et un réseau de diffusion.  Au total, 16 cœurs de calcul sont fabriqués à l'aide de la technologie de traitement à 7 nm, qui produisent 4 TeraOps à une fréquence de 1 GHz. » <br><br>  Comme ARM fonctionne avec un écosystème partenaire, sa puce est plus polyvalente et personnalisable que les autres puces AI / ML en cours de développement.  Au lieu d'une structure monolithique, il sépare le traitement par fonction, de sorte que chaque module informatique fonctionne sur une carte d'entités distincte.  Bratt a identifié quatre ingrédients clés: la planification statique, le pliage efficace, les mécanismes de rétrécissement et l'adaptation programmée aux futurs changements de conception. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fbe/7e4/ab1/fbe7e4ab11731ad3dafebc992b9c7bf2.png"><br>  <i><font color="gray">Fig.</font></i>  <i><font color="gray">2. Architecture ML du processeur ARM.</font></i>  <i><font color="gray">Source: ARM / Hot Chips</font></i> <br><br>  Pendant ce temps, Nvidia a choisi une tactique différente: créer un moteur dédié d'apprentissage en profondeur à côté du GPU pour optimiser le traitement d'image et de vidéo. <br><br><h1>  Conclusion </h1><br>  En utilisant certaines ou toutes ces approches, les fabricants de puces s'attendent à doubler leurs performances tous les deux ans, en suivant une croissance explosive des données, tout en restant dans le cadre serré des budgets de consommation d'énergie.  Mais ce n'est pas seulement plus informatique.  Il s'agit d'un changement dans la plate-forme de conception de puces et de systèmes, lorsque le volume croissant de données, plutôt que les limitations matérielles et logicielles, devient le principal facteur. <br><br>  «Lorsque les ordinateurs sont apparus dans les entreprises, il semblait à beaucoup que le monde autour de nous s'était accéléré», a déclaré Aart de Gues, président et chef de la direction de Synopsys.  - Ils ont fait des comptes sur des morceaux de papier avec des piles de livres.  Le grand livre s'est transformé en une pile de cartes perforées pour l'impression et l'informatique.  Un énorme changement s'est produit et nous le voyons à nouveau.  Avec l'avènement mental des simples ordinateurs de calcul, l'algorithme des actions n'a pas changé: vous pouvez tracer chaque étape.  Mais maintenant, quelque chose d'autre se produit qui pourrait conduire à une nouvelle accélération.  C’est comme sur un champ agricole d’inclure l’arrosage et d’appliquer un certain type d’engrais seulement un certain jour, lorsque la température atteint le niveau souhaité.  Cette utilisation du machine learning est une optimisation qui n'était pas évidente dans le passé. » <br><br>  Il n'est pas seul dans cette appréciation.  «Les nouvelles architectures seront adoptées», a déclaré Wally Raines, président et chef de la direction de Mentor, Siemens Business.  - Ils seront conçus.  L'apprentissage automatique sera utilisé dans de nombreux cas ou dans la plupart des cas, car votre cerveau apprend de sa propre expérience.  J'ai visité 20 entreprises ou plus qui développent des processeurs d'IA spécialisés d'un type ou d'un autre, et chacun d'eux a sa propre petite niche.  Mais vous verrez de plus en plus leur application dans des applications spécifiques, et ils compléteront l'architecture traditionnelle de von Neumann.  L'informatique neuromorphique deviendra courante.  Il s'agit d'une grande étape dans l'efficacité informatique et la réduction des coûts.  Les appareils mobiles et les capteurs commenceront à faire le travail que les serveurs font aujourd'hui. » </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr422787/">https://habr.com/ru/post/fr422787/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr422775/index.html">Conférence DEFCON 22. Andrew "Zoz" Brooks. Ne le gâchez pas! Partie 1</a></li>
<li><a href="../fr422777/index.html">Une introduction simple à l'ALU pour les réseaux de neurones: explication, signification physique et mise en œuvre</a></li>
<li><a href="../fr422781/index.html">Fintech digest: SWIFT continuera à travailler en Fédération de Russie, VISA vous permettra de transférer des fonds par numéro de téléphone, biométrie onéreuse</a></li>
<li><a href="../fr422783/index.html">Mieux, plus rapide, plus puissant: les composants de style v4</a></li>
<li><a href="../fr422785/index.html">Numérisation en usine: un regard sur le devant</a></li>
<li><a href="../fr422789/index.html">@Pythonetc août 2018</a></li>
<li><a href="../fr422791/index.html">Comment ne PAS apprendre l'anglais: erreurs courantes</a></li>
<li><a href="../fr422793/index.html">Conférence DEFCON 22. Andrew "Zoz" Brooks. Ne le gâchez pas! 2e partie</a></li>
<li><a href="../fr422795/index.html">Technologie et affaires: un nouveau modèle de coopération avec Zyxel en Russie</a></li>
<li><a href="../fr422797/index.html">Comment nous avons fabriqué un enregistreur vidéo cloud de petite taille à partir d'une caméra IP standard</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>