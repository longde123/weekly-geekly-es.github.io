<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëΩ üî∫ üåÇ Changements majeurs dans les principales architectures de puces ‚è≥ üàØÔ∏è üë®üèΩ‚Äçüî¨</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="L'introduction de l'IA au niveau de la puce vous permet de traiter plus de donn√©es localement, car une augmentation du nombre d'appareils ne donne plu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Changements majeurs dans les principales architectures de puces</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/422787/"><h4>  <font color="gray">L'introduction de l'IA au niveau de la puce vous permet de traiter plus de donn√©es localement, car une augmentation du nombre d'appareils ne donne plus le m√™me effet</font> </h4><br>  Les fabricants de puces travaillent sur de nouvelles architectures qui augmentent consid√©rablement la quantit√© de donn√©es trait√©es par watt et par cycle.  Le terrain est pr√©par√© pour l'une des plus grandes r√©volutions de l'architecture de puces de ces derni√®res d√©cennies. <br><br>  Tous les principaux fabricants de puces et de syst√®mes changent de direction de d√©veloppement.  Ils sont entr√©s dans la course aux architectures, qui pr√©voit un changement de paradigme en tout: des m√©thodes de lecture et d'√©criture √† la m√©moire, √† leur traitement et, finalement, √† la disposition de divers √©l√©ments sur une puce.  Bien que la miniaturisation se poursuive, personne ne parie sur la mise √† l'√©chelle pour faire face √† la croissance explosive des donn√©es des capteurs et √† l'augmentation du volume de trafic entre les machines. <br><a name="habracut"></a><br>  Parmi les changements dans les nouvelles architectures: <br><br><ul><li>  De nouvelles m√©thodes pour traiter une plus grande quantit√© de donn√©es en 1 cycle d'horloge, parfois avec moins de pr√©cision ou par priorit√© de certaines op√©rations, selon l'application. </li><li>  De nouvelles architectures de m√©moire qui changent la fa√ßon dont nous stockons, lisons, √©crivons et acc√©dons aux donn√©es. </li><li>  Modules de traitement plus sp√©cialis√©s situ√©s dans tout le syst√®me pr√®s de la m√©moire.  Au lieu d'un processeur central, les acc√©l√©rateurs sont s√©lectionn√©s en fonction du type de donn√©es et de l'application. </li><li>  Dans le domaine de l'IA, des travaux sont en cours pour combiner diff√©rents types de donn√©es sous forme de mod√®les, ce qui augmente efficacement la densit√© des donn√©es tout en minimisant les √©carts entre les diff√©rents types. </li><li>  Maintenant, la disposition du bo√Ætier est le composant principal de l'architecture, avec une attention de plus en plus grande accord√©e √† la facilit√© de changer ces conceptions. </li></ul><br>  ¬´Il existe plusieurs tendances qui affectent les avanc√©es technologiques¬ª, a d√©clar√© Stephen Wu, un ing√©nieur distingu√© de Rambus.  - Dans les centres de donn√©es, vous tirez le meilleur parti du mat√©riel et des logiciels.  Sous cet angle, les propri√©taires de centres de donn√©es envisagent l'√©conomie.  Introduire quelque chose de nouveau co√ªte cher.  Mais les goulots d'√©tranglement changent, des puces sp√©cialis√©es sont donc introduites pour un calcul plus efficace.  Et si vous r√©duisez les flux de donn√©es dans les deux sens vers les E / S et la m√©moire, cela peut avoir un impact important. ¬ª <br><br>  Les changements sont plus √©vidents au bord de l'infrastructure informatique, c'est-√†-dire parmi les capteurs d'extr√©mit√©.  Les fabricants ont soudain r√©alis√© que des dizaines de milliards d'appareils g√©n√©reraient trop de donn√©es: un tel volume ne pouvait pas √™tre envoy√© vers le cloud pour traitement.  Mais le traitement de toutes ces donn√©es en p√©riph√©rie pose d'autres probl√®mes: il n√©cessite des am√©liorations de performances majeures sans augmentation significative de la consommation √©lectrique. <br><br>  ¬´Il y a une nouvelle tendance vers une pr√©cision moindre¬ª, a d√©clar√© Robert Ober, architecte principal de la plate-forme Tesla chez Nvidia.  - Ce ne sont pas seulement des cycles de calcul.  Il s'agit d'un conditionnement de donn√©es plus intensif en m√©moire, o√π le format d'instructions 16 bits est utilis√©. ¬ª <br><br>  Aubert estime que gr√¢ce √† une s√©rie d'optimisations architecturales dans un avenir pr√©visible, vous pouvez doubler la vitesse de traitement tous les deux ans.  "Nous verrons une augmentation spectaculaire de la productivit√©", a-t-il d√©clar√©.  - Pour cela, vous devez faire trois choses.  Le premier est l'informatique.  Le second est la m√©moire.  La troisi√®me zone est la bande passante h√¥te et la bande passante d'E / S.  Beaucoup de travail doit √™tre fait pour optimiser le stockage et la pile r√©seau. ¬ª <br><br>  Quelque chose est d√©j√† en cours d'impl√©mentation.  Dans une pr√©sentation √† la conf√©rence Hot Chips 2018, Jeff Rupley, architecte en chef du Samsung Research Center de Samsung, a soulign√© plusieurs changements architecturaux majeurs du processeur M3.  Un inclut plus d'instructions par temps - six au lieu de quatre dans la derni√®re puce M2.  De plus, la pr√©diction de branchement sur les r√©seaux de neurones a √©t√© impl√©ment√©e et la file d'attente d'instructions a √©t√© doubl√©e. <br><br>  De tels changements d√©placent le point d'innovation de la fabrication directe de microcircuits √† l'architecture et au design d'une part et √† la disposition des √©l√©ments de l'autre c√¥t√© de la cha√Æne de production.  Bien que les innovations continuent de se poursuivre dans les processus technologiques, ce n'est qu'aux d√©pens de celui-ci qu'il est incroyablement difficile d'obtenir une augmentation de 15 √† 20% de la productivit√© et de la puissance dans chaque nouveau mod√®le de puce - et cela ne suffit pas pour faire face √† la croissance rapide du volume de donn√©es. <br><br>  ¬´Les changements ont lieu √† un rythme exponentiel¬ª, a d√©clar√© Victor Pan, pr√©sident et chef de la direction de Xilinx, dans un discours √† la conf√©rence Hot Chips, ¬´10 zettaoctets [10 <sup>21</sup> octets] de donn√©es seront g√©n√©r√©s chaque ann√©e, et la plupart d'entre eux ne sont pas structur√©s.¬ª <br><br><h1>  De nouvelles approches de la m√©moire </h1><br>  Travailler avec autant de donn√©es n√©cessite de repenser chaque composant du syst√®me, des m√©thodes de traitement des donn√©es √† leur stockage. <br><br>  ¬´Il y a eu de nombreuses tentatives pour cr√©er de nouvelles architectures de m√©moire¬ª, a d√©clar√© Carlos Machin, directeur principal de l'innovation chez eSilicon EMEA.  - Le probl√®me est que vous devez lire toutes les lignes et s√©lectionner un bit dans chacune.  Une option consiste √† cr√©er une m√©moire qui peut √™tre lue de gauche √† droite, ainsi que de haut en bas.  Vous pouvez aller encore plus loin et ajouter du calcul √† la m√©moire. ¬ª <br><br>  Ces changements comprennent la modification des m√©thodes de lecture de la m√©moire, l'emplacement et le type des √©l√©ments de traitement, ainsi que l'introduction de l'IA pour prioriser le stockage, le traitement et le mouvement des donn√©es dans tout le syst√®me. <br><br>  ¬´Et si, dans le cas de donn√©es √©parses, nous ne pouvons lire qu'un octet de ce tableau √† la fois - ou peut-√™tre huit octets cons√©cutifs du m√™me chemin d'octets sans gaspiller d'√©nergie sur d'autres octets ou chemins d'octets qui ne nous int√©ressent pas ?  "Demande Mark Greenberg, directeur du marketing produit Cadence."  - √Ä l'avenir, cela est possible.  Si vous regardez l'architecture de HBM2, par exemple, la pile est organis√©e en 16 canaux virtuels de 64 bits chacun, et vous n'avez besoin que de 4 mots 64 bits cons√©cutifs pour acc√©der √† n'importe quel canal virtuel.  Ainsi, il est possible de cr√©er des tableaux de donn√©es d'une largeur de 1024 bits, d'√©crire horizontalement, mais de lire verticalement quatre mots de 64 bits √† la fois. " <br><br>  La m√©moire est l'un des principaux composants de l'architecture de von Neumann, mais maintenant elle est √©galement devenue l'un des principaux domaines d'exp√©rimentation.  ¬´L'ennemi principal est les syst√®mes de m√©moire virtuelle, o√π les donn√©es sont d√©plac√©es de mani√®re plus artificielle¬ª, a d√©clar√© Dan Bouvier, architecte en chef des produits clients chez AMD.  - Ceci est une √©mission diffus√©e.  Nous sommes habitu√©s √† cela dans le domaine du graphisme.  Mais si nous r√©solvons les conflits dans la banque de m√©moire DRAM, nous obtenons un streaming beaucoup plus efficace.  Un GPU s√©par√© peut alors utiliser la DRAM dans une plage d'efficacit√© de 90%, ce qui est tr√®s bon.  Mais si vous configurez le streaming sans interruption, le CPU et l'APU tomberont √©galement dans la plage d'efficacit√© de 80% √† 85%. ¬ª <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6af/814/e8c/6af814e8c5d0a2f4b9274d165aa8f622.png"><br>  <i><font color="gray">Fig.</font></i>  <i><font color="gray">1. Architecture von Neumann.</font></i>  <i><font color="gray">Source: Ing√©nierie des semi-conducteurs</font></i> <br><br>  IBM d√©veloppe un type diff√©rent d'architecture de m√©moire, qui est essentiellement une version mise √† niveau de l'agr√©gation de disques.  L'objectif est qu'au lieu d'utiliser un seul lecteur, le syst√®me puisse utiliser arbitrairement toute la m√©moire disponible via un connecteur, que Jeff Stucheli, architecte mat√©riel IBM, appelle le "Swiss Army Knife" pour connecter les √©l√©ments.  L'avantage de l'approche est qu'elle vous permet de m√©langer et de faire correspondre diff√©rents types de donn√©es. <br><br>  ¬´Le processeur devient le centre d'une interface de signalisation haute performance¬ª, explique Stucelli.  ¬´Si vous modifiez la microarchitecture, le c≈ìur effectue plus d'op√©rations par cycle √† la m√™me fr√©quence.¬ª <br><br>  La connectivit√© et le d√©bit doivent garantir le traitement d'un volume radicalement accru de donn√©es g√©n√©r√©es.  "Les principaux goulots d'√©tranglement se trouvent maintenant dans les emplacements de d√©placement des donn√©es", a d√©clar√© Wu de Rambus.  "L'industrie a fait un excellent travail en augmentant la vitesse de l'informatique."  Mais si vous attendez des donn√©es ou des mod√®les de donn√©es sp√©cialis√©s, vous devez ex√©cuter la m√©moire plus rapidement.  Ainsi, si vous regardez la DRAM et la NVM, les performances d√©pendent du mod√®le de trafic.  Si les donn√©es sont en streaming, la m√©moire offrira de tr√®s bonnes performances.  Mais si les donn√©es arrivent par gouttes al√©atoires, elles sont moins efficaces.  Et peu importe ce que vous faites, avec une augmentation de volume, vous devez toujours le faire plus rapidement. ¬ª <br><br><h1>  Plus d'informatique, moins de trafic. </h1><br>  Le probl√®me est aggrav√© par le fait qu'il existe plusieurs types diff√©rents de donn√©es g√©n√©r√©es √† diff√©rentes fr√©quences et vitesses par des dispositifs en p√©riph√©rie.  Pour que ces donn√©es puissent circuler librement entre diff√©rents modules de traitement, la gestion doit devenir beaucoup plus efficace que par le pass√©. <br><br>  ¬´Il existe quatre configurations principales: plusieurs-√†-plusieurs, des sous-syst√®mes de m√©moire, des E / S basse consommation et des grilles et topologies en anneau¬ª, explique Charlie Janak, pr√©sident-directeur g√©n√©ral d'Arteris IP.  - Vous pouvez placer les quatre sur une puce, ce qui se produit avec les puces IoT cl√©s.  Ou vous pouvez ajouter des sous-syst√®mes HBM √† haut d√©bit.  Mais la complexit√© est √©norme, car certaines de ces charges de travail sont tr√®s sp√©cifiques et la puce a plusieurs t√¢ches diff√©rentes.  Si vous regardez certaines de ces micropuces, elles obtiennent d'√©normes quantit√©s de donn√©es.  C'est dans des syst√®mes tels que les radars et les lidars de voiture.  Ils ne peuvent exister sans certaines interconnexions avanc√©es. ¬ª <br><br>  La t√¢che est de savoir comment minimiser le mouvement des donn√©es, mais en m√™me temps maximiser le flux de donn√©es lorsque cela est n√©cessaire - et trouver en quelque sorte un √©quilibre entre le traitement local et centralis√© sans augmenter inutilement la consommation d'√©nergie. <br><br>  ¬´D'une part, il s'agit d'un probl√®me de bande passante¬ª, a d√©clar√© Rajesh Ramanujam, responsable du marketing produit pour NetSpeed ‚Äã‚ÄãSystems.  - Vous voulez r√©duire le trafic autant que possible, alors transf√©rez les donn√©es plus pr√®s du processeur.  Mais si vous avez encore besoin de d√©placer les donn√©es, il est conseill√© de les compacter autant que possible.  Mais rien n'existe par lui-m√™me.  Tout doit √™tre planifi√© au niveau du syst√®me.  A chaque √©tape, plusieurs axes interd√©pendants doivent √™tre consid√©r√©s.  Ils d√©terminent si vous utilisez la m√©moire de la mani√®re traditionnelle de lecture et d'√©criture, ou si vous utilisez de nouvelles technologies.  Dans certains cas, vous devrez peut-√™tre modifier la fa√ßon dont vous stockez les donn√©es elles-m√™mes.  Si vous avez besoin de performances sup√©rieures, cela signifie g√©n√©ralement une augmentation de la surface de la puce, ce qui affecte la dissipation thermique.  Et maintenant, compte tenu de la s√©curit√© fonctionnelle, la surcharge de donn√©es ne peut pas √™tre autoris√©e. ¬ª <br><br>  C'est pourquoi tant d'attention est accord√©e au traitement des donn√©es √† la p√©riph√©rie et √† la bande passante du canal par divers modules de traitement des donn√©es.  Mais au fur et √† mesure que vous d√©veloppez diff√©rentes architectures, la mani√®re et le lieu de mise en ≈ìuvre de ce traitement de donn√©es sont tr√®s diff√©rents. <br><br>  Par exemple, Marvell a introduit un contr√¥leur SSD avec IA int√©gr√©e pour g√©rer la lourde charge de calcul en p√©riph√©rie.  Le moteur AI peut √™tre utilis√© pour l'analyse directement √† l'int√©rieur du disque SSD. <br><br>  ¬´Vous pouvez charger des mod√®les directement dans le mat√©riel et effectuer le traitement mat√©riel sur le contr√¥leur SSD¬ª, a d√©clar√© Ned Varnitsa, ing√©nieur en chef de Marvell.  - Aujourd'hui, il rend le serveur dans le cloud (h√¥te).  Mais si chaque disque envoie des donn√©es vers le cloud, cela cr√©era une √©norme quantit√© de trafic r√©seau.  Il est pr√©f√©rable de faire le traitement en p√©riph√©rie, et l'h√¥te √©met uniquement une commande, qui n'est que des m√©tadonn√©es.  Plus vous avez de disques, plus la puissance de traitement est importante.  Il s'agit d'un √©norme avantage de la r√©duction du trafic. " <br><br>  Cette approche est particuli√®rement int√©ressante car elle s'adapte √† diff√©rentes donn√©es selon l'application.  Ainsi, l'h√¥te peut g√©n√©rer une t√¢che et l'envoyer au p√©riph√©rique de stockage pour traitement, apr√®s quoi seuls les m√©tadonn√©es ou les r√©sultats des calculs sont renvoy√©s.  Dans un autre sc√©nario, un p√©riph√©rique de stockage peut stocker des donn√©es, les pr√©traiter et g√©n√©rer des m√©tadonn√©es, des balises et des index, qui sont ensuite r√©cup√©r√©s par l'h√¥te au besoin pour une analyse plus approfondie. <br><br>  C'est l'une des options possibles.  Il y en a d'autres.  Rupli de Samsung a soulign√© l'importance du traitement et de la fusion des idiomes qui peuvent d√©coder deux instructions et les combiner en une seule op√©ration. <br><br><h1>  L'IA s'occupe du contr√¥le et de l'optimisation </h1><br>  √Ä tous les niveaux d'optimisation, l'intelligence artificielle est utilis√©e - c'est l'un des √©l√©ments vraiment nouveaux de l'architecture de la puce.  Au lieu de permettre au syst√®me d'exploitation et au middleware de g√©rer les fonctions, cette fonction de surveillance est r√©partie sur la puce, entre les puces et au niveau du syst√®me.  Dans certains cas, des r√©seaux de neurones mat√©riels sont introduits. <br><br>  ¬´Il ne s'agit pas tant de rassembler davantage, mais de changer l'architecture traditionnelle¬ª, explique Mike Gianfanya, vice-pr√©sident du marketing, eSilicon.  - Avec l'aide de l'IA et de l'apprentissage automatique, vous pouvez distribuer des √©l√©ments √† travers le syst√®me, obtenant un traitement plus efficace avec les pr√©visions.  Ou vous pouvez utiliser des puces distinctes qui fonctionnent ind√©pendamment dans le syst√®me ou dans le module. ¬ª <br><br>  ARM a d√©velopp√© sa premi√®re puce d'apprentissage automatique, qu'elle pr√©voit de publier plus tard cette ann√©e pour plusieurs march√©s.  ¬´Il s'agit d'un nouveau type de processeur¬ª, a d√©clar√© Ian Bratt, ing√©nieur √©m√©rite d'ARM.  - Il comprend un bloc fondamental - c'est un moteur informatique, ainsi qu'un moteur MAC, un moteur DMA avec un module de contr√¥le et un r√©seau de diffusion.  Au total, 16 c≈ìurs de calcul sont fabriqu√©s √† l'aide de la technologie de traitement √† 7 nm, qui produisent 4 TeraOps √† une fr√©quence de 1 GHz. ¬ª <br><br>  Comme ARM fonctionne avec un √©cosyst√®me partenaire, sa puce est plus polyvalente et personnalisable que les autres puces AI / ML en cours de d√©veloppement.  Au lieu d'une structure monolithique, il s√©pare le traitement par fonction, de sorte que chaque module informatique fonctionne sur une carte d'entit√©s distincte.  Bratt a identifi√© quatre ingr√©dients cl√©s: la planification statique, le pliage efficace, les m√©canismes de r√©tr√©cissement et l'adaptation programm√©e aux futurs changements de conception. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fbe/7e4/ab1/fbe7e4ab11731ad3dafebc992b9c7bf2.png"><br>  <i><font color="gray">Fig.</font></i>  <i><font color="gray">2. Architecture ML du processeur ARM.</font></i>  <i><font color="gray">Source: ARM / Hot Chips</font></i> <br><br>  Pendant ce temps, Nvidia a choisi une tactique diff√©rente: cr√©er un moteur d√©di√© d'apprentissage en profondeur √† c√¥t√© du GPU pour optimiser le traitement d'image et de vid√©o. <br><br><h1>  Conclusion </h1><br>  En utilisant certaines ou toutes ces approches, les fabricants de puces s'attendent √† doubler leurs performances tous les deux ans, en suivant une croissance explosive des donn√©es, tout en restant dans le cadre serr√© des budgets de consommation d'√©nergie.  Mais ce n'est pas seulement plus informatique.  Il s'agit d'un changement dans la plate-forme de conception de puces et de syst√®mes, lorsque le volume croissant de donn√©es, plut√¥t que les limitations mat√©rielles et logicielles, devient le principal facteur. <br><br>  ¬´Lorsque les ordinateurs sont apparus dans les entreprises, il semblait √† beaucoup que le monde autour de nous s'√©tait acc√©l√©r√©¬ª, a d√©clar√© Aart de Gues, pr√©sident et chef de la direction de Synopsys.  - Ils ont fait des comptes sur des morceaux de papier avec des piles de livres.  Le grand livre s'est transform√© en une pile de cartes perfor√©es pour l'impression et l'informatique.  Un √©norme changement s'est produit et nous le voyons √† nouveau.  Avec l'av√®nement mental des simples ordinateurs de calcul, l'algorithme des actions n'a pas chang√©: vous pouvez tracer chaque √©tape.  Mais maintenant, quelque chose d'autre se produit qui pourrait conduire √† une nouvelle acc√©l√©ration.  C‚Äôest comme sur un champ agricole d‚Äôinclure l‚Äôarrosage et d‚Äôappliquer un certain type d‚Äôengrais seulement un certain jour, lorsque la temp√©rature atteint le niveau souhait√©.  Cette utilisation du machine learning est une optimisation qui n'√©tait pas √©vidente dans le pass√©. ¬ª <br><br>  Il n'est pas seul dans cette appr√©ciation.  ¬´Les nouvelles architectures seront adopt√©es¬ª, a d√©clar√© Wally Raines, pr√©sident et chef de la direction de Mentor, Siemens Business.  - Ils seront con√ßus.  L'apprentissage automatique sera utilis√© dans de nombreux cas ou dans la plupart des cas, car votre cerveau apprend de sa propre exp√©rience.  J'ai visit√© 20 entreprises ou plus qui d√©veloppent des processeurs d'IA sp√©cialis√©s d'un type ou d'un autre, et chacun d'eux a sa propre petite niche.  Mais vous verrez de plus en plus leur application dans des applications sp√©cifiques, et ils compl√©teront l'architecture traditionnelle de von Neumann.  L'informatique neuromorphique deviendra courante.  Il s'agit d'une grande √©tape dans l'efficacit√© informatique et la r√©duction des co√ªts.  Les appareils mobiles et les capteurs commenceront √† faire le travail que les serveurs font aujourd'hui. ¬ª </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr422787/">https://habr.com/ru/post/fr422787/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr422775/index.html">Conf√©rence DEFCON 22. Andrew "Zoz" Brooks. Ne le g√¢chez pas! Partie 1</a></li>
<li><a href="../fr422777/index.html">Une introduction simple √† l'ALU pour les r√©seaux de neurones: explication, signification physique et mise en ≈ìuvre</a></li>
<li><a href="../fr422781/index.html">Fintech digest: SWIFT continuera √† travailler en F√©d√©ration de Russie, VISA vous permettra de transf√©rer des fonds par num√©ro de t√©l√©phone, biom√©trie on√©reuse</a></li>
<li><a href="../fr422783/index.html">Mieux, plus rapide, plus puissant: les composants de style v4</a></li>
<li><a href="../fr422785/index.html">Num√©risation en usine: un regard sur le devant</a></li>
<li><a href="../fr422789/index.html">@Pythonetc ao√ªt 2018</a></li>
<li><a href="../fr422791/index.html">Comment ne PAS apprendre l'anglais: erreurs courantes</a></li>
<li><a href="../fr422793/index.html">Conf√©rence DEFCON 22. Andrew "Zoz" Brooks. Ne le g√¢chez pas! 2e partie</a></li>
<li><a href="../fr422795/index.html">Technologie et affaires: un nouveau mod√®le de coop√©ration avec Zyxel en Russie</a></li>
<li><a href="../fr422797/index.html">Comment nous avons fabriqu√© un enregistreur vid√©o cloud de petite taille √† partir d'une cam√©ra IP standard</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>