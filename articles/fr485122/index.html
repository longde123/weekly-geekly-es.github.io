<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üé° üëâüèª üë®üèæ‚Äçüç≥ La rubrique ¬´Lisez des articles pour vous¬ª. Octobre - d√©cembre 2019 üëåüèæ üì£ üíÜüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr! Nous continuons √† publier des critiques d'articles scientifiques de membres de la communaut√© Open Data Science sur la cha√Æne #article_e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>La rubrique ¬´Lisez des articles pour vous¬ª. Octobre - d√©cembre 2019</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/485122/"><img src="https://habrastorage.org/webt/gx/-y/xl/gx-yxlo7xiz-5y8krpyoj3rgswq.png"><br><p><br>  Bonjour, Habr!  Nous continuons √† publier des critiques d'articles scientifiques de membres de la communaut√© Open Data Science sur la cha√Æne #article_essense.  Si vous souhaitez les recevoir avant tout le monde - rejoignez la <a href="http://ods.ai/">communaut√©</a> ! </p><br><p>  Articles pour aujourd'hui: </p><br><ol><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Poly-encodeurs: architectures de transformateur et strat√©gies de pr√©-formation pour une notation multi-phrases rapide et pr√©cise (Facebook, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Discriminateur implicite dans l'encodeur automatique variationnel (Indian Institute of Technology Ropar, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">L'autoformation avec Noisy Student am√©liore la classification ImageNet (Google Research, Carnegie Mellon University, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Contraste dynamique pour l'apprentissage de la repr√©sentation visuelle non supervis√© (Facebook, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Benchmarking Neural Network Robustness to Common Corruptions and Perturbations (Universit√© de Californie, Oregon State University, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">DistilBERT, une version distill√©e de BERT: plus petite, plus rapide, moins ch√®re et plus l√©g√®re (Hugging Face, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Mod√®les de langage Plug and Play: une approche simple pour la g√©n√©ration de texte contr√¥l√©e (Uber AI, Caltech, HKUST, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Repr√©sentation de la saillance profonde pour l'estimation de F0 dans la musique polyphonique (New York University, USA, 2017)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Analyse et am√©lioration de la qualit√© d'image de StyleGAN (NVIDIA, 2019)</a> </li></ol><a name="habracut"></a><br><div class="spoiler">  <b class="spoiler_title">Liens vers les anciennes collections de la s√©rie:</b> <div class="spoiler_text"><ul><li>  <a href="https://habr.com/ru/company/ods/blog/472672/">Juillet - septembre 2019</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/471514/">Janvier - juin 2019</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/352518/">F√©vrier - mars 2018</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/352508/">D√©cembre 2017 - janvier 2018</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/343822/">Octobre - novembre 2017</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/339094/">Septembre 2017</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/336624/">Ao√ªt 2017</a> </li></ul></div></div><br><h3 id="1-poly-encoders-transformer-architectures-and-pre-training-strategies-for-fast-and-accurate-multi-sentence-scoring">  1. Poly-encodeurs: architectures de transformateur et strat√©gies de pr√©-formation pour une notation multi-phrases rapide et pr√©cise </h3><br><p>  Auteurs: Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, Jason Weston (Facebook, 2019) <br>  <a href="https://arxiv.org/abs/1905.01969">‚Üí Article original</a> <br>  Auteur de la revue: Alexey (in slack zhirzemli) </p><br><p>  <strong>TLDR</strong> </p><br><p>  L'article propose une nouvelle approche pour la notation des paires de phrases (√©nonc√©s).  Cette proc√©dure est pertinente dans les t√¢ches de pr√©diction de la correspondance d'une r√©ponse √† un contexte conditionnel, ainsi que dans des t√¢ches telles que la pr√©diction de sens suivant.  La m√©thode Poly-Encoder propos√©e est compar√©e aux strat√©gies Bi-Encoder et Cross-Encoder.  La m√©thode combine l'avantage du Bi-Encoder (la capacit√© de mettre en cache la pr√©sentation des r√©ponses) et du Cross-Encoder (formation non inconditionnelle des encodeurs de contexte et de r√©ponse) </p><br><img src="https://habrastorage.org/webt/ax/qd/nl/axqdnlibzffxcyjtbfzeguhsdja.png" width="500" height="250"><br><p><br></p><br><p>  <strong>Notation multi-phrases</strong> </p><br><p>  (Un petit rappel sur les approches Bi et Cross Encoder. Pour ceux qui sont familiers, vous pouvez sauter) </p><br><p>  La t√¢che de d√©terminer la correspondance du contexte (demande ou d√©claration de l'utilisateur) avec l'ensemble des r√©ponses existantes est principalement pertinente dans les syst√®mes de dialogue et de recherche d'informations.  Il est r√©solu soit en trouvant une certaine vitesse (produit scalaire) entre les repr√©sentations cod√©es du contexte et de la r√©ponse, soit en codant conjointement le contexte et la r√©ponse en un seul vecteur avec une transformation lin√©aire ult√©rieure en un scalaire. </p><br><p>  La premi√®re approche est appel√©e Bi-Encoder et l'avantage √©vident de cette m√©thode est la possibilit√© de compter hors ligne les repr√©sentations de toutes les r√©ponses disponibles.  Ces vues sont mises en cache et pendant l'inf√©rence, il vous suffit de trouver le vecteur de requ√™te, de cr√©er un produit scalaire avec des vecteurs de r√©ponse et d'organiser le r√©sultat.  De plus, cette approche permet un √©chantillonnage n√©gatif plus efficace au stade de la formation.  A savoir, au sein de chaque lot, les repr√©sentations des √©chantillons positifs sont prises en compte et des exemples n√©gatifs peuvent √™tre pris directement √† partir du m√™me lot.  Essentiellement, r√©utiliser la passe avant pour des exemples positifs et n√©gatifs.  L'inconv√©nient de l'approche Bi-Encoder est le fait que les repr√©sentations de contexte et de r√©ponse apprennent presque ind√©pendamment.  Le seul point o√π au moins une sorte de flux d'informations est possible entre les vues de demande et de r√©ponse est le botnet sous la forme du produit scalaire final.  Au niveau des √©l√©ments textuels, les informations ne sont pas fauss√©es. </p><br><p>  La deuxi√®me approche est Cross-Encoder.  Elle implique une interaction plus puissante du contexte et de la r√©ponse dans le processus d'apprentissage et l'inf√©rence.  Ici, les s√©quences de jetons de demande et de r√©ponse sont concat√©n√©es en une seule.  Un jeton de s√©paration sp√©cial est plac√© entre eux, et une int√©gration sp√©ciale est ajout√©e √† chaque partie (demande, r√©ponse).  En fait, cette int√©gration d√©cale les repr√©sentations d'entr√©e des jetons de r√©ponse d'une certaine constante, de sorte que le mod√®le peut plus facilement les distinguer des jetons de demande.  Par cons√©quent, le mod√®le apprend √† trouver une repr√©sentation conjointe de la demande et de la r√©ponse, de sorte que la couche lin√©aire finale (vecteur -&gt; scalaire) renvoie une grande valeur de logits pour les paires de phrases qui se correspondent et une petite valeur sinon.  L'inconv√©nient de cette approche est l'impossibilit√© de compter hors ligne les repr√©sentations des r√©ponses: elles doivent √™tre √©valu√©es au stade de l'inf√©rence, avec un ensemble conditionnel de jetons de demande.  De plus, l'astuce consistant √† r√©utiliser les id√©es d'exemples n√©gatifs et positifs au stade de la formation ne fonctionnera plus ici.  Vous devrez pr√©lever des √©chantillons n√©gatifs avant la formation du lot. </p><br><p>  <strong>La motivation</strong> <br>  Ce qui suit est une solution qui vous permet d'att√©nuer les lacunes et de combiner les avantages des approches Bi et Cross Encoder.  L'id√©e est que nous voulons former un encodeur qui, d'une part, prendra en compte la d√©pendance conditionnelle des jetons de r√©ponse sur les jetons de demande, et d'autre part, l'utilisation de cette d√©pendance devrait avoir lieu sur des repr√©sentations pr√©-√©valu√©es de la r√©ponse et de la demande.  G√©om√©triquement, j'imagine personnellement quelque chose comme ceci: d√©placer le botnet (le produit final des deux soumissions) un peu plus bas sur le r√©seau.  Cr√©ez une interaction entre les vues de demande et de r√©ponse.  Dans le m√™me temps, la mise en ≈ìuvre d'une telle interaction n'est pas trop √©loign√©e de la couche finale, de sorte que la partie principale du codeur de requ√™te reste ind√©pendante du codeur de r√©ponse. </p><br><p>  <strong>Impl√©mentation</strong> <br>  La mise en ≈ìuvre d'une telle id√©e est assez simple: le codeur candidat fonctionne comme dans le cas du Bi-Encoder: on obtient la repr√©sentation de s√©quence sous forme vectorielle (jeton [CLS]) en utilisant le mod√®le √† transformateur (BERT).  Nous mettons en cache ces repr√©sentations apr√®s avoir form√© le mod√®le. </p><br><p>  L'encodeur de contexte, √† son tour, ne comprime pas la repr√©sentation de la s√©quence d'entr√©e en un seul vecteur.  Ici, nous laissons tous les vecteurs de s√©quence cod√©s par le mod√®le. </p><br><p>  Afin d'obtenir une √©valuation de la conformit√© du contexte (un ensemble de vecteurs) et du candidat (un vecteur), le m√©canisme d'attention est utilis√©.  Dans ce cas, le vecteur candidat est une demande et le vecteur de contexte est les cl√©s.  Il est consid√©r√© comme un produit scalaire et plus loin - softmax selon les valeurs r√©sultantes.  Les vecteurs de contexte sont pond√©r√©s par la distribution r√©sultante et additionn√©s.  En cons√©quence, nous obtenons la repr√©sentation du contexte sous la forme d'un seul vecteur.  Et de plus, comme dans le Bi-Encoder habituel, nous consid√©rons le produit scalaire du contexte et du candidat. </p><br><p>  De plus, l'article a propos√© un certain nombre de fa√ßons d'acc√©l√©rer la pond√©ration des vecteurs de contexte.  L'option la plus efficace √©tait un tel processus de comptage de l'attention, dans lequel seuls les m premiers vecteurs de la s√©quence de contexte √©taient pris. </p><br><p>  <strong>R√©sultats</strong> <br>  En cons√©quence, il s'est av√©r√© que Cross-Encoder fonctionne toujours mieux.  Mais Poly-Encoder n'est pas loin derri√®re en termes de mesures de qualit√©, et en termes de vitesse d'inf√©rence, il fonctionne des centaines de fois plus rapidement. </p><br><h3 id="2-implicit-discriminator-in-variational-autoencoder">  2. Discriminateur implicite dans l'encodeur automatique variationnel </h3><br><p>  Auteurs: Prateek Munjal, Akanksha Paul, Narayanan C. Krishnan (Indian Institute of Technology Ropar, 2019) <br>  <a href="https://arxiv.org/abs/1909.13062">‚Üí Article original</a> <br>  Auteur de la revue: Alex Chiron (dans sliron shiron8bit) </p><br><p>  Dans l'article, les auteurs ont propos√© une architecture qui tente de combiner les avantages des approches VAE et GAN pour la g√©n√©ration d'images, en contournant les inconv√©nients inh√©rents √† chaque approche: flou dans le cas des auto-encodeurs, mode effondrement / mode manquant en cas de formation contradictoire.  Ils y parviennent en raison des poids totaux entre l'encodeur et le discriminateur et le g√©n√©rateur / d√©codeur commun, ce qui, d'une part, r√©duit le nombre de poids du r√©seau et, d'autre part, nous permet d'obtenir des informations utiles du discriminateur par le biais de gradients si le g√©n√©rateur / d√©codeur ne tombe pas dans la distribution des donn√©es r√©elles. </p><br><p>  <strong>Pr√©sentation</strong> <br>  Dans les probl√®mes de g√©n√©ration, un r√¥le important est jou√© par la co√Øncidence de la distribution des donn√©es g√©n√©r√©es Q avec la distribution des donn√©es r√©elles P, qui est mesur√©e par la divergence de Kullback-Leibler.  Une caract√©ristique distinctive de cette mesure de l'√©loignement des distributions est qu'elle est asym√©trique.  En cons√©quence, nous obtiendrons des images diff√©rentes selon que nous consid√©rons Div_KL (P || Q) ou Div_KL (Q || P).  Si nous consid√©rons deux options pour comparer les distributions (dans l'image ci-dessous), alors avec Div_KL (P || Q) (alias forward-KL, alias z√©ro √©vitant), la deuxi√®me option donnera une valeur inf√©rieure, et pour Div_KL (Q || P) (c'est vers l'arri√®re-KL, c'est aussi un for√ßage nul) les distributions de la premi√®re option seront consid√©r√©es comme des distributions plus proches.  En fait, les r√©sultats de VAE et de GAN sont tr√®s diff√©rents: la perte de reconstruction (L2) aide √† minimiser la divergence vers l'avant-KL (et donc nous pr√©servons tous les modes, mais nous obtenons des images floues), et la formation avec un discriminateur permet de minimiser la divergence vers l'arri√®re-KL (les images sont obtenues plus clair, mais il y a un risque de sauter le mod) </p><br><img src="https://habrastorage.org/webt/y9/7k/cd/y97kcdipocff08h4dsoaqly3udq.png" width="500" height="250"><br><p><br></p><br><p>  <strong>Architecture, pertes et formation</strong> <br>  Comme mentionn√© pr√©c√©demment, les auteurs proposent de prendre en compte les lacunes des deux modes et de combiner les deux minimisations en raison de l'architecture du r√©seau (dans l'image ci-dessous), dans laquelle la plupart des poids du codeur et du discriminateur sont communs (seules les t√™tes enti√®rement connect√©es pr√©disant la `` r√©alit√© '' de l'image et des param√®tres sont s√©par√©es mu, sigma de la couche latente VAE), et aussi en raison du mode d'apprentissage.  L'encodeur et le g√©n√©rateur sont les m√™mes. La plupart des pertes utilis√©es sont assez standard: dans l'encodeur L_enc los, l'erreur de r√©cup√©ration L2 et la divergence Kullback-Leibler √† N (0,1) (L_prior) sont utilis√©es, le reste est une formation contradictoire (nous minimisons la sortie du discriminateur lors de la formation du discriminateur, maximisons-la lors de l'apprentissage d'un d√©codeur / g√©n√©rateur), mais il existe 2 particularit√©s: </p><br><ul><li><p>  Dans la perte li√©e √† la formation contradictoire, 2 types diff√©rents de donn√©es g√©n√©r√©es sont transmises au discriminateur: r√©cup√©r√©es via un codeur / d√©codeur et g√©n√©r√©es par un g√©n√©rateur / d√©codeur √† partir d'√©chantillons de N (0,1) </p><br></li><li><p>  Dans la perte du d√©codeur L_dec, il y a un membre dans lequel les caract√©ristiques de l'avant-derni√®re couche du discriminateur (encore une fois, c'est la derni√®re couche commune entre le discriminateur et le codeur) sont compar√©es pour les images r√©elles et restaur√©es. </p><br></li></ul><br><img src="https://habrastorage.org/webt/-d/n5/jh/-dn5jh_obvrbb4am3ujm37hd9qs.png" width="500" height="250"><br><p>  <strong>R√©sultats</strong> <br>  Les auteurs ont compar√© les r√©sultats avec VAE et d'autres travaux, essayant d'une mani√®re ou d'une autre de combiner VAE et GAN (VAE-GAN, alpha-GAN et AGE de Dmitry Ulyanov et Victor Lempitsky) sur les jeux de donn√©es celeba et cifar10 (merci pour non mnist), a re√ßu presque les meilleurs indicateurs concernant l'erreur de reconstruction et la m√©trique Frechet Inception Distance (compare les statistiques d'activation du maillage pr√©-form√© pour les images r√©elles et g√©n√©r√©es).  Il a √©t√© not√© s√©par√©ment que le classement par FID d√©pend fortement de l'architecture choisie, le r√©sultat est donc pr√©f√©rable de v√©rifier l'ensemble des ¬´experts¬ª (diff√©rentes architectures). </p><br><h3 id="3-self-training-with-noisy-student-improves-imagenet-classification">  3. L'auto-formation avec Noisy Student am√©liore la classification ImageNet </h3><br><p>  Auteurs: Qizhe Xie, Eduard Hovy, Minh-Thang Luong, Quoc V. Le (Google Research, Carnegie Mellon University, 2019) <br>  <a href="https://arxiv.org/abs/1911.04252">‚Üí Article original</a> <br>  Auteur de la revue: Alexander Belsky (in slack belskikh) </p><br><p>  Google a obtenu une impression de 87,4% de top1 et 98,2% de top5 de pr√©cision absolument impressionnante.  Zayuzali obscurcit les pseudoleiling et les r√©seaux tr√®s audacieux.  L'approche s'appelait Noisy Student. </p><br><img src="https://habrastorage.org/webt/es/s8/tm/ess8tmsezy4cjwydsqqfhjxclcu.png"><br><p><br></p><br><p>  <strong>L'algorithme est</strong> quelque chose comme ceci: </p><br><ol><li>  Nous prenons un mod√®le d'enseignant, nous enseignons une image normale. </li><li>  Nous g√©n√©rons des √©tiquettes psudo douces sur les images du jeu de donn√©es JFT. </li><li>  Nous enseignons le mod√®le √©tudiant sur des pseudo-√©tiquettes souples, et nous intervenons d√®s que possible: fortes augs, abandons et profondeur stochastique </li><li>  Prenez le mod√®le de l'√©l√®ve, utilisez-le comme enseignant √† l'√©tape 2 et r√©p√©tez le processus.Le jeu de donn√©es est √©quilibr√© selon les classes comme suit.  Pour commencer, nous avons pris EfficientNet-B0, form√© sur l'image, conduit ses pr√©dictions sur le jeu de donn√©es JFT.  Ils ont ensuite pris les exemples pour lesquels la confiance maximale est sup√©rieure √† 0,3.  Pour chaque classe, 130K images ont √©t√© prises (si apr√®s filtrage par 0,3 poubelle elles √©taient moins - dupliqu√©es, si plus - prises selon les port√©es de pr√©dicat les plus √©lev√©es).  Re√ßu 130 millions d'images, √©missions en double, 81 millions √† gauche </li></ol><br><p>  <strong>Architecture:</strong> <br>  EfficeintNet, en outre, le mod√®le √©tudiant prend un mod√®le enseignant beaucoup plus gros.  Ils ont √©galement scann√© EfficientNet lui-m√™me en EfficientNet-L0 / L1 / L2, r√©sultant en un mod√®le L2 avec 480M param√®tres (Resnet50 a 26M param√®tres, pour comparaison) </p><br><p>  <strong>Processus d'apprentissage:</strong> <br>  Butchesize 2048. Le mod√®le Sota L2 a enseign√© 350 √©poques.  Le plus grand mod√®le L2 √©tudi√© dans ce mode pendant 3,5 jours sur Cloud TPU v3 Pod avec 2048 c≈ìurs. </p><br><p>  <strong>Proc√©dure d'apprentissage it√©ratif:</strong> <br>  Au d√©but, ils ont enseign√© le B7 en tant qu'√©tudiant et en tant que professeur.  Ensuite, en utilisant B7 comme enseignants, ils ont enseign√© le plus gros L0 en tant qu'√©tudiant.  Puis, en changeant leur place comme ceci, nous sommes arriv√©s au mod√®le L2, que nous avons finalement utilis√© comme enseignant pour le m√™me mod√®le L2. R√©sultat :: sota: avec 2 fois moins de param√®tres de mod√®le par rapport √† la cellule pr√©c√©dente (FixRes ResNeXt-101 WSL 829M param√®tres) </p><br><p>  A √©galement obtenu de tr√®s bons <strong>r√©sultats</strong> sur ImageNet-A / C / P </p><br><img src="https://habrastorage.org/webt/ht/me/ce/htmeceti9jluqoyj84uqcy2fibo.png"><br><p><br></p><br><h3 id="4-momentum-contrast-for-unsupervised-visual-representation-learning">  4. Contraste de momentum pour l'apprentissage de la repr√©sentation visuelle non supervis√© </h3><br><p>  Auteurs de l'article: Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick (Facebook, 2019) <br>  <a href="https://arxiv.org/abs/1911.05722">‚Üí Article original</a> <br>  Auteur de la revue: Arseny Kravchenko (in slack arsenyinfo) </p><br><p>  SotA est un pr√©train non supervis√© pour plusieurs t√¢ches de vision par ordinateur (de la classification √† l'estimation de pose dense), test√© sur diff√©rents ensembles de donn√©es (imagenet, instagram) et principales t√¢ches (imagenet, COCO, paysages urbains, LVIS etc.). </p><br><img src="https://habrastorage.org/webt/li/5p/bn/li5pbnzez-zowzce2movvxthfea.png"><br><p><br></p><br><p>  Comment fonctionne le pr√©-entra√Ænement non supervis√©?  Nous proposons une sorte de t√¢che pour laquelle les √©tiquettes ne sont pas n√©cessaires, nous apprenons l'encodeur, le gelons, puis nous r√©solvons le probl√®me principal en ajoutant les couches manquantes (lin√©aire pour la classification, d√©codeurs pour la segmentation, etc.).  L'une des t√¢ches les plus populaires dans ce cr√©neau est la discrimination d'instance, bas√©e sur la perte contrastive, c'est-√†-dire  nous voulons que les caract√©ristiques des diff√©rentes augmentations de la m√™me image soient proches les unes des autres (par exemple, en termes de distance cosinus), et les caract√©ristiques des diff√©rentes sont √©loign√©es. </p><br><p>  Vous pouvez essayer d'enseigner cette t√¢che de bout en bout, mais cela d√©pend beaucoup de la taille du lot: la qualit√© d√©pend fortement de la vari√©t√© d'exemples √† l'int√©rieur du lot.  Les exp√©riences montrent qu'avec l'augmentation de la taille des lots, la qualit√© finale s'am√©liore.  Mais le lot est un peu similaire √† Moscou: ce n'est pas du caoutchouc, √ßa ne marchera pas longtemps pour l'augmenter au front. </p><br><p>  Les anciens types proches des cellules ont foir√© une banque de m√©moire: les caract√©ristiques des lots pr√©c√©dents ont √©t√© stock√©es s√©par√©ment dans la m√©moire et ont √©galement √©t√© utilis√©es pour g√©n√©rer des n√©gatifs, c'est-√†-dire  √©chantillons diff√©rents.  Cela a aid√© en partie, mais aussi imparfaitement: pendant l'entra√Ænement, les poids de l'encodeur changent et les anciennes fonctionnalit√©s vont mal. </p><br><p>  Enfin, l'id√©e de l'article: </p><br><ol><li>  Rempla√ßons une simple banque de m√©moire par une file d'attente o√π se trouvent des fonctionnalit√©s assez r√©centes; </li><li>  Nous conserverons deux versions de l'encodeur: l'une est utilis√©e pour le lot en cours et est entra√Æn√©e, et l'autre est plus stable, ses poids sont mis √† jour √† partir de la premi√®re version, mais avec un grand √©lan; </li><li>  Les fonctionnalit√©s du lot sont consid√©r√©es comme le premier encodeur, les fonctionnalit√©s de la file d'attente sont compt√©es par le deuxi√®me encodeur. </li></ol><br><p>  Cette approche permet de se rapprocher de la qualit√© de la formation de bout en bout, mais, gr√¢ce √† la longue programmation, elle permet d'obtenir les r√©sultats potentiels d'un lot irr√©aliste.  De cette fa√ßon, vous obtenez des mesures int√©ressantes pour diff√©rentes t√¢ches, y compris  √† certains endroits, c'est m√™me un peu mieux que l'image traditionnelle supervis√©e sur l'imaginette. </p><br><h3 id="5-benchmarking-neural-network-robustness-to-common-corruptions-and-perturbations">  5. Analyse comparative de la robustesse du r√©seau de neurones aux perturbations et perturbations courantes </h3><br><p>  Auteurs: Dan Hendrycks, Thomas Dietterich (University of California, Oregon State University, 2019) <br>  <a href="https://arxiv.org/abs/1903.12261">‚Üí Article original</a> <br>  Auteur de la revue: Vladimir Iglovikov (in ternaus slack) </p><br><img src="https://habrastorage.org/webt/fy/p3/zn/fyp3znumddg9tstty7aukiiuvwg.png" width="500" height="250"><br><p><br></p><br><p>  Il a √©t√© accept√© √† l'ICLR 2019 et si je comprends bien, c'est l'un de ces travaux DL qui n'a √©t√© form√© dans aucun r√©seau. </p><br><p>  La t√¢che √©tait comme √ßa - mais essayons l'augmentation pour la validation d'ImageNet, mais nous nous entra√Ænerons sur celle ininterrompue.  De plus, contrairement √† adevrsarial, nous n'avons pas pour t√¢che de rendre les transformations petites et invisibles √† l'≈ìil. </p><br><p>  <strong>Ce qui a √©t√© fait:</strong> </p><br><ol><li>  Un ensemble d'augmentations a √©t√© s√©lectionn√©.  Les auteurs disent que c'est le plus courant, mais, √† mon avis, ils mentent. <br>  Ils ont utilis√©: GaussianNoise, ISONoise, Downscale, Defocus, MotionBlur, ZoomBlur, FrostedGlassBlur, JpegCompression, Snow, Fog, Rain, Elastic transoform, etc. </li><li>  Toutes ces transformations ont √©t√© appliqu√©es √† la validation ImageNet.  L'ensemble de donn√©es r√©sultant a √©t√© nomm√© ImageNet-C </li><li>  Une variante appel√©e ImageNet-P a √©galement √©t√© propos√©e dans laquelle des ensembles de transformations de diff√©rentes forces ont √©t√© appliqu√©s √† chaque image. </li><li>  Une m√©trique a √©t√© propos√©e pour √©valuer la stabilit√© du mod√®le. </li><li>  Plusieurs mod√®les ont √©t√© √©valu√©s dans le cadre de cette m√©trique: AlexNet, VGG-11, VGG-19, Resnet-50, Resnet-18, VGG-19 + BN, etc. </li></ol><br><p>  <strong>Conclusions:</strong> </p><br><ol><li>  Plus l'augmentation est forte, plus la pr√©cision du mod√®le en souffre.  : capitan_obvious: </li><li>  Plus le mod√®le est complexe, plus il est stable. </li><li>  L'application de CLAHE dans les images avant l'inf√©rence aide un peu. </li><li>  des blocs d'agr√©gation comme l'aide de DenseNet ou Resnext. </li><li>  Les r√©seaux multi-√©chelles sont plus stables.  Un exemple de tels r√©seaux est MSDNet, Multigrid (je n'ai pas entendu parler de tels r√©seaux) </li></ol><br><p>  <a href="https://github.com/hendrycks/robustness">Code</a> </p><br><h3 id="6-distilbert-a-distilled-version-of-bert-smaller-faster-cheaper-and-lighter">  6. DistilBERT, une version distill√©e de BERT: plus petit, plus rapide, moins cher et plus l√©ger </h3><br><p>  Auteurs: Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf (Hugging Face, 2019) <br>  <a href="https://arxiv.org/abs/1910.01108">‚Üí Article original</a> <br>  Auteur de la revue: Yuri Kashnitsky (in yorko slack) </p><br><p>  L'article est court, il est tr√®s facile √† lire.  Au d√©but, quelques mots g√©n√©raux sur la course aux armements dans la PNL et l'empreinte environnementale.  De plus, l'id√©e de distillation (et Hinton l'a fait ici aussi) Dans la t√¢che de mod√©lisation du langage, nous pr√©disons g√©n√©ralement le mot suivant en contexte.  Habituellement, la perte d'entropie crois√©e compare le vecteur des probabilit√©s pr√©dites (la longueur de tout le dictionnaire) avec un vecteur binaire, o√π il n'y a qu'une seule unit√© indiquant le mot r√©el √† un endroit donn√© dans l'ensemble d'apprentissage.  Autrement dit, le deuxi√®me, le troisi√®me, etc.  le mot que le mod√®le consid√®re appropri√© est ignor√© par la perte.  Un exemple est donn√© dans l'article: "Je pense que c'est le d√©but d'un beau [MASQUE]", au lieu de [MASQUE] BERT veut remplacer d'abord le jour ou la vie, mais les mots futur, histoire et monde qui suivent selon la probabilit√© pr√©dite fonctionnent aussi bien.  Peut-on en quelque sorte tenir compte du fait que le mod√®le produit une bonne distribution de probabilit√©?  En gros, pour r√©compenser le mod√®le pour le fait qu'il n'y a pas de Murdock, de tol√©rance, de maternit√© et d'autres quelques mots appropri√©s dans le haut. </p><br><img src="https://habrastorage.org/webt/wg/xd/rx/wgxdrxth-vykjuhkxszaxakxdke.png" width="500" height="250"><br><p><br></p><br><p>  <strong>L'id√©e de la distillation</strong> <br>  L'id√©e d'un programme sp√©cifique enseignant-√©l√®ve est que nous avons un grand mod√®le d' <strong>enseignant</strong> ( <strong>enseignant</strong> , BERT) et un mod√®le plus petit ( <strong>√©tudiant</strong> , DistilBERT), qui transmettront les ¬´connaissances¬ª du mod√®le d'enseignant.  Le mod√®le de l'√©l√®ve optimisera la perte de distillation, √† savoir la perte d'entropie crois√©e, d√©finie pour les distributions de probabilit√© de l'enseignant et de l'√©l√®ve: L = Œ£ t_i * log (s_i).  Autrement dit, pour un mot sp√©cifique effac√© par le symbole [MASQUE], et qui doit √™tre pr√©dit par le contexte, nous comparons deux distributions de probabilit√© de l'apparition de chaque mot du dictionnaire: {t_i} et {s_i} - pr√©dites, respectivement, par le mod√®le et le mod√®le de l'enseignant √©tudiant.  Ainsi, un signal d'apprentissage riche est obtenu - le mod√®le √©tudiant sur chaque mot re√ßoit un signal calcul√© non seulement en comparant son vecteur de pr√©vision avec le mot r√©el dans l'√©chantillon d'apprentissage, mais en le comparant avec le vecteur de projection du mod√®le enseignant. </p><br><p>  <strong>Mod√®le DistilBERT</strong> <br>  L'id√©e est que l'√©l√®ve est un mod√®le plus petit que l'enseignant.   DistilBERT ‚Äî      BERT,    .   token-type embeddings  pooler, ,    .  ,  DistilBERT  40%  ‚Äî 66 .   110   BERT </p><br><p> <strong> DistilBERT</strong> <br>  DistilBERT  distillation loss     ‚Äî   masked language modeling loss,    BERT   cosine embedding loss ‚Äî           ( ,  ,      "" -   ,  "" ). :   ablation studies, ,   masked language modeling loss,    , ..    distillation loss  cosine embedding loss.   ,    RoBERTa   next sentence prediction   dynamic masking. </p><br><p>      ,  BERT (eng. wiki + Toronto Book Corpus) 90   8 V100 (16 GB).   RoBERTa    1024 V100 (32 GB). </p><br><p>  <strong>R√©sultats</strong> <br>     BERT ‚Äî "it performed surprisingly well",        DistilBERT ‚Äî  GLUE  surprisingly well ‚Äî     5  9   ,  BERT ,     SQuAD  IMDb ‚Äî  .   ,    DistilBERT   60% ‚Äî  . </p><br><p> <strong> </strong> <br>   DistilBERT  iPhone 7 Plus.   70% ,  BERT-base (  ),     200 .  ablation studies:     ,      ‚Äî distillation loss  cosine embedding loss. </p><br><p>      3          ,  DistilBERT ‚Äî     BERT,   40%  ,   60%    "97%   "    BERT (        ML). </p><br><p> -,      BERT,     . </p><br><p> <strong> :</strong> <br> <a href="https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"> Jay Alammar</a> <br> <a href="https://www.kaggle.com/kashnitsky/distillbert-catalyst-amazon-product-reviews">  , DistilBERT + Catalyst:   </a> </p><br><h3 id="7-plug-and-play-language-models-a-simple-approach-to-controlled-text-generation"> 7. Plug and Play Language Models: A Simple Approach To Controlled Text Generation </h3><br><p>  : Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu (Uber AI, Caltech, HKUST, 2019) <br> <a href="https://arxiv.org/abs/1912.02164">‚Üí  </a> <br>  :   (  Egor Timofeev) </p><br><p>               . ,           / /      (, .  <a href="https://arxiv.org/pdf/1909.05858.pdf">https://arxiv.org/pdf/1909.05858.pdf</a> ).     ,         ,     , ,      . </p><br><p> <strong></strong> <br>       (   x_prev    ),        p(x),      conditional LM (,    ‚Äî CTRL)    p(x|a). </p><br><p>       : p(x|a) ‚àù p(x)p(a|x),  p(x)  ,    (, GPT2),  p(a|x) ‚Äî     .       ‚Äî       ,   /.     ,       ,    . </p><br><p>   <strong></strong> : </p><br><ol><li>    ,  log(p(a|x)) ( ).     hidden state  . </li><li>      ,  hidden state      log(p(a|x)).   H_new. </li><li>   :           p(x).    ,    : -,        KL(H, H_new),  -,  .. post-norm fusion ( <a href="https://arxiv.org/pdf/1809.00125.pdf">https://arxiv.org/pdf/1809.00125.pdf</a> ),   p(x)   non conditional LM  ,     . </li><li>      . </li></ol><br><p>           ,  p(a|x). </p><br><p>  <strong>R√©sultats</strong> <br>       ,   -            topic relevance.    :  (GPT2) &lt;  +    &lt;&lt;       &lt;    + . </p><br><img src="https://habrastorage.org/webt/jx/zm/ye/jxzmyeaubsu6wtcp2np1zda32tk.png" width="500" height="250"><br><p><br></p><br><h3 id="8-deep-salience-representation-for-f0-estimation-in-polyphonic-music"> 8. Deep Salience Representation for F0 Estimation in Polyphonic Music </h3><br><p>  : Rachel M. Bittner, Brian McFee, Justin Salamon, Peter Li, Juan Pablo Bello ( New York University, USA, 2017) <br> <a href="https://bmcfee.github.io/papers/ismir2017_salience.pdf">‚Üí  </a> <br>  :   (  nglaz) </p><br><p>    .  ,                .        ,     ‚Äì    .       ,   -   .   constant-Q ,          (      )          . </p><br><img src="https://habrastorage.org/webt/7l/6y/c0/7l6yc0irzsti2kbks7avmvrb7w4.png" width="500" height="250"><br><p>     .  constant-Q     -   f_min  -    F.    f_min   f_min * h,      ,    ,     .    h   {0.5, 1, 2, 3, 4, 5},        .   ,          3- ,        2-  3-    (, ,  ). ,    ,     ,    ,   (0.5f, f, 2f, 3f, 4f, 5f),    .     ( 55)      .         ,           ,  dilated-. </p><br><p>  , ,     constant-Q       F,           . </p><br><p>    F0 estimation,    ,          .  2017 ,   ,   state-of-the-art.           ,      . </p><br><h3 id="9-analyzing-and-improving-the-image-quality-of-stylegan"> 9. Analyzing and Improving the Image Quality of StyleGAN </h3><br><p>  : Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila (NVIDIA, 2019) <br> <a href="http://arxiv.org/abs/1912.04958">‚Üí  </a> <br>  :   (  shiron8bit) </p><br><p> GAN-      ,     ,         .     ,   ,      ,   ,    ( FID)   : </p><br><ul><li>   droplet-like  (    / ),  AdaIN. </li><li>   ,   ProGAN-    /       end-to-end     MSG-GAN.     ,        /,            . </li><li>  Path Length Regularization. </li><li>     :       W,      ,       stylegan2. </li></ul><br><img src="https://habrastorage.org/webt/f6/v3/7p/f6v37pcy3wcpw0epu5rz1-r24qk.png" width="500" height="250"><br><p><br></p><br><p>  <strong>Artefacts de gouttelettes et AdaIN</strong> <br>  Les auteurs de l'article avancent l'argument suivant contre l'utilisation de la couche AdaIN: adain normalise chaque carte d'entit√©s, d√©truisant ainsi les informations sur les valeurs d'amplitude les unes par rapport aux autres, et droplet est une tentative du g√©n√©rateur de pousser ces informations d'une mani√®re diff√©rente.  En option pour affaiblir AdaIN, ce qui suit a √©t√© propos√©: nous ferons toute la mise √† l'√©chelle (modulation / d√©modulation) directement dans la convolution, en fonction du style provenant du bloc A et du d√©calage du signal sortant (au lieu de mu (y) / y_ {b, i} dans AdaIN) laissez le bloc B transformer le bruit.  Cette innovation a en m√™me temps permis d'acc√©l√©rer la formation dans les m√™mes conditions. </p><br><p>  <strong>√âchec de ProGAN</strong> <br>  Dans l'article sur MSG-GAN, il a √©t√© propos√© d'utiliser des connexions de saut, connectant les blocs g√©n√©rateurs correspondants et les blocs discriminateurs par r√©solution.  Les auteurs de Stylegan ont d√©velopp√© cette id√©e en r√©sumant les sorties des blocs g√©n√©rateurs de toutes les r√©solutions (avec sur√©chantillonnage) et en alimentant la version sous-√©chantillonn√©e correspondante de l'image √† l'entr√©e de chaque bloc discriminateur.  Il a √©t√© sugg√©r√© que les blocs r√©siduels soient utilis√©s comme deuxi√®me option, tandis que les connexions saut√©es dans le g√©n√©rateur et les blocs r√©siduels dans le discriminateur ont montr√© les meilleurs r√©sultats (le discriminateur est similaire √† LAPGAN, mais sans discriminateurs pour chaque r√©solution, les cartes de caract√©ristiques sont transmises plus loin). comme dans le cas de ProGAN, dans les it√©rations initiales, les parties de la grille responsables des r√©solutions inf√©rieures et de l'image globale apportent une plus grande contribution, puis l'accent est transf√©r√© sur les petits d√©tails. </p><br><p>  <strong>R√©gularisation de la longueur du trajet</strong> <br>  Notant que les faibles valeurs FID ne donnent pas toujours des images de haute qualit√©, et notant √©galement une corr√©lation entre la qualit√© de l'image et la m√©trique PPL (Perceptual Path Length - initialement la diff√©rence entre les caract√©ristiques vgg des images avec de petits pas en Z, mais la diff√©rence a √©t√© remplac√©e par LPIPS), les auteurs ont propos√© Path R√©gularisation de la longueur, qui consiste √† minimiser la fonctionnalit√© </p><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msubsup><mi>J</mi><mi>w</mi><mi>T</mi></msubsup><mi>y</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>n</mi><mi>a</mi><mi>b</mi><mi>l</mi><msub><mi>a</mi><mi>w</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>g</mi><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="22.258ex" height="2.78ex" viewBox="0 -883.9 9583.3 1197.1" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,1500010,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhgXNXNdml99aMULua_-8SJld1s8Pg#MJMATHI-4A" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,1500010,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhgXNXNdml99aMULua_-8SJld1s8Pg#MJMATHI-54" x="929" y="488"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,1500010,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhgXNXNdml99aMULua_-8SJld1s8Pg#MJMATHI-77" x="785" y="-212"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,1500010,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhgXNXNdml99aMULua_-8SJld1s8Pg#MJMATHI-79" x="1255" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,1500010,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhgXNXNdml99aMULua_-8SJld1s8Pg#MJMAIN-3D" x="2030" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,1500010,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhgXNXNdml99aMULua_-8SJld1s8Pg#MJMATHI-6E" x="3336" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,1500010,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhgXNXNdml99aMULua_-8SJld1s8Pg#MJMATHI-61" x="3937" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,1500010,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhgXNXNdml99aMULua_-8SJld1s8Pg#MJMATHI-62" x="4466" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,1500010,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhgXNXNdml99aMULua_-8SJld1s8Pg#MJMATHI-6C" x="4896" y="0"></use><g transform="translate(5194,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,1500010,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhgXNXNdml99aMULua_-8SJld1s8Pg#MJMATHI-61" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,1500010,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhgXNXNdml99aMULua_-8SJld1s8Pg#MJMATHI-77" x="748" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,1500010,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhgXNXNdml99aMULua_-8SJld1s8Pg#MJMAIN-28" x="6330" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,1500010,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhgXNXNdml99aMULua_-8SJld1s8Pg#MJMATHI-67" x="6720" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,1500010,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhgXNXNdml99aMULua_-8SJld1s8Pg#MJMAIN-28" x="7200" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,1500010,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhgXNXNdml99aMULua_-8SJld1s8Pg#MJMATHI-77" x="7590" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,1500010,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhgXNXNdml99aMULua_-8SJld1s8Pg#MJMAIN-29" x="8306" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,1500010,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhgXNXNdml99aMULua_-8SJld1s8Pg#MJMATHI-79" x="8696" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,1500010,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhgXNXNdml99aMULua_-8SJld1s8Pg#MJMAIN-29" x="9193" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msubsup><mi>J</mi><mi>w</mi><mi>T</mi></msubsup><mi>y</mi><mo>=</mo><mtext>&nbsp;</mtext><mi>n</mi><mi>a</mi><mi>b</mi><mi>l</mi><msub><mi>a</mi><mi>w</mi></msub><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mi>y</mi><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> J ^ T_w y = \ nabla_w (g (w) y) </script></p><br>  o√π g est le g√©n√©rateur lui-m√™me, J_w est le Jacobien dans les variables d'espace latent.  Dans ce cas, les calculs jacobiens peuvent √™tre effectu√©s via backprop, et il est √©galement dit que pour faciliter les calculs, le r√©gularisateur ne peut √™tre compt√© que pour 16 lots.  Le nombre a est calcul√© comme la moyenne mobile exponentielle de la norme jacobienne. L'utilisation de la r√©gularisation de la longueur du chemin permet une interpolation plus "fluide" de l'espace cach√© W, qui, en plus d'am√©liorer la qualit√© de l'image, peut am√©liorer la r√©versibilit√© (c'est-√†-dire trouver w qui donne une image donn√©e apr√®s avoir parcouru le g√©n√©rateur), et ouvre √©galement des perspectives en termes d'animation et d'interpolation entre les images cl√©s (dans la nouvelle architecture, entre les projections d'images similaires, il devrait y avoir des points responsables des images proches  I).  L'introduction de cette r√©gularisation a √©galement jou√© un r√¥le dans la simplification de la d√©tection des images g√©n√©r√©es par cette architecture. <br><p>  Le temps de formation pour 8 GPU √† une r√©solution de 1024 * 1024 √©tait de 2 √† 9 jours pour diff√©rentes configurations. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr485122/">https://habr.com/ru/post/fr485122/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr485104/index.html">Cr√©er une cl√© RFID universelle pour les interphones</a></li>
<li><a href="../fr485108/index.html">Statistiques des sp√©cialistes PMI certifi√©s en Russie au 01/10/2020</a></li>
<li><a href="../fr485110/index.html">Mon exp√©rience du travail √† distance efficace</a></li>
<li><a href="../fr485118/index.html">Clean Code par Robert Martin. Abstrait. Comment √©crire du code clair et beau?</a></li>
<li><a href="../fr485120/index.html">Ajoutez une API JSON tr√®s rapide √† notre application.</a></li>
<li><a href="../fr485124/index.html">Tests purs en PHP et PHPUnit</a></li>
<li><a href="../fr485126/index.html">Mu-mu, woof-woof, quack-quack: √©volution de la communication acoustique</a></li>
<li><a href="../fr485128/index.html">√âconomisez sur les licences Mikrotik CHR</a></li>
<li><a href="../fr485132/index.html">Rejoignez le Google Play Indie Games Festival</a></li>
<li><a href="../fr485136/index.html">Suivi et surveillance d'Istio: microservices et principe d'incertitude</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>