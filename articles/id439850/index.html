<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🌱 🎅🏾 👎🏼 Deteksi Emosi Kontekstual dalam Percakapan Tekstual Menggunakan Jaringan Saraf Tiruan 🕺🏼 🖇️ 😳</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Saat ini, berbicara dengan agen percakapan menjadi rutinitas sehari-hari, dan sangat penting bagi sistem dialog untuk menghasilkan respons semanusiawi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Deteksi Emosi Kontekstual dalam Percakapan Tekstual Menggunakan Jaringan Saraf Tiruan</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/439850/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/t6/sr/jr/t6srjrmjjmm6qn8gpld9emy4txu.gif"></div><br>  Saat ini, berbicara dengan agen percakapan menjadi rutinitas sehari-hari, dan sangat penting bagi sistem dialog untuk menghasilkan respons semanusiawi mungkin.  Sebagai salah satu aspek utama, perhatian utama harus diberikan untuk memberikan tanggapan yang sadar secara emosional kepada pengguna.  Dalam artikel ini, kita akan menggambarkan <b>arsitektur jaringan saraf berulang untuk deteksi emosi dalam percakapan tekstual</b> , yang berpartisipasi dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">SemEval-2019 Tugas 3 "EmoContext"</a> , yaitu, lokakarya tahunan tentang evaluasi semantik.  Tujuan tugas adalah untuk mengklasifikasikan emosi (yaitu bahagia, sedih, marah, dan lain-lain) dalam kumpulan data percakapan 3 putaran. <br><a name="habracut"></a><br>  Sisa artikel ini disusun sebagai berikut.  Bagian 1 memberikan gambaran singkat tentang tugas EmoContext dan data yang disediakan.  Bagian 2 dan 3 fokus pada teks pra-pemrosesan dan embeddings kata, akibatnya.  Pada bagian 4, kami menggambarkan arsitektur model LSTM yang digunakan dalam pengiriman kami.  Sebagai kesimpulan, kinerja akhir dari sistem kami dan kode sumber disajikan.  Model ini diimplementasikan dalam Python menggunakan perpustakaan Keras. <br><br><h2>  1. Data Pelatihan </h2><br>  SemEval-2019 Tugas 3 "EmoContext" difokuskan pada deteksi emosi kontekstual dalam percakapan tekstual.  Dalam EmoContext, diberi ucapan pengguna tekstual bersama dengan 2 putaran konteks dalam percakapan, kita harus mengklasifikasikan apakah emosi ucapan pengguna berikutnya adalah "bahagia", "sedih", "marah" atau "orang lain" (Tabel 1).  Hanya ada dua peserta percakapan: orang anonim (Tuen-1 dan Turn-3) dan chatbot <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Ruuh</a> berbasis <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">AI</a> (Turn-2).  Untuk deskripsi terperinci, lihat ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Chatterjee et al., 2019</a> ). <br><br>  <i>Tabel 1. Contoh yang menunjukkan dataset EmoContext ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Chatterjee et al., 2019</a> )</i> <br><div class="scrollable-table"><table><tbody><tr><th>  Pengguna (Putar-1) </th><th>  Agen Percakapan (Turn-1) </th><th>  Pengguna (Turn-2) </th><th>  Kelas yang benar </th></tr><tr><td>  Saya hanya memenuhi syarat untuk magang Nabard </td><td>  WOOT!  Itu berita bagus.  Selamat! </td><td>  Saya mulai menangis </td><td>  Senang </td></tr><tr><td>  Beraninya kau menampar anakku </td><td>  Jika Anda merusak mobil saya, saya akan melakukannya untuk Anda juga </td><td>  Coba lakukan itu sekali saja </td><td>  Marah </td></tr><tr><td>  Aku lebih disakiti olehmu </td><td>  Anda tidak bersungguh-sungguh. </td><td>  katakan aku cinta aku </td><td>  Sedih </td></tr><tr><td>  Saya akan melakukan malam. </td><td>  Baiklah  Buat saya tetap di dalam lingkaran. </td><td>  Tidak memberi WhatsApp no. </td><td>  Lainnya </td></tr></tbody></table></div><br>  Selama kompetisi, kami memiliki akses ke 30160 teks berlabel manusia yang disediakan oleh penyelenggara tugas, di mana masing-masing sekitar 5000 sampel dari kelas "angry", "sad", "happy" dan 15000 untuk kelas "others" (Tabel 2).  Set Dev dan tes, yang juga disediakan oleh penyelenggara, berbeda dengan set kereta, memiliki distribusi kehidupan nyata, yaitu sekitar 4% untuk setiap kelas emosional dan sisanya untuk kelas "lain-lain".  Data disediakan oleh Microsoft dan dapat ditemukan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">grup LinkedIn resmi</a> . <br><br>  <i>Tabel 2. Distribusi label kelas emosi dalam kumpulan data ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Chatterjee et al., 2019</a> ).</i> <br><div class="scrollable-table"><table><tbody><tr><th>  Dataset </th><th>  Senang </th><th>  Sedih </th><th>  Marah </th><th>  Lainnya </th><th>  Total </th></tr><tr><td>  Melatih <br></td><td>  14,07% <br></td><td>  18.11% <br></td><td>  18,26% <br></td><td>  49,56% <br></td><td>  30160 <br></td></tr><tr><td>  Dev <br></td><td>  5,15% <br></td><td>  4,54% <br></td><td>  5,45% <br></td><td>  84,86% <br></td><td>  2755 <br></td></tr><tr><td>  Tes <br></td><td>  5,16% <br></td><td>  4,54% <br></td><td>  5,41% <br></td><td>  84,90% <br></td><td>  5509 <br></td></tr><tr><td>  Jauh <br></td><td>  33,33% <br></td><td>  33,33% <br></td><td>  33,33% <br></td><td>  0% <br></td><td>  900rb <br></td></tr></tbody></table></div><br>  Selain data ini, kami mengumpulkan 900k tweet bahasa Inggris untuk membuat dataset jauh 300k tweet untuk setiap emosi.  Untuk membentuk dataset yang jauh, kami berdasarkan pada strategi Go et al.  (2009), di mana kami hanya mengasosiasikan tweet dengan kehadiran kata-kata yang berhubungan dengan emosi seperti '#angry', '#annoyed', '#happy', '#sad,' #surprised ', dll.  Daftar istilah kueri didasarkan pada istilah kueri dari SemEval-2018 AIT DISC ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Duppada et al., 2018</a> ). <br><br>  Metrik kinerja utama EmoContext adalah skor F1 mikro-rata-rata untuk tiga kelas emosi, yaitu 'sedih', 'bahagia', dan 'marah'. <br><br><pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocessData</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(dataFilePath, mode)</span></span></span><span class="hljs-function">:</span></span> conversations = [] labels = [] <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> io.open(dataFilePath, encoding=<span class="hljs-string"><span class="hljs-string">"utf8"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> finput: finput.readline() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> finput: line = line.strip().split(<span class="hljs-string"><span class="hljs-string">'\t'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>): line[i] = tokenize(line[i]) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> mode == <span class="hljs-string"><span class="hljs-string">"train"</span></span>: labels.append(emotion2label[line[<span class="hljs-number"><span class="hljs-number">4</span></span>]]) conv = line[<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">4</span></span>] conversations.append(conv) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> mode == <span class="hljs-string"><span class="hljs-string">"train"</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.array(conversations), np.array(labels) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.array(conversations) texts_train, labels_train = preprocessData(<span class="hljs-string"><span class="hljs-string">'./starterkitdata/train.txt'</span></span>, mode=<span class="hljs-string"><span class="hljs-string">"train"</span></span>) texts_dev, labels_dev = preprocessData(<span class="hljs-string"><span class="hljs-string">'./starterkitdata/dev.txt'</span></span>, mode=<span class="hljs-string"><span class="hljs-string">"train"</span></span>) texts_test, labels_test = preprocessData(<span class="hljs-string"><span class="hljs-string">'./starterkitdata/test.txt'</span></span>, mode=<span class="hljs-string"><span class="hljs-string">"train"</span></span>)</code> </pre> <br><h2>  2. Teks Pra-Pemrosesan </h2><br>  Sebelum tahap pelatihan apa pun, teks sudah diproses terlebih dahulu oleh alat teks Ekphrasis (Baziotis et al., 2017).  Alat ini membantu untuk melakukan koreksi ejaan, normalisasi kata, segmentasi, dan memungkinkan untuk menentukan token mana yang harus dihilangkan, dinormalisasi atau dijelaskan dengan tag khusus.  Kami menggunakan teknik berikut untuk tahap pra-pemrosesan. <br><br><ul><li>  URL, email, tanggal dan waktu, nama pengguna, persentase, mata uang, dan angka diganti dengan tag yang sesuai. </li><li>  Istilah yang berulang, disensor, memanjang, dan bermodal diberi keterangan dengan tag yang sesuai. </li><li>  Kata-kata yang memanjang secara otomatis diperbaiki berdasarkan pada corpus statistik kata bawaan. </li><li>  Hashtag dan kontraksi membongkar (yaitu segmentasi kata) dilakukan berdasarkan built-in statistik kata corpus. </li><li>  Kamus yang dibuat secara manual untuk mengganti istilah yang diekstrak dari teks digunakan untuk mengurangi beragam emosi. </li></ul><br>  Selain itu, Penekanan menyediakan tokenizer yang mampu mengidentifikasi sebagian besar emoji, emotikon, dan ekspresi rumit seperti kata yang disensor, ditekankan dan memanjang serta tanggal, waktu, mata uang, dan akronim. <br><br>  <i>Tabel 3. Contoh-contoh pra-pemrosesan teks.</i> <br><div class="scrollable-table"><table><tbody><tr><th>  Teks asli </th><th>  Teks pra-diproses </th></tr><tr><td>  AKU MERASA ANDA ... Saya membobol jutaan keping <img src="https://habrastorage.org/webt/2n/p4/l5/2np4l5uym3fkohcwlijjcma8eaw.png" width="100"></td><td>  &lt;allcaps&gt; saya merasa Anda &lt;/allcaps&gt;.  [Diulang] saya membobol jutaan keping <img src="https://habrastorage.org/webt/2n/p4/l5/2np4l5uym3fkohcwlijjcma8eaw.png" width="100"></td></tr><tr><td>  lelah dan aku juga merindukanmu :–( </td><td>  lelah dan aku juga merindukanmu &lt;sad&gt; </td></tr><tr><td>  Anda harus liiiiii mendengarkan ini: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">www.youtube.com/watch?v=99myH1orbs4</a> </td><td>  Anda harus mendengarkan &lt;elongated&gt; untuk ini: &lt;url&gt; </td></tr><tr><td>  Apartemen saya yang mengurusnya.  Sewa saya sekitar $ 650. </td><td>  apartemen saya mengurusnya.  sewa saya sekitar &lt;money&gt;. </td></tr></tbody></table></div><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ekphrasis.classes.preprocessor <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> TextPreProcessor <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ekphrasis.classes.tokenizer <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SocialTokenizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ekphrasis.dicts.emoticons <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> emoticons <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> io label2emotion = {<span class="hljs-number"><span class="hljs-number">0</span></span>: <span class="hljs-string"><span class="hljs-string">"others"</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>: <span class="hljs-string"><span class="hljs-string">"happy"</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>: <span class="hljs-string"><span class="hljs-string">"sad"</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>: <span class="hljs-string"><span class="hljs-string">"angry"</span></span>} emotion2label = {<span class="hljs-string"><span class="hljs-string">"others"</span></span>: <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-string"><span class="hljs-string">"happy"</span></span>: <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-string"><span class="hljs-string">"sad"</span></span>: <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-string"><span class="hljs-string">"angry"</span></span>: <span class="hljs-number"><span class="hljs-number">3</span></span>} emoticons_additional = { <span class="hljs-string"><span class="hljs-string">'(^・^)'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">':‑c'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;sad&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">'=‑d'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">":'‑)"</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">':‑d'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;laugh&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">':‑('</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;sad&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">';‑)'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">':‑)'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">':\\/'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;sad&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">'d=&lt;'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;annoyed&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">':‑/'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;annoyed&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">';‑]'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">'(^ ^)'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">'angru'</span></span>: <span class="hljs-string"><span class="hljs-string">'angry'</span></span>, <span class="hljs-string"><span class="hljs-string">"d‑':"</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;annoyed&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">":'‑("</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;sad&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">":‑["</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;annoyed&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">'( ? )'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">'x‑d'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;laugh&gt;'</span></span>, } text_processor = TextPreProcessor( <span class="hljs-comment"><span class="hljs-comment"># terms that will be normalized normalize=['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'url', 'date', 'number'], # terms that will be annotated annotate={"hashtag", "allcaps", "elongated", "repeated", 'emphasis', 'censored'}, fix_html=True, # fix HTML tokens # corpus from which the word statistics are going to be used # for word segmentation segmenter="twitter", # corpus from which the word statistics are going to be used # for spell correction corrector="twitter", unpack_hashtags=True, # perform word segmentation on hashtags unpack_contractions=True, # Unpack contractions (can't -&gt; can not) spell_correct_elong=True, # spell correction for elongated words # select a tokenizer. You can use SocialTokenizer, or pass your own # the tokenizer, should take as input a string and return a list of tokens tokenizer=SocialTokenizer(lowercase=True).tokenize, # list of dictionaries, for replacing tokens extracted from the text, # with other expressions. You can pass more than one dictionaries. dicts=[emoticons, emoticons_additional] ) def tokenize(text): text = " ".join(text_processor.pre_process_doc(text)) return text</span></span></code> </pre><br><h2>  3. Word Embeddings </h2><br>  Word embeddings telah menjadi bagian penting dari setiap pendekatan pembelajaran mendalam untuk sistem NLP.  Untuk menentukan vektor yang paling cocok untuk tugas pendeteksian emosi, kami mencoba Word2Vec ( <a href="">Mikolov et al., 2013</a> ), GloVe ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pennington et al., 2014</a> ) dan model FastText ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Joulin et al., 2017</a> ) serta DataStories yang sudah dilatih sebelumnya. kata vektor ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Baziotis et al., 2017</a> ).  Konsep kunci dari Word2Vec adalah untuk menemukan kata-kata, yang berbagi konteks umum dalam corpus pelatihan, dalam jarak dekat dalam ruang vektor.  Baik model Word2Vec dan Glove mempelajari pengkodean geometris kata-kata dari informasi yang terjadi bersamaan, tetapi pada dasarnya yang pertama adalah model yang dapat diprediksi, dan yang kedua adalah model yang berdasarkan hitungan.  Dengan kata lain, sementara Word2Vec mencoba untuk memprediksi kata target (arsitektur CBOW) atau konteks (arsitektur Lewati-gram), yaitu untuk meminimalkan fungsi kerugian, GloVe menghitung vektor kata yang melakukan pengurangan dimensionalitas pada matriks penghitungan kejadian bersama.  FastText sangat mirip dengan Word2Vec kecuali untuk fakta bahwa ia menggunakan karakter n-gram untuk mempelajari vektor kata, sehingga ia dapat memecahkan masalah kosakata yang tidak tersedia. <br><br>  Untuk semua teknik yang disebutkan di atas, kami menggunakan kereta bayi pelatihan standar yang disediakan oleh penulis.  Kami melatih model LSTM sederhana (dim = 64) berdasarkan masing-masing embeddings ini dan membandingkan efektivitas menggunakan cross-validation.  Menurut hasil, embeddings pra-pelatihan DataStories menunjukkan skor rata-rata F1 terbaik. <br><br>  Untuk memperkaya embedding kata yang dipilih dengan polaritas emosional dari kata-kata tersebut, kami mempertimbangkan untuk melakukan frase pra-pelatihan yang jauh dengan menyelaraskan embeddings pada dataset jauh yang berlabel secara otomatis.  Pentingnya menggunakan pra-pelatihan ditunjukkan dalam ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Deriu et al., 201</a> 7).  Kami menggunakan dataset jauh untuk melatih jaringan LSTM sederhana untuk mengklasifikasikan tweet yang marah, sedih, dan bahagia.  Lapisan embeddings dibekukan untuk zaman pelatihan pertama untuk menghindari perubahan yang signifikan dalam bobot embeddings, dan kemudian dibekukan untuk 5 epos berikutnya.  Setelah tahap pelatihan, embeddings yang telah disesuaikan disimpan untuk fase pelatihan selanjutnya dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">tersedia untuk umum</a> . <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getEmbeddings</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(file)</span></span></span><span class="hljs-function">:</span></span> embeddingsIndex = {} dim = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> io.open(file, encoding=<span class="hljs-string"><span class="hljs-string">"utf8"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> f: values = line.split() word = values[<span class="hljs-number"><span class="hljs-number">0</span></span>] embeddingVector = np.asarray(values[<span class="hljs-number"><span class="hljs-number">1</span></span>:], dtype=<span class="hljs-string"><span class="hljs-string">'float32'</span></span>) embeddingsIndex[word] = embeddingVector dim = len(embeddingVector) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> embeddingsIndex, dim <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getEmbeddingMatrix</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(wordIndex, embeddings, dim)</span></span></span><span class="hljs-function">:</span></span> embeddingMatrix = np.zeros((len(wordIndex) + <span class="hljs-number"><span class="hljs-number">1</span></span>, dim)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word, i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> wordIndex.items(): embeddingMatrix[i] = embeddings.get(word) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> embeddingMatrix <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.text <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Tokenizer embeddings, dim = getEmbeddings(<span class="hljs-string"><span class="hljs-string">'emosense.300d.txt'</span></span>) tokenizer = Tokenizer(filters=<span class="hljs-string"><span class="hljs-string">''</span></span>) tokenizer.fit_on_texts([<span class="hljs-string"><span class="hljs-string">' '</span></span>.join(list(embeddings.keys()))]) wordIndex = tokenizer.word_index print(<span class="hljs-string"><span class="hljs-string">"Found %s unique tokens."</span></span> % len(wordIndex)) embeddings_matrix = getEmbeddingMatrix(wordIndex, embeddings, dim)</code> </pre><br><h2>  4. Arsitektur Jaringan Saraf Tiruan </h2><br>  Jaringan saraf berulang (RNN) adalah keluarga jaringan saraf tiruan yang khusus menangani pemrosesan data sekuensial.  Berbeda dengan jaringan saraf tradisional, RRN dirancang untuk menangani data sekuensial dengan berbagi bobot internal mereka yang memproses urutan.  Untuk tujuan ini, grafik perhitungan RRN mencakup siklus, mewakili pengaruh informasi sebelumnya pada yang sekarang.  Sebagai perpanjangan dari RNNs, jaringan Memori Jangka Pendek (LSTM) telah diperkenalkan pada tahun 1997 ( <a href="">Hochreiter dan Schmidhuber, 1997</a> ).  Dalam LSTM sel berulang dihubungkan dengan cara tertentu untuk menghindari menghilang dan meledaknya masalah gradien.  LSTM tradisional hanya menyimpan informasi dari masa lalu karena mereka memproses urutan hanya dalam satu arah.  LSTM dua arah menggabungkan output dari dua lapisan LSTM tersembunyi yang bergerak dalam arah yang berlawanan, di mana satu bergerak maju melalui waktu, dan yang lain bergerak mundur melalui waktu, sehingga memungkinkan untuk menangkap informasi dari kedua negara masa lalu dan masa depan secara bersamaan ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Schuster dan Paliwal, 1997</a> ). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bdf/d46/a41/bdfd46a41a20ba916382a57bb7c17e19.png"><br>  <i>Gambar 1: Arsitektur versi yang lebih kecil dari arsitektur yang diusulkan.</i>  <i>Unit LSTM untuk belokan pertama dan belokan ketiga memiliki bobot yang dibagi.</i> <br><br>  Tinjauan tingkat tinggi dari pendekatan kami disediakan pada Gambar 1. Arsitektur yang diusulkan dari jaringan saraf terdiri dari unit embedding dan dua unit LSTM dua arah (redup = 64).  Mantan unit LSTM dimaksudkan untuk menganalisis ucapan pengguna pertama (yaitu giliran pertama dan putaran ketiga dari percakapan), dan yang terakhir dimaksudkan untuk menganalisis ucapan pengguna kedua (yaitu giliran kedua).  Kedua unit ini tidak hanya mempelajari representasi fitur semantik dan sentimen, tetapi juga cara menangkap fitur percakapan khusus pengguna, yang memungkinkan pengelompokan emosi secara lebih akurat.  Pada langkah pertama, setiap ucapan pengguna dimasukkan ke dalam unit LSTM dua arah yang sesuai menggunakan embeddings kata yang sudah dilatih sebelumnya.  Selanjutnya, ketiga peta fitur ini disatukan dalam vektor fitur yang rata dan kemudian diteruskan ke lapisan tersembunyi yang sepenuhnya terhubung (dim = 30), yang menganalisis interaksi antara vektor yang diperoleh.  Akhirnya, fitur-fitur ini melanjutkan melalui lapisan output dengan fungsi aktivasi softmax untuk memprediksi label kelas akhir.  Untuk mengurangi overfitting, lapisan regularisasi dengan noise Gaussian ditambahkan setelah lapisan embedding, lapisan dropout ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Srivastava et al., 2014</a> ) ditambahkan pada setiap unit LSTM (p = 0,2) dan sebelum lapisan yang terhubung sepenuhnya tersembunyi (p = 0,1). <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Input, Dense, Embedding, Concatenate, Activation, \ Dropout, LSTM, Bidirectional, GlobalMaxPooling1D, GaussianNoise <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">buildModel</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(embeddings_matrix, sequence_length, lstm_dim, hidden_layer_dim, num_classes, noise=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.1</span></span></span></span><span class="hljs-function"><span class="hljs-params">, dropout_lstm=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.2</span></span></span></span><span class="hljs-function"><span class="hljs-params">, dropout=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.2</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> turn1_input = Input(shape=(sequence_length,), dtype=<span class="hljs-string"><span class="hljs-string">'int32'</span></span>) turn2_input = Input(shape=(sequence_length,), dtype=<span class="hljs-string"><span class="hljs-string">'int32'</span></span>) turn3_input = Input(shape=(sequence_length,), dtype=<span class="hljs-string"><span class="hljs-string">'int32'</span></span>) embedding_dim = embeddings_matrix.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>] embeddingLayer = Embedding(embeddings_matrix.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], embedding_dim, weights=[embeddings_matrix], input_length=sequence_length, trainable=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) turn1_branch = embeddingLayer(turn1_input) turn2_branch = embeddingLayer(turn2_input) turn3_branch = embeddingLayer(turn3_input) turn1_branch = GaussianNoise(noise, input_shape=(<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, sequence_length, embedding_dim))(turn1_branch) turn2_branch = GaussianNoise(noise, input_shape=(<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, sequence_length, embedding_dim))(turn2_branch) turn3_branch = GaussianNoise(noise, input_shape=(<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, sequence_length, embedding_dim))(turn3_branch) lstm1 = Bidirectional(LSTM(lstm_dim, dropout=dropout_lstm)) lstm2 = Bidirectional(LSTM(lstm_dim, dropout=dropout_lstm)) turn1_branch = lstm1(turn1_branch) turn2_branch = lstm2(turn2_branch) turn3_branch = lstm1(turn3_branch) x = Concatenate(axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>)([turn1_branch, turn2_branch, turn3_branch]) x = Dropout(dropout)(x) x = Dense(hidden_layer_dim, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) output = Dense(num_classes, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)(x) model = Model(inputs=[turn1_input, turn2_input, turn3_input], outputs=output) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'categorical_crossentropy'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'acc'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model model = buildModel(embeddings_matrix, MAX_SEQUENCE_LENGTH, lstm_dim=<span class="hljs-number"><span class="hljs-number">64</span></span>, hidden_layer_dim=<span class="hljs-number"><span class="hljs-number">30</span></span>, num_classes=<span class="hljs-number"><span class="hljs-number">4</span></span>)</code> </pre> <br><h2>  5. Hasil </h2><br>  Dalam proses mencari arsitektur yang optimal, kami bereksperimen tidak hanya dengan jumlah sel dalam lapisan, fungsi aktivasi dan parameter regularisasi tetapi juga dengan arsitektur jaringan saraf.  Info terperinci tentang frasa ini dapat ditemukan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">koran aslinya</a> . <br><br>  Model yang dijelaskan pada bagian sebelumnya menunjukkan skor terbaik pada dataset dev, sehingga digunakan pada tahap evaluasi akhir kompetisi.  Pada dataset tes akhir, itu mencapai 72,59% skor rata-rata mikro F1 untuk kelas emosional, sedangkan skor maksimum di antara semua peserta adalah 79,59%.  Namun, ini jauh di atas garis dasar resmi yang dirilis oleh pengatur tugas, yaitu 58,68%. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kode sumber model dan embedding kata</a> tersedia di GitHub. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Versi lengkap artikel</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">makalah uraian tugas</a> dapat ditemukan di ACL Anthology. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Dataset pelatihan</a> terletak di grup kompetisi resmi di LinkedIn. <br><br>  Kutipan: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@inproceedings{smetanin-2019-emosense, title = "{E}mo{S}ense at {S}em{E}val-2019 Task 3: Bidirectional {LSTM} Network for Contextual Emotion Detection in Textual Conversations", author = "Smetanin, Sergey", booktitle = "Proceedings of the 13th International Workshop on Semantic Evaluation", year = "2019", address = "Minneapolis, Minnesota, USA", publisher = "Association for Computational Linguistics", url = "https://www.aclweb.org/anthology/S19-2034", pages = "210--214", }</span></span></code> </pre> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id439850/">https://habr.com/ru/post/id439850/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id439834/index.html">Tinjauan Teknologi IPMI</a></li>
<li><a href="../id439838/index.html">Aritmatika Madu: Penambahan dan Pengurangan oleh Lebah</a></li>
<li><a href="../id439840/index.html">Konferensi DUMP-2019: kami mengundang Anda untuk berbicara di bagian Desain, Manajemen, dan Pengujian</a></li>
<li><a href="../id439844/index.html">Mengapa Dodo Pizza 250 pengembang?</a></li>
<li><a href="../id439848/index.html">Bukan VPN tunggal. Cheat sheet tentang cara melindungi diri dan data Anda</a></li>
<li><a href="../id439852/index.html">Aplikasi Remote Control Rilis: Aspia 1.1.0</a></li>
<li><a href="../id439854/index.html">Eh, sekali, sekali lagi: apa yang harus dilakukan dengan klien di CRM setelah dia membeli</a></li>
<li><a href="../id439856/index.html">Yandex! Terima kasih untuk Uber</a></li>
<li><a href="../id439858/index.html">Prometheus + Grafana + Eksportir Node + Docker di Azure dengan pemberitahuan di Telegram</a></li>
<li><a href="../id439860/index.html">Ubuntu 18,04 Root pada ZFS</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>