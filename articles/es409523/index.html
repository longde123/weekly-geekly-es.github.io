<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>⛩️ 🏓 🍄 Conversación con el automóvil: la capacidad de escuchar y escuchar. 😖 🧖🏼 🧕🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nuestro día comienza con la frase "¡Buenos días!". Durante el día hablamos con colegas, parientes, amigos e incluso extraños que solicitan indicacione...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Conversación con el automóvil: la capacidad de escuchar y escuchar.</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ua-hosting/blog/409523/"><img src="https://habrastorage.org/webt/8c/wn/c6/8cwnc6nbik0i9ewk0vmfccar8v4.jpeg"><br><br>  Nuestro día comienza con la frase "¡Buenos días!".  Durante el día hablamos con colegas, parientes, amigos e incluso extraños que solicitan indicaciones para llegar al metro más cercano.  Hablamos incluso cuando no hay nadie a nuestro alrededor para percibir mejor nuestro propio razonamiento.  Todo esto es nuestro discurso, un regalo que es verdaderamente incomparable con muchas otras posibilidades del cuerpo humano.  El habla nos permite establecer conexiones sociales, expresar pensamientos y emociones, expresarnos, por ejemplo, en canciones. <a name="habracut"></a><br><br>  Y así, los autos inteligentes aparecieron en la vida de las personas.  Una persona, ya sea por curiosidad o por sed de nuevos logros, está tratando de enseñarle a la máquina a hablar.  Pero para hablar, necesitas escuchar y escuchar.  Hoy en día es difícil sorprender con un programa (por ejemplo, Siri) que puede reconocer el habla, encontrar un restaurante en el mapa, llamar a mamá e incluso contar un chiste.  Ella entiende mucho, no todo, por supuesto, pero mucho.  Pero no siempre fue así, naturalmente.  Hace décadas, era por felicidad, cuando una máquina podía entender al menos una docena de palabras. <br><br>  Hoy nos sumergiremos en la historia de cómo la humanidad pudo hablar con la máquina, los avances a lo largo de los siglos en esta área han servido como impulso para el desarrollo de la tecnología de reconocimiento de voz.  También observamos cómo los dispositivos modernos perciben y procesan nuestras voces.  Vamos <br><br><h2>  Los orígenes del reconocimiento de voz. </h2><br>  ¿Qué es el habla?  En términos generales, esto es sonido.  Entonces, para reconocer el habla, primero debe reconocer el sonido y grabarlo. <br><br>  Ahora tenemos iPods, reproductores de MP3, antes había grabadoras de cinta, incluso gramófonos y gramófonos anteriores.  Todos estos son dispositivos para reproducir sonidos.  ¿Pero quién fue el progenitor de todos ellos? <br><br><img src="https://habrastorage.org/webt/8s/mb/eu/8smbeu5msfrmoe0p2ci1muysue4.jpeg"><br>  <i>Thomas Edison con su invento.</i>  <i>Año 1878</i> <br><br>  Era un fonógrafo.  El 29 de noviembre de 1877, el gran inventor Thomas Edison demostró su nueva creación, capaz de grabar y reproducir sonidos.  Fue un avance que despertó el mayor interés de la sociedad. <br><br>  <b>El principio del fonógrafo.</b> <br><br><img src="https://habrastorage.org/webt/rp/fv/24/rpfv24kfbznfnx4daut4ejm8us8.jpeg"><br><br>  Las partes principales del mecanismo de grabación de sonido eran un cilindro recubierto de aluminio y una aguja de corte.  La aguja se movió a lo largo de un cilindro que giraba.  Y las vibraciones mecánicas fueron capturadas usando una membrana de micrófono.  Como resultado, la aguja dejó marcas en la lámina.  Como resultado, recibimos un cilindro con un registro.  Para reproducirlo, se utilizó inicialmente el mismo cilindro que al grabar.  Pero el papel de aluminio era demasiado frágil y se desgastaba rápidamente, porque los registros fueron de corta duración.  Luego comenzaron a aplicar cera, que cubría el cilindro.  Para prolongar la existencia de los registros, comenzaron a copiar utilizando galvanoplastia.  Mediante el uso de materiales más duros, las copias duraron mucho más. <br><br><img src="https://habrastorage.org/webt/5e/7r/c6/5e7rc6csca_nlluhjoawmsnwgjs.jpeg"><br>  <i>Ilustración esquemática de un fonógrafo en una patente.</i>  <i>1880, 18 de mayo</i> <br><br>  Teniendo en cuenta los inconvenientes anteriores, el fonógrafo, aunque era una máquina interesante, pero no fue barrido de los estantes.  Solo con el advenimiento del fonógrafo de disco, mejor conocido como el gramófono, llegó el reconocimiento público.  La novedad permitió hacer grabaciones más largas (el primer fonógrafo solo pudo grabar un par de minutos), lo que sirvió durante mucho tiempo.  Y el gramófono en sí estaba equipado con un altavoz que aumentaba el volumen de reproducción. <br><br>  Thomas Edison originalmente concibió el fonógrafo como un dispositivo para grabar conversaciones telefónicas, como las grabadoras de voz modernas.  Sin embargo, su creación ha ganado gran popularidad en la reproducción de obras musicales.  Habiendo servido como el comienzo para la formación de la industria discográfica. <br><br><h2>  Discurso "órgano" </h2><br>  Bell Labs es famoso por sus inventos en el campo de las telecomunicaciones.  Uno de esos inventos fue Voder. <br><br>  En 1928, Homer Dudley comenzó a trabajar en un vocoder, un dispositivo capaz de sintetizar el habla.  Hablaremos de él más tarde.  Ahora consideraremos su parte: el vader. <br><br><img src="https://habrastorage.org/webt/ei/h1/jr/eih1jrd-ectvwvrzuvpa5f2kkjq.jpeg"><br>  <i>Ilustración esquemática de un vader</i> <br><br>  El principio básico del vader era dividir el habla humana en componentes acústicos.  La máquina era extremadamente compleja y solo un operador capacitado podía operarla. <br><br>  Vader imitó los efectos del tracto vocal humano.  Hubo 2 sonidos principales que el operador podía elegir con su muñeca.  Los pedales se utilizaron para controlar el generador de oscilaciones discontinuas (zumbidos), que crearon vocales sonoras y sonidos nasales.  Un tubo de descarga de gas (silbido) creó sibilantes (consonantes fricativas).  Todos estos sonidos pasaron por uno de los 10 filtros, que fue seleccionado con las teclas.  También había teclas especiales para sonidos como "p" o "d", y para los africanos "j" en la palabra "mandíbula" y "ch" en la palabra "queso". <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/0rAyrmm7vv0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>Este pequeño extracto de la presentación del vader demuestra claramente el principio de su funcionamiento y las acciones del operador.</i> <br><br>  Un operador podría producir un discurso válido reconocible solo después de varios meses de dura práctica y entrenamiento. <br><br>  Por primera vez, el transportista se demostró en una exposición en Nueva York en 1939. <br><br><h2>  Guardar mediante síntesis de voz </h2><br>  Ahora considere un vocoder, parte del cual fue el controlador mencionado anteriormente. <br><br><img src="https://habrastorage.org/webt/dx/ha/06/dxha06zj1kaojh6_zn0tpo8i7g4.jpeg"><br>  <i>Uno de los modelos de vocoder: HY-2 (1961)</i> <br><br>  Originalmente, el vocoder tenía la intención de guardar los recursos de frecuencia de los enlaces de radio al transmitir mensajes de voz.  En lugar de la voz en sí, se transmitieron los valores de sus parámetros específicos, que fueron procesados ​​por el sintetizador de voz en la salida. <br><br>  La base del vocoder eran tres propiedades principales: <br><br><ul><li>  generador de ruido (sonidos consonantes); </li><li>  generador de tonos (vocales); </li><li>  filtros formales (recreando las características individuales del hablante). </li></ul><br>  A pesar de su propósito serio, el vocoder atrajo la atención de los músicos electrónicos.  La conversión de la señal fuente y su reproducción en otro dispositivo permitió lograr una variedad de efectos, como el efecto de un instrumento musical que canta con una "voz humana". <br><br><h2>  Máquina de conteo </h2><br>  En 1952, las tecnologías no eran tan avanzadas como ahora.  Pero esto no impidió que los científicos entusiastas se establecieran tareas imposibles, según muchos.  Entonces, caballeros Stephen Balashek (S. Balashek), Ralon Biddulf (R. Biddulph) y K.Kh.  Davis (KH Davis) decidió enseñarle a la máquina a entender su discurso.  Siguiendo la idea, el auto de Audrey nació.  Sus capacidades eran muy limitadas: solo podía reconocer números del 0 al 9. Pero esto ya era suficiente para declarar con seguridad un avance en la tecnología informática. <br><br><img src="https://habrastorage.org/webt/ij/k0/t9/ijk0t9mp7xm-wqeotducqfvoxiw.png"><br>  <i>Audrey con uno de sus creadores (según Internet, corrígeme si no es así)</i> <br><br>  A pesar de sus pequeñas capacidades, Audrey no podía presumir de las mismas dimensiones.  Era una "niña" bastante grande: el gabinete de retransmisión tenía casi 2 metros de altura y todos los elementos ocupaban una pequeña habitación.  Lo cual no es sorprendente para las computadoras de esa época. <br><br>  El procedimiento de interacción entre el operador y Audrey también tenía algunas condiciones.  El operador pronunció las palabras (números, en este caso) en el auricular de un teléfono normal, asegúrese de soportar una pausa de 350 milisegundos entre cada palabra.  Audrey aceptó la información, la tradujo a formato electrónico y encendió una bombilla específica correspondiente a un dígito en particular.  Sin mencionar el hecho de que no todos los operadores pueden obtener una respuesta exacta.  Para lograr una precisión del 97%, el operador tenía que ser una persona que había practicado "charlar" con Audrey durante mucho tiempo.  En otras palabras, Audrey entendía solo a sus creadores. <br><br>  Incluso teniendo en cuenta todas las deficiencias de Audrey, que no están asociadas con errores de diseño, sino con las limitaciones de la tecnología de aquellos tiempos, se convirtió en la primera estrella en el horizonte de máquinas que entienden la voz humana. <br><br><h2>  El futuro en la caja de zapatos. </h2><br>  En 1961, en el Laboratorio de Desarrollo de Sistemas Avanzados de IBM, se desarrolló un nuevo dispositivo milagroso: el Shoebox, que puede reconocer 16 palabras (en inglés exclusivamente) y números del 0 al 9. El autor de esta computadora fue William C. Dersch. <br><br><img src="https://habrastorage.org/webt/ho/fk/di/hofkdicpxszuykelh9vujhrrh7a.jpeg"><br>  <i>Caja de zapatos de IBM</i> <br><br>  El nombre inusual correspondía a la apariencia de la máquina, tenía el tamaño y la forma de una caja de zapatos.  Lo único que me llamó la atención fue el micrófono, que estaba conectado a los tres filtros de audio necesarios para reconocer los sonidos altos, medios y bajos.  Los filtros se conectaron a un decodificador lógico (circuito lógico diodo-transistor) y a un mecanismo de interruptor de luz. <br><br>  El operador se llevó el micrófono a la boca y pronunció una palabra (por ejemplo, el número 7).  La máquina convirtió datos acústicos en señales electrónicas.  El resultado de la comprensión fue la inclusión de una bombilla con la firma "7".  Además de comprender palabras individuales, Shoebox podría entender problemas aritméticos simples (como 5 + 6 o 7-3) y dar la respuesta correcta. <br><br>  Shoebox fue presentado por su creador en 1962 en la Seattle World Expo. <br><br><h2>  Conversación telefónica con el coche </h2><br>  En 1971, IBM, conocida por su amor por las invenciones y tecnologías innovadoras, decidió poner en práctica el reconocimiento de voz.  El sistema de identificación automática de llamadas permitió a un ingeniero ubicado en cualquier lugar de los Estados Unidos llamar a una computadora en Raleigh, Carolina del Norte.  La persona que llama puede hacer una pregunta y recibir una respuesta de voz.  La singularidad de este sistema radica en la comprensión de las muchas voces, dada su tonalidad, énfasis, volumen de voz, etc. <br><br><h2>  Arpía volando alto </h2><br>  La Oficina de Proyectos de Investigación Avanzada del Departamento de Defensa (DARPA para abreviar) anunció el lanzamiento de un programa de investigación y desarrollo de reconocimiento de voz en 1971 que tiene como objetivo crear una máquina que pueda reconocer 1,000 palabras.  Un proyecto audaz, dados los éxitos de su predecesor, en decenas de palabras.  Pero no hay límite para el ingenio humano.  Y en 1976, la Universidad Carnegie Mellon demuestra a Harpy, capaz de reconocer 1011 palabras. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/32KKg3aP3Vw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>Video demostración de Harpy</i> <br><br>  La universidad ya ha desarrollado sistemas de reconocimiento de voz: Hearsay-1 y Dragon.  Se utilizaron como base para implementar Harpy. <br><br>  En Hearsay-1, el conocimiento (es decir, un diccionario de máquina) se representa en forma de procedimientos, y en Dragon, en forma de una red de Markov con una transición probabilística a priori.  En Harpy, se decidió usar el último modelo, pero sin esta transición. <br><br><div class="spoiler">  <b class="spoiler_title">En este video, el principio de funcionamiento se describe con más detalle.</b> <div class="spoiler_text"><iframe width="560" height="315" src="https://www.youtube.com/embed/N3i6NoUZsSw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br></div></div><br>  En pocas palabras, puede representar una red: una secuencia de palabras y sus combinaciones, así como sonidos con una sola palabra, para que la máquina entienda la pronunciación diferente de la misma palabra. <br><br>  Harpy entendió a 5 operadores, incluidos tres hombres y dos mujeres.  Eso habló sobre las mayores capacidades informáticas de esta máquina.  La precisión del reconocimiento de voz fue aproximadamente del 95%. <br><br><h2>  Tangora de IBM </h2><br>  A principios de la década de 1980, IBM decidió desarrollar un sistema capaz de reconocer más de 20,000 palabras a mediados de la década.  Así nació Tangora, en cuyo trabajo se usaron modelos ocultos de Markov.  A pesar del vocabulario bastante impresionante, el sistema no requirió más de 20 minutos de colaboración con el nuevo operador (la persona que habla) para aprender a reconocer su discurso. <br><br><h2>  Muñeca viviente </h2><br>  En 1987, la compañía de juguetes Worlds of Wonder lanzó una novedad revolucionaria: una muñeca parlante llamada Julie.  La característica más impresionante del juguete danés fue la capacidad de entrenarlo para reconocer el discurso del propietario.  Julie podía hablar bastante bien.  Además, la muñeca estaba equipada con muchos sensores, gracias a los cuales reaccionó cuando la recogieron, le hicieron cosquillas o la transfirieron de una habitación oscura a una brillante. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/UkU9SbIictc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>El comercial de Worlds of Wonder, Julie, presenta sus características</i> <br><br>  Sus ojos y labios eran móviles, lo que creaba una imagen aún más viva.  Además de la muñeca en sí, fue posible comprar un libro en el que se hicieron dibujos y palabras en forma de pegatinas especiales.  Si sostienes las muñecas con los dedos sobre ellas, emitirá lo que "siente" al tacto.  Doll Julie fue el primer dispositivo con una función de reconocimiento de voz, que estaba disponible para cualquiera. <br><br><h2>  El primer software de dictado. </h2><br><img src="https://habrastorage.org/webt/ox/xe/b3/oxxeb39l8-xwudht04ehub99swu.jpeg"><br><br>  En 1990, Dragon Systems lanzó el primer software de computadora personal basado en el reconocimiento de voz: DragonDictate.  El programa funcionó exclusivamente en Windows.  El usuario tuvo que hacer pequeñas pausas entre cada palabra para que el programa pudiera analizarlas.  En el futuro, apareció una versión más avanzada que le permite hablar continuamente: Dragon NaturallySpeaking (es lo que está disponible ahora, mientras que DragonDictate original ha dejado de actualizarse desde Windows 98).  A pesar de su "lentitud", DragonDictate ha ganado gran popularidad entre los usuarios de PC, especialmente entre las personas con discapacidad. <br><br><h2>  Esfinge no egipcia </h2><br>  La Universidad Carnegie Mellon, que ya se ha "iluminado" anteriormente, se ha convertido en el lugar de nacimiento de otro sistema de reconocimiento de voz históricamente importante: Sphinx 2. <br><br><img src="https://habrastorage.org/webt/b6/9r/pw/b69rpwedqsvxgtayu6kfb-sdnek.jpeg"><br>  <i>Creador de la esfinge Xuedong Huang</i> <br><br>  El autor directo del sistema fue Xuedong Huang.  Sphinx 2 se distinguió de su predecesor por su velocidad.  El sistema se centró en el reconocimiento de voz en tiempo real para programas que usan lenguaje hablado (todos los días).  Entre las características de Sphinx 2 estaban: formación de hipótesis, cambio dinámico entre modelos de lenguaje, detección de equivalentes, etc. <br><br>  El código Sphinx 2 se ha utilizado en muchos productos comerciales.  Y en 2000, en el sitio web de SourceForge, Kevin Lenzo publicó el código fuente del sistema para su visualización general.  Aquellos que deseen estudiar el código fuente de Sphinx 2 y sus otras variaciones pueden seguir el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace</a> . <br><br><h2>  Dictado médico </h2><br>  En 1996, IBM lanzó MedSpeak, el primer producto comercial con reconocimiento de voz.  Se suponía que debía usar este programa en médicos para compilar registros médicos.  Por ejemplo, un radiólogo, al examinar las imágenes de la paciente, expresó sus comentarios, que el sistema MedSpeak tradujo al texto. <br><br>  Antes de pasar a los representantes más famosos de programas con reconocimiento de voz, echemos un vistazo rápido y breve a algunos eventos históricos más relacionados con esta tecnología. <br><br><h2>  Bombardeo histórico </h2><br><ul><li>  2002: Microsoft integra el reconocimiento de voz en todos sus productos de Office; </li><li>  2006 - La Agencia de Seguridad Nacional de EE. UU. Comienza a utilizar programas de reconocimiento de voz para identificar palabras clave limitadas en los registros de conversación; </li><li>  2007 (30 de enero): Microsoft lanza Windows Vista, el primer sistema operativo con reconocimiento de voz; </li><li>  2007 - Google presenta GOOG-411, un sistema de reenvío telefónico (una persona llama a un número, dice qué organización o persona necesita y el sistema los conecta).  El sistema funcionó dentro de los Estados Unidos y Canadá; </li><li>  2008 (14 de noviembre): Google lanza la búsqueda por voz en dispositivos móviles iPhone.  Este fue el primer uso de la tecnología de reconocimiento de voz en teléfonos móviles; </li></ul><br>  Y ahora llegamos al período de tiempo en que mucha gente se encontró con la tecnología de reconocimiento de voz. <br><br><h2>  Las damas no pelean </h2><br>  El 4 de octubre de 2011, Apple anunció Siri, cuya decodificación del nombre habla por sí sola: la interfaz de interpretación y reconocimiento de voz (es decir, la interfaz de interpretación y reconocimiento de voz). <br><br><img src="https://habrastorage.org/webt/6r/q4/iv/6rq4ivje-xftjg1t6c__vsbew4w.jpeg"><br><br>  La historia del desarrollo de Siri es muy larga (de hecho, tiene 40 años de trabajo) e interesante.  El hecho mismo de su existencia y su amplia funcionalidad es el trabajo conjunto de muchas empresas y universidades.  Sin embargo, no nos centraremos en este producto, porque el artículo no trata sobre Siri, sino sobre el reconocimiento de voz en general. <br><br>  Microsoft no quería robar la espalda, porque en 2014 (2 de abril) anunciaron su asistente digital virtual Cortana. <br><br><img src="https://habrastorage.org/webt/k0/ni/gx/k0nigx6urkm0vs09jg5gwxepw9w.jpeg"><br><br>  La funcionalidad de Cortana es similar a la de su competidor Siri, con la excepción de un sistema más flexible para configurar el acceso a la información. <br><br>  Debate sobre Cortana o Siri.  ¿Quién es mejor?  realizado desde su aparición en el mercado.  Como, en general, y la lucha entre usuarios de iOS y Android.  Pero eso es bueno.  Los productos de la competencia, en un intento de parecer mejores que sus rivales, proporcionarán más y más nuevas oportunidades, desarrollarán y utilizarán tecnologías y técnicas más avanzadas en la misma esfera de reconocimiento de voz.  Con solo un representante en cualquier campo de la tecnología de consumo, no hay necesidad de hablar sobre su rápido desarrollo. <br><br>  Un pequeño video divertido de la conversación entre Siri y Cortana (obviamente construido, pero no menos divertido).  ¡Atención! En este video hay blasfemias. <br><br><div class="spoiler">  <b class="spoiler_title">Siri vs Cortana</b> <div class="spoiler_text"><iframe width="560" height="315" src="https://www.youtube.com/embed/AQyf8xbITms" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br></div></div><br><h2>  Conversación con autos.  ¿Cómo nos entienden? </h2><br>  Como mencioné anteriormente, en términos generales, el habla es sonido.  ¿Y cuál es el sonido para el auto?  Estos son cambios (fluctuaciones) en la presión del aire, es decir  ondas de sonido  Para que la máquina (computadora o teléfono) pueda reconocer el habla, primero debe considerar estas fluctuaciones.  La frecuencia de medición debe ser de al menos 8,000 veces por segundo (incluso mejor - 44,100 veces por segundo).  Si las mediciones se llevan a cabo con grandes interrupciones de tiempo, obtendremos un sonido inexacto, lo que significa un discurso ilegible.  El proceso descrito anteriormente se denomina digitalización de 8 kHz o 44,1 kHz. <br><br><img src="https://habrastorage.org/webt/mo/rq/ta/morqtag1dhh1b4lkoxyot4sna3g.png"><br><br>  Cuando se recopilan datos sobre las vibraciones de las ondas de sonido, deben clasificarse.  Dado que en el montón general tenemos tanto el habla como los sonidos secundarios (ruido de la máquina, crujir de papel, el sonido de una computadora en funcionamiento, etc.).  La realización de operaciones matemáticas nos permite eliminar con precisión nuestro discurso, que necesita reconocimiento. <br><br>  El siguiente es el análisis de la onda de sonido seleccionada: el habla.  Dado que consta de muchos componentes separados que forman ciertos sonidos (por ejemplo, "ah" o "ee").  Destacar estas características y convertirlas en equivalentes numéricos le permite definir palabras específicas. <br><br>  ,  ,    40  (44,   ,       100), ..  .    ,         ,             .            .     ,   ,  «» ,     ( ,  , ,    ..),        .  , «t»  «sTar»  «t»  «ciTy»     -. <br><br><img src="https://habrastorage.org/webt/cx/i5/wv/cxi5wv0eromqfqqt8w9mkrtozai.jpeg"><br> <i>     «potato» () /      Harpy</i> <br><br>   ,     ,      . ,   «hang ten»,        — «hey, ngten»,         «ngten». <br><br>  ,        ,        .       ,     (),     ,        №2    №1.  «What do cats like for breakfast?»    «water gaslight four brick vast?». ,   .         .         , ,   ,   .          . <br><br>      ,        .    ,         ,     . <br><br><h2>  </h2><br>   ,          .    .     -  ,         (    ,   ),    .           .        ,  , ,    .       ,  -,  ,      . <br><br> <b>  .             25%       3  6 ! <br><br></b>     !  VPS (KVM)   ,       ,     — ! <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  VPS (KVM) c      </a> (  VPS (KVM) — E5-2650v4 (6 Cores) / 10GB DDR4 / 240GB SSD  4TB HDD / 1Gbps 10TB      —  $29 / ,    RAID1  RAID10)</b> ,          ,     ,   ,    ,     «»! <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">   .  c   Dell R730xd 5-2650 v4  9000   ?</a> <b>Dell R730xd  2  ?</b>    <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2 x Intel Dodeca-Core Xeon E5-2650v4 128GB DDR4 6x480GB SSD 1Gbps 100 TV desde $ 249</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en los Países Bajos y los EE. UU.</font></font></b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es409523/">https://habr.com/ru/post/es409523/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es409511/index.html">SORM a expensas de los operadores, y Yarovaya?</a></li>
<li><a href="../es409515/index.html">OMower SDK para robots con ruedas (código abierto, hardware abierto)</a></li>
<li><a href="../es409517/index.html">La mente comenzó cuando los dioses dejaron de hablar.</a></li>
<li><a href="../es409519/index.html">Telescopio James Webb controlado por temperaturas ultrabajas y vacío</a></li>
<li><a href="../es409521/index.html">Nuevos estabilizadores DJI Osmo Mobile 2 y Ronin-S</a></li>
<li><a href="../es409525/index.html">Cómo solo una compañía puede expandir los límites de lo que es posible en el campo de los vehículos aéreos no tripulados</a></li>
<li><a href="../es409527/index.html">VRChat: el nuevo salvaje oeste en realidad virtual</a></li>
<li><a href="../es409529/index.html">SpaceX completó con éxito la misión militar estadounidense de Zuma's Secret</a></li>
<li><a href="../es409531/index.html">Microsoft ha desarrollado una IA que puede leer texto y responder preguntas sobre lo que lee.</a></li>
<li><a href="../es409533/index.html">Nueva botnet infecta la tecnología de los mineros, reemplazando las direcciones de billetera</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>