<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ¤™ğŸ½ ğŸ‘©ğŸ¼â€âœˆï¸ ğŸ‘©â€ğŸ‘©â€ğŸ‘¦ Cara membuat AI-rasis tanpa banyak usaha ğŸ‘©ğŸ¾â€âš•ï¸ ğŸ‘« ğŸ©</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pelajaran peringatan. 

 Mari kita membuat penggolong nada suara! 

 Analisis sentimen (analisis sentimen) adalah tugas yang sangat umum dalam pemrose...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cara membuat AI-rasis tanpa banyak usaha</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/436506/"> Pelajaran peringatan. <br><br>  <b>Mari kita membuat penggolong nada suara!</b> <br><br>  Analisis sentimen (analisis sentimen) adalah tugas yang sangat umum dalam pemrosesan bahasa alami (NLP), dan ini tidak mengejutkan.  Penting bagi bisnis untuk memahami apa yang dikatakan orang: positif atau negatif.  Analisis semacam itu digunakan untuk memantau jejaring sosial, umpan balik pelanggan, dan bahkan dalam perdagangan saham algoritmik (sebagai hasilnya, bot <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">membeli saham Berkshire Hathaway setelah memposting ulasan positif tentang peran Anne Hathaway dalam film terakhir</a> ). <br><br>  Metode analisis kadang-kadang terlalu disederhanakan, tetapi merupakan salah satu cara termudah untuk mendapatkan hasil yang terukur.  Kirimkan saja teks - dan hasilnya positif dan negatif.  Tidak perlu berurusan dengan pohon parsing, membuat grafik atau representasi kompleks lainnya. <br><a name="habracut"></a><br>  Ini yang akan kita lakukan.  Kami akan mengikuti jalur yang paling tidak tahan dan membuat penggolong paling sederhana, yang mungkin terlihat sangat akrab bagi semua orang yang terlibat dalam pengembangan yang relevan di bidang NLP.  Sebagai contoh, model seperti itu dapat ditemukan di artikel <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Deep Averaging Networks</a></i> (Iyyer et al., 2015).  Kami sama sekali tidak mencoba untuk menantang hasil mereka atau mengkritik model;  kami hanya memberikan metode representasi vektor yang terkenal dari kata-kata. <br><br>  Rencana kerja: <br><br><ul><li>  Perkenalkan <b>representasi vektor</b> khas <b>kata-kata</b> untuk bekerja dengan makna (makna). </li><li>  Perkenalkan <b>pelatihan dan set data uji</b> dengan daftar standar kata-kata positif dan negatif. </li><li>  <b>Latih classifier</b> gradient descent untuk mengenali kata-kata positif dan negatif lainnya berdasarkan representasi vektor mereka. </li><li>  Gunakan classifier ini untuk menghitung <b>peringkat nada suara</b> untuk kalimat teks. </li><li>  <b>Untuk melihat monster</b> yang telah kita buat. </li></ul><br>  Dan kemudian kita akan melihat, "cara membuat AI-rasis tanpa upaya khusus."  Tentu saja, Anda tidak dapat meninggalkan sistem dalam bentuk yang mengerikan, jadi kami akan: <br><br><ul><li>  <b>Nilai masalah secara</b> statistik sehingga menjadi mungkin untuk mengukur kemajuan saat diselesaikan. </li><li>  <b>Tingkatkan data</b> untuk mendapatkan model semantik yang lebih akurat dan kurang rasis. </li></ul><br><h1>  Ketergantungan perangkat lunak </h1><br>  Tutorial ini ditulis dalam Python dan bergantung pada tumpukan pembelajaran mesin Python yang khas: <code>numpy</code> dan <code>scipy</code> untuk komputasi numerik, <code>pandas</code> untuk manajemen data, dan <code>scikit-learn</code> learning untuk pembelajaran mesin.  Pada akhirnya, kami juga <code>matplotlib</code> <code>seaborn</code> <code>matplotlib</code> dan <code>seaborn</code> untuk membuat diagram. <br><br>  Pada prinsipnya, <code>scikit-learn</code> dapat diganti dengan TensorFlow atau Keras, atau sesuatu seperti itu: mereka juga dapat melatih classifier pada gradient descent.  Tetapi kita tidak membutuhkan abstraksi mereka, karena di sini pelatihan berlangsung dalam satu tahap. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> statsmodels.formula.api <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.linear_model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SGDClassifier <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> accuracy_score <span class="hljs-comment"><span class="hljs-comment">#     %matplotlib inline seaborn.set_context('notebook', rc={'figure.figsize': (10, 6)}, font_scale=1.5)</span></span></code> </pre> <br><h1>  Langkah 1. Representasi vektor kata-kata </h1><br>  Representasi vektor sering digunakan ketika ada input teks.  Kata-kata menjadi vektor dalam ruang multidimensi, di mana vektor yang berdekatan mewakili makna yang sama.  Dengan menggunakan representasi vektor, Anda dapat membandingkan kata-kata dengan (secara kasar) artinya, dan tidak hanya dengan pencocokan persis. <br><br>  Pembelajaran yang berhasil membutuhkan ratusan gigabyte teks.  Untungnya, berbagai tim peneliti telah melakukan pekerjaan ini dan menyediakan model representasi vektor pra-terlatih yang tersedia untuk diunduh. <br><br>  Dua <b>set</b> data paling terkenal untuk bahasa Inggris adalah <b>word2vec</b> (dilatih tentang teks Google News) dan <b>GloVe</b> (pada halaman web Common Crawl).  Setiap dari mereka akan memberikan hasil yang serupa, tetapi kami akan mengambil model GloVe karena memiliki sumber data yang lebih transparan. <br><br>  GloVe hadir dalam tiga ukuran: 6 miliar, 42 miliar dan 840 miliar. Model terbaru adalah yang paling kuat, tetapi membutuhkan sumber daya pemrosesan yang signifikan.  Versi 42 miliar cukup bagus, dan kamus rapi dipangkas menjadi 1 juta kata.  Kita berada di jalur yang paling tidak resistan, jadi ambil versi 42 miliar. <br><br><blockquote>  <b>- Mengapa begitu penting untuk menggunakan model "terkenal"?</b> <br><br>  "Aku senang kamu bertanya tentang ini, lawan bicara hipotetis!"  Pada setiap langkah kami mencoba melakukan sesuatu yang sangat tipikal, dan model terbaik untuk representasi vektor kata untuk beberapa alasan belum ditentukan.  Saya harap artikel ini akan membangkitkan keinginan untuk menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">model modern berkualitas tinggi</a> , terutama yang memperhitungkan kesalahan algoritmik dan mencoba memperbaikinya.  Namun, lebih lanjut tentang itu nanti. </blockquote><br>  Unduh glove.42B.300d.zip dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">situs web GloVe</a> dan ekstrak <code>data/glove.42B.300d.txt</code> file <code>data/glove.42B.300d.txt</code> .  Selanjutnya, kita mendefinisikan fungsi untuk membaca vektor dalam format sederhana. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">load_embeddings</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""  DataFrame      ,   word2vec, GloVe, fastText  ConceptNet Numberbatch.            . """</span></span> labels = [] rows = [] <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(filename, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> infile: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(infile): items = line.rstrip().split(<span class="hljs-string"><span class="hljs-string">' '</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(items) == <span class="hljs-number"><span class="hljs-number">2</span></span>: <span class="hljs-comment"><span class="hljs-comment"># This is a header row giving the shape of the matrix continue labels.append(items[0]) values = np.array([float(x) for x in items[1:]], 'f') rows.append(values) arr = np.vstack(rows) return pd.DataFrame(arr, index=labels, dtype='f') embeddings = load_embeddings('data/glove.42B.300d.txt') embeddings.shape</span></span></code> </pre> <br> <code>(1917494, 300)</code> <br> <h1>  Langkah 2. Kamus nada emas standar </h1><br>  Sekarang kita memerlukan informasi kata mana yang dianggap positif dan mana yang negatif.  Ada banyak kamus seperti itu, tetapi kami akan mengambil kamus yang sangat sederhana (Hu dan Liu, 2004), yang digunakan dalam artikel oleh <i>Deep Averaging Networks</i> . <br><br>  Unduh kamus dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">situs web Bing Liu</a> dan ekstrak data dalam <code>data/positive-words.txt</code> dan <code>data/negative-words.txt</code> . <br><br>  Selanjutnya, kami menentukan cara membaca file-file ini dan menetapkannya sebagai <code>neg_words</code> dan <code>neg_words</code> : <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">load_lexicon</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""       (https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html)      Latin-1.      ,    - .    ,    ';'   ,   . """</span></span> lexicon = [] <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(filename, encoding=<span class="hljs-string"><span class="hljs-string">'latin-1'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> infile: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> infile: line = line.rstrip() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> line.startswith(<span class="hljs-string"><span class="hljs-string">';'</span></span>): lexicon.append(line) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> lexicon pos_words = load_lexicon(<span class="hljs-string"><span class="hljs-string">'data/positive-words.txt'</span></span>) neg_words = load_lexicon(<span class="hljs-string"><span class="hljs-string">'data/negative-words.txt'</span></span>)</code> </pre> <br><h1>  Langkah 3. Kami melatih model untuk memprediksi nada suara </h1><br>  Berdasarkan vektor kata-kata positif dan negatif, kami menggunakan perintah Pandas <code>.loc[]</code> untuk mencari representasi vektor dari semua kata. <br><br>  Beberapa kata tidak ada dalam kamus GloVe.  Paling sering ini adalah salah ketik seperti "fancinating".  Di sini kita melihat sekelompok <code>NaN</code> , yang menunjukkan tidak adanya vektor, dan menghapusnya dengan perintah <code>.dropna()</code> . <br><br> <code>pos_vectors = embeddings.loc[pos_words].dropna() <br> neg_vectors = embeddings.loc[neg_words].dropna()</code> <br> <br>  Sekarang kita membuat array data pada input (representasi vektor) dan output (1 untuk kata-kata positif dan -1 untuk negatif).  Kami juga memeriksa bahwa vektor dilampirkan ke kata-kata sehingga kami dapat menafsirkan hasilnya. <br><br> <code>vectors = pd.concat([pos_vectors, neg_vectors]) <br> targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index]) <br> labels = list(pos_vectors.index) + list(neg_vectors.index)</code> <br> <br><blockquote>  <b>- Tunggu sebentar.</b>  <b>Beberapa kata tidak positif atau negatif, mereka netral.</b>  <b>Bukankah seharusnya kelas ketiga diciptakan untuk kata-kata netral?</b> <br><br>  "Aku pikir dia akan berguna."  Nanti kita akan melihat masalah apa yang muncul karena penugasan nada suara untuk kata-kata netral.  Jika kita dapat dengan andal mengidentifikasi kata-kata netral, maka sangat mungkin untuk meningkatkan kompleksitas penggolong menjadi tiga kategori.  Tetapi Anda perlu menemukan kamus kata-kata netral, karena di kamus Liu hanya ada yang positif dan negatif. <br><br>  Jadi saya mencoba versi saya dengan 800 contoh kata dan menambah bobot untuk memprediksi kata-kata netral.  Tetapi hasil akhirnya tidak jauh berbeda dari apa yang akan Anda lihat sekarang. <br><br>  <b>- Bagaimana daftar ini membedakan kata-kata positif dan negatif?</b>  <b>Bukankah itu tergantung pada konteksnya?</b> <br><br>  - Pertanyaan bagus.  Analisis kunci umum tidak sesederhana kelihatannya.  Perbatasan cukup arbitrer di beberapa tempat.  Dalam daftar ini, kata "kurang ajar" ditandai sebagai "buruk", dan "ambisius" sebagai "baik".  "Komik" itu buruk, dan "lucu" itu bagus.  â€œPengembalian uangâ€ itu baik, meskipun biasanya disebutkan dalam konteks yang buruk ketika Anda berutang uang kepada seseorang atau Anda berutang pada seseorang. <br><br>  Semua orang mengerti bahwa nada suara ditentukan oleh konteks, tetapi dalam model sederhana Anda harus mengabaikan konteks dan berharap bahwa nada suara rata-rata akan ditebak dengan benar. </blockquote><br>  Menggunakan fungsi <code>train_test_split</code> , <code>train_test_split</code> secara bersamaan membagi vektor input, nilai output, dan label ke dalam data pelatihan dan pengujian, sambil menyisakan 10% untuk pengujian. <br><br><pre> <code class="python hljs">train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \ train_test_split(vectors, targets, labels, test_size=<span class="hljs-number"><span class="hljs-number">0.1</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">0</span></span>)</code> </pre> <br>  Sekarang buat classifier dan lulus vektor melalui iterasi melaluinya.  Kami menggunakan fungsi kerugian logistik sehingga classifier akhir dapat menyimpulkan probabilitas bahwa kata tersebut positif atau negatif. <br><br><pre> <code class="python hljs">model = SGDClassifier(loss=<span class="hljs-string"><span class="hljs-string">'log'</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">0</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">100</span></span>) model.fit(train_vectors, train_targets) SGDClassifier(alpha=<span class="hljs-number"><span class="hljs-number">0.0001</span></span>, average=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, class_weight=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, epsilon=<span class="hljs-number"><span class="hljs-number">0.1</span></span>, eta0=<span class="hljs-number"><span class="hljs-number">0.0</span></span>, fit_intercept=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, l1_ratio=<span class="hljs-number"><span class="hljs-number">0.15</span></span>, learning_rate=<span class="hljs-string"><span class="hljs-string">'optimal'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'log'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">100</span></span>, n_jobs=<span class="hljs-number"><span class="hljs-number">1</span></span>, penalty=<span class="hljs-string"><span class="hljs-string">'l2'</span></span>, power_t=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">0</span></span>, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">0</span></span>, warm_start=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)</code> </pre> <br>  Kami mengevaluasi penggolong pada vektor uji.  Ini menunjukkan akurasi 95%.  Tidak buruk. <br><br> <code>accuracy_score(model.predict(test_vectors), test_targets) <br> 0.95022624434389136</code> <br> <br>  Kami mendefinisikan fungsi prediksi nada suara untuk kata-kata tertentu, dan kemudian menggunakannya untuk beberapa contoh dari data uji. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">vecs_to_sentiment</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(vecs)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># predict_log_proba  log-    predictions = model.predict_log_proba(vecs) #        #  log-    . return predictions[:, 1] - predictions[:, 0] def words_to_sentiment(words): vecs = embeddings.loc[words].dropna() log_odds = vecs_to_sentiment(vecs) return pd.DataFrame({'sentiment': log_odds}, index=vecs.index) #  20      words_to_sentiment(test_labels).ix[:20]</span></span></code> </pre> <br><table border="1" width="350"><thead><tr><th></th><th>  nada suara </th></tr></thead><tbody><tr><th>  gelisah </th><td>  -9.931679 </td></tr><tr><th>  menyela </th><td>  -9.634706 </td></tr><tr><th>  dengan sabar </th><td>  1.466919 </td></tr><tr><th>  imajiner </th><td>  -2.989215 </td></tr><tr><th>  perpajakan </th><td>  0.468522 </td></tr><tr><th>  terkenal di dunia </th><td>  6.908561 </td></tr><tr><th>  murah </th><td>  9.237223 </td></tr><tr><th>  kekecewaan </th><td>  -8.737182 </td></tr><tr><th>  totaliter </th><td>  -10.851580 </td></tr><tr><th>  berperang </th><td>  -8.328674 </td></tr><tr><th>  membeku </th><td>  -8.456981 </td></tr><tr><th>  dosa </th><td>  -7.839670 </td></tr><tr><th>  rapuh </th><td>  -4.018289 </td></tr><tr><th>  tertipu </th><td>  -4,309344 </td></tr><tr><th>  belum terselesaikan </th><td>  -2.816172 </td></tr><tr><th>  secara cerdik </th><td>  2.339609 </td></tr><tr><th>  mendemonstrasikan </th><td>  -2.102152 </td></tr><tr><th>  riang </th><td>  8.747150 </td></tr><tr><th>  tidak populer </th><td>  -7.887475 </td></tr><tr><th>  bersimpati </th><td>  1.790899 </td></tr></tbody></table><br>  Terlihat bahwa classifier berfungsi.  Dia belajar menggeneralisasi nada suara dengan kata-kata di luar data pelatihan. <br><br><h1>  Langkah 4. Dapatkan skor nada suara untuk teks. </h1><br>  Ada banyak cara untuk menambahkan vektor ke perkiraan keseluruhan.  Sekali lagi, kita mengikuti jalan resistensi paling sedikit, jadi ambil saja nilai rata-rata. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re TOKEN_RE = re.compile(<span class="hljs-string"><span class="hljs-string">r"\w.*?\b"</span></span>) <span class="hljs-comment"><span class="hljs-comment"># regex  ,     (\w)   #   (.+?)    (\b).   #       . def text_to_sentiment(text): tokens = [token.casefold() for token in TOKEN_RE.findall(text)] sentiments = words_to_sentiment(tokens) return sentiments['sentiment'].mean()</span></span></code> </pre> <br>  Ada banyak yang harus ditanyakan tentang pengoptimalan: <br><br><ul><li>  Memperkenalkan hubungan terbalik antara bobot kata dan frekuensinya, sehingga preposisi yang sama tidak terlalu memengaruhi nada suara. </li><li>  Pengaturan agar kalimat pendek tidak berakhir dengan nilai nada suara yang ekstrem. </li><li>  Frasa akuntansi. </li><li>  Algoritma segmentasi kata yang lebih andal yang apostrof tidak merobohkan. </li><li>  Menghitung negatif seperti "tidak puas." </li></ul><br>  Tetapi semuanya membutuhkan kode tambahan dan tidak akan mengubah hasil secara fundamental.  Setidaknya sekarang Anda dapat secara kasar membandingkan berbagai penawaran: <br><br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"this example is pretty cool"</span></span>) <span class="hljs-number"><span class="hljs-number">3.889968926086298</span></span></code> </pre> <br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"this example is okay"</span></span>) <span class="hljs-number"><span class="hljs-number">2.7997773492425186</span></span></code> </pre> <br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"meh, this example sucks"</span></span>) <span class="hljs-number"><span class="hljs-number">-1.1774475917460698</span></span></code> </pre> <br><h1>  Langkah 5. Lihatlah monster yang kami buat </h1><br>  Tidak setiap kalimat memiliki nada suara yang diucapkan.  Mari kita lihat apa yang terjadi dengan kalimat netral: <br><br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"Let's go get Italian food"</span></span>) <span class="hljs-number"><span class="hljs-number">2.0429166109408983</span></span></code> </pre> <br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"Let's go get Chinese food"</span></span>) <span class="hljs-number"><span class="hljs-number">1.4094033658140972</span></span></code> </pre> <br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"Let's go get Mexican food"</span></span>) <span class="hljs-number"><span class="hljs-number">0.38801985560121732</span></span></code> </pre> <br>  Saya sudah menemukan fenomena seperti itu ketika menganalisis ulasan restoran dengan mempertimbangkan representasi kata-kata vektor.  Tanpa alasan yang jelas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">, semua restoran Meksiko memiliki skor keseluruhan yang lebih rendah</a> . <br><br>  Representasi vektor menangkap perbedaan semantik yang halus dalam konteks.  Karena itu, mereka mencerminkan prasangka masyarakat kita. <br><br>  Berikut adalah beberapa saran netral lainnya: <br><br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"My name is Emily"</span></span>) <span class="hljs-number"><span class="hljs-number">2.2286179364745311</span></span></code> </pre> <br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"My name is Heather"</span></span>) <span class="hljs-number"><span class="hljs-number">1.3976291151079159</span></span></code> </pre> <br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"My name is Yvette"</span></span>) <span class="hljs-number"><span class="hljs-number">0.98463802132985556</span></span></code> </pre> <br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"My name is Shaniqua"</span></span>) <span class="hljs-number"><span class="hljs-number">-0.47048131775890656</span></span></code> </pre> <br>  Ya ampun ... <br><br>  Sistem yang terkait dengan nama-nama orang perasaan yang sama sekali berbeda.  Anda dapat melihat ini dan banyak contoh lainnya dan melihat bahwa nada suara biasanya lebih tinggi untuk nama putih stereotip dan lebih rendah untuk nama hitam stereotip. <br><br>  Tes ini digunakan oleh Caliscan, Bryson dan Narayanan dalam makalah penelitian mereka yang diterbitkan dalam jurnal <i>Science</i> pada April 2017.  Ini membuktikan bahwa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">semantik bahasa corpus mengandung prasangka masyarakat</a> .  Kami akan menggunakan metode ini. <br><br><h1>  Langkah 6. Menilai masalah </h1><br>  Kami ingin memahami bagaimana cara menghindari kesalahan seperti itu.  Mari kita lewati lebih banyak data melalui classifier dan secara statistik mengukur "bias" nya. <br><br>  Di sini kami memiliki empat daftar nama yang mencerminkan latar belakang etnis yang berbeda, terutama di AS.  Dua yang pertama adalah daftar nama dominan "putih" dan "hitam", diadaptasi berdasarkan artikel oleh Kaliskan et al. Saya juga menambahkan nama-nama Spanyol dan Muslim dari bahasa Arab dan Urdu. <br><br>  Data ini digunakan untuk memverifikasi bias algoritma selama proses pembuatan ConceptNet: dapat ditemukan dalam modul <code>conceptnet5.vectors.evaluation.bias</code> .  Ada ide untuk memperluas kamus ke kelompok etnis lain, dengan mempertimbangkan tidak hanya nama, tetapi juga nama keluarga. <br><br>  Berikut daftarnya: <br><br><pre> <code class="python hljs">NAMES_BY_ETHNICITY = { <span class="hljs-comment"><span class="hljs-comment">#           . 'White': [ 'Adam', 'Chip', 'Harry', 'Josh', 'Roger', 'Alan', 'Frank', 'Ian', 'Justin', 'Ryan', 'Andrew', 'Fred', 'Jack', 'Matthew', 'Stephen', 'Brad', 'Greg', 'Jed', 'Paul', 'Todd', 'Brandon', 'Hank', 'Jonathan', 'Peter', 'Wilbur', 'Amanda', 'Courtney', 'Heather', 'Melanie', 'Sara', 'Amber', 'Crystal', 'Katie', 'Meredith', 'Shannon', 'Betsy', 'Donna', 'Kristin', 'Nancy', 'Stephanie', 'Bobbie-Sue', 'Ellen', 'Lauren', 'Peggy', 'Sue-Ellen', 'Colleen', 'Emily', 'Megan', 'Rachel', 'Wendy' ], 'Black': [ 'Alonzo', 'Jamel', 'Lerone', 'Percell', 'Theo', 'Alphonse', 'Jerome', 'Leroy', 'Rasaan', 'Torrance', 'Darnell', 'Lamar', 'Lionel', 'Rashaun', 'Tyree', 'Deion', 'Lamont', 'Malik', 'Terrence', 'Tyrone', 'Everol', 'Lavon', 'Marcellus', 'Terryl', 'Wardell', 'Aiesha', 'Lashelle', 'Nichelle', 'Shereen', 'Temeka', 'Ebony', 'Latisha', 'Shaniqua', 'Tameisha', 'Teretha', 'Jasmine', 'Latonya', 'Shanise', 'Tanisha', 'Tia', 'Lakisha', 'Latoya', 'Sharise', 'Tashika', 'Yolanda', 'Lashandra', 'Malika', 'Shavonn', 'Tawanda', 'Yvette' ], #         . 'Hispanic': [ 'Juan', 'JosÃ©', 'Miguel', 'LuÃ­s', 'Jorge', 'Santiago', 'MatÃ­as', 'SebastiÃ¡n', 'Mateo', 'NicolÃ¡s', 'Alejandro', 'Samuel', 'Diego', 'Daniel', 'TomÃ¡s', 'Juana', 'Ana', 'Luisa', 'MarÃ­a', 'Elena', 'SofÃ­a', 'Isabella', 'Valentina', 'Camila', 'Valeria', 'Ximena', 'Luciana', 'Mariana', 'Victoria', 'Martina' ], #       # ,   .     . # #          # -   .    #   ,    . # #       . 'Arab/Muslim': [ 'Mohammed', 'Omar', 'Ahmed', 'Ali', 'Youssef', 'Abdullah', 'Yasin', 'Hamza', 'Ayaan', 'Syed', 'Rishaan', 'Samar', 'Ahmad', 'Zikri', 'Rayyan', 'Mariam', 'Jana', 'Malak', 'Salma', 'Nour', 'Lian', 'Fatima', 'Ayesha', 'Zahra', 'Sana', 'Zara', 'Alya', 'Shaista', 'Zoya', 'Yasmin' ] }</span></span></code> </pre> <br>  Menggunakan Panda, kami akan menyusun daftar nama, asal etnis dan peringkat nada suara mereka yang dominan: <br><br><pre> <code class="plaintext hljs">def name_sentiment_table(): frames = [] for group, name_list in sorted(NAMES_BY_ETHNICITY.items()): lower_names = [name.lower() for name in name_list] sentiments = words_to_sentiment(lower_names) sentiments['group'] = group frames.append(sentiments) #           return pd.concat(frames) name_sentiments = name_sentiment_table()</code> </pre> <br>  Data sampel: <br><br> <code>name_sentiments.ix[::25]</code> <br> <table border="1" width="350"><thead><tr><th></th><th>  nada suara </th><th>  grup </th></tr></thead><tbody><tr><th>  mohammed </th><td>  0.834974 </td><td>  Arab / Muslim </td></tr><tr><th>  alya </th><td>  3.916803 </td><td>  Arab / Muslim </td></tr><tr><th>  terryl </th><td>  -2.858010 </td><td>  Hitam </td></tr><tr><th>  josÃ© </th><td>  0,432956 </td><td>  Hispanik </td></tr><tr><th>  luciana </th><td>  1.086073 </td><td>  Hispanik </td></tr><tr><th>  terima kasih </th><td>  0,391858 </td><td>  Putih </td></tr><tr><th>  mulai </th><td>  2.158679 </td><td>  Putih </td></tr></tbody></table><br>  Kami akan membuat grafik distribusi nada suara untuk setiap nama. <br><br><pre> <code class="python hljs">plot = seaborn.swarmplot(x=<span class="hljs-string"><span class="hljs-string">'group'</span></span>, y=<span class="hljs-string"><span class="hljs-string">'sentiment'</span></span>, data=name_sentiments) plot.set_ylim([<span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>])</code> </pre> <br> <code>(-10, 10)</code> <br> <br><img src="https://habrastorage.org/webt/qv/y7/ge/qvy7gel8rrvm5txo-nou6g0i0re.png"><br><br>  Atau sebagai histogram dengan interval kepercayaan untuk rata-rata 95%. <br><br><pre> <code class="python hljs">plot = seaborn.barplot(x=<span class="hljs-string"><span class="hljs-string">'group'</span></span>, y=<span class="hljs-string"><span class="hljs-string">'sentiment'</span></span>, data=name_sentiments, capsize=<span class="hljs-number"><span class="hljs-number">.1</span></span>)</code> </pre> <br><img src="https://habrastorage.org/webt/uv/ib/n6/uvibn6olthaxt6cxbd96szehq94.png"><br><br>  Terakhir, jalankan paket statistik <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">statsmodels yang</a> serius.  Ini akan menunjukkan betapa hebatnya bias algoritma (bersama dengan banyak statistik lainnya). <br><br><br>  <font color="gray">Hasil Regresi OLS</font> <br><table><tbody><tr><th>  Dep.  Variabel: </th><td>  sentimen </td><th>  R-kuadrat: </th><td>  0,208 </td></tr><tr><th>  Model: </th><td>  OLS </td><th>  Ajj  R-kuadrat: </th><td>  0,192 </td></tr><tr><th>  Metode: </th><td>  Kotak kuadrat </td><th>  F-statistik: </th><td>  04/13 </td></tr><tr><th>  Tanggal: </th><td>  Kamis, 13 Jul 2017 </td><th>  Prob (F-statistik): </th><td>  1.31e-07 </td></tr><tr><th>  Waktu: </th><td>  11:31:17 </td><th>  Log-Kemungkinan: </th><td>  -356,78 </td></tr><tr><th>  Tidak.  Pengamatan: </th><td>  153 </td><th>  AIC: </th><td>  721.6 </td></tr><tr><th>  Df Residual: </th><td>  149 </td><th>  BIC: </th><td>  733.7 </td></tr><tr><th>  Model Df: </th><td>  3 </td><th></th><td></td></tr><tr><th>  Jenis Kovarian: </th><td>  tidak keras </td><th></th><td></td></tr></tbody></table><br>  F-statistik adalah rasio variasi antara kelompok dengan variasi dalam kelompok, yang dapat diambil sebagai penilaian umum bias. <br><br>  Tepat di bawahnya ditunjukkan kemungkinan bahwa kita akan melihat F-statistik maksimum dengan hipotesis nol: yaitu, dengan tidak adanya perbedaan antara opsi yang dibandingkan.  Peluangnya sangat, sangat rendah.  Dalam sebuah artikel ilmiah, kami akan menyebut hasilnya "sangat signifikan secara statistik." <br><br>  Kita perlu meningkatkan nilai-F.  Semakin rendah semakin baik. <br><br> <code>ols_model.fvalue <br> 13.041597745167659</code> <br> <br><h1>  Langkah 7. Mencoba data lain. </h1><br>  Kami sekarang memiliki kesempatan untuk mengukur bias model secara numerik.  Mari kita coba sesuaikan.  Untuk melakukan ini, Anda perlu mengulangi banyak hal yang dulu hanya langkah terpisah dalam notepad Python. <br><br>  Jika saya menulis kode yang baik dan didukung, saya tidak akan menggunakan variabel global seperti <code>model</code> dan <code>embeddings</code> .  Tetapi kode spageti saat ini memungkinkan Anda untuk memeriksa setiap langkah dengan lebih baik dan memahami apa yang terjadi.  Kami menggunakan kembali bagian dari kode dan setidaknya mendefinisikan fungsi untuk mengulangi beberapa langkah: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">retrain_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(new_embs)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""      . """</span></span> <span class="hljs-keyword"><span class="hljs-keyword">global</span></span> model, embeddings, name_sentiments embeddings = new_embs pos_vectors = embeddings.loc[pos_words].dropna() neg_vectors = embeddings.loc[neg_words].dropna() vectors = pd.concat([pos_vectors, neg_vectors]) targets = np.array([<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> entry <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> pos_vectors.index] + [<span class="hljs-number"><span class="hljs-number">-1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> entry <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> neg_vectors.index]) labels = list(pos_vectors.index) + list(neg_vectors.index) train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \ train_test_split(vectors, targets, labels, test_size=<span class="hljs-number"><span class="hljs-number">0.1</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">0</span></span>) model = SGDClassifier(loss=<span class="hljs-string"><span class="hljs-string">'log'</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">0</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">100</span></span>) model.fit(train_vectors, train_targets) accuracy = accuracy_score(model.predict(test_vectors), test_targets) print(<span class="hljs-string"><span class="hljs-string">"Accuracy of sentiment: {:.2%}"</span></span>.format(accuracy)) name_sentiments = name_sentiment_table() ols_model = statsmodels.formula.api.ols(<span class="hljs-string"><span class="hljs-string">'sentiment ~ group'</span></span>, data=name_sentiments).fit() print(<span class="hljs-string"><span class="hljs-string">"F-value of bias: {:.3f}"</span></span>.format(ols_model.fvalue)) print(<span class="hljs-string"><span class="hljs-string">"Probability given null hypothesis: {:.3}"</span></span>.format(ols_model.f_pvalue)) <span class="hljs-comment"><span class="hljs-comment">#        Y plot = seaborn.swarmplot(x='group', y='sentiment', data=name_sentiments) plot.set_ylim([-10, 10])</span></span></code> </pre> <br><h3>  Kami mencoba word2vec </h3><br>  Dapat diasumsikan bahwa hanya GloVe yang memiliki masalah.  Mungkin ada banyak situs yang meragukan dalam database Common Crawl dan setidaknya 20 salinan Urban Dictionary of gaul jalanan.  Mungkin pada basis yang berbeda akan lebih baik: bagaimana dengan word2vec tua yang baik yang dilatih di Google News? <br><br>  Tampaknya sumber yang paling otoritatif untuk data word2vec adalah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">file ini di Google Drive</a> .  Unduh dan simpan sebagai <code>data/word2vec-googlenews-300.bin.gz</code> . <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   ConceptNet   word2vec   Pandas     from conceptnet5.vectors.formats import load_word2vec_bin w2v = load_word2vec_bin('data/word2vec-googlenews-300.bin.gz', nrows=2000000) #  word2vec    w2v.index = [label.casefold() for label in w2v.index] #  ,    w2v = w2v.reset_index().drop_duplicates(subset='index', keep='first').set_index('index') retrain_model(w2v)</span></span></code> </pre> <br> <code>Accuracy of sentiment: 94.30% <br> F-value of bias: 15.573 <br> Probability given null hypothesis: 7.43e-09</code> <br> <br>  Jadi word2vec ternyata lebih buruk dengan nilai F lebih dari 15. <br><br>  Pada prinsipnya, bodoh mengharapkan <i>berita</i> akan lebih terlindungi dari bias. <br><br><h3>  Mencoba ConceptNet Numberbatch </h3><br>  Akhirnya, saya dapat berbicara tentang proyek saya sendiri tentang representasi vektor kata-kata. <br><br>  ConceptNet dengan fitur presentasi vektor adalah grafik pengetahuan yang sedang saya kerjakan.  Ini menormalkan representasi vektor pada tahap pelatihan, mengidentifikasi dan menghapus beberapa sumber rasisme algoritmik dan seksisme.  Metode mengoreksi bias didasarkan pada artikel ilmiah oleh Bulukbashi et al. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">"Debiasing Word Embeddings"</a> dan digeneralisasi untuk menghilangkan beberapa jenis bias secara bersamaan.  Sejauh yang saya tahu, ini adalah satu-satunya sistem semantik di mana ada sesuatu seperti itu. <br><br>  Dari waktu ke waktu, kami mengekspor vektor yang sudah dihitung sebelumnya dari ConceptNet - rilis ini disebut <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ConceptNet Numberbatch</a> .  Pada bulan April 2017, rilis pertama dengan koreksi bias dirilis, jadi kami akan memuat vektor berbahasa Inggris dan melatih kembali model kami. <br><br>  <code><a href="">numberbatch-en-17.04b.txt.gz</a></code> , menyimpannya di <code>data/</code> direktori dan melatih kembali model: <br><br><pre> <code class="python hljs">retrain_model(load_embeddings(<span class="hljs-string"><span class="hljs-string">'data/numberbatch-en-17.04b.txt'</span></span>))</code> </pre> <br> <code>Accuracy of sentiment: 97.46% <br> F-value of bias: 3.805 <br> Probability given null hypothesis: 0.0118</code> <br> <br><img src="https://habrastorage.org/webt/5d/iu/uq/5diuuqrst8bca5-m7fljox--pro.png"><br><br>  Jadi, sudahkah ConceptNet Numberbatch benar-benar menyelesaikan masalah?  Tidak ada lagi rasisme algoritmik?  <b>Tidak.</b> <br><br>  Apakah rasisme menjadi jauh lebih sedikit?  <b>Tentu saja</b> <br><br>  Rentang utama untuk kelompok etnis tumpang tindih jauh lebih banyak daripada di vektor GloVe atau word2vec.  Dibandingkan dengan GloVe, nilai F menurun lebih dari tiga kali, dan dibandingkan dengan word2vec - lebih dari empat kali.  Dan secara umum, kami melihat perbedaan yang jauh lebih kecil dalam nada suara ketika membandingkan nama yang berbeda: ini seharusnya demikian, karena nama-nama tersebut benar-benar tidak boleh mempengaruhi hasil analisis. <br><br>  Tetapi sedikit korelasi tetap.  Mungkin saya bisa mengambil data dan parameter pelatihan seperti itu sehingga masalah tampaknya terpecahkan.  Tetapi ini akan menjadi pilihan yang buruk, karena <i>sebenarnya</i> masalahnya tetap ada, karena di ConceptNet kami tidak mengidentifikasi dan mengganti semua penyebab rasisme algoritmik.  Tapi ini awal yang baik. <br><br><h3>  Tidak ada jebakan </h3><br>  Harap dicatat bahwa dengan beralih ke ConceptNet Numberbatch, akurasi prediksi nada suara telah meningkat. <br><br>  Seseorang mungkin menyarankan bahwa memperbaiki rasisme algoritmik akan memperburuk hasil dengan beberapa cara lain.  Tapi tidak.  Anda mungkin memiliki data yang lebih baik dan kurang rasis.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Data benar-benar membaik dengan koreksi ini. </font><font style="vertical-align: inherit;">Rasisme word2vec dan GloVe yang diperoleh dari orang-orang tidak ada hubungannya dengan keakuratan algoritma.</font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Pendekatan lain </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tentu saja, ini hanya satu cara untuk menganalisis nada suara. Beberapa detail dapat diimplementasikan secara berbeda. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sebagai gantinya atau sebagai tambahan untuk mengubah basis vektor, Anda dapat mencoba untuk memperbaiki masalah ini secara langsung di output. Misalnya, umumnya menghilangkan penilaian nada suara untuk nama dan kelompok orang. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Secara umum ada opsi untuk menolak menghitung nada suara semua kata, dan menghitungnya hanya untuk kata-kata dari daftar. Ini mungkin bentuk paling umum dari analisis sentimen - tanpa pembelajaran mesin sama sekali. Hasilnya tidak akan memiliki bias yang lebih dari penulis daftar. Tetapi menolak pembelajaran mesin berarti mengurangi recall (recall), dan satu-satunya cara untuk mengadaptasi model ke dataset adalah dengan mengedit daftar secara manual.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sebagai pendekatan hibrid, Anda dapat membuat sejumlah besar estimasi perkiraan nada suara untuk kata-kata dan menginstruksikan seseorang untuk mengeditnya dengan sabar, membuat daftar kata-kata pengecualian dengan nol nada suara. </font><font style="vertical-align: inherit;">Tapi ini pekerjaan tambahan. </font><font style="vertical-align: inherit;">Di sisi lain, Anda benar-benar akan melihat cara kerja model. </font><font style="vertical-align: inherit;">Saya pikir dalam hal apa pun, ini harus dicari.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id436506/">https://habr.com/ru/post/id436506/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id436496/index.html">PVS-Studio untuk Java</a></li>
<li><a href="../id436498/index.html">Perangkat Lunak AG: Tidak Hanya ARIS</a></li>
<li><a href="../id436500/index.html">Bagaimana kerangka Rise of the Tomb Raider ditampilkan</a></li>
<li><a href="../id436502/index.html">Pampers Berlangganan atau Cara Menjual Lebih Banyak ke Pelanggan yang Sama</a></li>
<li><a href="../id436504/index.html">Sistem dalam Paket, atau Penutup Paket Apa Yang Ada Dalam Chip?</a></li>
<li><a href="../id436508/index.html">Investasi $ 10 juta dan pujian Wozniak - menciptakan komputer pendidikan untuk anak-anak</a></li>
<li><a href="../id436510/index.html">Data inti secara detail</a></li>
<li><a href="../id436512/index.html">Bagaimana kami menemukan rilis bermasalah dengan Graphite dan Moira. Rasakan Yandex.Money</a></li>
<li><a href="../id436514/index.html">Membuat cerita untuk Instagram dari PHP</a></li>
<li><a href="../id436518/index.html">Haiku Î²1 - buat / b / OS hebat lagi</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>