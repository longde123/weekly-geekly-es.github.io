<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚úåüèº ‚úäüèΩ üôÖüèª Cassandra para armazenar metadados: sucessos e falhas üë©üèº‚Äç‚öïÔ∏è üõåüèΩ üë©üèº‚Äçüé§</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Quais requisitos o armazenamento de metadados para um servi√ßo em nuvem atende? Sim, n√£o o mais comum, mas para empresas com suporte para data centers ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cassandra para armazenar metadados: sucessos e falhas</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/417617/"> Quais requisitos o armazenamento de metadados para um servi√ßo em nuvem atende?  Sim, n√£o o mais comum, mas para empresas com suporte para data centers geograficamente distribu√≠dos e Active-Active.  Obviamente, o sistema deve ter uma boa escala, <strong>toler√¢ncia a falhas e gostaria de poder implementar consist√™ncia personaliz√°vel das opera√ß√µes.</strong> <br><br>  Somente Cassandra √© adequado para todos esses requisitos e nada mais √© adequado.  Note-se que Cassandra √© muito legal, mas trabalhar com ele se parece com uma montanha-russa. <br><img src="https://habrastorage.org/webt/zs/tw/jb/zstwjb6bvwlg43rmuphw91_jtrm.jpeg"><br><br>  Em um relat√≥rio do Highload ++ 2017, <strong>Andrei Smirnov</strong> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">smira</a> ) decidiu que n√£o era interessante falar sobre o bem, mas falou detalhadamente sobre todos os problemas que tinha que enfrentar: sobre perda e corrup√ß√£o de dados, sobre zumbis e perda de desempenho.  Essas hist√≥rias s√£o realmente uma reminisc√™ncia de montanha-russa, mas para todos os problemas h√° uma solu√ß√£o, para a qual voc√™ √© bem-vindo ao gato. <br><br>  <strong><em>Sobre o palestrante:</em></strong> Andrey Smirnov trabalha para a Virtustream, uma empresa que implementa armazenamento em nuvem para empresas.  A ideia √© que a Amazon condicionalmente fa√ßa a nuvem para todos, e a Virtustream fa√ßa as coisas espec√≠ficas que uma grande empresa precisa. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/SAyClLjN6Sk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><a name="habracut"></a><br><h1>  Algumas palavras sobre Virtustream </h1><br>  Trabalhamos em uma equipe pequena completamente remota e estamos envolvidos em uma das solu√ß√µes em nuvem da Virtustream.  Esta √© uma nuvem de armazenamento de dados. <br><img src="https://habrastorage.org/webt/bo/rc/jh/borcjhczgtiycqzx8dz0bnh9zim.jpeg"><br><br>  Falando de maneira muito simples, essa √© uma API compat√≠vel com S3 na qual voc√™ pode armazenar objetos.  Para quem n√£o sabe o que √© o S3, √© apenas uma API HTTP com a qual voc√™ pode enviar objetos para a nuvem em algum lugar, recuper√°-los, exclu√≠-los, obter uma lista de objetos etc.  Al√©m disso - recursos mais complexos com base nessas opera√ß√µes simples. <br><br>  Temos alguns recursos distintos que a Amazon n√£o possui.  Uma delas s√£o as chamadas georregi√µes.  Na situa√ß√£o usual, quando voc√™ cria um reposit√≥rio e diz que armazenar√° objetos na nuvem, voc√™ deve selecionar uma regi√£o.  Uma regi√£o √© essencialmente um data center e seus objetos nunca sair√£o desse data center.  Se algo acontecer com ele, seus objetos n√£o estar√£o mais dispon√≠veis. <br><br>  Oferecemos regi√µes geogr√°ficas nas quais os dados est√£o localizados simultaneamente em v√°rios data centers (DC), pelo menos em dois, como na figura.  O cliente pode entrar em contato com qualquer data center, para ele √© transparente.  Os dados entre eles s√£o replicados, ou seja, trabalhamos no modo ativo-ativo e constantemente.  Isso fornece ao cliente recursos adicionais, incluindo: <br><br><ol><li>  maior confiabilidade de armazenamento, leitura e grava√ß√£o em caso de falha de DC ou perda de conectividade; <br></li><li>  disponibilidade de dados, mesmo se um dos controladores de dom√≠nio falhar; <br></li><li>  redirecionando as opera√ß√µes para o CD ‚Äúmais pr√≥ximo‚Äù. <br></li></ol><br>  Essa √© uma oportunidade interessante - mesmo que esses CDs estejam geograficamente distantes, alguns deles podem estar mais pr√≥ximos do cliente em diferentes momentos.  E acessar os dados no CD mais pr√≥ximo √© simplesmente mais r√°pido. <br><img src="https://habrastorage.org/webt/k-/ry/dl/k-rydl_mt74-eybwakpv1dpqjum.jpeg"><br><br>  Para dividir a constru√ß√£o sobre a qual falaremos em partes, apresentarei os objetos armazenados na nuvem em duas partes grandes: <br><br>  1. A primeira parte simples de um objeto s√£o <strong>dados</strong> .  Eles permanecem inalterados, foram baixados uma vez e √© tudo.  A √∫nica coisa que pode acontecer com eles mais tarde √© que podemos remov√™-los se n√£o forem mais necess√°rios. <br><br>  Nosso projeto anterior estava relacionado ao armazenamento de exabytes de dados, portanto, n√£o tivemos problemas com o armazenamento de dados.  Essa j√° era uma tarefa resolvida para n√≥s. <br><br>  2. <strong>Metadados</strong> .  Toda a l√≥gica de neg√≥cios, mais interessante, relacionada √† concorr√™ncia: acesso, registros, reescritas - na √°rea de metadados. <br><br>  Os metadados sobre o objeto levam em si a maior complexidade do projeto, os metadados armazenam um ponteiro para o bloco de dados armazenados do objeto. <br><br>  Do ponto de vista do usu√°rio, esse √© um objeto √∫nico, mas podemos dividi-lo em duas partes.  Hoje vou falar <strong>apenas sobre metadados</strong> . <br><br><h2>  Figuras <br></h2><br><ul><li>  <strong>Dados</strong> : 4 bytes. </li><li>  <strong>Clusters de metadados</strong> : 3. </li><li>  <strong>Objetos</strong> : 40 bilh√µes. </li><li>  <strong>Tamanho dos metadados</strong> : 160 TB (incluindo replica√ß√£o). </li><li>  <strong>Taxa de varia√ß√£o (metadados):</strong> 3000 objetos / s. </li></ul><br>  Se voc√™ observar esses indicadores com aten√ß√£o, a primeira coisa que chama sua aten√ß√£o √© o tamanho m√©dio muito pequeno do objeto armazenado.  Temos muitos metadados por unidade de volume de dados mestre.  Para n√≥s, n√£o foi menos uma surpresa do que talvez para voc√™ agora. <br><br>  Planejamos que ter√≠amos pelo menos uma ordem de dados, se n√£o 2, al√©m de metadados.  Ou seja, cada objeto ser√° significativamente maior e a quantidade de metadados ser√° menor.  Como os dados s√£o mais baratos de armazenar, menos opera√ß√µes com eles e os metadados s√£o muito mais caros, tanto no sentido de hardware quanto no sentido de atender e executar v√°rias opera√ß√µes neles. <br><br>  Al√©m disso, esses dados mudam a uma velocidade bastante alta.  Dei aqui o valor m√°ximo, o valor n√£o m√°ximo n√£o √© muito menor, mas, no entanto, uma carga bastante grande pode ser obtida em pontos espec√≠ficos no tempo. <br><br>  Esses n√∫meros j√° foram obtidos de um sistema operacional, mas vamos voltar um pouco ao tempo de projetar o armazenamento em nuvem. <br><br><h1>  Escolhendo um reposit√≥rio para metadados </h1><br>  Quando enfrentamos o desafio de que queremos ter regi√µes geogr√°ficas, ativo-ativo e precisamos armazenar metadados em algum lugar, pensamos que poderia ser? <br><br>  Obviamente, o reposit√≥rio (banco de dados) deve ter as seguintes propriedades: <br><br><ul><li>  <strong>Suporte ativo-ativo</strong> ; </li><li>  <strong>Escalabilidade.</strong> </li></ul><br>  Gostar√≠amos realmente que nosso produto fosse extremamente popular e n√£o sabemos como ele crescer√° ao mesmo tempo, para que o sistema seja dimensionado. <br><br><ul><li>  <strong>O equil√≠brio entre toler√¢ncia a falhas e confiabilidade de armazenamento.</strong> </li></ul><br>  Os metadados devem ser armazenados com seguran√ßa, porque se os perdermos e houver um link para os dados neles, perderemos o objeto inteiro. <br><br><ul><li>  <strong>Consist√™ncia personaliz√°vel de opera√ß√µes.</strong> </li></ul><br>  Devido ao fato de trabalharmos em v√°rios CDs e permitir a possibilidade de que os CDs n√£o estejam dispon√≠veis, al√©m disso, os CDs est√£o longe um do outro, n√£o podemos, durante a maioria das opera√ß√µes da API, exigir que essa opera√ß√£o seja executada simultaneamente em dois CDs.  Ser√° muito lento e imposs√≠vel se o segundo CD n√£o estiver dispon√≠vel.  Portanto, parte das opera√ß√µes deve funcionar localmente em um controlador de dom√≠nio. <br><br>  Mas, obviamente, algum tipo de converg√™ncia deve ocorrer em algum momento e, depois de resolver todos os conflitos, os dados devem estar vis√≠veis nos dois data centers.  Portanto, a consist√™ncia das opera√ß√µes deve ser ajustada. <br><br>  Do meu ponto de vista, Cassandra √© adequado para esses requisitos. <br><br><h1>  Cassandra </h1><br>  Eu ficaria muito feliz se n√£o tiv√©ssemos que usar Cassandra, porque para n√≥s era uma esp√©cie de nova experi√™ncia.  Mas nada mais √© adequado.  Parece-me que essa √© a situa√ß√£o mais triste do mercado para esses sistemas de armazenamento - n√£o h√° <strong>alternativa</strong> . <br><br><img src="https://habrastorage.org/webt/ge/l-/xo/gel-xoykdx5yx1-sb36hjinlsas.jpeg"><br><br><h3>  O que √© Cassandra? <br></h3><br>  Este √© um banco de dados de valor-chave distribu√≠do.  Do ponto de vista da arquitetura e das id√©ias nele incorporadas, parece-me que tudo √© legal.  Se eu fizesse, faria o mesmo.  Quando come√ßamos, pensamos em escrever nosso pr√≥prio sistema de armazenamento de metadados.  Mas quanto mais longe, mais e mais percebemos que ter√≠amos que fazer algo muito semelhante a Cassandra, e os esfor√ßos que vamos gastar nela n√£o valem a pena.  Durante todo o desenvolvimento <strong>, tivemos apenas um m√™s e meio</strong> .  Seria estranho gast√°-los escrevendo seu banco de dados. <br><br>  Se Cassandra estivesse em camadas como um bolo, eu selecionaria 3 camadas: <br><br>  1. <strong>Armazenamento KV local em cada n√≥.</strong> <br>  Este √© um cluster de n√≥s, cada um dos quais pode armazenar dados de valor-chave localmente. <br><br>  2. <strong>Dados de fragmenta√ß√£o em n√≥s (hash consistente).</strong> <br>  O Cassandra pode distribuir dados entre os n√≥s do cluster, incluindo replica√ß√£o, e o faz de maneira que o cluster possa aumentar ou diminuir de tamanho e os dados ser√£o redistribu√≠dos. <br><br>  3. Um <strong>coordenador para redirecionar solicita√ß√µes para outros n√≥s.</strong> <br>  Quando acessamos dados para algumas consultas de nosso aplicativo, o Cassandra pode distribuir nossa consulta em n√≥s para obter os dados desejados e com o n√≠vel de consist√™ncia de que precisamos - queremos l√™-los apenas no quorum, ou deseja quorum com dois CDs, etc. <br><img src="https://habrastorage.org/webt/zs/tw/jb/zstwjb6bvwlg43rmuphw91_jtrm.jpeg"><br><br>  Para n√≥s, dois anos com Cassandra - √© uma montanha-russa ou uma montanha-russa - o que voc√™ quiser.  Tudo come√ßou no fundo, n√£o t√≠nhamos experi√™ncia com Cassandra.  N√≥s est√°vamos com medo.  Come√ßamos e estava tudo bem.  Mas ent√£o caem e decolam constantemente: o problema, tudo est√° ruim, n√£o sabemos o que fazer, obtemos erros, resolvemos o problema etc. <br><br>  Essas montanhas-russas, em princ√≠pio, n√£o terminam at√© hoje. <br><br><h1>  Bom </h1><br>  O primeiro e o √∫ltimo cap√≠tulo, onde eu digo que Cassandra √© legal.  √â muito legal, um √≥timo sistema, mas se eu continuar dizendo o qu√£o bom √©, acho que voc√™ n√£o estar√° interessado.  Portanto, prestaremos mais aten√ß√£o ao mal, mas mais tarde. <br><br>  Cassandra √© muito boa. <br><br><ul><li>  Este √© um dos sistemas que nos permite ter <strong>um tempo de resposta em milissegundos</strong> , ou seja, obviamente, menos de 10 ms.  Isso √© bom para n√≥s, porque o tempo de resposta em geral √© importante para n√≥s.  A opera√ß√£o com metadados para n√≥s √© apenas parte de qualquer opera√ß√£o relacionada ao armazenamento de um objeto, esteja ele recebendo ou gravando. </li><li>  Do ponto de vista da grava√ß√£o, <strong>√©</strong> alcan√ßada <strong>alta escalabilidade</strong> .  Voc√™ pode escrever no Cassandra a uma velocidade louca e, em algumas situa√ß√µes, isso √© necess√°rio, por exemplo, quando movemos grandes quantidades de dados entre registros. </li><li>  Cassandra √© verdadeiramente <strong>tolerante a falhas</strong> .  A queda de um n√≥ n√£o leva a problemas imediatamente, embora mais cedo ou mais tarde eles comecem.  Cassandra declara que n√£o possui um √∫nico ponto de falha, mas, de fato, h√° pontos de falha em todos os lugares.  De fato, quem trabalhou com o banco de dados sabe que mesmo uma falha no n√≥ n√£o √© algo que geralmente sofre at√© de manh√£.  Geralmente, essa situa√ß√£o precisa ser corrigida mais rapidamente. </li><li>  <strong>Simplicidade.</strong>  Ainda assim, comparado a outros bancos de dados relacionais padr√£o do Cassandra, √© mais f√°cil entender o que est√° acontecendo.  Muitas vezes, algo d√° errado, e precisamos entender o que est√° acontecendo.  Cassandra tem mais chances de descobrir, chegar ao menor parafuso, provavelmente, do que em outro banco de dados. </li></ul><br><h1>  Cinco hist√≥rias ruins </h1><br>  Repito, Cassandra √© boa, funciona para n√≥s, mas vou contar cinco hist√≥rias sobre as ruins.  Eu acho que √© para isso que voc√™ l√™.  Darei as hist√≥rias em ordem cronol√≥gica, embora elas n√£o estejam muito ligadas umas √†s outras. <br><img src="https://habrastorage.org/webt/ao/15/oa/ao15oaiwdhwvl4w4u5pvcgwolrq.jpeg"><br><br>  Essa hist√≥ria foi a mais triste para n√≥s.  Como armazenamos dados do usu√°rio, a pior coisa poss√≠vel √© perd√™-los e <strong>perd√™-los para sempre</strong> , como aconteceu nessa situa√ß√£o.  Fornecemos maneiras de recuperar dados se os perdermos no Cassandra, mas os perdemos para que realmente n√£o pud√©ssemos recuperar. <br><br>  Para explicar como isso acontece, terei que falar um pouco sobre como tudo est√° organizado dentro de n√≥s. <br><img src="https://habrastorage.org/webt/6i/vf/gk/6ivfgkdspndo3kzyy153iveq4xq.jpeg"><br><br>  Da perspectiva do S3, existem algumas coisas b√°sicas: <br><br><ul><li>  Balde - pode ser imaginado como um enorme cat√°logo no qual o usu√°rio carrega um objeto (a seguir denominado balde). </li><li>  Cada objeto possui um nome (chave) e metadados associados: tamanho, tipo de conte√∫do e um ponteiro para os dados do objeto.  Ao mesmo tempo, o tamanho do balde n√£o √© limitado por nada.  Ou seja, pode ter 10 teclas, talvez 100 bilh√µes de teclas - n√£o h√° diferen√ßa. </li><li>  Qualquer opera√ß√£o competitiva √© poss√≠vel, ou seja, pode haver v√°rios preenchimentos competitivos na mesma chave, pode haver exclus√£o competitiva etc. </li></ul><br>  Em nossa situa√ß√£o, ativo-ativo, podem ocorrer opera√ß√µes, inclusive competitivamente em diferentes CDs, n√£o apenas em um.  Portanto, precisamos de algum tipo de esquema de conserva√ß√£o que nos permita implementar essa l√≥gica.  No final, escolhemos uma pol√≠tica simples: a √∫ltima vers√£o gravada vence.  √Äs vezes, v√°rias opera√ß√µes competitivas ocorrem, mas n√£o √© necess√°rio que nossos clientes fa√ßam isso de prop√≥sito.  Pode ser apenas uma solicita√ß√£o iniciada, mas o cliente n√£o esperou por uma resposta, algo mais aconteceu, tentou novamente etc. <br><br>  Portanto, temos duas tabelas base: <br><br><ol><li>  <strong>Tabela de objetos</strong> .  Nele, um par - o nome do bucket e o nome da chave - √© associado √† sua vers√£o atual.  Se o objeto for exclu√≠do, n√£o h√° nada nesta vers√£o.  Se o objeto existir, h√° sua vers√£o atual.  De fato, nesta tabela, apenas alteramos o campo da vers√£o atual. <br></li><li>  <strong>Tabela de vers√£o dos objetos</strong> .  Apenas inserimos novas vers√µes nessa tabela.  Cada vez que um novo objeto √© baixado, inserimos uma nova vers√£o na tabela de vers√µes, fornecemos um n√∫mero √∫nico, salvamos todas as informa√ß√µes sobre ele e, no final, atualizamos o link para ele na tabela de objetos. <br></li></ol><br>  A figura mostra um exemplo de como tabelas de objetos e vers√µes de objetos est√£o relacionadas. <br><img src="https://habrastorage.org/webt/rv/jm/3y/rvjm3y1ohf-9yiehp1ajlm4zjik.jpeg"><br><br>  Aqui est√° um objeto que possui duas vers√µes - uma atual e outra antiga, existe um objeto que j√° foi exclu√≠do e sua vers√£o ainda est√° l√°.  Precisamos limpar vers√µes desnecess√°rias de tempos em tempos, ou seja, excluir algo a que ningu√©m mais se refere.  Al√©m disso, n√£o precisamos exclu√≠-lo imediatamente, podemos faz√™-lo no modo adiado.  Esta √© a nossa limpeza interna, apenas exclu√≠mos o que n√£o √© mais necess√°rio. <br><br>  Houve um problema. <br><img src="https://habrastorage.org/webt/md/rc/xy/mdrcxyc9ojwdjuwchg7gsgkspio.jpeg"><br><br>  O problema era este: n√≥s temos ativo-ativo, dois CDs.  Em cada controlador de dom√≠nio, os metadados s√£o armazenados em tr√™s c√≥pias, ou seja, temos 3 + 3 - apenas 6 r√©plicas.  Quando os clientes nos contactam, realizamos opera√ß√µes com consist√™ncia (do ponto de vista do Cassandra, isso √© chamado LOCAL_QUORUM).  Ou seja, √© garantido que o registro (ou leitura) ocorreu em duas r√©plicas no controlador de dom√≠nio local.  Isso √© uma garantia - caso contr√°rio, a opera√ß√£o falhar√°. <br><br>  Cassandra sempre tentar√° escrever em todas as 6 linhas - 99% das vezes tudo ficar√° bem.  De fato, todas as 6 r√©plicas ser√£o as mesmas, mas garantidas para n√≥s 2. <br><br>  Tivemos uma situa√ß√£o dif√≠cil, embora nem fosse uma regi√£o geogr√°fica.  Mesmo para regi√µes comuns que est√£o em um controlador de dom√≠nio, ainda armazenamos a segunda c√≥pia dos metadados em outro controlador de dom√≠nio.  Esta √© uma longa hist√≥ria, n√£o vou dar todos os detalhes.  Mas, no final, tivemos um processo de limpeza que removeu vers√µes desnecess√°rias. <br><br>  E ent√£o o mesmo problema surgiu.  O processo de limpeza tamb√©m funcionou com a consist√™ncia do quorum local em um data center, porque n√£o h√° sentido em execut√°-lo em dois - eles ir√£o lutar entre si. <br><br>  Tudo estava bem at√© que nossos usu√°rios ainda gravassem em outro data center, o que n√£o suspeit√°vamos.  Tudo foi preparado para o amante dos feylover, mas eles j√° estavam usando. <br><img src="https://habrastorage.org/webt/sa/cs/6q/sacs6qjj7og_ay7jbmkhfkjh9ei.jpeg"><br><br>  Na maioria das vezes, tudo estava bem at√© que um dia surgiu uma situa√ß√£o em que uma entrada na tabela de vers√µes foi replicada nos dois CDs, mas o registro na tabela de objetos acabou em apenas um CD e n√£o terminou no segundo.  Consequentemente, o procedimento de limpeza, iniciado no primeiro CD (superior), viu que havia uma vers√£o √† qual ningu√©m estava se referindo e a excluiu.  E apaguei n√£o apenas a vers√£o, mas tamb√©m, √© claro, os dados - tudo √© completamente, porque √© apenas um objeto desnecess√°rio.  E essa remo√ß√£o √© irrevog√°vel. <br><br>  Obviamente, h√° um "boom" adicional, porque ainda temos um registro na tabela de objetos que se refere a uma vers√£o que n√£o existe mais. <br><br>  Ent√£o, a primeira vez que perdemos dados, e realmente irrevogavelmente - bom, um pouco. <br><br><h3>  Solu√ß√£o </h3><br>  O que fazer  Na nossa situa√ß√£o, tudo √© simples. <br><br>  Como temos dados armazenados em dois data centers, o processo de limpeza √© um processo de converg√™ncia e sincroniza√ß√£o.  N√≥s devemos ler os dados dos dois CDs.  Esse processo funcionar√° apenas quando os dois controladores de dom√≠nio estiverem dispon√≠veis.  Como eu disse que esse √© um processo atrasado que n√£o ocorre durante o processamento da API, isso n√£o √© assustador. <br><br>  <strong>Consist√™ncia ALL</strong> √© um recurso do Cassandra 2. No Cassandra 3, tudo fica um pouco melhor - h√° um n√≠vel de consist√™ncia, chamado quorum em cada CD.  Mas, em qualquer caso, h√° o problema de que √© <strong>lento</strong> , porque, primeiro, precisamos recorrer ao controlador de dom√≠nio remoto.  Em segundo lugar, no caso de consist√™ncia de todos os 6 n√≥s, isso significa que ele funciona na velocidade do pior desses 6 n√≥s. <br><br>  Mas, ao mesmo tempo, ocorre o chamado processo de <strong>reparo e leitura</strong> , quando nem todas as r√©plicas s√£o s√≠ncronas.  Ou seja, quando a grava√ß√£o falhou em algum lugar, esse processo as repara simultaneamente.  √â assim que Cassandra funciona. <br><br>  Quando isso aconteceu, recebemos uma reclama√ß√£o do cliente de que o objeto n√£o estava dispon√≠vel.  N√≥s descobrimos, entendemos o porqu√™ e a primeira coisa que quer√≠amos fazer era descobrir quantos outros objetos possu√≠mos.  Executamos um script que tentou encontrar uma constru√ß√£o semelhante a essa quando havia uma entrada em uma tabela, mas nenhuma entrada em outra. <br><br>  De repente, descobrimos que possu√≠mos <strong>10% desses registros</strong> .  Provavelmente nada pior poderia ter acontecido se n√£o tiv√©ssemos adivinhado que n√£o era esse o caso.  O problema era diferente. <br><br><img src="https://habrastorage.org/webt/kc/jt/_d/kcjt_dh03wmb-6szvtrgxqz-hme.jpeg"><br><br>  Os zumbis invadiram nosso banco de dados.  Este √© o nome semioficial para esse problema.  Para entender o que √©, voc√™ precisa falar sobre como a remo√ß√£o funciona no Cassandra. <br><img src="https://habrastorage.org/webt/k2/sd/2j/k2sd2jvngn9ouhiv6b3yre0s8vs.jpeg"><br><br>  Por exemplo, temos alguns dados <strong><em>x</em></strong> que s√£o registrados e perfeitamente replicados para todas as 6 r√©plicas.  Se quisermos exclu√≠-lo, a remo√ß√£o, como qualquer opera√ß√£o no Cassandra, pode n√£o ser executada em todos os n√≥s. <br><br>  Por exemplo, quer√≠amos garantir a consist√™ncia de 2 em 3 em um CD.  Deixe a opera√ß√£o de exclus√£o ser executada em cinco n√≥s, mas permane√ßa em um registro, por exemplo, porque o n√≥ n√£o estava dispon√≠vel naquele momento. <br><img src="https://habrastorage.org/webt/lu/d5/ot/lud5otv1dguftzwinkrb2wpeaaq.jpeg"><br><br>  Se excluirmos isso e tentarmos ler ‚ÄúQuero 2 de 3‚Äù com a mesma consist√™ncia, Cassandra, vendo o valor e sua aus√™ncia, interpreta isso como a presen√ßa de dados.  Ou seja, ao ler de volta, ela dir√°: ‚ÄúAh, existem dados!‚Äù, Embora os excluamos.  Portanto, voc√™ n√£o pode excluir dessa maneira. <br><img src="https://habrastorage.org/webt/m6/li/ol/m6liolpvsglhmd_9wjg1gvkascw.jpeg"><br><br>  Cassandra remove de forma diferente.  <strong>A exclus√£o √© realmente um registro</strong> .  Quando exclu√≠mos dados, Cassandra escreve um pequeno marcador chamado <strong>L√°pide</strong> (l√°pide).  Marca que os dados foram exclu√≠dos.  Portanto, se lermos o token de exclus√£o e os dados ao mesmo tempo, o Cassandra sempre prefere o token de exclus√£o nessa situa√ß√£o e diz que, na verdade, n√£o h√° dados.  √â disso que voc√™ precisa. <br><br>  <strong>Tombstone ‚Äî   </strong> , , ,      , -     ,     .   Tombstone     .   <strong>Tombstone   gc_grace_period </strong> .   ,   ,   . <br><br>   ? <br><br><h2> Repair <br></h2><br>  Cassandra  ,   Repair ().   ‚Äî  ,     .       ,  ,      ,     , / ,  , -  - ,    ..     . Repair  ,    . <br><img src="https://habrastorage.org/webt/hj/td/x0/hjtdx0thzgak5uhzjd_ejn09s1m.jpeg"><br><br>   , -   , -   .  Repair    ,    ,    .  - ,     ‚Äî     .     ,    . <br><img src="https://habrastorage.org/webt/qe/ee/kj/qeeekjf-dzxokqp6c4zi1aylljm.jpeg"><br><br>     Repair,       ,  ,      ,    ‚Äî ,   .   6     .     ‚Äî ,   ,     . <br><img src="https://habrastorage.org/webt/c7/qo/o2/c7qoo2bykcraic_gbj8fxbxrmoo.jpeg"><br><br>     ,      ‚Äî ,  -  .      ,    .        ,  - ,    ,       ,    . <br><br><h3>  Solu√ß√£o <br></h3><br>   ,   : <br><br><ul><li> <strong>Repair       </strong> . </li></ul><br>    ,      repair.    ,          ,       . <br><br><ul><li> <strong>    ,    Tombstones,   ,   repair.</strong> </li></ul><br>  repair ‚Äî   ,     repair. ,  ,          10-20 , , 3 .    Tombstone     ,     .      ,  ,      -. <br><img src="https://habrastorage.org/webt/18/yp/cc/18ypccovl1xcoxairec6nf3ssx0.jpeg"><br><br>      Cassandra,     .       . <br><br>  S3  .   ,      ‚Äî 10 , 100  .   API,     ‚Äî      .     , ,  , ,   ,         .  ,    ,  ,    ‚Äî     ,    .      . <br><br>    API? <br><img src="https://habrastorage.org/webt/1l/tl/hd/1ltlhdhdtgnwgxezzz8jsnavvky.jpeg"><br><br>   ,     ‚Äî , ,   ‚Äî    ,    ,    .    .              ‚Äî .   ,     ,   .   ,   ,      Cassandra.    ,         ‚Äî  ,  ,    ,      . <br><br>        ,          ,      ,  ,  .          ,      . ,   ,             . ,   - ,           . <br><br> Cassandra ,       .           ,       ,  ,   ,       ,     . <br><img src="https://habrastorage.org/webt/fk/os/a3/fkosa3zozy2gk_dzpgjvxdwm4k8.jpeg"><br><br>    ,   Cassandra  <strong>composite key</strong> .       ,    ‚Äî    ,   - ,      ‚Äî  .    ,   .   ? ,   ,    ! <br><br>      ,    ,  , ,      ‚Äî  ,          . <br><br>     .  Cassandra   ,   <strong>  Cassandra      </strong> .  ,     ,    Cassandra,        :  ,  ,   SQL  ..    ! <br><img src="https://habrastorage.org/webt/8o/_s/ka/8o_ska-swgmzixxiztlblopuze0.jpeg"><br><br>      .     Cassandra  ?    ,     ,   API.  ,   ,     ,   ,     (     )   . <strong>   ,   </strong>   . <br><br>    ,           .        ,   , ,    .   ,     ‚Äî   ‚Äî       . , ,  ,          . <br><br>   Cassandra   ,       .    : ¬´  100 ¬ª,    ,    ,  ,      ,        ,   100,    . <br><br> ,         (   ),    ‚Äî          ,    ,         .         ,   ,   ,   ,     ,   - .     100 ,   - ,     ,  .      ,         SQL    . <br><br> Cassandra       ,     ,     Java,    .  ,  <strong>Large Partition</strong> ,  .    ‚Äî , , ,  ,     ‚Äî  .         ,   , garbage collection    ..     . <br><br>   ,   ,  <strong>    ,   </strong> ,        . <br><br> ,        ,   -  . <br><img src="https://habrastorage.org/webt/qg/o9/oo/qgo9ooa3pgv_zqkv8iyby4ppq9g.jpeg"><br><br>   ,     ,           .      .     ,      Large Partition. <br><br>     : <br><br><ol><li>        ( ,  - ); <br></li><li>   ,    ,       .     ,     . <br></li></ol><br>   ,     ,   ,     key_hash   0.   , <strong>    ,         </strong> .       ,    .       ,      ,      . <br><br>  ,     . <br><img src="https://habrastorage.org/webt/zr/aw/xn/zrawxn-n6hr1huoqkenbqcgpeoo.jpeg"><br><br>    ‚Äî ,    ,    ,      - -      . <br><br>   ‚Äî      ,   N ?    ,  Large Partition,   ‚Äî     .  ,        .   :   .  ,    ,  ,    ,       -  .    ,           .    , ,     . <br><img src="https://habrastorage.org/webt/og/um/4y/ogum4yxqpvbvrdadna7r8adomwm.jpeg"><br><br>     ‚Äî   ,    ,   -  .    -  ,       ,       .    ,    ,    .   ,    ,        .. <br><br>         ‚Äî  ,    ?    ,   .    ? -     md5- ‚Äî      ,   -  30  ‚Äî     ,  - .    .     ,     ,   . <br><img src="https://habrastorage.org/webt/yp/ik/vy/ypikvyolprsxdju5hawmlm6_epq.jpeg"><br><br>      ,    , , ,   .       ‚Äî   ,    .    ,       .   ,    -  -   - ,  -  - ‚Äî  .     ,     .      . <br><br><h2>   </h2><br>    ,    ,     ,    . <br><br><ul><li>   . </li><li>         . </li><li>     Cassandra. </li><li> Online- (     ). </li></ul><br>  Agora temos algum estado do balde, de alguma forma √© dividido em parti√ß√µes.  Ent√£o entendemos que algumas parti√ß√µes s√£o muito grandes ou muito pequenas.  Precisamos encontrar uma nova parti√ß√£o, que, por um lado, seja √≥tima, ou seja, o tamanho de cada parti√ß√£o ser√° menor que alguns de nossos limites e eles ser√£o mais ou menos uniformes.  Nesse caso, a transi√ß√£o do estado atual para um novo deve exigir um n√∫mero m√≠nimo de a√ß√µes.  √â claro que qualquer transi√ß√£o requer mover chaves entre parti√ß√µes, mas quanto menos as movermos, melhor. <br><br>  N√≥s conseguimos.  Provavelmente, a parte que lida com a sele√ß√£o da distribui√ß√£o √© a parte mais dif√≠cil de todo o servi√ßo, se falarmos sobre trabalhar com metadados em geral.  Reescrevemos, reformulamos e ainda fazemos, porque sempre s√£o encontrados alguns clientes ou certos padr√µes de cria√ß√£o de chaves que atingem um ponto fraco desse esquema. <br><br>  Por exemplo, presumimos que o balde crescesse mais ou menos uniformemente.  Ou seja, escolhemos algum tipo de distribui√ß√£o e esper√°vamos que todas as parti√ß√µes crescessem de acordo com essa distribui√ß√£o.  Mas encontramos um cliente que sempre escreve no final, no sentido de que suas chaves est√£o sempre na ordem de classifica√ß√£o.  O tempo todo ele bate na √∫ltima parti√ß√£o, que cresce a uma velocidade que em um minuto pode chegar a 100 mil chaves.  E 100 mil √© aproximadamente o valor que cabe em uma parti√ß√£o. <br><br>  Simplesmente n√£o ter√≠amos tempo para processar essa adi√ß√£o de chaves com nosso algoritmo e tivemos que introduzir uma distribui√ß√£o preliminar especial para esse cliente.  Como sabemos como s√£o as chaves dele, se vemos que √© ele, come√ßamos a criar parti√ß√µes vazias com anteced√™ncia no final, para que ele possa escrever com calma l√°, e por enquanto ter√≠amos um pouco de descanso at√© a pr√≥xima itera√ß√£o, quando precisarmos novamente redistribua tudo. <br><br>  Tudo isso acontece online, no sentido de que n√£o paramos a opera√ß√£o.  Pode haver opera√ß√µes de leitura e grava√ß√£o, a qualquer momento voc√™ pode solicitar uma lista de chaves.  Sempre ser√° consistente, mesmo se estivermos no processo de reparticionamento. <br><br>  √â bem interessante, e acontece com Cassandra.  Aqui voc√™ pode jogar com truques relacionados ao fato de Cassandra ser capaz de resolver conflitos.  Se escrevermos dois valores diferentes na mesma linha, o valor que possui um carimbo de data / hora maior vence. <br><br>  Normalmente, o carimbo de data / hora √© o carimbo de data / hora atual, mas pode ser passado manualmente.  Por exemplo, queremos escrever um valor em uma string, que em qualquer caso deve ser esfregada se o cliente escrever algo.  Ou seja, estamos copiando alguns dados, mas queremos que o cliente, se de repente ele escreve conosco ao mesmo tempo, possa substitu√≠-lo.  Em seguida, podemos copiar nossos dados com um carimbo de data e hora um pouco do passado.  Qualquer grava√ß√£o atual ser√° deliberadamente desgastada, independentemente da ordem em que a grava√ß√£o foi feita. <br><br>  Esses truques permitem fazer isso online. <br><br><h2>  Solu√ß√£o </h2><br><ul><li>  Nunca, nunca <strong>permita a apar√™ncia de uma parti√ß√£o grande</strong> . </li><li>  <strong>Divida os dados por chave prim√°ria,</strong> dependendo da tarefa. </li></ul><br>  Se algo semelhante a uma parti√ß√£o grande estiver planejado no esquema de dados, tente imediatamente fazer algo a respeito - descubra como quebr√°-lo e como fugir dele.  Mais cedo ou mais tarde, isso surge, porque qualquer √≠ndice invertido surge mais cedo ou mais tarde em quase qualquer tarefa.  Eu j√° contei sobre essa hist√≥ria - temos uma chave de balde no objeto e precisamos obter uma lista de chaves do balde - na verdade, este √© um √≠ndice. <br><br>  Al√©m disso, a parti√ß√£o pode ser grande n√£o apenas a partir dos dados, mas tamb√©m a partir de Tombstones (marcadores de exclus√£o).  Do ponto de vista dos componentes internos do Cassandra (nunca os vemos de fora), os marcadores de exclus√£o tamb√©m s√£o dados e uma parti√ß√£o pode ser grande se muitas coisas forem exclu√≠das, porque a exclus√£o √© um registro.  Voc√™ n√£o deve esquecer isso tamb√©m. <br><img src="https://habrastorage.org/webt/-s/tw/la/-stwlarb11mcy5nlqaqrpxfc-ky.jpeg"><br><br>  Outra hist√≥ria que √© realmente constante √© que algo d√° errado do come√ßo ao fim.  Por exemplo, voc√™ v√™ que o tempo de resposta do Cassandra aumentou, ele responde lentamente.  Como entender e entender qual √© o problema?  Nunca h√° um sinal externo de que o problema est√° l√°. <br><img src="https://habrastorage.org/webt/c0/lr/5r/c0lr5rwf9w5zi-nx1ddd5k5blgk.jpeg"><br><br>  Por exemplo, darei um gr√°fico - este √© o tempo m√©dio de resposta do cluster como um todo.  Isso mostra que temos um problema - o tempo m√°ximo de resposta √© de 12 segundos - esse √© o tempo limite interno do Cassandra.  Isso significa que ela se esgotar√°.  Se o tempo limite for superior a 12 s, isso provavelmente significa que o coletor de lixo est√° funcionando e Cassandra nem tem tempo para responder no momento certo.  Ela responde a si mesma com tempo limite, mas o tempo de resposta para a maioria das solicita√ß√µes, como eu disse, deve ser em m√©dia de 10 ms. <br><br>  No gr√°fico, a m√©dia j√° excedeu centenas de milissegundos - algo deu errado.  Mas, olhando para esta foto, √© imposs√≠vel entender qual √© o motivo. <br><br><img src="https://habrastorage.org/webt/e6/t6/qk/e6t6qkz3yw6k80sjw7smsycclrc.jpeg"><br><br>  Mas se voc√™ expandir as mesmas estat√≠sticas nos n√≥s do Cassandra, poder√° ver que, em princ√≠pio, todos os n√≥s s√£o mais ou menos nada, mas o tempo de resposta para um n√≥ difere por ordens de magnitude.  Provavelmente, h√° algum tipo de problema com ele. <br><br>  As estat√≠sticas nos n√≥s alteram completamente a imagem.  Essas estat√≠sticas s√£o do lado do aplicativo.  Mas aqui √© realmente muito dif√≠cil entender qual √© o problema.  Quando um aplicativo acessa o Cassandra, ele acessa algum n√≥, usando-o como coordenador.  Ou seja, o aplicativo faz uma solicita√ß√£o e o coordenador o redireciona para as r√©plicas com os dados.  Eles j√° respondem, e o coordenador devolve a resposta final. <br><br>  Mas por que o coordenador responde devagar?  Talvez o problema esteja com ele, como tal, ou seja, ele diminui a velocidade e responde lentamente?  Ou talvez ele diminua a velocidade, porque as r√©plicas respondem lentamente a ele?  Se as r√©plicas responderem lentamente, do ponto de vista do aplicativo, parecer√° uma resposta lenta do coordenador, embora n√£o tenha nada a ver com isso. <br><br>  Aqui est√° uma situa√ß√£o feliz - √© claro que apenas um n√≥ responde lentamente, e provavelmente o problema est√° nele. <br><br><h3>  Complexidade de interpreta√ß√£o </h3><br><br><ul><li>  Tempo de resposta do coordenador (n√≥ x r√©plica). </li><li>  Uma tabela espec√≠fica ou o n√≥ inteiro? </li><li>  GC Pausar?  Pool de threads inadequado? </li><li>  Muitas SSTables descompactadas? </li></ul><br>  √â sempre dif√≠cil entender o que est√° errado.  Ele s√≥ <strong>precisa de muitas estat√≠sticas e monitoramento</strong> , tanto do lado do aplicativo quanto do pr√≥prio Cassandra, porque, se estiver muito ruim, nada √© vis√≠vel no Cassandra.  Voc√™ pode observar o n√≠vel de consultas individuais, no n√≠vel de cada tabela espec√≠fica, em cada n√≥ espec√≠fico. <br><br>  Pode haver, por exemplo, uma situa√ß√£o em que uma tabela do que √© chamado no Cassandra SSTables (arquivos separados) tem muito.  Para ler, Cassandra tem, grosso modo, que classificar todas as SSTables.  Se houver muitos deles, simplesmente o processo dessa classifica√ß√£o leva muito tempo e a leitura come√ßa a ceder. <br><br>  A solu√ß√£o √© compacta√ß√£o, o que reduz o n√∫mero dessas SSTables, mas deve-se observar que ele pode estar apenas em um n√≥ para uma tabela espec√≠fica.  Como o Cassandra, infelizmente, √© escrito em Java e executado na JVM, talvez o coletor de lixo tenha entrado em uma pausa t√£o grande que simplesmente n√£o tem tempo para responder.  Quando o coletor de lixo entra em pausa, n√£o apenas suas solicita√ß√µes diminuem, mas a <strong>intera√ß√£o no cluster Cassandra entre os n√≥s come√ßa a diminuir</strong> .  Os n√≥s um do outro come√ßam a ser considerados como tendo ca√≠do, isto √©, ca√≠do, morto. <br><br>  Uma situa√ß√£o ainda mais divertida come√ßa, porque quando um n√≥ acredita que outro n√≥ est√° inativo, primeiro n√£o envia solicita√ß√µes e, em segundo lugar, come√ßa a tentar salvar os dados que precisaria replicar para outro n√≥ em localmente, ent√£o ele come√ßa a se matar lentamente, etc. <br><br>  H√° situa√ß√µes em que esse problema pode ser resolvido simplesmente usando as configura√ß√µes corretas.  Por exemplo, pode haver recursos suficientes, tudo est√° bem e maravilhoso, mas apenas um Pool de Threads, cujo n√∫mero √© de tamanho fixo, precisa ser aumentado. <br><br>  Por fim, talvez seja necess√°rio limitar a competitividade do lado do motorista.  √Äs vezes, acontece que muitas solicita√ß√µes competitivas foram enviadas e, como qualquer banco de dados, Cassandra n√£o consegue lidar com elas e vai ao clinch quando o tempo de resposta aumenta exponencialmente, e estamos tentando dar cada vez mais trabalho. <br><br><h3>  Compreens√£o do contexto </h3><br>  Sempre h√° algum contexto para o problema - o que est√° acontecendo no cluster, se o Reparo est√° funcionando agora, em qual n√≥, em quais espa√ßos-chave, em que tabela. <br><br>  Por exemplo, tivemos problemas bastante rid√≠culos com o ferro.  Vimos que parte dos n√≥s √© lenta.  Mais tarde, foi descoberto que o motivo era que, no BIOS, seus processadores estavam no modo de economia de energia.  Por alguma raz√£o, durante a instala√ß√£o inicial do ferro, isso aconteceu e aproximadamente 50% dos recursos do processador foram usados ‚Äã‚Äãem compara√ß√£o com outros n√≥s. <br><br>  Compreender esse problema pode ser dif√≠cil, de fato.  O sintoma √© este - parece que o n√≥ faz compacta√ß√£o, mas faz isso lentamente.  √Äs vezes, √© conectado ao ferro, √†s vezes n√£o, mas esse √© apenas mais um bug do Cassandra. <br><br>  Portanto, o monitoramento √© obrigat√≥rio e precisa de muito.  Quanto mais complexo o recurso no Cassandra, mais distante fica da simples escrita e leitura, mais problemas existem com ele e mais r√°pido ele pode matar um banco de dados com um n√∫mero suficiente de consultas.  Portanto, se poss√≠vel, n√£o olhe para alguns chips "saborosos" e tente us√°-los, √© melhor evit√°-los o m√°ximo poss√≠vel.  Nem sempre √© poss√≠vel - √© claro, mais cedo ou mais tarde √© necess√°rio. <br><img src="https://habrastorage.org/webt/mx/m8/lx/mxm8lxirhudrxrdlcq26jpstvle.jpeg"><br><br>  A hist√≥ria mais recente √© sobre como Cassandra estragou os dados.  Nesta situa√ß√£o, aconteceu dentro de Cassandra.  Isso foi interessante. <br><br>  Vimos que, uma vez por semana, em nosso banco de dados, v√°rias dezenas de linhas danificadas aparecem - elas est√£o literalmente entupidas de lixo.  Al√©m disso, Cassandra valida os dados que v√£o para sua entrada.  Por exemplo, se for uma string, deve estar em utf8.  Mas nessas linhas havia lixo, e n√£o lixo, e Cassandra nem deu nada a ver com isso.  Quando tento excluir (ou fazer outra coisa), n√£o consigo excluir um valor que n√£o seja utf8, porque, em particular, n√£o posso inseri-lo em WHERE, porque a chave deve ser utf8. <br><br>  Linhas estragadas aparecem, como um flash, em algum momento e depois desaparecem por v√°rios dias ou semanas. <br><br>  Come√ßamos a procurar um problema.  Pensamos que talvez houvesse um problema em um n√≥ espec√≠fico com o qual est√°vamos mexendo, fazendo algo com dados, copiando SSTables.  Talvez, mesmo assim, voc√™ possa ver r√©plicas desses dados?  Talvez essas r√©plicas tenham um n√≥ comum, o menor fator comum?  Talvez alguns n√≥s falhem?  N√£o, nada disso. <br><br>  Talvez algo com um disco?  Os dados est√£o corrompidos no disco?  N√£o de novo. <br><br>  Talvez uma lembran√ßa?  N√£o!  Espalhados por um cluster. <br><br>  Talvez este seja algum tipo de problema de replica√ß√£o?  Um n√≥ estragou tudo e replicou ainda um valor ruim?  - n√£o. <br><br>  Finalmente, talvez este seja um problema de aplicativo? <br><br>  Al√©m disso, em algum momento, as linhas danificadas come√ßaram a aparecer em dois grupos de Cassandra.  Um trabalhou na vers√£o 2.1, o segundo no terceiro.  Parece que Cassandra √© diferente, mas o problema √© o mesmo.  Talvez o nosso servi√ßo envie dados incorretos?  Mas era dif√≠cil de acreditar.  Cassandra valida os dados de entrada; n√£o foi poss√≠vel gravar lixo.  Mas de repente? <br><br>  Nada se encaixa. <br><br><h3>  Uma agulha foi encontrada! </h3><br>  Lutamos muito at√© descobrir um pequeno problema: por que temos algum tipo de despejo de mem√≥ria da JVM nos n√≥s aos quais n√£o prestamos muita aten√ß√£o?  E, de alguma forma, parece suspeito no coletor de lixo de rastreamento de pilha ... E, por alguma raz√£o, algum rastreamento de pilha tamb√©m est√° entupido com lixo. <br><br>  No final, percebemos - oh, <strong>por algum motivo, estamos usando a JVM da vers√£o antiga de 2015</strong> .  Essa era a √∫nica coisa comum que unia clusters do Cassandra em diferentes vers√µes do Cassandra. <br><br>  Ainda n√£o sei qual era o problema, porque nada foi escrito sobre isso nas notas de vers√£o oficiais da JVM.  Mas ap√≥s a atualiza√ß√£o, tudo desapareceu, o problema n√£o surgiu mais.  Al√©m disso, isso n√£o ocorreu no cluster desde o primeiro dia, mas a partir de algum ponto, embora tenha funcionado na mesma JVM por um longo tempo. <br><br><h3>  Recupera√ß√£o de dados </h3><br>  Que li√ß√£o aprendemos disso: <br><br>  ‚óè O backup √© in√∫til. <br>  Os dados, como descobrimos, foram corrompidos no segundo em que foram gravados.  No momento em que os dados entraram no coordenador, eles j√° estavam corrompidos. <br><br>  ‚óè A restaura√ß√£o parcial de colunas n√£o danificadas √© poss√≠vel. <br>  Algumas colunas n√£o foram danificadas, pudemos ler esses dados, restaur√°-los parcialmente. <br><br>  ‚óè No final, tivemos que fazer a recupera√ß√£o de v√°rias fontes. <br>  T√≠nhamos metadados de backup no objeto, mas nos pr√≥prios dados.  Para se reconectar com o objeto, usamos logs etc. <br><br>  ‚óè Os logs n√£o t√™m pre√ßo! <br>  Conseguimos recuperar todos os dados que foram corrompidos, mas, no final, √© muito dif√≠cil confiar no banco de dados se ele os perder, mesmo sem nenhuma a√ß√£o da sua parte. <br><br><h3>  Solu√ß√£o </h3><br><ul><li>  Atualize a JVM ap√≥s testes extensivos. </li><li>  Monitoramento de falhas da JVM. </li><li>  Tenha uma c√≥pia independente dos dados do Cassandra. </li></ul><br><blockquote>  <strong>Dica:</strong> Tente ter algum tipo de c√≥pia independente de Cassandra dos dados dos quais voc√™ pode recuperar, se necess√°rio.  Esta pode ser a solu√ß√£o de √∫ltimo n√≠vel.  Deixe levar muito tempo, recursos, mas deve haver alguma op√ß√£o que permita retornar dados. </blockquote><br><h1>  Bugs </h1><br>  ‚óè <strong>Baixa qualidade de teste de libera√ß√£o</strong> <br>  Quando voc√™ come√ßa a trabalhar com o Cassandra, h√° uma sensa√ß√£o constante (especialmente se voc√™ estiver movendo, relativamente falando, de bancos de dados "bons", por exemplo, PostgreSQL) de que se voc√™ corrigisse um bug na vers√£o anterior, voc√™ definitivamente adicionaria um novo.  E o bug n√£o √© um absurdo, geralmente √© dados corrompidos ou outro comportamento incorreto. <br><br>  ‚óè <strong>Problemas persistentes com recursos complexos</strong> <br>  Quanto mais complexo o recurso, mais problemas, erros etc. com ele. <br><br>  ‚óè <strong>N√£o use reparo incremental no 2.1</strong> <br>  O famoso reparo, sobre o qual falei, que corrige a consist√™ncia dos dados, no modo padr√£o, quando ele pesquisa todos os n√≥s, funciona bem.  Mas n√£o no chamado modo incremental (quando o reparo ignora dados que n√£o foram alterados desde o reparo anterior, o que √© bastante l√≥gico).  Foi anunciado h√° muito tempo, formalmente, como um recurso, mas todos dizem: ‚ÄúN√£o, na vers√£o 2.1, nunca use-o!  Ele definitivamente sentir√° falta de algo.  Em 3 n√≥s consertamos. ‚Äù <br><br>  ‚óè <strong>Mas n√£o use reparo incremental no 3.x</strong> <br>  Quando a terceira vers√£o saiu, alguns dias depois eles disseram: ‚ÄúN√£o, voc√™ n√£o pode us√°-la na 3¬™.  H√° uma lista de 15 erros, portanto, em nenhum caso, n√£o use reparo incremental.  Em quarto lugar faremos melhor! ‚Äù <br><br>  Eu n√£o acredito neles.  E esse √© um grande problema, especialmente com o aumento do tamanho do cluster.  Portanto, voc√™ precisa monitorar constantemente o rastreador de erros e ver o que acontece.  Infelizmente, √© imposs√≠vel viver com eles sem ele. <br><br>  ‚óè <strong>Precisa acompanhar o JIRA</strong> <br><img src="https://habrastorage.org/webt/g0/1k/el/g01kela-ibcrrsorr1pjxo-pmdc.jpeg"><br><br><blockquote>  Se voc√™ espalhar todos os bancos de dados no espectro de previsibilidade, Cassandra estar√° √† esquerda na √°rea vermelha.  Isso n√£o significa que seja ruim, voc√™ apenas precisa estar preparado para o fato de que Cassandra √© imprevis√≠vel em qualquer sentido da palavra: tanto no modo como funciona como no fato de que algo pode acontecer. </blockquote><br><img src="https://habrastorage.org/webt/je/_1/w0/je_1w0808rlhzxo1bakk0zjj9ee.jpeg"><br><br>  Desejo que voc√™ encontre outros ancinhos e pise neles, porque, do meu ponto de vista, n√£o importa o qu√™, Cassandra √© boa e, certamente, n√£o √© chata.  Basta lembrar os solavancos na estrada! <br><br><blockquote>  <strong>Reuni√£o aberta de ativistas do HighLoad ++</strong> <br><br>  No dia 31 de julho em Moscou, √†s 19:00, ser√° <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">realizada</a> uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">reuni√£o de</a> palestrantes, o Comit√™ de Programa e ativistas da confer√™ncia de desenvolvedores de sistemas de alta carga HighLoad ++ 2018. Organizaremos um pequeno brainstorming sobre o programa deste ano para n√£o perder nada novo e importante.  A reuni√£o est√° aberta, mas voc√™ precisa se <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">registrar</a> . <br><br>  <strong>Chamada de trabalhos</strong> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aceitando</a> ativamente <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">solicita√ß√µes</a> de relat√≥rios no Highload ++ 2018. O Comit√™ de Programa aguarda seu resumo at√© o final do ver√£o. <br></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt417617/">https://habr.com/ru/post/pt417617/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt417605/index.html">No√ß√µes b√°sicas de modelagem 3D para impress√£o 3D</a></li>
<li><a href="../pt417607/index.html">Os testes A / B n√£o funcionam. Verifique o que voc√™ est√° fazendo de errado</a></li>
<li><a href="../pt417609/index.html">Especializa√ß√£o em programa√ß√£o esportiva no cursor</a></li>
<li><a href="../pt417613/index.html">Ceph como armazenamento conect√°vel: 5 informa√ß√µes pr√°ticas de um grande projeto</a></li>
<li><a href="../pt417615/index.html">Confiss√µes de Cracker de Disco para Apple II: Segredos das 4 da manh√£</a></li>
<li><a href="../pt417621/index.html">O que aconteceu quando quebramos a exibi√ß√£o?</a></li>
<li><a href="../pt417627/index.html">Hyper CRM ou Mini ERP? Neg√≥cios bagun√ßados</a></li>
<li><a href="../pt417629/index.html">Delphi e C ++ Builder Community Edition</a></li>
<li><a href="../pt417631/index.html">Tutorial em v√≠deo da grade CSS</a></li>
<li><a href="../pt417637/index.html">Desenvolvimento de um editor para cria√ß√£o de sites / desembarques (experi√™ncia)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>