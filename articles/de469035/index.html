<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëÆ üå∂Ô∏è üë∫ Die neuen AI-basierten Innovationen von Azure Media Services üñêüèΩ ‚ô®Ô∏è üêâ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Unsere Mission bei Microsoft ist es, jeden Menschen und jede Organisation auf dem Planeten zu bef√§higen, mehr zu erreichen. Die Medienbranche veransch...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Die neuen AI-basierten Innovationen von Azure Media Services</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/microsoft/blog/469035/">  Unsere Mission bei Microsoft ist es, jeden Menschen und jede Organisation auf dem Planeten zu bef√§higen, mehr zu erreichen.  Die Medienbranche veranschaulicht diese Mission.  Wir leben in einer Zeit, in der mehr Inhalte auf mehr Arten und auf mehr Ger√§ten als je zuvor erstellt und konsumiert werden.  Auf der IBC 2019 haben wir uns sehr gefreut, Ihnen die neuesten Innovationen vorstellen zu k√∂nnen, an denen wir gearbeitet haben, und wie sie dazu beitragen k√∂nnen, Ihre Medienworkflows zu ver√§ndern.  Lesen Sie weiter, um mehr zu erfahren. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/01f/1ab/900/01f1ab900cce076468f55a62bc103b2f.png"></a> <a name="habracut"></a><br>  Dieser Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">in unserem Blog</a> . <br><br><h2>  Video Indexer bietet Unterst√ºtzung f√ºr Animationen und mehrsprachige Inhalte </h2><br>  Wir haben unseren preisgekr√∂nten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Azure Media Services-Videoindexer</a> letztes Jahr allgemein auf der IBC verf√ºgbar gemacht, und dieses Jahr wird er noch besser.  Der Video Indexer extrahiert automatisch Erkenntnisse und Metadaten wie gesprochene W√∂rter, Gesichter, Emotionen, Themen und Marken aus Mediendateien, ohne dass Sie ein Experte f√ºr maschinelles Lernen sein m√ºssen.  Unsere neuesten Ank√ºndigungen enthalten eine Vorschau auf zwei sehr gefragte und differenzierte Funktionen zur animierten Zeichenerkennung und mehrsprachigen Sprachtranskription sowie mehrere Erg√§nzungen zu bestehenden Modellen, die heute in Video Indexer verf√ºgbar sind. <br><br><h2>  Animierte Zeichenerkennung </h2><br>  Animierte Inhalte oder Cartoons sind einer der beliebtesten Inhaltstypen, aber Standard-AI-Vision-Modelle f√ºr menschliche Gesichter funktionieren nicht gut mit ihnen, insbesondere wenn der Inhalt Zeichen ohne menschliche Merkmale enth√§lt.  In dieser neuen Vorschau-L√∂sung arbeitet Video Indexer mit dem Azure Custom Vision-Dienst von Microsoft zusammen, um eine neue Reihe von Modellen bereitzustellen, die animierte Zeichen automatisch erkennen und gruppieren und es Kunden erm√∂glichen, sie √ºber integrierte benutzerdefinierte Vision-Modelle einfach zu kennzeichnen und zu erkennen.  Diese Modelle sind in eine einzige Pipeline integriert, sodass jeder den Dienst ohne vorherige maschinelle Lernf√§higkeiten nutzen kann.  Die Ergebnisse sind √ºber das No-Code-Video-Indexer-Portal oder die REST-API f√ºr die einfache Integration in Ihre eigenen Anwendungen verf√ºgbar. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/01f/1ab/900/01f1ab900cce076468f55a62bc103b2f.png"></a> <br>  Wir haben diese animierten Charaktermodelle in Zusammenarbeit mit ausgew√§hlten Kunden erstellt, die echte animierte Inhalte f√ºr Schulungen und Tests beigesteuert haben.  Der Wert der neuen Funktionalit√§t wird von Andy Gutteridge, Senior Director, Studio- und Postproduktionstechnologie bei Viacom International Media Networks, gut ausgedr√ºckt. Er war einer der Datenverantwortlichen: "Die Hinzuf√ºgung einer zuverl√§ssigen AI-basierten animierten Erkennung wird es uns erm√∂glichen Entdecken und katalogisieren Sie schnell und effizient Zeichenmetadaten aus unserer Inhaltsbibliothek.  Am wichtigsten ist, dass unsere Kreativteams die M√∂glichkeit haben, die gew√ºnschten Inhalte sofort zu finden, den Zeitaufwand f√ºr das Medienmanagement zu minimieren und sich auf das Creative zu konzentrieren. ‚Äú <br><br>  Um mit der Erkennung animierter Zeichen zu beginnen, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">besuchen</a> Sie bitte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">unsere Dokumentationsseite</a> . <br><br><h2>  Mehrsprachige Identifizierung und Transkription </h2><br>  Einige Medienelemente wie Nachrichten, aktuelle Angelegenheiten und Interviews enthalten Audio mit Sprechern in verschiedenen Sprachen.  F√ºr die meisten vorhandenen Sprach-Text-Funktionen muss die Audioerkennungssprache im Voraus festgelegt werden. Dies ist ein Hindernis f√ºr die Transkription mehrsprachiger Videos.  Unsere neue automatische Identifizierung gesprochener Sprachen f√ºr mehrere Inhalte nutzt die Technologie des maschinellen Lernens, um die verschiedenen Sprachen zu identifizieren, die in einem Medienobjekt verwendet werden.  Sobald jedes Sprachsegment erkannt wurde, wird es in der identifizierten Sprache automatisch transkribiert, und alle Segmente werden wieder zusammen in eine Transkriptionsdatei integriert, die aus mehreren Sprachen besteht. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/593/b6c/37c/593b6c37c928baa329de15eb80f0a8af.png"></a> <br><br>  Die resultierende Transkription ist sowohl als Teil der Video Indexer JSON-Ausgabe als auch als Untertiteldateien verf√ºgbar.  Das Ausgabetranskript ist auch in Azure Search integriert, sodass Sie sofort in Videos nach den verschiedenen Sprachsegmenten suchen k√∂nnen.  Dar√ºber hinaus ist die mehrsprachige Transkription als Teil des Video Indexer-Portals verf√ºgbar, sodass Sie das Transkript und die identifizierte Sprache nach Zeit anzeigen oder zu den spezifischen Stellen im Video f√ºr jede Sprache springen und die mehrsprachige Transkription als Untertitel anzeigen k√∂nnen wie ein Video abgespielt wird.  Sie k√∂nnen die Ausgabe auch √ºber das Portal und die API in 54 verschiedene Sprachen √ºbersetzen. <br><br>  Weitere Informationen zur neuen mehrsprachigen Option und deren Verwendung in Video Indexer finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">in unserer Dokumentation</a> . <br><br>
<h2>  Zus√§tzliche aktualisierte und verbesserte Modelle </h2><br>  Wir f√ºgen au√üerdem neue und verbesserte Modelle in Video Indexer hinzu, darunter: <br><br><h3>  Extraktion von Personen und Standorten </h3><br>  Wir haben unsere aktuellen Funktionen zur Markenerkennung erweitert, um auch bekannte Namen und Standorte wie den Eiffelturm in Paris oder den Big Ben in London einzubeziehen.  Wenn diese im generierten Transkript oder auf dem Bildschirm √ºber die optische Zeichenerkennung (OCR) erscheinen, wird eine spezifische Einsicht erstellt.  Mit dieser neuen Funktion k√∂nnen Sie alle Personen, Standorte und Marken, die im Video angezeigt werden, sowie deren Zeitrahmen, Beschreibung und einen Link zu unserer Bing-Suchmaschine √ºberpr√ºfen und nach weiteren Informationen suchen. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/750/c43/023/750c43023844290c35eefa61f2867890.png"></a> <br><br><h3>  Editorial Shot Detection Modell </h3><br>  Diese neue Funktion f√ºgt eine Reihe von "Tags" in die Metadaten ein, die einer einzelnen Aufnahme in Insights JSON zugeordnet sind, um deren redaktionellen Typ darzustellen (z. B. Weitwinkelaufnahme, mittlere Aufnahme, Nahaufnahme, extreme Nahaufnahme, zwei Aufnahmen, mehrere Personen, Au√üenaufnahmen) und drinnen usw.).  Diese Eigenschaften des Aufnahmetyps sind n√ºtzlich, wenn Sie Videos in Clips und Trailern bearbeiten oder nach einem bestimmten Aufnahmestil f√ºr k√ºnstlerische Zwecke suchen. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/2e8/634/2b4/2e86342b4eb18285316689503e59ac63.png"></a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Weitere</a> Informationen zur Erkennung redaktioneller Aufnahmetypen finden Sie in Video Indexer. <br><br><h3>  Erweiterte Granularit√§t der IPTC-Zuordnung </h3><br>  Unser Themeninferenzmodell bestimmt das Thema von Videos basierend auf Transkription, optischer Zeichenerkennung (OCR) und erkannten Prominenten, auch wenn das Thema nicht explizit angegeben ist.  Wir ordnen diese abgeleiteten Themen vier verschiedenen Taxonomien zu: Wikipedia, Bing, IPTC und IAB.  Mit dieser Verbesserung schlie√üen wir jetzt die IPTC-Taxonomie der Stufe 2 ein. <br><br>  Der Tankvorteil dieser Verbesserungen ist so einfach wie das erneute Indizieren Ihrer aktuellen Video Indexer-Bibliothek. <br><br><h2>  Neue Live-Streaming-Funktionalit√§t </h2><br>  Au√üerdem f√ºhren wir in der Vorschau von Azure Media Services zwei neue Live-Streaming-Funktionen ein. <br><br><h3>  Live-Transkription l√§dt Ihre Live-Events mit AI auf </h3><br>  Wenn Sie Azure Media Services zum Streamen eines Live-Ereignisses verwenden, k√∂nnen Sie jetzt einen Ausgabestream abrufen, der zus√§tzlich zu den Video- und Audioinhalten eine automatisch generierte Textspur enth√§lt.  Diese Textspur wird mithilfe einer AI-basierten Live-Transkription des Audios des Beitrags-Feeds erstellt.  Vor und nach der Konvertierung von Sprache in Text werden benutzerdefinierte Methoden angewendet, um die Endbenutzererfahrung zu verbessern.  Die Textspur wird in IMSC1, TTML oder WebVTT gepackt, je nachdem, ob Sie in DASH, HLS CMAF oder HLS TS liefern. <br><br><h3>  Lineare Live-Codierung f√ºr OTT-Kan√§le (24/7 Over-the-Top) </h3><br>  Mit unseren v3-APIs k√∂nnen Sie Live-Kan√§le f√ºr OTT-Dienste erstellen, verwalten und streamen und alle anderen Funktionen von Azure Media Services wie Live-to-Video-on-Demand (VOD), Verpackung und Digital Rights Management (DRM) nutzen. <br><br>  Besuchen Sie die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Azure Media Services-Community-</a> Seite, um diese Vorschaufunktionen auszuprobieren. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/f8d/974/04d/f8d97404d2d98fde2f7f687fafd6e506.png"></a> <br><br><h2>  Neue Verpackungsmerkmale </h2><br><h3>  Unterst√ºtzung f√ºr Audiobeschreibungsspuren </h3><br>  Broadcast-Inhalte enthalten h√§ufig eine Audiospur, die zus√§tzlich zum normalen Programm-Audio verbale Erkl√§rungen zu Bildschirmaktionen enth√§lt.  Dies macht das Programmieren f√ºr sehbehinderte Zuschauer zug√§nglicher, insbesondere wenn der Inhalt sehr visuell ist.  Mit der neuen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Audiobeschreibungsfunktion</a> kann ein Kunde eine der Audiospuren als Audiobeschreibungsspur (AD) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kennzeichnen</a> , die wiederum von Spielern verwendet werden kann, um die AD-Spur f√ºr die Zuschauer erkennbar zu machen. <br><br><h3>  Einf√ºgen von ID3-Metadaten </h3><br>  Um das Einf√ºgen von Werbung oder benutzerdefinierten Metadatenereignissen auf einem Client-Player zu signalisieren, verwenden Rundfunkveranstalter h√§ufig zeitgesteuerte Metadaten, die in das Video eingebettet sind.  Zus√§tzlich zu den SCTE-35-Signalisierungsmodi unterst√ºtzen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">wir jetzt auch ID3v2 oder andere benutzerdefinierte Schemas,</a> die von einem Anwendungsentwickler zur Verwendung durch die Clientanwendung definiert wurden. <br><br><h2>  Microsoft Azure-Partner demonstrieren End-to-End-L√∂sungen </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bitmovin</a> stellt seine Bitmovin- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Videokodierung</a> und seinen Bitmovin-Videoplayer unter Microsoft Azure vor.  Kunden k√∂nnen diese Codierungs- und Player-L√∂sungen jetzt in Azure verwenden und erweiterte Funktionen wie 3-Pass-Codierung, AV1 / VVC-Codec-Unterst√ºtzung, mehrsprachige Untertitel und vorintegrierte Videoanalyse f√ºr QoS, Anzeige und Video-Tracking nutzen. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Evergent</a> zeigt seine User Lifecycle Management Platform in Azure.  Als f√ºhrender Anbieter von Managementl√∂sungen f√ºr Umsatz und Kundenlebenszyklus nutzt Evergent Azure AI, um Premium-Unterhaltungsdienstleistern die Verbesserung der Kundenakquise und -bindung zu erm√∂glichen, indem gezielte Pakete und Angebote an kritischen Punkten im Kundenlebenszyklus generiert werden. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Haivision</a> wird seinen intelligenten Cloud-Service f√ºr das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Medienrouting</a> , SRT Hub, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vorstellen</a> , mit dem Kunden End-to-End-Workflows beginnend mit der Aufnahme mithilfe von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Azure Data Box Edge</a> und der Transformation von Medienworkflows mithilfe von Hublets von Avid, Telestream, Wowza und Cinegy sowie Make.tv transformieren k√∂nnen . <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SES</a> hat f√ºr seine Kunden mit Satellitenkonnektivit√§t und verwalteten Mediendiensten eine Reihe von Mediendiensten in Broadcast-Qualit√§t auf Azure entwickelt.  SES zeigt L√∂sungen f√ºr vollst√§ndig verwaltete Playout-Dienste, einschlie√ülich Master-Playout, lokalisiertes Playout sowie Erkennung und Ersetzung von Anzeigen und hochwertige Mehrkanal-Live-Codierung rund um die Uhr in Azure. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SyncWords</a> stellt seine Technologie zur Untertitelautomatisierung und benutzerfreundlichen Cloud-basierten Tools in Azure zur Verf√ºgung.  Diese Angebote erleichtern Medienunternehmen das Hinzuf√ºgen automatisierter Untertitel- und Untertitelungsfunktionen f√ºr Fremdsprachen zu ihren Echtzeit- und Offline-Videoverarbeitungs-Workflows in Azure. <br><br>  Das globale Design- und Technologiedienstleistungsunternehmen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tata Elxsi</a> hat TEPlay, seine OTT-Plattform SaaS, in Azure Media Services integriert, um OTT-Inhalte aus der Cloud bereitzustellen.  Tata Elxsi hat auch FalconEye, seine QoE-√úberwachungsl√∂sung (Quality of Experience), die sich auf umsetzbare Metriken und Analysen konzentriert, zu Microsoft Azure gebracht. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verizon Media</a> stellt seine Streaming-Plattform in der Beta-Version von Azure zur Verf√ºgung.  Verizon Media Platform ist eine verwaltete OTT-L√∂sung f√ºr Unternehmen, die DRM, Anzeigeneinf√ºgung, personalisierte Eins-zu-Eins-Sitzungen, dynamischen Inhaltsaustausch und Videobereitstellung umfasst.  Die Integration bietet vereinfachte Workflows, globale Unterst√ºtzung und Skalierbarkeit sowie Zugriff auf eine Reihe einzigartiger Funktionen, die in Azure verf√ºgbar sind. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de469035/">https://habr.com/ru/post/de469035/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de469023/index.html">So finden Sie einen Job mit Umzug nach Europa: ein praktischer Leitfaden f√ºr IT-Experten</a></li>
<li><a href="../de469025/index.html">K√ºhlen Sie den Wein schnell ab! Russische Erfindung</a></li>
<li><a href="../de469027/index.html">Ivanovo! Mitap: Wie baue ich eine Karriere in Digital auf?</a></li>
<li><a href="../de469031/index.html">12 neue k√ºnstliche Intelligenz f√ºr Azure Media Services</a></li>
<li><a href="../de469033/index.html">Starten der Elbrus-Plattform f√ºr neuronale PuzzleLib-Netzwerke</a></li>
<li><a href="../de469037/index.html">Industrielle Steuerung. Datenerfassungssystem. ACS</a></li>
<li><a href="../de469039/index.html">Mehr als ein Spiel: Mahjong mit KI und maschinellem Lernen meistern</a></li>
<li><a href="../de469041/index.html">Wie sch√ºtzen Sie Ihr ERP-System?</a></li>
<li><a href="../de469043/index.html">C / C ++ von Python (C API)</a></li>
<li><a href="../de469045/index.html">Python in Visual Studio Code - Ver√∂ffentlichung im September 2019</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>