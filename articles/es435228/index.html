<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèº‚ÄçüöÄ üçº üë©‚Äç‚úàÔ∏è Kubernetes cluster por $ 20 por mes üçà üíáüèæ üåë</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="TL DR 


 Elevamos el cl√∫ster para el servicio de aplicaciones web sin estado con ingreso , encriptaci√≥n , sin usar herramientas de automatizaci√≥n com...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Kubernetes cluster por $ 20 por mes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/iponweb/blog/435228/"><h1 id="tl-dr">  TL  DR </h1><br><p>  Elevamos el cl√∫ster para el servicio de aplicaciones web <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sin estado</a> con <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ingreso</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">encriptaci√≥n</a> , sin usar herramientas de automatizaci√≥n como kubespray, kubeadm y cualquier otra. <br>  Tiempo de lectura: ~ 45-60 minutos, tiempo de reproducci√≥n: desde 3 horas. </p><br><h1 id="preambula">  Pre√°mbulo </h1><br><p>  Me pidieron que escribiera un art√≠culo por la necesidad de mi propio grupo de kubernetes para la experimentaci√≥n.  Las soluciones de instalaci√≥n y configuraci√≥n automatizadas de c√≥digo abierto no funcionaron en mi caso, ya que utilic√© distribuciones de Linux no convencionales.  El trabajo intensivo con kubernetes en IPONWEB lo alienta a tener dicha plataforma, resolviendo sus tareas de una manera c√≥moda, incluso para proyectos en el hogar. </p><br><h1 id="komponenty">  Componentes </h1><br><p>  Los siguientes componentes aparecer√°n en el art√≠culo: </p><br><p>  - <em>Su</em> Linux <em>favorito</em> - Us√© Gentoo (nodo-1: systemd / node-2: openrc), Ubuntu 18.04.1. <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Servidor Kubernetes</a> : kube-apiserver, kube-controller-manager, kube-Scheduler, kubelet, kube-proxy. <br>  - <a href="">Complementos</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Containerd</a> + <a href="">CNI (0.7.4)</a> : para la contenedorizaci√≥n, tomaremos containerd + CNI en lugar de Docker (aunque inicialmente toda la configuraci√≥n se carg√≥ en Docker, por lo que nada evitar√° que se use si es necesario). <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CoreDNS</a> : para organizar el descubrimiento del servicio de componentes que trabajan dentro del cl√∫ster kubernetes.  Se recomienda una versi√≥n no inferior a 1.2.5, ya que con esta versi√≥n existe una buena compatibilidad para que los n√∫cleos funcionen como un proceso que se ejecuta fuera del cl√∫ster. <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Franela</a> : para organizar una pila de red, comunicar hogares y contenedores entre ellos. <br>  - <em>Tu</em> db <em>favorito</em> . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/423/a71/5fc/423a715fc68fed1101c86d3335b0a8a8.jpg" alt="Para todos"></p><a name="habracut"></a><br><h1 id="ogranicheniya-i-dopuscheniya">  Limitaciones y Suposiciones </h1><br><ul><li>  El art√≠culo no examina el costo de las soluciones vps / vds en el mercado, as√≠ como la posibilidad de implementar m√°quinas en estos servicios.  Se supone que ya tiene algo expandido, o puede hacerlo usted mismo.  Adem√°s, la instalaci√≥n / configuraci√≥n de su base de datos favorita y el repositorio privado de Docker, si lo necesita, no est√°n cubiertos. </li><li>  Podemos usar los complementos containerd + cni y docker.  Este art√≠culo no considera el uso de Docker como herramienta de contenedorizaci√≥n.  Si desea usar docker, usted mismo podr√° configurar la <a href="">franela en consecuencia</a> , adem√°s deber√° configurar kubelet, es decir, eliminar todas las opciones relacionadas con el contenedor.  Como mostraron mis experimentos, docker y containerd en diferentes nodos como contenedores funcionar√°n correctamente. </li><li> No podemos usar <code>host-gw</code> backend para franela, lea la secci√≥n <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Configuraci√≥n de franela</a> para m√°s detalles </li><li>  No usaremos nada para monitorear, hacer copias de seguridad, guardar archivos de usuario (estado), almacenar archivos de configuraci√≥n y c√≥digo de aplicaci√≥n (git / hg / svn / etc.) </li></ul><br><h1 id="vvedenie">  Introduccion </h1><br><p>  En el curso del trabajo, utilic√© una gran cantidad de fuentes, pero quiero mencionar por separado una gu√≠a bastante detallada de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Kubernetes</a> , que cubre aproximadamente el 90% de la configuraci√≥n b√°sica de su propio cl√∫ster.  Si ya ha le√≠do este manual, puede pasar directamente a la secci√≥n <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Configuraci√≥n de franela</a> . </p><br><div class="spoiler">  <b class="spoiler_title">Designaciones</b> <div class="spoiler_text"><h2 id="spisok-terminov--glossariy">  Lista de T√©rminos / Glosario </h2><br><ul><li>  api-server: una m√°quina f√≠sica o virtual en la que se encuentra un conjunto de aplicaciones para ejecutar y el correcto funcionamiento de kubernetes kube-apiserver.  Para los prop√≥sitos de este art√≠culo, es etcd, kube-apiserver, kube-controller-manager, kube-Scheduler. </li><li>  master: una estaci√≥n de trabajo dedicada o instalaci√≥n de VPS, sin√≥nimo de api-server. </li><li>  nodo-X: una estaci√≥n de trabajo dedicada o instalaci√≥n de VPS, <code>X</code> indica el n√∫mero de serie de la estaci√≥n.  En este art√≠culo, todos los n√∫meros son √∫nicos y son clave para comprender: <br><ul><li>  nodo-1 - m√°quina n√∫mero 1 </li><li>  nodo-2 - m√°quina n√∫mero 2 </li></ul></li><li>  vCPU: CPU virtual, n√∫cleo del procesador.  El n√∫mero corresponde al n√∫mero de n√∫cleos: 1vCPU - un n√∫cleo, 2vCPU - dos, y as√≠ sucesivamente. </li><li>  usuario: usuario o espacio de usuario.  Cuando se usan <code>user$</code> instrucciones de la l√≠nea de comando <code>user$</code> , el t√©rmino se refiere a cualquier m√°quina cliente. </li><li>  trabajador: el nodo de trabajo en el que se realizar√°n los c√°lculos directos, como sin√≥nimo de <code>node-X</code> </li><li>  recurso es la entidad en la que opera el cl√∫ster de Kubernetes.  Los recursos de Kubernetes incluyen una gran cantidad de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">entidades relacionadas</a> . </li></ul></div></div><br><h1 id="setevaya-arhitektura-resheniya">  Soluciones de arquitectura de red </h1><br><p>  En el proceso de aumentar el cl√∫ster, no establec√≠ la tarea de optimizar los recursos de hierro de tal manera que se ajustaran al presupuesto de $ 20 por mes.  Solo era necesario ensamblar un cl√∫ster de trabajo con al menos dos nodos de trabajo (nodos).  Por lo tanto, inicialmente el cl√∫ster se ve√≠a as√≠: </p><br><ul><li>  m√°quina con 2 vCPU / 4G RAM: api-server + node-1 [20 $] </li><li>  m√°quina con 2 vCPU / 4G RAM: nodo-2 [$ 20] </li></ul><br><p>  Despu√©s de que funcion√≥ la primera versi√≥n del cl√∫ster, decid√≠ reconstruirlo para distinguir entre los nodos responsables de ejecutar aplicaciones dentro del cl√∫ster (nodos de trabajo, tambi√©n son trabajadores) y la API del servidor maestro. </p><br><p>  Como resultado, obtuve la respuesta a la pregunta: "C√≥mo obtener un cl√∫ster m√°s o menos econ√≥mico, pero que funcione, si no quiero colocar las aplicaciones m√°s gruesas all√≠". </p><br><div class="spoiler">  <b class="spoiler_title">Decisi√≥n de $ 20</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/getpro/habr/post_images/216/89c/a88/21689ca889156d11108e5f5327c606cc.png" alt="Dise√±o"><br>  (Planificado para ser as√≠) </p></div></div><br><div class="spoiler">  <b class="spoiler_title">Kubernetes Architecture Informaci√≥n general</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/getpro/habr/post_images/820/462/882/820462882e7cc92479190c067ac4a4f8.jpg" alt="Dise√±o"><br>  (Robado de Internet si alguien de repente todav√≠a no sabe o no ha visto) </p></div></div><br><h2 id="komponenty-i-ih-proizvoditelnost">  Componentes y su desempe√±o </h2><br><p>  El primer paso fue comprender cu√°ntos recursos necesito para ejecutar paquetes de software que est√°n directamente relacionados con el cl√∫ster.  La b√∫squeda de "requisitos de hardware" no dio resultados espec√≠ficos, por lo que tuve que abordar la tarea desde un punto de vista pr√°ctico.  Como una medici√≥n de MEM y CPU, tom√© estad√≠sticas de systemd; podemos suponer que las mediciones se llevaron a cabo de una manera muy aficionada, pero no tuve la tarea de obtener valores precisos, ya que todav√≠a no pod√≠a encontrar opciones m√°s baratas que $ 5 por instancia. </p><br><div class="spoiler">  <b class="spoiler_title">¬øPor qu√© exactamente $ 5?</b> <div class="spoiler_text"><p>  Era posible encontrar VPS / VDS m√°s barato al alojar servidores en Rusia o en la CEI, pero las tristes historias asociadas con ILV y sus acciones crean ciertos riesgos y dan lugar a un deseo natural de evitarlos. </p></div></div><br><p>  Entonces </p><br><ul><li>  Servidor maestro / Configuraci√≥n del servidor (Nodos maestros): <br><ul><li>  etcd (3.2.17): 80 - 100M, las m√©tricas se tomaron en un momento seleccionado al azar.  El consumo de memoria promedio de Etcd no super√≥ los 300M; </li><li>  kube-apiserver (1.12.x - 1.13.0): 237.6M ~ 300M; </li><li>  kube-controller-manager (1.12.x - 1.13.0): aproximadamente 90M, no se elev√≥ por encima de 100M; </li><li>  Kube-Scheduler (1.12.x - 1.13.0): aproximadamente 20M, el consumo por encima de 30-50M no es fijo. </li></ul></li><li>  Configuraci√≥n del servidor de trabajo (Nodos de trabajo): <br><ul><li>  kubelet (1.12.3 - 1.13.1): aproximadamente 35 Mb, el consumo por encima de 50M no es fijo; </li><li>  kube-proxy (1.12.3 - 1.13.1): aproximadamente 7.5 - 10M; </li><li>  franela (0.10.0): aproximadamente 15-20M; </li><li>  n√∫cleos (1.3.0): aproximadamente 25M; </li><li>  containerd (1.2.1): el consumo de containerd es bajo, pero las estad√≠sticas tambi√©n muestran los procesos de contenedor iniciados por el demonio. </li></ul></li></ul><br><div class="spoiler">  <b class="spoiler_title">¬øSe necesita containerd / docker en los nodos maestros?</b> <div class="spoiler_text"><p>  <strong>No, no es necesario</strong>  El nodo maestro no requiere acoplador o contenedor en s√≠, aunque hay una gran cantidad de manuales en Internet que, para alg√∫n prop√≥sito u otro, incluyen el uso del entorno para la contenedorizaci√≥n.  En la configuraci√≥n en cuesti√≥n, containerd se apag√≥ intencionalmente de la lista de dependencias, sin embargo, no destaco ninguna ventaja obvia de este enfoque. </p><br><p>  La configuraci√≥n proporcionada anteriormente es m√≠nima y suficiente para iniciar el cl√∫ster.  No se requiere ninguna acci√≥n / componente adicional, a menos que desee agregar algo como desee. </p></div></div><br><p>  Para construir un cl√∫ster de prueba o un cl√∫ster para proyectos dom√©sticos, 1vCPU / 1G RAM ser√° suficiente para que funcione el nodo maestro.  Por supuesto, la carga en el nodo maestro variar√° seg√∫n la cantidad de trabajadores involucrados, as√≠ como la disponibilidad y el volumen de solicitudes de terceros al servidor de API. </p><br><p>  Explot√© las configuraciones maestra y de trabajo de la siguiente manera: </p><br><ul><li>  1x Maestro con componentes instalados: etcd, kube-apiserver, kube-controller-manager, kube-Scheduler </li><li>  2x trabajadores con componentes instalados: containerd, coredns, franela, kubelet, kube-proxy </li></ul><br><h1 id="konfiguraciya">  Configuracion </h1><br><p>  Para configurar el asistente, se requieren los siguientes componentes: </p><br><ul><li><p>  etcd: para almacenar datos para api-server, as√≠ como para franela; </p><br></li><li><p>  kube-apiserver - en realidad, api-server; </p><br></li><li><p>  kube-controller-manager - para generar y procesar eventos; </p><br></li><li><p>  kube-Scheduler: para la distribuci√≥n de recursos registrados a trav√©s de api-server, por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">hearth</a> . <br>  Para la configuraci√≥n de caballos de batalla, se requieren los siguientes componentes: </p><br></li><li><p>  kubelet: para ejecutar los hogares, para configurar los ajustes de red; </p><br></li><li><p>  kube-proxy: para organizar el enrutamiento / equilibrio de los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">servicios</a> de kubernetes; </p><br></li><li><p>  n√∫cleos: para el descubrimiento de servicios dentro de contenedores en ejecuci√≥n; </p><br></li><li><p>  franela: para organizar el acceso a la red de contenedores que operan en diferentes nodos, as√≠ como para la distribuci√≥n din√°mica de redes entre nodos de cl√∫ster (nodo kubernetes). </p><br></li></ul><br><div class="spoiler">  <b class="spoiler_title">Coredns</b> <div class="spoiler_text"><p>  Aqu√≠ se debe hacer una peque√±a digresi√≥n: los n√∫cleos tambi√©n se pueden iniciar en el servidor maestro.  No hay restricciones que obliguen a los coredns a ejecutarse en los nodos de trabajo, excepto el matiz de configuraci√≥n coredns.service, que simplemente no se inicia en un servidor Ubuntu est√°ndar / no modificado debido a un conflicto con el servicio resuelto systemd.  No intent√© resolver este problema, ya que los servidores 2 ns ubicados en los nodos de trabajo estaban muy contentos conmigo. </p></div></div><br><p>  Para no perder tiempo ahora familiariz√°ndose con todos los detalles del proceso de configuraci√≥n de componentes, le sugiero que se familiarice con ellos en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">gu√≠a de Kubernetes</a> .  Me centrar√© en las caracter√≠sticas distintivas de mi opci√≥n de configuraci√≥n. </p><br><h2 id="fayly">  Archivos </h2><br><p>  Todos los archivos para el funcionamiento de los componentes del cl√∫ster para el asistente y los nodos de trabajo se colocan en <strong>/ var / lib / kubernetes /</strong> por conveniencia.  Si es necesario, puede colocarlos de otra manera. </p><br><h2 id="sertifikaty">  Certificaciones </h2><br><p>  La base para la generaci√≥n de certificados sigue siendo la misma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Kubernetes de la manera dif√≠cil</a> , pr√°cticamente no hay diferencias significativas.  Para regenerar certificados subordinados, se escribieron scripts de bash simples alrededor de aplicaciones <a href="">cfssl</a> ; esto fue muy √∫til en el proceso de depuraci√≥n. </p><br><p>  Puede generar certificados para sus necesidades utilizando los siguientes scripts, recetas de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Kubernetes de la manera m√°s dif√≠cil</a> u otras herramientas adecuadas. </p><br><div class="spoiler">  <b class="spoiler_title">Generaci√≥n de certificados utilizando scripts bash</b> <div class="spoiler_text"><p>  Puede <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">obtener</a> scripts aqu√≠: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">kubernetes bootstrap</a> .  Antes de comenzar, edite el archivo <a href="">certs / env.sh</a> , especificando su configuraci√≥n.  Un ejemplo: </p><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> certs <span class="hljs-comment"><span class="hljs-comment">#:   certs$ ./generate-keys.sh # ... certificate generate output #:  kubeconfig     certs$ ./generate-configkube.sh</span></span></code> </pre> <br><p>  Si us√≥ <code>env.sh</code> y especific√≥ correctamente todos los par√°metros, entonces no es necesario tocar los certificados generados.  Si cometi√≥ un error en alg√∫n momento, los certificados pueden regenerarse en partes.  Los scripts de bash anteriores son triviales, ordenarlos no es dif√≠cil. </p><br><p>  Una nota importante: a menudo no debe volver a crear los <code>ca.pem</code> y <code>ca-key.pem</code> , ya que son los certificados ra√≠z para todos los certificados posteriores, en otras palabras, tendr√° que recrear todos los certificados adjuntos y entregarlos a todas las m√°quinas y a todos los directorios necesarios. </p></div></div><br><h3 id="master">  El maestro </h3><br><p>  Los certificados necesarios para iniciar servicios en el nodo maestro deben colocarse en <code>/var/lib/kubernetes/</code> : </p><br><ul><li>  ca.pem: este certificado se usa en todas partes, solo se puede generar una vez y luego usarse sin cambios, as√≠ que tenga cuidado.  Cuando lo regenere, deber√° copiarlo a todos los nodos, as√≠ como actualizar los archivos kubeconfig que lo usan (tambi√©n en todas las m√°quinas). </li><li>  ca-key.pem es lo mismo que copiar sobre nodos. </li><li>  kube-controller-manager.pem: solo se necesita para kube-controller-manager. </li><li>  kube-controller-manager-key.pem: solo se necesita para kube-controller-manager. </li><li><p>  kubernetes.pem: se requiere para franela, n√∫cleos al conectarse a etcd, kube-apiserver. </p><br><div class="spoiler">  <b class="spoiler_title">Retiro te√≥rico</b> <div class="spoiler_text"><p>  Esta caracter√≠stica se basa en la l√≥gica de configuraci√≥n de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Kubernetes la v√≠a dif√≠cil</a> . <br>  En base a esto, este archivo ser√° necesario en todas partes, tanto en el asistente como en los nodos de trabajo.  No cambi√© el enfoque provisto por el manual original, ya que con su ayuda es posible organizar la operaci√≥n del cl√∫ster de manera m√°s r√°pida y clara y comprender el conjunto de dependencias. </p><br><p>  Mi opini√≥n personal es que para etcd necesita certificados separados que no se superpongan con los certificados utilizados para kubernetes. </p><br></div></div><br></li></ul><br><ul><li>  kubernetes-key.pem: permanece en servidores maestros. </li><li>  service-account.pem: solo se necesita para los demonios kube-controller-manager. </li><li>  service-account-key.pem - de manera similar. </li></ul><br><h3 id="rabochie-uzly">  Unidades de trabajo </h3><br><ul><li>  ca.pem: necesario para todos los servicios involucrados en los nodos de trabajo (kubelet, kube-proxy), as√≠ como para franela, n√∫cleos.  Entre otras cosas, su contenido se incluye en los archivos kubeconfig cuando se generan usando kubectl. </li><li>  kubernetes-key.pem: solo se necesita para que la franela y los n√∫cleos se conecten a etcd, que se encuentra en el nodo maestro api. </li><li>  kubernetes.pem: similar al anterior, necesario solo para franela y coredns. </li><li>  kubelet / node-1.pem - clave para la autorizaci√≥n nodo-1. </li><li>  kubelet / node-1-key.pem - clave para la autorizaci√≥n nodo-1. </li></ul><br><p>  <strong>Importante!</strong>  Si tiene m√°s de un nodo, cada nodo incluir√° los archivos <code>node-X-key.pem</code> , <code>node-X.pem</code> y <code>node-X.kubeconfig</code> dentro de kubelet. </p><br><div class="spoiler">  <b class="spoiler_title">Depuraci√≥n de certificados</b> <div class="spoiler_text"><h4 id="otladka-sertifikatov">  Depuraci√≥n de certificados </h4><br><p>  A veces puede que necesite ver c√≥mo est√° configurado el certificado para averiguar qu√© hosts IP / DNS se usaron para generarlo.  El <code>cfssl-certinfo -cert &lt;cert&gt;</code> nos ayudar√° con esto.  Por ejemplo, aprendemos esta informaci√≥n para <code>node-1.pem</code> : </p><br><pre> <code class="bash hljs">$ cfssl-certinfo -cert node-1.pem</code> </pre> <br><pre> <code class="json hljs">{ <span class="hljs-attr"><span class="hljs-attr">"subject"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"common_name"</span></span>: <span class="hljs-string"><span class="hljs-string">"system:node:node-1"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"country"</span></span>: <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organization"</span></span>: <span class="hljs-string"><span class="hljs-string">"system:nodes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organizational_unit"</span></span>: <span class="hljs-string"><span class="hljs-string">"Infrastructure Unit"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"locality"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"province"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"names"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"system:nodes"</span></span>, <span class="hljs-string"><span class="hljs-string">"Infrastructure Unit"</span></span>, <span class="hljs-string"><span class="hljs-string">"system:node:node-1"</span></span> ] }, <span class="hljs-attr"><span class="hljs-attr">"issuer"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"common_name"</span></span>: <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"country"</span></span>: <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organization"</span></span>: <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organizational_unit"</span></span>: <span class="hljs-string"><span class="hljs-string">"Infrastructure"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"locality"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"province"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"names"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-string"><span class="hljs-string">"Infrastructure"</span></span>, <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span> ] }, <span class="hljs-attr"><span class="hljs-attr">"serial_number"</span></span>: <span class="hljs-string"><span class="hljs-string">"161113741562559533299282037709313751074033027073"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"sans"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"w40k.net"</span></span>, <span class="hljs-string"><span class="hljs-string">"node-1"</span></span>, <span class="hljs-string"><span class="hljs-string">"178.79.168.130"</span></span>, <span class="hljs-string"><span class="hljs-string">"192.168.164.230"</span></span> ], <span class="hljs-attr"><span class="hljs-attr">"not_before"</span></span>: <span class="hljs-string"><span class="hljs-string">"2019-01-04T14:24:00Z"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"not_after"</span></span>: <span class="hljs-string"><span class="hljs-string">"2029-01-01T14:24:00Z"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"sigalg"</span></span>: <span class="hljs-string"><span class="hljs-string">"SHA256WithRSA"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"authority_key_id"</span></span>: <span class="hljs-string"><span class="hljs-string">"6:C8:94:67:59:55:19:82:AD:ED:6D:50:F1:89:B:8D:46:78:FD:9A"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"subject_key_id"</span></span>: <span class="hljs-string"><span class="hljs-string">"A1:5E:B3:3C:45:14:3D:C6:C:A:97:82:1:D5:2B:75:1A:A6:9D:B0"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"pem"</span></span>: <span class="hljs-string"><span class="hljs-string">"&lt;pem content&gt;"</span></span> }</code> </pre> </div></div><br><p>  Todos los dem√°s certificados para kubelet y kube-proxy est√°n integrados directamente en el kubeconfig correspondiente. </p><br><h2 id="kubeconfig">  kubeconfig </h2><br><p>  Todo el kubeconfig necesario se puede hacer usando <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Kubernetes de la manera dif√≠cil</a> , sin embargo, aqu√≠ comienzan algunas diferencias.  El manual utiliza configuraciones de <code>cni bridge</code> <code>kubedns</code> y <code>cni bridge</code> , tambi√©n cubre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">coredns</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">franela</a> .  Estos dos servicios, a su vez, usan <code>kubeconfig</code> para <code>kubeconfig</code> en el cl√∫ster. </p><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> certs <span class="hljs-comment"><span class="hljs-comment">#:  kubeconfig     certs$ ./generate-configkube.sh</span></span></code> </pre> <br><h3 id="master-1">  El maestro </h3><br><p>  Para el asistente, se necesitan los siguientes archivos kubeconfig (como se mencion√≥ anteriormente, despu√©s de la generaci√≥n se pueden tomar en <code>certs/kubeconfig</code> ): </p><br><pre> <code class="plaintext hljs">master /var/lib/kubernetes/$ tree -L 2 . +-- kube-controller-manager.kubeconfig L-- kube-scheduler  L-- kube-scheduler.kubeconfig</code> </pre> <br><p>  Estos archivos ser√°n necesarios para ejecutar cada uno de los componentes del servicio. </p><br><h3 id="rabochie-uzly-1">  Unidades de trabajo </h3><br><p>  Para los nodos de trabajo, se requieren los siguientes archivos kubeconfig: </p><br><pre> <code class="plaintext hljs">node-1 /var/lib/kubernetes/$ tree -L 2 . +-- coredns ¬¶  L-- coredns.kubeconfig +-- flanneld ¬¶  L-- flanneld.kubeconfig +-- kubelet ¬¶  L-- node-1.kubeconfig L-- kube-proxy  L-- kube-proxy.kubeconfig</code> </pre> <br><h2 id="zapusk-servisov">  Lanzamiento de servicio </h2><br><div class="spoiler">  <b class="spoiler_title">Servicios</b> <div class="spoiler_text"><p>  A pesar del hecho de que mis nodos de trabajo usan diferentes sistemas de inicializaci√≥n, los ejemplos y el repositorio dan opciones usando systemd.  Con su ayuda, es m√°s f√°cil comprender qu√© proceso y con qu√© par√°metros debe comenzar, adem√°s, no deber√≠an causar grandes problemas al estudiar servicios con banderas de destino. </p></div></div><br><p>  Para iniciar los servicios, debe copiar <code>service-name.service</code> a <code>/lib/systemd/system/</code> o cualquier otro directorio donde se encuentren los servicios para systemd, y luego encender e iniciar el servicio.  Ejemplo para kube-apiserver: </p><br><pre> <code class="bash hljs">$ systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> kube-apiserver.service $ systemctl start kube-apiserver.service</code> </pre> <br><p>  Por supuesto, todos los servicios deben ser <em>ecol√≥gicos</em> (es decir, en ejecuci√≥n y en funcionamiento).  Si encuentra un error, los <code>journalct -xe</code> o <code>journal -f -t kube-apiserver</code> lo ayudar√°n a comprender exactamente qu√© sali√≥ mal. </p><br><p>  No se apresure a iniciar todos los servidores a la vez, para empezar ser√° suficiente para habilitar etcd y kube-apiserver.  Si todo sali√≥ bien, e inmediatamente obtuvo los cuatro servicios en el asistente, el lanzamiento del asistente se puede considerar exitoso. </p><br><h3 id="master-2">  El maestro </h3><br><p>  Puede usar la configuraci√≥n <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">systemd</a> o generar scripts de inicio para la configuraci√≥n que est√° utilizando.  Como ya se mencion√≥, para el maestro que necesita: </p><br><p>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">systemd / etcd</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">systemd / kube-apiserver</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">systemd / kube-controller-manager</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">systemd / kube-Scheduler</a> </p><br><h3 id="rabochie-uzly-2">  Unidades de trabajo </h3><br><p>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">systemd / containerd</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">systemd / kubelet</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">systemd / kube-proxy</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">systemd / coredns</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">systemd / franela</a> </p><br><h3 id="klient">  Cliente </h3><br><p>  Para que el cliente funcione, solo copie <code>certs/kubeconfig/admin.kubeconfig</code> (despu√©s de generarlo o escribirlo usted mismo) en <code>${HOME}/.kube/config</code> </p><br><p>  Descargue <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">kubectl</a> y verifique el funcionamiento de kube-apiserver.  Perm√≠tanme recordarles una vez m√°s que en esta etapa, para que kube-apiserver funcione, solo deber√≠a funcionar etcd.  Los componentes restantes ser√°n necesarios para la operaci√≥n completa del cl√∫ster un poco m√°s tarde. </p><br><p>  Compruebe que kube-apiserver y kubectl funcionan: </p><br><pre> <code class="bash hljs">$ kubectl version Client Version: version.Info{Major:<span class="hljs-string"><span class="hljs-string">"1"</span></span>, Minor:<span class="hljs-string"><span class="hljs-string">"13"</span></span>, GitVersion:<span class="hljs-string"><span class="hljs-string">"v1.13.0"</span></span>, <span class="hljs-string"><span class="hljs-string">"extra info"</span></span>: <span class="hljs-string"><span class="hljs-string">"..."</span></span>} Server Version: version.Info{Major:<span class="hljs-string"><span class="hljs-string">"1"</span></span>, Minor:<span class="hljs-string"><span class="hljs-string">"13"</span></span>, GitVersion:<span class="hljs-string"><span class="hljs-string">"v1.13.0"</span></span>, <span class="hljs-string"><span class="hljs-string">"extra info"</span></span>: <span class="hljs-string"><span class="hljs-string">"..."</span></span>}</code> </pre> <br><h1 id="konfiguraciya-flannel">  Configuraci√≥n de franela </h1><br><p>  Como configuraci√≥n de franela, me decid√≠ por el backend <code>vxlan</code> .  Lea m√°s sobre backends <a href="">aqu√≠</a> . </p><br><div class="spoiler">  <b class="spoiler_title">host-gw y por qu√© no funcionar√°</b> <div class="spoiler_text"><p>  Debo decir de inmediato que ejecutar un cl√∫ster de kubernetes en un VPS probablemente lo limitar√° a usar el servidor <code>host-gw</code> .  Al no ser un ingeniero de redes experimentado, pas√© unos dos d√≠as depurando para comprender cu√°l era el problema con su uso en proveedores populares de VDS / VPS. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Linode.com</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">digitalocean</a> han sido probados.  La esencia del problema es que los proveedores no proporcionan L2 honesto para una red privada.  Esto, a su vez, hace que sea imposible mover el tr√°fico de red entre nodos en esta configuraci√≥n: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/e0c/c7e/add/e0cc7eadd6253cf4921df17ca6fe5d47.png" alt="Tr√°fico"></p><br><p>  Para que el tr√°fico de red funcione entre los nodos, ser√° suficiente el enrutamiento normal.  No olvide que net.ipv4.ip_forward debe establecerse en 1, y la cadena FORWARD en la tabla de filtros no debe contener reglas de prohibici√≥n para los nodos. </p><br><pre> <code class="bash hljs">node1$ ip route add 10.200.12.0/24 via 192.168.1.2 node2$ ip route add 10.200.8.0/24 via 192.168.1.1</code> </pre> <br><pre> <code class="plaintext hljs">[10.200.80.23 container-1]-&gt;[192.168.1.1 node-1]-&gt;[192.168.1.2 node-2]-&gt;[10.200.12.5 container-2]</code> </pre> <br><p>  Esto es exactamente lo que no funciona en el VPS / VDS indicado (y, muy probablemente, generalmente en todos). </p><br><p>  Por lo tanto, si la configuraci√≥n de una soluci√≥n con alto rendimiento de red entre los nodos <strong>es importante para</strong> usted, a√∫n debe gastar m√°s de $ 20 para organizar el cl√∫ster. </p></div></div><br><p>  Puede usar <code>set-flannel-config.sh</code> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">etc /</a> <code>set-flannel-config.sh</code> para establecer la configuraci√≥n de franela deseada.  <strong>Es importante recordar</strong> : si decide cambiar el backend, deber√° eliminar la configuraci√≥n en etcd y reiniciar todos los demonios de franela en todos los nodos, as√≠ que el√≠jalo con prudencia.  El valor predeterminado es vxlan. </p><br><pre> <code class="bash hljs">master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_CA_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/ca.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_CERT_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/kubernetes.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_KEY_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/kubernetes-key.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_ENDPOINTS=<span class="hljs-string"><span class="hljs-string">'https://127.0.0.1:2379'</span></span> master$ etcdctl ls /coreos.com/network/subnets/ /coreos.com/network/subnets/10.200.8.0-24 /coreos.com/network/subnets/10.200.12.0-24 master$ etcdctl get /coreos.com/network/subnets/10.200.8.0-24 {<span class="hljs-string"><span class="hljs-string">"PublicIP"</span></span>:<span class="hljs-string"><span class="hljs-string">"178.79.168.130"</span></span>,<span class="hljs-string"><span class="hljs-string">"BackendType"</span></span>:<span class="hljs-string"><span class="hljs-string">"vxlan"</span></span>,<span class="hljs-string"><span class="hljs-string">"BackendData"</span></span>:{<span class="hljs-string"><span class="hljs-string">"VtepMAC"</span></span>:<span class="hljs-string"><span class="hljs-string">"22:ca:ac:15:71:59"</span></span>}}</code> </pre> <br><p>  Despu√©s de haber registrado la configuraci√≥n deseada en etcd, debe configurar el servicio para ejecutarlo en cada uno de los nodos de trabajo. </p><br><h2 id="flannelservice">  servicio de franela </h2><br><p>  Un ejemplo para el servicio se puede tomar aqu√≠: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">systemd / flannel</a> </p><br><div class="spoiler">  <b class="spoiler_title">servicio de franela</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Flanneld overlay address etcd agent After=network.target [Service] Type=notify #: current host ip. don't change if ip have not changed Environment=PUBLIC_IP=178.79.168.130 Environment=FLANNEL_ETCD=https://192.168.153.60:2379 ExecStart=/usr/bin/flanneld \ -etcd-endpoints=${FLANNEL_ETCD} -etcd-prefix=${FLANNEL_ETCD_KEY} \ -etcd-cafile=/var/lib/kubernetes/ca.pem \ -etcd-certfile=/var/lib/kubernetes/kubernetes.pem \ -etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \ -etcd-prefix=/coreos.com/network \ -healthz-ip=127.0.0.1 \ -subnet-file=/run/flannel/subnet.env \ -public-ip=${PUBLIC_IP} \ -kubeconfig-file=/var/lib/kubernetes/config/kubeconfig/flanneld.kubeconfig \ $FLANNEL_OPTIONS ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker Restart=on-failure RestartSec=5 [Install] RequiredBy=docker.service</code> </pre> </div></div><br><h2 id="nastroyka">  Personalizaci√≥n </h2><br><p>  Como se describi√≥ anteriormente, necesitamos los archivos ca.pem, kubernetes.pem y kubernetes-key.pem para autorizaci√≥n en etcd.  Todos los dem√°s par√°metros no tienen ning√∫n significado sagrado.  Lo √∫nico que es realmente importante es configurar la direcci√≥n IP global a trav√©s de la cual los paquetes de red ir√°n entre redes: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/037/e54/803/037e5480319cedd1e662c925bce23b3e.png" alt="Redes de franela"><br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Superposici√≥n de redes de host m√∫ltiple con franela</a> ) </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable flanneld.service #:  node-1$ systemctl start flanneld</span></span></code> </pre> <br><p>  Despu√©s de que franela se inicie correctamente, debe encontrar la interfaz de red flannel.N en su sistema: </p><br><pre> <code class="plaintext hljs">node-1$ ifconfig flannel.100: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1450 inet 10.200.8.0 netmask 255.255.255.255 broadcast 0.0.0.0 inet6 fe80::20ca:acff:fe15:7159 prefixlen 64 scopeid 0x20&lt;link&gt; ether 22:ca:ac:15:71:59 txqueuelen 0 (Ethernet) RX packets 18853 bytes 1077085 (1.0 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 11856 bytes 264331154 (252.0 MiB) TX errors 0 dropped 47 overruns 0 carrier 0 collisions 0</code> </pre> <br><p>  Verificar que sus interfaces funcionen correctamente en todos los nodos es bastante simple.  En mi caso, el nodo 1 y el nodo 2 tienen redes 10.200.8.0/24 y 10.200.12.0/24, respectivamente, por lo que con una solicitud icmp regular verificamos su disponibilidad: </p><br><pre> <code class="plaintext hljs">#:  node-2  node-1 node-1 $ ping -c 1 10.200.12.0 PING 10.200.12.0 (10.200.12.0) 56(84) bytes of data. 64 bytes from 10.200.12.0: icmp_seq=1 ttl=64 time=4.58 ms #:  node-1  node-2 node-2 $ ping -c 1 10.200.8.0 PING 10.200.8.0 (10.200.8.0) 56(84) bytes of data. 64 bytes from 10.200.8.0: icmp_seq=1 ttl=64 time=1.44 ms</code> </pre> <br><p>  En caso de alg√∫n problema, se recomienda verificar si hay reglas de corte en iptables sobre UDP entre hosts. </p><br><h1 id="konfiguraciya-containerd">  Configuraci√≥n de contenedor </h1><br><p>  Coloque <a href="">etc / containerd / config.toml</a> en <code>/etc/containerd/config.toml</code> o donde sea conveniente para usted, lo principal es recordar cambiar la ruta al archivo de configuraci√≥n en el servicio (containerd.service, descrito a continuaci√≥n). </p><br><p>  Configuraci√≥n con algunas modificaciones del est√°ndar.  <strong>Es importante no configurar</strong> <code>enable_tls_streaming = true</code> si no comprende por qu√© lo est√° haciendo.  De lo contrario, <code>kubectl exec</code> dejar√° de funcionar y dar√° un error de que el certificado fue firmado por una parte desconocida. </p><br><h2 id="containerdservice">  containerd.service </h2><br><div class="spoiler"> <b class="spoiler_title">containerd.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target [Service] ; uncomment this if your overlay module are built as module ; ExecStartPre=/sbin/modprobe overlay ExecStart=/usr/bin/containerd \ -c /etc/containerd/config.toml Restart=always RestartSec=5 Delegate=yes KillMode=process OOMScoreAdjust=-999 LimitNOFILE=1048576 LimitNPROC=infinity LimitCORE=infinity [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-1">  Personalizaci√≥n </h2><br><p>  ,   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">cri-tools</a> . <br>  <a href="">etc/crictl.yaml</a>  <code>/etc/crictl.yaml</code> .      : </p><br><pre> <code class="bash hljs">node-1$ CONTAINERD_NAMESPACE=k8s.io crictl ps CONTAINER ID IMAGE CREATED STATE NAME ATTEMPT POD ID</code> </pre> <br><p>  ,    -    kubernetes , crictl       , ,    . </p><br><h1 id="konfiguraciya-cni-plugins">  CNI Plugins </h1><br><p>  CNI    ,      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a>   ,    ,   . </p><br><h1 id="nastroyka-2">  Personalizaci√≥n </h1><br><p>       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">cni plugins</a>        <code>/opt/cni/bin/</code> </p><br><p>  <a href="">/etc/cni/net.d</a>      : </p><br><div class="spoiler"> <b class="spoiler_title">/etc/cni/net.d/10-flannel.conflist</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.0", "name": "cbr0", "plugins": [ { "type": "flannel", "name": "kubenet", "delegate": { "hairpinMode": true, "isDefaultGateway": true } }, { "type": "portmap", "capabilities": { "portMappings": true }, "externalSetMarkChain": "KUBE-MARK-MASQ" } ] }</code> </pre> </div></div><br><div class="spoiler"> <b class="spoiler_title">/etc/cni/net.d/99-loopback.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.0", "type": "loopback" }</code> </pre> </div></div><br><p>       ,    .  ,       ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Red Hat  Docker  Podman</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Intro to Podman</a> </p><br><h1 id="konfiguraciya-kubelet">  Kubelet </h1><br><p>     kubelet  (     cni) ‚Äî    .   kubelet    hostname.         ,      ""   <code>kubectl logs</code> , <code>kubectl exec</code> , <code>kubectl port-forward</code> . </p><br><div class="spoiler"> <b class="spoiler_title"> kubelet-config.yaml</b> <div class="spoiler_text"><p>  ,   <a href="">etc/kubelet-config.yaml</a>   ,        ,     .     : </p><br><pre> <code class="plaintext hljs">systemReserved: cpu: 200m memory: 600Mi</code> </pre> <br><p>  ,        GO  kubernetes,  ,       .        .           0.2 vCPU  600 MB     . </p><br><p>   ,  , kubelet, kube-proxy, coredns, flannel    . ,               ‚Äî     2 vCPU / 4G ram,           ,     kubernetes + postgresql . </p><br><p>    - (micro nodes)        . </p></div></div><br><h2 id="kubeletservice"> kubelet.service </h2><br><p>  service    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">systemd/kubelet</a> </p><br><div class="spoiler"> <b class="spoiler_title">kubelet.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes Requires=containerd.service [Service] #Environment=NODE_IP=192.168.164.230 Environment=NODE_IP=178.79.168.130 #: node name given by env Environment=NODE_NAME=w40k.net ExecStart=kubelet \ --allow-privileged \ --root-dir=/var/lib/kubernetes/kubelet \ --config=/var/lib/kubernetes/kubelet/kubelet-config.yaml \ --kubeconfig=/var/lib/kubernetes/kubelet/node-1.kubeconfig \ --cni-bin-dir=/opt/cni/bin \ --cni-conf-dir=/etc/cni/net.d/ \ --network-plugin=cni \ --container-runtime=remote \ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \ --image-pull-progress-deadline=10m \ --node-ip=${NODE_IP} \ --hostname-override=${NODE_NAME} \ --v=1 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-3">  Personalizaci√≥n </h2><br><p>      ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RBAC</a> ,                kubelet. </p><br><p>  <a href="">etc/kubelet-default-rbac.yaml</a>  ,  kubelet        : </p><br><pre> <code class="bash hljs">user$ kubectl apply -f etc/kubelet-default-rbac.yaml</code> </pre> <br><p>  ,    ,        . </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable kubelet.service #:  node-1$ systemctl start kubelet</span></span></code> </pre> <br><p>    ,           api : </p><br><pre> <code class="plaintext hljs">$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME w40k.net Ready &lt;none&gt; 5m v1.13.1 178.79.168.130 &lt;none&gt; Gentoo/Linux 4.18.16-x86_64-linode118 containerd://1.2.1</code> </pre> <br><h1 id="konfiguraciya-kube-proxy">  Kube Proxy </h1><br><p> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">systemd/kubelet</a> .   ,   , <code>kube-proxy-config.yaml</code>     : <a href="">etc/kube-proxy</a> </p><br><h2 id="kube-proxyservice"> kube-proxy.service </h2><br><div class="spoiler"> <b class="spoiler_title">kube-proxy.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Kubernetes Proxy Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=kube-proxy \ --config=/var/lib/kubernetes/kube-proxy/kube-proxy-config.yaml Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-4">  Personalizaci√≥n </h2><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable kube-proxy.service #:  node-1$ systemctl start kube-proxy</span></span></code> </pre> <br><p>   kube-proxy   ""   iptables,         ,   -   kubernetes  (- ).   . </p><br><h1 id="konfiguraciya-coredns">  CoreDNS </h1><br><p> Corefile   : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">etc/coredns/Corefile</a> ,    : </p><br><div class="spoiler"> <b class="spoiler_title">/etc/coredns/Corefile</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">.:53 { errors log stdout health :8081 kubernetes cluster.local 10.200.0.0/16 { endpoint https://178.79.148.185:6443 tls /var/lib/kubernetes/kubernetes.pem /var/lib/kubernetes/kubernetes-key.pem /var/lib/kubernetes/ca.pem pods verified upstream /etc/resolv.conf kubeconfig /var/lib/kubernetes/config/kubeconfig/coredns.kubeconfig default } proxy . /etc/resolv.conf cache 30 }</code> </pre> </div></div><br><p>     coredns.kubeconfig  pem- (    )   worker . , coredns      systemd-resolved. ,         Ubuntu ,  ,  ,  ,  .        . </p><br><h2 id="corednsservice"> coredns.service </h2><br><div class="spoiler"> <b class="spoiler_title">coredns.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=CoreDNS Documentation=https://coredns.io/ After=network.target [Service] ExecStart=/usr/bin/coredns -conf /etc/coredns/Corefile Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-5">  Personalizaci√≥n </h2><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable coredns.service #:  node-1$ systemctl start coredns</span></span></code> </pre> <br><p> ,   ,   : </p><br><pre> <code class="plaintext hljs">node-1$ dig kubernetes.default.svc.cluster.local @127.0.0.1 #:    ;kubernetes.default.svc.cluster.local. IN A ;; ANSWER SECTION: kubernetes.default.svc.cluster.local. 5 IN A 10.32.0.1</code> </pre> <br><p>   , coredns   ip   kubernetes . <br> <strong></strong> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">kubernetes.default </a>   <strong></strong> kube-controller-manager,      : </p><br><pre> <code class="plaintext hljs">$ kubectl get svc -n default NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.32.0.1 &lt;none&gt; 443/TCP 26h</code> </pre> <br><h1 id="nginx-ingress--cert-manager"> nginx-ingress &amp; cert-manager </h1><br><p>   ,    .        nginx-ingress  cert-manager. </p><br><p> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">nginx kubernetes ingress</a> (master),  : </p><br><pre> <code class="bash hljs"> user$ git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/nginxinc/kubernetes-ingress.git user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> kubernetes-ingress/deployments user$ kubectl apply -f common/ns-and-sa.yaml user$ kubectl apply -f common/nginx-config.yaml user$ kubectl apply -f common/default-server-secret.yaml user$ kubectl apply -f daemon-set/nginx-ingress.yaml user$ kubectl apply -f rbac/rbac.yaml</code> </pre> <br><p> ‚Äî <a href="">cert manager</a> (v0.5.2) </p><br><pre> <code class="bash hljs"> user$ git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/jetstack/cert-manager.git user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> cert-manager &amp;&amp; git co v0.5.2 user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> contrib/manifests/cert-manager user$ kubectl apply -f with-rbac.yaml</code> </pre> <br><p>  ,    ,  ,    : </p><br><pre> <code class="plaintext hljs">NAMESPACE NAME READY STATUS RESTARTS AGE cert-manager cert-manager-554c76fbb7-t9762 1/1 Running 0 3h38m nginx-ingress nginx-ingress-sdztf 1/1 Running 0 10h nginx-ingress nginx-ingress-vrf85 1/1 Running 0 10h</code> </pre> <br><p>  cert-manager  nginx-ingress    running state,   ,    .          ,         <code>Running</code> .            . </p><br><h1 id="zapuskaem-prilozhenie">   </h1><br><p>   ,     .      ,   kubernetes resource : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">app/k8s</a> </p><br><pre> <code class="bash hljs">user$ kube apply -f ns-and-sa.yaml user$ kube apply -f configmap.yaml <span class="hljs-comment"><span class="hljs-comment">#:  secret-example.yaml       #: secret.yaml user$ kube apply -f secret.yaml user$ kube apply -f tls-production.yaml user$ kube apply -f deployment.yaml user$ kube apply -f service.yaml user$ kube apply -f ingress-production.yaml</span></span></code> </pre> <br><p>   ,     - .  ,    (      kubernetes-example.w40k.net),     ,    ,  cert-manager    nginx-ingress              .   ,    ingress  tls/ssl. </p><br><p>      : </p><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=http://no-">http://no-https.kubernetes-example.w40k.net/</a> ‚Äî  ssl;  ,  -   ,     . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://kubernetes-example.w40k.net/</a> ‚Äî   (,   ,   ),  ,     ,       kubernetes     . </li></ul><br><p>       ,      -   .    -       ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> </a> ,        . </p><br><h1 id="ssylki">  Referencias </h1><br><p> ,     ,   : </p><br><p> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Kubernetes the hard way</a> <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Multi-Host Networking Overlay with Flannel</a> <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Intro to Podman</a> <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Stateless Applications</a> <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">What is ingress</a> </p><br><p>   : </p><br><p> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Kubernetes Networking: Behind the scenes</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> ) <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">A Guide to the Kubernetes Networking Model</a> <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Understanding kubernetes networking: services</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> ) </p><br><h1 id="qa"> Q&amp;A </h1><br><p> &lt;tbd&gt;,           . </p><br><h1 id="otladochnaya-informaciya">   </h1><br><p>     , ,     .    ,       ,  -  ,    ,  . </p><br><div class="spoiler"> <b class="spoiler_title"> </b> <div class="spoiler_text"><h2 id="api-server"> Api Server </h2><br><p>   <code>kube-apiserver.service</code>    ,       api-server'   curl    http .            - . <br>     admin.kubeconfig  ${HOME}/.kube/config,   kubectl      api-server (kube-apiserver). </p><br><p>    (   )  HTTP 200 OK + ,  api-server  : </p><br><pre> <code class="plaintext hljs">curl -H "Authorization: Bearer e5qXNAtwwCHUUwyLilZmAoFPozrQwUpw" -k -L https://&lt;api-server-address&gt;:6443/api/v1/</code> </pre> <br><h2 id="kube-controller-manager"> Kube Controller Manager </h2><br><p>  ,  controller manager   api    ,      .        ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">service account'</a> : </p><br><pre> <code class="plaintext hljs">$ kubectl get sa NAME SECRETS AGE default 1 19h</code> </pre> <br><p>    ,   ,  kube-controller-manager  . </p><br><h2 id="kube-scheduler"> Kube Scheduler </h2><br><p>       .  ,    ,    <a href="">debug/job.yaml</a>        <code>kubectl describe &lt;type/resource&gt;</code> . <br>    <strong> </strong>  ,  kube controller manager . </p><br><pre> <code class="plaintext hljs">#:   job user$ kubectl apply -f debug/job.yaml job.batch/app created #:  ,   job user$ kubectl get pods -l job-name=app NAME READY STATUS RESTARTS AGE app-9kr9z 0/1 Completed 0 54s #: ,        #:   user$ kubectl describe pods app-9kr9z # ...   ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 12s default-scheduler Successfully assigned example/app-9kr9z to w40k.net</code> </pre> <br><p>   , default-scheduler   pod   w40k.net.    -  ,            ‚Äî    . </p><br><p>              . , ,   , ‚Äî      "".       systemd        . </p><br><p>   kube scheduler  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> </p><br><h2 id="kubelet"> Kubelet </h2><br><p> Kubelet    kubernetes     .  kubelet       .     kubernetes event ( <code>kubectl get events -o wide</code> )         . </p><br><p>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> (  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> ) </p><br><h2 id="kube-proxy-i-servisy"> Kube Proxy   </h2><br><p>     kube-proxy    : </p><br><ul><li>      (     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> Flannel</a> ,      ); </li><li>  iptables,   filter  nat . </li></ul><br><p> <strong></strong> , 10.32.0.0/24   "".  ,        .     iptables,     ,   ,     -    +.  <strong> </strong>  icmp    ,      ping'  .        ,     . </p><br><p>  ,     kube-proxy,               : </p><br><pre> <code class="plaintext hljs">#:    user$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend ClusterIP 10.32.0.195 &lt;none&gt; 80/TCP 5m #:     user$ kubectl get pods -o wide #:     ' NAME READY STATUS RESTARTS AGE IP NODE backend-896584448-4r94s 1/1 Running 0 11h 10.200.8.105 w40k.net backend-896584448-np992 1/1 Running 0 11h 10.200.12.68 docker.grart.net #:  10   /status/ endpoint ,       #:       node-1$ for i in `seq 10`; do curl -L http://10.32.0.195/status/; done okokokokokokokokokok node-1$ conntrack -L -d 10.32.0.195 tcp 6 62 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62158 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62158 [ASSURED] mark=0 use=1 tcp 6 60 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62144 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62144 [ASSURED] mark=0 use=1 tcp 6 58 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62122 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62122 [ASSURED] mark=0 use=1 tcp 6 59 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62142 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62142 [ASSURED] mark=0 use=1 tcp 6 58 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62130 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62130 [ASSURED] mark=0 use=1 tcp 6 61 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62150 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62150 [ASSURED] mark=0 use=1 tcp 6 56 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62116 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62116 [ASSURED] mark=0 use=1 tcp 6 57 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62118 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62118 [ASSURED] mark=0 use=1 tcp 6 59 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62132 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62132 [ASSURED] mark=0 use=1 tcp 6 56 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62114 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62114 [ASSURED] mark=0 use=1</code> </pre> <br><p>      src/dst (9  10 ).   ,  src      : </p><br><ul><li> 10.200.8.105 </li><li> 10.200.12.68 </li></ul><br><p>  ,    .      ,  -  ( ,    )  .         . </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#: node-1   10.200.8.105, node-2 10.200.12.68, #:      8000  #:  node-1 node-1$ curl -L http://10.200.8.105:8000/status/ ok node-1$ curl -L http://10.200.12.68:8000/status/ ok #:  node-2 node-2$ curl -L http://10.200.8.105:8000/status/ ok node-2$ curl -L http://10.200.12.68:8000/status/ ok</span></span></code> </pre> <br><p>    ,    ,    conntrack        ,  ,      kube-proxy.   ,       nat : </p><br><p> <code>node-1$ iptables -t nat -vnL</code> </p> <br><p>          . </p><br><p>                  .  ,    ,      .   ,       .  -       , ,   . </p><br><p>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> </p></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es435228/">https://habr.com/ru/post/es435228/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es435214/index.html">Lanzamiento de Linux 4.20: lo que ha cambiado en la nueva versi√≥n del kernel</a></li>
<li><a href="../es435216/index.html">C√≥mo hacer 200 a partir de dos l√≠neas de c√≥digo y por qu√© necesita hacer esto</a></li>
<li><a href="../es435220/index.html">Kotlin Native: realiza un seguimiento de los archivos</a></li>
<li><a href="../es435224/index.html">C√≥mo comunicarse en una oficina en ingl√©s: 14 expresiones idiom√°ticas √∫tiles</a></li>
<li><a href="../es435226/index.html">Restaurar datos desde cero</a></li>
<li><a href="../es435234/index.html">M√°s inteligente, m√°s preciso y m√°s preciso: c√≥mo la IA cambia los vuelos al espacio</a></li>
<li><a href="../es435236/index.html">Byte-machine para el fuerte (y no solo) en nativos americanos (parte 3)</a></li>
<li><a href="../es435240/index.html">Unreal Engine4: efecto de escaneo posterior al proceso</a></li>
<li><a href="../es435242/index.html">¬øPor qu√© tengo miedo de convertirme en un "hombre bombeado"</a></li>
<li><a href="../es435244/index.html">Proyecto ITER en 2018</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>