<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßîüèæ üë©üèø‚Äçüíª üë©üèº‚Äçü§ù‚Äçüë®üèΩ Aventuras com um cluster Kubernetes em casa üëãüèæ üïå üíÖ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nota perev. : O autor do artigo, Marshall Brekka, ocupa o cargo de diretor de design de sistemas na Fair.com, que oferece sua aplica√ß√£o para loca√ß√£o d...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Aventuras com um cluster Kubernetes em casa</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/435526/">  <i><b>Nota</b></i>  <i><b>perev.</b></i>  <i>: O autor do artigo, Marshall Brekka, ocupa o cargo de diretor de design de sistemas na Fair.com, que oferece sua aplica√ß√£o para loca√ß√£o de autom√≥veis.</i>  <i>Em seu tempo livre, ele gosta de usar sua vasta experi√™ncia para resolver problemas "caseiros" que dificilmente surpreender√£o qualquer nerd (portanto, a pergunta "Por qu√™?" - com rela√ß√£o √†s a√ß√µes descritas abaixo - √© omitida a priori).</i>  <i>Ent√£o, em sua publica√ß√£o, Marshall compartilha os resultados da recente implanta√ß√£o do Kubernetes em ... placas ARM.</i> <br><br><img src="https://habrastorage.org/webt/ul/nj/do/ulnjdoyysctwv-34jhuyn-wvsp8.png"><br><br>  Como muitos outros geeks, ao longo dos anos acumulei uma variedade de placas de desenvolvimento como o Raspberry Pi.  E, como muitos nerds, eles se espanaram nas prateleiras com o pensamento de que algum dia seriam √∫teis.  E agora para mim este dia finalmente chegou! <a name="habracut"></a><br><br>  Nas f√©rias de inverno, v√°rias semanas fora do trabalho apareceram, nas quais houve tempo suficiente para inventariar todo o ferro acumulado e decidir o que fazer com ele.  Aqui est√° o que eu tinha: <br><br><ul><li>  Gabinete RAID de 5 unidades com conex√£o USB3; </li><li>  Raspberry Pi Modelo B (modelo OG); </li><li>  CubbieBoard 1; </li><li>  Banana Pi M1; </li><li>  Netbook HP (2012?). </li></ul><br>  Dos 5 componentes de ferro listados, usei, a menos que RAID e um netbook como um NAS tempor√°rio.  No entanto, devido √† falta de suporte a USB3 no netbook, o RAID n√£o utilizou o potencial de velocidade total. <br><br><h2>  Objetivos de vida </h2><br>  Como trabalhar com RAID n√£o era o ideal ao usar um netbook, estabeleci os seguintes objetivos para obter a melhor configura√ß√£o: <br><br><ol><li>  NAS com USB3 e Ethernet gigabit; </li><li>  A melhor maneira de gerenciar o software no seu dispositivo </li><li>  (b√¥nus) a capacidade de transmitir conte√∫do multim√≠dia do RAID para o Fire TV. </li></ol><br>  Como nenhum dos dispositivos dispon√≠veis suportava USB3 e Ethernet de gigabit, infelizmente, tive que fazer compras adicionais.  A escolha recaiu no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ROC-RK3328-CC</a> .  Ela possu√≠a todas as especifica√ß√µes necess√°rias e suporte suficiente para sistemas operacionais. <br><br>  Tendo resolvido minhas necessidades de hardware (e aguardando a chegada dessa solu√ß√£o), mudei para o segundo objetivo. <br><br><h2>  Gerenciando o software no dispositivo </h2><br>  Em parte, meus projetos anteriores relacionados √†s placas de desenvolvimento falharam devido √† aten√ß√£o insuficiente aos problemas de reprodutibilidade e documenta√ß√£o.  Ao criar a pr√≥xima configura√ß√£o para minhas necessidades atuais, n√£o me preocupei em anotar as etapas seguidas ou os links para as postagens que segui.  E quando, depois de meses ou anos, algo deu errado e eu tentei resolver o problema, n√£o entendi como tudo estava organizado originalmente. <br><br>  Ent√£o eu disse a mim mesma que desta vez tudo ser√° diferente! <br><br><img src="https://habrastorage.org/webt/dm/vb/iv/dmvbivkoa65wfd1ve5mo5wh5jdc.jpeg"><br><br>  E ele se voltou para o fato de que eu sei o suficiente - para Kubernetes. <br><br>  Embora o K8s seja uma solu√ß√£o muito dif√≠cil para um problema bastante simples, depois de quase tr√™s anos gerenciando clusters usando v√°rias ferramentas (minhas pr√≥prias, kops etc.) no meu trabalho principal, estou muito familiarizado com esse sistema.  Al√©m disso, a implanta√ß√£o de K8s fora de um ambiente em nuvem e at√© em dispositivos ARM - tudo isso parecia uma tarefa interessante. <br><br>  Eu tamb√©m pensei que, como o hardware dispon√≠vel n√£o atende aos requisitos necess√°rios para o NAS, tentarei pelo menos montar um cluster a partir dele e, possivelmente, algum software que n√£o exige tanto recursos poder√° funcionar em dispositivos mais antigos. <br><br><h2>  Kubernetes no ARM </h2><br>  No trabalho, n√£o tive a oportunidade de usar o utilit√°rio <code>kubeadm</code> para implantar clusters, ent√£o decidi que agora era a hora de experiment√°-lo em a√ß√£o. <br><br>  O Raspbian foi escolhido como sistema operacional, porque √© famoso pelo melhor suporte para minhas placas. <br><br>  Encontrei um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">bom artigo</a> sobre como configurar o Kubernetes em um Raspberry Pi usando o HypriotOS.  Como n√£o tinha certeza da disponibilidade do HypriotOS para todos os meus pain√©is, adaptei essas instru√ß√µes ao Debian / Raspbian. <br><br><h3>  Componentes Necess√°rios </h3><br>  Primeiro, era necess√°ria a instala√ß√£o das seguintes ferramentas: <br><br><ul><li>  Docker, </li><li>  kubelet </li><li>  kubeadm, </li><li>  kubectl. </li></ul><br>  O Docker deve ser instalado usando um script especial - script de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">conveni√™ncia</a> (conforme indicado no caso de uso do Raspbian). <br><br><pre> <code class="bash hljs">curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh</code> </pre> <br>  Depois disso, instalei os componentes do Kubernetes de acordo com as instru√ß√µes do blog Hypriot, adaptando-os para que vers√µes espec√≠ficas sejam usadas para todas as depend√™ncias: <br><br><pre> <code class="bash hljs">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"deb http://apt.kubernetes.io/ kubernetes-xenial main"</span></span> &gt; /etc/apt/sources.list.d/kubernetes.list apt-get update apt-get install -y kubelet=1.13.1-00 kubectl=1.13.1-00 kubeadm=1.13.1-00</code> </pre> <br><h3>  Raspberry pi b </h3><br>  A primeira dificuldade surgiu ao tentar inicializar um cluster no Raspberry Pi B: <br><br><pre> <code class="bash hljs">$ kubeadm init Illegal instruction</code> </pre> <br>  Acontece que o Kubernetes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">removeu o suporte ao ARMv6</a> .  Bem, eu tamb√©m tenho CubbieBoard e Banana Pi. <br><br><h3>  Banana pi </h3><br>  Inicialmente, a mesma sequ√™ncia de a√ß√µes para o Banana Pi parecia ter mais sucesso; no entanto, o comando <code>kubeadm init</code> enquanto tentava esperar o plano de controle funcionar: <br><br><pre> <code class="plaintext hljs">error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster</code> </pre> <br>  Descobrindo com o <code>docker ps</code> que estava acontecendo com os cont√™ineres, vi que o <code>kube-controller-manager</code> e o <code>kube-scheduler</code> estavam trabalhando por pelo menos 4-5 minutos, mas o <code>kube-api-server</code> se levantou h√° apenas 1-2 minutos: <br><br><pre> <code class="bash hljs">$ docker ps CONTAINER ID COMMAND CREATED STATUS de22427ad594 <span class="hljs-string"><span class="hljs-string">"kube-apiserver --au‚Ä¶"</span></span> About a minute ago Up About a minute dc2b70dd803e <span class="hljs-string"><span class="hljs-string">"kube-scheduler --ad‚Ä¶"</span></span> 5 minutes ago Up 5 minutes 60b6cc418a66 <span class="hljs-string"><span class="hljs-string">"kube-controller-man‚Ä¶"</span></span> 5 minutes ago Up 5 minutes 1e1362a9787c <span class="hljs-string"><span class="hljs-string">"etcd --advertise-cl‚Ä¶"</span></span> 5 minutes ago Up 5 minutes</code> </pre> <br>  Obviamente, o <code>api-server</code> estava morrendo ou o processo de estr√¥ncio estava matando e reiniciando. <br><br>  Verificando os logs, vi procedimentos de inicializa√ß√£o muito padr√£o - houve um registro do in√≠cio da escuta da porta segura e uma longa pausa antes do aparecimento de v√°rios erros nos handshakes TLS: <br><br><pre> <code class="plaintext hljs">20:06:48.604881 naming_controller.go:284] Starting NamingConditionController 20:06:48.605031 establishing_controller.go:73] Starting EstablishingController 20:06:50.791098 log.go:172] http: TLS handshake error from 192.168.1.155:50280: EOF 20:06:51.797710 log.go:172] http: TLS handshake error from 192.168.1.155:50286: EOF 20:06:51.971690 log.go:172] http: TLS handshake error from 192.168.1.155:50288: EOF 20:06:51.990556 log.go:172] http: TLS handshake error from 192.168.1.155:50284: EOF 20:06:52.374947 log.go:172] http: TLS handshake error from 192.168.1.155:50486: EOF 20:06:52.612617 log.go:172] http: TLS handshake error from 192.168.1.155:50298: EOF 20:06:52.748668 log.go:172] http: TLS handshake error from 192.168.1.155:50290: EOF</code> </pre> <br>  E logo depois disso, o servidor encerra seu trabalho.  A pesquisa no Google levou a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">esse problema</a> , indicando uma poss√≠vel raz√£o para a opera√ß√£o lenta de algoritmos criptogr√°ficos em alguns dispositivos ARM. <br><br>  Fui mais longe e pensei que talvez o <code>api-server</code> recebendo muitas solicita√ß√µes repetidas do <code>scheduler</code> e do <code>controller-manager</code> . <br><br>  A remo√ß√£o desses arquivos do diret√≥rio de manifesto instruir√° o kubelet a interromper a execu√ß√£o dos pods correspondentes: <br><br><pre> <code class="bash hljs">mkdir /etc/kubernetes/manifests.bak mv /etc/kubernetes/manifests/kube-scheduler.yaml /etc/kubernetes/manifests.bak/ mv /etc/kubernetes/manifests/kube-controller-mananger.yaml /etc/kubernetes/manifests.bak/</code> </pre> <br>  A visualiza√ß√£o dos logs mais recentes <code>api-server</code> mostrou que agora o processo foi al√©m, no entanto, ele ainda morreu ap√≥s cerca de 2 minutos.  Lembrei-me de que o manifesto poderia conter uma amostra de vivacidade com tempos limite com valores muito baixos para um dispositivo t√£o lento. <br><br>  Portanto, verifiquei <code>/etc/kubernetes/manifests/kube-api-server.yaml</code> - e nele, √© claro ... <br><br><pre> <code class="plaintext hljs">livenessProbe: failureThreshold: 8 httpGet: host: 192.168.1.155 path: /healthz port: 6443 scheme: HTTPS initialDelaySeconds: 15 timeoutSeconds: 15</code> </pre> <br>  O pod foi <code>timeoutSeconds</code> ap√≥s 135 segundos ( <code>timeoutSeconds</code> + <code>timeoutSeconds</code> * <code>failureThreshold</code> ).  Aumentar <code>initialDelaySeconds</code> para 120 ... <br><br>  <b>Sucesso!</b>  Bem, ainda ocorrem erros de handshake (presumivelmente do kubelet), no entanto, o lan√ßamento ainda ocorreu: <br><br><pre> <code class="plaintext hljs">20:06:54.957236 log.go:172] http: TLS handshake error from 192.168.1.155:50538: EOF 20:06:55.004865 log.go:172] http: TLS handshake error from 192.168.1.155:50384: EOF 20:06:55.118343 log.go:172] http: TLS handshake error from 192.168.1.155:50292: EOF 20:06:55.252586 cache.go:39] Caches are synced for autoregister controller 20:06:55.253907 cache.go:39] Caches are synced for APIServiceRegistrationController controller 20:06:55.545881 controller_utils.go:1034] Caches are synced for crd-autoregister controller ... 20:06:58.921689 storage_rbac.go:187] created clusterrole.rbac.authorization.k8s.io/cluster-admin 20:06:59.049373 storage_rbac.go:187] created clusterrole.rbac.authorization.k8s.io/system:discovery 20:06:59.214321 storage_rbac.go:187] created clusterrole.rbac.authorization.k8s.io/system:basic-user</code> </pre> <br>  Quando o <code>api-server</code> se levantou, mudei os arquivos YAML para o controlador e o planejador de volta ao diret√≥rio do manifesto, ap√≥s o qual eles tamb√©m come√ßaram normalmente. <br><br>  Agora √© hora de garantir que o download seja bem-sucedido se voc√™ deixar todos os arquivos no diret√≥rio de origem: basta alterar o atraso permitido na inicializa√ß√£o do <code>livenessProbe</code> ? <br><br><pre> <code class="plaintext hljs">20:29:33.306983 reflector.go:134] k8s.io/client-go/informers/factory.go:132: Failed to list *v1.Service: Get https://192.168.1.155:6443/api/v1/services?limit=500&amp;resourceVersion=0: dial tcp 192.168.1.155:6443: i/o timeout 20:29:33.434541 reflector.go:134] k8s.io/client-go/informers/factory.go:132: Failed to list *v1.ReplicationController: Get https://192.168.1.155:6443/api/v1/replicationcontrollers?limit=500&amp;resourceVersion=0: dial tcp 192.168.1.155:6443: i/o timeout 20:29:33.435799 reflector.go:134] k8s.io/client-go/informers/factory.go:132: Failed to list *v1.PersistentVolume: Get https://192.168.1.155:6443/api/v1/persistentvolumes?limit=500&amp;resourceVersion=0: dial tcp 192.168.1.155:6443: i/o timeout 20:29:33.477405 reflector.go:134] k8s.io/client-go/informers/factory.go:132: Failed to list *v1beta1.PodDisruptionBudget: Get https://192.168.1.155:6443/apis/policy/v1beta1/poddisruptionbudgets?limit=500&amp;resourceVersion=0: dial tcp 192.168.1.155:6443: i/o timeout 20:29:33.493660 reflector.go:134] k8s.io/client-go/informers/factory.go:132: Failed to list *v1.PersistentVolumeClaim: Get https://192.168.1.155:6443/api/v1/persistentvolumeclaims?limit=500&amp;resourceVersion=0: dial tcp 192.168.1.155:6443: i/o timeout 20:29:37.974938 controller_utils.go:1027] Waiting for caches to sync for scheduler controller 20:29:38.078558 controller_utils.go:1034] Caches are synced for scheduler controller 20:29:38.078867 leaderelection.go:205] attempting to acquire leader lease kube-system/kube-scheduler 20:29:38.291875 leaderelection.go:214] successfully acquired lease kube-system/kube-scheduler</code> </pre> <br>  Sim, tudo funciona, embora esses dispositivos antigos, aparentemente, n√£o tenham a inten√ß√£o de iniciar o plano de controle, uma vez que conex√µes TLS repetidas causam freios significativos.  De uma forma ou de outra - √© recebida uma instala√ß√£o funcional do K8s no ARM!  Vamos mais longe ... <br><br><h3>  Montagem RAID </h3><br>  Como os cart√µes SD n√£o s√£o adequados para grava√ß√£o a longo prazo, decidi usar um armazenamento mais confi√°vel para as partes mais vol√°teis do sistema de arquivos - nesse caso, RAID.  4 se√ß√µes foram destacadas nele: <br><br><ul><li>  50 GB; </li><li>  2 √ó 20 GB; </li><li>  3,9 Tb. </li></ul><br>  Ainda n√£o propus uma finalidade espec√≠fica para parti√ß√µes de 20 gigabytes, mas queria deixar oportunidades adicionais para o futuro. <br><br>  No <code>/etc/fstab</code> para a parti√ß√£o de 50 GB, o ponto de montagem foi especificado como <code>/mnt/root</code> e para 3,9 TB - <code>/mnt/raid</code> .  Depois disso, montei os diret√≥rios com o etcd e o docker na parti√ß√£o de 50 GB: <br><br><pre> <code class="plaintext hljs">UUID=655a39e8-9a5d-45f3-ae14-73b4c5ed50c3 /mnt/root ext4 defaults,rw,user,auto,exec 0 0 UUID=0633df91-017c-4b98-9b2e-4a0d27989a5c /mnt/raid ext4 defaults,rw,user,auto 0 0 /mnt/root/var/lib/etcd /var/lib/etcd none defaults,bind 0 0 /mnt/root/var/lib/docker /var/lib/docker none defaults,bind 0 0</code> </pre> <br><h3>  Chegada ROC-RK3328-CC </h3><br>  Quando a nova placa foi entregue, instalei os componentes necess√°rios para os K8s <i>(consulte o in√≠cio do artigo)</i> e <code>kubeadm init</code> .  Alguns minutos de espera s√£o o sucesso e a sa√≠da do comando <code>join</code> para executar em outros n√≥s. <br><br>  √ìtimo!  Sem problemas com intervalos. <br><br>  E como o RAID tamb√©m ser√° usado nesta placa, as montagens precisar√£o ser configuradas novamente.  Para resumir todas as etapas: <br><br><h4>  1. Monte discos no / etc / fstab </h4><br><pre> <code class="plaintext hljs">UUID=655a39e8-9a5d-45f3-ae14-73b4c5ed50c3 /mnt/root ext4 defaults,rw,user,auto,exec 0 0 UUID=0633df91-017c-4b98-9b2e-4a0d27989a5c /mnt/raid ext4 defaults,rw,user,auto 0 0 /mnt/root/var/lib/etcd /var/lib/etcd none defaults,bind 0 0 /mnt/root/var/lib/docker /var/lib/docker none defaults,bind 0 0</code> </pre> <br><h4>  2. Instalando os bin√°rios Docker e K8s </h4><br><pre> <code class="bash hljs">curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh</code> </pre> <br><pre> <code class="bash hljs">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"deb http://apt.kubernetes.io/ kubernetes-xenial main"</span></span> &gt; /etc/apt/sources.list.d/kubernetes.list apt-get update apt-get install -y kubelet=1.13.1-00 kubectl=1.13.1-00 kubeadm=1.13.1-00</code> </pre> <br><h4>  3. Configurando um nome de host exclusivo (importante como muitos n√≥s foram adicionados) </h4><br><pre> <code class="bash hljs">hostnamectl <span class="hljs-built_in"><span class="hljs-built_in">set</span></span>-hostname k8s-master-1</code> </pre> <br><h4>  4. Inicializa√ß√£o do Kubernetes </h4><br>  Eu omito a fase com o plano de controle, porque eu quero poder planejar pods normais neste n√≥: <br><br><pre> <code class="bash hljs">kubeadm init --skip-phases mark-control-plane</code> </pre> <br><h4>  5. Instalando o Plug-in de Rede </h4><br>  As informa√ß√µes sobre isso no artigo Hypriot eram um pouco datadas, porque o plug-in de rede Weave agora tamb√©m √© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">suportado no ARM</a> : <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">export</span></span> KUBECONFIG=/etc/kubernetes/admin.conf kubectl apply -f <span class="hljs-string"><span class="hljs-string">"https://cloud.weave.works/k8s/net?k8s-version=</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$(kubectl version | base64 | tr -d '\n')</span></span></span><span class="hljs-string">"</span></span></code> </pre> <br><h4>  6. Adicionando r√≥tulos de host </h4><br>  Neste n√≥, vou iniciar o servidor NAS, por isso vou marc√°-lo com r√≥tulos para poss√≠vel uso futuro no planejador: <br><br><pre> <code class="bash hljs">kubectl label nodes k8s-master-1 marshallbrekka.raid=<span class="hljs-literal"><span class="hljs-literal">true</span></span> kubectl label nodes k8s-master-1 marshallbrekka.network=gigabit</code> </pre> <br><h3>  Conectando outros n√≥s ao cluster </h3><br>  A configura√ß√£o de outros dispositivos (Banana Pi, CubbieBoard) tamb√©m foi f√°cil.  Para eles, √© necess√°rio repetir as 3 primeiras etapas (alterando as configura√ß√µes para montar discos / m√≠dia flash, dependendo da disponibilidade) e executar o comando <code>kubeadm join</code> vez do <code>kubeadm init</code> . <br><br><h2>  Localizando cont√™ineres do Docker para ARM </h2><br>  A maioria dos cont√™ineres do Docker necess√°rios √© criada normalmente em um Mac, mas para o ARM √© um pouco mais complicado.  Tendo encontrado muitos artigos sobre como usar o QEMU para esses fins, cheguei √† <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">conclus√£o de</a> que a maioria dos aplicativos de que eu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">preciso</a> j√° est√° montada e muitos deles est√£o dispon√≠veis no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">servidor linux</a> . <br><br><h2>  Pr√≥ximas etapas </h2><br>  Ainda n√£o obtendo a configura√ß√£o inicial dos dispositivos em uma forma automatizada / com scripts, como eu gostaria, pelo menos compus um conjunto de comandos b√°sicos (montagens, <code>kubeadm</code> <code>docker</code> e <code>kubeadm</code> ) e os documentei no reposit√≥rio Git.  O restante dos aplicativos usados ‚Äã‚Äãtamb√©m recebeu configura√ß√µes YAML para K8s armazenadas no mesmo reposit√≥rio; portanto, agora √© muito f√°cil obter a configura√ß√£o necess√°ria do zero. <br><br>  No futuro, gostaria de obter o seguinte: <br><br><ol><li>  Tornar os sites principais altamente dispon√≠veis </li><li>  adicione monitoramento / notifica√ß√µes para saber sobre falhas em qualquer componente; </li><li>  Altere as configura√ß√µes de DCHP do roteador para usar um servidor DNS do cluster para simplificar a descoberta de aplicativos (quem deseja se lembrar dos endere√ßos IP internos?); </li><li>  execute o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MetalLB</a> para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">encaminhar os</a> servi√ßos de cluster para uma rede privada (DNS, etc.). </li></ol><br><br><h2>  PS do tradutor </h2><br>  Leia tamb√©m em nosso blog: <br><br><ul><li>  ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Dicas e truques do Kubernetes: sobre a aloca√ß√£o de n√≥s e a carga na aplica√ß√£o web</a> ‚Äù; </li><li>  ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Dicas e truques do Kubernetes: acesso a sites de desenvolvimento</a> ‚Äù; </li><li>  ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Dicas e truques do Kubernetes: acelerando a inicializa√ß√£o de grandes bancos de dados</a> ‚Äù; </li><li>  ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">11 maneiras de (n√£o) se tornar uma v√≠tima do Kubernetes Hacking</a> ‚Äù; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Brincar com o Kubernetes √© um servi√ßo para conhecer os K8s na pr√°tica</a> ." </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt435526/">https://habr.com/ru/post/pt435526/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt435510/index.html">Microeletr√¥nica, neurofisiologia e aprendizado de m√°quina agitam, mas n√£o misturam</a></li>
<li><a href="../pt435512/index.html">Royole Developers Show Smartphone dobr√°vel flex√≠vel</a></li>
<li><a href="../pt435514/index.html">Na R√∫ssia, eles est√£o desenvolvendo um processador para acelerar redes neurais</a></li>
<li><a href="../pt435520/index.html">N√≥s escrevemos nossa linguagem de programa√ß√£o, parte 3: Arquitetura do tradutor. An√°lise de estruturas de linguagem e express√µes matem√°ticas</a></li>
<li><a href="../pt435522/index.html">Instant√¢neos de eventos no Axonframework 3, melhorando o desempenho</a></li>
<li><a href="../pt435528/index.html">5 raz√µes para o sucesso: por que a Amazon se tornou a empresa mais cara do mundo</a></li>
<li><a href="../pt435530/index.html">Assinaturas pagas - Depend√™ncia da conex√£o autom√°tica em um dispositivo m√≥vel</a></li>
<li><a href="../pt435532/index.html">Tornado vs Aiohttp: uma jornada pela natureza de estruturas ass√≠ncronas</a></li>
<li><a href="../pt435534/index.html">Ci√™ncia de dados: livros b√°sicos</a></li>
<li><a href="../pt435536/index.html">Rob√¥s human√≥ides: benef√≠cios e problemas de mecanismos antropom√≥rficos</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>