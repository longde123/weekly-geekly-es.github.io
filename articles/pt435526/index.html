<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧔🏾 👩🏿‍💻 👩🏼‍🤝‍👨🏽 Aventuras com um cluster Kubernetes em casa 👋🏾 🕌 💅</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nota perev. : O autor do artigo, Marshall Brekka, ocupa o cargo de diretor de design de sistemas na Fair.com, que oferece sua aplicação para locação d...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Aventuras com um cluster Kubernetes em casa</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/435526/">  <i><b>Nota</b></i>  <i><b>perev.</b></i>  <i>: O autor do artigo, Marshall Brekka, ocupa o cargo de diretor de design de sistemas na Fair.com, que oferece sua aplicação para locação de automóveis.</i>  <i>Em seu tempo livre, ele gosta de usar sua vasta experiência para resolver problemas "caseiros" que dificilmente surpreenderão qualquer nerd (portanto, a pergunta "Por quê?" - com relação às ações descritas abaixo - é omitida a priori).</i>  <i>Então, em sua publicação, Marshall compartilha os resultados da recente implantação do Kubernetes em ... placas ARM.</i> <br><br><img src="https://habrastorage.org/webt/ul/nj/do/ulnjdoyysctwv-34jhuyn-wvsp8.png"><br><br>  Como muitos outros geeks, ao longo dos anos acumulei uma variedade de placas de desenvolvimento como o Raspberry Pi.  E, como muitos nerds, eles se espanaram nas prateleiras com o pensamento de que algum dia seriam úteis.  E agora para mim este dia finalmente chegou! <a name="habracut"></a><br><br>  Nas férias de inverno, várias semanas fora do trabalho apareceram, nas quais houve tempo suficiente para inventariar todo o ferro acumulado e decidir o que fazer com ele.  Aqui está o que eu tinha: <br><br><ul><li>  Gabinete RAID de 5 unidades com conexão USB3; </li><li>  Raspberry Pi Modelo B (modelo OG); </li><li>  CubbieBoard 1; </li><li>  Banana Pi M1; </li><li>  Netbook HP (2012?). </li></ul><br>  Dos 5 componentes de ferro listados, usei, a menos que RAID e um netbook como um NAS temporário.  No entanto, devido à falta de suporte a USB3 no netbook, o RAID não utilizou o potencial de velocidade total. <br><br><h2>  Objetivos de vida </h2><br>  Como trabalhar com RAID não era o ideal ao usar um netbook, estabeleci os seguintes objetivos para obter a melhor configuração: <br><br><ol><li>  NAS com USB3 e Ethernet gigabit; </li><li>  A melhor maneira de gerenciar o software no seu dispositivo </li><li>  (bônus) a capacidade de transmitir conteúdo multimídia do RAID para o Fire TV. </li></ol><br>  Como nenhum dos dispositivos disponíveis suportava USB3 e Ethernet de gigabit, infelizmente, tive que fazer compras adicionais.  A escolha recaiu no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ROC-RK3328-CC</a> .  Ela possuía todas as especificações necessárias e suporte suficiente para sistemas operacionais. <br><br>  Tendo resolvido minhas necessidades de hardware (e aguardando a chegada dessa solução), mudei para o segundo objetivo. <br><br><h2>  Gerenciando o software no dispositivo </h2><br>  Em parte, meus projetos anteriores relacionados às placas de desenvolvimento falharam devido à atenção insuficiente aos problemas de reprodutibilidade e documentação.  Ao criar a próxima configuração para minhas necessidades atuais, não me preocupei em anotar as etapas seguidas ou os links para as postagens que segui.  E quando, depois de meses ou anos, algo deu errado e eu tentei resolver o problema, não entendi como tudo estava organizado originalmente. <br><br>  Então eu disse a mim mesma que desta vez tudo será diferente! <br><br><img src="https://habrastorage.org/webt/dm/vb/iv/dmvbivkoa65wfd1ve5mo5wh5jdc.jpeg"><br><br>  E ele se voltou para o fato de que eu sei o suficiente - para Kubernetes. <br><br>  Embora o K8s seja uma solução muito difícil para um problema bastante simples, depois de quase três anos gerenciando clusters usando várias ferramentas (minhas próprias, kops etc.) no meu trabalho principal, estou muito familiarizado com esse sistema.  Além disso, a implantação de K8s fora de um ambiente em nuvem e até em dispositivos ARM - tudo isso parecia uma tarefa interessante. <br><br>  Eu também pensei que, como o hardware disponível não atende aos requisitos necessários para o NAS, tentarei pelo menos montar um cluster a partir dele e, possivelmente, algum software que não exige tanto recursos poderá funcionar em dispositivos mais antigos. <br><br><h2>  Kubernetes no ARM </h2><br>  No trabalho, não tive a oportunidade de usar o utilitário <code>kubeadm</code> para implantar clusters, então decidi que agora era a hora de experimentá-lo em ação. <br><br>  O Raspbian foi escolhido como sistema operacional, porque é famoso pelo melhor suporte para minhas placas. <br><br>  Encontrei um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">bom artigo</a> sobre como configurar o Kubernetes em um Raspberry Pi usando o HypriotOS.  Como não tinha certeza da disponibilidade do HypriotOS para todos os meus painéis, adaptei essas instruções ao Debian / Raspbian. <br><br><h3>  Componentes Necessários </h3><br>  Primeiro, era necessária a instalação das seguintes ferramentas: <br><br><ul><li>  Docker, </li><li>  kubelet </li><li>  kubeadm, </li><li>  kubectl. </li></ul><br>  O Docker deve ser instalado usando um script especial - script de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">conveniência</a> (conforme indicado no caso de uso do Raspbian). <br><br><pre> <code class="bash hljs">curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh</code> </pre> <br>  Depois disso, instalei os componentes do Kubernetes de acordo com as instruções do blog Hypriot, adaptando-os para que versões específicas sejam usadas para todas as dependências: <br><br><pre> <code class="bash hljs">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"deb http://apt.kubernetes.io/ kubernetes-xenial main"</span></span> &gt; /etc/apt/sources.list.d/kubernetes.list apt-get update apt-get install -y kubelet=1.13.1-00 kubectl=1.13.1-00 kubeadm=1.13.1-00</code> </pre> <br><h3>  Raspberry pi b </h3><br>  A primeira dificuldade surgiu ao tentar inicializar um cluster no Raspberry Pi B: <br><br><pre> <code class="bash hljs">$ kubeadm init Illegal instruction</code> </pre> <br>  Acontece que o Kubernetes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">removeu o suporte ao ARMv6</a> .  Bem, eu também tenho CubbieBoard e Banana Pi. <br><br><h3>  Banana pi </h3><br>  Inicialmente, a mesma sequência de ações para o Banana Pi parecia ter mais sucesso; no entanto, o comando <code>kubeadm init</code> enquanto tentava esperar o plano de controle funcionar: <br><br><pre> <code class="plaintext hljs">error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster</code> </pre> <br>  Descobrindo com o <code>docker ps</code> que estava acontecendo com os contêineres, vi que o <code>kube-controller-manager</code> e o <code>kube-scheduler</code> estavam trabalhando por pelo menos 4-5 minutos, mas o <code>kube-api-server</code> se levantou há apenas 1-2 minutos: <br><br><pre> <code class="bash hljs">$ docker ps CONTAINER ID COMMAND CREATED STATUS de22427ad594 <span class="hljs-string"><span class="hljs-string">"kube-apiserver --au…"</span></span> About a minute ago Up About a minute dc2b70dd803e <span class="hljs-string"><span class="hljs-string">"kube-scheduler --ad…"</span></span> 5 minutes ago Up 5 minutes 60b6cc418a66 <span class="hljs-string"><span class="hljs-string">"kube-controller-man…"</span></span> 5 minutes ago Up 5 minutes 1e1362a9787c <span class="hljs-string"><span class="hljs-string">"etcd --advertise-cl…"</span></span> 5 minutes ago Up 5 minutes</code> </pre> <br>  Obviamente, o <code>api-server</code> estava morrendo ou o processo de estrôncio estava matando e reiniciando. <br><br>  Verificando os logs, vi procedimentos de inicialização muito padrão - houve um registro do início da escuta da porta segura e uma longa pausa antes do aparecimento de vários erros nos handshakes TLS: <br><br><pre> <code class="plaintext hljs">20:06:48.604881 naming_controller.go:284] Starting NamingConditionController 20:06:48.605031 establishing_controller.go:73] Starting EstablishingController 20:06:50.791098 log.go:172] http: TLS handshake error from 192.168.1.155:50280: EOF 20:06:51.797710 log.go:172] http: TLS handshake error from 192.168.1.155:50286: EOF 20:06:51.971690 log.go:172] http: TLS handshake error from 192.168.1.155:50288: EOF 20:06:51.990556 log.go:172] http: TLS handshake error from 192.168.1.155:50284: EOF 20:06:52.374947 log.go:172] http: TLS handshake error from 192.168.1.155:50486: EOF 20:06:52.612617 log.go:172] http: TLS handshake error from 192.168.1.155:50298: EOF 20:06:52.748668 log.go:172] http: TLS handshake error from 192.168.1.155:50290: EOF</code> </pre> <br>  E logo depois disso, o servidor encerra seu trabalho.  A pesquisa no Google levou a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">esse problema</a> , indicando uma possível razão para a operação lenta de algoritmos criptográficos em alguns dispositivos ARM. <br><br>  Fui mais longe e pensei que talvez o <code>api-server</code> recebendo muitas solicitações repetidas do <code>scheduler</code> e do <code>controller-manager</code> . <br><br>  A remoção desses arquivos do diretório de manifesto instruirá o kubelet a interromper a execução dos pods correspondentes: <br><br><pre> <code class="bash hljs">mkdir /etc/kubernetes/manifests.bak mv /etc/kubernetes/manifests/kube-scheduler.yaml /etc/kubernetes/manifests.bak/ mv /etc/kubernetes/manifests/kube-controller-mananger.yaml /etc/kubernetes/manifests.bak/</code> </pre> <br>  A visualização dos logs mais recentes <code>api-server</code> mostrou que agora o processo foi além, no entanto, ele ainda morreu após cerca de 2 minutos.  Lembrei-me de que o manifesto poderia conter uma amostra de vivacidade com tempos limite com valores muito baixos para um dispositivo tão lento. <br><br>  Portanto, verifiquei <code>/etc/kubernetes/manifests/kube-api-server.yaml</code> - e nele, é claro ... <br><br><pre> <code class="plaintext hljs">livenessProbe: failureThreshold: 8 httpGet: host: 192.168.1.155 path: /healthz port: 6443 scheme: HTTPS initialDelaySeconds: 15 timeoutSeconds: 15</code> </pre> <br>  O pod foi <code>timeoutSeconds</code> após 135 segundos ( <code>timeoutSeconds</code> + <code>timeoutSeconds</code> * <code>failureThreshold</code> ).  Aumentar <code>initialDelaySeconds</code> para 120 ... <br><br>  <b>Sucesso!</b>  Bem, ainda ocorrem erros de handshake (presumivelmente do kubelet), no entanto, o lançamento ainda ocorreu: <br><br><pre> <code class="plaintext hljs">20:06:54.957236 log.go:172] http: TLS handshake error from 192.168.1.155:50538: EOF 20:06:55.004865 log.go:172] http: TLS handshake error from 192.168.1.155:50384: EOF 20:06:55.118343 log.go:172] http: TLS handshake error from 192.168.1.155:50292: EOF 20:06:55.252586 cache.go:39] Caches are synced for autoregister controller 20:06:55.253907 cache.go:39] Caches are synced for APIServiceRegistrationController controller 20:06:55.545881 controller_utils.go:1034] Caches are synced for crd-autoregister controller ... 20:06:58.921689 storage_rbac.go:187] created clusterrole.rbac.authorization.k8s.io/cluster-admin 20:06:59.049373 storage_rbac.go:187] created clusterrole.rbac.authorization.k8s.io/system:discovery 20:06:59.214321 storage_rbac.go:187] created clusterrole.rbac.authorization.k8s.io/system:basic-user</code> </pre> <br>  Quando o <code>api-server</code> se levantou, mudei os arquivos YAML para o controlador e o planejador de volta ao diretório do manifesto, após o qual eles também começaram normalmente. <br><br>  Agora é hora de garantir que o download seja bem-sucedido se você deixar todos os arquivos no diretório de origem: basta alterar o atraso permitido na inicialização do <code>livenessProbe</code> ? <br><br><pre> <code class="plaintext hljs">20:29:33.306983 reflector.go:134] k8s.io/client-go/informers/factory.go:132: Failed to list *v1.Service: Get https://192.168.1.155:6443/api/v1/services?limit=500&amp;resourceVersion=0: dial tcp 192.168.1.155:6443: i/o timeout 20:29:33.434541 reflector.go:134] k8s.io/client-go/informers/factory.go:132: Failed to list *v1.ReplicationController: Get https://192.168.1.155:6443/api/v1/replicationcontrollers?limit=500&amp;resourceVersion=0: dial tcp 192.168.1.155:6443: i/o timeout 20:29:33.435799 reflector.go:134] k8s.io/client-go/informers/factory.go:132: Failed to list *v1.PersistentVolume: Get https://192.168.1.155:6443/api/v1/persistentvolumes?limit=500&amp;resourceVersion=0: dial tcp 192.168.1.155:6443: i/o timeout 20:29:33.477405 reflector.go:134] k8s.io/client-go/informers/factory.go:132: Failed to list *v1beta1.PodDisruptionBudget: Get https://192.168.1.155:6443/apis/policy/v1beta1/poddisruptionbudgets?limit=500&amp;resourceVersion=0: dial tcp 192.168.1.155:6443: i/o timeout 20:29:33.493660 reflector.go:134] k8s.io/client-go/informers/factory.go:132: Failed to list *v1.PersistentVolumeClaim: Get https://192.168.1.155:6443/api/v1/persistentvolumeclaims?limit=500&amp;resourceVersion=0: dial tcp 192.168.1.155:6443: i/o timeout 20:29:37.974938 controller_utils.go:1027] Waiting for caches to sync for scheduler controller 20:29:38.078558 controller_utils.go:1034] Caches are synced for scheduler controller 20:29:38.078867 leaderelection.go:205] attempting to acquire leader lease kube-system/kube-scheduler 20:29:38.291875 leaderelection.go:214] successfully acquired lease kube-system/kube-scheduler</code> </pre> <br>  Sim, tudo funciona, embora esses dispositivos antigos, aparentemente, não tenham a intenção de iniciar o plano de controle, uma vez que conexões TLS repetidas causam freios significativos.  De uma forma ou de outra - é recebida uma instalação funcional do K8s no ARM!  Vamos mais longe ... <br><br><h3>  Montagem RAID </h3><br>  Como os cartões SD não são adequados para gravação a longo prazo, decidi usar um armazenamento mais confiável para as partes mais voláteis do sistema de arquivos - nesse caso, RAID.  4 seções foram destacadas nele: <br><br><ul><li>  50 GB; </li><li>  2 × 20 GB; </li><li>  3,9 Tb. </li></ul><br>  Ainda não propus uma finalidade específica para partições de 20 gigabytes, mas queria deixar oportunidades adicionais para o futuro. <br><br>  No <code>/etc/fstab</code> para a partição de 50 GB, o ponto de montagem foi especificado como <code>/mnt/root</code> e para 3,9 TB - <code>/mnt/raid</code> .  Depois disso, montei os diretórios com o etcd e o docker na partição de 50 GB: <br><br><pre> <code class="plaintext hljs">UUID=655a39e8-9a5d-45f3-ae14-73b4c5ed50c3 /mnt/root ext4 defaults,rw,user,auto,exec 0 0 UUID=0633df91-017c-4b98-9b2e-4a0d27989a5c /mnt/raid ext4 defaults,rw,user,auto 0 0 /mnt/root/var/lib/etcd /var/lib/etcd none defaults,bind 0 0 /mnt/root/var/lib/docker /var/lib/docker none defaults,bind 0 0</code> </pre> <br><h3>  Chegada ROC-RK3328-CC </h3><br>  Quando a nova placa foi entregue, instalei os componentes necessários para os K8s <i>(consulte o início do artigo)</i> e <code>kubeadm init</code> .  Alguns minutos de espera são o sucesso e a saída do comando <code>join</code> para executar em outros nós. <br><br>  Ótimo!  Sem problemas com intervalos. <br><br>  E como o RAID também será usado nesta placa, as montagens precisarão ser configuradas novamente.  Para resumir todas as etapas: <br><br><h4>  1. Monte discos no / etc / fstab </h4><br><pre> <code class="plaintext hljs">UUID=655a39e8-9a5d-45f3-ae14-73b4c5ed50c3 /mnt/root ext4 defaults,rw,user,auto,exec 0 0 UUID=0633df91-017c-4b98-9b2e-4a0d27989a5c /mnt/raid ext4 defaults,rw,user,auto 0 0 /mnt/root/var/lib/etcd /var/lib/etcd none defaults,bind 0 0 /mnt/root/var/lib/docker /var/lib/docker none defaults,bind 0 0</code> </pre> <br><h4>  2. Instalando os binários Docker e K8s </h4><br><pre> <code class="bash hljs">curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh</code> </pre> <br><pre> <code class="bash hljs">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"deb http://apt.kubernetes.io/ kubernetes-xenial main"</span></span> &gt; /etc/apt/sources.list.d/kubernetes.list apt-get update apt-get install -y kubelet=1.13.1-00 kubectl=1.13.1-00 kubeadm=1.13.1-00</code> </pre> <br><h4>  3. Configurando um nome de host exclusivo (importante como muitos nós foram adicionados) </h4><br><pre> <code class="bash hljs">hostnamectl <span class="hljs-built_in"><span class="hljs-built_in">set</span></span>-hostname k8s-master-1</code> </pre> <br><h4>  4. Inicialização do Kubernetes </h4><br>  Eu omito a fase com o plano de controle, porque eu quero poder planejar pods normais neste nó: <br><br><pre> <code class="bash hljs">kubeadm init --skip-phases mark-control-plane</code> </pre> <br><h4>  5. Instalando o Plug-in de Rede </h4><br>  As informações sobre isso no artigo Hypriot eram um pouco datadas, porque o plug-in de rede Weave agora também é <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">suportado no ARM</a> : <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">export</span></span> KUBECONFIG=/etc/kubernetes/admin.conf kubectl apply -f <span class="hljs-string"><span class="hljs-string">"https://cloud.weave.works/k8s/net?k8s-version=</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$(kubectl version | base64 | tr -d '\n')</span></span></span><span class="hljs-string">"</span></span></code> </pre> <br><h4>  6. Adicionando rótulos de host </h4><br>  Neste nó, vou iniciar o servidor NAS, por isso vou marcá-lo com rótulos para possível uso futuro no planejador: <br><br><pre> <code class="bash hljs">kubectl label nodes k8s-master-1 marshallbrekka.raid=<span class="hljs-literal"><span class="hljs-literal">true</span></span> kubectl label nodes k8s-master-1 marshallbrekka.network=gigabit</code> </pre> <br><h3>  Conectando outros nós ao cluster </h3><br>  A configuração de outros dispositivos (Banana Pi, CubbieBoard) também foi fácil.  Para eles, é necessário repetir as 3 primeiras etapas (alterando as configurações para montar discos / mídia flash, dependendo da disponibilidade) e executar o comando <code>kubeadm join</code> vez do <code>kubeadm init</code> . <br><br><h2>  Localizando contêineres do Docker para ARM </h2><br>  A maioria dos contêineres do Docker necessários é criada normalmente em um Mac, mas para o ARM é um pouco mais complicado.  Tendo encontrado muitos artigos sobre como usar o QEMU para esses fins, cheguei à <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">conclusão de</a> que a maioria dos aplicativos de que eu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">preciso</a> já está montada e muitos deles estão disponíveis no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">servidor linux</a> . <br><br><h2>  Próximas etapas </h2><br>  Ainda não obtendo a configuração inicial dos dispositivos em uma forma automatizada / com scripts, como eu gostaria, pelo menos compus um conjunto de comandos básicos (montagens, <code>kubeadm</code> <code>docker</code> e <code>kubeadm</code> ) e os documentei no repositório Git.  O restante dos aplicativos usados ​​também recebeu configurações YAML para K8s armazenadas no mesmo repositório; portanto, agora é muito fácil obter a configuração necessária do zero. <br><br>  No futuro, gostaria de obter o seguinte: <br><br><ol><li>  Tornar os sites principais altamente disponíveis </li><li>  adicione monitoramento / notificações para saber sobre falhas em qualquer componente; </li><li>  Altere as configurações de DCHP do roteador para usar um servidor DNS do cluster para simplificar a descoberta de aplicativos (quem deseja se lembrar dos endereços IP internos?); </li><li>  execute o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MetalLB</a> para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">encaminhar os</a> serviços de cluster para uma rede privada (DNS, etc.). </li></ol><br><br><h2>  PS do tradutor </h2><br>  Leia também em nosso blog: <br><br><ul><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Dicas e truques do Kubernetes: sobre a alocação de nós e a carga na aplicação web</a> ”; </li><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Dicas e truques do Kubernetes: acesso a sites de desenvolvimento</a> ”; </li><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Dicas e truques do Kubernetes: acelerando a inicialização de grandes bancos de dados</a> ”; </li><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">11 maneiras de (não) se tornar uma vítima do Kubernetes Hacking</a> ”; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Brincar com o Kubernetes é um serviço para conhecer os K8s na prática</a> ." </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt435526/">https://habr.com/ru/post/pt435526/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt435510/index.html">Microeletrônica, neurofisiologia e aprendizado de máquina agitam, mas não misturam</a></li>
<li><a href="../pt435512/index.html">Royole Developers Show Smartphone dobrável flexível</a></li>
<li><a href="../pt435514/index.html">Na Rússia, eles estão desenvolvendo um processador para acelerar redes neurais</a></li>
<li><a href="../pt435520/index.html">Nós escrevemos nossa linguagem de programação, parte 3: Arquitetura do tradutor. Análise de estruturas de linguagem e expressões matemáticas</a></li>
<li><a href="../pt435522/index.html">Instantâneos de eventos no Axonframework 3, melhorando o desempenho</a></li>
<li><a href="../pt435528/index.html">5 razões para o sucesso: por que a Amazon se tornou a empresa mais cara do mundo</a></li>
<li><a href="../pt435530/index.html">Assinaturas pagas - Dependência da conexão automática em um dispositivo móvel</a></li>
<li><a href="../pt435532/index.html">Tornado vs Aiohttp: uma jornada pela natureza de estruturas assíncronas</a></li>
<li><a href="../pt435534/index.html">Ciência de dados: livros básicos</a></li>
<li><a href="../pt435536/index.html">Robôs humanóides: benefícios e problemas de mecanismos antropomórficos</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>