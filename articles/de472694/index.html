<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§úüèª üëäüèΩ üå©Ô∏è Mehr als Ceph: MCS Block Cloud Storage ü§∑üèΩ üë®üèæ‚Äçüíª üë©üèΩ‚Äçüé®</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Fliegender Wagen, Afu Chan 

 Ich arbeite bei Mail.ru Cloud Solutons als Architekt und Entwickler, einschlie√ülich meiner Cloud. Es ist bekannt, dass e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Mehr als Ceph: MCS Block Cloud Storage</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/472694/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/wx/av/di/wxavdimhgftb4rj-bjzsjjbdwts.jpeg"></div> <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fliegender Wagen, Afu Chan</a></i> <br><br>  Ich arbeite bei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Mail.ru Cloud Solutons als</a> Architekt und Entwickler, einschlie√ülich meiner Cloud.  Es ist bekannt, dass eine verteilte Cloud-Infrastruktur einen produktiven Blockspeicher ben√∂tigt, von dem der Betrieb der mit ihnen erstellten PaaS-Dienste und -L√∂sungen abh√§ngt. <br><br>  Anfangs haben wir bei der Bereitstellung einer solchen Infrastruktur nur Ceph verwendet, aber nach und nach hat sich der Blockspeicher weiterentwickelt.  Wir wollten, dass <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">unsere Datenbanken</a> , der Dateispeicher und verschiedene Dienste mit maximaler Leistung funktionieren. Deshalb haben wir lokalisierte Speicher hinzugef√ºgt und die erweiterte Ceph-√úberwachung eingerichtet. <br><br>  Ich werde Ihnen sagen, wie es war - vielleicht ist diese Geschichte, die Probleme, auf die wir gesto√üen sind, und unsere L√∂sungen f√ºr diejenigen n√ºtzlich, die auch Ceph verwenden.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hier ist</a> √ºbrigens <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">eine</a> Videoversion dieses Berichts. <br><a name="habracut"></a><br><h2>  Von DevOps-Prozessen zu Ihrer eigenen Cloud </h2><br>  DevOps-Praktiken zielen darauf ab, das Produkt so schnell wie m√∂glich einzuf√ºhren: <br><br><ul><li>  Automatisierung von Prozessen - der gesamte Lebenszyklus: Montage, Pr√ºfung, Lieferung an den Test und produktiv.  Automatisieren Sie Prozesse schrittweise, beginnend mit kleinen Schritten. <br></li><li>  Infrastruktur als Code ist ein Modell, wenn der Infrastrukturkonfigurationsprozess dem Softwareprogrammierungsprozess √§hnlich ist.  Zuerst testen sie das Produkt, das Produkt hat bestimmte Anforderungen an die Infrastruktur und die Infrastruktur muss getestet werden.  In diesem Stadium, wenn W√ºnsche f√ºr sie auftauchen, m√∂chte ich die Infrastruktur ‚Äûoptimieren‚Äú - zuerst in der Testumgebung, dann im Lebensmittelgesch√§ft.  In der ersten Phase kann dies manuell erfolgen, dann geht es jedoch zur Automatisierung √ºber - zum Modell ‚ÄûInfrastruktur als Code‚Äú. <br></li><li>  Virtualisierung und Container - erscheinen im Unternehmen, wenn klar ist, dass Sie Prozesse auf eine industrielle Spur bringen m√ºssen, um neue Funktionen schneller und mit minimalem manuellen Eingriff einzuf√ºhren. <br></li></ul><br><img src="https://habrastorage.org/webt/wz/im/rw/wzimrwndijvqdzrmn6pjlunr_ci.jpeg">  <i>Die Architektur aller virtuellen Umgebungen ist √§hnlich: Gastcomputer mit Containern, Anwendungen, √∂ffentlichen und privaten Netzwerken, Speicher.</i> <br><br>  Allm√§hlich werden immer mehr Dienste in der virtuellen Infrastruktur bereitgestellt, die in und um DevOps-Prozesse aufgebaut ist, und die virtuelle Umgebung wird nicht nur zu einem Test (der f√ºr Entwicklung und Test verwendet wird), sondern auch produktiv. <br><br>  In der Regel werden sie in der Anfangsphase von den einfachsten grundlegenden Automatisierungstools umgangen.  Da jedoch neue Tools angezogen werden, muss fr√ºher oder sp√§ter eine vollwertige Cloud-Plattform bereitgestellt werden, um die fortschrittlichsten Tools wie Terraform verwenden zu k√∂nnen. <br><br>  In dieser Phase verwandelt sich die virtuelle Infrastruktur aus ‚ÄûHypervisoren, Netzwerken und Speicher‚Äú in eine vollwertige Cloud-Infrastruktur mit entwickelten Tools und Komponenten f√ºr die Orchestrierung von Prozessen.  Dann erscheint eine eigene Cloud, in der die Prozesse zum Testen und automatisierten Bereitstellen von Updates f√ºr vorhandene Dienste sowie zum Bereitstellen neuer Dienste stattfinden. <br><br>  Der zweite Weg zu Ihrer eigenen Cloud ist die Notwendigkeit, nicht von externen Ressourcen und externen Dienstanbietern abh√§ngig zu sein, dh eine gewisse technische Unabh√§ngigkeit f√ºr Ihre eigenen Dienste zu gew√§hrleisten. <br><br><img src="https://habrastorage.org/webt/5y/wb/pm/5ywbpmuwrruww-bfep4f34p4eja.jpeg">  <i>Die erste Cloud sieht fast wie eine virtuelle Infrastruktur aus - ein Hypervisor (eine oder mehrere), virtuelle Maschinen mit Containern, gemeinsam genutzter Speicher: Wenn Sie die Cloud nicht auf propriet√§ren L√∂sungen aufbauen, handelt es sich normalerweise um Ceph oder DRBD.</i> <br><br><h2>  Ausfallsicherheit und Leistung der Private Cloud </h2><br>  Die Cloud w√§chst, das Gesch√§ft h√§ngt immer mehr davon ab, das Unternehmen fordert zunehmend mehr Zuverl√§ssigkeit. <br><br>  Hier wird die verteilte Cloud der verteilten Cloud-Infrastruktur hinzugef√ºgt: zus√§tzliche Punkte, an denen sich die Ger√§te befinden.  Die Cloud verwaltet zwei, drei oder mehr Installationen, die eine fehlertolerante L√∂sung bieten. <br><br>  Gleichzeitig werden Daten von allen Standorten ben√∂tigt, und es gibt ein Problem: Innerhalb eines Standorts treten keine gro√üen Verz√∂gerungen bei der Daten√ºbertragung auf, aber zwischen Standorten werden die Daten langsamer √ºbertragen. <br><br><img src="https://habrastorage.org/webt/q6/__/3m/q6__3mcgjw-wsknmxy-etas8oky.jpeg">  <i>Installationsorte und gemeinsamer Speicher.</i>  <i>Rote Rechtecke sind Engp√§sse auf Netzwerkebene.</i> <br><br>  Der externe Teil der Infrastruktur ist aus Sicht des Verwaltungsnetzwerks oder des √∂ffentlichen Netzwerks nicht so ausgelastet, aber im internen Netzwerk sind die √ºbertragenen Datenmengen viel gr√∂√üer.  Und in verteilten Systemen beginnen Probleme, die sich in einer langen Servicezeit √§u√üern.  Wenn der Client zu einer Gruppe von Speicherknoten kommt, m√ºssen die Daten sofort in die zweite Gruppe repliziert werden, damit die √Ñnderungen nicht verloren gehen. <br><br>  F√ºr einige Prozesse ist die Datenreplikationslatenz akzeptabel, aber in F√§llen wie der Transaktionsverarbeitung k√∂nnen Transaktionen nicht verloren gehen.  Wenn die asynchrone Replikation verwendet wird, tritt eine Zeitverz√∂gerung auf, die zum Verlust eines Teils der Daten f√ºhren kann, wenn einer der ‚ÄûSchw√§nze‚Äú des Speichersystems (Datenspeichersystem) ausf√§llt.  Wenn die synchrone Replikation verwendet wird, erh√∂ht sich die Servicezeit. <br><br>  Es ist auch ganz nat√ºrlich, dass mit zunehmender Verarbeitungszeit (Latenz) des Speichers die Datenbanken langsamer werden und negative Auswirkungen bek√§mpft werden m√ºssen. <br><br>  In unserer Cloud suchen wir nach ausgewogenen L√∂sungen, um Zuverl√§ssigkeit und Leistung zu gew√§hrleisten.  Die einfachste Technik besteht darin, die Daten zu lokalisieren - und dann haben wir zus√§tzliche lokalisierte Ceph-Cluster hinzugef√ºgt. <br><br><img src="https://habrastorage.org/webt/qf/_g/k3/qf_gk33i3esefh0xgnjuhegczia.jpeg">  <i>Die gr√ºne Farbe zeigt zus√§tzliche lokalisierte Ceph-Cluster an.</i> <br><br>  Der Vorteil einer solch komplexen Architektur besteht darin, dass diejenigen, die eine schnelle Dateneingabe / -ausgabe ben√∂tigen, lokalisierte Speicher verwenden k√∂nnen.  Daten, f√ºr die die vollst√§ndige Verf√ºgbarkeit an zwei Standorten von entscheidender Bedeutung ist, befinden sich in einem verteilten Cluster.  Es funktioniert langsamer - aber die darin enthaltenen Daten werden auf beide Sites repliziert.  Wenn die Leistung nicht ausreicht, k√∂nnen Sie lokalisierte Ceph-Cluster verwenden. <br><br>  Die meisten √∂ffentlichen und privaten Clouds erreichen schlie√ülich ungef√§hr das gleiche Arbeitsmuster, wenn die Last je nach Anforderungen in verschiedenen Arten von Speichern (verschiedenen Arten von Festplatten) bereitgestellt wird. <br><br><h2>  Ceph-Diagnose: Aufbau einer √úberwachung </h2><br>  Bei der Bereitstellung und dem Start der Infrastruktur war es an der Zeit, deren Funktion sicherzustellen und Zeit und Anzahl der Ausf√§lle zu minimieren.  Daher war der n√§chste Schritt bei der Entwicklung der Infrastruktur der Aufbau von Diagnose und √úberwachung. <br><br>  Betrachten Sie die √úberwachungsaufgabe durchgehend - wir haben einen Stapel von Anwendungen in einer virtuellen Cloud-Umgebung: eine Anwendung, ein Gastbetriebssystem, ein Blockger√§t, die Treiber dieses Blockger√§ts auf einem Hypervisor, ein Speichernetzwerk und das eigentliche Speichersystem (Speichersystem).  Und all dies wurde noch nicht durch √úberwachung abgedeckt. <br><br><img src="https://habrastorage.org/webt/z1/nk/nk/z1nknkrnannwnfn2fa7jwf6w0jq.jpeg">  <i>Elemente, die nicht √ºberwacht werden.</i> <br><br>  Die √úberwachung erfolgt in mehreren Schritten, wir beginnen mit Festplatten.  Wir erhalten die Anzahl der Lese- / Schreibvorg√§nge, eine gewisse Genauigkeit, die Servicezeit (Megabyte pro Sekunde), die Warteschlangentiefe und andere Merkmale und erfassen SMART √ºber den Status der Festplatten. <br><br><img src="https://habrastorage.org/webt/2m/b_/7s/2mb_7s2vda6qq6dtem9iwsnlup8.jpeg">  <i>Die erste Phase: Wir behandeln √úberwachungsdisketten.</i> <br><br>  Die Festplatten√ºberwachung reicht nicht aus, um ein vollst√§ndiges Bild der Vorg√§nge im System zu erhalten.  Daher √ºberwachen wir ein kritisches Element der Infrastruktur - das Netzwerk des Speichersystems.  Es gibt tats√§chlich zwei davon - den internen Cluster und den Client, die Speichercluster mit Hypervisoren verbinden.  Hier erhalten wir die Datenpaket√ºbertragungsraten (Megabyte pro Sekunde, Pakete pro Sekunde), die Gr√∂√üe der Netzwerkwarteschlangen, Puffer und m√∂glicherweise Datenpfade. <br><br><img src="https://habrastorage.org/webt/pn/dd/wu/pnddwuxah0r9sngg8awzcxhzlom.jpeg">  <i>Zweite Stufe: Netzwerk√ºberwachung.</i> <br><br>  Sie h√∂ren oft damit auf, aber dies ist nicht m√∂glich, da der gr√∂√üte Teil der Infrastruktur noch nicht durch √úberwachung geschlossen wurde. <br><br>  Der gesamte verteilte Speicher, der in √∂ffentlichen und privaten Clouds verwendet wird, ist SDS, softwaredefinierter Speicher.  Sie k√∂nnen auf den L√∂sungen eines bestimmten Anbieters, Open Source-L√∂sungen, implementiert werden. Sie k√∂nnen mithilfe eines Stapels bekannter Technologien selbst etwas tun.  Es handelt sich jedoch immer um Sicherheitsdatenbl√§tter, und die Arbeit dieser Softwareteile muss √ºberwacht werden. <br><br><img src="https://habrastorage.org/webt/mt/o5/cb/mto5cbnz456tvs6mg_6i-b5lm8o.jpeg">  <i>Dritter Schritt: √úberwachung des Speicherd√§mons.</i> <br><br>  Die meisten Ceph-Betreiber verwenden Daten, die von Ceph-√úberwachungs- und Kontrolld√§monen (Monitor und Manager, auch bekannt als mgr) erfasst wurden.  Anfangs gingen wir den gleichen Weg, stellten jedoch sehr schnell fest, dass diese Informationen nicht ausreichten - Warnungen vor h√§ngenden Anfragen erscheinen zu sp√§t: Die Anfrage hing 30 Sekunden lang, erst dann sahen wir sie.  Solange es um die √úberwachung geht, w√§hrend die √úberwachung den Alarm ausl√∂st, vergehen mindestens drei Minuten.  Im besten Fall bedeutet dies, dass ein Teil des Speichers und der Anwendungen drei Minuten lang inaktiv ist. <br><br>  Nat√ºrlich haben wir uns entschlossen, die √úberwachung zu erweitern und sind zum Hauptelement von Ceph √ºbergegangen - dem OSD-Daemon.  Durch die √úberwachung des Object Storage-Daemons erhalten wir die ungef√§hre Betriebszeit, wie sie vom OSD angezeigt wird, sowie Statistiken zu blockierten Anforderungen - wer, wann, in welchem ‚Äã‚ÄãPG, wie lange. <br><br><h2>  Warum nur Ceph nicht genug ist und was man dagegen tun kann </h2><br>  Ceph allein reicht aus mehreren Gr√ºnden nicht aus.  Zum Beispiel haben wir einen Client mit einem Datenbankprofil.  Er stellte alle Datenbanken im All-Flash-Cluster bereit, die Latenz der dort ausgegebenen Vorg√§nge passte zu ihm, es gab jedoch Beschwerden √ºber Ausfallzeiten. <br><br>  Mit dem √úberwachungssystem k√∂nnen Sie nicht sehen, was in den Clients der virtuellen Umgebung geschieht.  Um das Problem zu identifizieren, haben wir daher die erweiterte Analyse verwendet, die mit dem Dienstprogramm blktrace von seiner virtuellen Maschine angefordert wurde. <br><br><img src="https://habrastorage.org/webt/uj/ch/vk/ujchvkl3ozoarbjooedieuqzf80.jpeg">  <i>Das Ergebnis einer erweiterten Analyse.</i> <br><br>  Die Analyseergebnisse enthalten Operationen, die mit den Flags W und WS gekennzeichnet sind.  Das W-Flag ist ein Datensatz, das WS-Flag ist ein synchroner Datensatz, der darauf wartet, dass das Ger√§t den Vorgang abschlie√üt.  Wenn wir mit Datenbanken arbeiten, haben fast alle SQL-Datenbanken einen Engpass - WAL (Write-Ahead-Protokoll). <br><br>  Die Datenbank schreibt immer zuerst Daten in das Protokoll, erh√§lt eine Best√§tigung von der Festplatte mit leeren Puffern und schreibt dann die Daten in die Datenbank selbst.  Wenn sie keine Best√§tigung eines Puffer-Resets erhalten hat, glaubt sie, dass ein Power-Reset eine vom Client best√§tigte Transaktion l√∂schen kann.  Dies ist f√ºr die Datenbank nicht akzeptabel, daher wird "SYNC / FLUSH schreiben" angezeigt und anschlie√üend die Daten geschrieben.  Wenn die Protokolle voll sind, erfolgt ihr Wechsel, und alles, was in den Seitencache gelangt ist, wird ebenfalls zwangsweise geflasht.  <i>Hinzugef√ºgt: Es gibt kein Zur√ºcksetzen im Bild selbst, dh Operationen mit dem Pre-Flush-Flag.</i>  <i>Sie sehen aus wie FWS - Pre-Flush + Write + Sync oder FWSF - Pre-Flush + Write + Sync + FUA</i> <br><br>  Wenn ein Client viele kleine Transaktionen hat, werden praktisch alle E / A zu einer sequentiellen Kette: Schreiben - Sp√ºlen - Schreiben - Sp√ºlen.  Da Sie mit der Datenbank nichts anfangen k√∂nnen, beginnen wir mit dem Speichersystem zu arbeiten.  In diesem Moment verstehen wir, dass die F√§higkeiten von Ceph nicht ausreichen. <br><br>  Zu diesem Zeitpunkt bestand die beste L√∂sung f√ºr uns darin, kleine und schnelle lokale Repositorys hinzuzuf√ºgen, die nicht mit Ceph-Tools implementiert wurden (wir haben die Funktionen im Grunde ersch√∂pft).  Und wir verwandeln Cloud-Speicher in etwas mehr als Ceph.  In unserem Fall haben wir viele lokale Storys hinzugef√ºgt (lokal in Bezug auf das Rechenzentrum, nicht den Hypervisor). <br><br><img src="https://habrastorage.org/webt/4o/hh/rr/4ohhrre0loyyk4gmxijlpwy6pry.jpeg">  <i>Zus√§tzliche lokalisierte Repositorys Ziel A und B.</i> <br><br>  Die Servicezeit eines solchen lokalen Speichers betr√§gt ungef√§hr 0,3 ms pro Stream.  Wenn es in einem anderen Rechenzentrum liegt, arbeitet es langsamer - mit einer Leistung von ca. 0,7 ms.  Dies ist ein signifikanter Anstieg im Vergleich zu Ceph, das 1,2 ms produziert und √ºber Rechenzentren verteilt ist - 2 ms.  Die Leistung solcher kleinen Fabriken, von denen wir mehr als ein Dutzend haben, betr√§gt ungef√§hr 100.000 pro Modul, 100.000 IOPS pro Datensatz. <br><br>  Nach einer solchen √Ñnderung der Infrastruktur dr√ºckt unsere Cloud weniger als eine Million IOPS zum Schreiben oder etwa zwei bis drei Millionen IOPS zum Lesen f√ºr alle Kunden aus: <br><br><img src="https://habrastorage.org/webt/hw/zq/aj/hwzqajyzsmewq-s2h6iw8mnf4bk.jpeg"><br><br>  Es ist wichtig zu beachten, dass diese Art von Speicher nicht die Haupterweiterungsmethode ist. Wir setzen die Hauptwette auf Ceph und das Vorhandensein von schnellem Speicher ist nur f√ºr Dienste wichtig, die eine Antwortzeit auf die Festplatte erfordern. <br><br><h2>  Neue Iterationen: Code- und Infrastrukturverbesserungen </h2><br>  Alle unsere Geschichten sind gemeinsame Ressourcen.  F√ºr eine solche Infrastruktur m√ºssen wir <b>eine Service-Level-Richtlinie implementieren</b> : Wir m√ºssen ein bestimmtes <b>Service-Level</b> bereitstellen und d√ºrfen nicht zulassen, dass ein Client versehentlich oder absichtlich durch Deaktivieren des Speichers in einen anderen Client eingreift. <br><br>  Dazu mussten wir die Finalisierung und den nicht trivialen Roll-out durchf√ºhren - die iterative Lieferung an das Produktive. <br><br>  Dieser Roll-out unterschied sich von den √ºblichen DevOps-Praktiken, bei denen alle Prozesse: Montage, Test, Code-Roll-out, Neustart des Service, falls erforderlich, mit einem Klick auf eine Schaltfl√§che beginnen und dann alles funktioniert.  Wenn Sie DevOps-Praktiken in der Infrastruktur implementieren, bleibt diese bis zum ersten Fehler bestehen. <br><br>  Aus diesem Grund hat sich die ‚ÄûVollautomatisierung‚Äú im Infrastruktur-Team nicht besonders etabliert.  Nat√ºrlich gibt es einen bestimmten Ansatz f√ºr die Test- und Bereitstellungsautomatisierung - dieser wird jedoch immer kontrolliert und die Bereitstellung wird von den SRE-Ingenieuren des Cloud-Teams initiiert. <br><br>  Wir haben √Ñnderungen in mehreren Diensten eingef√ºhrt: im Cinder-Backend, im Cinder-Frontend (Cinder-Client) und im Nova-Dienst.  √Ñnderungen wurden in mehreren Iterationen angewendet - jeweils eine Iteration.  Nach der dritten Iteration wurden die entsprechenden √Ñnderungen auf die Gastcomputer der Clients angewendet: Jemand migrierte, jemand selbst startete die VM neu (Hard Reboot) oder geplante Migration, um die Hypervisoren zu bedienen. <br><br>  Das n√§chste Problem, das auftrat, waren die <b>Spr√ºnge in der Schreibgeschwindigkeit</b> .  Wenn wir mit Netzwerkspeicher arbeiten, betrachtet der Standardhypervisor das Netzwerk als langsam und speichert daher alle Daten zwischen.  Er schreibt schnell, bis zu mehreren zehn Megabyte, und beginnt dann, den Cache zu leeren.  Es gab viele unangenehme Momente wegen solcher Spr√ºnge. <br><br>  Wir haben festgestellt, dass beim Einschalten des Caches die Leistung der SSD um 15% und beim Ausschalten des Caches die Leistung der Festplatte um 35% sinkt.  Es bedurfte einer weiteren Entwicklung, der Einf√ºhrung der verwalteten Cache-Verwaltung, wenn das Caching explizit f√ºr jeden Festplattentyp zugewiesen wurde.  Dies erm√∂glichte es uns, SSD ohne Cache und HDD zu betreiben - mit einem Cache konnten wir daher die Leistung nicht mehr verlieren. <br><br>  Die Praxis, einem Produkt eine Entwicklung zu liefern, ist √§hnlich - Iterationen.  Wir haben den Code eingef√ºhrt, den D√§mon neu gestartet und dann bei Bedarf virtuelle Gastmaschinen neu gestartet oder migriert, die √Ñnderungen unterliegen sollten.  Die Client-VM wurde von der Festplatte migriert, der Cache wurde aktiviert - alles funktioniert, oder im Gegenteil, der Client wurde mit der SSD migriert, der Cache wurde deaktiviert - alles funktioniert. <br><br>  Das dritte Problem ist der <b>fehlerhafte Betrieb von virtuellen Maschinen, die von GOLD-Images auf der Festplatte bereitgestellt werden</b> . <br><br>  Es gibt viele solcher Clients, und die Besonderheit der Situation besteht darin, dass die Arbeit der VM von selbst angepasst wurde: Das Problem trat garantiert w√§hrend der Bereitstellung auf, wurde jedoch behoben, w√§hrend der Client den technischen Support erreichte.  Zuerst haben wir die Kunden gebeten, eine halbe Stunde zu warten, bis sich die VM stabilisiert hat, aber dann haben wir begonnen, an der Qualit√§t des Dienstes zu arbeiten. <br><br>  Im Verlauf der Forschung haben wir festgestellt, dass die F√§higkeiten unserer √úberwachungsinfrastruktur immer noch nicht ausreichen. <br><br><img src="https://habrastorage.org/webt/bl/tj/ft/bltjftnwasbvxd_iep-wwfsepkw.jpeg">  <i>Die √úberwachung schloss den blauen Teil, und das Problem befand sich ganz oben in der Infrastruktur und wurde nicht von der √úberwachung abgedeckt.</i> <br><br>  Wir haben begonnen, uns mit dem zu befassen, was in dem Teil der Infrastruktur geschieht, der nicht √ºberwacht wurde.  Zu diesem Zweck verwendeten wir die erweiterte Ceph-Diagnostik (oder besser gesagt eine der Varianten des Ceph-Clients - librbd).  Mithilfe von Automatisierungstools haben wir √Ñnderungen an der Ceph-Client-Konfiguration vorgenommen, um √ºber den Unix-Domain-Socket auf interne Datenstrukturen zuzugreifen, und haben begonnen, Statistiken von Ceph-Clients auf dem Hypervisor zu erstellen. <br><br>  Was haben wir gesehen?  Es wurden keine Statistiken zum Ceph-Cluster / OSD / Cluster angezeigt, sondern Statistiken zu jeder Festplatte der virtuellen Client-Maschine, deren Festplatten sich in Ceph befanden - dh Statistiken, die dem Ger√§t zugeordnet sind. <br><br><img src="https://habrastorage.org/webt/us/hr/5j/ushr5jfx40pvedrcqwpvj9ai-qy.jpeg">  <i>Erweiterte Ergebnisse der √úberwachungsstatistik.</i> <br><br>  Es war die erweiterte Statistik, die deutlich machte, dass das Problem nur auf Datentr√§gern auftritt, die von anderen Datentr√§gern geklont wurden. <br><br>  Als n√§chstes haben wir uns Statistiken √ºber Operationen angesehen, insbesondere Lese- / Schreiboperationen.  Es stellte sich heraus, dass die Belastung der Bilder der oberen Ebene relativ gering ist, und bei den ersten Bildern, von denen der Klon stammt, ist sie gro√ü, aber nicht ausgeglichen: eine gro√üe Menge an Lesevorg√§ngen ohne Aufzeichnung. <br><br>  Das Problem ist lokalisiert, jetzt wird eine L√∂sung ben√∂tigt - Code oder Infrastruktur? <br><br>  Mit dem Ceph-Code kann nichts gemacht werden, er ist ‚Äûschwer‚Äú.  Dar√ºber hinaus h√§ngt die Sicherheit der Kundendaten davon ab.  Aber es gibt ein Problem, das gel√∂st werden muss, und wir haben die Architektur des Repositorys ge√§ndert.  Der HDD-Cluster wurde zu einem Hybrid-Cluster. Der Festplatte wurde eine bestimmte Menge SSD hinzugef√ºgt. Anschlie√üend wurden die Priorit√§ten der OSD-D√§monen ge√§ndert, sodass die SSD immer Vorrang hatte und zum prim√§ren OSD innerhalb der Platzierungsgruppe (PG) wurde. <br><br>  Wenn der Client die virtuelle Maschine von der geklonten Festplatte bereitstellt, werden die Lesevorg√§nge auf die SSD √ºbertragen.  Infolgedessen wurde die Wiederherstellung von der Festplatte schnell und nur andere Clientdaten als das Original-Image werden auf die Festplatte geschrieben.  Wir haben eine Verdreifachung der Produktivit√§t fast kostenlos erhalten (im Vergleich zu den anf√§nglichen Kosten der Infrastruktur). <br><br><h1>  Warum Infrastruktur√ºberwachung wichtig ist </h1><br><ol><li>  Die √úberwachungsinfrastruktur muss maximal im gesamten Stapel enthalten sein, beginnend mit der virtuellen Maschine und endend mit der Festplatte.  W√§hrend ein Client, der eine private oder √∂ffentliche Cloud verwendet, zu seiner Infrastruktur gelangt und die erforderlichen Informationen bereitstellt, √§ndert sich das Problem oder wird an einen anderen Ort verschoben. <br></li><li>  Die √úberwachung des gesamten Hypervisors, der virtuellen Maschine oder des Containers "in seiner Gesamtheit" ergibt fast nichts.  Wir haben versucht, aus dem Netzwerkverkehr zu verstehen, was mit Ceph passiert - es ist nutzlos, die Daten fliegen mit hoher Geschwindigkeit (ab 500 Megabyte pro Sekunde), es ist √§u√üerst schwierig, die erforderlichen auszuw√§hlen.  Es wird eine ungeheure Menge an Festplatten erfordern, um solche Statistiken zu speichern, und viel Zeit, um sie zu analysieren. <br></li><li>       ,     - .   :     ,           ,   ‚Äî     ,        . <br></li><li>   ‚Äî     .   ,   .   ‚Äî     ,      .        ,   .             ‚Äî    ,   ,      . <br></li><li>  MCS Cloud Solutions ‚Äî  ,            .             . <br></li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de472694/">https://habr.com/ru/post/de472694/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de472682/index.html">Verlangsamung des Alterns mit Arzneimittelsynergien bei C. elegans</a></li>
<li><a href="../de472684/index.html">√úberraschen Sie fsync () PostgreSQL</a></li>
<li><a href="../de472686/index.html">Videostudio basierend auf i486</a></li>
<li><a href="../de472688/index.html">So funktioniert das Rendern von 3D-Spielen: Vertex-Verarbeitung</a></li>
<li><a href="../de472690/index.html">Was ist neu in Zabbix 4.4?</a></li>
<li><a href="../de472702/index.html">JH Regenwasser ‚ÄûWie man Katzen weidet‚Äú: Rassen von Programmierern und Merkmale ihrer Zucht</a></li>
<li><a href="../de472708/index.html">Imperva enth√ºllte technische Details des Cloud WAF-Hacks</a></li>
<li><a href="../de472714/index.html">Wo kann der Front-End-Mitarbeiter nach Arbeit suchen und nicht darauf hereinfallen: Telegramm, Slack und nicht nur</a></li>
<li><a href="../de472716/index.html">Spring Bean korrekt aus dem Anwendungskontext eines Drittanbieters abrufen</a></li>
<li><a href="../de472720/index.html">ERP funktioniert nicht ... Was ist die Alternative? oder p√ºnktlich. F√ºr Russland?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>