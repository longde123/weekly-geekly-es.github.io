<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>⛹🏼 👂🏻 🌮 Datenextraktion beim maschinellen Lernen 👬 🤽🏻 🧜🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Möchten Sie drei Data Mining-Methoden für Ihr nächstes ML-Projekt kennenlernen? Dann lesen Sie die Übersetzung des Rebecca Vickery-Artikels, der im To...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Datenextraktion beim maschinellen Lernen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/plarium/blog/460675/">  Möchten Sie drei Data Mining-Methoden für Ihr nächstes ML-Projekt kennenlernen?  Dann lesen Sie die Übersetzung des Rebecca Vickery-Artikels, der im Towards Data Science-Blog auf Medium veröffentlicht wurde!  Sie wird für Anfänger interessant sein. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ba/ce/h3/baceh3syebhbwo_3rvemwpfl8l8.jpeg"></div><br>  Das Abrufen von Qualitätsdaten ist der erste und wichtigste Schritt in jedem maschinellen Lernprojekt.  Data Science-Spezialisten verwenden häufig verschiedene Methoden, um Datensätze zu erhalten.  Sie können öffentlich verfügbare Daten sowie Daten verwenden, die über die API verfügbar sind oder aus verschiedenen Datenbanken stammen, kombinieren diese Methoden jedoch meistens. <br><br>  Der Zweck dieses Artikels besteht darin, einen kurzen Überblick über drei verschiedene Methoden zum Abrufen von Daten mit Python zu geben.  Ich werde Ihnen sagen, wie das mit dem Jupyter Notebook geht.  In meinem vorherigen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel habe</a> ich über die Anwendung einiger Befehle geschrieben, die im Terminal ausgeführt werden. <a name="habracut"></a><br><br><h3>  SQL </h3><br>  Wenn Sie Daten aus einer relationalen Datenbank abrufen müssen, arbeiten Sie höchstwahrscheinlich mit der SQL-Sprache.  Mit der SQLAlchemy-Bibliothek können Sie Ihren Laptop-Code den gängigsten Datenbanktypen zuordnen.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hier</a> finden Sie Informationen darüber, welche Datenbanken unterstützt werden und wie Sie an jeden Typ binden können. <br><br>  Mit der SQLAlchemy-Bibliothek können Sie Tabellen durchsuchen und Daten abfragen oder Rohabfragen schreiben.  Zum Binden an die Datenbank benötigen Sie eine URL mit Ihren Anmeldeinformationen.  Als Nächstes müssen Sie die Methode <code>create_engine</code> initialisieren, um die Verbindung herzustellen. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sqlalchemy <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> create_engine engine = create_engine(<span class="hljs-string"><span class="hljs-string">'dialect+driver://username:password@host:port/database'</span></span>)</code> </pre> <br>  Jetzt können Sie Datenbankabfragen schreiben und Ergebnisse erhalten. <br><br><pre> <code class="python hljs">connection = engine.connect() result = connection.execute(<span class="hljs-string"><span class="hljs-string">"select * from my_table"</span></span>)</code> </pre> <br><h3>  Schaben </h3><br>  Web Scraping wird verwendet, um Daten von Websites herunterzuladen und die erforderlichen Informationen von deren Seiten zu extrahieren.  Dafür stehen viele Python-Bibliotheken zur Verfügung, aber die einfachste ist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beautiful Soup</a> . <br><br>  Sie können das Paket über pip installieren. <br><br><pre> <code class="python hljs">pip install BeautifulSoup4</code> </pre> <br>  Schauen wir uns ein einfaches Beispiel für die Verwendung an.  Wir werden Beautiful Soup und die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Urllib-</a> Bibliothek verwenden, um Hotelnamen und Preise von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TripAdvisor</a> zu kratzen. <br><br>  Zuerst importieren wir alle Bibliotheken, mit denen wir arbeiten werden. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> bs4 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BeautifulSoup <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> urllib.request</code> </pre> <br>  Laden Sie nun den Inhalt der Seite, die wir verschrotten werden.  Ich möchte Daten zu Preisen für Hotels auf der griechischen Insel Kreta sammeln und die URL-Adresse mit einer Liste der Hotels an diesem Ort verwenden. <br><br><img src="https://habrastorage.org/webt/vt/jj/7i/vtjj7icfxauctiw_juiik0oqgfo.png"><br><br>  Der folgende Code definiert die URL als Variable und verwendet die urllib-Bibliothek zum Öffnen der Seite und die Beautiful Soup-Bibliothek zum Lesen und Zurückgeben der Ergebnisse in einem einfachen Format.  Ein Teil der Ausgabedaten wird unter dem Code angezeigt. <br><br><pre> <code class="python hljs">URL = <span class="hljs-string"><span class="hljs-string">'https://www.tripadvisor.co.uk/Hotels-g189413-Crete-Hotels.html'</span></span> page = urllib.request.urlopen(URL) soup = BeautifulSoup(page, <span class="hljs-string"><span class="hljs-string">'html.parser'</span></span>) print(soup.prettify())</code> </pre> <br><img src="https://habrastorage.org/webt/ul/n3/sn/uln3sn1bwjzjeft182k2dgbp7qo.png"><br><br>  Lassen Sie uns nun eine Liste mit den Namen der Hotels auf der Seite erhalten.  Wir werden die Funktion <code>find_all</code> , mit der Teile des für uns interessanten Dokuments extrahiert werden.  Sie können es mit der Funktion <code>find_all</code> unterschiedlich <code>find_all</code> , um eine einzelne Zeile, einen regulären Ausdruck oder eine Liste zu übergeben.  Sie können auch eines der Tag-Attribute herausfiltern - genau diese Methode werden wir anwenden.  Wenn Sie mit HTML-Tags und -Attributen noch nicht vertraut sind, finden Sie in diesem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel</a> eine kurze Übersicht. <br><br>  Um zu verstehen, wie der Zugriff auf die Daten im Tag am besten bereitgestellt werden kann, müssen Sie den Code für dieses Element auf der Seite überprüfen.  Wir finden den Code für den Hotelnamen, indem wir mit der rechten Maustaste auf den Namen in der Liste klicken, wie in der folgenden Abbildung gezeigt. <br><br><img src="https://habrastorage.org/webt/uh/k5/zs/uhk5zsci0kajsubpkghjff3qnye.png"><br><br>  Nach dem Klicken auf <code>inspect</code> Elementcode angezeigt und der Abschnitt mit dem Namen des Hotels wird hervorgehoben. <br><br><img src="https://habrastorage.org/webt/pc/fh/ms/pcfhmsrk8knqsjzr2lzsif2d2j4.png"><br><br>  Wir sehen, dass der Name des Hotels der einzige Text in der Klasse mit dem Namen <code>listing_title</code> .  Nach der Klasse kommen der Code und der Name dieses Attributs zur Funktion <code>find_all</code> sowie zum Tag <code>div</code> . <br><br><pre> <code class="python hljs">content_name = soup.find_all(<span class="hljs-string"><span class="hljs-string">'div'</span></span>, attrs={<span class="hljs-string"><span class="hljs-string">'class'</span></span>: <span class="hljs-string"><span class="hljs-string">'listing_title'</span></span>}) print(content_name)</code> </pre> <br>  Jeder Abschnitt des Codes mit dem Namen des Hotels wird als Liste zurückgegeben. <br><br><img src="https://habrastorage.org/webt/gi/u2/b1/giu2b1wmq7holjcoeyfqohasg6u.png"><br><br>  Um Hotelnamen aus dem Code zu extrahieren, verwenden wir die Funktion <code>getText</code> der Beautiful Soup-Bibliothek. <br><br><pre> <code class="python hljs">content_name_list = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> div <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> content_name: content_name_list.append(div.getText().split(<span class="hljs-string"><span class="hljs-string">'\n'</span></span>)[<span class="hljs-number"><span class="hljs-number">0</span></span>]) print(content_name_list)</code> </pre> <br>  Hotelnamen werden als Liste zurückgegeben. <br><br><img src="https://habrastorage.org/webt/z0/bc/t-/z0bct-wezpzc8exsahj_fywmjnq.png"><br><br>  Ebenso erhalten wir Preisdaten.  Die Codestruktur für den Preis ist unten gezeigt. <br><br><img src="https://habrastorage.org/webt/v0/ok/oy/v0okoymlgmsbc6dtxicxwa_yz38.png"><br><br>  Wie Sie sehen, können wir mit einem Code arbeiten, der dem für Hotels verwendeten sehr ähnlich ist. <br><br><pre> <code class="python hljs">content_price = soup.find_all(<span class="hljs-string"><span class="hljs-string">'div'</span></span>, attrs={<span class="hljs-string"><span class="hljs-string">'class'</span></span>: <span class="hljs-string"><span class="hljs-string">'price-wrap'</span></span>}) print(content_price)</code> </pre><br>  Im Falle des Preises gibt es wenig Schwierigkeiten.  Sie können es sehen, indem Sie den folgenden Code ausführen: <br><br><pre> <code class="python hljs">content_price_list = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> div <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> content_price: content_price_list.append(div.getText().split(<span class="hljs-string"><span class="hljs-string">'\n'</span></span>)[<span class="hljs-number"><span class="hljs-number">0</span></span>]) print(content_price_list)</code> </pre> <br>  Das Ergebnis ist unten dargestellt.  Wenn in der Liste der Hotels zusätzlich zu einem Text eine Preissenkung angegeben ist, werden sowohl der Anfangspreis als auch der Endpreis zurückgegeben.  Um dieses Problem zu beheben, geben wir einfach den aktuellen Preis für heute zurück. <br><br><img src="https://habrastorage.org/webt/52/6j/mn/526jmnbmxhchiyee4jr3feptxom.png"><br><br>  Wir können eine einfache Logik verwenden, um den neuesten im Text angegebenen Preis zu erhalten. <br><br><pre> <code class="python hljs">content_price_list = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> a <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> content_price: a_split = a.getText().split(<span class="hljs-string"><span class="hljs-string">'\n'</span></span>)[<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(a_split) &gt; <span class="hljs-number"><span class="hljs-number">5</span></span>: content_price_list.append(a_split[<span class="hljs-number"><span class="hljs-number">-4</span></span>:]) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: content_price_list.append(a_split) print(content_price_list)</code> </pre> <br>  Dies ergibt folgendes Ergebnis: <br><br><img src="https://habrastorage.org/webt/kx/d0/sn/kxd0snvrauaecf_q4lvpjdu8-z0.png"><br><br><h3>  API </h3><br>  API - Anwendungsprogrammierschnittstelle (von der englischen Anwendungsprogrammierschnittstelle).  Aus Sicht des Data Mining handelt es sich um ein webbasiertes System, das einen Datenendpunkt bereitstellt, mit dem Sie über die Programmierung Kontakt aufnehmen können.  Normalerweise werden Daten im JSON- oder XML-Format zurückgegeben. <br><br>  Diese Methode wird sich wahrscheinlich beim maschinellen Lernen als nützlich erweisen.  Ich werde ein einfaches Beispiel für das Abrufen von Wetterdaten von der öffentlichen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dark Sky-</a> API geben.  Um eine Verbindung herzustellen, müssen Sie sich registrieren und haben 1000 kostenlose Anrufe pro Tag.  Dies sollte zum Testen ausreichen. <br><br>  Um auf Daten von Dark Sky zuzugreifen, verwende ich die <code>requests</code> .  Zunächst muss ich die richtige URL für die Anfrage erhalten.  Zusätzlich zur Vorhersage liefert Dark Sky historische Wetterdaten.  In diesem Beispiel nehme ich sie und erhalte die richtige URL aus der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation</a> . <br><br>  Die Struktur dieser URL lautet: <br><br><pre> <code class="python hljs">https://api.darksky.net/forecast/[key]/[latitude],[longitude],[time]</code> </pre> <br>  Wir werden die <code>requests</code> verwenden, um zu erhalten <br>  Ergebnisse für einen bestimmten Breiten- und Längengrad sowie Datum und Uhrzeit.  Stellen Sie sich vor, wir haben nach dem Extrahieren der täglichen Preisdaten für Hotels auf Kreta beschlossen, herauszufinden, ob die Preispolitik mit dem Wetter zusammenhängt. <br><br>  Nehmen wir zum Beispiel die Koordinaten eines der Hotels auf der Liste - Mitsis Laguna Resort &amp; Spa. <br><br><img src="https://habrastorage.org/webt/1j/4a/is/1j4aissalsvek5uw2opv3v9hdhw.png"><br><br>  Erstellen Sie zunächst eine URL mit den richtigen Koordinaten sowie der gewünschten Uhrzeit und dem gewünschten Datum.  Über die <code>requests</code> wir Zugriff auf Daten im JSON-Format. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> requests request_url = <span class="hljs-string"><span class="hljs-string">'https://api.darksky.net/forecast/fd82a22de40c6dca7d1ae392ad83eeb3/35.3378,-25.3741,2019-07-01T12:00:00'</span></span> result = requests.get(request_url).json() result</code> </pre><br>  Um das Lesen und Analysieren der Ergebnisse zu vereinfachen, können wir die Daten in einen Datenrahmen konvertieren. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd df = pd.DataFrame.from_dict(json_normalize(result), orient=<span class="hljs-string"><span class="hljs-string">'columns'</span></span>) df.head()</code> </pre> <br><img src="https://habrastorage.org/webt/jw/1w/sv/jw1wsv_9ixycnrfu44l7qhloxb4.png"><br><br>  Es gibt viele weitere Optionen zur Automatisierung der Datenextraktion mit diesen Methoden.  Beim Web-Scraping können Sie verschiedene Funktionen schreiben, um den Prozess zu automatisieren und das Extrahieren von Daten für mehr Tage und / oder Orte zu vereinfachen.  In diesem Artikel wollte ich genügend Codebeispiele überprüfen und bereitstellen.  Die folgenden Materialien werden detaillierter beschrieben: Ich werde Ihnen erklären, wie Sie große Datensätze erstellen und mit den oben beschriebenen Methoden analysieren. <br><br>  Danke für die Aufmerksamkeit! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de460675/">https://habr.com/ru/post/de460675/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de460665/index.html">Entwurf einer FAQ: Warum erscheinen alle drei Jahre C ++ - Standards?</a></li>
<li><a href="../de460667/index.html">Automatisierung des Testens kostenpflichtiger Dienste unter iOS</a></li>
<li><a href="../de460669/index.html">So gewährleisten Sie die Sicherheit der Entwicklung, sparen Zeit und Nerven</a></li>
<li><a href="../de460671/index.html">Eigentum und Ausleihe in D.</a></li>
<li><a href="../de460673/index.html">Enthülle die Magie von DiffUtil</a></li>
<li><a href="../de460683/index.html">Laravel Event Projector und Event Generation Concept</a></li>
<li><a href="../de460685/index.html">Wir verteilen Dateien von Google Drive mit nginx</a></li>
<li><a href="../de460687/index.html">Wie Dosen von innen aussehen</a></li>
<li><a href="../de460695/index.html">Was ist DAA und wie hilft dieses System Drohnen?</a></li>
<li><a href="../de460697/index.html">Kleinstmögliche Schriftart</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>