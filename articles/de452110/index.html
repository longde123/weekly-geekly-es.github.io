<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§≤üèª üõåüèø üê≠ Automatisieren Sie den Festplattenaustausch mit Ansible üöç üïò üë®‚Äçüëß‚Äçüë¶</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo an alle. Ich arbeite als f√ºhrender Systemadministrator in OK und bin f√ºr den stabilen Betrieb des Portals verantwortlich. Ich m√∂chte dar√ºber spr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Automatisieren Sie den Festplattenaustausch mit Ansible</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/odnoklassniki/blog/452110/"><img src="https://habrastorage.org/webt/s9/ga/q9/s9gaq9eke8gooeu1-nnsr5ocv7o.jpeg"><br><br>  Hallo an alle.  Ich arbeite als f√ºhrender Systemadministrator in OK und bin f√ºr den stabilen Betrieb des Portals verantwortlich.  Ich m√∂chte dar√ºber sprechen, wie wir den Prozess des automatischen Austauschs von Festplatten aufgebaut haben und dann als Administrator von diesem Prozess ausgeschlossen und durch einen Bot ersetzt wurden. <br><br>  Dieser Artikel ist eine Art Transliteration der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Leistung</a> bei HighLoad + 2018 <br><a name="habracut"></a><br><h2>  Erstellen eines Disc-Austauschprozesses </h2><br><h3>  Zuerst ein paar Zahlen </h3><br>  OK ist ein gigantischer Dienst, der von Millionen von Menschen genutzt wird.  Es wird von etwa 7.000 Servern bedient, die sich in 4 verschiedenen Rechenzentren befinden.  Die Server kosten mehr als 70.000 Laufwerke.  Wenn Sie sie √ºbereinander stapeln, erhalten Sie einen Turm mit einer H√∂he von mehr als 1 km. <br><br>  Festplatten sind eine Komponente des Servers, die am h√§ufigsten abst√ºrzt.  Bei solchen Volumina m√ºssen wir ungef√§hr 30 Scheiben pro Woche wechseln, und dieses Verfahren ist zu einer nicht sehr angenehmen Routine geworden. <br><br><img src="https://habrastorage.org/webt/i8/92/cg/i892cglklhooat_z_qrsexsmzfu.png"><br><br><h3>  Vorf√§lle </h3><br>  Wir haben ein vollwertiges Incident Management in unserem Unternehmen eingef√ºhrt.  Jeder Vorfall, den wir in Jira aufzeichnen und dann l√∂sen und zerlegen.  Wenn der Vorfall Auswirkungen auf die Benutzer hatte, werden wir auf jeden Fall dar√ºber nachdenken, wie wir in solchen F√§llen schneller reagieren, den Effekt reduzieren und nat√ºrlich eine Wiederholung verhindern k√∂nnen. <br><br>  Laufwerke sind keine Ausnahme.  Ihr Status wird von Zabbix √ºberwacht.  Wir √ºberwachen Nachrichten in Syslog auf Schreib- / Lesefehler, analysieren den Status von HW / SW-Raids, √ºberwachen SMART und berechnen den Verschlei√ü von SSDs. <br><br><h3>  Wie sich die Discs zuvor ge√§ndert haben </h3><br>  Wenn in Zabbix ein Ausl√∂ser aufleuchtet, wird in Jira ein Vorfall erstellt und automatisch an die entsprechenden Ingenieure in den Rechenzentren weitergeleitet.  Wir tun dies bei allen HW-Vorf√§llen, dh solchen, bei denen physische Arbeit mit den Ger√§ten im Rechenzentrum erforderlich ist. <br>  Ein Ingenieur eines Rechenzentrums ist eine Person, die Probleme im Zusammenhang mit der Hardware l√∂st und f√ºr die Installation, Wartung und Demontage von Servern verantwortlich ist.  Nachdem der Ingenieur ein Ticket erhalten hat, beginnt er mit der Arbeit.  In Plattenregalen wechselt er die Platten selbst.  Wenn er jedoch keinen Zugriff auf das gew√ºnschte Ger√§t hat, bittet der Techniker die diensthabenden Systemadministratoren um Hilfe.  Zun√§chst m√ºssen Sie die Festplatte aus der Rotation entfernen.  Dazu m√ºssen Sie die erforderlichen √Ñnderungen auf dem Server vornehmen, die Anwendung stoppen und die Bereitstellung der Festplatte aufheben. <br><br>  Der w√§hrend der Schicht diensthabende Systemadministrator ist f√ºr den Betrieb des gesamten Portals verantwortlich.  Er untersucht Vorf√§lle, repariert und hilft Entwicklern bei kleinen Aufgaben.  Er besch√§ftigt sich nicht nur mit Festplatten. <br><br>  Zuvor unterhielten sich Rechenzentrumsingenieure mit dem Systemadministrator.  Ingenieure schickten Links zu Jira-Tickets, der Administrator ging sie durch und f√ºhrte ein Arbeitsprotokoll in einem Notizblock.  Chats sind jedoch f√ºr solche Aufgaben unpraktisch: Die Informationen dort sind nicht strukturiert und gehen schnell verloren.  Und der Administrator konnte sich einfach vom Computer entfernen und einige Zeit nicht auf Anfragen reagieren, und der Ingenieur stand mit ein paar Festplatten am Server und wartete. <br><br>  Das Schlimmste war jedoch, dass die Administratoren nicht das ganze Bild sahen: Welche Festplattenvorf√§lle gibt es, bei denen das Problem m√∂glicherweise auftreten k√∂nnte?  Dies liegt an der Tatsache, dass wir alle HW-Vorf√§lle an Ingenieure weitergeben.  Ja, es war m√∂glich, alle Vorf√§lle im Admin-Dashboard anzuzeigen.  Aber es gibt viele von ihnen, und der Administrator war nur an einigen von ihnen beteiligt. <br><br>  Dar√ºber hinaus konnte der Techniker keine korrekten Priorit√§ten setzen, da er nichts √ºber den Zweck bestimmter Server und die Verteilung von Informationen auf Laufwerke wei√ü. <br><br><h3>  Neues Austauschverfahren </h3><br>  Das erste, was wir getan haben, war, alle Festplattenvorf√§lle in einen separaten Typ von "HW-Festplatte" zu packen und die Felder "Ger√§tenamen blockieren", "Gr√∂√üe" und "Festplattentyp" hinzuzuf√ºgen, damit diese Informationen im Ticket gespeichert werden und nicht m√ºssen st√§ndig chatten. <br><br><div style="text-align:center;"><img width="300" height="400" src="https://habrastorage.org/webt/0y/uz/jm/0yuzjmgoz4hgulcpf5o5vhwde18.png"></div><br>  Wir haben uns auch darauf geeinigt, dass wir im Rahmen eines Vorfalls nur eine Festplatte wechseln werden.  Dies vereinfachte den Automatisierungsprozess, die Statistikerfassung und die Arbeit erheblich. <br><br>  Au√üerdem wurde das Feld "Verantwortlicher Administrator" hinzugef√ºgt.  Der Systemadministrator wird dort automatisch ersetzt.  Dies ist sehr praktisch, da der Ingenieur jetzt immer sieht, wer verantwortlich ist.  Sie m√ºssen nicht zum Kalender gehen und suchen.  In diesem Feld konnten Tickets in das Dashboard des Administrators gestellt werden, in dem m√∂glicherweise seine Hilfe ben√∂tigt wurde. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9x/b1/ue/9xb1ueokh0450wgkhq0ije1lfqa.png"></div><br>  Um sicherzustellen, dass alle Teilnehmer den gr√∂√ütm√∂glichen Nutzen aus den Innovationen ziehen, haben wir Filter und Dashboards erstellt und den Jungs davon erz√§hlt.  Wenn Menschen die Ver√§nderungen verstehen, distanzieren sie sich nicht von ihnen als von etwas Unn√∂tigem.  F√ºr einen Techniker ist es wichtig, die Rack-Nummer, in der sich der Server befindet, sowie die Gr√∂√üe und den Typ der Festplatte zu kennen.  Der Administrator muss zun√§chst verstehen, um welche Art von Servergruppe es sich handelt und welche Auswirkungen dies beim Ersetzen einer Festplatte haben kann. <br><br>  Das Vorhandensein von Feldern und deren Anzeige ist praktisch, aber dies hat uns nicht vor der Notwendigkeit bewahrt, Chats zu verwenden.  Dazu musste ich den Workflow √§ndern. <br><br>  Fr√ºher war es so: <br><br><div style="text-align:center;"><img width="300" height="400" src="https://habrastorage.org/webt/we/wc/du/wewcdu4q8fqy-gd9wkbutorbabi.png"></div><br>  Heute arbeiten Ingenieure so weiter, wenn sie keine Administratorhilfe ben√∂tigen. <br><br>  Als erstes haben wir einen neuen <b>Untersuchungsstatus eingef√ºhrt</b> .  Das Ticket befindet sich in diesem Status, wenn der Techniker noch nicht entschieden hat, ob er einen Administrator ben√∂tigt oder nicht.  √úber diesen Status kann der Techniker das Ticket an den Administrator weitergeben.  Dar√ºber hinaus markieren wir Tickets mit diesem Status, wenn ein Festplattenwechsel erforderlich ist, sich jedoch keine Festplatte selbst auf der Site befindet.  Dies geschieht bei CDNs und Remote-Standorten. <br><br>  Wir haben auch den Status <b>Bereit</b> hinzugef√ºgt.  Das Ticket wird nach dem Ersetzen der Festplatte darauf √ºbertragen.  Das hei√üt, alles wurde bereits erledigt, aber HW / SW-RAID wird auf dem Server synchronisiert.  Dies kann sehr zeitaufw√§ndig sein. <br><br>  Wenn ein Administrator beteiligt ist, ist das Schema etwas komplizierter. <br><br><div style="text-align:center;"><img width="400" src="https://habrastorage.org/webt/ev/5a/58/ev5a58jyosbyuc8cfrhr7sru5zo.png"></div><br>  Ab dem Status " <b>√ñffnen</b> " kann ein Ticket sowohl von einem Systemadministrator als auch von einem Techniker √ºbertragen werden.  Im Status " <b>In Bearbeitung"</b> entfernt der Administrator die Festplatte aus der Rotation, sodass der Techniker sie einfach entfernen kann: Je nach Servergruppe wird die Hintergrundbeleuchtung eingeschaltet, die Bereitstellung der Festplatte aufgehoben und Anwendungen gestoppt. <br><br>  Dann wird das Ticket in <b>Ready to change</b> konvertiert: Dies ist ein Signal an den Techniker, dass die Festplatte herausgezogen werden kann.  Alle Felder in Jira sind bereits ausgef√ºllt, der Ingenieur wei√ü, welcher Typ und welche Gr√∂√üe die Festplatte hat.  Diese Daten werden entweder automatisch oder vom Administrator auf den vorherigen Status angeh√§ngt. <br><br>  Nach dem Ersetzen der Festplatte wird das Ticket in den Status " <b>Ge√§ndert</b> " versetzt.  Es wird √ºberpr√ºft, ob der richtige Datentr√§ger eingelegt wurde, das Markup durchgef√ºhrt, die Anwendung gestartet und einige Datenwiederherstellungsaufgaben ausgef√ºhrt wurden.  Das Ticket kann auch in den Status " <b>Bereit</b> " versetzt werden. In diesem Fall bleibt der Administrator verantwortlich, da er die Festplatte abwechselnd gestartet hat.  Der vollst√§ndige Umriss sieht so aus. <br><br><div style="text-align:center;"><img width="500" src="https://habrastorage.org/webt/xg/x7/_z/xgx7_zk2mlyubhzepgfw0oogbt4.png"></div><br>  Das Hinzuf√ºgen neuer Felder hat unser Leben viel einfacher gemacht.  Die Jungs begannen mit strukturierten Informationen zu arbeiten, es wurde klar, was und zu welchem ‚Äã‚ÄãZeitpunkt zu tun war.  Priorit√§ten sind viel relevanter geworden, da sie jetzt vom Administrator festgelegt werden. <br><br>  Das Bed√ºrfnis nach Chats ist verschwunden.  Nat√ºrlich kann der Administrator dem Techniker schreiben, "Sie m√ºssen hier schneller ersetzen" oder "bereits abends, haben Sie Zeit zum Ersetzen?"  Wir unterhalten uns jedoch nicht mehr t√§glich √ºber diese Themen. <br><br>  Die Festplatten begannen sich in Packungen zu wechseln.  Wenn der Administrator etwas fr√ºher zur Arbeit kam, er Freizeit hat und nichts passiert ist, kann er eine Reihe von Servern f√ºr den Austausch vorbereiten: Felder ablegen, Festplatten aus der Rotation entfernen und die Aufgabe an den Techniker √ºbertragen.  Ein Techniker kommt sp√§ter im Rechenzentrum an, sieht die Aufgabe, nimmt die erforderlichen Laufwerke aus dem Lager und √§ndert sie sofort.  Infolgedessen hat sich die Austauschgeschwindigkeit erh√∂ht. <br><br><h3>  Lehren aus dem Erstellen von Workflows </h3><br><ul><li>  <b>Beim Erstellen einer Prozedur m√ºssen Sie Informationen aus verschiedenen Quellen sammeln.</b> <br>  Einige unserer Administratoren wussten nicht, dass der Techniker die Festplatten selbst ausgetauscht hat.  Einige dachten, dass Ingenieure die MD RAID-Synchronisation √ºberwachten, obwohl einige von ihnen nicht einmal Zugriff darauf hatten.  Einige f√ºhrende Ingenieure haben dies getan, aber nicht immer, da der Prozess nirgendwo beschrieben wurde. </li><li>  <b>Das Verfahren sollte einfach und unkompliziert sein.</b> <br>  Es f√§llt einem Menschen schwer, viele Schritte im Kopf zu behalten.  Die wichtigsten Nachbarstatus in Jira m√ºssen auf dem Hauptbildschirm angezeigt werden.  Sie k√∂nnen sie umbenennen, z. B. In Bearbeitung rufen wir Ready to change auf.  Und die verbleibenden Status k√∂nnen im Dropdown-Men√º ausgeblendet werden, damit sie keine Kallusaugen verursachen.  Es ist jedoch besser, die Menschen nicht einzuschr√§nken und die M√∂glichkeit zu geben, den √úbergang zu vollziehen. <br>  Erkl√§ren Sie den Wert von Innovation.  Wenn die Leute verstehen, akzeptieren sie das neue Verfahren besser.  F√ºr uns war es sehr wichtig, dass die Leute den gesamten Prozess nicht aufriefen, sondern ihm folgten.  Dann haben wir auf dieser Automatisierung aufgebaut. </li><li>  <b>Warten, analysieren, verstehen.</b> <br>  Wir haben ungef√§hr einen Monat gebraucht, um das Verfahren, die technische Implementierung, die Besprechungen und die Diskussionen zu erstellen.  Und f√ºr die Umsetzung - mehr als drei Monate.  Ich habe gesehen, wie die Leute langsam anfangen, die Innovation zu nutzen.  In den fr√ºhen Stadien gab es viel Negativit√§t.  Er war jedoch v√∂llig unabh√§ngig vom Verfahren selbst, seiner technischen Umsetzung.  Beispielsweise hat ein Administrator Jira nicht verwendet, aber das Jira-Plugin in Confluence, und einige Dinge standen ihm nicht zur Verf√ºgung.  Hat ihm Jira gezeigt, hat der Administrator die Produktivit√§t und die Gesamtaufgaben sowie das Ersetzen von Festplatten erh√∂ht. </li></ul><br><h2>  Automatisierung des Antriebswechsels </h2><br>  Wir sind mehrmals zur Automatisierung des Austauschs von Festplatten √ºbergegangen.  Wir hatten bereits Betriebszeit, Skripte, aber alle arbeiteten entweder interaktiv oder im manuellen Modus. Sie mussten gestartet werden.  Und erst nach der Einf√ºhrung des neuen Verfahrens stellten wir fest, dass wir nur vermisst wurden. <br><br>  Da der Austauschprozess jetzt in Phasen unterteilt ist, von denen jede einen Ausf√ºhrenden und eine Liste von Aktionen enth√§lt, k√∂nnen wir die Automatisierung schrittweise und nicht alle gleichzeitig aktivieren.  Zum Beispiel kann der einfachste Schritt - Bereit (√úberpr√ºfen der RAID- / Datensynchronisation) einfach an den Bot delegiert werden.  Wenn der Bot ein wenig lernt, k√∂nnen Sie ihm eine verantwortungsvollere Aufgabe geben - das Drehen der Festplatte usw. <br><br><h3>  Zoo-Setups </h3><br>  Bevor wir √ºber den Bot sprechen, machen wir einen kurzen Ausflug zu unserem Installationszoo.  Dies ist vor allem auf die gigantische Gr√∂√üe unserer Infrastruktur zur√ºckzuf√ºhren.  Zweitens versuchen wir f√ºr jeden Service die optimale Konfiguration des Eisens zu w√§hlen.  Wir haben ungef√§hr 20 Hardware-RAID-Modelle, haupts√§chlich LSI und Adaptec, aber es gibt sowohl HP als auch DELL mit unterschiedlichen Versionen.  Jeder RAID-Controller verf√ºgt √ºber ein eigenes Verwaltungsdienstprogramm.  Der Befehlssatz und die Ausgabe dieser Befehle k√∂nnen von Version zu Version jedes RAID-Controllers unterschiedlich sein.  Wenn HW-RAID nicht verwendet wird, kann es Angst haben. <br><br>  Fast alle Neuinstallationen werden ohne Festplatten-Backup durchgef√ºhrt.  Wir versuchen, kein Hardware- und Software-RAID mehr zu verwenden, da wir unsere Systeme auf der Ebene von Rechenzentren und nicht von Servern reservieren.  Aber nat√ºrlich gibt es viele Legacy-Server, die unterst√ºtzt werden m√ºssen. <br><br>  Irgendwo werfen die Festplatten in den RAID-Controllern unformatierte Ger√§te, irgendwo verwenden sie JBOD.  Es gibt Konfigurationen mit einem Systemlaufwerk auf dem Server. Wenn Sie es ersetzen m√ºssen, m√ºssen Sie den Server bei der Installation des Betriebssystems und der Anwendungen mit denselben Versionen neu formatieren, dann Konfigurationsdateien hinzuf√ºgen und Anwendungen starten.  Es gibt auch viele Servergruppen, bei denen Redundanz nicht auf der Ebene des Festplattensubsystems, sondern direkt in den Anwendungen selbst ausgef√ºhrt wird. <br><br>  Insgesamt haben wir mehr als 400 eindeutige Servergruppen, auf denen etwa 100 verschiedene Anwendungen ausgef√ºhrt werden.  Um eine so gro√üe Anzahl von Optionen abzudecken, brauchten wir ein multifunktionales Automatisierungstool.  Es ist ratsam, ein einfaches DSL zu verwenden, damit nicht nur die Person, die dies geschrieben hat, es unterst√ºtzen kann. <br><br>  Wir haben uns f√ºr Ansible entschieden, weil es agentenlos ist: Es war nicht erforderlich, die Infrastruktur vorzubereiten und schnell zu starten.  Dar√ºber hinaus ist es in Python geschrieben, das im Team als Standard akzeptiert wird. <br><br><h3>  Allgemeines Schema </h3><br>  Schauen wir uns ein allgemeines Automatisierungsschema am Beispiel eines Vorfalls an.  Zabbix erkennt, dass das SDB-Laufwerk nicht in Betrieb ist, der Ausl√∂ser leuchtet auf und in Jira wird ein Ticket erstellt.  Der Administrator sah es sich an und stellte fest, dass dies kein Duplikat und kein falsch positives Ergebnis ist. Das hei√üt, Sie m√ºssen die Festplatte √§ndern und das laufende Ticket √ºbersetzen. <br><br><img src="https://habrastorage.org/webt/9s/fy/q5/9sfyq5cucem0dbbhahfd3_w7tac.png"><br>  Die in Python geschriebene DiskoBot-Anwendung fragt Jira regelm√§√üig nach neuen Tickets ab.  Es wird darauf hingewiesen, dass ein neues In Bearbeitung-Ticket angezeigt wurde und der entsprechende Thread ausgel√∂st wird, wodurch das Playbook in Ansible gestartet wird (dies erfolgt f√ºr jeden Status in Jira).  In diesem Fall wird Prepare2change gestartet. <br><br>  Ansible geht zum Host, entfernt die Festplatte aus der Rotation und meldet den Status √ºber R√ºckrufe an die Anwendung. <br><br><img src="https://habrastorage.org/webt/i2/bk/nq/i2bknqits8exw9dtr7sj4zydl58.png"><br>  Entsprechend den Ergebnissen √ºbertr√§gt der Bot das Ticket automatisch an Ready to change.  Der Techniker erh√§lt eine Benachrichtigung und wechselt die Festplatte. Anschlie√üend √ºbertr√§gt er das Ticket an Changed. <br><br><img src="https://habrastorage.org/webt/h9/m8/zw/h9m8zwfrzltmu0q1tz8opctrccu.png"><br>  Gem√§√ü dem obigen Schema kehrt das Ticket zum Bot zur√ºck, startet ein weiteres Playbook, geht zum Host und gibt die Festplatte in Rotation ein.  Der Bot schlie√üt das Ticket.  Hurra! <br><br><img src="https://habrastorage.org/webt/cv/js/w1/cvjsw1y9qrpkra2twsx-sy4jokc.png"><br>  Lassen Sie uns nun √ºber einige Komponenten des Systems sprechen. <br><br><h3>  Diskobot </h3><br>  Diese Anwendung ist in Python geschrieben.  Es w√§hlt Tickets von Jira gem√§√ü <abbr title="JIRA-Abfragesprache">JQL aus</abbr> .  Abh√§ngig vom Ticketstatus gelangt dieser zum entsprechenden Handler, der seinerseits den entsprechenden Ansible-Playbook-Status startet. <br><br>  JQL- und Abfrageintervalle sind in der Anwendungskonfigurationsdatei definiert. <br><br><pre><code class="plaintext hljs">jira_states: investigate: jql: '‚Ä¶ status = Open and "Disk Size" is EMPTY' interval: 180 inprogress: jql: '‚Ä¶ and "Disk Size" is not EMPTY and "Device Name" is not EMPTY' ready: jql: '‚Ä¶ and (labels not in ("dbot_ignore") or labels is EMPTY)' interval: 7200</code> </pre> <br>  Beispielsweise werden unter den Tickets im Status "In Bearbeitung" nur diejenigen mit den Feldern "Datentr√§gergr√∂√üe" und "Ger√§tename" ausgef√ºllt.  Ger√§tename ist der Name des Blockger√§ts, das zum Ausf√ºhren des Playbooks ben√∂tigt wird.  Die Festplattengr√∂√üe wird ben√∂tigt, damit der Techniker wei√ü, welche Festplattengr√∂√üe ben√∂tigt wird. <br><br>  Bei Tickets mit dem Status "Bereit" werden Tickets mit dem Label "dbot_ignore" herausgefiltert.  √úbrigens verwenden wir Jira-Labels sowohl zum Filtern als auch zum Markieren doppelter Tickets und zum Sammeln von Statistiken. <br><br>  Wenn das Playbook abst√ºrzt, weist Jira das Label dbot_failed zu, damit Sie es sp√§ter herausfinden k√∂nnen. <br><br><h3>  Interaktion mit Ansible </h3><br>  Die Anwendung interagiert mit Ansible √ºber die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ansible Python-API</a> .  In playbook_executor √ºbergeben wir den Dateinamen und die Variablen.  Auf diese Weise k√∂nnen Sie das Ansible-Projekt in Form von regul√§ren yml-Dateien beibehalten, anstatt es in Python-Code zu beschreiben. <br><br>  Auch in Ansible √ºber * extra_vars * wird der Name des Blockger√§ts, der Status des Tickets sowie callback_url verwendet, in dem der Ausgabeschl√ºssel verkabelt ist - er wird f√ºr den R√ºckruf in HTTP verwendet. <br><br>  F√ºr jeden Start wird ein tempor√§res Inventar generiert, das aus einem Host und der Gruppe besteht, zu der dieser Host geh√∂rt, sodass group_vars angewendet wird. <br><br>  Hier ist ein Beispiel f√ºr eine Aufgabe, in der ein HTTP-R√ºckruf implementiert ist. <br><br>  Das Ergebnis der Playbooks erhalten wir mit Callaback (s).  Es gibt zwei Arten: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ansible Callback Plugin</a> , es liefert Daten √ºber die Ergebnisse eines Playbooks.  Es beschreibt die Aufgaben, die gestartet, erfolgreich oder erfolglos ausgef√ºhrt wurden.  Dieser R√ºckruf wird am Ende des Playbooks aufgerufen. </li><li>  HTTP-R√ºckruf, um Informationen beim Abspielen eines Playbooks abzurufen.  In Ansible f√ºhren wir eine POST / GET-Anforderung an die Seite unserer Anwendung aus. </li></ul><br>  √úber HTTP-R√ºckrufe werden Variablen √ºbertragen, die w√§hrend der Ausf√ºhrung des Playbooks definiert wurden und die wir speichern und in nachfolgenden L√§ufen verwenden m√∂chten.  Wir schreiben diese Daten in SQLite. <br><br>  Auch durch HTTP-R√ºckruf hinterlassen wir Kommentare und √§ndern den Ticketstatus. <br><br><div class="spoiler">  <b class="spoiler_title">HTTP-R√ºckruf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs"># Make callback to Diskobot App # Variables: # callback_post_body: # A dict with follow keys. All keys are optional # msg: If exist it would be posted to Jira as comment # data: If exist it would be saved in Incident.variables # desire_state: Set desire_state for incident # status: If exist Proceed issue to that status - name: Callback to Diskobot app (jira comment/status) uri: url: "{{ callback_url }}/{{ devname }}" user: "{{ diskobot_user }}" password: "{{ diskobot_pass }}" force_basic_auth: True method: POST body: "{{ callback_post_body | to_json }}" body_format: json delegate_to: 127.0.0.1</code> </pre><br></div></div><br>  Wie viele Aufgaben derselben Art legen wir sie in einer separaten gemeinsamen Datei ab und f√ºgen sie bei Bedarf hinzu, um sie nicht st√§ndig in Spielb√ºchern zu wiederholen.  Hier wird die R√ºckruf-URL angezeigt, in der der Ausgabeschl√ºssel und der Hostname gesch√ºtzt sind.  Wenn Ansible diese POST-Anforderung ausf√ºhrt, erkennt der Bot, dass sie Teil eines solchen Vorfalls war. <br><br>  Und hier ist ein Beispiel aus einem Playbook, in dem wir eine Diskette von einem MD-Ger√§t angezeigt haben: <br><br><pre> <code class="plaintext hljs"> # Save mdadm configuration - include: common/callback.yml vars: callback_post_body: status: 'Ready to change' msg: "Removed disk from mdraid {{ mdadm_remove_disk.msg | comment_jira }}" data: mdadm_data: "{{ mdadm_remove_disk.removed }}" parted_info: "{{ parted_info | default() }}" when: - mdadm_remove_disk | changed - mdadm_remove_disk.removed</code> </pre><br>  Diese Aufgabe versetzt das Jira-Ticket in den Status "Bereit zum √Ñndern" und f√ºgt einen Kommentar hinzu.  Au√üerdem speichert die Variable mdam_data die Liste der md-Ger√§te, von denen die Festplatte gel√∂scht wurde, und den parted_-Speicherauszug der partierten Partition in parted_info. <br><br>  Wenn der Techniker eine neue Festplatte einlegt, k√∂nnen wir diese Variablen verwenden, um den Partitionsspeicherauszug wiederherzustellen und die Festplatte in die MD-Ger√§te einzulegen, von denen sie gel√∂scht wurde. <br><br><h3>  Ansible Check-Modus </h3><br>  Das Einschalten der Automatisierung war be√§ngstigend.  Aus diesem Grund haben wir beschlossen, alle Playbooks im Modus auszuf√ºhren <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Trockenlauf</a> , bei dem Ansible keine Aktionen auf den Servern ausf√ºhrt, sondern nur emuliert. <br><br>  Ein solcher Start wird √ºber ein separates R√ºckrufmodul ausgef√ºhrt, und das Ergebnis des Playbooks wird in Jira als Kommentar gespeichert. <br><br><img src="https://habrastorage.org/webt/aa/rz/0s/aarz0srsrqqxbru88guob9spjqm.png"><br><br>  Erstens erlaubte es, die Arbeit des Bots und der Spielb√ºcher zu validieren.  Zweitens wurde das Vertrauen der Administratoren in den Bot erh√∂ht. <br><br>  Als wir die Validierung durchlaufen haben und festgestellt haben, dass Sie Ansible nicht nur im Trockenlaufmodus ausf√ºhren k√∂nnen, haben wir in Jira die Schaltfl√§che "Diskobot ausf√ºhren" aktiviert, um dasselbe Playbook mit denselben Variablen auf demselben Host, jedoch im normalen Modus, zu starten. <br><br>  Dar√ºber hinaus wird die Schaltfl√§che verwendet, um das Playbook im Falle eines Fehlers neu zu starten. <br><br><h3>  Playbooks Struktur </h3><br>  Ich habe bereits erw√§hnt, dass der Bot je nach Status des Jira-Tickets verschiedene Playbooks startet. <br><br>  Erstens ist es viel einfacher, einen Eintrag zu arrangieren. <br>  Zweitens ist es in einigen F√§llen einfach notwendig. <br><br>  Wenn Sie beispielsweise eine Systemfestplatte ersetzen, m√ºssen Sie zuerst zum Bereitstellungssystem wechseln, eine Aufgabe erstellen. Nach der korrekten Bereitstellung kann der Server √ºber ssh aufgerufen werden, und Sie k√∂nnen die Anwendung darauf rollen.  Wenn wir dies alles in einem Playbook tun w√ºrden, k√∂nnte Ansible es aufgrund der Unzug√§nglichkeit des Hosts nicht ausf√ºhren. <br><br>  Wir verwenden Ansible-Rollen f√ºr jede Servergruppe.  Hier k√∂nnen Sie sehen, wie die Playbooks in einem von ihnen organisiert sind. <br><br><img src="https://habrastorage.org/webt/ef/0o/hu/ef0ohuo0obwa1htij7o4at6dvus.png"><br><br>  Dies ist praktisch, da sofort klar ist, wo sich welche Aufgaben befinden.  In main.yml, der Eingabe f√ºr die Ansible-Rolle, k√∂nnen wir nur den Ticketstatus oder allgemeine Aufgaben angeben, die f√ºr alle erforderlich sind, z. B. das √úbergeben der Identifikation oder das Empfangen eines Tokens. <br><br><h4>  Investigation.yml </h4><br>  L√§uft f√ºr Tickets im Status "Untersuchung" und "Offen".  Das Wichtigste f√ºr dieses Playbook ist der Name des Blockger√§ts.  Diese Informationen sind nicht immer verf√ºgbar. <br><br>  Um dies zu erreichen, analysieren wir die Jira-Zusammenfassung, den letzten Wert des Zabbix-Triggers.  Es kann den Namen des Blockger√§ts enthalten - zum Gl√ºck.  Oder es enth√§lt einen Mount-Punkt. Dann m√ºssen Sie zum Server gehen, das gew√ºnschte Laufwerk analysieren und berechnen.  Ein Trigger kann auch eine SCSI-Adresse oder andere Informationen √ºbertragen.  Es kommt aber auch vor, dass es keine Hinweise gibt und man analysieren muss. <br><br>  Nachdem wir den Namen des Blockger√§ts herausgefunden haben, sammeln wir Informationen √ºber den Typ und die Gr√∂√üe der Festplatte, um die Felder in Jira auszuf√ºllen.  Wir entfernen auch Informationen √ºber den Hersteller, das Modell, die Firmware, die ID und SMART und f√ºgen all dies in einen Kommentar im Jira-Ticket ein.  Der Administrator und der Techniker m√ºssen nicht mehr nach diesen Daten suchen.  :) :) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/m6/64/kr/m664krcgyi4vkc-rq0almmqv6uy.png"></div><br><br><h4>  prepare2change.yml </h4><br>  Die Ausgabe der Platte aus der Rotation, Vorbereitung f√ºr den Austausch.  Die schwierigste und entscheidende Phase.  Hier k√∂nnen Sie die Anwendung stoppen, wenn sie nicht gestoppt werden kann.  Oder ziehen Sie eine Festplatte heraus, die nicht √ºber gen√ºgend Replikate verf√ºgt und sich dadurch auf die Benutzer auswirkt, und verlieren Sie einige Daten.  Hier haben wir die meisten √úberpr√ºfungen und Benachrichtigungen im Chat. <br><br>  Im einfachsten Fall handelt es sich um das Entfernen eines Laufwerks aus HW / MD RAID. <br><br>  In komplexeren Situationen (in unseren Speichersystemen) m√ºssen Sie, wenn die Sicherung auf Anwendungsebene durchgef√ºhrt wird, mithilfe der API zur Anwendung wechseln, die Festplattenausgabe melden, sie deaktivieren und die Wiederherstellung starten. <br><br>  Wir migrieren jetzt massiv in die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Cloud</a> . Wenn der Server tr√ºbe ist, greift Diskobot auf die Cloud-API zu und sagt, dass dies mit diesem Minion - dem Server, auf dem die Container ausgef√ºhrt werden - funktioniert. Er fragt, ob alle Container von diesem Minion migriert werden sollen.  Gleichzeitig wird die Hintergrundbeleuchtung eingeschaltet, sodass der Techniker sofort sieht, welche herausgezogen werden muss. <br><br><h4>  ge√§ndert.yml </h4><br>  Nach dem Ersetzen einer Festplatte √ºberpr√ºfen wir zun√§chst deren Verf√ºgbarkeit. <br><br>  Ingenieure legen nicht immer neue Discs ein, daher haben wir eine √úberpr√ºfung auf SMART-Werte hinzugef√ºgt, die uns zufrieden stellen. <br><br><div class="spoiler">  <b class="spoiler_title">Welche Attribute betrachten wir?</b> <div class="spoiler_text">  Anzahl der neu zugewiesenen Sektoren (5) &lt;100 <br>  Aktuelle ausstehende Sektoranzahl (107) == 0 <br></div></div><br>  Wenn das Laufwerk den Test nicht besteht, wird der Techniker √ºber einen Austausch informiert.  Wenn alles in Ordnung ist, wird die Hintergrundbeleuchtung ausgeschaltet, das Markup wird angewendet und die Festplatte wird in Rotation eingelegt. <br><br><h4>  ready.yml </h4><br>  Der einfachste Fall: √úberpr√ºfen der HW / SW-Raid-Synchronisation oder Beenden der Datensynchronisation in der Anwendung. <br><br><h3>  Anwendungs-API </h3><br>  Ich habe mehrmals erw√§hnt, dass der Bot h√§ufig auf die Anwendungs-APIs zugreift.  Nat√ºrlich hatten nicht alle Anwendungen die notwendigen Methoden, deshalb musste ich sie verfeinern.  Hier sind die wichtigsten Methoden, die wir verwenden: <br><ul><li>  Status  Der Status eines Clusters oder einer Festplatte, um zu verstehen, ob es m√∂glich ist, damit zu arbeiten. <br></li><li>  Start / Stopp.  Aktivierung-Deaktivierung der Festplatte; <br></li><li>  Migrieren / Wiederherstellen.  Migration und Datenwiederherstellung w√§hrend und nach dem Austausch. <br></li></ul><br><h3>  Von Ansible gelernte Lektionen </h3><br>  Ich liebe Ansible wirklich.  Aber oft, wenn ich mir verschiedene OpenSource-Projekte anschaue und sehe, wie Leute Playbooks schreiben, bekomme ich ein bisschen Angst.  Komplexes logisches Geflecht aus Wann / Schleife, mangelnde Flexibilit√§t und Idempotenz aufgrund der h√§ufigen Verwendung von Shell / Befehl. <br><br>  Wir haben uns entschlossen, alles so weit wie m√∂glich zu vereinfachen und die Ansible-Modularit√§t zu nutzen.  Auf der h√∂chsten Ebene befinden sich Playbooks. Sie k√∂nnen von jedem Administrator geschrieben werden, einem Drittentwickler, der Ansible ein wenig kennt. <br><br><pre> <code class="plaintext hljs">- name: Blink disk become: True register: locate_action disk_locate: locate: '{{ locate }}' devname: '{{ devname }}' ids: '{{ locate_ids | default(pd_id) | default(omit) }}'</code> </pre><br><br>  Wenn es schwierig ist, eine Logik in Playbooks zu implementieren, platzieren wir sie in einem Ansible-Modul oder -Filter.  Skripte k√∂nnen sowohl in Python als auch in einer anderen Sprache geschrieben werden. <br><br>  Sie sind einfach und schnell zu schreiben.  Beispielsweise besteht das Plattenhervorhebungsmodul, dessen Verwendungsbeispiel oben angegeben ist, aus 265 Zeilen. <br><br><img width="400" height="400" src="https://habrastorage.org/webt/c4/ma/bp/c4mabpsyezbjbfp2likixvta24k.png"><br><br>  Auf der untersten Ebene befindet sich die Bibliothek.  F√ºr dieses Projekt haben wir eine separate Anwendung geschrieben, eine Art Abstraktion √ºber die Hardware- und Software-RAIDs, die die entsprechenden Anforderungen ausf√ºhren. <br><br><img width="400" height="400" src="https://habrastorage.org/webt/qf/vt/pr/qfvtprdi9jw2vxserynmxbrmdg8.png"><br><br>  Die gr√∂√üten St√§rken von Ansible sind die Einfachheit und die verst√§ndlichen Spielb√ºcher.  Ich glaube, dass Sie dies verwenden m√ºssen und keine be√§ngstigenden Yaml-Dateien und eine gro√üe Anzahl von Bedingungen, Shell-Code und Schleifen generieren m√ºssen. <br><br>  Wenn Sie unsere Erfahrungen mit der Ansible-API wiederholen m√∂chten, beachten Sie zwei Dinge: <br><br><ul><li>  Playbook_executor und im Allgemeinen Playbook k√∂nnen nicht abgelaufen werden.  Es gibt eine Zeit√ºberschreitung in der SSH-Sitzung, aber keine Zeit√ºberschreitung im Playbook.  Wenn wir versuchen, die Bereitstellung eines Laufwerks aufzuheben, das noch nicht im System vorhanden ist, wird das Playbook unbegrenzt ausgef√ºhrt. Daher mussten wir es in einem separaten Wrapper einpacken und nach Zeit√ºberschreitung beenden. <br></li><li>  Ansible ist gegabelt, daher ist seine API nicht threadsicher.  Wir starten alle unsere Playbooks und Single-Threaded. <br></li></ul><br>  Dadurch konnten wir den Austausch von ca. 80% der Laufwerke automatisieren.  Im Allgemeinen hat sich die Ersatzrate verdoppelt.  Heute betrachtet der Administrator nur den Vorfall und entscheidet, ob die Festplatte ge√§ndert werden soll oder nicht, und macht dann einen Klick. <br><br>  Jetzt stellen wir uns jedoch einem anderen Problem: Einige neue Administratoren wissen nicht, wie sie Laufwerke wechseln sollen.  :) :) </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de452110/">https://habr.com/ru/post/de452110/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de452094/index.html">.NET: Tools zum Arbeiten mit Multithreading und Asynchronit√§t. Teil 1</a></li>
<li><a href="../de452098/index.html">Protokolle des Frontend-Entwicklers Habr: Refactor und Reflex</a></li>
<li><a href="../de452102/index.html">Fotospiel f√ºr Drohnenliebhaber: kurz √ºber AirSelfie 2</a></li>
<li><a href="../de452106/index.html">Wir laden Redner zum Sommer-DIY-Meeting am 16. Juni 2019 ein</a></li>
<li><a href="../de452108/index.html">Docker: harmloser Rat</a></li>
<li><a href="../de452112/index.html">CRM ++</a></li>
<li><a href="../de452114/index.html">HolyJS 2019: Nachbesprechung von SEMrush (Teil 1)</a></li>
<li><a href="../de452116/index.html">Indizes in PostgreSQL - 8 (RUM)</a></li>
<li><a href="../de452118/index.html">Wissenschaftler bricht den Code des mysteri√∂sen Manuskripts von Voynich</a></li>
<li><a href="../de452122/index.html">"Pille vom D√§mon" in Bewegung</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>