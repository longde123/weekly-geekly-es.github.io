<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ò£Ô∏è üë©üèΩ‚Äç‚öïÔ∏è ‚ö∞Ô∏è Audio-KI: Extrahieren von Gesang aus Musik mithilfe von Faltungs-Neuronalen Netzen üë®‚Äçüë®‚Äçüë¶ üßñ ‚úçÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Musik hacken, um abgeleitete Inhalte zu demokratisieren 

 Haftungsausschluss: Alle in diesem Artikel beschriebenen geistigen Eigentumsrechte, Designs...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Audio-KI: Extrahieren von Gesang aus Musik mithilfe von Faltungs-Neuronalen Netzen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/441090/">  <i>Musik hacken, um abgeleitete Inhalte zu demokratisieren</i> <br><br><blockquote>  <b>Haftungsausschluss:</b> Alle in diesem Artikel beschriebenen geistigen Eigentumsrechte, Designs und Methoden sind in US10014002B2 und US9842609B2 offenbart. </blockquote><br>  Ich w√ºnschte, ich k√∂nnte nach 1965 zur√ºckkehren, mit einem Pass an die Haust√ºr des Abby Road-Studios klopfen, hineingehen und die echten Stimmen von Lennon und McCartney h√∂ren ... Nun, versuchen wir es.  Input: Beatles durchschnittliche MP3-Qualit√§t <i>Wir k√∂nnen es schaffen</i> .  Die obere Spur ist der Eingangsmix, die untere Spur ist der isolierte Gesang, den unser neuronales Netzwerk hervorgehoben hat. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://player.vimeo.com/video/305275806" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><a name="habracut"></a><br>  Formal ist dieses Problem als <i>Trennung von Schallquellen</i> oder <i>Trennung des Signals</i> (Audioquellentrennung) bekannt.  Es besteht darin, eines oder mehrere der urspr√ºnglichen Signale wiederherzustellen oder zu rekonstruieren, die infolge eines <i>linearen oder Faltungsprozesses</i> mit anderen Signalen gemischt werden.  Dieses Forschungsgebiet hat viele praktische Anwendungen, einschlie√ülich der Verbesserung der Klangqualit√§t (Sprachqualit√§t) und der Beseitigung von Rauschen, Musik-Remixen, r√§umlicher Klangverteilung, Remastering usw. Toningenieure nennen diese Technik manchmal Demixing.  Zu diesem Thema gibt es viele Ressourcen, von der blinden Trennung von Signalen mit Analyse unabh√§ngiger Komponenten (ICA) bis zur halbkontrollierten Faktorisierung nicht negativer Matrizen und dem Ende sp√§terer Ans√§tze auf der Grundlage neuronaler Netze.  Gute Informationen zu den ersten beiden Punkten finden Sie in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesen Mini-Guides</a> von CCRMA, die mir einmal sehr n√ºtzlich waren. <br><br>  <b>Aber bevor wir in die Entwicklung eintauchen ... einiges an angewandter Philosophie des maschinellen Lernens ...</b> <br><br>  Ich habe mich bereits mit der Signal- und Bildverarbeitung besch√§ftigt, bevor sich der Slogan ‚ÄûDeep Learning l√∂st alles‚Äú verbreitet hat. <b>Daher</b> kann ich Ihnen eine L√∂sung als <i>Feature-Engineering-</i> Reise vorstellen und zeigen, <b>warum ein neuronales Netzwerk der beste Ansatz f√ºr dieses spezielle Problem ist</b> .  Warum?  Sehr oft sehe ich Leute, die so etwas schreiben: <br><br>  <i>‚ÄûMit Deep Learning m√ºssen Sie sich keine Gedanken mehr √ºber die Auswahl von Funktionen machen.</i>  <i>es wird es f√ºr dich tun. "</i> <br><br>  oder schlimmer ... <br><br>  <i>"Der Unterschied zwischen maschinellem Lernen und tiefem Lernen</i> [hey ... tiefes Lernen ist immer noch maschinelles Lernen!] Ist das, <i>dass Sie in ML selbst die Attribute extrahieren, und beim tiefen Lernen geschieht dies automatisch innerhalb des Netzwerks."</i> <br><br>  Diese Verallgemeinerungen beruhen wahrscheinlich auf der Tatsache, dass DNNs sehr effektiv bei der Erkundung guter versteckter R√§ume sein k√∂nnen.  Aber so ist es unm√∂glich zu verallgemeinern.  Ich bin sehr ver√§rgert, wenn die j√ºngsten Absolventen und Praktiker den oben genannten Missverst√§ndnissen erliegen und den Ansatz des ‚ÄûDeep-Learning-It-All‚Äú verfolgen.  Es reicht aus, eine Menge Rohdaten zu werfen (auch nach einer kleinen Vorverarbeitung) - und alles wird so funktionieren, wie es sollte.  In der realen Welt m√ºssen Sie sich um Dinge wie Leistung, Echtzeitausf√ºhrung usw. k√ºmmern. Aufgrund solcher Missverst√§ndnisse werden Sie sehr lange im Experimentiermodus stecken bleiben ... <br><br>  <b>Feature Engineering bleibt eine sehr wichtige Disziplin beim Entwurf k√ºnstlicher neuronaler Netze.</b>  <b>Wie bei jeder anderen ML-Technik unterscheidet sie in den meisten F√§llen effektive L√∂sungen des Produktionsniveaus von erfolglosen oder ineffektiven Experimenten.</b>  <b>Ein tiefes Verst√§ndnis Ihrer Daten und ihrer Natur bedeutet immer noch viel ...</b> <br><br><h1>  Von A bis Z. </h1><br>  Ok, ich habe die Predigt beendet.  Nun wollen wir sehen, warum wir hier sind!  Wie bei jedem Datenverarbeitungsproblem sehen wir uns zun√§chst an, wie es aussieht.  Schauen Sie sich das n√§chste St√ºck Gesang aus der Original-Studioaufnahme an. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://player.vimeo.com/video/305288385" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Studio-Gesang 'One Last Time', Ariana Grande</font></i> <br><br>  Nicht zu interessant, oder?  Das liegt daran, dass wir das Signal <i>rechtzeitig</i> visualisieren.  Hier sehen wir nur Amplituden√§nderungen √ºber die Zeit.  Sie k√∂nnen jedoch alle m√∂glichen anderen Dinge extrahieren, z. B. Amplitudenh√ºllkurven (H√ºllkurve), quadratische Mittelwerte (RMS), die √Ñnderungsrate von positiven Amplitudenwerten zu negativen (Nulldurchgangsrate) usw., aber diese <i>Vorzeichen sind</i> zu <i>primitiv</i> und nicht ausreichend unterscheidbar. um in unserem Problem zu helfen.  Wenn wir Gesang aus einem Audiosignal extrahieren wollen, m√ºssen wir zuerst irgendwie die Struktur der menschlichen Sprache bestimmen.  Gl√ºcklicherweise hilft die Window <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fourier Transform</a> (STFT). <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://player.vimeo.com/video/305391461" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">STFT-Amplitudenspektrum - Fenstergr√∂√üe = 2048, √úberlappung = 75%, logarithmische Frequenzskala [Sonic Visualizer]</font></i> <br><br>  Obwohl ich die Sprachverarbeitung liebe und definitiv gerne mit <i>Eingangsfiltersimulationen, Cepstrums, Sottotami, LPC, MFCC</i> usw. <i>spiele,</i> √ºberspringen <i>wir</i> diesen ganzen Unsinn und konzentrieren <i>uns</i> auf die Hauptelemente unseres Problems, damit der Artikel von m√∂glichst vielen Menschen verstanden werden kann. nicht nur Signalverarbeitungsspezialisten. <br><br>  Was sagt uns die Struktur der menschlichen Sprache? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/541/f8d/c74/541f8dc74fecdb1e994d560b44da112d.png"><br><br>  Nun, wir k√∂nnen hier drei Hauptelemente definieren: <br><br><ul><li>  <b>Die Grundfrequenz</b> (f0), die durch die Schwingungsfrequenz unserer Stimmb√§nder bestimmt wird.  In diesem Fall singt Ariana im Bereich von 300-500 Hz. <br></li><li>  Eine Reihe von <b>Harmonischen</b> √ºber f0, die einer √§hnlichen Form oder einem √§hnlichen Muster folgen.  Diese Harmonischen erscheinen bei Frequenzen, die ein Vielfaches von f0 sind. <br></li><li>  <b>Stimmlose</b> Sprache, die Konsonanten wie 't', 'p', 'k', 's' (die nicht durch Vibration der Stimmb√§nder erzeugt werden), Atmung usw. enth√§lt. All dies √§u√üert sich in Form kurzer Bursts im Hochfrequenzbereich. </li></ul><br><h1>  Erster Versuch mit Regeln </h1><br>  Vergessen wir f√ºr eine Sekunde, was als maschinelles Lernen bezeichnet wird.  Kann eine Vokalextraktionsmethode basierend auf unserer Kenntnis des Signals entwickelt werden?  Lass mich versuchen ... <br><br>  <b><i>Naive</i> Stimmisolation V1.0:</b> <br><br><ol><li>  Identifizieren Sie Bereiche mit Gesang.  Das urspr√ºngliche Signal enth√§lt viele Dinge.  Wir m√∂chten uns auf die Bereiche konzentrieren, die wirklich Vokalinhalte enthalten, und alles andere ignorieren. <br></li><li>  Unterscheiden Sie zwischen stimmhafter und stimmloser Sprache.  Wie wir gesehen haben, sind sie sehr unterschiedlich.  Sie m√ºssen wahrscheinlich anders gehandhabt werden. <br></li><li>  Bewerten Sie die √Ñnderung der Grundfrequenz im Laufe der Zeit. <br></li><li>  Wenden Sie basierend auf Pin 3 eine Art Maske an, um Harmonische zu erfassen. <br></li><li>  Mach etwas mit Fragmenten stimmloser Sprache ... </li></ol><br><img src="https://habrastorage.org/getpro/habr/post_images/ecf/bb4/82e/ecfbb482ea6c1b29b96a13ce5cf8a5a2.gif"><br><br>  Wenn wir w√ºrdig arbeiten, sollte das Ergebnis eine <i>weiche</i> oder <i>Bitmaske sein</i> , deren Anwendung auf die Amplitude der STFT (elementweise Multiplikation) eine ungef√§hre Rekonstruktion der Amplitude der STFT-Vocals ergibt.  Dann kombinieren wir diese vokale STFT mit Informationen √ºber die Phase des urspr√ºnglichen Signals, berechnen die inverse STFT und erhalten das Zeitsignal der rekonstruierten Stimme. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/56d/fb7/012/56dfb70125de59f0e1ee03b58e6c79e6.png"><br><br>  Es von Grund auf neu zu machen ist schon eine gro√üe Aufgabe.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zur</a> Demonstration ist jedoch die Implementierung des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">pYIN-Algorithmus anwendbar</a> .  Es ist zwar beabsichtigt, Schritt 3 zu l√∂sen, aber mit den richtigen Einstellungen f√ºhrt es anst√§ndig die Schritte 1 und 2 aus und verfolgt die Stimmbasis auch bei Vorhandensein von Musik.  Das folgende Beispiel enth√§lt die Ausgabe nach der Verarbeitung dieses Algorithmus, ohne stimmlose Sprache zu verarbeiten. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://player.vimeo.com/video/305636014" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Na und...?  Er scheint die ganze Arbeit erledigt zu haben, aber es gibt keine gute Qualit√§t und N√§he.  Vielleicht werden wir diese Methode verbessern, indem wir mehr Zeit, Energie und Geld investieren ... <br><br>  Aber lass mich dich fragen ... <br><br>  Was passiert, wenn ein <b>paar Stimmen</b> auf dem Track erscheinen und es dennoch h√§ufig in mindestens 50% der modernen professionellen Tracks zu finden ist? <br><br>  Was passiert, wenn der Gesang durch <b>Hall, Delays</b> und andere Effekte verarbeitet wird?  Werfen wir einen Blick auf den letzten Refrain von Ariana Grande aus diesem Lied. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://player.vimeo.com/video/306589126" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  F√ºhlst du schon Schmerzen ...?  Ich ja. <br><br>  Solche Methoden nach strengen Regeln verwandeln sich sehr schnell in ein Kartenhaus.  Das Problem ist zu kompliziert.  Zu viele Regeln, zu viele Ausnahmen und zu viele verschiedene Bedingungen (Effekte und Mix-Einstellungen).  Ein mehrstufiger Ansatz impliziert auch, dass Fehler in einem Schritt die Probleme auf den n√§chsten Schritt ausweiten.  Das Verbessern jedes Schritts wird sehr teuer: Es wird eine gro√üe Anzahl von Iterationen erfordern, um es richtig zu machen.  Und zu guter Letzt ist es wahrscheinlich, dass wir am Ende einen sehr ressourcenintensiven F√∂rderer bekommen, der an sich alle Anstrengungen zunichte machen kann. <br><br>  <b>In einer solchen Situation ist es an der Zeit, √ºber einen <i>umfassenderen</i> Ansatz nachzudenken und ML einen Teil der grundlegenden Prozesse und Operationen herausfinden zu lassen, die zur L√∂sung des Problems erforderlich sind.</b>  <b>Aber wir m√ºssen noch unsere F√§higkeiten unter Beweis stellen und uns mit Feature-Engineering besch√§ftigen, und Sie werden sehen, warum.</b> <br><br><h1>  Hypothese: Verwenden Sie das neuronale Netzwerk als √úbertragungsfunktion, die Mixe in Gesang √ºbersetzt </h1><br>  Wenn Sie sich die Errungenschaften neuronaler Faltungsnetze bei der Verarbeitung von Fotos ansehen, warum sollten Sie hier nicht denselben Ansatz anwenden? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1f0/356/00a/1f035600aadc5f6bb50d7478984aa1d1.png"><br>  <i><font color="gray">Neuronale Netze l√∂sen erfolgreich Probleme wie das Einf√§rben von Bildern, das Sch√§rfen und die Aufl√∂sung.</font></i> <br><br>  Am Ende k√∂nnen Sie sich das Tonsignal ‚Äûals Bild‚Äú mit der kurzfristigen Fourier-Transformation vorstellen, oder?  Obwohl diese <i>Klangbilder</i> nicht der statistischen Verteilung nat√ºrlicher Bilder entsprechen, weisen sie dennoch r√§umliche Muster (in Zeit und Frequenzraum) auf, auf denen das Netzwerk trainiert werden soll. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/54c/b43/7fa/54cb437fa908fbe8b4d36bd120e2d009.png"><br>  <i><font color="gray">Links: Drum Beat und Baseline unten, mehrere Synthesizer-Sounds in der Mitte, alle gemischt mit Gesang.</font></i>  <i><font color="gray">Richtig: nur Gesang</font></i> <br><br>  Die Durchf√ºhrung eines solchen Experiments w√§re ein teures Unterfangen, da es schwierig ist, die erforderlichen Trainingsdaten zu erhalten oder zu generieren.  In der angewandten Forschung versuche ich jedoch immer, diesen Ansatz zu verwenden: Erstens, <b>um ein einfacheres Problem zu identifizieren, das dieselben Prinzipien best√§tigt</b> , aber nicht viel Arbeit erfordert.  Auf diese Weise k√∂nnen Sie die Hypothese bewerten, schneller iterieren und das Modell mit minimalen Verlusten korrigieren, wenn es nicht ordnungsgem√§√ü funktioniert. <br><br>  Die implizite Bedingung ist, dass das <b>neuronale Netzwerk die Struktur der menschlichen Sprache verstehen muss</b> .  Ein einfacheres Problem kann folgendes sein: <i>Kann ein neuronales Netzwerk das Vorhandensein von Sprache auf einem beliebigen Fragment einer Tonaufzeichnung bestimmen?</i>  Es handelt sich um einen zuverl√§ssigen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sprachaktivit√§tsdetektor (VAD)</a> , der in Form eines bin√§ren Klassifikators implementiert ist. <br><br><h3>  Wir gestalten den Raum der Zeichen </h3><br>  Wir wissen, dass Tonsignale wie Musik und menschliche Sprache auf Zeitabh√§ngigkeiten beruhen.  Einfach ausgedr√ºckt, passiert zu einem bestimmten Zeitpunkt nichts isoliert.  Wenn ich wissen m√∂chte, ob eine bestimmte Tonaufnahme eine Stimme enth√§lt, muss ich mir benachbarte Regionen ansehen.  Ein solcher <i>Zeitkontext</i> liefert gute Informationen dar√ºber, was in dem Bereich von Interesse geschieht.  Gleichzeitig ist es w√ºnschenswert, eine Klassifizierung mit sehr kleinen Zeitschritten durchzuf√ºhren, um eine menschliche Stimme mit der h√∂chstm√∂glichen Zeitaufl√∂sung zu erkennen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/471/d58/2d4/471d582d4d8dad39249584940137d4e3.gif"><br><br>  Z√§hlen wir ein wenig ... <br><br><ul><li>  Abtastfrequenz (fs): 22050 Hz (wir senken von 44100 auf 22050) <br></li><li>  STFT-Design: Fenstergr√∂√üe = 1024, Sprunggr√∂√üe = 256, Kreidelinieninterpolation f√ºr den Gewichtungsfilter unter Ber√ºcksichtigung der Wahrnehmung.  Da unsere Eingabe <i>real ist</i> , k√∂nnen Sie mit der H√§lfte der STFT arbeiten (eine Erkl√§rung w√ºrde den Rahmen dieses Artikels sprengen ...), w√§hrend Sie die DC-Komponente (optional) beibehalten, wodurch wir 513 Frequenzbereiche erhalten. <br></li><li>  Zielklassifizierungsaufl√∂sung: ein STFT-Rahmen (~ 11,6 ms = 256/22050) <br></li><li>  Zielzeitkontext: ~ 300 Millisekunden = 25 STFT-Frames. <br></li><li>  Die angestrebte Anzahl von Trainingsbeispielen: 500 Tausend. <br></li><li>  Angenommen, wir verwenden ein Schiebefenster mit einem Schritt von 1 STFT-Zeitrahmen, um Trainingsdaten zu generieren, ben√∂tigen wir ungef√§hr 1,6 Stunden beschrifteten Sound, um 500.000 Datenproben zu generieren </li></ul><br>  Mit den oben genannten Anforderungen sind die Ein- und Ausgabe unseres Bin√§rklassifikators wie folgt: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f57/01c/bd9/f5701cbd91fe9ba38f89b541b9d4492e.png"><br><br><h3>  Modell </h3><br>  Mit Keras werden wir ein kleines Modell eines neuronalen Netzwerks erstellen, um unsere Hypothese zu testen. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense, Dropout, Flatten, Conv2D, MaxPooling2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.optimizers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SGD <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.advanced_activations <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LeakyReLU model = Sequential() model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">513</span></span>, <span class="hljs-number"><span class="hljs-number">25</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(LeakyReLU()) model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(LeakyReLU()) model.add(MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>))) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.25</span></span>)) model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(LeakyReLU()) model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(LeakyReLU()) model.add(MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>))) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.25</span></span>)) model.add(Flatten()) model.add(Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>)) model.add(LeakyReLU()) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)) sgd = SGD(lr=<span class="hljs-number"><span class="hljs-number">0.001</span></span>, decay=<span class="hljs-number"><span class="hljs-number">1e-6</span></span>, momentum=<span class="hljs-number"><span class="hljs-number">0.9</span></span>, nesterov=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) model.compile(loss=keras.losses.binary_crossentropy, optimizer=sgd, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/447/ebe/19f/447ebe19f6ba953b4af0b4bed1d9e7af.png"></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/7d1/002/083/7d1002083c78486e36dd92959ec5afbd.png"><br><br>  Durch die Aufteilung von 80/20 Daten in Training und Tests nach ~ 50 Epochen erhalten wir die <b>Genauigkeit beim Testen von ~ 97%</b> .  Dies ist ein ausreichender Beweis daf√ºr, dass unser Modell in der Lage ist, zwischen Gesang in musikalischen Klangfragmenten (und Fragmenten ohne Gesang) zu unterscheiden.  Wenn wir einige Feature-Maps aus der 4. Faltungsschicht √ºberpr√ºfen, k√∂nnen wir schlie√üen, dass das neuronale Netzwerk seine Kernel f√ºr zwei Aufgaben optimiert zu haben scheint: Filtern von Musik und Filtern von Gesang ... <br><br><img src="https://habrastorage.org/getpro/habr/post_images/438/bce/536/438bce536c3aa746a3120e2364b512c8.png"><br>  <i><font color="gray">Ein Beispiel f√ºr eine Karte von Objekten am Ausgang der 4. Faltungsschicht.</font></i>  <i><font color="gray">Anscheinend ist die Ausgabe auf der linken Seite das Ergebnis von Kerneloperationen, bei denen versucht wurde, den Sprachinhalt beizubehalten, w√§hrend Musik ignoriert wurde.</font></i>  <i><font color="gray">Hohe Werte √§hneln der harmonischen Struktur der menschlichen Sprache.</font></i>  <i><font color="gray">Die Objektkarte rechts scheint das Ergebnis der entgegengesetzten Aufgabe zu sein.</font></i> <br><br><h1>  Vom Sprachdetektor zur Signaltrennung </h1><br>  Wie k√∂nnen wir, nachdem wir das einfachere Klassifizierungsproblem gel√∂st haben, zur wirklichen Trennung von Gesang und Musik √ºbergehen?  Nun, wenn wir uns die erste <i>naive</i> Methode <i>ansehen</i> , wollen wir immer noch irgendwie ein Amplitudenspektrogramm f√ºr den Gesang erhalten.  Dies wird nun zu einer Regressionsaufgabe.  Was wir tun wollen, ist, das entsprechende Amplitudenspektrum f√ºr die Vocals in diesem Zeitrahmen aus der STFT des urspr√ºnglichen Signals, dh aus der Mischung (mit einem ausreichenden Zeitkontext) zu berechnen. <br><br>  <b>Was ist mit dem Trainingsdatensatz?</b>  <b>(Sie k√∂nnen mich in diesem Moment fragen)</b> <br><br>  Verdammt ... warum so.  Ich wollte dies am Ende des Artikels ber√ºcksichtigen, um nicht vom Thema abgelenkt zu werden! <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9ac/67f/91d/9ac67f91d85022f6bbc75f296ce3f04a.png"><br><br>  Wenn unser Modell gut ausgebildet ist, m√ºssen Sie f√ºr eine logische Schlussfolgerung nur ein einfaches Schiebefenster f√ºr den STFT-Mix implementieren.  Verschieben Sie das Fenster nach jeder Vorhersage um 1 Zeitrahmen nach rechts, sagen Sie das n√§chste Bild mit Gesang voraus und verkn√ºpfen Sie es mit der vorherigen Vorhersage.  Nehmen wir f√ºr das Modell dasselbe Modell, das f√ºr den Sprachdetektor verwendet wurde, und nehmen Sie kleine √Ñnderungen vor: Die Ausgangswellenform ist jetzt (513.1), lineare Aktivierung am Ausgang, MSE als Funktion der Verluste.  Jetzt beginnen wir mit dem Training. <br><br>  <b>Freue dich noch nicht ...</b> <br><br>  Obwohl diese Darstellung von E / A nach mehrmaligem Training unseres Modells mit verschiedenen Parametern und Datennormalisierungen sinnvoll ist, gibt es keine Ergebnisse.  Es scheint, wir fragen zu viel ... <br><br>  Wir sind von einem bin√§ren Klassifikator zur <i>Regression</i> eines 513-dimensionalen Vektors √ºbergegangen.  Obwohl das Netzwerk das Problem bis zu einem gewissen Grad untersucht, weisen die wiederhergestellten Vocals immer noch offensichtliche Artefakte und St√∂rungen durch andere Quellen auf.  Selbst nach dem Hinzuf√ºgen zus√§tzlicher Ebenen und dem Erh√∂hen der Anzahl der Modellparameter √§ndern sich die Ergebnisse nicht wesentlich.  Und dann stellt sich die Frage: <b>Wie kann die Aufgabe f√ºr das Netzwerk durch T√§uschung ‚Äûvereinfacht‚Äú und gleichzeitig die gew√ºnschten Ergebnisse erzielt werden?</b> <br><br>  Was ist, wenn wir anstelle der Sch√§tzung der Amplitude der STFT-Vocals das Netzwerk trainieren, um eine bin√§re Maske zu erhalten, die bei Anwendung auf die STFT-Mischung ein vereinfachtes, aber <b>wahrnehmbar akzeptables</b> Amplitudenspektrogramm der Vocals liefert? <br><br>  Beim Experimentieren mit verschiedenen Heuristiken haben wir eine sehr einfache (und nat√ºrlich in Bezug auf die Signalverarbeitung unorthodoxe ...) Methode gefunden, um Vocals mithilfe von Bin√§rmasken aus Mixes zu extrahieren.  Ohne auf Details einzugehen, ist das Wesentliche wie folgt.  Stellen Sie sich die Ausgabe als Bin√§rbild vor, wobei der Wert '1' das <b>vorherrschende Vorhandensein von Stimminhalten</b> bei einer bestimmten Frequenz und einem bestimmten Zeitrahmen und der Wert '0' das vorherrschende Vorhandensein von Musik an einem bestimmten Ort angibt.  Wir k√∂nnen es die <i>Binarisierung der Wahrnehmung nennen</i> , nur um einen Namen zu finden.  Optisch sieht es ziemlich h√§sslich aus, um ehrlich zu sein, aber die Ergebnisse sind √ºberraschend gut. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/16f/857/721/16f85772187f89a73baf7fe0158aba2c.png"><br><br>  Jetzt wird unser Problem zu einer Art hybrider Regressionsklassifikation (sehr grob ...).  Wir fordern das Modell auf, ‚ÄûPixel‚Äú am Ausgang als vokal oder nicht vokal zu klassifizieren, obwohl die Aufgabe konzeptionell (sowie aus Sicht der verwendeten MSE-Verlustfunktion) regressiv bleibt. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e21/6a0/654/e216a065488c058c37e2758563ff4052.png"><br><br>  Obwohl diese Unterscheidung f√ºr einige unangemessen erscheint, ist sie f√ºr die F√§higkeit des Modells, die Aufgabe zu untersuchen, von gro√üer Bedeutung. Die zweite ist einfacher und begrenzter.  Gleichzeitig k√∂nnen wir so unser Modell in Bezug auf die Anzahl der Parameter angesichts der Komplexit√§t der Aufgabe relativ klein halten, was f√ºr das Arbeiten in Echtzeit sehr w√ºnschenswert ist, was in diesem Fall eine Entwurfsanforderung war.  Nach einigen kleinen √Ñnderungen sieht das endg√ºltige Modell so aus. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/416/d25/856/416d2585671e97a1f39c9584a30d4bbf.png"></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/21e/04e/4d5/21e04e4d5642a3282aa445846c64c576.png"><br><br><h3>  Wie kann ein Zeitbereichssignal wiederhergestellt werden? </h3><br>  In der Tat wie bei der <i>naiven Methode</i> .  In diesem Fall sagen wir f√ºr jeden Durchgang einen Zeitrahmen der bin√§ren Vokalmaske voraus.  Bei der Realisierung eines einfachen Schiebefensters mit einem Schritt von einem Zeitrahmen bewerten und kombinieren wir weiterhin aufeinanderfolgende Zeitrahmen, die letztendlich die gesamte vokale Bin√§rmaske bilden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a34/2a5/ae1/a342a5ae1b0ca37825978f7b92d574cb.gif"><br><br><h3>  Erstellen Sie ein Trainingsset </h3><br>  Wie Sie wissen, besteht eines der Hauptprobleme beim Unterrichten mit einem Lehrer (lassen Sie diese Spielzeugbeispiele mit vorgefertigten Datens√§tzen) in den richtigen Daten (in Quantit√§t und Qualit√§t) f√ºr das spezifische Problem, das Sie l√∂sen m√∂chten.  Basierend auf den beschriebenen Darstellungen von Eingabe und Ausgabe ben√∂tigen Sie zum Trainieren unseres Modells zun√§chst eine erhebliche Anzahl von Mixes und die entsprechenden, perfekt ausgerichteten und normalisierten Gesangsspuren.  Dieses Set kann auf verschiedene Arten erstellt werden, und wir haben eine Kombination von Strategien verwendet, von der manuellen Erstellung von Paaren [Mix &lt;-&gt; Vocals] basierend auf mehreren im Internet gefundenen Cappels bis hin zur Suche nach Rockband-Musikmaterial und Youtube-Scrapbooking.  Um Ihnen eine Vorstellung davon zu geben, wie m√ºhsam und schmerzhaft dieser Prozess ist, war Teil des Projekts die Entwicklung eines solchen Tools zum automatischen Erstellen von Paaren [Mix &lt;-&gt; Vocals]: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/617/b71/5ac/617b715acc8d8c913752054f84214c8b.png"><br><br>  Das neuronale Netzwerk ben√∂tigt eine sehr gro√üe Datenmenge, um die √úbertragungsfunktion f√ºr das Senden von Mixes an Gesang zu erlernen.  Unser endg√ºltiger Satz bestand aus ungef√§hr 15 Millionen Samples von 300-ms-Mixen und den entsprechenden bin√§ren Vokalmasken. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/21f/01a/02d/21f01a02d22dfc4f2d615e511cf470a6.png"><br><br><h3>  Pipeline-Architektur </h3><br>  Wie Sie wahrscheinlich wissen, ist das Erstellen eines ML-Modells f√ºr eine bestimmte Aufgabe nur die halbe Miete.  In der realen Welt m√ºssen Sie √ºber die Softwarearchitektur nachdenken, insbesondere wenn Sie in Echtzeit oder in der N√§he arbeiten m√ºssen. <br><br>  In dieser speziellen Implementierung kann die Rekonstruktion im Zeitbereich unmittelbar nach der Vorhersage der vollst√§ndigen bin√§ren Gesangsmaske (Standalone-Modus) oder, was noch interessanter ist, im Multithread-Modus erfolgen, in dem wir Daten empfangen und verarbeiten, Vocals wiederherstellen und Ton wiedergeben - alles in kleinen Segmenten nahe Streaming und sogar fast in Echtzeit, Verarbeitung von Musik, die im laufenden Betrieb mit minimaler Verz√∂gerung aufgenommen wird.  Eigentlich ist dies ein separates Thema, und ich werde es f√ºr einen weiteren Artikel <b>√ºber Echtzeit-ML-Pipelines belassen</b> ... <br><br><h1>  Ich glaube, ich habe genug gesagt, warum also nicht ein paar Beispiele anh√∂ren? </h1><br><h3>  Daft Punk - Get Lucky (Studioaufnahme) </h3><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://player.vimeo.com/video/315172280" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Hier k√∂nnen Sie einige minimale St√∂rungen durch die Trommeln h√∂ren ...</font></i> <br><br><h3>  Adele - Z√ºnde den Regen an (Live-Aufnahme!) </h3><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://player.vimeo.com/video/315172388" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Beachten Sie, wie unser Modell ganz am Anfang die Schreie der Menge als stimmlichen Inhalt extrahiert :).</font></i>  <i><font color="gray">In diesem Fall treten St√∂rungen durch andere Quellen auf.</font></i>  <i><font color="gray">Da es sich um eine Live-Aufnahme handelt, scheint es akzeptabel, dass die extrahierten Vocals eine schlechtere Qualit√§t aufweisen als die vorherigen.</font></i> <br><br><h1>  Ja und "etwas anderes" ... </h1><br><h1>  Wenn das System f√ºr Gesang funktioniert, warum nicht auf andere Instrumente anwenden ...? </h1><br>  Der Artikel ist bereits ziemlich umfangreich, aber angesichts der geleisteten Arbeit verdienen Sie es, die neueste Demo zu h√∂ren.  Mit genau der gleichen Logik wie beim Extrahieren von Gesang k√∂nnen wir versuchen, die Stereomusik in Komponenten (Schlagzeug, Bass, Gesang, andere) zu unterteilen, einige √Ñnderungen an unserem Modell vorzunehmen und nat√ºrlich das entsprechende Trainingsset zu haben :). <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://player.vimeo.com/video/315173879" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Danke f√ºrs Lesen.  Als letzte Anmerkung: Wie Sie sehen k√∂nnen, ist das tats√§chliche Modell unseres Faltungsnetzwerks nicht so speziell.  Der Erfolg dieser Arbeit wurde durch <b>Feature Engineering</b> und den √ºbersichtlichen Hypothesentestprozess bestimmt, √ºber den ich in zuk√ºnftigen Artikeln schreiben werde! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de441090/">https://habr.com/ru/post/de441090/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de441076/index.html">Skriptoptimierung mit Webpack SplitChunksPlugin</a></li>
<li><a href="../de441078/index.html">LG wird ein Smartphone mit einem OLED-Lautsprecherbildschirm vorstellen: ein paar Worte zum neuen Ger√§t und zur neuen Technologie</a></li>
<li><a href="../de441082/index.html">Mars-Tickets kosten weniger als 500.000 US-Dollar</a></li>
<li><a href="../de441084/index.html">Wohin gingen die Early Adopters?</a></li>
<li><a href="../de441088/index.html">Entwickler, denken Sie daran - der Datenverkehr Ihrer Anwendung wird √ºberwacht</a></li>
<li><a href="../de441092/index.html">Embedded World 2019 - die gr√∂√üte Ausstellung f√ºr eingebettete Elektronik</a></li>
<li><a href="../de441096/index.html">Simulator liest Artikel</a></li>
<li><a href="../de441098/index.html">SIEM-Tiefen: Out-of-Box-Korrelationen. Teil 4. Systemmodell als Kontext von Korrelationsregeln</a></li>
<li><a href="../de441102/index.html">Kaspersky Mobile Talks - ein Treffen f√ºr fortgeschrittene Entwickler</a></li>
<li><a href="../de441104/index.html">Abrufen von Informationen und Umgehen der Zwei-Faktor-Authentifizierung auf Bankkarten von TOP-10 (Ukraine)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>