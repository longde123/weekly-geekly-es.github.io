<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üåë ‚ùáÔ∏è üëµüèæ So skalieren Sie Rechenzentren. Yandex-Bericht ‚è±Ô∏è üôÖ üëõ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wir haben ein Netzwerk von Rechenzentren entwickelt, mit dem Sie Computercluster mit mehr als 100.000 Servern und einer Teilungsbandbreite von mehr al...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>So skalieren Sie Rechenzentren. Yandex-Bericht</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/476146/"> Wir haben ein Netzwerk von Rechenzentren entwickelt, mit dem Sie Computercluster mit mehr als 100.000 Servern und einer Teilungsbandbreite von mehr als einem Petabit pro Sekunde bereitstellen k√∂nnen. <br><br>  Aus dem Bericht von Dmitry Afanasyev erfahren Sie mehr √ºber die Grundprinzipien des neuen Designs, die Skalierungstopologien, die mit diesen Problemen einhergehen, L√∂sungen f√ºr diese Probleme sowie die Merkmale des Routings und der Skalierung der Weiterleitungsebenenfunktionen moderner Netzwerkger√§te in dicht verbundenen Topologien mit einer gro√üen Anzahl von ECMP-Routen .  Dar√ºber hinaus sprach Dima kurz √ºber die Organisation der externen Konnektivit√§t, die physische Ebene, das Kabelsystem und M√∂glichkeiten zur weiteren Steigerung der Kapazit√§t. <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/webt/rd/dy/sc/rddysciofis_rkvkotpo1tr52i0.jpeg"></a> <br><br>  - Guten Tag allerseits!  Mein Name ist Dmitry Afanasyev, ich bin ein Netzwerkarchitekt von Yandex und besch√§ftige mich haupts√§chlich mit dem Design von Rechenzentrumsnetzwerken. <br><a name="habracut"></a><br><img src="https://habrastorage.org/webt/zv/gl/5v/zvgl5vyexhel7a_6_mxp8wm4qoe.jpeg"><br><br>  Meine Geschichte dreht sich um das aktualisierte Rechenzentrumsnetzwerk von Yandex.  Dies ist gr√∂√ütenteils eine Weiterentwicklung unseres Designs, aber gleichzeitig gibt es einige neue Elemente.  Dies ist eine Review-Pr√§sentation, da in kurzer Zeit viele Informationen angepasst werden mussten.  Wir beginnen mit der Wahl einer logischen Topologie.  Dann wird es einen √úberblick √ºber die Steuerebene und die Probleme mit der Skalierbarkeit der Datenebene geben. Schauen wir uns einige Funktionen der Ger√§te an.  Wir werden auch auf die Entwicklungen im Rechenzentrum mit MPLS eingehen, √ºber die wir vor einiger Zeit gesprochen haben. <br><br><img src="https://habrastorage.org/webt/ew/lm/rc/ewlmrcydwmv1s3gz2irvhuyev-q.jpeg"><br><br>  Was ist Yandex in Bezug auf Workloads und Services?  Yandex ist ein typischer Hyperscaler.  Wenn Sie in Richtung Benutzer schauen, bearbeiten wir in erster Linie Benutzeranfragen.  Auch verschiedene Streaming-Dienste und Datenausgabe, weil wir auch Speicherdienste haben.  Wenn es n√§her am Backend liegt, werden dort Infrastrukturlasten und -dienste angezeigt, z. B. verteilte Objektspeicher, Datenreplikation und nat√ºrlich persistente Warteschlangen.  Eine der Hauptarten von Lasten ist MapReduce und dergleichen, Streaming-Verarbeitung, maschinelles Lernen usw. <br><br><img src="https://habrastorage.org/webt/qi/71/qd/qi71qdjq7zbaxhmfmqbd6y66fba.jpeg"><br><br>  Wie ist die Infrastruktur, auf der dies alles geschieht?  Auch hier sind wir ein sehr typischer Hyperskaler, obwohl wir uns vielleicht etwas n√§her an der Seite des Spektrums befinden, an der sich die kleineren Hyperskaler befinden.  Aber wir haben alle Attribute.  Wo immer m√∂glich, verwenden wir Standardhardware und horizontale Skalierung.  Das Ressourcen-Pooling w√§chst stetig: Wir arbeiten nicht mit separaten Maschinen und Racks, sondern kombinieren sie zu einem gro√üen Pool austauschbarer Ressourcen mit einigen zus√§tzlichen Services, die f√ºr die Planung und Zuordnung zust√§ndig sind. Wir arbeiten mit all diesen Pools. <br><br>  Wir haben also die n√§chste Ebene - das Computing-Cluster auf Betriebssystemebene.  Es ist sehr wichtig, dass wir den von uns verwendeten Technologie-Stack vollst√§ndig kontrollieren.  Wir kontrollieren Endpunkte (Hosts), Netzwerk und Software-Stack. <br><br>  Wir haben mehrere gro√üe Rechenzentren in Russland und im Ausland.  Sie werden mithilfe der MPLS-Technologie durch ein Backbone verbunden.  Unsere interne Infrastruktur basiert fast ausschlie√ülich auf IPv6. Da wir jedoch externen Datenverkehr verarbeiten m√ºssen, der noch immer haupts√§chlich √ºber IPv4 bereitgestellt wird, m√ºssen wir die √ºber IPv4 eingehenden Anforderungen irgendwie an die Front-End-Server weiterleiten und noch ein wenig an externe IPv4-Server weiterleiten. Internet - zum Beispiel zur Indizierung. <br><br>  In den letzten Iterationen des Netzwerkdesigns f√ºr Rechenzentren werden mehrstufige Clos-Topologien verwendet, in denen nur L3 verwendet wird.  Wir haben L2 vor einiger Zeit verlassen und atmeten erleichtert auf.  Schlie√ülich umfasst unsere Infrastruktur Hunderttausende von Computerinstanzen (Serverinstanzen).  Die maximale Clustergr√∂√üe betrug vor einiger Zeit etwa 10.000 Server.  Dies ist vor allem darauf zur√ºckzuf√ºhren, wie dieselben Betriebssysteme auf Cluster-Ebene - Scheduler, Ressourcenzuweisung usw. - funktionieren k√∂nnen. Da auf der Seite der Infrastruktursoftware Fortschritte erzielt wurden, sind jetzt etwa 100.000 Server in einem Computercluster vorgesehen Wir hatten die Aufgabe, Netzwerkfabriken aufzubauen, die eine effiziente B√ºndelung von Ressourcen in einem solchen Cluster erm√∂glichen. <br><br><img src="https://habrastorage.org/webt/ra/co/cb/racocb4wtrqihsf4qcpswzzihhc.jpeg"><br><br>  Was wollen wir von einem Rechenzentrumsnetzwerk?  Erstens - viel billige und ziemlich gleichm√§√üig verteilte Bandbreite.  Weil das Netzwerk das Substrat ist, √ºber das wir Ressourcen b√ºndeln k√∂nnen.  Die neue Zielgr√∂√üe betr√§gt ungef√§hr 100.000 Server in einem Cluster. <br><br>  Nat√ºrlich wollen wir auch eine skalierbare und stabile Steuerebene, da bei einer so gro√üen Infrastruktur selbst bei zuf√§lligen Ereignissen viele Kopfschmerzen auftreten und die Steuerebene uns keine Kopfschmerzen bereiten soll.  Gleichzeitig wollen wir den darin enthaltenen Zustand minimieren.  Je kleiner der Zustand, desto besser und stabiler funktioniert alles, desto einfacher ist die Diagnose. <br><br>  Nat√ºrlich brauchen wir Automatisierung, weil es unm√∂glich ist, eine solche Infrastruktur manuell zu verwalten, und das war vor einiger Zeit unm√∂glich.  Wann immer m√∂glich, ben√∂tigen wir soweit m√∂glich operative Unterst√ºtzung und CI / CD-Unterst√ºtzung. <br><br>  Bei solchen Gr√∂√üen von Rechenzentren und Clustern ist die Aufgabe, die schrittweise Bereitstellung und Erweiterung ohne Unterbrechung des Dienstes zu unterst√ºtzen, sehr akut geworden.  Wenn in Clustern die Gr√∂√üe von tausend Autos wahrscheinlich nahe bei zehntausend Maschinen liegt, k√∂nnen sie immer noch als ein Vorgang ausgerollt werden - das hei√üt, wir planen, die Infrastruktur zu erweitern, und mehrere tausend Maschinen werden als ein Vorgang hinzugef√ºgt, dann entsteht kein Cluster mit der Gr√∂√üe von hunderttausend Autos einfach so, es ist seit einiger zeit gebaut worden.  Und es ist w√ºnschenswert, dass die gesamte Zeit, die bereits abgepumpt wurde, die bereitgestellte Infrastruktur zur Verf√ºgung steht. <br><br>  Und eine Anforderung, die wir hatten und hinter uns gelassen haben: Dies ist die Unterst√ºtzung f√ºr Mandantenf√§higkeit, dh Virtualisierung oder Netzwerksegmentierung.  Jetzt m√ºssen wir dies nicht mehr auf der Netzwerkfactory-Ebene tun, da die Segmentierung an die Hosts ging und dies die Skalierung f√ºr uns sehr einfach machte.  Dank IPv6 und eines gro√üen Adressraums mussten in der internen Infrastruktur keine doppelten Adressen verwendet werden, die Adressierung war bereits eindeutig.  Aufgrund der Tatsache, dass Filterung und Netzwerksegmentierung auf Hosts angewendet wurden, m√ºssen in Rechenzentrumsnetzwerken keine virtuellen Netzwerkeinheiten erstellt werden. <br><br><img src="https://habrastorage.org/webt/nx/hi/9l/nxhi9l-ggxw-kvhbuoeldgkhnno.jpeg"><br><br>  Eine sehr wichtige Sache ist, dass wir nicht brauchen.  Wenn einige Funktionen aus dem Netzwerk entfernt werden k√∂nnen, vereinfacht dies die Lebensdauer erheblich und erweitert in der Regel die Auswahl an verf√ºgbarer Hardware und Software und vereinfacht die Diagnose erheblich. <br><br>  Was brauchen wir also nicht, was konnten wir ablehnen, nicht immer mit Freude in dem Moment, als dies geschah, sondern mit gro√üer Erleichterung, als der Prozess abgeschlossen war? <br><br>  Zun√§chst die Ablehnung von L2.  Wir brauchen L2 weder real noch emuliert.  Wird aufgrund der Tatsache, dass wir den Anwendungsstapel steuern, nicht in gro√üem Umfang verwendet.  Unsere Anwendungen sind horizontal skaliert, sie arbeiten mit L3-Adressierung, sie sorgen sich nicht wirklich darum, dass eine bestimmte Instanz ausf√§llt, sie rollen nur eine neue aus, sie m√ºssen nicht auf der alten Adresse rollen, da es eine separate Ebene f√ºr die Serviceerkennung und √úberwachung von Maschinen im Cluster gibt .  Wir verlagern diese Aufgabe nicht auf das Netzwerk.  Die Aufgabe des Netzwerks besteht darin, Pakete von Punkt A zu Punkt B zu liefern. <br><br>  Es gibt auch keine Situationen, in denen sich Adressen innerhalb des Netzwerks bewegen, und dies muss √ºberwacht werden.  In vielen Designs wird dies normalerweise zur Unterst√ºtzung der VM-Mobilit√§t ben√∂tigt.  Wir nutzen die Mobilit√§t virtueller Maschinen nicht in der internen Infrastruktur genau des gro√üen Yandex, und wir sind au√üerdem der Ansicht, dass dies mit der Netzwerkunterst√ºtzung auch dann nicht m√∂glich sein sollte.  Wenn Sie dies wirklich tun m√ºssen, m√ºssen Sie dies auf Hostebene tun und die Adressen, die in Overlays migrieren k√∂nnen, so steuern, dass das darunterliegende Routingsystem selbst (Transportnetzwerk) nicht ber√ºhrt oder zu viele dynamische √Ñnderungen vorgenommen werden. <br><br>  Eine andere Technologie, die wir nicht verwenden, ist Multicast.  Ich kann Ihnen im Detail sagen warum.  Das macht das Leben viel einfacher, denn wenn sich jemand mit ihm befasst und beobachtet, wie genau die Steuerebene eines Multicasts aussieht - in allen Installationen, mit Ausnahme der einfachsten, sind dies gro√üe Kopfschmerzen.  Dar√ºber hinaus ist es beispielsweise schwierig, eine gut funktionierende Open Source-Implementierung zu finden. <br><br>  Schlie√ülich gestalten wir unsere Netzwerke so, dass sie nicht zu viele √Ñnderungen aufweisen.  Wir k√∂nnen uns darauf verlassen, dass der Fluss externer Ereignisse im Routing-System gering ist. <br><br><img src="https://habrastorage.org/webt/qa/nz/e3/qanze3l9jiktreg59urwgnfmpec.jpeg"><br><br>  Welche Probleme treten auf und welche Einschr√§nkungen sollten bei der Entwicklung eines Rechenzentrumsnetzwerks ber√ºcksichtigt werden?  Kosten nat√ºrlich.  Skalierbarkeit, auf welches Niveau wir wachsen wollen.  Die Notwendigkeit einer Erweiterung, ohne den Dienst zu stoppen.  Bandbreitenverf√ºgbarkeit.  Die Sichtbarkeit dessen, was im Netzwerk passiert, f√ºr √úberwachungssysteme, f√ºr Betriebsteams.  Die Unterst√ºtzung f√ºr die Automatisierung ist wiederum so weit wie m√∂glich, da verschiedene Aufgaben auf verschiedenen Ebenen gel√∂st werden k√∂nnen, einschlie√ülich der Einf√ºhrung zus√§tzlicher Ebenen.  Gut und nicht- [wann immer m√∂glich] -abh√§ngig von Anbietern.  Obwohl in verschiedenen historischen Perioden, je nachdem, welcher Abschnitt zu betrachten war, war diese Unabh√§ngigkeit leichter oder schwerer zu erreichen.  Wenn wir einen Teil der Chips von Netzwerkger√§ten nehmen, dann reden wir bis vor kurzem √ºber die Unabh√§ngigkeit von Anbietern. Wenn wir auch Chips mit hohem Durchsatz wollen, kann dies sehr willk√ºrlich sein. <br><br><img src="https://habrastorage.org/webt/fd/bq/s1/fdbqs1_kegdd7s4twvg144y0rus.jpeg"><br><br>  Mit welcher logischen Topologie bauen wir unser Netzwerk auf?  Dies wird ein mehrstufiges Clos sein.  Tats√§chlich gibt es derzeit keine wirklichen Alternativen.  Und die Clos-Topologie ist gut genug, auch wenn wir sie mit verschiedenen fortgeschrittenen Topologien vergleichen, die jetzt eher im Bereich des akademischen Interesses liegen, wenn wir Switches mit einer gro√üen Basis haben. <br><br><img src="https://habrastorage.org/webt/4y/pk/tt/4ypkttuvzezqubmxx3te-envyzo.jpeg"><br><br>  Wie ist das geschichtete Clos-Netzwerk ungef√§hr aufgebaut und wie hei√üen die verschiedenen Elemente darin?  Zuallererst stieg der Wind auf, um herauszufinden, wo der Norden, wo der S√ºden, wo der Osten, wo der Westen ist.  Netze dieses Typs werden normalerweise von Personen gebaut, die einen sehr gro√üen Verkehr von West nach Ost haben.  Wie f√ºr den Rest der Elemente ist oben ein virtueller Schalter gezeigt, der aus kleineren Schaltern zusammengesetzt ist.  Dies ist die Grundidee des rekursiven Aufbaus von Clos-Netzwerken.  Wir nehmen Elemente mit einer Art Radix und verbinden sie, so dass das, was passiert ist, als Switch mit einer gr√∂√üeren Radix betrachtet werden kann.  Wenn Sie noch mehr ben√∂tigen, kann der Vorgang wiederholt werden. <br><br>  In F√§llen, in denen es zum Beispiel bei Clos mit zwei Ebenen m√∂glich ist, Komponenten, die in meinem Diagramm vertikal sind, eindeutig zu unterscheiden, werden sie normalerweise als Ebenen bezeichnet.  Wenn wir Clos-mit drei Ebenen von Wirbels√§ulenschaltern bauen w√ºrden (alle, die keine Borderline- und keine ToR-Schalter sind und nur f√ºr den Transit verwendet werden), w√ºrden die Flugzeuge komplizierter aussehen, zwei Ebenen sehen so aus.  Den Block der ToR- oder Leaf-Schalter und die zugeh√∂rigen Wirbels√§ulenschalter der ersten Ebene nennen wir Pod.  Wirbels√§ulenschalter der Stufe 1 oben auf dem Pod befinden sich oben auf dem Pod und oben auf dem Pod.  Die Schalter, die sich oben in der gesamten Fabrik befinden, sind die oberste Schicht der Fabrik, Top of Fabric. <br><br><img src="https://habrastorage.org/webt/k-/sk/dd/k-skddvn98f6y-jk2e_5zzvlm4a.jpeg"><br><br>  Nat√ºrlich stellt sich die Frage: Clos-Netze sind seit einiger Zeit aufgebaut, die Idee selbst stammt in der Regel aus der Zeit der klassischen Telefonie, TDM-Netze.  Vielleicht ist etwas Besseres aufgetaucht, vielleicht kannst du irgendwie etwas Besseres tun?  Ja und nein  Theoretisch ja, in der Praxis in naher Zukunft definitiv nicht.  Da es eine Reihe interessanter Topologien gibt, werden einige sogar in der Produktion verwendet. Dragonfly wird beispielsweise in HPC-Anwendungen verwendet.  Es gibt auch interessante Topologien wie Xpander, FatClique, Jellyfish.  Wenn Sie sich k√ºrzlich Berichte zu Konferenzen wie SIGCOMM oder NSDI ansehen, finden Sie eine relativ gro√üe Anzahl von Artikeln zu alternativen Topologien mit besseren Eigenschaften (die eine oder andere) als Clos. <br><br>  Alle diese Topologien haben jedoch eine interessante Eigenschaft.  Es verhindert ihre Implementierung in den Netzwerken von Rechenzentren, die wir auf Standardhardware aufbauen wollen und die angemessenes Geld kosten.  In all diesen alternativen Topologien ist der gr√∂√üte Teil des Bandes leider nicht √ºber die k√ºrzesten Wege zug√§nglich.  Daher verlieren wir sofort die F√§higkeit, die traditionelle Steuerebene zu verwenden. <br><br>  Theoretisch ist die L√∂sung des Problems bekannt.  Dies sind beispielsweise Modifikationen des Verbindungszustands unter Verwendung des k-k√ºrzesten Pfades, es gibt jedoch wiederum keine Protokolle, die in der Produktion implementiert und in gro√üem Umfang auf Ger√§ten verf√ºgbar w√§ren. <br><br>  Da der gr√∂√üte Teil der Kapazit√§t nicht auf k√ºrzestem Weg erreichbar ist, m√ºssen wir nicht nur die Steuerebene so √§ndern, dass alle diese Pfade ausgew√§hlt werden (und dies ist √ºbrigens ein viel gr√∂√üerer Zustand in der Steuerebene).  Die Weiterleitungsebene muss noch ge√§ndert werden. In der Regel sind mindestens zwei zus√§tzliche Funktionen erforderlich.  Dies ist eine Gelegenheit, alle Entscheidungen √ºber die Weiterleitung von Paketen einmalig zu treffen, beispielsweise auf einem Host.  Dies ist tats√§chlich Quell-Routing. In der Literatur zu Verbindungsnetzen wird es manchmal als All-at-Once-Weiterleitungsentscheidung bezeichnet.  Und adaptives Routing ist bereits eine Funktion, die wir f√ºr Netzwerkelemente ben√∂tigen. Dies f√ºhrt beispielsweise dazu, dass wir den n√§chsten Hop anhand der Informationen zur geringsten Last in der Warteschlange ausw√§hlen.  Beispielsweise sind andere Optionen m√∂glich. <br><br>  Die Richtung ist also interessant, aber leider k√∂nnen wir sie derzeit nicht anwenden. <br><br><img src="https://habrastorage.org/webt/5d/ki/zy/5dkizyihr8k0owfwar8yjdkx5m0.jpeg"><br><br>  Okay, entschied sich f√ºr die logische Topologie von Clos.  Wie werden wir es skalieren?  Mal sehen, wie es funktioniert und was getan werden kann. <br><br><img src="https://habrastorage.org/webt/ap/dy/-2/apdy-2dnjdljys76pfv4rw7sr-k.jpeg"><br><br>  Im Clos-Netzwerk gibt es zwei Hauptparameter, die wir irgendwie variieren und bestimmte Ergebnisse erzielen k√∂nnen: Radix-Elemente und die Anzahl der Ebenen im Netzwerk.  Ich zeige schematisch, wie der eine und der andere die Gr√∂√üe beeinflusst.  Idealerweise kombinieren wir beides. <br><br><img src="https://habrastorage.org/webt/5d/ki/zy/5dkizyihr8k0owfwar8yjdkx5m0.jpeg"><br><br>  Es ist ersichtlich, dass die Gesamtbreite des Clos-Netzwerks ein Produkt aller Ebenen von Wirbels√§ulen-Switches des s√ºdlichen Radix ist, wie viele Verbindungen wir haben, wie sie verzweigen.  So skalieren wir die Gr√∂√üe des Netzwerks. <br><br><img src="https://habrastorage.org/webt/ap/dy/-2/apdy-2dnjdljys76pfv4rw7sr-k.jpeg"><br><br>  In Bezug auf die Kapazit√§t, insbesondere bei ToR-Switches, gibt es zwei Skalierungsoptionen.  Entweder k√∂nnen wir unter Beibehaltung der allgemeinen Topologie schnellere Verkn√ºpfungen verwenden oder wir k√∂nnen mehr Ebenen hinzuf√ºgen. <br><br>  Wenn Sie sich die detaillierte Version des Clos-Netzwerks (in der unteren rechten Ecke) ansehen und zu diesem Bild mit dem Clos-Netzwerk unten zur√ºckkehren ... <br><br><img src="https://habrastorage.org/webt/4y/pk/tt/4ypkttuvzezqubmxx3te-envyzo.jpeg"><br><br>  ... dann ist das genau die gleiche Topologie, aber auf dieser Folie ist sie kompakter zusammengeklappt und die Werksebenen √ºberlagern sich.  Es ist ein und dasselbe. <br><br><img src="https://habrastorage.org/webt/n9/ny/zd/n9nyzdzmz4i8o_c-adybirjhfos.jpeg"><br><br>  Wie sieht die Skalierung eines Clos-Netzwerks in Zahlen aus?  Hier habe ich Daten dar√ºber, welche maximale Breite ein Netzwerk erhalten kann, wie viele Racks, ToR-Switches oder Leaf-Switches maximal vorhanden sind, und ob sie sich nicht in Racks befinden. Dies h√§ngt davon ab, welche Basis von Switches f√ºr Stacheln verwendet wird Ebenen und wie viele Ebenen wir verwenden. <br><br>  Es zeigt, wie viele Racks wir haben k√∂nnen, wie viele Server und wie viel all dies bei einer Leistung von 20 kW pro Rack verbrauchen kann.  Ein wenig zuvor erw√§hnte ich, dass wir eine Clustergr√∂√üe von etwa 100.000 Servern anstreben. <br><br>  Es ist ersichtlich, dass bei dieser gesamten Konstruktion zweieinhalb Optionen von Interesse sind.  Es gibt eine Option mit zwei Schichten von Stacheln und 64-Port-Switches, die etwas kurz ist.  Passgenaue Optionen f√ºr Wirbels√§ulen-Switches mit 128 Ports (mit 128 Radix) mit zwei Ebenen oder Switches mit 32 Radix mit drei Ebenen.  Und in allen F√§llen, in denen es mehr Radix und mehr Ebenen gibt, k√∂nnen Sie ein sehr gro√ües Netzwerk aufbauen, aber wenn Sie den erwarteten Verbrauch betrachten, gibt es in der Regel Gigawatt.  Sie k√∂nnen das Kabel verlegen, aber es ist unwahrscheinlich, dass wir an einem Ort so viel Strom bekommen.  Wenn Sie sich Statistiken ansehen, √∂ffentliche Daten zu Rechenzentren - es gibt nur sehr wenige Rechenzentren mit einer gesch√§tzten Kapazit√§t von mehr als 150 MW.  Dar√ºber hinaus befinden sich an Rechenzentrumsstandorten in der Regel mehrere gro√üe Rechenzentren ziemlich nahe beieinander. <br><br>  Es gibt noch einen weiteren wichtigen Parameter.  Wenn Sie sich die linke Spalte ansehen, wird dort die nutzbare Bandbreite angezeigt.  Es ist leicht zu bemerken, dass in einem Clos-Netzwerk ein erheblicher Teil der Ports f√ºr die Verbindung der Switches untereinander aufgewendet wird.  Nutzbare Bandbreite k√∂nnen Sie den Servern zur Verf√ºgung stellen.  Nat√ºrlich spreche ich √ºber bedingte Ports und √ºber den Strip.  In der Regel sind die Verbindungen innerhalb des Netzwerks schneller als die Verbindungen zu den Servern, aber je Bandeinheit gibt es noch einige B√§nder innerhalb des Netzwerks.  Und je mehr Ebenen wir verwenden, desto h√∂her sind die St√ºckkosten f√ºr die Bereitstellung dieses Streifens nach au√üen. <br><br>  Dar√ºber hinaus ist auch dieses zus√§tzliche Band nicht genau dasselbe.  W√§hrend die Zeitspannen kurz sind, k√∂nnen wir so etwas wie DAC (Direct Attach Copper, dh Twinax-Kabel) oder Multimode-Optiken verwenden, die noch mehr oder weniger vern√ºnftiges Geld kosten.  Sobald wir authentischer auf Spannweiten umschalten, handelt es sich in der Regel um Single-Mode-Optiken, und die Kosten f√ºr dieses zus√§tzliche Band steigen deutlich an. <br><br>  Wenn wir zur vorherigen Folie zur√ºckkehren und ein Clos-Netzwerk erstellen, ohne es erneut zu abonnieren, ist es einfach, das Diagramm zu betrachten und zu sehen, wie das Netzwerk aufgebaut ist. Wenn wir jede Ebene von Wirbels√§ulenschaltern hinzuf√ºgen, wiederholen wir den gesamten Streifen, der sich darunter befand.  Plus-Pegel - plus dasselbe Band, dieselbe Anzahl von Ports an den Switches wie auf dem vorherigen Pegel, dieselbe Anzahl von Transceivern.  Daher ist es sehr w√ºnschenswert, die Anzahl der Stufen von Wirbels√§ulenschaltern zu minimieren. <br><br>  Anhand dieses Bildes ist klar, dass wir wirklich auf so etwas wie Switches mit 128 Radix aufbauen wollen. <br><br><img src="https://habrastorage.org/webt/lh/hu/qu/lhhuquq763qnw40v3n66copxa5w.jpeg"><br><br>  Hier ist im Prinzip alles das Gleiche wie ich gerade gesagt habe, diese Folie wird eher sp√§ter betrachtet. <br><br><img src="https://habrastorage.org/webt/ci/yy/zi/ciyyzijzbhajnlsarxtn-tcoybo.jpeg"><br><br>   ,        ?     ,     -      .    ,     . ,      .  ,    .  ,   ,  ,  , .        ( ),       control plane    , , ,     ,       .        ,    ,     . <br><br> , ,    ,         SerDes- ‚Äî  -   .          forwarding .  ,    ,     ,  ,  ,     Clos-,     .      . <br><br>       ,    ,      .     ,      ,   ,    ,     ,      ,        ,    ,       ,   . <br><br><img src="https://habrastorage.org/webt/8y/in/ym/8yinymhp1wcbtuhl6tbvooghq24.jpeg"><br><br>      ‚Äî   , .  ,  ,   .     ,    ,  ,    -           ,    . ,    ,   ,  ,    . <br><br> ,  ,     ,   . -,    ,    .    ,    ,   128 ,              . <br><br>       ,  , ,    data plane.  . ,  ,   .  ,          ,      ,     . ,   ,   , ,    128    ,       .            .       .          . <br><br>   ,    -  ,    .   ( ),   ,    ‚Äî ToR-  leaf-,        .     -   ,    ,            , , -    .   ,     ,         ,    -    . <br><br><img src="https://habrastorage.org/webt/wc/yl/hl/wcylhl91sqnrvns5wqmembthdaa.jpeg"><br><br> ,        ,    . <br><br><img src="https://habrastorage.org/webt/uz/mo/ng/uzmongpipgv1bxt94ksbvlkruvi.jpeg"><br><br>     ?   .       ,       ,   ,       :    leaf-,   1,   2. ,     ‚Äî  twinax, multimode, single mode.    ,   ,    ,   ,    ,     . <br><br>       .    ,   ,   multimode ,      ,  ,  100-  .  , ,  ,  single mode ,    ,   single mode,  -      CWDM,   single mode (PSM)    ,     ,      ,      . <br><br>   :  ,      100  425  .      SFP28    ,  QSFP28  100 .     multimode   . <br><br>    - ,  -   , -    -   .  ,         .  , -   Pods      twinax- ( ). <br><br>  ,         ,     ,          CWDM.     . <br><br><img src="https://habrastorage.org/webt/ea/ci/pf/eacipf4xy2usawno2ccx4ucuwsm.jpeg"><br><br>   ,   ,      . ,   ,      50- SerDes      .  ,   ,           400G,      50G SerDes-   ,       100 Gbps per lane.   ,     50    100- SerDes  100 Gbps per lane,           . ,  50G SerDes   , ,    ,   100G SerDes         .   -    , ,    . <br><br><img src="https://habrastorage.org/webt/uz/mo/ng/uzmongpipgv1bxt94ksbvlkruvi.jpeg"><br><br>      .  ,      400-  200-    50G SerDes.  ,      ,  ,    ,       ,   , .   128.            ,  , , ,  . <br><br>         ,      ,    .     ,     ,    ,        , ,    100- ,       . <br><br><img src="https://habrastorage.org/webt/0b/r0/or/0br0oryllu-b_r4p9fcbino5bks.jpeg"><br><br>   ‚Äî   ,       . ,     .   leaf-     ‚Äî     ,     .       , ,    ‚Äî            . <br><br> ,  ,     ,      -. ,           ,   -       -,      .     .      ,  ,   ,       .           -  -,                -,     ,       ,  ,       .      :        .   -   , ¬´ ¬ª,      Clos-,      .     ,    ,   . <br><br><img src="https://habrastorage.org/webt/1t/hu/rq/1thurqskba0vbopspd3nhpfgsus.jpeg"><br><br>          .        -  ,   ,       ,      ,    -2-. <br><br>    .       ,      - 512  512 ,    ,      ,     -2.            Pods   -1,      -,      -2. <br><br><img src="https://habrastorage.org/webt/c6/gv/mr/c6gvmrqk7tjkdwxuvopyn76gvn8.jpeg"><br><br>  So sieht es aus.      -2 ()   -.  ,      .            -,  .    ,   ,   . <br><br><img src="https://habrastorage.org/webt/ip/gh/sd/ipghsdpwb_goedtygn0bpaj89kq.jpeg"><br><br>  :   ,  .    control plane-?      ,  -  ,  link state  ,    , ,  ,       .     ,    ‚Äî  ,     link state .     ,   ,    , ,        fanout,     control plane .          link state    . <br><br>  ‚Äî  BGP.    ,   RFC 7938   BGP   -.   :            ,  ,   ,   path hunting.    ,    , ,   valley free.  ,  ,   ,    .    ,   ,     .    .   . <br><br>   ,     ,     BGP.      eBGP,   link local,      :    ToR,      -1-  Pod,       Top of Fabric.      ,       BGP     ,   . <br><br><img src="https://habrastorage.org/webt/hw/ep/kf/hwepkfz0cr9y5fhmdix0xbvhv00.jpeg"><br><br> ,       ,      ,   ,     control plane. L3      ,       ,         .     ‚Äî  ,  ,  ,      multi-path,       multi-path   ,  ,    ,    .  ,          ,     ,    ,      ,   .     ,    multi-path,     ToR-. <br><br><img src="https://habrastorage.org/webt/md/ce/zb/mdcezbkgiaxpdjcs1llfgnsvvps.jpeg"><br><br>   ,  ,   ‚Äî           .     ,   ,    ,        ,     BGP,    .  ,  corner cases    ,     BGP        . <br><br>           RIFT,       . <br><br><img src="https://habrastorage.org/webt/0u/h7/uq/0uh7uqwufze4nmj1oqrb6kun2jm.jpeg"><br><br>     ‚Äî  ,   data plane   ,        .       : ECMP ,       Next Hop. <br><br>    ,  ,     Clos- ,     ,   ,   ,  ,   .       ,     ECMP,  single path-.  Alles ist gut.   ,   Clos-   ,      Top of fabric,   ,        .      ,            ,     .     ,     ECMP     state. <br><br>    data plane   ?    LPM (longest prefix match),   ,  100 .    Next Hop ,   , 2-4 .    ,    Next Hops ( adjacencies),   -  16  64.     .       :     MPLS  -?  ,    . <br><br><img src="https://habrastorage.org/webt/ze/ub/bj/zeubbjnb97nq_gvoprd374lotda.jpeg"><br><br>   .     ,        .                 white boxes  MPLS.   MPLS,   ,   ,  ,     ECMP.  Und hier ist warum. <br><br><img src="https://habrastorage.org/webt/fc/za/pr/fczaprpywdv5b1jhoqrn9lxq8ti.jpeg"><br><br>    ECMP-  IP.                 Next Hops ( adjacencies,          -).   ,           -,     Next Hop.  IP   ,             ,      Next Hops. <br><br><img src="https://habrastorage.org/webt/n5/bl/jw/n5bljwqyrivakkgc2_f8cl4oifw.jpeg"><br><br>   MPLS ,            .          Next Hops    .  , ,  . <br><br>  ,         4000 ToR-,   ‚Äî 64  ECMP,    -1   -2.    ,  ,    ECMP-,      ToR ,       Next Hops. <br><br><img src="https://habrastorage.org/webt/ut/rz/jf/utrzjfe-ddmgf0dxjbq5ipxukcs.jpeg"><br><br>   ,     Segment Routing   .          Next Hops.      wild card:           .   ,       . <br><br>  ,    -   .   ?     Clos- .     ,       Top of fabric.          .  ,         ,       Top of fabric,      , ,          . ,   ,      ,     ,      . <br><br>   ‚Äî    .  ,  Clos-  ,  ,  ,     ToR,       Top of fabric   ,   .      Pod, Edge Pod,    . <br><br>    . , ,  Facebook.     Fabric Aggregator  HGRID.   -,    -.   ,           .   ,   touch points,  .  ,       ,    -.      ,    -  ,       ,       .  , ,    .    overlays,    . <br><br><img src="https://habrastorage.org/webt/mw/sj/ci/mwsjcikq0-fdqoklr0nal6vtjtg.jpeg"><br><br>     ?    ‚Äî   CI/CD-.    ,   ,   ,  .     ,    ,     .  ,       ,     . <br><br>  ,       .       .      ‚Äî  . <br><br>      .      ,  RIFT.        congestion control , , ,      ,  RDMA   . <br><br>        ,    , , ,   overhead.    ‚Äî        HPC Cray Slingshot,     commodity Ethernet,        .   overhead . <br><br><img src="https://habrastorage.org/webt/ja/-z/yx/ja-zyxc9orfwrz1lryew_4jt7r4.jpeg"><br><br>     ,  ,   .  ‚Äî  .     ‚Äî  .   -  scale out ‚Äî .  ,     .    .  Vielen Dank. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de476146/">https://habr.com/ru/post/de476146/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de476134/index.html">MONQ - Monitoring und AIOps urspr√ºnglich aus Russland</a></li>
<li><a href="../de476138/index.html">Astronomen glauben, dass Kommunikationssatelliten wie SpaceX, OneWeb und andere Unternehmen die Zukunft der Astronomie bedrohen</a></li>
<li><a href="../de476140/index.html">Wie kann das H√∂ren verbessert werden, wenn Sie 7000 W√∂rter kennen, diese aber nicht nach Geh√∂r verstehen?</a></li>
<li><a href="../de476142/index.html">Warum sollten Sie keine Ausnahmen verwenden, um Ihren Ablauf in Java zu steuern?</a></li>
<li><a href="../de476144/index.html">Wie Sie einen Chat f√ºr Bots schreiben m√ºssen und nicht, am Beispiel meines Bots, um Secret Santa zu spielen</a></li>
<li><a href="../de476148/index.html">postfix + dovecot + mysql unter FreeBSD</a></li>
<li><a href="../de476156/index.html">Synchrone Anfrage-Antwort mit Apache Kafka</a></li>
<li><a href="../de476160/index.html">Die Geburt der Lernsoftware und ihrer Geschichte: von mechanischen Maschinen bis zu den ersten Computern</a></li>
<li><a href="../de476162/index.html">Wir erstellen eine moderne Webanwendung. Bekanntschaft mit dem Projekt und Arbeitsvorbereitung. Teil 1</a></li>
<li><a href="../de476164/index.html">"Das ist auch Datenanalyse." Sprechen Sie mit Michail Gelfand √ºber Bioinformatik</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>