<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏻‍✈️ 🤰🏻 ✊🏿 Tiefenkameras - stille Revolution (wenn Roboter sehen) Teil 1 ☑️ 🔭 👩🏽‍🎓</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Kürzlich habe ich beschrieben, dank welcher Roboter morgen VIEL besser zu denken beginnen wird (ein Beitrag über die Hardwarebeschleunigung neuronaler...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Tiefenkameras - stille Revolution (wenn Roboter sehen) Teil 1</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/457524/"><img src="https://habrastorage.org/getpro/habr/post_images/917/25a/9a4/91725a9a49451111a6d55d1015cc297c.png"><br><img src="https://habrastorage.org/getpro/habr/post_images/f58/7fd/ebe/f587fdebeedd2f17fe6f4122a68dff9f.png"><br><br>  Kürzlich habe ich beschrieben, dank welcher Roboter morgen VIEL besser zu denken beginnen wird (ein Beitrag über die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hardwarebeschleunigung neuronaler Netze</a> ).  Heute werden wir sehen, warum Roboter bald viel besser zu sehen sein werden.  In manchen Situationen viel besser als eine Person. <br><br>  Wir werden über Tiefenkameras sprechen, die Videos aufnehmen, in denen an diesem Punkt nicht die Farbe, sondern die Entfernung zum Objekt gespeichert ist.  Solche Kameras gibt es seit mehr als 20 Jahren, aber in den letzten Jahren ist die Geschwindigkeit ihrer Entwicklung um ein Vielfaches gestiegen, und wir können bereits über die Revolution sprechen.  Und Multi-Vektor.  In folgenden Bereichen findet eine rasche Entwicklung statt: <br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Strukturiertes Licht einer Kamera</a> oder einer strukturellen Lichtkamera, wenn ein Projektor (häufig Infrarot) und eine Kamera vorhanden sind, die das strukturelle Licht des Projektors aufzeichnen; <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Flugzeitkameras</a> oder Kameras, die auf der Messung der Verzögerung des reflektierten Lichts basieren; <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tiefe von Stereokameras</a> - die klassische und vielleicht berühmteste Richtung für die Bautiefe von Stereo; <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Lichtfeldkamera</a> - es handelt sich auch um Lichtfeldkameras oder plenoptische Kameras, über die es einen separaten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">detaillierten Beitrag gab</a> ; <br></li><li>  Und schließlich Kameras, die auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Lidar-Technologie</a> basieren, insbesondere die frischen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Solid State Lidars</a> , die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">fehlerfrei</a> etwa 100-mal länger als normale Lidars arbeiten und das übliche rechteckige Bild erzeugen. <br></li></ul><br>  Wen interessiert es, wie es aussehen wird, sowie einen Vergleich verschiedener Ansätze und ihrer aktuellen und morgigen Anwendung - willkommen unter dem Schnitt! <br><a name="habracut"></a><br>  Also!  Wir werden die Hauptrichtungen der Entwicklung von Tiefenkammern oder tatsächlich verschiedene Prinzipien zur Tiefenmessung analysieren.  Mit ihren Vor- und Nachteilen. <br><br><h1>  Methode 1: Strukturierte Lichtkamera </h1><br>  Beginnen wir mit einer der einfachsten, ältesten und relativ billigen Methoden zur Messung von tiefenstrukturiertem Licht.  Dieses Verfahren erschien im wesentlichen sofort, sobald Digitalkameras erschienen, d.h.  Vor über 40 Jahren und etwas später mit dem Aufkommen digitaler Projektoren stark vereinfacht. <br><br>  Die Grundidee ist sehr einfach.  Wir platzieren neben dem Projektor, der zum Beispiel horizontale (und dann vertikale) Streifen erzeugt, und neben der Kamera, die ein Bild mit Streifen aufnimmt, wie in dieser Abbildung gezeigt: <br><img src="https://habrastorage.org/getpro/habr/post_images/ed4/416/7c3/ed44167c3f6f9e1af2af2eda53f6ec97.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Autodesk: Strukturiertes Licht 3D-Scannen</a></i> <br><br>  Da Kamera und Projektor zueinander versetzt sind, werden die Streifen auch proportional zum Abstand zum Motiv verschoben.  Durch Messen dieser Verschiebung können wir den Abstand zum Objekt berechnen: <br><img src="https://habrastorage.org/webt/27/oo/xp/27ooxpjh4fgdijfazupjylg1ang.png">  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">http://www.vision-systems.com/</a></i> <br><br>  Mit dem billigsten Projektor (und der Preis beginnt bei 3.000 Rubel) und einem Smartphone können Sie die Tiefe statischer Szenen in einem dunklen Raum messen: <br><img src="https://habrastorage.org/getpro/habr/post_images/f41/e50/8b8/f41e508b881e7d1f1b36bbc0dfb9c179.png"><img src="https://habrastorage.org/getpro/habr/post_images/464/662/9ee/4646629ee8562fe3a76beaef82e4c284.png"><img src="https://habrastorage.org/getpro/habr/post_images/85a/493/ee0/85a493ee0b020b885518bfcddb2c9b1d.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Autodesk: Strukturiertes Licht 3D-Scannen</a></i> <br><br>  Es ist klar, dass in diesem Fall eine ganze Reihe von Problemen gelöst werden müssen - dies ist Projektorkalibrierung, Telefonkamera-Kalibrierung, Streifenverschiebungserkennung usw., aber all diese Aufgaben sind selbst für fortgeschrittene Schüler, die Programmieren lernen, durchaus möglich. <br><br>  Dieses Prinzip der Tiefenmessung wurde am bekanntesten, als Microsoft 2010 den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MS Kinect-Tiefensensor</a> für 150 US-Dollar herausbrachte, der zu dieser Zeit revolutionär billig war. <br><img src="https://habrastorage.org/getpro/habr/post_images/89c/3d5/8b2/89c3d58b24c16b18593782bd822a5bd2.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teilweise okkludierte Objektrekonstruktion mit mehreren Kinect-Sensoren</a></i> <br><br>  Trotz der Tatsache, dass Kinect nicht nur die Tiefe mit einem IR-Projektor und einer IR-Kamera maß, sondern auch reguläres RGB-Video aufnahm, vier Mikrofone mit Rauschunterdrückung hatte und sich an eine Person in der Größe anpassen konnte, die automatisch nach oben oder unten kippte, wurde es sofort in das Gerät integriert Datenverarbeitung, die der Konsole sofort eine fertige Tiefenkarte ausgab: <br><img src="https://habrastorage.org/getpro/habr/post_images/a48/49e/1a1/a4849e1a19f745aedb92800debafc6f7.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Implementierung natürlicher Schaltflächen für die Benutzeroberfläche mit Kinect</a></i> <br><br>  Insgesamt wurden rund 35 Millionen Geräte verkauft, was Kinect zur ersten Massentiefenkamera in der Geschichte macht.  Und wenn man bedenkt, dass es sicherlich Tiefenkameras gab, die normalerweise höchstens zu Hunderten verkauft wurden und mindestens eine Größenordnung teurer waren - dies war eine Revolution, die große Investitionen in diesem Bereich ermöglichte. <br><br>  Ein wichtiger Grund für den Erfolg war, dass es zum Zeitpunkt der Veröffentlichung der Xbox 360 durch Microsoft bereits einige Spiele gab, die Kinect aktiv als Sensor verwendeten.  Der Start war schnell: <br><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/895/5be/676/8955be6766420326e6fc7c1ab2371385.png"><br><br>  Darüber hinaus gelang es Kinect sogar, als am schnellsten verkauftes Gerät der Geschichte in das Guinness-Buch der Rekorde einzutreten.  Zwar hat Apple Microsoft bald von diesem Ort verdrängt, aber dennoch.  Für einen neuen experimentellen Sensor, der neben dem Hauptgerät zum am schnellsten verkauften elektronischen Gerät in der Geschichte wird, ist dies einfach eine großartige Leistung: <br><img src="https://habrastorage.org/getpro/habr/post_images/051/f2a/cf2/051f2acf275e677c10252fc38541ff82.png"><br><br>  In Vorträgen frage ich das Publikum gerne, woher all diese Millionen Kunden kommen.  Wer waren all diese Leute? <br><br>  In der Regel ahnt niemand, aber manchmal, besonders wenn das Publikum älter und erfahrener ist, geben sie die richtige Antwort: Der Verkauf wurde von amerikanischen Eltern getrieben, die mit Freude sahen, dass ihre Kinder auf der Konsole spielen und nicht mit einer dicken Beute auf der Couch sitzen konnten. und vor den Fernseher springen.  Es war ein Durchbruch !!!  Millionen von Müttern und Vätern beeilten sich, ein Gerät für ihre Kinder zu bestellen. <br><br>  Wenn es um die Gestenerkennung geht, glauben die Leute im Allgemeinen naiv, dass nur Daten von einer 2D-Kamera ausreichen.  Immerhin haben sie viele schöne Demos gesehen!  Die Realität ist viel strenger.  Die Erkennungsgenauigkeit von Gesten aus einem 2D-Videostream von einer Kamera und die Erkennungsgenauigkeit von Gesten aus einer Kameratiefe unterscheiden sich um eine Größenordnung.  Mit einer Tiefenkamera oder besser gesagt mit einer RGB-Kamera in Kombination mit einer Tiefenkamera (letztere ist wichtig) können Sie Gesten viel genauer und kostengünstiger erkennen (auch wenn der Raum dunkel ist), und dies hat der ersten Massentiefenkamera Erfolg gebracht. <br><br>  Über Kinect auf Habré zu der Zeit haben sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">viel geschrieben</a> , also ganz kurz, wie es funktioniert. <br><br>  Ein Infrarotprojektor liefert eine pseudozufällige Menge von Punkten im Raum, deren Verschiebung die Tiefe in einem bestimmten Pixel bestimmt: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/55f/c46/d72/55fc46d72e36087ac7522312d791af6f.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Depth Sensing Planar Structures: Erkennung von Büromöbelkonfigurationen</a></i> <br><br>  Die Kameraauflösung ist als 640 x 480 deklariert, aber tatsächlich gibt es ungefähr 320 x 240 mit ziemlich starker Filterung, und das Bild in realen Beispielen sieht so aus (das ist ziemlich beängstigend): <br><img src="https://habrastorage.org/webt/a9/ni/up/a9niup8_g7i8a1x3n_0oaikpee0.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teilweise okkludierte Objektrekonstruktion mit mehreren Kinect-Sensoren</a></i> <br><br>  Die „Schatten“ der Objekte sind deutlich sichtbar, da Kamera und Projektor weit genug voneinander entfernt sind.  Es ist ersichtlich, dass Verschiebungen mehrerer Punkte des Projektors vorgenommen werden, um die Tiefe vorherzusagen.  Darüber hinaus gibt es eine (harte) Filterung durch unmittelbare Nachbarn, aber die Tiefenkarte ist immer noch ziemlich verrauscht, insbesondere an den Grenzen.  Dies führt zu einem ziemlich wahrnehmbaren Geräusch auf der Oberfläche der resultierenden Objekte, das zusätzlich und nicht trivial geglättet werden muss: <br><img src="https://habrastorage.org/webt/ph/e-/dl/phe-dlp6dczaztxgy7q4p-tyxtg.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">J4K Java Library für das Kinect SDK von Microsoft</a></i> <br><br>  Trotzdem nur 150 US-Dollar ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">heute sind es bereits 69 US-Dollar</a> , obwohl <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">es</a> natürlich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">näher an 200 US-Dollar</a> liegt) - und Sie "sehen" die Tiefe!  Es gibt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">wirklich viele</a> Serienprodukte. <br><br>  Übrigens wurde im Februar dieses Jahres ein neuer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Azure Kinect</a> angekündigt: <br><img src="https://habrastorage.org/webt/l6/0y/b2/l60yb2lcfeuufoe6q6jdd0e-jg0.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Microsoft kündigt Azure Kinect an, das ab sofort vorbestellt werden kann</a></i> <br><br>  Die Auslieferung an Entwickler in den USA und in China sollte am 27. Juni beginnen, d. H.  buchstäblich jetzt.  Von den Funktionen wird neben der deutlich besseren Auflösung von RGB und der besseren Qualität von Tiefenkameras (sie versprechen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">1024 x 1024</a> bei 15 FPS und 512 x 512 bei 30 FPS und eine höhere Qualität ist in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Demo deutlich sichtbar</a> , die ToF-Kamera deutlich zu unterstützen) die Unterstützung für die Zusammenarbeit mehrerer Geräte sofort erklärt, weniger Exposition gegenüber Bei der Sonne beträgt der Fehler weniger als 1 cm in einer Entfernung von 4 Metern und 1-2 mm in einer Entfernung von weniger als 1 Meter, was äußerst interessant klingt. Wir warten also, wir warten: <br><img src="https://habrastorage.org/getpro/habr/post_images/0d1/972/90b/0d197290bf6363f157fa9955070c4b5a.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Einführung in Azure Kinect DK</a></i> <br><br>  Das nächste <b>Massenprodukt</b> , bei dem eine Tiefenkamera in einem strukturierten Licht realisiert wurde, war keine Spielekonsole, sondern ... (Trommelwirbel) richtig - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">iPhone X</a> ! <br><br>  Die Face ID-Technologie ist eine typische Tiefenkamera mit einem typischen Infrarot-Punktprojektor und einer Infrarotkamera (jetzt verstehen Sie übrigens, warum sie sich an den Rändern der Pony befinden und so weit wie möglich voneinander entfernt sind - dies ist eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stereobasis</a> ): <br><img src="https://habrastorage.org/getpro/habr/post_images/755/6e2/6de/7556e26dea5a132b997658d386d74e7f.png"><br><br>  Die Auflösung der Tiefenkarte ist sogar geringer als die von Kinect - ungefähr 150x200.  Es ist klar, dass, wenn Sie sagen: "Unsere Auflösung beträgt etwa 150 x 200 Pixel oder 0,03 Megapixel", die Leute kurz und prägnant sagen werden: "Sucks!"  Und wenn Sie sagen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">"Punktprojektor: Mehr als 30.000 unsichtbare Punkte werden auf Ihr Gesicht projiziert"</a> , sagen die Leute: "Wow, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">30.000 unsichtbare Punkte</a> , cool!".  Einige Blondinen werden fragen, ob Sommersprossen an unsichtbaren Stellen auftreten.  Und das Thema wird an die Massen gehen!  Daher war die zweite Option in der Werbung weitsichtig.  Die Auflösung ist aus drei Gründen gering: Erstens die Anforderungen an Miniatur, zweitens den Energieverbrauch und drittens die Preise. <br><br>  Trotzdem ist dies eine weitere Tiefenkamera in einem strukturierten Licht, die in eine Reihe von Millionen von Exemplaren eingedrungen ist und bereits von anderen Smartphone-Herstellern wiederholt wurde, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zum Beispiel (Überraschung-Überraschung!) Huawei</a> (das Apple im vergangenen Jahr beim Smartphone-Verkauf umgangen hat).  Nur Huawei hat rechts eine Kamera und links den Projektor, aber natürlich auch entlang der Ränder der „Pony“: <br><img src="https://habrastorage.org/webt/zt/2h/ab/zt2habvk5zl2pkyvmeerbzacjfq.png"><br>  <i>Quelle: Mit dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Huawei Mate 20 Pro-Update können Benutzer ein zweites Gesicht für die Gesichtsentsperrung hinzufügen</a></i> <br><br>  Gleichzeitig werden 300.000 Punkte deklariert, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dh zehnmal mehr als bei Apple</a> , und die Frontkamera ist besser <s>und die Schrift ist größer</s> .  Gibt es eine Übertreibung in Bezug auf 300.000 - es ist schwer zu sagen, aber Huawei demonstriert einen sehr guten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">3D-Scan von Objekten mit einer Frontkamera</a> .  Unabhängige Tests sind <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">beängstigender</a> , aber dies ist eindeutig der Anfang des Themas, und die Anfänge der Technologie energieeffizienter Miniatur-Tiefenkameras und Kameraankündigungen Ende dieses Jahres weisen bereits eine deutlich bessere Leistung auf. <br><br>  Gleichzeitig ist es verständlich, warum die Gesichtserkennungstechnologie in Telefonen verwendet wurde.  Erstens können Sie den Detektor jetzt nicht täuschen, indem Sie ein Foto Ihres Gesichts (oder ein Video vom Tablet) anzeigen.  Zweitens ändert sich das Gesicht stark, wenn sich die Beleuchtung ändert, aber seine Form nicht, wodurch wir die Person zusammen mit den Daten der RGB-Kamera genauer identifizieren können: <br><img src="https://habrastorage.org/getpro/habr/post_images/5c9/12b/e86/5c912be86bf5a47aa02bcce67e48f148.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TI Foto derselben Person</a></i> <br><br>  Offensichtlich weist der Infrarotsensor inhärente Probleme auf.  Erstens scheint unser relativ schwacher Projektor ein oder zwei Mal auf die Sonne, sodass diese Kameras auf der Straße nicht funktionieren.  Selbst im Schatten können große Probleme mit der Gesichtserkennung auftreten, wenn die weiße Wand eines Gebäudes von der Sonne beleuchtet wird.  Der Geräuschpegel in Kinect rollt auch dann über, wenn die Sonne von Wolken bedeckt ist: <br><img src="https://habrastorage.org/getpro/habr/post_images/e5b/477/bb5/e5b477bb581588840cb6d1219b7899dd.png"><br>  <i>Quelle: dieses und die nächsten beiden Bilder</i> - <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Materialien Basler AG</a></i> <br><br>  Ein weiteres großes Problem ist Reflexion und Reflexion.  Da auch Infrarotlicht reflektiert wird, ist es problematisch, mit Kinect einen teuren Edelstahlkessel, einen lackierten Tisch oder einen Glasschirm aufzunehmen: <br><img src="https://habrastorage.org/getpro/habr/post_images/872/913/e20/872913e207e550f6e7eb609510fd70f6.png"><br><br>  Und schließlich können sich zwei Kameras, die ein Objekt aufnehmen, gegenseitig stören.  Interessanterweise können Sie bei strukturiertem Licht den Projektor flackern lassen und verstehen, wo sich unsere Punkte befinden und wo nicht, aber dies ist eine separate und ziemlich komplizierte Geschichte: <br><img src="https://habrastorage.org/getpro/habr/post_images/c9f/427/3e3/c9f4273e3bf9682c1b6869cbfaaf22b5.png"><br><br>  Jetzt wissen Sie, wie man FaceID bricht ... <br><br>  Für mobile Geräte scheint strukturiertes Licht jedoch heute der vernünftigste Kompromiss zu sein: <br><img src="https://habrastorage.org/getpro/habr/post_images/cc8/77f/743/cc877f74308b460ca7cd306856ba637a.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Smartphone-Unternehmen, die sich bemühen, die Leistung und die Kosten der Apple 3D-Kamera anzupassen</a></i> <br><br>  Für strukturiertes Licht ist die Billigkeit eines herkömmlichen Sensors so, dass seine Verwendung in den meisten Fällen mehr als gerechtfertigt ist.  Was eine große Anzahl von Startups zum Leben erweckte, die nach der Formel arbeiteten: billiger Sensor + komplexe Software = durchaus akzeptables Ergebnis. <br><br>  Zum Beispiel hat unser ehemaliger Doktorand <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Maxim Fedyukov</a> , der sich seit 2004 mit 3D-Rekonstruktion beschäftigt, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Texel entwickelt</a> , dessen Hauptprodukt eine Plattform mit 4 Kinect-Kameras und -Software ist, die eine Person in 30 Sekunden in ein potenzielles Denkmal verwandelt.  Nun, oder eine Desktop-Figur.  Das ist, wer genug Geld hat.  Oder Sie können Ihren Freunden billige und fröhliche Fotos von Freunden Ihres 3D-Modells senden (aus irgendeinem Grund der beliebteste Fall aus irgendeinem Grund).  Jetzt senden sie ihre Plattformen und Software aus Großbritannien nach Australien: <br><iframe width="560" height="315" src="https://www.youtube.com/embed/VLaZ_jDuZ30" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erstellen eines 3D-Modells einer Person in 30 Sekunden</a></i> <br><br>  Als Ballerina kann ich nicht schön stehen, deshalb schaue ich nur nachdenklich auf die Flosse eines vorbeischwimmenden Hais: <br><img src="https://habrastorage.org/getpro/habr/post_images/fca/b59/2b2/fcab592b290ca2a9838748bc3ac82ce2.gif"><br>  <i>Quelle: Materialien des Autors</i> <br><br>  Im Allgemeinen brachte eine neue Art von Sensoren neue Kunstprojekte hervor.  Im Winter sah ich einen ziemlich neugierigen VR-Film, der mit Kinect gedreht wurde.  Im Folgenden finden Sie eine interessante Visualisierung des Tanzes, der ebenfalls mit Kinect erstellt wurde (anscheinend wurden 4 Kameras verwendet). Im Gegensatz zum vorherigen Beispiel haben sie nicht mit Lärm gekämpft, sondern eher lustige Details hinzugefügt: <br><img src="https://habrastorage.org/getpro/habr/post_images/cd3/072/e7f/cd3072e7ffef9e111604f9d6e83e640e.gif"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Eine Tanzperformance, die mit einem Kinect-Sensor aufgenommen und mit 3D-Software visualisiert wurde</a></i> <br><br>  Welche Trends sind in der Region zu beobachten: <br><ul><li>  Wie Sie wissen, sind digitale Sensoren moderner Kameras empfindlich gegenüber Infrarotstrahlung. Daher müssen Sie spezielle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sperrfilter verwenden,</a> damit das Bild nicht durch Infrarotrauschen beeinträchtigt wird (selbst die Richtung der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">künstlerischen Aufnahme im Infrarotbereich wird</a> angezeigt, auch wenn der Filter vom Sensor entfernt wird).  Dies bedeutet, dass enorme Geldbeträge in Miniaturisierung, höhere Auflösung und billigere Sensoren investiert werden, die als Infrarot (mit einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">speziellen Filter</a> ) verwendet werden können. <br></li><li>  In ähnlicher Weise verbessern sich die Algorithmen zur Verarbeitung von Tiefenkarten jetzt schnell, einschließlich der Methoden der sogenannten Kreuzfilterung, wenn Daten von einem RGB-Sensor und verrauschte Daten nach Tiefe es Ihnen ermöglichen, ein sehr gutes Tiefenvideo zusammen zu erhalten.  Gleichzeitig wird es mithilfe neuronaler Netzwerkansätze möglich, die Geschwindigkeit, mit der ein gutes Ergebnis erzielt wird, drastisch zu erhöhen. <br></li><li>  In diesem Bereich arbeiten alle Top-Unternehmen, insbesondere Smartphone-Hersteller. <br></li></ul><br>  Als Konsequenz: <br><ul><li>  Wir können in den nächsten 5 Jahren einen dramatischen Anstieg der Auflösung und Genauigkeit von Tiefenkameras mit strukturiertem Licht erwarten. <br></li><li>  Der Energieverbrauch mobiler Sensoren wird (wenn auch langsamer) gesenkt, was die Verwendung von Sensoren der nächsten Generation in Smartphones, Tablets und anderen mobilen Geräten vereinfacht. <br></li></ul><br>  Auf jeden Fall sehen wir jetzt die Kindheit der Technologie.  Die ersten Massenprodukte, bei denen das Debuggen der Produktion und Verwendung eines neuen ungewöhnlichen Datentyps gerade gestartet wird - Video mit Tiefe. <br><br><h1>  Methode 2: Flugzeitkamera </h1><br>  Der nächste Weg, um Tiefe zu bekommen, ist interessanter.  Es basiert auf der Messung der Umlauflichtverzögerung (ToF - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Time-of-Flight</a> ).  Wie Sie wissen, ist die Geschwindigkeit moderner Prozessoren hoch und die Lichtgeschwindigkeit gering.  In einem Taktzyklus des Prozessors bei 3 GHz schafft es das Licht, nur 10 Zentimeter zu fliegen.  Oder 10 Maßnahmen pro Meter.  Viel Zeit, wenn jemand mit der Optimierung auf niedriger Ebene beschäftigt war.  Dementsprechend installieren wir eine gepulste Lichtquelle und eine spezielle Kamera: <br><img src="https://habrastorage.org/getpro/habr/post_images/6e0/565/fc8/6e0565fc82326ab1f3487051f20ef58d.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Die Basler-Flugzeitkamera (ToF)</a></i> <br><br>  Tatsächlich müssen wir die Verzögerung messen, mit der das Licht zu jedem Punkt zurückkehrt: <br><img src="https://habrastorage.org/getpro/habr/post_images/26b/fd8/bc8/26bfd8bc85003daa2b45d9dd11dfb31d.png"><img src="https://habrastorage.org/getpro/habr/post_images/4f6/7a5/13d/4f67a513d6cd9fb6e9370d680a28db79.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Die Basler-Flugzeitkamera (ToF)</a></i> <br><br>  Wenn wir mehrere Sensoren mit unterschiedlichen Ladungsakkumulationszeiten haben, können wir bei Kenntnis der Zeitverschiebung relativ zur Quelle für jeden Sensor und der Blitzhelligkeit die Verschiebung und dementsprechend die Entfernung zum Objekt berechnen und die Anzahl der Sensoren erhöhen, um die Genauigkeit zu erhöhen: <br><img src="https://habrastorage.org/getpro/habr/post_images/e78/c93/d70/e78c93d70d056347e6e963bb191f3016.png"><br><img src="https://habrastorage.org/getpro/habr/post_images/27f/26f/e8d/27f26fe8d30826c317bbc54687fbe169.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Larry Li Flugzeitkamera - Eine Einführung</a></i> <br><br>  Das Ergebnis ist ein solches Schema der Kamera mit LED- oder seltener Laser ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">VCSEL</a> ) -Infrarotbeleuchtung: <br><img src="https://habrastorage.org/getpro/habr/post_images/1b8/5cf/e74/1b85cfe7436630ed22abf6f9ec4c0555.png"><br>  <i>Quelle: Eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sehr gute Stellenbeschreibung von ToF bei allaboutcircuits.com</a></i> <br><br>  In diesem Fall wird das Bild mit einer relativ niedrigen Auflösung erhalten (da mehrere Sensoren mit unterschiedlichen Abrufzeiten nebeneinander platziert werden müssen), möglicherweise jedoch mit hoher FPS.  Und die Probleme liegen hauptsächlich an den Grenzen von Objekten (was typisch für alle Tiefenkameras ist).  Aber ohne die für strukturiertes Licht typischen „Schatten“: <br><img src="https://habrastorage.org/getpro/habr/post_images/f25/aac/393/f25aac393d27eab254d2d2062f70aace.gif"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Video der Basler AG</a></i> <br><br>  Insbesondere dieser Kameratyp (ToF) hat Google einmal aktiv im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Google Tango-</a> Projekt getestet, das in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesem Video</a> gut vertreten war.  Die Bedeutung war einfach: Die Daten von Gyroskop, Beschleunigungsmesser, RGB-Kamera und Tiefenkamera zu kombinieren und eine dreidimensionale Szene vor dem Smartphone zu erstellen: <br><img src="https://habrastorage.org/getpro/habr/post_images/0e5/48d/a33/0e548da33324491beed0083067daf629.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Googles Projekt Tango ist jetzt für Smartphones ausgelegt</a></i> <br><br>  Das Projekt selbst ist nicht gelaufen (meiner Meinung nach war es seiner Zeit etwas voraus), aber es hat wichtige Voraussetzungen geschaffen, um eine Welle des Interesses an AR - Augmented Reality - zu erzeugen und dementsprechend Sensoren zu entwickeln, die damit arbeiten können.  Jetzt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">fließen</a> alle seine Erfolge in Googles <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ARCore ein</a> . <br><br>  Im Allgemeinen wächst das Marktvolumen von ToF-Kameras alle drei Jahre um etwa 30%, was ein ziemlich exponentielles Wachstum darstellt, und nur wenige Märkte wachsen so schnell: <br><img src="https://habrastorage.org/getpro/habr/post_images/9ae/e45/a34/9aee45a343486d009e472897358a57c6.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Potenzial von Flugzeitkameras und Marktdurchdringung</a></i> <br><br>  Ein ernstzunehmender Treiber des heutigen Marktes ist die schnelle (und auch exponentielle) Entwicklung von Industrierobotern, für die ToF-Kameras eine ideale Lösung sind.  Wenn Ihr Roboter beispielsweise Kartons verpackt, ist es mit einer normalen 2D-Kamera äußerst untrivial festzustellen, ob Sie anfangen, den Karton zu blockieren.  Und für eine ToF-Kamera ist es trivial, sie zu „sehen“ und zu verarbeiten.  Und sehr schnell.  Infolgedessen sehen wir einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Boom bei industriellen ToF-Kameras</a> : <br><img width="50%" src="https://habrastorage.org/webt/rg/7k/oh/rg7kohuwwuzhwji6zrqr19a54yy.png"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/843/1a9/ac6/8431a9ac68acfef82b2f9dc5e5579d32.png"><br><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/c66/2c5/5c1/c662c55c1309b666ef460c5944289fbd.png"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/5d7/514/fda/5d7514fda4a985c28cbc6695e1a3e27a.png"><br>  Dies führt natürlich auch dazu, dass hausgemachte Produkte mit Tiefenkameras erscheinen.  Zum Beispiel eine Überwachungskamera mit einer Nachtvideoeinheit und einer ToF-Tiefenkamera von der deutschen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PMD Technologies</a> , die seit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">mehr als 20 Jahren</a> 3D-Kameras entwickelt: <br><img src="https://habrastorage.org/webt/dj/uz/_m/djuz_mmy6htracd_tgkn_clt698.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">3D-Flugzeit-Tiefenmessung bringt Magie in die neue Lighthouse Smart Home-Kamera</a></i> <br><br>  Erinnerst du dich an den Unsichtbarkeitsumhang, unter dem sich Harry Potter versteckte? <br><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/4cc/5ac/d00/4cc5acd002df5032104a965cc01ec109.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Harry Potters Unsichtbarkeitsumhang erhält eine Ursprungsgeschichte und kann bald im wirklichen Leben existieren</a></i> <br><br>  Ich befürchte, dass die deutsche Kamera es ein- oder zweimal erkennt.  Und es wird schwierig sein, einen Bildschirm mit einem Bild vor eine solche Kamera zu stellen (dies ist kein ablenkender Schutz für Sie): <br><img src="https://habrastorage.org/getpro/habr/post_images/af4/649/03f/af464903f5fdbda4fca2c1734f476ec5.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fragment des Films „Mission Impossible: Phantom Protocol“</a></i> <br><br>  Es scheint, dass für die neuen CCTV-Kameras Hogwarts 'nicht-kindische Magie erforderlich sein wird, um sie mit einer ToF-Tiefenkamera auszutricksen, die ein solches Video in völliger Dunkelheit aufnehmen kann: <br><img width="25%" src="https://habrastorage.org/getpro/habr/post_images/b34/f9f/884/b34f9f8844825387a528da8d630a69cb.gif"><br>  Das Vorgeben, eine Wand, ein Bildschirm und andere Möglichkeiten zu sein, sich vor der Tatsache zu schützen, dass die kombinierte ToF + RGB-Kamera einen Fremdkörper erkennt, wird technisch schwieriger. <br><br>  Eine weitere massive friedliche Anwendung für Tiefenkameras ist die Gestenerkennung.  In naher Zukunft erwarten Sie Fernseher, Konsolen und Staubsaugerroboter, die nicht nur Sprachbefehle als intelligente Lautsprecher wahrnehmen können, sondern auch das nachlässige „Clean it!“  mit einer Handbewegung.  Dann wird die Fernbedienung (auch bekannt als faul) des Smart-TV völlig unnötig und Science-Fiction wird zum Leben erweckt.  Infolgedessen wurde das, was <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">2002 fantastisch war,</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">2013 experimentell</a> und schließlich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">2019 seriell</a> (obwohl die Leute nicht wissen werden, dass sich eine Tiefenkamera im Inneren befindet, <s>welchen Unterschied macht es, wie funktioniert diese Magie?</s> ): <br><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/9e6/bd8/2bd/9e6bd82bda38b0c8bb9cb939afee76a7.png"><img width="44%" src="https://habrastorage.org/getpro/habr/post_images/2a8/dcc/b9c/2a8dccb9cc86e1fe8ad57c613432e146.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Experimente</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Produkt</a></i> <br><br>  Und das gesamte Anwendungsspektrum ist natürlich noch breiter: <br><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/66d/fc8/23d/66dfc823dd2619f52003f3dbd92bbf34.gif"><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/0c5/819/751/0c5819751f59b53a33f4b0d37efb1caf.gif"><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/7b0/266/402/7b02664025b9c9c4fbab5b5825822a2b.gif"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Video von Tiefensensoren von Terabee</a></i> <i>(übrigens, welche</i> <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Art von</a></i> <i><b>Mäusen</b> laufen sie für 2 und 3 Videos auf dem Boden? Sehen Sie sie? Nur ein Scherz, es liegt Staub in der Luft - eine Gebühr für die geringe Größe des Sensors und die Nähe der Lichtquelle zum Sensor)</i> <br><br>  Übrigens - in den berühmten "Läden ohne Kassierer" von Amazon Go gibt es auch viele Kameras unter der Decke: <br><img src="https://habrastorage.org/webt/ca/k9/ds/cak9ds3gc_8-a9n2tgwxdjhvpg0.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Im überwachungsbetriebenen Supermarkt von Amazon ohne Kasse</a></i> <br><br>  Darüber hinaus schreibt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TechCrunch</a> : <i>„Sie werden durch separate <b>Tiefenerkennungskameras</b> (unter Verwendung einer <b>Flugzeittechnik</b> , wie ich von Kumar verstanden habe) ergänzt, die wie die anderen in den Hintergrund passen, alle mattschwarz.“</i>  Das heißt, das Wunder, aus welchem ​​Regal der Joghurt entnommen wird, wird unter anderem von den mysteriösen schwarz-matten ToF-Kameras bereitgestellt (eine gute Frage, sind sie auf dem Foto): <br><img src="https://habrastorage.org/webt/25/qj/g1/25qjg1s8_o5uyjmoyv9r-haadms.png"><br><br>  Leider sind direkte Informationen oft schwer zu finden.  Aber es gibt eine indirekte.  Zum Beispiel gab es eine Firma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Softkinetic</a> , die seit 2007 ToF-Kameras entwickelt.  8 Jahre später wurden sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">von Sony gekauft</a> (das übrigens bereit ist, neue Märkte unter der Marke <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sony Depthsensing</a> zu erobern).  Einer der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">besten</a> Softkinetic- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Mitarbeiter arbeitet</a> jetzt nur noch bei Amazon Go.  So ein Zufall!  Innerhalb von ein paar Jahren, wenn die Technologie eingeführt und die Hauptpatente angemeldet werden, werden die Details höchstwahrscheinlich bekannt gegeben. <br><br>  Nun, wie immer entzünden sich die Chinesen.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pico Zense</a> zum Beispiel präsentierte auf der CES 2019 eine sehr beeindruckende Reihe von ToF-Kameras, auch für den Außenbereich: <br><img src="https://habrastorage.org/getpro/habr/post_images/d8c/c1c/545/d8cc1c545595e42cdab55a1a790384c9.png"><br>  Sie versprechen überall eine Revolution.  LKWs werden durch automatisiertes Laden dichter beladen, Geldautomaten werden sicherer, durch Tiefenkameras in jedem wird die Navigation von Robotern einfacher und genauer, Menschen (und vor allem Kinder!) Werden im Stream um eine Größenordnung besser gezählt, neue Fitness-Simulatoren werden erscheinen. C. die Fähigkeit, die Richtigkeit der Übungen ohne einen Ausbilder zu kontrollieren, und so weiter und so fort.  Natürlich sind billige chinesische Kameras einer neuen Generationstiefe bereits bereit für all diese Pracht.  Nehmen Sie und bauen Sie ein! <br><br>  Interessanterweise verfügt das neueste serielle Huawei P30 Pro über einen ToF-Sensor neben den Hauptkameras, d. H.  Das langmütige Huawei ist besser in der Lage, Apple dazu zu bringen, frontal strukturierte Lichtsensoren herzustellen, und anscheinend hat Google (Project Tango, das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">geschlossen wurde</a> ) erfolgreicher eine Kamera neben den wichtigsten ToF-Kameras implementiert: <br><img width="60%" src="https://habrastorage.org/getpro/habr/post_images/fc9/eaf/763/fc9eaf76396c966eeab065ca641e8543.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ars Technica Huawei neuer Technologiebericht Ende März 2019</a></i> <br><br>  Details der Verwendung werden natürlich nicht bekannt gegeben, aber zusätzlich zur Beschleunigung der Fokussierung (die für die drei Hauptkameras mit unterschiedlichen Objektiven wichtig ist) kann dieser Sensor verwendet werden, um die Qualität der Unschärfe des Hintergrunds von Fotos zu verbessern (Simulation einer geringen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Schärfentiefe</a> ). <br><br>  Es ist auch offensichtlich, dass die nächste Generation von Tiefensensoren neben den Hauptkameras in AR-Anwendungen verwendet wird, wodurch die Genauigkeit des AR vom aktuellen „coolen, aber häufig fehlerhaften“ auf ein Massenarbeitsniveau erhöht wird.  Angesichts der chinesischen Erfolge ist die große Frage natürlich, inwieweit Google revolutionäre chinesische Hardware in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ARCore</a> unterstützen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">möchte</a> .  Patentkriege können den Technologiemarkt erheblich verlangsamen.  Die Entwicklung dieser dramatischen Geschichte werden wir in den nächsten zwei Jahren buchstäblich sehen. <br><br><h1>  Zwischensummen </h1><br>  Vor ungefähr 25 Jahren, als die ersten automatischen Türen auftauchten, habe ich persönlich beobachtet, wie respektable Onkel vor solchen Türen regelmäßig beschleunigten.  Erfolgreich zu öffnen oder hat keine Zeit?  Sie ist groß, schwer, Glas!  Ungefähr das Gleiche habe ich kürzlich bei einer Tour durch angesehene Professoren in einer automatischen Fabrik in China beobachtet.  Sie blieben etwas hinter der Gruppe zurück, um zu sehen, was passieren würde, wenn Sie am Roboter stehen, friedlich Teile tragen und unterwegs eine ruhige, angenehme Melodie spielen würden.  Auch ich bereue, konnte nicht widerstehen ... Weißt du, es hört auf!  Vielleicht reibungslos.  Vielleicht als toter Mann.  Tiefensensoren funktionieren! <br><img src="https://habrastorage.org/webt/6z/h_/xm/6zh_xmrkvzfljpw0hkymlngojrs.png"><br>  <i>Quelle: Auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dem neuen Campus von Huawei Technology</a></i> <br><br>  Das Hotel arbeitete auch als Reinigungsroboter, was ungefähr so ​​aussah: <br><img src="https://habrastorage.org/webt/di/rm/bc/dirmbceths3wfdn1sbtswf9tduw.png"><br>  Gleichzeitig wurden sie in der Fabrik stärker gemobbt als Roboter.  Natürlich nicht so hart wie im <b><i>Unmenschlichen</i></b> in jeder Hinsicht von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bosstown Dynamics</a> .  Aber ich persönlich habe beobachtet, wie sie auf der Straße aufgestanden sind, der Roboter hat versucht, eine Person zu umgehen, die Person hat sich bewegt und die Straße blockiert ... Eine Art Katz und Maus.  Im Allgemeinen scheint es so zu sein, dass unbemannte Fahrzeuge, wenn sie auf den Straßen auftauchen, zum ersten Mal häufiger als gewöhnlich geschnitten werden ... Oh, Leute-Leute ... Hmmm ... Wir waren jedoch abgelenkt. <br><br>  Zusammenfassung der wichtigsten Punkte: <br><ul><li>  Aufgrund eines anderen Funktionsprinzips können wir die Lichtquelle in der ToF-Kamera so nah wie möglich am Sensor positionieren (auch unter demselben Objektiv).  Darüber hinaus verfügen viele Industriemodelle über LEDs, die sich um den Sensor befinden.  Infolgedessen werden die „Schatten“ auf der Tiefenkarte radikal reduziert oder verschwinden sogar.  Das heißt,  Vereinfachte Arbeit mit komplexen Geometrieobjekten, was für Industrieroboter wichtig ist. <br></li><li>  Da die gepulste Beleuchtung in der Regel Infrarot bleibt, bleiben alle im letzten Abschnitt beschriebenen Nachteile der Infrarotkamera erhalten: Sonneneinstrahlung, Schwierigkeiten, wenn zwei Kameras nebeneinander arbeiten usw.  Industrieroboter arbeiten jedoch häufig in Innenräumen, und Kameras mit Laserbeleuchtung werden entwickelt. <br></li><li>  Leider ist es für ToF-Sensoren schwieriger, die allgemeine Verbesserung der Sensoren von RGB-Kameras zu erfassen. Daher ist ihre Entwicklung langsamer, aber überraschenderweise gibt es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SEHR viele</a> Neuigkeiten über die Einführung von ToF-Kameras, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">und es gibt nichts</a> (die nur die Integration von Sensoren in Smartphones angekündigt haben) Samsung, Google Pixel und Sony Xperia ...). <br></li><li>  Das neue Sony-Versprechen, dass 2 von 8 Telefonkameras (!!!) ToF-Tiefenkameras (!) Sein werden, d. H.  Tiefenkameras befinden sich auf beiden Seiten des Telefons: <img src="https://habrastorage.org/getpro/habr/post_images/5cb/319/b6a/5cb319b6ade6cec9232d201201d15a60.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hexa-Cam Sony-Handy erhält Kamera-Spezifikationen enthüllt</a></i> <br></li><li>  Infolgedessen werden <b>wir auch im kommenden Jahr viele interessante Dinge in diesem Bereich finden!</b>  Und nächstes Jahr werden bis zu 20% der neuen Telefone mit Tiefenkameras (Structured Light + ToF) ausgestattet sein.  Angesichts der Tatsache, dass 2017 nur Apple mit „30.000 Punkten“ in hervorragender Isolation auf dem Markt war und jetzt nicht weniger als 300.000 Punkte setzen, ist das Thema eindeutig gut gelaufen: <br><img src="https://habrastorage.org/getpro/habr/post_images/463/6e5/971/4636e597115004d4a161719cfcafbe9d.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Begrenztes Wachstum des Marktes für Smartphone-3D-Sensorik im Jahr 2019;</a></i>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Apple wird 2020 der wichtigste Wachstumsförderer sein</a></i> <br></li></ul><br>  Zweifelst du immer noch an der anhaltenden Revolution? <br><br>  Dies war der erste Teil!  Ein allgemeiner Vergleich wird im zweiten sein. <br><br>  Warten Sie in der nächsten Serie: <br><ul><li>  Methode 3, klassisch: Tiefe der Stereoanlage; <br></li><li>  Methode 4, neu gefangen: Tiefe aus der Plenoptik; <br></li><li>  Methode 5, schnell wachsend: Lidare, einschließlich Festkörperlidare; <br></li><li>  Einige Probleme bei der Verarbeitung von Videos mit Tiefe; <br></li><li>  Und zum Schluss ein kurzer Vergleich aller 5 Methoden und allgemeine Schlussfolgerungen. <br></li></ul><br><br>  <b><s>Karthago muss kaputt sein ... Das</s> ganze Video wird bis Ende des Jahrhunderts dreidimensional sein!</b> <br><br>  Bleib dran!  (Wenn ich genug Zeit habe, werde ich bis Ende des Jahres neue Kameras beschreiben, einschließlich Tests von frischem Kinect.) <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 2</a> <br><br><div class="spoiler">  <b class="spoiler_title">Danksagung</b> <div class="spoiler_text">  Ich möchte mich herzlich bedanken bei: <br><ul><li>  Labor für Computergrafik VMK Moscow State University  MV Lomonosov für seinen Beitrag zur Entwicklung der Computergrafik in Russland im Allgemeinen und für die Arbeit mit Tiefenkameras im Besonderen, <br></li><li>  Microsoft, Apple, Huawei und Amazon für kamerabasierte Produkte mit großer Tiefe, <br></li><li>  Texel für die Entwicklung russischer Hightech-Produkte mit Tiefenkameras, <br></li><li>  persönlich Konstantin Kozhemyakov, der viel getan hat, um diesen Artikel besser und visueller zu machen, <br></li><li>  und schließlich vielen Dank an Roman Kazantsev, Eugene Lyapustin, Egor Sklyarov, Maxim Fedyukov, Nikolai Oplachko und Ivan Molodetsky für eine große Anzahl vernünftiger Kommentare und Korrekturen, die diesen Text viel besser gemacht haben! <br></li></ul><br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de457524/">https://habr.com/ru/post/de457524/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de457512/index.html">Logische Replikation zwischen PostgreSQL-Versionen</a></li>
<li><a href="../de457514/index.html">Nevanger</a></li>
<li><a href="../de457516/index.html">Schreiben eines Bedrohungsmodells</a></li>
<li><a href="../de457518/index.html">Plasma Cash Chain als Lösung für das Blockchain-Skalierbarkeitstrilemma</a></li>
<li><a href="../de457522/index.html">Erhöhen Sie Ihren Mailinglistendienst oder verwenden Sie vorgefertigte Lösungen? Was ich über 5 Jahre bei UniSender gelernt habe</a></li>
<li><a href="../de457526/index.html">Technische Medien als Basar</a></li>
<li><a href="../de457532/index.html">Es ist höchste Zeit, Teil eines Open Source-Projekts zu werden</a></li>
<li><a href="../de457534/index.html">Zertifizierte Versionen - der Rechen, den wir wählen</a></li>
<li><a href="../de457538/index.html">Wie kann ich unterbrochene virtuelle Yandex.Cloud-Maschinen verwenden und bei der Lösung großer Probleme sparen?</a></li>
<li><a href="../de457540/index.html">Intel Optane DC Persistent Memory, ein Jahr später</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>