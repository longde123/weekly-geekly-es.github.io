<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèæ‚Äçüöí üõÄüèø üë∞üèΩ Pr√©sentation d'Airflow pour g√©rer les Spark Jobs √† ivi: espoirs et b√©quilles üö† üß£ üë´</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La t√¢che de d√©ployer des mod√®les d'apprentissage automatique en production est toujours p√©nible, car il est tr√®s inconfortable de sortir d'un ordinate...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Pr√©sentation d'Airflow pour g√©rer les Spark Jobs √† ivi: espoirs et b√©quilles</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ivi/blog/456630/">  La t√¢che de d√©ployer des mod√®les d'apprentissage automatique en production est toujours p√©nible, car il est tr√®s inconfortable de sortir d'un ordinateur portable jupyter confortable dans le monde de la surveillance et de la tol√©rance aux pannes. <br><br>  Nous avons d√©j√† √©crit sur la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">premi√®re it√©ration de refactorisation du</a> syst√®me de recommandation du cin√©ma en ligne ivi.  Au cours de la derni√®re ann√©e, nous n'avons presque pas finalis√© l'architecture de l'application (de global - passant seulement de python 2.7 et python 3.4 obsol√®te √† python 3.6 ¬´frais¬ª), mais nous avons ajout√© quelques nouveaux mod√®les ML et nous sommes imm√©diatement heurt√©s au probl√®me du d√©ploiement de nouveaux algorithmes en production.  Dans l'article, je vais parler de notre exp√©rience dans la mise en ≈ìuvre d'un outil de gestion de flux de t√¢ches comme Apache Airflow: pourquoi l'√©quipe avait ce besoin, ce qui ne convenait pas √† la solution existante, quelles b√©quilles devaient √™tre coup√©es en cours de route et ce qui en √©tait le r√©sultat. <br><br>  ‚Üí La version vid√©o du rapport peut √™tre visionn√©e sur YouTube (√† partir de 03:00:00) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qc/bw/tt/qcbwttjzivyzuedihk88u6nwn5o.png"></div><br><a name="habracut"></a><br><br><h2>  <font color="#fd004c">√âquipe Hydra</font> </h2><br>  Je vais vous parler un peu du projet: ivi, c'est plusieurs dizaines de milliers d'unit√©s de contenu, nous avons l'un des plus grands r√©pertoires juridiques de RuNet.  La page principale de la version Web ivi est une coupe personnalis√©e du catalogue, qui est con√ßue pour fournir √† l'utilisateur le contenu le plus riche et le plus pertinent en fonction de ses commentaires (vues, notes, etc.). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/vj/-g/nh/vj-gnheaviq-z7tibuzawo-qdqs.png"></div><br>  La partie en ligne du syst√®me de recommandation est une application backend Flask avec une charge allant jusqu'√† 600 RPS.  Hors ligne, le mod√®le est form√© sur plus de 250 millions de vues de contenu par mois.  Les pipelines de pr√©paration des donn√©es pour la formation sont impl√©ment√©s sur Spark, qui s'ex√©cute au-dessus du r√©f√©rentiel Hive. <br><br>  L'√©quipe compte maintenant 7 d√©veloppeurs qui sont engag√©s √† la fois dans la cr√©ation de mod√®les et leur d√©ploiement en production - il s'agit d'une √©quipe assez importante qui n√©cessite des outils pratiques pour g√©rer les flux de t√¢ches. <br><br><h2>  <font color="#fd004c">Architecture hors ligne</font> </h2><br>  Ci-dessous, vous voyez le diagramme d'infrastructure de flux de donn√©es pour le syst√®me de recommandation. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xs/10/nv/xs10nvc8r3mz9bd8osjrn6sqn3u.png"></div><br>  Deux stockages de donn√©es sont illustr√©s ici - Hive pour les commentaires des utilisateurs (vues, √©valuations) et Postgres pour diverses informations commerciales (types de mon√©tisation de contenu, etc.), tandis que le transfert de Postgres √† Hive est ajust√©.  Un pack d'applications Spark aspire les donn√©es de Hive: et forme nos mod√®les sur ces donn√©es (ALS pour les recommandations personnelles, divers mod√®les collaboratifs de similitude de contenu). <br><br>  Les applications Spark sont traditionnellement g√©r√©es √† partir d'une machine virtuelle d√©di√©e, que nous appelons hydra-updater en utilisant un tas de scripts cron + shell.  Ce bundle a √©t√© cr√©√© dans le d√©partement des op√©rations ivi dans des temps imm√©moriaux et a tr√®s bien fonctionn√©.  Shell-script √©tait un point d'entr√©e unique pour lancer des applications spark - c'est-√†-dire que chaque nouveau mod√®le a commenc√© √† tourner dans la prod uniquement apr√®s que les administrateurs ont termin√© ce script. <br><br>  Certains des artefacts de la formation des mod√®les sont stock√©s dans HDFS pour un stockage √©ternel (et attendent que quelqu'un les t√©l√©charge √† partir de l√† et les transf√®re vers le serveur o√π la partie en ligne tourne), et certains sont √©crits directement du pilote Spark vers le stockage rapide Redis, que nous utilisons comme g√©n√©ral m√©moire pour plusieurs dizaines de processus python de la partie en ligne. <br><br>  Une telle architecture a accumul√© un certain nombre d'inconv√©nients au fil du temps: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ri/x2/b0/rix2b0qjxi1qa0igp-bpl23bn04.png"></div><br>  Le diagramme montre que les flux de donn√©es ont une structure plut√¥t compliqu√©e et compliqu√©e - sans un outil simple et clair pour g√©rer ce bien, le d√©veloppement et le fonctionnement se transformeront en horreur, en d√©cadence et en souffrance. <br><br>  En plus de g√©rer les applications spark, le script d'administration fait beaucoup de choses utiles: red√©marrage des services dans la bataille, un vidage Redis et d'autres choses syst√®me.  De toute √©vidence, sur une longue p√©riode de fonctionnement, le script a envahi de nombreuses fonctions, car chaque nouveau mod√®le de la n√¥tre a g√©n√©r√© quelques dizaines de lignes.  Le script a commenc√© √† sembler trop surcharg√© en termes de fonctionnalit√©s.Par cons√©quent, en tant qu'√©quipe du syst√®me de recommandation, nous voulions retirer quelque part une partie des fonctionnalit√©s qui concernent le lancement et la gestion des applications Spark.  √Ä ces fins, nous avons d√©cid√© d'utiliser Airflow. <br><br><h2>  <font color="#fd004c">B√©quilles pour Airflow</font> </h2><br>  En plus de r√©soudre tous ces probl√®mes, bien s√ªr, nous en avons cr√©√© de nouveaux pour nous - d√©ployer Airflow pour lancer et surveiller les applications Spark s'est av√©r√© difficile. <br><br>  La principale difficult√© √©tait que personne ne remodelerait l'ensemble de l'infrastructure pour nous, car  Devops Resource est une chose rare.  Pour cette raison, nous avons d√ª non seulement impl√©menter Airflow, mais l'int√©grer dans le syst√®me existant, ce qui est beaucoup plus difficile √† voir √† partir de z√©ro. <br><br>  Je veux parler des douleurs que nous avons rencontr√©es pendant le processus de mise en ≈ìuvre et des b√©quilles que nous avons d√ª entailler pour obtenir Airflow. <br><br>  <b>La premi√®re et principale douleur</b> : comment int√©grer Airflow dans un grand script shell du d√©partement des op√©rations. <br><br>  Ici, la solution est la plus √©vidente - nous avons commenc√© √† d√©clencher des graphiques directement √† partir du script shell en utilisant le binaire airflow avec la cl√© trigger_dag.  Avec cette approche, nous n'utilisons pas le sheduler Airflow, et en fait l'application Spark est lanc√©e avec la m√™me couronne - ce n'est religieusement pas tr√®s correct.  Mais nous avons obtenu une int√©gration transparente avec une solution existante.  Voici √† quoi ressemble le d√©but du script shell de notre principale application Spark, qui est historiquement appel√©e hydramatrices. <br><br><pre><code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">log</span></span> <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$FUNCNAME</span></span></span><span class="hljs-string"> started"</span></span> <span class="hljs-built_in"><span class="hljs-built_in">local</span></span> RETVAL=0 <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> AIRFLOW_CONFIG=/opt/airflow/airflow.cfg AIRFLOW_API=api/dag_last_run/hydramatrices/all <span class="hljs-built_in"><span class="hljs-built_in">log</span></span> <span class="hljs-string"><span class="hljs-string">"run /var/www/airflow/bin/airflow trigger_dag hydramatrices"</span></span> /var/www/airflow/bin/airflow trigger_dag hydramatrices 2&gt;&amp;1 | tee -a <span class="hljs-variable"><span class="hljs-variable">$LOGFILE</span></span></code> </pre> <br>  <b>Douleur: Le</b> script shell du d√©partement des op√©rations doit en quelque sorte d√©terminer l'√©tat du graphique Airflow afin de contr√¥ler son propre flux d'ex√©cution. <br><br>  Crutch: nous avons √©tendu l'API REST Airflow avec un point de terminaison pour la surveillance DAG directement dans les scripts shell.  Maintenant, chaque graphique a trois √©tats: RUNNING, SUCCEED, FAILED. <br><br>  En fait, apr√®s avoir d√©marr√© les calculs dans Airflow, nous interrogeons simplement r√©guli√®rement le graphique en cours d'ex√©cution: nous bullet la requ√™te GET pour d√©terminer si le DAG s'est termin√© ou non.  Lorsque le point de terminaison de surveillance r√©pond de l'ex√©cution r√©ussie du graphique, le script shell continue d'ex√©cuter son flux. <br>  Je veux dire que l'API Airflow REST est juste une chose ardente qui vous permet de configurer de mani√®re flexible vos pipelines - par exemple, vous pouvez transmettre les param√®tres POST aux graphiques. <br><br>  L'extension API Airflow est juste une classe Python qui ressemble √† ceci: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> json <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> settings <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> DagBag, DagRun <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> flask <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Blueprint, request, Response airflow_api_blueprint = Blueprint(<span class="hljs-string"><span class="hljs-string">'airflow_api'</span></span>, __name__, url_prefix=<span class="hljs-string"><span class="hljs-string">'/api'</span></span>) AIRFLOW_DAGS = <span class="hljs-string"><span class="hljs-string">'{}/dags'</span></span>.format( os.path.dirname(os.path.dirname(os.path.abspath(__file__))) ) <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ApiResponse</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    GET """</span></span> STATUS_OK = <span class="hljs-number"><span class="hljs-number">200</span></span> STATUS_NOT_FOUND = <span class="hljs-number"><span class="hljs-number">404</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">pass</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">standard_response</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(status: int, payload: dict)</span></span></span><span class="hljs-function"> -&gt; Response:</span></span> json_data = json.dumps(payload) resp = Response(json_data, status=status, mimetype=<span class="hljs-string"><span class="hljs-string">'application/json'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> resp <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">success</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, payload: dict)</span></span></span><span class="hljs-function"> -&gt; Response:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.standard_response(self.STATUS_OK, payload) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">error</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, status: int, message: str)</span></span></span><span class="hljs-function"> -&gt; Response:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.standard_response(status, {<span class="hljs-string"><span class="hljs-string">'error'</span></span>: message}) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">not_found</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, message: str = </span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'Resource not found'</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function"> -&gt; Response:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.error(self.STATUS_NOT_FOUND, message)</code> </pre><br>  Nous utilisons l'API dans le script shell - nous interrogeons le point de terminaison toutes les 10 minutes: <br><br><pre> <code class="bash hljs"> TRIGGER=$? [ <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$TRIGGER</span></span></span><span class="hljs-string">"</span></span> -eq <span class="hljs-string"><span class="hljs-string">"0"</span></span> ] &amp;&amp; <span class="hljs-built_in"><span class="hljs-built_in">log</span></span> <span class="hljs-string"><span class="hljs-string">"trigger airflow DAG succeeded"</span></span> || { <span class="hljs-built_in"><span class="hljs-built_in">log</span></span> <span class="hljs-string"><span class="hljs-string">"trigger airflow DAG failed"</span></span>; <span class="hljs-built_in"><span class="hljs-built_in">return</span></span> 1; } CMD=<span class="hljs-string"><span class="hljs-string">"curl -s http://</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$HYDRA_SERVER</span></span></span><span class="hljs-string">/</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$AIRFLOW_API</span></span></span><span class="hljs-string"> | jq .dag_last_run.state"</span></span> STATE=$(<span class="hljs-built_in"><span class="hljs-built_in">eval</span></span> <span class="hljs-variable"><span class="hljs-variable">$CMD</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> [ <span class="hljs-variable"><span class="hljs-variable">$STATE</span></span> == \<span class="hljs-string"><span class="hljs-string">"running\" ]; do log "</span></span>Generating matrices <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> progress...<span class="hljs-string"><span class="hljs-string">" sleep 600 STATE=</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$(eval $CMD)</span></span></span><span class="hljs-string"> done [ </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$STATE</span></span></span><span class="hljs-string"> == \"success\" ] &amp;&amp; RETVAL=0 || RETVAL=1 [ </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$RETVAL</span></span></span><span class="hljs-string"> -eq 0 ] &amp;&amp; log "</span></span><span class="hljs-variable"><span class="hljs-variable">$FUNCNAME</span></span> succeeded<span class="hljs-string"><span class="hljs-string">" || log "</span></span><span class="hljs-variable"><span class="hljs-variable">$FUNCNAME</span></span> failed<span class="hljs-string"><span class="hljs-string">" return </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$RETVAL</span></span></span></span></code> </pre><br>  <b>Douleur</b> : si vous ex√©cutez un travail Spark √† l'aide de spark-submit en mode cluster, vous savez que les journaux dans STDOUT sont une feuille non informative avec les lignes ¬´SPARK APPLICATION_ID IS RUNNING¬ª.  Les journaux de l'application Spark elle-m√™me peuvent √™tre affich√©s, par exemple, √† l'aide de la commande Yarn logs.  Dans le script shell, ce probl√®me a √©t√© r√©solu simplement: un tunnel SSH a √©t√© ouvert √† l'une des machines du cluster et spark-submit a √©t√© ex√©cut√© en mode client pour cette machine.  Dans ce cas, STDOUT aura des journaux lisibles et compr√©hensibles.  Dans Airflow, nous avons d√©cid√© de toujours utiliser cluster-decide, et un tel nombre ne fonctionnera pas. <br><br>  Crutch: une fois que spark-submit a fonctionn√©, nous extrayons les journaux du pilote de HDFS par application_id et les affichons dans l'interface Airflow via l'op√©rateur Python print ().  Le seul point n√©gatif - dans l'interface Airflow, les journaux n'apparaissent qu'apr√®s que le spark-submit a fonctionn√©, vous devez surveiller le temps r√©el dans d'autres endroits - par exemple, le museau Web YARN. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_logs</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(config: BaseConfig, app_id: str)</span></span></span><span class="hljs-function"> -&gt; </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">None</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""   :param config: :param app_id: """</span></span> hdfs = HDFSInteractor(config) logs_path = <span class="hljs-string"><span class="hljs-string">'/tmp/logs/{username}/logs/{app_id}'</span></span>.format(username=config.CURRENT_USERNAME, app_id=app_id) logs_files = hdfs.files_in_folder(logs_path) logs_files = [file <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> file <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> logs_files <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> file[<span class="hljs-number"><span class="hljs-number">-4</span></span>:] != <span class="hljs-string"><span class="hljs-string">'.tmp'</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> file <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> logs_files: <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> hdfs.hdfs_client.read(os.path.join(logs_path, file), encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>, delimiter=<span class="hljs-string"><span class="hljs-string">'\n'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> reader: print_line = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> reader: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> re.search(<span class="hljs-string"><span class="hljs-string">'stdout'</span></span>, line) <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> len(line) &gt; <span class="hljs-number"><span class="hljs-number">30</span></span>: print_line = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> re.search(<span class="hljs-string"><span class="hljs-string">'stderr'</span></span>, line): print_line = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> print_line: print(line)</code> </pre><br>  <b>Douleur</b> : pour les testeurs et les d√©veloppeurs, ce serait bien d'avoir un banc de test Airflow, mais nous √©conomisons des ressources Devops, nous avons donc r√©fl√©chi √† la fa√ßon de d√©ployer l'environnement de test pendant longtemps. <br><br>  B√©quille: nous avons emball√© Airflow dans un conteneur Docker et Dockerfile l'a plac√© directement dans le r√©f√©rentiel avec des t√¢ches d'allumage.  Ainsi, chaque d√©veloppeur ou testeur peut augmenter son propre flux d'air sur une machine locale.  En raison du fait que les applications s'ex√©cutent en mode cluster, les ressources locales pour docker ne sont presque pas n√©cessaires. <br><br>  Une installation locale de l'√©tincelle √©tait cach√©e √† l'int√©rieur du conteneur docker et de sa configuration enti√®re via des variables d'environnement - vous n'avez plus besoin de passer plusieurs heures √† configurer l'environnement.  Ci-dessous, j'ai donn√© un exemple avec un fragment de fichier docker pour un conteneur avec Airflow, o√π vous pouvez voir comment Airflow est configur√© √† l'aide de variables d'environnement: <br><br><pre> <code class="bash hljs">FROM ubuntu:16.04 ARG AIRFLOW_VERSION=1.9.0 ARG AIRFLOW_HOME ARG USERNAME=airflow ARG USER_ID ARG GROUP_ID ARG LOCALHOST ARG AIRFLOW_PORT ARG PIPENV_PATH ARG PROJECT_HYDRAMATRICES_DOCKER_PATH RUN apt-get update \ &amp;&amp; apt-get install -y \ python3.6 \ python3.6-dev \ &amp;&amp; update-alternatives --install /usr/bin/python3 python3.6 /usr/bin/python3.6 0 \ &amp;&amp; apt-get -y install python3-pip RUN mv /root/.pydistutils.cf /root/.pydistutils.cfg RUN pip3 install pandas==0.20.3 \ apache-airflow==<span class="hljs-variable"><span class="hljs-variable">$AIRFLOW_VERSION</span></span> \ psycopg2==2.7.5 \ ldap3==2.5.1 \ cryptography <span class="hljs-comment"><span class="hljs-comment">#   ,       ENV PROJECT_HYDRAMATRICES_DOCKER_PATH=${PROJECT_HYDRAMATRICES_DOCKER_PATH} ENV PIPENV_PATH=${PIPENV_PATH} ENV SPARK_HOME=/usr/lib/spark2 ENV HADOOP_CONF_DIR=$PROJECT_HYDRAMATRICES_DOCKER_PATH/etc/hadoop-conf-preprod ENV PYTHONPATH=${SPARK_HOME}/python/lib/py4j-0.10.4-src.zip:${SPARK_HOME}/python/lib/pyspark.zip:${SPARK_HOME}/python/lib ENV PIP_NO_BINARY=numpy ENV AIRFLOW_HOME=${AIRFLOW_HOME} ENV AIRFLOW_DAGS=${AIRFLOW_HOME}/dags ENV AIRFLOW_LOGS=${AIRFLOW_HOME}/logs ENV AIRFLOW_PLUGINS=${AIRFLOW_HOME}/plugins #      Airflow (log url) BASE_URL="http://${AIRFLOW_CURRENT_HOST}:${AIRFLOW_PORT}" ; #   Airflow ENV AIRFLOW__WEBSERVER__BASE_URL=${BASE_URL} ENV AIRFLOW__WEBSERVER__ENDPOINT_URL=${BASE_URL} ENV AIRFLOW__CORE__AIRFLOW_HOME=${AIRFLOW_HOME} ENV AIRFLOW__CORE__DAGS_FOLDER=${AIRFLOW_DAGS} ENV AIRFLOW__CORE__BASE_LOG_FOLDER=${AIRFLOW_LOGS} ENV AIRFLOW__CORE__PLUGINS_FOLDER=${AIRFLOW_PLUGINS} ENV AIRFLOW__SCHEDULER__CHILD_PROCESS_LOG_DIRECTORY=${AIRFLOW_LOGS}/scheduler</span></span></code> </pre><br>  Gr√¢ce √† la mise en ≈ìuvre d'Airflow, nous avons obtenu les r√©sultats suivants: <br><br><ul><li>  R√©duction du cycle de publication: le d√©ploiement d'un nouveau mod√®le (ou d'un pipeline de pr√©paration de donn√©es) se r√©sume d√©sormais √† l'√©criture d'un nouveau graphique Airflow, les graphiques eux-m√™mes sont stock√©s dans le r√©f√©rentiel et d√©ploy√©s avec le code.  Ce processus est enti√®rement entre les mains du d√©veloppeur.  Les administrateurs sont contents, on ne les tire plus sur des bagatelles. </li><li>  Les journaux des applications Spark qui allaient directement en enfer sont d√©sormais stock√©s dans Aiflow avec une interface d'acc√®s pratique.  Vous pouvez voir les journaux de n'importe quel jour sans les s√©lectionner dans les r√©pertoires HDFS. </li><li>  Le calcul qui a √©chou√© peut √™tre red√©marr√© avec un seul bouton dans l'interface, c'est tr√®s pratique, m√™me June peut le g√©rer. </li><li>  Vous pouvez puiser des t√¢ches spark √† partir de l'interface sans avoir √† ex√©cuter les param√®tres Spark sur la machine locale.  Les testeurs sont satisfaits - tous les param√®tres pour que spark-submit fonctionne correctement sont d√©j√† d√©finis dans Dockerfile </li><li>  Petits pains standard Aiflow - planifications, red√©marrage des travaux abandonn√©s, beaux graphiques (par exemple, temps d'ex√©cution de l'application, statistiques des lancements r√©ussis et infructueux). </li></ul><br>  O√π aller ensuite?  Nous avons maintenant un grand nombre de sources et de puits de donn√©es, dont le nombre augmentera.  Les changements dans n'importe quelle classe de r√©f√©rentiel hydramatrices peuvent planter dans un autre pipeline (ou m√™me dans la partie en ligne): <br><br><ul><li>  Clickhouse d√©borde ‚Üí Hive </li><li>  pr√©traitement des donn√©es: Hive ‚Üí Hive </li><li>  d√©ployer des mod√®les c2c: Hive ‚Üí Redis </li><li>  pr√©paration d'annuaires (comme le type de mon√©tisation de contenu): Postgres ‚Üí Redis </li><li>  pr√©paration du mod√®le: FS local ‚Üí HDFS </li></ul><br>  Dans une telle situation, nous avons vraiment besoin d'un stand pour les tests automatiques des pipelines dans la pr√©paration des donn√©es.  Cela r√©duira consid√©rablement le co√ªt des tests de modifications dans le r√©f√©rentiel, acc√©l√©rera le d√©ploiement de nouveaux mod√®les en production et augmentera consid√©rablement le niveau d'endorphines dans les testeurs.  Mais sans Airflow, il serait impossible de d√©ployer un stand pour ce genre de test automatique! <br><br>  J'ai √©crit cet article pour parler de notre exp√©rience dans la mise en ≈ìuvre d'Airflow, qui peut √™tre utile √† d'autres √©quipes dans une situation similaire - vous avez d√©j√† un grand syst√®me de travail et vous voulez essayer quelque chose de nouveau, √† la mode et jeune.  Pas besoin d'avoir peur des mises √† jour du syst√®me de travail, vous devez essayer et exp√©rimenter - ces exp√©riences ouvrent g√©n√©ralement de nouveaux horizons pour un d√©veloppement ult√©rieur. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr456630/">https://habr.com/ru/post/fr456630/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr456614/index.html">How A Plague Tale: Innocence Frame Renders</a></li>
<li><a href="../fr456616/index.html">3 millions de roubles pour ceux qui savent coder</a></li>
<li><a href="../fr456618/index.html">Larabeer Moscou - 21 juin</a></li>
<li><a href="../fr456622/index.html">Comment cr√©er un OS certifi√© selon la protection de classe I</a></li>
<li><a href="../fr456624/index.html">Outils Python utiles</a></li>
<li><a href="../fr456632/index.html">Nous construisons le quatri√®me √©tage des mod√®les C ++ dans RESTinio. Pourquoi et comment?</a></li>
<li><a href="../fr456634/index.html">Recettes Nginx: CAS (Central Authorization Service)</a></li>
<li><a href="../fr456638/index.html">Comparaison du m√™me projet dans Rust, Haskell, C ++, Python, Scala et OCaml</a></li>
<li><a href="../fr456640/index.html">Analyse du concours d'intelligence √©conomique √† PHDays 9</a></li>
<li><a href="../fr456642/index.html">La premi√®re remise des dipl√¥mes du programme JetBrains Corporate Master et ITMO University</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>