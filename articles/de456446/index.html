<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©‚Äçüíº ü•´ üë© Ceph - von "auf dem Knie" bis "Produktion" üôÅ üßñüèæ ü•Ö</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wahl der CEPH. Teil 1 


 Wir hatten f√ºnf Racks, zehn optische Switches, konfiguriertes BGP, ein paar Dutzend SSDs und eine Reihe von SAS-Festplatten ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ceph - von "auf dem Knie" bis "Produktion"</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/456446/"><h1 id="vybor-ceph-chast-1">  Wahl der CEPH.  Teil 1 </h1><br><p>  <em>Wir hatten f√ºnf Racks, zehn optische Switches, konfiguriertes BGP, ein paar Dutzend SSDs und eine Reihe von SAS-Festplatten aller Farben und Gr√∂√üen sowie Proxmox und den Wunsch, die gesamte statische Aufladung in unseren eigenen S3-Speicher zu stellen.</em>  <em>Nicht, dass dies alles f√ºr die Virtualisierung notwendig gewesen w√§re, aber sobald Sie OpenSource verwendet haben, gehen Sie Ihrem Hobby bis zum Ende nach.</em>  <em>Das einzige, was mich st√∂rte, war BGP.</em>  <em>Es gibt niemanden auf der Welt, der hilfloser, verantwortungsloser und unmoralischer ist als das interne BGP-Routing.</em>  <em>Und ich wusste, dass wir uns ziemlich bald darauf einlassen w√ºrden.</em> </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/09e/a36/178/09ea3617814a9598a6aa9784abc14a76.jpg"></p><br><p>  Die Aufgabe war allt√§glich - es gab CEPH, es funktionierte nicht sehr gut.  Es war notwendig, "gut" zu machen. <br>  Der Cluster, den ich bekam, war heterogen, ausgepeitscht und praktisch nicht abgestimmt.  Es bestand aus zwei Gruppen verschiedener Knoten, wobei ein gemeinsames Gitter sowohl die Rolle des Clusters als auch des √∂ffentlichen Netzwerks spielte.  Die Knoten wurden mit vier Festplattentypen gepackt - zwei Arten von SSDs, die zu zwei separaten Platzierungsregeln zusammengesetzt waren, und zwei Arten von Festplatten unterschiedlicher Gr√∂√üe, die zu einer dritten Gruppe zusammengefasst waren.  Das Problem mit unterschiedlichen Gr√∂√üen wurde durch unterschiedliche OSD-Gewichte gel√∂st. </p><br><p>  Die Einstellung selbst war in zwei Teile unterteilt: <strong>Optimieren</strong> <strong>des Betriebssystems</strong> und <strong>Optimieren des CEPH selbst</strong> und seiner Einstellungen. </p><a name="habracut"></a><br><h2 id="prokachka-os">  Leveling OS </h2><br><h3 id="network">  Netzwerk </h3><br><p>  Die hohe Latenz wirkt sich sowohl beim Aufnehmen als auch beim Balancieren aus.  Bei der Aufzeichnung - da der Client keine Antwort auf die erfolgreiche Aufzeichnung erh√§lt, bis die Datenreplikate in anderen Platzierungsgruppen den Erfolg best√§tigen.  Da die Regeln f√ºr die Verteilung von Replikaten in der CRUSH-Map ein Replikat pro Host hatten, wurde immer das Netzwerk verwendet. </p><br><p>  Daher habe ich mich als erstes entschlossen, das aktuelle Netzwerk leicht zu konfigurieren, w√§hrend ich versuchte, mich davon zu √ºberzeugen, in separate Netzwerke zu wechseln. </p><br><p>  Drehen Sie zun√§chst die Einstellungen der Netzwerkkarten.  Begonnen mit dem Einrichten von Warteschlangen: </p><br><p>  was war: </p><br><div class="spoiler">  <b class="spoiler_title">ethtool -l ens1f1</b> <div class="spoiler_text"><pre><code class="plaintext hljs">root@ceph01:~# ethtool -l ens1f1 Channel parameters for ens1f1: Pre-set maximums: RX: 0 TX: 0 Other: 1 Combined: 63 Current hardware settings: RX: 0 TX: 0 Other: 1 Combined: 1 root@ceph01:~# ethtool -g ens1f1 Ring parameters for ens1f1: Pre-set maximums: RX: 4096 RX Mini: 0 RX Jumbo: 0 TX: 4096 Current hardware settings: RX: 256 RX Mini: 0 RX Jumbo: 0 TX: 256 root@ceph01:~# ethtool -l ens1f1 Channel parameters for ens1f1: Pre-set maximums: RX: 0 TX: 0 Other: 1 Combined: 63 Current hardware settings: RX: 0 TX: 0 Other: 1 Combined: 1</code> </pre> </div></div><br><p>  Es ist ersichtlich, dass die aktuellen Parameter weit von den Maximalwerten entfernt sind.  Erh√∂ht: </p><br><pre> <code class="plaintext hljs">root@ceph01:~#ethtool -G ens1f0 rx 4096 root@ceph01:~#ethtool -G ens1f0 tx 4096 root@ceph01:~#ethtool -L ens1f0 combined 63</code> </pre> <br><p>  Gef√ºhrt von einem ausgezeichneten Artikel </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data/</a> </p><br><p>  Die L√§nge der <strong>Versandwarteschlange</strong> von <strong>txqueuelen wurde</strong> von 1000 auf 10.000 erh√∂ht </p><br><pre> <code class="plaintext hljs">root@ceph01:~#ip link set ens1f0 txqueuelen 10000</code> </pre> <br><p>  Nun, folgen Sie der Dokumentation von Ceph selbst </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://ceph.com/geen-categorie/ceph-loves-jumbo-frames/</a> </p><br><p>  erh√∂hte <strong>MTU</strong> auf 9000. </p><br><pre> <code class="plaintext hljs">root@ceph01:~#ip link set dev ens1f0 mtu 9000</code> </pre> <br><p>  Zu / etc / network / interfaces hinzugef√ºgt, damit alle oben genannten Elemente beim Start geladen werden </p><br><div class="spoiler">  <b class="spoiler_title">cat / etc / network / interfaces</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">root@ceph01:~# cat /etc/network/interfaces auto lo iface lo inet loopback auto ens1f0 iface ens1f0 inet manual post-up /sbin/ethtool -G ens1f0 rx 4096 post-up /sbin/ethtool -G ens1f0 tx 4096 post-up /sbin/ethtool -L ens1f0 combined 63 post-up /sbin/ip link set ens1f0 txqueuelen 10000 mtu 9000 auto ens1f1 iface ens1f1 inet manual post-up /sbin/ethtool -G ens1f1 rx 4096 post-up /sbin/ethtool -G ens1f1 tx 4096 post-up /sbin/ethtool -L ens1f1 combined 63 post-up /sbin/ip link set ens1f1 txqueuelen 10000 mtu 9000</code> </pre> </div></div><br><p>  Danach begann er nach demselben Artikel, die 4.15-Kernel-Handles vorsichtig aufzuwickeln.  In Anbetracht dessen, dass wir auf 128G RAM-Knoten eine bestimmte Konfigurationsdatei f√ºr <strong>sysctl haben</strong> </p><br><div class="spoiler">  <b class="spoiler_title">cat /etc/sysctl.d/50-ceph.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">net.core.rmem_max = 56623104 #        54M net.core.wmem_max = 56623104 #        54M net.core.rmem_default = 56623104 #        . 54M net.core.wmem_default = 56623104 #         54M #    net.ipv4.tcp_rmem = 4096 87380 56623104 # (,  , )    tcp_rmem #  3  ,      TCP. # :   TCP       #   .     #       (moderate memory pressure). #       8  (8192). #  :  ,    #   TCP  .     #  /proc/sys/net/core/rmem_default,   . #       ( ) #  87830 .     65535  #     tcp_adv_win_scale  tcp_app_win = 0, #  ,       tcp_app_win. # :   ,     #     TCP.     , #    /proc/sys/net/core/rmem_max.  ¬´¬ª #     SO_RCVBUF     . net.ipv4.tcp_wmem = 4096 65536 56623104 net.core.somaxconn = 5000 #    ,  . net.ipv4.tcp_timestamps=1 #     (timestamps),    RFC 1323. net.ipv4.tcp_sack=1 #     TCP net.core.netdev_max_backlog=5000 ( 1000) #       ,  #    ,     . net.ipv4.tcp_max_tw_buckets=262144 #   ,    TIME-WAIT . #     ‚Äì ¬´¬ª     #    . net.ipv4.tcp_tw_reuse=1 #   TIME-WAIT   , #     . net.core.optmem_max=4194304 #   - ALLOCATABLE #    (4096 ) net.ipv4.tcp_low_latency=1 #  TCP/IP      #     . net.ipv4.tcp_adv_win_scale=1 #          , #    TCP-    . #   tcp_adv_win_scale ,     #   : # Bytes- bytes\2  -tcp_adv_win_scale #  bytes ‚Äì     .   tcp_adv_win_scale # ,       : # Bytes- bytes\2  tcp_adv_win_scale #    .  - ‚Äì 2, # ..     ¬º  ,   # tcp_rmem. net.ipv4.tcp_slow_start_after_idle=0 #    ,     # ,       . #   SSR  ,    #  . net.ipv4.tcp_no_metrics_save=1 #    TCP      . net.ipv4.tcp_syncookies=0 #   syncookie net.ipv4.tcp_ecn=0 #Explicit Congestion Notification (   )  # TCP-.      ¬´¬ª #       .     # -        #    . net.ipv4.conf.all.send_redirects=0 #   ICMP Redirect ‚Ä¶  .    #   ,        . #    . net.ipv4.ip_forward=0 #  .   ,     , #    . net.ipv4.icmp_echo_ignore_broadcasts=1 #   ICMP ECHO ,    net.ipv4.tcp_fin_timeout=10 #      FIN-WAIT-2   #   .  60 net.core.netdev_budget=600 # ( 300) #        , #          #  .    NIC ,    . # ,     SoftIRQs # ( )  CPU.    netdev_budget. #    300.    SoftIRQ  # 300   NIC     CPU net.ipv4.tcp_fastopen=3 # TFO TCP Fast Open #        TFO,      #    TCP .     ,  #  )</code> </pre> </div></div><br><p>  Mit <strong>Lustre Network wurde es</strong> auf separaten 10-Gbit / s-Netzwerkschnittstellen einem separaten Flachnetzwerk zugewiesen.  Auf jedem Computer wurden Mellanox <strong>10/25-</strong> Gbit <strong>/</strong> s-Dual-Port-Netzwerkkarten geliefert, <strong>die</strong> in zwei separaten 10-Gbit <strong>/</strong> s-Switches geliefert wurden.  Die Aggregation wurde unter Verwendung von OSPF durchgef√ºhrt, da das Verbinden mit lacp aus irgendeinem Grund eine Gesamtbandbreite von maximal 16 Gbit / s zeigte, w√§hrend ospf beide Dutzend auf jeder Maschine erfolgreich vollst√§ndig nutzte.  Zuk√ºnftige Pl√§ne waren die Verwendung von ROCE f√ºr diese Melanoxe, um die Latenz zu verringern.  So konfigurieren Sie diesen Teil des Netzwerks: </p><br><ol><li>  Da die Maschinen selbst externe IPs auf BGP haben, ben√∂tigen wir Software - <em>(oder besser gesagt, zum Zeitpunkt dieses Schreibens war es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">frr = 6.0-1</a> )</em> . </li><li>  Insgesamt hatten die Maschinen zwei Netzwerkschnittstellen mit zwei Schnittstellen - insgesamt 4 Ports.  Eine Netzwerkkarte wurde ab Werk mit zwei Ports betrachtet und BGP wurde darauf konfiguriert, die zweite - an zwei Ports wurden zwei verschiedene Switches betrachtet und OSPF wurde darauf eingestellt </li></ol><br><p>  Details zum OSPF-Setup: Die Hauptaufgabe besteht darin, zwei Links zusammenzufassen und Fehlertoleranz zu haben. <br>  Zwei Netzwerkschnittstellen werden in zwei einfachen flachen Netzwerken konfiguriert - 10.10.10.0/24 und 10.10.20.0/24 </p><br><pre> <code class="plaintext hljs">1: ens1f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9000 qdisc mq state UP group default qlen 1000 inet 10.10.10.2/24 brd 10.10.10.255 scope global ens1f0 2: ens1f1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9000 qdisc mq state UP group default qlen 1000 inet 10.10.20.2/24 brd 10.10.20.255 scope global ens1f1</code> </pre> <br><p>  von denen Autos sich sehen. </p><br><h3 id="disk">  DISK </h3><br><p>  Der n√§chste Schritt bestand darin, die Leistung der Festplatten zu optimieren.  F√ºr SSD habe ich den Scheduler auf <strong>noop ge√§ndert</strong> , f√ºr HDD - <strong>Deadline</strong> .  Wenn grob - dann arbeitet NOOP nach dem Prinzip "Wer war zuerst auf - und Hausschuhe", was auf Englisch wie "FIFO (First In, First Out)" klingt.  Anfragen werden in die Warteschlange gestellt, sobald sie verf√ºgbar sind.  DEADLINE ist schreibgesch√ºtzter, und der Prozess aus der Warteschlange erh√§lt zum Zeitpunkt des Vorgangs fast ausschlie√ülich Zugriff auf die Festplatte.  Dies ist gro√üartig f√ºr unser System - schlie√ülich funktioniert nur ein Prozess mit jeder Festplatte - der OSD-Daemon. <br>  (Wer in den E / A-Planer eintauchen m√∂chte, kann hier dar√ºber lesen: <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">http://www.admin-magazine.com/HPC/Articles/Linux-IO-Schedulers</a> </p><br><p>  Lesen Sie lieber auf Russisch: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://www.opennet.ru/base/sys/linux_shedulers.txt.html</a> ) </p><br><p>  In Empfehlungen zur Optimierung von Linux wird empfohlen, auch nr_request zu erh√∂hen </p><br><blockquote>  <em>nr_requests</em> <em><br></em>  <em>Der Wert von nr_requests bestimmt die Anzahl der E / A-Anforderungen, die gepuffert werden, bevor der E / A-Scheduler Daten an das Blockger√§t sendet / empf√§ngt, wenn Sie eine RAID-Karte / ein Blockger√§t verwenden, die eine gr√∂√üere Warteschlange als das I verarbeiten kann Der E / A-Scheduler ist auf eingestellt. Das Erh√∂hen des Werts von nr_requests kann dazu beitragen, die Serverlast zu verbessern und zu verringern, wenn gro√üe E / A-Mengen auf dem Server auftreten.</em>  <em>Wenn Sie Deadline oder CFQ als Scheduler verwenden, wird empfohlen, den Wert nr_request auf das Zweifache des Werts der Warteschlangentiefe festzulegen.</em> </blockquote><p>  ABER!  Die B√ºrger selbst sind die CEPH-Entwickler, die uns davon √ºberzeugen, dass ihr Priorit√§tssystem besser funktioniert </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/b7e/fd3/bd0/b7efd3bd03fedc88307e200905c8c6a9.gif"></p><br><div class="spoiler">  <b class="spoiler_title">WBThrottle und / oder nr_requests</b> <div class="spoiler_text"><blockquote>  WBThrottle und / oder nr_requests <br>  Der Dateispeicher verwendet gepufferte E / A zum Schreiben.  Dies bringt eine Reihe von Vorteilen mit sich, wenn sich das Dateispeicherprotokoll auf schnelleren Medien befindet.  Clientanforderungen werden benachrichtigt, sobald die Daten in das Protokoll geschrieben und zu einem sp√§teren Zeitpunkt mithilfe der Standard-Linux-Funktionalit√§t auf die Datenfestplatte selbst √ºbertragen werden.  Dies erm√∂glicht es OSD-Spindelplatten, beim Schreiben in kleinen Paketen Schreiblatenzen √§hnlich wie bei SSDs bereitzustellen.  Dieses verz√∂gerte verz√∂gerte Schreiben erm√∂glicht es dem Kernel selbst, die E / A-Anforderungen auf der Festplatte neu zu erstellen, in der Hoffnung, sie entweder zusammenzuf√ºhren oder den vorhandenen Plattenk√∂pfen zu erm√∂glichen, einen optimaleren Pfad auf ihren Platten zu w√§hlen.  Der Endeffekt besteht darin, dass Sie von jeder Festplatte etwas mehr E / A dr√ºcken k√∂nnen, als dies mit direkten oder synchronen E / A m√∂glich w√§re. </blockquote><p>  Ein bestimmtes Problem tritt jedoch auf, wenn das Volumen der eingehenden Datens√§tze in einem bestimmten Ceph-Cluster allen Funktionen der zugrunde liegenden Festplatten voraus ist.  In einem solchen Szenario kann die Gesamtzahl der ausstehenden E / A-Schreibvorg√§nge auf eine Festplatte unkontrolliert zunehmen und zu einer Warteschlange von E / A-Vorg√§ngen f√ºhren, die die gesamte Festplatten- und Ceph-Warteschlange f√ºllen.  Leseanforderungen funktionieren besonders schlecht, da sie zwischen Schreibanforderungen h√§ngen bleiben. Das L√∂schen auf die Hauptfestplatte kann einige Sekunden dauern. </p><br><p>  Um dieses Problem zu beheben, verf√ºgt Ceph √ºber einen im Dateispeicher integrierten R√ºckschreib-Drosselungsmechanismus namens WBThrottle.  Es wurde entwickelt, um die Gesamtzahl der ausstehenden Schreib-E / A-Vorg√§nge zu begrenzen, die in die Warteschlange gestellt werden k√∂nnen, und um den R√ºcksetzprozess fr√ºher zu starten, als dies aufgrund der Einbeziehung durch den Kernel selbst der Fall w√§re.  Leider zeigen Tests, dass die Standardwerte das vorhandene Verhalten m√∂glicherweise immer noch nicht auf ein Niveau reduzieren, das diesen Effekt auf die Latenz von Lesevorg√§ngen verringern kann.  Eine Anpassung kann dieses Verhalten √§ndern und die Gesamtl√§nge der Aufzeichnungswarteschlangen verringern und eine solche Auswirkung nicht stark machen.  Es gibt jedoch einen Kompromiss: Indem Sie die maximale Anzahl von Eintr√§gen reduzieren, die in die Warteschlange gestellt werden d√ºrfen, k√∂nnen Sie die F√§higkeit des Kernels selbst verringern, seine Effizienz bei der Bestellung eingehender Anforderungen zu maximieren.  Es lohnt sich ein wenig dar√ºber nachzudenken, dass Sie mehr f√ºr Ihre spezifische Anwendung und Workloads ben√∂tigen und sich an diese anpassen m√ºssen. </p><br><p>  Um die Tiefe einer solchen ausstehenden Schreibwarteschlange zu steuern, k√∂nnen Sie entweder die maximale Anzahl fehlgeschlagener E / A-Vorg√§nge mithilfe der WBThrottle-Einstellungen verringern oder den Maximalwert f√ºr fehlgeschlagene Vorg√§nge auf Blockebene Ihres Kernels verringern.  Sowohl das als auch ein anderes k√∂nnen das gleiche Verhalten effektiv steuern, und es sind Ihre Pr√§ferenzen, die die Grundlage f√ºr die Implementierung dieser Einstellung bilden. <br>  Es sollte auch beachtet werden, dass die Priorit√§t des Ceph-Betriebssystems f√ºr k√ºrzere Abfragen auf Plattenebene effizienter ist.  Wenn Sie die Gesamtwarteschlange auf eine bestimmte Festplatte reduzieren, wird der Hauptwarteschlangenstandort nach Ceph verschoben, wo Sie mehr Kontrolle dar√ºber haben, welche Priorit√§t die E / A-Operation hat.  Betrachten Sie das folgende Beispiel: </p><br><pre> <code class="plaintext hljs">echo 8 &gt; /sys/block/sda/queue/nr_requests</code> </pre> <br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">http://onreader.mdl.ru/MasteringCeph/content/Ch09.html#030202</a> </p></div></div><br><h3 id="common">  GEMEINSAM </h3><br><p>  Und noch ein paar Kernel-Einstellungen <del>  Ihr Auto ist weich und seidig </del>  Dr√ºcken Sie etwas mehr Leistung aus Eisen </p><br><div class="spoiler">  <b class="spoiler_title">cat /etc/sysctl.d/60-ceph2.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs"> kernel.pid_max = 4194303 #     25,       kernel.threads-max=2097152 # , , . vm.max_map_count=524288 #      . #        #         # malloc,    mmap, mprotect  madvise,     #  . fs.aio-max-nr=50000000 #   input-output #  Linux     - (AIO), #       - # ,    -  . #     , #      -. #  aio-max-nr     #  . vm.min_free_kbytes=1048576 #       . #  1Gb,       , #    OOM Killer   OSD.     #    ,      vm.swappiness=10 #       10% . #   128G ,  10%  12 .     . #    60%   ,   , #       vm.vfs_cache_pressure=1000 #    100.     #     . vm.zone_reclaim_mode=0 #         #  ,     . #     ,     . #       # ,    , zone_reclaim_mode #  ,   , # ,   ,   . vm.dirty_ratio=20 #   ,     ""  #    : #   128  . #   20  SSD,     CEPH  #     3G . #   40  HDD,      1G # 20%  128  25.6 . ,     , #    2.4G .         #    -    DevOps   . vm.dirty_background_ratio=3 #   ,    dirty pages  , #    pdflush/flush/kdmflush     fs.file-max=524288 #      ,,   ,    .</code> </pre> </div></div><br><h2 id="pogruzhenie-v--ceph">  Tauchen Sie bei CEPH </h2><br><p>  Einstellungen, auf die ich n√§her eingehen m√∂chte: </p><br><div class="spoiler">  <b class="spoiler_title">cat /etc/ceph/ceph.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">osd: journal_aio: true #  ,  journal_block_align: true #  i/o journal_dio: true #   journal_max_write_bytes: 1073714824 #     #      journal_max_write_entries: 10000 #      journal_queue_max_bytes: 10485760000 journal_queue_max_ops: 50000 rocksdb_separate_wal_dir: true #    wal #       # NVMe bluestore_block_db_create: true #       bluestore_block_db_size: '5368709120 #5G' bluestore_block_wal_create: true bluestore_block_wal_size: '1073741824 #1G' bluestore_cache_size_hdd: '3221225472 # 3G' #     #     bluestore_cache_size_ssd: '9663676416 # 9G' keyring: /var/lib/ceph/osd/ceph-$id/keyring osd_client_message_size_cap: '1073741824 #1G' osd_disk_thread_ioprio_class: idle osd_disk_thread_ioprio_priority: 7 osd_disk_threads: 2 #        osd_failsafe_full_ratio: 0.95 osd_heartbeat_grace: 5 osd_heartbeat_interval: 3 osd_map_dedup: true osd_max_backfills: 2 #       . osd_max_write_size: 256 osd_mon_heartbeat_interval: 5 osd_op_threads: 16 osd_op_num_threads_per_shard: 1 osd_op_num_threads_per_shard_hdd: 2 osd_op_num_threads_per_shard_ssd: 2 osd_pool_default_min_size: 1 #  .    osd_pool_default_size: 2 #  ,    #     #   osd_recovery_delay_start: 10.000000 osd_recovery_max_active: 2 osd_recovery_max_chunk: 1048576 osd_recovery_max_single_start: 3 osd_recovery_op_priority: 1 osd_recovery_priority: 1 #       osd_recovery_sleep: 2 osd_scrub_chunk_max: 4</code> </pre> </div></div><br><p>  Einige Parameter, die in Version 12.2.12 auf QA getestet wurden, fehlen in ceph Version 12.2.2, z. B. <strong>osd_recovery_threads.</strong>  Daher enthielten die Pl√§ne eine Aktualisierung des Produkts auf 12.2.12.  Die Praxis hat gezeigt, dass ein Cluster der Versionen 12.2.2 und 12.2.12 kompatibel ist, was ein fortlaufendes Update erm√∂glicht. </p><br><h3 id="testovyy-klaster">  Testcluster </h3><br><p>  Zum Testen war es nat√ºrlich notwendig, die gleiche Version wie im Kampf zu haben, aber als ich anfing, mit dem Cluster im Repository zu arbeiten, gab es nur eine neuere.  Da Sie in der Nebenversion nicht sehr gro√ü sind ( <strong>1393</strong> Zeilen in Konfigurationen gegen√ºber <strong>1436</strong> Zeilen in der neuen Version), haben wir beschlossen, eine neue zu testen (sie ist immer noch aktualisiert, warum sollten Sie den alten Papierkorb verwenden)? </p><br><p>  Das einzige, was sie versuchten, die alte Version zu verlassen, war das <strong>ceph-deploy-</strong> Paket <strong>,</strong> da ein Teil der Dienstprogramme (und ein Teil der Mitarbeiter) durch seine Syntax gesch√§rft wurde.  Die neue Version war ganz anders, hatte jedoch keine Auswirkungen auf den Betrieb des Clusters selbst, und die Versionen <strong>1.5.39</strong> haben <strong>sie verlassen</strong> </p><br><p>  Da der Befehl ceph-disk eindeutig angibt, dass er veraltet ist und den Befehl ceph-volume verwendet, meine Lieben, haben wir mit diesem Befehl begonnen, das OSD zu erstellen, ohne Zeit mit dem Veralten zu verschwenden. </p><br><p>  Der Plan war, einen Spiegel aus zwei SSD-Festplatten zu erstellen, auf denen wir die OSD-Protokolle ablegen, die sich wiederum auf den Spindel-SASs befinden.  So k√∂nnen wir uns vor Datenproblemen sch√ºtzen, wenn eine Festplatte mit einem Protokoll herunterf√§llt. </p><br><p>  Erstellen Sie einen Cluster mit Stahldokumentationen </p><br><div class="spoiler">  <b class="spoiler_title">cat /etc/ceph/ceph.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">root@ceph01-qa:~# cat /etc/ceph/ceph.conf #     [client] rbd_cache = true rbd_cache_max_dirty = 50331648 rbd_cache_max_dirty_age = 2 rbd_cache_size = 67108864 rbd_cache_target_dirty = 33554432 rbd_cache_writethrough_until_flush = true rbd_concurrent_management_ops = 10 rbd_default_format = 2 [global] auth_client_required = cephx auth_cluster_required = cephx auth_service_required = cephx cluster network = 10.10.10.0/24 debug_asok = 0/0 debug_auth = 0/0 debug_buffer = 0/0 debug_client = 0/0 debug_context = 0/0 debug_crush = 0/0 debug_filer = 0/0 debug_filestore = 0/0 debug_finisher = 0/0 debug_heartbeatmap = 0/0 debug_journal = 0/0 debug_journaler = 0/0 debug_lockdep = 0/0 debug_mon = 0/0 debug_monc = 0/0 debug_ms = 0/0 debug_objclass = 0/0 debug_objectcatcher = 0/0 debug_objecter = 0/0 debug_optracker = 0/0 debug_osd = 0/0 debug_paxos = 0/0 debug_perfcounter = 0/0 debug_rados = 0/0 debug_rbd = 0/0 debug_rgw = 0/0 debug_throttle = 0/0 debug_timer = 0/0 debug_tp = 0/0 fsid = d0000000d-4000-4b00-b00b-0123qwe123qwf9 mon_host = ceph01-q, ceph02-q, ceph03-q mon_initial_members = ceph01-q, ceph02-q, ceph03-q public network = 8.8.8.8/28 #  ,  )) rgw_dns_name = s3-qa.mycompany.ru #     rgw_host = s3-qa.mycompany.ru #    [mon] mon allow pool delete = true mon_max_pg_per_osd = 300 #     #     #  , ,    , #     OSD.     PG #     -    mon_osd_backfillfull_ratio = 0.9 mon_osd_down_out_interval = 5 mon_osd_full_ratio = 0.95 #   SSD     #   -      #   5%   (   1.2Tb) #   ,     # bluestore_block_db_size     #   mon_osd_nearfull_ratio = 0.9 mon_pg_warn_max_per_osd = 520 [osd] bluestore_block_db_create = true bluestore_block_db_size = 5368709120 #5G bluestore_block_wal_create = true bluestore_block_wal_size = 1073741824 #1G bluestore_cache_size_hdd = 3221225472 # 3G bluestore_cache_size_ssd = 9663676416 # 9G journal_aio = true journal_block_align = true journal_dio = true journal_max_write_bytes = 1073714824 journal_max_write_entries = 10000 journal_queue_max_bytes = 10485760000 journal_queue_max_ops = 50000 keyring = /var/lib/ceph/osd/ceph-$id/keyring osd_client_message_size_cap = 1073741824 #1G osd_disk_thread_ioprio_class = idle osd_disk_thread_ioprio_priority = 7 osd_disk_threads = 2 osd_failsafe_full_ratio = 0.95 osd_heartbeat_grace = 5 osd_heartbeat_interval = 3 osd_map_dedup = true osd_max_backfills = 4 osd_max_write_size = 256 osd_mon_heartbeat_interval = 5 osd_op_num_threads_per_shard = 1 osd_op_num_threads_per_shard_hdd = 2 osd_op_num_threads_per_shard_ssd = 2 osd_op_threads = 16 osd_pool_default_min_size = 1 osd_pool_default_size = 2 osd_recovery_delay_start = 10.0 osd_recovery_max_active = 1 osd_recovery_max_chunk = 1048576 osd_recovery_max_single_start = 3 osd_recovery_op_priority = 1 osd_recovery_priority = 1 osd_recovery_sleep = 2 osd_scrub_chunk_max = 4 osd_scrub_chunk_min = 2 osd_scrub_sleep = 0.1 rocksdb_separate_wal_dir = true</code> </pre> </div></div><br><pre> <code class="plaintext hljs">#   root@ceph01-qa:~#ceph-deploy mon create ceph01-q #        root@ceph01-qa:~#ceph-deploy gatherkeys ceph01-q #   .       - ,       # mon_initial_members = ceph01-q, ceph02-q, ceph03-q #         root@ceph01-qa:~#ceph-deploy mon create-initial #        root@ceph01-qa:~#cat ceph.bootstrap-osd.keyring &gt; /var/lib/ceph/bootstrap-osd/ceph.keyring root@ceph01-qa:~#cat ceph.bootstrap-mgr.keyring &gt; /var/lib/ceph/bootstrap-mgr/ceph.keyring root@ceph01-qa:~#cat ceph.bootstrap-rgw.keyring &gt; /var/lib/ceph/bootstrap-rgw/ceph.keyring #      root@ceph01-qa:~#ceph-deploy admin ceph01-q #  ,   root@ceph01-qa:~#ceph-deploy mgr create ceph01-q</code> </pre> <br><p> ,        ceph-deploy    12.2.12 ‚Äî      OSD  db    - </p><br><pre> <code class="plaintext hljs">root@ceph01-qa:~#ceph-volume lvm create --bluestore --data /dev/sde --block.db /dev/md0 blkid could not detect a PARTUUID for device: /dev/md1</code> </pre> <br><p> , blkid   PARTUUID,    : </p><br><pre> <code class="plaintext hljs">root@ceph01-qa:~#parted /dev/md0 mklabel GPT #   , #  GPT     #        = bluestore_block_db_size: '5368709120 #5G' #    20  OSD,     #    root@ceph01-qa:~#for i in {1..20}; do echo -e "n\n\n\n+5G\nw" | fdisk /dev/md0; done</code> </pre> <br><p>   ,     OSD     (, ,    ) </p><br><p>   OSD  bluestore     WAL,    db </p><br><pre> <code class="plaintext hljs">root@ceph01-qa:~#ceph-volume lvm create --bluestore --data /dev/sde --block.db /dev/md0 stderr: 2019-04-12 10:39:27.211242 7eff461b6e00 -1 bluestore(/var/lib/ceph/osd/ceph-0/) _read_fsid unparsable uuid stderr: 2019-04-12 10:39:27.213185 7eff461b6e00 -1 bdev(0x55824c273680 /var/lib/ceph/osd/ceph-0//block.wal) open open got: (22) Invalid argument stderr: 2019-04-12 10:39:27.213201 7eff461b6e00 -1 bluestore(/var/lib/ceph/osd/ceph-0/) _open_db add block device(/var/lib/ceph/osd/ceph-0//block.wal) returned: (22) Invalid argument stderr: 2019-04-12 10:39:27.999039 7eff461b6e00 -1 bluestore(/var/lib/ceph/osd/ceph-0/) mkfs failed, (22) Invalid argument stderr: 2019-04-12 10:39:27.999057 7eff461b6e00 -1 OSD::mkfs: ObjectStore::mkfs failed with error (22) Invalid argument stderr: 2019-04-12 10:39:27.999141 7eff461b6e00 -1 ** ERROR: error creating empty object store in /var/lib/ceph/osd/ceph-0/: (22) Invalid argumen</code> </pre> <br><p>     -  (   ,  )      WAL      OSD ‚Äî     (    WAL,  , ,   ). </p><br><p> ,         WAL  NVMe,     . </p><br><pre> <code class="plaintext hljs">root@ceph01-qa:~#ceph-volume lvm create --bluestore --data /dev/sdf --block.wal /dev/md0p2 --block.db /dev/md1p2</code> </pre> <br><p>  ,   OSD.      ,        ‚Äî    SSD  ,     SAS. </p><br><p>       20 ,     ,  ‚Äî . <br> , ,   : </p><br><div class="spoiler"> <b class="spoiler_title">ceph osd tree</b> <div class="spoiler_text"><p> root@eph01-q:~# ceph osd tree <br> ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF <br> -1 14.54799 root default <br> -3 9.09200 host ceph01-q <br> 0 ssd 1.00000 osd.0 up 1.00000 1.00000 <br> 1 ssd 1.00000 osd.1 up 1.00000 1.00000 <br> 2 ssd 1.00000 osd.2 up 1.00000 1.00000 <br> 3 ssd 1.00000 osd.3 up 1.00000 1.00000 <br> 4 hdd 1.00000 osd.4 up 1.00000 1.00000 <br> 5 hdd 0.27299 osd.5 up 1.00000 1.00000 <br> 6 hdd 0.27299 osd.6 up 1.00000 1.00000 <br> 7 hdd 0.27299 osd.7 up 1.00000 1.00000 <br> 8 hdd 0.27299 osd.8 up 1.00000 1.00000 <br> 9 hdd 0.27299 osd.9 up 1.00000 1.00000 <br> 10 hdd 0.27299 osd.10 up 1.00000 1.00000 <br> 11 hdd 0.27299 osd.11 up 1.00000 1.00000 <br> 12 hdd 0.27299 osd.12 up 1.00000 1.00000 <br> 13 hdd 0.27299 osd.13 up 1.00000 1.00000 <br> 14 hdd 0.27299 osd.14 up 1.00000 1.00000 <br> 15 hdd 0.27299 osd.15 up 1.00000 1.00000 <br> 16 hdd 0.27299 osd.16 up 1.00000 1.00000 <br> 17 hdd 0.27299 osd.17 up 1.00000 1.00000 <br> 18 hdd 0.27299 osd.18 up 1.00000 1.00000 <br> 19 hdd 0.27299 osd.19 up 1.00000 1.00000 <br> -5 5.45599 host ceph02-q <br> 20 ssd 0.27299 osd.20 up 1.00000 1.00000 <br> 21 ssd 0.27299 osd.21 up 1.00000 1.00000 <br> 22 ssd 0.27299 osd.22 up 1.00000 1.00000 <br> 23 ssd 0.27299 osd.23 up 1.00000 1.00000 <br> 24 hdd 0.27299 osd.24 up 1.00000 1.00000 <br> 25 hdd 0.27299 osd.25 up 1.00000 1.00000 <br> 26 hdd 0.27299 osd.26 up 1.00000 1.00000 <br> 27 hdd 0.27299 osd.27 up 1.00000 1.00000 <br> 28 hdd 0.27299 osd.28 up 1.00000 1.00000 <br> 29 hdd 0.27299 osd.29 up 1.00000 1.00000 <br> 30 hdd 0.27299 osd.30 up 1.00000 1.00000 <br> 31 hdd 0.27299 osd.31 up 1.00000 1.00000 <br> 32 hdd 0.27299 osd.32 up 1.00000 1.00000 <br> 33 hdd 0.27299 osd.33 up 1.00000 1.00000 <br> 34 hdd 0.27299 osd.34 up 1.00000 1.00000 <br> 35 hdd 0.27299 osd.35 up 1.00000 1.00000 <br> 36 hdd 0.27299 osd.36 up 1.00000 1.00000 <br> 37 hdd 0.27299 osd.37 up 1.00000 1.00000 <br> 38 hdd 0.27299 osd.38 up 1.00000 1.00000 <br> 39 hdd 0.27299 osd.39 up 1.00000 1.00000 <br> -7 6.08690 host ceph03-q <br> 40 ssd 0.27299 osd.40 up 1.00000 1.00000 <br> 41 ssd 0.27299 osd.41 up 1.00000 1.00000 <br> 42 ssd 0.27299 osd.42 up 1.00000 1.00000 <br> 43 ssd 0.27299 osd.43 up 1.00000 1.00000 <br> 44 hdd 0.27299 osd.44 up 1.00000 1.00000 <br> 45 hdd 0.27299 osd.45 up 1.00000 1.00000 <br> 46 hdd 0.27299 osd.46 up 1.00000 1.00000 <br> 47 hdd 0.27299 osd.47 up 1.00000 1.00000 <br> 48 hdd 0.27299 osd.48 up 1.00000 1.00000 <br> 49 hdd 0.27299 osd.49 up 1.00000 1.00000 <br> 50 hdd 0.27299 osd.50 up 1.00000 1.00000 <br> 51 hdd 0.27299 osd.51 up 1.00000 1.00000 <br> 52 hdd 0.27299 osd.52 up 1.00000 1.00000 <br> 53 hdd 0.27299 osd.53 up 1.00000 1.00000 <br> 54 hdd 0.27299 osd.54 up 1.00000 1.00000 <br> 55 hdd 0.27299 osd.55 up 1.00000 1.00000 <br> 56 hdd 0.27299 osd.56 up 1.00000 1.00000 <br> 57 hdd 0.27299 osd.57 up 1.00000 1.00000 <br> 58 hdd 0.27299 osd.58 up 1.00000 1.00000 <br> 59 hdd 0.89999 osd.59 up 1.00000 1.00000 </p></div></div><br><p>          : </p><br><pre> <code class="plaintext hljs">root@ceph01-q:~#ceph osd crush add-bucket rack01 root #  root root@ceph01-q:~#ceph osd crush add-bucket ceph01-q host #   root@ceph01-q:~#ceph osd crush move ceph01-q root=rack01 #     root@ceph01-q:~#osd crush add 28 1.0 host=ceph02-q #     #       root@ceph01-q:~# ceph osd crush remove osd.4 root@ceph01-q:~# ceph osd crush remove rack01</code> </pre> <br><p> ,      <strong></strong> ,            ‚Äî  <strong>ceph osd crush move ceph01-host root=rack01</strong> ,      .    CTRL+C     . </p><br><p>    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://tracker.ceph.com/issues/23386</a> </p><br><p>    crushmap     <strong>rule replicated_ruleset</strong> </p><br><pre> <code class="plaintext hljs">root@ceph01-prod:~#ceph osd getcrushmap -o crushmap.row #     root@ceph01-prod:~#crushtool -d crushmap.row -o crushmap.txt #   root@ceph01-prod:~#vim crushmap.txt #,  rule replicated_ruleset root@ceph01-prod:~#crushtool -c crushmap.txt -o new_crushmap.row #  root@ceph01-prod:~#ceph osd setcrushmap -i new_crushmap.row #  </code> </pre> <br><p> <strong>:</strong>      placement group  OSD.    ,   . </p><br><p>  ,        ‚Äî  ,     OSD ,        ,    root default. <br>  ,   ,      root  ssd     ,          default root.   OSD     . <br> <em>     ,     .     </em> </p><br><h3 id="kak-my-delali-razlichnye-gruppy-po-tipam-diskov">        . </h3><br><p>     root- ‚Äî  ssd   hdd </p><br><pre> <code class="plaintext hljs">root@ceph01-q:~#ceph osd crush add-bucket ssd-root root root@ceph01-q:~#ceph osd crush add-bucket hdd-root root</code> </pre> <br><p>        ‚Äî          </p><br><pre> <code class="plaintext hljs"># : root@ceph01-q:~#ceph osd crush add-bucket ssd-rack01 rack root@ceph01-q:~#ceph osd crush add-bucket ssd-rack02 rack root@ceph01-q:~#ceph osd crush add-bucket ssd-rack03 rack root@ceph01-q:~#ceph osd crush add-bucket hdd-rack01 rack root@ceph01-q:~#ceph osd crush add-bucket hdd-rack01 rack root@ceph01-q:~#ceph osd crush add-bucket hdd-rack01 rack #  root@ceph01-q:~#ceph osd crush add-bucket ssd-ceph01-q host root@ceph01-q:~#ceph osd crush add-bucket ssd-ceph02-q host root@ceph01-q:~#ceph osd crush add-bucket ssd-ceph03-q host root@ceph01-q:~#ceph osd crush add-bucket hdd-ceph01-q host root@ceph01-q:~#ceph osd crush add-bucket hdd-ceph02-q host root@ceph01-q:~#ceph osd crush add-bucket hdd-ceph02-q host</code> </pre> <br><p>          </p><br><pre> <code class="plaintext hljs">root@ceph01-q:~#   0  3  SSD,   ceph01-q,     root@ceph01-q:~# ssd-ceph01-q root@ceph01-q:~#ceph osd crush add 0 1 host=ssd-ceph01-q root@ceph01-q:~#ceph osd crush add 1 1 host=ssd-ceph01-q root@ceph01-q:~#ceph osd crush add 2 1 host=ssd-ceph01-q root@ceph01-q:~#ceph osd crush add 3 1 host=ssd-ceph01-q root-ceph01-q:~#    </code> </pre> <br><p>     ssd-root  hdd-root   root-default ,     </p><br><pre> <code class="plaintext hljs">root-ceph01-q:~#ceph osd crush remove default</code> </pre> <br><p>     ,        ‚Äî      root          ‚Äî        ,     (    root,    ) </p><br><p>        : <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">http://docs.ceph.com/docs/jewel/rados/operations/crush-map/#crushmaprules</a> </p><br><pre> <code class="plaintext hljs">root-ceph01-q:~#ceph osd crush rule create-simple rule-ssd ssd-root host firstn root-ceph01-q:~#ceph osd crush rule create-simple rule-hdd hdd-root host firstn root-ceph01-q:~#    ,     root-ceph01-q:~#   -        , root-ceph01-q:~#       root-ceph01-q:~#  ,   ,    root-ceph01-q:~#        : root-ceph01-q:~# ##ceph osd crush rule create-simple rule-ssd ssd-root rack firstn</code> </pre> <br><p>    ,            ‚Äî PROXMOX: </p><br><pre> <code class="plaintext hljs"> root-ceph01-q:~# #ceph osd pool create {NAME} {pg_num} {pgp_num} root-ceph01-q:~# ceph osd pool create ssd_pool 1024 1024 root-ceph01-q:~# ceph osd pool create hdd_pool 1024 1024</code> </pre> <br><p>         </p><br><pre> <code class="plaintext hljs"> root-ceph01-q:~#ceph osd crush rule ls #    root-ceph01-q:~#ceph osd crush rule dump rule-ssd | grep rule_id # ID  root-ceph01-q:~#ceph osd pool set ssd_pool crush_rule 2</code> </pre><br><p>               ‚Äî     ,    (    )   ,    . </p><br><p>      300    ,        ‚Äî        10 Tb    10 PG ‚Äî      (pg)   ‚Äî           ). </p><br><p>        PG ‚Äî         ‚Äî     . </p><br><p>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a> ,    CEPH. </p><br><p>  : </p><br><p> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">http://www.admin-magazine.com/HPC/Articles/Linux-IO-Schedulers</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">http://onreader.mdl.ru/MasteringCeph/content/Ch09.html#030202</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://tracker.ceph.com/issues/23386</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://ceph.com/pgcalc/</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de456446/">https://habr.com/ru/post/de456446/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de456432/index.html">Sicherheitswoche 25: Evernote-Sicherheitsl√ºcke und Hunderte von gehackten Online-Shops</a></li>
<li><a href="../de456434/index.html">Berufe der Zukunft: ‚ÄûMit wem werden Sie auf dem Mars arbeiten?‚Äú</a></li>
<li><a href="../de456436/index.html">Kurze JS-Aufgabe f√ºr Montag</a></li>
<li><a href="../de456440/index.html">Die Abenteuer der schwer fassbaren Malvari, Teil I.</a></li>
<li><a href="../de456442/index.html">Zulassung zum Undergraduate-Programm der St. Petersburg State University mit Unterst√ºtzung von Yandex und JetBrains</a></li>
<li><a href="../de456448/index.html">Regeln f√ºr die Auswahl eines JS-Frameworks</a></li>
<li><a href="../de456450/index.html">DO-RA.Avia zur √úberwachung der kosmischen Strahlung in der Luftfahrt</a></li>
<li><a href="../de456452/index.html">C ++ - Codebeispiele vor und nach Bereichen</a></li>
<li><a href="../de456462/index.html">Zusammenstellen einer Bibliothek von Winkelkomponenten als Webkomponenten</a></li>
<li><a href="../de456466/index.html">PHP-Generika heute (na ja, fast)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>