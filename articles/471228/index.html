<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèΩ‚Äçüåæ üòÉ üëµüèæ Grokay PyTorch üßõüèº üëáüèº üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola Habr! 

 Tenemos en reserva un libro largamente esperado sobre la biblioteca PyTorch . 



 Como aprender√° todo el material b√°sico necesario sobr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Grokay PyTorch</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/471228/"> Hola Habr! <br><br>  Tenemos en reserva un libro largamente esperado sobre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">la biblioteca PyTorch</a> . <br><br><img src="https://habrastorage.org/webt/bt/am/vl/btamvlvzqw01qwk-_2mq08t-sh4.jpeg"><br><br>  Como aprender√° todo el material b√°sico necesario sobre PyTorch de este libro, le recordamos los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">beneficios de un</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">proceso</a> llamado "grokking" o "comprensi√≥n profunda" del tema que desea aprender.  En la publicaci√≥n de hoy, te diremos c√≥mo Kai Arulkumaran golpe√≥ a PyTorch (sin foto).  Bienvenido a cat. <br><a name="habracut"></a><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">PyTorch</a> es un marco de aprendizaje profundo flexible que distingue autom√°ticamente entre objetos que usan redes neuronales din√°micas (es decir, redes que usan control de flujo din√°mico, como <code>if</code> y bucles while).  PyTorch admite aceleraci√≥n de GPU, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">entrenamiento distribuido</a> , varios tipos de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">optimizaci√≥n</a> y muchas otras caracter√≠sticas interesantes.  Aqu√≠ expuse algunas ideas sobre c√≥mo, en mi opini√≥n, deber√≠a usar PyTorch;  Todos los aspectos de la biblioteca y las pr√°cticas recomendadas no est√°n cubiertos aqu√≠, pero espero que este texto le sea √∫til. <br><br>  Las redes neuronales son una subclase de gr√°ficos computacionales.  Los gr√°ficos de computaci√≥n reciben datos como entrada, luego estos datos se enrutan (y se pueden convertir) en los nodos donde se procesan.  En el aprendizaje profundo, las neuronas (nodos) generalmente transforman los datos al aplicarles par√°metros y funciones diferenciables, de modo que los par√°metros pueden optimizarse para minimizar las p√©rdidas mediante el m√©todo de descenso de gradiente.  En un sentido m√°s amplio, observo que las funciones pueden ser estoc√°sticas y din√°micas de gr√°ficos.  Por lo tanto, si bien las redes neuronales se ajustan bien al paradigma de programaci√≥n de flujo de datos, la API PyTorch se enfoca en el paradigma de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">programaci√≥n imperativo</a> , y esta forma de interpretar los programas creados es mucho m√°s familiar.  Es por eso que el c√≥digo PyTorch es m√°s f√°cil de leer, es m√°s f√°cil juzgar el dise√±o de programas complejos, lo que, sin embargo, no requiere un compromiso serio en el rendimiento: de hecho, PyTorch es lo suficientemente r√°pido y proporciona muchas optimizaciones de las que usted, como usuario final, no puede preocuparse en absoluto. (sin embargo, si est√° realmente interesado en ellos, puede profundizar un poco m√°s y conocerlos). <br><br>  El resto de este art√≠culo es un an√°lisis del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ejemplo oficial en el conjunto de datos MNIST</a> .  Aqu√≠ <i>jugamos</i> PyTorch, por lo tanto, recomiendo entender el art√≠culo solo despu√©s de conocer los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">manuales oficiales para principiantes</a> .  Por conveniencia, el c√≥digo se presenta en forma de peque√±os fragmentos equipados con comentarios, es decir, no se distribuye en funciones / archivos separados que est√° acostumbrado a ver en c√≥digo modular puro. <br><br><h4>  Importaciones </h4><br><pre> <code class="plaintext hljs">import argparse import os import torch from torch import nn, optim from torch.nn import functional as F from torch.utils.data import DataLoader from torchvision import datasets, transforms</code> </pre> <br>  Todas estas son importaciones bastante est√°ndar, con la excepci√≥n de los m√≥dulos de <code>torchvision</code> , que se utilizan especialmente activamente para resolver tareas relacionadas con la visi√≥n por computadora. <br><br><h4>  Personalizaci√≥n </h4><br><pre> <code class="python hljs">parser = argparse.ArgumentParser(description=<span class="hljs-string"><span class="hljs-string">'PyTorch MNIST Example'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--batch-size'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">64</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'N'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'input batch size for training (default: 64)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--epochs'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">10</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'N'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'number of epochs to train (default: 10)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--lr'</span></span>, type=float, default=<span class="hljs-number"><span class="hljs-number">0.01</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'LR'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'learning rate (default: 0.01)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--momentum'</span></span>, type=float, default=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'M'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'SGD momentum (default: 0.5)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--no-cuda'</span></span>, action=<span class="hljs-string"><span class="hljs-string">'store_true'</span></span>, default=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, help=<span class="hljs-string"><span class="hljs-string">'disables CUDA training'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--seed'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">1</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'S'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'random seed (default: 1)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--save-interval'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">10</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'N'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'how many batches to wait before checkpointing'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--resume'</span></span>, action=<span class="hljs-string"><span class="hljs-string">'store_true'</span></span>, default=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, help=<span class="hljs-string"><span class="hljs-string">'resume training from checkpoint'</span></span>) args = parser.parse_args() use_cuda = torch.cuda.is_available() <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> args.no_cuda device = torch.device(<span class="hljs-string"><span class="hljs-string">'cuda'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">'cpu'</span></span>) torch.manual_seed(args.seed) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: torch.cuda.manual_seed(args.seed)</code> </pre> <br>  <code>argparse</code> es la forma est√°ndar de manejar argumentos de l√≠nea de comando en Python. <br><br>  Si necesita escribir c√≥digo dise√±ado para trabajar en diferentes dispositivos (usando aceleraci√≥n de GPU, cuando est√° disponible, pero si no se <code>torch.device</code> a los c√°lculos en la CPU), seleccione y guarde el <code>torch.device</code> apropiado, con el que puede determinar d√≥nde debe Los tensores se almacenan.  Para obtener m√°s informaci√≥n sobre c√≥mo crear dicho c√≥digo, consulte la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentaci√≥n oficial</a> .  El enfoque de PyTorch es llevar la selecci√≥n de dispositivos al control del usuario, lo que puede parecer indeseable en ejemplos simples.  Sin embargo, este enfoque simplifica enormemente el trabajo cuando tiene que lidiar con tensores, lo que a) es conveniente para la depuraci√≥n b) le permite usar dispositivos de manera efectiva de forma manual. <br><br>  Para la reproducibilidad de los experimentos, debe establecer valores iniciales aleatorios para todos los componentes que usan generaci√≥n de n√∫meros aleatorios (incluidos <code>random</code> o <code>numpy</code> , si los usa <code>numpy</code> ).  Tenga en cuenta: cuDNN utiliza algoritmos no deterministas y, opcionalmente, se desactiva mediante <code>torch.backends.cudnn.enabled = False</code> . <br><br><h4>  Datos </h4><br><pre> <code class="python hljs">data_path = os.path.join(os.path.expanduser(<span class="hljs-string"><span class="hljs-string">'~'</span></span>), <span class="hljs-string"><span class="hljs-string">'.torch'</span></span>, <span class="hljs-string"><span class="hljs-string">'datasets'</span></span>, <span class="hljs-string"><span class="hljs-string">'mnist'</span></span>) train_data = datasets.MNIST(data_path, train=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, download=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((<span class="hljs-number"><span class="hljs-number">0.1307</span></span>,), (<span class="hljs-number"><span class="hljs-number">0.3081</span></span>,))])) test_data = datasets.MNIST(data_path, train=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((<span class="hljs-number"><span class="hljs-number">0.1307</span></span>,), (<span class="hljs-number"><span class="hljs-number">0.3081</span></span>,))])) train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, num_workers=<span class="hljs-number"><span class="hljs-number">4</span></span>, pin_memory=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) test_loader = DataLoader(test_data, batch_size=args.batch_size, num_workers=<span class="hljs-number"><span class="hljs-number">4</span></span>, pin_memory=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><br>  Dado que los modelos de <code>torchvision</code> almacenan en <code>~/.torch/models/</code> , prefiero almacenar los conjuntos de <code>torchvision</code> torchvision en <code>~/.torch/datasets</code> .  Este es mi acuerdo de derechos de autor, pero es muy conveniente usarlo en proyectos desarrollados sobre la base de MNIST, CIFAR-10, etc.  En general, los conjuntos de datos deben almacenarse por separado del c√≥digo si tiene la intenci√≥n de reutilizar m√∫ltiples conjuntos de datos. <br><br>  <code>torchvision.transforms</code> contiene muchas opciones de conversi√≥n convenientes para im√°genes individuales, como el recorte y la normalizaci√≥n. <br><br>  Hay muchas opciones en el <code>batch_size</code> , pero adem√°s de <code>batch_size</code> y <code>shuffle</code> , tambi√©n debe tener en cuenta <code>num_workers</code> y <code>pin_memory</code> , ayudan a aumentar la eficiencia.  <code>num_workers &gt; 0</code> usa subprocesos para la carga de datos as√≠ncrona, y no bloquea el proceso principal para esto.  Un caso de uso t√≠pico es cargar datos (por ejemplo, im√°genes) desde un disco y, posiblemente, convertirlos;  Todo esto se puede hacer en paralelo, junto con el procesamiento de datos de red.  Es posible que sea necesario ajustar el grado de procesamiento para a) minimizar la cantidad de trabajadores y, en consecuencia, la cantidad de CPU y RAM utilizada (cada trabajador carga un lote separado, en lugar de muestras individuales incluidas en el lote) b) minimizar el tiempo que los datos esperan en la red.  <code>pin_memory</code> usa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">memoria fija</a> (en lugar de paginada) para acelerar las operaciones de transferencia de datos desde la RAM a la GPU (y no hace nada con el c√≥digo espec√≠fico de la CPU). <br><br><h4>  Modelo </h4><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Net</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> super(Net, self).__init__() self.conv1 = nn.Conv2d(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, kernel_size=<span class="hljs-number"><span class="hljs-number">5</span></span>) self.conv2 = nn.Conv2d(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>, kernel_size=<span class="hljs-number"><span class="hljs-number">5</span></span>) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(<span class="hljs-number"><span class="hljs-number">320</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>) self.fc2 = nn.Linear(<span class="hljs-number"><span class="hljs-number">50</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x)</span></span></span><span class="hljs-function">:</span></span> x = F.relu(F.max_pool2d(self.conv1(x), <span class="hljs-number"><span class="hljs-number">2</span></span>)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), <span class="hljs-number"><span class="hljs-number">2</span></span>)) x = x.view(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">320</span></span>) x = F.relu(self.fc1(x)) x = self.fc2(x) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> F.log_softmax(x, dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) model = Net().to(device) optimiser = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> args.resume: model.load_state_dict(torch.load(<span class="hljs-string"><span class="hljs-string">'model.pth'</span></span>)) optimiser.load_state_dict(torch.load(<span class="hljs-string"><span class="hljs-string">'optimiser.pth'</span></span>))</code> </pre> <br>  La inicializaci√≥n de la red generalmente se extiende a las variables miembro, capas que contienen par√°metros de aprendizaje y, posiblemente, par√°metros de aprendizaje individuales y memorias intermedias no capacitadas.  Luego, con un pase directo, se usan en combinaci√≥n con funciones de <code>F</code> que son puramente funcionales y no contienen par√°metros.  A algunas personas les gusta trabajar con redes puramente funcionales (por ejemplo, mantener par√°metros y usar <code>F.conv2d</code> lugar de <code>nn.Conv2d</code> ) o redes que consisten completamente en capas (por ejemplo, <code>nn.ReLU</code> lugar de <code>F.relu</code> ). <br><br>  <code>.to(device)</code> es una forma conveniente de enviar par√°metros (y memorias intermedias) del <code>device</code> a la GPU si el <code>device</code> configurado en GPU, porque de lo contrario (si el dispositivo est√° configurado en CPU) no se har√° nada.  Es importante transferir los par√°metros del dispositivo al dispositivo apropiado antes de pasarlos al optimizador;  de lo contrario, el optimizador no podr√° rastrear los par√°metros correctamente. <br><br>  Tanto las redes neuronales ( <code>nn.Module</code> ) como los optimizadores ( <code>optim.Optimizer</code> ) pueden guardar y cargar su estado interno, y se recomienda hacer esto con <code>.load_state_dict(state_dict)</code> : es necesario volver a cargar el estado de ambos para reanudar la capacitaci√≥n basada en diccionarios previamente guardados estados.  Guardar todo el objeto puede estar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">lleno de errores</a> .  Si guard√≥ los tensores en la GPU y desea cargarlos en la CPU u otra GPU, entonces la forma m√°s f√°cil es cargarlos directamente en la CPU utilizando la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">opci√≥n</a> <code>map_location</code> , por ejemplo, <code>torch.load('model.pth'</code> , <code>map_location='cpu'</code> ). <br><br>  Aqu√≠ hay algunos otros puntos que no se muestran aqu√≠, pero que vale la pena mencionar, que puede usar el flujo de control con un pase directo (por ejemplo, la ejecuci√≥n de la <code>if</code> puede depender de la variable miembro o de los datos en s√≠. Adem√°s, es perfectamente aceptable emitir en medio del proceso ( <code>print</code> ) tensores, lo que simplifica enormemente la depuraci√≥n. Finalmente, con un pase directo, se pueden usar muchos argumentos. Ilustrar√© este punto con una lista breve que no est√° vinculada a ninguna idea en particular: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, hx, drop=False)</span></span></span><span class="hljs-function">:</span></span> hx2 = self.rnn(x, hx) print(hx.mean().item(), hx.var().item()) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> hx.max.item() &gt; <span class="hljs-number"><span class="hljs-number">10</span></span> <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> self.can_drop <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> drop: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> hx <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> hx2</code> </pre> <br><h4>  Entrenamiento </h4><br><pre> <code class="python hljs">model.train() train_losses = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, (data, target) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(train_loader): data = data.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) target = target.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) optimiser.zero_grad() output = model(data) loss = F.nll_loss(output, target) loss.backward() train_losses.append(loss.item()) optimiser.step() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i % <span class="hljs-number"><span class="hljs-number">10</span></span> == <span class="hljs-number"><span class="hljs-number">0</span></span>: print(i, loss.item()) torch.save(model.state_dict(), <span class="hljs-string"><span class="hljs-string">'model.pth'</span></span>) torch.save(optimiser.state_dict(), <span class="hljs-string"><span class="hljs-string">'optimiser.pth'</span></span>) torch.save(train_losses, <span class="hljs-string"><span class="hljs-string">'train_losses.pth'</span></span>)</code> </pre> <br>  Los m√≥dulos de red se ponen en modo de entrenamiento de forma predeterminada, lo que en cierta medida afecta el funcionamiento de los m√≥dulos, sobre todo, la reducci√≥n y la normalizaci√≥n de lotes.  De una forma u otra, es mejor configurar estas cosas manualmente usando <code>.train()</code> , que filtra la bandera de "entrenamiento" para todos los m√≥dulos secundarios. <br><br>  Aqu√≠, el m√©todo <code>.to()</code> no solo acepta el dispositivo, sino que tambi√©n establece <code>non_blocking=True</code> , asegurando as√≠ la copia as√≠ncrona de datos a la GPU desde la memoria comprometida, permitiendo que la CPU permanezca operativa durante la transferencia de datos;  de lo contrario, <code>non_blocking=True</code> simplemente no es una opci√≥n. <br><br>  Antes de crear un nuevo conjunto de gradientes con <code>loss.backward()</code> y <code>optimiser.step()</code> con <code>optimiser.step()</code> , debe restablecer manualmente los gradientes de los par√°metros para optimizar con <code>optimiser.zero_grad()</code> .  Por defecto, PyTorch acumula gradientes, lo cual es muy conveniente si no tiene suficientes recursos para calcular todos los gradientes que necesita en una sola pasada. <br><br>  PyTorch utiliza un sistema de "cinta" de gradientes autom√°ticos: recopila informaci√≥n sobre qu√© operaciones y en qu√© orden se realizaron en los tensores, y luego los reproduce en la direcci√≥n opuesta para realizar la diferenciaci√≥n en orden inverso (diferenciaci√≥n en modo inverso).  Es por eso que es tan s√∫per flexible y permite gr√°ficos computacionales arbitrarios.  Si ninguno de estos tensores requiere gradientes (debe establecer <code>requires_grad=True</code> , creando un tensor para este prop√≥sito), ¬°entonces no se guarda ning√∫n gr√°fico!  Sin embargo, las redes generalmente tienen par√°metros que requieren gradientes, por lo que cualquier c√°lculo realizado sobre la base de la salida de la red se almacenar√° en el gr√°fico.  Entonces, si desea guardar los datos resultantes de este paso, deber√° deshabilitar manualmente los gradientes o (un enfoque m√°s com√∫n), guardar esta informaci√≥n como un n√∫mero de Python (usando <code>.item()</code> en el escalar PyTorch) o una matriz <code>numpy</code> .  Lea m√°s sobre <code>autograd</code> en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentaci√≥n oficial</a> . <br><br>  Una forma de acortar el gr√°fico computacional es usar <code>.detach()</code> cuando se pasa el estado oculto al aprender RNN con una versi√≥n truncada de backpropagation-through-time.  Tambi√©n es conveniente cuando se diferencian las p√©rdidas, cuando uno de los componentes es la salida de otra red, pero esta otra red no debe optimizarse con respecto a las p√©rdidas.  Como ejemplo, ense√±ar√© la parte discriminatoria en el material de salida que se genera cuando se trabaja con la GAN, o la capacitaci√≥n de pol√≠ticas en el algoritmo de actor cr√≠tico utilizando la funci√≥n objetivo como la funci√≥n base (por ejemplo, A2C).  Otra t√©cnica que impide el c√°lculo de gradientes es efectiva en la ense√±anza de GAN (capacitaci√≥n de la parte generadora en material discriminante) y lo t√≠pico en el ajuste fino es recorrer los par√°metros de red para los cuales se establece <code>param.requires_grad = False</code> . <br><br>  Es importante no solo registrar los resultados en el archivo de consola / registro, sino tambi√©n establecer puntos de control en los par√°metros del modelo (y el estado del optimizador) por si acaso.  Tambi√©n puede usar <code>torch.save()</code> para guardar objetos regulares de Python, o usar otra soluci√≥n est√°ndar: el <code>pickle</code> incorporado. <br><br><h4>  Prueba </h4><br><pre> <code class="python hljs">model.eval() test_loss, correct = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> torch.no_grad(): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> data, target <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> test_loader: data = data.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) target = target.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) output = model(data) test_loss += F.nll_loss(output, target, reduction=<span class="hljs-string"><span class="hljs-string">'sum'</span></span>).item() pred = output.argmax(<span class="hljs-number"><span class="hljs-number">1</span></span>, keepdim=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_data) acc = correct / len(test_data) print(acc, test_loss)</code> </pre> <br>  En respuesta a <code>.train()</code> redes deben ponerse expl√≠citamente en modo de evaluaci√≥n usando <code>.eval()</code> . <br><br>  Como se mencion√≥ anteriormente, cuando se usa una red, generalmente se compila un gr√°fico computacional.  Para evitar esto, use el <code>no_grad</code> contexto <code>no_grad</code> con <code>with torch.no_grad()</code> . <br><br><h4>  Algo mas </h4><br>  Esta es una secci√≥n adicional, en la que hice algunas digresiones m√°s √∫tiles. <br>  Aqu√≠ est√° la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentaci√≥n oficial que</a> explica trabajar con memoria. <br><br>  CUDA errores?  Es dif√≠cil solucionarlos, y generalmente est√°n conectados con inconsistencias l√≥gicas, seg√∫n las cuales se muestran mensajes de error m√°s sensibles en la CPU que en la GPU.  Lo mejor de todo, si planea trabajar con la GPU, puede cambiar r√°pidamente entre la CPU y la GPU.  Un consejo de desarrollo m√°s general es organizar el c√≥digo para que pueda verificarse r√°pidamente antes de comenzar una tarea completa.  Por ejemplo, prepare un conjunto de datos peque√±o o sint√©tico, ejecute una era train + test, etc.  Si el problema es un error de CUDA, o no puede cambiar a la CPU, configure CUDA_LAUNCH_BLOCKING = 1.  Esto har√° que el kernel CUDA se inicie sincr√≥nicamente y recibir√° mensajes de error m√°s precisos. <br><br>  Una nota sobre <code>torch.multiprocessing</code> o simplemente ejecutando m√∫ltiples scripts PyTorch al mismo tiempo.  Debido a que PyTorch utiliza bibliotecas BLAS de subprocesos m√∫ltiples para acelerar los c√°lculos de √°lgebra lineal en la CPU, generalmente est√°n involucrados varios n√∫cleos.  Si desea hacer varias cosas al mismo tiempo, utilizando un procesamiento multiproceso o varios scripts, puede ser aconsejable reducir manualmente su n√∫mero configurando la variable de entorno <code>OMP_NUM_THREADS</code> a 1 u otro valor bajo.  Por lo tanto, la probabilidad de deslizar el procesador se reduce.  La documentaci√≥n oficial tiene otros comentarios sobre el procesamiento multiproceso. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/471228/">https://habr.com/ru/post/471228/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../471212/index.html">10 consejos y trucos para ayudarte a convertirte en el mejor desarrollador de VueJS</a></li>
<li><a href="../471216/index.html">La larga historia de la gu√≠a: c√≥mo escrib√≠ un servicio para rutas de senderismo inteligentes durante 5 a√±os</a></li>
<li><a href="../471220/index.html">Cockpit: simplifique las tareas administrativas t√≠picas en Linux a trav√©s de una interfaz web conveniente</a></li>
<li><a href="../471222/index.html">Comprender las pol√≠ticas de privacidad de aplicaciones y servicios ayudar√° a las redes neuronales</a></li>
<li><a href="../471226/index.html">Linux tiene muchas caras: c√≥mo trabajar en cualquier distribuci√≥n</a></li>
<li><a href="../471232/index.html">Mi experiencia conectando LPS331AP a Omega Onion2</a></li>
<li><a href="../471236/index.html">Dos√≠metro para Seryozha. Parte III Radi√≥metro nacional</a></li>
<li><a href="../471240/index.html">"Bitchy Betty" e interfaces de audio modernas: ¬øpor qu√© hablan con voz femenina?</a></li>
<li><a href="../471242/index.html">Introducci√≥n a Bash Shell</a></li>
<li><a href="../471244/index.html">C√≥digo de Rosetta: mida la longitud del c√≥digo en una gran cantidad de lenguajes de programaci√≥n, estudie la proximidad de los idiomas entre s√≠</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>