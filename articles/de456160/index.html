<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõåüèª ‚è≤Ô∏è üåï Verst√§rkungslernen oder Evolutionsstrategien? - Beide üëÉüèæ üåø ü§úüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Habr! 

 Wir beschlie√üen selten, hier vor zwei Jahren √úbersetzungen von Texten ohne Code und mit klarem akademischen Fokus zu ver√∂ffentlichen - ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Verst√§rkungslernen oder Evolutionsstrategien? - Beide</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/456160/">  Hallo Habr! <br><br>  Wir beschlie√üen selten, hier vor zwei Jahren √úbersetzungen von Texten ohne Code und mit klarem akademischen Fokus zu ver√∂ffentlichen - aber heute werden wir eine Ausnahme machen.  Wir hoffen, dass das Dilemma im Titel des Artikels f√ºr viele unserer Leser von Belang ist und dass Sie das Originalwerk bereits gelesen haben oder das grundlegende Werk zu Evolutionsstrategien lesen werden, mit denen dieser Beitrag polemisiert wird.  Willkommen bei Katze! <br><br><img src="https://habrastorage.org/webt/-n/u-/i6/-nu-i6enynr12ma1d7utan_7ml8.jpeg"><br><a name="habracut"></a><br>  Im M√§rz 2017 sorgte OpenAI mit der Ver√∂ffentlichung des Artikels ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Evolutionsstrategien als skalierbare Alternative zum verst√§rkten Lernen</a> ‚Äú f√ºr Aufsehen in der Deep-Learning-Community.  In dieser Arbeit wurden beeindruckende Ergebnisse zugunsten der Tatsache beschrieben, dass das Licht im Training mit Verst√§rkung (RL) nicht konvergierte, und es ist ratsam, beim Training komplexer neuronaler Netze andere Methoden auszuprobieren.  Dann entstand eine Diskussion dar√ºber, wie wichtig verst√§rktes Lernen ist und wie sehr es den Status einer ‚Äûobligatorischen‚Äú Technologie beim Lernen zur L√∂sung von Problemen verdient.  Hier m√∂chte ich dar√ºber sprechen, dass Sie diese beiden Technologien nicht als konkurrierend betrachten sollten, von denen eine eindeutig besser ist als die andere.  im Gegenteil, sie erg√§nzen sich letztendlich.  Wenn Sie ein wenig dar√ºber nachdenken, was erforderlich ist, um eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gemeinsame KI</a> und solche Systeme zu schaffen, die w√§hrend ihrer gesamten Existenz lernen, beurteilen und planen k√∂nnen, werden wir mit ziemlicher Sicherheit zu dem Schluss kommen, dass diese oder jene kombinierte L√∂sung erforderlich sein wird .  √úbrigens war es die Natur, die zu der kombinierten Entscheidung kam, die w√§hrend der Evolution mit der komplexen Intelligenz von S√§ugetieren und anderen h√∂heren Tieren ausgestattet war. <br><br><h4>  Evolutionsstrategien </h4><br>  Die Hauptthese des OpenAI-Artikels war, dass sie anstelle des verst√§rkten Lernens in Kombination mit traditioneller Backpropagation das neuronale Netzwerk erfolgreich trainierten, um komplexe Probleme mithilfe der sogenannten ‚ÄûEvolutionsstrategie‚Äú (ES) zu l√∂sen.  Ein solcher ES-Ansatz besteht darin, die Verteilung der Gewichtswerte auf einer Netzwerkskala aufrechtzuerhalten, wobei viele Agenten parallel arbeiten und aus dieser Verteilung ausgew√§hlte Parameter verwenden.  Jeder Agent arbeitet in seiner eigenen Umgebung. Nach Abschluss einer bestimmten Anzahl von Episoden oder Phasen einer Episode gibt der Algorithmus eine Gesamtbelohnung zur√ºck, die als Fitness-Score ausgedr√ºckt wird.  Angesichts dieses Wertes kann die Verteilung der Parameter auf erfolgreichere Agenten verschoben werden, wodurch weniger erfolgreiche beraubt werden.  Millionen Mal, wenn eine solche Operation mit Hunderten von Agenten wiederholt wird, ist es m√∂glich, die Verteilung von Gewichten in einen Bereich zu verschieben, der es uns erm√∂glicht, eine Qualit√§tsrichtlinie f√ºr Agenten zu formulieren, um ihre Aufgabe zu l√∂sen.  In der Tat sind die in diesem Artikel vorgestellten Ergebnisse beeindruckend: Wenn Sie tausend Agenten parallel ausf√ºhren, kann die anthropomorphe Bewegung auf zwei Beinen in weniger als einer halben Stunde untersucht werden (w√§hrend selbst die fortschrittlichsten RL-Methoden mehr als eine Stunde ben√∂tigen).  F√ºr eine detailliertere √úbersicht empfehle ich, einen ausgezeichneten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beitrag</a> der Autoren des Experiments sowie den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">wissenschaftlichen Artikel</a> selbst zu lesen. <br><br><img src="https://habrastorage.org/webt/0j/lp/ms/0jlpmsa3-jz8ono405nv8c79ve8.gif"><br><br>  <i>Verschiedene Lernstrategien f√ºr eine anthropomorphe aufrechte Haltung, die mit der ES-Methode von OpenAI untersucht wurden.</i> <br><br><h4>  Black Box </h4><br>  Der gro√üe Vorteil dieser Methode ist, dass sie leicht zu parallelisieren ist.  W√§hrend RL-Methoden, beispielsweise A3C, den Informationsaustausch zwischen Workflows und dem Parameterserver erfordern, ben√∂tigt ES nur G√ºltigkeitssch√§tzungen und allgemeine Informationen zur Verteilung von Parametern.  Dank dieser Einfachheit umgeht diese Methode moderne RL-Methoden in Bezug auf die Skalierbarkeit.  All dies ist jedoch nicht umsonst: Sie m√ºssen das Netzwerk nach dem Prinzip einer Black Box optimieren.  In diesem Fall bedeutet die ‚ÄûBlack Box‚Äú, dass w√§hrend des Trainings die interne Struktur des Netzwerks vollst√§ndig ignoriert wird und nur das Gesamtergebnis (Belohnung f√ºr die Episode) verwendet wird. Dies h√§ngt davon ab, ob die Gewichte eines bestimmten Netzwerks von zuk√ºnftigen Generationen vererbt werden.  In Situationen, in denen wir kein ausgepr√§gtes Feedback von der Umgebung erhalten - und bei der L√∂sung vieler traditioneller RL-Aufgaben ist der Belohnungsfluss sehr selten -, wandelt sich das Problem von einer ‚Äûteilweise Black Box‚Äú zu einer ‚Äûvollst√§ndig Black Box‚Äú.  In diesem Fall ist es m√∂glich, die Produktivit√§t ernsthaft zu steigern, daher ist ein solcher Kompromiss nat√ºrlich gerechtfertigt.  "Wer braucht Farbverl√§ufe, wenn sie noch hoffnungslos laut sind?"  - Dies ist die allgemeine Meinung. <br><br>  In Situationen, in denen das Feedback aktiver ist, beginnen ES-Angelegenheiten jedoch schief zu laufen.  Das OpenAI-Team beschreibt, wie das einfache Klassifizierungsnetzwerk MNIST mit ES trainiert wurde, und diesmal war das Training 1000-mal langsamer.  Tatsache ist, dass das Gradientensignal bei der Klassifizierung von Bildern √§u√üerst informativ ist, wie dem Netzwerk eine bessere Klassifizierung beigebracht werden kann.  Daher h√§ngt das Problem weniger mit der RL-Technik als vielmehr mit sp√§rlichen Belohnungen in Umgebungen zusammen, die verrauschte Gradienten erzeugen. <br><br><h4>  L√∂sung von Natur aus gefunden </h4><br>  Wenn Sie versuchen, aus dem Beispiel der Natur zu lernen und √ºber M√∂glichkeiten zur Entwicklung der KI nachzudenken, kann KI in einigen F√§llen als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">problemorientierter Ansatz dargestellt werden</a> .  Letztendlich arbeitet die Natur innerhalb solcher Grenzen, die Informatiker einfach nicht haben.  Es besteht die Meinung, dass ein rein theoretischer Ansatz zur L√∂sung eines bestimmten Problems effektivere L√∂sungen liefern kann als empirische Alternativen.  Dennoch denke ich immer noch, dass es ratsam w√§re zu √ºberpr√ºfen, wie ein dynamisches System, das unter Bedingungen bestimmter Einschr√§nkungen (Erde) arbeitet, Wirkstoffe (Tiere, insbesondere S√§ugetiere) bildet, die zu flexiblem und komplexem Verhalten f√§hig sind.  W√§hrend einige dieser Einschr√§nkungen in den simulierten Welten der Datenwissenschaft nicht anwendbar sind, sind andere nur sehr gut. <br><br>  Nachdem wir das intellektuelle Verhalten von S√§ugetieren untersucht haben, sehen wir, dass es als Ergebnis des komplexen Zusammenspiels zweier eng miteinander verbundener Prozesse entsteht: <i>Lernen aus der Erfahrung anderer</i> und <i>Lernen aus unserer eigenen Erfahrung</i> .  Die erste wird oft aufgrund der nat√ºrlichen Selektion mit der Evolution identifiziert, aber hier verwende ich einen breiteren Begriff, um Epigenetik, Mikrobiome und andere Mechanismen zu ber√ºcksichtigen, die den Erfahrungsaustausch zwischen Organismen gew√§hrleisten, die nicht genetisch miteinander verwandt sind.  Der zweite Prozess, das Lernen aus erster Hand, sind alle Informationen, die ein Tier im Laufe seines Lebens aufnehmen kann, und diese Informationen stehen in direktem Zusammenhang mit der Interaktion dieses Tieres mit der Au√üenwelt.  Diese Kategorie umfasst alles vom Lernen, Objekte zu erkennen, bis hin zur Beherrschung der Kommunikation, die dem Bildungsprozess innewohnt. <br><br>  Grob gesagt k√∂nnen diese beiden in der Natur auftretenden Prozesse mit zwei Optionen zur Optimierung neuronaler Netze verglichen werden.  Evolutionsstrategien, bei denen Gradienteninformationen verwendet werden, um Informationen √ºber den K√∂rper zu aktualisieren, kommen dem Lernen aus den Erfahrungen anderer nahe.  In √§hnlicher Weise sind Gradientenmethoden, bei denen der Empfang einer bestimmten Erfahrung zu der einen oder anderen √Ñnderung des Verhaltens des Agenten f√ºhrt, vergleichbar mit dem Lernen aus Erfahrung.  Wenn Sie √ºber die Vielfalt des intellektuellen Verhaltens oder √ºber die F√§higkeiten nachdenken, die jeder dieser beiden Ans√§tze bei Tieren entwickelt, ist ein solcher Vergleich ausgepr√§gter.  In beiden F√§llen tragen ‚Äûevolution√§re Methoden‚Äú zur Untersuchung reaktiver Verhaltensweisen bei, die die Entwicklung einer bestimmten Fitness erm√∂glichen (ausreichend, um am Leben zu bleiben).  Das Erlernen des Gehens oder der Flucht aus der Gefangenschaft entspricht in vielen F√§llen einem eher ‚Äûinstinktiven‚Äú Verhalten, das bei vielen Tieren auf genetischer Ebene ‚Äûfest verdrahtet‚Äú ist.  Dar√ºber hinaus best√§tigt dieses Beispiel, dass evolution√§re Methoden in F√§llen anwendbar sind, in denen eine Signalbelohnung √§u√üerst selten ist (wie zum Beispiel die Tatsache einer erfolgreichen Aufzucht eines Jungen).  In einem solchen Fall ist es unm√∂glich, die Belohnung mit einer bestimmten Reihe von Ma√ünahmen zu korrelieren, die m√∂glicherweise viele Jahre vor dem Einsetzen dieser Tatsache begangen wurden.  Wenn wir andererseits den Fall betrachten, in dem ES versagt, n√§mlich die Klassifizierung von Bildern, dann sind die Ergebnisse bemerkenswert vergleichbar mit den Ergebnissen des Tiertrainings, die w√§hrend unz√§hliger verhaltenspsychologischer Experimente erzielt wurden, die √ºber mehr als hundert Jahre durchgef√ºhrt wurden. <br><br><h4>  Tiertraining </h4><br>  Die Methoden des verst√§rkten Lernens stammen in vielen F√§llen direkt aus der psychologischen Literatur zur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">operanten Konditionierung</a> , und die operante Konditionierung wurde auf der Grundlage der Tierpsychologie untersucht.  Richard Sutton, einer der beiden Gr√ºnder des Verst√§rkungstrainings, hat √ºbrigens einen Bachelor-Abschluss in Psychologie.  Im Kontext der operanten Konditionierung lernen Tiere, Belohnung oder Bestrafung mit bestimmten Verhaltensmustern zu assoziieren.  Trainer und Forscher k√∂nnen eine solche Assoziation irgendwie mit Belohnungen manipulieren und Tiere dazu bringen, Einfallsreichtum oder bestimmte Verhaltensweisen zu zeigen.  Die bei der Untersuchung von Tieren verwendete operative Konditionierung ist jedoch nichts anderes als eine ausgepr√§gtere Form dieser Konditionierung, auf deren Grundlage Tiere w√§hrend des gesamten Lebens trainiert werden.  Wir erhalten st√§ndig positive Verst√§rkungssignale aus der Umwelt und passen unser Verhalten entsprechend an.  Tats√§chlich glauben viele Neurophysiologen und Kognitionswissenschaftler, dass Menschen und andere Tiere sogar eine Ebene h√∂her agieren und st√§ndig lernen, die Ergebnisse ihres Verhaltens in zuk√ºnftigen Situationen vorherzusagen, wobei sie auf potenzielle Belohnungen z√§hlen. <br><br>  Die zentrale Rolle der Prognose im Selbststudium besteht darin, die oben beschriebene Dynamik in h√∂chstem Ma√üe zu ver√§ndern.  Das Signal, das zuvor als sehr verd√ºnnt angesehen wurde (episodische Belohnung), ist sehr dicht.  Theoretisch ist die Situation ungef√§hr so: Zu jedem Zeitpunkt berechnet das Gehirn von S√§ugetieren die Ergebnisse basierend auf einem komplexen Strom sensorischer Reize und Handlungen, w√§hrend das Tier einfach in diesen Strom eingetaucht wird.  In diesem Fall gibt das endg√ºltige Verhalten des Tieres ein dichtes Signal, das von der Korrektur der Vorhersagen und der Entwicklung des Verhaltens geleitet werden muss.  Das Gehirn nutzt all diese Signale, um Prognosen (und dementsprechend die Qualit√§t der ergriffenen Ma√ünahmen) f√ºr die Zukunft zu optimieren.  Einen √úberblick √ºber diesen Ansatz gibt das ausgezeichnete Buch ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Surfing Uncertainty</a> ‚Äú des Kognitionswissenschaftlers und Philosophen Andy Clark.  Wenn solche √úberlegungen auf das Training k√ºnstlicher Wirkstoffe hochgerechnet werden, zeigt das Verst√§rkungstraining einen grundlegenden Fehler: Das in diesem Paradigma verwendete Signal ist hoffnungslos schwach im Vergleich zu dem, was es sein k√∂nnte (oder sollte).  In F√§llen, in denen es unm√∂glich ist, die Signals√§ttigung zu erh√∂hen (m√∂glicherweise weil sie per Definition schwach ist oder mit einer geringen Reaktivit√§t verbunden ist), ist es wahrscheinlich besser, eine gut parallelisierte Trainingsmethode zu bevorzugen, z. B. ES. <br><br><h4>  Besseres Lernen neuronaler Netze </h4><br>  Basierend auf den Prinzipien einer h√∂heren Nervenaktivit√§t, die dem Gehirn von S√§ugetieren innewohnt, das st√§ndig mit Prognosen besch√§ftigt ist, konnten in letzter Zeit bestimmte Erfolge beim Verst√§rkungstraining erzielt werden, wobei nun die Bedeutung solcher Prognosen ber√ºcksichtigt wird.  Ich kann Ihnen zwei √§hnliche Werke empfehlen: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Handeln lernen, indem man die Zukunft vorhersagt</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verst√§rkungslernen mit unbeaufsichtigten Hilfsaufgaben</a> </li></ul><br>  In beiden Artikeln erg√§nzen die Autoren die typischen Standardrichtlinien f√ºr neuronale Netze mit Prognoseergebnissen hinsichtlich zuk√ºnftiger Umgebungsbedingungen.  Im ersten Artikel wird die Prognose auf eine Vielzahl von Messvariablen angewendet, im zweiten auf √Ñnderungen in der Umgebung und im Verhalten des Agenten als solchem.  In beiden F√§llen wird das mit einer positiven Verst√§rkung verbundene sp√§rliche Signal viel ges√§ttigter und informativer, was sowohl ein beschleunigtes Lernen als auch die Assimilation komplexerer Verhaltensmodelle erm√∂glicht.  Solche Verbesserungen sind nur verf√ºgbar, wenn mit Methoden gearbeitet wird, die das Gradientensignal verwenden, nicht jedoch mit Methoden, die nach dem Prinzip der "Black Box" arbeiten, wie beispielsweise ES. <br><br>  Dar√ºber hinaus sind Lern- und Gradientenmethoden aus erster Hand viel effektiver.  Selbst in den F√§llen, in denen es m√∂glich war, ein bestimmtes Problem mit der ES-Methode zu untersuchen, anstatt ein Verst√§rkungstraining zu verwenden, wurde der Gewinn dadurch erzielt, dass viel mehr Daten in die ES-Strategie einbezogen wurden als mit RL.  Wenn wir in diesem Fall √ºber die Prinzipien des Tiertrainings nachdenken, stellen wir fest, dass sich das Ergebnis des Trainings an einem fremden Beispiel nach vielen Generationen manifestiert, w√§hrend manchmal ein einziges Ereignis, das durch die eigene Erfahrung erlebt wird, ausreicht, damit das Tier die Lektion f√ºr immer lernen kann.  Ein solches <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Training ohne Beispiele</a> passt zwar noch nicht vollst√§ndig in herk√∂mmliche Gradientenmethoden, ist jedoch viel verst√§ndlicher als ES.  Es gibt zum Beispiel Ans√§tze wie die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">episodische neuronale Steuerung</a> , bei denen Q-Werte w√§hrend des Trainingsprozesses gespeichert werden, wonach das Programm mit ihnen √ºberpr√ºft wird, bevor Aktionen ausgef√ºhrt werden.  Es stellt sich eine Gradientenmethode heraus, mit der Sie lernen k√∂nnen, Probleme viel schneller als zuvor zu l√∂sen.  In dem Artikel √ºber die episodische neuronale Kontrolle erw√§hnen die Autoren den menschlichen Hippocampus, der auch nach einer einmal erlebten Erfahrung Informationen √ºber das Ereignis speichern kann und daher eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">entscheidende Rolle</a> im R√ºckrufprozess spielt.  Solche Mechanismen erfordern den Zugriff auf die interne Organisation des Agenten, was im ES-Paradigma per Definition ebenfalls unm√∂glich ist. <br><br><h4>  Warum also nicht kombinieren? </h4><br>  Wahrscheinlich h√§tte der gr√∂√üte Teil dieses Artikels den Eindruck hinterlassen k√∂nnen, dass ich darin RL-Methoden bef√ºrworte.  Tats√§chlich glaube ich jedoch, dass auf lange Sicht die beste L√∂sung eine Kombination beider Methoden w√§re, so dass jede in den Situationen verwendet wird, in denen sie am besten geeignet ist.  Offensichtlich gewinnt ES bei vielen reaktiven Richtlinien oder in Situationen mit sehr sp√§rlichen Signalen positiver Verst√§rkung, insbesondere wenn Sie √ºber die Rechenleistung verf√ºgen, mit der Sie massenparalleles Training durchf√ºhren k√∂nnen.  Auf der anderen Seite sind Gradientenmethoden mit verst√§rktem Lernen oder Lehrerausbildung n√ºtzlich, wenn uns ein umfassendes Feedback zur Verf√ºgung steht und die L√∂sung des Problems schnell und mit weniger Daten erlernt werden muss. <br><br>  Wenn wir uns der Natur zuwenden, stellen wir fest, dass die erste Methode im Wesentlichen den Grundstein f√ºr die zweite legt.  Aus diesem Grund haben S√§ugetiere w√§hrend der Evolution ein Gehirn entwickelt, das ein √§u√üerst effizientes Lernen aus dem Material komplexer Signale aus der Umwelt erm√∂glicht.  Die Frage bleibt also offen.  Vielleicht helfen uns evolution√§re Strategien dabei, effektive Lernarchitekturen zu erfinden, die f√ºr Gradientenlernmethoden n√ºtzlich sind.  Schlie√ülich ist die von der Natur gefundene L√∂sung in der Tat sehr erfolgreich. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de456160/">https://habr.com/ru/post/de456160/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de456148/index.html">News der Woche: Facebook lehnt Huawei in Anwendungen ab, Aurora OS statt Android, Mining-Strafen</a></li>
<li><a href="../de456150/index.html">Kleine Freude # 4: Radon - Codequalit√§t gemessen in Zahlen</a></li>
<li><a href="../de456154/index.html">Core UX Features & MVP beim Erstellen eines Produkts</a></li>
<li><a href="../de456156/index.html">Deshalb wird Schulalgebra ben√∂tigt.</a></li>
<li><a href="../de456158/index.html">Ein bisschen √ºber Kernbrennstoffquellen</a></li>
<li><a href="../de456162/index.html">Aurora, ein Unternehmen, das von Einwanderern aus Google, Tesla und Uber gegr√ºndet wurde, begann mit der Arbeit mit Autokonzernen</a></li>
<li><a href="../de456164/index.html">Loon-Ballons bieten nach einem Erdbeben der St√§rke 8,0 eine Notfallverbindung zum Netzwerk und zum Internet in Peru</a></li>
<li><a href="../de456168/index.html">Wo war dein Haus vor Millionen von Jahren?</a></li>
<li><a href="../de456170/index.html">So erstellen Sie eine Finanzanwendung: 5 APIs, die dem Entwickler helfen</a></li>
<li><a href="../de456172/index.html">Teil 2: RocketChip: RAM verbinden</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>