<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üéÇ üèÇüèæ ‚å®Ô∏è Prosesor tensor gratis dari Google di Colaboratory Cloud ‚è≥ ü§† ü§°</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Google baru-baru ini memberikan akses gratis ke unit pemrosesan tensor (TPU) pada platform pembelajaran mesin berbasis cloud Colaboratory . Prosesor t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Prosesor tensor gratis dari Google di Colaboratory Cloud</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/428117/">  Google baru-baru ini memberikan akses gratis ke unit pemrosesan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">tensor</a> (TPU) pada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">platform</a> pembelajaran mesin berbasis cloud <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Colaboratory</a> .  Prosesor tensor adalah sirkuit terintegrasi khusus (ASIC) yang dikembangkan oleh Google untuk tugas pembelajaran mesin menggunakan perpustakaan TensorFlow.  Saya memutuskan untuk mencoba mempelajari jaringan convolutional TPU pada Keras, yang mengenali objek dalam gambar CIFAR-10.  Kode solusi lengkap dapat dilihat dan dijalankan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">laptop</a> . <br><br><img src="https://habrastorage.org/webt/sl/ut/ho/slutho5dsyeduk2biql9rcsblnu.jpeg"><br>  <i>Foto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">cloud.google.com</a></i> <br><a name="habracut"></a><br><h2>  Prosesor tensor </h2><br>  Pada Habr√© sudah menulis bagaimana TPU disusun (di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> , di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> dan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> ), dan juga <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">mengapa TPU cocok untuk melatih jaringan saraf</a> .  Oleh karena itu, saya tidak akan menyelidiki detail arsitektur TPU, tetapi hanya mempertimbangkan fitur-fitur yang perlu dipertimbangkan ketika melatih jaringan saraf. <br><br>  Sekarang ada tiga generasi prosesor tensor, kinerja TPU generasi ketiga terakhir adalah 420 TFlop (triliunan operasi floating point per detik), berisi 128 GB Memori Bandwidth Tinggi.  Namun, hanya TPU generasi kedua yang tersedia di Colaboratory, yang memiliki 180 TFlop kinerja dan 64 GB memori.  Di masa depan, saya akan mempertimbangkan TPU ini. <br><br>  Prosesor tensor terdiri dari empat chip, yang masing-masing berisi dua core, total delapan core di TPU.  Pelatihan TPU dilakukan secara paralel pada semua inti menggunakan replikasi: setiap inti menjalankan salinan grafik TensorFlow dengan seperdelapan volume data. <br><br>  Dasar dari prosesor tensor adalah unit matriks (MXU).  Ia menggunakan struktur data licik dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">array sistolik</a> 128x128 untuk implementasi operasi matriks yang efisien.  Oleh karena itu, untuk memaksimalkan penggunaan sumber daya peralatan TPU, dimensi sampel mini atau fitur harus kelipatan 128 ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sumber</a> ).  Juga, karena sifat dari sistem memori TPU, diinginkan bahwa dimensi sampel-mini dan fitur-fiturnya menjadi kelipatan dari 8. <br><br><h2>  Platform Kolaboratori </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Colaboratory</a> adalah platform cloud Google untuk memajukan teknologi pembelajaran mesin.  Anda bisa mendapatkan mesin virtual dengan pustaka populer yang diinstal TensorFlow, Keras, sklearn, panda, dll. Secara gratis.  Yang paling nyaman adalah Anda bisa menjalankan laptop yang mirip dengan Jupyter di Colaboratory.  Laptop disimpan di Google Drive, Anda dapat mendistribusikannya dan bahkan mengatur kolaborasi.  Seperti inilah bentuk laptop di Colaboratory ( <i>gambar dapat diklik</i> ): <br><br> <a href=""><img src="https://habrastorage.org/webt/4b/gp/sn/4bgpsnbpkhwyqrid6fixnaurcbw.png"></a> <br><br>  Anda menulis kode di browser di laptop, itu berjalan di mesin virtual di Google Cloud.  Mobil dikeluarkan untuk Anda selama 12 jam, setelah itu berhenti.  Namun, tidak ada yang mencegah Anda memulai mesin virtual lain dan bekerja 12 jam lagi.  Perlu diingat bahwa setelah mesin virtual berhenti, semua data darinya dihapus.  Karena itu, jangan lupa untuk menyimpan data yang diperlukan ke komputer Anda atau Google Drive, dan setelah memulai ulang mesin virtual, unduh lagi. <br><br>  Instruksi terperinci untuk bekerja pada platform Kolaborator ada di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> , di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> dan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . <br><br><h2>  Hubungkan prosesor tensor ke Colaboratory </h2><br>  Secara default, Colaboratory tidak menggunakan akselerator perhitungan GPU atau TPU.  Anda dapat menghubungkan mereka di menu Runtime -&gt; Ubah jenis runtime -&gt; Akselerator perangkat keras.  Di daftar yang muncul, pilih "TPU": <br><img src="https://habrastorage.org/webt/1f/7r/vt/1f7rvtfjvdowdjwrz0ctgyyly7s.png" alt="gambar"><br><br>  Setelah memilih jenis akselerator, mesin virtual yang terhubung dengan laptop Colaboratory akan dimulai ulang dan TPU akan tersedia. <br><br>  Jika Anda mengunduh data apa pun ke mesin virtual, maka selama proses restart itu akan dihapus.  Anda harus mengunduh data lagi. <br><br><h2>  Keras Neural Network untuk CIFAR-10 Recognition </h2><br>  Sebagai contoh, mari kita coba latih jaringan saraf Keras pada TPU yang mengenali gambar dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dataset CIFAR-10</a> .  Ini adalah set data populer yang berisi gambar kecil dari objek 10 kelas: pesawat terbang, mobil, burung, kucing, rusa, anjing, katak, kuda, kapal dan truk.  Kelas tidak berpotongan, setiap objek dalam gambar hanya milik satu kelas. <br><br>  Unduh dataset CIFAR-10 menggunakan Keras: <br><br><pre><code class="python hljs">(x_train, y_train), (x_test, y_test) = cifar10.load_data()</code> </pre> <br>  Untuk membuat jaringan saraf, saya mendapat fungsi terpisah.  Kami akan membuat model yang sama dua kali: versi pertama model untuk TPU, yang akan kami latih, dan yang kedua untuk CPU, tempat kami akan mengenali objek. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">create_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> input_layer = Input(shape=(<span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), dtype=tf.float32, name=<span class="hljs-string"><span class="hljs-string">'Input'</span></span>) x = BatchNormalization()(input_layer) x = Conv2D(<span class="hljs-number"><span class="hljs-number">32</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) x = Conv2D(<span class="hljs-number"><span class="hljs-number">32</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>))(x) x = Dropout(<span class="hljs-number"><span class="hljs-number">0.25</span></span>)(x) x = BatchNormalization()(x) x = Conv2D(<span class="hljs-number"><span class="hljs-number">64</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) x = Conv2D(<span class="hljs-number"><span class="hljs-number">64</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) x = MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>))(x) x = Dropout(<span class="hljs-number"><span class="hljs-number">0.25</span></span>)(x) x = Flatten()(x) x = Dense(<span class="hljs-number"><span class="hljs-number">512</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) x = Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)(x) output_layer = Dense(<span class="hljs-number"><span class="hljs-number">10</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)(x) model = Model(inputs=[input_layer], outputs=[output_layer]) model.compile( optimizer=tf.train.AdamOptimizer(<span class="hljs-number"><span class="hljs-number">0.001</span></span>), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=[<span class="hljs-string"><span class="hljs-string">'sparse_categorical_accuracy'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model</code> </pre> <br>  Sejauh ini, pengoptimal Keras tidak dapat digunakan pada TPU, oleh karena itu, ketika menyusun model, pengoptimal dari TensorFlow ditentukan. <br><br>  Kami membuat model Keras untuk CPU, yang pada langkah berikutnya kami akan mengonversi menjadi model untuk TPU: <br><br><pre> <code class="python hljs">cpu_model = create_model()</code> </pre> <br><h2>  Ubah jaringan neural Keras menjadi model TPU </h2><br>  Model pada Keras dan TensorFlow dapat dilatih pada GPU tanpa perubahan.  Anda tidak dapat melakukan ini di TPU sejauh ini, jadi Anda harus mengubah model yang kami buat menjadi model untuk TPU. <br><br>  Pertama, Anda perlu mencari tahu di mana TPU tersedia bagi kami.  Pada platform Kolaboratori, ini dapat dilakukan dengan perintah berikut: <br><br><pre> <code class="python hljs">TPU_WORKER = <span class="hljs-string"><span class="hljs-string">'grpc://'</span></span> + os.environ[<span class="hljs-string"><span class="hljs-string">'COLAB_TPU_ADDR'</span></span>]</code> </pre> <br>  Dalam kasus saya, alamat TPU ternyata seperti ini - <code>grpc://10.102.233.146:8470</code> .  Alamat berbeda untuk peluncuran yang berbeda. <br><br>  Sekarang Anda bisa mendapatkan model untuk TPU menggunakan fungsi <code>keras_to_tpu_model</code> : <br><br><pre> <code class="python hljs">tf.logging.set_verbosity(tf.logging.INFO) tpu_model = tf.contrib.tpu.keras_to_tpu_model( cpu_model, strategy=tf.contrib.tpu.TPUDistributionStrategy( tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))</code> </pre> <br>  Baris pertama termasuk logging di tingkat Info.  Berikut adalah log konversi model: <br><br> <code>INFO:tensorflow:Querying Tensorflow master (b'grpc://10.102.233.146:8470') for TPU system metadata. <br> INFO:tensorflow:Found TPU system: <br> INFO:tensorflow:*** Num TPU Cores: 8 <br> INFO:tensorflow:*** Num TPU Workers: 1 <br> INFO:tensorflow:*** Num TPU Cores Per Worker: 8 <br> ... <br> WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.</code> <br> <br>  Anda dapat melihat bahwa TPU ditemukan di alamat yang kami tentukan sebelumnya, memiliki 8 core.  Kami juga melihat peringatan bahwa <code>tpu_model</code> bersifat eksperimental dan dapat diubah atau dihapus kapan saja.  Saya berharap bahwa seiring waktu dimungkinkan untuk melatih model Keras langsung di TPU tanpa konversi apa pun. <br><br><h2>  Kami melatih model di TPU </h2><br>  Model untuk TPU dapat dilatih dengan cara biasa untuk Keras dengan memanggil metode <code>fit</code> : <br><br><pre> <code class="python hljs">history = tpu_model.fit(x_train, y_train, batch_size=<span class="hljs-number"><span class="hljs-number">128</span></span>*<span class="hljs-number"><span class="hljs-number">8</span></span>, epochs=<span class="hljs-number"><span class="hljs-number">50</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">2</span></span>)</code> </pre> <br>  Apa saja fiturnya di sini.  Kami ingat bahwa agar dapat menggunakan TPU secara efisien, ukuran sampel mini harus kelipatan 128. Selain itu, pelatihan tentang seperdelapan dari semua data dalam sampel mini dilakukan pada setiap inti TPU.  Oleh karena itu, kami mengatur ukuran sampel mini selama pelatihan menjadi 128 * 8, kami mendapatkan 128 gambar untuk setiap inti TPU.  Anda dapat menggunakan ukuran yang lebih besar, misalnya 256 atau 512, maka kinerjanya akan lebih tinggi. <br><br>  Dalam kasus saya, pelatihan satu era membutuhkan rata-rata 6 detik. <br><br>  Kualitas pendidikan di era ke-50: <br> <code>Epoch 50/50 <br> - 6s - loss: 0.2727 - sparse_categorical_accuracy: 0.9006</code> <br> <br>  Bagian jawaban yang benar untuk data untuk pelatihan adalah 90,06%.  Kami memeriksa kualitas data uji menggunakan TPU: <br><br><pre> <code class="python hljs">scores = tpu_model.evaluate(x_test, y_test, verbose=<span class="hljs-number"><span class="hljs-number">0</span></span>, batch_size=batch_size * <span class="hljs-number"><span class="hljs-number">8</span></span>) print(<span class="hljs-string"><span class="hljs-string">"     : %.2f%%"</span></span> % (scores[<span class="hljs-number"><span class="hljs-number">1</span></span>]*<span class="hljs-number"><span class="hljs-number">100</span></span>))</code> </pre> <br> <code>     : 80.79%</code> <br> <br>  Sekarang simpan bobot model yang terlatih: <br><br><pre> <code class="python hljs">tpu_model.save_weights(<span class="hljs-string"><span class="hljs-string">"cifar10_model.h5"</span></span>)</code> </pre> <br>  TensorFlow akan memberi kita pesan bahwa bobot dipindahkan dari TPU ke CPU: <br> <code>INFO:tensorflow:Copying TPU weights to the CPU</code> <br> <br>  Perlu dicatat bahwa bobot jaringan yang terlatih disimpan pada disk mesin virtual Colaboratory.  Ketika mesin virtual dihentikan, maka semua data dari itu akan terhapus.  Jika Anda tidak ingin kehilangan bobot yang terlatih, simpanlah ke komputer Anda: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> google.colab <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> files files.download(<span class="hljs-string"><span class="hljs-string">"cifar10_model.h5"</span></span>)</code> </pre> <br><h2>  Mengenali objek pada CPU </h2><br>  Sekarang mari kita coba menggunakan model yang dilatih tentang TPU untuk mengenali objek dalam gambar menggunakan CPU.  Untuk melakukan ini, buat model lagi dan muat bobot yang dilatih pada TPU ke dalamnya: <br><br><pre> <code class="python hljs">model = create_model() model.load_weights(<span class="hljs-string"><span class="hljs-string">"cifar10_model.h5"</span></span>)</code> </pre> <br>  Model ini siap digunakan pada prosesor pusat.  Mari kita coba mengenali dengan bantuannya salah satu gambar dari test suite CIFAR-10: <br><br><pre> <code class="python hljs">index=<span class="hljs-number"><span class="hljs-number">111</span></span> plt.imshow(toimage(x_test[index])) plt.show()</code> </pre> <br><img src="https://habrastorage.org/webt/za/z3/f-/zaz3f-jatj-5crlgsih84gsakg0.png"><br><br>  Gambarannya kecil, tetapi Anda dapat memahami bahwa ini adalah pesawat terbang.  Kami mulai pengakuan: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#      CIFAR-10 classes=['', '', '', '', '', '', '', '', '', ''] x = x_test[index] #  , .. Keras    x = np.expand_dims(x, axis=0) #   prediction = model.predict(x) #       print(prediction) #     prediction = np.argmax(prediction) print(classes[prediction])</span></span></code> </pre> <br>  Kami mendapatkan daftar nilai output neuron, hampir semuanya mendekati nol, kecuali nilai pertama, yang sesuai dengan bidang. <br><br> <code>[[9.81738389e-01 2.91262069e-07 1.82225723e-02 9.78524668e-07 <br> 5.89265142e-07 6.76223244e-10 1.03252004e-10 9.23009047e-09 <br> 3.71878523e-05 3.16599618e-08]] <br> </code> <br> <br>  Pengakuan berhasil! <br><br><h2>  Ringkasan </h2><br>  Itu mungkin untuk menunjukkan operabilitas TPU pada platform Colaboratory, dapat digunakan untuk pelatihan jaringan saraf pada Keras.  Namun, dataset CIFAR-10 terlalu kecil, itu tidak cukup untuk memuat sumber daya TPU sepenuhnya.  Akselerasi dibandingkan dengan GPU ternyata kecil (Anda dapat memeriksa diri Anda dengan memilih GPU sebagai akselerator, bukan TPU dan melatih kembali model). <br><br>  Di Habr√© ada artikel yang <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">mengukur kinerja TPU dan GPU V100 dalam melatih jaringan ResNet-50</a> .  Pada tugas ini, TPU menunjukkan kinerja yang sama dengan empat GPU V100.  Sangat menyenangkan bahwa Google menyediakan akselerator pembelajaran jaringan saraf yang kuat secara gratis! <br><br>  Video mendemonstrasikan pelatihan jaringan saraf Keras pada TPU. <br><iframe width="560" height="315" src="https://www.youtube.com/embed/60xbDEpA49M" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h2>  Tautan yang bermanfaat </h2><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Laptop kolaboratif dengan kode belajar model Keras TPU penuh</a> . </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Notebook kolaboratif dengan contoh pelatihan Keras TPU untuk mengenali pakaian dan sepatu dari Fashion MNIST</a> . </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Prosesor tensor di Google Cloud</a> . </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Fitur arsitektur dan penggunaan prosesor tensor</a> . </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id428117/">https://habr.com/ru/post/id428117/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id428107/index.html">Kontainerisasi aplikasi Angular 6 SPA Template ASP .NET Core 2.1</a></li>
<li><a href="../id428109/index.html">Tembok perusahaan</a></li>
<li><a href="../id428111/index.html">Aritmatika presisi sewenang-wenang di Erlang</a></li>
<li><a href="../id428113/index.html">Untuk pertanyaan tentang kurva Bezier, kecepatan Arduino dan satu situs menarik, atau bagaimana saya menghabiskan akhir pekan</a></li>
<li><a href="../id428115/index.html">Pengembangan web untuk e-niaga: 5 tren teknologi untuk 2019</a></li>
<li><a href="../id428119/index.html">‚ÄúClass-field-proposal‚Äù atau ‚ÄúApa yang salah di tc39 commit‚Äù</a></li>
<li><a href="../id428121/index.html">Stan Drapkin. Perangkap Kriptografi Tingkat Tinggi di .NET</a></li>
<li><a href="../id428123/index.html">Minggu Keamanan 41: Kabar Baik</a></li>
<li><a href="../id428125/index.html">Siapa analitik produk dan mengapa mereka diperlukan dalam sebuah tim?</a></li>
<li><a href="../id428127/index.html">Cache nginx: semuanya baru - lama terlupakan</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>