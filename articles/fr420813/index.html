<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>☎️ 👨🏾‍🎓 🆚 Réseaux dans les coulisses de Kubernetes 🤙🏿 🤢 👩🏿‍💼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Remarque perev. : L'auteur de l'article original, Nicolas Leiva, est un architecte de solutions Cisco qui a décidé de partager avec ses collègues, ing...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Réseaux dans les coulisses de Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/420813/"> <i><b>Remarque</b></i>  <i><b>perev.</b></i>  <i>: L'auteur de l'article original, Nicolas Leiva, est un architecte de solutions Cisco qui a décidé de partager avec ses collègues, ingénieurs réseau, comment le réseau Kubernetes fonctionne de l'intérieur.</i>  <i>Pour ce faire, il explore sa configuration la plus simple dans le cluster, en appliquant activement le bon sens, sa connaissance des réseaux et des utilitaires Linux / Kubernetes standard.</i>  <i>Cela s'est avéré volumineux, mais très clair.</i> <br><br><img src="https://habrastorage.org/webt/gr/qw/d4/grqwd4putslwaijltw9yojfzjes.png"><br><br>  En plus du fait que le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">guide Kubernetes The Hard Way de</a> Kelsey Hightower fonctionne ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">même sur AWS!</a> ), J'ai aimé que le réseau soit maintenu propre et simple;  et c'est une excellente occasion de comprendre le rôle, par exemple, de l'interface réseau de conteneurs ( <a href="">CNI</a> ).  Cela dit, j'ajouterai que le réseau Kubernetes n'est pas vraiment très intuitif, surtout pour les débutants ... et n'oubliez pas non plus "qu'il n'y a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tout simplement pas</a> de réseau de conteneurs". <a name="habracut"></a><br><br>  Bien qu'il existe déjà de bons documents sur ce sujet (voir les liens <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> ), je n'ai pas pu trouver un tel exemple que je combinerais tout ce qui est nécessaire avec les conclusions des équipes que les ingénieurs de réseau aiment et détestent, démontrant ce qui se passe réellement dans les coulisses.  Par conséquent, j'ai décidé de collecter des informations à partir de nombreuses sources - j'espère que cela vous aide et que vous comprenez mieux comment tout est connecté les uns aux autres.  Cette connaissance est importante non seulement pour vous tester, mais aussi pour simplifier le processus de diagnostic des problèmes.  Vous pouvez suivre l'exemple dans votre cluster de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubernetes The Hard Way</a> : toutes les adresses IP et tous les paramètres sont extraits de là (à partir des validations de mai 2018, avant d'utiliser <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">les conteneurs Nabla</a> ). <br><br>  Et nous commencerons par la fin, lorsque nous aurons trois contrôleurs et trois nœuds de travail: <br><br><img src="https://habrastorage.org/webt/am/vs/6j/amvs6jnhsuxzwhyoyod6vjk8cby.png"><br><br>  Vous remarquerez peut-être qu'il y a également au moins trois sous-réseaux privés ici!  Un peu de patience, et ils seront tous pris en considération.  N'oubliez pas que même si nous nous référons à des préfixes IP très spécifiques, ils sont simplement extraits de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubernetes The Hard Way</a> , de sorte qu'ils n'ont qu'une signification locale, et vous êtes libre de choisir tout autre bloc d'adresses pour votre environnement conformément à la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">RFC 1918</a> .  Pour le cas d'IPv6, il y aura un article de blog séparé. <br><br><h2>  Réseau hôte (10.240.0.0/24) </h2><br>  Il s'agit d'un réseau interne dont tous les nœuds font partie.  Défini par l' <code>--private-network-ip</code> dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">GCP</a> ou l' <code>--private-ip-address</code> dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">AWS</a> lors de l'allocation des ressources informatiques. <br><br><h3>  Initialisation des nœuds de contrôleur dans GCP </h3><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 0 1 2; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> gcloud compute instances create controller-<span class="hljs-variable"><span class="hljs-variable">${i}</span></span> \ <span class="hljs-comment"><span class="hljs-comment"># ... --private-network-ip 10.240.0.1${i} \ # ... done</span></span></code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>controllers_gcp.sh</code></a> ) <br><br><h3>  Initialisation des nœuds de contrôleur dans AWS </h3><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 0 1 2; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-built_in"><span class="hljs-built_in">declare</span></span> controller_id<span class="hljs-variable"><span class="hljs-variable">${i}</span></span>=`aws ec2 run-instances \ <span class="hljs-comment"><span class="hljs-comment"># ... --private-ip-address 10.240.0.1${i} \ # ... done</span></span></code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>controllers_aws.sh</code></a> ) <br><br><img src="https://habrastorage.org/webt/gt/cj/p6/gtcjp6fgkqbv1nvs2ea9d9hhueo.png"><br><br>  Chaque instance aura deux adresses IP: privée du réseau hôte (contrôleurs - <code>10.240.0.1${i}/24</code> , travailleurs - <code>10.240.0.2${i}/24</code> ) et publique, désignée par le fournisseur de cloud, dont nous parlerons plus tard comment se rendre à <code>NodePorts</code> . <br><br><h3>  Gcp </h3><br><pre> <code class="bash hljs">$ gcloud compute instances list NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS controller-0 us-west1-c n1-standard-1 10.240.0.10 35.231.XXX.XXX RUNNING worker-1 us-west1-c n1-standard-1 10.240.0.21 35.231.XX.XXX RUNNING ...</code> </pre> <br><br><h3>  Aws </h3><br><pre> <code class="bash hljs">$ aws ec2 describe-instances --query <span class="hljs-string"><span class="hljs-string">'Reservations[].Instances[].[Tags[?Key==`Name`].Value[],PrivateIpAddress,PublicIpAddress]'</span></span> --output text | sed <span class="hljs-string"><span class="hljs-string">'$!N;s/\n/ /'</span></span> 10.240.0.10 34.228.XX.XXX controller-0 10.240.0.21 34.173.XXX.XX worker-1 ...</code> </pre> <br>  Tous les nœuds doivent pouvoir se pinguer si les <a href="">politiques de sécurité sont correctes</a> (et si le <code>ping</code> installé sur l'hôte). <br><br><h2>  Réseau de foyer (10.200.0.0/16) </h2><br>  Il s'agit du réseau dans lequel vivent les modules.  Chaque nœud de travail utilise un sous-réseau de ce réseau.  Dans notre cas, <code>POD_CIDR=10.200.${i}.0/24</code> pour le <code>worker-${i}</code> . <br><br><img src="https://habrastorage.org/webt/6i/yz/ih/6iyzihbxbzwugs4amvhfa9ysp5s.png"><br><br>  Pour comprendre comment tout est configuré, prenez du recul et examinez <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le modèle de réseau Kubernetes</a> , qui nécessite les éléments suivants: <br><br><ul><li>  Tous les conteneurs peuvent communiquer avec tout autre conteneur sans utiliser NAT. </li><li>  Tous les nœuds peuvent communiquer avec tous les conteneurs (et vice versa) sans utiliser NAT. </li><li>  L'IP que le conteneur voit doit être la même que les autres le voient. </li></ul><br>  Tout cela peut être implémenté de plusieurs façons, et Kubernetes transmet la configuration réseau au <a href="">plugin CNI</a> . <br><br><blockquote>  «Le plugin CNI est chargé d'ajouter une interface réseau à l' <b>espace de noms réseau</b> du conteneur (par exemple, une extrémité d'une <b>paire de veth</b> ) et d'apporter les modifications nécessaires sur l'hôte (par exemple, connecter la deuxième extrémité de veth à un pont).  Il doit ensuite attribuer une interface IP et configurer les routes conformément à la section Gestion des adresses IP en appelant le plugin IPAM souhaité. »  <i>(à partir de la <a href="">spécification de l'interface réseau</a> du <a href="">conteneur</a> )</i> </blockquote><br><img src="https://habrastorage.org/webt/5q/fs/vw/5qfsvwg2iuduy0q3doco11hbf-g.png"><br><br><h3>  Espace de noms réseau </h3><br><blockquote>  «L'espace de noms encapsule la ressource système globale dans une abstraction visible par les processus de cet espace de noms de telle sorte qu'ils aient leur propre instance isolée de la ressource globale.  Les modifications de la ressource globale sont visibles pour les autres processus inclus dans cet espace de noms, mais pas pour les autres processus. »  <i>(à <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">partir de la page de manuel des espaces de noms</a> )</i> </blockquote><br>  Linux fournit sept espaces de noms différents ( <code>Cgroup</code> , <code>IPC</code> , <code>Network</code> , <code>Mount</code> , <code>PID</code> , <code>User</code> , <code>UTS</code> ).  Les espaces de noms réseau ( <code>CLONE_NEWNET</code> ) définissent les ressources réseau disponibles pour le processus: «Chaque espace de noms réseau possède ses propres périphériques réseau, adresses IP, tables de routage IP, <code>/proc/net</code> , numéros de port, etc.» <i>( de l'article « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Espaces de noms en fonctionnement</a> »)</i> . <br><br><h3>  Périphériques Ethernet virtuels (Veth) </h3><br><blockquote>  «Une paire de réseaux virtuels (veth) offre une abstraction sous la forme d'un« canal », qui peut être utilisé pour créer des tunnels entre des espaces de noms réseau ou pour créer un pont vers un périphérique réseau physique dans un autre espace réseau.  Lorsque l'espace de noms est libéré, tous les périphériques Veth qu'il contient sont détruits. »  <i>(à partir de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">page de manuel des espaces de noms réseau</a> )</i> </blockquote><br>  Descendez au sol et voyez comment tout cela se rapporte au cluster.  Premièrement, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">les plugins réseau</a> de Kubernetes sont divers et les plugins CNI en font partie ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pourquoi pas CNM?</a> ).  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubelet</a> sur chaque nœud indique au <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">runtime</a> du conteneur le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">plug-in réseau à</a> utiliser.  L'interface <a href="">CNI</a> (Container Network Interface) se situe entre le runtime du conteneur et l'implémentation du réseau.  Et déjà le plugin CNI met en place le réseau. <br><br><blockquote>  «Le plugin CNI est sélectionné en passant l' <code>--network-plugin=cni</code> ligne de commande <code>--network-plugin=cni</code> à Kubelet.  Kubelet lit le fichier à partir de <code>--cni-conf-dir</code> (la valeur par défaut est <code>/etc/cni/net.d</code> ) et utilise la configuration CNI de ce fichier pour configurer le réseau pour chaque fichier. "  <i>(à partir des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">exigences du plugin réseau</a> )</i> </blockquote><br>  Les vrais fichiers binaires du plugin CNI sont dans <code>-- cni-bin-dir</code> (la valeur par défaut est <code>/opt/cni/bin</code> ). <br><br>  Veuillez noter que les <a href=""><code>kubelet.service</code></a> appel de <a href=""><code>kubelet.service</code></a> incluent <code>--network-plugin=cni</code> : <br><br><pre> <code class="plaintext hljs">[Service] ExecStart=/usr/local/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --network-plugin=cni \\ ...</code> </pre> <br>  Tout d'abord, Kubernetes crée un espace de noms réseau pour le foyer, avant même d'appeler des plugins.  Ceci est mis en œuvre en utilisant le conteneur de <code>pause</code> spécial, qui "sert de" conteneur parent "pour tous les conteneurs de foyer" <i>(à partir de l'article " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Le conteneur de pause tout-puissant</a> ")</i> .  Kubernetes exécute ensuite le plugin CNI pour attacher le conteneur de <code>pause</code> au réseau.  Tous les conteneurs de pod utilisent l' <code>netns</code> ce conteneur de <code>pause</code> . <br><br><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.1", "name": "bridge", "type": "bridge", "bridge": "cnio0", "isGateway": true, "ipMasq": true, "ipam": { "type": "host-local", "ranges": [ [{"subnet": "${POD_CIDR}"}] ], "routes": [{"dst": "0.0.0.0/0"}] } }</code> </pre> <br>  La <a href="">configuration CNI utilisée</a> indique l'utilisation du plugin de <code>bridge</code> pour configurer le pont logiciel Linux (L2) dans l'espace de noms racine appelé <code>cnio0</code> (le <a href="">nom par défaut</a> est <code>cni0</code> ), qui agit comme une passerelle ( <code>"isGateway": true</code> ). <br><br><img src="https://habrastorage.org/webt/bo/to/jp/botojpqu0f7fascfrbk-gen27a8.png"><br><br>  Une paire de veth sera également configurée pour connecter le foyer au pont nouvellement créé: <br><br><img src="https://habrastorage.org/webt/-6/tt/e7/-6tte7essirvuraiuypuln_syvm.png"><br><br>  Pour attribuer des informations L3, telles que des adresses IP, le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">plug</a> - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">in IPAM</a> ( <code>ipam</code> ) est appelé.  Dans ce cas, le type <code>host-local</code> est utilisé, "qui stocke l'état localement sur le système de fichiers hôte, ce qui garantit l'unicité des adresses IP sur un hôte" <i>(d'après la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code> host-local</code></a> )</i> .  Le plug-in IPAM renvoie ces informations au plug-in précédent ( <code>bridge</code> ), afin que toutes les routes spécifiées dans la configuration puissent être configurées ( <code>"routes": [{"dst": "0.0.0.0/0"}]</code> ).  Si <code>gw</code> pas spécifié, il <a href="">est extrait du sous-réseau</a> .  La route par défaut est également configurée dans l'espace de noms réseau du foyer, pointant vers le pont (qui est configuré comme le premier sous-réseau IP du foyer). <br><br>  Et le dernier détail important: nous avons demandé le <code>"ipMasq": true</code> ( <code>"ipMasq": true</code> ) pour le trafic provenant du réseau de foyer.  Nous n'avons pas vraiment besoin de NAT ici, mais voici la configuration de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubernetes The Hard Way</a> .  Par conséquent, pour être complet, je dois mentionner que les entrées dans les <code>iptables</code> plugin <code>bridge</code> sont configurées pour cet exemple particulier.  Tous les paquets du foyer, dont le destinataire ne se trouve pas dans la plage <code>224.0.0.0/4</code> , <a href="">seront derrière NAT</a> , ce qui ne répond pas tout à fait à l'exigence "tous les conteneurs peuvent communiquer avec tout autre conteneur sans utiliser NAT."  Eh bien, nous prouverons pourquoi le NAT n'est pas nécessaire ... <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/webt/y-/hy/ub/y-hyubecllmzx9go5ehai4shl78.jpeg"></a> <br><br><h3>  Acheminement du foyer </h3><br>  Nous sommes maintenant prêts à personnaliser les pods.  Examinons tous les espaces réseau des noms de l'un des nœuds de travail et analysons l'un d'eux après avoir créé le déploiement de <code>nginx</code> <a href="">partir d'ici</a> .  Nous utiliserons <code>lsns</code> avec l'option <code>-t</code> pour sélectionner le type d'espace de noms souhaité (c'est-à-dire <code>net</code> ): <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo lsns -t net NS TYPE NPROCS PID USER COMMAND 4026532089 net 113 1 root /sbin/init 4026532280 net 2 8046 root /pause 4026532352 net 4 16455 root /pause 4026532426 net 3 27255 root /pause</code> </pre> <br>  En utilisant l'option <code>-i</code> pour <code>ls</code> nous pouvons trouver leurs numéros d'inode: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ls -1i /var/run/netns 4026532352 cni-1d85bb0c-7c61-fd9f-2adc-f6e98f7a58af 4026532280 cni-7cec0838-f50c-416a-3b45-628a4237c55c 4026532426 cni-912bcc63-712d-1c84-89a7-9e10510808a0</code> </pre> <br>  Vous pouvez également répertorier tous les espaces de noms réseau à l'aide d' <code>ip netns</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip netns cni-912bcc63-712d-1c84-89a7-9e10510808a0 (id: 2) cni-1d85bb0c-7c61-fd9f-2adc-f6e98f7a58af (id: 1) cni-7cec0838-f50c-416a-3b45-628a4237c55c (id: 0)</code> </pre> <br>  Pour voir tous les processus en cours d'exécution dans l'espace réseau <code>cni-912bcc63–712d-1c84–89a7–9e10510808a0</code> ( <code>4026532426</code> ), vous pouvez exécuter, par exemple, la commande suivante: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ls -l /proc/[1-9]*/ns/net | grep 4026532426 | cut -f3 -d<span class="hljs-string"><span class="hljs-string">"/"</span></span> | xargs ps -p PID TTY STAT TIME COMMAND 27255 ? Ss 0:00 /pause 27331 ? Ss 0:00 nginx: master process nginx -g daemon off; 27355 ? S 0:00 nginx: worker process</code> </pre> <br>  On peut voir qu'en plus de faire une <code>pause</code> dans ce pod, nous avons lancé <code>nginx</code> .  Le conteneur de <code>pause</code> partage les espaces de noms <code>net</code> et <code>ipc</code> avec tous les autres conteneurs de pod.  Rappelez-vous le PID de la <code>pause</code> - 27255;  nous y reviendrons. <br><br>  Voyons maintenant ce que <code>kubectl</code> raconte à propos de ce pod: <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide | grep nginx nginx-65899c769f-wxdx6 1/1 Running 0 5d 10.200.0.4 worker-0</code> </pre> <br>  Plus de détails: <br><br><pre> <code class="bash hljs">$ kubectl describe pods nginx-65899c769f-wxdx6</code> </pre> <br><pre> <code class="plaintext hljs">Name: nginx-65899c769f-wxdx6 Namespace: default Node: worker-0/10.240.0.20 Start Time: Thu, 05 Jul 2018 14:20:06 -0400 Labels: pod-template-hash=2145573259 run=nginx Annotations: &lt;none&gt; Status: Running IP: 10.200.0.4 Controlled By: ReplicaSet/nginx-65899c769f Containers: nginx: Container ID: containerd://4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 Image: nginx ...</code> </pre> <br>  Nous voyons le nom du pod - <code>nginx-65899c769f-wxdx6</code> - et l'ID de l'un de ses conteneurs ( <code>nginx</code> ), mais rien n'a été dit à propos de la <code>pause</code> .  Creusez un nœud de travail plus profond pour faire correspondre toutes les données.  N'oubliez pas que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubernetes The Hard Way</a> n'utilise pas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Docker</a> , donc pour plus de détails sur le conteneur, nous nous référons à l'utilitaire de console <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">containerd</a> - ctr <i>(voir également l'article " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Intégration de containerd avec Kubernetes, remplaçant Docker, prêt pour la production</a> " - <b>transfert environ</b> )</i> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr namespaces ls NAME LABELS k8s.io</code> </pre> <br>  Connaissant l' <code>k8s.io</code> containerd ( <code>k8s.io</code> ), vous pouvez obtenir l'ID du conteneur <code>nginx</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers ls | grep nginx 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 docker.io/library/nginx:latest io.containerd.runtime.v1.linux</code> </pre> <br>  ... et <code>pause</code> une <code>pause</code> aussi: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers ls | grep pause 0866803b612f2f55e7b6b83836bde09bd6530246239b7bde1e49c04c7038e43a k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux 21640aea0210b320fd637c22ff93b7e21473178de0073b05de83f3b116fc8834 k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux</code> </pre> <br>  L'ID du conteneur <code>nginx</code> terminant par <code>…983c7</code> correspond à ce que nous avons obtenu de <code>kubectl</code> .  Voyons si nous pouvons déterminer quel conteneur de <code>pause</code> appartient au pod <code>nginx</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io task ls TASK PID STATUS ... d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 27255 RUNNING 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 27331 RUNNING</code> </pre> <br>  N'oubliez pas que les processus avec les PID 27331 et 27355 s'exécutent dans l'espace de noms réseau <code>cni-912bcc63–712d-1c84–89a7–9e10510808a0</code> ? <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers info d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 { <span class="hljs-string"><span class="hljs-string">"ID"</span></span>: <span class="hljs-string"><span class="hljs-string">"d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6"</span></span>, <span class="hljs-string"><span class="hljs-string">"Labels"</span></span>: { <span class="hljs-string"><span class="hljs-string">"io.cri-containerd.kind"</span></span>: <span class="hljs-string"><span class="hljs-string">"sandbox"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx-65899c769f-wxdx6"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.namespace"</span></span>: <span class="hljs-string"><span class="hljs-string">"default"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.uid"</span></span>: <span class="hljs-string"><span class="hljs-string">"0b35e956-8080-11e8-8aa9-0a12b8818382"</span></span>, <span class="hljs-string"><span class="hljs-string">"pod-template-hash"</span></span>: <span class="hljs-string"><span class="hljs-string">"2145573259"</span></span>, <span class="hljs-string"><span class="hljs-string">"run"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx"</span></span> }, <span class="hljs-string"><span class="hljs-string">"Image"</span></span>: <span class="hljs-string"><span class="hljs-string">"k8s.gcr.io/pause:3.1"</span></span>, ...</code> </pre> <br>  ... et: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers info 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 { <span class="hljs-string"><span class="hljs-string">"ID"</span></span>: <span class="hljs-string"><span class="hljs-string">"4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7"</span></span>, <span class="hljs-string"><span class="hljs-string">"Labels"</span></span>: { <span class="hljs-string"><span class="hljs-string">"io.cri-containerd.kind"</span></span>: <span class="hljs-string"><span class="hljs-string">"container"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.container.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx-65899c769f-wxdx6"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.namespace"</span></span>: <span class="hljs-string"><span class="hljs-string">"default"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.uid"</span></span>: <span class="hljs-string"><span class="hljs-string">"0b35e956-8080-11e8-8aa9-0a12b8818382"</span></span> }, <span class="hljs-string"><span class="hljs-string">"Image"</span></span>: <span class="hljs-string"><span class="hljs-string">"docker.io/library/nginx:latest"</span></span>, ...</code> </pre> <br>  Nous savons maintenant avec certitude quels conteneurs s'exécutent dans ce module ( <code>nginx-65899c769f-wxdx6</code> ) et l'espace de noms réseau ( <code>cni-912bcc63–712d-1c84–89a7–9e10510808a0</code> ): <br><br><ul><li>  nginx (ID: <code>4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7</code> ); </li><li>  pause (ID: <code>d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6</code> ). </li></ul><br><img src="https://habrastorage.org/webt/3h/cx/qq/3hcxqqv-mwlrm8ax9lu9jl0fixy.png"><br><br>  Comment est-ce sous ( <code>nginx-65899c769f-wxdx6</code> ) connecté au réseau?  Nous utilisons le PID 27255 précédemment reçu de <code>pause</code> pour exécuter des commandes dans son espace de noms réseau ( <code>cni-912bcc63–712d-1c84–89a7–9e10510808a0</code> ): <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns identify 27255 cni-912bcc63-712d-1c84-89a7-9e10510808a0</code> </pre> <br>  À ces fins, nous utiliserons <code>nsenter</code> avec l'option <code>-t</code> qui définit le PID cible et <code>-n</code> sans spécifier de fichier pour entrer dans l'espace de noms réseau du processus cible (27255).  Voici ce que dira l' <code>ip link show</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo nsenter -t 27255 -n ip link show 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 3: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 0a:58:0a:c8:00:04 brd ff:ff:ff:ff:ff:ff link-netnsid 0</code> </pre> <br>  ... et <code>ifconfig eth0</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo nsenter -t 27255 -n ifconfig eth0 eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 10.200.0.4 netmask 255.255.255.0 broadcast 0.0.0.0 inet6 fe80::2097:51ff:fe39:ec21 prefixlen 64 scopeid 0x20&lt;link&gt; ether 0a:58:0a:c8:00:04 txqueuelen 0 (Ethernet) RX packets 540 bytes 42247 (42.2 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 177 bytes 16530 (16.5 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0</code> </pre> <br>  Cela confirme que l'adresse IP obtenue précédemment via <code>kubectl get pod</code> est configurée sur l'interface <code>eth0</code> .  Cette interface fait partie d'une <b>paire de Veth</b> , dont une extrémité se trouve dans le foyer et l'autre dans l'espace de noms racine.  Pour découvrir l'interface de la seconde extrémité, nous utilisons <code>ethtool</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> cni-912bcc63-712d-1c84-89a7-9e10510808a0 ethtool -S eth0 NIC statistics: peer_ifindex: 7</code> </pre> <br>  Nous voyons que <code>ifindex</code> fête est 7. Vérifiez qu'il se trouve dans l'espace de noms racine.  Cela peut être fait en utilisant <code>ip link</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip link | grep <span class="hljs-string"><span class="hljs-string">'^7:'</span></span> 7: veth71f7d238@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master cnio0 state UP mode DEFAULT group default</code> </pre> <br>  Pour en être sûr enfin, voyons: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo cat /sys/class/net/veth71f7d238/ifindex 7</code> </pre> <br>  Génial, maintenant tout est clair avec le lien virtuel.  En utilisant <code>brctl</code> voyons qui d'autre est connecté au pont Linux: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ brctl show cnio0 bridge name bridge id STP enabled interfaces cnio0 8000.0a580ac80001 no veth71f7d238 veth73f35410 vethf273b35f</code> </pre> <br>  Donc, l'image est la suivante: <br><br><img src="https://habrastorage.org/webt/yu/gc/t6/yugct6efi7ztep277en4msjuzv4.png"><br><br><h3>  Vérification du routage </h3><br>  Comment transmettons-nous réellement le trafic?  Regardons la table de routage dans le pod d'espace de noms de réseau: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> cni-912bcc63-712d-1c84-89a7-9e10510808a0 ip route show default via 10.200.0.1 dev eth0 10.200.0.0/24 dev eth0 proto kernel scope link src 10.200.0.4</code> </pre> <br>  Au moins, nous savons comment accéder à l'espace de noms racine ( <code>default via 10.200.0.1</code> ).  Voyons maintenant la table de routage de l'hôte: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip route list default via 10.240.0.1 dev eth0 proto dhcp src 10.240.0.20 metric 100 10.200.0.0/24 dev cnio0 proto kernel scope link src 10.200.0.1 10.240.0.0/24 dev eth0 proto kernel scope link src 10.240.0.20 10.240.0.1 dev eth0 proto dhcp scope link src 10.240.0.20 metric 100</code> </pre> <br>  Nous savons comment transmettre des paquets à un routeur VPC (VPC <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">a un</a> routeur «implicite», qui a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">généralement une deuxième adresse à</a> partir de l'espace d'adressage IP principal du sous-réseau).  Maintenant: le routeur VPC sait-il comment se rendre au réseau de chaque foyer?  Non, il ne le fait pas, donc on suppose que les routes seront configurées par le plugin CNI ou <a href="">manuellement</a> (comme dans le manuel).  Apparemment, le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">plugin AWS CNI</a> fait exactement cela pour nous chez AWS.  N'oubliez pas qu'il existe de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">nombreux plugins CNI</a> , et nous envisageons un exemple de <b>configuration réseau simple</b> : <br><br><img src="https://habrastorage.org/webt/cn/v7/v_/cnv7v_qjfkidbtuljkbgkuzuaag.png"><br><br><h3>  Immersion profonde dans NAT </h3><br>  <code>kubectl create -f busybox.yaml</code> créez deux conteneurs <code>busybox</code> identiques avec Replication Controller: <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: ReplicationController metadata: name: busybox0 labels: app: busybox0 spec: replicas: 2 selector: app: busybox0 template: metadata: name: busybox0 labels: app: busybox0 spec: containers: - image: busybox command: - sleep - "3600" imagePullPolicy: IfNotPresent name: busybox restartPolicy: Always</code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>busybox.yaml</code></a> ) <br><br>  Nous obtenons: <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE busybox0-g6pww 1/1 Running 0 4s 10.200.1.15 worker-1 busybox0-rw89s 1/1 Running 0 4s 10.200.0.21 worker-0 ...</code> </pre> <br>  Les pings d'un conteneur à un autre doivent réussir: <br><br><pre> <code class="bash hljs">$ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it busybox0-rw89s -- ping -c 2 10.200.1.15 PING 10.200.1.15 (10.200.1.15): 56 data bytes 64 bytes from 10.200.1.15: seq=0 ttl=62 time=0.528 ms 64 bytes from 10.200.1.15: seq=1 ttl=62 time=0.440 ms --- 10.200.1.15 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.440/0.484/0.528 ms</code> </pre> <br>  Pour comprendre le mouvement du trafic, vous pouvez regarder les paquets en utilisant <code>tcpdump</code> ou <code>conntrack</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 10.200.1.15 icmp 1 29 src=10.200.0.21 dst=10.200.1.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1280 src=10.200.1.15 dst=10.240.0.20 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1280 mark=0 use=1</code> </pre> <br>  L'IP source du pod 10.200.0.21 est traduite en adresse IP de l'hôte 10.240.0.20. <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 10.200.1.15 icmp 1 28 src=10.240.0.20 dst=10.200.1.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1280 src=10.200.1.15 dst=10.240.0.20 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1280 mark=0 use=1</code> </pre> <br>  Dans iptables, vous pouvez voir que les nombres augmentent: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo iptables -t nat -Z POSTROUTING -L -v Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> out <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> destination ... 5 324 CNI-be726a77f15ea47ff32947a3 all -- any any 10.200.0.0/24 anywhere /* name: <span class="hljs-string"><span class="hljs-string">"bridge"</span></span> id: <span class="hljs-string"><span class="hljs-string">"631cab5de5565cc432a3beca0e2aece0cef9285482b11f3eb0b46c134e457854"</span></span> */ Zeroing chain `POSTROUTING<span class="hljs-string"><span class="hljs-string">'</span></span></code> </pre> <br>  D'un autre côté, si vous supprimez <code>"ipMasq": true</code> de la configuration du plugin CNI, vous pouvez voir ce qui suit (cette opération est effectuée exclusivement à des fins éducatives - nous ne recommandons pas de changer la configuration sur un cluster de travail!): <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE busybox0-2btxn 1/1 Running 0 16s 10.200.0.15 worker-0 busybox0-dhpx8 1/1 Running 0 16s 10.200.1.13 worker-1 ...</code> </pre> <br>  Ping doit toujours passer: <br><br><pre> <code class="bash hljs">$ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it busybox0-2btxn -- ping -c 2 10.200.1.13 PING 10.200.1.6 (10.200.1.6): 56 data bytes 64 bytes from 10.200.1.6: seq=0 ttl=62 time=0.515 ms 64 bytes from 10.200.1.6: seq=1 ttl=62 time=0.427 ms --- 10.200.1.6 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.427/0.471/0.515 ms</code> </pre> <br>  Et dans ce cas - sans utiliser NAT: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 10.200.1.13 icmp 1 29 src=10.200.0.15 dst=10.200.1.13 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1792 src=10.200.1.13 dst=10.200.0.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1792 mark=0 use=1</code> </pre> <br>  Nous avons donc vérifié que «tous les conteneurs peuvent communiquer avec n'importe quel autre conteneur sans utiliser NAT». <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 10.200.1.13 icmp 1 27 src=10.200.0.15 dst=10.200.1.13 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1792 src=10.200.1.13 dst=10.200.0.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1792 mark=0 use=1</code> </pre> <br><h2>  Réseau de clusters (10.32.0.0/24) </h2><br>  Vous avez peut-être remarqué dans l'exemple de la <code>busybox</code> que les adresses IP attribuées à la <code>busybox</code> étaient différentes dans chaque cas.  Et si nous voulions rendre ces conteneurs disponibles pour la communication d'autres foyers?  On pourrait prendre les adresses IP actuelles du pod, mais elles changeront.  Pour cette raison, vous devez configurer la ressource <code>Service</code> , qui procurera des requêtes par proxy à de nombreux foyers de courte durée. <br><br><blockquote>  «Le service dans Kubernetes est une abstraction qui définit l'ensemble logique des foyers et les politiques par lesquelles ils sont accessibles.»  <i>(à partir de la documentation des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">services Kubernetes</a> )</i> </blockquote><br>  Il existe différentes façons de publier un service;  le type par défaut est <code>ClusterIP</code> , qui définit l'adresse IP à partir du bloc CIDR du cluster (c'est-à-dire accessible uniquement à partir du cluster).  Un tel exemple est le module complémentaire de cluster DNS configuré dans Kubernetes The Hard Way. <br><br><pre> <code class="plaintext hljs"># ... apiVersion: v1 kind: Service metadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: "KubeDNS" spec: selector: k8s-app: kube-dns clusterIP: 10.32.0.10 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP # ...</code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>kube-dns.yaml</code></a> ) <br><br>  <code>kubectl</code> montre que le <code>Service</code> souvient des points de terminaison et les traduit: <br><br><pre> <code class="bash hljs">$ kubectl -n kube-system describe services ... Selector: k8s-app=kube-dns Type: ClusterIP IP: 10.32.0.10 Port: dns 53/UDP TargetPort: 53/UDP Endpoints: 10.200.0.27:53 Port: dns-tcp 53/TCP TargetPort: 53/TCP Endpoints: 10.200.0.27:53 ...</code> </pre> <br>  Comment exactement? .. <code>iptables</code> nouveau.  Passons en revue les règles créées pour cet exemple.  Leur liste complète peut être consultée avec la commande <code>iptables-save</code> . <br><br>  Dès que les paquets sont créés par le processus ( <code>OUTPUT</code> ) ou arrivent sur l'interface réseau ( <code>PREROUTING</code> ), ils passent par les chaînes <code>iptables</code> suivantes: <br><br><pre> <code class="bash hljs">-A PREROUTING -m comment --comment <span class="hljs-string"><span class="hljs-string">"kubernetes service portals"</span></span> -j KUBE-SERVICES -A OUTPUT -m comment --comment <span class="hljs-string"><span class="hljs-string">"kubernetes service portals"</span></span> -j KUBE-SERVICES</code> </pre> <br>  Les cibles suivantes correspondent aux paquets TCP envoyés au 53e port à 10.32.0.10 et sont transmises au destinataire 10.200.0.27 avec le 53e port: <br><br><pre> <code class="bash hljs">-A KUBE-SERVICES -d 10.32.0.10/32 -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp cluster IP"</span></span> -m tcp --dport 53 -j KUBE-SVC-ERIFXISQEP7F7OF4 -A KUBE-SVC-ERIFXISQEP7F7OF4 -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp"</span></span> -j KUBE-SEP-32LPCMGYG6ODGN3H -A KUBE-SEP-32LPCMGYG6ODGN3H -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp"</span></span> -m tcp -j DNAT --to-destination 10.200.0.27:53</code> </pre> <br>  De même pour les paquets UDP (destinataire 10.32.0.10:53 → 10.200.0.27:53): <br><br><pre> <code class="bash hljs">-A KUBE-SERVICES -d 10.32.0.10/32 -p udp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns cluster IP"</span></span> -m udp --dport 53 -j KUBE-SVC-TCOU7JCQXEZGVUNU -A KUBE-SVC-TCOU7JCQXEZGVUNU -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns"</span></span> -j KUBE-SEP-LRUTK6XRXU43VLIG -A KUBE-SEP-LRUTK6XRXU43VLIG -p udp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns"</span></span> -m udp -j DNAT --to-destination 10.200.0.27:53</code> </pre> <br>  Il existe d'autres types de <code>Services</code> dans Kubernetes.  En particulier, Kubernetes The Hard Way <code>NodePort</code> de <code>NodePort</code> - voir <a href="">Smoke Test: Services</a> . <br><br><pre> <code class="bash hljs">kubectl expose deployment nginx --port 80 --<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> NodePort</code> </pre> <br>  <code>NodePort</code> publie le service sur l'adresse IP de chaque nœud, en le plaçant sur un port statique (il s'appelle <code>NodePort</code> ).  <code>NodePort</code> est <code>NodePort</code> accessible depuis l'extérieur du cluster.  Vous pouvez vérifier le port dédié (dans ce cas - 31088) en utilisant <code>kubectl</code> : <br><br><pre> <code class="bash hljs">$ kubectl describe services nginx ... Type: NodePort IP: 10.32.0.53 Port: &lt;<span class="hljs-built_in"><span class="hljs-built_in">unset</span></span>&gt; 80/TCP TargetPort: 80/TCP NodePort: &lt;<span class="hljs-built_in"><span class="hljs-built_in">unset</span></span>&gt; 31088/TCP Endpoints: 10.200.1.18:80 ...</code> </pre> <br>  Under est désormais disponible sur Internet sous le nom <code>http://${EXTERNAL_IP}:31088/</code> .  Ici, <code>EXTERNAL_IP</code> est l'adresse IP publique de <b>toute instance de travail</b> .  Dans cet exemple, j'ai utilisé l'adresse IP publique de <b>worker-0</b> .  La demande est reçue par un hôte avec une adresse IP interne de 10.240.0.20 (le fournisseur de cloud est engagé dans le NAT public), cependant, le service est réellement démarré sur un autre hôte ( <b>travailleur-1</b> , qui peut être vu par l'adresse IP du point final - 10.200.1.18): <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 31088 tcp 6 86397 ESTABLISHED src=173.38.XXX.XXX dst=10.240.0.20 sport=30303 dport=31088 src=10.200.1.18 dst=10.240.0.20 sport=80 dport=30303 [ASSURED] mark=0 use=1</code> </pre> <br>  Le paquet est envoyé de <b>travailleur-0</b> à <b>travailleur-1</b> , où il trouve son destinataire: <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 80 tcp 6 86392 ESTABLISHED src=10.240.0.20 dst=10.200.1.18 sport=14802 dport=80 src=10.200.1.18 dst=10.240.0.20 sport=80 dport=14802 [ASSURED] mark=0 use=1</code> </pre> <br>  Un tel circuit est-il idéal?  Peut-être pas, mais ça marche.  Dans ce cas, les règles programmées <code>iptables</code> sont les suivantes: <br><br><pre> <code class="bash hljs">-A KUBE-NODEPORTS -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -m tcp --dport 31088 -j KUBE-SVC-4N57TFCL4MD7ZTDA -A KUBE-SVC-4N57TFCL4MD7ZTDA -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -j KUBE-SEP-UGTFMET44DQG7H7H -A KUBE-SEP-UGTFMET44DQG7H7H -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -m tcp -j DNAT --to-destination 10.200.1.18:80</code> </pre> <br>  En d'autres termes, l'adresse du destinataire des paquets avec le port 31088 est diffusée le 10.200.1.18.  Le port diffuse également, de 31088 à 80. <br><br>  Nous n'avons pas <code>LoadBalancer</code> un autre type de service - <code>LoadBalancer</code> - qui rend le service accessible au public à l'aide d'un équilibreur de charge de fournisseur de cloud, mais l'article s'est déjà révélé volumineux. <br><br><h2>  Conclusion </h2><br>  Il peut sembler qu'il y a beaucoup d'informations, mais nous n'avons touché que la pointe de l'iceberg.  À l'avenir, je vais parler d'IPv6, IPVS, eBPF et de quelques plugins CNI actuels intéressants. <br><br><h2>  PS du traducteur </h2><br>  Lisez aussi dans notre blog: <br><br><ul><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Guide illustré du réseautage chez Kubernetes</a> »; </li><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Comparaison des performances réseau pour Kubernetes</a> »; </li><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Expériences avec kube-proxy et inaccessibilité de l'hôte dans Kubernetes</a> »; </li><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Améliorer la fiabilité de Kubernetes: comment remarquer rapidement qu'un nœud est tombé</a> »; </li><li> « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Play with Kubernetes —      K8s</a> »; </li><li> « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">   Kubernetes   </a> » <i>( ,        Kubernetes)</i> ; </li><li> « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Container Networking Interface (CNI) —      Linux-</a> ». </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr420813/">https://habr.com/ru/post/fr420813/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr420799/index.html">MPS 2018.2: tests de générateur, plug-in GitHub, aspect VCS, notifications de migration, etc.</a></li>
<li><a href="../fr420803/index.html">Cours d'impression 3D. Économie de plastique lors de l'impression de modèles non fonctionnels à partir de 3Dtool</a></li>
<li><a href="../fr420805/index.html">[Français] Quand utiliser des flux parallèles</a></li>
<li><a href="../fr420809/index.html">Semaine de la sécurité 31: cinquante nuances d'insécurité sur Android</a></li>
<li><a href="../fr420811/index.html">Réseau de messagerie et de téléphonie décentralisé de nouvelle génération</a></li>
<li><a href="../fr420815/index.html">Comment le "décodage du monde numérique" a explosé: les 10 meilleurs rapports de DotNext 2018 Piter</a></li>
<li><a href="../fr420819/index.html">Top 10 des outils Python pour l'apprentissage automatique et la science des données</a></li>
<li><a href="../fr420821/index.html">Règle 10: 1 en programmation et en écriture</a></li>
<li><a href="../fr420825/index.html">Aujourd'hui sera le premier match entre OpenAI et Dota 2 professionnels (personnes gagnées). Nous comprenons comment fonctionne le bot</a></li>
<li><a href="../fr420827/index.html">Créez un projet maven simple à l'aide de Java EE + WildFly10 + JPA (Hibernate) + Postgresql + EJB + IntelliJ IDEA</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>