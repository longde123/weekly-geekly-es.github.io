<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚òéÔ∏è üë®üèæ‚Äçüéì üÜö R√©seaux dans les coulisses de Kubernetes ü§ôüèø ü§¢ üë©üèø‚Äçüíº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Remarque perev. : L'auteur de l'article original, Nicolas Leiva, est un architecte de solutions Cisco qui a d√©cid√© de partager avec ses coll√®gues, ing...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>R√©seaux dans les coulisses de Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/420813/"> <i><b>Remarque</b></i>  <i><b>perev.</b></i>  <i>: L'auteur de l'article original, Nicolas Leiva, est un architecte de solutions Cisco qui a d√©cid√© de partager avec ses coll√®gues, ing√©nieurs r√©seau, comment le r√©seau Kubernetes fonctionne de l'int√©rieur.</i>  <i>Pour ce faire, il explore sa configuration la plus simple dans le cluster, en appliquant activement le bon sens, sa connaissance des r√©seaux et des utilitaires Linux / Kubernetes standard.</i>  <i>Cela s'est av√©r√© volumineux, mais tr√®s clair.</i> <br><br><img src="https://habrastorage.org/webt/gr/qw/d4/grqwd4putslwaijltw9yojfzjes.png"><br><br>  En plus du fait que le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">guide Kubernetes The Hard Way de</a> Kelsey Hightower fonctionne ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">m√™me sur AWS!</a> ), J'ai aim√© que le r√©seau soit maintenu propre et simple;  et c'est une excellente occasion de comprendre le r√¥le, par exemple, de l'interface r√©seau de conteneurs ( <a href="">CNI</a> ).  Cela dit, j'ajouterai que le r√©seau Kubernetes n'est pas vraiment tr√®s intuitif, surtout pour les d√©butants ... et n'oubliez pas non plus "qu'il n'y a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tout simplement pas</a> de r√©seau de conteneurs". <a name="habracut"></a><br><br>  Bien qu'il existe d√©j√† de bons documents sur ce sujet (voir les liens <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> ), je n'ai pas pu trouver un tel exemple que je combinerais tout ce qui est n√©cessaire avec les conclusions des √©quipes que les ing√©nieurs de r√©seau aiment et d√©testent, d√©montrant ce qui se passe r√©ellement dans les coulisses.  Par cons√©quent, j'ai d√©cid√© de collecter des informations √† partir de nombreuses sources - j'esp√®re que cela vous aide et que vous comprenez mieux comment tout est connect√© les uns aux autres.  Cette connaissance est importante non seulement pour vous tester, mais aussi pour simplifier le processus de diagnostic des probl√®mes.  Vous pouvez suivre l'exemple dans votre cluster de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubernetes The Hard Way</a> : toutes les adresses IP et tous les param√®tres sont extraits de l√† (√† partir des validations de mai 2018, avant d'utiliser <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">les conteneurs Nabla</a> ). <br><br>  Et nous commencerons par la fin, lorsque nous aurons trois contr√¥leurs et trois n≈ìuds de travail: <br><br><img src="https://habrastorage.org/webt/am/vs/6j/amvs6jnhsuxzwhyoyod6vjk8cby.png"><br><br>  Vous remarquerez peut-√™tre qu'il y a √©galement au moins trois sous-r√©seaux priv√©s ici!  Un peu de patience, et ils seront tous pris en consid√©ration.  N'oubliez pas que m√™me si nous nous r√©f√©rons √† des pr√©fixes IP tr√®s sp√©cifiques, ils sont simplement extraits de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubernetes The Hard Way</a> , de sorte qu'ils n'ont qu'une signification locale, et vous √™tes libre de choisir tout autre bloc d'adresses pour votre environnement conform√©ment √† la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">RFC 1918</a> .  Pour le cas d'IPv6, il y aura un article de blog s√©par√©. <br><br><h2>  R√©seau h√¥te (10.240.0.0/24) </h2><br>  Il s'agit d'un r√©seau interne dont tous les n≈ìuds font partie.  D√©fini par l' <code>--private-network-ip</code> dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">GCP</a> ou l' <code>--private-ip-address</code> dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">AWS</a> lors de l'allocation des ressources informatiques. <br><br><h3>  Initialisation des n≈ìuds de contr√¥leur dans GCP </h3><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 0 1 2; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> gcloud compute instances create controller-<span class="hljs-variable"><span class="hljs-variable">${i}</span></span> \ <span class="hljs-comment"><span class="hljs-comment"># ... --private-network-ip 10.240.0.1${i} \ # ... done</span></span></code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>controllers_gcp.sh</code></a> ) <br><br><h3>  Initialisation des n≈ìuds de contr√¥leur dans AWS </h3><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 0 1 2; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-built_in"><span class="hljs-built_in">declare</span></span> controller_id<span class="hljs-variable"><span class="hljs-variable">${i}</span></span>=`aws ec2 run-instances \ <span class="hljs-comment"><span class="hljs-comment"># ... --private-ip-address 10.240.0.1${i} \ # ... done</span></span></code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>controllers_aws.sh</code></a> ) <br><br><img src="https://habrastorage.org/webt/gt/cj/p6/gtcjp6fgkqbv1nvs2ea9d9hhueo.png"><br><br>  Chaque instance aura deux adresses IP: priv√©e du r√©seau h√¥te (contr√¥leurs - <code>10.240.0.1${i}/24</code> , travailleurs - <code>10.240.0.2${i}/24</code> ) et publique, d√©sign√©e par le fournisseur de cloud, dont nous parlerons plus tard comment se rendre √† <code>NodePorts</code> . <br><br><h3>  Gcp </h3><br><pre> <code class="bash hljs">$ gcloud compute instances list NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS controller-0 us-west1-c n1-standard-1 10.240.0.10 35.231.XXX.XXX RUNNING worker-1 us-west1-c n1-standard-1 10.240.0.21 35.231.XX.XXX RUNNING ...</code> </pre> <br><br><h3>  Aws </h3><br><pre> <code class="bash hljs">$ aws ec2 describe-instances --query <span class="hljs-string"><span class="hljs-string">'Reservations[].Instances[].[Tags[?Key==`Name`].Value[],PrivateIpAddress,PublicIpAddress]'</span></span> --output text | sed <span class="hljs-string"><span class="hljs-string">'$!N;s/\n/ /'</span></span> 10.240.0.10 34.228.XX.XXX controller-0 10.240.0.21 34.173.XXX.XX worker-1 ...</code> </pre> <br>  Tous les n≈ìuds doivent pouvoir se pinguer si les <a href="">politiques de s√©curit√© sont correctes</a> (et si le <code>ping</code> install√© sur l'h√¥te). <br><br><h2>  R√©seau de foyer (10.200.0.0/16) </h2><br>  Il s'agit du r√©seau dans lequel vivent les modules.  Chaque n≈ìud de travail utilise un sous-r√©seau de ce r√©seau.  Dans notre cas, <code>POD_CIDR=10.200.${i}.0/24</code> pour le <code>worker-${i}</code> . <br><br><img src="https://habrastorage.org/webt/6i/yz/ih/6iyzihbxbzwugs4amvhfa9ysp5s.png"><br><br>  Pour comprendre comment tout est configur√©, prenez du recul et examinez <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le mod√®le de r√©seau Kubernetes</a> , qui n√©cessite les √©l√©ments suivants: <br><br><ul><li>  Tous les conteneurs peuvent communiquer avec tout autre conteneur sans utiliser NAT. </li><li>  Tous les n≈ìuds peuvent communiquer avec tous les conteneurs (et vice versa) sans utiliser NAT. </li><li>  L'IP que le conteneur voit doit √™tre la m√™me que les autres le voient. </li></ul><br>  Tout cela peut √™tre impl√©ment√© de plusieurs fa√ßons, et Kubernetes transmet la configuration r√©seau au <a href="">plugin CNI</a> . <br><br><blockquote>  ¬´Le plugin CNI est charg√© d'ajouter une interface r√©seau √† l' <b>espace de noms r√©seau</b> du conteneur (par exemple, une extr√©mit√© d'une <b>paire de veth</b> ) et d'apporter les modifications n√©cessaires sur l'h√¥te (par exemple, connecter la deuxi√®me extr√©mit√© de veth √† un pont).  Il doit ensuite attribuer une interface IP et configurer les routes conform√©ment √† la section Gestion des adresses IP en appelant le plugin IPAM souhait√©. ¬ª  <i>(√† partir de la <a href="">sp√©cification de l'interface r√©seau</a> du <a href="">conteneur</a> )</i> </blockquote><br><img src="https://habrastorage.org/webt/5q/fs/vw/5qfsvwg2iuduy0q3doco11hbf-g.png"><br><br><h3>  Espace de noms r√©seau </h3><br><blockquote>  ¬´L'espace de noms encapsule la ressource syst√®me globale dans une abstraction visible par les processus de cet espace de noms de telle sorte qu'ils aient leur propre instance isol√©e de la ressource globale.  Les modifications de la ressource globale sont visibles pour les autres processus inclus dans cet espace de noms, mais pas pour les autres processus. ¬ª  <i>(√† <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">partir de la page de manuel des espaces de noms</a> )</i> </blockquote><br>  Linux fournit sept espaces de noms diff√©rents ( <code>Cgroup</code> , <code>IPC</code> , <code>Network</code> , <code>Mount</code> , <code>PID</code> , <code>User</code> , <code>UTS</code> ).  Les espaces de noms r√©seau ( <code>CLONE_NEWNET</code> ) d√©finissent les ressources r√©seau disponibles pour le processus: ¬´Chaque espace de noms r√©seau poss√®de ses propres p√©riph√©riques r√©seau, adresses IP, tables de routage IP, <code>/proc/net</code> , num√©ros de port, etc.¬ª <i>( de l'article ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Espaces de noms en fonctionnement</a> ¬ª)</i> . <br><br><h3>  P√©riph√©riques Ethernet virtuels (Veth) </h3><br><blockquote>  ¬´Une paire de r√©seaux virtuels (veth) offre une abstraction sous la forme d'un¬´ canal ¬ª, qui peut √™tre utilis√© pour cr√©er des tunnels entre des espaces de noms r√©seau ou pour cr√©er un pont vers un p√©riph√©rique r√©seau physique dans un autre espace r√©seau.  Lorsque l'espace de noms est lib√©r√©, tous les p√©riph√©riques Veth qu'il contient sont d√©truits. ¬ª  <i>(√† partir de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">page de manuel des espaces de noms r√©seau</a> )</i> </blockquote><br>  Descendez au sol et voyez comment tout cela se rapporte au cluster.  Premi√®rement, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">les plugins r√©seau</a> de Kubernetes sont divers et les plugins CNI en font partie ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pourquoi pas CNM?</a> ).  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubelet</a> sur chaque n≈ìud indique au <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">runtime</a> du conteneur le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">plug-in r√©seau √†</a> utiliser.  L'interface <a href="">CNI</a> (Container Network Interface) se situe entre le runtime du conteneur et l'impl√©mentation du r√©seau.  Et d√©j√† le plugin CNI met en place le r√©seau. <br><br><blockquote>  ¬´Le plugin CNI est s√©lectionn√© en passant l' <code>--network-plugin=cni</code> ligne de commande <code>--network-plugin=cni</code> √† Kubelet.  Kubelet lit le fichier √† partir de <code>--cni-conf-dir</code> (la valeur par d√©faut est <code>/etc/cni/net.d</code> ) et utilise la configuration CNI de ce fichier pour configurer le r√©seau pour chaque fichier. "  <i>(√† partir des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">exigences du plugin r√©seau</a> )</i> </blockquote><br>  Les vrais fichiers binaires du plugin CNI sont dans <code>-- cni-bin-dir</code> (la valeur par d√©faut est <code>/opt/cni/bin</code> ). <br><br>  Veuillez noter que les <a href=""><code>kubelet.service</code></a> appel de <a href=""><code>kubelet.service</code></a> incluent <code>--network-plugin=cni</code> : <br><br><pre> <code class="plaintext hljs">[Service] ExecStart=/usr/local/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --network-plugin=cni \\ ...</code> </pre> <br>  Tout d'abord, Kubernetes cr√©e un espace de noms r√©seau pour le foyer, avant m√™me d'appeler des plugins.  Ceci est mis en ≈ìuvre en utilisant le conteneur de <code>pause</code> sp√©cial, qui "sert de" conteneur parent "pour tous les conteneurs de foyer" <i>(√† partir de l'article " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Le conteneur de pause tout-puissant</a> ")</i> .  Kubernetes ex√©cute ensuite le plugin CNI pour attacher le conteneur de <code>pause</code> au r√©seau.  Tous les conteneurs de pod utilisent l' <code>netns</code> ce conteneur de <code>pause</code> . <br><br><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.1", "name": "bridge", "type": "bridge", "bridge": "cnio0", "isGateway": true, "ipMasq": true, "ipam": { "type": "host-local", "ranges": [ [{"subnet": "${POD_CIDR}"}] ], "routes": [{"dst": "0.0.0.0/0"}] } }</code> </pre> <br>  La <a href="">configuration CNI utilis√©e</a> indique l'utilisation du plugin de <code>bridge</code> pour configurer le pont logiciel Linux (L2) dans l'espace de noms racine appel√© <code>cnio0</code> (le <a href="">nom par d√©faut</a> est <code>cni0</code> ), qui agit comme une passerelle ( <code>"isGateway": true</code> ). <br><br><img src="https://habrastorage.org/webt/bo/to/jp/botojpqu0f7fascfrbk-gen27a8.png"><br><br>  Une paire de veth sera √©galement configur√©e pour connecter le foyer au pont nouvellement cr√©√©: <br><br><img src="https://habrastorage.org/webt/-6/tt/e7/-6tte7essirvuraiuypuln_syvm.png"><br><br>  Pour attribuer des informations L3, telles que des adresses IP, le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">plug</a> - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">in IPAM</a> ( <code>ipam</code> ) est appel√©.  Dans ce cas, le type <code>host-local</code> est utilis√©, "qui stocke l'√©tat localement sur le syst√®me de fichiers h√¥te, ce qui garantit l'unicit√© des adresses IP sur un h√¥te" <i>(d'apr√®s la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code> host-local</code></a> )</i> .  Le plug-in IPAM renvoie ces informations au plug-in pr√©c√©dent ( <code>bridge</code> ), afin que toutes les routes sp√©cifi√©es dans la configuration puissent √™tre configur√©es ( <code>"routes": [{"dst": "0.0.0.0/0"}]</code> ).  Si <code>gw</code> pas sp√©cifi√©, il <a href="">est extrait du sous-r√©seau</a> .  La route par d√©faut est √©galement configur√©e dans l'espace de noms r√©seau du foyer, pointant vers le pont (qui est configur√© comme le premier sous-r√©seau IP du foyer). <br><br>  Et le dernier d√©tail important: nous avons demand√© le <code>"ipMasq": true</code> ( <code>"ipMasq": true</code> ) pour le trafic provenant du r√©seau de foyer.  Nous n'avons pas vraiment besoin de NAT ici, mais voici la configuration de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubernetes The Hard Way</a> .  Par cons√©quent, pour √™tre complet, je dois mentionner que les entr√©es dans les <code>iptables</code> plugin <code>bridge</code> sont configur√©es pour cet exemple particulier.  Tous les paquets du foyer, dont le destinataire ne se trouve pas dans la plage <code>224.0.0.0/4</code> , <a href="">seront derri√®re NAT</a> , ce qui ne r√©pond pas tout √† fait √† l'exigence "tous les conteneurs peuvent communiquer avec tout autre conteneur sans utiliser NAT."  Eh bien, nous prouverons pourquoi le NAT n'est pas n√©cessaire ... <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/webt/y-/hy/ub/y-hyubecllmzx9go5ehai4shl78.jpeg"></a> <br><br><h3>  Acheminement du foyer </h3><br>  Nous sommes maintenant pr√™ts √† personnaliser les pods.  Examinons tous les espaces r√©seau des noms de l'un des n≈ìuds de travail et analysons l'un d'eux apr√®s avoir cr√©√© le d√©ploiement de <code>nginx</code> <a href="">partir d'ici</a> .  Nous utiliserons <code>lsns</code> avec l'option <code>-t</code> pour s√©lectionner le type d'espace de noms souhait√© (c'est-√†-dire <code>net</code> ): <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo lsns -t net NS TYPE NPROCS PID USER COMMAND 4026532089 net 113 1 root /sbin/init 4026532280 net 2 8046 root /pause 4026532352 net 4 16455 root /pause 4026532426 net 3 27255 root /pause</code> </pre> <br>  En utilisant l'option <code>-i</code> pour <code>ls</code> nous pouvons trouver leurs num√©ros d'inode: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ls -1i /var/run/netns 4026532352 cni-1d85bb0c-7c61-fd9f-2adc-f6e98f7a58af 4026532280 cni-7cec0838-f50c-416a-3b45-628a4237c55c 4026532426 cni-912bcc63-712d-1c84-89a7-9e10510808a0</code> </pre> <br>  Vous pouvez √©galement r√©pertorier tous les espaces de noms r√©seau √† l'aide d' <code>ip netns</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip netns cni-912bcc63-712d-1c84-89a7-9e10510808a0 (id: 2) cni-1d85bb0c-7c61-fd9f-2adc-f6e98f7a58af (id: 1) cni-7cec0838-f50c-416a-3b45-628a4237c55c (id: 0)</code> </pre> <br>  Pour voir tous les processus en cours d'ex√©cution dans l'espace r√©seau <code>cni-912bcc63‚Äì712d-1c84‚Äì89a7‚Äì9e10510808a0</code> ( <code>4026532426</code> ), vous pouvez ex√©cuter, par exemple, la commande suivante: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ls -l /proc/[1-9]*/ns/net | grep 4026532426 | cut -f3 -d<span class="hljs-string"><span class="hljs-string">"/"</span></span> | xargs ps -p PID TTY STAT TIME COMMAND 27255 ? Ss 0:00 /pause 27331 ? Ss 0:00 nginx: master process nginx -g daemon off; 27355 ? S 0:00 nginx: worker process</code> </pre> <br>  On peut voir qu'en plus de faire une <code>pause</code> dans ce pod, nous avons lanc√© <code>nginx</code> .  Le conteneur de <code>pause</code> partage les espaces de noms <code>net</code> et <code>ipc</code> avec tous les autres conteneurs de pod.  Rappelez-vous le PID de la <code>pause</code> - 27255;  nous y reviendrons. <br><br>  Voyons maintenant ce que <code>kubectl</code> raconte √† propos de ce pod: <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide | grep nginx nginx-65899c769f-wxdx6 1/1 Running 0 5d 10.200.0.4 worker-0</code> </pre> <br>  Plus de d√©tails: <br><br><pre> <code class="bash hljs">$ kubectl describe pods nginx-65899c769f-wxdx6</code> </pre> <br><pre> <code class="plaintext hljs">Name: nginx-65899c769f-wxdx6 Namespace: default Node: worker-0/10.240.0.20 Start Time: Thu, 05 Jul 2018 14:20:06 -0400 Labels: pod-template-hash=2145573259 run=nginx Annotations: &lt;none&gt; Status: Running IP: 10.200.0.4 Controlled By: ReplicaSet/nginx-65899c769f Containers: nginx: Container ID: containerd://4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 Image: nginx ...</code> </pre> <br>  Nous voyons le nom du pod - <code>nginx-65899c769f-wxdx6</code> - et l'ID de l'un de ses conteneurs ( <code>nginx</code> ), mais rien n'a √©t√© dit √† propos de la <code>pause</code> .  Creusez un n≈ìud de travail plus profond pour faire correspondre toutes les donn√©es.  N'oubliez pas que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubernetes The Hard Way</a> n'utilise pas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Docker</a> , donc pour plus de d√©tails sur le conteneur, nous nous r√©f√©rons √† l'utilitaire de console <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">containerd</a> - ctr <i>(voir √©galement l'article " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Int√©gration de containerd avec Kubernetes, rempla√ßant Docker, pr√™t pour la production</a> " - <b>transfert environ</b> )</i> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr namespaces ls NAME LABELS k8s.io</code> </pre> <br>  Connaissant l' <code>k8s.io</code> containerd ( <code>k8s.io</code> ), vous pouvez obtenir l'ID du conteneur <code>nginx</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers ls | grep nginx 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 docker.io/library/nginx:latest io.containerd.runtime.v1.linux</code> </pre> <br>  ... et <code>pause</code> une <code>pause</code> aussi: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers ls | grep pause 0866803b612f2f55e7b6b83836bde09bd6530246239b7bde1e49c04c7038e43a k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux 21640aea0210b320fd637c22ff93b7e21473178de0073b05de83f3b116fc8834 k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux</code> </pre> <br>  L'ID du conteneur <code>nginx</code> terminant par <code>‚Ä¶983c7</code> correspond √† ce que nous avons obtenu de <code>kubectl</code> .  Voyons si nous pouvons d√©terminer quel conteneur de <code>pause</code> appartient au pod <code>nginx</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io task ls TASK PID STATUS ... d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 27255 RUNNING 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 27331 RUNNING</code> </pre> <br>  N'oubliez pas que les processus avec les PID 27331 et 27355 s'ex√©cutent dans l'espace de noms r√©seau <code>cni-912bcc63‚Äì712d-1c84‚Äì89a7‚Äì9e10510808a0</code> ? <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers info d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 { <span class="hljs-string"><span class="hljs-string">"ID"</span></span>: <span class="hljs-string"><span class="hljs-string">"d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6"</span></span>, <span class="hljs-string"><span class="hljs-string">"Labels"</span></span>: { <span class="hljs-string"><span class="hljs-string">"io.cri-containerd.kind"</span></span>: <span class="hljs-string"><span class="hljs-string">"sandbox"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx-65899c769f-wxdx6"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.namespace"</span></span>: <span class="hljs-string"><span class="hljs-string">"default"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.uid"</span></span>: <span class="hljs-string"><span class="hljs-string">"0b35e956-8080-11e8-8aa9-0a12b8818382"</span></span>, <span class="hljs-string"><span class="hljs-string">"pod-template-hash"</span></span>: <span class="hljs-string"><span class="hljs-string">"2145573259"</span></span>, <span class="hljs-string"><span class="hljs-string">"run"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx"</span></span> }, <span class="hljs-string"><span class="hljs-string">"Image"</span></span>: <span class="hljs-string"><span class="hljs-string">"k8s.gcr.io/pause:3.1"</span></span>, ...</code> </pre> <br>  ... et: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers info 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 { <span class="hljs-string"><span class="hljs-string">"ID"</span></span>: <span class="hljs-string"><span class="hljs-string">"4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7"</span></span>, <span class="hljs-string"><span class="hljs-string">"Labels"</span></span>: { <span class="hljs-string"><span class="hljs-string">"io.cri-containerd.kind"</span></span>: <span class="hljs-string"><span class="hljs-string">"container"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.container.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx-65899c769f-wxdx6"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.namespace"</span></span>: <span class="hljs-string"><span class="hljs-string">"default"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.uid"</span></span>: <span class="hljs-string"><span class="hljs-string">"0b35e956-8080-11e8-8aa9-0a12b8818382"</span></span> }, <span class="hljs-string"><span class="hljs-string">"Image"</span></span>: <span class="hljs-string"><span class="hljs-string">"docker.io/library/nginx:latest"</span></span>, ...</code> </pre> <br>  Nous savons maintenant avec certitude quels conteneurs s'ex√©cutent dans ce module ( <code>nginx-65899c769f-wxdx6</code> ) et l'espace de noms r√©seau ( <code>cni-912bcc63‚Äì712d-1c84‚Äì89a7‚Äì9e10510808a0</code> ): <br><br><ul><li>  nginx (ID: <code>4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7</code> ); </li><li>  pause (ID: <code>d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6</code> ). </li></ul><br><img src="https://habrastorage.org/webt/3h/cx/qq/3hcxqqv-mwlrm8ax9lu9jl0fixy.png"><br><br>  Comment est-ce sous ( <code>nginx-65899c769f-wxdx6</code> ) connect√© au r√©seau?  Nous utilisons le PID 27255 pr√©c√©demment re√ßu de <code>pause</code> pour ex√©cuter des commandes dans son espace de noms r√©seau ( <code>cni-912bcc63‚Äì712d-1c84‚Äì89a7‚Äì9e10510808a0</code> ): <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns identify 27255 cni-912bcc63-712d-1c84-89a7-9e10510808a0</code> </pre> <br>  √Ä ces fins, nous utiliserons <code>nsenter</code> avec l'option <code>-t</code> qui d√©finit le PID cible et <code>-n</code> sans sp√©cifier de fichier pour entrer dans l'espace de noms r√©seau du processus cible (27255).  Voici ce que dira l' <code>ip link show</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo nsenter -t 27255 -n ip link show 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 3: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 0a:58:0a:c8:00:04 brd ff:ff:ff:ff:ff:ff link-netnsid 0</code> </pre> <br>  ... et <code>ifconfig eth0</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo nsenter -t 27255 -n ifconfig eth0 eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 10.200.0.4 netmask 255.255.255.0 broadcast 0.0.0.0 inet6 fe80::2097:51ff:fe39:ec21 prefixlen 64 scopeid 0x20&lt;link&gt; ether 0a:58:0a:c8:00:04 txqueuelen 0 (Ethernet) RX packets 540 bytes 42247 (42.2 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 177 bytes 16530 (16.5 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0</code> </pre> <br>  Cela confirme que l'adresse IP obtenue pr√©c√©demment via <code>kubectl get pod</code> est configur√©e sur l'interface <code>eth0</code> .  Cette interface fait partie d'une <b>paire de Veth</b> , dont une extr√©mit√© se trouve dans le foyer et l'autre dans l'espace de noms racine.  Pour d√©couvrir l'interface de la seconde extr√©mit√©, nous utilisons <code>ethtool</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> cni-912bcc63-712d-1c84-89a7-9e10510808a0 ethtool -S eth0 NIC statistics: peer_ifindex: 7</code> </pre> <br>  Nous voyons que <code>ifindex</code> f√™te est 7. V√©rifiez qu'il se trouve dans l'espace de noms racine.  Cela peut √™tre fait en utilisant <code>ip link</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip link | grep <span class="hljs-string"><span class="hljs-string">'^7:'</span></span> 7: veth71f7d238@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master cnio0 state UP mode DEFAULT group default</code> </pre> <br>  Pour en √™tre s√ªr enfin, voyons: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo cat /sys/class/net/veth71f7d238/ifindex 7</code> </pre> <br>  G√©nial, maintenant tout est clair avec le lien virtuel.  En utilisant <code>brctl</code> voyons qui d'autre est connect√© au pont Linux: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ brctl show cnio0 bridge name bridge id STP enabled interfaces cnio0 8000.0a580ac80001 no veth71f7d238 veth73f35410 vethf273b35f</code> </pre> <br>  Donc, l'image est la suivante: <br><br><img src="https://habrastorage.org/webt/yu/gc/t6/yugct6efi7ztep277en4msjuzv4.png"><br><br><h3>  V√©rification du routage </h3><br>  Comment transmettons-nous r√©ellement le trafic?  Regardons la table de routage dans le pod d'espace de noms de r√©seau: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> cni-912bcc63-712d-1c84-89a7-9e10510808a0 ip route show default via 10.200.0.1 dev eth0 10.200.0.0/24 dev eth0 proto kernel scope link src 10.200.0.4</code> </pre> <br>  Au moins, nous savons comment acc√©der √† l'espace de noms racine ( <code>default via 10.200.0.1</code> ).  Voyons maintenant la table de routage de l'h√¥te: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip route list default via 10.240.0.1 dev eth0 proto dhcp src 10.240.0.20 metric 100 10.200.0.0/24 dev cnio0 proto kernel scope link src 10.200.0.1 10.240.0.0/24 dev eth0 proto kernel scope link src 10.240.0.20 10.240.0.1 dev eth0 proto dhcp scope link src 10.240.0.20 metric 100</code> </pre> <br>  Nous savons comment transmettre des paquets √† un routeur VPC (VPC <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">a un</a> routeur ¬´implicite¬ª, qui a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">g√©n√©ralement une deuxi√®me adresse √†</a> partir de l'espace d'adressage IP principal du sous-r√©seau).  Maintenant: le routeur VPC sait-il comment se rendre au r√©seau de chaque foyer?  Non, il ne le fait pas, donc on suppose que les routes seront configur√©es par le plugin CNI ou <a href="">manuellement</a> (comme dans le manuel).  Apparemment, le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">plugin AWS CNI</a> fait exactement cela pour nous chez AWS.  N'oubliez pas qu'il existe de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">nombreux plugins CNI</a> , et nous envisageons un exemple de <b>configuration r√©seau simple</b> : <br><br><img src="https://habrastorage.org/webt/cn/v7/v_/cnv7v_qjfkidbtuljkbgkuzuaag.png"><br><br><h3>  Immersion profonde dans NAT </h3><br>  <code>kubectl create -f busybox.yaml</code> cr√©ez deux conteneurs <code>busybox</code> identiques avec Replication Controller: <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: ReplicationController metadata: name: busybox0 labels: app: busybox0 spec: replicas: 2 selector: app: busybox0 template: metadata: name: busybox0 labels: app: busybox0 spec: containers: - image: busybox command: - sleep - "3600" imagePullPolicy: IfNotPresent name: busybox restartPolicy: Always</code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>busybox.yaml</code></a> ) <br><br>  Nous obtenons: <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE busybox0-g6pww 1/1 Running 0 4s 10.200.1.15 worker-1 busybox0-rw89s 1/1 Running 0 4s 10.200.0.21 worker-0 ...</code> </pre> <br>  Les pings d'un conteneur √† un autre doivent r√©ussir: <br><br><pre> <code class="bash hljs">$ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it busybox0-rw89s -- ping -c 2 10.200.1.15 PING 10.200.1.15 (10.200.1.15): 56 data bytes 64 bytes from 10.200.1.15: seq=0 ttl=62 time=0.528 ms 64 bytes from 10.200.1.15: seq=1 ttl=62 time=0.440 ms --- 10.200.1.15 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.440/0.484/0.528 ms</code> </pre> <br>  Pour comprendre le mouvement du trafic, vous pouvez regarder les paquets en utilisant <code>tcpdump</code> ou <code>conntrack</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 10.200.1.15 icmp 1 29 src=10.200.0.21 dst=10.200.1.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1280 src=10.200.1.15 dst=10.240.0.20 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1280 mark=0 use=1</code> </pre> <br>  L'IP source du pod 10.200.0.21 est traduite en adresse IP de l'h√¥te 10.240.0.20. <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 10.200.1.15 icmp 1 28 src=10.240.0.20 dst=10.200.1.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1280 src=10.200.1.15 dst=10.240.0.20 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1280 mark=0 use=1</code> </pre> <br>  Dans iptables, vous pouvez voir que les nombres augmentent: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo iptables -t nat -Z POSTROUTING -L -v Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> out <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> destination ... 5 324 CNI-be726a77f15ea47ff32947a3 all -- any any 10.200.0.0/24 anywhere /* name: <span class="hljs-string"><span class="hljs-string">"bridge"</span></span> id: <span class="hljs-string"><span class="hljs-string">"631cab5de5565cc432a3beca0e2aece0cef9285482b11f3eb0b46c134e457854"</span></span> */ Zeroing chain `POSTROUTING<span class="hljs-string"><span class="hljs-string">'</span></span></code> </pre> <br>  D'un autre c√¥t√©, si vous supprimez <code>"ipMasq": true</code> de la configuration du plugin CNI, vous pouvez voir ce qui suit (cette op√©ration est effectu√©e exclusivement √† des fins √©ducatives - nous ne recommandons pas de changer la configuration sur un cluster de travail!): <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE busybox0-2btxn 1/1 Running 0 16s 10.200.0.15 worker-0 busybox0-dhpx8 1/1 Running 0 16s 10.200.1.13 worker-1 ...</code> </pre> <br>  Ping doit toujours passer: <br><br><pre> <code class="bash hljs">$ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it busybox0-2btxn -- ping -c 2 10.200.1.13 PING 10.200.1.6 (10.200.1.6): 56 data bytes 64 bytes from 10.200.1.6: seq=0 ttl=62 time=0.515 ms 64 bytes from 10.200.1.6: seq=1 ttl=62 time=0.427 ms --- 10.200.1.6 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.427/0.471/0.515 ms</code> </pre> <br>  Et dans ce cas - sans utiliser NAT: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 10.200.1.13 icmp 1 29 src=10.200.0.15 dst=10.200.1.13 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1792 src=10.200.1.13 dst=10.200.0.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1792 mark=0 use=1</code> </pre> <br>  Nous avons donc v√©rifi√© que ¬´tous les conteneurs peuvent communiquer avec n'importe quel autre conteneur sans utiliser NAT¬ª. <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 10.200.1.13 icmp 1 27 src=10.200.0.15 dst=10.200.1.13 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1792 src=10.200.1.13 dst=10.200.0.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1792 mark=0 use=1</code> </pre> <br><h2>  R√©seau de clusters (10.32.0.0/24) </h2><br>  Vous avez peut-√™tre remarqu√© dans l'exemple de la <code>busybox</code> que les adresses IP attribu√©es √† la <code>busybox</code> √©taient diff√©rentes dans chaque cas.  Et si nous voulions rendre ces conteneurs disponibles pour la communication d'autres foyers?  On pourrait prendre les adresses IP actuelles du pod, mais elles changeront.  Pour cette raison, vous devez configurer la ressource <code>Service</code> , qui procurera des requ√™tes par proxy √† de nombreux foyers de courte dur√©e. <br><br><blockquote>  ¬´Le service dans Kubernetes est une abstraction qui d√©finit l'ensemble logique des foyers et les politiques par lesquelles ils sont accessibles.¬ª  <i>(√† partir de la documentation des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">services Kubernetes</a> )</i> </blockquote><br>  Il existe diff√©rentes fa√ßons de publier un service;  le type par d√©faut est <code>ClusterIP</code> , qui d√©finit l'adresse IP √† partir du bloc CIDR du cluster (c'est-√†-dire accessible uniquement √† partir du cluster).  Un tel exemple est le module compl√©mentaire de cluster DNS configur√© dans Kubernetes The Hard Way. <br><br><pre> <code class="plaintext hljs"># ... apiVersion: v1 kind: Service metadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: "KubeDNS" spec: selector: k8s-app: kube-dns clusterIP: 10.32.0.10 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP # ...</code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>kube-dns.yaml</code></a> ) <br><br>  <code>kubectl</code> montre que le <code>Service</code> souvient des points de terminaison et les traduit: <br><br><pre> <code class="bash hljs">$ kubectl -n kube-system describe services ... Selector: k8s-app=kube-dns Type: ClusterIP IP: 10.32.0.10 Port: dns 53/UDP TargetPort: 53/UDP Endpoints: 10.200.0.27:53 Port: dns-tcp 53/TCP TargetPort: 53/TCP Endpoints: 10.200.0.27:53 ...</code> </pre> <br>  Comment exactement? .. <code>iptables</code> nouveau.  Passons en revue les r√®gles cr√©√©es pour cet exemple.  Leur liste compl√®te peut √™tre consult√©e avec la commande <code>iptables-save</code> . <br><br>  D√®s que les paquets sont cr√©√©s par le processus ( <code>OUTPUT</code> ) ou arrivent sur l'interface r√©seau ( <code>PREROUTING</code> ), ils passent par les cha√Ænes <code>iptables</code> suivantes: <br><br><pre> <code class="bash hljs">-A PREROUTING -m comment --comment <span class="hljs-string"><span class="hljs-string">"kubernetes service portals"</span></span> -j KUBE-SERVICES -A OUTPUT -m comment --comment <span class="hljs-string"><span class="hljs-string">"kubernetes service portals"</span></span> -j KUBE-SERVICES</code> </pre> <br>  Les cibles suivantes correspondent aux paquets TCP envoy√©s au 53e port √† 10.32.0.10 et sont transmises au destinataire 10.200.0.27 avec le 53e port: <br><br><pre> <code class="bash hljs">-A KUBE-SERVICES -d 10.32.0.10/32 -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp cluster IP"</span></span> -m tcp --dport 53 -j KUBE-SVC-ERIFXISQEP7F7OF4 -A KUBE-SVC-ERIFXISQEP7F7OF4 -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp"</span></span> -j KUBE-SEP-32LPCMGYG6ODGN3H -A KUBE-SEP-32LPCMGYG6ODGN3H -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp"</span></span> -m tcp -j DNAT --to-destination 10.200.0.27:53</code> </pre> <br>  De m√™me pour les paquets UDP (destinataire 10.32.0.10:53 ‚Üí 10.200.0.27:53): <br><br><pre> <code class="bash hljs">-A KUBE-SERVICES -d 10.32.0.10/32 -p udp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns cluster IP"</span></span> -m udp --dport 53 -j KUBE-SVC-TCOU7JCQXEZGVUNU -A KUBE-SVC-TCOU7JCQXEZGVUNU -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns"</span></span> -j KUBE-SEP-LRUTK6XRXU43VLIG -A KUBE-SEP-LRUTK6XRXU43VLIG -p udp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns"</span></span> -m udp -j DNAT --to-destination 10.200.0.27:53</code> </pre> <br>  Il existe d'autres types de <code>Services</code> dans Kubernetes.  En particulier, Kubernetes The Hard Way <code>NodePort</code> de <code>NodePort</code> - voir <a href="">Smoke Test: Services</a> . <br><br><pre> <code class="bash hljs">kubectl expose deployment nginx --port 80 --<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> NodePort</code> </pre> <br>  <code>NodePort</code> publie le service sur l'adresse IP de chaque n≈ìud, en le pla√ßant sur un port statique (il s'appelle <code>NodePort</code> ).  <code>NodePort</code> est <code>NodePort</code> accessible depuis l'ext√©rieur du cluster.  Vous pouvez v√©rifier le port d√©di√© (dans ce cas - 31088) en utilisant <code>kubectl</code> : <br><br><pre> <code class="bash hljs">$ kubectl describe services nginx ... Type: NodePort IP: 10.32.0.53 Port: &lt;<span class="hljs-built_in"><span class="hljs-built_in">unset</span></span>&gt; 80/TCP TargetPort: 80/TCP NodePort: &lt;<span class="hljs-built_in"><span class="hljs-built_in">unset</span></span>&gt; 31088/TCP Endpoints: 10.200.1.18:80 ...</code> </pre> <br>  Under est d√©sormais disponible sur Internet sous le nom <code>http://${EXTERNAL_IP}:31088/</code> .  Ici, <code>EXTERNAL_IP</code> est l'adresse IP publique de <b>toute instance de travail</b> .  Dans cet exemple, j'ai utilis√© l'adresse IP publique de <b>worker-0</b> .  La demande est re√ßue par un h√¥te avec une adresse IP interne de 10.240.0.20 (le fournisseur de cloud est engag√© dans le NAT public), cependant, le service est r√©ellement d√©marr√© sur un autre h√¥te ( <b>travailleur-1</b> , qui peut √™tre vu par l'adresse IP du point final - 10.200.1.18): <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 31088 tcp 6 86397 ESTABLISHED src=173.38.XXX.XXX dst=10.240.0.20 sport=30303 dport=31088 src=10.200.1.18 dst=10.240.0.20 sport=80 dport=30303 [ASSURED] mark=0 use=1</code> </pre> <br>  Le paquet est envoy√© de <b>travailleur-0</b> √† <b>travailleur-1</b> , o√π il trouve son destinataire: <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 80 tcp 6 86392 ESTABLISHED src=10.240.0.20 dst=10.200.1.18 sport=14802 dport=80 src=10.200.1.18 dst=10.240.0.20 sport=80 dport=14802 [ASSURED] mark=0 use=1</code> </pre> <br>  Un tel circuit est-il id√©al?  Peut-√™tre pas, mais √ßa marche.  Dans ce cas, les r√®gles programm√©es <code>iptables</code> sont les suivantes: <br><br><pre> <code class="bash hljs">-A KUBE-NODEPORTS -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -m tcp --dport 31088 -j KUBE-SVC-4N57TFCL4MD7ZTDA -A KUBE-SVC-4N57TFCL4MD7ZTDA -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -j KUBE-SEP-UGTFMET44DQG7H7H -A KUBE-SEP-UGTFMET44DQG7H7H -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -m tcp -j DNAT --to-destination 10.200.1.18:80</code> </pre> <br>  En d'autres termes, l'adresse du destinataire des paquets avec le port 31088 est diffus√©e le 10.200.1.18.  Le port diffuse √©galement, de 31088 √† 80. <br><br>  Nous n'avons pas <code>LoadBalancer</code> un autre type de service - <code>LoadBalancer</code> - qui rend le service accessible au public √† l'aide d'un √©quilibreur de charge de fournisseur de cloud, mais l'article s'est d√©j√† r√©v√©l√© volumineux. <br><br><h2>  Conclusion </h2><br>  Il peut sembler qu'il y a beaucoup d'informations, mais nous n'avons touch√© que la pointe de l'iceberg.  √Ä l'avenir, je vais parler d'IPv6, IPVS, eBPF et de quelques plugins CNI actuels int√©ressants. <br><br><h2>  PS du traducteur </h2><br>  Lisez aussi dans notre blog: <br><br><ul><li>  ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Guide illustr√© du r√©seautage chez Kubernetes</a> ¬ª; </li><li>  ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Comparaison des performances r√©seau pour Kubernetes</a> ¬ª; </li><li>  ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Exp√©riences avec kube-proxy et inaccessibilit√© de l'h√¥te dans Kubernetes</a> ¬ª; </li><li>  ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Am√©liorer la fiabilit√© de Kubernetes: comment remarquer rapidement qu'un n≈ìud est tomb√©</a> ¬ª; </li><li> ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Play with Kubernetes ‚Äî      K8s</a> ¬ª; </li><li> ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">   Kubernetes   </a> ¬ª <i>( ,        Kubernetes)</i> ; </li><li> ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Container Networking Interface (CNI) ‚Äî      Linux-</a> ¬ª. </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr420813/">https://habr.com/ru/post/fr420813/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr420799/index.html">MPS 2018.2: tests de g√©n√©rateur, plug-in GitHub, aspect VCS, notifications de migration, etc.</a></li>
<li><a href="../fr420803/index.html">Cours d'impression 3D. √âconomie de plastique lors de l'impression de mod√®les non fonctionnels √† partir de 3Dtool</a></li>
<li><a href="../fr420805/index.html">[Fran√ßais] Quand utiliser des flux parall√®les</a></li>
<li><a href="../fr420809/index.html">Semaine de la s√©curit√© 31: cinquante nuances d'ins√©curit√© sur Android</a></li>
<li><a href="../fr420811/index.html">R√©seau de messagerie et de t√©l√©phonie d√©centralis√© de nouvelle g√©n√©ration</a></li>
<li><a href="../fr420815/index.html">Comment le "d√©codage du monde num√©rique" a explos√©: les 10 meilleurs rapports de DotNext 2018 Piter</a></li>
<li><a href="../fr420819/index.html">Top 10 des outils Python pour l'apprentissage automatique et la science des donn√©es</a></li>
<li><a href="../fr420821/index.html">R√®gle 10: 1 en programmation et en √©criture</a></li>
<li><a href="../fr420825/index.html">Aujourd'hui sera le premier match entre OpenAI et Dota 2 professionnels (personnes gagn√©es). Nous comprenons comment fonctionne le bot</a></li>
<li><a href="../fr420827/index.html">Cr√©ez un projet maven simple √† l'aide de Java EE + WildFly10 + JPA (Hibernate) + Postgresql + EJB + IntelliJ IDEA</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>