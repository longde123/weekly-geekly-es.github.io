<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🐁 🥧 🤛🏾 Surveillance et Kubernetes (revue et rapport vidéo) 🧗🏾 🌨️ 🌚</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Le 28 mai, lors de la conférence RootConf 2018, qui a eu lieu dans le cadre du festival RIT ++ 2018, dans la section «Logging and Monitoring», un rapp...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Surveillance et Kubernetes (revue et rapport vidéo)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/412901/">  Le 28 mai, lors de la conférence <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">RootConf</a> 2018, qui a eu lieu dans le cadre du festival RIT ++ 2018, dans la section «Logging and Monitoring», un rapport «Monitoring and Kubernetes» a été remis.  Il raconte l'expérience de la surveillance de la configuration avec Prometheus, qui a été obtenue par Flant à la suite de l'exploitation de dizaines de projets Kubernetes en production. <br><br><img src="https://habrastorage.org/webt/pm/o9/dm/pmo9dmnz9jf7b-yhej9shmir92q.jpeg"><br><br>  Par tradition, nous sommes heureux de présenter une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><b>vidéo avec un rapport</b></a> (environ une heure, <b>beaucoup plus</b> informatif <b>que l'</b> article) et la principale compression sous forme de texte.  C'est parti! <a name="habracut"></a><br><br><h2>  Qu'est-ce que la surveillance? </h2><br>  Il existe de nombreux systèmes de surveillance: <br><br><img src="https://habrastorage.org/webt/pa/07/i0/pa07i0ojuohdqwxbk6lvf86lgls.png"><br><br>  Il semblerait que la prise et l'installation de l'un d'entre eux - c'est tout, la question est close.  Mais la pratique montre que ce n'est pas le cas.  Et voici pourquoi: <br><br><ol><li>  <b>L'indicateur de vitesse indique la vitesse</b> .  Si nous mesurons la vitesse une fois par minute avec l'indicateur de vitesse, la vitesse moyenne, que nous calculons sur la base de ces données, ne coïncidera pas avec les données de l'odomètre.  Et si dans le cas d'une voiture, cela est évident, alors quand il s'agit de nombreux indicateurs pour le serveur, nous l'oublions souvent. <br><img src="https://habrastorage.org/webt/s9/13/fv/s913fvrbguhqbp3iuuobwrmazt8.png"><br>  <i>Ce que nous mesurons et comment nous avons réellement voyagé</i> </li><li>  <b>Plus de mesures</b> .  Plus nous obtiendrons d'indicateurs <i>différents</i> , plus le diagnostic des problèmes sera précis ... mais seulement à condition que ce soient des indicateurs vraiment utiles, et pas seulement tout ce que vous avez réussi à collecter. </li><li>  <b>Alertes</b> .  L'envoi d'alertes n'a rien de compliqué.  Cependant, deux problèmes typiques: a) les fausses alarmes se produisent si souvent que nous cessons de répondre aux alertes, b) les alertes arrivent à un moment où il est trop tard (tout a déjà explosé).  Et réaliser en contrôlant que ces problèmes ne se sont pas posés est un véritable art! </li></ol><br>  La surveillance est une tarte de trois couches, chacune étant essentielle: <br><br><ol><li>  Tout d'abord, il s'agit d'un système qui vous permet de <b>prévenir les accidents</b> , de <b>signaler les accidents</b> (s'ils n'ont pas pu être évités) et d'effectuer un <b>diagnostic rapide des</b> problèmes. </li><li>  Que faut-il pour cela?  <b>Des données précises</b> , <b>des graphiques utiles</b> (regardez-les et comprenez où est le problème), <b>des alertes pertinentes</b> (arrivez au bon moment et contiennent des informations claires). </li><li>  Et pour que tout cela fonctionne, un <b>système de surveillance est</b> nécessaire. </li></ol><br>  La configuration correcte d'un système de surveillance qui fonctionne vraiment n'est pas une tâche facile, nécessitant une approche réfléchie de la mise en œuvre même sans Kubernetes.  Mais que se passe-t-il avec son apparence? <br><br><h2>  Spécificités de la surveillance Kubernetes </h2><br><h3>  N ° 1.  Plus grand et plus rapide </h3><br>  Kubernetes change beaucoup parce que l'infrastructure s'agrandit et devient plus rapide.  Si auparavant, avec des serveurs de fer ordinaires, leur nombre était très limité et le processus d'ajout était très long (prenait des jours ou des semaines), puis avec les machines virtuelles, le nombre d'entités augmentait considérablement et le temps de leur introduction dans la bataille était réduit à quelques secondes. <br><br>  Avec Kubernetes, le nombre d'entités a augmenté d'un ordre de grandeur, leur ajout est complètement automatisé (la gestion de la configuration est nécessaire, car sans description, un nouveau pod ne peut tout simplement pas être créé), toute l'infrastructure est devenue très dynamique (par exemple, les pods sont supprimés et libérés à chaque fois sont créés à nouveau). <br><br><img src="https://habrastorage.org/webt/01/fv/cf/01fvcfbc_i2roepw7nh0q6womsk.png"><br><br>  Qu'est-ce que cela change? <br><br><ol><li>  En principe, nous cessons de regarder des pods ou des conteneurs individuels - maintenant nous ne nous intéressons <b>qu'aux groupes d'objets</b> . </li><li>  <b>La découverte du service devient strictement obligatoire</b> , car les "vitesses" sont déjà telles que, en principe, nous ne pouvons pas démarrer / supprimer manuellement de nouvelles entités, comme c'était le cas auparavant, lors de l'achat de nouveaux serveurs. </li><li>  <b>La quantité de données augmente considérablement</b> .  Si des mesures antérieures étaient collectées à partir de serveurs ou de machines virtuelles, désormais à partir de pods, dont le nombre est beaucoup plus important. </li><li>  Le changement le plus intéressant que j'ai appelé le « <b>flux de métadonnées</b> » et je vais vous en dire plus. </li></ol><br>  Je vais commencer par cette comparaison: <br><br><ul><li>  Lorsque vous envoyez votre enfant à la maternelle, on lui remet une boîte personnelle, qui lui est attribuée pour la prochaine année (ou plus) et sur laquelle son nom est indiqué. </li><li>  Lorsque vous venez à la piscine, votre casier n'est pas signé et il vous est délivré pour une «séance». </li></ul><br>  <b>Les systèmes de surveillance classiques pensent donc qu'ils sont un jardin d'enfants</b> , pas une piscine: ils supposent que l'objet de surveillance leur est venu pour toujours ou pour longtemps, et leur donnent des casiers en conséquence.  Mais les réalités de Kubernetes sont différentes: un pod est venu dans la piscine (c'est-à-dire a été créé), a nagé dedans (jusqu'à un nouveau déploiement) et est parti (a été détruit) - tout cela se produit rapidement et régulièrement.  Ainsi, le système de surveillance doit comprendre que les objets qu'il surveille ont une courte durée de vie et doit pouvoir l'oublier complètement au bon moment. <br><br><h3>  N ° 2.  La réalité parallèle existe </h3><br>  Autre point important - avec l'avènement de Kubernetes, nous avons simultanément deux «réalités»: <br><br><ol><li>  Monde Kubernetes dans lequel il y a des espaces de noms, des déploiements, des pods, des conteneurs.  C'est un monde complexe, mais il est logique, structuré. </li><li>  Le monde "physique", composé de nombreux (littéralement - tas) de conteneurs sur chaque nœud. </li></ol><br><img src="https://habrastorage.org/webt/1p/wc/xj/1pwcxjjt1xbgwqufldfm1upua10.png"><br>  <i>Un seul et même conteneur dans la «réalité virtuelle» de Kubernetes (ci-dessus) et le monde physique des nœuds (ci-dessous)</i> <br><br>  Et dans le processus de surveillance, nous devons constamment <b>comparer le monde physique des conteneurs avec la réalité de Kubernetes</b> .  Par exemple, lorsque nous regardons un espace de noms, nous voulons savoir où se trouvent tous ses conteneurs (ou les conteneurs de l'un de ses foyers).  Sans cela, les alertes ne seront pas visuelles et pratiques à utiliser - car il est important pour nous de comprendre quels objets ils signalent. <br><br><img src="https://habrastorage.org/webt/2p/n7/3t/2pn73tojozunuxdnjiba8yqw7-y.png"><br>  <i>Différents types d'alertes - cette dernière est plus visuelle et pratique au travail que les autres</i> <br><br>  <b>Les conclusions</b> sont les suivantes: <br><br><ol><li>  Le système de surveillance doit utiliser les primitives intégrées de Kubernetes. </li><li>  Il y a plus d'une réalité: souvent, les problèmes ne se produisent pas avec le foyer, mais avec un nœud particulier, et nous devons constamment comprendre dans quel type de «réalité» ils se trouvent. </li><li>  Dans un cluster, en règle générale, il existe plusieurs environnements (en plus de la production), ce qui signifie que cela doit être pris en compte (par exemple, pour ne pas recevoir d'alertes la nuit sur les problèmes de développement). </li></ol><br>  Donc, nous avons trois conditions nécessaires pour que tout fonctionne: <br><br><ol><li>  Nous comprenons bien ce qu'est la surveillance. </li><li>  Nous connaissons ses fonctionnalités, lesquelles apparaissent avec Kubernetes. </li><li>  Nous adoptons le Prométhée. </li></ol><br>  Et donc, pour vraiment s'entraîner, il ne reste plus qu'à faire <i>vraiment beaucoup d'</i> efforts!  Au fait, pourquoi exactement Prométhée? .. <br><br><h2>  Prométhée </h2><br>  Il existe deux façons de répondre à la question concernant le choix de Prométhée: <br><br><ol><li>  Découvrez qui et quoi sont généralement utilisés pour surveiller Kubernetes. </li><li>  Considérez ses avantages techniques. </li></ol><br>  Pour la première, j'ai utilisé les données d'enquête de The New Stack (du livre électronique <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">The State of the Kubernetes Ecosystem</a> ), selon lesquelles Prometheus est au moins plus populaire que les autres solutions (Open Source et SaaS), et si vous regardez, elle a un avantage statistique quintuple . <br><br>  Voyons maintenant comment Prometheus fonctionne, en parallèle avec la façon dont ses capacités se combinent avec Kubernetes et résolvent les défis associés. <br><br><h2>  Comment Prometheus est-il structuré? </h2><br>  Prometheus est écrit en Go et distribué comme un seul fichier binaire, dans lequel tout est intégré.  L'algorithme de base pour son fonctionnement est le suivant: <br><br><img src="https://habrastorage.org/webt/nh/xt/hp/nhxthp-dm3wveymrgbejbanainw.png"><br><br><ul><li>  <b>Le collecteur</b> lit la <b>table des cibles</b> , c.-à-d.  une liste des objets à surveiller et la fréquence de leur interrogation (par défaut - 60 secondes). </li><li>  Après cela, le collecteur envoie une demande HTTP à chaque module dont vous avez <b>besoin</b> et reçoit une réponse avec un ensemble de métriques - il peut y avoir cent, mille, dix mille ... Chaque métrique a un nom, une valeur et des <b>étiquettes</b> . </li><li>  La réponse reçue est <b>stockée</b> dans la <b>base de</b> données <b>TSDB</b> , où l'horodatage de sa réception et les étiquettes de l'objet dont elle provient ont été ajoutées aux données métriques reçues. <br><br><div class="spoiler">  <b class="spoiler_title">En bref sur TSDB</b> <div class="spoiler_text">  <i>TSDB - base de données de séries chronologiques (DB pour séries chronologiques) sur Go, qui vous permet de stocker des données pendant un nombre de jours spécifié et le fait très efficacement (en taille, en mémoire et en entrée / sortie).</i>  <i>Les données sont stockées uniquement localement, sans clustering et réplication, ce qui est un plus (cela fonctionne simplement et de manière garantie) et un moins (il n'y a pas de mise à l'échelle horizontale du stockage), mais dans le cas du partage Prometheus, c'est bien fait, fédération - plus à ce sujet plus tard.</i> <br></div></div></li><li>  Présenté dans le schéma, <b>Service Discovery</b> est un moteur de découverte de service intégré à Prometheus qui vous permet de recevoir des données «de la boîte» (via l'API Kubernetes) pour créer une table d'objectifs. </li></ul><br>  À quoi ressemble ce tableau?  Pour chaque entrée, il stocke l'URL utilisée pour obtenir les métriques, la fréquence des appels et les étiquettes. <br><br><img src="https://habrastorage.org/webt/xu/9m/c_/xu9mc_ikwk-sdzkrs_fjt6lsfj0.png"><br><br>  Les étiquettes sont utilisées pour la juxtaposition même des «mondes» de Kubernetes avec le physique.  Par exemple, pour trouver un pod avec Redis, nous devons avoir l'espace de noms de valeurs, le service (utilisé au lieu du déploiement en raison des caractéristiques techniques d'un cas particulier) et le pod réel.  Par conséquent, ces 3 étiquettes sont stockées dans des entrées de table d'objectifs pour les métriques Redis. <br><br>  Ces entrées dans le tableau sont formées sur la base de la <code>scrape_configs</code> Prometheus dans laquelle les objets de surveillance sont décrits: dans la section <code>scrape_configs</code> , des <code>scrape_configs</code> définis, qui indiquent par quelles étiquettes rechercher les objets à surveiller, comment les filtrer et quelles étiquettes enregistrer. <br><br><h2>  Quelles données Kubernetes collecte-t-il? </h2><br><ul><li>  Premièrement, l' <b>assistant</b> dans Kubernetes est assez compliqué - et il est essentiel de surveiller l'état de son travail (kube-apiserver, kube-controller-manager, kube-scheduler, kube-etcd3 ...), de plus, il est lié au nœud de cluster. </li><li>  Deuxièmement, il est important de savoir ce qui se passe à l' <b>intérieur de Kubernetes</b> . Pour ce faire, nous obtenons des données de: <br><ul><li>  <i>kubelet</i> - ce composant Kubernetes s'exécute sur chaque nœud du cluster (et se connecte à l'assistant K8s);  cAdvisor y est intégré (toutes les mesures par conteneurs), et il stocke également des informations sur les volumes persistants connectés; </li><li>  <i>kube-state-metrics</i> - en fait, il s'agit de l'exportateur Prometheus pour l'API Kubernetes (il vous permet d'obtenir des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">informations sur les objets</a> stockés dans Kubernetes: pods, services, déploiements, etc.; par exemple, nous ne le saurons pas sans lui état du récipient ou du foyer); </li><li>  <i>nœud-exportateur</i> - fournit des informations sur le nœud lui-même, les mesures de base sur le système Linux (cpu, diskstats, meminfo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">etc.</a> ). </li></ul></li><li>  Viennent <b>ensuite les composants Kubernetes</b> , tels que kube-dns, kube-prometheus-operator et kube-prometheus, ingress-nginx-controller, etc. </li><li>  La prochaine catégorie d'objets à surveiller est en fait le <b>logiciel</b> lancé dans Kubernetes.  Ce sont des services de serveur typiques comme nginx, php-fpm, Redis, MongoDB, RabbitMQ ... Nous le faisons nous-mêmes afin que lorsque nous ajoutons certaines étiquettes au service, il commence automatiquement à collecter les données nécessaires, ce qui crée le tableau de bord actuel dans Grafana. </li><li>  Enfin, la catégorie pour tout le reste est <b>personnalisée</b> .  Les outils Prometheus vous permettent d'automatiser la collecte de mesures arbitraires (par exemple, le nombre de commandes) en ajoutant simplement une étiquette <code>prometheus-custom-target</code> à la description du service. </li></ul><br><img src="https://habrastorage.org/webt/0s/e6/c3/0se6c3ygspys909x3ju6m9aq7uu.gif"><br><h2>  Graphiques </h2><br>  Les données reçues <i>(décrites ci-dessus) sont</i> utilisées pour envoyer des alertes et créer des graphiques.  Nous <b>dessinons des</b> graphiques en utilisant <b>Grafana</b> .  Et un «détail» important ici est <b>PromQL</b> , le langage de requête Prometheus qui s'intègre parfaitement avec Grafana. <br><br><img src="https://habrastorage.org/webt/_a/sl/7w/_asl7wbfscz1eyxstacdij_liio.png"><br><br>  Il est assez simple et pratique pour la plupart des tâches <i>(mais, par exemple, y joindre des jointures est déjà gênant, mais vous devez quand même le faire)</i> .  PromQL vous permet de résoudre toutes les tâches nécessaires: sélectionnez rapidement les mesures nécessaires, comparez les valeurs, effectuez des opérations arithmétiques sur celles-ci, groupez, travaillez avec des intervalles de temps et bien plus encore.  Par exemple: <br><br><img src="https://habrastorage.org/webt/3h/1d/0v/3h1d0vskm__dosoxpx1rzd5jis8.png"><br><br>  De plus, Prometheus a un <b>évaluateur</b> qui, en utilisant le même PromQL, peut accéder à TSDB avec la fréquence spécifiée.  Pourquoi ça?  Exemple: commencez à envoyer des alertes dans les cas où nous avons, selon les mesures disponibles, une erreur 500 sur le serveur Web au cours des 5 dernières minutes.  En plus des étiquettes qui étaient dans la demande, l'évaluateur ajoute des étiquettes supplémentaires aux données pour les alertes (que nous configurons), après quoi elles sont envoyées au format JSON à un autre composant Prometheus - <b>Alertmanager</b> . <br><br>  Prometheus envoie périodiquement (une fois toutes les 30 secondes) des alertes à Alertmanager, qui les déduplique (après avoir reçu la première alerte, il l'enverra et les suivantes ne seront plus envoyées). <br><br><img src="https://habrastorage.org/webt/ju/0e/lg/ju0elg1u57wtxfjopy6wz38vtfg.png"><br><br>  <i><b>Remarque</b> : Nous n'utilisons pas Alertmanager à la maison, mais envoyons les données de Prometheus directement à notre système, avec lequel nos assistants travaillent, mais cela n'a pas d'importance dans le schéma général.</i> <br><br><h2>  Prométhée à Kubernetes: la vue d'ensemble </h2><br>  Voyons maintenant comment fonctionne l'ensemble de cet ensemble Prometheus dans Kubernetes: <br><br><img src="https://habrastorage.org/webt/w-/gu/ue/w-guue_2b8q12romg-qv7uw2hpi.png"><br><br><ul><li>  Kubernetes a son propre espace de noms pour Prométhée <i>(nous avons <code>kube-prometheus</code> dans l'illustration)</i> . </li><li>  Cet espace de noms héberge le pod avec l'installation de Prometheus, qui, toutes les 30 secondes, collecte les métriques de toutes les cibles reçues via Service Discovery dans le cluster. </li><li>  Il abrite également un pod avec Alertmanager, qui reçoit des données de Prometheus et envoie des alertes <i>(par courrier, Slack, PagerDuty, WeChat, intégration tierce, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">etc.</a> )</i> . </li><li>  Prometheus fait face à un équilibreur de charge - un service régulier à Kubernetes - et Grafana accède à Prometheus par son intermédiaire.  Pour <b>garantir la tolérance aux pannes, Prometheus</b> utilise plusieurs modules avec des installations Prometheus, chacun collectant toutes les données et les stockant dans son TSDB.  Grâce à l'équilibreur, Grafana frappe l'un d'eux. </li><li>  Le nombre de pods avec Prometheus est contrôlé par le paramètre <i>StatefulSet</i> - nous ne fabriquons généralement pas plus de deux pods, mais vous pouvez augmenter ce nombre.  De la même manière, Alertmanager est déployé via StatefulSet, pour la tolérance de panne dont au moins 3 pods sont déjà requis (puisqu'un quorum est nécessaire pour prendre des décisions sur l'envoi d'alertes). <br></li></ul><br>  Qu'est-ce qui manque ici? .. <br><br><h2>  Fédération pour Prométhée </h2><br>  Lorsque les données sont collectées toutes les 30 (ou 60) secondes, l'endroit où les stocker se termine très rapidement, et pire encore, cela nécessite beaucoup de ressources informatiques (lors de la réception et du traitement d'un si grand nombre de points de TSDB).  Mais nous voulons stocker et avoir la possibilité de télécharger des informations pour <b>des intervalles de temps importants <i>et</i> e</b> .  Comment y parvenir? <br><br>  Il suffit d'ajouter <b>une installation supplémentaire de Prometheus</b> (nous l'appelons à <i>long terme</i> ) au schéma général, dans lequel Service Discovery est désactivé, et dans le tableau des objectifs, il existe le seul enregistrement statique menant au Prometheus <i>principal</i> ( <i>principal</i> ).  <b>C'est possible grâce à la <a href="">fédération</a></b> : Prometheus vous permet de renvoyer les dernières valeurs de toutes les métriques en une seule requête.  Ainsi, la première installation de Prometheus fonctionne toujours (accède toutes les 60 ou, par exemple, 30 secondes) à toutes les cibles du cluster Kubernetes, et la seconde - une fois toutes les 5 minutes, reçoit les données de la première et les stocke pour pouvoir regarder les données pendant une longue période ( mais sans détails approfondis). <br><br><img src="https://habrastorage.org/webt/zc/uj/k3/zcujk3s5oxler1lhwaaswadmiyy.png"><br>  <i>La deuxième installation de Prometheus ne nécessite pas de découverte de service, et le tableau des objectifs sera composé d'une ligne</i> <br><br><img src="https://habrastorage.org/webt/od/zr/ow/odzrowlvdyq3qmhfldvy085naou.png"><br>  <i>Le tout avec des installations Prometheus de deux types: principal (haut) et long terme</i> <br><br>  La touche finale consiste à <b>connecter Grafana</b> aux deux installations de Prometheus et à créer des tableaux de bord d'une manière spéciale afin que vous puissiez basculer entre les sources de données ( <i>principales</i> ou à <i>long terme</i> ).  Pour ce faire, à l'aide du moteur de modèle, remplacez la variable <code>$prometheus</code> au lieu de la source de données dans tous les panneaux. <br><br><img src="https://habrastorage.org/webt/ju/v3/9u/juv39un0v2qdtwegrt07qpg-1yi.png"><br><br><h2>  Quoi d'autre est important dans les graphiques? </h2><br>  La prise en charge des primitives Kubernetes et la possibilité de <b>passer</b> rapidement de l'image globale (ou une "vue" inférieure) à un service spécifique et vice versa sont deux points clés à prendre en compte lors de l'organisation des plannings. <br><br>  La prise en charge des primitives (espaces de noms, pods, etc.) a déjà été mentionnée - c'est une condition nécessaire en principe pour un travail confortable dans les réalités de Kubernetes.  Et voici un exemple de drill down: <br><br><ul><li>  Nous regardons les graphiques de consommation de ressources par trois projets (c'est-à-dire, trois espaces de noms) - nous voyons que la partie principale du CPU (ou de la mémoire, ou du réseau, ...) tombe sur le projet A. </li><li>  Nous regardons les mêmes graphiques, mais déjà pour les services du projet A: lequel consomme le plus de CPU? </li><li>  Nous nous tournons vers les cartes du service souhaité: quel pod est «à blâmer»? </li><li>  Nous passons aux diagrammes du pod souhaité: quel conteneur faut-il "blâmer"?  C'est le but recherché! </li></ul><br><img src="https://habrastorage.org/webt/fq/vb/ly/fqvblyn0wdnoqqzqgwi0ublftjy.png"><br><h2>  Résumé </h2><br><ul><li>  Énoncez avec précision ce qu'est la surveillance.  <i>(Laissez le "gâteau à trois couches" servir de rappel à cela ... ainsi que le fait que le faire cuire avec compétence n'est pas facile même sans Kubernetes!)</i> </li><li>  N'oubliez pas que Kubernetes ajoute des spécificités obligatoires: regroupement de cibles, découverte de services, grandes quantités de données, flux de métadonnées.  De plus: <br><ul><li>  oui, certains d'entre eux sont résolus comme par magie («out of the box») dans Prométhée; </li><li>  cependant, il reste une autre partie qui doit être surveillée de manière indépendante et réfléchie. </li></ul></li></ul><br>  Et rappelez-vous que le <b>contenu est plus important qu'un système</b> , c'est-à-dire  les graphiques et alertes corrects sont principaux, et non Prometheus (ou tout autre logiciel similaire) en tant que tels. <br><br><img src="https://habrastorage.org/webt/ml/61/ou/ml61oub7wmavnyfdmjepkm7bd-y.png"><br><br><h2>  Vidéos et diapositives </h2><br>  Vidéo de la performance (environ une heure): <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/zj6SlzzBRaA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  Présentation du rapport: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h2>  PS </h2><br>  Autres reportages sur notre blog: <br><br><ul><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Bases de données et Kubernetes</a> ";  <i>(Dmitry Stolyarov; 8 novembre 2018 à HighLoad ++)</i> ; </li><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Meilleures pratiques CI / CD avec Kubernetes et GitLab</a> »;  <i>(Dmitry Stolyarov; 7 novembre 2017 à HighLoad ++)</i> ; </li><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Notre expérience avec Kubernetes dans les petits projets</a> »;  <i>(Dmitry Stolyarov; 6 juin 2017 à RootConf)</i> ; </li><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Nous collectons des images Docker pour CI / CD rapidement et facilement avec dapp</a> » <i>(Dmitry Stolyarov; 8 novembre 2016 sur HighLoad ++)</i> ; </li><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Pratiques de livraison continue avec Docker</a> » <i>(Dmitry Stolyarov; 31 mai 2016 à RootConf)</i> . </li></ul><br>  Vous pouvez également être intéressé par les publications suivantes: <br><br><ul><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Le dispositif et le mécanisme de fonctionnement de l'opérateur Prométhée à Kubernetes</a> »; </li><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Surveillance avec Prométhée à Kubernetes en 15 minutes</a> »; </li><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Infrastructure avec Kubernetes en tant que service abordable</a> .» </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr412901/">https://habr.com/ru/post/fr412901/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr412891/index.html">Citrix XenServer 7.0 I / O non optimisé Agent de gestion non installé</a></li>
<li><a href="../fr412893/index.html">Pour atteindre un programmeur senior en quatre ans: la méthode "School 21"</a></li>
<li><a href="../fr412895/index.html">Vesta Matveeva: la lutte contre la cybercriminalité est un choix moral</a></li>
<li><a href="../fr412897/index.html">Surveillance des produits Atlassian avec Prometheus</a></li>
<li><a href="../fr412899/index.html">Week-end lecture: 30 documents sur le son, l'histoire des marques audio et l'industrie cinématographique</a></li>
<li><a href="../fr412903/index.html">Comment nous avons peint Habr</a></li>
<li><a href="../fr412905/index.html">À propos de l'analyse LL: une approche de l'analyse à travers le concept de coupe de chaîne</a></li>
<li><a href="../fr412911/index.html">Les développeurs parlent de fonctionnalités découpées dans les jeux</a></li>
<li><a href="../fr412913/index.html">"Baikal-T1" a été mis en vente pour 3990 roubles</a></li>
<li><a href="../fr412915/index.html">Détermination de la densité de gaz à partir des résultats de la mesure de la pression et de la température avec des capteurs Arduino</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>