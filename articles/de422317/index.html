<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üóùÔ∏è üéÅ ‚¨úÔ∏è Warum sind TPUs so gut f√ºr tiefes Lernen? ‚òÄÔ∏è üë®üèø‚Äçü§ù‚Äçüë®üèæ üÜô</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Tensorprozessor der 3. Generation 

 Google Tensor Processor ist ein ASIC (Special Purpose Integrated Circuit), der von Google von Grund auf f√ºr masch...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Warum sind TPUs so gut f√ºr tiefes Lernen?</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/422317/"><img src="https://habrastorage.org/webt/hc/p7/cd/hcp7cda1npc6ylbq16nwwcsyxd4.jpeg"><br>  <i>Tensorprozessor der 3. Generation</i> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Google Tensor Processor</a> ist ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ASIC</a> (Special Purpose Integrated Circuit), der von Google von Grund auf f√ºr maschinelle Lernaufgaben entwickelt wurde.  Er arbeitet an mehreren wichtigen Google-Produkten, darunter √úbersetzen, Fotos, Suchassistent und Google Mail.  Cloud TPU bietet allen Entwicklern und Datenwissenschaftlern, die modernste Modelle f√ºr maschinelles Lernen in der Google Cloud einf√ºhren, die Vorteile der Skalierbarkeit und Benutzerfreundlichkeit.  Auf der Google Next '18 haben wir angek√ºndigt, dass Cloud TPU v2 jetzt f√ºr alle Benutzer verf√ºgbar ist, einschlie√ülich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kostenloser Testkonten</a> , und Cloud TPU v3 f√ºr Alpha-Tests verf√ºgbar ist. <br><a name="habracut"></a><br><img src="https://habrastorage.org/getpro/habr/post_images/f34/b73/cee/f34b73ceecf379f47dd446f0cc8b1a7c.jpg"><br><br>  Aber viele Leute fragen - was ist der Unterschied zwischen CPU, GPU und TPU?  Wir haben eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Demo-Site erstellt,</a> auf der sich die Pr√§sentation und die Animation befinden, die diese Frage beantworten.  In diesem Beitrag m√∂chte ich auf bestimmte Merkmale des Inhalts dieser Website eingehen. <br><br><h2>  Wie funktionieren neuronale Netze? </h2><br>  Bevor Sie mit dem Vergleich von CPU, GPU und TPU beginnen, schauen wir uns an, welche Berechnungen f√ºr maschinelles Lernen erforderlich sind - und insbesondere f√ºr neuronale Netze. <br><br>  Stellen Sie sich zum Beispiel vor, wir verwenden ein einschichtiges neuronales Netzwerk, um handgeschriebene Zahlen zu erkennen, wie in der folgenden Abbildung dargestellt: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2d6/bf4/585/2d6bf4585be28678f66652fd77ecb2f8.png"><br><br>  Wenn das Bild ein Raster von 28 x 28 Pixel in Graustufen ist, kann es in einen Vektor mit 784 Werten (Messungen) konvertiert werden.  Ein Neuron, das die Zahl 8 erkennt, nimmt diese Werte und multipliziert sie mit den Parameterwerten (rote Linien im Diagramm). <br><br>  Der Parameter fungiert als Filter und extrahiert Merkmale der Daten, die die √Ñhnlichkeit von Bild und Form 8 anzeigen: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/020/141/559/0201415596ab1132ba07a3b430a2fa34.gif"><br><br>  Dies ist die einfachste Erkl√§rung f√ºr die Klassifizierung von Daten durch neuronale Netze.  Multiplikation von Daten mit den entsprechenden Parametern (F√§rbung von Punkten) und deren Addition (Summe der Punkte rechts).  Das h√∂chste Ergebnis zeigt die beste √úbereinstimmung zwischen den eingegebenen Daten und dem entsprechenden Parameter an, was h√∂chstwahrscheinlich die richtige Antwort ist. <br><br>  Einfach ausgedr√ºckt, m√ºssen neuronale Netze eine gro√üe Anzahl von Multiplikationen und Additionen von Daten und Parametern durchf√ºhren.  Oft organisieren wir sie in Form einer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Matrixmultiplikation</a> , die in der Schule in der Algebra auftreten kann.  Daher besteht das Problem darin, eine gro√üe Anzahl von Matrixmultiplikationen so schnell wie m√∂glich durchzuf√ºhren und so wenig Energie wie m√∂glich zu verbrauchen. <br><br><h2>  Wie funktioniert eine CPU? </h2><br>  Wie geht die CPU mit dieser Aufgabe um?  Die CPU ist ein Allzweckprozessor, der auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">von Neumann-Architektur</a> basiert.  Dies bedeutet, dass die CPU wie folgt mit Software und Speicher arbeitet: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/963/029/620/96302962031a0a23ecf34b868d194161.gif"><br><br>  Der Hauptvorteil der CPU ist die Flexibilit√§t.  Dank der von Neumann-Architektur k√∂nnen Sie v√∂llig unterschiedliche Software f√ºr Millionen verschiedener Zwecke herunterladen.  Die CPU kann f√ºr Textverarbeitung, Raketentriebwerkssteuerung, Bankgesch√§fte und Bildklassifizierung unter Verwendung eines neuronalen Netzwerks verwendet werden. <br><br>  Da die CPU jedoch so flexibel ist, wei√ü das Ger√§t nicht immer im Voraus, wie der n√§chste Vorgang aussehen wird, bis es die n√§chste Anweisung aus der Software liest.  Die CPU muss die Ergebnisse jeder Berechnung im Speicher innerhalb der CPU speichern (die sogenannten Register oder den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">L1-Cache</a> ).  Der Zugriff auf diesen Speicher wird zu einem Minus der CPU-Architektur, die als von Neumann-Architekturengpass bekannt ist.  Und obwohl eine gro√üe Anzahl von Berechnungen f√ºr neuronale Netze zuk√ºnftige Schritte vorhersehbar macht, f√ºhrt jede <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arithmetische Logikvorrichtung der</a> CPU (ALU, eine Komponente, die Multiplikatoren und Addierer speichert und steuert) Operationen nacheinander aus und greift jedes Mal auf Speicher zu, was den Gesamtdurchsatz begrenzt und eine erhebliche Menge an Energie verbraucht . <br><br><h2>  Wie die GPU funktioniert </h2><br>  Um den Durchsatz im Vergleich zur CPU zu erh√∂hen, verwendet die GPU eine einfache Strategie: Warum nicht Tausende von ALUs in den Prozessor integrieren?  Die moderne GPU enth√§lt etwa 2500 - 5000 ALU auf dem Prozessor, wodurch Tausende von Multiplikationen und Additionen gleichzeitig durchgef√ºhrt werden k√∂nnen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fbb/bcb/17b/fbbbcb17b7732e20d2658d5e76023beb.gif"><br><br>  Eine solche Architektur funktioniert gut mit Anwendungen, die eine massive Parallelisierung erfordern, wie beispielsweise der Matrixmultiplikation in einem neuronalen Netzwerk.  Bei einer typischen Trainingslast von Deep Learning (GO) erh√∂ht sich der Durchsatz in diesem Fall im Vergleich zur CPU um eine Gr√∂√üenordnung.  Daher ist die GPU heute die beliebteste Prozessorarchitektur f√ºr GO. <br><br>  Die GPU bleibt jedoch ein Allzweckprozessor, der eine Million verschiedener Anwendungen und Software unterst√ºtzen muss.  Und das bringt uns zur√ºck zum Grundproblem des von Neumann-Architekturengpasses.  F√ºr jede Berechnung in Tausenden von ALUs, GPUs, muss auf Register oder gemeinsam genutzten Speicher verwiesen werden, um Zwischenberechnungsergebnisse zu lesen und zu speichern.  Da die GPU auf Tausenden ihrer ALUs mehr paralleles Rechnen ausf√ºhrt, verbraucht sie auch proportional mehr Energie f√ºr den Speicherzugriff und nimmt einen gro√üen Bereich ein. <br><br><h2>  Wie funktioniert TPU? </h2><br>  Bei der Entwicklung von TPU bei Google haben wir eine Architektur erstellt, die f√ºr eine bestimmte Aufgabe entwickelt wurde.  Anstatt einen Allzweckprozessor zu entwickeln, haben wir einen Matrixprozessor entwickelt, der auf die Arbeit mit neuronalen Netzen spezialisiert ist.  TPU wird nicht in der Lage sein, mit einem Textverarbeitungsprogramm zu arbeiten, Raketentriebwerke zu steuern oder Bankgesch√§fte zu t√§tigen, aber es kann eine gro√üe Anzahl von Multiplikationen und Additionen f√ºr neuronale Netze mit einer unglaublichen Geschwindigkeit verarbeiten, w√§hrend es viel weniger Energie verbraucht und in ein kleineres physisches Volumen passt. <br><br>  Die Hauptsache, die ihm dies erm√∂glicht, ist die radikale Beseitigung des von Neumann-Architekturengpasses.  Da die Hauptaufgabe von TPU die Matrixverarbeitung ist, waren die Schaltungsentwickler mit allen erforderlichen Berechnungsschritten vertraut.  Daher konnten sie Tausende von Multiplikatoren und Addierern platzieren und physikalisch verbinden, um eine gro√üe physikalische Matrix zu bilden.  Dies wird als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pipeline-Array-Architektur bezeichnet</a> .  Im Fall von Cloud TPU v2 werden zwei Pipeline-Arrays von 128 x 128 verwendet, was insgesamt 32.768 ALUs f√ºr 16-Bit-Gleitkommawerte auf einem Prozessor ergibt. <br><br>  Mal sehen, wie ein Pipeline-Array Berechnungen f√ºr ein neuronales Netzwerk durchf√ºhrt.  Zun√§chst l√§dt die TPU die Parameter aus dem Speicher in eine Matrix aus Multiplikatoren und Addierern. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9ec/de3/fc6/9ecde3fc6d69116db89aacd83bdf15e5.gif"><br><br>  Die TPU l√§dt dann die Daten aus dem Speicher.  Nach Abschluss jeder Multiplikation wird das Ergebnis an die folgenden Faktoren √ºbertragen, w√§hrend Additionen durchgef√ºhrt werden.  Daher ist die Ausgabe die Summe aller Multiplikationen der Daten und Parameter.  W√§hrend des gesamten Prozesses der volumetrischen Berechnung und Daten√ºbertragung ist der Zugriff auf den Speicher v√∂llig unn√∂tig. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/04a/ef8/b31/04aef8b31b8eb550ba093df4eb811d58.gif"><br><br>  Daher zeigt TPU einen h√∂heren Durchsatz bei der Berechnung f√ºr neuronale Netze, verbraucht viel weniger Energie und nimmt weniger Platz ein. <br><br><h2>  Vorteil: 5 mal weniger Kosten </h2><br>  Was sind die Vorteile der TPU-Architektur?  Kosten.  Hier sind die Kosten f√ºr Cloud TPU v2 f√ºr August 2018 zum Zeitpunkt des Schreibens: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e2f/340/d86/e2f340d861ef00ac9ceac0e7d6dc5f24.png"><br>  Normale und TPU-Arbeitskosten f√ºr verschiedene Regionen von Google Cloud <br><br>  Die Stanford University verteilt eine Reihe von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DAWNBench-</a> Tests, mit denen die Leistung von Deep-Learning-Systemen gemessen wird.  Dort k√∂nnen Sie verschiedene Kombinationen von Aufgaben, Modellen und Computerplattformen sowie die entsprechenden Testergebnisse anzeigen. <br><br>  Zum Ende des Wettbewerbs im April 2018 betrugen die Mindestschulungskosten f√ºr Prozessoren mit einer anderen Architektur als TPU 72,40 USD (f√ºr die Schulung von ResNet-50 mit einer Genauigkeit von 93% f√ºr ImageNet vor <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ort</a> ).  Mit Cloud TPU v2 kann dieses Training f√ºr 12,87 USD durchgef√ºhrt werden.  Dies ist weniger als 1/5 der Kosten.  Dies ist die Kraft der Architektur, die speziell f√ºr neuronale Netze entwickelt wurde. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de422317/">https://habr.com/ru/post/de422317/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de422303/index.html">Bester SQL Builder - Verwenden Sie jOOQ unter Android</a></li>
<li><a href="../de422305/index.html">Verteilung der Anzahl russischer Arbeitnehmer nach Gehalt basierend auf einer gro√üen Online-Umfrage auf einer nicht spezialisierten Plattform</a></li>
<li><a href="../de422309/index.html">So sch√ºtzen Sie Daten in neuronalen Cloud-Netzen - eine neue Verschl√ºsselungsmethode wird vorgeschlagen</a></li>
<li><a href="../de422311/index.html">Interessant und n√ºtzlich von Python. Teil 2</a></li>
<li><a href="../de422315/index.html">Wie man einen Insektenj√§ger √ºberlebt: t√§glicher Kampf ums Einkommen</a></li>
<li><a href="../de422319/index.html">Zum ersten Mal stieg das russische Team in den gr√∂√üten wissenschaftlichen Beschleuniger IndieBio ein</a></li>
<li><a href="../de422321/index.html">Optimierung der Arbeit mit Prototypen in JavaScript-Engines</a></li>
<li><a href="../de422323/index.html">Hacker: Russland und China</a></li>
<li><a href="../de422325/index.html">DevDay zum Testen: Entspannen Sie sich. Testen Sie es einfach</a></li>
<li><a href="../de422327/index.html">Projektplan gegen R√ºckstand: Kampf ohne Chancen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>