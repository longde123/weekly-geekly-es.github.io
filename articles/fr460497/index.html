<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🦅 ✌🏽 ♀️ Utilisation intuitive des méthodes de Monte Carlo avec les chaînes de Markov 🧓🏼 💽 👤</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Est-ce facile? J'ai essayé 
 Alexey Kuzmin, directeur du développement des données et du travail chez DomKlik, professeur en science des données à Net...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Utilisation intuitive des méthodes de Monte Carlo avec les chaînes de Markov</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/netologyru/blog/460497/"><h4>  Est-ce facile?  J'ai essayé </h4><br>  <i>Alexey Kuzmin, directeur du développement des données et du travail chez DomKlik, professeur en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">science des données</a> à Netology, a traduit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">un article de</a> Rahul Agarwal sur la façon dont les méthodes de Monte Carlo fonctionnent avec les chaînes de Markov pour résoudre les problèmes avec un grand espace d'état.</i> <br><a name="habracut"></a><br>  Tout le monde associé à la science des données a déjà entendu parler des méthodes de Monte Carlo avec les chaînes de Markov (MCMC).  Parfois, le sujet est abordé lors de l'étude des statistiques bayésiennes, parfois lors de l'utilisation d'outils comme Prophète. <br><br>  Mais MCMC est difficile à comprendre.  Chaque fois que je lis sur ces méthodes, j'ai remarqué que l'essence de MCMC est cachée dans les couches profondes de bruit mathématique, et il est difficile de distinguer ce bruit.  J'ai dû passer plusieurs heures à comprendre ce concept. <br><br>  Dans cet article, une tentative d'expliquer les méthodes de Monte Carlo avec des chaînes de Markov est disponible, de sorte qu'il devienne clair à quoi elles servent.  Je vais me concentrer sur quelques autres façons d'utiliser ces méthodes dans mon prochain post. <br><br>  Commençons donc.  MCMC se compose de deux termes: chaînes de Monte Carlo et de Markov.  Parlons de chacun d'eux. <br><br><h2>  Monte Carlo </h2><br><img src="https://habrastorage.org/webt/i8/wx/2n/i8wx2nylvemkcfvwp7rxvznjfa0.jpeg"><br><br>  En termes simples <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">, les méthodes de Monte Carlo</a> peuvent être définies comme de simples simulations. <br><br>  Monte Carlo Methods tire son nom du Monte Carlo Casino de Monaco.  Dans de nombreux jeux de cartes, vous devez connaître la probabilité de gagner le croupier.  Parfois, le calcul de cette probabilité peut être mathématiquement compliqué ou insoluble.  Mais nous pouvons toujours exécuter une simulation informatique pour jouer plusieurs fois à l'ensemble du jeu et considérer la probabilité comme le nombre de victoires divisé par le nombre de parties jouées. <br>  C'est tout ce que vous devez savoir sur les méthodes de Monte Carlo.  Oui, c'est juste une technique de modélisation simple avec un nom de fantaisie. <br><br><h2>  Chaînes de Markov </h2><br><img src="https://habrastorage.org/webt/7r/my/np/7rmynpvle1yn4xk5tuwqdvkd7mo.jpeg"><br><br>  Étant donné que le terme MCMC se compose de deux parties, vous devez toujours comprendre ce que sont <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">les chaînes de Markov</a> .  Mais avant de passer aux chaînes de Markov, parlons un peu des propriétés de Markov. <br><br>  Supposons qu'il existe un système d'états possibles M et que vous passez d'un état à un autre.  Que rien ne vous dérange encore.  Un exemple spécifique d'un tel système est la météo, qui passe de chaud à froid à modéré.  Un autre exemple est le marché boursier, qui passe d'un état baissier à un état haussier et stagnant. <br><br>  <i>La propriété de Markov</i> suggère que pour un processus donné qui se trouve dans l'état X <sub>n</sub> à un instant particulier, la probabilité X <sub>n + 1</sub> = k (où k est l'un des M-états vers lesquels le processus peut aller) ne dépend que de quelle est cette condition en ce moment.  Et pas sur la façon dont il a atteint son état actuel. <br>  En termes mathématiques, nous pouvons écrire ceci sous la forme de la formule suivante: <br><img src="https://habrastorage.org/webt/tw/kq/se/twkqseq2tra2alhiqdyfuyswnla.png"><br>  Pour plus de clarté, vous ne vous souciez pas de la séquence de conditions que le marché a prises pour devenir haussier.  La probabilité que le prochain État soit «baissier» n'est déterminée que par le fait que le marché est actuellement dans un état «haussier».  Cela a également du sens dans la pratique. <br><br>  Un processus avec une propriété Markov est appelé processus Markov.  Pourquoi la chaîne Markov est-elle importante?  En raison de sa distribution stationnaire. <br><br><h3>  Qu'est-ce que la distribution stationnaire? </h3><br>  Je vais essayer d'expliquer la distribution stationnaire en la calculant pour l'exemple ci-dessous.  Supposons que vous ayez un processus Markov pour le marché boursier, comme indiqué ci-dessous. <br><img src="https://habrastorage.org/webt/1k/o-/bb/1ko-bbb9hzmv8j9o_an1ocvyurs.png"><br>  Vous disposez d'une matrice de probabilité de transition qui détermine la probabilité d'une transition de l'état X <sub>i</sub> à X <sub>j</sub> . <br><img src="https://habrastorage.org/webt/or/u7/jp/oru7jpnpioj12jbrcw7t4o6sk94.png"><br>  Matrice de probabilité de transition, Q <br><br>  Dans la matrice de probabilité transitoire Q, la probabilité que le prochain état soit «bull», étant donné l'état actuel de «bull» = 0,9;  la probabilité que l'état suivant soit «baissier» si l'état actuel est «taureau» = 0,075.  Et ainsi de suite. <br><br>  Eh bien, commençons par un état particulier.  Notre état sera déterminé par le vecteur [taureau, ours, stagnation].  Si nous commençons par un état «baissier», le vecteur sera comme ceci: [0,1,0].  Nous pouvons calculer la distribution de probabilité pour l'état suivant en multipliant le vecteur d'état actuel par la matrice de probabilité de transition. <br><img src="https://habrastorage.org/webt/or/jy/85/orjy85onzacwcu2wlgftmugzygk.png"><br>  <b>Notez que les probabilités totalisent 1.</b> <br><br>  La distribution des états suivante peut être trouvée par la formule: <br><img src="https://habrastorage.org/webt/ge/wk/fu/gewkfuf7xziuajc2ox7tn9blfcm.png"><br><br>  Et ainsi de suite.  À la fin, vous atteindrez un état stationnaire dans lequel l'état se stabilise: <br><img src="https://habrastorage.org/webt/iy/5y/65/iy5y65fxgxo4foqpe3fpq2hs-dq.png"><br><br>  Pour la matrice de probabilité de transition Q décrite ci-dessus, la distribution stationnaire s est <br><img src="https://habrastorage.org/webt/ae/ut/_8/aeut_8m8wsypenpgkig3onnzwdc.png"><br>  Vous pouvez obtenir une distribution stationnaire avec le code suivant: <br><br><pre><code class="python hljs">Q = np.matrix([[<span class="hljs-number"><span class="hljs-number">0.9</span></span>,<span class="hljs-number"><span class="hljs-number">0.075</span></span>,<span class="hljs-number"><span class="hljs-number">0.025</span></span>],[<span class="hljs-number"><span class="hljs-number">0.15</span></span>,<span class="hljs-number"><span class="hljs-number">0.8</span></span>,<span class="hljs-number"><span class="hljs-number">0.05</span></span>],[<span class="hljs-number"><span class="hljs-number">0.25</span></span>,<span class="hljs-number"><span class="hljs-number">0.25</span></span>,<span class="hljs-number"><span class="hljs-number">0.5</span></span>]]) init_s = np.matrix([[<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span> , <span class="hljs-number"><span class="hljs-number">0</span></span>]]) epsilon =<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> epsilon&gt;<span class="hljs-number"><span class="hljs-number">10e-9</span></span>:    next_s = np.dot(init_s,Q)    epsilon = np.sqrt(np.sum(np.square(next_s - init_s)))    init_s = next_s print(init_s) ------------------------------------------------------------------ matrix([[<span class="hljs-number"><span class="hljs-number">0.62499998</span></span>, <span class="hljs-number"><span class="hljs-number">0.31250002</span></span>, <span class="hljs-number"><span class="hljs-number">0.0625</span></span>  ]])</code> </pre> <br>  Vous pouvez également partir de n'importe quel autre état - obtenir la même distribution stationnaire.  Modifiez l'état initial dans le code si vous voulez vous en assurer. <br><br>  Nous pouvons maintenant répondre à la question de savoir pourquoi la distribution stationnaire est si importante. <br><br>  La distribution stationnaire est importante car elle peut être utilisée pour déterminer la probabilité qu'un système se trouve dans un certain état au hasard. <br><br>  Pour notre exemple, on peut dire que dans 62,5% des cas le marché sera dans un état «haussier», 31,25% dans un état «baissier» et 6,25% en stagnation. <br><br>  Intuitivement, vous pouvez voir cela comme une errance aléatoire autour de la chaîne. <br><br><img src="https://habrastorage.org/webt/i3/e6/xt/i3e6xtqko2iip-janj6dvfbnv4q.png"><br>  Marche aléatoire <br><br>  Vous êtes à un certain point et choisissez l'état suivant, en observant la distribution de probabilité de l'état suivant, en tenant compte de l'état actuel.  Nous pouvons visiter certains nœuds plus souvent que d'autres, en fonction des probabilités de ces nœuds. <br><br>  C'est ainsi que Google a résolu le problème de la recherche à l'aube d'Internet.  Le problème était de trier les pages, selon leur importance.  Google a résolu le problème en utilisant l'algorithme Pagerank.  L'algorithme de Google Pagerank devrait considérer l'état comme une page et la probabilité d'une page dans une distribution stationnaire comme son importance relative. <br><br>  Nous passons maintenant directement à l'examen des méthodes MCMC. <br><br><h2>  Quelles sont les méthodes de Monte Carlo avec les chaînes de Markov (MCMC) </h2><br>  Avant de répondre à ce qu'est MCMC, permettez-moi de poser une question.  Nous connaissons la distribution bêta.  Nous connaissons sa fonction de densité de probabilité.  Mais peut-on prélever un échantillon de cette distribution?  Pouvez-vous trouver un moyen de le faire? <br><br><img src="https://habrastorage.org/webt/ks/ai/wd/ksaiwdv7lomes7g55ihqbhxushs.png"><br>  Pensez ... <br><br>  MCMC vous permet de choisir parmi n'importe quelle distribution de probabilité.  Ceci est particulièrement important lorsque vous devez effectuer une sélection à partir de la distribution postérieure. <br><img src="https://habrastorage.org/webt/12/kn/-5/12kn-58z9ub6t2ft1lctptwix28.png"><br>  La figure montre le théorème de Bayes. <br><br>  Par exemple, vous devez faire un échantillon à partir d'une distribution postérieure.  Mais est-il facile de calculer la composante postérieure avec la constante de normalisation (évidence)?  Dans la plupart des cas, vous pouvez les trouver sous la forme d'un produit de vraisemblance et de probabilité a priori.  Mais calculer la constante de normalisation (p (D)) ne fonctionne pas.  Pourquoi?  Examinons de plus près. <br><br>  Supposons que H ne prenne que 3 valeurs: <br><br>  p (D) = p (H = H1) .p (D | H = H1) + p (H = H2) .p (D | H = H2) + p (H = H3) .p (D | H = H3) <br><br>  Dans ce cas, p (D) est facile à calculer.  Mais que faire si la valeur de H est continue?  Serait-il possible de calculer cela aussi facilement, surtout si H prenait des valeurs infinies?  Pour ce faire, une intégrale complexe devrait être résolue. <br><br>  Nous voulons faire une sélection aléatoire à partir de la distribution postérieure, mais nous voulons également considérer p (D) comme une constante. <br><br>  Wikipédia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">écrit</a> : <br><br>  Les méthodes de Monte Carlo avec des chaînes de Markov sont une classe d'algorithmes d'échantillonnage à partir d'une distribution de probabilité, basée sur la construction d'une chaîne de Markov, qui en tant que distribution stationnaire a la forme souhaitée.  L'état de la chaîne après une série d'étapes est ensuite utilisé comme sélection dans la distribution souhaitée.  La qualité de l'échantillonnage s'améliore avec l'augmentation du nombre d'étapes. <br><br>  Regardons un exemple.  Disons que vous avez besoin d'un échantillon de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">distribution bêta</a> .  Sa densité: <br><img src="https://habrastorage.org/webt/ag/xh/wj/agxhwjqglfhcz2bl88eu3ov84ja.png"><br><br>  où C est la constante de normalisation.  En fait, c'est une fonction de α et β, mais je veux montrer qu'elle n'est pas nécessaire pour un échantillon de la distribution bêta, nous allons donc la prendre comme constante. <br><br>  Le problème de distribution bêta est vraiment difficile, sinon pratiquement insoluble.  En réalité, vous devrez peut-être travailler avec des fonctions de distribution plus complexes, et parfois vous ne connaîtrez pas les constantes de normalisation. <br><br>  Les méthodes MCMC facilitent la vie en fournissant des algorithmes qui pourraient créer une chaîne de Markov ayant une distribution bêta comme distribution stationnaire, étant donné que nous pouvons choisir parmi une distribution uniforme (ce qui est relativement simple). <br><br>  Si nous commençons par un état aléatoire et passons à l'état suivant sur la base d'un algorithme plusieurs fois, nous créerons finalement une chaîne de Markov avec une distribution bêta en tant que distribution stationnaire.  Et les états que nous nous trouvons depuis longtemps peuvent être utilisés comme échantillon de la distribution bêta. <br><br>  L'un de ces algorithmes MCMC est l'algorithme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Metropolis-Hastings.</a> <br><br><h2>  Algorithme de Metropolis-Hastings </h2><br><img src="https://habrastorage.org/webt/qq/oj/89/qqoj89il8-rsyd3xo-sqawnrug0.jpeg"><br><br><h3>  Intuition: </h3><br>  Quel est donc le but? <br><br>  <i>Intuitivement, nous voulons parcourir une partie de la surface (notre chaîne de Markov) de manière à ce que le temps que nous passons à chaque emplacement soit proportionnel à la hauteur de la surface à cet emplacement (la densité de probabilité souhaitée à partir de laquelle nous voulons effectuer une sélection).</i> <i><br><br></i>  <i>Ainsi, par exemple, nous aimerions passer deux fois plus de temps sur une colline de 100 mètres de haut que sur une colline voisine de 50 mètres.</i>  <i>C'est bien que nous puissions faire cela même si nous ne connaissons pas les hauteurs absolues des points sur la surface: tout ce que vous devez savoir, ce sont les hauteurs relatives.</i>  <i>Par exemple, si le sommet de la colline A est deux fois plus élevé que le sommet de la colline B, alors nous aimerions passer deux fois plus de temps en A qu'en B.</i> <i><br><br></i>  <i>Il existe des schémas plus complexes pour proposer de nouveaux lieux et de nouvelles règles pour leur adoption, mais l'idée principale est la suivante:</i> <i><br><br></i> <ol><li>  <i>Choisissez un nouvel emplacement "suggéré".</i> </li><li>  <i>Découvrez à quel point cet emplacement est supérieur ou inférieur à celui actuel.</i> </li><li>  <i>Rester sur place ou déménager dans un nouvel endroit avec une probabilité proportionnelle à la hauteur des lieux.</i> </li></ol> <i><br></i>  <i>Le but du MCMC est de choisir parmi une certaine distribution de probabilité sans avoir à connaître sa hauteur exacte à aucun moment (pas besoin de connaître C).</i> <i><br></i>  <i>Si le processus «errance» est correctement configuré, vous pouvez vous assurer que cette proportionnalité (entre le temps passé et la hauteur de distribution) est atteinte</i> . <br><br><h3>  Algorithme: </h3><br>  Définissons et décrivons maintenant la tâche en termes plus formels.  Soit s = (s1, s2, ..., sM) la distribution stationnaire souhaitée.  Nous voulons créer une chaîne de Markov avec une telle distribution stationnaire.  Nous commençons par une chaîne de Markov arbitraire avec des états M avec la matrice de transition P, de sorte que pij représente la probabilité de transition de l'état i à j. <br><br>  Intuitivement, nous savons parcourir la chaîne de Markov, mais la chaîne de Markov n'a pas la distribution stationnaire requise.  Cette chaîne a une distribution stationnaire (dont nous n'avons pas besoin).  Notre objectif est de changer la façon dont nous nous promenons dans la chaîne de Markov afin que la chaîne ait la distribution stationnaire souhaitée. <br><br>  Pour ce faire: <br><br><ol><li>  Commencez avec un état initial aléatoire i. </li><li>  Sélectionnez aléatoirement un nouvel état supposé en examinant les probabilités de transition dans la ième ligne de la matrice de transition P. </li><li>  Calculez une mesure appelée probabilité de décision, définie comme suit: aij = min (sj.pji / si.pij, 1). </li><li>  Lancez maintenant une pièce qui atterrit à la surface de l'aigle avec une probabilité aij.  Si un aigle tombe, acceptez l'offre, c'est-à-dire passez à l'état suivant, sinon, rejetez l'offre, c'est-à-dire restez dans l'état actuel. <br></li><li>  Répétez plusieurs fois. <br></li></ol><br>  Après un grand nombre de tests, cette chaîne convergera et aura une distribution stationnaire s.  Ensuite, nous pouvons utiliser des états de chaîne comme échantillon de n'importe quelle distribution. <br><br>  En procédant ainsi pour échantillonner la distribution bêta, la seule fois où vous avez à utiliser la densité de probabilité est de rechercher la probabilité de prendre une décision.  Pour ce faire, divisez sj par si (c'est-à-dire que la constante de normalisation C est annulée). <br><br><h3>  Sélection bêta </h3><br><img src="https://habrastorage.org/webt/h9/ac/zw/h9aczwarm4hfwm0pya3hai-rg2e.jpeg"><br><br>  Passons maintenant au problème de l'échantillonnage à partir de la distribution bêta. <br><br>  Une distribution bêta est une distribution continue sur [0,1] et peut avoir des valeurs infinies sur [0,1].  Supposons qu'une chaîne de Markov arbitraire P avec des états infinis sur [0,1] a une matrice de transition P telle que pij = pji = tous les éléments de la matrice. <br><br>  Nous n'avons pas besoin de Matrix P, comme nous le verrons plus loin, mais je veux que la description du problème soit aussi proche que possible de l'algorithme que nous avons proposé. <br><br><ul><li>  Commençons par un état initial aléatoire i obtenu à partir d'une distribution uniforme sur (0,1). </li><li>  Sélectionnez aléatoirement un nouvel état supposé en regardant les probabilités de transition dans la ième ligne de la matrice de transition P. Supposons que nous choisissions un autre état Unif (0,1) comme état supposé j. </li><li>  Calculez la mesure, qui est appelée la probabilité de prendre une décision: </li></ul><br><img src="https://habrastorage.org/webt/lp/jo/-0/lpjo-0phzn3o8zl83oniptticmu.png"><br>  Ce qui simplifie: <br><img src="https://habrastorage.org/webt/4c/he/pi/4chepi6_1om84fk52t8jquzpmku.png"><br>  Puisque pji = pij, et où <br><img src="https://habrastorage.org/webt/pm/qc/y2/pmqcy2hanok1y-mnhbxk49dqbve.png"><br><ul><li>  Lancez maintenant une pièce.  Il est probable qu'un aigle tombera.  Si un aigle tombe, vous devez accepter l'offre, c'est-à-dire passer à l'état suivant.  Sinon, il vaut la peine de rejeter l'offre, c'est-à-dire de rester dans le même état. </li><li>  Répétez le test plusieurs fois. </li></ul><br><h3>  Code: </h3><br>  Il est temps de passer de la théorie à la pratique.  Nous allons écrire notre échantillon bêta en Python. <br><br><pre> <code class="python hljs">impo rt rand om <span class="hljs-comment"><span class="hljs-comment"># Lets define our Beta Function to generate s for any particular state. We don't care for the normalizing constant here. def beta_s(w,a,b): return w**(a-1)*(1-w)**(b-1) # This Function returns True if the coin with probability P of heads comes heads when flipped. def random_coin(p): unif = random.uniform(0,1) if unif&gt;=p: return False else: return True # This Function runs the MCMC chain for Beta Distribution. def beta_mcmc(N_hops,a,b): states = [] cur = random.uniform(0,1) for i in range(0,N_hops): states.append(cur) next = random.uniform(0,1) ap = min(beta_s(next,a,b)/beta_s(cur,a,b),1) # Calculate the acceptance probability if random_coin(ap): cur = next return states[-1000:] # Returns the last 100 states of the chain</span></span></code> </pre><br>  Comparez les résultats avec la distribution bêta réelle. <br><br><pre> <code class="python hljs">impo rt num py <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pylab <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pl <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> scipy.special <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> ss %matplotlib inline pl.rcParams[<span class="hljs-string"><span class="hljs-string">'figure.figsize'</span></span>] = (<span class="hljs-number"><span class="hljs-number">17.0</span></span>, <span class="hljs-number"><span class="hljs-number">4.0</span></span>) <span class="hljs-comment"><span class="hljs-comment"># Actual Beta PDF. def beta(a, b, i): e1 = ss.gamma(a + b) e2 = ss.gamma(a) e3 = ss.gamma(b) e4 = i ** (a - 1) e5 = (1 - i) ** (b - 1) return (e1/(e2*e3)) * e4 * e5 # Create a function to plot Actual Beta PDF with the Beta Sampled from MCMC Chain. def plot_beta(a, b): Ly = [] Lx = [] i_list = np.mgrid[0:1:100j] for i in i_list: Lx.append(i) Ly.append(beta(a, b, i)) pl.plot(Lx, Ly, label="Real Distribution: a="+str(a)+", b="+str(b)) pl.hist(beta_mcmc(100000,a,b),normed=True,bins =25, histtype='step',label="Simulated_MCMC: a="+str(a)+", b="+str(b)) pl.legend() pl.show() plot_beta(0.1, 0.1) plot_beta(1, 1) plot_beta(2, 3)</span></span></code> </pre><br><br><img src="https://habrastorage.org/webt/6z/b_/zb/6zb_zbywfexkiagddl4lpusmcko.png"><br><br>  Comme vous pouvez le voir, les valeurs sont très similaires à la distribution bêta.  Ainsi, le réseau MCMC a atteint un état stationnaire <br><br>  Dans le code ci-dessus, nous avons créé un échantillonneur bêta, mais le même concept s'applique à toute autre distribution à partir de laquelle nous voulons effectuer une sélection. <br><br><h2>  Conclusions </h2><br><img src="https://habrastorage.org/webt/5f/oh/h5/5fohh5w_hsavzw3yvbryxewnnkw.png"><br><br>  C'était un super article.  Félicitations si vous l'avez lu jusqu'à la fin. <br><br>  En substance, les méthodes MCMC peuvent être complexes, mais elles nous offrent une grande flexibilité.  Vous pouvez sélectionner n'importe quelle fonction de distribution en utilisant la sélection via le MCMC.  En règle générale, ces méthodes sont utilisées pour échantillonner à partir de distributions postérieures. <br><br>  Vous pouvez également utiliser MCMC pour résoudre des problèmes avec un grand espace d'état.  Par exemple, dans un problème de sac à dos ou pour le déchiffrement.  Je vais essayer de vous fournir des exemples plus intéressants dans le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">prochain</a> post.  Restez à l'écoute. <br><br><h2>  Des éditeurs </h2><br><ul><li>  Cours <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Python pour travailler avec des données</a> </li><li>  Cours d' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">apprentissage automatique en</a> ligne </li><li>  Cours en ligne " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">BIG DATA from scratch</a> " </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr460497/">https://habr.com/ru/post/fr460497/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr460485/index.html">Comment un tournoi en ligne peut décourager "la fin de la semaine prochaine"</a></li>
<li><a href="../fr460489/index.html">TOP 11 erreurs dans le développement du BCP</a></li>
<li><a href="../fr460491/index.html">Capteur de température et d'humidité Arduino avec envoi et traçage (partie 1)</a></li>
<li><a href="../fr460493/index.html">«Killer apps» pour PC des années 80: VisiCalc et WordStar</a></li>
<li><a href="../fr460495/index.html">Container-to-pipeline: CRI-O est désormais la valeur par défaut dans OpenShift Container Platform 4</a></li>
<li><a href="../fr460499/index.html">Trois lauréats du prix Dijkstra: comment Hydra 2019 et SPTDC 2019 se sont-ils déroulés?</a></li>
<li><a href="../fr460501/index.html">Exemple d'implémentation d'intégration continue à l'aide de BuildBot</a></li>
<li><a href="../fr460503/index.html">Configuration sans fil du Raspberry PI 3 B +</a></li>
<li><a href="../fr460505/index.html">Attirer trois croix, ou pourquoi les projets sont si difficiles à terminer à temps</a></li>
<li><a href="../fr460509/index.html">Comment les mandataires résidents aident-ils dans les affaires: un cas réel d'utilisation d'Infatica dans l'exploration de données</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>