<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü¶Ö ‚úåüèΩ ‚ôÄÔ∏è Utilisation intuitive des m√©thodes de Monte Carlo avec les cha√Ænes de Markov üßìüèº üíΩ üë§</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Est-ce facile? J'ai essay√© 
 Alexey Kuzmin, directeur du d√©veloppement des donn√©es et du travail chez DomKlik, professeur en science des donn√©es √† Net...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Utilisation intuitive des m√©thodes de Monte Carlo avec les cha√Ænes de Markov</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/netologyru/blog/460497/"><h4>  Est-ce facile?  J'ai essay√© </h4><br>  <i>Alexey Kuzmin, directeur du d√©veloppement des donn√©es et du travail chez DomKlik, professeur en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">science des donn√©es</a> √† Netology, a traduit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">un article de</a> Rahul Agarwal sur la fa√ßon dont les m√©thodes de Monte Carlo fonctionnent avec les cha√Ænes de Markov pour r√©soudre les probl√®mes avec un grand espace d'√©tat.</i> <br><a name="habracut"></a><br>  Tout le monde associ√© √† la science des donn√©es a d√©j√† entendu parler des m√©thodes de Monte Carlo avec les cha√Ænes de Markov (MCMC).  Parfois, le sujet est abord√© lors de l'√©tude des statistiques bay√©siennes, parfois lors de l'utilisation d'outils comme Proph√®te. <br><br>  Mais MCMC est difficile √† comprendre.  Chaque fois que je lis sur ces m√©thodes, j'ai remarqu√© que l'essence de MCMC est cach√©e dans les couches profondes de bruit math√©matique, et il est difficile de distinguer ce bruit.  J'ai d√ª passer plusieurs heures √† comprendre ce concept. <br><br>  Dans cet article, une tentative d'expliquer les m√©thodes de Monte Carlo avec des cha√Ænes de Markov est disponible, de sorte qu'il devienne clair √† quoi elles servent.  Je vais me concentrer sur quelques autres fa√ßons d'utiliser ces m√©thodes dans mon prochain post. <br><br>  Commen√ßons donc.  MCMC se compose de deux termes: cha√Ænes de Monte Carlo et de Markov.  Parlons de chacun d'eux. <br><br><h2>  Monte Carlo </h2><br><img src="https://habrastorage.org/webt/i8/wx/2n/i8wx2nylvemkcfvwp7rxvznjfa0.jpeg"><br><br>  En termes simples <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">, les m√©thodes de Monte Carlo</a> peuvent √™tre d√©finies comme de simples simulations. <br><br>  Monte Carlo Methods tire son nom du Monte Carlo Casino de Monaco.  Dans de nombreux jeux de cartes, vous devez conna√Ætre la probabilit√© de gagner le croupier.  Parfois, le calcul de cette probabilit√© peut √™tre math√©matiquement compliqu√© ou insoluble.  Mais nous pouvons toujours ex√©cuter une simulation informatique pour jouer plusieurs fois √† l'ensemble du jeu et consid√©rer la probabilit√© comme le nombre de victoires divis√© par le nombre de parties jou√©es. <br>  C'est tout ce que vous devez savoir sur les m√©thodes de Monte Carlo.  Oui, c'est juste une technique de mod√©lisation simple avec un nom de fantaisie. <br><br><h2>  Cha√Ænes de Markov </h2><br><img src="https://habrastorage.org/webt/7r/my/np/7rmynpvle1yn4xk5tuwqdvkd7mo.jpeg"><br><br>  √âtant donn√© que le terme MCMC se compose de deux parties, vous devez toujours comprendre ce que sont <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">les cha√Ænes de Markov</a> .  Mais avant de passer aux cha√Ænes de Markov, parlons un peu des propri√©t√©s de Markov. <br><br>  Supposons qu'il existe un syst√®me d'√©tats possibles M et que vous passez d'un √©tat √† un autre.  Que rien ne vous d√©range encore.  Un exemple sp√©cifique d'un tel syst√®me est la m√©t√©o, qui passe de chaud √† froid √† mod√©r√©.  Un autre exemple est le march√© boursier, qui passe d'un √©tat baissier √† un √©tat haussier et stagnant. <br><br>  <i>La propri√©t√© de Markov</i> sugg√®re que pour un processus donn√© qui se trouve dans l'√©tat X <sub>n</sub> √† un instant particulier, la probabilit√© X <sub>n + 1</sub> = k (o√π k est l'un des M-√©tats vers lesquels le processus peut aller) ne d√©pend que de quelle est cette condition en ce moment.  Et pas sur la fa√ßon dont il a atteint son √©tat actuel. <br>  En termes math√©matiques, nous pouvons √©crire ceci sous la forme de la formule suivante: <br><img src="https://habrastorage.org/webt/tw/kq/se/twkqseq2tra2alhiqdyfuyswnla.png"><br>  Pour plus de clart√©, vous ne vous souciez pas de la s√©quence de conditions que le march√© a prises pour devenir haussier.  La probabilit√© que le prochain √âtat soit ¬´baissier¬ª n'est d√©termin√©e que par le fait que le march√© est actuellement dans un √©tat ¬´haussier¬ª.  Cela a √©galement du sens dans la pratique. <br><br>  Un processus avec une propri√©t√© Markov est appel√© processus Markov.  Pourquoi la cha√Æne Markov est-elle importante?  En raison de sa distribution stationnaire. <br><br><h3>  Qu'est-ce que la distribution stationnaire? </h3><br>  Je vais essayer d'expliquer la distribution stationnaire en la calculant pour l'exemple ci-dessous.  Supposons que vous ayez un processus Markov pour le march√© boursier, comme indiqu√© ci-dessous. <br><img src="https://habrastorage.org/webt/1k/o-/bb/1ko-bbb9hzmv8j9o_an1ocvyurs.png"><br>  Vous disposez d'une matrice de probabilit√© de transition qui d√©termine la probabilit√© d'une transition de l'√©tat X <sub>i</sub> √† X <sub>j</sub> . <br><img src="https://habrastorage.org/webt/or/u7/jp/oru7jpnpioj12jbrcw7t4o6sk94.png"><br>  Matrice de probabilit√© de transition, Q <br><br>  Dans la matrice de probabilit√© transitoire Q, la probabilit√© que le prochain √©tat soit ¬´bull¬ª, √©tant donn√© l'√©tat actuel de ¬´bull¬ª = 0,9;  la probabilit√© que l'√©tat suivant soit ¬´baissier¬ª si l'√©tat actuel est ¬´taureau¬ª = 0,075.  Et ainsi de suite. <br><br>  Eh bien, commen√ßons par un √©tat particulier.  Notre √©tat sera d√©termin√© par le vecteur [taureau, ours, stagnation].  Si nous commen√ßons par un √©tat ¬´baissier¬ª, le vecteur sera comme ceci: [0,1,0].  Nous pouvons calculer la distribution de probabilit√© pour l'√©tat suivant en multipliant le vecteur d'√©tat actuel par la matrice de probabilit√© de transition. <br><img src="https://habrastorage.org/webt/or/jy/85/orjy85onzacwcu2wlgftmugzygk.png"><br>  <b>Notez que les probabilit√©s totalisent 1.</b> <br><br>  La distribution des √©tats suivante peut √™tre trouv√©e par la formule: <br><img src="https://habrastorage.org/webt/ge/wk/fu/gewkfuf7xziuajc2ox7tn9blfcm.png"><br><br>  Et ainsi de suite.  √Ä la fin, vous atteindrez un √©tat stationnaire dans lequel l'√©tat se stabilise: <br><img src="https://habrastorage.org/webt/iy/5y/65/iy5y65fxgxo4foqpe3fpq2hs-dq.png"><br><br>  Pour la matrice de probabilit√© de transition Q d√©crite ci-dessus, la distribution stationnaire s est <br><img src="https://habrastorage.org/webt/ae/ut/_8/aeut_8m8wsypenpgkig3onnzwdc.png"><br>  Vous pouvez obtenir une distribution stationnaire avec le code suivant: <br><br><pre><code class="python hljs">Q = np.matrix([[<span class="hljs-number"><span class="hljs-number">0.9</span></span>,<span class="hljs-number"><span class="hljs-number">0.075</span></span>,<span class="hljs-number"><span class="hljs-number">0.025</span></span>],[<span class="hljs-number"><span class="hljs-number">0.15</span></span>,<span class="hljs-number"><span class="hljs-number">0.8</span></span>,<span class="hljs-number"><span class="hljs-number">0.05</span></span>],[<span class="hljs-number"><span class="hljs-number">0.25</span></span>,<span class="hljs-number"><span class="hljs-number">0.25</span></span>,<span class="hljs-number"><span class="hljs-number">0.5</span></span>]]) init_s = np.matrix([[<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span> , <span class="hljs-number"><span class="hljs-number">0</span></span>]]) epsilon =<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> epsilon&gt;<span class="hljs-number"><span class="hljs-number">10e-9</span></span>:    next_s = np.dot(init_s,Q)    epsilon = np.sqrt(np.sum(np.square(next_s - init_s)))    init_s = next_s print(init_s) ------------------------------------------------------------------ matrix([[<span class="hljs-number"><span class="hljs-number">0.62499998</span></span>, <span class="hljs-number"><span class="hljs-number">0.31250002</span></span>, <span class="hljs-number"><span class="hljs-number">0.0625</span></span>  ]])</code> </pre> <br>  Vous pouvez √©galement partir de n'importe quel autre √©tat - obtenir la m√™me distribution stationnaire.  Modifiez l'√©tat initial dans le code si vous voulez vous en assurer. <br><br>  Nous pouvons maintenant r√©pondre √† la question de savoir pourquoi la distribution stationnaire est si importante. <br><br>  La distribution stationnaire est importante car elle peut √™tre utilis√©e pour d√©terminer la probabilit√© qu'un syst√®me se trouve dans un certain √©tat au hasard. <br><br>  Pour notre exemple, on peut dire que dans 62,5% des cas le march√© sera dans un √©tat ¬´haussier¬ª, 31,25% dans un √©tat ¬´baissier¬ª et 6,25% en stagnation. <br><br>  Intuitivement, vous pouvez voir cela comme une errance al√©atoire autour de la cha√Æne. <br><br><img src="https://habrastorage.org/webt/i3/e6/xt/i3e6xtqko2iip-janj6dvfbnv4q.png"><br>  Marche al√©atoire <br><br>  Vous √™tes √† un certain point et choisissez l'√©tat suivant, en observant la distribution de probabilit√© de l'√©tat suivant, en tenant compte de l'√©tat actuel.  Nous pouvons visiter certains n≈ìuds plus souvent que d'autres, en fonction des probabilit√©s de ces n≈ìuds. <br><br>  C'est ainsi que Google a r√©solu le probl√®me de la recherche √† l'aube d'Internet.  Le probl√®me √©tait de trier les pages, selon leur importance.  Google a r√©solu le probl√®me en utilisant l'algorithme Pagerank.  L'algorithme de Google Pagerank devrait consid√©rer l'√©tat comme une page et la probabilit√© d'une page dans une distribution stationnaire comme son importance relative. <br><br>  Nous passons maintenant directement √† l'examen des m√©thodes MCMC. <br><br><h2>  Quelles sont les m√©thodes de Monte Carlo avec les cha√Ænes de Markov (MCMC) </h2><br>  Avant de r√©pondre √† ce qu'est MCMC, permettez-moi de poser une question.  Nous connaissons la distribution b√™ta.  Nous connaissons sa fonction de densit√© de probabilit√©.  Mais peut-on pr√©lever un √©chantillon de cette distribution?  Pouvez-vous trouver un moyen de le faire? <br><br><img src="https://habrastorage.org/webt/ks/ai/wd/ksaiwdv7lomes7g55ihqbhxushs.png"><br>  Pensez ... <br><br>  MCMC vous permet de choisir parmi n'importe quelle distribution de probabilit√©.  Ceci est particuli√®rement important lorsque vous devez effectuer une s√©lection √† partir de la distribution post√©rieure. <br><img src="https://habrastorage.org/webt/12/kn/-5/12kn-58z9ub6t2ft1lctptwix28.png"><br>  La figure montre le th√©or√®me de Bayes. <br><br>  Par exemple, vous devez faire un √©chantillon √† partir d'une distribution post√©rieure.  Mais est-il facile de calculer la composante post√©rieure avec la constante de normalisation (√©vidence)?  Dans la plupart des cas, vous pouvez les trouver sous la forme d'un produit de vraisemblance et de probabilit√© a priori.  Mais calculer la constante de normalisation (p (D)) ne fonctionne pas.  Pourquoi?  Examinons de plus pr√®s. <br><br>  Supposons que H ne prenne que 3 valeurs: <br><br>  p (D) = p (H = H1) .p (D | H = H1) + p (H = H2) .p (D | H = H2) + p (H = H3) .p (D | H = H3) <br><br>  Dans ce cas, p (D) est facile √† calculer.  Mais que faire si la valeur de H est continue?  Serait-il possible de calculer cela aussi facilement, surtout si H prenait des valeurs infinies?  Pour ce faire, une int√©grale complexe devrait √™tre r√©solue. <br><br>  Nous voulons faire une s√©lection al√©atoire √† partir de la distribution post√©rieure, mais nous voulons √©galement consid√©rer p (D) comme une constante. <br><br>  Wikip√©dia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">√©crit</a> : <br><br>  Les m√©thodes de Monte Carlo avec des cha√Ænes de Markov sont une classe d'algorithmes d'√©chantillonnage √† partir d'une distribution de probabilit√©, bas√©e sur la construction d'une cha√Æne de Markov, qui en tant que distribution stationnaire a la forme souhait√©e.  L'√©tat de la cha√Æne apr√®s une s√©rie d'√©tapes est ensuite utilis√© comme s√©lection dans la distribution souhait√©e.  La qualit√© de l'√©chantillonnage s'am√©liore avec l'augmentation du nombre d'√©tapes. <br><br>  Regardons un exemple.  Disons que vous avez besoin d'un √©chantillon de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">distribution b√™ta</a> .  Sa densit√©: <br><img src="https://habrastorage.org/webt/ag/xh/wj/agxhwjqglfhcz2bl88eu3ov84ja.png"><br><br>  o√π C est la constante de normalisation.  En fait, c'est une fonction de Œ± et Œ≤, mais je veux montrer qu'elle n'est pas n√©cessaire pour un √©chantillon de la distribution b√™ta, nous allons donc la prendre comme constante. <br><br>  Le probl√®me de distribution b√™ta est vraiment difficile, sinon pratiquement insoluble.  En r√©alit√©, vous devrez peut-√™tre travailler avec des fonctions de distribution plus complexes, et parfois vous ne conna√Ætrez pas les constantes de normalisation. <br><br>  Les m√©thodes MCMC facilitent la vie en fournissant des algorithmes qui pourraient cr√©er une cha√Æne de Markov ayant une distribution b√™ta comme distribution stationnaire, √©tant donn√© que nous pouvons choisir parmi une distribution uniforme (ce qui est relativement simple). <br><br>  Si nous commen√ßons par un √©tat al√©atoire et passons √† l'√©tat suivant sur la base d'un algorithme plusieurs fois, nous cr√©erons finalement une cha√Æne de Markov avec une distribution b√™ta en tant que distribution stationnaire.  Et les √©tats que nous nous trouvons depuis longtemps peuvent √™tre utilis√©s comme √©chantillon de la distribution b√™ta. <br><br>  L'un de ces algorithmes MCMC est l'algorithme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Metropolis-Hastings.</a> <br><br><h2>  Algorithme de Metropolis-Hastings </h2><br><img src="https://habrastorage.org/webt/qq/oj/89/qqoj89il8-rsyd3xo-sqawnrug0.jpeg"><br><br><h3>  Intuition: </h3><br>  Quel est donc le but? <br><br>  <i>Intuitivement, nous voulons parcourir une partie de la surface (notre cha√Æne de Markov) de mani√®re √† ce que le temps que nous passons √† chaque emplacement soit proportionnel √† la hauteur de la surface √† cet emplacement (la densit√© de probabilit√© souhait√©e √† partir de laquelle nous voulons effectuer une s√©lection).</i> <i><br><br></i>  <i>Ainsi, par exemple, nous aimerions passer deux fois plus de temps sur une colline de 100 m√®tres de haut que sur une colline voisine de 50 m√®tres.</i>  <i>C'est bien que nous puissions faire cela m√™me si nous ne connaissons pas les hauteurs absolues des points sur la surface: tout ce que vous devez savoir, ce sont les hauteurs relatives.</i>  <i>Par exemple, si le sommet de la colline A est deux fois plus √©lev√© que le sommet de la colline B, alors nous aimerions passer deux fois plus de temps en A qu'en B.</i> <i><br><br></i>  <i>Il existe des sch√©mas plus complexes pour proposer de nouveaux lieux et de nouvelles r√®gles pour leur adoption, mais l'id√©e principale est la suivante:</i> <i><br><br></i> <ol><li>  <i>Choisissez un nouvel emplacement "sugg√©r√©".</i> </li><li>  <i>D√©couvrez √† quel point cet emplacement est sup√©rieur ou inf√©rieur √† celui actuel.</i> </li><li>  <i>Rester sur place ou d√©m√©nager dans un nouvel endroit avec une probabilit√© proportionnelle √† la hauteur des lieux.</i> </li></ol> <i><br></i>  <i>Le but du MCMC est de choisir parmi une certaine distribution de probabilit√© sans avoir √† conna√Ætre sa hauteur exacte √† aucun moment (pas besoin de conna√Ætre C).</i> <i><br></i>  <i>Si le processus ¬´errance¬ª est correctement configur√©, vous pouvez vous assurer que cette proportionnalit√© (entre le temps pass√© et la hauteur de distribution) est atteinte</i> . <br><br><h3>  Algorithme: </h3><br>  D√©finissons et d√©crivons maintenant la t√¢che en termes plus formels.  Soit s = (s1, s2, ..., sM) la distribution stationnaire souhait√©e.  Nous voulons cr√©er une cha√Æne de Markov avec une telle distribution stationnaire.  Nous commen√ßons par une cha√Æne de Markov arbitraire avec des √©tats M avec la matrice de transition P, de sorte que pij repr√©sente la probabilit√© de transition de l'√©tat i √† j. <br><br>  Intuitivement, nous savons parcourir la cha√Æne de Markov, mais la cha√Æne de Markov n'a pas la distribution stationnaire requise.  Cette cha√Æne a une distribution stationnaire (dont nous n'avons pas besoin).  Notre objectif est de changer la fa√ßon dont nous nous promenons dans la cha√Æne de Markov afin que la cha√Æne ait la distribution stationnaire souhait√©e. <br><br>  Pour ce faire: <br><br><ol><li>  Commencez avec un √©tat initial al√©atoire i. </li><li>  S√©lectionnez al√©atoirement un nouvel √©tat suppos√© en examinant les probabilit√©s de transition dans la i√®me ligne de la matrice de transition P. </li><li>  Calculez une mesure appel√©e probabilit√© de d√©cision, d√©finie comme suit: aij = min (sj.pji / si.pij, 1). </li><li>  Lancez maintenant une pi√®ce qui atterrit √† la surface de l'aigle avec une probabilit√© aij.  Si un aigle tombe, acceptez l'offre, c'est-√†-dire passez √† l'√©tat suivant, sinon, rejetez l'offre, c'est-√†-dire restez dans l'√©tat actuel. <br></li><li>  R√©p√©tez plusieurs fois. <br></li></ol><br>  Apr√®s un grand nombre de tests, cette cha√Æne convergera et aura une distribution stationnaire s.  Ensuite, nous pouvons utiliser des √©tats de cha√Æne comme √©chantillon de n'importe quelle distribution. <br><br>  En proc√©dant ainsi pour √©chantillonner la distribution b√™ta, la seule fois o√π vous avez √† utiliser la densit√© de probabilit√© est de rechercher la probabilit√© de prendre une d√©cision.  Pour ce faire, divisez sj par si (c'est-√†-dire que la constante de normalisation C est annul√©e). <br><br><h3>  S√©lection b√™ta </h3><br><img src="https://habrastorage.org/webt/h9/ac/zw/h9aczwarm4hfwm0pya3hai-rg2e.jpeg"><br><br>  Passons maintenant au probl√®me de l'√©chantillonnage √† partir de la distribution b√™ta. <br><br>  Une distribution b√™ta est une distribution continue sur [0,1] et peut avoir des valeurs infinies sur [0,1].  Supposons qu'une cha√Æne de Markov arbitraire P avec des √©tats infinis sur [0,1] a une matrice de transition P telle que pij = pji = tous les √©l√©ments de la matrice. <br><br>  Nous n'avons pas besoin de Matrix P, comme nous le verrons plus loin, mais je veux que la description du probl√®me soit aussi proche que possible de l'algorithme que nous avons propos√©. <br><br><ul><li>  Commen√ßons par un √©tat initial al√©atoire i obtenu √† partir d'une distribution uniforme sur (0,1). </li><li>  S√©lectionnez al√©atoirement un nouvel √©tat suppos√© en regardant les probabilit√©s de transition dans la i√®me ligne de la matrice de transition P. Supposons que nous choisissions un autre √©tat Unif (0,1) comme √©tat suppos√© j. </li><li>  Calculez la mesure, qui est appel√©e la probabilit√© de prendre une d√©cision: </li></ul><br><img src="https://habrastorage.org/webt/lp/jo/-0/lpjo-0phzn3o8zl83oniptticmu.png"><br>  Ce qui simplifie: <br><img src="https://habrastorage.org/webt/4c/he/pi/4chepi6_1om84fk52t8jquzpmku.png"><br>  Puisque pji = pij, et o√π <br><img src="https://habrastorage.org/webt/pm/qc/y2/pmqcy2hanok1y-mnhbxk49dqbve.png"><br><ul><li>  Lancez maintenant une pi√®ce.  Il est probable qu'un aigle tombera.  Si un aigle tombe, vous devez accepter l'offre, c'est-√†-dire passer √† l'√©tat suivant.  Sinon, il vaut la peine de rejeter l'offre, c'est-√†-dire de rester dans le m√™me √©tat. </li><li>  R√©p√©tez le test plusieurs fois. </li></ul><br><h3>  Code: </h3><br>  Il est temps de passer de la th√©orie √† la pratique.  Nous allons √©crire notre √©chantillon b√™ta en Python. <br><br><pre> <code class="python hljs">impo rt rand om <span class="hljs-comment"><span class="hljs-comment"># Lets define our Beta Function to generate s for any particular state. We don't care for the normalizing constant here. def beta_s(w,a,b): return w**(a-1)*(1-w)**(b-1) # This Function returns True if the coin with probability P of heads comes heads when flipped. def random_coin(p): unif = random.uniform(0,1) if unif&gt;=p: return False else: return True # This Function runs the MCMC chain for Beta Distribution. def beta_mcmc(N_hops,a,b): states = [] cur = random.uniform(0,1) for i in range(0,N_hops): states.append(cur) next = random.uniform(0,1) ap = min(beta_s(next,a,b)/beta_s(cur,a,b),1) # Calculate the acceptance probability if random_coin(ap): cur = next return states[-1000:] # Returns the last 100 states of the chain</span></span></code> </pre><br>  Comparez les r√©sultats avec la distribution b√™ta r√©elle. <br><br><pre> <code class="python hljs">impo rt num py <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pylab <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pl <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> scipy.special <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> ss %matplotlib inline pl.rcParams[<span class="hljs-string"><span class="hljs-string">'figure.figsize'</span></span>] = (<span class="hljs-number"><span class="hljs-number">17.0</span></span>, <span class="hljs-number"><span class="hljs-number">4.0</span></span>) <span class="hljs-comment"><span class="hljs-comment"># Actual Beta PDF. def beta(a, b, i): e1 = ss.gamma(a + b) e2 = ss.gamma(a) e3 = ss.gamma(b) e4 = i ** (a - 1) e5 = (1 - i) ** (b - 1) return (e1/(e2*e3)) * e4 * e5 # Create a function to plot Actual Beta PDF with the Beta Sampled from MCMC Chain. def plot_beta(a, b): Ly = [] Lx = [] i_list = np.mgrid[0:1:100j] for i in i_list: Lx.append(i) Ly.append(beta(a, b, i)) pl.plot(Lx, Ly, label="Real Distribution: a="+str(a)+", b="+str(b)) pl.hist(beta_mcmc(100000,a,b),normed=True,bins =25, histtype='step',label="Simulated_MCMC: a="+str(a)+", b="+str(b)) pl.legend() pl.show() plot_beta(0.1, 0.1) plot_beta(1, 1) plot_beta(2, 3)</span></span></code> </pre><br><br><img src="https://habrastorage.org/webt/6z/b_/zb/6zb_zbywfexkiagddl4lpusmcko.png"><br><br>  Comme vous pouvez le voir, les valeurs sont tr√®s similaires √† la distribution b√™ta.  Ainsi, le r√©seau MCMC a atteint un √©tat stationnaire <br><br>  Dans le code ci-dessus, nous avons cr√©√© un √©chantillonneur b√™ta, mais le m√™me concept s'applique √† toute autre distribution √† partir de laquelle nous voulons effectuer une s√©lection. <br><br><h2>  Conclusions </h2><br><img src="https://habrastorage.org/webt/5f/oh/h5/5fohh5w_hsavzw3yvbryxewnnkw.png"><br><br>  C'√©tait un super article.  F√©licitations si vous l'avez lu jusqu'√† la fin. <br><br>  En substance, les m√©thodes MCMC peuvent √™tre complexes, mais elles nous offrent une grande flexibilit√©.  Vous pouvez s√©lectionner n'importe quelle fonction de distribution en utilisant la s√©lection via le MCMC.  En r√®gle g√©n√©rale, ces m√©thodes sont utilis√©es pour √©chantillonner √† partir de distributions post√©rieures. <br><br>  Vous pouvez √©galement utiliser MCMC pour r√©soudre des probl√®mes avec un grand espace d'√©tat.  Par exemple, dans un probl√®me de sac √† dos ou pour le d√©chiffrement.  Je vais essayer de vous fournir des exemples plus int√©ressants dans le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">prochain</a> post.  Restez √† l'√©coute. <br><br><h2>  Des √©diteurs </h2><br><ul><li>  Cours <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Python pour travailler avec des donn√©es</a> </li><li>  Cours d' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">apprentissage automatique en</a> ligne </li><li>  Cours en ligne " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">BIG DATA from scratch</a> " </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr460497/">https://habr.com/ru/post/fr460497/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr460485/index.html">Comment un tournoi en ligne peut d√©courager "la fin de la semaine prochaine"</a></li>
<li><a href="../fr460489/index.html">TOP 11 erreurs dans le d√©veloppement du BCP</a></li>
<li><a href="../fr460491/index.html">Capteur de temp√©rature et d'humidit√© Arduino avec envoi et tra√ßage (partie 1)</a></li>
<li><a href="../fr460493/index.html">¬´Killer apps¬ª pour PC des ann√©es 80: VisiCalc et WordStar</a></li>
<li><a href="../fr460495/index.html">Container-to-pipeline: CRI-O est d√©sormais la valeur par d√©faut dans OpenShift Container Platform 4</a></li>
<li><a href="../fr460499/index.html">Trois laur√©ats du prix Dijkstra: comment Hydra 2019 et SPTDC 2019 se sont-ils d√©roul√©s?</a></li>
<li><a href="../fr460501/index.html">Exemple d'impl√©mentation d'int√©gration continue √† l'aide de BuildBot</a></li>
<li><a href="../fr460503/index.html">Configuration sans fil du Raspberry PI 3 B +</a></li>
<li><a href="../fr460505/index.html">Attirer trois croix, ou pourquoi les projets sont si difficiles √† terminer √† temps</a></li>
<li><a href="../fr460509/index.html">Comment les mandataires r√©sidents aident-ils dans les affaires: un cas r√©el d'utilisation d'Infatica dans l'exploration de donn√©es</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>