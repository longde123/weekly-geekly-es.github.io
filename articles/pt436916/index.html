<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßñüèº üêê üëÇ VShard - escala horizontal em Tarantool ‚õ∞Ô∏è üöõ ü¶ñ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Meu nome √© Vladislav, participo do desenvolvimento do Tarantool - DBMS e servidor de aplicativos em uma garrafa. E hoje vou lhe contar como implementa...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>VShard - escala horizontal em Tarantool</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/436916/"><img src="https://habrastorage.org/webt/4p/e8/fo/4pe8foryc_t_l5joliydwpislhm.png"><br><br>  Meu nome √© Vladislav, participo do desenvolvimento do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tarantool</a> - DBMS e servidor de aplicativos em uma garrafa.  E hoje vou lhe contar como implementamos a escala horizontal no Tarantool usando o m√≥dulo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">VShard</a> . <br><a name="habracut"></a><br>  Primeiro, um pouco de teoria. <br><br>  Existem dois tipos de escala: horizontal e vertical.  Horizontal √© dividido em dois tipos: replica√ß√£o e sharding.  A replica√ß√£o √© usada para dimensionar a computa√ß√£o, o sharding √© usado para dimensionar os dados. <br><br>  O sharding √© dividido em dois tipos: sharding por intervalos e sharding por hashes. <br><br>  Ao compartilhar com intervalos, calculamos alguma chave de fragmento de cada registro no cluster.  Essas chaves de fragmento s√£o projetadas em uma linha reta, dividida em intervalos que adicionamos a diferentes n√≥s f√≠sicos. <br><br>  O sharding com hashes √© mais simples: a partir de cada registro no cluster, consideramos uma fun√ß√£o hash, adicionamos registros com o mesmo valor da fun√ß√£o hash a um n√≥ f√≠sico. <br><br>  Vou falar sobre o dimensionamento horizontal usando o shard sharding. <br><br><h1>  Implementa√ß√£o anterior </h1><br>  O primeiro m√≥dulo de escala horizontal que tivemos foi o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tarantool Shard</a> .  Esse √© um sharding muito simples por hashes, que considera a chave de shard da chave prim√°ria de todas as entradas no cluster. <br><br><pre><code class="plaintext hljs">function shard_function(primary_key) return guava(crc32(primary_key), shard_count) end</code> </pre> <br>  Mas ent√£o surgiu uma tarefa que o Tarantool Shard n√£o conseguiu lidar por tr√™s raz√µes fundamentais. <br><br>  Primeiro, a <b>localidade dos dados relacionados logicamente era</b> necess√°ria.  Quando temos dados conectados logicamente, sempre queremos armazen√°-los no mesmo n√≥ f√≠sico, independentemente de como a topologia do cluster √© alterada ou o equil√≠brio √© executado.  E o Tarantool Shard n√£o garante isso.  Ele considera o hash apenas pelas chaves prim√°rias e, ao fazer o reequil√≠brio, at√© os registros com o mesmo hash podem ser separados por algum tempo - a transfer√™ncia n√£o √© at√¥mica. <br><br>  O problema da falta de localidade dos dados nos impediu mais.  Eu darei um exemplo  Existe um banco no qual o cliente abriu uma conta.  Os dados da conta e do cliente sempre devem ser fisicamente armazenados juntos, para que possam ser lidos em uma solicita√ß√£o, trocados em uma transa√ß√£o, por exemplo, ao transferir dinheiro de uma conta.  Se voc√™ usar o sharding cl√°ssico com o Tarantool Shard, os valores das fun√ß√µes do shard ser√£o diferentes para contas e clientes.  Os dados podem estar em diferentes n√≥s f√≠sicos.  Isso complica muito o trabalho de leitura e transacional com esse cliente. <br><br><pre> <code class="plaintext hljs">format = {{'id', 'unsigned'}, {'email', 'string'}} box.schema.create_space('customer', {format = format}) format = {{'id', 'unsigned'}, {'customer_id', 'unsigned'}, {'balance', 'number'}} box.schema.create_space('account', {format = format})</code> </pre><br>  No exemplo acima, os campos de <code>id</code> n√£o podem corresponder facilmente a contas e clientes.  Eles s√£o conectados atrav√©s do campo da conta <code>customer_id</code> e <code>id</code> <code>customer_id</code> .  O mesmo campo de <code>id</code> quebraria a exclusividade da chave prim√°ria da conta.  E de outra maneira, o Shard n√£o √© capaz de fragmentar. <br><br>  O pr√≥ximo problema √© <b>o compartilhamento lento</b> .  Esse √© o problema cl√°ssico de todos os shards em hashes.  A conclus√£o √© que, quando alteramos a composi√ß√£o de um cluster, geralmente alteramos a fun√ß√£o shard, porque geralmente depende do n√∫mero de n√≥s.  E quando a fun√ß√£o muda, voc√™ precisa passar por todas as entradas do cluster e recalcular a fun√ß√£o shard novamente.  Talvez transfira algumas anota√ß√µes.  E enquanto os estamos transferindo, n√£o sabemos se os dados necess√°rios para a pr√≥xima solicita√ß√£o de entrada j√° foram transferidos, talvez eles estejam no processo de transfer√™ncia.  Portanto, durante o novo compartilhamento, √© necess√°rio que cada leitura fa√ßa uma solicita√ß√£o para duas fun√ß√µes de shard: a antiga e a nova.  Os pedidos est√£o se tornando duas vezes mais lentos e, para n√≥s, era inaceit√°vel. <br><br>  Outro recurso do Tarantool Shard foi que, quando alguns n√≥s nos conjuntos de r√©plicas falham, ele mostra <b>acessibilidade de leitura ruim</b> . <br><br><h1>  Nova solu√ß√£o </h1><br>  Para resolver os tr√™s problemas descritos, criamos o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tarantool VShard</a> .  Sua principal diferen√ßa √© que o n√≠vel de armazenamento de dados √© virtualizado: os armazenamentos virtuais apareceram sobre os f√≠sicos e os registros s√£o distribu√≠dos entre eles.  Esses armazenamentos s√£o chamados de bucket'ami.  O usu√°rio n√£o precisa pensar sobre o que e em qual n√≥ f√≠sico se encontra.  Bucket √© uma unidade de dados indivis√≠vel at√¥mica, como no sharding cl√°ssico, uma tupla.  O VShard sempre armazena o bucket inteiro em um n√≥ f√≠sico e durante o compartilhamento compartilhado transfere todos os dados de um bucket atomicamente.  Devido a isso, a localidade √© fornecida.  N√≥s s√≥ precisamos colocar os dados em um bucket e sempre podemos ter certeza de que esses dados estar√£o juntos com quaisquer altera√ß√µes no cluster. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/42e/a4f/87b/42ea4f87b5c0f0b05bdf0e0c75b356fe.png"><br><br>  Como posso colocar dados em um balde?  No esquema que introduzimos anteriormente para o cliente do banco, adicionaremos o <code>bucket id</code> do <code>bucket id</code> √†s tabelas de acordo com o novo campo.  Se os dados vinculados forem os mesmos, os registros estar√£o no mesmo bloco.  A vantagem √© que podemos armazenar esses registros com o mesmo <code>bucket id</code> em espa√ßos diferentes e at√© em mecanismos diferentes.  A <code>bucket id</code> fornecida, independentemente de como esses registros s√£o armazenados. <br><br><pre> <code class="plaintext hljs">format = {{'id', 'unsigned'}, {'email', 'string'}, {'bucket_id', 'unsigned'}} box.schema.create_space('customer', {format = format}) format = {{'id', 'unsigned'}, {'customer_id', 'unsigned'}, {'balance', 'number'}, {'bucket_id', 'unsigned'}} box.schema.create_space('account', {format = format})</code> </pre><br>  Por que estamos t√£o ansiosos por isso?  Se tivermos sharding cl√°ssico, os dados poder√£o se espalhar por todos os armazenamentos f√≠sicos que temos apenas.  No exemplo com o banco, ao solicitar todas as contas de um cliente, voc√™ precisar√° ativar todos os n√≥s.  Acontece a dificuldade de ler O (N), onde N √© o n√∫mero de reservas f√≠sicas.  Muito lento. <br><br>  Gra√ßas ao bucket'am e √† localidade pelo <code>bucket id</code> sempre podemos ler dados de um n√≥ em uma solicita√ß√£o, independentemente do tamanho do cluster. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2bb/524/8b7/2bb5248b7757ea6249f47a3dca46a681.png"><br><br>  Voc√™ precisa calcular o <code>bucket id</code> do <code>bucket id</code> e atribuir os mesmos valores.  Para alguns, isso √© uma vantagem, para algu√©m uma desvantagem.  Considero uma vantagem que voc√™ pode escolher a fun√ß√£o para calcular o <code>bucket id</code> do <code>bucket id</code> . <br><br>  Qual √© a principal diferen√ßa entre o sharding cl√°ssico e o sharding virtual com bucket? <br><br>  No primeiro caso, quando alteramos a composi√ß√£o do cluster, temos dois estados: o atual (antigo) e o novo, nos quais devemos ir.  No processo de transi√ß√£o, voc√™ precisa n√£o apenas transferir os dados, mas tamb√©m recalcular as fun√ß√µes de hash para todos os registros.  Isso √© muito inconveniente, porque a qualquer momento n√£o sabemos quais dados j√° foram transferidos e quais n√£o.  Al√©m disso, isso n√£o √© confi√°vel nem at√¥mico, pois para a transfer√™ncia at√¥mica de um conjunto de registros com o mesmo valor da fun√ß√£o hash, √© necess√°rio armazenar persistentemente o estado da transfer√™ncia, caso a recupera√ß√£o seja necess√°ria.  Existem conflitos, erros, voc√™ precisa reiniciar o procedimento v√°rias vezes. <br><br>  O sharding virtual √© muito mais simples.  N√£o temos dois estados selecionados do cluster, apenas o estado do bucket.  O cluster se torna mais manobr√°vel, gradualmente se move de um estado para outro.  E agora existem mais de dois estados.  Gra√ßas a uma transi√ß√£o suave, voc√™ pode alterar o saldo em tempo real, excluir o armazenamento rec√©m-adicionado.  Ou seja, a controlabilidade do balanceamento √© bastante aumentada, torna-se granular. <br><br><h1>  Use </h1><br>  Digamos que escolhemos uma fun√ß√£o para o <code>bucket id</code> e inserimos tantos dados no cluster que n√£o havia mais espa√ßo.  Agora, queremos adicionar n√≥s e, para que os dados sejam movidos para eles mesmos.  No VShard, isso √© feito da seguinte maneira.  Primeiro, inicie novos n√≥s e Tarantools neles e atualize a configura√ß√£o do VShard.  Ele descreve todos os membros do cluster, todas as r√©plicas, conjuntos de r√©plicas, mestres, URIs atribu√≠dos e muito mais.  Adicionamos novos n√≥s √† configura√ß√£o e, usando a fun√ß√£o <code>VShard.storage.cfg</code> , usamos em todos os n√≥s do cluster. <br><br><pre> <code class="plaintext hljs">function create_user(email) local customer_id = next_id() local bucket_id = crc32(customer_id) box.space.customer:insert(customer_id, email, bucket_id) end function add_account(customer_id) local id = next_id() local bucket_id = crc32(customer_id) box.space.account:insert(id, customer_id, 0, bucket_id) end</code> </pre> <br>  Como voc√™ se lembra, no sharding cl√°ssico com uma altera√ß√£o no n√∫mero de n√≥s, a fun√ß√£o shard tamb√©m muda.  No VShard, isso n√£o acontece, temos um n√∫mero fixo de armazenamentos virtuais - bucket'ov.  Essa √© a constante que voc√™ seleciona ao iniciar o cluster.  Pode parecer que, por isso, a escalabilidade seja limitada, mas n√£o realmente.  Voc√™ pode escolher um grande n√∫mero de bucket'ov, dezenas e centenas de milhares.  O principal √© que deve haver pelo menos duas ordens de magnitude a mais do que o n√∫mero m√°ximo de conjuntos de r√©plicas que voc√™ ter√° no cluster. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/422/499/979/422499979e5b8c5728c3df2b967cf599.gif"><br><br>  Como o n√∫mero de armazenamentos virtuais n√£o muda - e a fun√ß√£o shard depende apenas desse valor -, podemos adicionar quantos armazenamentos f√≠sicos forem necess√°rios, sem recontar a fun√ß√£o shard. <br><br>  Como os pacotes s√£o distribu√≠dos por lojas f√≠sicas por conta pr√≥pria?  Quando VShard.storage.cfg √© chamado em um dos n√≥s, o processo de reequil√≠brio √© ativado.  Este √© um processo anal√≠tico que calcula o equil√≠brio perfeito em um cluster.  Ele vai a todos os n√≥s f√≠sicos, pergunta quem tem quantos bucket'ov e constr√≥i rotas para o seu movimento, a fim de calcular a m√©dia da distribui√ß√£o.  O rebalanceador envia rotas para armaz√©ns lotados e eles come√ßam a enviar baldes.  Ap√≥s algum tempo, o cluster fica equilibrado. <br><br>  Mas em projetos reais, o conceito de equil√≠brio perfeito pode ser diferente.  Por exemplo, quero armazenar menos dados em um conjunto de r√©plicas do que no outro, porque h√° menos espa√ßo no disco r√≠gido.  O VShard acha que tudo est√° bem equilibrado e, na verdade, meu armazenamento est√° prestes a transbordar.  Fornecemos um mecanismo para ajustar as regras de balanceamento usando pesos.  Cada conjunto de r√©plicas e reposit√≥rio pode ser ponderado.  Quando o balanceador decide para quem enviar quantos bucket'ov, ele leva em considera√ß√£o o <b>relacionamento de</b> todos os pares de pesos. <br><br>  Por exemplo, uma loja pesa 100 e a outra 200. A primeira armazenar√° duas vezes menos bucket'ov que a segunda.  Observe que estou falando especificamente sobre a <b>propor√ß√£o de</b> pesos.  Significados absolutos n√£o t√™m efeito.  Voc√™ pode escolher pesos com base em uma distribui√ß√£o de cluster de 100%: uma loja possui 30% e outra 70%.  Voc√™ pode considerar a capacidade de armazenamento em gigabytes ou medir pesos no n√∫mero de bucket'ov.  O principal √© observar a atitude que voc√™ precisa. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b4e/889/298/b4e889298991781b8c3ee01f4c066a6e.png"><br><br>  Esse sistema tem um efeito colateral interessante: se voc√™ atribuir um peso zero a alguma loja, o balanceador solicitar√° que a loja distribua todos os seus baldes.  Depois disso, voc√™ pode remover todo o conjunto de r√©plicas da configura√ß√£o. <br><br><h1>  Transfer√™ncia de ca√ßamba at√¥mica </h1><br>  Temos um dep√≥sito, ele aceita algum tipo de solicita√ß√£o de leitura e grava√ß√£o e, em seguida, o balanceador pede para transferi-lo para outro armazenamento.  O bucket deixa de aceitar solicita√ß√µes de grava√ß√£o; caso contr√°rio, eles ter√£o tempo para atualiz√°-lo durante a transfer√™ncia, ter√£o tempo para atualizar a atualiza√ß√£o port√°til, a atualiza√ß√£o port√°til e assim por diante.  Portanto, o registro est√° bloqueado e voc√™ ainda pode ler no balde.  A transfer√™ncia de peda√ßos para um novo local come√ßa.  Ap√≥s a conclus√£o da transfer√™ncia, o bucket come√ßar√° novamente a aceitar solicita√ß√µes.  No antigo local, ele ainda est√°, mas j√° foi marcado como lixo e, posteriormente, o coletor de lixo o exclui peda√ßo por peda√ßo. <br><br>  Cada bloco est√° associado aos metadados armazenados fisicamente no disco.  Todas as etapas acima s√£o salvas no disco e, n√£o importa o que aconte√ßa com o reposit√≥rio, o estado do bucket ser√° restaurado automaticamente. <br><br>  Voc√™ pode ter perguntas: <br><br><ul><li>  <b>O que acontecer√° com as solicita√ß√µes que funcionaram com o bucket quando come√ßaram a port√°-lo?</b> <br><br>  Existem dois tipos de links nos metadados de cada bloco: leitura e grava√ß√£o.  Quando o usu√°rio faz uma solicita√ß√£o para o bucket, ele indica como ele trabalhar√° com ele, somente leitura ou leitura e grava√ß√£o.  Para cada solicita√ß√£o, o contador de refer√™ncia correspondente √© incrementado. <br><br>  Por que preciso de um contador de refer√™ncia para solicita√ß√µes de leitura?  Digamos que o balde seja transferido silenciosamente, e aqui o coletor de lixo vem e deseja excluir esse balde.  Ele v√™ que a contagem de links √© maior que zero, ent√£o voc√™ n√£o pode exclu√≠-la.  E quando as solicita√ß√µes forem processadas, o coletor de lixo poder√° concluir seu trabalho. <br><br>  O contador de refer√™ncia para pedidos de grava√ß√£o garante que o bucket nem comece a ser transportado enquanto pelo menos um pedido de grava√ß√£o estiver trabalhando com ele.  Mas as solicita√ß√µes de grava√ß√£o podem vir constantemente e, em seguida, o dep√≥sito nunca ser√° transferido.  O fato √© que, se o balanceador manifestou desejo de transferi-lo, novas solicita√ß√µes de grava√ß√£o come√ßar√£o a ser bloqueadas e o sistema atual aguardar√° a conclus√£o de algum tempo limite.  Se as solicita√ß√µes n√£o forem conclu√≠das no tempo alocado, o sistema come√ßar√° novamente a aceitar novas solicita√ß√µes de grava√ß√£o, adiando a transfer√™ncia do bucket por algum tempo.  Assim, o balanceador far√° as tentativas de transfer√™ncia at√© que uma seja bem-sucedida. <br><br>  O VShard possui uma API bucket_ref de baixo n√≠vel, caso voc√™ tenha poucos recursos de alto n√≠vel.  Se voc√™ realmente deseja fazer algo, basta acessar esta API a partir do c√≥digo. </li><li>  <b>√â poss√≠vel n√£o bloquear registros?</b> <br><br>  Isso √© imposs√≠vel.  Se o dep√≥sito contiver dados cr√≠ticos que precisam de acesso de grava√ß√£o constante, voc√™ ter√° que bloquear completamente sua transfer√™ncia.  Existe uma fun√ß√£o <code>bucket_pin</code> para isso, ela anexa firmemente o bucket ao conjunto de r√©plicas atual, impedindo sua transfer√™ncia.  Nesse caso, o bucket'y vizinho poder√° se mover sem restri√ß√µes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b6a/848/fa7/b6a848fa775b0066ac6f69b73d97ed76.png"><br><br>  Existe uma ferramenta ainda mais poderosa que o bloqueio do conjunto de r√©plicas <code>bucket_pin</code> .  Isso n√£o √© mais feito no c√≥digo, mas atrav√©s da configura√ß√£o.  O bloqueio pro√≠be o movimento de qualquer bucket'ov deste conjunto de r√©plicas e a recep√ß√£o de novos.  Consequentemente, todos os dados estar√£o constantemente dispon√≠veis para grava√ß√£o. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/65b/744/39c/65b74439c5b5743eda1168bdb320f8f4.png"></li></ul><br><h1>  VShard.router </h1><br>  O VShard consiste em dois subm√≥dulos: VShard.storage e VShard.router.  Eles podem ser criados e dimensionados independentemente, mesmo em uma inst√¢ncia.  Ao acessar o cluster, n√£o sabemos onde est√° o dep√≥sito, e o VShard.router o procurar√° pelo <code>bucket id</code> . <br><br>  Vejamos um exemplo de como isso se parece.  Retornamos ao cluster banc√°rio e √†s contas de clientes.  Quero poder extrair todas as contas de um cliente espec√≠fico do cluster.  Para fazer isso, escrevo a fun√ß√£o usual para pesquisa local: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f78/e2b/df2/f78e2bdf2d977fcb9fb320b031592171.png"><br><br>  Ela procura todas as contas de clientes pelo ID dele.  Agora eu preciso decidir em qual dos reposit√≥rios chamar essa fun√ß√£o.  Para fazer isso, calculo o <code>bucket id</code> do <code>bucket id</code> partir do <code>bucket id</code> do cliente em minha solicita√ß√£o e solicito ao VShard.router que me chame de tal fun√ß√£o no armazenamento em que o bucket com o <code>bucket id</code> resultante vive.  H√° uma tabela de roteamento no subm√≥dulo, na qual a localiza√ß√£o do bucket no conjunto de r√©plicas √© especificada.  E o VShard.router proxies meu pedido. <br><br>  Obviamente, pode acontecer que, nesse momento, o novo compartilhamento tenha come√ßado e o balde tenha come√ßado a se mover.  O roteador em segundo plano atualiza gradualmente a tabela em grandes partes: consulta os reposit√≥rios em busca de suas tabelas de bucket atuais. <br><br>  Pode at√© acontecer que voltemos ao balde que acabou de ser movido e o roteador ainda n√£o conseguiu atualizar sua tabela de roteamento.  Em seguida, ele voltar√° para o reposit√≥rio antigo e ele informar√° ao roteador onde procurar o bucket ou simplesmente responder√° que ele n√£o possui os dados necess√°rios.  Em seguida, o roteador percorrer√° todos os armazenamentos em busca do balde desejado.  E tudo isso √© transparente para n√≥s, nem perceberemos uma falta na tabela de roteamento. <br><br><h1>  Instabilidade de leitura </h1><br>  Lembre-se de quais problemas tivemos inicialmente: <br><br><ul><li>  N√£o havia localidade de dados.  Decidimos adicionando bucket'ov. </li><li>  Compartilhar novamente reduziu a velocidade e diminuiu a velocidade.  Implementou a transfer√™ncia de dados at√¥micos bucket'ami, livrou-se de recontar fun√ß√µes de shard. </li><li>  Leitura inst√°vel. </li></ul><br>  O √∫ltimo problema foi resolvido pelo VShard.router usando o subsistema de failover de leitura autom√°tica. <br><br>  O roteador efetua ping periodicamente no armazenamento especificado na configura√ß√£o.  E ent√£o alguns deles pararam de tocar.  O roteador possui uma conex√£o de backup quente para cada r√©plica e, se a atual parar de responder, ser√° direcionada para outra.  A solicita√ß√£o de leitura ser√° processada normalmente, porque podemos ler em r√©plicas (mas n√£o em escrever).  Podemos definir a prioridade das r√©plicas pelas quais o roteador deve selecionar o failover para as leituras.  Fazemos isso com zoneamento. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5c3/5bf/dbd/5c35bfdbddd67fe8217f06730673bd43.png"><br><br>  Atribu√≠mos um n√∫mero de zona a cada r√©plica e a cada roteador e definimos uma tabela na qual indicamos a dist√¢ncia entre cada par de zonas.  Quando o roteador decide para onde enviar uma solicita√ß√£o de leitura, ele selecionar√° uma r√©plica na zona mais pr√≥xima da sua. <br><br>  Como fica na configura√ß√£o: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/445/799/5ae/4457995ae5c7bc1761684cf7d4f3b2e4.png"><br><br>  Em geral, voc√™ pode se referir a uma r√©plica arbitr√°ria, mas se o cluster for grande e complexo, muito distribu√≠do, o zoneamento ser√° muito √∫til.  Racks de servidor diferentes podem ser zonas, para n√£o carregar a rede com tr√°fego.  Ou podem ser pontos geograficamente distantes um do outro. <br><br>  O zoneamento tamb√©m ajuda com o desempenho vari√°vel da r√©plica.  Por exemplo, em cada conjunto de r√©plicas, temos uma r√©plica de backup, que n√£o deve aceitar solicita√ß√µes, mas apenas armazena uma c√≥pia dos dados.  Em seguida, chegamos √† zona, que estar√° muito longe de todos os roteadores da tabela, e eles se voltar√£o para ele no caso mais extremo. <br><br><h1>  Instabilidade de grava√ß√£o </h1><br>  Como estamos falando de failover de leitura, e o failover de grava√ß√£o ao alterar o assistente?  Aqui, o VShard n√£o √© t√£o otimista: a elei√ß√£o de um novo mestre n√£o √© implementada nele, voc√™ ter√° que fazer isso sozinho.  Quando a selecionamos de alguma forma, √© necess√°rio que essa inst√¢ncia agora assuma a autoridade do mestre.  Atualizamos a configura√ß√£o especificando <code>master = false</code> para o antigo mestre e <code>master = true</code> para o novo, aplicamos via VShard.storage.cfg e rolamos para o armazenamento.  Ent√£o tudo acontece automaticamente.  O antigo mestre para de aceitar solicita√ß√µes de grava√ß√£o e come√ßa a sincronizar com o novo, porque pode haver dados que j√° foram aplicados no antigo, mas o novo ainda n√£o chegou.  Depois disso, o novo mestre entra na fun√ß√£o e come√ßa a aceitar solicita√ß√µes, e o antigo mestre se torna uma r√©plica.  √â assim que o failover de grava√ß√£o funciona no VShard. <br><br><pre> <code class="plaintext hljs">replicas = new_cfg.sharding[uud].replicas replicas[old_master_uuid].master = false replicas[new_master_uuid].master = true vshard.storage.cfg(new_cfg)</code> </pre> <br><h1>  Como agora seguir toda essa variedade de eventos? </h1><br>  No caso geral, duas al√ßas s√£o suficientes - <code>VShard.storage.info</code> e <code>VShard.router.info</code> . <br><br>  VShard.storage.info exibe informa√ß√µes em v√°rias se√ß√µes. <br><br><pre> <code class="plaintext hljs">vshard.storage.info() --- - replicasets: &lt;replicaset_2&gt;: uuid: &lt;replicaset_2&gt; master: uri: storage@127.0.0.1:3303 &lt;replicaset_1&gt;: uuid: &lt;replicaset_1&gt; master: missing bucket: receiving: 0 active: 0 total: 0 garbage: 0 pinned: 0 sending: 0 status: 2 replication: status: slave Alerts: - ['MISSING_MASTER', 'Master is not configured for ''replicaset &lt;replicaset_1&gt;']</code> </pre> <br>  A primeira √© a se√ß√£o de replica√ß√£o.  O status do conjunto de r√©plicas ao qual voc√™ aplicou esta fun√ß√£o √© exibido: qual atraso de replica√ß√£o possui, com quem tem conex√µes e com quem n√£o est√° dispon√≠vel, quem est√° dispon√≠vel e n√£o est√° dispon√≠vel, qual assistente est√° configurado para qual etc. <br><br>  Na se√ß√£o Balde, √© poss√≠vel ver em tempo real quantos bucket'ov est√£o atualmente migrando para o conjunto de r√©plicas atual, quantos o est√£o deixando, quantos est√£o trabalhando atualmente, quantos est√£o marcados como lixo e quantos est√£o anexados. <br><br>  A se√ß√£o Alert √© uma mistura de todos os problemas que o VShard conseguiu determinar independentemente: o mestre n√£o est√° configurado, o n√≠vel de redund√¢ncia √© insuficiente, o mestre est√° l√°, mas todas as r√©plicas falharam etc. <br><br>  E a √∫ltima se√ß√£o √© uma luz que acende em vermelho quando as coisas ficam realmente ruins.  √â um n√∫mero de zero a tr√™s, quanto mais, pior. <br><br>  VShard.router.info tem as mesmas se√ß√µes, mas elas significam um pouco diferente. <br><br><pre> <code class="plaintext hljs">vshard.router.info() --- - replicasets: &lt;replicaset_2&gt;: replica: &amp;0 status: available uri: storage@127.0.0.1:3303 uuid: 1e02ae8a-afc0-4e91-ba34-843a356b8ed7 bucket: available_rw: 500 uuid: &lt;replicaset_2&gt; master: *0 &lt;replicaset_1&gt;: replica: &amp;1 status: available uri: storage@127.0.0.1:3301 uuid: 8a274925-a26d-47fc-9e1b-af88ce939412 bucket: available_rw: 400 uuid: &lt;replicaset_1&gt; master: *1 bucket: unreachable: 0 available_ro: 800 unknown: 200 available_rw: 700 status: 1 alerts: - ['UNKNOWN_BUCKETS', '200 buckets are not discovered']</code> </pre> <br>  A primeira se√ß√£o √© replica√ß√£o.      ,    :    ,  replica set'  ,          ,   ,   replica set'  bucket'     ,     . <br><br>   Bucket    bucket',              ;    bucket'   ;  ,       replica set'. <br><br>   Alert,  ,   ,   failover,   bucket'. <br><br> ,         . <br><br><h1>     VShard? </h1><br>  ‚Äî    bucket'.       <code>int32_max</code> ?     bucket'   ‚Äî  30      16   .     bucket',     .           bucket',          bucket'.    ,          . <br><br>  ‚Äî   -   <code>bucket id</code> .    ,    -   ,   bucket ‚Äî           .      ,   bucket'   ,  VShard    bucket'.       -,      bucket'  bucket,  -.    . <br><br><h1>  Sum√°rio </h1><br> Vshard : <br><br><ul><li>  ; </li><li>  ; </li><li>    ; </li><li>  read failover; </li><li>    bucket'. </li></ul><br> VShard   .  -    .  ‚Äî  <b>   </b> .     ,       .           . <br><br>  ‚Äî <b>lock-free  bucket'</b> .   ,       bucket'      .      ,     . <br><br>  ‚Äî <b>  </b> .          : -    ,   ,    ?        . <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">  </a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt436916/">https://habr.com/ru/post/pt436916/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt436904/index.html">Limite de 32 K para dados na ROM dos microcontroladores AVR</a></li>
<li><a href="../pt436908/index.html">6 maneiras de ocultar dados em um aplicativo Android</a></li>
<li><a href="../pt436910/index.html">Dicas para criar fluxos de trabalho personalizados no GitLab CI</a></li>
<li><a href="../pt436912/index.html">Tend√™ncias de CRM de 2019: divertido de ler, perigoso de acreditar</a></li>
<li><a href="../pt436914/index.html">Problemas de crescimento de inicializa√ß√£o - Monitoramento</a></li>
<li><a href="../pt436918/index.html">Criando um jogo para o Game Boy, parte 2</a></li>
<li><a href="../pt436920/index.html">Transpiler PAS2JS de Pascal para JavaScript: incompat√≠vel com Delphi e solu√ß√µes alternativas</a></li>
<li><a href="../pt436922/index.html">Otimizando o tempo de inicializa√ß√£o do Prometheus 2.6.0 com pprof</a></li>
<li><a href="../pt436924/index.html">Algumas palavras sobre a organiza√ß√£o de competi√ß√µes rob√≥ticas</a></li>
<li><a href="../pt436926/index.html">Her√≥is da autentica√ß√£o de dois fatores ou como "andar no lugar dos outros"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>