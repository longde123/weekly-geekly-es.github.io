<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßúüèº üíó üõÄ √â poss√≠vel treinar um agente para negociar no mercado de a√ß√µes com refor√ßos? Implementa√ß√£o da linguagem R üßîüèΩ ‚ö´Ô∏è üîö</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Vamos criar um agente de aprendizado de refor√ßo de prot√≥tipo (RL) que domine a habilidade de negocia√ß√£o. 

 Dado que a implementa√ß√£o do prot√≥tipo func...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>√â poss√≠vel treinar um agente para negociar no mercado de a√ß√µes com refor√ßos? Implementa√ß√£o da linguagem R</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/433182/"> Vamos criar um agente de aprendizado de refor√ßo de prot√≥tipo (RL) que domine a habilidade de negocia√ß√£o. <br><br>  Dado que a implementa√ß√£o do prot√≥tipo funciona na linguagem R, incentivo os usu√°rios e programadores a se aproximarem das id√©ias apresentadas neste artigo. <br><br>  Esta √© uma tradu√ß√£o do meu artigo em ingl√™s: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Can Reforcement Learning Trade Stock?</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Implementa√ß√£o em R.</a> <br><br>  <i>Quero alertar os ca√ßadores de c√≥digo de que, nesta nota, existe apenas um c√≥digo para uma rede neural adaptada para R.</i> <br><br>  Se n√£o me distingui em russo, aponte os erros (o texto foi preparado com a ajuda de um tradutor autom√°tico). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/97b/b88/e48/97bb88e4896efcd9f281cfb1b72830cf.png" alt="imagem"><br><a name="habracut"></a><br><h3>  Introdu√ß√£o ao problema </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/986/550/1f7/9865501f78bdeb575a3017a0c2466d54.png" alt="imagem"><br><br>  Aconselho que voc√™ comece a mergulhar no t√≥pico deste artigo: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">DeepMind</a> <br><br>  Ele apresenta a id√©ia de usar a Deep Q-Network (DQN) para aproximar uma fun√ß√£o de valor cr√≠tica nos processos de tomada de decis√£o de Markov. <br><br>  Tamb√©m recomendo aprofundar a matem√°tica usando a pr√©-impress√£o deste livro de Richard S. Sutton e Andrew J. Barto: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aprendizagem por Refor√ßo</a> <br><br>  A seguir, apresentarei uma vers√£o estendida do DQN original, que inclui mais id√©ias que ajudam o algoritmo a convergir de maneira r√°pida e eficiente, a saber: <br><br>  <b>NN de duelo profundo profundo</b> com sele√ß√£o priorit√°ria no buffer de reprodu√ß√£o da experi√™ncia. <br><br>  O que torna essa abordagem melhor do que o DQN cl√°ssico? <br><br><ul><li>  Duplo: existem duas redes, uma das quais √© treinada e a outra avalia os seguintes valores de Q </li><li>  Duelo: Existem neur√¥nios que claramente valorizam e beneficiam </li><li>  Barulhento: existem matrizes de ru√≠do aplicadas aos pesos das camadas intermedi√°rias, onde os desvios m√©dios e padr√£o s√£o pesos treinados </li><li>  Prioridade da amostragem: os lotes de observa√ß√£o do buffer de reprodu√ß√£o cont√™m exemplos, por causa dos quais o treinamento anterior de fun√ß√µes levou a grandes res√≠duos que podem ser armazenados na matriz auxiliar. </li></ul><br><h4>  Bem, e o com√©rcio realizado pelo agente DQN?  Este √© um t√≥pico interessante como tal. </h4><br>  Existem raz√µes pelas quais isso √© interessante: <br><br><ul><li>  Liberdade absoluta de escolha de representa√ß√µes de status, a√ß√µes, pr√™mios e arquitetura da NN.  Voc√™ pode enriquecer o espa√ßo de entrada com tudo o que considerar digno de experimentar, desde not√≠cias a outras a√ß√µes e √≠ndices. </li><li>  A correspond√™ncia da l√≥gica de negocia√ß√£o com a l√≥gica de aprendizado por refor√ßo √© que: o agente executa a√ß√µes discretas (ou cont√≠nuas), raramente √© recompensado (ap√≥s o fechamento da transa√ß√£o ou o prazo expirar), o ambiente √© parcialmente observ√°vel e pode conter informa√ß√µes sobre as pr√≥ximas etapas, a negocia√ß√£o √© um jogo epis√≥dico. </li><li>  Voc√™ pode comparar os resultados do DQN com v√°rios benchmarks, como √≠ndices e sistemas t√©cnicos de negocia√ß√£o. </li><li>  O agente pode aprender continuamente novas informa√ß√µes e, assim, adaptar-se √†s novas regras do jogo. </li></ul><br>  Para n√£o esticar o material, observe o c√≥digo deste NN, que quero compartilhar, pois essa √© uma das partes misteriosas de todo o projeto. <br><br><h4>  C√≥digo R para uma rede neural de valor usando Keras para criar nosso agente RL </h4><br><div class="spoiler">  <b class="spoiler_title">C√≥digo</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># configure critic NN ------------ library('keras') library('R6') learning_rate &lt;- 1e-3 state_names_length &lt;- 12 # just for example a_CustomLayer &lt;- R6::R6Class( "CustomLayer" , inherit = KerasLayer , public = list( call = function(x, mask = NULL) { x - k_mean(x, axis = 2, keepdims = T) } ) ) a_normalize_layer &lt;- function(object) { create_layer(a_CustomLayer, object, list(name = 'a_normalize_layer')) } v_CustomLayer &lt;- R6::R6Class( "CustomLayer" , inherit = KerasLayer , public = list( call = function(x, mask = NULL) { k_concatenate(list(x, x, x), axis = 2) } , compute_output_shape = function(input_shape) { output_shape = input_shape output_shape[[2]] &lt;- input_shape[[2]] * 3L output_shape } ) ) v_normalize_layer &lt;- function(object) { create_layer(v_CustomLayer, object, list(name = 'v_normalize_layer')) } noise_CustomLayer &lt;- R6::R6Class( "CustomLayer" , inherit = KerasLayer , lock_objects = FALSE , public = list( initialize = function(output_dim) { self$output_dim &lt;- output_dim } , build = function(input_shape) { self$input_dim &lt;- input_shape[[2]] sqr_inputs &lt;- self$input_dim ** (1/2) self$sigma_initializer &lt;- initializer_constant(.5 / sqr_inputs) self$mu_initializer &lt;- initializer_random_uniform(minval = (-1 / sqr_inputs), maxval = (1 / sqr_inputs)) self$mu_weight &lt;- self$add_weight( name = 'mu_weight', shape = list(self$input_dim, self$output_dim), initializer = self$mu_initializer, trainable = TRUE ) self$sigma_weight &lt;- self$add_weight( name = 'sigma_weight', shape = list(self$input_dim, self$output_dim), initializer = self$sigma_initializer, trainable = TRUE ) self$mu_bias &lt;- self$add_weight( name = 'mu_bias', shape = list(self$output_dim), initializer = self$mu_initializer, trainable = TRUE ) self$sigma_bias &lt;- self$add_weight( name = 'sigma_bias', shape = list(self$output_dim), initializer = self$sigma_initializer, trainable = TRUE ) } , call = function(x, mask = NULL) { #sample from noise distribution e_i = k_random_normal(shape = list(self$input_dim, self$output_dim)) e_j = k_random_normal(shape = list(self$output_dim)) #We use the factorized Gaussian noise variant from Section 3 of Fortunato et al. eW = k_sign(e_i) * (k_sqrt(k_abs(e_i))) * k_sign(e_j) * (k_sqrt(k_abs(e_j))) eB = k_sign(e_j) * (k_abs(e_j) ** (1/2)) #See section 3 of Fortunato et al. noise_injected_weights = k_dot(x, self$mu_weight + (self$sigma_weight * eW)) noise_injected_bias = self$mu_bias + (self$sigma_bias * eB) output = k_bias_add(noise_injected_weights, noise_injected_bias) output } , compute_output_shape = function(input_shape) { output_shape &lt;- input_shape output_shape[[2]] &lt;- self$output_dim output_shape } ) ) noise_add_layer &lt;- function(object, output_dim) { create_layer( noise_CustomLayer , object , list( name = 'noise_add_layer' , output_dim = as.integer(output_dim) , trainable = T ) ) } critic_input &lt;- layer_input( shape = c(as.integer(state_names_length)) , name = 'critic_input' ) common_layer_dense_1 &lt;- layer_dense( units = 20 , activation = "tanh" ) critic_layer_dense_v_1 &lt;- layer_dense( units = 10 , activation = "tanh" ) critic_layer_dense_v_2 &lt;- layer_dense( units = 5 , activation = "tanh" ) critic_layer_dense_v_3 &lt;- layer_dense( units = 1 , name = 'critic_layer_dense_v_3' ) critic_layer_dense_a_1 &lt;- layer_dense( units = 10 , activation = "tanh" ) # critic_layer_dense_a_2 &lt;- layer_dense( # units = 5 # , activation = "tanh" # ) critic_layer_dense_a_3 &lt;- layer_dense( units = length(acts) , name = 'critic_layer_dense_a_3' ) critic_model_v &lt;- critic_input %&gt;% common_layer_dense_1 %&gt;% critic_layer_dense_v_1 %&gt;% critic_layer_dense_v_2 %&gt;% critic_layer_dense_v_3 %&gt;% v_normalize_layer critic_model_a &lt;- critic_input %&gt;% common_layer_dense_1 %&gt;% critic_layer_dense_a_1 %&gt;% #critic_layer_dense_a_2 %&gt;% noise_add_layer(output_dim = 5) %&gt;% critic_layer_dense_a_3 %&gt;% a_normalize_layer critic_output &lt;- layer_add( list( critic_model_v , critic_model_a ) , name = 'critic_output' ) critic_model_1 &lt;- keras_model( inputs = critic_input , outputs = critic_output ) critic_optimizer = optimizer_adam(lr = learning_rate) keras::compile( critic_model_1 , optimizer = critic_optimizer , loss = 'mse' , metrics = 'mse' ) train.x &lt;- rnorm(state_names_length * 10) train.x &lt;- array(train.x, dim = c(10, state_names_length)) predict(critic_model_1, train.x) layer_name &lt;- 'noise_add_layer' intermediate_layer_model &lt;- keras_model(inputs = critic_model_1$input, outputs = get_layer(critic_model_1, layer_name)$output) predict(intermediate_layer_model, train.x)[1,] critic_model_2 &lt;- critic_model_1</span></span></code> </pre> <br></div></div><br>  Eu usei esta fonte para adaptar o c√≥digo Python para a parte de ru√≠do da rede: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">github repo</a> <br><br>  Essa rede neural √© assim: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3dc/20c/e1e/3dc20ce1eedec47b871ef59ab7911ef3.png" alt="imagem"><br><br>  Lembre-se de que na arquitetura de duelo usamos igualdade (equa√ß√£o 1): <br><br>  Q = A '+ V, onde <br><br>  A '= A - m√©dia (A); <br><br>  Q = valor da a√ß√£o do estado; <br><br>  V = valor do estado; <br><br>  A = vantagem. <br><br>  Outras vari√°veis ‚Äã‚Äãno c√≥digo falam por si.  Al√©m disso, essa arquitetura √© boa apenas para uma tarefa espec√≠fica, portanto, n√£o a tome como garantida. <br><br>  O restante do c√≥digo provavelmente ser√° gen√©rico o suficiente para publica√ß√£o, e ser√° interessante para o programador escrever voc√™ mesmo. <br><br>  E agora - os experimentos.  Os testes do trabalho do agente foram realizados em uma caixa de areia, longe das realidades da negocia√ß√£o em um mercado ao vivo, com um corretor real. <br><br><h3>  Fase I </h3><br>  Executamos nosso agente em um conjunto de dados sint√©tico.  Nosso custo de transa√ß√£o √© de 0,5: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fb2/f15/3b6/fb2f153b6c4e14ff1dc19d61a524f5e8.png" alt="imagem"><br><br>  O resultado √© excelente.  A recompensa epis√≥dica m√©dia m√°xima neste experimento <br>  deve ser 1,5. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/417/0a0/a84/4170a0a842560a2a2fd3112d601dba54.jpg" alt="imagem"><br><br>  Vemos: perda de cr√≠ticas (a chamada rede de valores na abordagem ator-cr√≠tico), recompensa m√©dia por um epis√≥dio, recompensa acumulada, amostra de recompensas recentes. <br><br><h3>  Fase II </h3><br>  Ensinamos a nosso agente um s√≠mbolo de a√ß√µes escolhido arbitrariamente que demonstra um comportamento interessante: um in√≠cio plano, r√°pido crescimento no meio e um final sombrio.  Em nosso kit de treinamento, cerca de 4300 dias.  O custo da transa√ß√£o √© definido em 0,1 d√≥lar (propositadamente baixo);  A recompensa √© lucro / perda em USD ap√≥s o fechamento de um acordo para compra / venda de 1,0 a√ß√µes. <br><br>  Fonte: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">finance.yahoo.com/quote/algn?ltr=1</a> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d6d/22c/540/d6d22c540514afc7261f549bea8c74ac.png" alt="imagem"><br><br>  <i>NASDAQ: ALGN</i> <br><br>  Ap√≥s definir alguns par√¢metros (deixando a arquitetura NN a mesma), chegamos ao seguinte resultado: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/612/008/50c/61200850c8a8c0b7963bbee12ca1a2bd.jpg" alt="imagem"><br><br>  Acabou n√£o sendo ruim, pois no final o agente aprendeu a lucrar pressionando tr√™s bot√µes no console. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d8d/d86/afb/d8dd86afb84b4f4625012a5c3ba9dc4e.png" alt="imagem"><br><br>  <i>marcador vermelho = vender, marcador verde = comprar, marcador cinza = n√£o fazer nada.</i> <br><br>  Observe que, no auge, a recompensa m√©dia por epis√≥dio excedeu o valor realista da transa√ß√£o que pode ser encontrado em negocia√ß√µes reais. <br><br>  √â uma pena que as a√ß√µes estejam caindo como loucas devido a m√°s not√≠cias ... <br><br><h3>  Observa√ß√µes finais </h3><br>  Negociar com a RL n√£o √© apenas dif√≠cil, mas tamb√©m √∫til.  Quando seu rob√¥ faz isso melhor do que voc√™, √© hora de gastar tempo pessoal para obter educa√ß√£o e sa√∫de. <br><br>  Espero que tenha sido uma viagem interessante para voc√™.  Se voc√™ gostou desta hist√≥ria, acene com a m√£o.  Se houver muito interesse, posso continuar e mostrar como os m√©todos de gradiente de pol√≠tica funcionam usando a linguagem R e a API Keras. <br><br>  Tamb√©m quero agradecer aos meus amigos que est√£o interessados ‚Äã‚Äãem redes neurais por seus conselhos. <br><br>  Se voc√™ ainda tiver d√∫vidas, eu estou sempre aqui. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt433182/">https://habr.com/ru/post/pt433182/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt433172/index.html">Apertamos o modo multijogador no jogo para celular "Crie palavras de palavras" no iOS e Android escritas em C ++</a></li>
<li><a href="../pt433174/index.html">Nem todos os patches s√£o igualmente √∫teis.</a></li>
<li><a href="../pt433176/index.html">API remota do Docker de autentica√ß√£o de certificado com verifica√ß√£o de revoga√ß√£o</a></li>
<li><a href="../pt433178/index.html">Como restauramos um arquivo .wav danificado</a></li>
<li><a href="../pt433180/index.html">Resolvendo problemas de tipo de dados no Ruby ou Torne os dados confi√°veis ‚Äã‚Äãnovamente</a></li>
<li><a href="../pt433184/index.html">Lan√ßamento do ASP.NET Core 2.2. Novidades (2 de 3)</a></li>
<li><a href="../pt433186/index.html">N√£o basta contar pol√≠gonos para otimizar modelos 3D</a></li>
<li><a href="../pt433188/index.html">Um projeto de lei sobre o trabalho aut√¥nomo de Runet foi enviado √† Duma do Estado</a></li>
<li><a href="../pt433192/index.html">Kubernetes: uma solu√ß√£o de projeto pessoal incrivelmente acess√≠vel</a></li>
<li><a href="../pt433194/index.html">Luz noturna agendada</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>