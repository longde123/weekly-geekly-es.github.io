<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤸🏻 😂 🚝 Guia de solução de problemas visuais para Kubernetes 👲🏾 🙏 🚌</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nota perev. : Este artigo faz parte dos materiais disponíveis gratuitamente no projeto learnk8s , que ensina como trabalhar com empresas Kubernetes e ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Guia de solução de problemas visuais para Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/484954/">  <i><b>Nota</b></i>  <i><b>perev.</b></i>  <i>: Este artigo faz parte dos materiais disponíveis gratuitamente no projeto <a href="https://learnk8s.io/">learnk8s</a> , que ensina como trabalhar com empresas Kubernetes e administradores individuais.</i>  <i>Nele, Daniele Polencic, gerente de projetos, compartilha uma instrução clara sobre as etapas a serem seguidas em caso de problemas gerais para aplicativos em execução no cluster K8s.</i> <br><br><img src="https://habrastorage.org/webt/ch/5u/xa/ch5uxanj-3ivwqu88swqyoi6bsu.png"><br><br>  TL; DR: aqui está um diagrama que ajudará você a depurar a implantação no Kubernetes: <a name="habracut"></a><br><br> <a href=""><img src="https://habrastorage.org/webt/4r/qp/si/4rqpsie8dnplkqxahaqntpi2ssw.png"></a> <br><br>  <i>Fluxograma para localizar e corrigir erros em um cluster.</i>  <i>No original (em inglês), está disponível em <a href="https://learnk8s.io/a/troubleshooting-kubernetes.pdf">PDF</a> e <a href="">como imagem</a> .</i> <br><br>  Ao implantar um aplicativo no Kubernetes, você geralmente precisa definir três componentes: <br><br><ul><li>  <b>Implantação</b> é uma receita para criar cópias de um aplicativo chamado pods; </li><li>  <b>Serviço</b> - um balanceador de carga interno que distribui o tráfego entre os pods; </li><li>  <b>Ingresso</b> - uma descrição de como o tráfego fluirá do mundo externo para o Serviço. </li></ul><br>  Aqui está um breve resumo gráfico: <br><br>  1) No Kubernetes, os aplicativos recebem tráfego do mundo externo através de duas camadas de balanceadores de carga: interno e externo. <br><br><img src="https://habrastorage.org/webt/3v/cy/z9/3vcyz9a-2ciiqbh9he7idgvo7uy.png"><br><br>  2) O balanceador interno é chamado Serviço, o externo - Ingresso. <br><br><img src="https://habrastorage.org/webt/23/mn/zc/23mnzcfo_b3niccdivn4bc4vzei.png"><br><br>  3) A implantação cria pods e os monitora (eles não são criados manualmente). <br><br><img src="https://habrastorage.org/webt/4j/c2/h9/4jc2h9pgzbxmkon4ewkbeuf0vhc.png"><br><br>  Suponha que você queira implantar um aplicativo simples no <i>Hello World</i> .  A configuração do YAML terá a seguinte aparência: <br><br><pre><code class="plaintext hljs">apiVersion: apps/v1 kind: Deployment # &lt;&lt;&lt; metadata: name: my-deployment labels: track: canary spec: selector: matchLabels: any-name: my-app template: metadata: labels: any-name: my-app spec: containers: - name: cont1 image: learnk8s/app:1.0.0 ports: - containerPort: 8080 --- apiVersion: v1 kind: Service # &lt;&lt;&lt; metadata: name: my-service spec: ports: - port: 80 targetPort: 8080 selector: name: app --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress # &lt;&lt;&lt; metadata: name: my-ingress spec: rules: - http: paths: - backend: serviceName: app servicePort: 80 path: /</code> </pre> <br>  A definição é bastante longa e é fácil ficar confuso sobre como os componentes estão relacionados. <br><br>  Por exemplo: <br><br><ul><li>  Quando você deve usar a porta 80 e quando - 8080? </li><li>  Devo criar uma nova porta para cada serviço para que eles não entrem em conflito? </li><li>  Os nomes dos rótulos são importantes?  Eles deveriam ser os mesmos em todos os lugares? </li></ul><br>  Antes de focar na depuração, lembremos como os três componentes estão relacionados entre si.  Vamos começar com implantação e serviço. <br><br><h2>  Implantação de Conexão'a e Serviço'a </h2><br>  Você ficará surpreso, mas as implantações e o serviço não estão conectados de forma alguma.  Em vez disso, o Serviço aponta diretamente para os Pods ignorando a Implantação. <br><br>  Portanto, estamos interessados ​​em saber como os Pods e os serviços estão relacionados entre si.  Três coisas para lembrar: <br><br><ol><li>  Um <code>selector</code> serviço deve corresponder a pelo menos um rótulo de Pod. </li><li>  <code>targetPort</code> deve corresponder ao <code>containerPort</code> contêiner dentro do Pod. </li><li>  <code>port</code> Service pode ser qualquer coisa.  Serviços diferentes podem usar a mesma porta porque possuem endereços IP diferentes. </li></ol><br>  O diagrama a seguir representa todos os itens acima em forma gráfica: <br><br>  1) Imagine que o serviço direcione o tráfego para um determinado pod: <br><br><img src="https://habrastorage.org/webt/2a/e5/8f/2ae58fcgoi7aifmcr5rl_0bseym.png"><br><br>  2) Ao criar um pod, você deve especificar <code>containerPort</code> para cada container nos pods: <br><br><img src="https://habrastorage.org/webt/xc/fa/ow/xcfaowomhbtqhebhodgzhzrkupc.png"><br><br>  3) Ao criar o serviço, você deve especificar <code>port</code> e <code>targetPort</code> .  <i>Mas qual deles está se conectando ao contêiner?</i> <br><br><img src="https://habrastorage.org/webt/vg/wj/nd/vgwjnde0xyzdblwamomfxjaxb40.png"><br><br>  4) Via <code>targetPort</code> .  Deve corresponder a <code>containerPort</code> . <br><br><img src="https://habrastorage.org/webt/q4/yx/qn/q4yxqnkxxilupalikahmqqp09x8.png"><br><br>  5) Digamos que a porta 3000 esteja aberta no contêiner e, em seguida, o valor <code>targetPort</code> deve ser o mesmo. <br><br><img src="https://habrastorage.org/webt/cq/tj/-s/cqtj-srznih70qh7bxs3w_l7bis.png"><br><br>  No arquivo YAML, os rótulos e as <code>ports</code> / <code>targetPort</code> devem corresponder: <br><br><pre> <code class="plaintext hljs">apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment labels: track: canary spec: selector: matchLabels: any-name: my-app template: metadata: labels: # &lt;&lt;&lt; any-name: my-app # &lt;&lt;&lt; spec: containers: - name: cont1 image: learnk8s/app:1.0.0 ports: - containerPort: 8080 # &lt;&lt;&lt; --- apiVersion: v1 kind: Service metadata: name: my-service spec: ports: - port: 80 targetPort: 8080 # &lt;&lt;&lt; selector: # &lt;&lt;&lt; any-name: my-app # &lt;&lt;&lt;</code> </pre> <br>  <i>E a <code>track: canary</code> na parte superior da seção Implantação?</i>  <i>Deve corresponder?</i> <br><br>  Este rótulo refere-se à implantação e não é usado pelo serviço para rotear o tráfego.  Em outras palavras, ele pode ser excluído ou atribuído a um valor diferente. <br><br>  <i>E o seletor <code>matchLabels</code> ?</i> <br><br>  <b>Sempre deve corresponder aos rótulos do Pod</b> , pois é usado pelo Deployment para rastrear os pods. <br><br>  <i>Suponha que você tenha feito as edições corretas.</i>  <i>Como verificá-los?</i> <br><br>  Você pode verificar o rótulo do pod com o seguinte comando: <br><br><pre> <code class="bash hljs">kubectl get pods --show-labels</code> </pre> <br>  Ou, se os pods pertencerem a vários aplicativos: <br><br><pre> <code class="bash hljs">kubectl get pods --selector any-name=my-app --show-labels</code> </pre> <br>  Onde <code>any-name=my-app</code> é o rótulo <code>any-name: my-app</code> . <br><br>  <i>Há alguma dificuldade?</i> <br><br>  Você pode se conectar ao pod!  Para fazer isso, use o comando <code>port-forward</code> no kubectl.  Ele permite que você se conecte ao serviço e verifique a conexão. <br><br><pre> <code class="bash hljs">kubectl port-forward service/&lt;service name&gt; 3000:80</code> </pre> <br>  Aqui: <br><br><ul><li>  <code>service/&lt;service name&gt;</code> - nome do serviço;  no nosso caso, é o <code>my-service</code> ; </li><li>  3000 - a porta que você deseja abrir no computador; </li><li>  80 - porta especificada no campo de <code>port</code> do serviço. </li></ul><br>  Se você conseguiu estabelecer uma conexão, as configurações estão corretas. <br><br>  Se a conexão não pôde ser estabelecida, há um problema com os rótulos ou as portas não coincidem. <br><br><h2>  Conexão de serviço e ingresso </h2><br>  A próxima etapa no fornecimento de acesso ao aplicativo está relacionada à configuração do Ingress.  O Ingress deve saber como encontrar o serviço, encontrar os pods e direcionar o tráfego para eles.  O Ingress localiza o serviço desejado por nome e porta aberta. <br><br>  Na descrição do Ingress e Service, dois parâmetros devem corresponder: <br><br><ol><li>  <code>servicePort</code> no Ingress deve corresponder ao parâmetro <code>port</code> no Service; </li><li>  <code>serviceName</code> no Ingress deve corresponder ao campo de <code>name</code> no Service. </li></ol><br>  O diagrama a seguir resume a conexão de portas: <br><br>  1) Como você já sabe, o Serviço escuta em uma determinada <code>port</code> : <br><br><img src="https://habrastorage.org/webt/9q/t0/fz/9qt0fzsyme9mnrd4ki07ezamnkg.png"><br><br>  2) O Ingress possui um parâmetro chamado <code>servicePort</code> : <br><br><img src="https://habrastorage.org/webt/rn/du/yw/rnduyw4xvfmpjmhup8fy9ao2d1a.png"><br><br>  3) Este parâmetro ( <code>servicePort</code> ) deve sempre corresponder à <code>port</code> na definição de serviço: <br><br><img src="https://habrastorage.org/webt/1d/ap/ty/1daptyulxphnbb2dt6uben-lnzk.png"><br><br>  4) Se a porta 80 for especificada em Serviço, <code>servicePort</code> também deverá ser 80: <br><br><img src="https://habrastorage.org/webt/nc/mi/dl/ncmidlxiegmtmhtozaxxqhznaya.png"><br><br>  Na prática, você precisa prestar atenção às seguintes linhas: <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: Service metadata: name: my-service # &lt;&lt;&lt; spec: ports: - port: 80 # &lt;&lt;&lt; targetPort: 8080 selector: any-name: my-app --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: my-ingress spec: rules: - http: paths: - backend: serviceName: my-service # &lt;&lt;&lt; servicePort: 80 # &lt;&lt;&lt; path: /</code> </pre> <br>  <i>Como verificar se o Ingress está funcionando?</i> <br><br>  Você pode usar o método com o <code>kubectl port-forward</code> , mas em vez do serviço, você precisa se conectar ao controlador do Ingress. <br><br>  Primeiro, você precisa descobrir o nome do pod com o controlador do Ingress: <br><br><pre> <code class="bash hljs">kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS kube-system coredns-5644d7b6d9-jn7cq 1/1 Running kube-system etcd-minikube 1/1 Running kube-system kube-apiserver-minikube 1/1 Running kube-system kube-controller-manager-minikube 1/1 Running kube-system kube-proxy-zvf2h 1/1 Running kube-system kube-scheduler-minikube 1/1 Running kube-system nginx-ingress-controller-6fc5bcc 1/1 Running</code> </pre> <br>  Localize o pod de entrada (ele pode se referir a um espaço para nome diferente) e execute o comando de <code>describe</code> para descobrir os números de porta: <br><br><pre> <code class="bash hljs">kubectl describe pod nginx-ingress-controller-6fc5bcc \ --namespace kube-system \ | grep Ports Ports: 80/TCP, 443/TCP, 18080/TCP</code> </pre> <br>  Por fim, conecte-se ao pod: <br><br><pre> <code class="bash hljs">kubectl port-forward nginx-ingress-controller-6fc5bcc 3000:80 --namespace kube-system</code> </pre> <br>  Agora, toda vez que você enviar uma solicitação para a porta 3000 no computador, ela será redirecionada para a porta 80 do pod com o controlador do Ingress.  Acessando <a href="http://localhost:3000/">http: // localhost: 3000</a> , você verá a página criada pelo aplicativo. <br><br><h2>  Resumo da porta </h2><br>  Vamos lembrar novamente quais portas e etiquetas devem corresponder: <br><br><ol><li>  O seletor na definição de Serviço deve corresponder ao rótulo do pod; </li><li>  <code>targetPort</code> na definição de Serviço deve corresponder ao <code>containerPort</code> container dentro do pod; </li><li>  <code>port</code> na definição de serviço pode ser qualquer coisa.  Serviços diferentes podem usar a mesma porta porque possuem endereços IP diferentes; </li><li>  <code>servicePort</code> Ingress deve corresponder à <code>port</code> na definição de Serviço; </li><li>  O nome do serviço deve corresponder ao campo <code>serviceName</code> no Ingress. </li></ol><br>  Infelizmente, não basta saber como estruturar corretamente sua configuração YAML. <br><br>  <i>O que acontece quando algo dá errado?</i> <br><br>  Talvez o pod não inicie ou trava. <br><br><h2>  3 etapas para solucionar falhas de aplicativos no Kubernetes </h2><br>  Antes de depurar uma implantação, você precisa entender bem como o Kubernetes funciona. <br><br>  Como existem três componentes em cada aplicativo baixado para o K8s, eles devem ser depurados em uma determinada ordem, começando da parte inferior. <br><br><ol><li>  Primeiro você precisa ter certeza de que os pods estão funcionando, então ... </li><li>  Verifique se o serviço entrega tráfego aos pods e, em seguida, ... </li><li>  Verifique se o Ingress está configurado corretamente. </li></ol><br>  Apresentação visual: <br><br>  1) Inicie a busca de problemas deve estar na parte inferior.  Primeiro verifique se os pods têm status <code>Ready</code> e <code>Running</code> : <br><br><img src="https://habrastorage.org/webt/f-/lc/iz/f-lcizmfav5sb1sc7hvu8samwes.png"><br><br>  2) Se os pods estiverem <code>Ready</code> , você deve descobrir se o serviço distribui o tráfego entre os pods: <br><br><img src="https://habrastorage.org/webt/yg/we/bu/ygwebumu8ga9lmd7krineuw38mq.png"><br><br>  3) Finalmente, você precisa analisar a conexão entre o serviço e o Ingress: <br><br><img src="https://habrastorage.org/webt/y7/ze/uz/y7zeuzkhzgsdzcjmng2ei4fjrxg.png"><br><br><h2>  1. Diagnóstico de pods </h2><br>  Na maioria dos casos, o problema está no pod.  Verifique se os pods estão <code>Ready</code> e em <code>Running</code> .  Você pode verificar isso usando o comando: <br><br><pre> <code class="bash hljs">kubectl get pods NAME READY STATUS RESTARTS AGE app1 0/1 ImagePullBackOff 0 47h app2 0/1 Error 0 47h app3-76f9fcd46b-xbv4k 1/1 Running 1 47h</code> </pre> <br>  Na saída do comando acima, o último pod está listado como Em <code>Running</code> e <code>Ready</code> , mas para os outros dois, não. <br><br>  <i>Como entender o que deu errado?</i> <br><br>  Existem quatro comandos úteis para diagnosticar pods: <br><br><ol><li>  <code>kubectl logs &lt; pod'&gt;</code> permite extrair logs de contêineres no pod; </li><li>  <code>kubectl describe pod &lt; pod'&gt;</code> permite visualizar uma lista de eventos associados ao pod; </li><li>  <code>kubectl get pod &lt; pod'&gt;</code> permite que você obtenha a configuração YAML do <code>kubectl get pod &lt; pod'&gt;</code> armazenado no Kubernetes; </li><li>  <code>kubectl exec -ti &lt; pod'&gt; bash</code> permite executar um shell de comando interativo em um dos contêineres do pod </li></ol><br>  <i>Qual escolher?</i> <br><br>  O fato é que não há equipe universal.  Uma combinação destes deve ser usada. <br><br><h3>  Problemas comuns no pod </h3><br>  Existem dois tipos principais de erros de pod: erros de inicialização e erros de tempo de execução. <br><br>  Erros de inicialização: <br><br><ul><li> <code>ImagePullBackoff</code> </li> <li> <code>ImageInspectError</code> </li> <li> <code>ErrImagePull</code> </li> <li> <code>ErrImageNeverPull</code> </li> <li> <code>RegistryUnavailable</code> </li> <li> <code>InvalidImageName</code> </li> </ul><br>  Erros de tempo de execução: <br><br><ul><li> <code>CrashLoopBackOff</code> </li> <li> <code>RunContainerError</code> </li> <li> <code>KillContainerError</code> </li> <li> <code>VerifyNonRootError</code> </li> <li> <code>RunInitContainerError</code> </li> <li> <code>CreatePodSandboxError</code> </li> <li> <code>ConfigPodSandboxError</code> </li> <li> <code>KillPodSandboxError</code> </li> <li> <code>SetupNetworkError</code> </li> <li> <code>TeardownNetworkError</code> </li> </ul><br>  Alguns erros são mais comuns que outros.  Aqui estão alguns erros comuns e como corrigi-los. <br><br><h4>  ImagePullBackOff </h4><br>  Este erro aparece quando o Kubernetes não pode obter uma imagem para um dos contêineres de pod.  Aqui estão os três motivos mais comuns para isso: <br><br><ol><li>  O nome da imagem está especificado incorretamente - por exemplo, você cometeu um erro ou a imagem não existe; </li><li>  Uma tag inexistente para a imagem é especificada; </li><li>  A imagem é armazenada em um registro privado e o Kubernetes não tem autoridade para acessá-la. </li></ol><br>  As duas primeiras razões são fáceis de eliminar - basta corrigir o nome e a etiqueta da imagem.  No caso deste último, você deve inserir as credenciais para o registro privado em Segredo e adicionar links a ele em pods.  A documentação do Kubernetes <a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/">tem um exemplo</a> de como isso pode ser feito. <br><br><h4>  CrashLoopBackOff </h4><br>  O Kubenetes lançará um erro CrashLoopBackOff se o contêiner não puder iniciar.  Isso geralmente acontece quando: <br><br><ol><li>  Há um erro no aplicativo que impede que ele seja iniciado; </li><li>  O contêiner está <a href="https://stackoverflow.com/questions/41604499/my-kubernetes-pods-keep-crashing-with-crashloopbackoff-but-i-cant-find-any-lo">configurado incorretamente</a> ; </li><li>  O teste de vida falhou muitas vezes. </li></ol><br>  Você deve tentar acessar os logs do contêiner para descobrir o motivo de sua falha.  Se o acesso aos logs for difícil, porque o contêiner é reiniciado muito rapidamente, você pode usar o seguinte comando: <br><br><pre> <code class="bash hljs">kubectl logs &lt;pod-name&gt; --previous</code> </pre> <br>  Ele exibe mensagens de erro de uma reencarnação anterior do contêiner. <br><br><h4>  RunContainerError </h4><br>  Este erro ocorre quando o contêiner falha ao iniciar.  Corresponde ao momento anterior ao lançamento do aplicativo.  Geralmente, sua causa é a configuração incorreta, por exemplo: <br><br><ul><li>  Tentativa de montar um volume inexistente, como ConfigMap ou Secrets; </li><li>  tente montar um volume somente leitura como leitura e gravação. </li></ul><br>  O <code>kubectl describe pod &lt;pod-name&gt;</code> é adequado para analisar esses erros. <br><br><h3>  Pods pendentes </h3><br>  Após a criação, o pod permanece no estado <code>Pending</code> . <br><br>  <i>Por que isso está acontecendo?</i> <br><br>  Aqui estão os possíveis motivos (suponho que o agendador esteja funcionando bem): <br><br><ol><li>  O cluster não possui recursos suficientes, como poder de processamento e memória, para executar o pod. </li><li>  O objeto <code>ResourceQuota</code> é instalado no espaço para nome correspondente e a criação de um pod fará com que o espaço para nome ultrapasse a cota. </li><li>  O pod está vinculado a Pending <code>PersistentVolumeClaim</code> . </li></ol><br>  Nesse caso, é recomendável usar o comando <code>kubectl describe</code> e verificar a seção <code>Events</code> : <br><br><pre> <code class="bash hljs">kubectl describe pod &lt;pod name&gt;</code> </pre> <br>  No caso de erros relacionados ao <code>ResourceQuotas</code> , é recomendável visualizar os logs do cluster usando o comando <br><br><pre> <code class="bash hljs">kubectl get events --sort-by=.metadata.creationTimestamp</code> </pre> <br><h3>  Pods não prontos </h3><br>  Se o pod estiver listado como Em <code>Running</code> , mas não estiver no estado <code>Ready</code> , a <i>sonda de</i> prontidão não <i>terá</i> êxito. <br><br>  Quando isso acontece, o pod não se conecta ao serviço e o tráfego não flui para ele.  O teste de prontidão falhou devido a problemas de aplicativo.  Nesse caso, para encontrar o erro, você precisa analisar a seção <code>Events</code> na saída do comando <code>kubectl describe</code> . <br><br><h2>  2. Diagnóstico de serviços </h2><br>  Se os pods estiverem listados como Em <code>Running</code> e <code>Ready</code> , mas ainda não houver resposta do aplicativo, verifique as configurações do serviço. <br><br>  Os serviços estão envolvidos no roteamento de tráfego para os pods, dependendo de seus rótulos.  Portanto, a primeira coisa a fazer é verificar quantos pods funcionam com o serviço.  Para fazer isso, você pode verificar os pontos de extremidade no serviço: <br><br><pre> <code class="bash hljs">kubectl describe service &lt;service-name&gt; | grep Endpoints</code> </pre> <br>  O ponto de extremidade é um par de valores no formato <code>&lt;IP-:&gt;</code> e pelo menos um desses pares deve estar presente na saída (ou seja, pelo menos um pod funciona com o serviço). <br><br>  Se a seção <code>Endpoins</code> vazia, duas opções serão possíveis: <br><br><ol><li>  não há pods com o rótulo correto (dica: verifique se o espaço para nome está selecionado corretamente); </li><li>  Há um erro nas etiquetas de serviço no seletor. </li></ol><br>  Se você vir uma lista de pontos de extremidade, mas ainda não conseguir acessar o aplicativo, o provável culpado é o erro no <code>targetPort</code> na descrição do serviço. <br><br>  <i>Como verificar a capacidade de manutenção do serviço?</i> <br><br>  Independentemente do tipo de serviço, você pode usar o <code>kubectl port-forward</code> para se conectar a ele: <br><br><pre> <code class="bash hljs">kubectl port-forward service/&lt;service-name&gt; 3000:80</code> </pre> <br>  Aqui: <br><br><ul><li>  <code>&lt;service-name&gt;</code> - o nome do serviço; </li><li>  3000 - a porta que você abre no computador; </li><li>  80 - porta no lado do serviço. </li></ul><br><h2>  3. Diagnóstico do ingresso </h2><br>  Se você ler este lugar, então: <br><br><ul><li>  os pods estão listados como <code>Running</code> and <code>Ready</code> ; </li><li>  o serviço distribui com sucesso o tráfego entre os pods. </li></ul><br>  No entanto, você ainda não pode "alcançar" o aplicativo. <br><br>  Isso significa que, muito provavelmente, o controlador do Ingress está configurado incorretamente.  Como o controlador do Ingress é um componente de terceiros no cluster, existem vários métodos de depuração, dependendo do seu tipo. <br><br>  Mas antes de recorrer a ferramentas especiais para configurar o Ingress, você pode fazer algo muito simples.  O Ingress usa <code>serviceName</code> e <code>servicePort</code> para se conectar ao serviço.  Você deve verificar se eles estão configurados corretamente.  Você pode fazer isso usando o comando: <br><br><pre> <code class="bash hljs">kubectl describe ingress &lt;ingress-name&gt;</code> </pre> <br>  Se a coluna <code>Backend</code> - <code>Backend</code> estiver vazia, há uma grande chance de um erro de configuração.  Se os back-end estiverem instalados, mas ainda não houver acesso ao aplicativo, o problema poderá estar relacionado a: <br><br><ul><li>  Configurações de acessibilidade do ingresso da Internet pública; </li><li>  configurações de acessibilidade de cluster da Internet pública. </li></ul><br>  Você pode identificar problemas de infraestrutura conectando-se diretamente ao pod do Ingress.  Para fazer isso, primeiro encontre o pod do controlador do Ingress (ele pode estar em um espaço para nome diferente): <br><br><pre> <code class="bash hljs">kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS kube-system coredns-5644d7b6d9-jn7cq 1/1 Running kube-system etcd-minikube 1/1 Running kube-system kube-apiserver-minikube 1/1 Running kube-system kube-controller-manager-minikube 1/1 Running kube-system kube-proxy-zvf2h 1/1 Running kube-system kube-scheduler-minikube 1/1 Running kube-system nginx-ingress-controller-6fc5bcc 1/1 Running</code> </pre> <br>  Use o comando de <code>describe</code> para definir a porta: <br><br><pre> <code class="bash hljs">kubectl describe pod nginx-ingress-controller-6fc5bcc --namespace kube-system \ | grep Ports</code> </pre> <br>  Por fim, conecte-se ao pod: <br><br><pre> <code class="bash hljs">kubectl port-forward nginx-ingress-controller-6fc5bcc 3000:80 --namespace kube-system</code> </pre> <br>  Agora, todos os pedidos da porta 3000 no computador serão redirecionados para o pod da porta 80. <br><br>  <i>Isso funciona agora?</i> <br><br><ul><li>  Nesse caso, o problema está na infraestrutura.  É necessário descobrir exatamente como o tráfego é roteado para o cluster. </li><li>  Caso contrário, o problema está no controlador do Ingress. </li></ul><br>  Se você não conseguir que o controlador do Ingress funcione, será necessário depurá-lo. <br><br>  Existem muitas variedades de controladores do Ingress.  Os mais populares são Nginx, HAProxy, Traefik etc. <i>(para obter mais informações sobre soluções existentes, consulte <a href="https://habr.com/ru/company/flant/blog/447180/">nossa revisão</a> - aprox. Transl.) Você</i> deve usar o guia de solução de problemas na documentação do controlador correspondente.  Como o <a href="https://github.com/kubernetes/ingress-nginx">Ingress Nginx</a> é o controlador de ingresso mais popular, incluímos algumas dicas sobre como resolver problemas relacionados neste artigo. <br><br><h3>  Depurando um controlador Nginx do Ingress </h3><br><br>  O projeto Ingress-nginx possui um <a href="https://kubernetes.github.io/ingress-nginx/kubectl-plugin/">plugin</a> oficial <a href="https://kubernetes.github.io/ingress-nginx/kubectl-plugin/">para o kubectl</a> .  O <code>kubectl ingress-nginx</code> pode ser usado para: <br><br><ul><li>  análise de logs, backends, certificados, etc; </li><li>  conexão com o Ingress; </li><li>  estudando a configuração atual. </li></ul><br>  As três equipes a seguir o ajudarão com isso: <br><br><ul><li>  <code>kubectl ingress-nginx lint</code> - verifica o <code>nginx.conf</code> ; </li><li>  <code>kubectl ingress-nginx backend</code> - examina o back-end (semelhante ao <code>kubectl describe ingress &lt;ingress-name&gt;</code> ); </li><li>  <code>kubectl ingress-nginx logs</code> - verifica os logs. </li></ul><br>  Observe que, em alguns casos, pode ser necessário especificar o espaço para nome correto para o controlador Ingress usando o <code>--namespace &lt;name&gt;</code> . <br><br><h2>  Sumário </h2><br>  Diagnosticar o Kubernetes pode ser uma tarefa assustadora, se você não sabe por onde começar.  O problema sempre deve ser abordado de acordo com o princípio de baixo para cima: comece com pods e depois vá para o serviço e o Ingress.  Os métodos de depuração descritos no artigo podem ser aplicados a outros objetos, como: <br><br><ul><li>  Jobs ociosos e CronJobs; </li><li>  StatefulSets e DaemonSets. </li></ul><br>  Agradeço a <a href="https://github.com/errge">Gergely Risko</a> , <a href="https://medium.com/%40weibeld">Daniel Weibel</a> e <a href="https://www.linkedin.com/in/charles-christyraj-0bab8a36/">Charles Christyraj</a> pelos valiosos comentários e adições. <br><br><h2>  PS do tradutor </h2><br>  Leia também em nosso blog: <br><br><ul><li>  “ <a href="https://habr.com/ru/company/flant/blog/436112/">Plugin Kubectl-debug para depuração nos pods do Kubernetes</a> ”; </li><li>  “ <a href="https://habr.com/ru/company/flant/blog/443458/">6 erros divertidos do sistema na operação do Kubernetes [e sua solução]</a> ”; </li><li>  “ <a href="https://habr.com/ru/company/flant/blog/462707/">Ferramentas para desenvolvedores de aplicativos em execução no Kubernetes</a> ”; </li><li>  " <a href="https://habr.com/ru/company/flant/blog/471892/">6 histórias práticas da nossa vida cotidiana no SRE</a> ". </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt484954/">https://habr.com/ru/post/pt484954/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt484936/index.html">Conhecimento e competências da equipe: encontre, veja, bombeie</a></li>
<li><a href="../pt484944/index.html">O que eu estou no ACID ou não nos convém</a></li>
<li><a href="../pt484946/index.html">Modelagem GPR</a></li>
<li><a href="../pt484948/index.html">NEC lançou um cabo submarino com um recorde de 20 pares de fibras ópticas</a></li>
<li><a href="../pt484952/index.html">Substituindo Redux por Observables e React Hooks</a></li>
<li><a href="../pt484964/index.html">Configurando o balanceamento de carga no InfoWatch Traffic Monitor</a></li>
<li><a href="../pt484966/index.html">Modelo pronto para teste usando o Spring</a></li>
<li><a href="../pt484968/index.html">WPF DataGrid. Lutar por modelo</a></li>
<li><a href="../pt484972/index.html">Lançamento do Wine 5.0</a></li>
<li><a href="../pt484974/index.html">Telhas Wang para simulação de máquina de Turing</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>