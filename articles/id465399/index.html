<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üçé üìí üíê Menyebarkan penyimpanan yang didistribusikan oleh CEPH dan menghubungkannya ke Kubernetes üê§ üéè üóæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bagian 1 Kami menyebarkan lingkungan untuk bekerja dengan layanan microser. Bagian 1 menginstal Kubernetes HA pada bare metal (Debian) 
 Halo, para pe...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Menyebarkan penyimpanan yang didistribusikan oleh CEPH dan menghubungkannya ke Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/465399/"><p><img src="https://habrastorage.org/webt/w-/ha/t9/w-hat9zskv2ab6vdpumzdxllw8w.png"></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bagian 1 Kami menyebarkan lingkungan untuk bekerja dengan layanan microser.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bagian 1 menginstal Kubernetes HA pada bare metal (Debian)</a> </p><br><h2>  Halo, para pembaca Habr! </h2><br><p>  Dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">posting</a> sebelumnya <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">,</a> saya berbicara tentang cara menggunakan kluster failover Kubernetes.  Tetapi faktanya adalah bahwa di Kubernetes nyaman untuk menggunakan aplikasi stateless yang tidak perlu mempertahankan statusnya atau bekerja dengan data.  Tetapi dalam kebanyakan kasus, kita perlu menyimpan data dan tidak kehilangannya saat memulai kembali perapian. <br>  Kubernet menggunakan volume untuk tujuan ini.  Ketika kami bekerja dengan solusi cloud Kubernetes, tidak ada masalah khusus.  Kami hanya perlu memesan volume yang diperlukan dari Google, Amazon atau penyedia cloud lainnya dan, dipandu oleh <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi</a> , menghubungkan volume yang diterima ke pod. <br>  Ketika kita berurusan dengan bare metal, segalanya menjadi sedikit lebih rumit.  Hari ini saya ingin berbicara tentang salah satu solusi berdasarkan penggunaan ceph. </p><br><p>  Dalam publikasi ini saya akan memberi tahu: </p><br><ul><li>  cara menggunakan penyimpanan yang didistribusikan ceph </li><li>  Cara menggunakan Ceph saat bekerja dengan Kubernetes <a name="habracut"></a></li></ul><br><h2>  Pendahuluan </h2><br><p>  Untuk mulai dengan, saya ingin menjelaskan kepada siapa artikel ini akan berguna.  Pertama, untuk pembaca yang menggunakan cluster menurut publikasi pertama saya untuk terus membangun arsitektur layanan-mikro.  Kedua, bagi orang-orang yang ingin mencoba menggunakan ceph cluster sendiri dan mengevaluasi kinerjanya. </p><br><p>  Dalam publikasi ini, saya tidak akan menyentuh topik perencanaan klaster untuk kebutuhan apa pun, saya hanya akan berbicara tentang prinsip dan konsep umum.  Saya tidak akan mempelajari "penyetelan" dan penyetelan dalam, ada banyak publikasi tentang hal ini, termasuk tentang Habr.  Artikel ini akan lebih bersifat pengantar, tetapi pada saat yang sama akan memungkinkan Anda untuk mendapatkan solusi yang dapat Anda beradaptasi dengan kebutuhan Anda di masa depan. </p><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Daftar host, sumber daya host, OS dan versi perangkat lunak</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Struktur Cluster Ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Konfigurasikan node cluster sebelum instalasi</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Instal ceph-deploy</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Membuat cluster ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pengaturan jaringan</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Instal paket ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Instalasi dan inisialisasi monitor</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Menambahkan OSD</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Hubungkan ceph ke kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Membuat kumpulan data</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Menciptakan rahasia klien</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Menyebarkan penyedia rbd ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Membuat kelas penyimpanan</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Tes ligamen Kubernetes + ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Daftar bahan yang digunakan dalam persiapan artikel</a> </li></ol><br><a name="vm"></a><br><h2>  Daftar Host dan Persyaratan Sistem </h2><br><div class="scrollable-table"><table><tbody><tr><th>  <b>Nama</b> </th><th>  <b>Alamat IP</b> </th><th>  <b>Komentar</b> </th></tr><tr><td>  ceph01-test </td><td>  10.73.88.52 </td><td>  ceph-node01 </td></tr><tr><td>  ceph02-test </td><td>  10.73.88.53 </td><td>  ceph-node02 </td></tr><tr><td>  ceph03-test </td><td>  10.73.88.54 </td><td>  ceph-node03 </td></tr></tbody></table></div><br><p>  Saat menulis artikel, saya menggunakan mesin virtual dengan konfigurasi ini </p><br><p><img src="https://habrastorage.org/webt/kd/-z/bd/kd-zbdwb_76g1vvvgmtojoirseo.png"></p><br><p>  Masing-masing memiliki OS 9.5 Debian diinstal.  Ini adalah mesin uji, masing-masing dengan dua disk, yang pertama untuk OS, yang kedua untuk OSD Cef </p><br><p>  Saya akan menyebarkan cluster melalui utilitas ceph-deploy.  Anda dapat menggunakan ceph cluster dalam mode manual, semua langkah dijelaskan dalam dokumentasi, tetapi tujuan artikel ini adalah untuk mengetahui seberapa cepat Anda dapat menggunakan ceph dan mulai menggunakannya di kubernetes. <br>  Ceph cukup rakus untuk sumber daya, terutama RAM.  Untuk kecepatan yang baik, disarankan untuk menggunakan drive SSD. </p><br><p>  Anda dapat membaca lebih lanjut tentang persyaratan dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi resmi Ceph.</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><br></a> </p><br><a name="ceph"></a><br><h2>  Struktur Cluster Ceph </h2><br><p>  <strong>MON</strong> <br>  <em>Monitor adalah daemon yang bertindak sebagai koordinator dari mana cluster dimulai.</em>  <em>Segera setelah kami memiliki setidaknya satu monitor yang berfungsi, kami memiliki cluster Ceph.</em>  <em>Monitor menyimpan informasi tentang kesehatan dan kondisi cluster dengan menukar berbagai kartu dengan monitor lain.</em>  <em>Klien beralih ke monitor untuk mengetahui OSD mana yang akan ditulis / dibaca data.</em>  <em>Saat Anda menggunakan penyimpanan baru, hal pertama yang Anda lakukan adalah membuat monitor (atau beberapa).</em>  <em>Cluster dapat hidup pada satu monitor, tetapi disarankan untuk membuat 3 atau 5 monitor untuk menghindari jatuhnya seluruh sistem karena jatuhnya satu monitor.</em>  <em>Hal utama adalah bahwa jumlah ini harus ganjil untuk menghindari situasi otak-terpisah.</em>  <em>Monitor bekerja dalam kuorum, jadi jika lebih dari setengah monitor jatuh, cluster akan diblokir untuk mencegah inkonsistensi data.</em> <br>  <strong>Mgr</strong> <br>  <em>Daemon Ceph Manager bekerja dengan daemon monitor untuk memberikan kontrol tambahan.</em> <em><br></em>  <em>Sejak versi 12.x, daemon ceph-mgr menjadi penting untuk operasi normal.</em> <em><br></em>  <em>Jika daemon mgr tidak berjalan, Anda akan melihat peringatan tentang ini.</em> <br>  <strong>OSD (Perangkat Penyimpanan Objek)</strong> <br>  <em>OSD adalah unit penyimpanan yang menyimpan data itu sendiri dan memproses permintaan klien dengan bertukar data dengan OSD lain.</em>  <em>Ini biasanya sebuah disk.</em>  <em>Dan biasanya untuk setiap OSD ada daemon OSD terpisah yang dapat berjalan pada mesin mana pun di mana disk ini diinstal.</em> </p><br><p>  Ketiga daemon akan bekerja pada setiap mesin di cluster kami.  Dengan demikian, monitor dan daemon manajer sebagai daemon layanan dan OSD untuk satu drive mesin virtual kami. </p><br><a name="before"></a><br><h2>  Konfigurasikan node cluster sebelum instalasi </h2><br><p>  Dokumentasi ceph menentukan alur kerja berikut: </p><br><p><img src="https://habrastorage.org/webt/wz/9i/gu/wz9igu71hiezyom4zvtbxp6ye30.png"></p><br><p>  Saya akan bekerja dari node pertama dari ceph01-test cluster, itu akan menjadi Admin Node, itu juga akan berisi file konfigurasi untuk utilitas ceph-deploy.  Agar utilitas penyebaran ceph berfungsi dengan benar, semua node cluster harus dapat diakses melalui ssh dengan simpul Admin.  Untuk kenyamanan, saya akan menulis nama pendek host untuk cluster </p><br><pre><code class="plaintext hljs">10.73.88.52 ceph01-test 10.73.88.53 ceph02-test 10.73.88.54 ceph03-tset</code> </pre> <br><p>  Dan salin kunci ke host lain.  Semua perintah akan saya jalankan dari root. </p><br><pre> <code class="plaintext hljs">ssh-copy-id ceph02-test ssh-copy-id ceph03-test</code> </pre> <br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Dokumentasi Pengaturan</a> </p><br><anchor>  ceph-deploy </anchor><br><h2>  Instal ceph-deploy </h2><br><p>  Langkah pertama adalah menginstal ceph-deploy pada mesin ceph01-test </p><br><pre> <code class="plaintext hljs">wget -q -O- 'https://download.ceph.com/keys/release.asc' | apt-key add -</code> </pre> <br><p>  Selanjutnya, Anda harus memilih rilis yang ingin Anda masukkan.  Tetapi di sini ada kesulitan, saat ini ceph untuk Debian OS hanya mendukung paket yang bercahaya. <br>  Jika Anda ingin memasang rilis yang lebih baru, Anda harus menggunakan cermin, misalnya <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">https://mirror.croit.io/debian-mimic/dists/</a> </p><br><p>  Tambahkan repositori dengan meniru pada ketiga node </p><br><pre> <code class="plaintext hljs">apt install curl apt-transport-https -y curl https://mirror.croit.io/keys/release.gpg &gt; /usr/share/keyrings/croit-signing-key.gpg echo 'deb [signed-by=/usr/share/keyrings/croit-signing-key.gpg] https://mirror.croit.io/debian-mimic/ stretch main' &gt; /etc/apt/sources.list.d/croit-ceph.list apt update apt install ceph-deploy</code> </pre> <br><p>  Jika luminous sudah cukup untuk Anda, maka Anda dapat menggunakan repositori resmi </p><br><pre> <code class="plaintext hljs">echo deb https://download.ceph.com/debian-luminous/ $(lsb_release -sc) main | tee /etc/apt/sources.list.d/ceph.list apt-transport-https apt update apt install ceph-deploy</code> </pre> <br><p>  Kami juga menginstal NTP pada ketiga node. </p><br><div class="spoiler">  <b class="spoiler_title">karena rekomendasi ini ada dalam dokumentasi ceph</b> <div class="spoiler_text"><p>  Kami merekomendasikan menginstal NTP pada node Ceph (terutama pada node Monitor Ceph) untuk mencegah masalah yang timbul dari penyimpangan jam. <br></p></div></div><br><pre> <code class="plaintext hljs">apt install ntp</code> </pre> <br><p>  Pastikan Anda mengaktifkan layanan NTP.  Pastikan bahwa setiap simpul Ceph menggunakan server NTP yang sama.  Anda dapat melihat detail lebih lanjut <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">di sini</a> </p><br><a name="ceph-install"></a><br><h2>  Membuat cluster ceph </h2><br><p>  Buat direktori untuk file config dan file ceph-deploy </p><br><pre> <code class="plaintext hljs">mkdir my-cluster cd my-cluster</code> </pre> <br><p>  Mari kita membuat konfigurasi cluster baru, saat membuat, menunjukkan bahwa akan ada tiga monitor di cluster kami </p><br><pre> <code class="plaintext hljs">ceph-deploy new ceph01-test ceph02-test ceph03-test</code> </pre> <br><a name="ceph-network"></a><br><h2>  Pengaturan jaringan </h2><br><p>  Sekarang poin penting, saatnya berbicara tentang jaringan untuk ceph.  Ceph menggunakan dua jaringan publik dan sebuah jaringan cluster untuk bekerja <br><img src="https://habrastorage.org/webt/jz/dp/wz/jzdpwzxrzpdqr7k9v4u53hmc05s.png"></p><br><p>  Seperti yang Anda lihat dari diagram jaringan publik, ini adalah level pengguna dan aplikasi, dan jaringan cluster adalah jaringan yang dengannya data direplikasi. <br>  Sangat diinginkan untuk memisahkan kedua jaringan ini dari satu sama lain.  Juga, jaringan kluster kecepatan jaringan diinginkan setidaknya 10 Gb. <br>  Tentu saja, Anda dapat menyimpan semuanya di jaringan yang sama.  Tapi ini penuh dengan fakta bahwa segera setelah volume replikasi antara OSD meningkat, misalnya, ketika OSD baru (disk) jatuh atau ditambahkan, beban jaringan akan SANGAT meningkat.  Jadi kecepatan dan stabilitas infrastruktur Anda akan sangat tergantung pada jaringan yang digunakan oleh ceph. <br>  Sayangnya, kluster virtualisasi saya tidak memiliki jaringan yang terpisah, dan saya akan menggunakan segmen jaringan yang umum. <br>  Konfigurasi jaringan untuk cluster dilakukan melalui file config, yang kami hasilkan dengan perintah sebelumnya. </p><br><pre> <code class="plaintext hljs">/my-cluster# cat ceph.conf [global] fsid = 2e0d92b0-e803-475e-9060-0871b63b6e7f mon_initial_members = ceph01-test, ceph02-test, ceph03-test mon_host = 10.73.88.52,10.73.88.53,10.73.88.54 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx</code> </pre> <br><p>  Seperti yang dapat kita lihat, penyebaran cef tidak membuat pengaturan jaringan default untuk kita, jadi saya akan menambahkan parameter jaringan publik = {jaringan publik / netmask} ke bagian global dari konfigurasi.  Jaringan saya adalah 10.73.0.0/16, jadi setelah menambahkan konfigurasi saya akan terlihat seperti ini </p><br><pre> <code class="plaintext hljs">[global] fsid = 2e0d92b0-e803-475e-9060-0871b63b6e7f mon_initial_members = ceph01-test, ceph02-test, ceph03-test mon_host = 10.73.88.52,10.73.88.53,10.73.88.54 public network = 10.73.0.0/16 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx</code> </pre> <br><p>  Jika Anda ingin memisahkan jaringan cluster dari publik, tambahkan parameter cluster network = {cluster-network / netmask} <br>  Anda <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dapat membaca</a> lebih lanjut tentang jaringan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dalam dokumentasi.</a> </p><br><a name="ceph-pack"></a><br><h2>  Instal paket ceph </h2><br><p>  Menggunakan ceph-deploy, kami menginstal semua paket ceph yang kami butuhkan pada tiga node kami. <br>  Untuk melakukan ini, pada ceph01-test, jalankan <br>  Jika versi ini meniru maka </p><br><pre> <code class="plaintext hljs">ceph-deploy install --release mimic ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  Jika versinya bercahaya maka </p><br><pre> <code class="plaintext hljs">ceph-deploy install --release luminous ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  Dan tunggu sampai semuanya ditetapkan. </p><br><a name="ceph-mon"></a><br><h2>  Instalasi dan inisialisasi monitor </h2><br><p>  Setelah semua paket diinstal, kami akan membuat dan memulai monitor cluster kami. <br>  Uji coba lakukan hal berikut </p><br><pre> <code class="plaintext hljs">ceph-deploy mon create-initial</code> </pre> <br><p>  Monitor akan dibuat dalam proses, daemon akan diluncurkan, dan ceph-deploy akan memeriksa kuorum. <br>  Sekarang sebar konfigurasi pada node cluster. </p><br><pre> <code class="plaintext hljs">ceph-deploy admin ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  Dan periksa status cluster kami, jika Anda melakukan semuanya dengan benar, maka statusnya seharusnya <br>  HEALTH_OK </p><br><pre> <code class="plaintext hljs">~/my-cluster# ceph status cluster: id: 2e0d92b0-e803-475e-9060-0871b63b6e7f health: HEALTH_OK services: mon: 3 daemons, quorum ceph01-test,ceph02-test,ceph03-test mgr: no daemons active osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs:</code> </pre> <br><p>  Buat mgr </p><br><pre> <code class="plaintext hljs">ceph-deploy mgr create ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  Dan periksa kembali statusnya </p><br><pre> <code class="plaintext hljs">ceph -s</code> </pre> <br><p>  Sebuah garis akan muncul </p><br><pre> <code class="plaintext hljs">mgr: ceph01-test(active), standbys: ceph02-test, ceph03-test</code> </pre> <br><p>  Kami menulis konfigurasi untuk semua host di kluster </p><br><pre> <code class="plaintext hljs">ceph-deploy admin ceph01-test ceph02-test ceph03-test</code> </pre> <br><a name="ceph-osd"></a><br><h2>  Menambahkan OSD </h2><br><p>  Saat ini kami memiliki cluster yang berfungsi, tetapi belum memiliki disk (osd dalam terminologi ceph) untuk menyimpan informasi. </p><br><p>  OSD dapat ditambahkan dengan perintah berikut (tampilan umum) </p><br><pre> <code class="plaintext hljs">ceph-deploy osd create --data {device} {ceph-node}</code> </pre> <br><p>  Dalam testbed saya, disk / dev / sdb dialokasikan di bawah osd, jadi dalam kasus saya perintahnya adalah sebagai berikut </p><br><pre> <code class="plaintext hljs">ceph-deploy osd create --data /dev/sdb ceph01-test ceph-deploy osd create --data /dev/sdb ceph02-test ceph-deploy osd create --data /dev/sdb ceph03-test</code> </pre> <br><p>  Periksa apakah semua OSD berfungsi. </p><br><pre> <code class="plaintext hljs">ceph -s</code> </pre> <br><p>  Kesimpulan </p><br><pre> <code class="plaintext hljs"> cluster: id: 2e0d92b0-e803-475e-9060-0871b63b6e7f health: HEALTH_OK services: mon: 3 daemons, quorum ceph01-test,ceph02-test,ceph03-test mgr: ceph01-test(active) osd: 3 osds: 3 up, 3 in</code> </pre> <br><p>  Anda juga dapat mencoba beberapa perintah yang berguna untuk OSD. </p><br><pre> <code class="plaintext hljs">ceph osd df ID CLASS WEIGHT REWEIGHT SIZE USE AVAIL %USE VAR PGS 0 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 1 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 2 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 TOTAL 15 GiB 3.0 GiB 12 GiB 20.05</code> </pre> <br><p>  dan </p><br><pre> <code class="plaintext hljs">ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.01469 root default -3 0.00490 host ceph01-test 0 hdd 0.00490 osd.0 up 1.00000 1.00000 -5 0.00490 host ceph02-test 1 hdd 0.00490 osd.1 up 1.00000 1.00000 -7 0.00490 host ceph03-test 2 hdd 0.00490 osd.2 up 1.00000 1.00000</code> </pre><br><p>  Jika semuanya OK, maka kami memiliki ceph cluster yang berfungsi.  Pada bagian selanjutnya saya akan memberi tahu cara menggunakan ceph dengan kubernetes </p><br><a name="kubernetes"></a><br><h1>  Hubungkan ceph ke kubernetes </h1><br><p>  Sayangnya, saya tidak akan dapat menjelaskan secara terperinci operasi volume Kubernetes dalam artikel ini, jadi saya akan mencoba mencocokkannya menjadi satu paragraf. <br>  Kubernetes menggunakan kelas penyimpanan untuk bekerja dengan volume data data, setiap kelas penyimpanan memiliki penyedia sendiri, Anda dapat menganggapnya sebagai semacam "driver" untuk bekerja dengan volume penyimpanan data yang berbeda.  Daftar lengkap yang mendukung kubernetes dapat ditemukan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi resmi</a> . <br>  Kubernetes sendiri juga memiliki dukungan untuk bekerja dengan rbd, tetapi tidak ada klien rbd yang diinstal pada gambar kube-controller-manager resmi, jadi Anda perlu menggunakan gambar yang berbeda. <br>  Perlu juga dicatat bahwa volume (pvc) yang dibuat sebagai rbd hanya dapat ReadWriteOnce (RWO) dan, yang berarti Anda dapat memasang volume yang dibuat HANYA ke satu perapian. </p><br><p>  Agar cluster kami dapat bekerja dengan volume ceph, kita perlu: <br>  dalam cluster Ceph: </p><br><ul><li>  buat kumpulan data dalam ceph cluster </li><li>  buat klien dan kunci akses ke kumpulan data </li><li>  dapatkan rahasia admin ceph </li></ul><br><p>  Agar cluster kami dapat bekerja dengan volume ceph, kita perlu: <br>  dalam cluster Ceph: </p><br><ul><li>  buat kumpulan data dalam ceph cluster </li><li>  buat klien dan kunci akses ke kumpulan data </li><li>  dapatkan rahasia admin ceph </li></ul><br><p>  di kluster Kubernetes: </p><br><ul><li>  buat rahasia admin ceph dan kunci klien ceph </li><li>  instal ceph rbd provisioner atau ubah gambar kube-controller-manager menjadi gambar yang mendukung rbd </li><li>  buat rahasia dengan kunci klien ceph </li><li>  buat kelas penyimpanan </li><li>  instal ceph-common pada catatan pekerja kubernetes </li></ul><br><a name="ceph-pool"></a><br><h2>  Membuat kumpulan data </h2><br><p>  Di ceph cluster, buat kumpulan untuk volume kubernetes </p><br><pre> <code class="plaintext hljs">ceph osd pool create kube 8 8</code> </pre> <br><p>  Di sini saya akan membuat penjelasan kecil, angka 8 8 pada akhirnya adalah angka pg dan pgs.  Nilai-nilai ini tergantung pada ukuran cluster ceph Anda.  Ada kalkulator khusus yang menghitung jumlah pg dan pg, misalnya, resmi dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ceph</a> <br>  Untuk memulai, saya sarankan untuk membiarkannya secara default, jika di masa depan jumlah ini dapat ditingkatkan (hanya dapat dikurangi dengan versi Nautilus). </p><br><a name="ceph-key"></a><br><h2>  Membuat klien untuk kumpulan data </h2><br><p>  Buat klien untuk kumpulan baru </p><br><pre> <code class="plaintext hljs">ceph auth add client.kube mon 'allow r' osd 'allow rwx pool=kube'</code> </pre> <br><p>  Kami akan menerima kunci untuk klien, di masa depan kami akan membutuhkannya untuk membuat kubernet rahasia </p><br><pre> <code class="plaintext hljs">ceph auth get-key client.kube AQDd5aldka5KJRAAkpWTQYUMQi+5dfGDqSyxkg==</code> </pre> <br><h2>  Mendapatkan kunci admin </h2><br><p>  Dan dapatkan kunci admin </p><br><pre> <code class="plaintext hljs">ceph auth get client.admin 2&gt;&amp;1 |grep "key = " |awk '{print $3'} AQAv+Itdx4DwKBAAKVhWRS3+eEPqV3Xrnlg9KA==</code> </pre> <br><p>  <strong>Pada ceph cluster, semua pekerjaan selesai dan sekarang kita harus pergi ke mesin yang memiliki akses ke cluster kubernetes</strong> <br>  Saya akan bekerja dengan uji master01 (10.73.71.25) dari gugus yang digunakan oleh saya dalam publikasi pertama. </p><br><a name="kubernetes-secrets"></a><br><h2>  Menciptakan rahasia klien </h2><br><p>  Buat file dengan token klien yang kami terima (jangan lupa ganti dengan token Anda) </p><br><pre> <code class="plaintext hljs">echo AQDd5aldka5KJRAAkpWTQYUMQi+5dfGDqSyxkg== &gt; /tmp/key.client</code> </pre> <br><p>  Dan buat rahasia yang akan kita gunakan di masa depan </p><br><pre> <code class="plaintext hljs">kubectl create secret generic ceph-secret --from-file=/tmp/key.client --namespace=kube-system --type=kubernetes.io/rbd</code> </pre> <br><h2>  Buat rahasia admin </h2><br><p>  Buat file dengan token admin (jangan lupa ganti dengan token Anda) </p><br><pre> <code class="plaintext hljs">echo AQAv+Itdx4DwKBAAKVhWRS3+eEPqV3Xrnlg9KA== &gt; /tmp/key.admin</code> </pre> <br><p>  Setelah itu buat rahasia admin </p><br><pre> <code class="plaintext hljs">kubectl create secret generic ceph-admin-secret --from-file=/tmp/key.admin --namespace=kube-system --type=kubernetes.io/rbd</code> </pre> <br><p>  Periksa apakah rahasia telah dibuat </p><br><pre> <code class="plaintext hljs">kubectl get secret -n kube-system | grep ceph ceph-admin-secret kubernetes.io/rbd 1 8m31s ceph-secret kubernetes.io/rbd 1 7m32s</code> </pre> <br><a name="kubernetes-provisioner"></a><br><h2>  Metode pertama menggunakan Ceph rbd provisioner </h2><br><p>  Kami mengkloning repositori kubernetes-incubator / external-storage dari github, ia memiliki semua yang Anda butuhkan untuk membuat teman-teman cluster kubernetes dengan repositori ceph. </p><br><pre> <code class="plaintext hljs">git clone https://github.com/kubernetes-incubator/external-storage.git cd external-storage/ceph/rbd/deploy/ NAMESPACE=kube-system sed -r -i "s/namespace: [^ ]+/namespace: $NAMESPACE/g" ./rbac/clusterrolebinding.yaml ./rbac/rolebinding.yaml</code> </pre> <br><pre> <code class="plaintext hljs">kubectl -n $NAMESPACE apply -f ./rbac</code> </pre> <br><p>  Kesimpulan </p><br><pre> <code class="plaintext hljs">clusterrole.rbac.authorization.k8s.io/rbd-provisioner created clusterrolebinding.rbac.authorization.k8s.io/rbd-provisioner created deployment.extensions/rbd-provisioner created role.rbac.authorization.k8s.io/rbd-provisioner created rolebinding.rbac.authorization.k8s.io/rbd-provisioner created serviceaccount/rbd-provisioner created</code> </pre> <br><h2>  Metode dua: Ganti gambar kube-controller-manager </h2><br><p>  Tidak ada dukungan rbd di gambar kube-controller-manager resmi, jadi kita perlu mengubah gambar controller-manager. <br>  Untuk melakukan ini, pada setiap penyihir Kubernetes, Anda perlu mengedit file kube-controller-manager.yaml dan mengganti gambar dengan gcr.io/google_containers/hyperkube:v1.15.2.  Perhatikan versi gambar yang harus cocok dengan versi cluster Kubernetes Anda. </p><br><pre> <code class="plaintext hljs">vim /etc/kubernetes/manifests/kube-controller-manager.yaml</code> </pre> <br><p>  Setelah itu, Anda perlu me-restart kube-controller-manager </p><br><pre> <code class="plaintext hljs">ubectl get pods -A | grep manager kube-system kube-controller-manager-master01-test 1/1 Running 0 5m54s kube-system kube-controller-manager-master02-test 1/1 Running 0 5m54s kube-system kube-controller-manager-master03-test 1/1 Running 9111 103d</code> </pre> <br><p>  Pod harus diperbarui secara otomatis, tetapi jika karena alasan ini tidak terjadi, Anda dapat membuatnya kembali secara manual, melalui penghapusan. </p><br><pre> <code class="plaintext hljs">kubectl delete pod -n kube-system kube-controller-manager-master01-test kubectl delete pod -n kube-system kube-controller-manager-master02-test kubectl delete pod -n kube-system kube-controller-manager-master03-test</code> </pre> <br><p>  Periksa apakah semuanya baik-baik saja </p><br><pre> <code class="plaintext hljs">kubectl describe pod -n kube-system kube-controller-manager-master02-test | grep Image: Image: gcr.io/google_containers/hyperkube:v1.15.2</code> </pre> <br><p>  - </p><br><a name="storage-class"></a><br><h2>  Membuat kelas penyimpanan </h2><br><p>  <strong>Metode satu - jika Anda menggunakan pemberi ketentuan ceph.com/rbd</strong> <br>  Buat file yaml dengan deskripsi kelas penyimpanan kami.  Juga, semua file yang digunakan di bawah ini dapat diunduh <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">di repositori saya</a> di direktori ceph </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./storage-class.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ceph-rbd provisioner: ceph.com/rbd parameters: monitors: 10.73.88.52:6789, 10.73.88.53:6789, 10.73.88.54:6789 pool: kube adminId: admin adminSecretNamespace: kube-system adminSecretName: ceph-admin-secret userId: kube userSecretNamespace: kube-system userSecretName: ceph-secret imageFormat: "2" imageFeatures: layering EOF</code> </pre> <br><p>  Dan menanamkannya di cluster kami </p><br><pre> <code class="plaintext hljs">kubectl apply -f storage-class.yaml</code> </pre> <br><p>  Periksa apakah semuanya baik-baik saja </p><br><pre> <code class="plaintext hljs">kubectl get sc NAME PROVISIONER AGE ceph-rbd ceph.com/rbd 7s</code> </pre> <br><p>  <strong>Metode dua - jika Anda menggunakan kubernetes.io/rbd</strong> <br>  Buat kelas penyimpanan </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./storage-class-hyperkube.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ceph-rbd provisioner: kubernetes.io/rbd allowVolumeExpansion: true parameters: monitors: 10.73.88.52:6789, 10.73.88.53:6789, 10.73.88.54:6789 pool: kube adminId: admin adminSecretNamespace: kube-system adminSecretName: ceph-admin-secret userId: kube userSecretNamespace: kube-system userSecretName: ceph-secret imageFormat: "2" imageFeatures: layering EOF</code> </pre> <br><p>  Dan menanamkannya di cluster kami </p><br><pre> <code class="plaintext hljs">kubectl apply -f storage-class-hyperkube.yaml storageclass.storage.k8s.io/ceph-rbd created</code> </pre> <br><p>  Periksa apakah semuanya baik-baik saja </p><br><pre> <code class="plaintext hljs">kubectl get sc NAME PROVISIONER AGE ceph-rbd kubernetes.io/rbd 107s</code> </pre> <br><a name="test"></a><br><h2>  Tes ligamen Kubernetes + ceph </h2><br><p>  Sebelum menguji ceph + kubernetes, Anda harus menginstal paket ceph-common pada SETIAP workcode cluster. </p><br><pre> <code class="plaintext hljs">apt install curl apt-transport-https -y curl https://mirror.croit.io/keys/release.gpg &gt; /usr/share/keyrings/croit-signing-key.gpg echo 'deb [signed-by=/usr/share/keyrings/croit-signing-key.gpg] https://mirror.croit.io/debian-mimic/ stretch main' &gt; /etc/apt/sources.list.d/croit-ceph.list apt update apt install ceph-common</code> </pre> <br><p>  Buat file yaml, PersistentVolumeClaim </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./claim.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: claim1 spec: accessModes: - ReadWriteOnce storageClassName: ceph-rbd resources: requests: storage: 1Gi EOF</code> </pre> <br><p>  Bunuh dia </p><br><pre> <code class="plaintext hljs">kubectl apply -f claim.yaml</code> </pre> <br><p>  Periksa apakah PersistentVolumeClaim dibuat. </p><br><pre> <code class="plaintext hljs">bectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE claim1 Bound pvc-d1e47825-289c-4201-acb8-033e62a3fe81 1Gi RWO ceph-rbd 44m</code> </pre> <br><p>  Dan juga secara otomatis membuat PersistentVolume. </p><br><pre> <code class="plaintext hljs">kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-d1e47825-289c-4201-acb8-033e62a3fe81 1Gi RWO Delete Bound default/claim1 ceph-rbd 37m</code> </pre> <br><p>  Mari kita buat satu test pod di mana kita memasukkan pvc yang dibuat di direktori / mnt.  Jalankan file ini /mnt/test.txt dengan teks "Hello World!" </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./create-file-pod.yaml kind: Pod apiVersion: v1 metadata: name: create-file-pod spec: containers: - name: test-pod image: gcr.io/google_containers/busybox:1.24 command: - "/bin/sh" args: - "-c" - "echo Hello world! &gt; /mnt/test.txt &amp;&amp; exit 0 || exit 1" volumeMounts: - name: pvc mountPath: "/mnt" restartPolicy: "Never" volumes: - name: pvc persistentVolumeClaim: claimName: claim1 EOF</code> </pre> <br><p>  Kami akan membunuhnya dan memverifikasi bahwa ia telah menyelesaikan tugasnya </p><br><pre> <code class="plaintext hljs">kubectl apply -f create-file-pod.yaml kubectl get pods -w</code> </pre> <br><p>  Mari kita tunggu statusnya </p><br><pre> <code class="plaintext hljs">create-file-pod 0/1 Completed 0 16s</code> </pre> <br><p>  Mari kita buat yang lain, hubungkan volume kita ke sana tetapi sudah di / mnt / test, dan setelah itu pastikan bahwa file yang dibuat oleh volume pertama sudah ada. </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./test-pod.yaml kind: Pod apiVersion: v1 metadata: name: test-pod spec: containers: - name: test-pod image: gcr.io/google_containers/busybox:1.24 command: - "/bin/sh" args: - "-c" - "sleep 600" volumeMounts: - name: pvc mountPath: "/mnt/test" restartPolicy: "Never" volumes: - name: pvc persistentVolumeClaim: claimName: claim1 EOF</code> </pre> <br><p>  Jalankan kubectl dapatkan po -w dan tunggu sampai pod berjalan <br>  Setelah itu, mari kita masuki dan periksa apakah volumenya terkoneksi dan file kita di direktori / mnt / test </p><br><pre> <code class="plaintext hljs">kubectl exec test-pod -ti sh cat /mnt/test/test.txt Helo world!</code> </pre> <br><p>  Terima kasih sudah membaca sampai akhir.  Maaf atas keterlambatan posting. <br>  Saya siap menjawab semua pertanyaan dalam pesan pribadi atau di jejaring sosial yang ditunjukkan pada profil saya. <br>  Dalam publikasi kecil berikutnya saya akan memberi tahu Anda cara menggunakan penyimpanan S3 berdasarkan ceph cluster yang dibuat, dan kemudian sesuai dengan rencana dari publikasi pertama. </p><br><a name="book"></a><br><p>  Bahan yang digunakan untuk publikasi </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Dokumentasi Resmi Ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Memperkenalkan repositori Ceph dalam gambar</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Dokumentasi resmi Kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">kubernetes-inkubator / external-storage</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">repositori kubernetes-ceph-percona dengan file sampel</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id465399/">https://habr.com/ru/post/id465399/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id465379/index.html">Kontrol pompa insulin nirkabel yang berdiri sendiri di rumah</a></li>
<li><a href="../id465391/index.html">Mengikuti jejak gerakan Scala Rusia. Bagian 1</a></li>
<li><a href="../id465393/index.html">Daya Baterai untuk Perangkat MySensors</a></li>
<li><a href="../id465395/index.html">Apa perbedaan utama antara Dependency Injection dan Service Locator?</a></li>
<li><a href="../id465397/index.html">Bagaimana penerjemah Nitro muncul, yang membantu pengembang dalam pelokalan dan dukungan teknis</a></li>
<li><a href="../id465401/index.html">5 Kegiatan untuk Mempercepat Pemecahan Masalah di Tim TI Apa Pun</a></li>
<li><a href="../id465403/index.html">Achtung! Kamera baru di jalan atau informasi terbaru tentang radar dan detektor radar</a></li>
<li><a href="../id465407/index.html">1. Tinjauan Umum tentang Sakelar Lapisan Perusahaan Ekstrim</a></li>
<li><a href="../id465409/index.html">Praktik Terbaik Vue.js Untuk Pengembangan Web</a></li>
<li><a href="../id465415/index.html">Kami berbicara tentang DevOps dalam bahasa yang dapat dipahami</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>