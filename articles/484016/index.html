<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë´ üëßüèª üè¥Û†ÅßÛ†Å¢Û†Å≥Û†Å£Û†Å¥Û†Åø Los fundamentos del aprendizaje profundo en el ejemplo de autoencoder de depuraci√≥n, n√∫mero de parte 1 üòØ ‚è±Ô∏è üßúüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Si lee la capacitaci√≥n sobre codificadores autom√°ticos en el sitio web keras.io, entonces uno de los primeros mensajes es algo as√≠: en la pr√°ctica, lo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Los fundamentos del aprendizaje profundo en el ejemplo de autoencoder de depuraci√≥n, n√∫mero de parte 1</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/484016/"><p>  Si lee la capacitaci√≥n sobre codificadores autom√°ticos en el sitio web keras.io, entonces uno de los primeros mensajes es algo as√≠: en la pr√°ctica, los codificadores autom√°ticos casi nunca se usan, pero a menudo se habla de ellos en capacitaciones y la gente viene, as√≠ que decidimos escribir nuestro propio tutorial sobre ellos: </p><br><p>  <em>Su principal reclamo a la fama proviene de aparecer en muchas clases introductorias de aprendizaje autom√°tico disponibles en l√≠nea.</em>  <em>Como resultado, muchos reci√©n llegados al campo adoran los autoencoders y no pueden tener suficiente de ellos.</em>  <em>¬°Esta es la raz√≥n por la que existe este tutorial!</em> </p><br><p>  Sin embargo, una de las tareas pr√°cticas para las que se pueden aplicar a uno mismo es la b√∫squeda de anomal√≠as, y personalmente la necesitaba en el marco del proyecto nocturno. </p><br><p>  En Internet, hay muchos tutoriales sobre codificadores autom√°ticos, ¬øpara qu√© escribir uno m√°s?  Bueno, para ser honesto, hubo varias razones para esto: </p><br><ul><li>  Hab√≠a una sensaci√≥n de que, de hecho, los tutoriales eran aproximadamente 3 o 4, todo lo dem√°s se reescribi√≥ en sus propias palabras; </li><li>  Casi todo: en el MNIST'e sufriente con im√°genes de 28x28; </li><li>  En mi humilde opini√≥n, no desarrollan una intuici√≥n sobre c√≥mo deber√≠a funcionar todo esto, sino que simplemente ofrecen repetir; </li><li>  Y el factor m√°s importante, personalmente, cuando reemplac√© MNIST con <strong>mi propio conjunto de datos, todo dej√≥ de funcionar est√∫pidamente</strong> . </li></ul><br><p>  A continuaci√≥n se describe mi camino en el que se rellenan los conos.  Si toma cualquiera de los modelos planos (no convolucionales) propuestos de la masa de tutoriales y lo pega est√∫pidamente, entonces, nada, sorprendentemente, no funciona.  El prop√≥sito del art√≠culo es entender por qu√© y, me parece, obtener alg√∫n tipo de comprensi√≥n intuitiva de c√≥mo funciona todo esto. </p><br><p>  No soy un especialista en aprendizaje autom√°tico y uso los enfoques a los que estoy acostumbrado en el trabajo diario.  Para los cient√≠ficos de datos con experiencia, probablemente todo este art√≠culo ser√° descabellado, pero para los principiantes, me parece que puede surgir algo nuevo. </p><a name="habracut"></a><br><div class="spoiler">  <b class="spoiler_title">que tipo de proyecto</b> <div class="spoiler_text"><p>  En pocas palabras sobre el proyecto, aunque el art√≠culo no es sobre √©l.  Hay un receptor ADS-B, captura datos de aviones que vuelan y los escribe, aviones, coordenadas en la base.  A veces, los aviones se comportan de una manera inusual: circulan para quemar combustible antes de aterrizar, o simplemente los vuelos privados pasan por rutas est√°ndar (corredores).  Es interesante aislar de aproximadamente un millar de aviones por d√≠a aquellos que no se comportaron como el resto.  Admito plenamente que las desviaciones b√°sicas se pueden calcular m√°s f√°cilmente, pero estaba interesado en probar <del>  la magia </del>  redes neuronales </p></div></div><br><p>  Empecemos  Tengo un conjunto de datos de 4000 im√°genes en blanco y negro de 64x64 p√≠xeles, se parece a esto: </p><br><p><img src="https://habrastorage.org/webt/vw/5r/mh/vw5rmhetruoniuc7p4ksjulshde.png"></p><br><p>  Solo algunas l√≠neas sobre un fondo negro, y en la imagen de 64x64 se rellena aproximadamente el 2% de los puntos.  Si miras muchas fotos, entonces, por supuesto, resulta que la mayor√≠a de las l√≠neas son bastante similares. </p><br><p>  No entrar√© en detalles sobre c√≥mo se carg√≥ y proces√≥ el conjunto de datos, porque el prop√≥sito del art√≠culo, de nuevo, no es este.  Solo muestra un fragmento de c√≥digo aterrador. </p><br><div class="spoiler">  <b class="spoiler_title">C√≥digo</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># only for google colab %tensorflow_version 2.x import tensorflow as tf import numpy as np import matplotlib.pyplot as plt import os import zipfile import datetime import tensorflow_addons as tfa BATCH_SIZE = 128 AUTOTUNE=tf.data.experimental.AUTOTUNE def load_image(fpath): img_raw = tf.io.read_file(fpath) img = tf.io.decode_png(img_raw, channels=1, dtype=tf.uint8) return tf.image.convert_image_dtype(img, dtype=tf.float32) ## for splitting test/train def is_test(x, y): return x % 4 == 0 def is_train(x, y): return not is_test(x,y) ## for image augmentation def random_flip_flop(img): return tf.image.random_flip_left_right(img) def transform_aug(shift_val): def random_transform(img): return tfa.image.translate(img,tf.random.uniform([2], -1*shift_val, shift_val)) return random_transform def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000, transform=0, flip=False): if cache: if isinstance(cache, str): ds = ds.cache(cache) else: ds = ds.cache() ds = ds.shuffle(buffer_size=shuffle_buffer_size) if transform != 0: ds = ds.map(transform_aug(transform)) if flip: ds = ds.map(random_flip_flop) ds = ds.repeat() ds = ds.batch(BATCH_SIZE) ds = ds.prefetch(buffer_size=AUTOTUNE) return ds def prepare_input_output(x): return (x, x) list_ds = tf.data.Dataset.list_files("/content/planes64/*") imgs_df = list_ds.map(load_image) train = imgs_df.enumerate().filter(is_train).map(lambda x,y: y) train_ds = prepare_for_training(train, transform=10, flip=True) train_ds = train_ds.map(prepare_input_output) val = imgs_df.enumerate().filter(is_test).map(lambda x, y: y) val_ds = val.map(prepare_input_output).batch(BATCH_SIZE, drop_remainder=True)</span></span></code> </pre> </div></div><br><p>  Aqu√≠, por ejemplo, est√° el primer modelo propuesto con keras.io, en el que trabajaron y se entrenaron en mnist: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># this is the size of our encoded representations encoding_dim = 32 # 32 floats -&gt; compression of factor 24.5, assuming the input is 784 floats # this is our input placeholder input_img = Input(shape=(784,)) # "encoded" is the encoded representation of the input encoded = Dense(encoding_dim, activation='relu')(input_img) # "decoded" is the lossy reconstruction of the input decoded = Dense(784, activation='sigmoid')(encoded)</span></span></code> </pre> <br><p>  En mi caso, el modelo se define as√≠: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>/<span class="hljs-number"><span class="hljs-number">10</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">"sigmoid"</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Hay ligeras diferencias entre el hecho de aplanar y remodelar directamente en el modelo, y el de "comprimir" no 25 veces, sino solo 10. Esto no deber√≠a afectar nada. </p><br><p>  Como una funci√≥n de p√©rdida: error cuadr√°tico medio, el optimizador no es fundamental, por cierto, Adam.  En adelante, entrenamos 20 eras, 100 pasos por era. </p><br><p>  Si nos fijamos en las m√©tricas, todo est√° en llamas.  Precisi√≥n == 0.993.  Si nos fijamos en los horarios de entrenamiento, todo es un poco m√°s triste, alcanzamos una meseta en la regi√≥n de la tercera era. </p><br><p><img src="https://habrastorage.org/webt/fo/rq/r7/forqr7krelrj1xq1qis_jg2jeoq.png"></p><br><p>  Bueno, si miras directamente el resultado del codificador, obtienes una imagen generalmente triste (el original est√° en la parte superior y el resultado de la codificaci√≥n-decodificaci√≥n est√° debajo): </p><br><p><img src="https://habrastorage.org/webt/xo/mn/oq/xomnoqmjj80uaal0echobmal9xm.png"></p><br><p>  En general, cuando intenta averiguar por qu√© algo no funciona, es un enfoque lo suficientemente bueno como para dividir toda la funcionalidad en bloques grandes y verificar cada uno de ellos de forma aislada.  Entonces hag√°moslo. </p><br><p>  En el original del tutorial: se suministran datos planos a la entrada del modelo y se toman en la salida.  ¬øPor qu√© no ver mis acciones sobre aplanar y remodelar?  Aqu√≠ hay un modelo sin operaci√≥n: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Resultado: <br><img src="https://habrastorage.org/webt/vx/z0/aj/vxz0ajadu2qiktdq5ndzj5xje8w.png"></p><br><p>  No hay nada que ense√±ar aqu√≠.  Bueno, al mismo tiempo, demostr√≥ que mi funci√≥n de visualizaci√≥n tambi√©n funciona. </p><br><p>  A continuaci√≥n, intente hacer que el modelo no sea no operativo, sino lo m√°s tonto posible: simplemente corte la capa de compresi√≥n, deje una capa del tama√±o de la entrada.  Como dicen en todos los tutoriales, dicen, es muy importante que su modelo aprenda caracter√≠sticas, y no solo una funci√≥n de identidad.  Bueno, eso es exactamente lo que intentaremos obtener, simplemente pasemos la imagen resultante a la salida. </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">"sigmoid"</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Ella est√° aprendiendo algo, precisi√≥n == 0.995 y nuevamente tropieza en una meseta. <br><img src="https://habrastorage.org/webt/ro/t6/jk/rot6jkf2ertweb7weeh8d_layui.png"></p><br><p>  Pero, en general, est√° claro que no funciona muy bien.  De todos modos, qu√© aprender all√≠, pasar la entrada a la salida y eso es todo. </p><br><p>  Si lee la documentaci√≥n de Keras sobre capas densas, describe lo que hacen: <code>output = activation(dot(input, kernel) + bias)</code> <br>  Para que la salida coincida con la entrada, dos cosas simples son suficientes: sesgo = 0 y n√∫cleo: la matriz de identidad (es importante no dejar la matriz llena de unidades aqu√≠; estas son cosas muy diferentes).  Afortunadamente, esto y aquello se pueden hacer f√°cilmente desde la documentaci√≥n para el mismo <code>Dense</code> . </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation = <span class="hljs-string"><span class="hljs-string">"sigmoid"</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, kernel_initializer = tf.keras.initializers.Identity())) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Porque  establecemos el peso de inmediato, entonces no puedes aprender nada, de inmediato es bueno: <br><img src="https://habrastorage.org/webt/dm/tk/5a/dmtk5ardo9xksg8c5v5febzikdo.png"></p><br><p>  Pero si comienzas a entrenar, entonces comienza, a primera vista, sorprendentemente: el modelo comienza con una precisi√≥n == 1.0, pero cae r√°pidamente. <br>  Eval√∫e el resultado antes del entrenamiento: <code>8/Unknown - 1s 140ms/step - loss: 0.2488 - accuracy: 1.0000[0.24875330179929733, 1.0]</code> .  Entrenamiento: </p><br><pre> <code class="plaintext hljs">Epoch 1/20 100/100 [==============================] - 6s 56ms/step - loss: 0.1589 - accuracy: 0.9990 - val_loss: 0.0944 - val_accuracy: 0.9967 Epoch 2/20 100/100 [==============================] - 5s 51ms/step - loss: 0.0836 - accuracy: 0.9964 - val_loss: 0.0624 - val_accuracy: 0.9958 Epoch 3/20 100/100 [==============================] - 5s 50ms/step - loss: 0.0633 - accuracy: 0.9961 - val_loss: 0.0470 - val_accuracy: 0.9958 Epoch 4/20 100/100 [==============================] - 5s 48ms/step - loss: 0.0520 - accuracy: 0.9961 - val_loss: 0.0423 - val_accuracy: 0.9961 Epoch 5/20 100/100 [==============================] - 5s 48ms/step - loss: 0.0457 - accuracy: 0.9962 - val_loss: 0.0357 - val_accuracy: 0.9962</code> </pre> <br><p>  S√≠, y no est√° muy claro, ya tenemos un modelo ideal: la imagen sale 1 en 1 y la p√©rdida (error cuadr√°tico medio) muestra casi 0,25. </p><br><p>  Esto, por cierto, es una pregunta frecuente en los foros: la p√©rdida est√° disminuyendo, pero la precisi√≥n no est√° creciendo, ¬øc√≥mo puede ser? <br>  Aqu√≠ vale la pena recordar una vez m√°s la definici√≥n de la capa Densa: <code>output = activation(dot(input, kernel) + bias)</code> y la palabra activaci√≥n mencionada en ella, que ignor√© con √©xito anteriormente.  Con pesos de la matriz de identidad y sin sesgo, obtenemos <code>output = activation(input)</code> . </p><br><p>  En realidad, la funci√≥n de activaci√≥n en nuestro c√≥digo fuente ya est√° indicada, sigmoide, la copi√© bastante est√∫pidamente y eso es todo.  Y en los tutoriales se recomienda usarlo en todas partes.  Pero tienes que resolverlo. </p><br><p>  Para empezar, puede leer en la documentaci√≥n lo que escriben al respecto: <code>The sigmoid activation: (1.0 / (1.0 + exp(-x)))</code> .  Eso personalmente no me dice nada, porque no soy fantasma una vez para construir tales gr√°ficos en mi cabeza. <br>  Pero puedes construir con bol√≠grafos: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.ticker <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plticker range_tensor = tf.range(<span class="hljs-number"><span class="hljs-number">-4</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, dtype=tf.float32) fig, ax = plt.subplots(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(range_tensor.numpy(), tf.keras.activations.sigmoid(range_tensor).numpy()) ax.grid(which=<span class="hljs-string"><span class="hljs-string">'major'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">'-'</span></span>, linewidth=<span class="hljs-string"><span class="hljs-string">'0.5'</span></span>, color=<span class="hljs-string"><span class="hljs-string">'red'</span></span>) ax.grid(which=<span class="hljs-string"><span class="hljs-string">'minor'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">':'</span></span>, linewidth=<span class="hljs-string"><span class="hljs-string">'0.5'</span></span>, color=<span class="hljs-string"><span class="hljs-string">'black'</span></span>) ax.yaxis.set_major_locator(plticker.MultipleLocator(base=<span class="hljs-number"><span class="hljs-number">0.5</span></span>) ) plt.minorticks_on()</code> </pre> <br><p><img src="https://habrastorage.org/webt/tp/bs/1b/tpbs1bjym9vqahdihjhm1aubrne.png"></p><br><p>  Y aqu√≠ queda claro que en cero el sigmoide toma el valor 0.5 y en la unidad, alrededor de 0.73.  Y los puntos que tenemos son negros (0.0) o blancos (1.0).  Por lo tanto, resulta que el error cuadr√°tico medio de la funci√≥n de identidad no es cero. </p><br><p>  Incluso puedes mirar los bol√≠grafos, aqu√≠ hay una l√≠nea de la imagen resultante: </p><br><pre> <code class="python hljs">array([<span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.7310586</span></span>, <span class="hljs-number"><span class="hljs-number">0.7310586</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> ], dtype=float32)</code> </pre> <br><p>  Y eso es todo, de hecho, muy bueno, porque aparecen varias preguntas a la vez: </p><br><ul><li>  ¬øPor qu√© esto no era visible en la visualizaci√≥n anterior? </li><li>  ¬øPor qu√© entonces precisi√≥n == 1.0, porque las im√°genes originales son 0 y 1. </li></ul><br><p>  Con la visualizaci√≥n, todo es sorprendentemente simple.  Para mostrar las im√°genes, utilic√© matplotlib: <code>plt.imshow(res_imgs[i][:, :, 0])</code> .  Y, como de costumbre, si va a la documentaci√≥n, todo se escribir√° all√≠: <code>The Normalize instance used to scale scalar data to the [0, 1] range before mapping to colors using cmap. By default, a linear scaling mapping the lowest value to 0 and the highest to 1 is used.</code> <code>The Normalize instance used to scale scalar data to the [0, 1] range before mapping to colors using cmap. By default, a linear scaling mapping the lowest value to 0 and the highest to 1 is used.</code>  Es decir  la biblioteca normaliz√≥ cuidadosamente mi 0.5 y 0.73 en el rango de 0 a 1. Cambie el c√≥digo: </p><br><pre> <code class="python hljs">plt.imshow(res_imgs[i][:, :, <span class="hljs-number"><span class="hljs-number">0</span></span>], norm=matplotlib.colors.Normalize(<span class="hljs-number"><span class="hljs-number">0.0</span></span>, <span class="hljs-number"><span class="hljs-number">1.0</span></span>))</code> </pre> <br><p><img src="https://habrastorage.org/webt/rn/qn/ta/rnqntag1dohhikail8y6myq7siy.png"></p><br><p>  Y aqu√≠ est√° la pregunta con precisi√≥n.  Para empezar, por costumbre, vamos a la documentaci√≥n, leemos <code>tf.keras.metrics.Accuracy</code> y all√≠ parece que escriben comprensible: </p><br><pre> <code class="plaintext hljs">For example, if y_true is [1, 2, 3, 4] and y_pred is [0, 2, 3, 4] then the accuracy is 3/4 or .75.</code> </pre> <br><p>  Pero en este caso, nuestra precisi√≥n deber√≠a haber sido 0. Yo, como resultado, me enterr√© en la fuente y es bastante claro para m√≠: </p><br><pre> <code class="plaintext hljs"> When you pass the strings 'accuracy' or 'acc', we convert this to one of `tf.keras.metrics.BinaryAccuracy`, `tf.keras.metrics.CategoricalAccuracy`, `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well.</code> </pre> <br><p>  Adem√°s, en la documentaci√≥n del sitio, por alguna raz√≥n, este p√°rrafo no se encuentra en la descripci√≥n de <code>.compile</code> . </p><br><p>  Aqu√≠ hay un fragmento de c√≥digo de <a href="https://github.com/tensorflow/tensorflow/blob/66c48046f169f3565d12e5fea263f6d731f9bfd2/tensorflow/python/keras/engine/compile_utils.py">https://github.com/tensorflow/tensorflow/blob/66c48046f169f3565d12e5fea263f6d731f9bfd2/tensorflow/python/keras/engine/compile_utils.py</a> </p><br><pre> <code class="python hljs">y_t_rank = len(y_t.shape.as_list()) y_p_rank = len(y_p.shape.as_list()) y_t_last_dim = y_t.shape.as_list()[<span class="hljs-number"><span class="hljs-number">-1</span></span>] y_p_last_dim = y_p.shape.as_list()[<span class="hljs-number"><span class="hljs-number">-1</span></span>] is_binary = y_p_last_dim == <span class="hljs-number"><span class="hljs-number">1</span></span> is_sparse_categorical = ( y_t_rank &lt; y_p_rank <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> y_t_last_dim == <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> y_p_last_dim &gt; <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> metric <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> [<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>, <span class="hljs-string"><span class="hljs-string">'acc'</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> is_binary: metric_obj = metrics_mod.binary_accuracy <span class="hljs-keyword"><span class="hljs-keyword">elif</span></span> is_sparse_categorical: metric_obj = metrics_mod.sparse_categorical_accuracy <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: metric_obj = metrics_mod.categorical_accuracy</code> </pre> <br><p>  <code>y_t</code> es y_true, o la salida esperada, <code>y_p</code> es y_predicted, o el resultado predicho. <br>  Tenemos el formato de datos: <code>shape=(64,64,1)</code> , por lo que resulta que la precisi√≥n se considera como binary_accuracy.  Inter√©s por el bien de c√≥mo se considera: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">binary_accuracy</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(y_true, y_pred, threshold=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.5</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> threshold = math_ops.cast(threshold, y_pred.dtype) y_pred = math_ops.cast(y_pred &gt; threshold, y_pred.dtype) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> K.mean(math_ops.equal(y_true, y_pred), axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>)</code> </pre> <br><p>  Es curioso que aqu√≠ tengamos suerte: por defecto, todo se considera una unidad que es m√°s de 0.5, y 0.5 y menos - cero.  Entonces, la precisi√≥n es cien por ciento para nuestro modelo de identidad, aunque en realidad los n√∫meros no son iguales.  Bueno, est√° claro que si realmente queremos, entonces podemos corregir el umbral y reducir la precisi√≥n a cero, por ejemplo, solo que no es realmente necesario.  Esta es una m√©trica, no afecta la capacitaci√≥n, solo necesita comprender que puede calcularla de mil maneras diferentes y obtener indicadores completamente diferentes.  Solo como ejemplo, puede extraer varias m√©tricas con bol√≠grafos y transferirles nuestros datos: </p><br><pre> <code class="python hljs">m = tf.keras.metrics.BinaryAccuracy() m.update_state(x_batch, res_imgs) print(m.result().numpy())</code> </pre> <br><p>  Nos dar√° <code>1.0</code> . </p><br><p>  Y aqui </p><br><pre> <code class="python hljs">m = tf.keras.metrics.Accuracy() m.update_state(x_batch, res_imgs) print(m.result().numpy())</code> </pre> <br><p>  Nos dar√° <code>0.0</code> en los mismos datos. </p><br><p>  Por cierto, se puede usar el mismo c√≥digo para jugar con funciones de p√©rdida y comprender c√≥mo funcionan.  Si lees los tutoriales en codificadores autom√°ticos, entonces b√°sicamente sugieren usar una de las dos funciones de p√©rdida: error cuadr√°tico medio o 'binary_crossentropy'.  Tambi√©n puedes mirarlos al mismo tiempo. </p><br><p>  Les recuerdo que por mi parte ya di modelos de <code>evaluate</code> : </p><br><pre> <code class="plaintext hljs">8/Unknown - 2s 221ms/step - loss: 0.2488 - accuracy: 1.0000[0.24876083992421627, 1.0]</code> </pre> <br><p>  Es decir  p√©rdida == 0.2488.  Veamos por qu√© es esto.  Personalmente, me parece que es el m√°s simple y m√°s comprensible: la diferencia entre y_true y y_predict se resta p√≠xel por p√≠xel, cada resultado se eleva al cuadrado y luego se busca el promedio. </p><br><pre> <code class="python hljs">tf.keras.backend.mean(tf.math.squared_difference(x_batch[<span class="hljs-number"><span class="hljs-number">0</span></span>], res_imgs[<span class="hljs-number"><span class="hljs-number">0</span></span>]))</code> </pre> <br><p>  Y a la salida: </p><br><pre> <code class="plaintext hljs">&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.24826494&gt;</code> </pre> <br><p>  Aqu√≠ la intuici√≥n es muy simple: la mayor√≠a de los p√≠xeles vac√≠os, el modelo produce 0.5, obtienen 0.25 - diferencia al cuadrado para ellos. </p><br><p>  Con la crossenttrtopy binaria, las cosas son un poco m√°s complicadas, y hay art√≠culos completos sobre c√≥mo funciona esto, pero personalmente siempre fue m√°s f√°cil para m√≠ leer las fuentes, y ah√≠ parece algo as√≠: </p><br><pre> <code class="python hljs"> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> from_logits: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> isinstance(output, (ops.EagerTensor, variables_module.Variable)): output = _backtrack_identity(output) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> output.op.type == <span class="hljs-string"><span class="hljs-string">'Sigmoid'</span></span>: <span class="hljs-comment"><span class="hljs-comment"># When sigmoid activation function is used for output operation, we # use logits from the sigmoid function directly to compute loss in order # to prevent collapsing zero when training. assert len(output.op.inputs) == 1 output = output.op.inputs[0] return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output) # Compute cross entropy from probabilities. bce = target * math_ops.log(output + epsilon()) bce += (1 - target) * math_ops.log(1 - output + epsilon()) return -bce</span></span></code> </pre> <br><p>  Para ser honesto, me dediqu√© el cerebro a estas pocas l√≠neas de c√≥digo durante mucho tiempo.  En primer lugar, queda claro de inmediato que dos implementaciones pueden funcionar: se <code>sigmoid_cross_entropy_with_logits</code> a <code>sigmoid_cross_entropy_with_logits</code> o funcionar√° el √∫ltimo par de l√≠neas.  La diferencia es que <code>sigmoid_cross_entropy_with_logits</code> funciona con logits (como su nombre lo indica, doh), y el c√≥digo principal funciona con probabilidades. </p><br><p>  ¬øQui√©nes son los logits?  Si lees un mill√≥n de art√≠culos diferentes sobre el tema, entonces mencionar√°n definiciones matem√°ticas, f√≥rmulas, algo m√°s.  En la pr√°ctica, todo parece sorprendentemente simple (corr√≠geme si me equivoco).  El resultado bruto de la predicci√≥n es logits.  Bueno, o log-odds, las probabilidades logar√≠tmicas que se miden en <strong>log</strong> istic un <strong>its</strong> - logistic loros. </p><br><div class="spoiler">  <b class="spoiler_title">Hay una peque√±a digresi√≥n: ¬øpor qu√© hay logaritmos?</b> <div class="spoiler_text"><p>  Las probabilidades son la relaci√≥n entre la cantidad de eventos que necesitamos y la cantidad de eventos que no necesitamos (en contraste con la probabilidad, que es la relaci√≥n entre los eventos que necesitamos y la cantidad de todos los eventos en general).  Por ejemplo, el n√∫mero de victorias de nuestro equipo con el n√∫mero de derrotas.  Y hay un problema.  Continuando con el ejemplo con las victorias de los equipos, nuestro equipo puede ser medio perdedor y tener la posibilidad de ganar 1/2 (uno a dos), y tal vez extremadamente perdedor, y tener la posibilidad de ganar 1/100.  Y en la direcci√≥n opuesta: media empinada y 2/1, m√°s empinada que las monta√±as m√°s altas, y luego 100/1.  Y resulta que toda la gama de equipos perdedores se describe con n√∫meros del 0 al 1 y equipos geniales, del 1 al infinito.  Como resultado, es inconveniente comparar, no hay simetr√≠a, trabajar con esto en general es inconveniente para todos, las matem√°ticas salen feas.  Y si tomas el logaritmo de las probabilidades, entonces todo se vuelve sim√©trico: </p><br><pre> <code class="plaintext hljs">ln(1/2) == -0.69 ln(2/1) == 0.69 ln(1/100) == -4.6 ln(100/1) == 4.6</code> </pre> </div></div><br><p>  En el caso de tensorflow, esto es bastante arbitrario, porque, estrictamente hablando, la salida de la capa no es matem√°ticamente log-odds, pero se acepta.  Si el valor bruto es de -‚àû a + ‚àû, entonces logits.  Entonces se pueden convertir en probabilidades.  Hay dos opciones para esto: softmax y su caso especial, sigmoide.  Softmax: tome un vector de logits y convi√©rtalos en un vector de probabilidades, y aun as√≠ la suma de la probabilidad de todos los eventos en √©l resulta ser 1. Sigmoid (en el caso de tf) tambi√©n toma un vector de logits, pero convierte cada uno de ellos en probabilidades por separado, independientemente del resto </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># 1+ln(0.5) == 0.30685281944 tf.math.softmax(tf.constant([0.30685281944, 1.0, 0.30685281944])) ## &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.25, 0.5 , 0.25], dtype=float32)&gt; tf.math.sigmoid(tf.constant([0.30685281944, 1.0, 0.30685281944])) ## &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.57611686, 0.7310586 , 0.57611686], dtype=float32)&gt;</span></span></code> </pre> <br><p>  Puedes verlo de esta manera.  Hay tareas de clasificaci√≥n de etiquetas m√∫ltiples, hay tareas de clasificaci√≥n de clases m√∫ltiples.  Multiclase: esto es si necesita determinar las manzanas en la imagen o las naranjas, y tal vez incluso las pi√±as.  Y multilabel es cuando puede haber un jarr√≥n de frutas en la imagen y hay que decir que tiene manzanas y naranjas, pero no hay pi√±as.  Si queremos multiclase, necesitamos softmax, si queremos multilabel, necesitamos sigmoide. <br>  Aqu√≠ tenemos el caso de multilabel: es necesario que cada p√≠xel (clase) individual indique si est√° instalado. </p><br><p>  Volviendo al flujo de tensor y por qu√© en la crossentrop√≠a binaria (al menos en otras funciones de crossentrop√≠a es casi lo mismo) hay dos ramas globales.  La crossentrop√≠a siempre funciona con probabilidades, hablaremos de esto un poco m√°s tarde.  Luego, hay simplemente dos formas: las probabilidades ya ingresan a la entrada o los logits llegan a la entrada, y luego se les aplica sigmoide para obtener la probabilidad.  Dio la casualidad de que aplicar sigmoide y calcular la entrop√≠a cruzada result√≥ ser mejor que simplemente calcular la entrop√≠a cruzada a partir de las probabilidades (la fuente de la funci√≥n <code>sigmoid_cross_entropy_with_logits</code> tiene una conclusi√≥n matem√°tica, m√°s para los curiosos que pueden buscar en google 'entrop√≠a cruzada de estabilidad num√©rica'), por lo tanto, incluso los desarrolladores de tensorflow recomiendan no pasar la probabilidad a ingrese funciones de crossentrop√≠a y devuelva logits sin procesar.  Bueno, justo en el c√≥digo, las funciones de p√©rdida se verifican si la √∫ltima capa es sigmoidea, luego la cortar√°n y tomar√°n la entrada de activaci√≥n, en lugar de su salida, para calcular, enviando todo para ser considerado en <code>sigmoid_cross_entropy_with_logits</code> . </p><br><p>  Bien, lo solucion√©, ahora binary_crossentropy.  Hay dos explicaciones "intuitivas" populares que miden la entrop√≠a cruzada. </p><br><p>  M√°s formal: imagine que hay un cierto modelo que para n clases conoce la probabilidad de su ocurrencia (y <sub>0</sub> , y <sub>1</sub> , ..., y <sub>n</sub> ).  Y ahora en la vida, cada una de estas clases ha surgido k <sub>n</sub> veces (k <sub>1</sub> , k <sub>1</sub> , ..., k <sub>n</sub> ).  La probabilidad de tal evento es el producto de la probabilidad para cada clase individual - (y <sub>1</sub> ^ k <sub>1</sub> ) (y <sub>2</sub> ^ k <sub>2</sub> ) ... (y <sub>n</sub> ^ k <sub>n</sub> ).  En principio, esta es una definici√≥n normal de entrop√≠a cruzada, la probabilidad de un conjunto de datos se expresa en t√©rminos de la probabilidad de otro conjunto de datos.  El problema con esta definici√≥n es que resultar√° ser de 0 a 1 y, a menudo, ser√° muy peque√±o; no es conveniente comparar tales valores. <br>  Si tomamos el logaritmo de esto, entonces k <sub>1</sub> log (y <sub>1</sub> ) + k <sub>2</sub> log (y <sub>2</sub> ) saldr√° y as√≠ sucesivamente.  El rango de valores pasa de -‚àû a 0. Multiplica todo esto por -1 / n - y el rango de 0 a + ‚àû sale, adem√°s, porque  se expresa como la suma de los valores para cada clase, el cambio en cada clase se refleja en el valor general de una manera muy predecible. </p><br><p>  M√°s simple: la entrop√≠a cruzada muestra cu√°ntos bits adicionales se necesitan para expresar la muestra en t√©rminos del modelo original.  Si estuvi√©ramos all√≠ para hacer un logaritmo con base 2, entonces ir√≠amos directamente a bits.  Utilizamos logaritmos naturales en todas partes, por lo que muestran el n√∫mero de nat ( <a href="https://en.wikipedia.org/wiki/Nat_(unit">https://en.wikipedia.org/wiki/Nat_(unit</a> )), no bits. </p><br><p>  La entrop√≠a cruzada binaria, a su vez, es un caso especial de entrop√≠a cruzada ordinaria, cuando el n√∫mero de clases es dos.  Entonces tenemos suficiente conocimiento de la probabilidad de ocurrencia de una clase - y <sub>1</sub> , y la probabilidad de ocurrencia de la segunda ser√° (1-y <sub>1</sub> ). </p><br><p>  Pero, me parece, me resbal√≥ un poco.  Perm√≠tame recordarle que la √∫ltima vez que intentamos construir un codificador autom√°tico de identidad, nos mostr√≥ una imagen hermosa e incluso una precisi√≥n de 1.0, pero en realidad los n√∫meros resultaron ser horribles.  Por el bien del experimento, puede realizar un par de pruebas m√°s: <br>  1) la activaci√≥n se puede eliminar por completo, habr√° una identidad limpia <br>  2) puede probar otras funciones de activaci√≥n, por ejemplo, el mismo relu </p><br><p>  Sin activaci√≥n: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, kernel_initializer=tf.keras.initializers.Identity())) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Obtenemos el modelo de identidad perfecto: </p><br><pre> <code class="python hljs">model.evaluate(x=val.map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: (x,x)).batch(BATCH_SIZE, drop_remainder=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) <span class="hljs-comment"><span class="hljs-comment"># 8/Unknown - 1s 173ms/step - loss: 0.0000e+00 - accuracy: 1.0000[0.0, 1.0]</span></span></code> </pre> <br><p>  El entrenamiento, por cierto, no conducir√° a nada, porque la p√©rdida == 0.0. </p><br><p>  Ahora con relu.  Su gr√°fico se ve as√≠: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.ticker <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plticker range_tensor = tf.range(<span class="hljs-number"><span class="hljs-number">-4</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, dtype=tf.float32) fig, ax = plt.subplots(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(range_tensor.numpy(), tf.keras.activations.relu(range_tensor).numpy()) ax.grid(which=<span class="hljs-string"><span class="hljs-string">'major'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">'-'</span></span>, linewidth=<span class="hljs-string"><span class="hljs-string">'0.5'</span></span>, color=<span class="hljs-string"><span class="hljs-string">'red'</span></span>) ax.grid(which=<span class="hljs-string"><span class="hljs-string">'minor'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">':'</span></span>, linewidth=<span class="hljs-string"><span class="hljs-string">'0.5'</span></span>, color=<span class="hljs-string"><span class="hljs-string">'black'</span></span>) ax.yaxis.set_major_locator(plticker.MultipleLocator(base=<span class="hljs-number"><span class="hljs-number">1</span></span>) ) plt.minorticks_on()</code> </pre> <br><p><img src="https://habrastorage.org/webt/wq/ph/iw/wqphiwwmhtmxwfogld4nstyfp9w.png"></p><br><p>  Por debajo de cero - cero, por encima - y = x, es decir  en teor√≠a, deber√≠amos obtener el mismo efecto que en ausencia de activaci√≥n, un modelo ideal. </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, kernel_initializer=tf.keras.initializers.Identity())) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.compile(optimizer=<span class="hljs-string"><span class="hljs-string">"adam"</span></span>, loss=<span class="hljs-string"><span class="hljs-string">"binary_crossentropy"</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">"accuracy"</span></span>]) model.evaluate(x=val.map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: (x,x)).batch(BATCH_SIZE, drop_remainder=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) <span class="hljs-comment"><span class="hljs-comment"># 8/Unknown - 1s 158ms/step - loss: 0.0000e+00 - accuracy: 1.0000[0.0, 1.0]</span></span></code> </pre> <br><p>  De acuerdo, descubrimos el modelo de identidad, incluso con alguna parte de la teor√≠a se hizo m√°s claro.  Ahora intentemos entrenar el mismo modelo para que se convierta en identidad. </p><br><p>  Por diversi√≥n, realizar√© este experimento con tres funciones de activaci√≥n.  Para empezar, relu, porque se mostr√≥ mucho antes (todo es como antes, pero el kernel_initializer se elimina, por lo que por defecto ser√° <code>glorot_uniform</code> ): </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Aprende maravillosamente: </p><br><p><img src="https://habrastorage.org/webt/hv/wz/oo/hvwzoodkp6dxopq7pjkopeeuorw.png"></p><br><p>  El resultado fue bastante bueno, precisi√≥n: 0.9999, p√©rdida (ms): 2e-04 despu√©s de 20 eras y puede entrenar m√°s. </p><br><p><img src="https://habrastorage.org/webt/4y/io/jt/4yiojttsafqnf796ha6all0qmsy.png"></p><br><p>  A continuaci√≥n, intente con sigmoide: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Ya ense√±√© algo similar antes, con la √∫nica diferencia de que el sesgo est√° deshabilitado aqu√≠.  √âl estudia humildemente, va a una meseta en la regi√≥n de la era 50, precisi√≥n: 0.9970, p√©rdida: 0.01 despu√©s de 60 eras. </p><br><p>  El resultado nuevamente no es impresionante: </p><br><p><img src="https://habrastorage.org/webt/_7/fl/o_/_7flo_xh5gkgh8oennqthe2ymhk.png"></p><br><p>  Bueno, tambi√©n verifique tanh: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'tanh'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  El resultado es comparable a relu - precisi√≥n: 0.9999, p√©rdida: 6e-04 despu√©s de 20 eras, y puede entrenar m√°s: </p><br><p><img src="https://habrastorage.org/webt/m4/ib/xc/m4ibxctlge5cxs7eqozjt5uwfsc.png"></p><br><p><img src="https://habrastorage.org/webt/sk/r3/p8/skr3p8etvlatcf-mnc6q9sabtfk.png"></p><br><p>  De hecho, me atormenta la cuesti√≥n de si se puede hacer algo para que el sigmoide muestre un resultado comparable.  Exclusivamente por inter√©s deportivo. </p><br><p>  Por ejemplo, puede intentar agregar BatchNormalization: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)) model.add(tf.keras.layers.BatchNormalization()) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Y luego ocurre alg√∫n tipo de magia.  En la decimotercera era, precisi√≥n: 1.0.  Y los resultados ardientes: </p><br><p><img src="https://habrastorage.org/webt/6x/md/di/6xmddijppdnstc8ire1mxlcwkca.png"></p><br><p>  III ... en este colgador de acantilados terminar√© la primera parte, porque el texto es demasiado aburrido, y no est√° claro si alguien lo necesita o no.  En la segunda parte, entender√© qu√© sucedi√≥ la magia, experimentar√© con diferentes optimizadores, tratar√© de construir un codificador-decodificador honesto, golpear√© mi cabeza sobre la mesa.  Espero que alguien haya estado interesado y servicial. </p></div></div><p>Source: <a href="https://habr.com/ru/post/484016/">https://habr.com/ru/post/484016/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../484004/index.html">@Pythonetc Diciembre 2019</a></li>
<li><a href="../484006/index.html">Consejos y trucos de mi canal de Telegram @pythonetc, diciembre de 2019</a></li>
<li><a href="../484008/index.html">¬øQu√© es ser un l√≠der de equipo?</a></li>
<li><a href="../484012/index.html">Agilice el proceso de escritura en un bloc de notas</a></li>
<li><a href="../484014/index.html">10 mitos SEO para dejar atr√°s en 2020</a></li>
<li><a href="../484018/index.html">Lado t√©cnico de la navegaci√≥n.</a></li>
<li><a href="../484020/index.html">¬øA qui√©n est√°s tratando de impresionar con tus plazos?</a></li>
<li><a href="../484026/index.html">Parte 6: Portar MemTest86 + a RISC-V</a></li>
<li><a href="../484028/index.html">Horseshoe Bend - tableta convertible con pantalla plegable</a></li>
<li><a href="../484034/index.html">Implementaci√≥n del esquema de trabajo del almacenamiento dirigido de bienes basado en la unidad de contabilidad de almac√©n 1C Integrated Automation 2</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>