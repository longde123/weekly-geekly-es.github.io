<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🕵🏻 👐🏻 👩🏼‍🤝‍👨🏿 Es lebe der Overclocker: Wie die Flüssigkeitskühlung in Rechenzentren zu dominieren begann 🌇 🥄 🧒🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="„Hochgeschwindigkeitscomputer können nicht ohne Luft auskommen“ 

 Es gibt einen Moment im Film Iron Man 2, in dem Tony Stark einen alten Film seines ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Es lebe der Overclocker: Wie die Flüssigkeitskühlung in Rechenzentren zu dominieren begann</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/442576/"><h3>  „Hochgeschwindigkeitscomputer können nicht ohne Luft auskommen“ </h3><br><iframe width="560" height="315" src="https://www.youtube.com/embed/reQs3g5LO8E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Es gibt einen Moment im Film Iron Man 2, in dem Tony Stark einen alten Film seines verstorbenen Vaters sieht, in dem er sagt: „Ich bin durch die Technologie meiner Zeit eingeschränkt, aber eines Tages kann man es herausfinden.  Und dann wirst du die Welt verändern. "  Es ist fantastisch, aber die Idee, die es ausdrückt, ist ziemlich real.  Die Ideen der Ingenieure sind ihrer Zeit oft weit voraus.  Es gab schon immer Gadgets auf Star Trek, aber der Rest der Welt musste jahrzehntelang daran arbeiten, Tablets und E-Books zu erstellen. <br><br>  Das Flüssigkeitskühlungskonzept passt perfekt in diese Kategorie.  Die Idee selbst existiert seit den 1960er Jahren, blieb aber radikal im Vergleich zu einer viel billigeren und sichereren Option für die Luftkühlung.  Es dauerte mehr als 40 Jahre, bis sich die Flüssigkeitskühlung in den 2000er Jahren ein wenig entwickelte, und selbst dann war es hauptsächlich das Vorrecht von PC-Enthusiasten, ihre CPUs weit über die von Intel und AMD empfohlenen Grenzen hinaus zu verteilen. <br><a name="habracut"></a><br>  Flüssigkeitskühlsysteme werden heute immer beliebter.  Ein solches System für PCs kann für weniger als 100 US-Dollar gekauft werden, und die Handwerksproduktion für industrielle Anwendungen und Rechenzentren (wie CoolIT, Asetek, Green Revolution Computing, Ebullient) bietet Flüssigkeitskühlung () für Server.  ZhO werden hauptsächlich in Supercomputern, Hochgeschwindigkeits-Computing oder anderen Situationen verwendet, in denen eine große Menge an Computerleistung erforderlich ist und Prozessoren mit fast 100% Last arbeiten. Solche Optionen werden jedoch immer häufiger. <br><br>  Es gibt zwei beliebte Arten von ZhO: direkte Chipkühlung und Eintauchen.  Bei direkter Kühlung wird der Kühler wie ein Standardkühler an die CPU angeschlossen, stattdessen werden zwei Rohre an die CPU angeschlossen.  Einer kommt mit kaltem Wasser, einem Kühlkörper, der CPU-Wärme absorbiert, und der andere bleibt heiß.  Dann kühlt es ab und kehrt in einem geschlossenen Kreislauf, der dem Blutfluss ähnelt, zur CPU zurück. <br><br>  Während des Eintauchkühlens ist das Gerät mit Flüssigkeit gefüllt, die offensichtlich keinen Strom leiten sollte.  Dieser Ansatz ist den Kühlbecken von Kernreaktoren am ähnlichsten.  Die Tauchkühlung bleibt eine weiter fortgeschrittene Option und erfordert teurere Kühlmittel als eine direkte Verbindung, bei der Sie normales Wasser verwenden können.  Darüber hinaus besteht immer die Gefahr von Leckagen.  Daher ist die bei weitem beliebteste Option die direkte Verbindung. <br><br>  Nehmen Sie als eines der Hauptbeispiele Alphabet.  Als diese Muttergesellschaft von Google im Mai 2018 Prozessoren für AI TensorFlow 3.0 einführte, sagte Direktor Sundar Pichai, dass diese Chips so leistungsstark sind, dass "wir zum ersten Mal Flüssigkeitskühlung in Rechenzentren installieren mussten".  Alphabet musste diesen Preis für eine achtfache Produktivitätssteigerung zahlen. <br><br>  Auf der anderen Seite kündigten Skybox Datacenters kürzlich Pläne zur Schaffung eines riesigen Supercomputers mit 40.000 Servern von DownUnder GeoSolutions (DUG) an, der für die Exploration von Öl und Gas ausgelegt ist.  Dieses Projekt wird 250 Petaflops Rechenleistung produzieren, mehr als jedes andere, und es wird erwartet, dass die Server durch Flüssigkeit gekühlt werden, wenn sie in mit dielektrischer Flüssigkeit gefüllte Tanks getaucht werden. <br><br>  In jedem Fall ist „Flüssigkeitskühlung die Kühlung der Zukunft und wird es auch immer sein“, sagte Craig Pennington, Vice President of Design, Betreiber eines Rechenzentrums bei Equinix.  "Es scheint offensichtlich, dass dies der richtige Ansatz ist, aber niemand hat ihn angewendet." <br><br>  Wie hat sich JO von der esoterischen Kunst zu einer fast allgemein akzeptierten Methode in modernen Rechenzentren entwickelt?  Wie bei allen Technologien geschah dies teilweise aufgrund von Evolution, Versuch und Irrtum und einer Vielzahl von technischen Lösungen.  Für ZhO sollten sich die heutigen Rechenzentren jedoch bei den frühen Overclockern bedanken, die unbesungene Helden dieser Methode sind. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2c7/137/4b2/2c71374b2b2d2158d46d7e0ddb0fdabc.png"><br>  <i>Das Bedienfeld des IBM System 360-Datenverarbeitungssystems</i> <br><br><h2>  Was verstehen wir unter Flüssigkeitskühlung? </h2><br>  Die Flüssigkeitskühlung wurde 1964 zu einer beliebten Idee, als IBM das Problem der Unterkühlung für den System 360-Mainframe untersuchte. Es war einer der Mainframes des Unternehmens.  Die Serien 700 und 7000 existierten mehr als zehn Jahre, und System / 360 „begann die Ära der Computerkompatibilität - zum ersten Mal, dass verschiedene Maschinen aus der Produktlinie zusammenarbeiten konnten“, wie sie in IBM schreiben.  Das Konzept war einfach: Gekühltes Wasser musste durch ein Gerät fließen, das es auf eine Temperatur unter Raumtemperatur abkühlt, und dann wurde das Wasser direkt in das System eingespeist.  Die von IBM verwendete Schaltung wird jetzt als Rückkühlung bezeichnet, wenn der Kühlkörper hinter dem Mainframe montiert ist.  Das Gerät saugte mit Lüftern heiße Luft aus dem Mainframe an, und dann wurde diese Luft durch Wasser gekühlt, ähnlich wie ein Kühler den Motor eines Autos kühlt. <br><br>  Seitdem haben Ingenieure dieses Grundkonzept perfektioniert, und es sind zwei dominante Formen von VF entstanden: Eintauchen und direkter Kontakt.  Eintauchen ist was es ist;  Die Elektronik befindet sich in einem Flüssigkeitsbad, das aus offensichtlichen Gründen kein Wasser sein kann.  Die Flüssigkeit darf keinen Strom leiten, dh kein Isolator sein (Unternehmen wie 3M entwickeln dafür sogar speziell Flüssigkeiten). <br><br>  Tauchen hat jedoch viele Probleme und Nachteile.  Der in der Flüssigkeit befindliche Server ist nur von oben erreichbar.  Daher sollten sich dort externe Ports befinden.  Die Serverplatzierung von 1U-Gehäusen in einem Rack wäre unpraktisch, sodass der Server nicht nacheinander platziert werden kann.  Ein Dielektrikum, und normalerweise ein Mineral, ist klein, sehr teuer und im Falle einer Leckage schwer zu reinigen.  Spezielle Festplatten sind erforderlich, und die Änderung des Rechenzentrums erfordert erhebliche Investitionen.  Daher wird, wie im Fall des oben erwähnten Supercomputers, das Eintauchen am besten in ein neues Rechenzentrum durchgeführt, anstatt das alte zu wiederholen. <br><br>  Im Gegensatz dazu besteht der direkte Kontakt von JO darin, dass sich der Kühler (oder Wärmetauscher) wie ein normaler Kühler auf dem Chip befindet.  Anstelle eines Ventilators werden zwei Wasserleitungen verwendet - eine bringt kaltes Wasser zum Kühlen und die zweite bläst heißes Wasser, das durch Kontakt mit dem Kühler erwärmt wird.  Diese Form von ZhO wurde die beliebteste, sie wurde von Herstellern wie HP Enterprise, Dell EMC und IBM sowie von Herstellern von Gehäusen Chatsworth Systems und Schneider Electric übernommen. <br><br>  Bei der direkten Kühlung wird Wasser verwendet, das jedoch sehr empfindlich auf seine Qualität reagiert.  Ungefiltertes Leitungswasser darf nicht verwendet werden.  Schauen Sie sich einfach Ihren Wasserhahn oder Duschkopf an.  Niemand braucht Kalziumansammlung in Servern.  Zumindest die direkte Kühlung erfordert reines destilliertes Wasser und manchmal seine Mischung mit Frostschutzmittel.  Die Herstellung eines solchen Kühlmittels ist eine Wissenschaft für sich. <br><br><h2>  Intels Verbindung </h2><br>  Wie sind wir von IBM-Heizkörpern zu modernen, extravaganten Kühlsystemen übergegangen?  Nochmals vielen Dank an Overclocker.  Um die Jahrhundertwende wurde die Flüssigkeitskühlung bei PC-Overclockern und Amateuren, die ihre Computer zusammenbauten und ihre Geschwindigkeit über die offiziellen Grenzen hinaus steigern wollten, immer beliebter.  Es war jedoch eine esoterische Kunst ohne Standarddesigns.  Jeder hat etwas anderes gemacht.  Die Person, die all dies zusammenbaute, musste so ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Geber sein,</a> dass die Montage von IKEA-Produkten völliger Unsinn schien.  Die meisten Kühlsysteme passten nicht einmal in die Gehäuse. <br><br>  Anfang 2004 begann sich die Situation aufgrund interner Änderungen der Intel-Richtlinien zu ändern.  Ein Ingenieur aus dem Designzentrum in Hillsboro, Oregon, in dem die meisten Chips des Unternehmens entworfen werden, obwohl er seinen Hauptsitz in Santa Clara, Kalifornien, hat, arbeitet seit mehreren Jahren an einem speziellen Kühlprojekt.  Das Projekt kostete das Unternehmen 1 Million US-Dollar und zielte darauf ab, einen Flüssigkeitskühler für Intel-Prozessoren zu entwickeln.  Leider wollte Intel es gerade herunterfahren. <br><br>  Der Ingenieur hoffte auf ein anderes Ergebnis.  Um das Projekt zu retten, kam er auf diese Idee bei Falcon Northwest, einem in Portland ansässigen Unternehmen, das Gaming-Add-Ons für Computer entwickelte.  "Der Grund war, dass das Unternehmen glaubte, dass die Flüssigkeitskühlung die Menschen zum Übertakten ermutigte, und diese Aktivität war zu diesem Zeitpunkt verboten", sagte Kelt Reeves, Präsident von Falcon Northwest.  Und in dieser Position hatte Intel seine eigene Logik.  Zu dieser Zeit verkauften prinzipienlose Einzelhändler aus Asien übertaktete PCs unter dem Deckmantel leistungsfähigerer und mit schlechter Kühlung, und in den Augen der Öffentlichkeit wurde dies irgendwie zu einem Intel-Problem.  Daher lehnte das Unternehmen das Übertakten ab. <br><br>  Dieser Ingenieur aus Oregon glaubte jedoch, dass Intel nachgeben wird, wenn es ihm gelingt, Kunden und einen Markt für einen solchen Kühler zu finden.  (Darüber hinaus war das resultierende Intel-Produkt qualitativ viel besser als das, was auf dem Markt erhältlich war, sagte Reeves.)  Nach einigen internen Überredungen und Verhandlungen zwischen den Unternehmen erlaubte Intel Falcon daher, Kühlsysteme zu verkaufen - insbesondere, weil Intel sie bereits zu Tausenden produzierte.  Der einzige Haken war, dass Falcon nicht erwähnen konnte, dass Intel beteiligt war.  Falcon stimmte zu und war bald der erste Hersteller, der vollständig versiegelte All-in-One-PC-Systeme lieferte. <br><br>  Reeves stellte fest, dass diese hochmoderne ZhO-Lösung nicht besonders benutzerfreundlich war.  Falcon musste das Gehäuse an den Kühler anpassen und eine Kühlplatte für Wasser erfinden.  Im Laufe der Zeit lernten Kühlerhersteller wie ThermalTake und Corsair, was Intel tat, und begannen, konsequente Verbesserungen vorzunehmen.  Seitdem sind mehrere Produkte und Hersteller erschienen, zum Beispiel CoolIT und Asetek, die ZhO speziell für Rechenzentren hergestellt haben.  Einige ihrer Produkte - zum Beispiel Rohre, die nicht brechen, nicht reißen und nicht mit einer Garantie von bis zu sieben Jahren auslaufen - wurden letztendlich unter Lizenz an Hersteller von Kühlsystemen für den Endverbraucher vergeben, und ein solcher Technologieaustausch in beide Richtungen ist zur Norm geworden. <br><br>  Und da dieser Markt in verschiedene Richtungen wächst, hat sogar Intel seine Meinung geändert.  Jetzt wirbt sie für Übertaktungsfunktionen für die Prozessoren der K- und X-Serie und kümmert sich nicht einmal darum, normale Kühler zusammen mit der Top-CPU für Gamer zu verkaufen. <br><br>  "ZhO ist bereits eine bewährte Technologie - jeder tut dies auf der Verbraucherseite", sagte Reeves.  Intel hat aufgehört, reguläre Kühler mit den leistungsstärksten CPUs zu beliefern, da diese JO benötigen.  Es wurde bereits bewiesen und ein Segen von Intel wurde erhalten.  Ich glaube nicht, dass es jemanden gibt, der sagt, dass Komplettlösungen dafür nicht zuverlässig genug sind. " <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ad3/8c7/8a8/ad38c78a8953abdcdb563f84b7fadae7.jpg"><br>  <i>Tauchkühlung im Rechenzentrum.</i>  <i>Die Kästen sind mit dielektrischer Flüssigkeit gefüllt, die durch die Rohre fließt.</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/221/cab/4fd/221cab4fd3522b8580d2e27d394f51d9.jpg"><br>  <i>Flüssigkeitskühlung aus Skybox-Rechenzentren mit Eintauchen.</i>  <i>Wärmetauscher sind in Computergeräte eingetaucht, und dielektrische Flüssigkeit verlässt den Tank nicht.</i>  <i>Ein Wasserkreislauf führt durch die Räume und nähert sich jedem Wärmetauscher.</i> <br><br><h2>  Fakten zugunsten der Praktikabilität der Flüssigkeitskühlung </h2><br>  Traditionelle Rechenzentren sorgten lange Zeit für einen Doppelboden mit kleinen Öffnungen, durch die die kalte Luft stieg und von den Servern angesaugt wurde.  Dies wurde als CRAC oder Computerraumklimaanlage bezeichnet.  Das Problem ist, dass es jetzt nicht mehr ausreicht, kalte Luft durch Öffnungen im Boden zu blasen. <br><br>  Der Hauptgrund für den jüngsten Boom bei der Flüssigkeitskühlung ist die Notwendigkeit.  Die heutigen Prozessoren werden zu heiß und die Server sind zu nahe, als dass die Luft sie effizient kühlen könnte, sagt Google.  Die Wärmekapazität von Wasser ist 3300-mal höher als die von Luft, und ein Wasserkühlsystem kann 300 Liter Wasser pro Minute pumpen, verglichen mit 20 Kubikmetern Luft pro Minute. <br><br>  Einfach ausgedrückt kann Wasser viel effizienter und auf viel kleinerem Raum abkühlen.  Nach vielen Jahren des Versuchs, den Stromverbrauch zu senken, können Prozessorhersteller daher die Leistung streuen und die Spannung verdrehen, um maximale Leistung zu erzielen - in dem Wissen, dass die Flüssigkeitskühlung damit umgehen kann. <br><br>  "Wir werden gebeten, Chips zu kühlen, deren Stromverbrauch bald über 500 Watt liegen wird", sagte Jeff Lyon, Direktor von CoolIT.  - Einige Prozessoren, die noch nicht auf den Markt gekommen sind, verbrauchen jeweils 300 Watt.  All dies entwickelt sich auf Wunsch der KI und des maschinellen Lernens.  Die Abkühlrate reicht einfach nicht aus. “ <br><br>  Laut Lyon erwägt CoolIT, das Kühlsystem auf Chipsätze, Leistungssteuerungssysteme, Netzwerkchips und Speicher auszudehnen.  "Es wird nichts Radikales im Umgang mit dem Gedächtnis geben", fügte er hinzu.  - Es gibt RAM-Optionen mit fortschrittlicher Verpackung, die 18 Watt pro DIMM verbrauchen.  Ein typisches DIMM verbraucht 4-6 Watt.  Unter Systemen mit viel Speicher finden wir Server mit 16 oder sogar 24 installierten DIMMs, was viel Wärme bedeutet. “ <br><br>  Nacheinander stehen die Hersteller vor solchen Anfragen.  Equinix beobachtet, wie die durchschnittliche Dichte von 5 kW auf 7-8 kW und jetzt auf 15-16 kW ansteigt, wobei einige Geräte bereits eine Dichte von 40 kW aufweisen.  „Die Gesamtmenge an Luft, die gepumpt werden muss, wird also zu groß.  Es wird nicht sofort geschehen, aber in den nächsten Jahren wird die Flüssigkeitskühlung grundlegend eingeführt “, sagte Pennington von Equinix. <br><br><h2>  Ein bisschen über Immersionskühlung </h2><br>  Green Revolution Cooling konzentriert sich auf Immersionskühlung, und sein Direktor Peter Poulin sagt, dass Immersionskühlung aus zwei Gründen aus Sicht der Energieeffizienz besser ist als direkte Kühlung.  Zunächst werden die Lüfter von allen Servern entfernt.  Nur so wird der Energieverbrauch um durchschnittlich 15% gesenkt.  Und ein Kunde des Unternehmens reduzierte es um 30%. <br><br>  Die Beseitigung von Fans hat noch einen weiteren indirekten Vorteil: die Stille.  Trotz der Tatsache, dass die Server häufig sehr kleine Lüfter verwenden, sind die Server-Lüfter furchtbar laut, und es ist aufgrund von Hitze und Lärm unangenehm, sich im Rechenzentrum zu befinden.  Die Flüssigkeitskühlung macht das Arbeiten an diesen Orten viel angenehmer. <br><br>  Ein weiterer Vorteil ist, dass nur sehr wenig Energie benötigt wird, um das Tauchkühlsystem zu unterstützen.  Es gibt nur drei bewegliche Teile: eine Pumpe zum Umwälzen eines Kühlers, eine Pumpe zum Bewegen zu einem Kühlturm und einen Kühlturmlüfter.  Nach dem Austausch flüssigkeitsgekühlter Luft kann der Stromverbrauch auf 5% der Ausgaben für die Klimatisierung sinken.  "Sie erhalten eine enorme Reduzierung des Energieverbrauchs, wodurch Sie viele andere Dinge tun können", sagte Poulnin.  "Je nach Verbraucher kann das Rechenzentrum energieeffizienter sein oder die mit dem Bau von Rechenzentren verbundenen CO2-Emissionen reduzieren." <br><br><h2>  Fakten für eine energieeffiziente Flüssigkeitskühlung </h2><br>  Der Energieverbrauch ist seit langem ein Problem für die Rechenzentrumsbranche (die US-Umweltschutzbehörde verfolgt diese Zahl seit mindestens zehn Jahren).  Die heutigen Rechenzentren sind riesige Unternehmen, die schätzungsweise 2% des gesamten weltweiten Stroms verbrauchen und so viel CO <sub>2</sub> in die Atmosphäre abgeben wie die Luftfahrtindustrie.  Daher lässt das Interesse an diesem Thema nicht nach.  Glücklicherweise reduziert die Flüssigkeitskühlung die Stromkosten. <br><br>  Die ersten Einsparungen sind auf die Unterbrechung der Klimaanlage im Rechenzentrum zurückzuführen.  Das zweite ist die Beseitigung der Fans.  Jedes Server-Rack hat viele Lüfter, die Luft blasen, aber ihre Anzahl kann je nach Dichte auf eine kleine Anzahl oder auf Null reduziert werden. <br><br>  Und mit der „Trockenkühlung“ -Technologie, bei der es nicht zu Gefrieren kommt, können Sie noch größere Einsparungen erzielen.  Die direkt angeschlossene Kühlung führte das Wasser zunächst durch einen Kühlschrank, der es auf 15 bis 25 Grad Celsius abkühlte.  Am Ende stellte sich jedoch heraus, dass Flüssigkeitskühler, die Wasser durch eine lange Folge von Rohren und Ventilatoren leiteten, mit heißem Wasser beheizte Rohre und natürliche Wärmediffusion das Wasser auch auf eine ausreichende Temperatur abkühlten. <br><br>  "Weil dieser Prozess so effizient ist, müssen Sie sich keine Sorgen machen, das Wasser auf eine niedrige Temperatur abzukühlen", sagt Pennington.  - Warmes Wasser entzieht den Servern immer noch effektiv die gesamte Wärme.  Du brauchst keinen Kompressionszyklus, du kannst nur trockene Kühler verwenden. " <br><br>  Trockene Kühler sparen auch Wasser.  Ein großes Rechenzentrum, das Kühlschränke verwendet, kann jährlich Millionen Liter Wasser verbrauchen, ein Rechenzentrum mit Trockenkühlern verbraucht jedoch kein Wasser.  Dies spart sowohl Energie als auch Wasser, was sehr nützlich sein kann, wenn sich das Rechenzentrum in der Stadt befindet. <br><br>  "Wir verbrauchen nicht viel Wasser", sagte Pennington.  - Wenn Sie alles sorgfältig entwerfen, erhalten Sie ein geschlossenes System.  Wasser fließt nicht ein und nicht aus, Sie müssen nur etwa einmal im Jahr Wasser hinzufügen, um die Tanks voll zu halten.  Du fügst deinem Auto nicht ständig Wasser hinzu, das ist bei uns der Fall. " <br><br><h2>  Akzeptanz folgt Wirksamkeit </h2><br>  Ein Beispiel aus der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Praxis</a> : Laut Brian Payne, Vice President für Produktmanagement und Marketing bei PowerEdge Dell EMC, hat Dell durch die Umstellung auf Flüssigkeitskühlung die Energieeffizienz ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PUE</a> ) um 56% gesteigert.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PUE ist das Verhältnis der Energie, die für die Kühlung des Systems aufgewendet werden muss, zu der Energie, die für den Betrieb des Systems benötigt wird. Tatsächlich ist dies das Verhältnis der Gesamtenergie, die das Rechenzentrum verbraucht, zu der Energie, die direkt für die Stromversorgung der IT-Infrastruktur aufgewendet wird. perev]. Ein PUE von 3 bedeutet, dass zweimal mehr Energie für die Kühlung eines Systems aufgewendet wird als für die Systemleistung, und PUE = 2 bedeutet, dass sowohl Energie als auch Kühlung gleichermaßen verbraucht werden. PUE kann nicht gleich 1 sein, da eine Kühlung erforderlich ist, aber die Betreiber von Rechenzentren sind davon besessen, die Zahl so nahe wie möglich an 1,0 heranzuführen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zusätzlich zur Verbesserung des PUE kann die Rechenleistung, die Dell-Kunden erhalten, bis zu 23% betragen, was das System nicht über alle Maßen überlastet. „Basierend auf den erforderlichen Infrastrukturinvestitionen prognostizieren wir die jährliche Rendite des Systems“, sagt Payne. - Ich würde dies mit dem Kauf einer energieeffizienteren Klimaanlage für zu Hause vergleichen. Sie investieren ein wenig, spüren aber im Laufe der Zeit die Vorteile von Stromrechnungen. “</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nehmen Sie als völlig anderer Anhänger der Flüssigkeitskühlung das Supercomputer-Zentrum in Ohio, OSC. Dieser Cluster verwendet 1800 Knoten. Nach dem Wechsel zu JO erreichte das Zentrum, wie Doug Johnson, Chef-Systemarchitekt, sagte, PUE = 1,5. OSC verwendet einen externen Kreislauf, sodass Wasser aus dem Gebäude entfernt und auf Umgebungstemperatur abgekühlt wird, die im Sommer durchschnittlich 30 ° C und im Winter viel weniger beträgt. Die Späne erreichen 70 ° C, und selbst wenn sich das Wasser auf 40 ° C erwärmt, bleibt es viel kälter als die Späne und erfüllt seinen Zweck.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie viele der frühen Anwender der neuen Technologie ist auch für OSC alles neu. Vor fünf Jahren nutzte das Zentrum ZhO überhaupt nicht und belegt heute 25%. Das Zentrum hofft, dass die Messlatte in drei Jahren auf 75% steigen wird und nach einigen Jahren vollständig auf ZhO umgestellt wird. Aber selbst im heutigen Zustand benötigt die Kühlung des Zentrums laut Johnson viermal weniger Energie als vor dem Übergang zu ZhO, und im Allgemeinen reduzierte diese Lösung den Gesamtenergieverbrauch um 2/3. "Ich denke, dass der Prozentsatz steigen wird, wenn wir beginnen, die GPU in das Kühlsystem zu integrieren."</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aus Sicht des Kunden erfordert die Evaluierung einer neuen Technologie Zeit und Energie. Aus diesem Grund hat ein großes Unternehmen wie Dell vereinbart, mit CoolIT zusammenzuarbeiten, um für ZhO zu werben. </font><font style="vertical-align: inherit;">Es ist nicht verwunderlich, dass in erster Linie unter den Anliegen der Kunden die Möglichkeit einer Leckage bleibt. </font><font style="vertical-align: inherit;">Trotz aller Schwankungen stellt sich jedoch heraus, dass sie im Moment keine andere Wahl haben, um die beste Leistung zu erzielen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">„Es gab schon immer Angst vor Undichtigkeiten“, sagt Lyon von CoolIT. </font><font style="vertical-align: inherit;">- Die Situation hat sich geändert, und jetzt gibt es einfach keine anderen Optionen. </font><font style="vertical-align: inherit;">Hochgeschwindigkeitscomputer können genau das nicht. "</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de442576/">https://habr.com/ru/post/de442576/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de442566/index.html">Wie auf dem Mond: Reverse Engineering eines Hybrid-Operationsverstärkermoduls</a></li>
<li><a href="../de442568/index.html">Sicherheitswoche 10: Sicherheitslücken bei NVIDIA-Treibern</a></li>
<li><a href="../de442570/index.html">Sigma Regeln. Handwerk oder neuer Standard für SOC</a></li>
<li><a href="../de442572/index.html">Verwenden des Datapath Config Tools</a></li>
<li><a href="../de442574/index.html">Die Grundlage für eine verallgemeinerte Theorie neuronaler Netze wird geschaffen</a></li>
<li><a href="../de442578/index.html">Linux 5.0 Release</a></li>
<li><a href="../de442580/index.html">Reverse Engineering im Binärformat am Beispiel von Korg .SNG-Dateien</a></li>
<li><a href="../de442582/index.html">Wie wir es mit Mobbing versucht haben</a></li>
<li><a href="../de442584/index.html">Dokumente zum Gebäude: kleine Freuden der Automatisierung am Beispiel des Dunklen Turms</a></li>
<li><a href="../de442586/index.html">Durch die Sicherheitsanfälligkeit im Telegramm kann das Kennwort für den lokalen Code beliebiger Länge umgangen werden</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>