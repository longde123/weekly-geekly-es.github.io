<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🖕 🤦🏼 👈🏽 Bagaimana kami menguji beberapa basis data time series 🤞🏾 🕵🏾 😞</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Selama beberapa tahun terakhir, database Time-series telah berevolusi dari hal yang aneh (sangat terspesialisasi dalam sistem pemantauan terbuka (dan ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bagaimana kami menguji beberapa basis data time series</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/itsumma/blog/462111/"><img src="https://habrastorage.org/webt/u_/ty/r0/u_tyr0djrlkmaz-swo9flhddymo.jpeg"><br><br>  Selama beberapa tahun terakhir, database Time-series telah berevolusi dari hal yang aneh (sangat terspesialisasi dalam sistem pemantauan terbuka (dan terkait dengan solusi spesifik) atau proyek Big Data) menjadi "barang konsumen".  Di wilayah Federasi Rusia, terima kasih khusus harus diberikan kepada Yandex dan ClickHouse untuk ini.  Hingga saat ini, jika Anda perlu menyimpan sejumlah besar data deret waktu, Anda harus menerima kebutuhan untuk meningkatkan tumpukan Hadoop yang mengerikan dan menyertainya, atau berkomunikasi dengan protokol khusus untuk setiap sistem. <br><br>  Tampaknya pada 2019, sebuah artikel tentang TSDB mana yang harus digunakan akan terdiri dari hanya satu kalimat: "gunakan saja ClickHouse".  Tapi ... ada nuansa. <br><br>  Memang, ClickHouse secara aktif berkembang, basis pengguna tumbuh, dan dukungan sangat aktif, tetapi apakah kita telah menjadi sandera bagi kesuksesan publik ClickHouse, yang telah membayangi solusi lain, yang mungkin lebih efektif / dapat diandalkan? <br><br>  Pada awal tahun lalu, kami mulai memproses sistem pemantauan kami sendiri, di mana muncul pertanyaan memilih basis yang sesuai untuk penyimpanan data.  Saya ingin menceritakan tentang sejarah pilihan ini di sini. <br><a name="habracut"></a><br><h4>  Pernyataan masalah </h4><br>  Pertama-tama, kata pengantar yang diperlukan.  Mengapa kita membutuhkan sistem pemantauan kita sendiri dan bagaimana pengaturannya? <br><br>  Kami mulai memberikan layanan dukungan pada 2008, dan pada 2010 menjadi jelas bahwa sulit untuk mengumpulkan data tentang proses yang terjadi di infrastruktur klien dengan solusi yang ada pada saat itu (kami berbicara tentang, Tuhan memaafkan saya, Cacti, Zabbix dan yang baru lahir). Grafit). <br><br>  Persyaratan utama kami adalah: <br><br><ul><li>  mendukung (pada saat itu - lusinan, dan di masa depan - ratusan) pelanggan dalam sistem yang sama dan pada saat yang sama kehadiran sistem manajemen peringatan terpusat; </li><li>  fleksibilitas dalam mengelola sistem peringatan (eskalasi peringatan antara petugas, jadwal akuntansi, basis pengetahuan); </li><li>  kemungkinan perincian grafik yang dalam (Zabbix pada waktu itu menggambar grafik dalam bentuk gambar); </li><li>  penyimpanan jangka panjang dari sejumlah besar data (satu tahun atau lebih) dan kemampuan untuk memilihnya dengan cepat. </li></ul><br>  Pada artikel ini, kami tertarik pada poin terakhir. <br><br>  Berbicara tentang penyimpanan, persyaratannya adalah sebagai berikut: <br><br><ul><li>  sistem harus bekerja dengan cepat; </li><li>  diinginkan bahwa sistem memiliki antarmuka SQL; </li><li> sistem harus stabil dan memiliki basis pengguna aktif dan dukungan (setelah kami dihadapkan dengan kebutuhan untuk mendukung sistem seperti MemcacheDB, yang kami hentikan pengembangannya, atau penyimpanan MooseFS yang didistribusikan, bugtracker yang dilakukan dalam bahasa Cina: mengulangi cerita ini untuk proyek kami kepada kami tidak mau); </li><li>  Korespondensi dengan teorema CAP: Konsistensi (diperlukan) - data harus relevan, kami tidak ingin sistem manajemen notifikasi tidak menerima data baru dan meludahkan peringatan tentang tidak kedatangan data untuk semua proyek;  Toleransi Partisi (perlu) - kami tidak ingin mendapatkan sistem Split Brain;  Ketersediaan (tidak penting, dalam kasus replika aktif) - kita dapat beralih ke sistem cadangan jika terjadi kecelakaan, dengan kode. </li></ul><br>  Anehnya, pada saat itu MySQL adalah solusi sempurna bagi kami.  Struktur data kami sangat sederhana: id server, id penghitung, cap waktu, dan nilai;  pengambilan sampel cepat untuk data panas disediakan oleh kumpulan buffer yang besar, dan pengambilan sampel data historis disediakan oleh SSD. <br><br><img src="https://habrastorage.org/webt/ii/cd/es/iicdesd_tmiqwygfha8r4bepjgg.png"><br><br>  Dengan demikian, kami mencapai sampel data dua minggu baru, dengan merinci hingga 200 ms kedua sebelum data sepenuhnya diberikan, dan tinggal di sistem ini untuk beberapa waktu. <br><br>  Sementara itu, waktu berlalu dan jumlah data bertambah.  Pada 2016, volume data mencapai puluhan terabyte, yang dalam hal penyimpanan SSD sewaan merupakan pengeluaran yang signifikan. <br><br>  Pada titik ini, basis data kolom digunakan secara aktif, yang kami mulai pikirkan secara aktif: di basis data kolom, data disimpan, seperti yang dapat Anda pahami, dalam kolom, dan jika Anda melihat data kami, mudah untuk melihat sejumlah besar pengambilan yang bisa dilakukan. Jika menggunakan basis data kolom, kompres dengan kompresi. <br><br><img src="https://habrastorage.org/webt/zm/gu/x3/zmgux307lo7r3i7s9uykpgozadm.png"><br><br>  Namun, sistem kunci untuk pekerjaan perusahaan terus bekerja secara stabil, dan saya tidak ingin bereksperimen dengan transisi ke hal lain. <br><br>  Pada 2017, di konferensi Persona Live di San Jose, mungkin pertama kalinya pengembang Clickhouse mengumumkan sendiri.  Pada pandangan pertama, sistem ini siap produksi (baik, Yandex.Metrica adalah produksi yang keras), dukungannya cepat dan sederhana, dan yang paling penting, operasinya sederhana.  Sejak 2018, kami telah memulai proses transisi.  Tetapi pada saat itu, ada banyak sistem TSDB "dewasa" dan teruji waktu, dan kami memutuskan untuk mengalokasikan waktu yang cukup dan membandingkan alternatif untuk memastikan bahwa tidak ada solusi alternatif Clickhouse, sesuai dengan persyaratan kami. <br><br>  Selain persyaratan penyimpanan yang sudah ditunjukkan, yang baru muncul: <br><br><ul><li>  sistem baru harus menyediakan setidaknya kinerja yang sama dengan MySQL, pada jumlah zat besi yang sama; </li><li>  penyimpanan sistem baru harus menempati ruang yang jauh lebih sedikit; </li><li>  DBMS harus tetap mudah dikelola; </li><li>  Saya ingin meminimalkan aplikasi ketika mengubah DBMS. </li></ul><br><h4>  Sistem apa yang mulai kami pertimbangkan </h4><br>  <b><u>Apache Hive / Apache Impala</u></b> <br>  Tumpukan Hadoop yang sudah usang.  Bahkan, ini adalah antarmuka SQL yang dibangun di atas menyimpan data dalam format asli pada HDFS. <br><br>  Pro <br><br><ul><li>  Dengan operasi yang stabil, sangat mudah untuk mengukur data. </li><li>  Ada solusi kolom untuk penyimpanan data (lebih sedikit ruang). </li><li>  Eksekusi sangat cepat dari tugas paralel di hadapan sumber daya. </li></ul><br>  Cons <br><br><ul><li>  Ini adalah Hadoop, dan sulit dioperasikan.  Jika kita tidak siap untuk mengambil solusi yang sudah jadi di cloud (dan kita tidak siap untuk biaya), seluruh tumpukan harus dirakit dan didukung oleh admin, tetapi saya benar-benar tidak menginginkan ini. </li><li>  Data dikumpulkan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sangat cepat</a> . </li></ul><br>  Namun: <br><br><img src="https://habrastorage.org/webt/zi/nv/qx/zinvqxvmxc43df-hd4xjqwprtmi.png"><br><br>  Kecepatan dicapai dengan meningkatkan jumlah server komputasi.  Sederhananya, jika kita adalah perusahaan besar yang bergerak di bidang analitik dan bisnis, sangat penting untuk mengumpulkan informasi secepat mungkin (bahkan dengan biaya menggunakan sejumlah besar sumber daya komputasi) - ini mungkin pilihan kita.  Tapi kami belum siap melipatgandakan taman besi untuk mempercepat tugas. <br><br>  <b><u>Druid / pinot</u></b> <br><br>  Sudah jauh lebih banyak tentang TSDB khusus, tetapi sekali lagi - Hadoop-stack. <br><br>  Ada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">artikel bagus membandingkan pro dan kontra Druid dan Pinot dibandingkan dengan ClickHouse</a> . <br><br>  Dalam beberapa kata: Druid / Pinot terlihat lebih baik daripada Clickhouse dalam kasus di mana: <br><br><ul><li>  Anda memiliki sifat data yang heterogen (dalam kasus kami, kami hanya mencatat deret waktu metrik server, dan, pada kenyataannya, ini adalah satu tabel. Tetapi mungkin ada kasus lain: seri waktu peralatan, seri waktu ekonomi, dll. - masing-masing dengan strukturnya sendiri, yang harus dikumpulkan dan diproses). </li><li>  Apalagi ada banyak data ini. </li><li>  Tabel dan data dengan deret waktu muncul dan menghilang (yaitu, beberapa set data masuk, dianalisis dan dihapus). </li><li>  Tidak ada kriteria yang jelas dimana data dapat dipartisi. </li></ul><br>  Dalam kasus yang berlawanan, ClickHouse menunjukkan dirinya lebih baik, dan ini adalah kasus kami. <br><br>  <b><u>Clickhouse</u></b> <br><br><ul><li>  Seperti SQL. </li><li>  Mudah dikelola. </li><li>  Orang bilang itu berhasil. </li></ul><br>  Itu jatuh ke dalam daftar pengujian. <br><br>  <b><u>Influxdb</u></b> <br><br>  Alternatif asing untuk ClickHouse.  Dari minus: Ketersediaan Tinggi hanya hadir dalam versi komersial, tetapi harus dibandingkan. <br><br>  Itu jatuh ke dalam daftar pengujian. <br><br>  <b><u>Cassandra</u></b> <br><br>  Di satu sisi, kita tahu bahwa itu digunakan untuk menyimpan rentang waktu metrik oleh sistem pemantauan seperti, misalnya, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">SignalFX</a> atau OkMeter.  Namun, ada yang spesifik. <br><br>  Cassandra bukan database kolom dalam arti biasa.  Itu terlihat lebih seperti huruf kecil, tetapi setiap baris dapat memiliki jumlah kolom yang berbeda, sehingga mudah untuk mengatur representasi kolom.  Dalam pengertian ini, jelas bahwa dengan batas 2 miliar kolom, Anda dapat menyimpan beberapa data dalam kolom (ya, seri waktu yang sama).  Sebagai contoh, di MySQL ada batasan pada 4096 kolom dan mudah untuk menemukan kesalahan dengan kode 1117 jika Anda mencoba melakukan hal yang sama. <br><br>  Mesin Cassandra berfokus pada penyimpanan sejumlah besar data dalam sistem terdistribusi tanpa penyihir, dan dalam CAP-teorema Cassandra lebih banyak tentang AP, yaitu tentang aksesibilitas data dan ketahanan terhadap partisi.  Dengan demikian, alat ini dapat menjadi sangat bagus jika Anda hanya perlu menulis ke basis data ini dan jarang membacanya.  Dan di sini logis menggunakan Cassandra sebagai penyimpanan "dingin".  Artinya, sebagai tempat jangka panjang yang dapat diandalkan untuk menyimpan sejumlah besar data historis yang jarang diperlukan, tetapi dapat diperoleh jika perlu.  Namun demikian, demi kelengkapan, kami akan mengujinya.  Tetapi, seperti yang saya katakan sebelumnya, tidak ada keinginan untuk secara aktif menulis ulang kode untuk solusi DB yang dipilih, jadi kami akan mengujinya agak terbatas - tanpa mengadaptasi struktur database dengan spesifikasi Cassandra. <br><br>  <b><u>Prometheus</u></b> <br><br>  Yah, dan karena ketertarikan, kami memutuskan untuk menguji kinerja toko Prometheus - hanya untuk memahami apakah kami lebih cepat daripada solusi saat ini atau lebih lambat dan berapa banyak. <br><br><h4>  Metodologi dan Hasil Uji </h4><br>  Jadi, kami menguji 5 database dalam 6 konfigurasi berikut: ClickHouse (1 node), ClickHouse (tabel terdistribusi dari 3 node), InfluxDB, Mysql 8, Cassandra (3 node) dan Prometheus.  Rencana pengujian adalah sebagai berikut: <br><br><ol><li>  isi data historis untuk minggu ini (840 juta nilai per hari; 208 ribu metrik); </li><li>  menghasilkan beban perekaman (6 mode muatan dipertimbangkan, lihat di bawah); </li><li>  bersamaan dengan rekaman, kami membuat sampel secara berkala, meniru permintaan pengguna yang bekerja dengan grafik.  Agar tidak terlalu menyulitkan hal-hal, kami memilih data dengan 10 metrik (sama banyak dari mereka pada grafik CPU) per minggu. </li></ol><br>  Kami memuat dengan meniru perilaku agen pemantauan kami, yang mengirimkan nilai ke setiap metrik setiap 15 detik.  Dalam hal ini, kami tertarik untuk memvariasikan: <br><br><ul><li>  jumlah total metrik ke mana data ditulis; </li><li>  interval pengiriman nilai dalam satu metrik; </li><li>  ukuran batch. </li></ul><br>  Tentang ukuran bets.  Karena hampir semua basis eksperimental kami tidak disarankan untuk dimuat dengan sisipan tunggal, kami akan memerlukan relay, yang mengumpulkan metrik masuk dan mengelompokkannya sebanyak mungkin dan menulisnya ke pangkalan dengan memasukkan paket. <br><br>  Juga, untuk lebih memahami bagaimana menafsirkan data yang diterima nanti, bayangkan bahwa kita tidak hanya mengirim banyak metrik, tetapi metrik tersebut disusun ke dalam server - 125 metrik per server.  Di sini, server hanyalah entitas virtual - hanya untuk memahami bahwa, misalnya, 10.000 metrik sesuai dengan sekitar 80 server. <br><br>  Jadi, dengan mempertimbangkan semua ini, 6 mode beban perekaman pangkalan kami: <br><br><img src="https://habrastorage.org/webt/lr/li/oo/lrlioosoevoybfuw4--rsftdugi.jpeg"><br><br>  Ada dua poin.  Pertama, untuk cassandra ukuran batch seperti itu ternyata terlalu besar, di sana kami menggunakan nilai 50 atau 100. Dan kedua, karena prometeus bekerja secara ketat dalam mode tarik, mis.  dia berjalan dan mengumpulkan data dari sumber metrik (dan bahkan pushgateway, meskipun namanya, tidak secara fundamental mengubah situasi), beban terkait diimplementasikan menggunakan kombinasi konfigurasi statis. <br><br>  Hasil tes adalah sebagai berikut: <br><br><img src="https://habrastorage.org/webt/0r/sz/vc/0rszvcd_aeoiqwlrixsgw0tauti.jpeg"><br><br><img src="https://habrastorage.org/webt/3r/gy/2g/3rgy2guskcodctly7nevknbygss.jpeg"><br><br><img src="https://habrastorage.org/webt/v8/mg/tm/v8mgtm0ytjkjkneal1mjcvw5n8w.jpeg"><br><br>  <b>Yang perlu diperhatikan</b> : sampel cepat luar biasa dari Prometheus, sampel sangat lambat dari Cassandra, sampel lambat yang tidak dapat diterima dari InfluxDB;  ClickHouse menang dalam hal kecepatan perekaman, dan Prometheus tidak berpartisipasi dalam kompetisi, karena ia memasukkan dalam dirinya sendiri dan kami tidak mengukur apa pun. <br><br>  <u><b>Hasilnya</b></u> : ClickHouse dan InfluxDB menunjukkan diri mereka yang terbaik dari semuanya, tetapi sebuah cluster dari Influx hanya dapat dibangun berdasarkan versi Enterprise, yang membutuhkan biaya, dan ClickHouse tidak memerlukan biaya apa pun dan dibuat di Rusia.  Adalah logis bahwa di AS pilihannya mungkin mendukung inInfluxDB, dan dalam kasus kami mendukung ClickHouse. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id462111/">https://habr.com/ru/post/id462111/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id462095/index.html">Bekerja dengan cahaya dan optik: bagaimana memulai karir di universitas - pengalaman lulusan empat program master khusus</a></li>
<li><a href="../id462097/index.html">Tur hijau muda</a></li>
<li><a href="../id462101/index.html">Laporan dari Java Virtual Machine Language Summit 2019</a></li>
<li><a href="../id462107/index.html">Marathon Gratis “Data Besar dan Pahlawan Super: Pengalaman Analisis Data Pertama Anda”</a></li>
<li><a href="../id462109/index.html">Lihat hampir tidak terlihat, juga dalam warna: teknik untuk memvisualisasikan objek melalui diffuser</a></li>
<li><a href="../id462113/index.html">Lingkungan yang tidak tercela: tidak seorang pun boleh menulis kode kualitas</a></li>
<li><a href="../id462115/index.html">Pasang langit berbintang di WebGL dalam 1009 byte JavaScript</a></li>
<li><a href="../id462117/index.html">Bagaimana Memaksimalkan Nilai Penanganan Kesalahan Produk?</a></li>
<li><a href="../id462119/index.html">Delta Smart City Solutions: Pernahkah Anda bertanya-tanya seberapa hijau sebuah bioskop bisa?</a></li>
<li><a href="../id462121/index.html">Cepat Fungsional</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>