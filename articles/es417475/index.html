<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ú¥Ô∏è üòº ‚úåÔ∏è Codificaci√≥n de borrado Glusterfs +: cuando necesita mucho, barato y confiable ‚úäüèº üíÜ ü•Ç</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pocas personas tienen un Glaster en Rusia, y cualquier experiencia es interesante. Lo tenemos grande e industrial y, a juzgar por la discusi√≥n en el √∫...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Codificaci√≥n de borrado Glusterfs +: cuando necesita mucho, barato y confiable</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/croccloudteam/blog/417475/">  Pocas personas tienen un Glaster en Rusia, y cualquier experiencia es interesante.  Lo tenemos grande e industrial y, a juzgar por la discusi√≥n en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">√∫ltimo post</a> , en demanda.  Habl√© sobre el comienzo de la experiencia de migrar copias de seguridad del almacenamiento de Enterprise a Glusterfs. <br><br>  Esto no es lo suficientemente hardcore.  No nos detuvimos y decidimos recoger algo m√°s serio.  Por lo tanto, aqu√≠ hablaremos sobre cosas como la codificaci√≥n de borrado, el fragmentaci√≥n, el reequilibrio y su aceleraci√≥n, las pruebas de estr√©s, etc. <br><br><img src="https://habrastorage.org/webt/8t/ol/2d/8tol2dsnki7fr_jfcvhxwldsxdk.jpeg"><br><br><ul><li>  M√°s teor√≠a de volum / subwolum </li><li>  repuesto en caliente </li><li>  sanar / sanar completo / reequilibrar </li><li>  Conclusiones despu√©s de reiniciar 3 nodos (nunca haga esto) </li><li>  ¬øC√≥mo afecta la grabaci√≥n a diferentes velocidades desde diferentes m√°quinas virtuales y la activaci√≥n / desactivaci√≥n de fragmentos a la carga de subvolumen? </li><li>  reequilibrio despu√©s de la salida del disco </li><li>  reequilibrio r√°pido </li></ul><br><a name="habracut"></a><h3>  Que querias </h3><br>  <b>La tarea es simple:</b> recolectar una tienda barata pero confiable.  Barato como sea posible, confiable, para que no sea aterrador almacenar nuestros propios archivos en venta.  Adios  Luego, despu√©s de largas pruebas y copias de seguridad en otro sistema de almacenamiento, tambi√©n clientes. <br><br>  <b>Aplicaci√≥n (IO secuencial)</b> : <br><br>  - Copias de seguridad <br>  - Infraestructuras de prueba <br>  - Prueba de almacenamiento para archivos multimedia pesados. <br>  Estamos aqui <br>  - Archivo de batalla e infraestructura de prueba seria <br>  - Almacenamiento de datos importantes. <br><br>  Como la √∫ltima vez, el requisito principal es la velocidad de la red entre las instancias de Glaster.  10G al principio est√° bien. <br><br><h3>  Teor√≠a: ¬øqu√© es el volumen disperso? </h3><br>  El volumen disperso se basa en la tecnolog√≠a de codificaci√≥n de borrado (EC), que proporciona una protecci√≥n bastante efectiva contra fallas de disco o servidor.  Es como RAID 5 o 6, pero en realidad no.  Almacena el fragmento codificado del archivo para cada ladrillo de tal manera que solo se requiere un subconjunto de los fragmentos almacenados en los briks restantes para restaurar el archivo.  El administrador configura el n√∫mero de ladrillos que pueden no estar disponibles sin p√©rdida de acceso a los datos durante la creaci√≥n del volumen. <br><br><img src="https://habrastorage.org/webt/rt/br/t1/rtbrt12s-0oyc9avxlsp32qzsus.png"><br><br><h3>  ¬øQu√© es un subvolumen? </h3><br>  La esencia del subvolumen en la terminolog√≠a de GlusterFS se manifiesta junto con vol√∫menes distribuidos.  En la distribuci√≥n distribuida, la codificaci√≥n de borrado funcionar√° solo en el marco del subwoofer.  Y en el caso, por ejemplo, con los datos distribuidos replicados se replicar√°n dentro del marco del subwoofer. <br>  Cada uno de ellos se distribuye en diferentes servidores, lo que les permite perder o salir libremente para sincronizar.  En la figura, los servidores (f√≠sicos) est√°n marcados en verde, los sub-lobos est√°n punteados.  Cada uno de ellos se presenta como un disco (volumen) al servidor de aplicaciones: <br><br><img src="https://habrastorage.org/webt/w-/fp/dj/w-fpdjqguwigusvhtprq_6su3z8.png"><br><br>  Se decidi√≥ que la configuraci√≥n 4 + 2 distribuida y dispersa en 6 nodos parece bastante confiable, podemos perder 2 servidores o 2 discos dentro de cada subwoofer, mientras continuamos teniendo acceso a los datos. <br><br>  Ten√≠amos a nuestra disposici√≥n 6 viejos DELL PowerEdge R510 con 12 ranuras de disco y unidades SATA de 48x2TB 3.5.  En principio, si hay un servidor con 12 ranuras de disco y que tiene hasta 12 TB de unidades en el mercado, podemos recolectar almacenamiento de hasta 576 TB de espacio utilizable.  Pero no olvide que, aunque los tama√±os m√°ximos de HDD contin√∫an creciendo de a√±o en a√±o, su rendimiento se detiene y la reconstrucci√≥n de un disco de 10-12 TB puede llevarle una semana. <br><br><img src="https://habrastorage.org/webt/xo/ud/6f/xoud6fl7sdtknj7vrbn_5fgslpu.png"><br><br>  <b>Creaci√≥n de volumen:</b> <br>  Una descripci√≥n detallada de c√≥mo preparar ladrillos, puede leer en mi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">publicaci√≥n anterior</a> <br><br><pre><code class="bash hljs">gluster volume create freezer disperse-data 4 redundancy 2 transport tcp \ $(<span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> {0..7} ; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> {sl051s,sl052s,sl053s,sl064s,sl075s,sl078s}:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick<span class="hljs-variable"><span class="hljs-variable">$i</span></span>/freezer ; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span>)</code> </pre> <br>  Creamos, pero no tenemos prisa por lanzar y montar, ya que todav√≠a tenemos que aplicar varios par√°metros importantes. <br><br>  <b>Lo que tenemos</b> <br><br><img src="https://habrastorage.org/webt/8j/il/e9/8jile9swj-qws3hgdjo3gtht3oo.png"><br><br>  Todo parece bastante normal, pero hay una advertencia. <br><br>  <b>Consiste en grabar tal volumen en los ladrillos:</b> <br>  Los archivos se colocan uno por uno en los sub-lobos, y no se distribuyen uniformemente entre ellos, por lo tanto, tarde o temprano nos encontraremos con su tama√±o, y no con el tama√±o de todo el volumen.  El tama√±o m√°ximo de archivo que podemos poner en este repositorio es el tama√±o utilizable del subwoofer menos el espacio ya ocupado en √©l.  En mi caso, es &lt;8 Tb. <br><br>  <b>Que hacer</b>  <b>Como ser</b> <br>  Este problema se resuelve mediante el fragmentaci√≥n o el volumen de la banda, pero, como la pr√°ctica ha demostrado, la banda funciona muy mal. <br><br>  Por lo tanto, intentaremos fragmentar. <br><br>  <b>Lo que se est√° fragmentando, en detalle</b> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> . <br><br>  <b>Lo que es fragmentaci√≥n, en resumen</b> : <br>  Cada archivo que coloque en un volumen se dividir√° en partes (fragmentos), que est√°n organizados de manera relativamente uniforme en sub-lobos.  El administrador especifica el tama√±o del fragmento, el valor est√°ndar es de 4 MB. <br><br>  <b>Active el sharding despu√©s de crear un volumen, pero antes de que comience</b> : <br><br><pre> <code class="bash hljs">gluster volume <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> freezer features.shard on</code> </pre> <br>  <b>Establecemos el tama√±o del fragmento (¬øcu√°l es √≥ptimo? Dudes de oVirt recomienda 512 MB)</b> : <br><br><pre> <code class="bash hljs">gluster volume <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> freezer features.shard-block-size 512MB</code> </pre> <br>  Emp√≠ricamente, resulta que el tama√±o real del fragmento en los ladrillos cuando se usa el volumen disperso 4 + 2 es igual al tama√±o del bloque de fragmento / 4, en nuestro caso 512M / 4 = 128M. <br><br>  Cada fragmento de acuerdo con la l√≥gica de codificaci√≥n de borrado se descompone de acuerdo con los ladrillos dentro del marco del submundo con estas piezas: 4 * 128M + 2 * 128M <br><br>  <b>Dibuje los casos de falla que Gluster sobrevive con esta configuraci√≥n:</b> <br>  En esta configuraci√≥n, podemos sobrevivir a la ca√≠da de 2 nodos o 2 de cualquier disco dentro del mismo subvolumen. <br><br>  Para las pruebas, decidimos deslizar el almacenamiento resultante bajo nuestra nube y ejecutar fio desde m√°quinas virtuales. <br><br>  Activamos la grabaci√≥n secuencial de 15 m√°quinas virtuales y hacemos lo siguiente. <br><br>  <b>Reinicio del 1er nodo:</b> <br>  17:09 <br>  Parece no cr√≠tico (~ 5 segundos de indisponibilidad por el par√°metro ping.timeout). <br><br>  17:19 <br>  Lanzado sanar por completo. <br>  El n√∫mero de entradas de curaci√≥n solo est√° creciendo, probablemente debido al alto nivel de escritura en el cl√∫ster. <br><br>  17:32 <br>  Se decidi√≥ desactivar la grabaci√≥n desde la VM. <br>  El n√∫mero de entradas de curaci√≥n comenz√≥ a disminuir. <br><br>  17:50 <br>  sanar hecho. <br><br>  <b>Reiniciar 2 nodos:</b> <br><br>  <i>Se observan los mismos resultados que con el primer nodo.</i> <br><br>  <b>Reiniciar 3 nodos:</b> <br>  <i>Punto de montaje emitido El punto final de transporte no est√° conectado, las m√°quinas virtuales recibieron ioerror.</i> <i><br></i>  <i>Despu√©s de encender los nodos, el Glaster se restableci√≥, sin interferencia de nuestro lado, y comenz√≥ el proceso de tratamiento.</i> <br><br>  Pero 4 de cada 15 m√°quinas virtuales no pudieron aumentar.  Vi errores en el hipervisor: <br><br><pre> <code class="bash hljs">2018.04.27 13:21:32.719 ( volumes.py:0029): I: Attaching volume vol-BA3A1BE1 (/GLU/volumes/33/33e3bc8c-b53e-4230-b9be-b120079c0ce1) with attach <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> generic... 2018.04.27 13:21:32.721 ( qmp.py:0166): D: Querying QEMU: __com.redhat_drive_add({<span class="hljs-string"><span class="hljs-string">'file'</span></span>: u<span class="hljs-string"><span class="hljs-string">'/GLU/volumes/33/33e3bc8c-b53e-4230-b9be-b120079c0ce1'</span></span>, <span class="hljs-string"><span class="hljs-string">'iops_rd'</span></span>: 400, <span class="hljs-string"><span class="hljs-string">'media'</span></span>: <span class="hljs-string"><span class="hljs-string">'disk'</span></span>, <span class="hljs-string"><span class="hljs-string">'format'</span></span>: <span class="hljs-string"><span class="hljs-string">'qcow2'</span></span>, <span class="hljs-string"><span class="hljs-string">'cache'</span></span>: <span class="hljs-string"><span class="hljs-string">'none'</span></span>, <span class="hljs-string"><span class="hljs-string">'detect-zeroes'</span></span>: <span class="hljs-string"><span class="hljs-string">'unmap'</span></span>, <span class="hljs-string"><span class="hljs-string">'id'</span></span>: <span class="hljs-string"><span class="hljs-string">'qdev_1k7EzY85TIWm6-gTBorE3Q'</span></span>, <span class="hljs-string"><span class="hljs-string">'iops_wr'</span></span>: 400, <span class="hljs-string"><span class="hljs-string">'discard'</span></span>: <span class="hljs-string"><span class="hljs-string">'unmap'</span></span>})... 2018.04.27 13:21:32.784 ( instance.py:0298): E: Failed to attach volume vol-BA3A1BE1 to the instance: Device <span class="hljs-string"><span class="hljs-string">'qdev_1k7EzY85TIWm6-gTBorE3Q'</span></span> could not be initialized Traceback (most recent call last): File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/ic/instance.py"</span></span>, line 292, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> emulation_started c2.qemu.volumes.attach(controller.qemu(), device) File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/c2/qemu/volumes.py"</span></span>, line 36, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> attach c2.qemu.query(qemu, drive_meth, drive_args) File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/c2/qemu/_init_.py"</span></span>, line 247, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> query <span class="hljs-built_in"><span class="hljs-built_in">return</span></span> c2.qemu.qmp.query(qemu.pending_messages, qemu.qmp_socket, <span class="hljs-built_in"><span class="hljs-built_in">command</span></span>, args, suppress_logging) File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/c2/qemu/qmp.py"</span></span>, line 194, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> query message[<span class="hljs-string"><span class="hljs-string">"error"</span></span>].get(<span class="hljs-string"><span class="hljs-string">"desc"</span></span>, <span class="hljs-string"><span class="hljs-string">"Unknown error"</span></span>) QmpError: Device <span class="hljs-string"><span class="hljs-string">'qdev_1k7EzY85TIWm6-gTBorE3Q'</span></span> could not be initialized qemu-img: Could not open <span class="hljs-string"><span class="hljs-string">'/GLU/volumes/33/33e3bc8c-b53e-4230-b9be-b120079c0ce1'</span></span>: Could not <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> image <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> determining its format: Input/output error</code> </pre><br>  <b>Pago dif√≠cil 3 nodos con fragmentaci√≥n desactivada</b> <br><br><pre> <code class="bash hljs">Transport endpoint is not connected (107) /GLU/volumes/e0/e0bf9a42-8915-48f7-b509-2f6dd3f17549: ERROR: cannot <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> (Input/output error)</code> </pre> <br>  Tambi√©n perdemos datos, no es posible restaurar. <br><br>  <b>Pague suavemente 3 nodos con fragmentaci√≥n, ¬øhabr√° corrupci√≥n de datos?</b> <br>  Hay, pero mucho menos (¬øcoincidencia?), Perd√≠ 3 de 30 unidades. <br><br>  <b>Conclusiones:</b> <br><br><ol><li>  La curaci√≥n de estos archivos se cuelga sin cesar, el reequilibrio no ayuda.  Llegamos a la conclusi√≥n de que los archivos en los que estaba activando la grabaci√≥n activa cuando el tercer nodo estaba apagado se pierden para siempre. </li><li>  ¬°Nunca recargue m√°s de 2 nodos en una configuraci√≥n 4 + 2 en producci√≥n! </li><li>  ¬øC√≥mo no perder datos si realmente desea reiniciar 3+ nodos?  P Detener la grabaci√≥n en el punto de montaje y / o detener el volumen. </li><li>  Los nodos o ladrillos deben reemplazarse lo antes posible.  Para esto, es altamente deseable tener, por ejemplo, 1-2 ladrillos de repuesto en caliente en cada nodo para un reemplazo r√°pido.  Y un nodo adicional con ladrillos en caso de un volcado de nodo. </li></ol><br><img src="https://habrastorage.org/webt/-m/rj/ti/-mrjtikde2imxdydeka4w-hvjjk.png"><br><br>  Tambi√©n es muy importante probar los casos de reemplazo de unidades <br><br>  <b>Salidas de briks (discos):</b> <b><br></b>  <b>17:20</b> <br>  Golpeamos un ladrillo: <br><br><pre> <code class="bash hljs">/dev/sdh 1.9T 598G 1.3T 33% /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick6</code> </pre> <br>  <b>17:22</b> <br><pre> <code class="bash hljs">gluster volume replace-brick freezer sl051s:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick_spare_1/freezer sl051s:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick2/freezer commit force</code> </pre> <br>  Puede ver dicha reducci√≥n al momento de reemplazar el ladrillo (registro de 1 fuente): <br><br><img src="https://habrastorage.org/webt/jk/96/ns/jk96ns2kzhy0radczfpxlfpodso.png"><br><br>  El proceso de reemplazo es bastante largo, con un peque√±o nivel de grabaci√≥n por cl√∫ster y configuraciones predeterminadas de 1 TB, se tarda aproximadamente un d√≠a en recuperarse. <br><br>  <b>Par√°metros ajustables para el tratamiento:</b> <br><br><pre> <code class="bash hljs">gluster volume <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> cluster.background-self-heal-count 20 <span class="hljs-comment"><span class="hljs-comment"># Default Value: 8 # Description: This specifies the number of per client self-heal jobs that can perform parallel heals in the background. gluster volume set cluster.heal-timeout 500 # Default Value: 600 # Description: time interval for checking the need to self-heal in self-heal-daemon gluster volume set cluster.self-heal-window-size 2 # Default Value: 1 # Description: Maximum number blocks per file for which self-heal process would be applied simultaneously. gluster volume set cluster.data-self-heal-algorithm diff # Default Value: (null) # Description: Select between "full", "diff". The "full" algorithm copies the entire file from source to # sink. The "diff" algorithm copies to sink only those blocks whose checksums don't match with those of # source. If no option is configured the option is chosen dynamically as follows: If the file does not exist # on one of the sinks or empty file exists or if the source file size is about the same as page size the # entire file will be read and written ie "full" algo, otherwise "diff" algo is chosen. gluster volume set cluster.self-heal-readdir-size 2KB # Default Value: 1KB # Description: readdirp size for performing entry self-heal</span></span></code> </pre> <br>  <i>Opci√≥n: disperse.background-heals</i> <i><br></i>  <i>Valor predeterminado: 8</i> <i><br></i>  <i>Descripci√≥n: esta opci√≥n se puede utilizar para controlar el n√∫mero de curaciones paralelas</i> <i><br><br></i>  <i>Opci√≥n: disperse.heal-wait-qlength</i> <i><br></i>  <i>Valor predeterminado: 128</i> <i><br></i>  <i>Descripci√≥n: esta opci√≥n se puede utilizar para controlar el n√∫mero de curaciones que pueden esperar</i> <i><br><br></i>  <i>Opci√≥n: disperse.shd-max-threads</i> <i><br></i>  <i>Valor predeterminado: 1</i> <i><br></i>  <i>Descripci√≥n: N√∫mero m√°ximo de curaciones paralelas que SHD puede hacer por ladrillo local.</i>  <i>Esto puede reducir sustancialmente los tiempos de curaci√≥n, pero tambi√©n puede aplastar sus ladrillos si no tiene el hardware de almacenamiento para soportar esto.</i> <i><br><br></i>  <i>Opci√≥n: disperse.shd-wait-qlength</i> <i><br></i>  <i>Valor predeterminado: 1024</i> <i><br></i>  <i>Descripci√≥n: esta opci√≥n se puede usar para controlar el n√∫mero de curaciones que pueden esperar en SHD por subvolumen</i> <i><br><br></i>  <i>Opci√≥n: disperse.cpu-extensions</i> <i><br></i>  <i>Valor predeterminado: auto</i> <i><br></i>  <i>Descripci√≥n: fuerce las extensiones de la CPU para que se utilicen para acelerar los c√°lculos del campo de galois.</i> <i><br><br></i>  <i>Opci√≥n: disperse.self-heal-window-size</i> <i><br></i>  <i>Valor predeterminado: 1</i> <i><br></i>  <i>Descripci√≥n: N√∫mero m√°ximo de bloques (128 KB) por archivo para el cual el proceso de autocuraci√≥n se aplicar√≠a simult√°neamente.</i> <br><br>  Stood: <br><br><pre> <code class="bash hljs">disperse.shd-max-threads: 6 disperse.self-heal-window-size: 4 cluster.self-heal-readdir-size: 2KB cluster.data-self-heal-algorithm: diff cluster.self-heal-window-size: 2 cluster.heal-timeout: 500 cluster.background-self-heal-count: 20 cluster.disperse-self-heal-daemon: <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> disperse.background-heals: 18</code> </pre> <br>  Con nuevos par√°metros, se complet√≥ 1 TB de datos en 8 horas (¬°3 veces m√°s r√°pido!) <br><br>  <b>El momento desagradable es que el resultado es un brik m√°s grande de lo que era</b> <br><br>  <b>fue:</b> <pre> <code class="bash hljs">Filesystem Size Used Avail Use% Mounted on /dev/sdd 1.9T 645G 1.2T 35% /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick2</code> </pre> <br>  <b>se convirti√≥ en:</b> <pre> <code class="bash hljs">Filesystem Size Used Avail Use% Mounted on /dev/sdj 1.9T 1019G 843G 55% /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/hot_spare_brick_0</code> </pre> <br>  Es necesario entender.  Probablemente la cosa est√° inflando discos delgados.  Con el reemplazo posterior del ladrillo aumentado, el tama√±o se mantuvo igual. <br><br>  <b>Reequilibrio:</b> <br>  <i>Despu√©s de expandir o reducir (sin migrar datos) un volumen (usando los comandos add-brick y remove-brick respectivamente), necesita reequilibrar los datos entre los servidores.</i>  <i>En un volumen no replicado, todos los ladrillos deben estar activos para realizar la operaci√≥n de reemplazo de ladrillos (opci√≥n de inicio).</i>  <i>En un volumen replicado, al menos uno de los ladrillos en la r√©plica debe estar arriba.</i> <br><br>  <b>Reequilibrio de conformaci√≥n:</b> <br><br>  <i>Opci√≥n: cluster.rebal-throttle</i> <i><br></i>  <i>Valor predeterminado: normal</i> <i><br></i>  <i>Descripci√≥n: establece el n√∫mero m√°ximo de migraciones de archivos paralelos permitidas en un nodo durante la operaci√≥n de reequilibrio.</i>  <i>El valor predeterminado es normal y permite un m√°ximo de [($ (unidades de procesamiento) - 4) / 2), 2] archivos a b</i> <i><br></i>  <i>e migramos a la vez.</i>  <i>Lazy permitir√° que solo se migre un archivo a la vez y el agresivo permitir√° un m√°ximo de [($ (unidades de procesamiento) - 4) / 2), 4]</i> <br><br>  <i>Opci√≥n: cluster.lock-migraci√≥n</i> <i><br></i>  <i>Valor predeterminado: apagado</i> <i><br></i>  <i>Descripci√≥n: si est√° habilitada, esta caracter√≠stica migrar√° los bloqueos posix asociados con un archivo durante el reequilibrio</i> <br><br>  <i>Opci√≥n: cluster.weighted-rebalance</i> <i><br></i>  <i>Valor predeterminado: activado</i> <i><br></i>  <i>Descripci√≥n: cuando est√° habilitado, los archivos se asignar√°n a ladrillos con una probabilidad proporcional a su tama√±o.</i>  <i>De lo contrario, todos los ladrillos tendr√°n la misma probabilidad (comportamiento heredado).</i> <br><br>  <b>Comparaci√≥n de escritura y luego lectura de los mismos par√°metros fio (resultados m√°s detallados de las pruebas de rendimiento, en PM):</b> <br><br><pre> <code class="bash hljs">fio --fallocate=keep --ioengine=libaio --direct=1 --buffered=0 --iodepth=1 --bs=64k --name=<span class="hljs-built_in"><span class="hljs-built_in">test</span></span> --rw=write/<span class="hljs-built_in"><span class="hljs-built_in">read</span></span> --filename=/dev/vdb --runtime=6000</code> </pre><br><img src="https://habrastorage.org/webt/fw/up/j0/fwupj0gj9m6vn25bepslaaox01e.png"><br><br><img src="https://habrastorage.org/webt/xi/yf/pd/xiyfpdsecqbfc52fudoz4nwklty.png"><br><br><img src="https://habrastorage.org/webt/nh/xp/es/nhxpestbf-gfjkcwogumbbqabc4.jpeg"><br><br>  <b>Si es interesante, compare la velocidad de rsync con el tr√°fico a los nodos Glaster:</b> <br><br><img src="https://habrastorage.org/webt/6i/cz/ox/6iczoxword1qaauuhkwm3vfkk-q.png"><br><br><img src="https://habrastorage.org/webt/42/ka/eq/42kaeqkdbcuzhq8rwdrqybgkc5u.png"><br><br>  <i>Se puede ver que aproximadamente 170 MB / s / tr√°fico a 110 MB / s / carga √∫til.</i>  <i>Resulta que esto es el 33% del tr√°fico adicional, as√≠ como 1/3 de la redundancia de Erasure Coding.</i> <br><br>  <b>El consumo de memoria en el lado del servidor con y sin carga no cambia:</b> <br><br><img src="https://habrastorage.org/webt/nz/sz/zc/nzszzcu0eqrck4gpmhvazhm8x7i.png"><br><img src="https://habrastorage.org/webt/nz/sz/zc/nzszzcu0eqrck4gpmhvazhm8x7i.png"><br><br>  <b>La carga en los hosts del cl√∫ster con la carga m√°xima en el volumen:</b> <br><br><img src="https://habrastorage.org/webt/vb/u3/3c/vbu33cgi-rmgjn7c2w1guz5ps3c.png"></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es417475/">https://habr.com/ru/post/es417475/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de486156/index.html">Da habe ich unterrichtet und dann ein Trainingshandbuch in Python geschrieben</a></li>
<li><a href="../de486158/index.html">Visualisierung der neuronalen maschinellen √úbersetzung (seq2seq-Modelle mit Aufmerksamkeitsmechanismus)</a></li>
<li><a href="../de486164/index.html">Coronavirus 2019-nCoV. FAQ zu Atemschutz und Desinfektion</a></li>
<li><a href="../de486174/index.html">Ich habe keinen Umsatz</a></li>
<li><a href="../es417473/index.html">Almacenamiento confiable con DRBD9 y Proxmox (Parte 1: NFS)</a></li>
<li><a href="../es417477/index.html">Escritorio caliente</a></li>
<li><a href="../es417479/index.html">Concatenaci√≥n de cadenas de bricolaje m√°s r√°pida en Go</a></li>
<li><a href="../es417481/index.html">Acerca de los generadores en JavaScript ES6, y por qu√© es opcional estudiarlos</a></li>
<li><a href="../es417483/index.html">Comparaci√≥n de marcos JS: React, Vue e Hyperapp</a></li>
<li><a href="../es417485/index.html">[marcador] Cheat sheet del administrador del sistema para herramientas de red Linux</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>