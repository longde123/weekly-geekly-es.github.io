<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ùáÔ∏è üë©‚Äçüé§ ü¶Ñ Problemas de an√°lise de big data üôéüèΩ üõ©Ô∏è üêá</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Quais s√£o os desafios da an√°lise de big data 
 O Big Data cria recursos que n√£o s√£o compartilhados pelos conjuntos de dados tradicionais. Esses recurs...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Problemas de an√°lise de big data</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/456088/"><h3>  Quais s√£o os desafios da an√°lise de big data </h3><br>  O Big Data cria recursos que n√£o s√£o compartilhados pelos conjuntos de dados tradicionais.  Esses recursos criam problemas significativos para a an√°lise de dados e motivam o desenvolvimento de novos m√©todos estat√≠sticos.  Diferentemente dos conjuntos de dados tradicionais, em que o tamanho da amostra geralmente √© maior que a medi√ß√£o, o Big Data √© caracterizado por um grande tamanho de amostra e alta dimens√£o.  Primeiro, discutiremos o efeito de tamanhos grandes de amostras na compreens√£o da heterogeneidade: por um lado, tamanhos grandes de amostras nos permitem descobrir padr√µes ocultos associados a pequenos subgrupos da popula√ß√£o e pouca generalidade entre toda a popula√ß√£o.  Por outro lado, modelar a heterogeneidade interna do Big Data requer m√©todos estat√≠sticos mais sofisticados.  Em segundo lugar, discutiremos v√°rios fen√¥menos √∫nicos associados √† alta dimensionalidade, incluindo ac√∫mulo de ru√≠do, correla√ß√£o falsa e endogeneidade aleat√≥ria.  Esses recursos exclusivos invalidam os procedimentos estat√≠sticos tradicionais. <br><a name="habracut"></a><br><h3>  Heterogeneidade </h3><br>  O Big Data geralmente √© criado combinando v√°rias fontes de dados correspondentes a diferentes subgrupos.  Cada subgrupo pode exibir alguns recursos exclusivos que n√£o s√£o compartilhados por outros.  Em condi√ß√µes cl√°ssicas, quando o tamanho da amostra √© pequeno ou moderado, os pontos de dados de pequenas subpopula√ß√µes s√£o geralmente classificados como "desvios" e √© sistematicamente dif√≠cil de modelar devido ao n√∫mero insuficiente de observa√ß√µes.  No entanto, na era do Big Data, o grande tamanho da amostra nos permite entender melhor a heterogeneidade, lan√ßando luz sobre estudos como o estudo da rela√ß√£o entre certas covari√°veis ‚Äã‚Äã(por exemplo, genes ou SNPs) e resultados raros (por exemplo, doen√ßas raras ou doen√ßas em pequenas popula√ß√µes) e a compreens√£o por que certos tratamentos (como quimioterapia) beneficiam uma popula√ß√£o e prejudicam outra.  Para ilustrar melhor esse ponto, apresentamos o seguinte modelo para a popula√ß√£o: <br><br><p><math> </math> $$ display $$ Œª1p1 (y; Œ∏1 (x)) + ‚ãØ + Œªmpm (y; Œ∏m (x)), Œª1p1 (y; Œ∏1 (x)) + ‚ãØ + Œªmpm (y; Œ∏m (x)), ( 1) $$ exibi√ß√£o $$ </p><br>  Onde Œªj ‚â• 0 representa a fra√ß√£o do j-√©simo subgrupo, pj (y; Œ∏j (x)) √© a distribui√ß√£o de probabilidade da resposta do j-√©simo subgrupo, dadas as covari√°veis ‚Äã‚Äãde x com Œ∏j (x) como vetor de par√¢metro.  Na pr√°tica, muitas subpopula√ß√µes s√£o raramente observadas, ou seja, Œªj √© muito pequeno.  Quando o tamanho da amostra n √© moderado, nŒªj pode ser pequeno, o que torna imposs√≠vel derivar par√¢metros dependentes de covari√°veis ‚Äã‚ÄãŒ∏j (x) devido √† falta de informa√ß√µes.  No entanto, como o Big Data possui um grande tamanho de amostra n, o tamanho da amostra nŒªj para o j-√©simo grupo populacional pode ser moderadamente grande, mesmo que Œªj seja muito pequeno.  Isso nos permite tirar uma conclus√£o mais precisa sobre os par√¢metros da subpopula√ß√£o Œ∏j (¬∑).  Em resumo, a principal vantagem do Big Data √© o entendimento da heterogeneidade das subpopula√ß√µes, como os benef√≠cios de certos tratamentos personalizados que n√£o s√£o poss√≠veis com um tamanho de amostra pequeno ou moderado. <br><br>  O Big Data tamb√©m nos permite, devido ao grande tamanho da amostra, identificar uma comunidade fraca entre toda a popula√ß√£o.  Por exemplo, avaliar o benef√≠cio do cora√ß√£o de um copo de vinho tinto por dia pode ser dif√≠cil sem um grande tamanho de amostra.  Da mesma forma, os riscos √† sa√∫de associados √† exposi√ß√£o a certos fatores ambientais s√≥ podem ser avaliados de maneira mais convincente quando o tamanho da amostra √© grande o suficiente. <br><br>  Al√©m dos benef√≠cios acima, a heterogeneidade do Big Data tamb√©m apresenta desafios significativos para a infer√™ncia estat√≠stica.  A deriva√ß√£o do modelo de mistura em (1) para grandes conjuntos de dados requer m√©todos estat√≠sticos e computacionais complexos.  Em pequenas medi√ß√µes, m√©todos padr√£o, como o algoritmo de maximiza√ß√£o de espera para os modelos de mistura final, podem ser usados.  Em larga escala, no entanto, precisamos racionalizar cuidadosamente o procedimento de avalia√ß√£o para evitar o excesso de ajuste ou a acumula√ß√£o de ru√≠do e desenvolver bons algoritmos computacionais. <br><br><h3>  Acumula√ß√£o de ru√≠do </h3><br>  A an√°lise de big data exige que avaliemos e verifiquemos muitos par√¢metros ao mesmo tempo.  Os erros de estimativa se acumulam quando a decis√£o ou regra de previs√£o depende de um grande n√∫mero desses par√¢metros.  Esse efeito do ac√∫mulo de ru√≠do √© especialmente s√©rio em grandes dimens√µes e pode at√© dominar sinais verdadeiros.  Isso geralmente √© tratado pela suposi√ß√£o de escassez. <br><br>  Tome, por exemplo, uma classifica√ß√£o multidimensional.  Uma classifica√ß√£o ruim √© devida √† presen√ßa de muitas fraquezas que n√£o contribuem para a redu√ß√£o de erros de classifica√ß√£o.  Como exemplo, considere o problema de classifica√ß√£o quando os dados vierem de duas classes: <br><br><p><math> </math> $$ exibir $$ X1 e Y1, ........ Xn‚àºNd (Œº1, Id), Yn‚àºNd (Œº2, Id) .X1, ..., Xn‚àºNd (Œº1, Id) e Y1, ..., Yn‚àº Nd (Œº2, Id).  (2) $$ exibi√ß√£o $$ </p><br>  Queremos construir uma regra de classifica√ß√£o que classifique uma nova observa√ß√£o Z‚ààRdZ‚ààRd na primeira ou na segunda classe.  Para ilustrar o efeito do ac√∫mulo de ru√≠do na classifica√ß√£o, definimos n = 100 ed = 1000. Definimos Œº1 = 0Œº1 = 0 e Œº2 como esparsos, ou seja,  apenas os 10 primeiros registros de Œº2 s√£o diferentes de zero com um valor de 3 e todos os outros registros s√£o zero.  A Figura 1 mostra os dois primeiros componentes principais usando o primeiro m = 2, 40, 200 elementos e at√© 1000 elementos.  Como mostrado nesses gr√°ficos, quando m = 2, obtemos um alto grau de discrimina√ß√£o.  No entanto, o poder discriminador se torna muito baixo quando m √© muito grande devido ao ac√∫mulo de ru√≠do.  As 10 primeiras fun√ß√µes contribuem para a classifica√ß√£o, enquanto as demais n√£o.  Portanto, quando m&gt; 10, os procedimentos n√£o recebem sinais adicionais, mas acumulam ru√≠do: quanto mais m, mais ru√≠do se acumula, o que piora o procedimento de classifica√ß√£o devido √† dimensionalidade.  Em m = 40, os sinais acumulados compensam o ru√≠do acumulado, de modo que os dois primeiros componentes principais ainda possuem boa capacidade de reconhecimento.  Quando m = 200, o ru√≠do acumulado excede o ganho do sinal. <br><br>  A discuss√£o acima motiva o uso de modelos esparsos e a escolha de vari√°veis ‚Äã‚Äãpara superar o efeito do ac√∫mulo de ru√≠do.  Por exemplo, no modelo de classifica√ß√£o (2), em vez de usar todas as fun√ß√µes, poder√≠amos escolher um subconjunto de recursos que atinjam a melhor rela√ß√£o sinal / ru√≠do.  Esse modelo esparso fornece maior efici√™ncia de classifica√ß√£o.  Em outras palavras, a escolha de vari√°veis ‚Äã‚Äãdesempenha um papel fundamental na supera√ß√£o do ac√∫mulo de ru√≠do na classifica√ß√£o e previs√£o de regress√£o.  No entanto, a sele√ß√£o de vari√°veis ‚Äã‚Äãem grandes dimens√µes √© desafiadora devido √† correla√ß√£o falsa, endogeneidade aleat√≥ria, heterogeneidade e erros de medi√ß√£o. <br><br><h3>  Correla√ß√£o falsa </h3><br>  A alta dimensionalidade tamb√©m cont√©m uma correla√ß√£o falsa, citando o fato de que muitas vari√°veis ‚Äã‚Äãaleat√≥rias n√£o correlacionadas podem ter altas correla√ß√µes amostrais em grandes dimens√µes.  A correla√ß√£o falsa pode levar a descobertas cient√≠ficas err√¥neas e conclus√µes estat√≠sticas incorretas. <br><br>  Considere o problema de estimar o vetor de coeficiente Œ≤ de um modelo linear <br><br><p><math> </math> $$ display $$ y = XŒ≤ + œµ, Var (œµ) = œÉ2Id, y = XŒ≤ + œµ, Var (œµ) = œÉ2Id, (3) $$ display $$ </p><br>  onde y‚ààRny‚ààRn representa o vetor de resposta, X = [x1, ..., xn] T‚ààRn √ó dX = [x1, ..., xn] T‚ààRn √ó d representa a matriz de proje√ß√£o, ‚ààRnœµ‚ààRn representa o vetor aleat√≥rio independente noise e Id √© a matriz de identidade d √ó d.  Para lidar com o problema da acumula√ß√£o de ru√≠do, quando o tamanho d √© compar√°vel ou maior que o tamanho da amostra n, sup√µe-se que a resposta d√™ apenas um pequeno n√∫mero de vari√°veis, ou seja, Œ≤ √© um vetor esparso.  De acordo com essa suposi√ß√£o de escassez, uma vari√°vel pode ser selecionada para evitar ac√∫mulo de ru√≠do, melhorar o desempenho da previs√£o e melhorar a interpretabilidade de um modelo com uma representa√ß√£o conservadora. <br><br>  Para tamanhos grandes, mesmo para um modelo t√£o simples como (3), a escolha das vari√°veis ‚Äã‚Äã√© dif√≠cil devido √† presen√ßa de uma correla√ß√£o falsa.  Em particular, com alta dimensionalidade, vari√°veis ‚Äã‚Äãimportantes podem ser fortemente correlacionadas com v√°rias vari√°veis ‚Äã‚Äãfalsas que n√£o s√£o cientificamente relacionadas.  Considere um exemplo simples que ilustra esse fen√¥meno.  Seja x1, ..., xn observa√ß√µes independentes de um vetor aleat√≥rio gaussiano d-dimensional X = (X1, ..., Xd) T‚àºNd (0, Id) X = (X1, ..., Xd) T‚àºNd (0, Id) ‚Å† .  Simulamos repetidamente os dados com n = 60 ed = 800 e 6400 1000 vezes.  A Figura 2a mostra a distribui√ß√£o emp√≠rica do coeficiente m√°ximo de correla√ß√£o absoluta da amostra entre a primeira vari√°vel e o restante √© definido como <br><br><p><math> </math> $$ display $$ rÀÜ = maxj‚â•2 | CorrÀÜ (X1, Xj) |, r ^ = maxj‚â•2 | Corr ^ (X1, Xj) |, (4) $$ display $$ </p><br>  onde Corr ^ (X1, Xj) Corr ^ (X1, Xj) √© a correla√ß√£o de amostra entre as vari√°veis ‚Äã‚ÄãX1 e Xj.  Vemos que a correla√ß√£o absoluta m√°xima da amostra aumenta com a dimens√£o crescente. <br><br>  Al√©m disso, podemos calcular a correla√ß√£o m√∫ltipla absoluta m√°xima entre X1 e combina√ß√µes lineares de v√°rias vari√°veis ‚Äã‚Äãsecund√°rias irrelevantes: <br><br><p><math> </math> $$ display $$ RÀÜ = max | S | = 4m√°x {Œ≤j} 4j = 1‚à£‚à£‚à£‚à£CorrÀÜ (X1, ‚àëj‚ààSŒ≤jXj) ‚à£‚à£‚à£‚à£.R ^ = max | S | = 4m√°x {Œ≤j} j = 14 | Corr ^ (X1, ‚àëj‚ààSŒ≤jXj) |.  (5) $$ exibi√ß√£o $$ </p><br>  Usando a configura√ß√£o padr√£o, √© dada a distribui√ß√£o emp√≠rica do coeficiente absoluto m√°ximo de correla√ß√£o amostral entre X1 e ‚àëj ‚àà SŒ≤jXj, onde S √© qualquer subconjunto do quarto tamanho de {2, ..., d} e Œ≤j √© o coeficiente de regress√£o de m√≠nimos quadrados Xj quando X1 regride em {Xj} j ‚àà S. Novamente, vemos que, embora X1 seja completamente independente de X2, ..., Xd, a correla√ß√£o entre X1 e a combina√ß√£o linear mais pr√≥xima de quaisquer quatro vari√°veis ‚Äã‚Äãde {Xj} j ‚â† 1 a X1 pode ser muito alta. <br><br>  A correla√ß√£o falsa tem um efeito significativo na escolha das vari√°veis ‚Äã‚Äãe pode levar a descobertas cient√≠ficas err√¥neas.  Seja XS = (Xj) j ‚àà S um vetor aleat√≥rio indexado por S e seja S setS ^ o conjunto selecionado que tenha uma correla√ß√£o parasit√°ria mais alta com X1, como na Fig.  2. Por exemplo, quando n = 60 ed = 6400, vemos que X1 √© praticamente indistingu√≠vel de XSXS ^ para o conjunto SS ^  |  SÀÜ  = 4  S ^  = 4‚Å†.  Se X1 representa o n√≠vel de express√£o do gene respons√°vel pela doen√ßa, n√£o podemos distingui-lo dos outros quatro genes em SS ^, que t√™m um poder progn√≥stico semelhante, embora eles, do ponto de vista cient√≠fico, n√£o importem. <br><br>  Al√©m da escolha das vari√°veis, uma correla√ß√£o falsa tamb√©m pode levar a uma conclus√£o estat√≠stica incorreta.  Explicamos isso considerando novamente o mesmo modelo linear que em (3).  Aqui gostar√≠amos de avaliar o erro padr√£o œÉ do restante, que se manifesta notavelmente nas conclus√µes estat√≠sticas dos coeficientes de regress√£o, sele√ß√£o do modelo, teste de conformidade e regress√£o marginal.  Seja SÀÜS ^ o conjunto de vari√°veis ‚Äã‚Äãselecionadas e PSÀÜPS ^ seja a matriz de proje√ß√£o no espa√ßo da coluna XSÀÜXS ^ ‚Å†.  Estimativa padr√£o da varia√ß√£o residual com base nas vari√°veis ‚Äã‚Äãselecionadas: <br><br><p><math> </math> $$ display $$ œÉÀÜ2 = yT (In - PSÀÜ) yn‚àí | SÀÜ | .œÉ ^ 2 = yT (In - PS ^) yn‚àí | S ^ |.  (6) $$ exibi√ß√£o $$ </p><br>  O avaliador (6) √© imparcial quando as vari√°veis ‚Äã‚Äãn√£o s√£o selecionadas dos dados e o modelo est√° correto.  No entanto, a situa√ß√£o √© completamente diferente quando as vari√°veis ‚Äã‚Äãs√£o selecionadas com base nos dados.  Em particular, os autores mostraram que, quando existem muitas vari√°veis ‚Äã‚Äãfalsas, œÉ2 √© seriamente subestimado, isso leva a conclus√µes estat√≠sticas err√¥neas, incluindo a escolha de modelos ou testes de signific√¢ncia e a descobertas cient√≠ficas err√¥neas, como a busca de genes errados para mecanismos moleculares.  Eles tamb√©m oferecem um m√©todo avan√ßado de valida√ß√£o cruzada para facilitar o problema. <br><br><h3>  Endogeneidade aleat√≥ria </h3><br>  A endogenicidade aleat√≥ria √© outro problema sutil que surge da alta dimensionalidade.  No cen√°rio de regress√£o, Y = jdj = 1Œ≤jXj + ŒµY = ‚àëj = 1dŒ≤jXj + Œµ‚Å†, o termo ‚Äúendogeneidade‚Äù significa que alguns preditores {Xj} se correlacionam com o ru√≠do residual Œµ.  O modelo esparso usual assume <br><br><p><math> </math> $$ display $$ Y = Œ≤jŒ≤jXj + Œµ, e E (ŒµXj) = 0 para j = 1, ..., d, Y = ‚àëjŒ≤jXJ + Œµ, e E (ŒµXj) = 0 para j = 1, ..., d , (7) $$ exibir $$ </p><br>  com um pequeno conjunto S = {j: Œ≤j ‚â† 0}.  A suposi√ß√£o ex√≥gena (7) de que o ru√≠do residual Œµ n√£o se correlaciona com todos os preditores √© crucial para a confiabilidade da maioria dos m√©todos estat√≠sticos existentes, incluindo consist√™ncia na escolha das vari√°veis.  Embora essa suposi√ß√£o pare√ßa inocente, √© f√°cil viol√°-la em grandes dimens√µes, pois algumas vari√°veis ‚Äã‚Äã{Xj} se correlacionam aleatoriamente com Œµ, o que torna a maioria dos procedimentos multidimensionais estatisticamente inv√°lidos. <br><br>  Para explicar o problema da endogeneidade com mais detalhes, suponha que a resposta desconhecida Y esteja associada √†s tr√™s covari√°veis ‚Äã‚Äãda seguinte maneira: <br><br><p><math> </math> $$ display $$ Y = X1 + X2 + X3 + Œµ, comEŒµXj = 0, para j = 1, 2, 3.Y = X1 + X2 + X3 + Œµ, comEŒµXj = 0, para j = 1, 2, 3 . $$ display $$ </p><br>  Na fase de coleta de dados, n√£o conhecemos o modelo verdadeiro e, portanto, coletamos tantas covari√°veis ‚Äã‚Äãquanto potencialmente associadas a Y na esperan√ßa de incluir todos os termos em S em (7).  A prop√≥sito, alguns desses Xj (para os jj 1, 2, 3) podem estar associados ao ru√≠do residual Œµ.  Isso refuta a suposi√ß√£o de modelagem ex√≥gena em (7).  De fato, quanto mais covari√°veis ‚Äã‚Äãs√£o coletadas ou medidas, mais complexa √© essa suposi√ß√£o. <br><br>  Ao contr√°rio da correla√ß√£o falsa, a endogeneidade aleat√≥ria refere-se √† exist√™ncia real de correla√ß√µes entre vari√°veis ‚Äã‚Äãn√£o intencionais.  O primeiro √© semelhante ao fato de que duas pessoas s√£o semelhantes, mas n√£o t√™m uma conex√£o gen√©tica, e o segundo √© como um conhecido que ocorre facilmente em uma cidade grande.  Em um sentido mais geral, a endogeneidade resulta do vi√©s de escolha, erros de medi√ß√£o e vari√°veis ‚Äã‚Äãausentes.  Esses fen√¥menos geralmente surgem ao analisar o Big Data, principalmente por dois motivos: <br><br><ul><li>  Gra√ßas aos novos m√©todos de medi√ß√£o de alto desempenho, os cientistas podem coletar o maior n√∫mero poss√≠vel de fun√ß√µes e se esfor√ßar para isso.  Isso, consequentemente, aumenta a probabilidade de que alguns deles possam estar correlacionados com o ru√≠do residual. </li><li>  O Big Data geralmente √© combinado de v√°rias fontes com esquemas de gera√ß√£o de dados potencialmente diferentes.  Isso aumenta a probabilidade de vi√©s nos erros de sele√ß√£o e medi√ß√£o, que tamb√©m causam potencial endogeneidade aleat√≥ria. </li></ul><br>  A endogeneidade aleat√≥ria aparece em conjuntos de dados reais e como podemos testar isso na pr√°tica?  Estamos considerando um estudo gen√¥mico no qual 148 amostras de microarrays s√£o baixadas dos bancos de dados GEO e ArrayExpress.  Essas amostras foram criadas na plataforma Affymetrix HGU133a para pessoas com c√¢ncer de pr√≥stata.  O conjunto de dados obtido cont√©m 22.283 sondas, o que corresponde a 12.719 genes.  Neste exemplo, estamos interessados ‚Äã‚Äãem um gene chamado ‚Äúmembro da fam√≠lia 1 do receptor do dom√≠nio discoidina‚Äù (abreviado DDR1).  O DDR1 codifica tirosina-quinases receptoras, que desempenham um papel importante na conex√£o das c√©lulas com seu microambiente.  Sabe-se que o DDR1 est√° intimamente relacionado ao c√¢ncer de pr√≥stata e queremos estudar sua rela√ß√£o com outros genes em pacientes com c√¢ncer.  Tomamos a express√£o do gene DDR1 como vari√°vel de resposta Y e a express√£o de todos os 12.718 genes restantes como preditores.  No painel esquerdo, fig.  A Figura 3 mostra a distribui√ß√£o emp√≠rica das correla√ß√µes entre a resposta e os preditores individuais. <br><br>  Para ilustrar a exist√™ncia de endogeneidade, ajustamos a regress√£o de m√≠nimos quadrados L1 (Lasso) aos dados, e a penalidade √© automaticamente selecionada usando uma valida√ß√£o cruzada de 10 vezes (37 genes selecionados).  Em seguida, restauraremos a regress√£o de m√≠nimos quadrados usual para o modelo selecionado para calcular o vetor residual.  No painel direito, fig.  3, constru√≠mos uma distribui√ß√£o emp√≠rica de correla√ß√µes entre preditores e res√≠duos.  Vemos que o ru√≠do residual se correlaciona fortemente com muitos preditores.  Para garantir que essas correla√ß√µes n√£o sejam causadas por uma correla√ß√£o puramente falsa, introduzimos uma "distribui√ß√£o zero" de correla√ß√µes falsas reorganizando aleatoriamente as ordens de linha na matriz do projeto, para que os preditores sejam realmente independentes do ru√≠do residual.  Comparando essas duas distribui√ß√µes, vemos que a distribui√ß√£o de correla√ß√µes entre preditores e ru√≠do residual nos dados brutos (marcados como ‚Äúdados brutos‚Äù) tem uma cauda mais pesada do que nos dados rearranjados (marcados como ‚Äúdados rearranjados‚Äù).  Este resultado fornece fortes evid√™ncias de endogenicidade. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt456088/">https://habr.com/ru/post/pt456088/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt456076/index.html">Meetup sobre PyDaCon no grupo Mail.ru: 22 de junho</a></li>
<li><a href="../pt456078/index.html">Proje√ß√£o de conflito corporativo na conectividade de rede</a></li>
<li><a href="../pt456082/index.html">Como desenvolvemos recomenda√ß√µes personalizadas de produtos</a></li>
<li><a href="../pt456084/index.html">Kubernetes 1.15: Vis√£o geral dos destaques</a></li>
<li><a href="../pt456086/index.html">Storyboards para iOS: an√°lise dos pr√≥s e contras, melhores pr√°ticas</a></li>
<li><a href="../pt456090/index.html">Introdu√ß√£o ao teste de unidade no Unity</a></li>
<li><a href="../pt456092/index.html">Sete sinais preocupantes de que voc√™ depende do clima, mesmo que voc√™ n√£o pense</a></li>
<li><a href="../pt456094/index.html">Lemos as fichas t√©cnicas 2: SPI no STM32; Temporizadores PWM e interrup√ß√µes no STM8</a></li>
<li><a href="../pt456096/index.html">O que o leitor comum de tempos de nerd faz enquanto paira nas nuvens</a></li>
<li><a href="../pt456100/index.html">Agora na nova embalagem - o Kingston A400 no formato M.2 chega ao mercado</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>