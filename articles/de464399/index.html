<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏼‍🚒 🛥️ 😂 Web Scraping in R, Teil 2. Beschleunigen Sie den Prozess durch paralleles Rechnen und Verwenden des Rcrawler-Pakets 🔺 🈳 😼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In einem früheren Artikel habe ich mithilfe von Scraping-Parsing Filmbewertungen von IMDB- und Kinopoisk-Websites gesammelt und verglichen. Repository...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Web Scraping in R, Teil 2. Beschleunigen Sie den Prozess durch paralleles Rechnen und Verwenden des Rcrawler-Pakets</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/464399/"><p><img src="https://habrastorage.org/webt/vy/vh/m_/vyvhm_gjoiuzkbfemd_0fsnbw74.png"></p><br><p>  In einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">früheren Artikel habe</a> ich mithilfe von Scraping-Parsing Filmbewertungen von IMDB- und Kinopoisk-Websites gesammelt und verglichen.  Repository auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Github</a> . </p><br><p>  Der Code hat seine Aufgabe gut erledigt, aber das Scraping wird oft verwendet, um nicht ein paar Seiten, sondern ein paar dreitausend zu "kratzen", und der Code aus dem vorherigen Artikel ist nicht für ein so "großes" Scraping geeignet.  Genauer gesagt wird es nicht optimal sein.  Im Prinzip hindert Sie praktisch nichts daran, damit Tausende von Seiten zu crawlen.  Praktisch, weil Sie einfach nicht so viel Zeit haben <a name="habracut"></a></p><br><p><img src="https://habrastorage.org/webt/ik/-y/rk/ik-yrkuvryxwpdvidot66ijfzn8.jpeg"></p><br><p>  <em>Als ich mich entschied, <a href="">Scraping_imdb.R</a> zu verwenden, um 1000 Seiten zu crawlen</em> </p><br>
<h5 id="optimizaciya-koda-odnokratnoe-ispolzovanie-funkcii-read_html">  Codeoptimierung.  Eine einmalige Verwendung der Funktion <code>read_html</code> </h5><br><p>  In diesem Artikel werden 100 Links zu den Seiten des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Labyrinth-</a> Buchladens verwendet, um den Betrieb und die Geschwindigkeit des Codes zu überprüfen. </p><br><p>  Eine explizite Änderung, die den Prozess beschleunigen kann, ist die einmalige Verwendung der "langsamsten" <code>read_html</code> - <code>read_html</code> .  Ich möchte Sie daran erinnern, dass sie die HTML-Seite "liest".  In der ersten Version des Codes für <code>read_html</code> habe ich jedes Mal <code>read_html</code> wenn ich einen Wert erhalten wollte ( <code>read_html</code> , Jahr, Genre, Bewertung).  Jetzt wurden die Spuren dieser „Schande“ von GitHuba gelöscht, aber es ist so.  Dies macht keinen Sinn, da die mit <code>read_html</code> erstellte <code>read_html</code> Informationen über die gesamte Seite enthält und es ausreicht, diese Variable der Funktion <code>html_nodes</code> und nicht jedes Mal mit dem Lesen von HTML zu beginnen, um unterschiedliche Daten daraus zu erhalten.  So können Sie Zeit proportional zur Anzahl der Werte sparen, die Sie erhalten möchten.  Aus dem Labyrinth erhalte ich jeweils sieben Werte. Code, der nur einen einzigen Lesevorgang einer HTML-Seite verwendet, funktioniert etwa siebenmal schneller.  Nicht schlecht!  Aber bevor ich wieder "beschleunige", werde ich abschweifen und über interessante Punkte sprechen, die sich beim Schaben von der Labyrinth-Website ergeben. </p><br><h5 id="osobennosti-skrepinga-stranic-na-labirinte">  Funktionen des Seitenkratzens im Labyrinth </h5><br><p>  In diesem Teil werde ich nicht auf das Verfahren zum Abrufen und Löschen der im vorherigen Artikel erwähnten Daten eingehen.  Ich werde nur die Momente erwähnen, die ich zum ersten Mal erlebt habe, als ich Code für das Scrapbooking eines Buchladens geschrieben habe. </p><br><p>  Zunächst ist die Struktur zu erwähnen.  Sie fühlt sich nicht sehr wohl.  Im Gegensatz dazu geben beispielsweise auf der Read-Cities-Website in Abschnitten des Genres mit "leeren Filtern" nur 17 Seiten aus.  Natürlich passen nicht alle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">8011</a> Bücher des Genres "Contemporary Foreign Prose" darauf. </p><br><p>  Daher habe ich mir nichts Besseres ausgedacht, als die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://www.labirint.ru/books/</a> **** Links mit einer einfachen Büste zu umgehen.  Ehrlich gesagt ist die Methode nicht die beste (schon allein deshalb, weil die meisten "alten" Bücher außer dem Namen keine Informationen enthalten und daher praktisch nutzlos sind). Wenn also jemand eine elegantere Lösung anbietet, bin ich froh.  Aber ich fand heraus, dass es unter der stolzen ersten Nummer auf der Website des Labyrinths ein Buch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">"Wie man Mondschein macht" gibt.</a>  Leider ist es bereits unmöglich, dieses Wissensspeicher zu kaufen. </p><br><p>  Alle Adressen während der Aufzählung können in zwei Typen unterteilt werden: </p><br><ul><li>  Seiten, die existieren </li><li>  Seiten, die nicht existieren </li></ul><br><p>  Bestehende Seiten können wiederum in zwei Teile unterteilt werden: </p><br><ul><li>  Seiten, die alle notwendigen Informationen enthalten </li><li>  Seiten, die nicht alle erforderlichen Informationen enthalten </li></ul><br><p>  Am Ende habe ich eine Datentabelle mit sieben Spalten: </p><br><ol><li>  ISBN - ISBN-Buchnummer </li><li>  PREIS - Buchpreis </li><li>  NAME - Buchtitel </li><li>  AUTOR - Autor des Buches </li><li>  VERLAG - Verlag </li><li>  JAHR - Erscheinungsjahr </li><li>  SEITE - Anzahl der Seiten </li></ol><br><p>  Mit den Seiten mit vollständigen Informationen ist alles klar, sie erfordern keine Änderungen im Vergleich zum Code für Filmseiten. </p><br><p>  Seiten, auf denen einige Daten nicht verfügbar sind, sind mit ihnen nicht so einfach.  Eine Suche auf der Seite gibt nur die gefundenen Werte zurück und die Ausgabelänge verringert sich um die Anzahl der Elemente, die nicht gefunden werden.  Dies wird die gesamte Struktur brechen.  Um dies zu vermeiden, wurde jedem Argument ein if ... else-Konstrukt hinzugefügt, das die Länge des nach Verwendung der Funktion <code>html_nodes</code> erhaltenen Vektors <code>html_nodes</code> Wenn es Null ist, wird <code>NA</code> , um <code>html_nodes</code> zu vermeiden. </p><br><pre> <code class="plaintext hljs"> PUBLISHER &lt;- unlist(lapply(list_html, function(n){ publishing &lt;- if(n != "NA") { publishing_html &lt;- html_nodes(n, ".publisher a") publishing &lt;- if(length(publishing_html) == 0){ NA } else { publishing &lt;- html_text(publishing_html) } } else { NA } }))</code> </pre> <br><p>  Aber wie Sie hier sehen können, sind es bis zu zwei Wenns und bis zu zwei andere.  Nur die "internen" if..esle sind für die Lösung des oben beschriebenen Problems relevant.  Exterieur löst das Problem mit nicht vorhandenen Seiten. </p><br><p>  Seiten, die einfach nicht die meisten Probleme haben.  Wenn Werte auf Seiten mit fehlenden Daten verschoben werden und die Eingabe <code>read_html</code> nicht vorhandene Seite <code>read_html</code> , gibt die Funktion <code>read_html</code> Fehler aus und der Code wird nicht mehr ausgeführt.  Weil  Irgendwie ist es nicht möglich, solche Seiten im Voraus zu erkennen. Es muss sichergestellt werden, dass der Fehler nicht den gesamten Prozess stoppt. </p><br><p>  Die <code>possibly</code> des <code>possibly</code> Pakets hilft uns dabei.  Die Bedeutung von <code>possibly</code> (außer <code>possibly</code> <code>quietly</code> und <code>safely</code> ) besteht darin, die gedruckte Ausgabe von Nebenwirkungen (z. B. Fehler) durch einen Wert zu ersetzen, der zu uns passt.  <code>possibly</code> eine <code>possibly(.f, otherwise)</code> Struktur <code>possibly(.f, otherwise)</code> und wenn ein Fehler im Code auftritt, verwendet er den Standardwert (ansonsten), anstatt die Ausführung zu stoppen.  In unserem Fall sieht es so aus: </p><br><pre> <code class="plaintext hljs">book_html &lt;- possibly(read_html, "NA")(n)</code> </pre> <br><p>  n ist eine Liste der Adressen der Seiten der Site, die wir abgekratzt haben.  Bei der Ausgabe erhalten wir eine Liste der Länge n, in der Elemente von vorhandenen Seiten in der "normalen" Form vorliegen, um die Funktion <code>read_html</code> , und Elemente von nicht vorhandenen Seiten aus dem Zeichenvektor "NA" bestehen.  Bitte beachten Sie, dass der Standardwert ein Zeichenvektor sein muss, da wir in Zukunft darauf verweisen werden.  Wenn wir nur <code>NA</code> schreiben, wie im PUBLISHER-Codeteil, ist dies nicht möglich.  Um Verwirrung zu vermeiden, können Sie den ansonsten angegebenen Wert von NA in einen anderen ändern. </p><br><p>  Und jetzt zurück zum Code, um den Namen des Herausgebers zu erhalten.  Extern, wenn ... sonst für die gleichen Zwecke wie intern benötigt wird, jedoch in Bezug auf nicht vorhandene Seiten.  Wenn die Variable <code>book_html</code> "NA" ist, ist jeder der "abgekratzten" Werte auch gleich <code>NA</code> (hier können Sie bereits die "echte" <code>NA</code> anstelle eines symbolischen Betrügers verwenden).  Am Ende erhalten wir also eine Tabelle mit der folgenden Form: </p><br><div class="scrollable-table"><table><thead><tr><th>  ISBN </th><th>  PREIS </th><th>  NAME </th><th>  AUTOR </th><th>  VERLAG </th><th>  Jahr </th><th>  Seite </th></tr></thead><tbody><tr><td>  4665305770322 </td><td>  1488 </td><td>  Set String Art "Netter Welpe" (30 * 30 cm) (DH6021) </td><td>  NA </td><td>  Ingwerkatze </td><td>  2019 </td><td>  NA </td></tr><tr><td>  NA </td><td>  NA </td><td>  NA </td><td>  NA </td><td>  NA </td><td>  NA </td><td>  NA </td></tr><tr><td>  9785171160814 </td><td>  273 </td><td>  Arkady Averchenko: Lustige Geschichten für Kinder </td><td>  Autor: Averchenko Arkady Timofeevich, Künstler: Vlasova Anna Yulievna </td><td>  Kind </td><td>  2019 </td><td>  288 </td></tr></tbody></table></div><br><p>  Nun zurück mit der Beschleunigung des Kratzprozesses. </p><br><h5 id="parallelnoe-vychislenie-v-r-sravnenie-skorosti-i-podvodnye-kamni-pri-ispolzovanii-funkcii-read_html">  Paralleles Rechnen in R. Geschwindigkeitsvergleich und Fallstricke bei Verwendung der Funktion <code>read_html</code> </h5><br><p>  Standardmäßig werden alle Berechnungen in R auf demselben Prozessorkern ausgeführt.  Und während dieser unglückliche Kern im Gesicht arbeitet und Daten von Tausenden von Seiten für uns „kratzt“, „kühlt“ sich der Rest unserer Kameraden ab und führt einige andere Aufgaben aus.  Die Verwendung von Parallel Computing hilft dabei, alle Prozessorkerne für die Verarbeitung / den Empfang von Daten zu gewinnen, was den Prozess beschleunigt. </p><br><p>  Ich werde nicht weiter auf das Design des parallelen Rechnens auf R eingehen. Sie können <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> zum Beispiel mehr darüber lesen.  Ich habe die Parallelität von R so verstanden, dass Kopien von R in separaten Clustern entsprechend der Anzahl der angegebenen Kernel erstellt werden, die über <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sockets</a> miteinander interagieren. </p><br><p>  Ich erzähle Ihnen von dem Fehler, den ich bei der Verwendung von Parallel Computing gemacht habe.  Ursprünglich war mein Plan folgender: Mit Parallel Computing erhalte ich eine Liste mit 100 "gelesenen" <code>read_html</code> Seiten und <code>read_html</code> dann im normalen Modus nur die Daten, die ich benötige.  Zuerst lief alles gut: Ich bekam eine Liste und verbrachte viel weniger Zeit damit als im normalen Modus R. Aber erst als ich versuchte, mit dieser Liste zu interagieren, erhielt ich eine Fehlermeldung: </p><br><pre> <code class="plaintext hljs">Error: external pointer is not valid</code> </pre> <br><p>  Infolgedessen erkannte ich das Problem, indem ich mir Beispiele im Internet ansah, und fand danach nach dem Gesetz der Gemeinheit Henrik Bengtssons Erklärung in der Vignette für das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><strong>zukünftige</strong></a> Paket.  Tatsache ist, dass die XML-Funktionen des <code>xml2</code> Pakets nicht exportierbare Objekte sind. <br>  )  Diese Objekte sind an diese R-Sitzung „gebunden“ und können nicht auf einen anderen Prozess übertragen werden, was ich versucht habe.  Daher sollte die beim parallelen Rechnen gestartete Funktion einen „vollständigen Zyklus“ von Vorgängen enthalten: Lesen einer HTML-Seite, Empfangen und Bereinigen der erforderlichen Daten. </p><br><p>  Das Erstellen von Parallel Computing selbst erfordert nicht viel Zeit und Codezeilen.  Als erstes müssen Sie die Bibliotheken herunterladen.  Das Github-Repository gibt an, welche Pakete für welche Methoden benötigt werden.  Hier zeige ich paralleles Rechnen mit der <code>parLapply</code> Funktion des <code>parallel</code> Pakets.  Führen <code>doParallel</code> einfach <code>doParallel</code> ( <code>parallel</code> wird in diesem Fall automatisch gestartet).  Wenn Sie die Anzahl der Kerne Ihres Prozessors plötzlich nicht mehr kennen oder vergessen haben, ermitteln Sie, wie viele von ihnen <code>detectCores</code> werden <code>detectCores</code> </p><br><pre> <code class="plaintext hljs"># detectCores - ,     number_cl &lt;- detectCores()</code> </pre> <br><p>  Erstellen Sie als Nächstes parallele Kopien von R: </p><br><pre> <code class="plaintext hljs"> # makePSOCKcluster -    R,    cluster &lt;- makePSOCKcluster(number_cl) registerDoParallel(cluster)</code> </pre> <br><p>  Jetzt schreiben wir eine Funktion, die alle erforderlichen Prozeduren ausführt.  Ich stelle das seitdem fest  Es werden neue Sitzungen erstellt. R-Pakete, deren Funktionen in unserer eigenen Funktion verwendet werden, sollten in den Hauptteil der Funktion geschrieben werden.  In <a href="">spider_parallel.R wird</a> das <code>stringr</code> Paket dadurch zweimal ausgeführt: zuerst zum <code>stringr</code> der <code>stringr</code> und dann zum Löschen der Daten. </p><br><p>  Und dann unterscheidet sich das Verfahren fast nicht von der üblichen <code>lapply</code> Funktion.  In <code>parLapply</code> stellen wir eine Liste von Adressen, unsere eigene Funktion und als einzige Ergänzung eine Variable mit den von uns erstellten Clustern <code>parLapply</code> . </p><br><pre> <code class="plaintext hljs"># parLapply -  lapply     big_list &lt;- parLapply(cluster, list_url, scraping_parellel_func) #    stopCluster(cluster)</code> </pre> <br><p>  Das ist alles, jetzt bleibt es Zeit, die verbrachte Zeit zu vergleichen. </p><br><h5 id="sravnenie-skorosti-posledovatelnogo-i-parallelnogo-vychisleniya">  Vergleich der seriellen und parallelen Rechengeschwindigkeit </h5><br><p>  Dies wird der kürzeste Punkt sein.  Paralleles Rechnen war fünfmal schneller als gewöhnlich: </p><br><p>  Scraping-Geschwindigkeit ohne Parallel-Computing </p><br><div class="scrollable-table"><table><thead><tr><th>  der Benutzer </th><th>  das System </th><th>  bestanden </th></tr></thead><tbody><tr><td>  13.57 </td><td>  0,40 </td><td>  112,84 </td></tr></tbody></table></div><br><p>  Scraping-Geschwindigkeit mit Parallel Computing </p><br><div class="scrollable-table"><table><thead><tr><th>  der Benutzer </th><th>  das System </th><th>  bestanden </th></tr></thead><tbody><tr><td>  0,14 </td><td>  0,05 </td><td>  21.12 </td></tr></tbody></table></div><br><p>  Was soll ich sagen?  Paralleles Rechnen kann viel Zeit sparen, ohne dass Schwierigkeiten beim Erstellen des Codes entstehen.  Mit zunehmender Anzahl von Kernen steigt die Geschwindigkeit fast proportional zu ihrer Anzahl.  Mit einigen Änderungen haben wir den Code zuerst siebenmal beschleunigt (die Berechnung von <code>read_html</code> bei jedem Schritt <code>read_html</code> ) und dann weitere fünfmal mithilfe paralleler Berechnungen.  Spider-Skripte <a href="">ohne</a> paralleles Rechnen mit den Paketen <a href=""><code>parallel</code></a> und <a href=""><code>foreach</code></a> befinden sich im Repository von Github. </p><br><h5 id="nebolshoy-obzor-paketa-rcrawler-sravnenie-skorosti">  Eine kleine Übersicht über das <code>Rcrawler</code> Paket.  Geschwindigkeitsvergleich. </h5><br><p>  Es gibt verschiedene andere Möglichkeiten, HTML-Seiten in R zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><strong>verschrotten</strong></a> , aber ich werde mich auf das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><strong>Rcrawler-</strong></a> Paket konzentrieren.  Das Unterscheidungsmerkmal von anderen Tools in der R-Sprache ist die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Möglichkeit,</a> Websites <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zu</a> crawlen.  Sie können die gleichnamige <code>Rcrawler</code> Funktion auf die <code>Rcrawler</code> Adresse setzen und methodisch Seite für Seite die gesamte Site umgehen.  <code>Rcrawler</code> verfügt über viele Argumente zum Einrichten der Suche (z. B. können Sie nach Schlüsselwörtern, Sektoren der Site suchen (nützlich, wenn die Site aus einer großen Anzahl von Seiten besteht), Suchtiefe, Ignorieren von URL-Parametern, die doppelte Seiten erstellen, und vieles mehr Die Funktionen wurden bereits für parallele Berechnungen festgelegt, die durch die Argumente <code>no_cores</code> (Anzahl der beteiligten Prozessorkerne) und <code>no_conn</code> (Anzahl der parallelen Anforderungen) angegeben werden. </p><br><p>  In unserem Fall gibt es beim Scraping von den angegebenen Adressen eine <code>ContentScraper</code> Funktion.  Standardmäßig wird kein paralleles Rechnen verwendet, daher müssen Sie alle oben beschriebenen Manipulationen wiederholen.  Ich mochte die Funktion selbst - sie bietet viele Optionen zum Einrichten des Scrapings und ist auf einer intuitiven Ebene gut verstanden.  Auch hier können Sie if..else nicht für fehlende Seiten oder fehlende Werte verwenden, as  Die Funktionsausführung wird nicht gestoppt. </p><br><pre> <code class="plaintext hljs">#   ContentScraper: # CssPatterns -    CSS    . # ExcludeCSSPat -    CSS ,    . # ,   CSS     CSS ,    . # ManyPerPattern -  FALSE,       , #  .  TRUE,     ,   . # PatternsName -      .   #   c  ,      t_func &lt;- function(n){ library(Rcrawler) t &lt;- ContentScraper(n, CssPatterns = c("#product-title", ".authors", ".buying-price-val-number", ".buying-pricenew-val-number", ".publisher", ".isbn", ".pages2"), ExcludeCSSPat = c(".prodtitle-availibility", ".js-open-block-page_count"), ManyPerPattern = FALSE, PatternsName = c("title", "author", "price1", "price2", "publisher", "isbn", "page")) return(t) }</code> </pre> <br><p>  Bei all den positiven Eigenschaften hat die <code>ContentScraper</code> Funktion ein sehr schwerwiegendes Minus - die Arbeitsgeschwindigkeit. </p><br><p>  Rcrawler ContentScraper <code>ContentScraper</code> <code>Rcrawler</code> ohne paralleles Computing </p><br><div class="scrollable-table"><table><thead><tr><th>  der Benutzer </th><th>  das System </th><th>  bestanden </th></tr></thead><tbody><tr><td>  47,47 </td><td>  0,29 </td><td>  212,24 </td></tr></tbody></table></div><br><p>  Rcrawler ContentScraper- <code>ContentScraper</code> <code>Rcrawler</code> mithilfe von Parallel Computing </p><br><div class="scrollable-table"><table><thead><tr><th>  der Benutzer </th><th>  das System </th><th>  bestanden </th></tr></thead><tbody><tr><td>  0,01 </td><td>  0,00 </td><td>  67,97 </td></tr></tbody></table></div><br><p>  Daher sollte Rcrawler verwendet werden, wenn Sie die Site umgehen müssen, ohne zuvor URL-Adressen anzugeben, sowie mit einer kleinen Anzahl von Seiten.  In anderen Fällen überwiegt die langsame Geschwindigkeit alle möglichen Vorteile der Verwendung dieses Pakets. </p><br><p>  <em>Für Kommentare, Vorschläge und Beschwerden wäre ich dankbar</em> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Github-</a> Repository-Link <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Mein</a> Kreisprofil </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de464399/">https://habr.com/ru/post/de464399/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de464385/index.html">Python als ultimativer Fall von C ++. Teil 1/2</a></li>
<li><a href="../de464387/index.html">Russischer Fußabdruck in der skandinavischen Saga der Videospiele, Ende</a></li>
<li><a href="../de464391/index.html">10 interessante Berichte von Hacker-Konferenzen</a></li>
<li><a href="../de464393/index.html">Wie finde ich Programmierkurse und was sind die Beschäftigungsgarantien?</a></li>
<li><a href="../de464395/index.html">Blockchain RSA-basierter Zufall</a></li>
<li><a href="../de464403/index.html">So führen Sie ein Java-Projekt auf einem Shell-Runner aus, wenn Sie in ein GitLab-Repository pushen</a></li>
<li><a href="../de464405/index.html">Python als ultimativer Fall von C ++. Teil 2/2</a></li>
<li><a href="../de464407/index.html">Wie die weltweit größten Videoüberwachungssysteme funktionieren</a></li>
<li><a href="../de464409/index.html">Wie sich die Politik des 19. Jahrhunderts heute auf die Standorte von Rechenzentren auswirkte</a></li>
<li><a href="../de464411/index.html">PVS-Studio: Motor des Fortschritts</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>