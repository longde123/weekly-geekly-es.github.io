<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèº‚Äçüöí üõ•Ô∏è üòÇ Web Scraping in R, Teil 2. Beschleunigen Sie den Prozess durch paralleles Rechnen und Verwenden des Rcrawler-Pakets üî∫ üà≥ üòº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In einem fr√ºheren Artikel habe ich mithilfe von Scraping-Parsing Filmbewertungen von IMDB- und Kinopoisk-Websites gesammelt und verglichen. Repository...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Web Scraping in R, Teil 2. Beschleunigen Sie den Prozess durch paralleles Rechnen und Verwenden des Rcrawler-Pakets</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/464399/"><p><img src="https://habrastorage.org/webt/vy/vh/m_/vyvhm_gjoiuzkbfemd_0fsnbw74.png"></p><br><p>  In einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">fr√ºheren Artikel habe</a> ich mithilfe von Scraping-Parsing Filmbewertungen von IMDB- und Kinopoisk-Websites gesammelt und verglichen.  Repository auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Github</a> . </p><br><p>  Der Code hat seine Aufgabe gut erledigt, aber das Scraping wird oft verwendet, um nicht ein paar Seiten, sondern ein paar dreitausend zu "kratzen", und der Code aus dem vorherigen Artikel ist nicht f√ºr ein so "gro√ües" Scraping geeignet.  Genauer gesagt wird es nicht optimal sein.  Im Prinzip hindert Sie praktisch nichts daran, damit Tausende von Seiten zu crawlen.  Praktisch, weil Sie einfach nicht so viel Zeit haben <a name="habracut"></a></p><br><p><img src="https://habrastorage.org/webt/ik/-y/rk/ik-yrkuvryxwpdvidot66ijfzn8.jpeg"></p><br><p>  <em>Als ich mich entschied, <a href="">Scraping_imdb.R</a> zu verwenden, um 1000 Seiten zu crawlen</em> </p><br>
<h5 id="optimizaciya-koda-odnokratnoe-ispolzovanie-funkcii-read_html">  Codeoptimierung.  Eine einmalige Verwendung der Funktion <code>read_html</code> </h5><br><p>  In diesem Artikel werden 100 Links zu den Seiten des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Labyrinth-</a> Buchladens verwendet, um den Betrieb und die Geschwindigkeit des Codes zu √ºberpr√ºfen. </p><br><p>  Eine explizite √Ñnderung, die den Prozess beschleunigen kann, ist die einmalige Verwendung der "langsamsten" <code>read_html</code> - <code>read_html</code> .  Ich m√∂chte Sie daran erinnern, dass sie die HTML-Seite "liest".  In der ersten Version des Codes f√ºr <code>read_html</code> habe ich jedes Mal <code>read_html</code> wenn ich einen Wert erhalten wollte ( <code>read_html</code> , Jahr, Genre, Bewertung).  Jetzt wurden die Spuren dieser ‚ÄûSchande‚Äú von GitHuba gel√∂scht, aber es ist so.  Dies macht keinen Sinn, da die mit <code>read_html</code> erstellte <code>read_html</code> Informationen √ºber die gesamte Seite enth√§lt und es ausreicht, diese Variable der Funktion <code>html_nodes</code> und nicht jedes Mal mit dem Lesen von HTML zu beginnen, um unterschiedliche Daten daraus zu erhalten.  So k√∂nnen Sie Zeit proportional zur Anzahl der Werte sparen, die Sie erhalten m√∂chten.  Aus dem Labyrinth erhalte ich jeweils sieben Werte. Code, der nur einen einzigen Lesevorgang einer HTML-Seite verwendet, funktioniert etwa siebenmal schneller.  Nicht schlecht!  Aber bevor ich wieder "beschleunige", werde ich abschweifen und √ºber interessante Punkte sprechen, die sich beim Schaben von der Labyrinth-Website ergeben. </p><br><h5 id="osobennosti-skrepinga-stranic-na-labirinte">  Funktionen des Seitenkratzens im Labyrinth </h5><br><p>  In diesem Teil werde ich nicht auf das Verfahren zum Abrufen und L√∂schen der im vorherigen Artikel erw√§hnten Daten eingehen.  Ich werde nur die Momente erw√§hnen, die ich zum ersten Mal erlebt habe, als ich Code f√ºr das Scrapbooking eines Buchladens geschrieben habe. </p><br><p>  Zun√§chst ist die Struktur zu erw√§hnen.  Sie f√ºhlt sich nicht sehr wohl.  Im Gegensatz dazu geben beispielsweise auf der Read-Cities-Website in Abschnitten des Genres mit "leeren Filtern" nur 17 Seiten aus.  Nat√ºrlich passen nicht alle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">8011</a> B√ºcher des Genres "Contemporary Foreign Prose" darauf. </p><br><p>  Daher habe ich mir nichts Besseres ausgedacht, als die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://www.labirint.ru/books/</a> **** Links mit einer einfachen B√ºste zu umgehen.  Ehrlich gesagt ist die Methode nicht die beste (schon allein deshalb, weil die meisten "alten" B√ºcher au√üer dem Namen keine Informationen enthalten und daher praktisch nutzlos sind). Wenn also jemand eine elegantere L√∂sung anbietet, bin ich froh.  Aber ich fand heraus, dass es unter der stolzen ersten Nummer auf der Website des Labyrinths ein Buch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">"Wie man Mondschein macht" gibt.</a>  Leider ist es bereits unm√∂glich, dieses Wissensspeicher zu kaufen. </p><br><p>  Alle Adressen w√§hrend der Aufz√§hlung k√∂nnen in zwei Typen unterteilt werden: </p><br><ul><li>  Seiten, die existieren </li><li>  Seiten, die nicht existieren </li></ul><br><p>  Bestehende Seiten k√∂nnen wiederum in zwei Teile unterteilt werden: </p><br><ul><li>  Seiten, die alle notwendigen Informationen enthalten </li><li>  Seiten, die nicht alle erforderlichen Informationen enthalten </li></ul><br><p>  Am Ende habe ich eine Datentabelle mit sieben Spalten: </p><br><ol><li>  ISBN - ISBN-Buchnummer </li><li>  PREIS - Buchpreis </li><li>  NAME - Buchtitel </li><li>  AUTOR - Autor des Buches </li><li>  VERLAG - Verlag </li><li>  JAHR - Erscheinungsjahr </li><li>  SEITE - Anzahl der Seiten </li></ol><br><p>  Mit den Seiten mit vollst√§ndigen Informationen ist alles klar, sie erfordern keine √Ñnderungen im Vergleich zum Code f√ºr Filmseiten. </p><br><p>  Seiten, auf denen einige Daten nicht verf√ºgbar sind, sind mit ihnen nicht so einfach.  Eine Suche auf der Seite gibt nur die gefundenen Werte zur√ºck und die Ausgabel√§nge verringert sich um die Anzahl der Elemente, die nicht gefunden werden.  Dies wird die gesamte Struktur brechen.  Um dies zu vermeiden, wurde jedem Argument ein if ... else-Konstrukt hinzugef√ºgt, das die L√§nge des nach Verwendung der Funktion <code>html_nodes</code> erhaltenen Vektors <code>html_nodes</code> Wenn es Null ist, wird <code>NA</code> , um <code>html_nodes</code> zu vermeiden. </p><br><pre> <code class="plaintext hljs"> PUBLISHER &lt;- unlist(lapply(list_html, function(n){ publishing &lt;- if(n != "NA") { publishing_html &lt;- html_nodes(n, ".publisher a") publishing &lt;- if(length(publishing_html) == 0){ NA } else { publishing &lt;- html_text(publishing_html) } } else { NA } }))</code> </pre> <br><p>  Aber wie Sie hier sehen k√∂nnen, sind es bis zu zwei Wenns und bis zu zwei andere.  Nur die "internen" if..esle sind f√ºr die L√∂sung des oben beschriebenen Problems relevant.  Exterieur l√∂st das Problem mit nicht vorhandenen Seiten. </p><br><p>  Seiten, die einfach nicht die meisten Probleme haben.  Wenn Werte auf Seiten mit fehlenden Daten verschoben werden und die Eingabe <code>read_html</code> nicht vorhandene Seite <code>read_html</code> , gibt die Funktion <code>read_html</code> Fehler aus und der Code wird nicht mehr ausgef√ºhrt.  Weil  Irgendwie ist es nicht m√∂glich, solche Seiten im Voraus zu erkennen. Es muss sichergestellt werden, dass der Fehler nicht den gesamten Prozess stoppt. </p><br><p>  Die <code>possibly</code> des <code>possibly</code> Pakets hilft uns dabei.  Die Bedeutung von <code>possibly</code> (au√üer <code>possibly</code> <code>quietly</code> und <code>safely</code> ) besteht darin, die gedruckte Ausgabe von Nebenwirkungen (z. B. Fehler) durch einen Wert zu ersetzen, der zu uns passt.  <code>possibly</code> eine <code>possibly(.f, otherwise)</code> Struktur <code>possibly(.f, otherwise)</code> und wenn ein Fehler im Code auftritt, verwendet er den Standardwert (ansonsten), anstatt die Ausf√ºhrung zu stoppen.  In unserem Fall sieht es so aus: </p><br><pre> <code class="plaintext hljs">book_html &lt;- possibly(read_html, "NA")(n)</code> </pre> <br><p>  n ist eine Liste der Adressen der Seiten der Site, die wir abgekratzt haben.  Bei der Ausgabe erhalten wir eine Liste der L√§nge n, in der Elemente von vorhandenen Seiten in der "normalen" Form vorliegen, um die Funktion <code>read_html</code> , und Elemente von nicht vorhandenen Seiten aus dem Zeichenvektor "NA" bestehen.  Bitte beachten Sie, dass der Standardwert ein Zeichenvektor sein muss, da wir in Zukunft darauf verweisen werden.  Wenn wir nur <code>NA</code> schreiben, wie im PUBLISHER-Codeteil, ist dies nicht m√∂glich.  Um Verwirrung zu vermeiden, k√∂nnen Sie den ansonsten angegebenen Wert von NA in einen anderen √§ndern. </p><br><p>  Und jetzt zur√ºck zum Code, um den Namen des Herausgebers zu erhalten.  Extern, wenn ... sonst f√ºr die gleichen Zwecke wie intern ben√∂tigt wird, jedoch in Bezug auf nicht vorhandene Seiten.  Wenn die Variable <code>book_html</code> "NA" ist, ist jeder der "abgekratzten" Werte auch gleich <code>NA</code> (hier k√∂nnen Sie bereits die "echte" <code>NA</code> anstelle eines symbolischen Betr√ºgers verwenden).  Am Ende erhalten wir also eine Tabelle mit der folgenden Form: </p><br><div class="scrollable-table"><table><thead><tr><th>  ISBN </th><th>  PREIS </th><th>  NAME </th><th>  AUTOR </th><th>  VERLAG </th><th>  Jahr </th><th>  Seite </th></tr></thead><tbody><tr><td>  4665305770322 </td><td>  1488 </td><td>  Set String Art "Netter Welpe" (30 * 30 cm) (DH6021) </td><td>  NA </td><td>  Ingwerkatze </td><td>  2019 </td><td>  NA </td></tr><tr><td>  NA </td><td>  NA </td><td>  NA </td><td>  NA </td><td>  NA </td><td>  NA </td><td>  NA </td></tr><tr><td>  9785171160814 </td><td>  273 </td><td>  Arkady Averchenko: Lustige Geschichten f√ºr Kinder </td><td>  Autor: Averchenko Arkady Timofeevich, K√ºnstler: Vlasova Anna Yulievna </td><td>  Kind </td><td>  2019 </td><td>  288 </td></tr></tbody></table></div><br><p>  Nun zur√ºck mit der Beschleunigung des Kratzprozesses. </p><br><h5 id="parallelnoe-vychislenie-v-r-sravnenie-skorosti-i-podvodnye-kamni-pri-ispolzovanii-funkcii-read_html">  Paralleles Rechnen in R. Geschwindigkeitsvergleich und Fallstricke bei Verwendung der Funktion <code>read_html</code> </h5><br><p>  Standardm√§√üig werden alle Berechnungen in R auf demselben Prozessorkern ausgef√ºhrt.  Und w√§hrend dieser ungl√ºckliche Kern im Gesicht arbeitet und Daten von Tausenden von Seiten f√ºr uns ‚Äûkratzt‚Äú, ‚Äûk√ºhlt‚Äú sich der Rest unserer Kameraden ab und f√ºhrt einige andere Aufgaben aus.  Die Verwendung von Parallel Computing hilft dabei, alle Prozessorkerne f√ºr die Verarbeitung / den Empfang von Daten zu gewinnen, was den Prozess beschleunigt. </p><br><p>  Ich werde nicht weiter auf das Design des parallelen Rechnens auf R eingehen. Sie k√∂nnen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> zum Beispiel mehr dar√ºber lesen.  Ich habe die Parallelit√§t von R so verstanden, dass Kopien von R in separaten Clustern entsprechend der Anzahl der angegebenen Kernel erstellt werden, die √ºber <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sockets</a> miteinander interagieren. </p><br><p>  Ich erz√§hle Ihnen von dem Fehler, den ich bei der Verwendung von Parallel Computing gemacht habe.  Urspr√ºnglich war mein Plan folgender: Mit Parallel Computing erhalte ich eine Liste mit 100 "gelesenen" <code>read_html</code> Seiten und <code>read_html</code> dann im normalen Modus nur die Daten, die ich ben√∂tige.  Zuerst lief alles gut: Ich bekam eine Liste und verbrachte viel weniger Zeit damit als im normalen Modus R. Aber erst als ich versuchte, mit dieser Liste zu interagieren, erhielt ich eine Fehlermeldung: </p><br><pre> <code class="plaintext hljs">Error: external pointer is not valid</code> </pre> <br><p>  Infolgedessen erkannte ich das Problem, indem ich mir Beispiele im Internet ansah, und fand danach nach dem Gesetz der Gemeinheit Henrik Bengtssons Erkl√§rung in der Vignette f√ºr das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><strong>zuk√ºnftige</strong></a> Paket.  Tatsache ist, dass die XML-Funktionen des <code>xml2</code> Pakets nicht exportierbare Objekte sind. <br>  )  Diese Objekte sind an diese R-Sitzung ‚Äûgebunden‚Äú und k√∂nnen nicht auf einen anderen Prozess √ºbertragen werden, was ich versucht habe.  Daher sollte die beim parallelen Rechnen gestartete Funktion einen ‚Äûvollst√§ndigen Zyklus‚Äú von Vorg√§ngen enthalten: Lesen einer HTML-Seite, Empfangen und Bereinigen der erforderlichen Daten. </p><br><p>  Das Erstellen von Parallel Computing selbst erfordert nicht viel Zeit und Codezeilen.  Als erstes m√ºssen Sie die Bibliotheken herunterladen.  Das Github-Repository gibt an, welche Pakete f√ºr welche Methoden ben√∂tigt werden.  Hier zeige ich paralleles Rechnen mit der <code>parLapply</code> Funktion des <code>parallel</code> Pakets.  F√ºhren <code>doParallel</code> einfach <code>doParallel</code> ( <code>parallel</code> wird in diesem Fall automatisch gestartet).  Wenn Sie die Anzahl der Kerne Ihres Prozessors pl√∂tzlich nicht mehr kennen oder vergessen haben, ermitteln Sie, wie viele von ihnen <code>detectCores</code> werden <code>detectCores</code> </p><br><pre> <code class="plaintext hljs"># detectCores - ,     number_cl &lt;- detectCores()</code> </pre> <br><p>  Erstellen Sie als N√§chstes parallele Kopien von R: </p><br><pre> <code class="plaintext hljs"> # makePSOCKcluster -    R,    cluster &lt;- makePSOCKcluster(number_cl) registerDoParallel(cluster)</code> </pre> <br><p>  Jetzt schreiben wir eine Funktion, die alle erforderlichen Prozeduren ausf√ºhrt.  Ich stelle das seitdem fest  Es werden neue Sitzungen erstellt. R-Pakete, deren Funktionen in unserer eigenen Funktion verwendet werden, sollten in den Hauptteil der Funktion geschrieben werden.  In <a href="">spider_parallel.R wird</a> das <code>stringr</code> Paket dadurch zweimal ausgef√ºhrt: zuerst zum <code>stringr</code> der <code>stringr</code> und dann zum L√∂schen der Daten. </p><br><p>  Und dann unterscheidet sich das Verfahren fast nicht von der √ºblichen <code>lapply</code> Funktion.  In <code>parLapply</code> stellen wir eine Liste von Adressen, unsere eigene Funktion und als einzige Erg√§nzung eine Variable mit den von uns erstellten Clustern <code>parLapply</code> . </p><br><pre> <code class="plaintext hljs"># parLapply -  lapply     big_list &lt;- parLapply(cluster, list_url, scraping_parellel_func) #    stopCluster(cluster)</code> </pre> <br><p>  Das ist alles, jetzt bleibt es Zeit, die verbrachte Zeit zu vergleichen. </p><br><h5 id="sravnenie-skorosti-posledovatelnogo-i-parallelnogo-vychisleniya">  Vergleich der seriellen und parallelen Rechengeschwindigkeit </h5><br><p>  Dies wird der k√ºrzeste Punkt sein.  Paralleles Rechnen war f√ºnfmal schneller als gew√∂hnlich: </p><br><p>  Scraping-Geschwindigkeit ohne Parallel-Computing </p><br><div class="scrollable-table"><table><thead><tr><th>  der Benutzer </th><th>  das System </th><th>  bestanden </th></tr></thead><tbody><tr><td>  13.57 </td><td>  0,40 </td><td>  112,84 </td></tr></tbody></table></div><br><p>  Scraping-Geschwindigkeit mit Parallel Computing </p><br><div class="scrollable-table"><table><thead><tr><th>  der Benutzer </th><th>  das System </th><th>  bestanden </th></tr></thead><tbody><tr><td>  0,14 </td><td>  0,05 </td><td>  21.12 </td></tr></tbody></table></div><br><p>  Was soll ich sagen?  Paralleles Rechnen kann viel Zeit sparen, ohne dass Schwierigkeiten beim Erstellen des Codes entstehen.  Mit zunehmender Anzahl von Kernen steigt die Geschwindigkeit fast proportional zu ihrer Anzahl.  Mit einigen √Ñnderungen haben wir den Code zuerst siebenmal beschleunigt (die Berechnung von <code>read_html</code> bei jedem Schritt <code>read_html</code> ) und dann weitere f√ºnfmal mithilfe paralleler Berechnungen.  Spider-Skripte <a href="">ohne</a> paralleles Rechnen mit den Paketen <a href=""><code>parallel</code></a> und <a href=""><code>foreach</code></a> befinden sich im Repository von Github. </p><br><h5 id="nebolshoy-obzor-paketa-rcrawler-sravnenie-skorosti">  Eine kleine √úbersicht √ºber das <code>Rcrawler</code> Paket.  Geschwindigkeitsvergleich. </h5><br><p>  Es gibt verschiedene andere M√∂glichkeiten, HTML-Seiten in R zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><strong>verschrotten</strong></a> , aber ich werde mich auf das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><strong>Rcrawler-</strong></a> Paket konzentrieren.  Das Unterscheidungsmerkmal von anderen Tools in der R-Sprache ist die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">M√∂glichkeit,</a> Websites <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zu</a> crawlen.  Sie k√∂nnen die gleichnamige <code>Rcrawler</code> Funktion auf die <code>Rcrawler</code> Adresse setzen und methodisch Seite f√ºr Seite die gesamte Site umgehen.  <code>Rcrawler</code> verf√ºgt √ºber viele Argumente zum Einrichten der Suche (z. B. k√∂nnen Sie nach Schl√ºsselw√∂rtern, Sektoren der Site suchen (n√ºtzlich, wenn die Site aus einer gro√üen Anzahl von Seiten besteht), Suchtiefe, Ignorieren von URL-Parametern, die doppelte Seiten erstellen, und vieles mehr Die Funktionen wurden bereits f√ºr parallele Berechnungen festgelegt, die durch die Argumente <code>no_cores</code> (Anzahl der beteiligten Prozessorkerne) und <code>no_conn</code> (Anzahl der parallelen Anforderungen) angegeben werden. </p><br><p>  In unserem Fall gibt es beim Scraping von den angegebenen Adressen eine <code>ContentScraper</code> Funktion.  Standardm√§√üig wird kein paralleles Rechnen verwendet, daher m√ºssen Sie alle oben beschriebenen Manipulationen wiederholen.  Ich mochte die Funktion selbst - sie bietet viele Optionen zum Einrichten des Scrapings und ist auf einer intuitiven Ebene gut verstanden.  Auch hier k√∂nnen Sie if..else nicht f√ºr fehlende Seiten oder fehlende Werte verwenden, as  Die Funktionsausf√ºhrung wird nicht gestoppt. </p><br><pre> <code class="plaintext hljs">#   ContentScraper: # CssPatterns -    CSS    . # ExcludeCSSPat -    CSS ,    . # ,   CSS     CSS ,    . # ManyPerPattern -  FALSE,       , #  .  TRUE,     ,   . # PatternsName -      .   #   c  ,      t_func &lt;- function(n){ library(Rcrawler) t &lt;- ContentScraper(n, CssPatterns = c("#product-title", ".authors", ".buying-price-val-number", ".buying-pricenew-val-number", ".publisher", ".isbn", ".pages2"), ExcludeCSSPat = c(".prodtitle-availibility", ".js-open-block-page_count"), ManyPerPattern = FALSE, PatternsName = c("title", "author", "price1", "price2", "publisher", "isbn", "page")) return(t) }</code> </pre> <br><p>  Bei all den positiven Eigenschaften hat die <code>ContentScraper</code> Funktion ein sehr schwerwiegendes Minus - die Arbeitsgeschwindigkeit. </p><br><p>  Rcrawler ContentScraper <code>ContentScraper</code> <code>Rcrawler</code> ohne paralleles Computing </p><br><div class="scrollable-table"><table><thead><tr><th>  der Benutzer </th><th>  das System </th><th>  bestanden </th></tr></thead><tbody><tr><td>  47,47 </td><td>  0,29 </td><td>  212,24 </td></tr></tbody></table></div><br><p>  Rcrawler ContentScraper- <code>ContentScraper</code> <code>Rcrawler</code> mithilfe von Parallel Computing </p><br><div class="scrollable-table"><table><thead><tr><th>  der Benutzer </th><th>  das System </th><th>  bestanden </th></tr></thead><tbody><tr><td>  0,01 </td><td>  0,00 </td><td>  67,97 </td></tr></tbody></table></div><br><p>  Daher sollte Rcrawler verwendet werden, wenn Sie die Site umgehen m√ºssen, ohne zuvor URL-Adressen anzugeben, sowie mit einer kleinen Anzahl von Seiten.  In anderen F√§llen √ºberwiegt die langsame Geschwindigkeit alle m√∂glichen Vorteile der Verwendung dieses Pakets. </p><br><p>  <em>F√ºr Kommentare, Vorschl√§ge und Beschwerden w√§re ich dankbar</em> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Github-</a> Repository-Link <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Mein</a> Kreisprofil </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de464399/">https://habr.com/ru/post/de464399/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de464385/index.html">Python als ultimativer Fall von C ++. Teil 1/2</a></li>
<li><a href="../de464387/index.html">Russischer Fu√üabdruck in der skandinavischen Saga der Videospiele, Ende</a></li>
<li><a href="../de464391/index.html">10 interessante Berichte von Hacker-Konferenzen</a></li>
<li><a href="../de464393/index.html">Wie finde ich Programmierkurse und was sind die Besch√§ftigungsgarantien?</a></li>
<li><a href="../de464395/index.html">Blockchain RSA-basierter Zufall</a></li>
<li><a href="../de464403/index.html">So f√ºhren Sie ein Java-Projekt auf einem Shell-Runner aus, wenn Sie in ein GitLab-Repository pushen</a></li>
<li><a href="../de464405/index.html">Python als ultimativer Fall von C ++. Teil 2/2</a></li>
<li><a href="../de464407/index.html">Wie die weltweit gr√∂√üten Video√ºberwachungssysteme funktionieren</a></li>
<li><a href="../de464409/index.html">Wie sich die Politik des 19. Jahrhunderts heute auf die Standorte von Rechenzentren auswirkte</a></li>
<li><a href="../de464411/index.html">PVS-Studio: Motor des Fortschritts</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>