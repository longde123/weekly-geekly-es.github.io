<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü¶é üë®üèΩ‚Äçüî¨ üì≥ AI-basierte Fotorestaurierung üçí üè´ üë®üèæ‚Äçü§ù‚Äçüë®üèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo allerseits! Ich bin ein Forschungsingenieur im Computer Vision Team der Mail.ru Group. In diesem Artikel werde ich eine Geschichte dar√ºber erz√§h...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>AI-basierte Fotorestaurierung</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/459696/"><img src="https://habrastorage.org/webt/ya/mt/mm/yamtmmcino7skf3gyqzpsrgqla4.jpeg"><br><br>  Hallo allerseits!  Ich bin ein Forschungsingenieur im Computer Vision Team der Mail.ru Group.  In diesem Artikel werde ich eine Geschichte dar√ºber erz√§hlen, wie wir ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AI-basiertes Fotorestaurierungsprojekt</a> f√ºr alte Milit√§rfotos erstellt haben.  Was ist ‚ÄûFotorestaurierung‚Äú?  Es besteht aus drei Schritten: <br><br><ul><li>  wir finden alle Bildfehler: Br√ºche, Kratzer, L√∂cher; <br></li><li>  Wir malen die entdeckten Fehler basierend auf den Pixelwerten um sie herum. <br></li><li>  Wir f√§rben das Bild. <br></li></ul><br>  Au√üerdem beschreibe ich jeden Schritt der Fotowiederherstellung und erkl√§re Ihnen, wie wir unsere Daten erhalten haben, welche Netze wir trainiert haben, was wir erreicht haben und welche Fehler wir gemacht haben. <br><a name="habracut"></a><br><h1>  Auf der Suche nach M√§ngeln </h1><br>  Wir m√∂chten alle Pixel in Bezug auf Fehler in einem hochgeladenen Foto finden.  Zuerst m√ºssen wir herausfinden, welche Art von Bildern die Leute hochladen werden.  Wir sprachen mit den Gr√ºndern des Projekts "Immortal Regiment", einer nichtkommerziellen Organisation, die die alten Fotos des Zweiten Weltkriegs speichert und ihre Daten mit uns teilte.  Bei der Analyse haben wir festgestellt, dass die meisten Personen Einzel- oder Gruppenportr√§ts mit einer moderaten bis gro√üen Anzahl von Fehlern hochladen. <br><br>  Dann mussten wir ein Trainingsset sammeln.  Der Trainingssatz f√ºr eine Segmentierungsaufgabe besteht aus einem Bild und einer Maske, in der alle Fehler markiert sind.  Der einfachste Weg, dies zu tun, besteht darin, die Pr√ºfer die Segmentierungsmasken erstellen zu lassen.  Nat√ºrlich wissen die Leute sehr gut, wie man Fehler findet, aber das w√ºrde zu lange dauern. <br><br><img src="https://habrastorage.org/webt/yg/6y/iu/yg6yiue75v7msnxyffapttyugs8.jpeg"><br><br>  Es kann eine Stunde oder den ganzen Arbeitstag dauern, bis die fehlerhaften Pixel in einem Foto markiert sind.  Daher ist es nicht einfach, in wenigen Wochen einen Trainingssatz mit mehr als 100 Bildern zu sammeln.  Aus diesem Grund haben wir versucht, unsere Daten zu erweitern und unsere eigenen Fehler zu erstellen: Wir haben ein gutes Foto gemacht, Fehler mithilfe von zuf√§lligen Schritten auf dem Bild hinzugef√ºgt und am Ende eine Maske erhalten, die die Bildteile mit den Fehlern zeigt.  Ohne Erweiterungen haben wir 68 manuell beschriftete Fotos im Trainingssatz und 11 Fotos im Validierungssatz. <br><br>  Der beliebteste Segmentierungsansatz: Nehmen Sie Unet mit einem vorab trainierten Encoder und minimieren Sie die Summe aus BCE ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">bin√§re Kreuzentropie</a> ) und DICE ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">S√∏rensen - W√ºrfelkoeffizient</a> ). <br><br>  Welche Probleme treten auf, wenn wir diesen Segmentierungsansatz f√ºr unsere Aufgabe verwenden? <br><br><ul><li>  Selbst wenn es so aussieht, als ob das Foto Unmengen von Fehlern enth√§lt, dass es sehr alt und sch√§big ist, ist der Bereich mit Fehlern immer noch viel kleiner als der unbesch√§digte.  Um dieses Problem zu l√∂sen, k√∂nnen wir das positive Klassengewicht in BCE erh√∂hen.  Ein optimales Gewicht w√§re das Verh√§ltnis von sauberen zu fehlerhaften Pixeln. <br></li><li>  Das zweite Problem besteht darin, dass bei Verwendung eines sofort einsatzbereiten Unet mit vorab trainiertem Encoder (z. B. Albunet-18) viele Positionsdaten verloren gehen.  Die erste Schicht von Albunet-18 besteht aus einer Faltung mit einem Kern 5 und einem Schritt, der zwei entspricht.  Dadurch kann das Netz schnell arbeiten.  Wir haben die Nettobetriebszeit abgewogen, um eine bessere Fehlerlokalisierung zu erzielen: Wir haben das maximale Pooling nach der ersten Schicht entfernt, den Schritt auf 1 verringert und den Faltungskern auf 3 verringert. <br></li><li>  Wenn wir mit kleinen Bildern arbeiten, indem wir sie beispielsweise auf 256 x 256 oder 512 x 512 Pixel komprimieren, verschwinden kleine Fehler aufgrund der Interpolation.  Daher m√ºssen wir mit gr√∂√üeren Bildern arbeiten.  Derzeit segmentieren wir Fehler in Fotos in der Gr√∂√üe 1024 x 1024 in der Produktion.  Deshalb mussten wir das Netz auf Big Image Crops trainieren.  Dies f√ºhrt jedoch zu Problemen mit einer kleinen Stapelgr√∂√üe auf einer einzelnen GPU. <br></li><li>  W√§hrend des Trainings k√∂nnen wir ungef√§hr 20 Bilder auf eine GPU passen.  Aus diesem Grund erhalten wir ungenaue Mittel- und Standardabweichungswerte in BatchNorm-Layern.  Wir k√∂nnen dieses Problem mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">In-Place BatchNorm</a> l√∂sen, das einerseits Speicherplatz spart und andererseits √ºber eine synchronisierte BatchNorm-Version verf√ºgt, die Statistiken √ºber alle GPUs hinweg synchronisiert.  Jetzt berechnen wir den Mittelwert und die Standardabweichung nicht f√ºr 20 Bilder auf einer einzelnen GPU, sondern f√ºr 80 Bilder von 4 GPUs.  Dies verbessert die Nettokonvergenz. <br></li></ul><br>  Nachdem wir das BCE-Gewicht erh√∂ht, die Architektur ge√§ndert und In-Place-BatchNorm verwendet haben, haben wir die Segmentierung verbessert.  Es w√ºrde jedoch nicht zu viel kosten, etwas noch besser zu machen, indem Sie Test Time Augmentation hinzuf√ºgen.  Wir k√∂nnen das Netz einmal auf einem Eingabebild ausf√ºhren, es dann spiegeln und das Netz erneut ausf√ºhren, um alle kleinen Fehler zu finden. <br><br><img src="https://habrastorage.org/webt/3c/vj/g0/3cvjg04qc_nqsl8lop44jvtjfym.jpeg"><br><br>  Das Netz konvergiert in 18 Stunden auf vier GeForce 1080Ti.  Inferenz dauert 290 ms.  Es ist ziemlich lang, aber das ist der Preis f√ºr unsere √ºberdurchschnittliche Leistung.  Validierungsw√ºrfel sind gleich 0,35 und ROCAUC - 0,93. <br><br><h1>  Bildinpainting </h1><br>  Gleiches gilt f√ºr die Segmentierungsaufgabe, die wir f√ºr Unet verwendet haben.  Zum Malen haben wir ein Originalbild und eine Maske hochgeladen, in der wir den gesamten sauberen Bereich mit Einsen und Nullen markiert haben - alle Pixel, die wir malen m√∂chten.  So haben wir Daten gesammelt: F√ºr jedes Foto aus einem Open-Source-Bilddatensatz, z. B. OpenImagesV4, f√ºgen wir die Fehler hinzu, die denen im wirklichen Leben √§hneln.  Dann hatten wir das Netz trainiert, um die fehlenden Teile wiederherzustellen. <br><br>  Wie k√∂nnen wir Unet f√ºr diese Aufgabe √§ndern? <br><br>  Wir k√∂nnen anstelle einer urspr√ºnglichen teilweise Faltung verwenden.  Die Idee ist, dass wir beim Falten eines Bereichs mit einem Kernel die Fehlerpixelwerte nicht ber√ºcksichtigen.  Dies macht das Inpainting pr√§ziser.  Wir zeigen Ihnen ein Beispiel aus dem aktuellen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NVIDIA-Papier</a> .  Sie verwendeten Unet mit einer standardm√§√üigen zweidimensionalen Faltung im mittleren Bild und einer teilweisen Faltung - im Bild rechts. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ec1/5ba/bdb/ec15babdbf1cd219be4a5e3ffa4ae50f.jpg"><br><br>  Wir haben das Netz f√ºnf Tage lang trainiert.  Am letzten Tag haben wir BatchNorms eingefroren, um die R√§nder des lackierten Teils weniger sichtbar zu machen. <br><br>  Die Verarbeitung eines 512 x 512-Bildes dauert netto 50 ms.  Validierung PSNR entspricht 26,4.  Sie k√∂nnen sich jedoch nicht vollst√§ndig auf die Metriken in dieser Aufgabe verlassen.  Um das beste Modell auszuw√§hlen, f√ºhren wir mehrere gute Modelle f√ºr Bewertungsbilder aus, anonymisieren die Ergebnisse und stimmen dann f√ºr diejenigen ab, die uns am besten gefallen haben.  So haben wir unser endg√ºltiges Modell ausgew√§hlt. <br><br>  Ich habe bereits erw√§hnt, dass wir den sauberen Bildern k√ºnstlich einige Fehler hinzugef√ºgt haben.  Sie sollten w√§hrend des Trainings immer die maximale Gr√∂√üe der hinzugef√ºgten Fehler verfolgen.  In einem Fall, in dem Sie ein Bild mit einem sehr gro√üen Fehler in das Netz einspeisen, das in der Trainingsphase noch nie behandelt wurde, l√§uft das Netz aus und f√ºhrt zu einem nicht zutreffenden Ergebnis.  Wenn Sie daher gro√üe Fehler beheben m√ºssen, erweitern Sie Ihr Trainingsset um diese. <br><br>  Hier ist das Beispiel, wie unser Algorithmus funktioniert: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c48/2cd/253/c482cd253865ee12a834475a2e30d619.jpg"><br><br><h1>  F√§rbung </h1><br>  Wir haben die M√§ngel segmentiert und lackiert;  der dritte Schritt - Farbrekonstruktion.  Wie ich bereits sagte, gibt es viele Einzel- und Gruppenportr√§ts unter den Fotos des Unsterblichen Regiments.  Wir wollten, dass unser Netz gut mit ihnen zusammenarbeitet.  Wir haben uns f√ºr eine eigene Farbgebung entschieden, da keiner der vorhandenen Dienste die Portr√§ts schnell und effizient f√§rben konnte.  Wir m√∂chten, dass unsere kolorierten Fotos glaubw√ºrdiger sind. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cec/b9a/b6c/cecb9ab6c8e1b76b567f49eac1261957.jpg"><br><br>  GitHub verf√ºgt √ºber ein beliebtes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Repository</a> f√ºr die Farbf√§rbung von Fotos.  Es macht einen guten Job, hat aber immer noch einige Probleme.  Zum Beispiel neigt es dazu, Kleidung blau zu malen.  Deshalb haben wir es auch abgelehnt. <br><br>  Deshalb haben wir uns entschlossen, einen Algorithmus f√ºr die Bildf√§rbung zu erstellen.  Die naheliegendste Idee: Nehmen Sie ein Schwarzwei√übild auf und sagen Sie drei Kan√§le voraus: Rot, Gr√ºn und Blau.  Wir k√∂nnen unsere Arbeit jedoch erleichtern: Arbeiten Sie nicht mit der RGB-Farbdarstellung, sondern mit der YCbCr-Farbdarstellung.  Die Y-Komponente ist die Helligkeit (Luma).  Ein hochgeladenes Schwarzwei√übild ist ein Y-Kanal, und wir werden es wiederverwenden.  Jetzt m√ºssen wir Cb und Cr vorhersagen: Cb ist der Unterschied zwischen blauer Farbe und Helligkeit und Cr - der Unterschied zwischen roter Farbe und Helligkeit. <br><br><img src="https://habrastorage.org/webt/yo/au/zi/yoauzi06k3bd0uyod2rjnpxgvms.jpeg"><br><br>  Warum haben wir uns f√ºr die YCbCr-Darstellung entschieden?  Ein menschliches Auge reagiert empfindlicher auf Helligkeits√§nderungen als auf Farb√§nderungen.  Aus diesem Grund verwenden wir die Y-Komponente (Helligkeit), f√ºr die ein menschliches Auge am empfindlichsten ist, wieder und sagen Cb und Cr voraus, mit denen wir m√∂glicherweise einen Fehler machen, da wir Farbfehler nicht sehr gut bemerken k√∂nnen.  Diese spezielle Eigenschaft wurde zu Beginn des Farbfernsehens h√§ufig verwendet, als die Kanalkapazit√§t nicht ausreichte, um alle Farben zu √ºbertragen.  Das Bild wurde in YCbCr unver√§ndert zur Y-Komponente √ºbertragen, und Cb und Cr wurden um die H√§lfte reduziert. <br><br><h1>  So erstellen Sie eine Basislinie </h1><br>  Wir k√∂nnen Unet mit einem vorab trainierten Encoder verwenden und den L1-Verlust zwischen den vorhandenen und den vorhergesagten CbCr-Werten minimieren.  Wir m√∂chten Portr√§ts ausmalen und ben√∂tigen daher neben OpenImages-Fotos mehr aufgabenspezifische Fotos. <br><br>  Wo k√∂nnen wir kolorierte Fotos von Menschen in Milit√§runiform bekommen?  Es gibt Leute im Internet, die alte Fotos als Hobby oder zu einem Preis kolorieren.  Sie tun es sehr sorgf√§ltig und versuchen sehr genau zu sein.  Wenn sie eine Uniform, Schulterklappen und Medaillen ausmalen, beziehen sie sich auf die Archivmaterialien, sodass die Ergebnisse ihrer Arbeit vertrauensw√ºrdig sind.  Insgesamt haben wir 200 manuell kolorierte Bilder mit Personen in Milit√§runiform verwendet. <br><br>  Die andere n√ºtzliche Datenquelle ist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die</a> Website der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Roten Armee der Arbeiter und Bauern</a> .  Einer seiner Gr√ºnder lie√ü sich in nahezu jeder sowjetischen Uniform des Zweiten Weltkriegs fotografieren. <br><br><img src="https://habrastorage.org/webt/yh/b7/u7/yhb7u74fa3feihqo0k-jpqcyxgk.jpeg"><br><br>  In einigen Bildern ahmte er die Posen von Menschen aus den ber√ºhmten Archivfotos nach.  Es ist gut, dass seine Bilder einen wei√üen Hintergrund haben: Dadurch konnten wir die Daten sehr gut erweitern, indem wir verschiedene nat√ºrliche Objekte in den Hintergrund einf√ºgten.  Wir haben auch einige regelm√§√üige Portr√§ts verwendet, die durch Abzeichen und andere Attribute aus der Kriegszeit erg√§nzt wurden. <br><br>  Wir haben AlbuNet-50 trainiert - es ist ein Unet, das vorab geschultes ResNet-50 als Encoder verwendet.  Das Netz lieferte ad√§quate Ergebnisse: Die Haut war rosa, die Augen - grau-blau, die Schulterklappen - gelblich.  Das Problem war jedoch, dass einige Bereiche auf dem Foto unber√ºhrt bleiben.  Dies wurde durch die Tatsache verursacht, dass laut Fehler L1 ein solches Optimum gefunden wird, bei dem es besser ist, nichts zu tun, als zu versuchen, eine Farbe vorherzusagen. <br><br><img src="https://habrastorage.org/webt/ov/zh/bn/ovzhbnv-6ch0nnoa4fdbh3nygym.jpeg"><br>  <i>Wir vergleichen unser Ergebnis mit einem Foto von Ground Truth - einer manuellen Kolorierung von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Klimbim</a></i> <br><br>  Wie k√∂nnen wir dieses Problem l√∂sen?  Wir brauchen einen Diskriminator: ein neuronales Netzwerk, das ein Bild empf√§ngt und uns sagt, ob es realistisch aussieht oder nicht.  Eines der Bilder unten ist manuell eingef√§rbt und das andere - von unserem Generator AlbuNet-50.  Wie unterscheidet der Mensch manuell und automatisch farbige Fotos?  Durch das Betrachten von Details.  K√∂nnen Sie anhand unserer Basisl√∂sung erkennen, wo sich das automatisch eingef√§rbte Foto befindet? <br><br><img src="https://habrastorage.org/webt/fk/er/n_/fkern_az5kgkgr2kwamcoxr_gtg.jpeg"><br><br><div class="spoiler">  <b class="spoiler_title">Antwort</b> <div class="spoiler_text">  Das Bild links wird manuell und rechts automatisch eingef√§rbt. </div></div><br>  Wir verwenden den Diskriminator aus dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Self-Attention GAN-</a> Papier.  Es ist ein kleines Faltungsnetz mit sogenannter Selbstaufmerksamkeit in den obersten Schichten.  Dadurch k√∂nnen wir den Bilddetails "mehr Aufmerksamkeit schenken".  Wir verwenden auch die spektrale Normalisierung.  Weitere Informationen finden Sie im oben genannten Artikel.  Wir haben das Netz mit einer Kombination aus L1-Verlust und Verlust durch den Diskriminator trainiert.  Jetzt f√§rbt das Netz die Bilddetails besser ein und der Hintergrund sieht konsistenter aus.  Ein weiteres Beispiel: Auf der linken Seite ist die Arbeit von net nur mit L1-Verlust trainiert;  rechts - mit einer Kombination von L1-Diskriminatorverlusten. <br><br><img src="https://habrastorage.org/webt/nd/3p/91/nd3p91aw1mzzoidhra1egef3zki.jpeg"><br><br>  Der Trainingsprozess mit vier GeForce 1080Ti dauerte zwei Tage.  Die Verarbeitung eines 512 x 512-Bildes dauert netto 30 ms.  Validierung MSE - 34.4.  Genau wie beim Inpainting m√∂chten Sie sich nicht auf Metriken verlassen.  Aus diesem Grund haben wir sechs Modelle mit den besten Validierungsmetriken ausgew√§hlt und blind f√ºr das beste Modell gestimmt. <br><br>  Als wir bereits ein Produktionssystem erstellt und eine Website gestartet haben, haben wir weiter experimentiert und sind zu dem Schluss gekommen, dass wir nicht den L1-Verlust pro Pixel, sondern den Wahrnehmungsverlust besser minimieren k√∂nnen.  Um dies zu berechnen, geben wir die Netzvorhersagen und ein Boden-Wahrheitsfoto an das VGG-16-Netz weiter, nehmen die Feature-Maps auf den unteren Ebenen und vergleichen sie mit MSE.  Dieser Ansatz malt mehr Bereiche und liefert farbenfrohere Ergebnisse. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/676/9c8/b64/6769c8b64fdf00cb66dcd73edcd39e81.jpg"><br><br><h1>  R√ºckblick </h1><br>  Unet ist ein ziemlich cooles Modell.  Bei der ersten Segmentierungsaufgabe hatten wir w√§hrend des Trainings ein Problem und arbeiteten mit hochaufl√∂senden Bildern. Deshalb verwenden wir In-Place BatchNorm.  Bei unserer zweiten Aufgabe (Inpainting) haben wir Partial Convolution anstelle einer Standardaufgabe verwendet, um bessere Ergebnisse zu erzielen.  Bei der Arbeit an der Kolorierung haben wir ein kleines Diskriminatornetz hinzugef√ºgt, das den Generator f√ºr unrealistische Bilder bestraft.  Wir haben auch einen Wahrnehmungsverlust verwendet. <br><br>  Zweite Schlussfolgerung - Gutachter sind unerl√§sslich.  Und das nicht nur w√§hrend der Erstellung von Segmentierungsmasken, sondern auch f√ºr die endg√ºltige Ergebnisvalidierung.  Am Ende geben wir dem Benutzer drei Fotos: ein Originalbild mit unlackierten Fehlern, ein koloriertes Foto mit unlackierten Fehlern und ein einfach koloriertes Foto f√ºr den Fall, dass der Algorithmus f√ºr die Fehlersuche und -lackierung falsch war. <br><br>  Wir haben einige Bilder aus dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">War Album-Projekt aufgenommen</a> und sie √ºber diese Neuronets verarbeitet.  Hier sind die Ergebnisse, die wir erhalten haben: <br><br><img src="https://habrastorage.org/webt/rm/4z/sb/rm4zsbvc0j_h_r2nobp4xj2p4ei.jpeg"><br><br>  Dar√ºber hinaus k√∂nnen Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> die Originalbilder und alle Verarbeitungsstufen genauer betrachten. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de459696/">https://habr.com/ru/post/de459696/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de459682/index.html">Datenqualit√§t im Speicher</a></li>
<li><a href="../de459684/index.html">Karte der Moskauer U-Bahn und der ganzen Welt f√ºr Android</a></li>
<li><a href="../de459688/index.html">Urbanismus in China: Weniger Hipster, mehr Wissenschaft und IT</a></li>
<li><a href="../de459692/index.html">Wie wir Materialmodifikationen entdeckten, die etablierten chemischen Prinzipien widersprechen</a></li>
<li><a href="../de459694/index.html">Museum DataArt. Packen Sie Radio 86RK aus und starten Sie es</a></li>
<li><a href="../de459698/index.html">Wie kann Oracle BI 12c gezwungen werden, so viele Sitzungsvariablen zu erstellen, wie ein Programmierer ben√∂tigt?</a></li>
<li><a href="../de459704/index.html">LLVM IR und Go</a></li>
<li><a href="../de459706/index.html">5 Gr√ºnde, warum Sie Redux in React-Anwendungen vergessen sollten</a></li>
<li><a href="../de459708/index.html">Design der Spieloberfl√§che. Brent Fox Worum geht es in dem Buch?</a></li>
<li><a href="../de459710/index.html">√úberlebe eine Frontalkollision und warum Amnesie nicht das ist, was du denkst</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>