<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘©ğŸ½â€âš•ï¸ ğŸ¥ˆ ğŸ‘¨ğŸ¼â€ğŸ¨ Pengembangan basis data di Dropbox. Jalur dari satu basis data global MySQL ke ribuan server ğŸ§‘ğŸ¿â€ğŸ¤â€ğŸ§‘ğŸ¿ ğŸ§– ğŸ‘¨ğŸ½â€ğŸ¤â€ğŸ‘¨ğŸ»</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ketika Dropbox baru saja dimulai, satu pengguna di Hacker News berkomentar bahwa itu dapat diimplementasikan dengan beberapa skrip bash menggunakan FT...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Pengembangan basis data di Dropbox. Jalur dari satu basis data global MySQL ke ribuan server</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/417315/">  Ketika Dropbox baru saja dimulai, satu pengguna di Hacker News berkomentar bahwa itu dapat diimplementasikan dengan beberapa skrip bash menggunakan FTP dan Git.  Sekarang ini tidak dapat dikatakan dengan cara apa pun, ini adalah penyimpanan file cloud besar dengan miliaran file baru setiap hari, yang tidak hanya disimpan entah bagaimana dalam database, tetapi sedemikian rupa sehingga setiap database dapat dipulihkan ke titik mana pun dalam enam hari terakhir. <br><br>  Di bawah potongan, transkrip laporan <strong>Glory Bakhmutov</strong> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=" class="user_link">m0sth8</a> ) di Highload ++ 2017, tentang bagaimana basis data di Dropbox dikembangkan dan bagaimana mereka diatur sekarang. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/hUFFsLoCRNU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <strong>Tentang pembicara:</strong> Glory to Bakhmutov - insinyur keandalan situs di tim Dropbox, sangat menyukai Go dan terkadang muncul di podcast golangshow.com. <br><br><h2>  Isi <br></h2><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Secara singkat tentang arsitektur Dropbox</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Sejarah pengembangan</a> basis data dan cara kerja arsitektur Dropbox saat ini </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Operasi paling sederhana pada basis data</a> (feylovers, backup, klon, promosi) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Otomasi</a> - yang mengelola semua basis data dan menjalankan operasi </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pemantauan</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pengujian, pementasan dan DRT</a> </li></ul><br><img src="https://habrastorage.org/webt/dh/gt/p7/dhgtp7pgu4eyl6rc3ncfnaf9s3i.jpeg"><br><a name="habracut"></a><br><a name="dropbox_architecture"></a><h2>  Arsitektur Dropbox dalam bahasa sederhana </h2><br>  Dropbox muncul pada 2008.  Ini pada dasarnya adalah penyimpanan file cloud.  Ketika Dropbox baru saja dimulai, pengguna di Hacker News berkomentar bahwa itu dapat diimplementasikan dengan beberapa skrip bash menggunakan FTP dan Git.  Namun, bagaimanapun, Dropbox sedang berkembang, dan sekarang ini adalah layanan yang cukup besar dengan lebih dari 1,5 miliar pengguna, 200 ribu bisnis, dan sejumlah besar (beberapa miliar!) File baru setiap hari. <br><br>  <strong>Seperti apa bentuk Dropbox?</strong> <br><img src="https://habrastorage.org/webt/ed/em/km/edemkm1wqvlbv6jb0qobp5c2dgc.jpeg"><br><br>  Kami memiliki beberapa klien (antarmuka web, API untuk aplikasi yang menggunakan Dropbox, aplikasi desktop).  Semua klien ini menggunakan API dan berkomunikasi dengan dua layanan besar yang secara logis dapat dibagi menjadi: <br><br><ol><li>  <strong>Metaserver</strong> </li><li>  <strong>Blockserver</strong> </li></ol><br>  Metaserver menyimpan meta-informasi tentang file: ukuran, komentar di atasnya, tautan ke file ini di Dropbox, dll.  Blockserver hanya menyimpan informasi tentang file: folder, jalur, dll. <br><br>  <strong>Bagaimana cara kerjanya?</strong> <br><br>  Misalnya, Anda memiliki file video.avi dengan beberapa jenis video. <br><img src="https://habrastorage.org/webt/kc/i9/op/kci9op0l7f_tecaxetg7uzpzemw.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Tautan dari slide</a></em> <br><br><ul><li>  Klien membagi file ini menjadi beberapa potongan (dalam hal ini, masing-masing 4 MB), menghitung checksum dan mengirimkan permintaan ke Metaserver: "Saya punya file * .avi, saya ingin mengunggahnya, jumlah hash adalah ini dan itu." </li><li>  Metaserver mengembalikan jawabannya: "Saya tidak punya blok ini, ayo unduh!"  Atau dia dapat menjawab bahwa dia memiliki semua atau beberapa blok, dan hanya yang tersisa yang perlu dimuat. </li></ul><br><img src="https://habrastorage.org/webt/qa/zr/79/qazr79svhai2ouk6v8lta1zcp-u.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Tautan dari slide</a></em> <br><br><ul><li>  Setelah itu, klien pergi ke Blockserver, mengirimkan jumlah hash dan blok data itu sendiri, yang disimpan di Blockserver. </li><li>  Blockserver mengkonfirmasi operasi. </li></ul><br><img src="https://habrastorage.org/webt/kl/dx/xw/kldxxw1ncgj4ytt1pqq5bh95iue.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Tautan dari slide</a></em> <br><br>  Tentu saja, ini adalah skema yang sangat disederhanakan, protokolnya jauh lebih rumit: ada sinkronisasi antara klien dalam jaringan yang sama, ada driver kernel, kemampuan untuk menyelesaikan tabrakan, dll.  Ini adalah protokol yang cukup rumit, tetapi berfungsi seperti ini secara skematis. <br><img src="https://habrastorage.org/webt/ev/-x/_h/ev-x_hwfhwpfixld61yqiraelc4.jpeg"><br><br>  Ketika klien menyimpan sesuatu di Metaserver, semua informasi masuk ke MySQL.  Blockserver juga menyimpan informasi tentang file, bagaimana mereka disusun, apa blok mereka terdiri, di MySQL.  Blockserver juga menyimpan blok itu sendiri di Block Storage, yang, pada gilirannya, menyimpan informasi tentang di mana blok berada, di server mana dan bagaimana diproses, juga di MYSQL. <br><br><blockquote>  Untuk menyimpan exabytes file pengguna, kami secara bersamaan menyimpan informasi tambahan dalam database beberapa lusin petabyte yang tersebar di 6 ribu server. </blockquote><br><a name="history_development"></a><h2>  Sejarah Pengembangan Database </h2><br>  Bagaimana basis data berkembang di Dropbox? <br><img src="https://habrastorage.org/webt/oe/w9/ok/oew9okiqdeivyhvhycznly7kvbe.jpeg"><br><br>  Pada 2008, semuanya dimulai dengan satu Metaserver dan satu basis data global.  Semua informasi yang diperlukan Dropbox untuk disimpan di suatu tempat, ia menyimpannya di satu-satunya MySQL global.  Ini tidak berlangsung lama, karena jumlah pengguna bertambah, dan masing-masing database dan tablet di dalam basis data membengkak lebih cepat daripada yang lain. <br><img src="https://habrastorage.org/webt/ry/lt/h9/rylth906zfbbcou6nz7ve_yqib8.jpeg"><br><br>  Oleh karena itu, pada tahun 2011 beberapa tabel dikirimkan ke server terpisah: <br><br><ul><li>  <strong>Pengguna</strong> , dengan informasi tentang pengguna, misalnya, masuk dan token oAuth; </li><li>  <strong>Host</strong> , dengan informasi file dari Blockserver; </li><li>  <strong>Lain-lain</strong> , yang tidak terlibat dalam memproses permintaan dari produksi, tetapi digunakan untuk fungsi utilitas, seperti pekerjaan batch. </li></ul><br><img src="https://habrastorage.org/webt/ja/ec/ja/jaecja2eklv8znsqhf5lt-dyez8.jpeg"><br><br>  Tetapi setelah 2012, Dropbox mulai tumbuh sangat banyak, sejak itu <strong>kami</strong> telah tumbuh <strong>sekitar 100 juta pengguna per tahun</strong> . <br><img src="https://habrastorage.org/webt/ie/cr/-s/iecr-syyi6qprj2zj45qx4l42k4.jpeg"><br><br>  Itu perlu untuk memperhitungkan pertumbuhan sebesar itu, dan karena itu pada akhir 2011 kami memiliki pecahan - basis yang terdiri dari 1.600 pecahan.  Awalnya, hanya 8 server dengan masing-masing 200 pecahan.  Sekarang ini adalah 400 server master dengan masing-masing 4 pecahan. <br><img src="https://habrastorage.org/webt/b3/v9/vy/b3v9vyjqzwau2kgmadhgb2vg0vo.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Tautan dari slide</a></em> <br><br>  Pada 2012, kami menyadari bahwa membuat tabel dan memperbaruinya dalam database untuk setiap logika bisnis tambahan sangat sulit, suram, dan bermasalah.  Oleh karena itu, pada tahun 2012, kami menemukan penyimpanan grafik kami sendiri, yang kami sebut <strong>Edgestore</strong> , dan sejak itu semua logika bisnis dan meta-informasi yang dihasilkan aplikasi disimpan di Edgestore. <br><br>  Edgestore pada dasarnya abstrak MySQL dari klien.  Klien memiliki entitas tertentu yang saling terhubung oleh tautan dari API gRPC ke Edgestore Core, yang mengubah data ini menjadi MySQL dan entah bagaimana menyimpannya di sana (pada dasarnya, ia memberikan semua ini dari cache). <br><img src="https://habrastorage.org/webt/bj/s7/dz/bjs7dz7-cdsvjblcgftjdeybrgk.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Tautan dari slide</a></em> <br><br>  <strong>Pada 2015, kami meninggalkan Amazon S3</strong> , mengembangkan penyimpanan cloud kami sendiri yang disebut Magic Pocket.  Ini berisi informasi tentang di mana file blok berada, di server mana, tentang pergerakan blok-blok ini antara server, disimpan di MySQL. <br><img src="https://habrastorage.org/webt/f_/bz/lm/f_bzlm3tk9e3lwqo64x4kfuhhok.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Tautan dari slide</a></em> <br><br>  Tetapi MySQL digunakan dengan cara yang sangat rumit - pada intinya, sebagai tabel hash terdistribusi besar.  Ini adalah beban yang sangat berbeda, terutama pada pembacaan catatan acak.  90% pemanfaatannya adalah I / O. <br><br><h2>  Arsitektur basis data </h2><br>  Pertama, kami segera mengidentifikasi beberapa prinsip yang digunakan untuk membangun arsitektur basis data kami: <br><br><ol><li>  <strong>Keandalan dan daya tahan</strong> .  Ini adalah prinsip paling penting dan apa yang pelanggan harapkan dari kami - data tidak boleh hilang. </li><li>  <strong>Optimalitas solusi</strong> adalah prinsip yang sama pentingnya.  Misalnya, pencadangan harus dilakukan dengan cepat dan dikembalikan dengan cepat juga. </li><li>  <strong>Kesederhanaan solusi</strong> - baik secara arsitektur maupun dalam hal layanan dan dukungan pengembangan lebih lanjut. </li><li>  <strong>Biaya kepemilikan</strong> .  Jika sesuatu mengoptimalkan solusi, tetapi sangat mahal, ini tidak cocok untuk kita.  Sebagai contoh, seorang budak yang satu hari di belakang master sangat nyaman untuk cadangan, tetapi kemudian Anda perlu menambahkan 1.000 lebih ke 6.000 server - biaya kepemilikan budak seperti itu sangat tinggi. </li></ol><br>  Semua prinsip harus dapat <strong>diverifikasi dan diukur</strong> , yaitu, mereka harus memiliki metrik.  Jika kita berbicara tentang biaya kepemilikan, maka kita harus menghitung berapa banyak server yang kita miliki, misalnya, masuk ke basis data, berapa banyak server masuk ke cadangan, dan berapa biaya untuk Dropbox pada akhirnya.  Ketika kami memilih solusi baru, kami menghitung semua metrik dan fokus pada mereka.  Ketika memilih solusi apa pun, kami sepenuhnya dipandu oleh prinsip-prinsip ini. <br><br><h2>  Topologi dasar </h2><br>  Basis data disusun sebagai berikut: <br><br><ul><li>  Di pusat data utama, kami memiliki master, di mana semua catatan terjadi. </li><li>  Server master memiliki dua server slave tempat replikasi semisync terjadi.  Server sering mati (sekitar 10 per minggu), jadi kita perlu dua server budak. </li><li>  Server budak berada dalam kelompok yang terpisah.  Cluster adalah ruangan yang sepenuhnya terpisah di pusat data yang tidak terhubung satu sama lain.  Jika satu ruangan terbakar, yang kedua tetap berfungsi sepenuhnya. </li><li>  Juga di pusat data lain kita memiliki master semu (master perantara), yang sebenarnya hanya seorang budak, yang memiliki budak lain. </li></ul><br><img src="https://habrastorage.org/webt/k6/6s/x6/k66sx6siyp6efjxxmfrot21ueha.jpeg"><br><br>  Topologi seperti itu dipilih karena jika pusat data pertama tiba-tiba mati di dalam kita, maka di pusat data kedua kita memiliki <strong>topologi yang hampir lengkap</strong> .  Kami cukup mengubah semua alamat di Discovery, dan klien dapat bekerja. <br><br><h3>  Topologi khusus </h3><br>  Kami juga memiliki topologi khusus. <br><br>  Topologi <strong>Magic Pocket</strong> terdiri dari satu server master dan dua server slave.  Ini dilakukan karena Magic Pocket sendiri menggandakan data antar zona.  Jika kehilangan satu cluster, itu dapat mengembalikan semua data dari zona lain melalui kode penghapusan. <br><img src="https://habrastorage.org/webt/gk/o7/bi/gko7bifb-4ted4cvwmzgyn5lasw.jpeg"><br><br>  Topologi <strong>aktif-aktif</strong> adalah topologi khusus yang digunakan oleh Edgestore.  Ia memiliki satu master dan dua budak di masing-masing dari dua pusat data, dan mereka adalah budak untuk satu sama lain.  Ini adalah <strong>skema yang</strong> sangat <strong>berbahaya</strong> , tetapi Edgestore pada levelnya tahu persis data apa yang menguasai kisaran apa yang bisa ditulisnya.  Karena itu, topologi ini tidak pecah. <br><img src="https://habrastorage.org/webt/xe/nv/wx/xenvwx3ls9ct8fssverq3htck10.jpeg"><br><br><h3>  Contoh </h3><br>  Kami telah menginstal server yang cukup sederhana dengan konfigurasi 4-5 tahun yang lalu: <br><br><ul><li>  <strong>2x Xeon 10 core;</strong> </li><li>  <strong>5TB (8 SSD Raid 0 *);</strong> </li><li>  <strong>Memori 384 GB.</strong> </li></ul><br>  * Raid 0 - karena lebih mudah dan lebih cepat untuk mengganti seluruh server daripada drive. <br><br><h4>  Contoh tunggal </h4><br>  Di server ini, kami memiliki satu instance MySQL besar tempat beberapa pecahan berada.  Contoh MySQL ini segera mengalokasikan sendiri hampir semua memori.  Proses lain juga berjalan di server: proksi, pengumpulan statistik, log, dll. <br><br><img src="https://habrastorage.org/webt/z8/yd/vw/z8ydvwabte3v8pytwbrl1vi8yc0.jpeg"><br><br>  Solusi ini bagus karena: <br><br>  + <strong>Mudah dikelola</strong> .  Jika Anda perlu mengganti instance MySQL, cukup ganti server. <br><br>  + <strong>Lakukan faylovers</strong> . <br><br>  Di sisi lain: <br><br>  - Bermasalah bahwa setiap operasi terjadi pada seluruh instance MySQL dan langsung pada semua pecahan.  Misalnya, jika Anda perlu mencadangkan, kami mencadangkan semua pecahan sekaligus.  Jika Anda perlu membuat faylover, kami membuat faylover keempat pecahan sekaligus.  Dengan demikian, aksesibilitas menderita 4 kali lebih banyak. <br><br>  - Masalah dengan mereplikasi satu pecahan mempengaruhi pecahan lainnya.  Replikasi MySQL tidak paralel, dan semua pecahan bekerja pada satu utas.  Jika sesuatu terjadi pada satu beling, maka sisanya juga menjadi korban. <br><br>  Jadi sekarang kita pindah ke topologi yang berbeda. <br><br><h4>  Multi instance </h4><br><img src="https://habrastorage.org/webt/lg/7x/ks/lg7xks5vbogjaf6slr7tidlc6ty.jpeg"><br><br>  Dalam versi baru, beberapa contoh MySQL diluncurkan pada server sekaligus, masing-masing dengan satu beling.  Apa yang lebih baik <br><br>  + Kami dapat <strong>melakukan operasi hanya pada satu beling tertentu</strong> .  Artinya, jika Anda membutuhkan faylover, alihkan hanya satu beling, jika Anda membutuhkan cadangan, kami hanya mencadangkan satu beling.  Ini berarti bahwa operasi sangat dipercepat - 4 kali untuk server empat-beling. <br><br>  + <strong>Pecahan hampir tidak saling mempengaruhi</strong> . <br><br>  + <strong>Peningkatan replikasi.</strong>  Kami dapat mencampur berbagai kategori dan kelas basis data.  Edgestore membutuhkan banyak ruang, misalnya, semua 4 TB, dan Magic Pocket hanya memakan 1 TB, tetapi memiliki pemanfaatan 90%.  Artinya, kita dapat menggabungkan berbagai kategori yang menggunakan I / O dan sumber daya mesin dengan cara yang berbeda, dan memulai 4 aliran replikasi. <br><br>  Tentu saja, solusi ini memiliki kekurangan: <br><br>  - Kekurangan terbesar adalah bahwa <strong>jauh lebih sulit untuk mengelola semua ini</strong> .  Kami membutuhkan penjadwal cerdas yang akan memahami di mana ia dapat mengambil contoh ini, di mana akan ada beban optimal. <br><br>  - <strong>Lebih sulit dari kegagalan</strong> . <br><br>  Karena itu, kami baru saja beralih ke keputusan ini. <br><br><h3>  Penemuan </h3><br>  Klien entah bagaimana harus tahu cara terhubung ke database yang diinginkan, jadi kami memiliki Discovery, yang seharusnya: <br><br><ol><li>  Beri tahu klien dengan sangat cepat tentang perubahan topologi.  Jika kita mengganti master dan slave, klien harus segera mempelajarinya. <br></li><li>  Topologi tidak boleh bergantung pada topologi replikasi MySQL, karena dengan beberapa operasi kita mengubah topologi MySQL.  Sebagai contoh, ketika kita melakukan split, pada langkah persiapan pada master target, di mana kita akan mentransfer bagian dari pecahan, beberapa server slave dikonfigurasi ulang untuk master target ini.  Pelanggan tidak perlu tahu tentang ini. <br></li><li>  Penting bahwa ada atomicity operasi dan verifikasi negara.  Tidak mungkin dua server berbeda dari database yang sama untuk menjadi master pada saat yang sama. <br></li></ol><br><h4>  Bagaimana Discovery Dikembangkan </h4><br>  Awalnya semuanya sederhana: alamat basis data dalam kode sumber di konfigurasi.  Ketika kami perlu memperbarui alamat, maka semuanya baru saja dikerahkan dengan sangat cepat. <br><img src="https://habrastorage.org/webt/7d/yb/oo/7dyboo0h7eo4o9_9xp-n-6lzosy.jpeg"><br><br>  Sayangnya, ini tidak berfungsi jika ada banyak server. <br><img src="https://habrastorage.org/webt/3u/26/pf/3u26pfl_s796zdaoix-zdplb9du.jpeg"><br><br>  Di atas adalah Penemuan pertama yang kita miliki.  Ada skrip database yang mengubah papan nama di ConfigDB - itu adalah papan nama MySQL yang terpisah, dan klien sudah mendengarkan database ini dan secara berkala mengambil data dari sana. <br><img src="https://habrastorage.org/webt/ml/qn/mh/mlqnmhmmteylazgl4itjokes_mu.jpeg"><br><br>  Tabelnya sangat sederhana, ada kategori database, kunci shard, kelas master database / slave, proxy, dan alamat database.  Bahkan, klien meminta kategori, kelas DB, kunci shard, dan alamat MySQL dikembalikan ke mana ia sudah bisa membuat koneksi. <br><img src="https://habrastorage.org/webt/vb/ht/en/vbhteniyw4x7s56a1xwckuz268e.jpeg"><br><br>  Segera setelah ada banyak server, Memcache ditambahkan dan klien mulai berkomunikasi dengannya. <br><br>  Tapi kemudian kami ulang.  Script MySQL mulai berkomunikasi melalui gRPC, melalui thin client dengan layanan yang kami sebut RegisterService.  Ketika beberapa perubahan terjadi, RegisterService memiliki antrian, dan dia mengerti bagaimana menerapkan perubahan ini.  RegisterService menyimpan data dalam AFS.  AFS adalah sistem internal kami yang didasarkan pada ZooKeeper. <br><img src="https://habrastorage.org/webt/mi/lm/yz/milmyzvyvayuv2av8pah4neh9zq.jpeg"><br><br>  Solusi kedua, yang tidak ditampilkan di sini, menggunakan ZooKeeper secara langsung, dan ini menciptakan masalah karena masing-masing beling adalah simpul di ZooKeeper.  Misalnya, 100 ribu klien terhubung ke ZooKeeper, jika mereka tiba-tiba mati karena beberapa jenis bug bersama-sama, maka 100 ribu permintaan ke ZooKeeper akan segera datang, yang hanya akan menjatuhkannya dan tidak akan bisa naik. <br><br>  Karenanya, <strong>sistem AFS</strong> dikembangkan <strong>, yang digunakan oleh seluruh Dropbox</strong> .  Bahkan, itu abstrak pekerjaan dengan ZooKeeper untuk semua pelanggan.  Daemon AFS secara lokal berjalan di setiap server dan menyediakan file API yang sangat sederhana dari formulir: membuat file, menghapus file, meminta file, menerima pemberitahuan tentang perubahan file dan membandingkan serta menukar operasi.  Artinya, Anda dapat mencoba mengganti file dengan beberapa versi, dan jika versi ini telah berubah selama perubahan, operasi dibatalkan. <br><br>  Pada dasarnya, seperti abstraksi atas ZooKeeper, di mana ada algoritma backoff dan jitter lokal.  ZooKeeper tidak lagi mogok saat dimuat.  Dengan AFS, kami mengambil cadangan dalam S3 dan GIT, lalu AFS lokal sendiri memberi tahu klien bahwa data telah berubah. <br><img src="https://habrastorage.org/webt/ry/_0/wv/ry_0wvkyux23gicrlwmfceq0eoe.jpeg"><br><br>  Dalam AFS, data disimpan sebagai file, yaitu API sistem file.  Misalnya, di atas adalah file shard.slave_proxy - terbesar, dibutuhkan sekitar 28 Kb, dan ketika kami mengubah kategori kelas shard dan slave_proxy, maka semua klien yang berlangganan file ini menerima pemberitahuan.  Mereka membaca kembali file ini, yang berisi semua informasi yang diperlukan.  Menggunakan kunci beling, mereka mendapatkan kategori dan mengkonfigurasi ulang kumpulan koneksi ke database. <br><br><a name="perations_databases"></a><h2>  Operasi </h2><br>  Kami menggunakan operasi yang sangat sederhana: promosi, klon, backup / pemulihan. <br><img src="https://habrastorage.org/webt/q3/5c/df/q35cdfiso51bhhiitqja71qbysc.jpeg"><br><br>  <strong>Operasi adalah mesin keadaan sederhana</strong> .  Ketika kita masuk ke operasi, kita melakukan beberapa pemeriksaan, misalnya, spin-check, yang beberapa kali oleh waktu habis memeriksa apakah kita dapat melakukan operasi ini.  Setelah itu, kami melakukan beberapa tindakan persiapan yang tidak mempengaruhi sistem eksternal.  Selanjutnya, operasi itu sendiri. <br><br>  Semua langkah dalam operasi memiliki <strong>langkah mundur</strong> (undo).  Jika ada masalah dengan operasi, operasi mencoba mengembalikan sistem ke posisi semula.  Jika semuanya baik-baik saja, maka pembersihan terjadi dan operasi selesai. <br><br>  Kami memiliki mesin keadaan sederhana untuk operasi apa pun. <br><br><h4>  <strong>Promosi (pergantian master)</strong> </h4><br>  Ini adalah operasi yang sangat umum dalam database.  Ada pertanyaan tentang bagaimana melakukan perubahan pada server master panas yang berfungsi - itu akan dipertaruhkan.  Hanya saja semua operasi ini dilakukan pada server slave, dan kemudian slave perubahan dengan tempat utama.  Karena itu, <strong>operasi promosi sangat sering dilakukan</strong> . <br><img src="https://habrastorage.org/webt/xx/79/jv/xx79jvszxb9wjffwqf4ld_euofo.jpeg"><br><br>  Kita perlu memperbarui kernel - kita melakukan swap, kita perlu memperbarui versi MySQL - kita memperbarui pada slave, beralih ke master, perbarui di sana. <br><img src="https://habrastorage.org/webt/wc/op/o9/wcopo9jlmz9aeoynpv-hbbseois.jpeg"><br><br>  Kami telah mencapai promosi yang sangat cepat.  Misalnya, <strong>untuk empat pecahan, kami sekarang memiliki promosi sekitar 10-15 detik.</strong>  Grafik di atas menunjukkan bahwa dengan ketersediaan promosi menderita 0,0003%. <br><br>  Tetapi promosi normal tidak begitu menarik, karena ini adalah operasi biasa yang dilakukan setiap hari.  Kegagalan itu menarik. <br><br><h4>  <strong>Failover (pengganti master rusak)</strong> <br></h4><br>  Kegagalan berarti database sudah mati. <br><br><ul><li>  Jika server benar-benar mati, ini hanya kasus yang ideal. </li><li>  Bahkan, ternyata server sebagian hidup. </li><li>  Terkadang server mati sangat lambat.  Pengendali serangan, sistem disk gagal, beberapa permintaan mengembalikan jawaban, tetapi beberapa aliran diblokir dan tidak mengembalikan jawaban. </li><li>  Kebetulan master hanya kelebihan beban dan tidak menanggapi pemeriksaan kesehatan kami.  Tetapi jika kita melakukan promosi, master baru juga akan kelebihan beban, dan itu hanya akan bertambah buruk. </li></ul><br>  Penggantian server master yang meninggal terjadi sekitar <strong>2-3 kali sehari</strong> , ini adalah proses yang sepenuhnya otomatis, tidak diperlukan campur tangan manusia.  Bagian kritis memakan waktu sekitar 30 detik, dan memiliki banyak pemeriksaan tambahan untuk melihat apakah server benar-benar hidup, atau mungkin sudah mati. <br><br>  Di bawah ini adalah contoh diagram tentang bagaimana faylover bekerja. <br><img src="https://habrastorage.org/webt/ks/5d/6o/ks5d6oovtnchnmlr2zlgpwvmt5g.jpeg"><br><br>  Di bagian yang dipilih, kita <strong>me-reboot server master</strong> .  Ini diperlukan karena kami memiliki MySQL 5.6, dan replikasi semisync di dalamnya tidak lossless.  Oleh karena itu, pembacaan hantu dimungkinkan, dan kami membutuhkan master ini, meskipun belum mati, bunuh secepat mungkin sehingga klien memutuskan sambungan darinya.  Karena itu, kami melakukan hard reset melalui Ipmi - ini adalah operasi terpenting pertama yang harus kami lakukan.  Dalam versi MySQL 5.7, ini tidak begitu kritis. <br><br>  <strong>Sinkronisasi cluster.</strong>  Mengapa kita perlu sinkronisasi klaster? <br><img src="https://habrastorage.org/webt/gh/pa/go/ghpago_p12c1jittnig4rwhc1sg.jpeg"><br><br>  Jika kita mengingat gambar sebelumnya dengan topologi kita, satu server master memiliki tiga server slave: dua dalam satu pusat data, satu di yang lain.  Dengan promosi, kita perlu master untuk berada di pusat data utama yang sama.  Tetapi kadang-kadang, ketika budak dimuat, dengan semisync terjadi bahwa budak-semisync menjadi budak di pusat data lain, karena tidak dimuat.  Oleh karena itu, pertama-tama kita perlu menyinkronkan seluruh cluster, dan kemudian sudah melakukan promosi pada slave di pusat data yang kita butuhkan.  Ini dilakukan dengan sangat sederhana: <br><br><ul><li>  Kami menghentikan semua utas I / O pada semua server slave. </li><li>  Setelah itu, kita sudah tahu pasti bahwa master "read-only", karena semisync telah terputus dan tidak ada orang lain yang dapat menulis apa pun di sana. </li><li>  Selanjutnya, kami memilih budak dengan Set GTID yang diambil / dieksekusi terbesar, yaitu, dengan transaksi terbesar yang diunduh atau sudah diterapkan. </li><li>  Kami mengkonfigurasi ulang semua server slave ke slave yang dipilih ini, memulai utas I / O, dan mereka disinkronkan. </li><li>  Kami menunggu sampai mereka disinkronkan, setelah itu kami memiliki seluruh cluster menjadi disinkronkan.   ,     executed GTID set       . </li></ul><br>    â€” <strong> </strong> .   <strong>promotion</strong> ,    : <br><img src="https://habrastorage.org/webt/bd/s-/b8/bds-b8fbieqxhtc4dy9ookntiri.jpeg"><br><br><ul><li>    slave    -,  ,   master,     promotion. </li><li>    slave-   master,   ,  ACLs,  ,  - proxy, , - . </li><li>      read_only = 0,   ,    master  ,   .        master     . </li><li>       - .     -    ,  ,   ,    , ,  proxy  . </li><li>     . </li></ul><br>   ,       rollback   ,   .       rollback  reboot.   ,    , ,  â€” change master â€”    master   . <br><br><h4> <strong></strong> </h4><br>  â€”      .   ,    ,   ,    ,    . <br><br> <strong> </strong> <br><br> â—   slave <br><br>   ,       slave-,      .   . <br><br> â—       <br><br>     ,     ,      .             . <br><br> â—       <br><br>  ,     ,      .          .      3  . <br><br><blockquote>    ,   ,   ,     : <br><br><ol><li>      .       1  40 . <br></li><li>            . <br></li></ol></blockquote><br>    ,     .   1   40 ,      ,      ,     . <br><br><h4> <strong></strong> </h4><br>    ,  .           .     4  . <br><img src="https://habrastorage.org/webt/bv/-k/_z/bv-k_znrl7zi2obmotthihjogyo.jpeg"><br><br><ul><li>    <strong> 24 </strong> .         HDFS,      . </li><li> <strong> 6 </strong>     unsharded databases,        Global DB.      , ,  ,     . </li><li> <strong> 3 </strong>          S3. </li><li> <strong> 3 </strong>     S3     . </li></ul><br><img src="https://habrastorage.org/webt/e1/yx/3s/e1yx3s-1ikyhxnuzeympjsvem14.jpeg"><br><br>       . ,    3 ,   HDFS     3 ,   6   S3.     . <br><br>  ,   . <br><img src="https://habrastorage.org/webt/sn/hz/1l/snhz1lmio2naq40wziys-jggaaw.jpeg"><br><br>         ,      ,   .       ,   ,    recovery  -   .  ,      ,  -       .      100  ,   . <br><br>     ,    ,    ,    ,   ,     ,  ,     .        . <br><br><h5>   </h5><br><img src="https://habrastorage.org/webt/4m/4b/kb/4m4bkboro5zunrljkwxbybj7jsi.jpeg"><br><br>     hot-,      Percona xtrabackup.     â€”stream=xbstream,        ,   .     script-splitter,        ,      . <br><br> MySQL              2x.     3 , ,   ,    1 500 .     ,      ,    HDFS   S3. <br><br>        . <br><img src="https://habrastorage.org/webt/j3/il/jm/j3iljma8c0rekqweak5ngqvaxkk.jpeg"><br><br>  ,    ,    HDFS   S3,    , splitter       xtrabackup,      .   crash-recovery. <br><br>      hot   ,  crash-recovery    .         ,    .     binlog,      master. <br><br> <strong>   binlogs?</strong> <br><br>     binlog'.    master ,    4 ,   100 ,    HDFS. <br><br>      :   Binlog Backuper,         . ,  ,   binlog       HDFS. <br><img src="https://habrastorage.org/webt/on/o3/ce/ono3cesuissuuwuzcglcfautfjo.jpeg"><br><br> ,       4   ,    5 ,    ,    ,    .    HDFS   S3    . <br><br><h5>   </h5><br>      . <br><br>   : <br><br><ol><li>        â€”  10 ,  45  â€”   . <br></li><li>      ,       scheduler  multi instance      slave  master    . <br></li><li>    â€”      ,   .  ,     ,    ,    ,     ,  ,    .  pt-table-checksum   ,      . <br></li></ol><br> <strong></strong> ,        : <br><br><ol><li>       1  10 ,      .    crash-recovery,     . <br></li><li>            . <br></li></ol><br><img src="https://habrastorage.org/webt/2m/bd/ar/2mbdarekbku4hsxygdufshzyhnm.jpeg"><br><br>     slave   -,     .    ,      .   . <br><br><h4>  ++ </h4><br>     .       Hardware ,          (HDD)  10 ,       + crash recovery xtrabackup,      . ,         ,    . , ,     ,   ,   HDD  ,    HDFS  . <br><br><h4>  </h4><br>    ,  â€”   : <br><br><ol><li>         ; <br></li><li>       . <br></li></ol><br>  ,     HDFS,       ,   ,       . <br><br><a name="automation"></a><h2>  Otomasi </h2><br>  ,  6 000      .         ,   ,     â€” : <br><br><ul><li> Auto-replace; </li><li> DBManager; </li><li> Naoru, Wheelhouse </li></ul><br><h3> Auto-replace </h3><br>   ,   ,   ,    ,     â€” ,     -.   ,   . <br><br> <strong>Availability ()</strong> â€”         ,         .      â€”   recovery  ,         . <br><img src="https://habrastorage.org/webt/-k/em/tr/-kemtrpvhgocinlvlmnsuj1eq-s.jpeg"><br><br>    MySQL  ,   heartbeat. Heartbeat â€”   timestamp. <br><img src="https://habrastorage.org/webt/cn/r8/fn/cnr8fn-fpy_ddnthddjem3afr4o.jpeg"><br><br>    ,     , ,  master   read-write.          heartbeat. <br><br>    auto-replace ,    . <br><img src="https://habrastorage.org/webt/3u/r9/sx/3ur9sxfxf8dsxtgsgvjz3ublvye.jpeg"> <em>           <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="></a> ,   91 .</em> <br><br> <strong>  ?</strong> <br><br><ul><li>   ,     heartbeat    . ,     .  heartbeat', ,    heartbeat'  30 . </li><li> Selanjutnya, lihat apakah nomor mereka memenuhi nilai ambang batas.  Jika tidak, maka ada sesuatu yang salah dengan server - karena tidak mengirim detak jantung. </li><li>  Setelah itu, kami melakukan pemeriksaan terbalik untuk berjaga-jaga - tiba-tiba kedua layanan ini mati, ada sesuatu dengan jaringan, atau database global tidak dapat menulis detak jantung karena suatu alasan.  Dalam pemeriksaan terbalik, kami terhubung ke database yang rusak dan memeriksa statusnya. </li><li>  Jika semuanya gagal, kita melihat apakah posisi master sedang berkembang atau tidak, apakah ada catatan di sana.  Jika tidak ada yang terjadi, maka server ini pasti tidak berfungsi. </li><li>  Langkah terakhir sebenarnya adalah penggantian otomatis. </li></ul><br>  Penggantian otomatis sangat konservatif, ia tidak pernah ingin melakukan banyak operasi otomatis. <br><br><ol><li>  Pertama, kami memeriksa apakah ada operasi topologi baru-baru ini?  Mungkin server ini baru saja ditambahkan dan sesuatu di dalamnya belum berjalan. </li><li>  Kami memeriksa untuk melihat apakah ada penggantian di cluster yang sama setiap saat. </li><li>  Periksa batas kegagalan apa yang kami miliki.  Jika kita memiliki banyak masalah sekaligus - 10, 20 - maka kita tidak akan secara otomatis menyelesaikannya semua, karena kita dapat secara tidak sengaja mengganggu operasi semua basis data. </li></ol><br>  Karena itu, kami <strong>hanya memecahkan satu masalah pada satu waktu</strong> . <br><br>  Karenanya, untuk server slave, kami mulai mengkloning dan menghapusnya dari topologi, dan jika master, maka kami meluncurkan feylover, yang disebut promosi darurat. <br><br><h3>  DBManager </h3><br>  DBManager adalah layanan untuk mengelola basis data kami.  Itu memiliki: <br><br><ul><li>  penjadwal tugas pintar yang tahu persis kapan pekerjaan dimulai; </li><li>  log dan semua informasi: siapa, kapan dan apa yang diluncurkan - ini adalah sumber kebenaran; </li><li>  titik sinkronisasi. </li></ul><br><img src="https://habrastorage.org/webt/xx/g6/pi/xxg6pitu-pau9ifyqivpa9wul3e.jpeg"><br><br>  DBManager cukup sederhana secara arsitektur. <br><br><ul><li>  Ada klien, baik DBA yang melakukan sesuatu melalui antarmuka web, atau skrip / layanan yang menulis DBA yang mengakses melalui gRPC. </li><li>  Ada sistem eksternal seperti Wheelhouse dan Naoru, yang masuk ke DBManager via gRPC. </li><li>  Ada penjadwal yang mengerti operasi apa, kapan dan di mana dia bisa memulai. </li><li>  Ada pekerja yang sangat bodoh yang, ketika operasi datang kepadanya, memulainya, memeriksa oleh PID.  Pekerja dapat memulai kembali, proses tidak terganggu.  Semua pekerja ditempatkan sedekat mungkin ke server tempat operasi berlangsung, sehingga, misalnya, saat memperbarui ACLS, kami tidak perlu melakukan banyak perjalanan bolak-balik. </li><li>  Pada setiap host SQL kami memiliki DBAgent - ini adalah server RPC.  Ketika Anda perlu melakukan beberapa operasi di server, kami mengirim permintaan RPC. </li></ul><br>  Kami memiliki antarmuka web untuk DBManager, di mana Anda dapat melihat tugas yang sedang berjalan, log untuk tugas-tugas ini, siapa yang memulai dan kapan, operasi apa yang dilakukan untuk server dari database tertentu, dll. <br><img src="https://habrastorage.org/webt/yj/tq/3m/yjtq3mrfimptba1kizre2bwie48.jpeg"><br><br>  Ada antarmuka CLI yang cukup sederhana di mana Anda dapat menjalankan tugas dan juga melihatnya dalam tampilan yang nyaman. <br><img src="https://habrastorage.org/webt/qi/rc/vp/qircvpuoutswvmcpnuca4wem9cu.jpeg"><br><br><h3>  Remediasi </h3><br>  Kami juga memiliki sistem untuk menanggapi masalah.  Ketika ada sesuatu yang rusak, misalnya, drive macet, atau beberapa layanan tidak berfungsi, <strong>Naoru</strong> berfungsi <strong>.</strong>  Ini adalah sistem yang bekerja di seluruh Dropbox, semua orang menggunakannya, dan ini dibuat khusus untuk tugas-tugas kecil seperti itu.  Saya berbicara tentang Naoru dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">laporan</a> saya pada tahun 2016. <br><br>  <strong>Wheelhouse</strong> didasarkan pada mesin <strong>negara</strong> dan dirancang untuk proses panjang.  Sebagai contoh, kita perlu memperbarui kernel pada semua MySQL di seluruh cluster kami dari 6.000 mesin.  Wheelhouse melakukan ini dengan jelas - pembaruan di server slave, meluncurkan promosi, slave menjadi master, pembaruan di server master.  Operasi ini dapat memakan waktu satu atau dua bulan. <br><br><a name="monitoring"></a><h2>  Pemantauan </h2><br><img src="https://habrastorage.org/webt/vo/n_/ys/von_ysxtakb7m5ctiadwzewi3dw.jpeg"><br><br>  Ini sangat penting. <br><br><blockquote>  Jika Anda tidak memonitor sistem, maka kemungkinan besar itu tidak berfungsi. </blockquote><br>  Kami memantau semua yang ada di MySQL - semua informasi yang kami dapat dari MySQL disimpan di suatu tempat, kami dapat mengaksesnya tepat waktu.  Kami menyimpan informasi tentang InnoDb, statistik permintaan, transaksi, panjang transaksi, persentasi panjang transaksi, replikasi, di jaringan - semuanya - semua metrik. <br><br><h3>  Peringatan </h3><br>  Kami memiliki 992 peringatan yang dikonfigurasi.  Bahkan, tidak ada yang melihat metrik, bagi saya tampaknya tidak ada orang yang datang untuk bekerja dan mulai melihat grafik metrik, ada tugas yang lebih menarik. <br><img src="https://habrastorage.org/webt/q3/nj/sl/q3njslzahbxmh585uzeatrn4plm.jpeg"><br><br>  Oleh karena itu, ada peringatan yang berfungsi saat nilai ambang tertentu tercapai.  <strong>Kami memiliki 992 peringatan, apa pun yang terjadi, kami akan mengetahuinya</strong> . <br><br><h3>  Insiden </h3><br><img src="https://habrastorage.org/webt/bi/ce/tp/bicetpzgstf2lggazas6t27ru10.jpeg"><br><br>  Kami memiliki PagerDuty - layanan di mana peringatan dikirimkan kepada orang-orang yang bertanggung jawab yang mulai mengambil tindakan. <br><img src="https://habrastorage.org/webt/6n/hl/sr/6nhlsrj28tloxq4xg5ld3a-mpvy.jpeg"><br><br>  Dalam hal ini, terjadi kesalahan dalam promosi darurat, dan segera setelah itu peringatan dicatat bahwa master jatuh.  Setelah itu, petugas jaga memeriksa apa yang mencegah promosi darurat, dan melakukan operasi manual yang diperlukan. <br><br>  Kami pasti akan menganalisis setiap insiden yang telah terjadi, untuk setiap insiden kami memiliki tugas di pelacak tugas.  Sekalipun insiden ini merupakan masalah di lansiran kami, kami juga membuat tugas, karena jika masalahnya ada dalam logika dan ambang lansiran, maka semua itu perlu diubah.  Peringatan seharusnya tidak hanya merusak kehidupan orang.  Peringatan selalu menyakitkan, terutama pada jam 4 pagi. <br><br><a name="testing"></a><h2>  Pengujian </h2><br>  Seperti pemantauan, saya yakin semua orang menguji.  Selain tes unit yang kami gunakan untuk menutupi kode kami, kami memiliki tes integrasi yang kami uji: <br><br><ul><li>  semua topologi yang kita miliki; </li><li>  semua operasi pada topologi ini. </li></ul><br>  Jika kami memiliki operasi promosi, kami menguji operasi promosi dalam tes integrasi.  Jika kita memiliki kloning, kita melakukan kloning untuk semua topologi yang kita miliki. <br><br>  <strong>Contoh topologi</strong> <br><img src="https://habrastorage.org/webt/dv/hh/va/dvhhvacdschtqr7s_ynecm0lyk0.jpeg"><br><br>  Kami memiliki topologi untuk semua kesempatan: 2 pusat data dengan multi instance, dengan pecahan, tanpa pecahan, dengan cluster, satu pusat data - umumnya hampir semua topologi - bahkan yang tidak kami gunakan, hanya untuk melihat. <br><img src="https://habrastorage.org/webt/f-/oy/pl/f-oypluoyzosnphjl_aukp4vsks.jpeg"><br><br>  Dalam file ini, kita hanya memiliki pengaturan, server mana dan dengan apa yang perlu kita tingkatkan.  Sebagai contoh, kita perlu meningkatkan master, dan kita mengatakan bahwa kita perlu melakukan ini dengan data instance ini dan itu, dengan database ini dan itu pada port ini dan itu.  Hampir semuanya berjalan bersama dengan Bazel, yang menciptakan topologi berdasarkan file-file ini, memulai server MySQL, dan kemudian pengujian dimulai. <br><img src="https://habrastorage.org/webt/bg/zd/b_/bgzdb_dwnggh3kf8uv9f7mn9jpe.jpeg"><br><br>  Tes ini terlihat sangat sederhana: kami mengindikasikan topologi mana yang sedang digunakan.  Dalam tes ini, kami menguji auto_replace. <br><br><ul><li>  Kami membuat layanan auto_replace, kami memulainya. </li><li>  Kami membunuh master dalam topologi kami, menunggu sebentar dan melihat bahwa target-budak telah menjadi master.  Jika tidak, maka tes gagal. </li></ul><br><h3>  Tahapan </h3><br>  Lingkungan panggung adalah basis data yang sama dengan dalam produksi, tetapi tidak ada lalu lintas pengguna, tetapi ada beberapa lalu lintas sintetis yang mirip dengan produksi melalui Percona Playback, sysbench dan sistem serupa. <br><br>  Di Percona Playback, kami merekam lalu lintas, lalu kehilangannya di lingkungan panggung dengan intensitas berbeda, kami bisa kehilangan 2-3 kali lebih cepat.  Artinya, itu buatan, tetapi sangat dekat dengan beban nyata. <br><br>  Ini diperlukan karena dalam pengujian integrasi kami tidak dapat menguji produksi kami.  Kami tidak dapat menguji lansiran atau fakta bahwa metrik berfungsi.  Pada tahap pengujian, kami menguji peringatan, metrik, operasi, secara berkala membunuh server dan melihat bahwa mereka dikumpulkan secara normal. <br><br>  Plus, kami menguji semua otomatisasi bersama, karena dalam tes integrasi, kemungkinan besar, satu bagian dari sistem diuji, dan dalam pementasan, semua sistem otomatis bekerja secara bersamaan.  Terkadang Anda berpikir bahwa sistem akan berperilaku seperti ini dan bukan sebaliknya, tetapi mungkin berperilaku dengan cara yang sama sekali berbeda. <br><br><h3>  DRT (Pengujian pemulihan bencana) </h3><br>  Kami juga melakukan pengujian dalam produksi - langsung dari basis nyata.  Ini disebut pengujian pemulihan Bencana.  Mengapa kita membutuhkan ini? <br><br>  â— Kami ingin menguji jaminan kami. <br><br>  Ini dilakukan oleh banyak perusahaan besar.  Misalnya, Google memiliki satu layanan yang bekerja sangat stabil - 100% dari waktu - bahwa semua layanan yang menggunakannya memutuskan bahwa layanan ini benar-benar 100% stabil dan tidak pernah macet.  Karena itu, Google harus menjatuhkan layanan ini dengan sengaja, sehingga pengguna memperhitungkan kemungkinan ini. <br><br>  Jadi kami - kami memiliki jaminan bahwa MySQL berfungsi - dan terkadang tidak berfungsi!  Dan kami memiliki jaminan bahwa itu mungkin tidak berfungsi untuk jangka waktu tertentu, pelanggan harus mempertimbangkan ini.  Dari waktu ke waktu, kami membunuh master produksi, atau jika kami ingin membuat faylover, kami membunuh semua budak untuk melihat bagaimana replikasi semisync berperilaku. <br><br>  â— Pelanggan siap untuk kesalahan ini (penggantian dan kematian master) <br><br>  Kenapa itu bagus?  Kami memiliki kasus ketika selama promosi 4 pecahan dari 1600, ketersediaan turun menjadi 20%.  Tampaknya ada sesuatu yang salah, untuk 4 pecahan dari 1600 harus ada beberapa nomor lainnya.  Kegagalan untuk sistem ini jarang terjadi, sekitar sebulan sekali, dan semua orang memutuskan: "Ya, itu failover, itu terjadi." <br><br>  Pada titik tertentu, ketika kami beralih ke sistem baru, satu orang memutuskan untuk mengoptimalkan dua layanan perekaman detak jantung dan menggabungkannya menjadi satu.  Layanan ini melakukan sesuatu yang lain dan, pada akhirnya, mati dan detak jantung berhenti merekam.  Kebetulan untuk klien ini kami memiliki 8 faylovers sehari.  Semuanya awam - ketersediaan 20%. <br><br>  Ternyata dalam klien ini tetap hidup adalah 6 jam.  Oleh karena itu, begitu master meninggal, kami memiliki semua koneksi ditahan selama 6 jam.  Kolam tidak dapat terus bekerja - koneksinya tetap, terbatas dan tidak berfungsi.  Itu diperbaiki. <br><br>  Kami melakukan feylover lagi - tidak lagi 20%, tetapi masih banyak.  Masih ada yang salah.  Ternyata ada bug dalam implementasi pool.  Ketika diminta, kolam berubah menjadi banyak pecahan, dan kemudian menghubungkan semua ini.  Jika beberapa pecahan demam, beberapa kondisi lomba terjadi dalam kode Go, dan seluruh kolam tersumbat.  Semua pecahan ini tidak bisa lagi berfungsi. <br><br>  Pengujian pemulihan bencana sangat berguna, karena klien harus siap untuk kesalahan ini, mereka harus memeriksa kode mereka. <br><br>  â— Plus Pengujian pemulihan bencana bagus karena berlangsung selama jam kerja dan semuanya sudah ada, lebih sedikit stres, orang tahu apa yang akan terjadi sekarang.  Ini tidak terjadi di malam hari, dan itu hebat. <br><br><h2>  Kesimpulan </h2><br>  1. Semuanya perlu diotomatisasi, jangan pernah mengalaminya. <br>  Setiap kali seseorang naik ke sistem dengan tangan kita, semuanya mati dan rusak di sistem kita - setiap saat!  - bahkan pada operasi sederhana.  Misalnya, seorang budak meninggal, seseorang harus menambahkan yang kedua, tetapi memutuskan untuk menghapus budak yang mati dengan tangannya dari topologi.  Namun, bukannya almarhum, ia menyalin ke perintah live - master dibiarkan tanpa budak sama sekali.  Operasi semacam itu tidak boleh dilakukan secara manual. <br><br>  2. Tes harus kontinu dan otomatis (dan dalam produksi). <br>  Sistem Anda berubah, infrastruktur Anda berubah.  Jika Anda memeriksanya sekali, dan sepertinya berhasil, ini tidak berarti itu akan berfungsi besok.  Karena itu, Anda perlu terus melakukan pengujian otomatis setiap hari, termasuk dalam produksi. <br><br>  3. Pastikan untuk memiliki klien (perpustakaan). <br>  Pengguna mungkin tidak tahu cara kerja basis data.  Mereka mungkin tidak mengerti mengapa waktu habis diperlukan, tetap hidup.  Karena itu, lebih baik memiliki pelanggan ini - Anda akan lebih tenang. <br><br>  4. Penting untuk menentukan prinsip Anda untuk membangun sistem dan jaminan Anda, dan selalu mematuhinya. <br><br>  Dengan demikian, Anda dapat mendukung 6 ribu server database. <br><br><div class="spoiler">  <b class="spoiler_title">Dalam pertanyaan setelah laporan, dan terutama jawaban mereka, ada juga banyak informasi berguna.</b> <div class="spoiler_text"><h2>  Tanya Jawab <br></h2><br><blockquote>  - Apa yang akan terjadi jika ada ketidakseimbangan beban pecahan - beberapa informasi meta tentang beberapa file ternyata lebih populer?  Apakah mungkin untuk meredakan pecahan ini, atau beban pada pecahan tidak berbeda di mana pun berdasarkan pesanan besarnya? </blockquote><br>  Dia tidak berbeda dengan urutan besarnya.  Hampir terdistribusi secara normal.  Kami memiliki pelambatan, yaitu, kami tidak dapat membebani pecahan sebenarnya, kami melambat di tingkat klien.  Secara umum, beberapa bintang mengunggah foto, dan beling itu praktis meledak.  Lalu kami melarang tautan ini <br><br><blockquote>  - Anda mengatakan Anda memiliki 992 peringatan.  Bisakah Anda menguraikan apa itu - apakah di luar kotak atau itu dibuat?  Jika dibuat, apakah itu kerja manual atau sesuatu seperti pembelajaran mesin? </blockquote><br>  Ini semua dibuat secara manual.  Kami memiliki sistem internal kami sendiri yang disebut Vortex, tempat metrik disimpan, peringatan didukung di dalamnya.  Ada file yaml yang mengatakan bahwa ada kondisi, misalnya, bahwa cadangan harus dijalankan setiap hari, dan jika kondisi ini terpenuhi, peringatan tidak berfungsi.  Jika tidak dieksekusi, maka peringatan datang. <br><br>  Ini adalah pengembangan internal kami, karena hanya sedikit orang yang dapat menyimpan metrik sebanyak yang kami butuhkan. <br><br><blockquote>  - Seberapa kuat saraf untuk melakukan DRT?  Anda jatuh, TERKENAL, tidak naik, dengan setiap menit lebih banyak panik. </blockquote><br>  Secara umum, bekerja di basis data benar-benar menyebalkan.  Jika database macet, layanan tidak berfungsi, seluruh Dropbox tidak berfungsi.  Ini adalah rasa sakit yang nyata.  DRT berguna karena merupakan jam tangan bisnis.  Yaitu, saya siap, saya duduk di meja saya, saya minum kopi, saya segar, saya siap untuk melakukan apa saja. <br><br>  Lebih buruk ketika itu terjadi pukul 4 pagi, dan itu bukan DRT.  Misalnya, kegagalan besar terakhir yang kami alami baru-baru ini.  Saat menyuntikkan sistem baru, kami lupa mengatur Skor OOM untuk MySQL kami.  Ada layanan lain yang membaca binlog.  Pada titik tertentu, operator kami adalah manual - lagi secara manual!  - menjalankan perintah untuk menghapus beberapa informasi dalam tabel checksum Percona.  Hanya penghapusan sederhana, operasi sederhana, tetapi operasi ini menghasilkan binlog yang sangat besar.  Layanan membaca binlog ini ke dalam memori, OOM Killer datang dan berpikir siapa yang harus dibunuh?  Dan kami lupa mengatur Skor OOM, dan itu membunuh MySQL! <br><br>  Kami memiliki 40 master sekarat pukul 4 pagi.  Ketika 40 master mati, itu benar-benar sangat menakutkan dan berbahaya.  DRT tidak menakutkan dan tidak berbahaya.  Kami berbaring sekitar satu jam. <br><br>  Ngomong-ngomong, DRT adalah cara yang baik untuk melatih saat-saat seperti itu sehingga kita tahu persis urutan tindakan yang diperlukan jika sesuatu rusak secara massal. <br><br><blockquote>  - Saya ingin belajar lebih banyak tentang beralih master-master.  Pertama, mengapa sebuah cluster tidak digunakan, misalnya?  Cluster database, yaitu, bukan master-slave dengan switching, tetapi aplikasi master-master, sehingga jika ada yang jatuh, maka itu tidak menakutkan. </blockquote><br>  Apakah maksud Anda seperti replikasi grup, galera cluster, dll?  Sepertinya saya bahwa aplikasi grup belum siap seumur hidup.  Sayangnya, kami belum mencoba Galera.  Ini bagus ketika faylover ada di dalam protokol Anda, tetapi, sayangnya, mereka memiliki banyak masalah lain, dan tidak mudah untuk beralih ke solusi ini. <br><br><blockquote>  - Tampaknya di MySQL 8 ada sesuatu seperti InnoDb cluster.  Tidak mencoba? </blockquote><br>  Kami masih memiliki nilai 5,6.  Saya tidak tahu kapan kami akan beralih ke 8. Mungkin kami akan mencoba. <br><br><blockquote>  - Dalam hal ini, jika Anda memiliki satu master besar, ketika berpindah dari satu master ke master lainnya, ternyata antriannya terakumulasi di server slave dengan beban tinggi.  Jika master dipadamkan, apakah perlu untuk mencapai antrian, sehingga budak beralih ke mode master - atau apakah dilakukan dengan cara yang berbeda? </blockquote><br>  Beban pada master diatur oleh semisync.  Semisync membatasi rekaman master ke kinerja server slave.  Tentu saja, mungkin transaksi itu datang, semisync berhasil, tetapi para budak kehilangan transaksi ini untuk waktu yang sangat lama.  Anda kemudian harus menunggu sampai budak kehilangan transaksi ini sampai akhir. <br><br><blockquote>  - Tapi kemudian data baru akan datang untuk dikuasai, dan itu akan diperlukan ... </blockquote><br>  Ketika kami memulai proses promosi, kami menonaktifkan I / O.  Setelah itu, master tidak dapat menulis apa pun karena semisync direplikasi.  Pembacaan hantu mungkin datang, sayangnya, tetapi ini sudah merupakan masalah lain. <br><br><blockquote>  - Ini semua mesin negara yang indah - skrip apa yang ditulis dan seberapa sulit untuk menambahkan langkah baru?  Apa yang perlu dilakukan untuk orang yang menulis sistem ini? </blockquote><br>  Semua skrip ditulis dengan Python, semua layanan ditulis dalam Go.  Ini adalah kebijakan kami.  Mengubah logika itu mudah - cukup dengan kode Python yang menghasilkan diagram keadaan. <br><br><blockquote>  - Dan Anda dapat membaca lebih lanjut tentang pengujian.  Bagaimana tes ditulis, bagaimana mereka menggunakan node dalam mesin virtual - apakah ini wadah? </blockquote><br>  Ya  Kami akan menguji dengan bantuan Bazel.  Ada beberapa file konfigurasi (json) dan Bazel mengambil skrip yang menciptakan topologi untuk pengujian kami menggunakan file konfigurasi ini.  Topologi yang berbeda dijelaskan di sana. <br><br>  Itu semua bekerja untuk kita dalam wadah buruh pelabuhan: baik itu bekerja di CI atau di Devbox.  Kami memiliki sistem Devbox.  Kita semua berkembang di beberapa server jarak jauh, dan ini dapat bekerja di dalamnya, misalnya.  Di sana juga berjalan di dalam Bazel, di dalam wadah buruh pelabuhan atau di Bazel Sandbox.  Bazel sangat rumit tapi menyenangkan. <br><br><blockquote>  - Ketika Anda membuat 4 instance pada satu server, apakah Anda kehilangan efisiensi memori? </blockquote><br>  Setiap instance menjadi lebih kecil.  Dengan demikian, semakin sedikit memori yang beroperasi dengan MySQL, semakin mudah untuk hidup.  Sistem apa pun lebih mudah dioperasikan dengan sedikit memori.  Di tempat ini, kami tidak kehilangan apapun.  Kami memiliki grup C paling sederhana yang membatasi instance ini dari memori. <br><br><blockquote>  - Jika Anda memiliki 6.000 server yang menyimpan basis data, dapatkah Anda menyebutkan berapa miliar petabyte yang disimpan dalam file Anda? </blockquote><br>  Ini adalah puluhan exabytes, kami menuangkan data dari Amazon selama setahun. <br><br><blockquote>  - Ternyata pada awalnya Anda memiliki 8 server, 200 pecahan pada mereka, kemudian 400 server dengan 4 pecahan masing-masing.  Anda memiliki 1600 pecahan - apakah ini semacam nilai hard-coded?  Bisakah kamu tidak melakukannya lagi?  Apakah akan sakit jika Anda membutuhkan, misalnya, 3.200 pecahan? </blockquote><br>  Ya, ini awalnya 1600. Ini dilakukan kurang dari 10 tahun yang lalu, dan kami masih hidup.  Tapi kami masih memiliki 4 pecahan - 4 kali kami masih bisa menambah ruang. <br><br><blockquote>  - Bagaimana server mati, terutama karena alasan apa?  Apa yang terjadi lebih sering, lebih jarang, dan yang sangat menarik, apakah blok spontan terjadi? </blockquote><br>  Yang paling penting adalah bahwa disk terbang keluar.  Kami memiliki RAID 0 - disk macet, master meninggal.  Ini adalah masalah utama, tetapi lebih mudah bagi kami untuk mengganti server ini.  Google lebih mudah untuk mengganti pusat data, kami masih memiliki server.  Kami hampir tidak pernah memiliki checksum Korupsi.  Sejujurnya, saya tidak ingat kapan terakhir kali.  Kami hanya sering memperbarui wizard.  Waktu hidup kami untuk satu master terbatas hingga 60 hari.  Itu tidak bisa hidup lebih lama, setelah itu kami menggantinya dengan server baru, karena untuk beberapa alasan sesuatu terus-menerus terakumulasi di MySQL, dan setelah 60 hari kami melihat bahwa masalah mulai terjadi.  Mungkin tidak di MySQL, mungkin di Linux. <br><br>   ,          .     60 ,    .      . <br><br><blockquote> â€”  ,    6        . ,   JPEG   ,     JPEG,  ,      ?  , ,      -   ?    â€”      ,       ? </blockquote><br>     ,  .   â€”  Dropbox    . <br><br><blockquote> â€”      ?         ?     , ,  - ,    , ? ,   10   . ,  7     ,    6    ,    .    ? </blockquote><br>   Dropbox  - ,       .   .  ,    ,       ,   -  . <br><br>  ,    .  ,  ,      ,      .        - ,     6 ,   ,     ,    ,    . <br></div></div><br><blockquote>       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="></a> ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">facebook</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">youtube-</a> â€”          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Highload++ 2018</a> .      , <strong> 1 </strong>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">  </a> . <br></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id417315/">https://habr.com/ru/post/id417315/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id417303/index.html">Retas untuk mendukung tombol headset Android Windows</a></li>
<li><a href="../id417305/index.html">Ultima Online: tampilan belakang panggung</a></li>
<li><a href="../id417307/index.html">Glaukoma - belum pernah mendengar tentang dia? Temui pembunuh visi diam seri</a></li>
<li><a href="../id417309/index.html">Manajer ITSM Untungnya: Bagaimana Profesi Masa Depan Membantu Memperluas Perbatasan Meja Layanan</a></li>
<li><a href="../id417311/index.html">Membuat bot untuk berpartisipasi dalam AI mini cup 2018 berdasarkan pada jaringan saraf berulang</a></li>
<li><a href="../id417317/index.html">Di Highload ++ 2018, kecepatan penuh ada di depan</a></li>
<li><a href="../id417319/index.html">Sistem dalam kasus atau Apa yang sebenarnya di bawah penutup mikroprosesor</a></li>
<li><a href="../id417321/index.html">Bagaimana kita mencari guru kursus online di antara pengembang?</a></li>
<li><a href="../id417323/index.html">Masalah memastikan aksesibilitas proyek 100%</a></li>
<li><a href="../id417325/index.html">Netrology Open Day, Tema Ilmu Data</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>