<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üçÜ üîè ‚è∞ Maske-R CNN vom Anf√§nger bis zum Profi üëêüèª üïü üõ´</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nachdem ich die Informationen aus dem Bild und an der Ausgabe analysieren musste, um den Typ des Objekts und den Typ zu ermitteln, sowie die Menge der...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Maske-R CNN vom Anf√§nger bis zum Profi</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/483018/"><p><img src="https://lh6.googleusercontent.com/2ZHVaccuXIFgG1Z5qnHROW6cKyxjEe1RLo0SwRAaOxxyQI7Q_Ymbs7ZuZ6bOs56v7oMaCInsarFjOPZRDWL5hNdKpVhlVtHINo9u3gae4utQB-FARZPFxEV5UmkYY8LVJdTjZ0PK"></p><br><p> Nachdem ich die Informationen aus dem Bild und an der Ausgabe analysieren musste, um den Typ des Objekts und den Typ zu ermitteln, sowie die Menge der Bilder zu analysieren, musste ich die Kennung des Objekts und die im Bild verbrachte Zeit angeben, um festzustellen, wie sich das Objekt bewegte und welche Kameras sichtbar wurden.  Beginnen wir vielleicht mit den ersten beiden. Die Analyse des Personals im Aggregat wird im n√§chsten Teil er√∂rtert. </p><a name="habracut"></a><br><p>  Nun, wir werden unsere Aufgaben detaillierter beschreiben: </p><br><ul><li>  Repariere Personen und Autos - w√§hle sie im Bild aus und generiere die entsprechenden Klasseninstanzen mit den erforderlichen Feldern. </li><li>  Bestimmen Sie die Nummer des Autos, wenn es in den Rahmen einer bestimmten Kamera gefallen ist </li><li>  Vergleichen Sie den aktuellen Frame mit dem vorherigen, um die Gleichheit der Objekte zu ermitteln </li></ul><br><p>  Ok, dachte ich und nahm eine dicke Schlange, Python, das hei√üt.  Es wurde beschlossen, das neuronale Netz <a href="https://github.com/matterport/Mask_RCNN">Mask R-Cnn</a> in Verbindung mit seiner Einfachheit und den <a href="https://habr.com/ru/post/421299/">modernen Funktionen zu verwenden</a> .  Nat√ºrlich werden wir auch OpenCV f√ºr die Bildbearbeitung verwenden. </p><br><h2 id="ustanovka-sredy">  Umgebungs-Setup </h2><br><p>  Wir werden Windows 10 verwenden, da es sehr wahrscheinlich ist, dass Sie es verwenden. <br>  Es versteht sich, dass Sie bereits 64-Bit-Python haben.  Wenn nicht, k√∂nnen Sie das Paket beispielsweise <a href="https://www.python.org/downloads/release/python-374/">von hier</a> herunterladen </p><br><h3 id="ustanovka-paketov">  Paketinstallation </h3><br><pre><code class="plaintext hljs">git clone https://github.com/matterport/Mask_RCNN cd Mask_RCNN pip3 install -r requirements.txt python3 setup.py install</code> </pre> <br><p>  Wenn es aus irgendeinem Grund nicht m√∂glich ist, aus dem Quellcode zu kompilieren, gibt es eine Version von pip: </p><br><pre> <code class="plaintext hljs">pip3 install mrcnn --user</code> </pre> <br><p>  Das Paket enth√§lt nat√ºrlich alle <a href="https://github.com/matterport/Mask_RCNN/blob/master/requirements.txt">Abh√§ngigkeiten</a> . </p><br><h2 id="etap-1-sozdanie-prosteyshey-programmy-raspoznavatelya">  Stufe 1. Erstellen eines einfachen Erkenners. </h2><br><p>  Wir werden die notwendigen Importe machen </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mrcnn.config <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mrcnn <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mrcnn.model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MaskRCNN</code> </pre> <br><p>  F√ºr ein neuronales Netzwerk muss eine Konfiguration mit √ºberschriebenen Feldern erstellt werden </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">MaskRCNNConfig</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(mrcnn.config.Config)</span></span></span><span class="hljs-class">:</span></span> NAME = <span class="hljs-string"><span class="hljs-string">"coco_pretrained_model_config"</span></span> GPU_COUNT = <span class="hljs-number"><span class="hljs-number">1</span></span> IMAGES_PER_GPU = <span class="hljs-number"><span class="hljs-number">1</span></span> DETECTION_MIN_CONFIDENCE = <span class="hljs-number"><span class="hljs-number">0.8</span></span> <span class="hljs-comment"><span class="hljs-comment">#     NUM_CLASSES = 81</span></span></code> </pre> <br><p>  Geben Sie den Speicherort der Datei mit der Waage an.  Lassen Sie es sich in diesem Beispiel in dem Ordner mit dieser Datei befinden.  Wenn dies nicht der Fall ist, wird es heruntergeladen. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mrcnn.utils DATASET_FILE = <span class="hljs-string"><span class="hljs-string">"mask_rcnn_coco.h5"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> os.path.exists(DATASET_FILE): mrcnn.utils.download_trained_weights(DATASET_FILE)</code> </pre> <br><p>  Lassen Sie uns unser Modell mit den obigen Einstellungen erstellen </p><br><pre> <code class="python hljs">model = MaskRCNN(mode=<span class="hljs-string"><span class="hljs-string">"inference"</span></span>, model_dir=<span class="hljs-string"><span class="hljs-string">"logs"</span></span>, config=MaskRCNNConfig()) model.load_weights(DATASET_FILE, by_name=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><p>  Und vielleicht beginnen wir mit der Verarbeitung aller Bilder im <code>images</code> des aktuellen Verzeichnisses. </p><br><pre> <code class="python hljs">IMAGE_DIR = os.path.join(os.getcwd(), <span class="hljs-string"><span class="hljs-string">"images"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> filename <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> os.listdir(IMAGE_DIR): image = cv2.imread(os.path.join(IMAGE_DIR, filename)) rgb_image = image[:, :, ::<span class="hljs-number"><span class="hljs-number">-1</span></span>] detections = model.detect([rgb_image], verbose=<span class="hljs-number"><span class="hljs-number">1</span></span>)[<span class="hljs-number"><span class="hljs-number">0</span></span>]</code> </pre> <br><p>  Was werden wir in Entdeckungen sehen? </p><br><pre> <code class="python hljs"> print(detections)</code> </pre> <br><p>  Zum Beispiel etwas √Ñhnliches: </p><br><pre> <code class="plaintext hljs">{'rois': array([[ 303, 649, 542, 1176],[ 405, 2, 701, 319]]), 'class_ids': array([3, 3]), 'scores': array([0.99896, 0.99770015], dtype=float32), 'masks': array()}</code> </pre> <br><p>  In diesem Fall wurden 2 Objekte gefunden. <br>  <code>rois</code> - Koordinatenfelder der unteren linken und oberen rechten Ecke <br>  <code>class_ids</code> - numerische Bezeichner der gefundenen Objekte, wobei wir wissen m√ºssen, dass 1 eine Person, 3 ein Auto und 8 ein Lastwagen ist. <br>  <code>scores</code> - Soweit das Modell von der L√∂sung √ºberzeugt ist, kann dieser Parameter √ºber <code>DETECTION_MIN_CONFIDENCE</code> in der Konfiguration <code>DETECTION_MIN_CONFIDENCE</code> werden, wodurch alle unangemessenen Optionen abgeschnitten werden. <br>  <code>masks</code> - die Kontur des Objekts.  Daten werden zum Zeichnen einer Objektmaske verwendet.  Weil  Sie sind recht umfangreich und nicht f√ºr das menschliche Verst√§ndnis gedacht, ich werde sie im Artikel nicht zitieren. </p><br><p>  Ok, wir k√∂nnten hier aufh√∂ren, aber wir wollen uns das Bild ansehen, das Anleitungen zur Verwendung neuronaler Netze mit sch√∂n ausgew√§hlten Objekten normalerweise geben? </p><br><p>  Es w√§re einfacher, die Funktion <code>mrcnn.visualize.display_instances</code> , aber wir werden dies nicht tun, wir werden unsere eigene schreiben. </p><br><p>  Die Funktion nimmt ein Bild auf und die wichtigsten Parameter werden aus dem W√∂rterbuch der ersten Schritte ermittelt. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">visualize_detections</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(image, masks, boxes, class_ids, scores)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np bgr_image = image[:, :, ::<span class="hljs-number"><span class="hljs-number">-1</span></span>] CLASS_NAMES = [<span class="hljs-string"><span class="hljs-string">'BG'</span></span>,<span class="hljs-string"><span class="hljs-string">"person"</span></span>, <span class="hljs-string"><span class="hljs-string">"bicycle"</span></span>, <span class="hljs-string"><span class="hljs-string">"car"</span></span>, <span class="hljs-string"><span class="hljs-string">"motorcycle"</span></span>, <span class="hljs-string"><span class="hljs-string">"bus"</span></span>, <span class="hljs-string"><span class="hljs-string">"truck"</span></span>] COLORS = mrcnn.visualize.random_colors(len(CLASS_NAMES)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(boxes.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]): y1, x1, y2, x2 = boxes[i] classID = class_ids[i] label = CLASS_NAMES[classID] font = cv2.FONT_HERSHEY_DUPLEX color = [int(c) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> c <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> np.array(COLORS[classID]) * <span class="hljs-number"><span class="hljs-number">255</span></span>] text = <span class="hljs-string"><span class="hljs-string">"{}: {:.3f}"</span></span>.format(label, scores[i]) size = <span class="hljs-number"><span class="hljs-number">0.8</span></span> width = <span class="hljs-number"><span class="hljs-number">2</span></span> cv2.rectangle(bgr_image, (x1, y1), (x2, y2), color, width) cv2.putText(bgr_image, text, (x1, y1<span class="hljs-number"><span class="hljs-number">-20</span></span>), font, size, color, width)</code> </pre><br><p><img src="https://lh3.googleusercontent.com/dnSAiGZW32zMK92_T8yyk2nXCFCKECQ_eSdNiVv5Bzpz9TOsBZT6_jyAY-LfUT4c0jCzfdgFOCpy6_0HzT54CAPo3vvkZ6VgR1U5cdmSTb0zLLpAxJjX-_pTNUnpIExXsao_u29b"></p><br><div class="spoiler">  <b class="spoiler_title">Quellbild</b> <div class="spoiler_text"><p><img src="https://sun9-10.userapi.com/c854324/v854324789/e2f73/nJHcTGLnWI4.jpg"></p></div></div><br><p>  Obwohl einer der Hauptvorteile dieses neuronalen Netzwerks die L√∂sung der Probleme der Instanzsegmentierung ist - das Erhalten der Konturen von Objekten, haben wir es noch nicht verwendet, wir werden es analysieren. </p><br><p>  F√ºgen Sie zum Implementieren von Masken ein paar Linien hinzu, bevor Sie f√ºr jedes gefundene Objekt ein Rechteck zeichnen. </p><br><pre> <code class="python hljs">mask = masks[:, :, i] <span class="hljs-comment"><span class="hljs-comment">#   image = mrcnn.visualize.apply_mask(image, mask, color, alpha=0.6) #  </span></span></code> </pre> <br><p>  Ergebnis: <br><img src="https://lh6.googleusercontent.com/2ZHVaccuXIFgG1Z5qnHROW6cKyxjEe1RLo0SwRAaOxxyQI7Q_Ymbs7ZuZ6bOs56v7oMaCInsarFjOPZRDWL5hNdKpVhlVtHINo9u3gae4utQB-FARZPFxEV5UmkYY8LVJdTjZ0PK"></p><br><div class="spoiler">  <b class="spoiler_title">Version mit wei√üen Masken</b> <div class="spoiler_text"><p><img src="https://lh5.googleusercontent.com/JUUlUQfOCc9jeeuzMDiSc2Hd06P2aMVli2UPSRkUoxlNVwdlwEfi-BHmuyzOsx-nlvm19lPmYlyxFGYV9xGzpppXRORmDBLtLYH6UcYRh1zO47ROLb04HgUggDoz14Zk2AWZ3ta3"></p></div></div><br><h2 id="etap-ii-pervye-uspehi-raspoznavanie-nomerov-mashin">  Stufe II.  Erste Erfolge.  Erkennung der Anzahl der Autos. </h2><br><p>  Zur Erkennung ben√∂tigen wir einen klaren Rahmen des Autos in der N√§he, daher wurde beschlossen, nur Rahmen vom Kontrollpunkt zu nehmen und diese dann mit √Ñhnlichkeiten zu vergleichen (mehr dazu im n√§chsten Kapitel).  Diese Methode ergibt jedoch zu viel Ungenauigkeit, weil  Maschinen k√∂nnen visuell sehr √§hnlich sein und mein Algorithmus kann solche Situationen noch nicht vermeiden. </p><br><p>  Es wurde beschlossen, eine fertige <a href="https://github.com/ria-com/nomeroff-net">Bibliothek</a> des ukrainischen Herstellers <a href="https://github.com/ria-com/nomeroff-net">nomeroff-net zu verwenden</a> (keine Werbung).  Weil  Fast der gesamte Code befindet sich in den Beispielen f√ºr das Modell. Ich werde dann keine vollst√§ndige Beschreibung geben. </p><br><p>  Ich kann nur sagen, dass diese Funktion mit dem Originalbild gestartet werden kann oder die erkannte Maschine aus dem Rahmen ausgeschnitten und an diese Funktion √ºbergeben werden kann. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.image <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> mpimg <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os sys.path.append(cfg.NOMEROFF_NET_DIR) <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> NomeroffNet <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> filters, RectDetector, TextDetector, OptionsDetector, Detector, textPostprocessing nnet = Detector(cfg.MASK_RCNN_DIR, cfg.MASK_RCNN_LOG_DIR) nnet.loadModel(<span class="hljs-string"><span class="hljs-string">"latest"</span></span>) rectDetector = RectDetector() optionsDetector = OptionsDetector() optionsDetector.load(<span class="hljs-string"><span class="hljs-string">"latest"</span></span>) textDetector = TextDetector.get_static_module(<span class="hljs-string"><span class="hljs-string">"ru"</span></span>)() textDetector.load(<span class="hljs-string"><span class="hljs-string">"latest"</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">detectCarNumber</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(imgPath: str)</span></span></span><span class="hljs-function"> -&gt; str:</span></span> img = mpimg.imread(imgPath) NP = nnet.detect([img]) cvImgMasks = filters.cv_img_mask(NP) arrPoints = rectDetector.detect(cvImgMasks) zones = rectDetector.get_cv_zonesBGR(img, arrPoints) regionIds, stateIds, _c = optionsDetector.predict(zones) regionNames = optionsDetector.getRegionLabels(regionIds) <span class="hljs-comment"><span class="hljs-comment"># find text with postprocessing by standart textArr = textDetector.predict(zones) textArr = textPostprocessing(textArr, regionNames) return textArr</span></span></code> </pre> <br><p>  Der AusgabetextArr repr√§sentiert ein Array von Zeichenfolgen mit der Anzahl der auf dem Frame gefundenen Computer. Beispiel: <br>  <code>["293163"]</code> oder <code>[""]</code> , <code>[]</code> - wenn keine √ºbereinstimmenden Nummern gefunden wurden. </p><br><h2 id="etap-iii-opoznaem-obekty-na-shozhest">  Stufe III.  Identifizieren Sie Objekte anhand ihrer √Ñhnlichkeit. </h2><br><p>  Jetzt m√ºssen wir verstehen, wie ein Objekt einmal repariert wird, um zu verstehen, dass es sich im n√§chsten Frame um ihn handelt.  In diesem Stadium gehen wir davon aus, dass wir nur eine Kamera haben und nur zwischen verschiedenen Frames unterscheiden. </p><br><p>  Dazu m√ºssen Sie herausfinden, wie wir die beiden Objekte vergleichen. </p><br><p>  Ich werde f√ºr diese Zwecke einen <a href="https://en.wikipedia.org/wiki/Scale-invariant_feature_transform">Siebalgorithmus</a> vorschlagen.  Wir machen eine Reservierung, dass es nicht Teil des Hauptteils von OpenCV ist, deshalb m√ºssen wir Contrib-Module zus√§tzlich liefern.  Leider ist der Algorithmus patentiert und seine Verwendung in kommerziellen Programmen ist begrenzt.  Aber wir konzentrieren uns auf Forschungsaktivit√§ten, oder? </p><br><pre> <code class="plaintext hljs">pip3 install opencv-contrib-python --user</code> </pre> <br><p>  ~~ √úberladen Sie den Operator == ~~ Wir schreiben eine Funktion, die 2 verglichene Objekte in Form von Matrizen nimmt.  Zum Beispiel bekommen wir sie nach dem Aufruf der Funktion <code>cv2.open(path)</code> </p><br><p>  Wir werden eine Implementierung unseres Algorithmus schreiben. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">compareImages</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(img1, img2)</span></span></span><span class="hljs-function"> -&gt; bool:</span></span> sift = cv2.xfeatures2d.SIFT_create()</code> </pre> <br><p>  Finden Sie wichtige Punkte und Deskriptoren mit SIFT.  Vielleicht <code>help(somefunc)</code> ich f√ºr diese Funktionen keine Hilfe an, da Sie sie in der interaktiven Shell immer als <code>help(somefunc)</code> </p><br><pre> <code class="python hljs"> kp1, des1 = sift.detectAndCompute(img1, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>) kp2, des2 = sift.detectAndCompute(img2, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>)</code> </pre> <br><p>  Richten Sie unseren Algorithmus ein. </p><br><pre> <code class="python hljs"> FLANN_INDEX_KDTREE = <span class="hljs-number"><span class="hljs-number">0</span></span> indexParams = dict(algorithm=FLANN_INDEX_KDTREE, trees=<span class="hljs-number"><span class="hljs-number">5</span></span>) searchParams = dict(checks=<span class="hljs-number"><span class="hljs-number">50</span></span>) flann = cv2.FlannBasedMatcher(indexParams, searchParams)</code> </pre> <br><p>  F√ºhren Sie es jetzt aus. </p><br><pre> <code class="python hljs"> matches = flann.knnMatch(des1, des2, k=<span class="hljs-number"><span class="hljs-number">2</span></span>)</code> </pre> <br><p>  Z√§hlen Sie die √Ñhnlichkeiten zwischen den Bildern. </p><br><pre> <code class="python hljs"> matchesCount = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> m, n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> matches: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> m.distance &lt; cfg.cencitivity*n.distance: matchesCount += <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> matchesCount &gt; cfg.MIN_MATCH_COUNT</code> </pre> <br><p>  Versuchen Sie jetzt, es zu verwenden <br>  Dazu m√ºssen wir nach dem Erkennen von Objekten diese aus dem Originalbild ausschneiden </p><br><p>  Ich k√∂nnte nichts besseres schreiben, als es f√ºr langsames Ged√§chtnis zu speichern und dann von dort zu lesen. </p><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">extractObjects</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(objects, binaryImage, outputImageDirectory, filename=None)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> item <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> objects: y1, x1, y2, x2 = item.coordinates <span class="hljs-comment"><span class="hljs-comment">#       cropped = binaryImage[y1:y2, x1:x2] beforePoint, afterPoint = filename.split(".") outputDirPath = os.path.join(os.path.split(outputImageDirectory)[0], "objectsOn" + beforePoint) if not os.path.exists(outputDirPath): os.mkdir(outputDirPath) coordinates = str(item).replace(" ", ",") pathToObjectImage = "{}{}.jpg".format(item.type, coordinates) cv2.imwrite(os.path.join(outputDirPath, str(pathToObjectImage)), cropped)</span></span></code> </pre> <br><p>  Jetzt haben wir die Objekte im <code>&lt;outputImageDirectory&gt;/objectsOn&lt;imageFilename&gt;</code> </p><br><p>  Wenn wir mindestens zwei solcher Verzeichnisse haben, k√∂nnen wir die darin enthaltenen Objekte vergleichen.  F√ºhren Sie die zuvor geschriebene Funktion aus </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> compareImages(previousObjects, currentObjects): print(‚Äú  !‚Äù)</code> </pre> <br><p>  Oder wir k√∂nnen eine andere Aktion ausf√ºhren, z. B. diese Objekte mit demselben Bezeichner kennzeichnen. </p><br><p>  Nat√ºrlich neigt dieses Netzwerk, wie alle neuronalen Netze, manchmal zu fehlerhaften Ergebnissen. </p><br><p>  Im Allgemeinen haben wir die 3 Aufgaben erledigt, die zu Beginn festgelegt wurden, sodass wir abrunden werden.  Ich bezweifle, dass dieser Artikel die Augen von Leuten ge√∂ffnet hat, die mindestens ein Programm geschrieben haben, das die Probleme der Bilderkennung / Bildsegmentierung l√∂st, aber ich hoffe, dass ich mindestens einem unerfahrenen Entwickler geholfen habe. </p><br><p>  Den vollst√§ndigen Quellcode des Projekts finden Sie <a href="https://github.com/Sapfir0/premier-eye">hier</a> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de483018/">https://habr.com/ru/post/de483018/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de483004/index.html">Kuleshov-Effekt in Disco Elysium: Wie Kontext Sinn schafft</a></li>
<li><a href="../de483008/index.html">Eine andere Zukunft - eine Spaltung der Menschheit</a></li>
<li><a href="../de483012/index.html">Antiquit√§ten: Roland MT-32, ein alternativer Sound f√ºr DOS-Spiele</a></li>
<li><a href="../de483014/index.html">PostgreSQL-Nachrichtenwarteschlangen mit PgQ</a></li>
<li><a href="../de483016/index.html">Eine kurze Geschichte der Weltraummikroprozessoren, Teil Zwei</a></li>
<li><a href="../de483024/index.html">‚ÄûWas haben Unternehmen mit Ihrer Privatsph√§re gemacht?‚Äú, Arthur Khachuyan</a></li>
<li><a href="../de483026/index.html">Java / Spring: So generieren Sie mit Speedment eine CRUD REST-API vollst√§ndig</a></li>
<li><a href="../de483030/index.html">API, die Sie zum Weinen bringt</a></li>
<li><a href="../de483032/index.html">Umzug aus der GUS in die Tschechische Republik, eigene Erfahrung (Teil 2)</a></li>
<li><a href="../de483036/index.html">n-Queens Completion Problem - linearer L√∂sungsalgorithmus</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>