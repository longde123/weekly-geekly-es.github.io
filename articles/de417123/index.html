<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üêøÔ∏è üôÉ üëø Integration von Spark Streaming und Kafka üëéüèª ü§∂üèæ üîº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Kollegen! Wir erinnern Sie daran, dass wir vor nicht allzu langer Zeit ein Buch √ºber Spark ver√∂ffentlicht haben und derzeit ein Buch √ºber Kafka ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Integration von Spark Streaming und Kafka</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/417123/">  Hallo Kollegen!  Wir erinnern Sie daran, dass wir vor nicht allzu langer Zeit ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Buch √ºber Spark</a> ver√∂ffentlicht haben und derzeit ein <a href="">Buch √ºber Kafka</a> das neueste Korrekturlesen durchl√§uft. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/it/cf/nj/itcfnjaffoo8apwikyd7_yfym5s.jpeg"></div><br>  Wir hoffen, dass diese B√ºcher erfolgreich genug sind, um das Thema fortzusetzen - zum Beispiel f√ºr die √úbersetzung und Ver√∂ffentlichung von Literatur zu Spark Streaming.  Wir wollten Ihnen heute eine √úbersetzung zur Integration dieser Technologie in Kafka anbieten. <br><a name="habracut"></a><br>  <b>1. Begr√ºndung</b> <br><br>  Apache Kafka + Spark Streaming ist eine der besten Kombinationen zum Erstellen von Echtzeitanwendungen.  In diesem Artikel werden wir die Details einer solchen Integration ausf√ºhrlich diskutieren.  Au√üerdem sehen wir uns ein Beispiel mit Spark Streaming-Kafka an.  Anschlie√üend diskutieren wir den ‚ÄûEmpf√§ngeransatz‚Äú und die Option der direkten Integration von Kafka und Spark Streaming.  Beginnen wir also mit der Integration von Kafka und Spark Streaming. <br><br><img src="https://habrastorage.org/webt/8x/jl/cp/8xjlcpzhwdwi4w2g87iifbquvr0.jpeg"><br><br>  <b>2. Integration von Kafka und Spark Streaming</b> <br><br>  Bei der Integration von Apache Kafka und Spark Streaming gibt es zwei m√∂gliche Ans√§tze zum Konfigurieren von Spark Streaming zum Empfangen von Daten von Kafka - d. H.  zwei Ans√§tze zur Integration von Kafka und Spark Streaming.  Erstens k√∂nnen Sie Empf√§nger und die √ºbergeordnete Kafka-API verwenden.  Der zweite (neuere) Ansatz ist die Arbeit ohne Empf√§nger.  F√ºr beide Ans√§tze gibt es unterschiedliche Programmiermodelle, die sich beispielsweise in Bezug auf Leistung und semantische Garantien unterscheiden. <br><br><img src="https://habrastorage.org/webt/91/mn/sk/91mnsklu_81q0nx9aadnjgya4fc.png"><br><br>  Lassen Sie uns diese Ans√§tze genauer betrachten. <br><br>  <i><b>a.</b></i>  <i><b>Empf√§ngerbasierter Ansatz</b></i> <br><br>  In diesem Fall wird der Empfang der Daten vom Empf√§nger bereitgestellt.  Mit der von Kafka bereitgestellten High-Level-Verbrauchs-API implementieren wir den Empf√§nger.  Ferner werden die empfangenen Daten in Spark Artists gespeichert.  Anschlie√üend werden Jobs in Kafka - Spark Streaming gestartet, in denen Daten verarbeitet werden. <br><br>  Bei Verwendung dieses Ansatzes bleibt jedoch das Risiko eines Datenverlusts im Fehlerfall (mit der Standardkonfiguration) bestehen.  Infolgedessen muss in Kafka - Spark Streaming zus√§tzlich ein Write-Ahead-Protokoll eingef√ºgt werden, um Datenverluste zu vermeiden.  Somit werden alle von Kafka empfangenen Daten synchron im Write-Ahead-Protokoll in einem verteilten Dateisystem gespeichert.  Deshalb k√∂nnen auch nach einem Systemausfall alle Daten wiederhergestellt werden. <br><br>  Als N√§chstes sehen wir uns an, wie dieser Ansatz mit Empf√§ngern in einer Anwendung mit Kafka - Spark Streaming verwendet wird. <br><br>  <i>ich.</i>  <i>Bindung</i> <br><br>  Jetzt verbinden wir unsere Streaming-Anwendung mit dem folgenden Artefakt f√ºr Scala / Java-Anwendungen. Wir verwenden die Projektdefinitionen f√ºr SBT / Maven. <br><br><pre><code class="java hljs">groupId = org.apache.spark artifactId = spark-streaming-kafka-<span class="hljs-number"><span class="hljs-number">0</span></span>-<span class="hljs-number"><span class="hljs-number">8_2.11</span></span> version = <span class="hljs-number"><span class="hljs-number">2.2</span></span>.0</code> </pre> <br>  Bei der Bereitstellung unserer Anwendung m√ºssen wir jedoch die oben genannte Bibliothek und ihre Abh√§ngigkeiten hinzuf√ºgen. Dies wird f√ºr Python-Anwendungen ben√∂tigt. <br><br>  <i>ii.</i>  <i>Programmierung</i> <br><br>  Erstellen Sie als N√§chstes einen <code>DStream</code> Eingabestream, indem Sie <code>KafkaUtils</code> in den Stream-Anwendungscode importieren: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.streaming.kafka._ val kafkaStream = KafkaUtils.createStream(streamingContext, [ZK quorum], [consumer group id], [per-topic number of Kafka partitions to consume])</code> </pre> <br>  Dar√ºber hinaus k√∂nnen Sie mit den Optionen createStream Schl√ºsselklassen und Werteklassen sowie die entsprechenden Klassen f√ºr deren Dekodierung angeben. <br><br>  <i>iii.</i>  <i>Bereitstellung</i> <br><br>  Wie bei jeder Spark-Anwendung wird der Befehl spark-submit zum Starten verwendet.  Die Details unterscheiden sich jedoch geringf√ºgig in Scala / Java-Anwendungen und in Python-Anwendungen. <br><br>  Dar√ºber hinaus k√∂nnen Sie mit <code>‚Äìpackages</code> das <code>spark-streaming-Kafka-0-8_2.11</code> und seine Abh√§ngigkeiten direkt zum <code>spark-submit</code> <code>spark-streaming-Kafka-0-8_2.11</code> hinzuf√ºgen. Dies ist n√ºtzlich f√ºr Python-Anwendungen, bei denen es unm√∂glich ist, Projekte mit SBT / Maven zu verwalten. <br><br><pre> <code class="java hljs">./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-<span class="hljs-number"><span class="hljs-number">0</span></span>-<span class="hljs-number"><span class="hljs-number">8_2.11</span></span>:<span class="hljs-number"><span class="hljs-number">2.2</span></span>.0 ...</code> </pre> <br>  Sie k√∂nnen auch das JAR-Archiv der Maven-Artefakt- <code>spark-streaming-Kafka-0-8-assembly</code> aus dem Maven-Repository herunterladen.  F√ºgen Sie es dann zu <code>spark-submit</code> mit - <code>jars</code> . <br><br>  <i>b.</i>  <i>Direkte Ansprache (keine Empf√§nger)</i> <br><br>  Nach dem Ansatz unter Verwendung von Empf√§ngern wurde ein neuerer Ansatz entwickelt - der ‚Äûdirekte‚Äú.  Es bietet zuverl√§ssige End-to-End-Garantien.  In diesem Fall fragen wir Kafka regelm√§√üig nach Offsets von Offsets f√ºr jedes Thema / jeden Abschnitt und sorgen nicht f√ºr die Datenlieferung durch die Empf√§nger.  Zus√§tzlich wird die Gr√∂√üe des Lesefragments bestimmt, dies ist f√ºr die korrekte Verarbeitung jedes Pakets notwendig.  Schlie√ülich wird eine einfache konsumierende API verwendet, um Bereiche mit Daten von Kafka mit den angegebenen Offsets zu lesen, insbesondere wenn Datenverarbeitungsjobs gestartet werden.  Der gesamte Vorgang ist wie das Lesen von Dateien aus einem Dateisystem. <br><br>  Hinweis: Diese Funktion wurde in Spark 1.3 f√ºr Scala und die Java-API sowie in Spark 1.4 f√ºr die Python-API angezeigt. <br><br>  Lassen Sie uns nun diskutieren, wie dieser Ansatz in unserer Streaming-Anwendung angewendet wird. <br>  Die Consumer-API wird unter folgendem Link ausf√ºhrlicher beschrieben: <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Apache Kafka Verbraucher |</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beispiele f√ºr Kafka Consumer</a> <br><br>  ich.  Bindung <br><br>  Dieser Ansatz wird zwar nur in Scala / Java-Anwendungen unterst√ºtzt.  Erstellen Sie mit dem folgenden Artefakt das SBT / Maven-Projekt. <br><br><pre> <code class="java hljs">groupId = org.apache.spark artifactId = spark-streaming-kafka-<span class="hljs-number"><span class="hljs-number">0</span></span>-<span class="hljs-number"><span class="hljs-number">8_2.11</span></span> version = <span class="hljs-number"><span class="hljs-number">2.2</span></span>.0</code> </pre> <br>  <i>ii.</i>  <i>Programmierung</i> <br><br>  Importieren Sie als N√§chstes KafkaUtils und erstellen Sie einen Eingabe- <code>DStream</code> im Stream-Anwendungscode: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.streaming.kafka._ val directKafkaStream = KafkaUtils.createDirectStream[ [key <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class">], [</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">value</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">class</span></span></span><span class="hljs-class">], [</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">key</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">decoder</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">class</span></span></span><span class="hljs-class">], [</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">value</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">decoder</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">class</span></span></span><span class="hljs-class">] ]( </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">streamingContext</span></span></span><span class="hljs-class">, [</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">map</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">of</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Kafka</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">parameters</span></span></span><span class="hljs-class">], [</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">set</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">of</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">topics</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">to</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">consume</span></span></span><span class="hljs-class">])</span></span></code> </pre> <br>  In den Kafka-Parametern m√ºssen Sie entweder <code>metadata.broker.list</code> oder <code>bootstrap.servers</code> angeben.  Daher werden standardm√§√üig Daten ab dem letzten Versatz in jedem Abschnitt von Kafka verwendet.  Wenn Sie jedoch m√∂chten, dass der Messwert vom kleinsten Fragment <code>auto.offset.reset</code> m√ºssen Sie in den Kafka-Parametern die Konfigurationsoption <code>auto.offset.reset</code> . <br><br>  <code>KafkaUtils.createDirectStream</code> Sie mit den Optionen <code>KafkaUtils.createDirectStream</code> , k√∂nnen Sie au√üerdem von einem beliebigen Versatz aus mit dem Lesen beginnen.  Dann werden wir Folgendes tun, um auf die in jedem Paket verbrauchten Kafka-Fragmente zuzugreifen. <br><br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">//      ,        var offsetRanges = Array.empty[OffsetRange] directKafkaStream.transform { rdd =&gt; offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges rdd }.map { ... }.foreachRDD { rdd =&gt; for (o &lt;- offsetRanges) { println(s"${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}") } ... }</span></span></code> </pre> <br>  Wenn wir die √úberwachung von Kafka basierend auf Zookeeper mithilfe spezieller Tools organisieren m√∂chten, k√∂nnen wir Zookeeper mithilfe ihrer Hilfe selbst aktualisieren. <br><br>  <i>iii.</i>  <i>Bereitstellung</i> <br><br>  Der Bereitstellungsprozess √§hnelt in diesem Fall dem Bereitstellungsprozess in der Variante mit dem Empf√§nger. <br><br>  <b>3. Die Vorteile eines direkten Ansatzes</b> <br><br>  Der zweite Ansatz zur Integration von Spark Streaming in Kafka √ºbertrifft den ersten aus folgenden Gr√ºnden: <br><br>  <b><i>a.</i></b>  <b><i>Vereinfachte Parallelit√§t</i></b> <br><br>  In diesem Fall m√ºssen Sie nicht viele Kafka-Eingabestreams erstellen und kombinieren.  Mit Kafka-Spark-Streaming werden jedoch so viele RDD-Segmente erstellt, wie Kafka-Segmente f√ºr den Verbrauch vorhanden sind.  Alle diese Kafka-Daten werden parallel gelesen.  Daher k√∂nnen wir sagen, dass wir eine Eins-zu-Eins-Entsprechung zwischen den Segmenten Kafka und RDD haben werden, und ein solches Modell ist verst√§ndlicher und einfacher zu konfigurieren. <br><br>  <i><b>b.</b></i>  <i><b>Wirksamkeit</b></i> <br><br>  Um Datenverluste w√§hrend des ersten Ansatzes vollst√§ndig zu vermeiden, mussten die Informationen in einem Protokoll f√ºhrender Datens√§tze gespeichert und dann repliziert werden.  Tats√§chlich ist dies ineffizient, da die Daten zweimal repliziert werden: das erste Mal von Kafka selbst und das zweite Mal vom Write-Ahead-Protokoll.  Beim zweiten Ansatz wird dieses Problem beseitigt, da kein Empf√§nger vorhanden ist und daher kein f√ºhrendes Schreibjournal ben√∂tigt wird.  Wenn wir einen ausreichend langen Datenspeicher in Kafka haben, k√∂nnen Sie Nachrichten direkt von Kafka wiederherstellen. <br><br>  <b><i>s</i></b>  <b><i>Genau einmalige Semantik</i></b> <br><br>  Grunds√§tzlich haben wir beim ersten Ansatz die √ºbergeordnete Kafka-API verwendet, um verbrauchte Lesefragmente in Zookeeper zu speichern.  Dies ist jedoch die Gewohnheit, Daten von Kafka zu konsumieren.  W√§hrend Datenverluste zuverl√§ssig beseitigt werden k√∂nnen, besteht eine geringe Wahrscheinlichkeit, dass bei einigen Fehlern einzelne Datens√§tze zweimal verwendet werden k√∂nnen.  Der springende Punkt ist die Inkonsistenz zwischen dem zuverl√§ssigen Daten√ºbertragungsmechanismus in Kafka - Spark Streaming und dem Fragmentlesen in Zookeeper.  Daher verwenden wir im zweiten Ansatz die einfache Kafka-API, f√ºr die kein Zugriff auf Zookeeper erforderlich ist.  Hier werden die gelesenen Fragmente in Kafka - Spark Streaming verfolgt, dazu werden Kontrollpunkte verwendet.  In diesem Fall wird die Inkonsistenz zwischen Spark Streaming und Zookeeper / Kafka beseitigt. <br><br>  Daher empf√§ngt Spark Streaming auch bei Fehlern jeden Datensatz streng einmal.  Hier m√ºssen wir sicherstellen, dass unsere Ausgabeoperation, in der die Daten in einem externen Speicher gespeichert werden, entweder idempotent oder eine atomare Transaktion ist, in der sowohl die Ergebnisse als auch die Offsets gespeichert werden.  So wird genau einmalige Semantik bei der Ableitung unserer Ergebnisse erreicht. <br><br>  Es gibt jedoch einen Nachteil: Offsets in Zookeeper werden nicht aktualisiert.  Mit den auf Zookeeper basierenden √úberwachungstools von Kafka k√∂nnen Sie daher den Fortschritt nicht verfolgen. <br>  Wenn die Verarbeitung auf diese Weise angeordnet ist, k√∂nnen wir jedoch weiterhin auf Offsets verweisen. Wir wenden uns an jedes Paket und aktualisieren Zookeeper selbst. <br><br>  Das ist alles, wor√ºber wir √ºber die Integration von Apache Kafka und Spark Streaming sprechen wollten.  Wir hoffen es hat euch gefallen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de417123/">https://habr.com/ru/post/de417123/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de417111/index.html">Online-Konferenzen: Streaming vs Webinar</a></li>
<li><a href="../de417113/index.html">Italienischer 3D-Drucker in Russland: Raise3D N1 Dual - Modellierung und Prototyping</a></li>
<li><a href="../de417115/index.html">Flutter.io begraben oder verbrennen?</a></li>
<li><a href="../de417117/index.html">Reverse Engineering des NES-Emulators im Spiel f√ºr GameCube</a></li>
<li><a href="../de417119/index.html">Paginierung in Vue.js</a></li>
<li><a href="../de417125/index.html">RTC Meetup .Net: Zum ersten Meeting einladen</a></li>
<li><a href="../de417127/index.html">Tesla unterzeichnet Vereinbarung zum Bau von Gigafactory 3 in China</a></li>
<li><a href="../de417129/index.html">Universum des Geistes</a></li>
<li><a href="../de417131/index.html">Wie man Transaktionen in MongoDB jetzt f√ºhlt</a></li>
<li><a href="../de417135/index.html">Unity3D: Wie kann man den Beleuchtungsgrad eines Punktes in einer Szene herausfinden?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>