<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🐿️ 🙃 👿 Integration von Spark Streaming und Kafka 👎🏻 🤶🏾 🔼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Kollegen! Wir erinnern Sie daran, dass wir vor nicht allzu langer Zeit ein Buch über Spark veröffentlicht haben und derzeit ein Buch über Kafka ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Integration von Spark Streaming und Kafka</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/417123/">  Hallo Kollegen!  Wir erinnern Sie daran, dass wir vor nicht allzu langer Zeit ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Buch über Spark</a> veröffentlicht haben und derzeit ein <a href="">Buch über Kafka</a> das neueste Korrekturlesen durchläuft. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/it/cf/nj/itcfnjaffoo8apwikyd7_yfym5s.jpeg"></div><br>  Wir hoffen, dass diese Bücher erfolgreich genug sind, um das Thema fortzusetzen - zum Beispiel für die Übersetzung und Veröffentlichung von Literatur zu Spark Streaming.  Wir wollten Ihnen heute eine Übersetzung zur Integration dieser Technologie in Kafka anbieten. <br><a name="habracut"></a><br>  <b>1. Begründung</b> <br><br>  Apache Kafka + Spark Streaming ist eine der besten Kombinationen zum Erstellen von Echtzeitanwendungen.  In diesem Artikel werden wir die Details einer solchen Integration ausführlich diskutieren.  Außerdem sehen wir uns ein Beispiel mit Spark Streaming-Kafka an.  Anschließend diskutieren wir den „Empfängeransatz“ und die Option der direkten Integration von Kafka und Spark Streaming.  Beginnen wir also mit der Integration von Kafka und Spark Streaming. <br><br><img src="https://habrastorage.org/webt/8x/jl/cp/8xjlcpzhwdwi4w2g87iifbquvr0.jpeg"><br><br>  <b>2. Integration von Kafka und Spark Streaming</b> <br><br>  Bei der Integration von Apache Kafka und Spark Streaming gibt es zwei mögliche Ansätze zum Konfigurieren von Spark Streaming zum Empfangen von Daten von Kafka - d. H.  zwei Ansätze zur Integration von Kafka und Spark Streaming.  Erstens können Sie Empfänger und die übergeordnete Kafka-API verwenden.  Der zweite (neuere) Ansatz ist die Arbeit ohne Empfänger.  Für beide Ansätze gibt es unterschiedliche Programmiermodelle, die sich beispielsweise in Bezug auf Leistung und semantische Garantien unterscheiden. <br><br><img src="https://habrastorage.org/webt/91/mn/sk/91mnsklu_81q0nx9aadnjgya4fc.png"><br><br>  Lassen Sie uns diese Ansätze genauer betrachten. <br><br>  <i><b>a.</b></i>  <i><b>Empfängerbasierter Ansatz</b></i> <br><br>  In diesem Fall wird der Empfang der Daten vom Empfänger bereitgestellt.  Mit der von Kafka bereitgestellten High-Level-Verbrauchs-API implementieren wir den Empfänger.  Ferner werden die empfangenen Daten in Spark Artists gespeichert.  Anschließend werden Jobs in Kafka - Spark Streaming gestartet, in denen Daten verarbeitet werden. <br><br>  Bei Verwendung dieses Ansatzes bleibt jedoch das Risiko eines Datenverlusts im Fehlerfall (mit der Standardkonfiguration) bestehen.  Infolgedessen muss in Kafka - Spark Streaming zusätzlich ein Write-Ahead-Protokoll eingefügt werden, um Datenverluste zu vermeiden.  Somit werden alle von Kafka empfangenen Daten synchron im Write-Ahead-Protokoll in einem verteilten Dateisystem gespeichert.  Deshalb können auch nach einem Systemausfall alle Daten wiederhergestellt werden. <br><br>  Als Nächstes sehen wir uns an, wie dieser Ansatz mit Empfängern in einer Anwendung mit Kafka - Spark Streaming verwendet wird. <br><br>  <i>ich.</i>  <i>Bindung</i> <br><br>  Jetzt verbinden wir unsere Streaming-Anwendung mit dem folgenden Artefakt für Scala / Java-Anwendungen. Wir verwenden die Projektdefinitionen für SBT / Maven. <br><br><pre><code class="java hljs">groupId = org.apache.spark artifactId = spark-streaming-kafka-<span class="hljs-number"><span class="hljs-number">0</span></span>-<span class="hljs-number"><span class="hljs-number">8_2.11</span></span> version = <span class="hljs-number"><span class="hljs-number">2.2</span></span>.0</code> </pre> <br>  Bei der Bereitstellung unserer Anwendung müssen wir jedoch die oben genannte Bibliothek und ihre Abhängigkeiten hinzufügen. Dies wird für Python-Anwendungen benötigt. <br><br>  <i>ii.</i>  <i>Programmierung</i> <br><br>  Erstellen Sie als Nächstes einen <code>DStream</code> Eingabestream, indem Sie <code>KafkaUtils</code> in den Stream-Anwendungscode importieren: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.streaming.kafka._ val kafkaStream = KafkaUtils.createStream(streamingContext, [ZK quorum], [consumer group id], [per-topic number of Kafka partitions to consume])</code> </pre> <br>  Darüber hinaus können Sie mit den Optionen createStream Schlüsselklassen und Werteklassen sowie die entsprechenden Klassen für deren Dekodierung angeben. <br><br>  <i>iii.</i>  <i>Bereitstellung</i> <br><br>  Wie bei jeder Spark-Anwendung wird der Befehl spark-submit zum Starten verwendet.  Die Details unterscheiden sich jedoch geringfügig in Scala / Java-Anwendungen und in Python-Anwendungen. <br><br>  Darüber hinaus können Sie mit <code>–packages</code> das <code>spark-streaming-Kafka-0-8_2.11</code> und seine Abhängigkeiten direkt zum <code>spark-submit</code> <code>spark-streaming-Kafka-0-8_2.11</code> hinzufügen. Dies ist nützlich für Python-Anwendungen, bei denen es unmöglich ist, Projekte mit SBT / Maven zu verwalten. <br><br><pre> <code class="java hljs">./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-<span class="hljs-number"><span class="hljs-number">0</span></span>-<span class="hljs-number"><span class="hljs-number">8_2.11</span></span>:<span class="hljs-number"><span class="hljs-number">2.2</span></span>.0 ...</code> </pre> <br>  Sie können auch das JAR-Archiv der Maven-Artefakt- <code>spark-streaming-Kafka-0-8-assembly</code> aus dem Maven-Repository herunterladen.  Fügen Sie es dann zu <code>spark-submit</code> mit - <code>jars</code> . <br><br>  <i>b.</i>  <i>Direkte Ansprache (keine Empfänger)</i> <br><br>  Nach dem Ansatz unter Verwendung von Empfängern wurde ein neuerer Ansatz entwickelt - der „direkte“.  Es bietet zuverlässige End-to-End-Garantien.  In diesem Fall fragen wir Kafka regelmäßig nach Offsets von Offsets für jedes Thema / jeden Abschnitt und sorgen nicht für die Datenlieferung durch die Empfänger.  Zusätzlich wird die Größe des Lesefragments bestimmt, dies ist für die korrekte Verarbeitung jedes Pakets notwendig.  Schließlich wird eine einfache konsumierende API verwendet, um Bereiche mit Daten von Kafka mit den angegebenen Offsets zu lesen, insbesondere wenn Datenverarbeitungsjobs gestartet werden.  Der gesamte Vorgang ist wie das Lesen von Dateien aus einem Dateisystem. <br><br>  Hinweis: Diese Funktion wurde in Spark 1.3 für Scala und die Java-API sowie in Spark 1.4 für die Python-API angezeigt. <br><br>  Lassen Sie uns nun diskutieren, wie dieser Ansatz in unserer Streaming-Anwendung angewendet wird. <br>  Die Consumer-API wird unter folgendem Link ausführlicher beschrieben: <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Apache Kafka Verbraucher |</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beispiele für Kafka Consumer</a> <br><br>  ich.  Bindung <br><br>  Dieser Ansatz wird zwar nur in Scala / Java-Anwendungen unterstützt.  Erstellen Sie mit dem folgenden Artefakt das SBT / Maven-Projekt. <br><br><pre> <code class="java hljs">groupId = org.apache.spark artifactId = spark-streaming-kafka-<span class="hljs-number"><span class="hljs-number">0</span></span>-<span class="hljs-number"><span class="hljs-number">8_2.11</span></span> version = <span class="hljs-number"><span class="hljs-number">2.2</span></span>.0</code> </pre> <br>  <i>ii.</i>  <i>Programmierung</i> <br><br>  Importieren Sie als Nächstes KafkaUtils und erstellen Sie einen Eingabe- <code>DStream</code> im Stream-Anwendungscode: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.streaming.kafka._ val directKafkaStream = KafkaUtils.createDirectStream[ [key <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class">], [</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">value</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">class</span></span></span><span class="hljs-class">], [</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">key</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">decoder</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">class</span></span></span><span class="hljs-class">], [</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">value</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">decoder</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">class</span></span></span><span class="hljs-class">] ]( </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">streamingContext</span></span></span><span class="hljs-class">, [</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">map</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">of</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Kafka</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">parameters</span></span></span><span class="hljs-class">], [</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">set</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">of</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">topics</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">to</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">consume</span></span></span><span class="hljs-class">])</span></span></code> </pre> <br>  In den Kafka-Parametern müssen Sie entweder <code>metadata.broker.list</code> oder <code>bootstrap.servers</code> angeben.  Daher werden standardmäßig Daten ab dem letzten Versatz in jedem Abschnitt von Kafka verwendet.  Wenn Sie jedoch möchten, dass der Messwert vom kleinsten Fragment <code>auto.offset.reset</code> müssen Sie in den Kafka-Parametern die Konfigurationsoption <code>auto.offset.reset</code> . <br><br>  <code>KafkaUtils.createDirectStream</code> Sie mit den Optionen <code>KafkaUtils.createDirectStream</code> , können Sie außerdem von einem beliebigen Versatz aus mit dem Lesen beginnen.  Dann werden wir Folgendes tun, um auf die in jedem Paket verbrauchten Kafka-Fragmente zuzugreifen. <br><br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">//      ,        var offsetRanges = Array.empty[OffsetRange] directKafkaStream.transform { rdd =&gt; offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges rdd }.map { ... }.foreachRDD { rdd =&gt; for (o &lt;- offsetRanges) { println(s"${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}") } ... }</span></span></code> </pre> <br>  Wenn wir die Überwachung von Kafka basierend auf Zookeeper mithilfe spezieller Tools organisieren möchten, können wir Zookeeper mithilfe ihrer Hilfe selbst aktualisieren. <br><br>  <i>iii.</i>  <i>Bereitstellung</i> <br><br>  Der Bereitstellungsprozess ähnelt in diesem Fall dem Bereitstellungsprozess in der Variante mit dem Empfänger. <br><br>  <b>3. Die Vorteile eines direkten Ansatzes</b> <br><br>  Der zweite Ansatz zur Integration von Spark Streaming in Kafka übertrifft den ersten aus folgenden Gründen: <br><br>  <b><i>a.</i></b>  <b><i>Vereinfachte Parallelität</i></b> <br><br>  In diesem Fall müssen Sie nicht viele Kafka-Eingabestreams erstellen und kombinieren.  Mit Kafka-Spark-Streaming werden jedoch so viele RDD-Segmente erstellt, wie Kafka-Segmente für den Verbrauch vorhanden sind.  Alle diese Kafka-Daten werden parallel gelesen.  Daher können wir sagen, dass wir eine Eins-zu-Eins-Entsprechung zwischen den Segmenten Kafka und RDD haben werden, und ein solches Modell ist verständlicher und einfacher zu konfigurieren. <br><br>  <i><b>b.</b></i>  <i><b>Wirksamkeit</b></i> <br><br>  Um Datenverluste während des ersten Ansatzes vollständig zu vermeiden, mussten die Informationen in einem Protokoll führender Datensätze gespeichert und dann repliziert werden.  Tatsächlich ist dies ineffizient, da die Daten zweimal repliziert werden: das erste Mal von Kafka selbst und das zweite Mal vom Write-Ahead-Protokoll.  Beim zweiten Ansatz wird dieses Problem beseitigt, da kein Empfänger vorhanden ist und daher kein führendes Schreibjournal benötigt wird.  Wenn wir einen ausreichend langen Datenspeicher in Kafka haben, können Sie Nachrichten direkt von Kafka wiederherstellen. <br><br>  <b><i>s</i></b>  <b><i>Genau einmalige Semantik</i></b> <br><br>  Grundsätzlich haben wir beim ersten Ansatz die übergeordnete Kafka-API verwendet, um verbrauchte Lesefragmente in Zookeeper zu speichern.  Dies ist jedoch die Gewohnheit, Daten von Kafka zu konsumieren.  Während Datenverluste zuverlässig beseitigt werden können, besteht eine geringe Wahrscheinlichkeit, dass bei einigen Fehlern einzelne Datensätze zweimal verwendet werden können.  Der springende Punkt ist die Inkonsistenz zwischen dem zuverlässigen Datenübertragungsmechanismus in Kafka - Spark Streaming und dem Fragmentlesen in Zookeeper.  Daher verwenden wir im zweiten Ansatz die einfache Kafka-API, für die kein Zugriff auf Zookeeper erforderlich ist.  Hier werden die gelesenen Fragmente in Kafka - Spark Streaming verfolgt, dazu werden Kontrollpunkte verwendet.  In diesem Fall wird die Inkonsistenz zwischen Spark Streaming und Zookeeper / Kafka beseitigt. <br><br>  Daher empfängt Spark Streaming auch bei Fehlern jeden Datensatz streng einmal.  Hier müssen wir sicherstellen, dass unsere Ausgabeoperation, in der die Daten in einem externen Speicher gespeichert werden, entweder idempotent oder eine atomare Transaktion ist, in der sowohl die Ergebnisse als auch die Offsets gespeichert werden.  So wird genau einmalige Semantik bei der Ableitung unserer Ergebnisse erreicht. <br><br>  Es gibt jedoch einen Nachteil: Offsets in Zookeeper werden nicht aktualisiert.  Mit den auf Zookeeper basierenden Überwachungstools von Kafka können Sie daher den Fortschritt nicht verfolgen. <br>  Wenn die Verarbeitung auf diese Weise angeordnet ist, können wir jedoch weiterhin auf Offsets verweisen. Wir wenden uns an jedes Paket und aktualisieren Zookeeper selbst. <br><br>  Das ist alles, worüber wir über die Integration von Apache Kafka und Spark Streaming sprechen wollten.  Wir hoffen es hat euch gefallen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de417123/">https://habr.com/ru/post/de417123/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de417111/index.html">Online-Konferenzen: Streaming vs Webinar</a></li>
<li><a href="../de417113/index.html">Italienischer 3D-Drucker in Russland: Raise3D N1 Dual - Modellierung und Prototyping</a></li>
<li><a href="../de417115/index.html">Flutter.io begraben oder verbrennen?</a></li>
<li><a href="../de417117/index.html">Reverse Engineering des NES-Emulators im Spiel für GameCube</a></li>
<li><a href="../de417119/index.html">Paginierung in Vue.js</a></li>
<li><a href="../de417125/index.html">RTC Meetup .Net: Zum ersten Meeting einladen</a></li>
<li><a href="../de417127/index.html">Tesla unterzeichnet Vereinbarung zum Bau von Gigafactory 3 in China</a></li>
<li><a href="../de417129/index.html">Universum des Geistes</a></li>
<li><a href="../de417131/index.html">Wie man Transaktionen in MongoDB jetzt fühlt</a></li>
<li><a href="../de417135/index.html">Unity3D: Wie kann man den Beleuchtungsgrad eines Punktes in einer Szene herausfinden?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>