<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>😂 💍 🍅 Kubernetes Reservierung: Es besteht ⛹🏾 👨🏿‍⚖️ 👨🏽‍💻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Mein Name ist Sergey, ich komme von ITSumma und ich möchte Ihnen sagen, wie wir mit der Reservierung in Kubernetes umgehen. In letzter Zeit habe ich v...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Kubernetes Reservierung: Es besteht</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/itsumma/blog/452078/">  Mein Name ist Sergey, ich komme von ITSumma und ich möchte Ihnen sagen, wie wir mit der Reservierung in Kubernetes umgehen.  In letzter Zeit habe ich viele Beratungsarbeiten zur Implementierung einer Vielzahl von Devops-Lösungen für verschiedene Teams durchgeführt, und insbesondere arbeite ich eng an Projekten mit K8s.  Auf der Uptime Day 4-Konferenz, die sich der Redundanz in komplexen Architekturen widmete, hielt ich eine Präsentation über redundante Würfel, und hier ist seine kostenlose Nacherzählung.  Ich werde nur im Voraus warnen, dass er kein direkter Leitfaden zum Handeln ist, sondern eine Verallgemeinerung der Gedanken zu diesem Thema. <br><br><img src="https://habrastorage.org/webt/fi/pj/ox/fipjoxmwx-hrd0tvm0_bvgjaa-e.jpeg"><br><br>  Im Prinzip sind Überwachung und Redundanz die beiden Hauptinstrumente, um die Ausfallsicherheit eines Projekts zu erhöhen.  Aber in der Cuber ist alles von selbst ausgeglichen, Sie sagen, alles ist von selbst skaliert, und wenn etwas passiert, wird es von selbst aufsteigen ... Das heißt, während der ersten oberflächlichen Untersuchung des Themas hat mir das Internet die Frage beantwortet: "Wie funktioniert K8s Backup?" ? "  Viele Leute denken, dass ein Cuber so eine magische Sache ist, die alle Infrastrukturprobleme beseitigt und das Projekt niemals zum Erliegen bringt.  Aber ... die Welt ist nicht so, wie es scheint. <br><a name="habracut"></a><br>  Wie sind wir zuvor mit dem Sicherungsprozess umgegangen?  Wir hatten identische Plattformen für die Platzierung - entweder waren es virtuelle Maschinen oder es waren Eisenserver, auf die wir drei grundlegende Praktiken angewendet haben: <br><br><ol><li>  Codesynchronisation und Statik </li><li>  Konfigurationssynchronisation </li><li>  Datenbankreplikation </li></ol><br>  Und voila: In jedem Moment wechseln wir zur Reserveseite, alle sind glücklich, wir stehen auf, wir sind anderer Meinung. <br><br><img src="https://habrastorage.org/webt/mw/ym/zw/mwymzwrfdl9vsaf_hg-wm3wvphm.jpeg"><br><br>  Und was bieten sie uns, um die ständige Verfügbarkeit unserer Kubernetes-Anwendung zu erhöhen?  Das erste, was in der inoffiziellen Dokumentation steht, ist, viele Maschinen zu platzieren, viele Master zu erstellen - ihre Anzahl muss die Bedingungen für das Erreichen eines Quorums innerhalb des Clusters erfüllen, und so wird etcd, api, MC, Scheduler auf jedem der Master ausgelöst ... Und anscheinend ist alles in Ordnung : Wenn mehrere Arbeitsknoten oder Master ausfallen, wird unser Cluster neu ausgeglichen und die Anwendung funktioniert weiterhin.  Sieht wieder nach Magie aus!  Oft befindet sich unser Cluster jedoch im selben Rechenzentrum, was zu bestimmten Fragen führen kann.  Was ist, wenn ein Bagger ankam und ein Kabel ausgrub, blitzschnell, gab es eine universelle Flut?  Alles ist abgedeckt, unser Cluster ist nicht mehr.  Wie kann man sich der Reservierung unter Berücksichtigung dieser Seite des Problems nähern? <br><br>  Zunächst sollten Sie einen anderen Cluster in der Hot Reserve haben, dh einen Cluster, zu dem Sie jederzeit wechseln können.  In diesem Fall sollten die Infrastrukturen aus Sicht des Cuber völlig identisch sein.  Das heißt, wenn es nicht standardmäßige Plugins für die Arbeit mit dem Dateisystem gibt, benutzerdefinierte Lösungen für Ingress, sollten diese auf Ihren zwei (oder drei oder zehn, es gibt genug Geld und Administratoren) Clustern vollständig identisch sein.  Es müssen zwei Arten von Anwendungen klar definiert werden (Deployment'ov, Statefulset'ov, Daemonset'ov, Cronjob'ov usw.): Welche von ihnen können ständig an einer Reserve arbeiten und welche sollten besser nicht vor dem direkten Wechsel gestartet werden. <br><br>  Sollte unser Backup-Cluster also vollständig mit unserem Kampf-Cluster identisch sein?  Nein.  Früher haben wir im Rahmen der Arbeit mit monolithischen Projekten mit Eiseninfrastruktur eine fast völlig identische Umgebung beibehalten, aber im Rahmen des Cuber denke ich, dass dies nicht sein sollte.  Schauen wir uns an, warum. <br><br>  Beginnen wir zum Beispiel mit den grundlegenden Entitäten von Kubernetes - Bereitstellungen - sie müssen identisch sein.  Es sollten Anwendungen gestartet werden, die die Verkehrsverarbeitung jederzeit abfangen und es unserem Projekt ermöglichen, weiter zu leben.  Wenn es sich um Konfigurationsdateien handelt, müssen wir hier prüfen, ob sie identisch sein sollten oder nicht.  Das heißt, wenn wir, kluge Leute, keine verbotenen Substanzen verwenden und die Basis nicht in K8s behalten, müssen wir Zugriffseinstellungen in den Konfigurationskarten für die Kampfbasis haben (deren Sicherungsprozess separat erstellt wird).  Um den Zugriff auf die Sicherungsdatenbankinstanz sicherzustellen, benötigen wir daher eine separate Konfigurationsdatei (configmap).  Genau so arbeiten wir mit Geheimnissen: Passwörter für den Zugriff auf die Datenbank, API-Schlüssel;  Zu jeder Zeit kann entweder ein Kampfgeheimnis oder eine Reserve mit uns zusammenarbeiten.  Insgesamt haben wir bereits zwei Kubernetes-Entitäten, deren Backup-Versionen nicht mit den Kampfversionen identisch sein sollten.  Die nächste Entität, die es wert ist, darüber nachzudenken, ist Cronjob.  Cronjobs in Reserve sollten keinesfalls mit den Cronjob-Produktionsclustern identisch sein!  Wenn wir den Sicherungscluster erhöhen und ihn vollständig aktivieren, während alle Cronjobs aktiviert sind, erhalten beispielsweise zwei Briefe gleichzeitig von Ihnen anstelle von einem.  Oder eine Art Synchronisation von Daten mit externen Quellen findet zweimal statt, wir beginnen zu verletzen, zu weinen, zu schreien und zu schwören. <br><br><img src="https://habrastorage.org/webt/m3/n2/yx/m3n2yxmvocsvsfiavdmevq4vx18.jpeg"><br><br>  Aber wie bieten uns Leute aus dem Internet an, einen Backup-Cluster zu organisieren?  Die zweitbeliebteste Antwort nach "Warum?"  - Nutzung der Kubernetes Federation. <br><br>  Was ist das?  Dies ist beispielsweise ein großer Meta-Cluster.  Wenn wir uns die Architektur des Cuber vorstellen - wo wir einen Master, mehrere Knoten haben - dann haben wir aus Sicht der Föderation auch einen Master und mehrere Knoten, nur jeder Knoten ist ein separater Cluster.  Das heißt, wir arbeiten mit denselben Entitäten, mit denselben Grundelementen wie mit einem einzelnen Cuber, aber wir drehen und drehen nicht unsere physischen Maschinen, sondern ganze Cluster.  Im Rahmen des Bundes sind wir in der vollständigen Synchronisation der Bundesressourcen von den Eltern zu den Nachkommen.  Wenn wir beispielsweise eine Bereitstellung über den Verbund gestartet haben, wird diese in jedem unserer Tochtercluster bereitgestellt.  Wenn wir eine Konfigurationskarte verwenden, besteht das Geheimnis darin, sie an den Verband weiterzuleiten - sie wird sich auf alle unsere untergeordneten Cluster ausbreiten.  Gleichzeitig ermöglicht uns der Verband, unsere Ressourcen für Kinder anzupassen.  Das heißt, wir haben eine Konfigurationskarte erstellt, diese über den Verbund bereitgestellt. Wenn wir dann etwas an bestimmten Clustern beheben müssen, bearbeiten wir sie in einem separaten Cluster, und diese Änderung wird nirgendwo synchronisiert. <br><br>  Kubernetes Federation ist noch nicht so lange ein vorhandenes Tool und unterstützt nicht alle Ressourcen, die K8s selbst bereitstellt: Zum Zeitpunkt der Veröffentlichung einer der ersten Versionen der Dokumentation ging es darum, nur Konfigurationszuordnungen, Bereitstellung für Replikatsätze und Ingress zu unterstützen.  Geheimnisse wurden nicht unterstützt, die Arbeit mit Volumen wurde ebenfalls nicht unterstützt.  Zu begrenzter Satz.  Insbesondere wenn wir Spaß haben möchten, beispielsweise durch die benutzerdefinierte Ressourcendefinition, um unsere eigenen Ressourcen auf die Kubernetes zu übertragen, werden wir sie nicht in den Verbund übertragen.  Das heißt, sozusagen ... eine Entscheidung, die der Wahrheit sehr ähnlich ist, aber sie lässt uns regelmäßig in den Fuß schießen.  Auf der anderen Seite können Sie mit dem Verband unser Replikatset flexibel verwalten.  Beispielsweise möchten wir, dass 10 Replikate unserer Anwendung gestartet werden. Standardmäßig teilt der Verbund diese Anzahl proportional auf die Anzahl der Cluster auf.  Und das alles kann auch konfiguriert werden!  Das heißt, Sie können angeben, dass Sie 6 Replikate unserer Anwendung in einem Kampfcluster und nur 4 Replikate unserer Anwendung in einem Sicherungscluster aufbewahren müssen, um Ressourcen zu sparen oder zu Ihrer eigenen Unterhaltung.  Welches ist auch sehr praktisch.  Aber mit dem Verband müssen wir einige neue Lösungen verwenden, etwas unterwegs erledigen und uns zwingen, ein bisschen mehr nachzudenken ... <br><br>  Ist es möglich, den Prozess der Buchung eines Cubers irgendwie einfacher anzugehen?  Welche Werkzeuge haben wir überhaupt? <br><br>  Erstens haben wir immer eine Art CD / CD-System, das heißt, wir gehen nicht manuell herum, schreiben nicht erstellen / anwenden auf den Servern.  Das System erzeugt Yaml'ics für unsere Container. <br><br>  Zweitens gibt es mehrere Cluster, wir haben entweder eine oder mehrere (wenn wir klug sind) Registrierungen, die wir auch genommen und reserviert haben.  Und es gibt ein wunderbares kubectl-Dienstprogramm, das mit mehreren Clustern gleichzeitig arbeiten kann. <br><br><img src="https://habrastorage.org/webt/vs/ib/ed/vsibeda9uobcn8jtrfztjkln3ag.jpeg"><br><br>  Also: Meiner Meinung nach ist die einfachste und korrekteste Lösung zum Erstellen eines Sicherungsclusters eine primitive parallele Bereitstellung.  Es gibt eine Art Pipeline im ci / cd-System;  Zuerst bauen wir unsere Container, testen und rollen Anwendungen über kubectl in mehreren unabhängigen Clustern aus.  Wir können simultane Berechnungen für mehrere Cluster erstellen.  Dementsprechend beschließen wir auch, in dieser Phase Konfigurationen zu liefern.  Sie können die Konfigurationen für unseren Kampfcluster, die Konfigurationen für den Sicherungscluster vordefinieren und auf ci / cd-Ebene des Systems die Pro-Umgebung in den Pro-Cluster und die Sicherungsumgebung in den Sicherungscluster übertragen.  Im Vergleich zum Verband müssen Sie nicht für jeden untergeordneten Cluster eine Bundesressource definieren und etwas neu definieren.  Wir haben das im Voraus gemacht.  Was für gute Leute wir sind. <br><br>  Aber ... da ist ... ich schrieb, da ist die "Wurzel allen Übels", aber es gibt tatsächlich zwei davon.  Das erste ist das Dateisystem.  Es gibt eine Art PV, oder wir verwenden externen Speicher.  Wenn wir Dateien im Cluster speichern, müssen wir die alten Praktiken befolgen, die aus der Zeit der Eiseninfrastrukturen übrig geblieben sind: Synchronisieren Sie beispielsweise mit lsync.  Nun, oder jede andere Krücke, die Sie persönlich bevorzugen.  Wir rollen alles zu anderen Autos und leben. <br><br>  Zweitens und in der Tat ist die Datenbank ein noch wichtigerer Stolperstein.  Wenn wir kluge Leute sind und die Datenbank nicht im Cube behalten, ist das Sichern von Daten nach demselben alten Schema die Master-Slave-Replikation und das Umschalten. Wir werden das Replikat einholen und gut leben.  Wenn wir jedoch unsere Datenbank im Cluster behalten, gibt es im Prinzip viele vorgefertigte Lösungen zum Organisieren derselben Master-Slave-Replik, viele Lösungen zum Erhöhen der Datenbank im Cube. <br>  Es wurden bereits eine Milliarde Berichte über Datenbanksicherungen gelesen, eine Milliarde Artikel wurden geschrieben, hier wird tatsächlich nichts Neues benötigt.  Folgen Sie im Allgemeinen Ihrem Traum, leben Sie, wie Sie möchten, erfinden Sie auch einige komplizierte Krücken, aber denken Sie darüber nach, wie Sie all dies reservieren werden. <br><br>  Und nun darüber, wie wir im Prinzip im Brandfall zum Backup-Standort wechseln müssen.  Zunächst stellen wir zustandslose Anwendungen parallel bereit.  Sie haben keinen Einfluss auf die Geschäftslogik unserer Anwendungen, unseres Projekts. Wir können ständig zwei Sätze laufender Anwendungen behalten und sie können anfangen, Datenverkehr zu empfangen.  Beim Wechsel zur Sicherungssite ist es sehr wichtig zu prüfen, ob die Konfigurationen neu definiert werden müssen  Zum Beispiel haben wir einen Kubernetes-Verkaufscluster, es gibt einen Backup-Kubernetes-Cluster, es gibt eine externe Master-Datenbank und es gibt eine Backup-Master-Datenbank.  Wir haben vier Möglichkeiten, wie diese Anwendungen im Produkt miteinander interagieren können.  Unsere Basis kann wechseln, und es stellt sich heraus, dass wir den Datenverkehr auf die neue Basis im Prod-Cluster umschalten müssen, oder wir können den Cluster abrufen und sind in die Reserve umgezogen, aber wir arbeiten weiterhin mit der Pro-Base, also der dritten Option, wenn wir sie erhalten haben Dann wird es vermasselt, und wir wechseln beide Anwendungen. Definieren Sie unsere Konfiguration neu, damit neue Anwendungen bereits mit der neuen Datenbank funktionieren. <br><br>  Welche Schlussfolgerungen können aus all dem gezogen werden? <br><br><img src="https://habrastorage.org/webt/rl/81/0a/rl810a1jyyc78gkz0cop0gjzhma.jpeg"><br><br>  Die erste Schlussfolgerung: Mit einer Reserve gut leben.  Aber teuer.  Aber im Idealfall mit mehr als einer Reserve leben.  Idealerweise müssen Sie in der Regel mit mehreren Reserven leben.  Erstens sollte sich die Reserve mindestens nicht in einem DC befinden, und zweitens zumindest in einem anderen Hoster.  Es ist oft passiert - und in meiner Praxis war es das auch.  Leider kann ich die Projekte nicht benennen, gerade als es im Rechenzentrum einen Brand gab ... Ich bin so: Wir wechseln in die Reserve!  Und Backup-Server im selben Rack standen ... <br><br>  Oder stellen Sie sich vor, Amazon sei in Russland verboten worden (und das war es auch).  Und alles: das Gefühl der Tatsache, dass in einem anderen Amazonas unsere Reserve liegt?  Es ist auch nicht verfügbar.  Also wiederhole ich: Wir behalten die Reserve zumindest in einem anderen DC und vorzugsweise bei einem anderen Host. <br><br>  Die zweite Schlussfolgerung: Wenn Ihre Anwendung mit einigen externen Quellen kommuniziert (es kann sich entweder um eine Datenbank oder eine externe API handeln), müssen Sie sie als Dienst mit einem externen Endpunkt definieren, damit Sie sie zum Zeitpunkt des Wechsels nicht neu reparieren müssen 15 Ihrer Anwendungen, die auf derselben Basis klopfen.  Definieren Sie die Datenbank als separaten Dienst, klopfen Sie darauf, als ob sie sich in Ihrem Cluster befindet: Wenn Sie eine Datenbank haben, ändern Sie die IP an einem Ort und leben weiterhin glücklich. <br><br>  Und zum Schluss: Ich liebe den „Würfel“ und experimentiere damit.  Ich teile auch gerne die Ergebnisse dieser Experimente und im Allgemeinen meine persönlichen Erfahrungen.  Aus diesem Grund habe ich eine Reihe von Webinaren über K8s aufgezeichnet. Weitere Informationen finden Sie in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">unserem Youtube-Kanal</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de452078/">https://habr.com/ru/post/de452078/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de452066/index.html">Excelsior JET stellt die Entwicklung seines AOT-Compilers nach 18 Jahren Arbeit ein</a></li>
<li><a href="../de452068/index.html">12. Check Point Erste Schritte R80.20. Protokolle und Berichte</a></li>
<li><a href="../de452072/index.html">Wir implementieren CircularRevealAnimation auf Flutter und veröffentlichen gleichzeitig die Bibliothek auf pub.dev</a></li>
<li><a href="../de452074/index.html">Das erste Spiel über die Einheit oder was ich sechs Monate gebraucht habe</a></li>
<li><a href="../de452076/index.html">UC-Browser brechen</a></li>
<li><a href="../de452082/index.html">Flexibler Ablauf von In-App-Updates: Beschleunigen Sie den App-Update-Prozess unter Android</a></li>
<li><a href="../de452086/index.html">Was ist in meinem Pixel für Sie: Erstellen von Nanopixeln mit Plasmon-Metaoberflächen</a></li>
<li><a href="../de452088/index.html">Straßenerkennung durch semantische Segmentierung</a></li>
<li><a href="../de452090/index.html">Erstellen eines prozeduralen Puzzle-Generators</a></li>
<li><a href="../de452092/index.html">In-App-Updates: Beschleunigen von Android-Anwendungsupdates</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>