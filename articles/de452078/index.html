<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ˜‚ ğŸ’ ğŸ… Kubernetes Reservierung: Es besteht â›¹ğŸ¾ ğŸ‘¨ğŸ¿â€âš–ï¸ ğŸ‘¨ğŸ½â€ğŸ’»</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Mein Name ist Sergey, ich komme von ITSumma und ich mÃ¶chte Ihnen sagen, wie wir mit der Reservierung in Kubernetes umgehen. In letzter Zeit habe ich v...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Kubernetes Reservierung: Es besteht</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/itsumma/blog/452078/">  Mein Name ist Sergey, ich komme von ITSumma und ich mÃ¶chte Ihnen sagen, wie wir mit der Reservierung in Kubernetes umgehen.  In letzter Zeit habe ich viele Beratungsarbeiten zur Implementierung einer Vielzahl von Devops-LÃ¶sungen fÃ¼r verschiedene Teams durchgefÃ¼hrt, und insbesondere arbeite ich eng an Projekten mit K8s.  Auf der Uptime Day 4-Konferenz, die sich der Redundanz in komplexen Architekturen widmete, hielt ich eine PrÃ¤sentation Ã¼ber redundante WÃ¼rfel, und hier ist seine kostenlose NacherzÃ¤hlung.  Ich werde nur im Voraus warnen, dass er kein direkter Leitfaden zum Handeln ist, sondern eine Verallgemeinerung der Gedanken zu diesem Thema. <br><br><img src="https://habrastorage.org/webt/fi/pj/ox/fipjoxmwx-hrd0tvm0_bvgjaa-e.jpeg"><br><br>  Im Prinzip sind Ãœberwachung und Redundanz die beiden Hauptinstrumente, um die Ausfallsicherheit eines Projekts zu erhÃ¶hen.  Aber in der Cuber ist alles von selbst ausgeglichen, Sie sagen, alles ist von selbst skaliert, und wenn etwas passiert, wird es von selbst aufsteigen ... Das heiÃŸt, wÃ¤hrend der ersten oberflÃ¤chlichen Untersuchung des Themas hat mir das Internet die Frage beantwortet: "Wie funktioniert K8s Backup?" ? "  Viele Leute denken, dass ein Cuber so eine magische Sache ist, die alle Infrastrukturprobleme beseitigt und das Projekt niemals zum Erliegen bringt.  Aber ... die Welt ist nicht so, wie es scheint. <br><a name="habracut"></a><br>  Wie sind wir zuvor mit dem Sicherungsprozess umgegangen?  Wir hatten identische Plattformen fÃ¼r die Platzierung - entweder waren es virtuelle Maschinen oder es waren Eisenserver, auf die wir drei grundlegende Praktiken angewendet haben: <br><br><ol><li>  Codesynchronisation und Statik </li><li>  Konfigurationssynchronisation </li><li>  Datenbankreplikation </li></ol><br>  Und voila: In jedem Moment wechseln wir zur Reserveseite, alle sind glÃ¼cklich, wir stehen auf, wir sind anderer Meinung. <br><br><img src="https://habrastorage.org/webt/mw/ym/zw/mwymzwrfdl9vsaf_hg-wm3wvphm.jpeg"><br><br>  Und was bieten sie uns, um die stÃ¤ndige VerfÃ¼gbarkeit unserer Kubernetes-Anwendung zu erhÃ¶hen?  Das erste, was in der inoffiziellen Dokumentation steht, ist, viele Maschinen zu platzieren, viele Master zu erstellen - ihre Anzahl muss die Bedingungen fÃ¼r das Erreichen eines Quorums innerhalb des Clusters erfÃ¼llen, und so wird etcd, api, MC, Scheduler auf jedem der Master ausgelÃ¶st ... Und anscheinend ist alles in Ordnung : Wenn mehrere Arbeitsknoten oder Master ausfallen, wird unser Cluster neu ausgeglichen und die Anwendung funktioniert weiterhin.  Sieht wieder nach Magie aus!  Oft befindet sich unser Cluster jedoch im selben Rechenzentrum, was zu bestimmten Fragen fÃ¼hren kann.  Was ist, wenn ein Bagger ankam und ein Kabel ausgrub, blitzschnell, gab es eine universelle Flut?  Alles ist abgedeckt, unser Cluster ist nicht mehr.  Wie kann man sich der Reservierung unter BerÃ¼cksichtigung dieser Seite des Problems nÃ¤hern? <br><br>  ZunÃ¤chst sollten Sie einen anderen Cluster in der Hot Reserve haben, dh einen Cluster, zu dem Sie jederzeit wechseln kÃ¶nnen.  In diesem Fall sollten die Infrastrukturen aus Sicht des Cuber vÃ¶llig identisch sein.  Das heiÃŸt, wenn es nicht standardmÃ¤ÃŸige Plugins fÃ¼r die Arbeit mit dem Dateisystem gibt, benutzerdefinierte LÃ¶sungen fÃ¼r Ingress, sollten diese auf Ihren zwei (oder drei oder zehn, es gibt genug Geld und Administratoren) Clustern vollstÃ¤ndig identisch sein.  Es mÃ¼ssen zwei Arten von Anwendungen klar definiert werden (Deployment'ov, Statefulset'ov, Daemonset'ov, Cronjob'ov usw.): Welche von ihnen kÃ¶nnen stÃ¤ndig an einer Reserve arbeiten und welche sollten besser nicht vor dem direkten Wechsel gestartet werden. <br><br>  Sollte unser Backup-Cluster also vollstÃ¤ndig mit unserem Kampf-Cluster identisch sein?  Nein.  FrÃ¼her haben wir im Rahmen der Arbeit mit monolithischen Projekten mit Eiseninfrastruktur eine fast vÃ¶llig identische Umgebung beibehalten, aber im Rahmen des Cuber denke ich, dass dies nicht sein sollte.  Schauen wir uns an, warum. <br><br>  Beginnen wir zum Beispiel mit den grundlegenden EntitÃ¤ten von Kubernetes - Bereitstellungen - sie mÃ¼ssen identisch sein.  Es sollten Anwendungen gestartet werden, die die Verkehrsverarbeitung jederzeit abfangen und es unserem Projekt ermÃ¶glichen, weiter zu leben.  Wenn es sich um Konfigurationsdateien handelt, mÃ¼ssen wir hier prÃ¼fen, ob sie identisch sein sollten oder nicht.  Das heiÃŸt, wenn wir, kluge Leute, keine verbotenen Substanzen verwenden und die Basis nicht in K8s behalten, mÃ¼ssen wir Zugriffseinstellungen in den Konfigurationskarten fÃ¼r die Kampfbasis haben (deren Sicherungsprozess separat erstellt wird).  Um den Zugriff auf die Sicherungsdatenbankinstanz sicherzustellen, benÃ¶tigen wir daher eine separate Konfigurationsdatei (configmap).  Genau so arbeiten wir mit Geheimnissen: PasswÃ¶rter fÃ¼r den Zugriff auf die Datenbank, API-SchlÃ¼ssel;  Zu jeder Zeit kann entweder ein Kampfgeheimnis oder eine Reserve mit uns zusammenarbeiten.  Insgesamt haben wir bereits zwei Kubernetes-EntitÃ¤ten, deren Backup-Versionen nicht mit den Kampfversionen identisch sein sollten.  Die nÃ¤chste EntitÃ¤t, die es wert ist, darÃ¼ber nachzudenken, ist Cronjob.  Cronjobs in Reserve sollten keinesfalls mit den Cronjob-Produktionsclustern identisch sein!  Wenn wir den Sicherungscluster erhÃ¶hen und ihn vollstÃ¤ndig aktivieren, wÃ¤hrend alle Cronjobs aktiviert sind, erhalten beispielsweise zwei Briefe gleichzeitig von Ihnen anstelle von einem.  Oder eine Art Synchronisation von Daten mit externen Quellen findet zweimal statt, wir beginnen zu verletzen, zu weinen, zu schreien und zu schwÃ¶ren. <br><br><img src="https://habrastorage.org/webt/m3/n2/yx/m3n2yxmvocsvsfiavdmevq4vx18.jpeg"><br><br>  Aber wie bieten uns Leute aus dem Internet an, einen Backup-Cluster zu organisieren?  Die zweitbeliebteste Antwort nach "Warum?"  - Nutzung der Kubernetes Federation. <br><br>  Was ist das?  Dies ist beispielsweise ein groÃŸer Meta-Cluster.  Wenn wir uns die Architektur des Cuber vorstellen - wo wir einen Master, mehrere Knoten haben - dann haben wir aus Sicht der FÃ¶deration auch einen Master und mehrere Knoten, nur jeder Knoten ist ein separater Cluster.  Das heiÃŸt, wir arbeiten mit denselben EntitÃ¤ten, mit denselben Grundelementen wie mit einem einzelnen Cuber, aber wir drehen und drehen nicht unsere physischen Maschinen, sondern ganze Cluster.  Im Rahmen des Bundes sind wir in der vollstÃ¤ndigen Synchronisation der Bundesressourcen von den Eltern zu den Nachkommen.  Wenn wir beispielsweise eine Bereitstellung Ã¼ber den Verbund gestartet haben, wird diese in jedem unserer Tochtercluster bereitgestellt.  Wenn wir eine Konfigurationskarte verwenden, besteht das Geheimnis darin, sie an den Verband weiterzuleiten - sie wird sich auf alle unsere untergeordneten Cluster ausbreiten.  Gleichzeitig ermÃ¶glicht uns der Verband, unsere Ressourcen fÃ¼r Kinder anzupassen.  Das heiÃŸt, wir haben eine Konfigurationskarte erstellt, diese Ã¼ber den Verbund bereitgestellt. Wenn wir dann etwas an bestimmten Clustern beheben mÃ¼ssen, bearbeiten wir sie in einem separaten Cluster, und diese Ã„nderung wird nirgendwo synchronisiert. <br><br>  Kubernetes Federation ist noch nicht so lange ein vorhandenes Tool und unterstÃ¼tzt nicht alle Ressourcen, die K8s selbst bereitstellt: Zum Zeitpunkt der VerÃ¶ffentlichung einer der ersten Versionen der Dokumentation ging es darum, nur Konfigurationszuordnungen, Bereitstellung fÃ¼r ReplikatsÃ¤tze und Ingress zu unterstÃ¼tzen.  Geheimnisse wurden nicht unterstÃ¼tzt, die Arbeit mit Volumen wurde ebenfalls nicht unterstÃ¼tzt.  Zu begrenzter Satz.  Insbesondere wenn wir SpaÃŸ haben mÃ¶chten, beispielsweise durch die benutzerdefinierte Ressourcendefinition, um unsere eigenen Ressourcen auf die Kubernetes zu Ã¼bertragen, werden wir sie nicht in den Verbund Ã¼bertragen.  Das heiÃŸt, sozusagen ... eine Entscheidung, die der Wahrheit sehr Ã¤hnlich ist, aber sie lÃ¤sst uns regelmÃ¤ÃŸig in den FuÃŸ schieÃŸen.  Auf der anderen Seite kÃ¶nnen Sie mit dem Verband unser Replikatset flexibel verwalten.  Beispielsweise mÃ¶chten wir, dass 10 Replikate unserer Anwendung gestartet werden. StandardmÃ¤ÃŸig teilt der Verbund diese Anzahl proportional auf die Anzahl der Cluster auf.  Und das alles kann auch konfiguriert werden!  Das heiÃŸt, Sie kÃ¶nnen angeben, dass Sie 6 Replikate unserer Anwendung in einem Kampfcluster und nur 4 Replikate unserer Anwendung in einem Sicherungscluster aufbewahren mÃ¼ssen, um Ressourcen zu sparen oder zu Ihrer eigenen Unterhaltung.  Welches ist auch sehr praktisch.  Aber mit dem Verband mÃ¼ssen wir einige neue LÃ¶sungen verwenden, etwas unterwegs erledigen und uns zwingen, ein bisschen mehr nachzudenken ... <br><br>  Ist es mÃ¶glich, den Prozess der Buchung eines Cubers irgendwie einfacher anzugehen?  Welche Werkzeuge haben wir Ã¼berhaupt? <br><br>  Erstens haben wir immer eine Art CD / CD-System, das heiÃŸt, wir gehen nicht manuell herum, schreiben nicht erstellen / anwenden auf den Servern.  Das System erzeugt Yaml'ics fÃ¼r unsere Container. <br><br>  Zweitens gibt es mehrere Cluster, wir haben entweder eine oder mehrere (wenn wir klug sind) Registrierungen, die wir auch genommen und reserviert haben.  Und es gibt ein wunderbares kubectl-Dienstprogramm, das mit mehreren Clustern gleichzeitig arbeiten kann. <br><br><img src="https://habrastorage.org/webt/vs/ib/ed/vsibeda9uobcn8jtrfztjkln3ag.jpeg"><br><br>  Also: Meiner Meinung nach ist die einfachste und korrekteste LÃ¶sung zum Erstellen eines Sicherungsclusters eine primitive parallele Bereitstellung.  Es gibt eine Art Pipeline im ci / cd-System;  Zuerst bauen wir unsere Container, testen und rollen Anwendungen Ã¼ber kubectl in mehreren unabhÃ¤ngigen Clustern aus.  Wir kÃ¶nnen simultane Berechnungen fÃ¼r mehrere Cluster erstellen.  Dementsprechend beschlieÃŸen wir auch, in dieser Phase Konfigurationen zu liefern.  Sie kÃ¶nnen die Konfigurationen fÃ¼r unseren Kampfcluster, die Konfigurationen fÃ¼r den Sicherungscluster vordefinieren und auf ci / cd-Ebene des Systems die Pro-Umgebung in den Pro-Cluster und die Sicherungsumgebung in den Sicherungscluster Ã¼bertragen.  Im Vergleich zum Verband mÃ¼ssen Sie nicht fÃ¼r jeden untergeordneten Cluster eine Bundesressource definieren und etwas neu definieren.  Wir haben das im Voraus gemacht.  Was fÃ¼r gute Leute wir sind. <br><br>  Aber ... da ist ... ich schrieb, da ist die "Wurzel allen Ãœbels", aber es gibt tatsÃ¤chlich zwei davon.  Das erste ist das Dateisystem.  Es gibt eine Art PV, oder wir verwenden externen Speicher.  Wenn wir Dateien im Cluster speichern, mÃ¼ssen wir die alten Praktiken befolgen, die aus der Zeit der Eiseninfrastrukturen Ã¼brig geblieben sind: Synchronisieren Sie beispielsweise mit lsync.  Nun, oder jede andere KrÃ¼cke, die Sie persÃ¶nlich bevorzugen.  Wir rollen alles zu anderen Autos und leben. <br><br>  Zweitens und in der Tat ist die Datenbank ein noch wichtigerer Stolperstein.  Wenn wir kluge Leute sind und die Datenbank nicht im Cube behalten, ist das Sichern von Daten nach demselben alten Schema die Master-Slave-Replikation und das Umschalten. Wir werden das Replikat einholen und gut leben.  Wenn wir jedoch unsere Datenbank im Cluster behalten, gibt es im Prinzip viele vorgefertigte LÃ¶sungen zum Organisieren derselben Master-Slave-Replik, viele LÃ¶sungen zum ErhÃ¶hen der Datenbank im Cube. <br>  Es wurden bereits eine Milliarde Berichte Ã¼ber Datenbanksicherungen gelesen, eine Milliarde Artikel wurden geschrieben, hier wird tatsÃ¤chlich nichts Neues benÃ¶tigt.  Folgen Sie im Allgemeinen Ihrem Traum, leben Sie, wie Sie mÃ¶chten, erfinden Sie auch einige komplizierte KrÃ¼cken, aber denken Sie darÃ¼ber nach, wie Sie all dies reservieren werden. <br><br>  Und nun darÃ¼ber, wie wir im Prinzip im Brandfall zum Backup-Standort wechseln mÃ¼ssen.  ZunÃ¤chst stellen wir zustandslose Anwendungen parallel bereit.  Sie haben keinen Einfluss auf die GeschÃ¤ftslogik unserer Anwendungen, unseres Projekts. Wir kÃ¶nnen stÃ¤ndig zwei SÃ¤tze laufender Anwendungen behalten und sie kÃ¶nnen anfangen, Datenverkehr zu empfangen.  Beim Wechsel zur Sicherungssite ist es sehr wichtig zu prÃ¼fen, ob die Konfigurationen neu definiert werden mÃ¼ssen  Zum Beispiel haben wir einen Kubernetes-Verkaufscluster, es gibt einen Backup-Kubernetes-Cluster, es gibt eine externe Master-Datenbank und es gibt eine Backup-Master-Datenbank.  Wir haben vier MÃ¶glichkeiten, wie diese Anwendungen im Produkt miteinander interagieren kÃ¶nnen.  Unsere Basis kann wechseln, und es stellt sich heraus, dass wir den Datenverkehr auf die neue Basis im Prod-Cluster umschalten mÃ¼ssen, oder wir kÃ¶nnen den Cluster abrufen und sind in die Reserve umgezogen, aber wir arbeiten weiterhin mit der Pro-Base, also der dritten Option, wenn wir sie erhalten haben Dann wird es vermasselt, und wir wechseln beide Anwendungen. Definieren Sie unsere Konfiguration neu, damit neue Anwendungen bereits mit der neuen Datenbank funktionieren. <br><br>  Welche Schlussfolgerungen kÃ¶nnen aus all dem gezogen werden? <br><br><img src="https://habrastorage.org/webt/rl/81/0a/rl810a1jyyc78gkz0cop0gjzhma.jpeg"><br><br>  Die erste Schlussfolgerung: Mit einer Reserve gut leben.  Aber teuer.  Aber im Idealfall mit mehr als einer Reserve leben.  Idealerweise mÃ¼ssen Sie in der Regel mit mehreren Reserven leben.  Erstens sollte sich die Reserve mindestens nicht in einem DC befinden, und zweitens zumindest in einem anderen Hoster.  Es ist oft passiert - und in meiner Praxis war es das auch.  Leider kann ich die Projekte nicht benennen, gerade als es im Rechenzentrum einen Brand gab ... Ich bin so: Wir wechseln in die Reserve!  Und Backup-Server im selben Rack standen ... <br><br>  Oder stellen Sie sich vor, Amazon sei in Russland verboten worden (und das war es auch).  Und alles: das GefÃ¼hl der Tatsache, dass in einem anderen Amazonas unsere Reserve liegt?  Es ist auch nicht verfÃ¼gbar.  Also wiederhole ich: Wir behalten die Reserve zumindest in einem anderen DC und vorzugsweise bei einem anderen Host. <br><br>  Die zweite Schlussfolgerung: Wenn Ihre Anwendung mit einigen externen Quellen kommuniziert (es kann sich entweder um eine Datenbank oder eine externe API handeln), mÃ¼ssen Sie sie als Dienst mit einem externen Endpunkt definieren, damit Sie sie zum Zeitpunkt des Wechsels nicht neu reparieren mÃ¼ssen 15 Ihrer Anwendungen, die auf derselben Basis klopfen.  Definieren Sie die Datenbank als separaten Dienst, klopfen Sie darauf, als ob sie sich in Ihrem Cluster befindet: Wenn Sie eine Datenbank haben, Ã¤ndern Sie die IP an einem Ort und leben weiterhin glÃ¼cklich. <br><br>  Und zum Schluss: Ich liebe den â€WÃ¼rfelâ€œ und experimentiere damit.  Ich teile auch gerne die Ergebnisse dieser Experimente und im Allgemeinen meine persÃ¶nlichen Erfahrungen.  Aus diesem Grund habe ich eine Reihe von Webinaren Ã¼ber K8s aufgezeichnet. Weitere Informationen finden Sie in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">unserem Youtube-Kanal</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de452078/">https://habr.com/ru/post/de452078/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de452066/index.html">Excelsior JET stellt die Entwicklung seines AOT-Compilers nach 18 Jahren Arbeit ein</a></li>
<li><a href="../de452068/index.html">12. Check Point Erste Schritte R80.20. Protokolle und Berichte</a></li>
<li><a href="../de452072/index.html">Wir implementieren CircularRevealAnimation auf Flutter und verÃ¶ffentlichen gleichzeitig die Bibliothek auf pub.dev</a></li>
<li><a href="../de452074/index.html">Das erste Spiel Ã¼ber die Einheit oder was ich sechs Monate gebraucht habe</a></li>
<li><a href="../de452076/index.html">UC-Browser brechen</a></li>
<li><a href="../de452082/index.html">Flexibler Ablauf von In-App-Updates: Beschleunigen Sie den App-Update-Prozess unter Android</a></li>
<li><a href="../de452086/index.html">Was ist in meinem Pixel fÃ¼r Sie: Erstellen von Nanopixeln mit Plasmon-MetaoberflÃ¤chen</a></li>
<li><a href="../de452088/index.html">StraÃŸenerkennung durch semantische Segmentierung</a></li>
<li><a href="../de452090/index.html">Erstellen eines prozeduralen Puzzle-Generators</a></li>
<li><a href="../de452092/index.html">In-App-Updates: Beschleunigen von Android-Anwendungsupdates</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>