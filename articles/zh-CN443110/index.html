<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏽‍🤝‍👨🏼 📍 🎇 使用GlusterFS和MetalLB在裸机上配置Kubernetes HA集群。 第2/3部分 〽️ 🧚🏻 💔</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="第1/3部分 
 第3/3部分 


 您好，欢迎回来！ 这是在裸机上建立Kubernetes集群的文章的第二部分。 之前我们使用外部etcd，master-master和负载平衡来配置Kubernetes HA集群。 好了，现在是时候设置一个额外的环境和实用程序，以使群集更有用并尽可能接近工作状态...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>使用GlusterFS和MetalLB在裸机上配置Kubernetes HA集群。 第2/3部分</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/443110/"><p><img src="https://habrastorage.org/webt/oa/xl/av/oaxlavwz_atdglepw3r_vn6hmxm.jpeg"></p><br><p>  <strong>第1/3部分</strong> <br>  <strong>第3/3部分</strong> </p><br><p> 您好，欢迎回来！ 这是在裸机上建立Kubernetes集群的文章的第二部分。 之前我们使用外部etcd，master-master和负载平衡来配置Kubernetes HA集群。 好了，现在是时候设置一个额外的环境和实用程序，以使群集更有用并尽可能接近工作状态了。 </p><br><p> 在本文的这一部分中，我们将重点介绍配置群集服务的内部负载平衡器-这就是MetalLB。 我们还将在工作节点之间安装和配置分布式文件存储。 我们将对Kubernetes中可用的持久卷使用GlusterFS。 <br> 完成所有步骤后，我们的集群图将如下所示： </p><br><p> <a href=""><img src="https://habrastorage.org/webt/_v/yp/pe/_vyppenp91uzmkowqv1qcyomnrc.jpeg"></a> </p><a name="habracut"></a><br><h3 id="1-nastroyka-metallb-v-kachestve-vnutrennego-balansirovschika-nagruzki">  1.将MetalLB设置为内部负载平衡器。 </h3><br><p> 关于MetalLB的几句话，直接来自文档页面： </p><br><blockquote> MetalLB是使用标准路由协议的Kubernetes裸机集群的负载均衡器实现。 <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes</a>不提供用于裸机的网络负载平衡器（ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">服务类型LoadBalancer</a> ）的实现。  Kubernetes随附的所有Network LB实施选项都是中间件，它可以访问各种IaaS平台（GCP，AWS，Azure等）。 如果您不在IaaS支持的平台（GCP，AWS，Azure等）上工作，则LoadBalancer在创建后将无限期保持“待机”状态。 <br><br>  BM服务器运营商有两个效率较低的工具，即NodePort和externalIPs服务，用于将用户流量输入其集群。 这两种选择在生产上都有明显的缺陷，这使BM群成为Kubernetes生态系统中的二等公民。 <br><br>  MetalLB试图通过提供与标准网络设备集成的Network LB实施来纠正这种不平衡，因此BM群集上的外部服务也可以以最快的速度“正常工作”。 </blockquote><p> 因此，使用此工具，我们使用负载平衡器在Kubernetes集群中启动服务，这要归功于MetalLB团队。 设置过程确实非常简单明了。 </p><br><p> 在示例的前面，我们为群集的需要选择了192.168.0.0/24子网。 现在，将其中一些子网用于将来的负载均衡器。 </p><br><p> 我们使用配置的<strong>kubectl</strong>实用程序进入机器系统并运行： </p><br><pre><code class="plaintext hljs">control# kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.7.3/manifests/metallb.yaml</code> </pre> <br><p> 这将在<code>metallb-system</code>中的群集中部署MetalLB。 确保所有MetalLB组件均正常运行： </p><br><pre> <code class="plaintext hljs">control# kubectl get pod --namespace=metallb-system NAME READY STATUS RESTARTS AGE controller-7cc9c87cfb-ctg7p 1/1 Running 0 5d3h speaker-82qb5 1/1 Running 0 5d3h speaker-h5jw7 1/1 Running 0 5d3h speaker-r2fcg 1/1 Running 0 5d3h</code> </pre> <br><p> 现在使用configmap配置MetalLB。 在此示例中，我们使用第2层自定义，有关其他自定义选项的信息，请参见MetalLB文档。 </p><br><p> 在集群的子网的选定IP范围内的任何目录中创建<strong>metallb-config.yaml文件</strong> ： </p><br><pre> <code class="plaintext hljs">control# vi metallb-config.yaml apiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: default protocol: layer2 addresses: - 192.168.0.240-192.168.0.250</code> </pre> <br><p> 并应用以下设置： </p><br><pre> <code class="plaintext hljs">control# kubectl apply -f metallb-config.yaml</code> </pre> <br><p> 如有必要，稍后检查并修改configmap： </p><br><pre> <code class="plaintext hljs">control# kubectl describe configmaps -n metallb-system control# kubectl edit configmap config -n metallb-system</code> </pre> <br><p> 现在我们有了自己配置的本地负载均衡器。 我们以Nginx服务为例，看看它是如何工作的。 </p><br><pre> <code class="plaintext hljs">control# vi nginx-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 control# vi nginx-service.yaml apiVersion: v1 kind: Service metadata: name: nginx spec: type: LoadBalancer selector: app: nginx ports: - port: 80 name: http</code> </pre> <br><p> 然后创建一个测试部署和Nginx服务： </p><br><pre> <code class="plaintext hljs">control# kubectl apply -f nginx-deployment.yaml control# kubectl apply -f nginx-service.yaml</code> </pre> <br><p> 现在-检查结果： </p><br><pre> <code class="plaintext hljs">control# kubectl get po NAME READY STATUS RESTARTS AGE nginx-deployment-6574bd76c-fxgxr 1/1 Running 0 19s nginx-deployment-6574bd76c-rp857 1/1 Running 0 19s nginx-deployment-6574bd76c-wgt9n 1/1 Running 0 19s control# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx LoadBalancer 10.100.226.110 192.168.0.240 80:31604/TCP 107s</code> </pre> <br><p> 如之前部署中所述，创建了3个Nginx Pod。  Nginx服务将根据循环平衡方案将流量定向到所有这些Pod。 您还可以看到从我们的MetalLB负载平衡器收到的外部IP。 </p><br><p> 现在尝试汇总到IP地址192.168.0.240，您将看到Nginx index.html页面。 切记删除测试部署和Nginx服务。 </p><br><pre> <code class="plaintext hljs">control# kubectl delete svc nginx service "nginx" deleted control# kubectl delete deployment nginx-deployment deployment.extensions "nginx-deployment" deleted</code> </pre> <br><p> 好吧，这就是MetalLB的全部内容，让我们继续-我们将为Kubernetes卷配置GlusterFS。 </p><br><h3 id="2-nastroyka-glusterfs-s-heketi-na-rabochih-nodah">  2.在工作节点上使用Heketi配置GlusterFS。 </h3><br><p> 实际上，没有内部卷就无法使用Kubernetes集群。 如您所知，炉膛是短暂的，即 可以随时创建和删除它们。 其中的所有数据都将丢失。 因此，在实际集群中，需要分布式存储以确保节点和内部应用程序之间的设置和数据交换。 </p><br><p> 在Kubernetes中，可以通过多种方式使用卷;选择所需的卷。 在此示例中，我将演示如何为任何内部应用程序创建GlusterFS存储，就像持久卷一样。 之前，我为此在所有Kubernetes工作节点上都使用了GlusterFS的“系统”安装，然后在GlusterFS目录中简单地创建了hostPath卷。 </p><br><p> 现在我们有了一个新的便捷的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><strong>Heketi</strong></a>工具。 </p><br><p>  Heketi文档中的几句话： </p><br><blockquote> 用于GlusterFS的RESTful卷管理基础结构。 <br><br>  Heketi提供了一个RESTful管理界面，可用于管理GlusterFS卷的生命周期。 感谢Heketi，诸如OpenStack Manila，Kubernetes和OpenShift之类的云服务可以动态地为GlusterFS卷提供支持的任何类型的可靠性。  Heketi自动确定集群中块的位置，提供块及其副本在不同故障区域的位置。  Heketi还支持任意数量的GlusterFS群集，从而使云服务能够提供在线文件存储，而不仅仅是单个GlusterFS群集。 </blockquote><p> 听起来不错，此外，此工具还将使我们的VM集群更接近Kubernetes大型云集群。 最后，您将能够创建<strong>PersistentVolumeClaims</strong> ，它将自动生成等等。 </p><br><p> 您可以使用其他系统硬盘驱动器来配置GlusterFS，也可以仅创建一些虚拟块设备。 在此示例中，我将使用第二种方法。 </p><br><p> 在所有三个工作节点上创建虚拟块设备： </p><br><pre> <code class="plaintext hljs">worker1-3# dd if=/dev/zero of=/home/gluster/image bs=1M count=10000</code> </pre> <br><p> 您将获得一个大约10 GB的文件。 然后使用<strong>Lostup-</strong>将其添加到这些节点，作为回送设备： </p><br><pre> <code class="plaintext hljs">worker1-3# losetup /dev/loop0 /home/gluster/image</code> </pre> <br><blockquote>  <em>请注意：如果您已经有某种回送设备0，那么您将需要选择任何其他号码。</em> </blockquote><p> 我花了时间，找出了为什么赫凯蒂不想正常工作。 因此，为防止将来的配置出现任何问题，请首先确保我们已加载<strong>dm_thin_pool</strong>内核<strong>模块</strong>并在所有工作节点上安装了<strong>glusterfs-client</strong>软件包。 </p><br><pre> <code class="plaintext hljs">worker1-3# modprobe dm_thin_pool worker1-3# apt-get update &amp;&amp; apt-get -y install glusterfs-client</code> </pre> <br><p> 好了，现在您需要在所有工作节点上都存在文件<strong>/ home / gluster / image</strong>和设备<strong>/ dev / loop0</strong> 。 请记住要创建一个systemd服务，该服务将在每次这些服务器启动时自动启动<strong>Lostup</strong>和<strong>Modprobe</strong> 。 </p><br><pre> <code class="plaintext hljs">worker1-3# vi /etc/systemd/system/loop_gluster.service [Unit] Description=Create the loopback device for GlusterFS DefaultDependencies=false Before=local-fs.target After=systemd-udev-settle.service Requires=systemd-udev-settle.service [Service] Type=oneshot ExecStart=/bin/bash -c "modprobe dm_thin_pool &amp;&amp; [ -b /dev/loop0 ] || losetup /dev/loop0 /home/gluster/image" [Install] WantedBy=local-fs.target</code> </pre> <br><p> 并打开它： </p><br><pre> <code class="plaintext hljs">worker1-3# systemctl enable /etc/systemd/system/loop_gluster.service Created symlink /etc/systemd/system/local-fs.target.wants/loop_gluster.service → /etc/systemd/system/loop_gluster.service.</code> </pre> <br><p> 准备工作已经完成，我们准备将GlusterFS和Heketi部署到我们的集群中。 为此，我将使用这个很酷的<a href="">指南</a> 。 大多数命令是从外部控制计算机启动的，很小的命令是从群集内的任何主节点启动的。 </p><br><p> 首先，复制存储库并创建DaemonSet GlusterFS： </p><br><pre> <code class="plaintext hljs">control# git clone https://github.com/heketi/heketi control# cd heketi/extras/kubernetes control# kubectl create -f glusterfs-daemonset.json</code> </pre> <br><p> 现在，让我们为GlusterFS标记三个工作节点； 标记它们后，将创建GlusterFS容器： </p><br><pre> <code class="plaintext hljs">control# kubectl label node worker1 storagenode=glusterfs control# kubectl label node worker2 storagenode=glusterfs control# kubectl label node worker3 storagenode=glusterfs control# kubectl get pod NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 1m6s glusterfs-hzdll 1/1 Running 0 1m9s glusterfs-p8r59 1/1 Running 0 2m1s</code> </pre> <br><p> 现在创建一个Heketi服务帐户： </p><br><pre> <code class="plaintext hljs">control# kubectl create -f heketi-service-account.json</code> </pre> <br><p> 我们为此服务帐户提供了管理Gluster Pod的功能。 为此，请为我们新创建的服务帐户创建所需的集群功能： </p><br><pre> <code class="plaintext hljs">control# kubectl create clusterrolebinding heketi-gluster-admin --clusterrole=edit --serviceaccount=default:heketi-service-account</code> </pre> <br><p> 现在让我们创建一个Kubernetes秘密密钥来阻止我们的Heketi实例的配置： </p><br><pre> <code class="plaintext hljs">control# kubectl create secret generic heketi-config-secret --from-file=./heketi.json</code> </pre> <br><p> 在Heketi下创建第一个源，我们将其用于第一个设置操作，然后删除： </p><br><pre> <code class="plaintext hljs">control# kubectl create -f heketi-bootstrap.json service "deploy-heketi" created deployment "deploy-heketi" created control# kubectl get pod NAME READY STATUS RESTARTS AGE deploy-heketi-1211581626-2jotm 1/1 Running 0 2m glusterfs-5dtdj 1/1 Running 0 6m6s glusterfs-hzdll 1/1 Running 0 6m9s glusterfs-p8r59 1/1 Running 0 7m1s</code> </pre> <br><p> 创建并启动Bootstrap Heketi服务后，由于外部控制节点不在集群内部，因此我们将需要切换到主节点之一，在此我们将运行多个命令，因此我们无法访问工作Pod和集群的内部网络。 </p><br><p> 首先，让我们下载heketi-client实用程序并将其复制到bin系统文件夹中： </p><br><pre> <code class="plaintext hljs">master1# wget https://github.com/heketi/heketi/releases/download/v8.0.0/heketi-client-v8.0.0.linux.amd64.tar.gz master1# tar -xzvf ./heketi-client-v8.0.0.linux.amd64.tar.gz master1# cp ./heketi-client/bin/heketi-cli /usr/local/bin/ master1# heketi-cli heketi-cli v8.0.0</code> </pre> <br><p> 现在找到heketi pod的IP地址，并将其导出为系统变量： </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf describe pod deploy-heketi-1211581626-2jotm For me this pod have a 10.42.0.1 ip master1# curl http://10.42.0.1:57598/hello Handling connection for 57598 Hello from Heketi master1# export HEKETI_CLI_SERVER=http://10.42.0.1:57598</code> </pre> <br><p> 现在，让我们为Heketi提供应管理的有关GlusterFS集群的信息。 我们通过拓扑文件提供它。 拓扑是JSON清单，其中包含GlusterFS使用的所有节点，磁盘和群集的列表。 </p><br><blockquote> 注意 确保<code>hostnames/manage</code>指示确切的名称（如<code>kubectl get node</code>部分中所述），并且<code>hostnames/storage</code>是存储节点的IP地址。 </blockquote><br><pre> <code class="plaintext hljs">master1:~/heketi-client# vi topology.json { "clusters": [ { "nodes": [ { "node": { "hostnames": { "manage": [ "worker1" ], "storage": [ "192.168.0.7" ] }, "zone": 1 }, "devices": [ "/dev/loop0" ] }, { "node": { "hostnames": { "manage": [ "worker2" ], "storage": [ "192.168.0.8" ] }, "zone": 1 }, "devices": [ "/dev/loop0" ] }, { "node": { "hostnames": { "manage": [ "worker3" ], "storage": [ "192.168.0.9" ] }, "zone": 1 }, "devices": [ "/dev/loop0" ] } ] } ] }</code> </pre> <br><p> 然后下载此文件： </p><br><pre> <code class="plaintext hljs">master1:~/heketi-client# heketi-cli topology load --json=topology.json Creating cluster ... ID: e83467d0074414e3f59d3350a93901ef Allowing file volumes on cluster. Allowing block volumes on cluster. Creating node worker1 ... ID: eea131d392b579a688a1c7e5a85e139c Adding device /dev/loop0 ... OK Creating node worker2 ... ID: 300ad5ff2e9476c3ba4ff69260afb234 Adding device /dev/loop0 ... OK Creating node worker3 ... ID: 94ca798385c1099c531c8ba3fcc9f061 Adding device /dev/loop0 ... OK</code> </pre> <br><p> 接下来，我们使用Heketi提供用于存储数据库的卷。 团队名称有点奇怪，但是一切都井井有条。 还创建一个heketi存储库： </p><br><pre> <code class="plaintext hljs">master1:~/heketi-client# heketi-cli setup-openshift-heketi-storage master1:~/heketi-client# kubectl --kubeconfig /etc/kubernetes/admin.conf create -f heketi-storage.json secret/heketi-storage-secret created endpoints/heketi-storage-endpoints created service/heketi-storage-endpoints created job.batch/heketi-storage-copy-job created</code> </pre> <br><p> 这些都是您需要从主节点运行的所有命令。 让我们回到控制节点并从那里继续。 首先，请确保最后一个正在运行的命令已成功执行： </p><br><pre> <code class="plaintext hljs">control# kubectl get pod NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 39h glusterfs-hzdll 1/1 Running 0 39h glusterfs-p8r59 1/1 Running 0 39h heketi-storage-copy-job-txkql 0/1 Completed 0 69s</code> </pre> <br><p>  heketi-storage-copy-job工作已经完成。 </p><br><blockquote> 如果当前在您的工作节点上未安装任何已安装的<strong>glusterfs-client</strong>软件包，则会发生错误。 </blockquote><p> 现在是时候删除Heketi Bootstrap安装文件并进行一些清理了： </p><br><pre> <code class="plaintext hljs">control# kubectl delete all,service,jobs,deployment,secret --selector="deploy-heketi"</code> </pre> <br><p> 在最后阶段，我们需要创建Heketi的长期副本： </p><br><pre> <code class="plaintext hljs">control# cd ./heketi/extras/kubernetes control:~/heketi/extras/kubernetes# kubectl create -f heketi-deployment.json secret/heketi-db-backup created service/heketi created deployment.extensions/heketi created control# kubectl get pod NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 39h glusterfs-hzdll 1/1 Running 0 39h glusterfs-p8r59 1/1 Running 0 39h heketi-b8c5f6554-knp7t 1/1 Running 0 22m</code> </pre> <br><p> 如果当前在您的工作节点上未安装任何已安装的glusterfs-client软件包，则会发生错误。 差不多完成了，现在Heketi数据库存储在GlusterFS卷中，并且每次重新启动Heketi炉床时都不会重置。 </p><br><p> 要开始使用具有动态资源分配功能的GlusterFS集群，我们需要创建一个StorageClass。 </p><br><p> 首先，让我们找到Gluster存储端点，它将作为参数（heketi-storage-endpoints）传递给StorageClass： </p><br><pre> <code class="plaintext hljs">control# kubectl get endpoints NAME ENDPOINTS AGE heketi 10.42.0.2:8080 2d16h ....... ... ..</code> </pre> <br><p> 现在创建一些文件： </p><br><pre> <code class="plaintext hljs">control# vi storage-class.yml apiVersion: storage.k8s.io/v1beta1 kind: StorageClass metadata: name: slow provisioner: kubernetes.io/glusterfs parameters: resturl: "http://10.42.0.2:8080" control# vi test-pvc.yml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: gluster1 annotations: volume.beta.kubernetes.io/storage-class: "slow" spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi</code> </pre> <br><p> 使用以下文件创建类和pvc： </p><br><pre> <code class="plaintext hljs">control# kubectl create -f storage-class.yaml storageclass "slow" created control# kubectl get storageclass NAME PROVISIONER AGE slow kubernetes.io/glusterfs 2d8h control# kubectl create -f test-pvc.yaml persistentvolumeclaim "gluster1" created control# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE gluster1 Bound pvc-27f733cd-1c77-11e9-bb07-7efe6b0e6fa5 1Gi RWO slow 2d8h</code> </pre> <br><p> 我们还可以查看PV量： </p><br><pre> <code class="plaintext hljs">control# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-27f733cd-1c77-11e9-bb07-7efe6b0e6fa5 1Gi RWO Delete Bound default/gluster1 slow 2d8h</code> </pre> <br><p> 现在，我们有了一个与<strong>PersistentVolumeClaim</strong>关联的动态创建的GlusterFS卷，并且可以在任何子图中使用此语句。 </p><br><p> 在Nginx下创建一个简单的文件并进行测试： </p><br><pre> <code class="plaintext hljs">control# vi nginx-test.yml apiVersion: v1 kind: Pod metadata: name: nginx-pod1 labels: name: nginx-pod1 spec: containers: - name: nginx-pod1 image: gcr.io/google_containers/nginx-slim:0.8 ports: - name: web containerPort: 80 volumeMounts: - name: gluster-vol1 mountPath: /usr/share/nginx/html volumes: - name: gluster-vol1 persistentVolumeClaim: claimName: gluster1 control# kubectl create -f nginx-test.yaml pod "nginx-pod1" created</code> </pre> <br><p> 在下面浏览（等待几分钟，如果该图像尚不存在，则可能需要下载该图像）： </p><br><pre> <code class="plaintext hljs">control# kubectl get pods NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 4d10h glusterfs-hzdll 1/1 Running 0 4d10h glusterfs-p8r59 1/1 Running 0 4d10h heketi-b8c5f6554-knp7t 1/1 Running 0 2d18h nginx-pod1 1/1 Running 0 47h</code> </pre> <br><p> 现在进入容器并创建index.html文件： </p><br><pre> <code class="plaintext hljs">control# kubectl exec -ti nginx-pod1 /bin/sh # cd /usr/share/nginx/html # echo 'Hello there from GlusterFS pod !!!' &gt; index.html # ls index.html # exit</code> </pre> <br><p> 您将需要找到炉床的内部IP地址，并从任何主节点上将其卷曲： </p><br><pre> <code class="plaintext hljs">master1# curl 10.40.0.1 Hello there from GlusterFS pod !!!</code> </pre> <br><p> 这样，我们只需测试新的持久卷。 </p><br><blockquote> 签出新GlusterFS群集的一些有用命令是： <code>heketi-cli cluster list</code>和<code>heketi-cli volume list</code> 。 如果<strong>安装了heketi-cli，</strong>它们可以在您的计算机上运行。 在此示例中，这是节点<strong>master1</strong> 。 </blockquote><br><pre> <code class="plaintext hljs">master1# heketi-cli cluster list Clusters: Id:e83467d0074414e3f59d3350a93901ef [file][block] master1# heketi-cli volume list Id:6fdb7fef361c82154a94736c8f9aa53e Cluster:e83467d0074414e3f59d3350a93901ef Name:vol_6fdb7fef361c82154a94736c8f9aa53e Id:c6b69bd991b960f314f679afa4ad9644 Cluster:e83467d0074414e3f59d3350a93901ef Name:heketidbstorage</code> </pre> <br><p> 在此阶段，我们成功设置了带有文件存储的内部负载平衡器，并且我们的集群现在已接近运行状态。 </p><br><p> 在本文的下一部分中，我们将专注于创建集群监视系统，并在其中启动一个测试项目以使用我们配置的所有资源。 </p><br><p> 保持联系，万事如意！ </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN443110/">https://habr.com/ru/post/zh-CN443110/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN443098/index.html">使用磁铁和激光将数据写入光盘</a></li>
<li><a href="../zh-CN443100/index.html">计算Windows计算器中的错误</a></li>
<li><a href="../zh-CN443102/index.html">行为改变作为一种产品：为什么玛丽·近藤（Marie Kondo）与红杉资本（Sequoia Capital）进行了4000万美元的融资？</a></li>
<li><a href="../zh-CN443104/index.html">在Python中使用模糊三角数计算符号表达式</a></li>
<li><a href="../zh-CN443106/index.html">宣布推出USB4：有关标准的知识</a></li>
<li><a href="../zh-CN443112/index.html">您确定可以信任您的VPN吗？</a></li>
<li><a href="../zh-CN443114/index.html">DevProject奖：我在DeveloperWeek 2019上的演讲</a></li>
<li><a href="../zh-CN443120/index.html">国家杜马将继续打击非法出售SIM卡的行为</a></li>
<li><a href="../zh-CN443122/index.html">由于MongoDB公开开放，造成Verifications.io服务的8.09亿电子邮件地址泄漏</a></li>
<li><a href="../zh-CN443124/index.html">React.lazy？ 但是，如果您没有组件怎么办？</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>