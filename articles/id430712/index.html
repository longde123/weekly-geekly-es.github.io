<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>💩 ❣️ 🕵🏽 NeurIPS: Cara Menaklukkan Konferensi ML Terbaik 🔎 ✅ 🚎</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="NeurIPS –– konferensi yang saat ini dianggap sebagai acara paling top di dunia pembelajaran mesin. Hari ini saya akan bercerita tentang pengalaman say...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>NeurIPS: Cara Menaklukkan Konferensi ML Terbaik</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/430712/"><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">NeurIPS</a> –– konferensi yang saat ini dianggap sebagai acara paling top di dunia pembelajaran mesin.  Hari ini saya akan bercerita tentang pengalaman saya dalam berpartisipasi dalam kontes NeurIPS: bagaimana bersaing dengan akademisi terbaik di dunia, mengambil hadiah dan menerbitkan artikel. </p><br><img src="https://habrastorage.org/webt/hb/kq/-v/hbkq-vnd_xgxhvcixlo-u8b_pmk.jpeg"><a name="habracut"></a><br><hr><br><h1 id="v-chem-sut-konferencii">  Apa inti dari konferensi ini? </h1><br><p>  NeurIPS mendukung pengenalan metode pembelajaran mesin dalam berbagai disiplin ilmu.  Sekitar 10 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">lagu</a> diluncurkan setiap tahun untuk memecahkan masalah mendesak dunia akademik.  Menurut hasil kompetisi, para pemenang berbicara di konferensi dengan laporan, perkembangan baru dan algoritma.  Yang paling utama saya bersemangat tentang pembelajaran yang diperkuat (Reinforcement Learning atau RL), itu sebabnya saya telah berpartisipasi dalam kontes RL yang didedikasikan untuk NeurIPS untuk tahun kedua sekarang. </p><br><h1 id="pochemu-neurips">  Mengapa NeurIPS </h1><br><img src="https://habrastorage.org/webt/ei/c2/us/eic2usvfs-brxmsjczvkvygpfwq.png"><br><br>  NeurIPS terutama berfokus pada sains, bukan uang.  Dengan berpartisipasi dalam kontes, Anda melakukan sesuatu yang sangat penting, menangani masalah-masalah mendesak. <br><p>  Kedua, konferensi ini adalah acara global, para ilmuwan dari berbagai negara berkumpul di satu tempat, yang masing-masingnya dapat Anda bicarakan. </p><br><p>  Selain itu, seluruh konferensi diisi dengan pencapaian ilmiah terbaru dan hasil mutakhir, sangat penting bagi orang-orang dari bidang ilmu data untuk mengetahui dan memantau mereka. </p><br><h1 id="kak-nachat">  Bagaimana memulai? </h1><br><p>  Mulai berpartisipasi dalam kompetisi semacam itu cukup sederhana.  Jika Anda begitu memahami DL sehingga Anda bisa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">melatih ResNet</a> –– ini sudah cukup: daftar dan pergi.  Selalu ada papan peringkat publik tempat Anda dapat menilai tingkat Anda dengan sadar dibandingkan dengan peserta lain.  Dan jika ada sesuatu yang tidak jelas –– selalu ada saluran di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">slack</a> / discord / gitter / etc untuk membahas semua masalah yang muncul.  Jika topiknya benar-benar "milikmu", maka tidak ada yang akan menghentikan Anda dari menerima hasil yang dihargai - di semua kompetisi di mana saya berpartisipasi, semua pendekatan dan solusi dipelajari dan diimplementasikan tepat dalam perjalanan kompetisi. </p><br><h1 id="neurips-na-primere-konkretnogo-keysa-learning-to-run">  Studi Kasus NeurIPS: Belajar Berlari </h1><br><img src="https://habrastorage.org/webt/hu/ws/d5/huwsd5weqocxiuqv3hugygmfqea.jpeg"><br><br><h3 id="problematika">  Masalah </h3><br><p>  Kiprah seseorang adalah hasil dari interaksi otot, tulang, organ penglihatan dan telinga bagian dalam.  Dalam kasus gangguan pada sistem saraf pusat, gangguan motorik tertentu dapat terjadi, termasuk gangguan gaya berjalan - abasia. <br>  Para peneliti dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Stanford Laboratory of Neuromuscular Biomechanics</a> memutuskan untuk menghubungkan pembelajaran mesin dengan masalah perawatan untuk dapat bereksperimen dan menguji teori mereka pada model virtual kerangka, dan bukan pada orang yang masih hidup. </p><br><h3 id="postanovka-zadachi">  Pernyataan masalah </h3><br><p>  Peserta diberi kerangka manusia virtual (dalam simulator <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">OpenSim</a> ), yang memiliki prostesis sebagai pengganti satu kaki.  Tugasnya adalah mengajarkan kerangka untuk bergerak ke arah tertentu dengan kecepatan tertentu.  Selama simulasi, baik arah dan kecepatan bisa berubah. </p><br><img src="https://habrastorage.org/webt/od/vj/np/odvjnpxb7xogj5h_5ll85iokhp0.jpeg"><br><br>  Untuk mendapatkan model kontrol kerangka virtual, diusulkan untuk menggunakan pembelajaran penguatan.  Simulator memberi kami beberapa keadaan kerangka S (vektor ~ 400 angka).  Itu perlu untuk memprediksi tindakan apa yang perlu dilakukan (kekuatan aktivasi otot-otot kaki adalah vektor dari 19 angka).  Dalam proses simulasi, kerangka itu diberikan penghargaan R - sebagai jenis konstan dikurangi penalti untuk menyimpang dari kecepatan dan arah yang diberikan. <br><div class="spoiler">  <b class="spoiler_title">Tentang pelatihan penguatan</b> <div class="spoiler_text"><p>  Reinforcement Learning (RL) adalah area yang berkaitan dengan teori keputusan dan pencarian kebijakan perilaku yang optimal. </p><br><p>  Ingat bagaimana mereka mengajar <del>  kucing </del>  trik baru doggie.  Ulangi beberapa tindakan, berikan yummy untuk melakukan trik, dan jangan berikan untuk tidak terpenuhinya.  Anjing harus memahami semua ini dan menemukan strategi perilaku ("kebijakan" atau "kebijakan" dalam hal RL), yang memaksimalkan jumlah permen yang diterima. </p><br><p>  Secara resmi, kami memiliki agen (anjing) yang terlatih tentang sejarah interaksi dengan lingkungan (orang).  Pada saat yang sama, lingkungan, mengevaluasi tindakan agen, memberinya hadiah (lezat) - semakin baik perilaku agen, semakin besar hadiahnya.  Karenanya, tugas agen adalah menemukan kebijakan yang memaksimalkan hadiah untuk seluruh waktu interaksi dengan lingkungan. </p><br><p>  Mengembangkan topik ini lebih lanjut, solusi berbasis aturan - perangkat lunak 1.0, ketika semua aturan ditetapkan oleh pengembang, pembelajaran yang diawasi - perangkat lunak 2.0, ketika sistem belajar sendiri menggunakan contoh-contoh yang tersedia dan menemukan ketergantungan data, pembelajaran penguatan adalah selangkah lebih maju ketika sistem itu sendiri belajar untuk meneliti, bereksperimen dan menemukan dependensi yang diperlukan dalam keputusannya.  Semakin jauh kita melangkah, semakin baik kita mencoba mengulangi bagaimana seseorang belajar. </p></div></div><br><h3 id="osobennosti-zadachi">  Fitur Tugas </h3><br><p>  Tugas ini terlihat seperti perwakilan khas pembelajaran yang diperkuat untuk tugas dengan ruang tindakan kontinu (RL untuk ruang tindakan kontinu).  Ini berbeda dari RL biasa karena daripada memilih tindakan tertentu (menekan tombol joystick), tindakan ini diperlukan untuk memprediksi secara akurat (dan ada banyak kemungkinan yang tak terbatas). </p><br><p>  Pendekatan dasar untuk solusi ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Deep Deterministic Policy Gradient</a> ) ditemukan kembali pada tahun 2015, yang untuk waktu yang lama dengan standar DL, wilayah ini terus berkembang secara aktif dalam aplikasi robotika dan aplikasi RL dunia nyata.  Ada sesuatu untuk diperbaiki: pendekatan yang kuat (agar tidak merusak robot asli), efisiensi sampel (agar tidak mengumpulkan data dari robot nyata selama berbulan-bulan) dan masalah RL lainnya (eksplorasi vs eksploitasi trade-off, dll).  Dalam kompetisi ini, mereka tidak memberi kami robot nyata - hanya simulasi, tetapi simulator itu sendiri 2.000 kali lebih lambat daripada rekan-rekan Open Source (di mana semua orang memeriksa algoritma RL mereka), dan karenanya membawa masalah efisiensi sampel ke tingkat yang baru. </p><br><h3 id="etapy-sorevnovaniya">  Tahapan Persaingan </h3><br><p>  Kompetisi itu sendiri berlangsung dalam tiga tahap, di mana tugas dan kondisi agak berubah. </p><br><ul><li>  Tahap 1: kerangka belajar berjalan lurus dengan kecepatan 3 meter per detik.  Tugas itu dianggap selesai jika agen melewati 300 langkah. </li><li>  Tahap 2: kecepatan dan arah berubah dengan frekuensi reguler.  Panjang jarak meningkat menjadi 1000 langkah. </li><li>  Tahap 3: solusi akhir harus dikemas dalam gambar buruh pelabuhan dan dikirim untuk verifikasi.  Secara total, 10 paket dapat dibuat. </li></ul><br><p>  Metrik kualitas utama dianggap sebagai hadiah total untuk simulasi, yang menunjukkan seberapa baik kerangka berpegang pada arah dan kecepatan tertentu sepanjang jarak. </p><br><p>  Selama tahap 1 dan 2, kemajuan masing-masing peserta ditampilkan di leaderboard.  Solusi terakhir harus dikirim sebagai gambar buruh pelabuhan.  Ini memberikan batasan pada jam kerja dan sumber daya. </p><br><div class="spoiler">  <b class="spoiler_title">Coolstory: leaderboard publik dan RL</b> <div class="spoiler_text"><p>  Karena ketersediaan leaderboard, tidak ada yang menunjukkan model terbaik mereka untuk memberikan "sedikit lebih banyak dari biasanya" di babak final dan mengejutkan rival. </p></div></div><br><h6 id="pochemu-tak-vazhny-docker-obrazy">  Mengapa gambar buruh pelabuhan sangat penting </h6><br><p>  Tahun lalu, sebuah insiden kecil terjadi ketika mengevaluasi keputusan di babak pertama.  Pada saat itu, pemeriksaan melalui interaksi http dengan platform, dan wajah kondisi untuk pengujian ditemukan.  Orang bisa mengetahui di mana situasi tertentu agen dievaluasi dan melatihnya hanya di bawah kondisi ini.  Yang tentu saja tidak menyelesaikan masalah sebenarnya.  Itulah sebabnya mereka memutuskan untuk mentransfer sistem pengiriman ke gambar buruh pelabuhan dan meluncurkannya di server jarak jauh penyelenggara.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Dbrain</a> menggunakan sistem yang sama untuk menghitung hasil kompetisi tepat untuk alasan yang sama. </p><br><h1 id="klyuchevye-momenty">  Poin-poin penting </h1><br><h3 id="komanda">  Tim </h3><br><img src="https://habrastorage.org/webt/ty/ur/gp/tyurgpqbzb2zl2wimtzri0mnpwk.jpeg"><br><br>  Hal pertama yang penting bagi keberhasilan seluruh perusahaan adalah tim.  Tidak peduli seberapa baik Anda (dan seberapa kuat cakarnya) - partisipasi dalam tim sangat meningkatkan peluang keberhasilan.  Alasannya sederhana - berbagai pendapat dan pendekatan, memeriksa kembali hipotesis, kemampuan untuk memparalelkan pekerjaan dan melakukan lebih banyak eksperimen.  Semua ini sangat penting ketika menyelesaikan masalah baru yang harus Anda hadapi. <br><p>  Idealnya, pengetahuan dan keterampilan Anda harus berada pada level yang sama dan saling melengkapi.  Jadi, misalnya, tahun ini saya menanam tim kami di PyTorch, dan saya mendapat beberapa gagasan awal tentang penerapan sistem pelatihan agen terdistribusi. </p><br><p>  Bagaimana cara menemukan tim?  Pertama, Anda dapat bergabung dengan barisan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ods</a> dan mencari orang-orang yang berpikiran sama di sana.  Kedua, untuk RL-fellows ada ruang obrolan terpisah di telegram - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">klub RL</a> .  Ketiga, Anda dapat mengambil kursus yang luar biasa dari ShAD - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Praktis RL</a> , setelah itu Anda pasti akan mendapatkan beberapa kenalan. </p><br><p>  Namun, perlu diingat kebijakan "penyerahan - atau tidak."  Jika Anda ingin bersatu, pertama-tama ambil keputusan Anda, kirim, tunjukkan di papan peringkat dan tunjukkan level Anda.  Seperti yang ditunjukkan oleh latihan, tim semacam itu jauh lebih seimbang. </p><br><h3 id="motivaciya">  Motivasi </h3><br><p>  Seperti yang sudah saya tulis, jika topiknya adalah "milikmu", maka tidak ada yang akan menghentikan Anda.  Ini berarti bahwa wilayah tersebut tidak hanya menyukai Anda, tetapi menginspirasi Anda - Anda membakarnya, Anda ingin menjadi yang terbaik di dalamnya. <br>  Saya bertemu RL 4 tahun yang lalu - selama perjalanan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Berkeley 188x - Pengantar AI</a> - dan masih tidak bisa berhenti bertanya-tanya pada kemajuan di bidang ini. </p><br><h3 id="sistematichnost">  Sistematis </h3><br><p>  Ketiga, tetapi sama pentingnya - Anda harus dapat melakukan apa yang Anda janjikan, berinvestasi dalam kompetisi setiap hari dan hanya ... menyelesaikannya.  Setiap hari  Tidak ada bakat bawaan yang dapat dibandingkan dengan kemampuan untuk melakukan sesuatu, bahkan sedikit, tetapi setiap hari.  Untuk inilah motivasi dibutuhkan.  Agar berhasil, saya sarankan membaca <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">DeepWork</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">AMA ternaus</a> . </p><br><h3 id="time-management">  Manajemen waktu </h3><br><p>  Keahlian lain yang sangat penting adalah kemampuan untuk mendistribusikan kekuatan seseorang dan menggunakan waktu luang dengan benar.  Menggabungkan kerja penuh waktu dan partisipasi dalam kompetisi adalah tugas yang tidak sepele.  Hal terpenting dalam kondisi ini adalah jangan sampai terbakar dan tahan seluruh beban.  Untuk melakukan ini, Anda perlu mengatur waktu Anda dengan benar, dengan sungguh-sungguh menilai kekuatan Anda dan jangan lupa untuk bersantai tepat waktu. </p><br><h3 id="overwork">  Terlalu banyak pekerjaan </h3><br><p>  Pada tahap akhir kompetisi, sebuah situasi biasanya muncul di mana secara harfiah dalam seminggu Anda harus melakukan tidak hanya banyak, tetapi SANGAT banyak.  Untuk hasil terbaik, Anda harus bisa memaksa diri Anda untuk duduk dan melakukan lompatan terakhir ke hadiah yang didambakan. </p><br><div class="spoiler">  <b class="spoiler_title">Coolstory: batas waktu setelah batas waktu</b> <div class="spoiler_text"><p>  Karena apa, secara umum, Anda mungkin perlu mendaur ulang untuk kepentingan kompetisi?  Jawabannya cukup sederhana - batas waktu transfer.  Pada kompetisi seperti itu, panitia seringkali tidak dapat memprediksi segalanya, karena cara termudah adalah memberi peserta lebih banyak waktu.  Tahun ini kompetisi diperpanjang 3 kali: pertama selama sebulan, kemudian selama seminggu dan pada saat terakhir (24 jam sebelum batas waktu) - selama 2 hari.  Dan jika selama dua transfer pertama Anda hanya perlu mengatur waktu ekstra dengan benar, maka dalam dua hari terakhir Anda hanya perlu membajak. </p></div></div><br><h3 id="theory">  Teori </h3><br><img src="https://habrastorage.org/webt/gf/rg/9q/gfrg9ql1ukvjmlbglwcizlfcpto.png"><br><p>  Antara lain, jangan lupakan teori - untuk mewaspadai apa yang terjadi di lapangan dan bisa mencatat yang relevan.  Jadi, misalnya, untuk menyelesaikan tahun lalu, tim kami berangkat dari artikel berikut: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kontrol berkelanjutan dengan pembelajaran penguatan dalam</a> adalah artikel dasar tentang pembelajaran penguatan dalam untuk tugas dengan ruang tindakan berkelanjutan. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Parameter Space Noise for Exploration</a> - studi tentang penambahan kebisingan pada bobot agen untuk studi lingkungan yang lebih baik.  Berdasarkan pengalaman - salah satu teknik terbaik untuk eksplorasi di RL. </li></ul><br><p>  Tahun ini, beberapa lagi ditambahkan kepada mereka: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Perspektif Distribusi pada Pembelajaran Penguatan</a> - Pandangan baru pada prediksi hadiah yang mungkin.  Alih-alih hanya memprediksi rata-rata, distribusi hadiah di masa depan dihitung. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pembelajaran Penguatan Distribusi dengan Regresi Kuantil</a> adalah kelanjutan dari pekerjaan sebelumnya, tetapi dengan "kuantisasi" distribusi. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pemutaran Pengalaman Prioritas Terdistribusi</a> - bekerja dari arah pembelajaran penguatan yang mendalam pada skala.  Tentang cara mengatur arsitektur percobaan dengan benar untuk memaksimalkan penggunaan sumber daya yang tersedia dan meningkatkan kecepatan agen pelatihan. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Gradien Kebijakan Deterministik Distribusi Terdistribusi</a> - kombinasi dari tiga artikel sebelumnya untuk tugas dengan ruang tindakan yang berkelanjutan. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Mengatasi Kesalahan Perkiraan Fungsi dalam Metode Aktor-Kritik</a> - pekerjaan luar biasa untuk meningkatkan kekokohan agen RL.  Saya sarankan membacanya. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pembelajaran Hirarki Penguatan Data yang Efisien</a> adalah pengembangan dari artikel sebelumnya di bidang pembelajaran penguatan hierarkis (HRL). </li></ul><br><div class="spoiler">  <b class="spoiler_title">Bacaan tambahan</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning dengan Stochastic Actor</a> - penulis mengusulkan metode pelatihan kebijakan stokastik dengan pembelajaran penguatan kebijakan di luar kebijakan.  Berkat artikel ini, menjadi mungkin untuk melatih para politisi non-deterministik bahkan dalam tugas dengan ruang tindakan yang berkesinambungan. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kebijakan Ruang Laten untuk Pembelajaran Penguatan Hierarkis</a> adalah kelanjutan dari artikel HRL sebelumnya dengan kebijakan stokastik multi-level. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Keragaman Adalah Yang Anda Butuhkan: Keterampilan Belajar Tanpa Fungsi Hadiah</a> - artikel ini berisi pendekatan dengan mempelajari banyak kebijakan stokastik level rendah acak tanpa imbalan dari lingkungan.  Selanjutnya, ketika kita telah menetapkan fungsi hadiah, penghargaan yang paling berkorelasi dengan penghargaan dapat digunakan untuk mengajar politik tingkat tinggi di atas. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Penguatan Pembelajaran dan Kontrol sebagai Probabilistic Inference: Tutorial and Review</a> - ikhtisar dari semua jenis metode pembelajaran penguatan entropi maksimum dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Sergey Levine</a> . </li></ul><br><p>  Saya juga menyarankan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">OpenAI pilihan artikel</a> tentang penguatan pembelajaran dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">versinya untuk mendeley</a> .  Dan jika Anda tertarik dengan topik pelatihan penguatan, bergabunglah dengan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">klub</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">RL</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">makalah RL</a> . </p></div></div><br><h3 id="practice">  Berlatih </h3><br><img src="https://habrastorage.org/webt/xp/g7/it/xpg7itebqdpi3cwex33uzrzkidg.jpeg"><br><br>  Mengetahui teori saja tidak cukup - penting untuk dapat mempraktikkan semua pendekatan ini dan membangun sistem validasi yang benar untuk mengevaluasi keputusan.  Sebagai contoh, tahun ini kami mengetahui bahwa agen kami mengatasi beberapa kasus regional dengan buruk hanya 2 hari sebelum akhir kompetisi.  Karena itu, kami tidak punya waktu untuk memperbaiki model kami sepenuhnya dan tidak benar-benar mendapatkan beberapa poin ke tempat kedua yang didambakan.  Jika kami menemukan ini bahkan dalam seminggu - hasilnya bisa lebih baik. <br><div class="spoiler">  <b class="spoiler_title">Coolstory: episode III</b> <div class="spoiler_text"><p>  Penghargaan rata-rata untuk 10 episode uji berfungsi sebagai penilaian akhir dari solusi. </p><br><img src="https://habrastorage.org/webt/jq/bj/yc/jqbjyctkjettuu2bqd19xcahssk.png"><br><p>  Grafik menunjukkan hasil pengujian agen kami: 9 dari 10 episode, kerangka kami baik-baik saja (rata-rata - 9955,66), tetapi satu episode .... Episode 3 tidak diberikan kepadanya (hadiah 9870).  Kesalahan inilah yang menyebabkan jatuhnya kecepatan akhir ke 9947 (-8 poin). </p></div></div><br><h3 id="udacha">  Semoga beruntung </h3><br><p>  Dan akhirnya - jangan lupa tentang keberuntungan dangkal.  Jangan berpikir bahwa ini adalah poin yang kontroversial.  Sebaliknya, sedikit keberuntungan memberikan kontribusi besar untuk pekerjaan konstan pada diri sendiri: bahkan jika probabilitas keberuntungan hanya 10%, seseorang yang mencoba berpartisipasi dalam kompetisi 100 kali akan berhasil lebih banyak daripada seseorang yang mencoba hanya 1 kali dan meninggalkan ide. </p><br><h1 id="tuda-i-obratno-reshenie-proshlogo-goda--trete-mestohttpswwwcrowdaiorgchallengesnips-2017-learning-to-runwinners">  Pulang pergi: keputusan tahun lalu - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">tempat ketiga</a> </h1><br><img src="https://habrastorage.org/webt/mq/lx/i_/mqlxi_alc8pt0acnzwoi8twb8oo.jpeg"><br><br>  Tahun lalu, tim kami - Mikhail Pavlov dan saya - berpartisipasi dalam kompetisi NeurIPS untuk pertama kalinya dan motivasi utamanya adalah berpartisipasi dalam kompetisi NeurIPS pertama dalam pembelajaran penguatan.  Kemudian saya baru saja menyelesaikan kursus <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Praktis RL</a> di SHAD dan ingin menguji keterampilan yang diperoleh.  Sebagai hasilnya, kami mengambil tempat ketiga terhormat, hanya kalah dari nnaisene (Schmidhuber) dan tim universitas dari Cina.  Pada saat itu, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">solusi</a> kami “cukup sederhana” dan didasarkan pada DDPG Terdistribusi dengan parameter noise ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">publikasi</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">presentasi pada ml</a> . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pelatihan</a> ). <br><h1 id="reshenie-etogo-goda--trete-mestohttpswwwcrowdaiorgchallengesnips-2018-ai-for-prosthetics-challengeleaderboards">  Keputusan tahun ini adalah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">tempat ketiga</a> </h1><br><img src="https://habrastorage.org/webt/gf/qq/to/gfqqtoneh51dn47m3f7oicyqixk.jpeg"><br><p>  Ada beberapa perubahan tahun ini.  Pertama, tidak ada keinginan untuk hanya berpartisipasi dalam kompetisi ini, saya ingin memenangkannya.  Kedua, komposisi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">tim</a> juga <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">telah</a> berubah: Alexey Grinchuk, Anton Pechenko dan saya.  Ambil dan menangkan - tidak berhasil, tetapi kami kembali mengambil tempat ke-3. <br>  Solusi kami akan secara resmi disajikan di NeurIPS, dan sekarang kami akan membatasi diri pada sejumlah kecil detail.  Berdasarkan keputusan tahun lalu dan keberhasilan pembelajaran penguatan kebijakan tahun ini (artikel di atas), kami menambahkan sejumlah perkembangan kami sendiri, yang akan kami bicarakan di NeurIPS, dan mendapat Kritik Ensemble Distributed Quantile, dengan mana kami mengambil tempat ketiga. </p><br><p>  Semua praktik terbaik kami - sistem pembelajaran terdistribusi, algoritma, dll. Akan dipublikasikan dan tersedia di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Catalyst.RL</a> setelah NeurIPS. </p><br><div class="spoiler">  <b class="spoiler_title">Coolstory: anak laki-laki besar - senjata besar</b> <div class="spoiler_text"><p>  Tim kami dengan percaya diri pergi ke peringkat 1 sepanjang kompetisi.  Namun, orang-orang besar punya rencana lain - 2 pemain besar memasuki kompetisi 2 minggu sebelum akhir kompetisi: FireWork (Baidu) dan nnaisense (Schmidhuber).  Dan jika tidak ada yang bisa dilakukan dengan Google Cina, maka dengan tim Schmidhuber cukup lama kami dapat dengan jujur ​​berjuang untuk tempat kedua, kehilangan hanya dengan margin minimal.  Bagiku itu cukup bagus untuk kekasih. </p></div></div><br><h1 id="zachem-eto-vse">  Kenapa ini semua? </h1><br><ul><li>  Komunikasi  Peneliti top datang ke konferensi dengan siapa Anda dapat mengobrol langsung, yang tidak akan memberikan korespondensi email. </li><li>  Publikasi  Jika solusi mengambil hadiah, maka tim diundang ke konferensi (atau mungkin lebih dari satu) untuk mempresentasikan keputusannya dan mempublikasikan artikel. </li><li>  Tawaran pekerjaan dan PhD.  Publikasi dan hadiah dalam konferensi semacam itu secara signifikan meningkatkan peluang Anda untuk mendapatkan posisi di perusahaan-perusahaan terkemuka seperti OpenAI, DeepMind, Google, Facebook, Microsoft. </li><li>  Nilai dunia nyata.  NeurIPS dilakukan untuk memecahkan masalah mendesak dari dunia akademik dan dunia nyata.  Anda dapat yakin bahwa hasilnya tidak akan sampai ke meja, tetapi benar-benar akan diminati dan akan membantu meningkatkan dunia. </li><li>  Berkendara  Menyelesaikan kontes seperti itu ... hanya menarik.  Dalam sebuah kompetisi, Anda dapat memunculkan banyak ide baru, menguji berbagai pendekatan - hanya untuk menjadi yang terbaik.  Dan mari kita jujur, kapan lagi Anda bisa mengendarai kerangka, bermain game dan semua ini dengan tampilan serius dan demi ilmu pengetahuan? </li></ul><br><div class="spoiler">  <b class="spoiler_title">Coolstory: visa dan RL</b> <div class="spoiler_text"><p>  Saya sangat tidak menyarankan mencoba menjelaskan kepada orang Amerika memeriksa Anda bahwa Anda akan pergi ke konferensi, karena Anda melatih kerangka virtual untuk berjalan dalam simulasi.  Pergi saja ke konferensi dengan bicara. </p></div></div><br><h1 id="itogi">  Ringkasan </h1><br><p>  Berpartisipasi dalam NeurIPS adalah pengalaman yang sulit ditaksir terlalu tinggi.  Jangan takut dengan berita utama profil tinggi - Anda hanya perlu menenangkan diri dan mulai memutuskan. </p><br><p>  Dan pergi ke <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Catalyst.RL</a> , lalu apa. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id430712/">https://habr.com/ru/post/id430712/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id430700/index.html">Sistem terpadu untuk merekam tayangan film online akan mulai berfungsi di Rusia</a></li>
<li><a href="../id430704/index.html">Bagaimana Teknologi Kecerdasan Buatan Membantu Aviasales Tumbuh: Tujuh Contoh</a></li>
<li><a href="../id430706/index.html">Teori Evolusi Baru</a></li>
<li><a href="../id430708/index.html">Tic Tac Toe “Tanpa Batas”</a></li>
<li><a href="../id430710/index.html">Apa yang harus dilakukan jika Black Friday besok dan server Anda belum siap</a></li>
<li><a href="../id430714/index.html">VMware membeli Heptio - apa artinya untuk Kubernetes</a></li>
<li><a href="../id430718/index.html">Untuk objek apa layak menggunakan pengawasan video cloud?</a></li>
<li><a href="../id430720/index.html">Intel RealSense D435i: pembaruan kecil dan penyimpangan sejarah singkat</a></li>
<li><a href="../id430722/index.html">Kinerja PHP: perencanaan, pembuatan profil, optimisasi</a></li>
<li><a href="../id430724/index.html">DEFCON 21. Konferensi DNS dapat berbahaya bagi kesehatan Anda. Bagian 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>