<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üèçÔ∏è ‚è≤Ô∏è ü¶Ü Migration von Nagios zu Icinga2 in Australien üíΩ ü§∞üèΩ üîÜ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo an alle. 


 Ich bin der Linux-Systemadministrator und bin 2015 mit einem unabh√§ngigen Berufsvisum von Russland nach Australien gezogen. In dem ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Migration von Nagios zu Icinga2 in Australien</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/444060/"><p>  Hallo an alle. </p><br><p>  Ich bin der Linux-Systemadministrator und bin 2015 mit einem unabh√§ngigen Berufsvisum von Russland nach Australien gezogen. In dem Artikel geht es jedoch nicht darum, wie man einem Ferkel einen Traktor gibt.  Solche Artikel sind bereits ausreichend (bei Interesse schreibe ich auch dar√ºber), daher m√∂chte ich dar√ºber sprechen, wie ich bei meiner Arbeit in Australien als Linux-Ops-Ingenieur die Migration von einem System initiiert habe √úberwachung zu einem anderen.  Insbesondere - Nagios =&gt; Icinga2. </p><br><p>  Der Artikel ist teilweise technisch und teilweise √ºber die Kommunikation mit Menschen und Probleme im Zusammenhang mit dem Unterschied in Kultur und Arbeitsmethoden. </p><a name="habracut"></a><br><p>  Leider hebt das "Code" -Tag den Puppet- und Yaml-Code nicht hervor, so dass ich "Klartext" verwenden musste. </p><br><p>  Am Morgen des 21. Dezember 2016 wurde nichts krank.  Wie √ºblich las ich Habr in der ersten halben Stunde des Arbeitstages mit einem nicht registrierten Anonymus, nahm Kaffee auf und stie√ü auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesen Artikel</a> . </p><br><p>  Da Nagios in meiner Firma verwendet wurde, ohne dar√ºber nachzudenken, habe ich ein Ticket in Redmine erstellt und den Link in den allgemeinen Chat geworfen, weil ich es f√ºr wichtig hielt.  Die Initiative ist sogar in Australien strafbar, daher hat der leitende Ingenieur dieses Problem an mich geh√§ngt, seit ich es entdeckt habe. </p><br><div class="spoiler">  <b class="spoiler_title">Bildschirm von Redmine</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/ge/uc/gu/geucgutwhu3_zu4uwes2ozuwa3e.jpeg"></div></div><br><p>  In unserer Abteilung ist es √ºblich, vor der Abgabe unserer Meinung mindestens eine Alternative anzubieten, auch wenn die Wahl offensichtlich ist. Deshalb habe ich zun√§chst gegoogelt, welche √úberwachungssysteme im Allgemeinen derzeit relevant sind, da ich in Russland am letzten Ort, an dem ich gearbeitet habe, mein eigenes pers√∂nliches Aufzeichnungssystem hatte. sehr primitiv, aber dennoch ziemlich funktionierend und erf√ºllt alle ihm zugewiesenen Aufgaben.  Python, St. Petersburg Polytechnic und die U-Bahn-Regel.  Nein, die U-Bahn ist schei√üe.  Dies ist pers√∂nlich (11 Jahre Arbeit) und verdient einen separaten Artikel, aber nicht jetzt. </p><br><p>  Ein wenig √ºber die Regeln f√ºr √Ñnderungen an der Infrastrukturkonfiguration an meinem aktuellen Standort.  Wir verwenden Puppet, Gitlab und das Prinzip der Infrastruktur als Code, so dass: </p><br><ul><li>  Keine manuellen √Ñnderungen √ºber SSH durch manuelles √Ñndern von Dateien auf virtuellen Maschinen.  F√ºr drei Jahre Arbeit habe ich viele Male einen Hut daf√ºr erhalten, den letzten vor einer Woche, und ich glaube nicht, dass es das letzte Mal war.  In der Tat - korrigieren Sie eine Zeile in der Konfiguration, starten Sie den Dienst neu und pr√ºfen Sie, ob das Problem behoben wurde - 10 Sekunden.  Erstellen Sie einen neuen Zweig in Gitlab, √ºbertragen Sie die √Ñnderungen, warten Sie, bis r10k auf Puppetmaster funktioniert, f√ºhren Sie Puppet --environment = mybranch aus und warten Sie noch ein paar Minuten, bis dies alles funktioniert - mindestens 5 Minuten. </li><li>  Alle √Ñnderungen werden durch Erstellen einer Zusammenf√ºhrungsanforderung in Gitlab vorgenommen. Sie m√ºssen die Genehmigung von mindestens einem Teammitglied einholen.  Wesentliche √Ñnderungen am Teamleiter erfordern zwei oder drei Genehmigungen. </li><li>  Alle √Ñnderungen sind auf die eine oder andere Weise textuell (da Puppet-Manifeste, Hiera-Skripte und -Daten Text sind), Bin√§rdateien werden dringend empfohlen, und es sind gute Gr√ºnde erforderlich, um sie zu genehmigen. </li></ul><br><p>  Die Optionen, die ich mir angesehen habe, sind: </p><br><ul><li>  Munin - Wenn mehr als 10 Server in der Infrastruktur vorhanden sind, wird die Verwaltung zur H√∂lle (aus <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesem Artikel</a> . Ich hatte nicht viel Lust, dies zu √ºberpr√ºfen, also habe ich mein Wort daf√ºr genommen). </li><li>  Zabbix - hat schon lange in Russland nachgesehen, aber dann war es f√ºr meine Aufgaben √ºberfl√ºssig.  Hier - musste aufgrund der Verwendung von Puppet als Konfigurationsmanager und Gitlab als Versionskontrollsystem fallengelassen werden.  In diesem Moment speichert Zabbix nach meinem Verst√§ndnis die gesamte Konfiguration in einer Datenbank. Daher war nicht klar, wie die Konfiguration unter den aktuellen Bedingungen verwaltet und √Ñnderungen verfolgt werden sollen. </li><li>  Prometheus ist das, worauf wir am Ende kommen werden, gemessen an der Stimmung in der Abteilung, aber zu diesem Zeitpunkt konnte ich es nicht beherrschen und konnte keine wirklich funktionierende Probe (Proof of Concept) demonstrieren, also musste ich mich weigern. </li><li>  Es gab mehrere andere Optionen, die entweder eine vollst√§ndige Neugestaltung des Systems erforderten oder noch in den Kinderschuhen steckten / aufgegeben wurden und aus demselben Grund abgelehnt wurden. </li></ul><br><p>  Am Ende habe ich mich aus drei Gr√ºnden f√ºr Icinga2 entschieden: </p><br><p>  1 - Kompatibilit√§t mit Nrpe (einem Client-Dienst, der Befehle von Nagios √ºberpr√ºft).  Dies war sehr wichtig, da wir zu dieser Zeit 135 (jetzt sind es 165 davon im Jahr 2019) virtuelle Maschinen mit einer Reihe von selbstgeschriebenen Diensten / Schecks hatten und all dies zu wiederholen, w√§re eine schreckliche H√§morrhoide. <br>  2 - Alle Konfigurationsdateien sind Textdateien, wodurch es einfach ist, diese Angelegenheit zu bearbeiten, Zusammenf√ºhrungsanforderungen zu erstellen und zu sehen, was hinzugef√ºgt oder gel√∂scht wurde. <br>  3 ist ein lebendiges und wachsendes OpenSource-Projekt.  Wir lieben OpenSource sehr und leisten einen praktikablen Beitrag dazu, indem wir Pull-Anfragen und -Probleme erstellen, um Probleme zu l√∂sen. </p><br><p>  Also lass uns gehen, Icinga2. </p><br><p>  Das erste, was ich zu bew√§ltigen hatte, war die Tr√§gheit meiner Kollegen.  Jeder ist an Nagios / Najios (obwohl sie auch hier keine Kompromisse bei der Aussprache eingehen konnten) und die CheckMK-Oberfl√§che gew√∂hnt.  Die Icinga-Oberfl√§che sieht v√∂llig anders aus (es war ein Minus), aber es ist m√∂glich, das, was Sie sehen m√ºssen, mit Filtern buchst√§blich anhand eines beliebigen Parameters flexibel zu konfigurieren (es war ein Plus, aber ich habe insbesondere daf√ºr gek√§mpft). </p><br><div class="spoiler">  <b class="spoiler_title">Filter</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/kx/bf/0e/kxbf0eqj-gh7bfkpgtzgjjgvhwm.jpeg"></div></div><br><p>  Sch√§tzen Sie das Verh√§ltnis der Gr√∂√üe der Bildlaufleiste zur Gr√∂√üe des Bildlauffelds. </p><br><p>  Das zweite - jeder ist es gewohnt, die gesamte Infrastruktur auf einem Monitor zu sehen, da Sie mit CheckMk mit mehreren Nagios-Hosts arbeiten k√∂nnen, aber Icinga wusste nicht, wie es geht (tats√§chlich, aber mehr dazu weiter unten).  Eine Alternative war Thruk, aber ihr Design verursachte Erbrechen bei allen Teammitgliedern, au√üer bei einem - demjenigen, der es vorgeschlagen hatte (nicht bei mir). </p><br><div class="spoiler">  <b class="spoiler_title">Thruk Firebox - Einstimmige Teamentscheidung</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/tv/1c/_l/tv1c_lkx-h9rbwgpdy63jdohnve.png"></div></div><br><p>  Nach ein paar Tagen eines Brainstormings schlug ich die Idee der Cluster√ºberwachung vor, wenn sich ein Master-Host in der Produktionszone und zwei Untergebene befinden - einer in dev / test und ein externer Host bei einem anderen Anbieter, um unsere Dienste aus der Sicht eines Kunden oder eines Au√üenstehenden zu √ºberwachen Beobachter.  Diese Konfiguration erm√∂glichte es mir, alle Probleme in einer webbasierten Oberfl√§che zu sehen, und es funktionierte f√ºr mich, aber Puppet ... Das Problem mit Puppet war, dass der Master-Host nun alle Hosts und Dienste / Pr√ºfungen im System kennen und sie auf Zonen verteilen musste (dev-test, staging-prod, ext), aber das Senden von √Ñnderungen √ºber die Icinga-API dauert einige Sekunden, das Kompilieren des Puppet-Verzeichnisses aller Dienste f√ºr alle Hosts jedoch einige Minuten.  Das wird mir immer noch angelastet, obwohl ich bereits mehrmals erkl√§rt habe, wie alles funktioniert und warum alles so lange dauert. </p><br><p>  Drittens - ein Haufen SnowFlakes (Schneeflocken) - Dinge, die aus dem allgemeinen System herausgeschlagen werden, weil sie etwas Besonderes haben, sodass die allgemeinen Regeln f√ºr sie nicht gelten.  Es wurde durch einen Frontalangriff entschieden - wenn es einen Alarm gibt, aber tats√§chlich alles in Ordnung ist, dann m√ºssen Sie hier tiefer graben und verstehen, warum es mich alarmiert, obwohl es nicht sollte.  Oder umgekehrt - warum Nagios in Panik ger√§t, Icinga jedoch nicht. </p><br><p>  Viertens: Nagios hat drei Jahre f√ºr mich gearbeitet und anfangs gab es mehr Vertrauen in ihn als in mein neues Hipster-System. Jedes Mal, wenn Icinga in Panik geriet, tat niemand etwas, bis Nagios sich √ºber das gleiche Thema aufregte.  Aber sehr selten gab Icinga fr√ºher als Nagios echte Alarme aus, und ich halte dies f√ºr einen ernsthaften Pfosten, den ich im Abschnitt "Schlussfolgerungen" er√∂rtern werde. </p><br><p>  Infolgedessen verz√∂gerte sich die Inbetriebnahme um mehr als 5 Monate (geplant am 28. Juni 2018, tats√§chlich - 3. Dezember 2018), haupts√§chlich wegen der ‚ÄûParit√§tspr√ºfung‚Äú - diesem Mist, wenn es in Nagios mehrere Dienste gibt, √ºber die niemand spricht Ich habe in den letzten Jahren nichts geh√∂rt, aber JETZT haben sie verdammt ohne Grund Kritik abgegeben und ich musste erkl√§ren, warum sie nicht in meinem Panel waren und sie zu Icinga hinzuf√ºgen, um "Parit√§tspr√ºfung ist abgeschlossen" (Alle Dienste / Pr√ºfungen) in Nagios entsprechen Dienstleistungen / Schecks in Icinga) </p><br><p>  Implementierung: <br>  Der erste ist der Code vs Data War wie Puppet Style.  Alle Daten, hier alles im Allgemeinen, sollten in Hiera sein und sonst nichts.  Der gesamte Code befindet sich in PP-Dateien.  Variablen, Abstraktionen, Funktionen - alles geht in pp. <br>  Infolgedessen verf√ºgen wir √ºber eine Reihe virtueller Maschinen (165 zum Zeitpunkt des Schreibens) und 68 Webanwendungen, die auf den Zustand und die G√ºltigkeit von SSL-Zertifikaten √ºberwacht werden m√ºssen.  Aufgrund historischer H√§morrhoiden werden Informationen f√ºr √úberwachungsanwendungen aus einem separaten Gitlab-Repository entnommen, und das Datenformat hat sich seit Puppet 3 nicht ge√§ndert, was zu zus√§tzlichen Konfigurationsschwierigkeiten f√ºhrt. </p><br><div class="spoiler">  <b class="spoiler_title">Puppet-Code f√ºr Anwendungen, Vorsicht</b> <div class="spoiler_text"><pre><code class="plaintext hljs">define profiles::services::monitoring::docker_apps( Hash $app_list, Hash $apps_accessible_from, Hash $apps_access_list, Hash $webhost_defaults, Hash $webcheck_defaults, Hash $service_overrides, Hash $targets, Hash $app_checks, ) { #### APPS #### $zone = $name $app_list.each | String $app_name, Hash $app_data | { $notify_group = { 'notify_group' =&gt; ($webcheck_defaults[$zone]['notify_group'] + pick($app_data['notify_group'], {} )) } # adds notifications for default group (systems) + any group defined in int/pm_docker_apps.eyaml $data = merge($webhost_defaults, $apps_accessible_from, $app_data) $site_domain = $app_data['site_domain'] $regexp = pick($app_data['check_regex'], 'html') # Pick a regex to check $check_url = $app_data['check_url'] ? { undef =&gt; { 'http_uri' =&gt; '/' }, default =&gt; { 'http_uri' =&gt; $app_data['check_url'] } } $check_regex = $regexp ?{ 'absent' =&gt; {}, default =&gt; {'http_expect_body_regex' =&gt; $regexp} } $site_domain.each | String $vhost, Hash $vdata | { # Split an app by domains if there are two or more $vhost_name = {'http_vhost' =&gt; $vhost} $vars = $data['vars'] + $vhost_name + $check_regex + $check_url $web_ipaddress = is_array($vdata['web_ipaddress']) ? { # Make IP-address an array if it's not, because askizzy has 2 ips and it's an array true =&gt; $vdata['web_ipaddress'], false =&gt; [$vdata['web_ipaddress']], } $access_from_zones = [$zone] + $apps_access_list[$data['accessible_from']] # Merge default zone (where the app is defined) and extra zones if they exist $web_ipaddress.each | String $ip_address | { # For each IP (if we have multiple) $suffix = length($web_ipaddress) ? { # If we have more than one - add IP as a suffix to this hostname to avoid duplicating resources 1 =&gt; '', default =&gt; "_${ip_address}" } $octets = split($ip_address, '\.') $ip_tag = "${octets[2]}.${octets[3]}" # Using last octet only causes a collision between nginx-vip 203.15.70.94 and ext. ip 49.255.194.94 $access_from_zones.each | $zone_prefix |{ $zone_target = $targets[$zone_prefix] $nginx_vip_name = "${zone_prefix}_nginx-vip-${ip_tag}" # If it's a host for ext - prefix becomes 'ext_' (ext_nginx-vip...) $nginx_host_vip = { $nginx_vip_name =&gt; { ensure =&gt; present, target =&gt; $zone_target, address =&gt; $ip_address, check_command =&gt; 'hostalive', groups =&gt; ['nginx_vip',], } } $ssl_vars = $app_checks['ssl'] $regex_vars = $app_checks['http'] + $vars + $webcheck_defaults[$zone] + $notify_group if !defined( Profiles::Services::Monitoring::Host[$nginx_vip_name] ) { ensure_resources('profiles::services::monitoring::host', $nginx_host_vip) } if !defined( Icinga2::Object::Service["${nginx_vip_name}_ssl"] ) { icinga2::object::service {"${nginx_vip_name}_ssl": ensure =&gt; $data['ensure'], assign =&gt; ["host.name == $nginx_vip_name",], groups =&gt; ['webchecks',], check_command =&gt; 'ssl', check_interval =&gt; $service_overrides['ssl']['check_interval'], target =&gt; $targets['services'], apply =&gt; true, vars =&gt; $ssl_vars } } if $regexp != 'absent'{ if !defined(Icinga2::Object::Service["${vhost}${$suffix} regex"]){ icinga2::object::service {"${vhost}${$suffix} regex": ensure =&gt; $data['ensure'], assign =&gt; ["match(*_nginx-vip-${ip_tag}, host.name)",], groups =&gt; ['webchecks',], check_command =&gt; 'http', check_interval =&gt; $service_overrides['regex']['check_interval'], target =&gt; $targets['services'], enable_flapping =&gt; true, apply =&gt; true, vars =&gt; $regex_vars } } } } } } } }</code> </pre> </div></div><br><p>  Der Host- und Service-Konfigurationscode sieht ebenfalls schrecklich aus: </p><br><div class="spoiler">  <b class="spoiler_title">Monitoring / config.pp</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">class profiles::services::monitoring::config( Array $default_config, Array $hostgroups, Hash $hosts = {}, Hash $host_defaults, Hash $services, Hash $service_defaults, Hash $service_overrides, Hash $webcheck_defaults, Hash $servicegroups, String $servicegroup_target, Hash $user_defaults, Hash $users, Hash $oncall, Hash $usergroup_defaults, Hash $usergroups, Hash $notifications, Hash $notification_defaults, Hash $notification_commands, Hash $timeperiods, Hash $webhost_defaults, Hash $apps_access_list, Hash $check_commands, Hash $hosts_api = {}, Hash $targets = {}, Hash $host_api_defaults = {}, ) { # Profiles::Services::Monitoring::Hostgroup &lt;&lt;| |&gt;&gt; # will be enabled when we move to icinga completely #### APPS #### case $location { 'int', 'ext': { $apps_by_zone = {} } 'pm': { $int_apps = hiera('int_docker_apps') $int_app_defaults = hiera('int_docker_app_common') $st_apps = hiera('staging_docker_apps') $srs_apps = hiera('pm_docker_apps_srs') $pm_apps = hiera('pm_docker_apps') + $st_apps + $srs_apps $pm_app_defaults = hiera('pm_docker_app_common') $apps_by_zone = { 'int' =&gt; $int_apps, 'pm' =&gt; $pm_apps, } $app_access_by_zone = { 'int' =&gt; {'accessible_from' =&gt; $int_app_defaults['accessible_from']}, 'pm' =&gt; {'accessible_from' =&gt; $pm_app_defaults['accessible_from']}, } } default: { fail('Please ensure the node has $location fact set (int, pm, ext)') } } file { '/etc/icinga2/conf.d/': ensure =&gt; directory, recurse =&gt; true, purge =&gt; true, owner =&gt; 'icinga', group =&gt; 'icinga', mode =&gt; '0750', notify =&gt; Service['icinga2'], } $default_config.each | String $file_name |{ file {"/etc/icinga2/conf.d/${file_name}": ensure =&gt; present, source =&gt; "puppet:///modules/profiles/services/monitoring/default_config/${file_name}", owner =&gt; 'icinga', group =&gt; 'icinga', mode =&gt; '0640', } } $app_checks = { 'ssl' =&gt; $services['webchecks']['checks']['ssl']['vars'], 'http' =&gt; $services['webchecks']['checks']['http_regexp']['vars'] } $apps_by_zone.each | String $zone, Hash $app_list | { profiles::services::monitoring::docker_apps{$zone: app_list =&gt; $app_list, apps_accessible_from =&gt; $app_access_by_zone[$zone], apps_access_list =&gt; $apps_access_list, webhost_defaults =&gt; $webhost_defaults, webcheck_defaults =&gt; $webcheck_defaults, service_overrides =&gt; $service_overrides, targets =&gt; $targets, app_checks =&gt; $app_checks, } } #### HOSTS #### # Profiles::Services::Monitoring::Host &lt;&lt;| |&gt;&gt; # This is for spaceship invasion when it's ready. $hosts_has_large_disks = query_nodes('mountpoints.*.size_bytes &gt;= 1099511627776') $hosts.each | String $hostgroup, Hash $list_of_hosts_with_settings | { # Splitting site lists by hostgroups - docker_host/gluster_host/etc $list_of_hosts_in_group = $list_of_hosts_with_settings['hosts'] $hostgroup_settings = $list_of_hosts_with_settings['settings'] $merged_hostgroup_settings = deep_merge($host_defaults, $list_of_hosts_with_settings['settings']) $list_of_hosts_in_group.each | String $host_name, Hash $host_settings |{ # Splitting grouplists by hosts # Is this host in the array $hosts_has_large_disks ? If so set host.vars.has_large_disks if ( $hosts_has_large_disks.reduce(false) | $found, $value| { ( $value =~ "^${host_name}" ) or $found } ) { $vars_has_large_disks = { 'has_large_disks' =&gt; true } } else { $vars_has_large_disks = {} } $host_data = deep_merge($merged_hostgroup_settings, $host_settings) $hostgroup_settings_vars = pick($hostgroup_settings['vars'], {}) $host_settings_vars = pick($host_settings['vars'], {}) $host_notify_group = delete_undef_values($host_defaults['vars']['notify_group'] + $hostgroup_settings_vars['notify_group'] + $host_settings_vars['notify_group']) $host_data_vars = delete_undef_values(deep_merge($host_data['vars'] , {'notify_group' =&gt; $host_notify_group}, $vars_has_large_disks)) # Merging vars separately $hostgroups = delete_undef_values([$hostgroup] + $host_data['groups']) profiles::services::monitoring::host{$host_name: ensure =&gt; $host_data['ensure'], display_name =&gt; $host_data['display_name'], address =&gt; $host_data['address'], groups =&gt; $hostgroups, target =&gt; $host_data['target'], check_command =&gt; $host_data['check_command'], check_interval =&gt; $host_data['check_interval'], max_check_attempts =&gt; $host_data['max_check_attempts'], vars =&gt; $host_data_vars, template =&gt; $host_data['template'], } } } if !empty($hosts_api){ # All hosts managed by API $hosts_api.each | String $zone, Hash $hosts_api_zone | { # Split api hosts by zones $hosts_api_zone.each | String $hostgroup, Hash $list_of_hosts_with_settings | { # Splitting site lists by hostgroups - docker_host/gluster_host/etc $list_of_hosts_in_group = $list_of_hosts_with_settings['hosts'] $hostgroup_settings = $list_of_hosts_with_settings['settings'] $merged_hostgroup_settings = deep_merge($host_api_defaults, $list_of_hosts_with_settings['settings']) $list_of_hosts_in_group.each | String $host_name, Hash $host_settings |{ # Splitting grouplists by hosts # Is this host in the array $hosts_has_large_disks ? If so set host.vars.has_large_disks if ( $hosts_has_large_disks.reduce(false) | $found, $value| { ( $value =~ "^${host_name}" ) or $found } ) { $vars_has_large_disks = { 'has_large_disks' =&gt; true } } else { $vars_has_large_disks = {} } $host_data = deep_merge($merged_hostgroup_settings, $host_settings) $hostgroup_settings_vars = pick($hostgroup_settings['vars'], {}) $host_settings_vars = pick($host_settings['vars'], {}) $host_api_notify_group = delete_undef_values($host_defaults['vars']['notify_group'] + $hostgroup_settings_vars['notify_group'] + $host_settings_vars['notify_group']) $host_data_vars = delete_undef_values(deep_merge($host_data['vars'] , {'notify_group' =&gt; $host_api_notify_group}, $vars_has_large_disks)) $hostgroups = delete_undef_values([$hostgroup] + $host_data['groups']) if defined(Profiles::Services::Monitoring::Host[$host_name]){ $hostname = "${host_name}_from_${zone}" } else { $hostname = $host_name } profiles::services::monitoring::host{$hostname: ensure =&gt; $host_data['ensure'], display_name =&gt; $host_data['display_name'], address =&gt; $host_data['address'], groups =&gt; $hostgroups, target =&gt; "${host_data['target_base']}/${zone}/hosts.conf", check_command =&gt; $host_data['check_command'], check_interval =&gt; $host_data['check_interval'], max_check_attempts =&gt; $host_data['max_check_attempts'], vars =&gt; $host_data_vars, template =&gt; $host_data['template'], } } } } } #### END OF HOSTS #### #### SERVICES #### $services.each | String $service_group, Hash $s_list |{ # Service_group and list of services in that group $service_list = $s_list['checks'] # List of actual checks, separately from SG settings $service_list.each | String $service_name, Hash $data |{ $merged_defaults = merge($service_defaults, $s_list['settings']) # global service defaults + service group defaults $merged_data = merge($merged_defaults, $data) $settings_vars = pick($s_list['settings']['vars'], {}) $this_service_vars = pick($data['vars'], {}) $all_service_vars = delete_undef_values($service_defaults['vars'] + $settings_vars + $this_service_vars) # If we override default check_timeout, but not nrpe_timeout, make nrpe_timeout the same as check_timeout if ( $merged_data['check_timeout'] and ! $this_service_vars['nrpe_timeout'] ) { # NB: Icinga will convert 1m to 60 automatically! $nrpe = { 'nrpe_timeout' =&gt; $merged_data['check_timeout'] } } else { $nrpe = {} } # By default we use nrpe and all commands are run via nrpe. So vars.nrpe_command = $service_name is a default value # If it's server-side Icinga command - we don't need 'nrpe_command' # but there is no harm to have that var and the code is shorter if $merged_data['check_command'] == 'nrpe'{ $check_command = $merged_data['vars']['nrpe_command'] ? { undef =&gt; { 'nrpe_command' =&gt; $service_name }, default =&gt; { 'nrpe_command' =&gt; $merged_data['vars']['nrpe_command'] } } }else{ $check_command = {} } # Assembling $vars from Global Default service settings, servicegroup settings, this particular check settings and let's not forget nrpe settings. if $all_service_vars['graphite_template'] { $graphite_template = {'check_command' =&gt; $all_service_vars['graphite_template']} }else{ $graphite_template = {'check_command' =&gt; $service_name} } $service_notify = [] + pick($settings_vars['notify_group'], []) + pick($this_service_vars['notify_group'], []) # pick is required everywhere, otherwise becomes "The value '' cannot be converted to Numeric" $service_notify_group = $service_notify ? { [] =&gt; $service_defaults['vars']['notify_group'], default =&gt; $service_notify } # Assing default group (systems) if no other groups are defined $vars = $all_service_vars + $nrpe + $check_command + $graphite_template + {'notify_group' =&gt; $service_notify_group} # This needs to be merged separately, because merging it as part of MERGED_DATA overwrites arrays instead of merging them, so we lose some "assign" and "ignore" values $assign = delete_undef_values($service_defaults['assign'] + $s_list['settings']['assign'] + $data['assign']) $ignore = delete_undef_values($service_defaults['ignore'] + $s_list['settings']['ignore'] + $data['ignore']) icinga2::object::service {$service_name: ensure =&gt; $merged_data['ensure'], apply =&gt; $merged_data['apply'], enable_flapping =&gt; $merged_data['enable_flapping'], assign =&gt; $assign, ignore =&gt; $ignore, groups =&gt; [$service_group], check_command =&gt; $merged_data['check_command'], check_interval =&gt; $merged_data['check_interval'], check_timeout =&gt; $merged_data['check_timeout'], check_period =&gt; $merged_data['check_period'], display_name =&gt; $merged_data['display_name'], event_command =&gt; $merged_data['event_command'], retry_interval =&gt; $merged_data['retry_interval'], max_check_attempts =&gt; $merged_data['max_check_attempts'], target =&gt; $merged_data['target'], vars =&gt; $vars, template =&gt; $merged_data['template'], } } } #### END OF SERVICES #### #### OTHER BORING STUFF #### $servicegroups.each | $servicegroup, $description |{ icinga2::object::servicegroup{ $servicegroup: target =&gt; $servicegroup_target, display_name =&gt; $description } } $hostgroups.each| String $hostgroup |{ profiles::services::monitoring::hostgroup { $hostgroup:} } $notifications.each | String $name, Hash $settings |{ $assign = pick($notification_defaults['assign'], []) + $settings['assign'] $ignore = pick($notification_defaults['ignore'], []) + $settings['ignore'] $merged_settings = $settings + $notification_defaults icinga2::object::notification{$name: target =&gt; $merged_settings['target'], apply =&gt; $merged_settings['apply'], apply_target =&gt; $merged_settings['apply_target'], command =&gt; $merged_settings['command'], interval =&gt; $merged_settings['interval'], states =&gt; $merged_settings['states'], types =&gt; $merged_settings['types'], assign =&gt; delete_undef_values($assign), ignore =&gt; delete_undef_values($ignore), user_groups =&gt; $merged_settings['user_groups'], period =&gt; $merged_settings['period'], vars =&gt; $merged_settings['vars'], } } # Merging notification settings for users with other settings $users_oncall = deep_merge($users, $oncall) # Magic. Do not touch. create_resources('icinga2::object::user', $users_oncall, $user_defaults) create_resources('icinga2::object::usergroup', $usergroups, $usergroup_defaults) create_resources('icinga2::object::timeperiod',$timeperiods) create_resources('icinga2::object::checkcommand', $check_commands) create_resources('icinga2::object::notificationcommand', $notification_commands) profiles::services::sudoers { 'icinga_runs_ping_l2': ensure =&gt; present, sudoersd_template =&gt; 'profiles/os/redhat/centos7/sudoers/icinga.erb', } }</code> </pre> </div></div><br><p>  Ich arbeite immer noch an dieser Nudel und verbessere sie, wann immer es m√∂glich ist.  Es war jedoch dieser Code, der es erm√∂glichte, eine einfache und klare Syntax in Hiera zu verwenden: </p><br><div class="spoiler">  <b class="spoiler_title">Daten</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">profiles::services::monitoring::config::services: perf_checks: settings: check_interval: '2m' assign: - 'host.vars.type == linux' checks: procs: {} load: {} memory: {} disk: check_interval: '5m' vars: notification_period: '24x7' disk_iops: vars: notifications: - 'silent' cpu: vars: notifications: - 'silent' dns_fqdn: check_interval: '15m' ignore: - 'xenserver in host.groups' vars: notifications: - 'silent' iftraffic_nrpe: vars: notifications: - 'silent' logging: settings: assign: - 'logserver in host.groups' checks: rsyslog: {} nginx_limit_req_other: {} nginx_limit_req_s2s: {} nginx_limit_req_s2x: {} nginx_limit_req_srs: {} logstash: {} logstash_api: vars: notifications: - 'silent'</code> </pre> </div></div><br><p>  Alle √úberpr√ºfungen sind in Gruppen unterteilt. Jede Gruppe verf√ºgt √ºber Standardeinstellungen, z. B. wo und wie oft diese √úberpr√ºfungen ausgef√ºhrt werden sollen, welche Benachrichtigungen an wen gesendet werden sollen. </p><br><p>  Bei jeder Pr√ºfung k√∂nnen Sie jede Option √ºberschreiben, und all dies summiert sich schlie√ülich zu den Standardeinstellungen aller Pr√ºfungen als Ganzes.  Daher werden solche Nudeln in config.pp geschrieben - alle Standardeinstellungen werden mit den Einstellungen der Gruppen und dann mit jeder einzelnen Pr√ºfung zusammengef√ºhrt. </p><br><p>  Eine sehr wichtige √Ñnderung war auch die M√∂glichkeit, Funktionen in den Einstellungen zu verwenden, z. B. die Funktion zum √Ñndern des Ports, der Adresse und der URL, um http_regex zu √ºberpr√ºfen. </p><br><pre> <code class="plaintext hljs">http_regexp: assign: - 'host.vars.http_regex' - 'static_sites in host.groups' check_command: 'http' check_interval: '1m' retry_interval: '20s' max_check_attempts: 6 http_port: '{{ if(host.vars.http_port) { return host.vars.http_port } else { return 443 } }}' vars: notification_period: 'host.vars.notification_period' http_vhost: '{{ if(host.vars.http_vhost) { return host.vars.http_vhost } else { return host.name } }}' http_ssl: '{{ if(host.vars.http_ssl) { return false } else { return true } }}' http_expect_body_regex: 'host.vars.http_regex' http_uri: '{{ if(host.vars.http_uri) { return host.vars.http_uri } else { return "/" } }}' http_onredirect: 'follow' http_warn_time: 8 http_critical_time: 15 http_timeout: 30 http_sni: true</code> </pre> <br><p>  Dies bedeutet, dass - wenn die <em>Hostdefinition</em> eine <em>http_port-</em> Variable enth√§lt - diese verwendet wird, andernfalls 443. Die Jabber-Weboberfl√§che h√§ngt beispielsweise an 9090 und Unifi an 7443. <br>  <em>http_vhost</em> bedeutet, DNS zu ignorieren und diese Adresse zu √ºbernehmen. <br>  Wenn uri im Host angegeben ist, gehen Sie mit, andernfalls nehmen Sie "/". </p><br><p>  Mit http_ssl kam eine lustige Geschichte heraus - diese Infektion wollte nicht bei Bedarf die Verbindung trennen.  Ich bin lange Zeit dumm √ºber diese Zeile gestolpert, bis mir klar wurde, dass die Variable in der Host-Definition: </p><br><pre> <code class="plaintext hljs">http_ssl: false</code> </pre> <br><p>  Ersatz in Ausdruck </p><br><pre> <code class="plaintext hljs">if(host.vars.http_ssl) { return false } else { return true }</code> </pre> <br><p>  als <strong>falsch</strong> und am Ende stellt sich heraus </p><br><pre> <code class="plaintext hljs">if(false) { return false } else { return true }</code> </pre> <br><p>  Das hei√üt, die SSL-Pr√ºfung ist immer aktiv.  Es wurde durch Ersetzen der Syntax entschieden: </p><br><pre> <code class="plaintext hljs">http_ssl: no</code> </pre> <br><p>  <strong>Schlussfolgerungen</strong> : </p><br><p>  Vorteile: </p><br><ul><li>  Wir haben jetzt ein √úberwachungssystem und nicht zwei, wie in den letzten 7 bis 8 Monaten, oder eines, das veraltet und anf√§llig ist. </li><li>  Die Datenstruktur von Hosts / Services (Checks) ist jetzt (meiner Meinung nach) viel lesbarer und verst√§ndlicher.  F√ºr andere war dies nicht so offensichtlich, deshalb musste ich ein paar Seiten im lokalen Wiki schneiden, um zu erkl√§ren, wie es funktioniert und wo es bearbeitet werden kann. </li><li>  Es ist m√∂glich, √úberpr√ºfungen mithilfe von Variablen und Funktionen flexibel zu konfigurieren, z. B. um http_regexp zu √ºberpr√ºfen. Das gew√ºnschte Muster, der R√ºckkehrcode, die URL und der Port k√∂nnen in den Hosteinstellungen festgelegt werden. </li><li>  Es gibt mehrere Dashboards, f√ºr die Sie jeweils eine eigene Liste der angezeigten Alarme definieren und all dies √ºber Puppet- und Merge-Anforderungen verwalten k√∂nnen. </li></ul><br><p>  Nachteile: </p><br><ul><li>  Tr√§gheit der Teammitglieder - Nagios hat gearbeitet, gearbeitet und gearbeitet, und dies ist eine st√§ndige St√∂rung und Verlangsamung Ihrer Isinga.  Und wie kann ich die Geschichte sehen?  Und verdammt noch mal, es wird nicht aktualisiert ... (Das eigentliche Problem ist, dass der Alarmverlauf nicht automatisch aktualisiert wird, nur durch F5) </li><li>  Die Tr√§gheit des Systems - wenn ich in der Weboberfl√§che auf "Jetzt pr√ºfen" klicke - h√§ngt das Ergebnis der Ausf√ºhrung vom Wetter auf dem Mars ab, insbesondere von komplexen Diensten, deren Ausf√ºhrung mehrere zehn Sekunden dauert.  Ein √§hnliches Ergebnis ist normal. <img src="https://habrastorage.org/webt/ue/73/wa/ue73wa4yt4bhedebd1n66kizsf8.jpeg"></li><li>  Im Allgemeinen arbeitete Nagios laut der Halbjahresstatistik der beiden nebeneinander arbeitenden Systeme immer schneller als Icinga, was mich sehr √§rgerte.  Es scheint mir, dass es etwas gibt, das mit Timern get√§uscht wird, und eine f√ºnfmin√ºtige √úberpr√ºfung der Tatsache findet alle 5:30 oder so etwas statt. </li><li>  Wenn Sie den Dienst jederzeit neu starten (systemctl restart icinga2), l√∂sen alle zu diesem Zeitpunkt laufenden √úberpr√ºfungen einen kritischen Alarm &lt;beendet durch Signal 15&gt; auf dem Bildschirm aus und von der Seite sieht es so aus, als w√§re alles gefallen ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">best√§tigter Fehler)</a> ) </li></ul><br><p>  Aber im Allgemeinen - es funktioniert. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de444060/">https://habr.com/ru/post/de444060/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de444048/index.html">Tipps und Tricks zur digitalen Forensik: Erkennen von √Ñnderungen durch Gruppenrichtlinien, die von Eindringlingen verursacht werden</a></li>
<li><a href="../de444050/index.html">Diskussion: Wird die DNA-Speicherung massiv?</a></li>
<li><a href="../de444052/index.html">Wie wir bei IntelliJ IDEA nach Lambda-Ausdr√ºcken suchen</a></li>
<li><a href="../de444056/index.html">Internetprovider auf der Krim haben die Preise f√ºr Dienstleistungen stark erh√∂ht</a></li>
<li><a href="../de444058/index.html">Wenn Kinder verstehen, dass ihr ganzes Leben bereits online ist</a></li>
<li><a href="../de444062/index.html">Licht auf! Nachttransformationen des Lakhta Centers</a></li>
<li><a href="../de444064/index.html">Neue Ideen f√ºr eine neue Zukunft</a></li>
<li><a href="../de444068/index.html">Wer schaut zu?</a></li>
<li><a href="../de444070/index.html">Entwicklung eines Hexapods von Grund auf neu (Teil 4) - mathematische Trajektorien und Sequenzen</a></li>
<li><a href="../de444072/index.html">Android Shopping - Abrechnungsbibliothek abspielen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>