<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏻‍🍳 👩🏾‍🤝‍👩🏼 🦎 Kubernetes集群每月20美元 🍩 🤚🏾 🤰🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="TL 博士 


 我们提高了集群，以使用ingress ， letsencrypt服务无状态 Web应用程序，而无需使用kubespray，kubeadm等自动化工具。 
 阅读时间：〜45-60分钟，播放时间：3小时起。 
 前言 


 我需要自己的Kubernetes集群进行实验，提示我写一...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Kubernetes集群每月20美元</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/iponweb/blog/435228/"><h1 id="tl-dr">  TL 博士 </h1><br><p> 我们提高了集群，以使用<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">ingress</a> ， <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">letsencrypt</a>服务<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">无状态</a> Web应用程序，而无需使用kubespray，kubeadm等自动化工具。 <br> 阅读时间：〜45-60分钟，播放时间：3小时起。 </p><br><h1 id="preambula"> 前言 </h1><br><p> 我需要自己的Kubernetes集群进行实验，提示我写一篇文章。 在我的情况下，开源自动安装和配置解决方案不起作用，因为我使用了非主流Linux发行版。 在IPONWEB中与kubernetes进行深入的合作会鼓励您拥有这样一个平台，以舒适的方式解决您的任务，包括用于家庭项目。 </p><br><h1 id="komponenty"> 组成部分 </h1><br><p> 以下组件将出现在文章中： </p><br><p>  - <em>您最喜欢的</em> Linux-我使用了Gentoo（node-1：systemd / node-2：openrc），Ubuntu 18.04.1。 <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">-Kubernetes服务器</a> -kube-apiserver，kube-controller-manager，kube-scheduler，kubelet，kube-proxy <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">容器</a> + <a href="">CNI插件（0.7.4）</a> -为了进行容器化，我们将使用容器+ CNI代替docker（尽管最初整个配置<a href="">都已</a>上传到docker，所以在必要时不会阻止任何使用）。 <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">-CoreDNS-</a>用于组织在kubernetes集群中工作的组件的服务发现。 推荐使用不低于1.2.5的版本，因为此版本对coredns起到了健全的支持，使其可以在集群外部运行。 <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">法兰绒</a> -用于组织网络堆栈，在彼此之间通信炉床和容器。 <br>  - <em>您最喜欢的</em>数据库。 </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/423/a71/5fc/423a715fc68fed1101c86d3335b0a8a8.jpg" alt="对于所有"></p><a name="habracut"></a><br><h1 id="ogranicheniya-i-dopuscheniya"> 局限性和假设 </h1><br><ul><li> 本文不讨论市场上vps / vds解决方案的成本，也不考虑在这些服务上部署计算机的可能性。 假定您已经进行了扩展，或者您可以自己进行扩展。 此外，如果需要，还不会涵盖您喜欢的数据库和私有Docker存储库的安装/配置。 </li><li> 我们可以同时使用容器+ cni插件和docker。 本文不考虑将Docker用作容器化工具。 如果要使用docker，您自己可以相应地配置<a href="">法兰绒</a> ，此外，您还需要配置kubelet，即删除与containerd相关的所有选项。 如我的实验所示，将docker和container放置在不同的节点上，因为容器可以正常工作。 </li><li>我们无法将<code>host-gw</code>后端用于法兰绒，请阅读“ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">法兰绒配置”</a>部分以获取更多详细信息 </li><li> 我们将不会使用任何东西来监视，备份，保存用户文件（状态），存储配置文件和应用程序代码（git / hg / svn /等） </li></ul><br><h1 id="vvedenie"> 引言 </h1><br><p> 在工作过程中，我使用了大量资源，但我想分别提及一个相当详尽的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes困难方法</a>指南，该指南涵盖了其集群基本配置的90％。 如果您已经阅读了本手册，则可以安全地直接进入“ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Flannel Configuration”</a>部分。 </p><br><div class="spoiler">  <b class="spoiler_title">名称</b> <div class="spoiler_text"><h2 id="spisok-terminov--glossariy"> 术语/词汇表 </h2><br><ul><li>  api-server-物理或虚拟机，用于运行和正确运行kubernetes kube-apiserver的一组应用程序位于其中。 出于本文的目的，它是etcd，kube-apiserver，kube-controller-manager，kube-scheduler。 </li><li>  master-专用工作站或VPS安装，是api-server的同义词。 </li><li>  node-X-专用工作站或VPS安装， <code>X</code>表示工作站的序列号。 在本文中，所有数字都是唯一的，是理解的关键： <br><ul><li> 节点1-机器编号1 </li><li> 节点2-机器编号2 </li></ul></li><li>  vCPU-虚拟CPU，处理器核心。 该数目与内核数相对应：1vCPU-一个内核，2vCPU-两个，依此类推。 </li><li> 用户-用户或用户空间。 在命令行指令中使用<code>user$</code>时，该术语指的是任何客户端计算机。 </li><li>  worker-将在其上执行直接计算的工作节点，与<code>node-X</code> </li><li> 资源是Kubernetes集群所基于的实体。  Kubernetes资源包括大量<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">相关实体</a> 。 </li></ul></div></div><br><h1 id="setevaya-arhitektura-resheniya"> 网络架构解决方案 </h1><br><p> 在提高集群的过程中，我没有将优化铁资源的任务设置为适合每月20美元的预算。 只需要组装一个至少包含两个工作节点（节点）的工作集群。 因此，群集最初看起来像这样： </p><br><ul><li> 具有2个vCPU / 4G RAM的计算机：api服务器+节点1 [20 $] </li><li> 带2个vCPU / 4G RAM的计算机：node-2 [$ 20] </li></ul><br><p> 在集群的第一个版本工作之后，我决定对其进行重建，以便区分负责在集群中运行应用程序的节点（工作节点，它们也是工作节点）和主服务器的API。 </p><br><p> 结果，我得到了以下问题的答案：“如果我不想在其中放置最厚的应用程序，那么如何获得一个或多或少便宜但运行良好的群集。” </p><br><div class="spoiler">  <b class="spoiler_title">$ 20决定</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/getpro/habr/post_images/216/89c/a88/21689ca889156d11108e5f5327c606cc.png" alt="设计方案"><br>  （计划是这样） </p></div></div><br><div class="spoiler">  <b class="spoiler_title">Kubernetes Architecture一般信息</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/getpro/habr/post_images/820/462/882/820462882e7cc92479190c067ac4a4f8.jpg" alt="设计方案"><br>  （如果某人突然仍然不知道或没有看到，则从互联网上窃取） </p></div></div><br><h2 id="komponenty-i-ih-proizvoditelnost"> 组件及其性能 </h2><br><p> 第一步是了解我需要多少资源才能运行与群集直接相关的软件包。 搜索“硬件需求”并没有给出具体结果，因此我不得不从实际的角度来解决这个任务。 作为对MEM和CPU的度量，我从systemd那里获得了统计数据-我们可以假设测量是以非常业余的方式进行的，但是我没有获得准确值的任务，因为我仍然找不到每实例5美元以下的更便宜的选择。 </p><br><div class="spoiler">  <b class="spoiler_title">为什么要5美元呢？</b> <div class="spoiler_text"><p> 在俄罗斯或独联体国家/地区托管服务器时，可能会发现VPS / VDS便宜，但与ILV及其行动有关的悲惨故事带来了一定的风险，并自然而然地希望避免这种情况。 </p></div></div><br><p> 因此： </p><br><ul><li> 主服务器/服务器配置（主节点）： <br><ul><li>  etcd（3.2.17）：80-100M，指标是在随机选择的时间获得的。  Etcd平均内存消耗不超过300M； </li><li>  kube-apiserver（1.12.x-1.13.0）：237.6M〜300M; </li><li>  kube-controller-manager（1.12.x-1.13.0）：大约9000万，没有超过100M； </li><li>  kube-scheduler（1.12.x-1.13.0）：大约20M，超过30-50M的消耗量是固定的。 </li></ul></li><li> 工作服务器配置（工作节点）： <br><ul><li>  kubelet（1.12.3-1.13.1）：大约35 Mb，50M以上的消耗量不是固定的； </li><li>  kube-proxy（1.12.3-1.13.1）：大约7.5-10M； </li><li> 法兰绒（0.10.0）：大约15-20M； </li><li>  coredns（1.3.0）：约2500万； </li><li>  containerd（1.2.1）：容器使用率很低，但统计数据还显示守护程序启动的容器进程。 </li></ul></li></ul><br><div class="spoiler">  <b class="spoiler_title">主节点上是否需要容器/泊坞窗？</b> <div class="spoiler_text"><p>  <strong>不，不需要</strong> 。 主节点不需要docker或容器本身，尽管Internet上有大量手册出于某些目的或其他目的包括使用环境进行容器化。 在有问题的配置中，有意从依赖项列表中关闭了containerd，但是，我没有强调这种方法的任何明显优势。 </p><br><p> 上面提供的配置很小，足以启动集群。 除非您要添加任何内容，否则不需要其他操作/组件。 </p></div></div><br><p> 要构建用于家庭项目的测试群集或群集，主节点可以正常运行1vCPU / 1G RAM。 当然，主节点上的负载将根据所涉及的工作程序数量以及向api服务器发送的第三方请求的可用性和数量而有所不同。 </p><br><p> 我按如下所示分解了主配置和工作配置： </p><br><ul><li>  1个具有已安装组件的Master：etcd，kube-apiserver，kube-controller-manager，kube-scheduler </li><li>  2x具有已安装组件的工作者：容器，coredns，法兰绒，kubelet，kube-proxy </li></ul><br><h1 id="konfiguraciya"> 构型 </h1><br><p> 要配置向导，需要以下组件： </p><br><ul><li><p>  etcd-用于存储api服务器以及法兰绒的数据； </p><br></li><li><p>  kube-apiserver-实际上是api-server; </p><br></li><li><p>  kube-controller-manager-用于生成和处理事件； </p><br></li><li><p>  kube-scheduler-用于分配通过api服务器注册的资源-例如， <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">hearth</a> 。 <br> 对于主力配置，需要以下组件： </p><br></li><li><p>  kubelet-运行炉膛，配置网络设置； </p><br></li><li><p>  kube-proxy-用于组织kubernetes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">服务的</a>路由/平衡; </p><br></li><li><p>  coredns-用于在正在运行的容器中发现服务； </p><br></li><li><p> 法兰绒-用于组织在不同节点上运行的容器的网络访问，以及用于在群集节点（kubernetes节点）之间动态分配网络。 </p><br></li></ul><br><div class="spoiler">  <b class="spoiler_title">核心域名</b> <div class="spoiler_text"><p> 这里应该做个小题外话：coredns也可以在主服务器上启动。 除了coredns.service配置的细微差别外，没有其他限制会迫使coredns在工作节点上运行，该配置细微差别是由于与systemd-resolved服务冲突而根本无法在标准/未修改的Ubuntu服务器上启动。 我没有尝试解决此问题，因为位于工作节点上的2 ns服务器对我很满意。 </p></div></div><br><p> 为了不浪费时间现在熟悉组件配置过程的所有细节，我建议您在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes困难方法</a>指南中熟悉它们。 我将重点介绍配置选项的独特功能。 </p><br><h2 id="fayly"> 档案 </h2><br><p> 为了方便起见，所有用于向导和工作节点的集群组件运行的文件都放在<strong>/ var / lib / kubernetes /</strong>中。 如有必要，可以用其他方式放置它们。 </p><br><h2 id="sertifikaty"> 资质认证 </h2><br><p> 生成证书的基础仍然是<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes的艰辛方式</a> ，实际上没有明显差异。 为了重新生成从属证书，围绕<a href="">cfssl</a>应用程序编写了简单的bash脚本-这在调试过程中非常有用。 </p><br><p> 您可以使用下面的脚本， <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes的硬方法</a>或其他合适的工具来生成满足您需求的证书。 </p><br><div class="spoiler">  <b class="spoiler_title">使用bash脚本生成证书</b> <div class="spoiler_text"><p> 您可以在此处<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">获取</a>脚本： <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">kubernetes bootstrap</a> 。 开始之前，编辑文件<a href="">certs / env.sh</a> ，指定设置。 一个例子： </p><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> certs <span class="hljs-comment"><span class="hljs-comment">#:   certs$ ./generate-keys.sh # ... certificate generate output #:  kubeconfig     certs$ ./generate-configkube.sh</span></span></code> </pre> <br><p> 如果您使用<code>env.sh</code>并正确指定了所有参数，则无需触摸生成的证书。 如果您在某个时候犯了一个错误，那么可以分部分地重新生成证书。 上面的bash脚本很简单，将它们整理出来并不困难。 </p><br><p> 重要说明-您不应该经常重新创建<code>ca.pem</code>和<code>ca-key.pem</code>证书，因为它们是所有后续证书的根证书，换句话说，您将必须重新创建所有附带的证书，并将它们交付给所有计算机和所有必要的目录。 </p></div></div><br><h3 id="master"> 大师 </h3><br><p> 在主节点上启动服务所需的证书应放在<code>/var/lib/kubernetes/</code> ： </p><br><ul><li>  ca.pem-该证书在任何地方都可以使用，只能生成一次，然后使用且无需更改，因此请当心。 重新生成它时，您将需要将其复制到所有节点，并使用它更新kubeconfig文件（也在所有计算机上）。 </li><li>  ca-key.pem与在节点上复制相同。 </li><li>  kube-controller-manager.pem-仅用于kube-controller-manager。 </li><li>  kube-controller-manager-key.pem-仅用于kube-controller-manager。 </li><li><p>  kubernetes.pem-绒毛，连接到etcd的coredns，kube-apiserver时必需。 </p><br><div class="spoiler">  <b class="spoiler_title">理论撤退</b> <div class="spoiler_text"><p> 该功能基于<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes的硬方式</a>配置逻辑。 <br> 基于此，在向导和工作节点上的任何地方都将需要此文件。 我没有更改原始手册提供的方法，因为有了它的帮助，可以更快，更清晰地组织集群操作并理解整个依赖关系。 </p><br><p> 我个人的观点是，对于etcd，您需要单独的证书，该证书不能与用于kubernetes的证书重叠。 </p><br></div></div><br></li></ul><br><ul><li>  kubernetes-key.pem-保留在主服务器上。 </li><li>  service-account.pem-仅kube-controller-manager守护程序需要。 </li><li>  service-account-key.pem-类似。 </li></ul><br><h3 id="rabochie-uzly"> 工作单位 </h3><br><ul><li>  ca.pem-工作节点上涉及的所有服务（kubelet，kube-proxy）以及法兰绒，coredns所需的。 其中，使用kubectl生成时，其内容包含在kubeconfig文件中。 </li><li>  kubernetes-key.pem-仅用于法兰绒和coredns连接到位于api主节点上的etcd。 </li><li>  kubernetes.pem-与前一个类似，仅法兰绒和coredns才需要。 </li><li>  kubelet / node-1.pem-授权节点1的密钥。 </li><li>  kubelet / node-1-key.pem-授权节点1的密钥。 </li></ul><br><p>  <strong>重要！</strong> 如果您有多个节点，则每个节点将在kubelet中包含<code>node-X-key.pem</code> ， <code>node-X.pem</code>和<code>node-X.kubeconfig</code>文件。 </p><br><div class="spoiler">  <b class="spoiler_title">证书调试</b> <div class="spoiler_text"><h4 id="otladka-sertifikatov"> 证书调试 </h4><br><p> 有时您可能需要查看证书的配置方式，以找出用于生成证书的IP / DNS主机。  <code>cfssl-certinfo -cert &lt;cert&gt;</code>命令将帮助我们解决此问题。 例如，我们了解有关<code>node-1.pem</code> ： </p><br><pre> <code class="bash hljs">$ cfssl-certinfo -cert node-1.pem</code> </pre> <br><pre> <code class="json hljs">{ <span class="hljs-attr"><span class="hljs-attr">"subject"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"common_name"</span></span>: <span class="hljs-string"><span class="hljs-string">"system:node:node-1"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"country"</span></span>: <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organization"</span></span>: <span class="hljs-string"><span class="hljs-string">"system:nodes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organizational_unit"</span></span>: <span class="hljs-string"><span class="hljs-string">"Infrastructure Unit"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"locality"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"province"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"names"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"system:nodes"</span></span>, <span class="hljs-string"><span class="hljs-string">"Infrastructure Unit"</span></span>, <span class="hljs-string"><span class="hljs-string">"system:node:node-1"</span></span> ] }, <span class="hljs-attr"><span class="hljs-attr">"issuer"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"common_name"</span></span>: <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"country"</span></span>: <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organization"</span></span>: <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organizational_unit"</span></span>: <span class="hljs-string"><span class="hljs-string">"Infrastructure"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"locality"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"province"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"names"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-string"><span class="hljs-string">"Infrastructure"</span></span>, <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span> ] }, <span class="hljs-attr"><span class="hljs-attr">"serial_number"</span></span>: <span class="hljs-string"><span class="hljs-string">"161113741562559533299282037709313751074033027073"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"sans"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"w40k.net"</span></span>, <span class="hljs-string"><span class="hljs-string">"node-1"</span></span>, <span class="hljs-string"><span class="hljs-string">"178.79.168.130"</span></span>, <span class="hljs-string"><span class="hljs-string">"192.168.164.230"</span></span> ], <span class="hljs-attr"><span class="hljs-attr">"not_before"</span></span>: <span class="hljs-string"><span class="hljs-string">"2019-01-04T14:24:00Z"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"not_after"</span></span>: <span class="hljs-string"><span class="hljs-string">"2029-01-01T14:24:00Z"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"sigalg"</span></span>: <span class="hljs-string"><span class="hljs-string">"SHA256WithRSA"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"authority_key_id"</span></span>: <span class="hljs-string"><span class="hljs-string">"6:C8:94:67:59:55:19:82:AD:ED:6D:50:F1:89:B:8D:46:78:FD:9A"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"subject_key_id"</span></span>: <span class="hljs-string"><span class="hljs-string">"A1:5E:B3:3C:45:14:3D:C6:C:A:97:82:1:D5:2B:75:1A:A6:9D:B0"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"pem"</span></span>: <span class="hljs-string"><span class="hljs-string">"&lt;pem content&gt;"</span></span> }</code> </pre> </div></div><br><p>  kubelet和kube-proxy的所有其他证书直接嵌入到相应的kubeconfig中。 </p><br><h2 id="kubeconfig">  kubeconfig </h2><br><p> 所有必需的kubeconfig都可以使用<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes艰难地完成</a> ，但是，这里有些区别开始了。 该手册使用<code>kubedns</code>和<code>cni bridge</code>配置，还涵盖了<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">coredns</a>和<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">法兰绒</a> 。 这两个服务依次使用<code>kubeconfig</code>到集群。 </p><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> certs <span class="hljs-comment"><span class="hljs-comment">#:  kubeconfig     certs$ ./generate-configkube.sh</span></span></code> </pre> <br><h3 id="master-1"> 大师 </h3><br><p> 对于该向导，需要以下kubeconfig文件（如上所述，生成后可以在<code>certs/kubeconfig</code> ）： </p><br><pre> <code class="plaintext hljs">master /var/lib/kubernetes/$ tree -L 2 . +-- kube-controller-manager.kubeconfig L-- kube-scheduler  L-- kube-scheduler.kubeconfig</code> </pre> <br><p> 这些文件是运行每个服务组件所必需的。 </p><br><h3 id="rabochie-uzly-1"> 工作单位 </h3><br><p> 对于工作节点，需要以下kubeconfig文件： </p><br><pre> <code class="plaintext hljs">node-1 /var/lib/kubernetes/$ tree -L 2 . +-- coredns ¦  L-- coredns.kubeconfig +-- flanneld ¦  L-- flanneld.kubeconfig +-- kubelet ¦  L-- node-1.kubeconfig L-- kube-proxy  L-- kube-proxy.kubeconfig</code> </pre> <br><h2 id="zapusk-servisov"> 服务启动 </h2><br><div class="spoiler">  <b class="spoiler_title">服务项目</b> <div class="spoiler_text"><p> 尽管我的工作节点使用不同的初始化系统，但是示例和存储库使用systemd提供了选项。 在他们的帮助下，最容易理解您需要启动哪个过程和哪个参数，此外，在研究带有目标标志的服务时，它们不会引起大问题。 </p></div></div><br><p> 要启动服务，您需要将<code>service-name.service</code>复制到<code>/lib/systemd/system/</code>或systemd服务所在的任何其他目录，然后打开并启动该服务。  kube-apiserver的示例： </p><br><pre> <code class="bash hljs">$ systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> kube-apiserver.service $ systemctl start kube-apiserver.service</code> </pre> <br><p> 当然，所有服务都必须是<em>绿色的</em> （即运行和运行中）。 如果遇到错误， <code>journalct -xe</code>或<code>journal -f -t kube-apiserver</code>将帮助您了解到底出了什么问题。 </p><br><p> 不要急于立即启动所有服务器，因为启动足以启用etcd和kube-apiserver。 如果一切顺利，并且您立即获得了该向导的所有四项服务，则可以将向导启动视为成功。 </p><br><h3 id="master-2"> 大师 </h3><br><p> 您可以使用<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">systemd</a>设置或为正在使用的配置生成初始化脚本。 如前所述，对于母版，您需要： </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">-systemd / etcd</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">-systemd / kube-apiserver</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">-systemd / kube-controller-manager</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">-systemd / kube-scheduler</a> </p><br><h3 id="rabochie-uzly-2"> 工作单位 </h3><br><p>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">系统化/容器化</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">-systemd / kubelet</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">-systemd / kube-proxy</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">-systemd / coredns</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">系统/法兰绒</a> </p><br><h3 id="klient"> 顾客 </h3><br><p> 为了使客户端正常工作，只需在<code>${HOME}/.kube/config</code> <code>certs/kubeconfig/admin.kubeconfig</code>复制<code>certs/kubeconfig/admin.kubeconfig</code> （在生成或自己编写后） <code>${HOME}/.kube/config</code> </p><br><p> 下载<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">kubectl</a>并检查kube-apiserver的操作。 让我再次提醒您，在此阶段，为了使kube-apiserver正常工作，只有etcd应该起作用。 稍后，群集的全部操作将需要其余组件。 </p><br><p> 检查kube-apiserver和kubectl是否工作： </p><br><pre> <code class="bash hljs">$ kubectl version Client Version: version.Info{Major:<span class="hljs-string"><span class="hljs-string">"1"</span></span>, Minor:<span class="hljs-string"><span class="hljs-string">"13"</span></span>, GitVersion:<span class="hljs-string"><span class="hljs-string">"v1.13.0"</span></span>, <span class="hljs-string"><span class="hljs-string">"extra info"</span></span>: <span class="hljs-string"><span class="hljs-string">"..."</span></span>} Server Version: version.Info{Major:<span class="hljs-string"><span class="hljs-string">"1"</span></span>, Minor:<span class="hljs-string"><span class="hljs-string">"13"</span></span>, GitVersion:<span class="hljs-string"><span class="hljs-string">"v1.13.0"</span></span>, <span class="hljs-string"><span class="hljs-string">"extra info"</span></span>: <span class="hljs-string"><span class="hljs-string">"..."</span></span>}</code> </pre> <br><h1 id="konfiguraciya-flannel"> 绒布配置 </h1><br><p> 作为法兰绒配置，我选择了<code>vxlan</code>后端。  <a href="">在此处</a>阅读有关后端的更多信息。 </p><br><div class="spoiler">  <b class="spoiler_title">host-gw，以及为什么它不起作用</b> <div class="spoiler_text"><p> 我必须马上说，在VPS上运行kubernetes集群可能会限制您使用<code>host-gw</code>后端。 由于不是经验丰富的网络工程师，我花了大约两天的时间进行调试，以了解在流行的VDS / VPS提供商上使用它时出现的问题。 </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Linode.com</a>和<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">digitalocean</a>已经过测试。 问题的实质是提供商没有为专用网络提供诚实的L2。 反过来，这使得在这种配置下无法在节点之间移动网络流量： </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/e0c/c7e/add/e0cc7eadd6253cf4921df17ca6fe5d47.png" alt="交通量"></p><br><p> 为了使网络流量能够在节点之间工作，正常的路由就足够了。 不要忘记将net.ipv4.ip_forward设置为1，并且过滤器表中的FORWARD链不应包含节点的禁止规则。 </p><br><pre> <code class="bash hljs">node1$ ip route add 10.200.12.0/24 via 192.168.1.2 node2$ ip route add 10.200.8.0/24 via 192.168.1.1</code> </pre> <br><pre> <code class="plaintext hljs">[10.200.80.23 container-1]-&gt;[192.168.1.1 node-1]-&gt;[192.168.1.2 node-2]-&gt;[10.200.12.5 container-2]</code> </pre> <br><p> 这恰恰对指示的VPS / VDS（并且很可能通常对所有VDS / VDS）不起作用。 </p><br><p> 因此，如果在节点之间配置具有较高网络性能的解决方案<strong>对</strong>您<strong>来说很重要，</strong>那么您仍然必须花费20多美元来组织集群。 </p></div></div><br><p> 您可以从<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">etc / flannel</a>使用<code>set-flannel-config.sh</code>来设置所需的绒布配置。  <strong>重要的是要记住</strong> ：如果决定更改后端，则需要删除etcd中的配置并重新启动所有节点上的所有法兰绒守护程序，因此请明智地选择它。 默认值为vxlan。 </p><br><pre> <code class="bash hljs">master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_CA_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/ca.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_CERT_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/kubernetes.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_KEY_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/kubernetes-key.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_ENDPOINTS=<span class="hljs-string"><span class="hljs-string">'https://127.0.0.1:2379'</span></span> master$ etcdctl ls /coreos.com/network/subnets/ /coreos.com/network/subnets/10.200.8.0-24 /coreos.com/network/subnets/10.200.12.0-24 master$ etcdctl get /coreos.com/network/subnets/10.200.8.0-24 {<span class="hljs-string"><span class="hljs-string">"PublicIP"</span></span>:<span class="hljs-string"><span class="hljs-string">"178.79.168.130"</span></span>,<span class="hljs-string"><span class="hljs-string">"BackendType"</span></span>:<span class="hljs-string"><span class="hljs-string">"vxlan"</span></span>,<span class="hljs-string"><span class="hljs-string">"BackendData"</span></span>:{<span class="hljs-string"><span class="hljs-string">"VtepMAC"</span></span>:<span class="hljs-string"><span class="hljs-string">"22:ca:ac:15:71:59"</span></span>}}</code> </pre> <br><p> 在etcd中注册所需的配置后，您需要配置服务以在每个工作节点上运行它。 </p><br><h2 id="flannelservice"> 法兰绒服务 </h2><br><p> 可以在此处使用该服务的示例： <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">systemd / flannel</a> </p><br><div class="spoiler">  <b class="spoiler_title">法兰绒服务</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Flanneld overlay address etcd agent After=network.target [Service] Type=notify #: current host ip. don't change if ip have not changed Environment=PUBLIC_IP=178.79.168.130 Environment=FLANNEL_ETCD=https://192.168.153.60:2379 ExecStart=/usr/bin/flanneld \ -etcd-endpoints=${FLANNEL_ETCD} -etcd-prefix=${FLANNEL_ETCD_KEY} \ -etcd-cafile=/var/lib/kubernetes/ca.pem \ -etcd-certfile=/var/lib/kubernetes/kubernetes.pem \ -etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \ -etcd-prefix=/coreos.com/network \ -healthz-ip=127.0.0.1 \ -subnet-file=/run/flannel/subnet.env \ -public-ip=${PUBLIC_IP} \ -kubeconfig-file=/var/lib/kubernetes/config/kubeconfig/flanneld.kubeconfig \ $FLANNEL_OPTIONS ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker Restart=on-failure RestartSec=5 [Install] RequiredBy=docker.service</code> </pre> </div></div><br><h2 id="nastroyka"> 客制化 </h2><br><p> 如前所述，我们需要ca.pem，kubernetes.pem和kubernetes-key.pem文件在etcd中进行授权。 所有其他参数不具有任何神圣含义。 真正重要的唯一事情是配置全局IP地址，网络数据包将通过该IP地址在网络之间传递： </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/037/e54/803/037e5480319cedd1e662c925bce23b3e.png" alt="法兰绒网络"><br>  （ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">带法兰绒的多主机网络覆盖</a> ） </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable flanneld.service #:  node-1$ systemctl start flanneld</span></span></code> </pre> <br><p> 成功启动法兰绒后，您应该在系统上找到flannel.N网络接口： </p><br><pre> <code class="plaintext hljs">node-1$ ifconfig flannel.100: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1450 inet 10.200.8.0 netmask 255.255.255.255 broadcast 0.0.0.0 inet6 fe80::20ca:acff:fe15:7159 prefixlen 64 scopeid 0x20&lt;link&gt; ether 22:ca:ac:15:71:59 txqueuelen 0 (Ethernet) RX packets 18853 bytes 1077085 (1.0 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 11856 bytes 264331154 (252.0 MiB) TX errors 0 dropped 47 overruns 0 carrier 0 collisions 0</code> </pre> <br><p> 检查您的接口在所有节点上是否正常工作非常简单。 在我的情况下，节点1和节点2分别具有10.200.8.0/24和10.200.12.0/24网络，因此对于常规icmp请求，我们检查其可用性： </p><br><pre> <code class="plaintext hljs">#:  node-2  node-1 node-1 $ ping -c 1 10.200.12.0 PING 10.200.12.0 (10.200.12.0) 56(84) bytes of data. 64 bytes from 10.200.12.0: icmp_seq=1 ttl=64 time=4.58 ms #:  node-1  node-2 node-2 $ ping -c 1 10.200.8.0 PING 10.200.8.0 (10.200.8.0) 56(84) bytes of data. 64 bytes from 10.200.8.0: icmp_seq=1 ttl=64 time=1.44 ms</code> </pre> <br><p> 如果出现任何问题，建议检查主机之间基于UDP的iptables中是否存在任何剪切规则。 </p><br><h1 id="konfiguraciya-containerd"> 容器配置 </h1><br><p> 将<a href="">etc / containerded / config.toml</a>放在<code>/etc/containerd/config.toml</code>或您方便的任何地方，主要要记住要更改服务中配置文件的路径（containerd.service，如下所述）。 </p><br><p> 对标准进行一些修改的配置。 如果您不了解执行此操作的原因，请不要<strong>将</strong> <code>enable_tls_streaming = true</code> <strong>设置为重要</strong> 。 否则， <code>kubectl exec</code>将停止工作，并给出错误消息，表明该证书是由未知方签名的。 </p><br><h2 id="containerdservice"> containerd.service </h2><br><div class="spoiler"> <b class="spoiler_title">containerd.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target [Service] ; uncomment this if your overlay module are built as module ; ExecStartPre=/sbin/modprobe overlay ExecStart=/usr/bin/containerd \ -c /etc/containerd/config.toml Restart=always RestartSec=5 Delegate=yes KillMode=process OOMScoreAdjust=-999 LimitNOFILE=1048576 LimitNPROC=infinity LimitCORE=infinity [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-1"> 客制化 </h2><br><p>  ,   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">cri-tools</a> . <br>  <a href="">etc/crictl.yaml</a>  <code>/etc/crictl.yaml</code> .      : </p><br><pre> <code class="bash hljs">node-1$ CONTAINERD_NAMESPACE=k8s.io crictl ps CONTAINER ID IMAGE CREATED STATE NAME ATTEMPT POD ID</code> </pre> <br><p>  ,    -    kubernetes , crictl       , ,    . </p><br><h1 id="konfiguraciya-cni-plugins">  CNI Plugins </h1><br><p>  CNI    ,      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="></a>   ,    ,   . </p><br><h1 id="nastroyka-2"> 客制化 </h1><br><p>       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">cni plugins</a>        <code>/opt/cni/bin/</code> </p><br><p>  <a href="">/etc/cni/net.d</a>      : </p><br><div class="spoiler"> <b class="spoiler_title">/etc/cni/net.d/10-flannel.conflist</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.0", "name": "cbr0", "plugins": [ { "type": "flannel", "name": "kubenet", "delegate": { "hairpinMode": true, "isDefaultGateway": true } }, { "type": "portmap", "capabilities": { "portMappings": true }, "externalSetMarkChain": "KUBE-MARK-MASQ" } ] }</code> </pre> </div></div><br><div class="spoiler"> <b class="spoiler_title">/etc/cni/net.d/99-loopback.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.0", "type": "loopback" }</code> </pre> </div></div><br><p>       ,    .  ,       ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Red Hat  Docker  Podman</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Intro to Podman</a> </p><br><h1 id="konfiguraciya-kubelet">  Kubelet </h1><br><p>     kubelet  (     cni) —    .   kubelet    hostname.         ,      ""   <code>kubectl logs</code> , <code>kubectl exec</code> , <code>kubectl port-forward</code> . </p><br><div class="spoiler"> <b class="spoiler_title"> kubelet-config.yaml</b> <div class="spoiler_text"><p>  ,   <a href="">etc/kubelet-config.yaml</a>   ,        ,     .     : </p><br><pre> <code class="plaintext hljs">systemReserved: cpu: 200m memory: 600Mi</code> </pre> <br><p>  ,        GO  kubernetes,  ,       .        .           0.2 vCPU  600 MB     . </p><br><p>   ,  , kubelet, kube-proxy, coredns, flannel    . ,               —     2 vCPU / 4G ram,           ,     kubernetes + postgresql . </p><br><p>    - (micro nodes)        . </p></div></div><br><h2 id="kubeletservice"> kubelet.service </h2><br><p>  service    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">systemd/kubelet</a> </p><br><div class="spoiler"> <b class="spoiler_title">kubelet.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes Requires=containerd.service [Service] #Environment=NODE_IP=192.168.164.230 Environment=NODE_IP=178.79.168.130 #: node name given by env Environment=NODE_NAME=w40k.net ExecStart=kubelet \ --allow-privileged \ --root-dir=/var/lib/kubernetes/kubelet \ --config=/var/lib/kubernetes/kubelet/kubelet-config.yaml \ --kubeconfig=/var/lib/kubernetes/kubelet/node-1.kubeconfig \ --cni-bin-dir=/opt/cni/bin \ --cni-conf-dir=/etc/cni/net.d/ \ --network-plugin=cni \ --container-runtime=remote \ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \ --image-pull-progress-deadline=10m \ --node-ip=${NODE_IP} \ --hostname-override=${NODE_NAME} \ --v=1 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-3"> 客制化 </h2><br><p>      ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">RBAC</a> ,                kubelet. </p><br><p>  <a href="">etc/kubelet-default-rbac.yaml</a>  ,  kubelet        : </p><br><pre> <code class="bash hljs">user$ kubectl apply -f etc/kubelet-default-rbac.yaml</code> </pre> <br><p>  ,    ,        . </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable kubelet.service #:  node-1$ systemctl start kubelet</span></span></code> </pre> <br><p>    ,           api : </p><br><pre> <code class="plaintext hljs">$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME w40k.net Ready &lt;none&gt; 5m v1.13.1 178.79.168.130 &lt;none&gt; Gentoo/Linux 4.18.16-x86_64-linode118 containerd://1.2.1</code> </pre> <br><h1 id="konfiguraciya-kube-proxy">  Kube Proxy </h1><br><p> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">systemd/kubelet</a> .   ,   , <code>kube-proxy-config.yaml</code>     : <a href="">etc/kube-proxy</a> </p><br><h2 id="kube-proxyservice"> kube-proxy.service </h2><br><div class="spoiler"> <b class="spoiler_title">kube-proxy.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Kubernetes Proxy Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=kube-proxy \ --config=/var/lib/kubernetes/kube-proxy/kube-proxy-config.yaml Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-4"> 客制化 </h2><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable kube-proxy.service #:  node-1$ systemctl start kube-proxy</span></span></code> </pre> <br><p>   kube-proxy   ""   iptables,         ,   -   kubernetes  (- ).   . </p><br><h1 id="konfiguraciya-coredns">  CoreDNS </h1><br><p> Corefile   : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">etc/coredns/Corefile</a> ,    : </p><br><div class="spoiler"> <b class="spoiler_title">/etc/coredns/Corefile</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">.:53 { errors log stdout health :8081 kubernetes cluster.local 10.200.0.0/16 { endpoint https://178.79.148.185:6443 tls /var/lib/kubernetes/kubernetes.pem /var/lib/kubernetes/kubernetes-key.pem /var/lib/kubernetes/ca.pem pods verified upstream /etc/resolv.conf kubeconfig /var/lib/kubernetes/config/kubeconfig/coredns.kubeconfig default } proxy . /etc/resolv.conf cache 30 }</code> </pre> </div></div><br><p>     coredns.kubeconfig  pem- (    )   worker . , coredns      systemd-resolved. ,         Ubuntu ,  ,  ,  ,  .        . </p><br><h2 id="corednsservice"> coredns.service </h2><br><div class="spoiler"> <b class="spoiler_title">coredns.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=CoreDNS Documentation=https://coredns.io/ After=network.target [Service] ExecStart=/usr/bin/coredns -conf /etc/coredns/Corefile Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-5"> 客制化 </h2><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable coredns.service #:  node-1$ systemctl start coredns</span></span></code> </pre> <br><p> ,   ,   : </p><br><pre> <code class="plaintext hljs">node-1$ dig kubernetes.default.svc.cluster.local @127.0.0.1 #:    ;kubernetes.default.svc.cluster.local. IN A ;; ANSWER SECTION: kubernetes.default.svc.cluster.local. 5 IN A 10.32.0.1</code> </pre> <br><p>   , coredns   ip   kubernetes . <br> <strong></strong> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">kubernetes.default </a>   <strong></strong> kube-controller-manager,      : </p><br><pre> <code class="plaintext hljs">$ kubectl get svc -n default NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.32.0.1 &lt;none&gt; 443/TCP 26h</code> </pre> <br><h1 id="nginx-ingress--cert-manager"> nginx-ingress &amp; cert-manager </h1><br><p>   ,    .        nginx-ingress  cert-manager. </p><br><p> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">nginx kubernetes ingress</a> (master),  : </p><br><pre> <code class="bash hljs"> user$ git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/nginxinc/kubernetes-ingress.git user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> kubernetes-ingress/deployments user$ kubectl apply -f common/ns-and-sa.yaml user$ kubectl apply -f common/nginx-config.yaml user$ kubectl apply -f common/default-server-secret.yaml user$ kubectl apply -f daemon-set/nginx-ingress.yaml user$ kubectl apply -f rbac/rbac.yaml</code> </pre> <br><p> — <a href="">cert manager</a> (v0.5.2) </p><br><pre> <code class="bash hljs"> user$ git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/jetstack/cert-manager.git user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> cert-manager &amp;&amp; git co v0.5.2 user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> contrib/manifests/cert-manager user$ kubectl apply -f with-rbac.yaml</code> </pre> <br><p>  ,    ,  ,    : </p><br><pre> <code class="plaintext hljs">NAMESPACE NAME READY STATUS RESTARTS AGE cert-manager cert-manager-554c76fbb7-t9762 1/1 Running 0 3h38m nginx-ingress nginx-ingress-sdztf 1/1 Running 0 10h nginx-ingress nginx-ingress-vrf85 1/1 Running 0 10h</code> </pre> <br><p>  cert-manager  nginx-ingress    running state,   ,    .          ,         <code>Running</code> .            . </p><br><h1 id="zapuskaem-prilozhenie">   </h1><br><p>   ,     .      ,   kubernetes resource : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">app/k8s</a> </p><br><pre> <code class="bash hljs">user$ kube apply -f ns-and-sa.yaml user$ kube apply -f configmap.yaml <span class="hljs-comment"><span class="hljs-comment">#:  secret-example.yaml       #: secret.yaml user$ kube apply -f secret.yaml user$ kube apply -f tls-production.yaml user$ kube apply -f deployment.yaml user$ kube apply -f service.yaml user$ kube apply -f ingress-production.yaml</span></span></code> </pre> <br><p>   ,     - .  ,    (      kubernetes-example.w40k.net),     ,    ,  cert-manager    nginx-ingress              .   ,    ingress  tls/ssl. </p><br><p>      : </p><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=http://no-">http://no-https.kubernetes-example.w40k.net/</a> —  ssl;  ,  -   ,     . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">https://kubernetes-example.w40k.net/</a> —   (,   ,   ),  ,     ,       kubernetes     . </li></ul><br><p>       ,      -   .    -       ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="> </a> ,        . </p><br><h1 id="ssylki"> 参考文献 </h1><br><p> ,     ,   : </p><br><p> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes the hard way</a> <br> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Multi-Host Networking Overlay with Flannel</a> <br> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Intro to Podman</a> <br> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Stateless Applications</a> <br> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">What is ingress</a> </p><br><p>   : </p><br><p> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes Networking: Behind the scenes</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="></a> ) <br> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">A Guide to the Kubernetes Networking Model</a> <br> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Understanding kubernetes networking: services</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="></a> ) </p><br><h1 id="qa"> Q&amp;A </h1><br><p> &lt;tbd&gt;,           . </p><br><h1 id="otladochnaya-informaciya">   </h1><br><p>     , ,     .    ,       ,  -  ,    ,  . </p><br><div class="spoiler"> <b class="spoiler_title"> </b> <div class="spoiler_text"><h2 id="api-server"> Api Server </h2><br><p>   <code>kube-apiserver.service</code>    ,       api-server'   curl    http .            - . <br>     admin.kubeconfig  ${HOME}/.kube/config,   kubectl      api-server (kube-apiserver). </p><br><p>    (   )  HTTP 200 OK + ,  api-server  : </p><br><pre> <code class="plaintext hljs">curl -H "Authorization: Bearer e5qXNAtwwCHUUwyLilZmAoFPozrQwUpw" -k -L https://&lt;api-server-address&gt;:6443/api/v1/</code> </pre> <br><h2 id="kube-controller-manager"> Kube Controller Manager </h2><br><p>  ,  controller manager   api    ,      .        ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">service account'</a> : </p><br><pre> <code class="plaintext hljs">$ kubectl get sa NAME SECRETS AGE default 1 19h</code> </pre> <br><p>    ,   ,  kube-controller-manager  . </p><br><h2 id="kube-scheduler"> Kube Scheduler </h2><br><p>       .  ,    ,    <a href="">debug/job.yaml</a>        <code>kubectl describe &lt;type/resource&gt;</code> . <br>    <strong> </strong>  ,  kube controller manager . </p><br><pre> <code class="plaintext hljs">#:   job user$ kubectl apply -f debug/job.yaml job.batch/app created #:  ,   job user$ kubectl get pods -l job-name=app NAME READY STATUS RESTARTS AGE app-9kr9z 0/1 Completed 0 54s #: ,        #:   user$ kubectl describe pods app-9kr9z # ...   ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 12s default-scheduler Successfully assigned example/app-9kr9z to w40k.net</code> </pre> <br><p>   , default-scheduler   pod   w40k.net.    -  ,            —    . </p><br><p>              . , ,   , —      "".       systemd        . </p><br><p>   kube scheduler  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="></a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="></a> </p><br><h2 id="kubelet"> Kubelet </h2><br><p> Kubelet    kubernetes     .  kubelet       .     kubernetes event ( <code>kubectl get events -o wide</code> )         . </p><br><p>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="></a> (  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="></a> ) </p><br><h2 id="kube-proxy-i-servisy"> Kube Proxy   </h2><br><p>     kube-proxy    : </p><br><ul><li>      (     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="> Flannel</a> ,      ); </li><li>  iptables,   filter  nat . </li></ul><br><p> <strong></strong> , 10.32.0.0/24   "".  ,        .     iptables,     ,   ,     -    +.  <strong> </strong>  icmp    ,      ping'  .        ,     . </p><br><p>  ,     kube-proxy,               : </p><br><pre> <code class="plaintext hljs">#:    user$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend ClusterIP 10.32.0.195 &lt;none&gt; 80/TCP 5m #:     user$ kubectl get pods -o wide #:     ' NAME READY STATUS RESTARTS AGE IP NODE backend-896584448-4r94s 1/1 Running 0 11h 10.200.8.105 w40k.net backend-896584448-np992 1/1 Running 0 11h 10.200.12.68 docker.grart.net #:  10   /status/ endpoint ,       #:       node-1$ for i in `seq 10`; do curl -L http://10.32.0.195/status/; done okokokokokokokokokok node-1$ conntrack -L -d 10.32.0.195 tcp 6 62 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62158 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62158 [ASSURED] mark=0 use=1 tcp 6 60 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62144 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62144 [ASSURED] mark=0 use=1 tcp 6 58 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62122 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62122 [ASSURED] mark=0 use=1 tcp 6 59 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62142 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62142 [ASSURED] mark=0 use=1 tcp 6 58 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62130 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62130 [ASSURED] mark=0 use=1 tcp 6 61 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62150 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62150 [ASSURED] mark=0 use=1 tcp 6 56 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62116 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62116 [ASSURED] mark=0 use=1 tcp 6 57 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62118 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62118 [ASSURED] mark=0 use=1 tcp 6 59 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62132 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62132 [ASSURED] mark=0 use=1 tcp 6 56 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62114 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62114 [ASSURED] mark=0 use=1</code> </pre> <br><p>      src/dst (9  10 ).   ,  src      : </p><br><ul><li> 10.200.8.105 </li><li> 10.200.12.68 </li></ul><br><p>  ,    .      ,  -  ( ,    )  .         . </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#: node-1   10.200.8.105, node-2 10.200.12.68, #:      8000  #:  node-1 node-1$ curl -L http://10.200.8.105:8000/status/ ok node-1$ curl -L http://10.200.12.68:8000/status/ ok #:  node-2 node-2$ curl -L http://10.200.8.105:8000/status/ ok node-2$ curl -L http://10.200.12.68:8000/status/ ok</span></span></code> </pre> <br><p>    ,    ,    conntrack        ,  ,      kube-proxy.   ,       nat : </p><br><p> <code>node-1$ iptables -t nat -vnL</code> </p> <br><p>          . </p><br><p>                  .  ,    ,      .   ,       .  -       , ,   . </p><br><p>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="></a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="></a> </p></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN435228/">https://habr.com/ru/post/zh-CN435228/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN435214/index.html">Linux 4.20发布-新内核版本中的更改</a></li>
<li><a href="../zh-CN435216/index.html">如何从两行代码中获得200，以及为什么需要这样做</a></li>
<li><a href="../zh-CN435220/index.html">Kotlin Native：跟踪文件</a></li>
<li><a href="../zh-CN435224/index.html">如何在英语办公室沟通：14个有用的习语</a></li>
<li><a href="../zh-CN435226/index.html">从头开始还原数据</a></li>
<li><a href="../zh-CN435234/index.html">更智能，更精确，更精确：人工智能如何改变太空飞行</a></li>
<li><a href="../zh-CN435236/index.html">美洲印第安人堡垒的字节机（不仅如此）（第3部分）</a></li>
<li><a href="../zh-CN435240/index.html">虚幻引擎4-后处理扫描效果</a></li>
<li><a href="../zh-CN435242/index.html">我为什么害怕成为一个“泵人”</a></li>
<li><a href="../zh-CN435244/index.html">2018年ITER项目</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>