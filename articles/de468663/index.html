<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèø‚ÄçüöÄ ü•Ñ üõçÔ∏è End2 End Approach in automatischen Spracherkennungsaufgaben üòß ‚òÇÔ∏è üí¥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Was ist End2End-Spracherkennung und warum wird sie ben√∂tigt? Was ist der Unterschied zum klassischen Ansatz? Und warum, um ein gutes End2End-basiertes...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>End2 End Approach in automatischen Spracherkennungsaufgaben</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ru_mts/blog/468663/">  Was ist End2End-Spracherkennung und warum wird sie ben√∂tigt?  Was ist der Unterschied zum klassischen Ansatz?  Und warum, um ein gutes End2End-basiertes Modell zu trainieren, ben√∂tigen wir eine gro√üe Datenmenge - in unserem heutigen Beitrag. <br><br><h4>  Der klassische Ansatz zur Spracherkennung </h4><br>  Bevor Sie √ºber den End2End-Ansatz sprechen, sollten Sie zun√§chst √ºber den klassischen Ansatz zur Spracherkennung sprechen.  Wie ist er? <br><br><img src="https://habrastorage.org/webt/xk/xm/sc/xkxmscrxxx0n2hvkoxdxyflqaoq.png"><br><a name="habracut"></a><br><h4>  Merkmalsextraktion </h4><br>  Tats√§chlich ist dies keine vollst√§ndig lineare Folge von Aktionsbl√∂cken.  Lassen Sie uns n√§her auf jeden Block eingehen.  Wir haben eine Art Eingabesprache, die auf den ersten Block f√§llt - Feature Extraction.  Dies ist ein Block, der Zeichen aus der Sprache zieht.  Es muss bedacht werden, dass die Sprache selbst eine ziemlich komplizierte Sache ist.  Sie m√ºssen in der Lage sein, irgendwie damit zu arbeiten, daher gibt es Standardmethoden zum Isolieren von Merkmalen aus der Signalverarbeitungstheorie.  Zum Beispiel Mel-Cepstral-Koeffizienten (MFCC) und so weiter. <br><br><h4>  Akustisches Modell </h4><br>  Die n√§chste Komponente ist das akustische Modell.  Es kann auf tiefen neuronalen Netzen oder auf Mischungen von Gau√üschen Verteilungen und versteckten Markov-Modellen basieren.  Sein Hauptziel ist es, aus einem Abschnitt des akustischen Signals die Wahrscheinlichkeitsverteilungen verschiedener Phoneme in diesem Abschnitt zu erhalten. <br><br>  Als n√§chstes kommt der Decoder, der anhand des Ergebnisses aus dem letzten Schritt nach dem wahrscheinlichsten Pfad im Diagramm sucht.  Rescoring ist der letzte Schliff bei der Anerkennung, dessen Hauptaufgabe darin besteht, die Hypothesen neu abzuw√§gen und das Endergebnis zu erzielen. <br><br><img src="https://habrastorage.org/webt/pp/hh/2i/pphh2ietnprdh5aeqtbvh06kzpu.png"><br><br>  Lassen Sie uns n√§her auf das akustische Modell eingehen.  Wie ist sie  Wir haben einige Sprachaufnahmen, die in ein bestimmtes System eingegeben werden, das auf GMM (monauraler Gausovy-Mix) oder HMM basiert.  Das hei√üt, wir haben Darstellungen in Form von Phonemen, wir verwenden Monophone, dh kontextunabh√§ngige Phoneme.  Weiter von diesen machen wir Mischungen von Gau√üschen Verteilungen basierend auf kontextsensitiven Phonemen.  Es verwendet Clustering basierend auf Entscheidungsb√§umen. <br><br>  Dann versuchen wir, eine Ausrichtung aufzubauen.  Eine solche v√∂llig nicht triviale Methode erm√∂glicht es uns, ein akustisches Modell zu erhalten.  Es klingt nicht sehr einfach, es ist sogar noch komplizierter, es gibt viele Nuancen und Funktionen.  Infolgedessen kann ein Modell, das in Hunderten von Stunden trainiert wurde, die Akustik sehr gut simulieren. <br><br><img src="https://habrastorage.org/webt/vt/-x/gl/vt-xgltq9lf3fujagn2jkqdriqy.png"><br><br><h4>  Decoder </h4><br>  Was ist ein Decoder?  Dies ist das Modul, das den wahrscheinlichsten √úbergangspfad gem√§√ü dem HCLG-Diagramm ausw√§hlt, das aus 4 Teilen besteht: <br><br>  H-Modul basierend auf HMM <br>  C-Kontextabh√§ngigkeitsmodul <br>  L Aussprachemodul <br>  G Sprachmodellmodul <br><br>  Wir bauen ein Diagramm auf diesen vier Komponenten auf, auf dessen Grundlage wir unsere akustischen Merkmale in bestimmte verbale Konstruktionen dekodieren. <br><br>  Plus oder Minus, es ist klar, dass der klassische Ansatz ziemlich umst√§ndlich und schwierig ist, es ist schwierig zu trainieren, da er aus einer gro√üen Anzahl separater Teile besteht, f√ºr die Sie jeweils Ihre eigenen Daten f√ºr das Training vorbereiten m√ºssen. <br><br><h4>  II End2End-Ansatz </h4><br>  Was ist End2End-Spracherkennung und warum wird sie ben√∂tigt?  Dies ist ein bestimmtes System, das die Folge von akustischen Zeichen in der Folge von Graphemen (Buchstaben) oder W√∂rtern direkt widerspiegeln soll.  Sie k√∂nnen auch sagen, dass dies ein System ist, das Kriterien optimiert, die sich direkt auf die endg√ºltige Metrik der Qualit√§tsbewertung auswirken.  Zum Beispiel ist unsere Aufgabe speziell die Wortfehlerrate.  Wie gesagt, es gibt nur eine Motivation - diese komplexen mehrstufigen Komponenten in Form einer einfachen Komponente darzustellen, die W√∂rter oder Grapheme direkt aus der Eingabesprache anzeigt, ausgibt. <br><br><h4>  Simulationsproblem </h4><br>  Hier haben wir sofort ein Problem: Tonsprache ist eine Sequenz, und am Ausgang m√ºssen wir auch eine Sequenz angeben.  Und bis 2006 gab es keine ad√§quate M√∂glichkeit, dies zu modellieren.  Was ist das Problem der Modellierung?  F√ºr jede Aufnahme musste ein komplexes Markup erstellt werden, was impliziert, in welcher Sekunde wir einen bestimmten Ton oder Buchstaben aussprechen.  Dies ist ein sehr umst√§ndliches komplexes Layout, und daher wurde eine gro√üe Anzahl von Studien zu diesem Thema nicht durchgef√ºhrt.  2006 wurde ein interessanter Artikel von Alex Graves ‚ÄûConnectionist Temporal Classification‚Äú (CTC) ver√∂ffentlicht, in dem dieses Problem im Prinzip gel√∂st wird.  Der Artikel wurde jedoch ver√∂ffentlicht, und zu diesem Zeitpunkt war nicht gen√ºgend Rechenleistung vorhanden.  Und echte funktionierende Spracherkennungsalgorithmen erschienen viel sp√§ter. <br><br>  Insgesamt haben wir: Der CTC-Algorithmus wurde vor dreizehn Jahren von Alex Graves als Werkzeug vorgeschlagen, mit dem Sie akustische Modelle trainieren / trainieren k√∂nnen, ohne dass dieses komplexe Markup erforderlich ist - die Ausrichtung der Eingabe- und Ausgabesequenzrahmen.  Basierend auf diesem Algorithmus erschienen zun√§chst Arbeiten, die nicht vollst√§ndig end2end waren, und als Ergebnis wurden Phoneme ausgegeben.  Es ist erw√§hnenswert, dass kontextsensitive Phoneme, die auf STS basieren, eines der besten Ergebnisse bei der Erkennung der Redefreiheit erzielen.  Es ist aber auch erw√§hnenswert, dass dieser Algorithmus, der direkt auf die W√∂rter angewendet wird, im Moment irgendwo zur√ºckbleibt. <br><br><img src="https://habrastorage.org/webt/ns/oe/e2/nsoee2tomucbndnfsaj__7va-o4.png"><br><br><h4>  Was ist STS? </h4><br>  Jetzt werden wir etwas detaillierter dar√ºber sprechen, was das STS ist und warum es ben√∂tigt wird, welche Funktion es ausf√ºhrt.  STS ist erforderlich, um das akustische Modell zu trainieren, ohne dass eine bildweise Ausrichtung zwischen Ton und Transkription erforderlich ist.  Bild f√ºr Bild ist die Ausrichtung, wenn wir sagen, dass ein bestimmtes Bild aus einem Ton einem solchen Bild aus der Transkription entspricht.  Wir haben einen konventionellen Encoder, der akustische Zeichen als Eingabe akzeptiert - er gibt eine Art Verschleierung des Zustands aus, auf deren Grundlage wir mit softmax bedingte Wahrscheinlichkeiten erhalten.  Der Encoder besteht normalerweise aus mehreren Schichten von LSTMs oder anderen Variationen von RNNs.  Es ist erw√§hnenswert, dass STS zus√§tzlich zu normalen Zeichen mit einem Sonderzeichen arbeitet, das als leeres Zeichen oder leeres Symbol bezeichnet wird.  Um das Problem zu l√∂sen, das sich aus der Tatsache ergibt, dass nicht jeder akustische Rahmen einen Rahmen in der Transkription hat und umgekehrt (dh wir haben Buchstaben oder T√∂ne, die viel l√§nger klingen und es gibt kurze T√∂ne, sich wiederholende T√∂ne) und dort dieses leere Symbol. <br><br>  Das STS selbst soll die endg√ºltige Wahrscheinlichkeit von Zeichenfolgen maximieren und eine m√∂gliche Ausrichtung verallgemeinern.  Da wir diesen Algorithmus in neuronalen Netzen verwenden m√∂chten, m√ºssen wir verstehen, wie seine Vorw√§rts- und R√ºckw√§rtsbetriebsarten funktionieren.  Wir werden uns nicht mit der mathematischen Rechtfertigung und den Merkmalen der Funktionsweise dieses Algorithmus befassen, da dies sonst sehr lange dauern wird. <br><br>  Was haben wir: Die erste ASR, die auf dem STS-Algorithmus basiert, erscheint 2014.  Wieder pr√§sentierte Alex Graves eine Publikation, die auf dem zeichenweisen STS basiert und die Eingabesprache direkt in einer Folge von W√∂rtern anzeigt.  Einer der Kommentare in diesem Artikel ist, dass die Verwendung eines externen Soundmodells wichtig ist, um ein gutes Ergebnis zu erzielen. <br><br><h4>  5 M√∂glichkeiten zur Verbesserung des Algorithmus </h4><br>  Es gibt viele verschiedene Variationen und Verbesserungen des obigen Algorithmus.  Hier sind zum Beispiel die f√ºnf beliebtesten in letzter Zeit. <br><br>  ‚Ä¢ Das Sprachmodell wird beim ersten Durchgang in die Dekodierung einbezogen <br>  o [Hannun et al., 2014] [Maas et al., 2015]: Direkte First-Pass-Decodierung mit einem LM im Gegensatz zur erneuten Bewertung wie in [Graves &amp; Jaitly, 2014] <br>  o [Miao et al., 2015]: EESEN-Framework f√ºr die Dekodierung mit WFSTs, Open Source Toolkit <br>  ‚Ä¢ Umfangreiches Training auf der GPU;  Datenerweiterung  mehrere Sprachen <br>  o [Hannun et al., 2014;  DeepSpeech] [Amodei et al., 2015;  DeepSpeech2]: GPU-Training in gro√üem Ma√üstab;  Datenerweiterung;  Mandarin und Englisch <br>  ‚Ä¢ Verwendung langer Einheiten: W√∂rter anstelle von Zeichen <br>  o [Soltau et al., 2017]: CTC-Ziele auf Wortebene, die auf 125.000 Sprachstunden trainiert wurden.  Leistung nahe oder besser als ein herk√∂mmliches System, auch ohne Verwendung eines LM! <br>  o [Audhkhasi et al., 2017]: Direkte Akustik-zu-Wort-Modelle auf der Telefonzentrale <br><br>  Es lohnt sich, auf die Implementierung von DeepSpeach als gutes Beispiel f√ºr eine end2end CTC-L√∂sung und auf eine Variante zu achten, die eine verbale Ebene verwendet.  Aber es gibt eine Einschr√§nkung: Um ein solches Modell zu trainieren, ben√∂tigen Sie 125.000 Stunden beschriftete Daten, was in rauen Realit√§ten tats√§chlich ziemlich viel ist. <br><br><h4>  Was ist wichtig an STS zu beachten </h4><br><ul><li>  Probleme oder Auslassungen.  F√ºr die Effizienz ist es wichtig, Annahmen √ºber die Unabh√§ngigkeit zu treffen.  Das hei√üt, der STS geht davon aus, dass die Ausgabe des Netzwerks in verschiedenen Frames bedingt unabh√§ngig ist, was tats√§chlich falsch ist.  Diese Annahme soll jedoch vereinfachen, ohne sie wird alles viel komplizierter. </li><li>  Um eine gute Leistung des STS-Modells zu erzielen, ist die Verwendung eines externen Sprachmodells erforderlich, da die direkte gierige Dekodierung nicht sehr gut funktioniert. </li></ul><br><h4>  Achtung </h4><br>  Welche Alternative haben wir f√ºr dieses STS?  Es ist wahrscheinlich f√ºr niemanden ein Geheimnis, dass es so etwas wie Aufmerksamkeit oder ‚ÄûAufmerksamkeit‚Äú gibt, die sich bis zu einem gewissen Grad revolutioniert haben und direkt von den Aufgaben der maschinellen √úbersetzung ausgegangen sind.  Und jetzt basieren vor allem Entscheidungen zur Sequenz-Sequenz-Modellierung auf diesem Mechanismus.  Wie ist er?  Versuchen wir es herauszufinden.  Zum ersten Mal √ºber Aufmerksamkeit bei Spracherkennungsaufgaben erschienen 2015 Ver√∂ffentlichungen.  Jemand Chen und Cherowski gab gleichzeitig zwei √§hnliche und unterschiedliche Ver√∂ffentlichungen heraus. <br><br>  Lassen Sie uns auf das erste eingehen - es hei√üt H√∂ren, teilnehmen und buchstabieren.  In unserer klassischen Simulation wird in der Sequenz, in der wir einen Codierer und Decodierer haben, ein weiteres Element hinzugef√ºgt, das als Aufmerksamkeit bezeichnet wird.  Der Echnoder f√ºhrt die Funktionen aus, die das akustische Modell verwendet hat.  Seine Aufgabe ist es, die eingegebene Sprache in akustische Merkmale auf hohem Niveau umzuwandeln.  Unser Decoder f√ºhrt die Aufgaben aus, die wir zuvor f√ºr das Sprachmodell und das Aussprachemodell (Lexikon) ausgef√ºhrt haben. Er sagt jedes Ausgabe-Token autoregressiv als Funktion der vorherigen voraus.  Und die Aufmerksamkeit selbst wird direkt sagen, welcher Eingaberahmen am relevantesten / wichtigsten ist, um diese Ausgabe vorherzusagen. <br><br><img src="https://habrastorage.org/webt/gi/hd/1c/gihd1cppd9nsldy12rqffbuheoe.png"><br><br>  Was sind diese Bl√∂cke?  Der √ñko-Encoder im Artikel wird als Listener beschrieben. Es handelt sich um klassische bidirektionale RNNs, die auf LSTMs oder etwas anderem basieren.  Im Allgemeinen nichts Neues - das System simuliert einfach die Eingabesequenz in komplexe Merkmale. <br><br>  Andererseits erzeugt die Aufmerksamkeit aus diesen Vektoren einen bestimmten Kontextvektor C, der dabei hilft, den Decoder direkt korrekt zu decodieren, den Decoder selbst, der beispielsweise auch einige LSTMs sind, die in die Eingabesequenz von dieser Aufmerksamkeitsschicht decodiert werden, die bereits die wichtigsten Zustandszeichen hervorgehoben hat. eine Ausgabesequenz von Zeichen. <br><br>  Es gibt auch unterschiedliche Darstellungen dieser Aufmerksamkeit selbst - was den Unterschied zwischen diesen beiden Ver√∂ffentlichungen von Chen und Charowski ausmacht.  Sie verwenden unterschiedliche Aufmerksamkeit.  Chen verwendet die Aufmerksamkeit des Punktprodukts und Charowski die additive Aufmerksamkeit. <br><br><img src="https://habrastorage.org/webt/r4/oa/tu/r4oatu1zxse9xn13cmmbx01viem.png"><br><br><h4>  Wohin als n√§chstes? </h4><br>  Dies ist ein Plus oder Minus aller wichtigen Erfolge, die bisher in Fragen der Nicht-Online-Spracherkennung erzielt wurden.  Welche Verbesserungen sind hier m√∂glich?  Wohin als n√§chstes?  Am offensichtlichsten ist die Verwendung eines Modells f√ºr Wortst√ºcke anstelle der direkten Verwendung von Graphemen.  Es k√∂nnen einige separate Morpheme oder etwas anderes sein. <br><br>  Was ist die Motivation f√ºr die Verwendung von Wortscheiben?  Typischerweise weisen Sprachmodelle der verbalen Ebene im Vergleich zur Graphemebene eine viel geringere Verwirrung auf.  Durch das Modellieren von Wortst√ºcken k√∂nnen Sie einen st√§rkeren Decoder des Sprachmodells erstellen.  Die Modellierung l√§ngerer Elemente kann die Speichereffizienz in einem auf LSTMs basierenden Decoder verbessern.  Au√üerdem k√∂nnen Sie sich m√∂glicherweise an das Auftreten von Frequenzw√∂rtern erinnern.  L√§ngere Elemente erm√∂glichen die Dekodierung in weniger Schritten, was die Inferenz dieses Modells direkt beschleunigt. <br><br>  Ein Modell f√ºr Wortst√ºcke erm√∂glicht es uns auch, das Problem von OOV-W√∂rtern (au√üerhalb des Wortschatzes) zu l√∂sen, die in einem Sprachmodell auftreten, da wir jedes Wort mit Wortst√ºcken modellieren k√∂nnen.  Es ist erw√§hnenswert, dass solche Modelle trainiert werden, um die Wahrscheinlichkeit eines Sprachmodells √ºber einen Trainingsdatensatz zu maximieren.  Diese Modelle sind positionsabh√§ngig, und wir k√∂nnen den Greedy-Algorithmus zum Decodieren verwenden. <br><br>  Welche anderen Verbesserungen neben dem Modell der Wortst√ºcke k√∂nnen sein?  Es gibt einen Mechanismus, der als Mehrkopfaufmerksamkeit bezeichnet wird.  Es wurde erstmals 2017 f√ºr die maschinelle √úbersetzung beschrieben.  Multi-Head-Aufmerksamkeit impliziert einen Mechanismus mit mehreren sogenannten K√∂pfen, mit denen Sie eine unterschiedliche Verteilung derselben Aufmerksamkeit erzeugen k√∂nnen, wodurch die Ergebnisse direkt verbessert werden. <br><br><h4>  Online-Modelle </h4><br>  Wir kommen zum interessantesten Teil - dies sind Online-Modelle.  Es ist wichtig zu beachten, dass LAS kein Streaming ist.  Das hei√üt, dieses Modell kann nicht im Online-Decodierungsmodus arbeiten.  Wir werden die beiden bislang beliebtesten Online-Modelle betrachten.  RNN-Wandler und neuronaler Wandler. <br><br>  RNN Transducer wurde von Graves in den Jahren 2012-2017 vorgeschlagen.  Die Hauptidee besteht darin, unser STS-Modell mithilfe eines rekursiven Modells etwas zu komplizieren. <br><br><img src="https://habrastorage.org/webt/5c/y6/lc/5cy6lcuv2l7q1nymrr9j4idp5pi.png"><br><br>  Es ist erw√§hnenswert, dass beide Komponenten gemeinsam auf verf√ºgbare akustische Daten trainiert werden.  Wie bei STS erfordert dieser Ansatz keine Rahmenausrichtung im Trainingsdatensatz.  Wie wir auf dem Bild sehen: Links ist unser klassischer STS und rechts der RNN-Wandler.  Und wir haben zwei neue Elemente - das <b>vorhergesagte Netzwerk</b> und das <b>Join-Netzwerk</b> . <br><br>  Der STS-Encoder ist genau der gleiche - es ist der Eingangspegel RNN, der die Verteilung √ºber alle Ausrichtungen mit allen Ausgangssequenzen bestimmt, die die L√§nge der Eingangssequenz nicht √ºberschreiten - dies wurde 2006 von Graves beschrieben.  Die Aufgabe solcher Text-zu-Sprache-Konvertierungen ist jedoch auch ausgeschlossen, wenn die Eingabesequenz, die l√§nger als die Eingabesequenz des STS ist, die Beziehung zwischen den Ausgaben nicht modelliert.  Der Wandler erweitert genau dieses STS, bestimmt die Verteilung der Ausgangssequenzen aller L√§ngen und modelliert gemeinsam die Abh√§ngigkeit von Eingabe-Ausgabe und Ausgabe-Ausgabe. <br><br>  Es stellt sich heraus, dass unser Modell letztendlich in der Lage ist, die Abh√§ngigkeiten der Ausgabe von der Eingabe und der Ausgabe von der Ausgabe des letzten Schritts zu verarbeiten. <br><br>  Was ist ein <b>vorhergesagtes Netzwerk</b> oder ein <b>vorhergesagtes</b> Netzwerk?  Sie versucht, jedes Element unter Ber√ºcksichtigung der vorherigen zu modellieren, daher √§hnelt es dem Standard-RNN mit der Vorhersage des n√§chsten Schritts.  Nur mit der zus√§tzlichen F√§higkeit, Nullhypothesen zu erstellen. <br><br>  Wie wir auf dem Bild sehen, haben wir ein vorhergesagtes Netzwerk, das den vorherigen Wert des Ausgangs empf√§ngt, und es gibt einen Encoder, der den aktuellen Wert des Eingangs empf√§ngt.  Und am Ausgang haben wir wieder so den aktuellen Wert <img src="https://habrastorage.org/webt/x4/db/gw/x4dbgwm67dwzli8xp3ysvhqm-tu.png">  . <br><br>  <b>Neuronaler Wandler</b> .  Dies ist eine Komplikation des klassischen seq-2seq-Ansatzes.  Die akustische Eingangssequenz wird vom Codierer verarbeitet, um bei jedem Zeitschritt verborgene Zustandsvektoren zu erzeugen.  Es scheint alles wie gewohnt zu sein.  Es gibt jedoch ein zus√§tzliches Transducer-Element, das bei jedem Schritt einen Block von Eingaben empf√§ngt und bis zu M-Ausgabe-Token unter Verwendung eines Modells generiert, das auf seq-2seq √ºber dieser Eingabe basiert.  Der Wandler beh√§lt seinen Zustand in Bl√∂cken bei, indem er periodische Verbindungen mit den vorherigen Zeitschritten verwendet. <br><br><img src="https://habrastorage.org/webt/hh/pn/wx/hhpnwx3l2sco6phy37tmuawvcni.png"><br><br>  <i>Die Abbildung zeigt den Wandler, der Token f√ºr den Block f√ºr die im Block des entsprechenden Ym verwendete Sequenz erzeugt.</i> <br><br>  Daher haben wir den aktuellen Status der Spracherkennung basierend auf dem End2End-Ansatz untersucht.  Es ist erw√§hnenswert, dass diese Ans√§tze heute leider eine gro√üe Datenmenge erfordern.  Und die tats√§chlichen Ergebnisse, die mit dem klassischen Ansatz erzielt werden und 200 bis 500 Stunden Tonaufnahmen erfordern, die f√ºr das Training eines guten Modells auf der Basis von End2End vorgesehen sind, erfordern mehrere oder vielleicht zehnmal mehr Daten.  Dies ist das gr√∂√üte Problem bei diesen Ans√§tzen.  Aber vielleicht wird sich bald alles √§ndern. <br><br>  <i>F√ºhrender Entwickler des AI MTS-Zentrums Nikita Semenov.</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de468663/">https://habr.com/ru/post/de468663/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de468641/index.html">Tutorial zum Erstellen einer Blockchain-L√∂sung in Hyperledger Composer</a></li>
<li><a href="../de468645/index.html">Zwei Seiten reichten aus, um die 30-j√§hrige Hypothese aus dem Bereich der Informatik zu belegen.</a></li>
<li><a href="../de468647/index.html">Riskante Musik auf einem alten IBM Mainframe-Zeilendrucker</a></li>
<li><a href="../de468653/index.html">Wie hoch ist die Aufl√∂sung des menschlichen Auges (oder wie viele Megapixel sehen wir zu einem bestimmten Zeitpunkt)?</a></li>
<li><a href="../de468657/index.html">T√§nze mit Unterst√ºtzung: Arten und Formen der Unterst√ºtzung. Unterst√ºtzungssysteme, die im Kampf arbeiten</a></li>
<li><a href="../de468665/index.html">Aber ist es Zeit, einen Bew√§sserungsapparat zu kaufen?</a></li>
<li><a href="../de468673/index.html">Workshop "Gew√§hrleistung der Sicherheit personenbezogener Daten" - 3. Oktober, St. Petersburg</a></li>
<li><a href="../de468677/index.html">Die Ank√ºndigung des Smartphones Xiaomi Mi Mix Alpha</a></li>
<li><a href="../de468679/index.html">Das ABC der Sicherheit in Kubernetes: Authentifizierung, Autorisierung, Pr√ºfung</a></li>
<li><a href="../de468683/index.html">Theorie und Praxis der Standardisierung von Docker-Diensten</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>