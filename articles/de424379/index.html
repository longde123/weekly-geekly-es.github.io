<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõãÔ∏è üë©üèæ‚Äçüîß ü¶í Erkennung von Umweltzeichen mit Azure Custom Vision aus einer mobilen Anwendung üç≠ üà∫ ü•´</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In diesem Artikel m√∂chte ich √ºber die Verwendung des Custom Vision-Dienstes zum Erkennen von Umweltzeichenfotos aus einer mobilen Anwendung sprechen. ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Erkennung von Umweltzeichen mit Azure Custom Vision aus einer mobilen Anwendung</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/424379/"><p>  In diesem Artikel m√∂chte ich √ºber die Verwendung des Custom Vision-Dienstes zum Erkennen von Umweltzeichenfotos aus einer mobilen Anwendung sprechen. </p><br><p>  CustomVision ist Teil der Cloud-basierten kognitiven Dienste von Azure. <br>  Dar√ºber, welche Technologien untersucht werden mussten, wie mit CustomVision gearbeitet werden sollte, was es ist und was damit erreicht werden kann - weiter. </p><br><p><img src="https://habrastorage.org/webt/bc/7h/ox/bc7hoxzxf64einuj6yh30ftnuk8.png"></p><a name="habracut"></a><br><p>  Die Aufgabe, Umweltzeichen zu erkennen, trat vor drei Jahren auf, als meine Frau und ich anfingen, √ºber eine mobile Anwendung zu diskutieren, die ihre Organisation (eine NGO auf dem Gebiet der √ñkologie) zur Verbreitung von Informationen √ºber Umweltzeichen erstellen wollte. </p><br><h3 id="chto-takoe-ekomarkirovka">  Was ist Umweltzeichen? </h3><br><p>  Das Umweltzeichen ist ein Zertifikat und ein entsprechendes Logo, das von Zertifizierungsorganisationen ausgestellt wurde, die die Produkte oder Dienstleistungen des Herstellers und Lieferanten auf √úbereinstimmung mit bestimmten Kriterien im Zusammenhang mit dem Lebenszyklus der Produktdienstleistung pr√ºfen und sich auf deren Umweltfreundlichkeit konzentrieren.  Nach der Zertifizierung kann der Hersteller das Umweltzeichen auf seinen Produkten anbringen. </p><br><p>  Das Umweltzeichen kann aufgrund seiner Zusammensetzung auch der Kunststoffkennzeichnung zugeordnet werden, um die Gr√∂√üenbestimmung und Verarbeitung sowie andere √§hnliche Zeichen zu vereinfachen. </p><br><p>  Hier ist zum Beispiel ein Zeichen: </p><br><p><img src="https://habrastorage.org/webt/_q/b6/od/_qb6odkao8hmg469ujzuqsjyjoc.png"></p><br><h3 id="process-vybora-tehnologii-raspoznavaniya">  Auswahlverfahren f√ºr Erkennungstechnologien </h3><br><p>  Die beiden Hauptmerkmale der Anwendung waren die Suche nach Gesch√§ften mit umweltfreundlichen Produkten und die Erkennung von Umweltzeichen.  Wenn technologisch alles bei der Suche nach Gesch√§ften relativ einfach ist, dann ist es bei der Erkennung nicht sehr einfach.  Das Wort ist in Mode, aber wie man es macht, war nicht klar.  Und ich fing an, das Thema zu studieren. </p><br><p>  Die Markierungslogos sind standardisiert und ideale Erkennungsobjekte - er richtete das Telefon auf das Bild auf der Warenverpackung, machte ein Foto und die Anwendung gibt an, welche Art von Zeichen es bedeutet und ob es vertrauensw√ºrdig sein sollte. </p><br><p>  Ich begann dar√ºber nachzudenken, wie man verschiedene Optionen erkennt und analysiert - ich habe OpenCV mit seinen Erkennungsalgorithmen (Haarkaskaden, SWIFT, Vorlagenabgleich usw.) ausprobiert, aber die Erkennungsqualit√§t war nicht sehr gut - nicht mehr als 70% mit einem Trainingssatz von mehreren zehn Bildern . </p><br><p>  Vielleicht habe ich irgendwo etwas falsch verstanden und etwas falsch gemacht, aber wir haben auch einen anderen Freund gebeten, dieses Thema zu untersuchen, und er sagte auch, dass 70% der Haar-Kaskaden das Maximum in einem solchen Datensatz sind. </p><br><p>  Parallel dazu tauchten immer h√§ufiger Materialien √ºber verschiedene neuronale Netze und den erfolgreichen Einsatz neuronaler Netze zur L√∂sung solcher Probleme auf.  Aber √ºberall blitzten einige erschreckende Gr√∂√üen von Datens√§tzen auf (Hunderte oder Tausende von Bildern f√ºr jede Klasse), die mir Python, TensorFlow und die Notwendigkeit meines Backends nicht vertraut waren - all dies war ein wenig be√§ngstigend. </p><br><p>  Als .NET-Entwickler habe ich mir Accord.NET angesehen, aber auch nicht schnell etwas gefunden, das sofort passt. </p><br><p>  Zu diesem Zeitpunkt waren wir damit besch√§ftigt, den Antrag fertigzustellen und im Produkt zu starten, und ich habe das Verfahren mit Anerkennung verschoben. </p><br><p>  Vor ungef√§hr einem Jahr stie√ü ich auf einen Artikel, in dem die fr√ºhe Vorschau von Microsoft Custom Vision, ein Cloud-Bildklassifizierungsdienst, beschrieben wurde.  Ich habe es an 3 Zeichen getestet und es hat mir gefallen - ein verst√§ndliches Portal, auf dem Sie den Klassifizierer ohne technisches Wissen trainieren und testen k√∂nnen. In 10 bis 20 Sekunden werden 100 Bilder trainiert. Die Qualit√§t der Klassifizierung liegt sogar bei 30 Bildern jedes Zeichens √ºber 90%. was ben√∂tigt wird. </p><br><p>  Ich teilte den Fund mit meiner Frau und wir begannen, eine weniger funktionierende internationale Version der Anwendung zu erstellen, die keine Informationen √ºber Waren und Gesch√§fte enth√§lt, aber Umweltzeichen erkennen kann. </p><br><p>  Kommen wir zu den technischen Details einer laufenden Erkennungsanwendung. </p><br><h3 id="custom-vision">  Kundenspezifische Vision </h3><br><p>  CV ist Teil von Cognitive Services in Azure.  Jetzt kann es offiziell ausgestellt werden und wird mit einem Azure-Abonnement bezahlt, obwohl es immer noch in der Vorschau aufgef√ºhrt ist. </p><br><p>  Dementsprechend werden CognitiveServices wie jedes andere Azure-Produkt im Azure-Portal angezeigt und verwaltet. </p><br><p>  CV bietet zwei REST-APIs, eine f√ºr das Training und eine f√ºr die Vorhersage.  Im Detail werde ich die Interaktion mit der Vorhersage weiter beschreiben </p><br><p>  Zus√§tzlich zum Azure-Portal und der API haben CV-Benutzer Zugriff auf das Portal customvision.ai, in dem sie sehr einfach und visuell Bilder hochladen, Markierungen darauf platzieren und Bilder und Erkennungsergebnisse anzeigen k√∂nnen, die √ºber die API √ºbertragen wurden. </p><br><p>  Das customvision.ai-Portal und die API k√∂nnen ohne Bindung an Azure gestartet werden. Zu Testzwecken wird ein Projekt auch ohne Azure-Abonnement erstellt.  Wenn Sie jedoch in Zukunft ein Produktionsprojekt aus Ihrem Testprojekt erstellen m√∂chten, ist es besser, dies sofort zu tun. Andernfalls mussten wir die Bilder aus dem Testprojekt manuell kopieren und in der Produktion neu markieren. </p><br><p>  Um ein Projekt in Azure zu erstellen, m√ºssen Sie sich dort registrieren und ein Abonnement erstellen.  Dies ist relativ einfach, Probleme k√∂nnen nur bei der Eingabe und Validierung von Daten von einer Kreditkarte auftreten - manchmal passiert dies. </p><br><p>  Nach der Registrierung m√ºssen Sie eine ComputerVision-Instanz √ºber das Azure-Portal erstellen </p><br><p><img src="https://habrastorage.org/webt/jk/cf/vv/jkcfvvmhkp80zpptg1lpfinwxzi.png"></p><br><p>  Nach dem Erstellen von Ressourcen in Azure sind diese in customvision.ai verf√ºgbar </p><br><p>  Auf dem Portal customvision.ai k√∂nnen Sie Bilder hochladen und mit Tags versehen. Ein Bild kann mehrere Tags enthalten, ohne jedoch Bereiche hervorzuheben.  Das hei√üt, das Bild geh√∂rt mehreren Klassen an, aber in dieser Phase der Entwicklung des Dienstes ist es unm√∂glich, ein bestimmtes Fragment im Bild auszuw√§hlen und es der Klasse zuzuweisen. </p><br><p>  Nach dem Markieren m√ºssen Sie das Training durch Dr√ºcken der Train-Taste starten. Das Training eines Modells mit 70 Tags und 3.000 Bildern dauert etwa 30 Sekunden. </p><br><p>  Die Trainingsergebnisse werden in der Iterationsentit√§t gespeichert.  Tats√§chlich implementiert Iteration die Versionierung. </p><br><p>  Jede Iteration kann unabh√§ngig verwendet werden. Das hei√üt, Sie k√∂nnen eine Iteration erstellen, das Ergebnis testen und l√∂schen, wenn es nicht passt oder in eine Standard√ºbersetzung √ºbersetzt wird, und die aktuelle Standarditeration ersetzen. Anschlie√üend wird die gesamte Erkennung von Anwendungen aus dieser Iteration in das Modell √ºbernommen. </p><br><p>  Die Qualit√§t des Modells wird in Form von Pr√§zision und R√ºckruf (weitere Details <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> ) sowohl f√ºr alle Klassen gleichzeitig als auch separat angezeigt. </p><br><p><img src="https://habrastorage.org/webt/j-/es/_m/j-es_mwi8onzwum9uc2a9k2p4y0.png"></p><br><p>  So sieht ein Projekt mit Bildern aus, die bereits hochgeladen und durch Schulungen geleitet wurden. </p><br><p><img src="https://habrastorage.org/webt/os/iq/pi/osiqpinmfbcxb8-zp8bunbr7lni.png"></p><br><p>  Auf dem Portal k√∂nnen Sie die Bilderkennung mithilfe des Schnelltests von einer Festplatte oder URL ausf√ºhren und die Erkennung anhand eines einzelnen Bildes testen. </p><br><p>  Auf der Registerkarte Vorhersagen k√∂nnen Sie die Ergebnisse aller neuesten Erkennungen anzeigen. Die Prozents√§tze der Markierungen werden direkt im Bild angezeigt. </p><br><p><img src="https://habrastorage.org/webt/zy/-f/-k/zy-f-kqcj7v4npxr4adnakn0724.png"></p><br><p>  Die M√∂glichkeit, alle Erkennungsergebnisse mit nur wenigen Mausklicks anzuzeigen und zum Trainingssatz hinzuzuf√ºgen, hilft sehr - jeder kann dies ohne KI- oder Programmierkenntnisse tun. </p><br><h3 id="ispolzovanie-api">  API-Nutzung </h3><br><p>  Custom Vision Service verf√ºgt √ºber eine sehr einfache und intuitive REST-API f√ºr Schulung und Erkennung. </p><br><p>  Unsere Anwendung verwendet nur die Erkennungs-API und ich werde √ºber ihre Verwendung sprechen </p><br><p>  URL f√ºr die Anerkennung dieser Art: </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://southcentralus.api.cognitive.microsoft.com/customvision/v2.0/Prediction/{Your</a> project GUID} / image </p><br><p>  wo <br>  <strong>southcentralus **</strong> - Der Name der Azure-Region, in der sich der Dienst befindet.  Bisher ist der Service nur in der Region South Central US verf√ºgbar.  Dies bedeutet nicht, dass Sie es nur dort verwenden k√∂nnen!  Er lebt einfach dort - Sie k√∂nnen es von jedem Ort aus nutzen, an dem es Internet gibt. <br>  <strong>{Ihre Projekt-GUID} **</strong> - Kennung Ihres Projekts.  Sie k√∂nnen es auf dem Portal customvision.ai sehen </p><br><p>  Zur Erkennung muss das Bild per POST gesendet werden.  Sie k√∂nnen auch eine √∂ffentlich zug√§ngliche Bild-URL senden, die der Dienst selbst herunterl√§dt. </p><br><p>  Dar√ºber hinaus m√ºssen Sie den Headern, in die Sie einen der bei der Registrierung ausgestellten Zugriffsschl√ºssel √ºbertragen k√∂nnen, den Header "Prediction-Key" hinzuf√ºgen. Diese sind sowohl im customvision.ai-Portal als auch im Azure-Portal verf√ºgbar. </p><br><p>  Das Ergebnis enth√§lt das folgende Feld: </p><br><pre><code class="hljs powershell"><span class="hljs-string"><span class="hljs-string">"Predictions"</span></span>:[ {<span class="hljs-string"><span class="hljs-string">"TagId"</span></span>:<span class="hljs-string"><span class="hljs-string">"35ac2ad0-e3ef-4e60-b81f-052a1057a1ca"</span></span>,<span class="hljs-string"><span class="hljs-string">"Tag"</span></span>:<span class="hljs-string"><span class="hljs-string">"dog"</span></span>,<span class="hljs-string"><span class="hljs-string">"Probability"</span></span>:<span class="hljs-number"><span class="hljs-number">0.102716163</span></span>}, {<span class="hljs-string"><span class="hljs-string">"TagId"</span></span>:<span class="hljs-string"><span class="hljs-string">"28e1a872-3776-434c-8cf0-b612dd1a953c"</span></span>,<span class="hljs-string"><span class="hljs-string">"Tag"</span></span>:<span class="hljs-string"><span class="hljs-string">"cat"</span></span>,<span class="hljs-string"><span class="hljs-string">"Probability"</span></span>:<span class="hljs-number"><span class="hljs-number">0.02037274</span></span>} ]</code> </pre> <br><p>  Wobei Wahrscheinlichkeit die Wahrscheinlichkeit angibt, dass das Bild zum angegebenen Tag (Klasse) geh√∂rt. </p><br><p>  In C # sieht es so aus </p><br><pre> <code class="cs hljs"> <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> client = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> HttpClient(); client.DefaultRequestHeaders.Add(<span class="hljs-string"><span class="hljs-string">"Prediction-Key"</span></span>, <span class="hljs-string"><span class="hljs-string">"{Acess key}"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">string</span></span> url = <span class="hljs-string"><span class="hljs-string">"https://southcentralus.api.cognitive.microsoft.com/customvision/v2.0/Prediction/{Your project GUID}/image"</span></span>; HttpResponseMessage response; List&lt;RecognitionResult&gt; recognitions = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> List&lt;RecognitionResult&gt;(); <span class="hljs-keyword"><span class="hljs-keyword">using</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">var</span></span> content = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ByteArrayContent(imageBytes)) { content.Headers.ContentType = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> MediaTypeHeaderValue (<span class="hljs-string"><span class="hljs-string">"application/octet-stream"</span></span>); response = <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> client.PostAsync(url, content); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (response.IsSuccessStatusCode) { <span class="hljs-keyword"><span class="hljs-keyword">string</span></span> strRes = <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> response.Content.ReadAsStringAsync(); <span class="hljs-keyword"><span class="hljs-keyword">dynamic</span></span> res = (<span class="hljs-keyword"><span class="hljs-keyword">dynamic</span></span>) JsonConvert.DeserializeObject(strRes); <span class="hljs-keyword"><span class="hljs-keyword">foreach</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">var</span></span> pr <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> res.predictions) { recognitions.Add( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> RecognitionResult() { Tag = pr.tagName, RecognPercent = pr.probability }); } } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { Debug.WriteLine( <span class="hljs-string"><span class="hljs-string">"Non successful response. "</span></span> + response.ToString()); } }</code> </pre> <br><p>  Wie Sie sehen k√∂nnen - absolut nichts kompliziertes.  Die ganze Magie geschieht auf der Serviceseite. </p><br><h3 id="prilozhenie-i-nekotorye-podobrannye-parametry">  Die Anwendung und einige ausgew√§hlte Parameter. </h3><br><p>  Die Anwendung ist recht einfach und besteht aus einer Liste von Umweltzeichen, Informationen dar√ºber, was Umweltzeichen sind, wie sie unterteilt sind und dem Scanner selbst. </p><br><p>  Der Hauptteil ist in Xamarin.Forms geschrieben, aber das Scannerfenster funktioniert mit der Kamera und musste als Rendering ausgef√ºhrt und f√ºr jede Plattform separat implementiert werden </p><br><p>  Der Grad, in dem die Anwendung entscheidet, dass das Umweltzeichen genau&gt; = 90% erkannt wird, w√§hrend fast alle Bilder erkannt werden, wenn sie mehr oder weniger von akzeptabler Qualit√§t sind und keine anderen Zeichen im Bild vorhanden sind. <br>  Diese Zahl wurde empirisch abgeleitet - wir haben mit 80 begonnen, aber festgestellt, dass 90 falsch positive Ergebnisse reduziert.  Und es gibt ziemlich viele davon - viele Markierungen sind √§hnlich und enthalten √§hnliche Elemente, und das Farbschema wird auf Gr√ºn verschoben. </p><br><p>  Dies ist beispielsweise nicht das Bild mit der h√∂chsten Qualit√§t, das mit einer Genauigkeit von 91% korrekt erkannt wurde </p><br><p><img src="https://habrastorage.org/webt/uc/ru/cp/ucrucpzt4yiudf1ooaco4t7aska.jpeg"></p><br><p>  B gleichzeitig wurde diese Klasse an 45 Bildern trainiert. </p><br><p>  Ich hoffe, der Artikel war n√ºtzlich und erm√∂glicht interessierten Lesern, einen Blick auf die neuen AI- und ML-Tools zu werfen. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de424379/">https://habr.com/ru/post/de424379/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de424369/index.html">Meine Lieblingsdatei in der Chromium-Codebasis</a></li>
<li><a href="../de424371/index.html">Stellen Sie vCloud Extender bereit</a></li>
<li><a href="../de424373/index.html">Wo in der IT arbeiten, Problem 1: Voximplant</a></li>
<li><a href="../de424375/index.html">Mayku FormBox Vacuum Moulder Review: Teile sich ausbreiten lassen</a></li>
<li><a href="../de424377/index.html">Playme TIO-Test: DVR mit Magnethalterung der Spitzenklasse</a></li>
<li><a href="../de424381/index.html">Game Server Hosting in einem professionellen Rechenzentrum</a></li>
<li><a href="../de424383/index.html">Eine vollst√§ndige Anleitung zur ordnungsgem√§√üen Verwendung von Animationen in UX</a></li>
<li><a href="../de424385/index.html">DJI GO 4 Ultimate Guide: Einstellungen f√ºr Startbildschirm und Kamera</a></li>
<li><a href="../de424387/index.html">Wir laden Sie zum GO.PITER-Meeting ein</a></li>
<li><a href="../de424389/index.html">Neue Wissenschaft, um die Ecke zu schauen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>