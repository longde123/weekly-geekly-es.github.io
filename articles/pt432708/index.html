<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üïì üåÑ ‚õπüèº SFUs em cascata: aprimorando a escalabilidade e a qualidade da m√≠dia em aplicativos WebRTC ‚ú¥Ô∏è üëêüèæ üëäüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="H√° duas dificuldades na implanta√ß√£o de servidores de m√≠dia para o WebRTC: dimensionamento, ou seja, indo al√©m do uso de um servidor e otimizando atras...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>SFUs em cascata: aprimorando a escalabilidade e a qualidade da m√≠dia em aplicativos WebRTC</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/Voximplant/blog/432708/">  H√° duas dificuldades na implanta√ß√£o de servidores de m√≠dia para o WebRTC: dimensionamento, ou seja,  indo al√©m do uso de um servidor e otimizando atrasos para todos os usu√°rios da confer√™ncia.  Embora o sharding simples, no esp√≠rito de "enviar todos os usu√°rios da confer√™ncia X para o servidor Y", seja facilmente escal√°vel horizontalmente, est√° longe de ser o ideal em termos de atrasos.  Distribuir a confer√™ncia entre servidores n√£o apenas pr√≥ximos dos usu√°rios, mas tamb√©m interconectados - parece uma solu√ß√£o para ambos os problemas.  Hoje, preparamos uma tradu√ß√£o detalhada de Boris Grozev da Jitsi: problemas de SFUs em cascata, com uma descri√ß√£o da abordagem e algumas dificuldades, al√©m de detalhes de implementa√ß√£o.  Vale dizer que as confer√™ncias Voximplant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tamb√©m usam SFU</a> ;  Atualmente, estamos trabalhando na cascata do SFU, que deve aparecer em nossa plataforma no pr√≥ximo ano. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yg/nu/on/ygnuonxaklzagcy26fi8ga3hj1u.jpeg"></div><br>  <font color="gray">Neur√¥nios do mouse.</font>  <font color="gray">Imagem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">NIHD</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CC-BY-2.0</a> )</font> <br><a name="habracut"></a><br>  As comunica√ß√µes em tempo real s√£o muito sens√≠veis √† rede: largura de banda, lat√™ncia e perda de pacotes.  Uma diminui√ß√£o na taxa de bits leva a uma diminui√ß√£o na qualidade do v√≠deo, um longo atraso na rede leva a um longo atraso para os usu√°rios finais.  A perda de pacotes pode tornar o som intermitente e levar a congelamentos no v√≠deo (devido a pulos de quadros). <br><br>  Portanto, √© muito importante para a confer√™ncia escolher a rota ideal entre os dispositivos / usu√°rios finais.  Quando h√° apenas dois usu√°rios, √© f√°cil: o WebRTC usa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">o protocolo ICE</a> para estabelecer uma conex√£o entre os participantes.  Se poss√≠vel, os participantes se conectam diretamente, caso contr√°rio, um servidor TURN √© usado.  O WebRTC pode resolver um nome de dom√≠nio para obter o endere√ßo de um servidor TURN, para que voc√™ possa selecionar facilmente um TURN local com base no DNS, por exemplo, usando as propriedades do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">AWS Route53</a> . <br><br>  No entanto, quando o roteamento de v√°rios participantes ocorre por meio de um servidor de m√≠dia central, a situa√ß√£o se torna complicada.  Muitos servi√ßos WebRTC usam SFUs (Selective Forwarding Units) para transferir √°udio e v√≠deo entre 3 ou mais participantes com mais efici√™ncia. <br><br><h2>  Problema com uma estrela </h2><br>  Na topologia em estrela, todos os participantes se conectam a um √∫nico servidor atrav√©s do qual trocam fluxos de m√≠dia.  Obviamente, a escolha do local do servidor √© de grande import√¢ncia: se todos os participantes estiverem nos EUA, usar um servidor em Sydney n√£o √© uma boa ideia. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ql/kc/om/qlkcomjzkd6g2ouz09pkdzhlvpw.png"></div><br>  Muitos servi√ßos usam uma abordagem simples que funciona bem na maioria dos casos: eles escolhem um servidor mais pr√≥ximo do primeiro participante da confer√™ncia.  No entanto, h√° momentos em que essa solu√ß√£o n√£o √© √≥tima.  Imagine que temos tr√™s participantes da figura acima.  Se um australiano (chamador C) for o primeiro a ingressar na confer√™ncia, o algoritmo escolher√° um servidor na Austr√°lia; no entanto, o servidor 1 nos EUA ser√° a melhor escolha, porque  ele est√° mais perto da maioria dos participantes. <br><br>  O cen√°rio descrito n√£o √© muito frequente, mas ocorre.  Se assumirmos que o usu√°rio est√° conectado em uma ordem aleat√≥ria, a situa√ß√£o descrita ocorre com ‚Öì de todas as confer√™ncias com 3 participantes, um dos quais √© muito exclu√≠do. <br><br>  Outro cen√°rio e mais frequente: temos dois grupos de participantes em locais diferentes.  Nesse caso, a ordem de conex√£o n√£o √© importante; sempre teremos um grupo de participantes pr√≥ximos que s√£o for√ßados a trocar m√≠dia com um servidor remoto.  Por exemplo, 2 participantes da Austr√°lia (C&amp;D) e 2 dos EUA (A&amp;B). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uh/t9/me/uht9mexxe1vpojj608kyvf1ccb8.png"></div><br>  Mudar para o Servidor 1 n√£o ser√° ideal para membros de C&amp;D.  O servidor 2 n√£o √© ideal para A&amp;B.  Ou seja, n√£o importa qual servidor seja usado, sempre haver√° participantes conectados ao servidor remoto (= n√£o otimizado). <br><br>  Mas se n√£o tiv√©ssemos um √∫nico limite de servidor?  Poder√≠amos conectar cada participante ao servidor mais pr√≥ximo, restaria apenas conectar esses servidores. <br><br><h2>  Solu√ß√£o: Em cascata </h2><br>  Adiamos a quest√£o de como conectar os servidores;  vamos primeiro ver qual ser√° o efeito. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qc/lk/nj/qclknjbtsc5nkj5sp1evoqavla8.png"></div><br>  A conex√£o SFU entre C e D n√£o mudou - o Servidor 2. ainda √© usado, o Servidor 1 √© usado para os participantes A e B, e isso √© obviamente melhor.  O mais interessante √© a conex√£o entre, por exemplo, A e C: em vez de A &lt;=&gt; Servidor 2 &lt;=&gt; C, a rota A &lt;=&gt; Servidor 1 &lt;=&gt; Servidor 2 &lt;=&gt; C √© usada. <br><br><h2>  Efeito impl√≠cito na taxa de c√¢mbio </h2><br>  O mix SFU tem seus pr√≥s e contras.  Por um lado, na situa√ß√£o descrita, o tempo de troca entre os participantes se torna mais longo quando novos saltos na rede s√£o adicionados.  Por outro lado, h√° uma diminui√ß√£o nesse momento quando falamos sobre a conex√£o ‚Äúcliente‚Äù - ‚Äúprimeiro servidor‚Äù, porque podemos restaurar o fluxo de m√≠dia com um atraso menor pelo princ√≠pio do salto por salto. <br><br>  Como isso funciona?  O WebRTC usa RTP (geralmente sobre UDP) para transmitir m√≠dia.  Isso significa que o transporte n√£o √© confi√°vel.  Quando um pacote UDP √© perdido, voc√™ pode ignorar a perda ou solicitar um reenvio (retransmiss√£o) usando o pacote <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">RTCP NACK</a> - a op√ß√£o j√° est√° na consci√™ncia do aplicativo.  Por exemplo, um aplicativo pode ignorar a perda de pacotes de √°udio e solicitar a retransmiss√£o de alguns (mas n√£o todos) pacotes de v√≠deo, dependendo se eles s√£o necess√°rios para decodificar os quadros subseq√ºentes ou n√£o. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ia/ly/oi/ialyoi-naszu3kclgng8r6i6kx4.png"></div><br>  <font color="gray">Retransmiss√£o de pacotes RTP, servidor √∫nico</font> <br><br>  Quando h√° cascata, a retransmiss√£o pode ser limitada ao servidor local, ou seja, realizado em cada site individual.  Por exemplo, na rota A-S1-S2-C, se um pacote for perdido entre A e S1, ent√£o S1 notar√° isso e solicitar√° uma retransmiss√£o;  semelhante √† perda entre S2 e C. E mesmo se o pacote for perdido entre servidores, o lado receptor tamb√©m poder√° solicitar uma retransmiss√£o. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/si/zs/ak/sizsaktt0nq72ymcfvkkojwfjq0.png"></div><br>  <font color="gray">Retransmiss√£o de pacotes RTP, dois servidores.</font>  <font color="gray">Observe que o servidor 2 n√£o solicita o pacote 2, porque o NACK chegou logo ap√≥s o envio do pacote.</font> <br><br>  O cliente usa um buffer de tremula√ß√£o para atrasar a reprodu√ß√£o do v√≠deo e conseguir receber pacotes atrasados ‚Äã‚Äã/ retransmitidos.  O tamanho do buffer muda dinamicamente, dependendo do tempo de troca entre as partes.  Quando ocorrem retransmiss√µes de salto por salto, o atraso diminui e, como resultado, o buffer pode ser menor - como resultado, o atraso geral tamb√©m diminui. <br><br>  Resumidamente: mesmo que o tempo de troca entre os participantes seja maior, isso pode levar a uma diminui√ß√£o no atraso na transfer√™ncia de m√≠dia entre os participantes.  Ainda temos que estudar esse efeito na pr√°tica. <br><br><h2>  Apresentando SFUs em cascata: Jitsi Meet Case </h2><br><h3>  Alarme vs.  M√≠dia </h3><br>  Vamos dar uma olhada no alarme.  Desde o in√≠cio, o Jitsi Meet compartilhou o conceito de servidor de sinaliza√ß√£o ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Jicofo</a> ) e servidor de m√≠dia / SFU.  Isso permitiu a introdu√ß√£o de suporte em cascata √© relativamente simples.  Primeiro, poder√≠amos lidar com toda a l√≥gica de sinaliza√ß√£o em um s√≥ lugar;  segundo, j√° t√≠nhamos um protocolo de sinaliza√ß√£o entre Jicofo e o servidor de m√≠dia.  Precis√°vamos apenas expandir um pouco a funcionalidade: j√° suport√°vamos v√°rias SFUs conectadas a um servidor de sinaliza√ß√£o, tivemos que adicionar a capacidade de uma SFU para conectar-se a muitos servidores de sinaliza√ß√£o. <br><br>  Como resultado, dois pools de servidores independentes apareceram: um para inst√¢ncias jicofo e outro para inst√¢ncias de servidor de m√≠dia, consulte o diagrama: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/zu/vm/j-/zuvmj-aptvmxeaz_-_2dpy0gnxs.png"></div><br>  <font color="gray">Um exemplo de organiza√ß√£o de servidores na AWS com a possibilidade de uma cascata entre diferentes data centers.</font> <br><br>  A segunda parte do sistema √© a comunica√ß√£o ponte a ponte.  Quer√≠amos tornar essa parte o mais simples poss√≠vel, para que n√£o haja sinaliza√ß√£o complicada entre as pontes.  Todo alarme varia entre jicofo e jitsi-videobridge;  A conex√£o de ponte √© usada apenas para mensagens de √°udio / v√≠deo e link de dados. <br><br><h3>  Protocolo Octo </h3><br>  Para gerenciar essa intera√ß√£o, adotamos o protocolo Octo, que agrupa pacotes RTP em cabe√ßalhos simples de tamanho fixo e tamb√©m permite enviar mensagens de texto.  Na implementa√ß√£o atual, as pontes s√£o conectadas por uma topologia de malha completa (malha completa), mas outras topologias tamb√©m s√£o poss√≠veis.  Por exemplo, use um servidor central (estrela para pontes) ou uma estrutura em √°rvore para cada ponte. <br><br>  <i>Explica√ß√£o: Em vez de agrup√°-lo em um cabe√ßalho Octo, voc√™ pode usar a extens√£o de cabe√ßalho RTP, que far√° fluxos entre pontes no RTP puro (S).</i>  <i>Vers√µes futuras do Octo podem usar essa abordagem.</i> <i><br><br></i>  <i>Segunda Explica√ß√£o: Octo n√£o significa nada.</i>  <i>Inicialmente, quer√≠amos usar um servidor central e isso nos lembrou um polvo.</i>  <i>Ent√£o, o nome do projeto apareceu.</i> <i><br></i> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xt/vv/ln/xtvvlnyyumqkz65qlvsxweuss94.png"></div>  <font color="gray">Octo Header Format</font> <br><br>  Na terminologia do Jitsi, quando uma ponte faz parte de uma confer√™ncia com v√°rias pontes, ela possui um canal Octo adicional (de fato, um canal para √°udio e outro para v√≠deo).  Este canal √© respons√°vel por enviar / receber m√≠dia de / para outras pontes.  Cada ponte possui uma porta livre para Octo (4096 por padr√£o), portanto, precisamos do campo ID da confer√™ncia para lidar com v√°rias confer√™ncias. <br><br>  No momento, o protocolo n√£o possui mecanismos de seguran√ßa internos e delegamos essa responsabilidade nos n√≠veis mais baixos.  Essa √© a coisa mais pr√≥xima que faremos no futuro pr√≥ximo, mas por enquanto as pontes devem estar em uma rede segura (por exemplo, uma inst√¢ncia separada do AWS VPC). <br><br><h3>  Simulcast </h3><br>  O Simulcast permite que cada participante envie v√°rios fluxos de m√≠dia com taxas de bits diferentes, enquanto a ponte ajuda a determinar quais s√£o necess√°rias.  Para que isso funcione corretamente, transferimos todos os fluxos de simulcast entre as pontes.  Gra√ßas a isso, voc√™ pode alternar rapidamente entre fluxos, porque a ponte local n√£o precisa solicitar um novo fluxo.  Entretanto, isso n√£o √© ideal do ponto de vista do tr√°fego ponte a ponte, pois  alguns threads raramente s√£o usados ‚Äã‚Äãe carregam apenas a largura de banda sem nenhum objetivo. <br><br><h3>  Sele√ß√£o de membro ativo </h3><br>  Tamb√©m quer√≠amos a oportunidade de assinar um participante / orador ativo da confer√™ncia.  Acabou sendo simples - ensinamos cada ponte a determinar independentemente o participante principal e, em seguida, notificamos nossos clientes locais.  Isso significa que a determina√ß√£o ocorre v√°rias vezes, mas n√£o √© onerosa e permite simplificar alguns pontos (por exemplo, voc√™ n√£o precisa decidir qual ponte deve ser respons√°vel pelo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">DSI</a> e se preocupar com o roteamento de mensagens). <br><br><h3>  Sele√ß√£o de ponte </h3><br>  Na implementa√ß√£o atual, esse algoritmo √© simples.  Quando um novo participante ingressa na confer√™ncia, Jicofo deve determinar qual ponte atribuir a ele.  Isso √© feito com base na regi√£o do participante e no congestionamento das pontes.  Se na mesma regi√£o houver uma ponte livre, ela ser√° designada.  Caso contr√°rio, alguma outra ponte ser√° usada. <br><br>  Para mais informa√ß√µes sobre o Octo, consulte a <a href="">documenta√ß√£o</a> . <br><br><h2>  Expandir o SFU em cascata </h2><br>  Para implanta√ß√£o, usamos m√°quinas no Amazon AWS.  T√≠nhamos servidores (alarmes e m√≠dia) em 6 regi√µes: <br><br><ul><li>  us-east-1 (Virg√≠nia do Norte); </li><li>  us-west-2 (Oregon); </li><li>  eu-oeste-1 (Irlanda); </li><li>  eu-central-1 (Frankfurt); </li><li>  ap-se-1 (Singapura); </li><li>  ap-se-2 (Sydney). </li></ul><br>  Usamos inst√¢ncias <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">HAProxy</a> georreferenciadas para determinar a regi√£o membro.  O dom√≠nio meet.jit.si √© gerenciado pelo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Route53</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">resolve</a> a inst√¢ncia HAProxy, que adiciona a regi√£o aos cabe√ßalhos HTTP da solicita√ß√£o enviada.  Posteriormente, o cabe√ßalho √© usado como o valor da vari√°vel <code>config.deploymentInfo.userRegion</code> , dispon√≠vel no cliente, gra√ßas ao arquivo <code>/config.js</code> . <br><br>  A interface jitsi mostra quantas pontes s√£o usadas e a quais usu√°rios espec√≠ficos est√£o conectados - para fins de diagn√≥stico e demonstra√ß√£o.  Passar o mouse sobre o canto superior esquerdo do v√≠deo local mostrar√° o n√∫mero total de servidores e o servidor ao qual voc√™ est√° conectado.  Da mesma forma, voc√™ pode ver os par√¢metros do segundo participante.  Voc√™ tamb√©m ver√° o tempo de troca entre o navegador e o navegador do interlocutor (par√¢metro E2E RTT). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/up/hs/r3/uphsr3-ddqg6cjkhcu6sgis1g9s.png"></div><br>  <font color="gray">Ao ver quem est√° conectado a qual servidor, voc√™ pode ver se a cascata √© usada.</font> <br><br><h2>  Conclus√£o </h2><br>  Octo apareceu originalmente como um teste A / B.  Os primeiros resultados foram bons, agora o Octo est√° dispon√≠vel para todos.  Ainda h√° muito tr√°fego para passar por ele e uma an√°lise mais detalhada do desempenho;  tamb√©m est√° planejado usar esses desenvolvimentos para apoiar confer√™ncias ainda maiores (quando um SFU n√£o √© mais suficiente). </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt432708/">https://habr.com/ru/post/pt432708/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt432698/index.html">Timlids fant√°sticos e onde eles moram</a></li>
<li><a href="../pt432700/index.html">Venha! @ # Com sua "toxicidade"</a></li>
<li><a href="../pt432702/index.html">Conte√∫do 2018: olhe mais amplo, v√° mais fundo</a></li>
<li><a href="../pt432704/index.html">Usando um term√¥metro sem fio externo Buro H999 com dispositivos caseiros</a></li>
<li><a href="../pt432706/index.html">Equipe um para muitos no sucesso do cliente: por que voc√™ precisa?</a></li>
<li><a href="../pt432710/index.html">Chamadas de spam. √â poss√≠vel combat√™-los?</a></li>
<li><a href="../pt432714/index.html">Semana da Seguran√ßa 50: 2019 Previs√µes</a></li>
<li><a href="../pt432716/index.html">Traili. Gpuhub. Cybercortex</a></li>
<li><a href="../pt432718/index.html">Controlador, v√° com calma! Retiramos o c√≥digo no UIView</a></li>
<li><a href="../pt432720/index.html">China baniu a Apple</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>