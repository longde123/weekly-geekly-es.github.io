<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üçü üéÖ üóÉÔ∏è Migration de Nagios vers Icinga2 en Australie ‚ûø ‚ùî üçæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour √† tous. 


 Je suis l'administrateur syst√®me Linux, j'ai d√©m√©nag√© de Russie en Australie avec un visa professionnel ind√©pendant en 2015, mais ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Migration de Nagios vers Icinga2 en Australie</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/444060/"><p>  Bonjour √† tous. </p><br><p>  Je suis l'administrateur syst√®me Linux, j'ai d√©m√©nag√© de Russie en Australie avec un visa professionnel ind√©pendant en 2015, mais l'article ne portera pas sur la fa√ßon d'obtenir un porcelet sur un tracteur.  De tels articles sont d√©j√† suffisants (n√©anmoins, s'il y a un int√©r√™t, j'√©crirai √† ce sujet √©galement), donc je voudrais parler de la fa√ßon dont, dans mon travail en Australie en tant qu'ing√©nieur Linux, j'ai initi√© la migration √† partir d'un syst√®me surveillance √† l'autre.  Plus pr√©cis√©ment - Nagios =&gt; Icinga2. </p><br><p>  L'article est en partie technique et en partie sur la communication avec les gens et les probl√®mes li√©s √† la diff√©rence de culture et de m√©thodes de travail. </p><a name="habracut"></a><br><p>  Malheureusement, la balise "code" ne met pas en √©vidence le code Puppet et yaml, j'ai donc d√ª utiliser du "texte en clair". </p><br><p>  Rien de mauvais augure le matin du 21 d√©cembre 2016.  Comme d'habitude, j'ai lu Habr avec un anonymus non enregistr√© dans la premi√®re demi-heure de la journ√©e de travail, absorbant le caf√© et suis tomb√© sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cet article</a> . </p><br><p>  Depuis que Nagios a √©t√© utilis√© dans mon entreprise, sans y r√©fl√©chir √† deux fois, j'ai cr√©√© un ticket dans Redmine et jet√© le lien dans le chat g√©n√©ral, parce que je pensais que c'√©tait important.  L'initiative est punissable m√™me en Australie, donc l'ing√©nieur en chef m'a accroch√© ce probl√®me depuis que je l'ai d√©couvert. </p><br><div class="spoiler">  <b class="spoiler_title">√âcran de Redmine</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/ge/uc/gu/geucgutwhu3_zu4uwes2ozuwa3e.jpeg"></div></div><br><p>  Dans notre d√©partement, avant d'exprimer notre avis, il est de coutume de proposer au moins une alternative, m√™me si le choix est √©vident, j'ai donc commenc√© par googler quels syst√®mes de monitoring en g√©n√©ral sont actuellement pertinents, car en Russie au dernier endroit o√π j'ai travaill√©, j'avais mon propre syst√®me d'enregistrement personnel, tr√®s primitif, mais n√©anmoins assez fonctionnel et remplissant toutes les t√¢ches qui lui sont assign√©es.  Python, Polytechnique de Saint-P√©tersbourg et la r√®gle du m√©tro.  Non, le m√©tro craint.  C'est personnel (11 ans de travail) et digne d'un article s√©par√©, mais pas maintenant. </p><br><p>  Un peu sur les r√®gles pour apporter des modifications √† la configuration de l'infrastructure √† mon emplacement actuel.  Nous utilisons Puppet, Gitlab et le principe de l'infrastructure en tant que code, de sorte que: </p><br><ul><li>  Aucune modification manuelle via SSH en modifiant manuellement les fichiers sur les machines virtuelles.  Pendant trois ans de travail, j'ai re√ßu un chapeau pour cela plusieurs fois, la derni√®re il y a une semaine et je ne pense pas que ce soit la derni√®re fois.  Eh bien, en fait - corrigez une ligne dans la configuration, red√©marrez le service et voyez si le probl√®me a √©t√© r√©solu - 10 secondes.  Cr√©ez une nouvelle branche dans Gitlab, appuyez sur les modifications, attendez que r10k fonctionne sur Puppetmaster, ex√©cutez Puppet --environment = mybranch et attendez quelques minutes de plus jusqu'√† ce que tout fonctionne - 5 minutes minimum. </li><li>  Toutes les modifications sont apport√©es en cr√©ant une demande de fusion dans Gitlab et vous devez obtenir l'approbation d'au moins un membre de l'√©quipe.  Les changements majeurs apport√©s au chef d'√©quipe n√©cessitent deux ou trois approbations. </li><li>  Toutes les modifications sont textuelles d'une mani√®re ou d'une autre (puisque les manifestes Puppet, les scripts et les donn√©es Hiera sont du texte), les binaires sont fortement d√©conseill√©s et de bonnes raisons sont n√©cessaires pour les approuver. </li></ul><br><p>  Ainsi, les options que j'ai examin√©es sont: </p><br><ul><li>  Munin - s'il y a plus de 10 serveurs dans l'infrastructure, l'administration se transforme en enfer (√† partir de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cet article</a> . Je n'avais pas beaucoup envie de v√©rifier cela, alors j'ai pris ma parole). </li><li>  Zabbix - a longtemps regard√©, de retour en Russie, mais c'√©tait redondant pour mes t√¢ches.  Ici - a d√ª √™tre abandonn√© en raison de l'utilisation de Puppet comme gestionnaire de configuration et de Gitlab comme syst√®me de contr√¥le de version.  √Ä ce moment, si je comprends bien, Zabbix stocke la configuration enti√®re dans une base de donn√©es, et donc il n'√©tait pas clair comment g√©rer la configuration dans les conditions actuelles et comment suivre les changements. </li><li>  Prom√©th√©e est ce √† quoi nous arriverons √† la fin, √† en juger par l'humeur du d√©partement, mais √† l'√©poque je ne pouvais pas le ma√Ætriser et je ne pouvais pas d√©montrer un √©chantillon vraiment fonctionnel (Proof of Concept), donc j'ai d√ª refuser. </li><li>  Il y avait plusieurs autres options qui n√©cessitaient une refonte compl√®te du syst√®me, ou √©taient √† leurs balbutiements / abandonn√©es et pour la m√™me raison ont √©t√© rejet√©es. </li></ul><br><p>  Au final, j'ai opt√© pour Icinga2 pour trois raisons: </p><br><p>  1 - compatibilit√© avec Nrpe (un service client qui ex√©cute des v√©rifications sur les commandes de Nagios).  C'√©tait tr√®s important, car √† cette √©poque, nous avions 135 (maintenant il y en a 165 en 2019) des machines virtuelles avec un tas de services / contr√¥les auto-√©crits et refaire tout cela serait une terrible h√©morro√Ødes. <br>  2 - tous les fichiers de configuration sont du texte, ce qui facilite la modification de ce sujet, la cr√©ation de demandes de fusion avec la possibilit√© de voir ce qui a √©t√© ajout√© ou supprim√©. <br>  3 est un projet OpenSource vivant et en pleine croissance.  Nous aimons beaucoup OpenSource et y apportons une contribution possible en cr√©ant des demandes et des probl√®mes d'extraction pour r√©soudre les probl√®mes. </p><br><p>  Alors allons-y, Icinga2. </p><br><p>  La premi√®re chose √† laquelle j'ai d√ª faire face a √©t√© l'inertie de mes coll√®gues.  Tout le monde est habitu√© √† Nagios / Najios (bien que m√™me ici, ils ne pouvaient pas faire de compromis sur la fa√ßon de prononcer cela) et √† l'interface CheckMK.  L'interface icinga est compl√®tement diff√©rente (c'√©tait un inconv√©nient), mais il est possible de configurer de mani√®re flexible ce que vous devez voir avec les filtres litt√©ralement par n'importe quel param√®tre (c'√©tait un plus, mais je me suis battu pour cela notamment). </p><br><div class="spoiler">  <b class="spoiler_title">Filtres</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/kx/bf/0e/kxbf0eqj-gh7bfkpgtzgjjgvhwm.jpeg"></div></div><br><p>  Estimez le rapport entre la taille de la barre de d√©filement et la taille du champ de d√©filement. </p><br><p>  Deuxi√®mement, tout le monde a l'habitude de voir toute l'infrastructure sur un seul moniteur, car CheckMk vous permet de travailler avec plusieurs h√¥tes Nagios, mais Icinga ne savait pas comment le faire (en fait, il l'a fait, mais plus √† ce sujet ci-dessous).  Une alternative √©tait une chose appel√©e Thruk, mais sa conception a fait vomir tous les membres de l'√©quipe, sauf un - celui qui l'a propos√© (pas moi). </p><br><div class="spoiler">  <b class="spoiler_title">Thruk Firebox - D√©cision unanime de l'√©quipe</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/tv/1c/_l/tv1c_lkx-h9rbwgpdy63jdohnve.png"></div></div><br><p>  Apr√®s quelques jours d'une temp√™te de cerveaux, j'ai propos√© l'id√©e de la surveillance du cluster, quand il y a un h√¥te ma√Ætre dans la zone de production et deux subordonn√©s - un en dev / test et un h√¥te externe situ√© chez un autre fournisseur afin de surveiller nos services du point de vue d'un client ou d'un √©tranger observateur.  Cette configuration m'a permis de voir tous les probl√®mes dans une seule interface Web et cela a fonctionn√© pour moi, mais Puppet ... Le probl√®me avec Puppet √©tait que l'h√¥te ma√Ætre devait maintenant conna√Ætre tous les h√¥tes et services / v√©rifications du syst√®me et devait les r√©partir entre les zones (dev-test, staging-prod, ext), mais l'envoi des modifications via l'API Icinga prend quelques secondes, mais la compilation du r√©pertoire Puppet de tous les services pour tous les h√¥tes prend quelques minutes.  C'est toujours de ma faute, m√™me si j'ai d√©j√† expliqu√© √† plusieurs reprises comment tout fonctionne et pourquoi tout cela prend autant de temps. </p><br><p>  Troisi√®mement - un tas de SnowFlakes (flocons de neige) - des choses qui sont √©limin√©es du syst√®me g√©n√©ral, car elles ont quelque chose de sp√©cial, donc les r√®gles g√©n√©rales ne s'appliquent pas √† elles.  Cela a √©t√© d√©cid√© par une attaque frontale - s'il y a une alarme, mais en fait tout est en ordre, alors ici, vous devez creuser plus profond√©ment et comprendre pourquoi cela m'alerte, m√™me si cela ne devrait pas.  Ou vice versa - pourquoi Nagios panique, mais pas Icinga. </p><br><p>  Quatri√®mement - Nagios a travaill√© ici pour moi pendant trois ans et au d√©but, il y avait plus de confiance en lui qu'en mon nouveau syst√®me hipster, donc √† chaque fois Icinga soulevait la panique - personne n'a rien fait jusqu'√† ce que Nagios soit enthousiasm√© par le m√™me probl√®me.  Mais tr√®s rarement Icinga a √©mis de v√©ritables alarmes avant Nagios et je consid√®re cela comme un montant s√©rieux, dont je parlerai dans la section "Conclusions". </p><br><p>  En cons√©quence, la mise en service a √©t√© retard√©e de plus de 5 mois (pr√©vu le 28 juin 2018 en fait - 3 d√©cembre 2018), principalement √† cause du ¬´contr√¥le de parit√©¬ª - cette merde quand il y a plusieurs services √† Nagios dont personne ne parle Je n'ai rien entendu au cours des deux derni√®res ann√©es, mais MAINTENANT, ils ont foutrement √©mis des critiques sans raison et j'ai d√ª expliquer pourquoi ils n'√©taient pas sur mon panel et ont d√ª les ajouter √† Icinga pour "le contr√¥le de parit√© est termin√©" (Tous les services / contr√¥les √† Nagios correspondent aux services / contr√¥les √† Icinga) </p><br><p>  R√©alisation: <br>  Le premier est la guerre Code vs Data, comme Puppet Style.  Toutes les donn√©es, ici tout en g√©n√©ral, devraient √™tre dans Hiera et rien d'autre.  Tout le code est dans des fichiers .pp.  Variables, abstractions, fonctions - tout se passe en pp. <br>  En cons√©quence, nous avons un tas de machines virtuelles (165 au moment de la r√©daction) et 68 applications Web qui doivent √™tre surveill√©es pour l'int√©grit√© et la validit√© des certificats SSL.  Mais en raison d'h√©morro√Ødes historiques, les informations pour la surveillance des applications proviennent d'un r√©f√©rentiel gitlab distinct et le format des donn√©es n'a pas chang√© depuis Puppet 3, ce qui cr√©e des difficult√©s de configuration suppl√©mentaires. </p><br><div class="spoiler">  <b class="spoiler_title">Puppet-code pour les applications, faites attention</b> <div class="spoiler_text"><pre><code class="plaintext hljs">define profiles::services::monitoring::docker_apps( Hash $app_list, Hash $apps_accessible_from, Hash $apps_access_list, Hash $webhost_defaults, Hash $webcheck_defaults, Hash $service_overrides, Hash $targets, Hash $app_checks, ) { #### APPS #### $zone = $name $app_list.each | String $app_name, Hash $app_data | { $notify_group = { 'notify_group' =&gt; ($webcheck_defaults[$zone]['notify_group'] + pick($app_data['notify_group'], {} )) } # adds notifications for default group (systems) + any group defined in int/pm_docker_apps.eyaml $data = merge($webhost_defaults, $apps_accessible_from, $app_data) $site_domain = $app_data['site_domain'] $regexp = pick($app_data['check_regex'], 'html') # Pick a regex to check $check_url = $app_data['check_url'] ? { undef =&gt; { 'http_uri' =&gt; '/' }, default =&gt; { 'http_uri' =&gt; $app_data['check_url'] } } $check_regex = $regexp ?{ 'absent' =&gt; {}, default =&gt; {'http_expect_body_regex' =&gt; $regexp} } $site_domain.each | String $vhost, Hash $vdata | { # Split an app by domains if there are two or more $vhost_name = {'http_vhost' =&gt; $vhost} $vars = $data['vars'] + $vhost_name + $check_regex + $check_url $web_ipaddress = is_array($vdata['web_ipaddress']) ? { # Make IP-address an array if it's not, because askizzy has 2 ips and it's an array true =&gt; $vdata['web_ipaddress'], false =&gt; [$vdata['web_ipaddress']], } $access_from_zones = [$zone] + $apps_access_list[$data['accessible_from']] # Merge default zone (where the app is defined) and extra zones if they exist $web_ipaddress.each | String $ip_address | { # For each IP (if we have multiple) $suffix = length($web_ipaddress) ? { # If we have more than one - add IP as a suffix to this hostname to avoid duplicating resources 1 =&gt; '', default =&gt; "_${ip_address}" } $octets = split($ip_address, '\.') $ip_tag = "${octets[2]}.${octets[3]}" # Using last octet only causes a collision between nginx-vip 203.15.70.94 and ext. ip 49.255.194.94 $access_from_zones.each | $zone_prefix |{ $zone_target = $targets[$zone_prefix] $nginx_vip_name = "${zone_prefix}_nginx-vip-${ip_tag}" # If it's a host for ext - prefix becomes 'ext_' (ext_nginx-vip...) $nginx_host_vip = { $nginx_vip_name =&gt; { ensure =&gt; present, target =&gt; $zone_target, address =&gt; $ip_address, check_command =&gt; 'hostalive', groups =&gt; ['nginx_vip',], } } $ssl_vars = $app_checks['ssl'] $regex_vars = $app_checks['http'] + $vars + $webcheck_defaults[$zone] + $notify_group if !defined( Profiles::Services::Monitoring::Host[$nginx_vip_name] ) { ensure_resources('profiles::services::monitoring::host', $nginx_host_vip) } if !defined( Icinga2::Object::Service["${nginx_vip_name}_ssl"] ) { icinga2::object::service {"${nginx_vip_name}_ssl": ensure =&gt; $data['ensure'], assign =&gt; ["host.name == $nginx_vip_name",], groups =&gt; ['webchecks',], check_command =&gt; 'ssl', check_interval =&gt; $service_overrides['ssl']['check_interval'], target =&gt; $targets['services'], apply =&gt; true, vars =&gt; $ssl_vars } } if $regexp != 'absent'{ if !defined(Icinga2::Object::Service["${vhost}${$suffix} regex"]){ icinga2::object::service {"${vhost}${$suffix} regex": ensure =&gt; $data['ensure'], assign =&gt; ["match(*_nginx-vip-${ip_tag}, host.name)",], groups =&gt; ['webchecks',], check_command =&gt; 'http', check_interval =&gt; $service_overrides['regex']['check_interval'], target =&gt; $targets['services'], enable_flapping =&gt; true, apply =&gt; true, vars =&gt; $regex_vars } } } } } } } }</code> </pre> </div></div><br><p>  Le code de configuration de l'h√¥te et du service est √©galement horrible: </p><br><div class="spoiler">  <b class="spoiler_title">monitoring / config.pp</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">class profiles::services::monitoring::config( Array $default_config, Array $hostgroups, Hash $hosts = {}, Hash $host_defaults, Hash $services, Hash $service_defaults, Hash $service_overrides, Hash $webcheck_defaults, Hash $servicegroups, String $servicegroup_target, Hash $user_defaults, Hash $users, Hash $oncall, Hash $usergroup_defaults, Hash $usergroups, Hash $notifications, Hash $notification_defaults, Hash $notification_commands, Hash $timeperiods, Hash $webhost_defaults, Hash $apps_access_list, Hash $check_commands, Hash $hosts_api = {}, Hash $targets = {}, Hash $host_api_defaults = {}, ) { # Profiles::Services::Monitoring::Hostgroup &lt;&lt;| |&gt;&gt; # will be enabled when we move to icinga completely #### APPS #### case $location { 'int', 'ext': { $apps_by_zone = {} } 'pm': { $int_apps = hiera('int_docker_apps') $int_app_defaults = hiera('int_docker_app_common') $st_apps = hiera('staging_docker_apps') $srs_apps = hiera('pm_docker_apps_srs') $pm_apps = hiera('pm_docker_apps') + $st_apps + $srs_apps $pm_app_defaults = hiera('pm_docker_app_common') $apps_by_zone = { 'int' =&gt; $int_apps, 'pm' =&gt; $pm_apps, } $app_access_by_zone = { 'int' =&gt; {'accessible_from' =&gt; $int_app_defaults['accessible_from']}, 'pm' =&gt; {'accessible_from' =&gt; $pm_app_defaults['accessible_from']}, } } default: { fail('Please ensure the node has $location fact set (int, pm, ext)') } } file { '/etc/icinga2/conf.d/': ensure =&gt; directory, recurse =&gt; true, purge =&gt; true, owner =&gt; 'icinga', group =&gt; 'icinga', mode =&gt; '0750', notify =&gt; Service['icinga2'], } $default_config.each | String $file_name |{ file {"/etc/icinga2/conf.d/${file_name}": ensure =&gt; present, source =&gt; "puppet:///modules/profiles/services/monitoring/default_config/${file_name}", owner =&gt; 'icinga', group =&gt; 'icinga', mode =&gt; '0640', } } $app_checks = { 'ssl' =&gt; $services['webchecks']['checks']['ssl']['vars'], 'http' =&gt; $services['webchecks']['checks']['http_regexp']['vars'] } $apps_by_zone.each | String $zone, Hash $app_list | { profiles::services::monitoring::docker_apps{$zone: app_list =&gt; $app_list, apps_accessible_from =&gt; $app_access_by_zone[$zone], apps_access_list =&gt; $apps_access_list, webhost_defaults =&gt; $webhost_defaults, webcheck_defaults =&gt; $webcheck_defaults, service_overrides =&gt; $service_overrides, targets =&gt; $targets, app_checks =&gt; $app_checks, } } #### HOSTS #### # Profiles::Services::Monitoring::Host &lt;&lt;| |&gt;&gt; # This is for spaceship invasion when it's ready. $hosts_has_large_disks = query_nodes('mountpoints.*.size_bytes &gt;= 1099511627776') $hosts.each | String $hostgroup, Hash $list_of_hosts_with_settings | { # Splitting site lists by hostgroups - docker_host/gluster_host/etc $list_of_hosts_in_group = $list_of_hosts_with_settings['hosts'] $hostgroup_settings = $list_of_hosts_with_settings['settings'] $merged_hostgroup_settings = deep_merge($host_defaults, $list_of_hosts_with_settings['settings']) $list_of_hosts_in_group.each | String $host_name, Hash $host_settings |{ # Splitting grouplists by hosts # Is this host in the array $hosts_has_large_disks ? If so set host.vars.has_large_disks if ( $hosts_has_large_disks.reduce(false) | $found, $value| { ( $value =~ "^${host_name}" ) or $found } ) { $vars_has_large_disks = { 'has_large_disks' =&gt; true } } else { $vars_has_large_disks = {} } $host_data = deep_merge($merged_hostgroup_settings, $host_settings) $hostgroup_settings_vars = pick($hostgroup_settings['vars'], {}) $host_settings_vars = pick($host_settings['vars'], {}) $host_notify_group = delete_undef_values($host_defaults['vars']['notify_group'] + $hostgroup_settings_vars['notify_group'] + $host_settings_vars['notify_group']) $host_data_vars = delete_undef_values(deep_merge($host_data['vars'] , {'notify_group' =&gt; $host_notify_group}, $vars_has_large_disks)) # Merging vars separately $hostgroups = delete_undef_values([$hostgroup] + $host_data['groups']) profiles::services::monitoring::host{$host_name: ensure =&gt; $host_data['ensure'], display_name =&gt; $host_data['display_name'], address =&gt; $host_data['address'], groups =&gt; $hostgroups, target =&gt; $host_data['target'], check_command =&gt; $host_data['check_command'], check_interval =&gt; $host_data['check_interval'], max_check_attempts =&gt; $host_data['max_check_attempts'], vars =&gt; $host_data_vars, template =&gt; $host_data['template'], } } } if !empty($hosts_api){ # All hosts managed by API $hosts_api.each | String $zone, Hash $hosts_api_zone | { # Split api hosts by zones $hosts_api_zone.each | String $hostgroup, Hash $list_of_hosts_with_settings | { # Splitting site lists by hostgroups - docker_host/gluster_host/etc $list_of_hosts_in_group = $list_of_hosts_with_settings['hosts'] $hostgroup_settings = $list_of_hosts_with_settings['settings'] $merged_hostgroup_settings = deep_merge($host_api_defaults, $list_of_hosts_with_settings['settings']) $list_of_hosts_in_group.each | String $host_name, Hash $host_settings |{ # Splitting grouplists by hosts # Is this host in the array $hosts_has_large_disks ? If so set host.vars.has_large_disks if ( $hosts_has_large_disks.reduce(false) | $found, $value| { ( $value =~ "^${host_name}" ) or $found } ) { $vars_has_large_disks = { 'has_large_disks' =&gt; true } } else { $vars_has_large_disks = {} } $host_data = deep_merge($merged_hostgroup_settings, $host_settings) $hostgroup_settings_vars = pick($hostgroup_settings['vars'], {}) $host_settings_vars = pick($host_settings['vars'], {}) $host_api_notify_group = delete_undef_values($host_defaults['vars']['notify_group'] + $hostgroup_settings_vars['notify_group'] + $host_settings_vars['notify_group']) $host_data_vars = delete_undef_values(deep_merge($host_data['vars'] , {'notify_group' =&gt; $host_api_notify_group}, $vars_has_large_disks)) $hostgroups = delete_undef_values([$hostgroup] + $host_data['groups']) if defined(Profiles::Services::Monitoring::Host[$host_name]){ $hostname = "${host_name}_from_${zone}" } else { $hostname = $host_name } profiles::services::monitoring::host{$hostname: ensure =&gt; $host_data['ensure'], display_name =&gt; $host_data['display_name'], address =&gt; $host_data['address'], groups =&gt; $hostgroups, target =&gt; "${host_data['target_base']}/${zone}/hosts.conf", check_command =&gt; $host_data['check_command'], check_interval =&gt; $host_data['check_interval'], max_check_attempts =&gt; $host_data['max_check_attempts'], vars =&gt; $host_data_vars, template =&gt; $host_data['template'], } } } } } #### END OF HOSTS #### #### SERVICES #### $services.each | String $service_group, Hash $s_list |{ # Service_group and list of services in that group $service_list = $s_list['checks'] # List of actual checks, separately from SG settings $service_list.each | String $service_name, Hash $data |{ $merged_defaults = merge($service_defaults, $s_list['settings']) # global service defaults + service group defaults $merged_data = merge($merged_defaults, $data) $settings_vars = pick($s_list['settings']['vars'], {}) $this_service_vars = pick($data['vars'], {}) $all_service_vars = delete_undef_values($service_defaults['vars'] + $settings_vars + $this_service_vars) # If we override default check_timeout, but not nrpe_timeout, make nrpe_timeout the same as check_timeout if ( $merged_data['check_timeout'] and ! $this_service_vars['nrpe_timeout'] ) { # NB: Icinga will convert 1m to 60 automatically! $nrpe = { 'nrpe_timeout' =&gt; $merged_data['check_timeout'] } } else { $nrpe = {} } # By default we use nrpe and all commands are run via nrpe. So vars.nrpe_command = $service_name is a default value # If it's server-side Icinga command - we don't need 'nrpe_command' # but there is no harm to have that var and the code is shorter if $merged_data['check_command'] == 'nrpe'{ $check_command = $merged_data['vars']['nrpe_command'] ? { undef =&gt; { 'nrpe_command' =&gt; $service_name }, default =&gt; { 'nrpe_command' =&gt; $merged_data['vars']['nrpe_command'] } } }else{ $check_command = {} } # Assembling $vars from Global Default service settings, servicegroup settings, this particular check settings and let's not forget nrpe settings. if $all_service_vars['graphite_template'] { $graphite_template = {'check_command' =&gt; $all_service_vars['graphite_template']} }else{ $graphite_template = {'check_command' =&gt; $service_name} } $service_notify = [] + pick($settings_vars['notify_group'], []) + pick($this_service_vars['notify_group'], []) # pick is required everywhere, otherwise becomes "The value '' cannot be converted to Numeric" $service_notify_group = $service_notify ? { [] =&gt; $service_defaults['vars']['notify_group'], default =&gt; $service_notify } # Assing default group (systems) if no other groups are defined $vars = $all_service_vars + $nrpe + $check_command + $graphite_template + {'notify_group' =&gt; $service_notify_group} # This needs to be merged separately, because merging it as part of MERGED_DATA overwrites arrays instead of merging them, so we lose some "assign" and "ignore" values $assign = delete_undef_values($service_defaults['assign'] + $s_list['settings']['assign'] + $data['assign']) $ignore = delete_undef_values($service_defaults['ignore'] + $s_list['settings']['ignore'] + $data['ignore']) icinga2::object::service {$service_name: ensure =&gt; $merged_data['ensure'], apply =&gt; $merged_data['apply'], enable_flapping =&gt; $merged_data['enable_flapping'], assign =&gt; $assign, ignore =&gt; $ignore, groups =&gt; [$service_group], check_command =&gt; $merged_data['check_command'], check_interval =&gt; $merged_data['check_interval'], check_timeout =&gt; $merged_data['check_timeout'], check_period =&gt; $merged_data['check_period'], display_name =&gt; $merged_data['display_name'], event_command =&gt; $merged_data['event_command'], retry_interval =&gt; $merged_data['retry_interval'], max_check_attempts =&gt; $merged_data['max_check_attempts'], target =&gt; $merged_data['target'], vars =&gt; $vars, template =&gt; $merged_data['template'], } } } #### END OF SERVICES #### #### OTHER BORING STUFF #### $servicegroups.each | $servicegroup, $description |{ icinga2::object::servicegroup{ $servicegroup: target =&gt; $servicegroup_target, display_name =&gt; $description } } $hostgroups.each| String $hostgroup |{ profiles::services::monitoring::hostgroup { $hostgroup:} } $notifications.each | String $name, Hash $settings |{ $assign = pick($notification_defaults['assign'], []) + $settings['assign'] $ignore = pick($notification_defaults['ignore'], []) + $settings['ignore'] $merged_settings = $settings + $notification_defaults icinga2::object::notification{$name: target =&gt; $merged_settings['target'], apply =&gt; $merged_settings['apply'], apply_target =&gt; $merged_settings['apply_target'], command =&gt; $merged_settings['command'], interval =&gt; $merged_settings['interval'], states =&gt; $merged_settings['states'], types =&gt; $merged_settings['types'], assign =&gt; delete_undef_values($assign), ignore =&gt; delete_undef_values($ignore), user_groups =&gt; $merged_settings['user_groups'], period =&gt; $merged_settings['period'], vars =&gt; $merged_settings['vars'], } } # Merging notification settings for users with other settings $users_oncall = deep_merge($users, $oncall) # Magic. Do not touch. create_resources('icinga2::object::user', $users_oncall, $user_defaults) create_resources('icinga2::object::usergroup', $usergroups, $usergroup_defaults) create_resources('icinga2::object::timeperiod',$timeperiods) create_resources('icinga2::object::checkcommand', $check_commands) create_resources('icinga2::object::notificationcommand', $notification_commands) profiles::services::sudoers { 'icinga_runs_ping_l2': ensure =&gt; present, sudoersd_template =&gt; 'profiles/os/redhat/centos7/sudoers/icinga.erb', } }</code> </pre> </div></div><br><p>  Je travaille toujours sur cette nouille et je l'am√©liore autant que possible.  Cependant, c'est ce code qui a permis d'utiliser une syntaxe simple et claire dans Hiera: </p><br><div class="spoiler">  <b class="spoiler_title">Les donn√©es</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">profiles::services::monitoring::config::services: perf_checks: settings: check_interval: '2m' assign: - 'host.vars.type == linux' checks: procs: {} load: {} memory: {} disk: check_interval: '5m' vars: notification_period: '24x7' disk_iops: vars: notifications: - 'silent' cpu: vars: notifications: - 'silent' dns_fqdn: check_interval: '15m' ignore: - 'xenserver in host.groups' vars: notifications: - 'silent' iftraffic_nrpe: vars: notifications: - 'silent' logging: settings: assign: - 'logserver in host.groups' checks: rsyslog: {} nginx_limit_req_other: {} nginx_limit_req_s2s: {} nginx_limit_req_s2x: {} nginx_limit_req_srs: {} logstash: {} logstash_api: vars: notifications: - 'silent'</code> </pre> </div></div><br><p>  Tous les contr√¥les sont divis√©s en groupes, chaque groupe a des param√®tres par d√©faut tels que o√π et √† quelle fr√©quence ex√©cuter ces contr√¥les, quelles notifications envoyer et √† qui. </p><br><p>  Dans chaque v√©rification, vous pouvez remplacer n'importe quelle option, et tout cela finit par s'ajouter aux param√®tres par d√©faut de toutes les v√©rifications dans leur ensemble.  Par cons√©quent, ces nouilles sont √©crites dans config.pp - il y a une fusion de tous les param√®tres par d√©faut avec les param√®tres des groupes, puis avec chaque v√©rification individuelle. </p><br><p>  En outre, un changement tr√®s important a √©t√© la possibilit√© d'utiliser des fonctions dans les param√®tres, par exemple, la fonction de changer le port, l'adresse et l'URL pour v√©rifier http_regex. </p><br><pre> <code class="plaintext hljs">http_regexp: assign: - 'host.vars.http_regex' - 'static_sites in host.groups' check_command: 'http' check_interval: '1m' retry_interval: '20s' max_check_attempts: 6 http_port: '{{ if(host.vars.http_port) { return host.vars.http_port } else { return 443 } }}' vars: notification_period: 'host.vars.notification_period' http_vhost: '{{ if(host.vars.http_vhost) { return host.vars.http_vhost } else { return host.name } }}' http_ssl: '{{ if(host.vars.http_ssl) { return false } else { return true } }}' http_expect_body_regex: 'host.vars.http_regex' http_uri: '{{ if(host.vars.http_uri) { return host.vars.http_uri } else { return "/" } }}' http_onredirect: 'follow' http_warn_time: 8 http_critical_time: 15 http_timeout: 30 http_sni: true</code> </pre> <br><p>  Cela signifie - s'il y a une variable <em>http_port</em> dans la <em>d√©finition d'</em> h√¥te - utilisez-la, sinon 443. Par exemple, l'interface Web jabber se bloque √† 9090 et Unifi - √† 7443. <br>  <em>http_vhost</em> signifie ignorer DNS et prendre cette adresse. <br>  Si uri est sp√©cifi√© dans l'h√¥te, suivez-le, sinon prenez "/". </p><br><p>  Une histoire amusante est sortie avec http_ssl - cette infection ne voulait pas se d√©connecter √† la demande.  J'ai b√™tement tr√©buch√© sur cette ligne pendant longtemps, jusqu'√† ce qu'il me vienne √† l'esprit qu'il y avait une variable dans la d√©finition d'h√¥te: </p><br><pre> <code class="plaintext hljs">http_ssl: false</code> </pre> <br><p>  Remplace l'expression </p><br><pre> <code class="plaintext hljs">if(host.vars.http_ssl) { return false } else { return true }</code> </pre> <br><p>  comme <strong>faux</strong> et √† la fin il s'av√®re </p><br><pre> <code class="plaintext hljs">if(false) { return false } else { return true }</code> </pre> <br><p>  c'est-√†-dire que la v√©rification ssl est toujours active.  Il a √©t√© d√©cid√© en rempla√ßant la syntaxe: </p><br><pre> <code class="plaintext hljs">http_ssl: no</code> </pre> <br><p>  <strong>Conclusions</strong> : </p><br><p>  Avantages: </p><br><ul><li>  Nous avons maintenant un syst√®me de surveillance, et non deux, comme ce fut le cas au cours des 7 √† 8 derniers mois, ou un, d√©pass√© et vuln√©rable. </li><li>  La structure des donn√©es des h√¥tes / services (contr√¥les) est d√©sormais (√† mon avis) beaucoup plus lisible et compr√©hensible.  Pour d'autres, ce n'√©tait pas si √©vident, j'ai donc d√ª couper quelques pages sur le wiki local pour expliquer comment cela fonctionne et o√π le modifier. </li><li>  Il est possible de configurer de mani√®re flexible les v√©rifications √† l'aide de variables et de fonctions, par exemple, pour v√©rifier http_regexp, le mod√®le souhait√©, le code retour, l'url et le port peuvent √™tre d√©finis dans les param√®tres de l'h√¥te. </li><li>  Il existe plusieurs tableaux de bord, pour chacun desquels vous pouvez d√©finir votre propre liste d'alarmes affich√©es et g√©rer tout cela via des demandes de marionnettes et de fusion. </li></ul><br><p>  Inconv√©nients: </p><br><ul><li>  Inertie des membres de l'√©quipe - Nagios a travaill√©, travaill√© et travaill√©, et cela votre Isinga bogue et ralentit constamment.  Et comment puis-je voir l'histoire?  Et, bon sang, il n'est pas mis √† jour ... (Le vrai probl√®me est que l'historique des alarmes n'est pas mis √† jour automatiquement, uniquement par F5) </li><li>  L'inertie du syst√®me - lorsque je clique sur ¬´v√©rifier maintenant¬ª dans l'interface Web - le r√©sultat de l'ex√©cution d√©pend de la m√©t√©o sur Mars, en particulier des services complexes qui prennent des dizaines de secondes √† compl√©ter.  Un r√©sultat similaire est une chose normale. <img src="https://habrastorage.org/webt/ue/73/wa/ue73wa4yt4bhedebd1n66kizsf8.jpeg"></li><li>  En g√©n√©ral, selon les statistiques semestrielles des deux syst√®mes fonctionnant c√¥te √† c√¥te, Nagios fonctionnait toujours plus vite que Icinga et cela m'a vraiment √©nerv√©.  Il me semble qu'il y a quelque chose dup√© par des minuteries et un contr√¥le de cinq minutes sur le fait va tous les 5h30 ou quelque chose comme √ßa. </li><li>  Si vous red√©marrez le service √† tout moment (systemctl restart icinga2) - toutes les v√©rifications qui √©taient en cours √† ce moment d√©clencheront une alarme critique &lt;termin√©e par le signal 15&gt; sur l'√©cran et de c√¥t√©, il semblera que tout soit tomb√© ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">bug confirm√©</a> ) </li></ul><br><p>  Mais en g√©n√©ral - cela fonctionne. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr444060/">https://habr.com/ru/post/fr444060/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr444048/index.html">Trucs et astuces de Digital Forensics: comment d√©tecter les modifications de strat√©gie de groupe induites par un intrus</a></li>
<li><a href="../fr444050/index.html">Discussion: le stockage d'ADN deviendra-t-il massif</a></li>
<li><a href="../fr444052/index.html">Comment nous √† IntelliJ IDEA recherchons les expressions lambda</a></li>
<li><a href="../fr444056/index.html">Les fournisseurs d'acc√®s Internet en Crim√©e ont fortement augment√© les prix des services</a></li>
<li><a href="../fr444058/index.html">Quand les enfants comprennent que toute leur vie est d√©j√† en ligne</a></li>
<li><a href="../fr444062/index.html">Allume! Transformations nocturnes du centre de Lakhta</a></li>
<li><a href="../fr444064/index.html">De nouvelles id√©es pour un nouvel avenir</a></li>
<li><a href="../fr444068/index.html">Qui regarde?</a></li>
<li><a href="../fr444070/index.html">D√©velopper un hexapode √† partir de z√©ro (partie 4) - trajectoires et s√©quences math√©matiques</a></li>
<li><a href="../fr444072/index.html">Android Shopping - Biblioth√®que de facturation Google Play</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>