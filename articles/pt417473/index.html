<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ôìÔ∏è ‚ôÇÔ∏è üç¥ Armazenamento confi√°vel com DRBD9 e Proxmox (parte 1: NFS) üõë üêí ‚ö´Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Provavelmente, todos os que ficaram pelo menos uma vez intrigados com a busca por armazenamento definido por software de alto desempenho, mais cedo ou...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Armazenamento confi√°vel com DRBD9 e Proxmox (parte 1: NFS)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/417473/"><p><img src="https://habrastorage.org/getpro/habr/post_images/f6a/771/043/f6a7710433c8887d6bbbe4792cc178e1.jpg" alt="imagem"></p><br><p>  Provavelmente, todos os que ficaram pelo menos uma vez intrigados com a busca por <strong>armazenamento definido por software de</strong> alto desempenho, mais cedo ou mais tarde, ouviram falar do <strong>DRBD</strong> , ou talvez at√© <strong>tenham</strong> lidado com isso. </p><br><p> √â verdade que, no auge da popularidade do <strong>Ceph</strong> e <strong>GlusterFS</strong> , que funcionam muito bem em princ√≠pio e, o mais importante, imediatamente, todos se esqueceram um pouco disso.  Al√©m disso, a vers√£o anterior n√£o suportava a replica√ß√£o para mais de dois n√≥s e, por causa disso, eram frequentemente encontrados problemas com o <strong>c√©rebro dividido</strong> , o que claramente n√£o aumentava sua popularidade. </p><br><p>  A solu√ß√£o n√£o √© realmente nova, mas bastante competitiva.  Com custos relativamente baixos para CPU e RAM, o <strong>DRBD</strong> fornece sincroniza√ß√£o realmente r√°pida e segura no n√≠vel do <strong>dispositivo de bloco</strong> .  Durante todo esse tempo, os desenvolvedores do LINBIT - DRBD n√£o param e o refinam constantemente.  Come√ßando com a vers√£o <strong>DRBD9</strong> , deixa de ser apenas um espelho de rede e se torna algo mais. </p><br><p>  Em primeiro lugar, a id√©ia de criar um √∫nico <strong>dispositivo de bloco distribu√≠do</strong> para v√°rios servidores recuou para o segundo plano e agora o LINBIT est√° tentando fornecer ferramentas para orquestrar e gerenciar muitos dispositivos drbd em um cluster criado sobre <strong>parti√ß√µes</strong> <strong>LVM</strong> e <strong>ZFS</strong> . </p><br><p>  Por exemplo, o DRBD9 suporta at√© 32 r√©plicas, RDMA, n√≥s sem disco e novas ferramentas de orquestra√ß√£o permitem o uso de instant√¢neos, migra√ß√£o online e muito mais. </p><br><p>  Apesar do <strong>DRBD9</strong> possuir ferramentas de integra√ß√£o com <strong>Proxmox</strong> , <strong>Kubernetes</strong> , <strong>OpenStack</strong> e <strong>OpenNebula</strong> , no momento elas est√£o em algum modo de transi√ß√£o, quando novas ferramentas ainda n√£o s√£o suportadas em todos os lugares, e as antigas ser√£o anunciadas como <em>obsoletas</em> muito em breve.  Estes s√£o <strong>DRBDmanage</strong> e <strong>Linstor</strong> . </p><br><p>  Aproveitarei esse momento para n√£o entrar muito nos detalhes de cada um deles, mas examinar com mais detalhes a configura√ß√£o e os princ√≠pios do trabalho com o pr√≥prio <strong>DRBD9</strong> . <a name="habracut"></a>  Voc√™ ainda precisa descobrir isso, apenas porque a configura√ß√£o tolerante a falhas do controlador Linstor implica em instal√°-lo em um desses dispositivos. </p><br><p>  Neste artigo, gostaria de falar sobre o <strong>DRBD9</strong> e a possibilidade de seu uso no <strong>Proxmox</strong> sem plug-ins de terceiros. </p><br><h2 id="drbdmanage-i-linstor">  DRBDmanage e Linstor </h2><br><p>  Em primeiro lugar, vale a pena mencionar mais uma vez sobre o <strong>DRBDmanage</strong> , que se integra muito bem ao <strong>Proxmox</strong> .  O LINBIT fornece um plug-in DRBDmanage pronto para o Proxmox que permite usar todas as suas fun√ß√µes diretamente da interface do <strong>Proxmox</strong> . </p><br><p>  Parece realmente incr√≠vel, mas infelizmente tem algumas desvantagens. </p><br><ul><li> Primeiro, os nomes de volumes marcados, o <strong>grupo LVM</strong> ou <strong>o pool ZFS</strong> devem ter o nome <code>drbdpool</code> . </li><li>  Incapacidade de usar mais de <strong>um</strong> pool por n√≥ </li><li>  Devido √†s especificidades da solu√ß√£o, o <strong>volume</strong> do <strong>controlador</strong> pode estar apenas em um LVM regular e n√£o de outra forma </li><li>  <strong>Falhas</strong> peri√≥dicas no <strong>dbus</strong> , usadas de perto pelo <strong>DRBDmanage</strong> para interagir com os n√≥s. </li></ul><br><p>  Como resultado, o LINBIT decidiu substituir toda a l√≥gica complexa do DRBDmanage por um aplicativo simples que se comunica com os n√≥s usando uma <strong>conex√£o tcp</strong> regular e funciona sem nenhuma m√°gica.  Ent√£o havia <strong>Linstor</strong> . </p><br><p>  <strong>Linstor</strong> realmente funciona muito bem.  Infelizmente, os desenvolvedores escolheram o <strong>java</strong> como o idioma principal para escrever o servidor Linstor, mas n√£o deixe que isso o assuste, pois o pr√≥prio Linstor se preocupa apenas em <strong>distribuir configura√ß√µes</strong> DRBD e <strong>fatiar</strong> parti√ß√µes LVM / ZFS nos n√≥s. </p><br><blockquote>  Ambas as solu√ß√µes s√£o gratuitas e distribu√≠das sob a licen√ßa <strong>GPL3</strong> gratuita <strong>.</strong> </blockquote><p>  Voc√™ pode ler sobre cada um deles e sobre como configurar o plug-in mencionado para o <strong>Proxmox</strong> no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">wiki oficial do Proxmox</a> </p><br><h2 id="otkazoustoychivyy-nfs-server">  Servidor NFS de failover </h2><br><p>  Infelizmente, no momento da reda√ß√£o deste artigo, o <strong>Linstor s√≥</strong> tinha integra√ß√£o com o <strong>Kubernetes</strong> .  Mas no final do ano, os drivers s√£o esperados para o resto dos <strong>sistemas Proxmox</strong> , <strong>OpenNebula</strong> , <strong>OpenStack</strong> . </p><br><p>  At√© agora, por√©m, n√£o existe uma solu√ß√£o pronta, mas n√£o gostamos da antiga, de uma forma ou de outra.  Vamos tentar usar o DRBD9 da maneira antiga para organizar o <strong>acesso do NFS</strong> a uma parti√ß√£o compartilhada. </p><br><p>  No entanto, essa solu√ß√£o tamb√©m n√£o ter√° vantagens, porque o servidor NFS permitir√° organizar o <strong>acesso competitivo</strong> ao sistema de arquivos do reposit√≥rio a partir de v√°rios servidores, sem a necessidade de sistemas de arquivos de cluster complexos com DLM, como OCFS e GFS2. </p><br><p>  Nesse caso, voc√™ poder√° alternar as fun√ß√µes do n√≥ <strong>Prim√°rio</strong> / <strong>Secund√°rio</strong> simplesmente migrando o cont√™iner com o servidor NFS na interface Proxmox. </p><br><p>  Voc√™ tamb√©m pode armazenar qualquer arquivo dentro deste sistema de arquivos, bem como discos e backups virtuais. </p><br><p>  Caso voc√™ use o <strong>Kubernetes</strong> , poder√° organizar o acesso <strong>ReadWriteMany</strong> para os seus <strong>PersistentVolumes</strong> . </p><br><h2 id="proxmox-i-lxc-konteynery">  Cont√™ineres Proxmox e LXC </h2><br><p>  Agora a pergunta √©: por que o Proxmox? </p><br><p>  Em princ√≠pio, para construir esse esquema, poder√≠amos usar o Kubernetes, bem como o esquema usual, com um gerenciador de cluster.  Mas o <strong>Proxmox</strong> fornece uma <strong>interface</strong> pronta, muito multifuncional e ao mesmo tempo simples e intuitiva para quase tudo que voc√™ precisa.  Est√° pronto para o uso em <strong>cluster</strong> e suporta o mecanismo de <strong>esgrima</strong> baseado no softdog.  E, ao usar <strong>cont√™ineres LXC,</strong> permite atingir tempos m√≠nimos m√≠nimos ao alternar. <br>  A solu√ß√£o resultante n√£o ter√° um √∫nico <strong>ponto de falha</strong> . </p><br><p>  De fato, usaremos o Proxmox principalmente como um <strong>gerenciador de cluster</strong> , onde podemos considerar um <strong>cont√™iner LXC</strong> separado como um servi√ßo em execu√ß√£o em um cluster HA cl√°ssico, apenas com a diferen√ßa de que o cont√™iner tamb√©m vem com seu <strong>sistema raiz</strong> .  Ou seja, voc√™ n√£o precisa instalar v√°rias inst√¢ncias de servi√ßo em cada servidor separadamente; √© poss√≠vel fazer isso apenas uma vez dentro do cont√™iner. <br>  Se voc√™ j√° trabalhou com o <strong>software gerenciador de cluster</strong> e fornece <strong>HA</strong> para aplicativos, entender√° o que quero dizer. </p><br><h2 id="obschaya-shema">  Esquema geral </h2><br><p>  Nossa solu√ß√£o ser√° semelhante ao esquema de replica√ß√£o padr√£o de um banco de dados. </p><br><ul><li>  N√≥s temos <strong>tr√™s n√≥s</strong> </li><li>  Cada n√≥ possui um <strong>dispositivo drbd</strong> distribu√≠do. </li><li>  O dispositivo possui um sistema de arquivos regular ( <strong>ext4</strong> ) </li><li>  Apenas um servidor pode ser um <strong>mestre</strong> </li><li>  O <strong>servidor NFS</strong> no <strong>cont√™iner LXC √©</strong> iniciado no assistente. </li><li>  Todos os n√≥s acessam o dispositivo estritamente atrav√©s do <strong>NFS.</strong> </li><li>  Se necess√°rio, o assistente pode passar para outro n√≥, junto com o <strong>servidor NFS</strong> </li></ul><br><p>  <strong>O DRBD9</strong> possui um recurso muito interessante que simplifica bastante tudo: <br>  O dispositivo drbd automaticamente se torna <strong>Prim√°rio</strong> no momento em que √© montado em algum n√≥.  Se o dispositivo estiver marcado como <strong>Prim√°rio</strong> , qualquer tentativa de mont√°-lo em outro n√≥ resultar√° em um erro de acesso.  Isso garante bloqueio e prote√ß√£o garantida contra acesso simult√¢neo ao dispositivo. </p><br><p>  Por que tudo isso simplifica bastante?  Como quando o cont√™iner √© iniciado, o <strong>Proxmox</strong> monta automaticamente este dispositivo e ele se torna <strong>Prim√°rio</strong> nesse n√≥. Quando o cont√™iner para, desmonta-o pelo contr√°rio e o dispositivo se torna <strong>Secund√°rio</strong> novamente. <br>  Portanto, n√£o precisamos mais nos preocupar com a troca de dispositivos <strong>Prim√°rio</strong> / <strong>Secund√°rio</strong> , o Proxmox far√° isso <strong>automaticamente</strong> , Hurra! </p><br><h2 id="nastroyka-drbd">  Configura√ß√£o DRBD </h2><br><p>  Bem, descobrimos a id√©ia. Agora, vamos √† implementa√ß√£o. </p><br><p>  Por padr√£o <strong>, a oitava vers√£o do drbd</strong> √© fornecida <strong>com o kernel Linux</strong> , infelizmente <strong>n√£o</strong> nos <strong>conv√©m</strong> e precisamos instalar a nona vers√£o do m√≥dulo. </p><br><p>  Conecte o reposit√≥rio LINBIT e instale tudo o que voc√™ precisa: </p><br><pre> <code class="bash hljs">wget -O- https://packages.linbit.com/package-signing-pubkey.asc | apt-key add - <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"deb http://packages.linbit.com/proxmox/ proxmox-5 drbd-9.0"</span></span> \ &gt; /etc/apt/sources.list.d/linbit.list apt-get update &amp;&amp; apt-get -y install pve-headers drbd-dkms drbd-utils drbdtop</code> </pre> <br><ul><li>  <code>pve-headers</code> - <code>pve-headers</code> kernel necess√°rios para construir o m√≥dulo </li><li>  <code>drbd-dkms</code> - m√≥dulo do kernel no formato DKMS </li><li>  <code>drbd-utils</code> - utilit√°rios b√°sicos de gerenciamento DRBD </li><li>  <code>drbdtop</code> √© uma ferramenta interativa como top apenas para DRBD </li></ul><br><p>  Ap√≥s a instala√ß√£o do <strong>m√≥dulo,</strong> verificaremos se est√° tudo em ordem com ele: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># modprobe drbd # cat /proc/drbd version: 9.0.14-1 (api:2/proto:86-113)</span></span></code> </pre> <br><p>  Se voc√™ vir a <strong>oitava vers√£o</strong> na sa√≠da do comando, algo deu errado e o m√≥dulo do kernel <strong>na √°rvore</strong> √© carregado.  Verifique o <code>dkms status</code> descobrir qual √© o motivo. </p><br><p>  Cada n√≥ que temos ter√° o mesmo <strong>dispositivo drbd</strong> rodando sobre parti√ß√µes regulares.  Primeiro, precisamos preparar esta se√ß√£o para o drbd em cada n√≥. </p><br><p>  Essa parti√ß√£o pode ser qualquer <strong>dispositivo de bloco</strong> , pode ser lvm, zvol, uma parti√ß√£o de disco ou o disco inteiro.  Neste artigo, usarei um disco nvme separado com uma parti√ß√£o sob drbd: <code>/dev/nvme1n1p1</code> </p><br><p>  Vale a pena notar que os nomes dos dispositivos tendem a mudar algumas vezes; portanto, √© melhor adotar imediatamente o h√°bito de usar o link simb√≥lico constante para o dispositivo. </p><br><p>  Voc√™ pode encontrar um link simb√≥lico para <code>/dev/nvme1n1p1</code> desta maneira: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># find /dev/disk/ -lname '*/nvme1n1p1' /dev/disk/by-partuuid/847b9713-8c00-48a1-8dff-f84c328b9da2 /dev/disk/by-path/pci-0000:0e:00.0-nvme-1-part1 /dev/disk/by-id/nvme-eui.0000000001000000e4d25c33da9f4d01-part1 /dev/disk/by-id/nvme-INTEL_SSDPEKKA010T7_BTPY703505FB1P0H-part1</span></span></code> </pre> <br><p>  Descrevemos nosso recurso nos tr√™s n√≥s: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cat /etc/drbd.d/nfs1.res resource nfs1 { meta-disk internal; device /dev/drbd100; protocol C; net { after-sb-0pri discard-zero-changes; after-sb-1pri discard-secondary; after-sb-2pri disconnect; } on pve1 { address 192.168.2.11:7000; disk /dev/disk/by-partuuid/95e7eabb-436e-4585-94ea-961ceac936f7; node-id 0; } on pve2 { address 192.168.2.12:7000; disk /dev/disk/by-partuuid/aa7490c0-fe1a-4b1f-ba3f-0ddee07dfee3; node-id 1; } on pve3 { address 192.168.2.13:7000; disk /dev/disk/by-partuuid/847b9713-8c00-48a1-8dff-f84c328b9da2; node-id 2; } connection-mesh { hosts pve1 pve2 pve3; } }</span></span></code> </pre> <br><p>  √â aconselh√°vel usar uma <strong>rede separada</strong> para sincroniza√ß√£o drbd. </p><br><p>  Agora crie os metadados para drbd e execute-o: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm create-md nfs1 initializing activity log initializing bitmap (320 KB) to all zero Writing meta data... New drbd meta data block successfully created. success # drbdadm up nfs1</span></span></code> </pre> <br><p>  Repita estas etapas nos tr√™s n√≥s e verifique o status: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm status nfs1 role:Secondary disk:Inconsistent pve2 role:Secondary peer-disk:Inconsistent pve3 role:Secondary peer-disk:Inconsistent</span></span></code> </pre> <br><p>  Agora, nosso disco <strong>inconsistente est√°</strong> nos tr√™s n√≥s, isso ocorre porque o drbd n√£o sabe qual disco deve ser tomado como original.  Devemos marcar um deles como <strong>Prim√°rio</strong> para que seu estado seja sincronizado com os outros n√≥s: </p><br><pre> <code class="bash hljs">drbdadm primary --force nfs1 drbdadm secondary nfs1</code> </pre> <br><p>  Imediatamente ap√≥s isso, a <strong>sincroniza√ß√£o</strong> come√ßar√°: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm status nfs1 role:Secondary disk:UpToDate pve2 role:Secondary replication:SyncSource peer-disk:Inconsistent done:26.66 pve3 role:Secondary replication:SyncSource peer-disk:Inconsistent done:14.20</span></span></code> </pre><br><p>  N√£o precisamos esperar que termine e podemos executar outras etapas em paralelo.  Eles podem ser executados em <strong>qualquer n√≥</strong> , independentemente do estado atual do disco local no DRBD.  Todas as solicita√ß√µes ser√£o redirecionadas automaticamente para o dispositivo com o estado <strong>UpToDate</strong> . </p><br><p>  N√£o se esque√ßa de ativar a <strong>execu√ß√£o autom√°tica do</strong> servi√ßo drbd nos n√≥s: </p><br><pre> <code class="hljs pgsql">systemctl <span class="hljs-keyword"><span class="hljs-keyword">enable</span></span> drbd.service</code> </pre> <br><h2 id="nastroyka-lxc-konteynera">  Configurando um Cont√™iner LXC </h2><br><p>  Vamos omitir a parte de configura√ß√£o do <strong>cluster Proxmox</strong> de tr√™s n√≥s, essa parte est√° bem descrita no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">wiki oficial</a> </p><br><p>  Como eu disse antes, nosso <strong>servidor NFS</strong> funcionar√° em um <strong>cont√™iner LXC</strong> .  Manteremos o cont√™iner no dispositivo <code>/dev/drbd100</code> que acabamos de criar. </p><br><p>  Primeiro, precisamos criar um <strong>sistema de arquivos</strong> nele: </p><br><pre> <code class="hljs powershell">mkfs <span class="hljs-literal"><span class="hljs-literal">-t</span></span> ext4 <span class="hljs-literal"><span class="hljs-literal">-O</span></span> mmp <span class="hljs-literal"><span class="hljs-literal">-E</span></span> mmp_update_interval=<span class="hljs-number"><span class="hljs-number">5</span></span> /dev/drbd100</code> </pre> <br><p>  <strong>O Proxmox,</strong> por padr√£o, inclui <strong>prote√ß√£o multimount</strong> no n√≠vel do sistema de arquivos; em princ√≠pio, podemos fazer sem ele, porque  Por padr√£o, o DRBD possui sua pr√≥pria prote√ß√£o; ele simplesmente pro√≠be a segunda <strong>Prim√°ria</strong> do dispositivo, mas o cuidado n√£o nos prejudica. </p><br><p>  Agora baixe o modelo do Ubuntu: </p><br><pre> <code class="hljs pgsql"># wget http://download.proxmox.com/images/<span class="hljs-keyword"><span class="hljs-keyword">system</span></span>/ubuntu<span class="hljs-number"><span class="hljs-number">-16.04</span></span>-standard_16<span class="hljs-number"><span class="hljs-number">.04</span></span><span class="hljs-number"><span class="hljs-number">-1</span></span>_amd64.tar.gz -P /var/lib/vz/<span class="hljs-keyword"><span class="hljs-keyword">template</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">cache</span></span>/</code> </pre> <br><p>  E crie nosso cont√™iner a partir dele: </p><br><pre> <code class="hljs powershell">pct create <span class="hljs-number"><span class="hljs-number">101</span></span> local:vztmpl/ubuntu<span class="hljs-literal"><span class="hljs-literal">-16</span></span>.<span class="hljs-number"><span class="hljs-number">04</span></span><span class="hljs-literal"><span class="hljs-literal">-standard_16</span></span>.<span class="hljs-number"><span class="hljs-number">04</span></span><span class="hljs-literal"><span class="hljs-literal">-1_amd64</span></span>.tar.gz \ -<span class="hljs-literal"><span class="hljs-literal">-hostname</span></span>=nfs1 \ -<span class="hljs-literal"><span class="hljs-literal">-net0</span></span>=name=eth0,bridge=vmbr0,gw=<span class="hljs-number"><span class="hljs-number">192.168</span></span>.<span class="hljs-number"><span class="hljs-number">1.1</span></span>,ip=<span class="hljs-number"><span class="hljs-number">192.168</span></span>.<span class="hljs-number"><span class="hljs-number">1.11</span></span>/<span class="hljs-number"><span class="hljs-number">24</span></span> \ -<span class="hljs-literal"><span class="hljs-literal">-rootfs</span></span>=volume=/dev/drbd100,shared=<span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br><p>  Neste comando, indicamos que o <strong>sistema raiz do</strong> nosso cont√™iner estar√° no dispositivo <code>/dev/drbd100</code> e adicionamos o par√¢metro <code>shared=1</code> para permitir a <strong>migra√ß√£o do</strong> cont√™iner entre os n√≥s. </p><br><p>  Se algo der errado, voc√™ sempre poder√° corrigi-lo atrav√©s da interface <strong>Proxmox</strong> ou na <code>/etc/pve/lxc/101.conf</code> cont√™iner <code>/etc/pve/lxc/101.conf</code> </p><br><p>  O Proxmox ir√° descompactar o modelo e preparar <strong>o sistema raiz do</strong> cont√™iner para n√≥s.  Depois disso, podemos lan√ßar nosso cont√™iner: </p><br><pre> <code class="hljs pgsql">pct <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span></code> </pre> <br><h2 id="nastroyka-nfs-servera">  Configure um servidor NFS. </h2><br><p>  Por padr√£o, o Proxmox <strong>n√£o permite que</strong> o <strong>servidor NFS</strong> seja executado no cont√™iner, mas existem v√°rias maneiras de habilit√°-lo. </p><br><p>  Uma delas √© apenas adicionar <code>lxc.apparmor.profile: unconfined</code> √† <code>/etc/pve/lxc/100.conf</code> do nosso cont√™iner <code>/etc/pve/lxc/100.conf</code> . </p><br><p>  Ou podemos <strong>habilitar o NFS</strong> para todos os cont√™ineres continuamente, para isso, precisamos atualizar o modelo padr√£o do LXC em todos os n√≥s, adicionar as seguintes linhas em <code>/etc/apparmor.d/lxc/lxc-default-cgns</code> : </p><br><pre> <code class="hljs nginx"> <span class="hljs-attribute"><span class="hljs-attribute">mount</span></span> fstype=nfs, mount fstype=nfs4, mount fstype=nfsd, mount fstype=rpc_pipefs,</code> </pre> <br><p>  Ap√≥s as altera√ß√µes, reinicie o cont√™iner: </p><br><pre> <code class="hljs pgsql">pct shutdown <span class="hljs-number"><span class="hljs-number">101</span></span> pct <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span></code> </pre> <br><p>  Agora vamos entrar nele: </p><br><pre> <code class="hljs perl">pct <span class="hljs-keyword"><span class="hljs-keyword">exec</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span> bash</code> </pre> <br><p>  Instale atualiza√ß√µes e <strong>servidor NFS</strong> : </p><br><pre> <code class="hljs powershell">apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> update apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> upgrade apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install nfs<span class="hljs-literal"><span class="hljs-literal">-kernel</span></span><span class="hljs-literal"><span class="hljs-literal">-server</span></span></code> </pre> <br><p>  Crie uma <strong>exporta√ß√£o</strong> : </p><br><pre> <code class="hljs haskell"><span class="hljs-title"><span class="hljs-title">echo</span></span> '/<span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class"> *(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">rw</span></span></span><span class="hljs-class">,</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">no_root_squash</span></span></span><span class="hljs-class">,</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">no_subtree_check</span></span></span><span class="hljs-class">)' &gt;&gt; /etc/exports mkdir /</span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class"> exportfs -a</span></span></code> </pre> <br><h2 id="nastroyka-ha">  Configura√ß√£o de HA </h2><br><p>  No momento da escrita, o proxmox <strong>HA-manager</strong> possui um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">bug</a> que n√£o permite que o cont√™iner de HA conclua com √™xito seu trabalho, como resultado dos processos de <strong>espa√ßo</strong> no <strong>kernel</strong> do <strong>servidor nfs</strong> que n√£o foram completamente mortos impedem que o dispositivo drbd saia do <strong>Secund√°rio</strong> .  Se voc√™ j√° encontrou essa situa√ß√£o, n√£o entre em p√¢nico e apenas execute <code>killall -9 nfsd</code> no n√≥ em que o cont√™iner foi iniciado e, em seguida, o dispositivo drbd dever√° "liberar" e passar√° para o <strong>secund√°rio</strong> . </p><br><p>  Para corrigir esse erro, execute os seguintes comandos em todos os n√≥s: </p><br><pre> <code class="hljs powershell">sed <span class="hljs-literal"><span class="hljs-literal">-i</span></span> <span class="hljs-string"><span class="hljs-string">'s/forceStop =&gt; 1,/forceStop =&gt; 0,/'</span></span> /usr/share/perl5/PVE/HA/Resources/PVECT.pm systemctl restart pve<span class="hljs-literal"><span class="hljs-literal">-ha</span></span><span class="hljs-literal"><span class="hljs-literal">-lrm</span></span>.service</code> </pre> <br><p>  Agora podemos passar para a configura√ß√£o do <strong>gerenciador de alta disponibilidade</strong> .  Vamos criar um grupo HA separado para o nosso dispositivo: </p><br><pre> <code class="hljs powershell">ha<span class="hljs-literal"><span class="hljs-literal">-manager</span></span> groupadd nfs1 -<span class="hljs-literal"><span class="hljs-literal">-nodes</span></span> pve1,pve2,pve3 -<span class="hljs-literal"><span class="hljs-literal">-nofailback</span></span>=<span class="hljs-number"><span class="hljs-number">1</span></span> -<span class="hljs-literal"><span class="hljs-literal">-restricted</span></span>=<span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br><p>  Nosso <strong>recurso</strong> funcionar√° apenas nos n√≥s especificados para este grupo.  Adicione nosso cont√™iner a este grupo: </p><br><pre> <code class="hljs powershell">ha<span class="hljs-literal"><span class="hljs-literal">-manager</span></span> add ct:<span class="hljs-number"><span class="hljs-number">101</span></span> -<span class="hljs-literal"><span class="hljs-literal">-group</span></span>=nfs1 -<span class="hljs-literal"><span class="hljs-literal">-max_relocate</span></span>=<span class="hljs-number"><span class="hljs-number">3</span></span> -<span class="hljs-literal"><span class="hljs-literal">-max_restart</span></span>=<span class="hljs-number"><span class="hljs-number">3</span></span></code> </pre> <br><p>  S√≥ isso.  Simples, certo? </p><br><p>  A <strong>bola nfs</strong> resultante pode ser conectada imediatamente ao Proxmox para armazenar e executar outras m√°quinas e cont√™ineres virtuais. </p><br><h2 id="rekomendacii-i-tyuning">  Recomenda√ß√µes e ajuste </h2><br><h5 id="drbd">  DRBD </h5><br><p>  Como observei acima, √© sempre aconselh√°vel usar uma rede separada para replica√ß√£o.  √â altamente recomend√°vel usar <strong>adaptadores de rede de 10 gigabit</strong> , caso contr√°rio, voc√™ ter√° velocidade de porta. <br>  Se a replica√ß√£o parecer lenta o suficiente, tente algumas das op√ß√µes para <strong>DRBD</strong> .  Aqui est√° a configura√ß√£o, que na minha opini√£o √© ideal para minha <strong>rede 10G</strong> : </p><br><pre> <code class="hljs swift"># cat /etc/drbd.d/global_common.conf global { usage-<span class="hljs-built_in"><span class="hljs-built_in">count</span></span> yes; udev-always-use-vnr; } common { handlers { } startup { } options { } disk { <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-fill-target 10M; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-<span class="hljs-built_in"><span class="hljs-built_in">max</span></span>-rate 720M; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-plan-ahead <span class="hljs-number"><span class="hljs-number">10</span></span>; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-<span class="hljs-built_in"><span class="hljs-built_in">min</span></span>-rate 20M; } net { <span class="hljs-built_in"><span class="hljs-built_in">max</span></span>-buffers 36k; sndbuf-size 1024k; rcvbuf-size 2048k; } }</code> </pre> <br><p>  Voc√™ pode obter mais informa√ß√µes sobre cada par√¢metro na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o oficial do DRBD.</a> </p><br><h5 id="nfs-server">  Servidor NFS </h5><br><p>  Para acelerar a opera√ß√£o do <strong>servidor NFS,</strong> pode ajudar a aumentar o n√∫mero total de <strong>inst√¢ncias em</strong> execu√ß√£o <strong>do</strong> servidor NFS.  Por padr√£o - <strong>8</strong> , pessoalmente, me ajudou a aumentar esse n√∫mero para <strong>64</strong> . </p><br><p>  Para conseguir isso, atualize o par√¢metro <code>RPCNFSDCOUNT=64</code> em <code>/etc/default/nfs-kernel-server</code> . <br>  E reinicie os daemons: </p><br><pre> <code class="hljs pgsql">systemctl <span class="hljs-keyword"><span class="hljs-keyword">restart</span></span> nfs-utils systemctl <span class="hljs-keyword"><span class="hljs-keyword">restart</span></span> nfs-<span class="hljs-keyword"><span class="hljs-keyword">server</span></span></code> </pre> <br><h5 id="nfsv3-vs-nfsv4">  NFSv3 vs NFSv4 </h5><br><p>  Sabe a diferen√ßa entre <strong>NFSv3</strong> e <strong>NFSv4</strong> ? </p><br><ul><li>  <strong>O NFSv3</strong> √© um <strong>protocolo sem estado,</strong> como regra, tolera melhor falhas e se recupera mais rapidamente. </li><li>  <strong>O NFSv4</strong> √© um <strong>protocolo com estado</strong> , funciona mais r√°pido e pode ser vinculado a determinadas portas tcp, mas devido √† presen√ßa de estado, √© mais sens√≠vel a falhas.  Ele tamb√©m tem a capacidade de usar a autentica√ß√£o usando o Kerberos e v√°rios outros recursos interessantes. </li></ul><br><p>  No entanto, quando voc√™ executa <code>showmount -e nfs_server</code> , o protocolo NFSv3 √© usado.  O Proxmox tamb√©m usa o NFSv3.  O NFSv3 tamb√©m √© comumente usado para organizar m√°quinas de inicializa√ß√£o de rede. </p><br><p>  Em geral, se voc√™ n√£o tiver um motivo espec√≠fico para usar o NFSv4, tente us√°-lo, pois √© menos doloroso para falhas devido √† falta de um estado como tal. </p><br><p>  Voc√™ pode montar a bola usando o NFSv3 especificando o par√¢metro <code>-o vers=3</code> para o comando <strong>mount</strong> : </p><br><pre> <code class="bash hljs">mount -o vers=3 nfs_server:/share /mnt</code> </pre> <br><p>  Se desejar, voc√™ pode desativar o NFSv4 para o servidor, para fazer isso, adicione a op√ß√£o <code>--no-nfs-version 4</code> √† vari√°vel <code>--no-nfs-version 4</code> e reinicie o servidor, por exemplo: </p><br><pre> <code class="bash hljs">RPCNFSDCOUNT=<span class="hljs-string"><span class="hljs-string">"64 --no-nfs-version 4"</span></span></code> </pre> <br><h2 id="iscsi-i-lvm">  iSCSI e LVM </h2><br><p>  Da mesma forma, um <strong>daemon tgt</strong> regular pode ser configurado dentro do cont√™iner, o iSCSI produzir√° um desempenho significativamente mais alto para opera√ß√µes de E / S, e o cont√™iner funcionar√° mais tranq√ºilamente porque o servidor tgt funciona completamente no espa√ßo do usu√°rio. </p><br><p>  Normalmente, um <strong>LUN</strong> exportado √© dividido em v√°rias partes usando o <strong>LVM</strong> .  No entanto, h√° v√°rias nuances a serem consideradas, por exemplo: como os <strong>bloqueios</strong> LVM <strong>s√£o</strong> fornecidos para o compartilhamento de um grupo exportado em v√°rios hosts. </p><br><p>  Talvez essas e outras nuances sejam descritas no <strong><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pr√≥ximo artigo</a></strong> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt417473/">https://habr.com/ru/post/pt417473/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../In146148/index.html">Runetology (152): Slon.ru ‡§ï‡•á ‡§™‡•ç‡§∞‡§Æ‡•Å‡§ñ ‡§Æ‡•à‡§ï‡•ç‡§∏‡§ø‡§Æ ‡§ï‡§æ‡§∂‡•Å‡§≤‡§ø‡§Ç‡§∏‡•ç‡§ï‡•Ä</a></li>
<li><a href="../In146149/index.html">‡§ì‡§°‡•á‡§∏‡§æ ‡§Æ‡•á‡§Ç ‡§ó‡•ç‡§∞‡•Ä‡§∑‡•ç‡§Æ‡§ï‡§æ‡§≤‡•Ä‡§® ‡§¨‡§æ‡§∞‡§ï‡•à‡§Æ‡•ç‡§™</a></li>
<li><a href="../In146150/index.html">‡§Æ‡•Ä‡§°‡§ø‡§Ø‡§æ ‡§™‡•ç‡§≤‡•á‡§Ø‡§∞ ‡§ï‡•ã ‡§®‡•á‡§ü‡§ü‡•â‡§™ ‡§Æ‡•á‡§Ç ‡§ï‡•à‡§∏‡•á ‡§¨‡§¶‡§≤‡•á‡§Ç?</a></li>
<li><a href="../In146151/index.html">‡§è‡§ú‡§æ‡§á‡§≤ ‡§°‡§æ‡§á‡§µ ‡§°‡•á‡§™‡•ç‡§•: ‡§è‡§ú‡§æ‡§á‡§≤ ‡§á‡§µ‡•à‡§≤‡•ç‡§Ø‡•Ç‡§è‡§∂‡§® ‡§´‡•ç‡§∞‡•á‡§Æ‡§µ‡§∞‡•ç‡§ï</a></li>
<li><a href="../In146152/index.html">‡§ü‡•à‡§≤‡•á‡§Ç‡§ü ‡§Æ‡•à‡§™ ‡§∞‡§ø‡§ú‡•ç‡§Ø‡•Ç‡§Æ‡•á ‡§µ‡§ø‡§ú‡§º‡•Å‡§Ö‡§≤‡§æ‡§á‡§ú‡§º‡§∞ - ‡§∞‡§ø‡§≤‡•Ä‡§ú‡§º ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§∞‡§æ‡§∏‡•ç‡§§‡•á ‡§™‡§∞ ‡§è‡§°‡§µ‡•á‡§Ç‡§ö‡§∞‡•ç‡§∏</a></li>
<li><a href="../pt417475/index.html">Codifica√ß√£o Glusterfs + apagamento: quando voc√™ precisar de muito, barato e confi√°vel</a></li>
<li><a href="../pt417477/index.html">Secret√°ria quente</a></li>
<li><a href="../pt417479/index.html">Concatena√ß√£o de string mais r√°pida do tipo fa√ßa voc√™ mesmo no Go</a></li>
<li><a href="../pt417481/index.html">Sobre geradores no JavaScript ES6 e por que √© opcional estud√°-los</a></li>
<li><a href="../pt417483/index.html">Compara√ß√£o de estruturas JS: React, Vue e Hyperapp</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>