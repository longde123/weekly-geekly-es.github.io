<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚õπüèº ü•ò ü•© Apache Ignite + Apache Spark-Datenrahmen: zusammen mehr Spa√ü üå± üßü ‚õπüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Habr! Mein Name ist Nikolai Izhikov, ich arbeite f√ºr Sberbank Technologies im Open Source Solutions Development Team. Hinter 15 Jahren kommerzie...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Apache Ignite + Apache Spark-Datenrahmen: zusammen mehr Spa√ü</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/sberbank/blog/427297/">  Hallo Habr!  Mein Name ist Nikolai Izhikov, ich arbeite f√ºr Sberbank Technologies im Open Source Solutions Development Team.  Hinter 15 Jahren kommerzieller Entwicklung in Java.  Ich bin ein Apache Ignite-Committer und Apache Kafka-Mitarbeiter. <br><br>  Unter der Katze finden Sie eine Video- und Textversion meines Berichts √ºber Apache Ignite Meetup, wie Sie Apache Ignite mit Apache Spark verwenden und welche Funktionen wir daf√ºr implementiert haben. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f42/5f3/df5/f425f3df59ff99d03d4a3e6aff3b2655.png"><br><a name="habracut"></a><br><h2>  Was Apache Spark kann </h2><br>  Was ist Apache Spark?  Mit diesem Produkt k√∂nnen Sie schnell verteilte Computer- und Analyseabfragen durchf√ºhren.  Grunds√§tzlich ist Apache Spark in Scala geschrieben. <br><br>  Apache Spark verf√ºgt √ºber eine umfangreiche API zum Herstellen einer Verbindung zu verschiedenen Speichersystemen oder zum Empfangen von Daten.  Eine der Funktionen des Produkts ist eine universelle SQL-√§hnliche Abfrage-Engine f√ºr Daten, die aus verschiedenen Quellen empfangen werden.  Wenn Sie √ºber mehrere Informationsquellen verf√ºgen, diese kombinieren und einige Ergebnisse erzielen m√∂chten, ist Apache Spark genau das Richtige f√ºr Sie. <br><br>  Eine der wichtigsten Abstraktionen, die Spark bereitstellt, ist Data Frame, DataSet.  In Bezug auf eine relationale Datenbank ist dies eine Tabelle, eine Quelle, die Daten auf strukturierte Weise bereitstellt.  Die Struktur, der Typ jeder Spalte, ihr Name usw. sind bekannt.  Datenrahmen k√∂nnen aus verschiedenen Quellen erstellt werden.  Beispiele hierf√ºr sind JSON-Dateien, relationale Datenbanken, verschiedene Hadoop-Systeme und Apache Ignite. <br><br>  Spark unterst√ºtzt Joins in SQL-Abfragen.  Sie k√∂nnen Daten aus verschiedenen Quellen kombinieren und Ergebnisse erhalten sowie analytische Abfragen durchf√ºhren.  Dar√ºber hinaus gibt es eine API zum Speichern von Daten.  Wenn Sie die Abfragen abgeschlossen und eine Studie durchgef√ºhrt haben, bietet Spark die M√∂glichkeit, die Ergebnisse auf dem Empf√§nger zu speichern, der diese Funktion unterst√ºtzt, und dementsprechend das Problem der Datenverarbeitung zu l√∂sen. <br><br><h2>  Welche Funktionen haben wir f√ºr die Integration von Apache Spark in Apache Ignite implementiert? </h2><br><ol><li>  Lesen von Daten aus Apache Ignite SQL-Tabellen. </li><li>  Schreiben von Daten in Apache Ignite SQL-Tabellen. </li><li>  IgniteCatalog in IgniteSparkSession - die M√∂glichkeit, alle vorhandenen Ignite SQL-Tabellen zu verwenden, ohne sich "von Hand" zu registrieren. </li><li>  SQL-Optimierung - Die M√∂glichkeit, SQL-Anweisungen in Ignite auszuf√ºhren. </li></ol><br>  Apache Spark kann Daten aus Apache Ignite SQL-Tabellen lesen und in Form einer solchen Tabelle schreiben.  Jeder in Spark gebildete DataFrame kann als Apache Ignite SQL-Tabelle gespeichert werden. <br><br>  Mit Apache Ignite k√∂nnen Sie alle vorhandenen Ignite SQL-Tabellen in Spark Session verwenden, ohne sich "von Hand" zu registrieren. Verwenden Sie dazu IgniteCatalog in der Standarderweiterung SparkSession - IgniteSparkSession. <br><br>  Hier m√ºssen Sie etwas tiefer in das Spark-Ger√§t einsteigen.  In Bezug auf eine regul√§re Datenbank ist ein Verzeichnis ein Ort, an dem Metainformationen gespeichert werden: Welche Tabellen sind verf√ºgbar, welche Spalten befinden sich in ihnen usw.  Wenn eine Anfrage eintrifft, werden Metainformationen aus dem Katalog abgerufen und die SQL-Engine macht etwas mit Tabellen und Daten.  Standardm√§√üig m√ºssen in Spark alle gelesenen Tabellen (egal, aus einer relationalen Datenbank, Ignite, Hadoop) manuell in Sitzungen registriert werden.  Als Ergebnis erhalten Sie die M√∂glichkeit, eine SQL-Abfrage f√ºr diese Tabellen durchzuf√ºhren.  Spark erf√§hrt davon. <br><br>  Um mit den Daten zu arbeiten, die wir auf Ignite hochgeladen haben, m√ºssen wir die Tabellen registrieren.  Anstatt jede Tabelle mit unseren H√§nden zu registrieren, haben wir die M√∂glichkeit implementiert, automatisch auf alle Ignite-Tabellen zuzugreifen. <br><br>  Was ist die Funktion hier?  Aus irgendeinem Grund wei√ü ich nicht, dass das Verzeichnis in Spark eine interne API ist, d. H.  Ein Au√üenstehender kann nicht kommen und seine eigene Katalogimplementierung erstellen.  Und da Spark aus Hadoop hervorgegangen ist, unterst√ºtzt es nur Hive.  Und Sie m√ºssen alles andere mit Ihren H√§nden registrieren.  Benutzer fragen h√§ufig, wie Sie dies umgehen und sofort SQL-Abfragen durchf√ºhren k√∂nnen.  Ich habe ein Verzeichnis implementiert, mit dem Sie Ignite-Tabellen durchsuchen und darauf zugreifen k√∂nnen, ohne ~ und sms ~ zu registrieren, und diesen Patch zun√§chst in der Spark-Community vorgeschlagen, auf die ich eine Antwort erhalten habe: Ein solcher Patch ist aus internen Gr√ºnden nicht interessant.  Und sie gaben die interne API nicht heraus. <br><br>  Jetzt ist der Ignite-Katalog eine interessante Funktion, die mithilfe der internen API von Spark implementiert wurde.  Um dieses Verzeichnis zu verwenden, haben wir eine eigene Implementierung der Sitzung. Dies ist die √ºbliche SparkSession, in der Sie Anforderungen stellen und Daten verarbeiten k√∂nnen.  Die Unterschiede bestehen darin, dass wir ExternalCatalog f√ºr die Arbeit mit Ignite-Tabellen sowie IgniteOptimization integriert haben, die im Folgenden beschrieben werden. <br><br>  <b>SQL-Optimierung</b> - Die M√∂glichkeit, SQL-Anweisungen in Ignite auszuf√ºhren.  Standardm√§√üig liest Spark beim Ausf√ºhren von Verkn√ºpfungen, Gruppierungen, Aggregatberechnungen und anderen komplexen SQL-Abfragen Daten zeilenweise.  Die Datenquelle kann nur Zeilen effizient herausfiltern. <br><br>  Wenn Sie Join oder Gruppierung verwenden, zieht Spark alle Daten aus der Tabelle mithilfe der angegebenen Filter in den Arbeitsspeicher des Workers und gruppiert sie erst dann oder f√ºhrt andere SQL-Vorg√§nge aus.  Im Fall von Ignite ist dies nicht optimal, da Ignite selbst eine verteilte Architektur hat und die darin gespeicherten Daten kennt.  Daher kann Ignite selbst Aggregate effizient berechnen und Gruppierungen durchf√ºhren.  Dar√ºber hinaus kann es viele Daten geben, und um sie zu gruppieren, m√ºssen Sie alles subtrahieren und alle Daten in Spark erh√∂hen, was ziemlich teuer ist. <br><br>  Spark bietet eine API, mit der Sie den urspr√ºnglichen Plan der SQL-Abfrage √§ndern, eine Optimierung durchf√ºhren und den Teil der SQL-Abfrage, der dort ausgef√ºhrt werden kann, an Ignite weiterleiten k√∂nnen.  Dies ist sowohl hinsichtlich der Geschwindigkeit als auch des Speicherverbrauchs effektiv, da wir damit keine Daten abrufen, die sofort gruppiert werden. <br><br><h2>  Wie funktioniert es? </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/b28/1df/0ef/b281df0ef5f2ea2a08f73267ef7f5edb.png"><br><br>  Wir haben einen Ignite-Cluster - dies ist die untere Bildh√§lfte.  Es gibt keinen Zookeeper, da es nur f√ºnf Knoten gibt.  Es gibt Spark-Worker. In jedem Worker wird der Ignite-Client-Knoten ausgel√∂st.  Dadurch k√∂nnen wir eine Anfrage stellen, die Daten lesen und mit dem Cluster interagieren.  Au√üerdem steigt der Clientknoten in IgniteSparkSession an, damit das Verzeichnis funktioniert. <br><br><h2>  Datenrahmen entz√ºnden </h2><br>  Wir wenden uns dem Code zu: Wie liest man Daten aus einer SQL-Tabelle?  Bei Spark ist alles ganz einfach und gut: Wir sagen, wir wollen einige Daten berechnen, geben das Format an - dies ist eine bestimmte Konstante.  Dar√ºber hinaus haben wir mehrere Optionen - den Pfad zur Konfigurationsdatei f√ºr den Clientknoten, der beim Lesen von Daten beginnt.  Wir geben an, welche Tabelle wir lesen m√∂chten, und weisen Spark an, sie zu laden.  Wir bekommen die Daten und k√∂nnen damit machen, was wir wollen. <br><br><pre><code class="scala hljs">spark.read .format(<span class="hljs-type"><span class="hljs-type">FORMAT_IGNITE</span></span>) .option(<span class="hljs-type"><span class="hljs-type">OPTION_CONFIG_FILE</span></span>, <span class="hljs-type"><span class="hljs-type">TEST_CONFIG_FILE</span></span>) .option(<span class="hljs-type"><span class="hljs-type">OPTION_TABLE</span></span>, <span class="hljs-string"><span class="hljs-string">"person"</span></span>) .load()</code> </pre> <br>  Nachdem wir die Daten generiert haben - optional aus Ignite, aus einer beliebigen Quelle - k√∂nnen wir genauso einfach alles speichern, indem wir das Format und die entsprechende Tabelle angeben.  Wir befehlen Spark zu schreiben, wir geben ein Format an.  In der Konfiguration schreiben wir vor, zu welchem ‚Äã‚ÄãCluster eine Verbindung hergestellt werden soll.  Geben Sie die Tabelle an, in der wir speichern m√∂chten.  Dar√ºber hinaus k√∂nnen wir Dienstprogrammoptionen vorschreiben - geben Sie den Prim√§rschl√ºssel an, den wir in dieser Tabelle erstellen.  Wenn die Daten einfach gest√∂rt werden, ohne eine Tabelle zu erstellen, wird dieser Parameter nicht ben√∂tigt.  Klicken Sie am Ende auf Speichern und die Daten werden geschrieben. <br><br><pre> <code class="scala hljs">tbl.write. format(<span class="hljs-type"><span class="hljs-type">FORMAT_IGNITE</span></span>). option(<span class="hljs-type"><span class="hljs-type">OPTION_CONFIG_FILE</span></span>, <span class="hljs-type"><span class="hljs-type">CFG_PATH</span></span>). option(<span class="hljs-type"><span class="hljs-type">OPTION_TABLE</span></span>, tableName). option(<span class="hljs-type"><span class="hljs-type">OPTION_CREATE_TABLE_PRIMARY_KEY_FIELDS</span></span>, pk). save</code> </pre><br>  Nun wollen wir sehen, wie alles funktioniert. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b35/41a/b86/b3541ab86eca15cd240765bf15907979.png"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">LoadDataExample.scala</a> <br><br>  Diese offensichtliche Anwendung demonstriert zun√§chst die Aufnahmefunktionen.  Zum Beispiel habe ich die Daten zu Fu√üballspielen ausgew√§hlt und Statistiken von einer bekannten Ressource heruntergeladen.  Es enth√§lt Informationen zu Turnieren: Ligen, Spiele, Spieler, Teams, Spielerattribute, Teamattribute - Daten, die Fu√üballspiele in Ligen europ√§ischer L√§nder (England, Frankreich, Spanien usw.) beschreiben. <br><br>  Ich m√∂chte sie auf Ignite hochladen.  Wir erstellen eine Spark-Sitzung, geben die Adresse des Assistenten an und rufen das Laden dieser Tabellen auf, wobei wir Parameter √ºbergeben.  Das Beispiel ist in Scala, nicht in Java, weil Scala weniger ausf√ºhrlich und zum Beispiel besser ist. <br><br>  Wir √ºbertragen den Dateinamen, lesen ihn und geben an, dass er mehrzeilig ist. Dies ist eine Standard-JSON-Datei.  Dann schreiben wir in Ignite.  Die Struktur unserer Datei ist nirgends zu beschreiben - Spark selbst bestimmt, welche Daten wir haben und wie sie strukturiert sind.  Wenn alles reibungslos verl√§uft, wird eine Tabelle erstellt, in der alle erforderlichen Felder der erforderlichen Datentypen vorhanden sind.  So k√∂nnen wir alles in Ignite laden. <br><br>  Wenn die Daten geladen sind, k√∂nnen wir sie in Ignite sehen und sofort verwenden.  Als einfaches Beispiel eine Abfrage, mit der Sie wissen, welche Mannschaft die meisten Spiele gespielt hat.  Wir haben zwei Kolumnen: Heimteam und Ausw√§rtsteam, Gastgeber und G√§ste.  Wir w√§hlen, gruppieren, z√§hlen, summieren und verbinden Daten mit dem Befehl, um den Namen des Befehls einzugeben.  Ta-dam - und die Daten von json-chiks, die wir in Ignite erhalten haben.  Wir sehen Paris Saint-Germain, Toulouse - wir haben viele Daten √ºber die franz√∂sischen Mannschaften. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8a4/202/52b/8a420252be6fb8df3a9083d7411911a9.png"><br><br>  Wir fassen zusammen.  Wir haben jetzt Daten aus der Quelle, der JSON-Datei, zu Ignite hochgeladen, und das ziemlich schnell.  Aus Sicht von Big Data ist dies m√∂glicherweise nicht zu gro√ü, aber f√ºr einen lokalen Computer angemessen.  Das Tabellenschema wird in seiner urspr√ºnglichen Form aus der JSON-Datei √ºbernommen.  Die Tabelle wurde erstellt, die Spaltennamen wurden aus der Quelldatei kopiert, der Prim√§rschl√ºssel wurde erstellt.  ID ist √ºberall und der Prim√§rschl√ºssel ist ID.  Diese Daten sind in Ignite eingegangen, wir k√∂nnen sie verwenden. <br><br><h2>  IgniteSparkSession und IgniteCatalog </h2><br>  Mal sehen, wie es funktioniert. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/654/24a/4ee/65424a4eeda4a4c2c6cce7038e13d1a9.png"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CatalogExample.scala</a> <br><br>  Auf relativ einfache Weise k√∂nnen Sie auf alle Ihre Daten zugreifen und diese abfragen.  Im letzten Beispiel haben wir die Standard-Spark-Sitzung gestartet.  Und es gab dort keine Ignite-Spezifit√§t - au√üer dass Sie ein JAR mit der richtigen Datenquelle platzieren m√ºssen - v√∂llig Standardarbeit √ºber die √∂ffentliche API.  Wenn Sie jedoch automatisch auf Ignite-Tabellen zugreifen m√∂chten, k√∂nnen Sie unsere Erweiterung verwenden.  Der Unterschied ist, dass wir anstelle von SparkSession IgniteSparkSession schreiben. <br><br>  Sobald Sie ein IgniteSparkSession-Objekt erstellen, werden im Verzeichnis alle Tabellen angezeigt, die gerade in Ignite geladen wurden.  Sie k√∂nnen ihr Diagramm und alle Informationen sehen.  Spark kennt bereits die Tabellen von Ignite und Sie k√∂nnen problemlos alle Daten abrufen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/dec/f1b/a0c/decf1ba0c5db2e0d84e50a0e88b6c192.png"><br><br><h2>  Igniteoptimization </h2><br>  Wenn Sie in Ignite mit JOIN komplexe Abfragen durchf√ºhren, zieht Spark zuerst die Daten heraus und erst dann gruppiert JOIN sie.  Um den Prozess zu optimieren, haben wir die IgniteOptimization-Funktion entwickelt. Sie optimiert den Spark-Abfrageplan und erm√∂glicht es Ihnen, die Teile der Anforderung weiterzuleiten, die in Ignite in Ignite ausgef√ºhrt werden k√∂nnen.  Wir zeigen Optimierung auf eine bestimmte Anfrage. <br><br><pre> <code class="sql hljs">SQL Query: <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span>   city_id,   <span class="hljs-keyword"><span class="hljs-keyword">count</span></span>(*) <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span>   person p <span class="hljs-keyword"><span class="hljs-keyword">GROUP</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> city_id <span class="hljs-keyword"><span class="hljs-keyword">HAVING</span></span> <span class="hljs-keyword"><span class="hljs-keyword">count</span></span>(*) &gt; <span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br>  Wir erf√ºllen die Anfrage.  Wir haben einen Personentisch - einige Angestellte, Leute.  Jeder Mitarbeiter kennt den Ausweis der Stadt, in der er lebt.  Wir wollen wissen, wie viele Menschen in jeder Stadt leben.  Wir filtern - in welcher Stadt mehr als eine Person lebt.  Hier ist der erste Plan, den Spark erstellt: <br><br><pre> <code class="scala hljs">== <span class="hljs-type"><span class="hljs-type">Analyzed</span></span> <span class="hljs-type"><span class="hljs-type">Logical</span></span> <span class="hljs-type"><span class="hljs-type">Plan</span></span> == city_id: bigint, count(<span class="hljs-number"><span class="hljs-number">1</span></span>): bigint <span class="hljs-type"><span class="hljs-type">Project</span></span> [city_id#<span class="hljs-number"><span class="hljs-number">19</span></span>L, count(<span class="hljs-number"><span class="hljs-number">1</span></span>)#<span class="hljs-number"><span class="hljs-number">52</span></span>L] +- <span class="hljs-type"><span class="hljs-type">Filter</span></span> (count(<span class="hljs-number"><span class="hljs-number">1</span></span>)#<span class="hljs-number"><span class="hljs-number">54</span></span>L &gt; cast(<span class="hljs-number"><span class="hljs-number">1</span></span> as bigint))  +- <span class="hljs-type"><span class="hljs-type">Aggregate</span></span> [city_id#<span class="hljs-number"><span class="hljs-number">19</span></span>L], [city_id#<span class="hljs-number"><span class="hljs-number">19</span></span>L, count(<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-type"><span class="hljs-type">AS</span></span> count(<span class="hljs-number"><span class="hljs-number">1</span></span>)#<span class="hljs-number"><span class="hljs-number">52</span></span>L, count(<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-type"><span class="hljs-type">AS</span></span> count(<span class="hljs-number"><span class="hljs-number">1</span></span>)#<span class="hljs-number"><span class="hljs-number">54</span></span>L] +- <span class="hljs-type"><span class="hljs-type">SubqueryAlias</span></span> p    +- <span class="hljs-type"><span class="hljs-type">SubqueryAlias</span></span> person       +- <span class="hljs-type"><span class="hljs-type">Relation</span></span>[<span class="hljs-type"><span class="hljs-type">NAME</span></span>#<span class="hljs-number"><span class="hljs-number">11</span></span>,<span class="hljs-type"><span class="hljs-type">BIRTH_DATE</span></span>#<span class="hljs-number"><span class="hljs-number">12</span></span>,<span class="hljs-type"><span class="hljs-type">IS_RESIDENT</span></span>#<span class="hljs-number"><span class="hljs-number">13</span></span>,<span class="hljs-type"><span class="hljs-type">SALARY</span></span>#<span class="hljs-number"><span class="hljs-number">14</span></span>,<span class="hljs-type"><span class="hljs-type">PENSION</span></span>#<span class="hljs-number"><span class="hljs-number">15</span></span>,<span class="hljs-type"><span class="hljs-type">ACCOUNT</span></span>#<span class="hljs-number"><span class="hljs-number">16</span></span>,<span class="hljs-type"><span class="hljs-type">AGE</span></span>#<span class="hljs-number"><span class="hljs-number">17</span></span>,<span class="hljs-type"><span class="hljs-type">ID</span></span>#<span class="hljs-number"><span class="hljs-number">18</span></span>L,<span class="hljs-type"><span class="hljs-type">CITY_ID</span></span>#<span class="hljs-number"><span class="hljs-number">19</span></span>L]         <span class="hljs-type"><span class="hljs-type">IgniteSQLRelation</span></span>[table=<span class="hljs-type"><span class="hljs-type">PERSON</span></span>]</code> </pre><br>  Die Beziehung ist nur eine Ignite-Tabelle.  Es gibt keine Filter - wir pumpen einfach alle Daten aus der Personentabelle √ºber das Netzwerk aus dem Cluster aus.  Dann aggregiert Spark all dies - entsprechend der Anforderung und gibt das Ergebnis der Anforderung zur√ºck. <br><br>  Es ist leicht zu erkennen, dass all dieser Teilbaum mit Filter und Aggregation in Ignite ausgef√ºhrt werden kann.  Dies ist viel effizienter als das Abrufen aller Daten aus einer potenziell gro√üen Tabelle in Spark - genau das leistet unsere IgniteOptimization-Funktion.  Nach der Analyse und Optimierung des Baums erhalten wir den folgenden Plan: <br><br><pre> <code class="scala hljs">== <span class="hljs-type"><span class="hljs-type">Optimized</span></span> <span class="hljs-type"><span class="hljs-type">Logical</span></span> <span class="hljs-type"><span class="hljs-type">Plan</span></span> == <span class="hljs-type"><span class="hljs-type">Relation</span></span>[<span class="hljs-type"><span class="hljs-type">CITY_ID</span></span>#<span class="hljs-number"><span class="hljs-number">19</span></span>L,<span class="hljs-type"><span class="hljs-type">COUNT</span></span>(<span class="hljs-number"><span class="hljs-number">1</span></span>)#<span class="hljs-number"><span class="hljs-number">52</span></span>L]   <span class="hljs-type"><span class="hljs-type">IgniteSQLAccumulatorRelation</span></span>(     columns=[<span class="hljs-type"><span class="hljs-type">CITY_ID</span></span>, <span class="hljs-type"><span class="hljs-type">COUNT</span></span>(<span class="hljs-number"><span class="hljs-number">1</span></span>)], qry=<span class="hljs-type"><span class="hljs-type">SELECT</span></span> <span class="hljs-type"><span class="hljs-type">CITY_ID</span></span>, <span class="hljs-type"><span class="hljs-type">COUNT</span></span>(<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-type"><span class="hljs-type">FROM</span></span> <span class="hljs-type"><span class="hljs-type">PERSON</span></span> <span class="hljs-type"><span class="hljs-type">GROUP</span></span> <span class="hljs-type"><span class="hljs-type">BY</span></span> city_id <span class="hljs-type"><span class="hljs-type">HAVING</span></span> count(<span class="hljs-number"><span class="hljs-number">1</span></span>) &gt; <span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br>  Als Ergebnis erhalten wir nur eine Beziehung, da wir den gesamten Baum optimiert haben.  Und im Inneren k√∂nnen Sie bereits sehen, dass Ignite eine Anfrage sendet, die nahe genug an der urspr√ºnglichen Anfrage liegt. <br><br>  Angenommen, wir verbinden verschiedene Datenquellen: Zum Beispiel haben wir einen DataFrame von Ignite, den zweiten von json, den dritten erneut von Ignite und den vierten von einer Art relationaler Datenbank.  In diesem Fall wird nur der Teilbaum im Plan optimiert.  Wir optimieren, was wir k√∂nnen, legen es in Ignite ab und Spark erledigt den Rest.  Dadurch erhalten wir einen Geschwindigkeitsgewinn. <br><br>  Ein weiteres Beispiel mit JOIN: <br><br><pre> <code class="sql hljs">SQL Query - <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> jt1.id <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> id1, jt1.val1, jt2.id <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> id2, jt2.val2 <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> jt1 <span class="hljs-keyword"><span class="hljs-keyword">JOIN</span></span> jt2 <span class="hljs-keyword"><span class="hljs-keyword">ON</span></span> jt1.val1 = jt2.val2</code> </pre><br>  Wir haben zwei Tische.  Wir halten nach Wert zusammen und w√§hlen aus allen aus - IDs, Werte.  Spark bietet einen solchen Plan an: <br><br><pre> <code class="scala hljs">== <span class="hljs-type"><span class="hljs-type">Analyzed</span></span> <span class="hljs-type"><span class="hljs-type">Logical</span></span> <span class="hljs-type"><span class="hljs-type">Plan</span></span> == id1: bigint, val1: string, id2: bigint, val2: string <span class="hljs-type"><span class="hljs-type">Project</span></span> [id#<span class="hljs-number"><span class="hljs-number">4</span></span>L <span class="hljs-type"><span class="hljs-type">AS</span></span> id1#<span class="hljs-number"><span class="hljs-number">84</span></span>L, val1#<span class="hljs-number"><span class="hljs-number">3</span></span>, id#<span class="hljs-number"><span class="hljs-number">6</span></span>L <span class="hljs-type"><span class="hljs-type">AS</span></span> id2#<span class="hljs-number"><span class="hljs-number">85</span></span>L, val2#<span class="hljs-number"><span class="hljs-number">5</span></span>] +- <span class="hljs-type"><span class="hljs-type">Join</span></span> <span class="hljs-type"><span class="hljs-type">Inner</span></span>, (val1#<span class="hljs-number"><span class="hljs-number">3</span></span> = val2#<span class="hljs-number"><span class="hljs-number">5</span></span>) :- <span class="hljs-type"><span class="hljs-type">SubqueryAlias</span></span> jt1 : +- <span class="hljs-type"><span class="hljs-type">Relation</span></span>[<span class="hljs-type"><span class="hljs-type">VAL1</span></span>#<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-type"><span class="hljs-type">ID</span></span>#<span class="hljs-number"><span class="hljs-number">4</span></span>L] <span class="hljs-type"><span class="hljs-type">IgniteSQLRelation</span></span>[table=<span class="hljs-type"><span class="hljs-type">JT1</span></span>] +- <span class="hljs-type"><span class="hljs-type">SubqueryAlias</span></span> jt2    +- <span class="hljs-type"><span class="hljs-type">Relation</span></span>[<span class="hljs-type"><span class="hljs-type">VAL2</span></span>#<span class="hljs-number"><span class="hljs-number">5</span></span>,<span class="hljs-type"><span class="hljs-type">ID</span></span>#<span class="hljs-number"><span class="hljs-number">6</span></span>L] <span class="hljs-type"><span class="hljs-type">IgniteSQLRelation</span></span>[table=<span class="hljs-type"><span class="hljs-type">JT2</span></span>]</code> </pre> <br>  Wir sehen, dass er alle Daten aus einer Tabelle herausziehen wird, alle Daten aus der zweiten, sie in sich zusammenf√ºgen und die Ergebnisse geben wird.  Nach der Verarbeitung und Optimierung erhalten wir genau die gleiche Anfrage, die an Ignite geht, wo sie relativ schnell ausgef√ºhrt wird. <br><br><pre> <code class="scala hljs">== <span class="hljs-type"><span class="hljs-type">Optimized</span></span> <span class="hljs-type"><span class="hljs-type">Logical</span></span> <span class="hljs-type"><span class="hljs-type">Plan</span></span> == <span class="hljs-type"><span class="hljs-type">Relation</span></span>[<span class="hljs-type"><span class="hljs-type">ID</span></span>#<span class="hljs-number"><span class="hljs-number">84</span></span>L,<span class="hljs-type"><span class="hljs-type">VAL1</span></span>#<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-type"><span class="hljs-type">ID</span></span>#<span class="hljs-number"><span class="hljs-number">85</span></span>L,<span class="hljs-type"><span class="hljs-type">VAL2</span></span>#<span class="hljs-number"><span class="hljs-number">5</span></span>] <span class="hljs-type"><span class="hljs-type">IgniteSQLAccumulatorRelation</span></span>(columns=[<span class="hljs-type"><span class="hljs-type">ID</span></span>, <span class="hljs-type"><span class="hljs-type">VAL1</span></span>, <span class="hljs-type"><span class="hljs-type">ID</span></span>, <span class="hljs-type"><span class="hljs-type">VAL2</span></span>], qry= <span class="hljs-type"><span class="hljs-type">SELECT</span></span> <span class="hljs-type"><span class="hljs-type">JT1</span></span>.<span class="hljs-type"><span class="hljs-type">ID</span></span> <span class="hljs-type"><span class="hljs-type">AS</span></span> id1, <span class="hljs-type"><span class="hljs-type">JT1</span></span>.<span class="hljs-type"><span class="hljs-type">VAL1</span></span>, <span class="hljs-type"><span class="hljs-type">JT2</span></span>.<span class="hljs-type"><span class="hljs-type">ID</span></span> <span class="hljs-type"><span class="hljs-type">AS</span></span> id2, <span class="hljs-type"><span class="hljs-type">JT2</span></span>.<span class="hljs-type"><span class="hljs-type">VAL2</span></span> <span class="hljs-type"><span class="hljs-type">FROM</span></span> <span class="hljs-type"><span class="hljs-type">JT1</span></span> <span class="hljs-type"><span class="hljs-type">JOIN</span></span> <span class="hljs-type"><span class="hljs-type">JT2</span></span> <span class="hljs-type"><span class="hljs-type">ON</span></span> <span class="hljs-type"><span class="hljs-type">JT1</span></span>.val1 = <span class="hljs-type"><span class="hljs-type">JT2</span></span>.val2 <span class="hljs-type"><span class="hljs-type">WHERE</span></span> <span class="hljs-type"><span class="hljs-type">JT1</span></span>.val1 <span class="hljs-type"><span class="hljs-type">IS</span></span> <span class="hljs-type"><span class="hljs-type">NOT</span></span> <span class="hljs-type"><span class="hljs-type">NULL</span></span> <span class="hljs-type"><span class="hljs-type">AND</span></span> <span class="hljs-type"><span class="hljs-type">JT2</span></span>.val2 <span class="hljs-type"><span class="hljs-type">IS</span></span> <span class="hljs-type"><span class="hljs-type">NOT</span></span> <span class="hljs-type"><span class="hljs-type">NULL</span></span>)</code> </pre> <br>  Ich werde Ihnen ein Beispiel zeigen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ba4/39a/493/ba439a493e76dd573966cad413c07650.png"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OptimizationExample.scala</a> <br><br>  Wir erstellen eine IgniteSpark-Sitzung, in der alle unsere Optimierungsfunktionen bereits automatisch enthalten sind.  Hier lautet die Anfrage: Finde die Spieler mit der h√∂chsten Bewertung und zeige ihre Namen an.  In der Spielertabelle ihre Attribute und Daten.  Wir schlie√üen uns an, filtern Junk-Daten und zeigen Spieler mit der h√∂chsten Bewertung an.  Lassen Sie uns sehen, welche Art von Plan wir nach der Optimierung erhalten haben, und die Ergebnisse dieser Abfrage anzeigen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c7d/c51/9ab/c7dc519abdfa6b3b1d7a8396ef9725b3.png"><br><br>  Wir fangen an.  Wir sehen bekannte Nachnamen: Messi, Buffon, Ronaldo usw.  √úbrigens treffen sich einige aus irgendeinem Grund in zwei Formen - sowohl Messi als auch Ronaldo.  Fu√üballliebhaber finden es vielleicht seltsam, dass unbekannte Spieler auf der Liste erscheinen.  Dies sind Torh√ºter, Spieler mit ziemlich hohen Eigenschaften - vor dem Hintergrund anderer Spieler.  Nun schauen wir uns den Abfrageplan an, der ausgef√ºhrt wurde.  In Spark wurde fast nichts getan, dh wir haben die gesamte Anfrage erneut an Ignite gesendet. <br><br><h2>  Apache Ignite-Entwicklung </h2><br>  Unser Projekt ist ein Open Source-Produkt, daher freuen wir uns immer √ºber Patches und Feedback von Entwicklern.  Ihre Hilfe, Feedback, Patches sind sehr willkommen.  Wir warten auf sie.  90% der Ignite-Community spricht Russisch.  Zum Beispiel waren f√ºr mich bis zu meiner Arbeit an Apache Ignite nicht die besten Englischkenntnisse abschreckend.  Es lohnt sich kaum, auf Russisch auf eine Entwicklerliste zu schreiben, aber selbst wenn Sie etwas falsch schreiben, werden sie Ihnen antworten und Ihnen helfen. <br><br>  Was kann an dieser Integration verbessert werden?  Wie kann ich helfen, wenn Sie einen solchen Wunsch haben?  Liste unten.  Sternchen zeigen die Komplexit√§t an. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/de4/d43/ed0/de4d43ed01894ce6b02865ad9f6aef5d.png"><br>  Um die Optimierung zu testen, m√ºssen Sie Tests mit komplexen Abfragen schreiben.  Oben habe ich einige offensichtliche Fragen gezeigt.  Es ist klar, dass etwas fallen kann, wenn Sie viele Gruppierungen und viele Verkn√ºpfungen schreiben.  Dies ist eine sehr einfache Aufgabe - kommen Sie und machen Sie es.  Wenn wir aufgrund der Testergebnisse Fehler finden, m√ºssen diese behoben werden.  Dort wird es schwieriger. <br><br>  Eine weitere klare und interessante Aufgabe ist die Integration von Spark in einen Thin Client.  Zun√§chst k√∂nnen einige S√§tze von IP-Adressen angegeben werden. Dies reicht aus, um dem Ignite-Cluster beizutreten, was bei der Integration in ein externes System praktisch ist.  Wenn Sie sich pl√∂tzlich der L√∂sung f√ºr dieses Problem anschlie√üen m√∂chten, werde ich Ihnen pers√∂nlich dabei helfen. <br><br>  Wenn Sie der Apache Ignite-Community beitreten m√∂chten, finden Sie hier einige n√ºtzliche Links: <br><br><ul><li>  <i>Beginnen Sie hier - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=http://yu">https://ignite.apache.org/community/resources.html</a></i> <br></li><li>  <i>Quellen hier - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://github.com/apache/ignite/</a></i> <br></li><li>  <i>Docks hier - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://apacheignite.readme.io/docs</a></i> <br></li><li>  <i>Fehler hier - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://issues.apache.org/jira/browse/IGNITE</a></i> <br></li><li>  <i>Sie k√∂nnen hier schreiben - dev@ignite.apache.org, user@ignite.apache.org</i> <br></li></ul><br>  Wir haben eine reaktionsschnelle Entwicklerliste, die Ihnen helfen wird.  Es ist noch lange nicht ideal, aber im Vergleich zu anderen Projekten ist es wirklich lebendig. <br><br>  <i>Wenn Sie Java oder C ++ kennen, suchen Sie Arbeit und m√∂chten Open Source (Apache Ignite, Apache Kafka, Tarantool usw.) entwickeln. Schreiben Sie hier: join-open-source@sberbank.ru.</i> <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/CzbAweNKEVY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de427297/">https://habr.com/ru/post/de427297/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de427285/index.html">Schule √ºber die Grundlagen digitaler Schaltkreise: Nowosibirsk - Ok, Krasnojarsk - machen Sie sich bereit</a></li>
<li><a href="../de427289/index.html">Geologische 3D-Modellierung, Protokollierung und Technologie von Aramco Innovations</a></li>
<li><a href="../de427291/index.html">Minimieren Sie den Datenverkehr in ASP.NET Web Forms, anklickbare Div- und regelm√§√üige Serverabfragen</a></li>
<li><a href="../de427293/index.html">JavaScript-Entwurfsmuster</a></li>
<li><a href="../de427295/index.html">JavaScript-Currying-Funktionen</a></li>
<li><a href="../de427299/index.html">Lassen Sie uns noch etwas sammeln? Konstruktor 3 in 1 "Mondflotte"</a></li>
<li><a href="../de427301/index.html">GitHub st√ºrzt Datenbank ab</a></li>
<li><a href="../de427303/index.html">Windows verlangsamen Teil 2: Prozesse erstellen</a></li>
<li><a href="../de427307/index.html">Java-Backend-Testpraxis + Rest-Assured</a></li>
<li><a href="../de427309/index.html">Wie sich herausstellte, dass PVS-Studio aufmerksamer war als dreieinhalb Programmierer</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>