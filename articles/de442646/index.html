<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ò†Ô∏è üõ∂ üà≥ Kubernetes Networks: Ingress ü¶Ü üìë üî£</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Heute ver√∂ffentlichen wir eine √úbersetzung des dritten Teils des Kubernetes Networking Guide. Der erste Teil befasste sich mit Pods, der zweite mit Di...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Kubernetes Networks: Ingress</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/442646/">  Heute ver√∂ffentlichen wir eine √úbersetzung des dritten Teils des Kubernetes Networking Guide.  Der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erste</a> Teil befasste sich mit Pods, der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zweite</a> mit Diensten, und heute werden wir √ºber Lastausgleich und Kubernetes-Ressourcen vom Typ Ingress sprechen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/te/wp/ce/tewpcee5cggzqu97irog_mj_qgo.png"></div><a name="habracut"></a><h2>  <font color="#3AC1EF">Routing ist kein Lastausgleich</font> </h2><br>  Im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vorherigen</a> Artikel dieser Serie haben wir eine Konfiguration betrachtet, die aus zwei Herden und einem Dienst besteht, dem eine IP-Adresse zugewiesen wurde, die als "Cluster-IP" bezeichnet wird.  Anfragen f√ºr Herde wurden an diese Adresse gesendet.  Hier werden wir weiter an unserem Trainingssystem arbeiten, beginnend dort, wo wir das letzte Mal unseren Abschluss gemacht haben.  Denken Sie daran, dass die Cluster-IP-Adresse des Dienstes <code>10.3.241.152</code> zu einem Bereich von IP-Adressen geh√∂rt, der sich von dem im <code>10.3.241.152</code> und dem in dem Netzwerk verwendeten Netzwerk unterscheidet, in dem sich die Knoten befinden.  Ich habe das durch diesen Adressraum definierte Netzwerk als "Dienstnetzwerk" bezeichnet, obwohl es kaum einen besonderen Namen verdient, da keine Ger√§te mit diesem Netzwerk verbunden sind und sein Adressraum tats√§chlich ausschlie√ülich aus Routing-Regeln besteht.  Es wurde zuvor gezeigt, wie dieses Netzwerk auf der Basis der Kubernetes-Komponente namens <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kube-proxy implementiert wird</a> und mit dem Linux-Kernel- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Netfilter-</a> Modul interagiert, um den an den IP-Cluster gesendeten Datenverkehr abzufangen und umzuleiten, unter dem gearbeitet werden soll. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/615/35f/1ec/61535f1ec0169dbd13732aba4c9a5621.png"></div><br>  <i><font color="#999999">Netzwerkdiagramm</font></i> <br><br>  Bisher haben wir √ºber ‚ÄûVerbindungen‚Äú und ‚ÄûAnfragen‚Äú gesprochen und sogar das schwer zu interpretierende Konzept des ‚ÄûVerkehrs‚Äú verwendet. Um jedoch die Funktionen des Kubernetes Ingress-Mechanismus zu verstehen, m√ºssen wir genauere Begriffe verwenden.  Verbindungen und Anforderungen funktionieren also auf der 4. Ebene des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OSI-Modells</a> (tcp) oder auf der 7. Ebene (http, rpc usw.).  Netfilter-Regeln sind Routing-Regeln und funktionieren mit IP-Paketen auf der dritten Ebene.  Alle Router, einschlie√ülich Netfilter, treffen Entscheidungen mehr oder weniger nur auf der Grundlage der im Paket enthaltenen Informationen.  Im Allgemeinen interessieren sie sich daf√ºr, woher das Paket kommt und wohin es geht.  Um dieses Verhalten in Bezug auf die dritte Ebene des OSI-Modells zu beschreiben, muss daher gesagt werden, dass jedes Paket, das f√ºr den um <code>10.3.241.152:80</code> Dienst <code>10.3.241.152:80</code> und an der Schnittstelle des <code>eth0</code> Knotens <code>eth0</code> , von netfilter verarbeitet wird und gem√§√ü Die f√ºr unseren Service festgelegten Regeln werden an die IP-Adresse eines funktionsf√§higen Herdes umgeleitet. <br><br>  Es liegt auf der Hand, dass jeder Mechanismus, mit dem externe Clients auf Pods zugreifen k√∂nnen, dieselbe Routing-Infrastruktur verwenden sollte.  Infolgedessen greifen diese externen Clients auf die IP-Adresse und den Port des Clusters zu, da sie der ‚ÄûZugangspunkt‚Äú zu allen Mechanismen sind, √ºber die wir bisher gesprochen haben.  Sie erlauben uns, uns keine Gedanken dar√ºber zu machen, wo genau es zu einem bestimmten Zeitpunkt ausgef√ºhrt wird.  Es ist jedoch keineswegs klar, wie alles funktioniert. <br><br>  Der Cluster-IP-Dienst ist nur √ºber die Ethernet-Schnittstelle des Knotens erreichbar.  Nichts au√üerhalb des Clusters wei√ü, was mit Adressen aus dem Bereich zu tun ist, zu dem diese Adresse geh√∂rt.  Wie kann ich Datenverkehr von einer √∂ffentlichen IP-Adresse zu einer Adresse umleiten, die nur erreichbar ist, wenn das Paket bereits beim Host angekommen ist? <br><br>  Wenn wir versuchen, eine L√∂sung f√ºr dieses Problem zu finden, k√∂nnen Sie bei der Suche nach einer L√∂sung unter anderem die Netzfilterregeln mit dem Dienstprogramm <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">iptables untersuchen</a> .  Wenn Sie dies tun, k√∂nnen Sie etwas entdecken, das auf den ersten Blick ungew√∂hnlich erscheint: Die Regeln f√ºr den Dienst sind nicht auf ein bestimmtes Quellnetzwerk beschr√§nkt.  Dies bedeutet, dass alle Pakete, die irgendwo generiert werden und auf der Ethernet-Schnittstelle des Knotens ankommen und eine Zieladresse von <code>10.3.241.152:80</code> haben, als regelkonform erkannt und an das Sub weitergeleitet werden.  K√∂nnen wir den Clients einfach einen IP-Cluster geben, indem wir ihn m√∂glicherweise an einen geeigneten Dom√§nennamen binden, und dann eine Route einrichten, mit der wir die Zustellung dieser Pakete an einen der Knoten organisieren k√∂nnen? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5f3/6ba/ea7/5f36baea7589e3559632de385f3f2bf6.png"></div><br>  <i><font color="#999999">Externer Client und Cluster</font></i> <br><br>  Wenn alles auf diese Weise eingerichtet ist, wird sich herausstellen, dass ein solches Design funktioniert.  Clients greifen auf die Cluster-IP zu, Pakete folgen der Route, die zum Host f√ºhrt, und werden dann nach unten umgeleitet.  In diesem Moment scheint es Ihnen, dass eine solche L√∂sung begrenzt sein kann, aber sie leidet unter einigen ernsthaften Problemen.  Das erste ist, dass die Knoten, in der Tat das Konzept der Verg√§nglichkeit, sie unterscheiden sich in dieser Hinsicht nicht besonders von den Herden.  Sie sind der materiellen Welt nat√ºrlich etwas n√§her als Pods, aber sie k√∂nnen auf neue virtuelle Maschinen migrieren, Cluster k√∂nnen vergr√∂√üert oder verkleinert werden und so weiter.  Router arbeiten auf der dritten Ebene des OSI-Modells, und Pakete k√∂nnen nicht zwischen normal funktionierenden Diensten und solchen unterscheiden, die nicht ordnungsgem√§√ü funktionieren.  Sie erwarten, dass der n√§chste √úbergang auf der Route zug√§nglich und stabil sein wird.  Wenn der Knoten nicht erreichbar ist, wird die Route funktionsunf√§hig und bleibt in den meisten F√§llen viel Zeit.  Selbst wenn die Route fehleranf√§llig ist, f√ºhrt ein solches Schema dazu, dass der gesamte externe Verkehr √ºber einen einzelnen Knoten geleitet wird, was wahrscheinlich nicht optimal ist. <br><br>  Unabh√§ngig davon, wie wir den Clientverkehr zum System bringen, m√ºssen wir dies tun, damit er nicht vom Status eines einzelnen Clusterknotens abh√§ngt.  Tats√§chlich gibt es keine zuverl√§ssige M√∂glichkeit, dies nur mit Routing zu tun, ohne den Router aktiv zu verwalten.  Tats√§chlich spielt kube-proxy in Bezug auf den Netzfilter genau diese Rolle, die Rolle des Steuerungssystems.  Die Ausweitung der Verantwortung von Kubernetes auf die Verwaltung eines externen Routers war f√ºr Systemarchitekten wahrscheinlich nicht sehr sinnvoll, zumal wir bereits bew√§hrte Tools zur Verteilung des Clientverkehrs auf mehrere Server haben.  Sie werden als Load Balancer bezeichnet, und es ist nicht verwunderlich, dass sie die wirklich zuverl√§ssige L√∂sung f√ºr Kubernetes Ingress sind.  Um genau zu verstehen, wie dies geschieht, m√ºssen wir von der dritten OSI-Ebene aufstehen und erneut √ºber Verbindungen sprechen. <br><br>  Um den Load Balancer zum Verteilen des Clientverkehrs zwischen Clusterknoten verwenden zu k√∂nnen, ben√∂tigen wir eine √∂ffentliche IP-Adresse, mit der Clients eine Verbindung herstellen k√∂nnen, sowie die Adressen der Knoten selbst, an die der Load Balancer Anforderungen umleiten kann.  Aus den oben genannten Gr√ºnden k√∂nnen wir nicht einfach eine stabile statische Route zwischen dem Gateway-Router und den Knoten mithilfe eines dienstbasierten Netzwerks (IP-Cluster) erstellen. <br><br>  Unter den anderen Adressen, mit denen Sie arbeiten k√∂nnen, k√∂nnen nur die Adressen des Netzwerks notiert werden, mit dem die Ethernet-Schnittstellen der Knoten verbunden sind, in diesem Beispiel <code>10.100.0.0/24</code> .  Der Router wei√ü bereits, wie Pakete an diese Schnittstellen weitergeleitet werden, und die vom Load Balancer an den Router gesendeten Verbindungen werden dorthin geleitet, wo sie hingeh√∂rt.  Wenn der Client jedoch √ºber Port 80 eine Verbindung zu unserem Dienst herstellen m√∂chte, k√∂nnen wir nicht einfach Pakete an diesen Port √ºber die Netzwerkschnittstellen der Knoten senden. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4cd/c77/40b/4cdc7740be5ecfc3069a8b3fb157c605.png"></div><br>  <i><font color="#999999">Load Balancer, erfolgloser Versuch, auf Port 80 der Host-Netzwerkschnittstelle zuzugreifen</font></i> <br><br>  Der Grund, warum dies nicht m√∂glich ist, liegt auf der Hand.  Wir sprechen n√§mlich √ºber die Tatsache, dass um <code>10.100.0.3:80</code> kein Prozess auf Verbindungen wartet (und wenn <code>10.100.0.3:80</code> , dann ist dies definitiv nicht der gleiche Prozess), und √ºber die Netfilter-Regeln, die, wie wir gehofft hatten, die Anforderung und abfangen w√ºrden Sie senden es ihm. Sie arbeiten nicht an dieser Zieladresse.  Sie antworten nur auf ein Cluster-IP-Netzwerk, das auf Diensten basiert, <code>10.3.241.152:80</code> auf die Adresse <code>10.3.241.152:80</code> .  Infolgedessen k√∂nnen diese Pakete bei ihrer Ankunft nicht an die Zieladresse <code>ECONNREFUSED</code> , und der Kernel gibt eine <code>ECONNREFUSED</code> Antwort aus.  Dies bringt uns in eine verwirrende Position: Es ist nicht einfach, mit einem Netzwerk f√ºr die Paketumleitung zu arbeiten, zu dem Netfilter konfiguriert ist, wenn Daten vom Gateway zu Knoten umgeleitet werden, und ein Netzwerk, f√ºr das das Routing einfach zu konfigurieren ist, ist nicht das Netzwerk, zu dem Netfilter Pakete umleitet.  Um dieses Problem zu l√∂sen, k√∂nnen Sie eine Br√ºcke zwischen diesen Netzwerken erstellen.  Genau das macht Kubernetes mit einem Dienst wie NodePort. <br><br><h2>  <font color="#3AC1EF">Dienste wie NodePort</font> </h2><br>  Dem Dienst, den wir beispielsweise im vorherigen Artikel erstellt haben, ist kein Typ zugewiesen, daher wurde der Standardtyp <code>ClusterIP</code> .  Es gibt zwei weitere Arten von Diensten, die sich in zus√§tzlichen Funktionen unterscheiden, und der, an dem wir <code>NodePort</code> interessiert sind, ist <code>NodePort</code> .  Hier ist ein Beispiel f√ºr eine Beschreibung eines Dienstes dieses Typs: <br><br><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: service-test spec: type: NodePort selector:   app: service_test_pod ports: - port: 80   targetPort: http</code> </pre> <br>  Dienste vom Typ <code>NodePort</code> sind Dienste vom Typ <code>ClusterIP</code> , die eine zus√§tzliche M√∂glichkeit bieten: Der Zugriff auf diese Dienste kann sowohl √ºber die dem Host zugewiesene IP-Adresse als auch √ºber die dem Cluster im <code>ClusterIP</code> zugewiesene Adresse erfolgen.  Dies wird auf relativ einfache Weise erreicht: Wenn Kubernetes einen NodePort-Dienst erstellt, weist kube-proxy einen Port im Bereich von 30000-32767 zu und √∂ffnet diesen Port auf der <code>eth0</code> Schnittstelle jedes Knotens (daher der Name des <code>NodePort</code> - <code>NodePort</code> ).  Verbindungen zu diesem Port (wir werden solche <code>NodePort</code> Ports nennen) werden an die Cluster-IP des Dienstes umgeleitet.  Wenn wir den oben beschriebenen Dienst erstellen und den <code>kubectl get svc service-test</code> , k√∂nnen wir den ihm zugewiesenen Port sehen. <br><br><pre> <code class="plaintext hljs">$ kubectl get svc service-test NAME           CLUSTER-IP EXTERNAL-IP   PORT(S) AGE service-test   10.3.241.152 &lt;none&gt;        80:32213/TCP 1m</code> </pre> <br>  In diesem Fall wird dem Dienst NodePort 32213 zugewiesen.  Dies bedeutet, dass wir jetzt √ºber einen beliebigen Knoten in unserem experimentellen Cluster um <code>10.100.0.2:32213</code> oder um <code>10.100.0.3:32213</code> Verbindung zum Dienst herstellen k√∂nnen.  In diesem Fall wird der Datenverkehr an den Dienst umgeleitet. <br><br>  Nachdem dieser Teil des Systems seinen Platz eingenommen hat, verf√ºgen wir √ºber alle Fragmente der Pipeline, um die durch Clientanforderungen verursachte Last auf alle Knoten des Clusters auszugleichen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/da5/78e/735/da578e735e468dce8077f8a1d27d8490.png"></div><br>  <i><font color="#999999">NodePort-Dienst</font></i> <br><br>  In der vorherigen Abbildung stellt der Client √ºber eine √∂ffentliche IP-Adresse eine Verbindung zum Load Balancer her. Der Load Balancer w√§hlt den Knoten aus und stellt um <code>10.100.0.3:32213</code> Verbindung zu ihm her. Kube-proxy akzeptiert diese Verbindung und leitet sie an den Dienst weiter, auf den √ºber Cluster IP <code>10.3.241.152:80</code> .  Hier wird die Anforderung gem√§√ü den von netfilter festgelegten Regeln erfolgreich verarbeitet und unter der Adresse <code>10.0.2.2:8080</code> an den Server-Pod <code>10.0.2.2:8080</code> .  Vielleicht sieht das alles etwas kompliziert aus und ist es bis zu einem gewissen Grad auch, aber es ist nicht einfach, eine einfachere L√∂sung zu finden, die all die gro√üartigen Funktionen unterst√ºtzt, die uns Pods und service-basierte Netzwerke bieten. <br><br>  Dieser Mechanismus ist jedoch nicht ohne eigene Probleme.  Durch die Verwendung von Diensten wie <code>NodePort</code> Kunden √ºber einen nicht standardm√§√üigen Port auf Dienste zugreifen.  Oft ist dies kein Problem, da der Load Balancer ihnen einen regul√§ren Port zur Verf√ºgung stellen und <code>NodePort</code> vor Endbenutzern verbergen kann.  In einigen Szenarien, z. B. bei Verwendung eines externen Load Balancers f√ºr die Google Cloud-Plattform, kann es jedoch erforderlich sein, <code>NodePort</code> Clients <code>NodePort</code> .  Es sollte beachtet werden, dass solche Ports zus√§tzlich begrenzte Ressourcen darstellen, obwohl 2768 Ports wahrscheinlich selbst f√ºr die gr√∂√üten Cluster ausreichen.  In den meisten F√§llen k√∂nnen Sie Kubernetes Portnummern zuf√§llig ausw√§hlen lassen, diese jedoch bei Bedarf selbst festlegen.  Ein weiteres Problem sind einige Einschr√§nkungen hinsichtlich der Speicherung von Quell-IP-Adressen in Anforderungen.  Informationen zur L√∂sung dieser Probleme finden Sie in der Kubernetes-Dokumentation. <br><br>  Ports <code>NodePorts</code> ist der grundlegende Mechanismus, mit dem der gesamte externe Datenverkehr in den Kubernetes-Cluster gelangt.  Sie selbst pr√§sentieren uns jedoch keine vorgefertigte L√∂sung.  Aus den oben genannten Gr√ºnden ist vor dem Cluster immer eine Art Load Balancer erforderlich, unabh√§ngig davon, ob es sich bei den Clients um interne oder externe Entit√§ten in einem √∂ffentlichen Netzwerk handelt. <br><br>  Die Plattformarchitekten stellten dies fest und stellten zwei M√∂glichkeiten zur Verf√ºgung, um den Load Balancer √ºber die Kubernetes-Plattform selbst zu konfigurieren.  Lassen Sie uns das diskutieren. <br><br><h2>  <font color="#3AC1EF">Dienste wie LoadBalancer und Ressourcen vom Typ Ingress</font> </h2><br>  Dienste wie <code>LoadBalancer</code> und Ressourcen vom Typ <code>Ingress</code> geh√∂ren zu den komplexesten Kubernetes-Mechanismen.  Wir werden jedoch nicht zu viel Zeit mit ihnen verbringen, da ihre Verwendung nicht zu grundlegenden √Ñnderungen in allem f√ºhrt, wor√ºber wir bisher gesprochen haben.  Der gesamte externe Datenverkehr gelangt nach wie vor √ºber <code>NodePort</code> in den Cluster. <br><br>  Architekten k√∂nnten hier aufh√∂ren und denjenigen, die Cluster erstellen, erlauben, sich nur um √∂ffentliche IP-Adressen und Load Balancer zu k√ºmmern.  In bestimmten Situationen, z. B. beim Starten eines Clusters auf normalen Servern oder zu Hause, ist dies genau das, was sie tun.  In Umgebungen, die API-gesteuerte Netzwerkressourcenkonfigurationen unterst√ºtzen, k√∂nnen Sie mit Kubernetes alles, was Sie ben√∂tigen, an einem Ort konfigurieren. <br><br>  Der erste und einfachste Ansatz zur L√∂sung dieses Problems ist die Verwendung von Kubernetes-Diensten wie <code>LoadBalancer</code> .  Solche Dienste verf√ºgen √ºber alle Funktionen von Diensten wie <code>NodePort</code> und k√∂nnen dar√ºber hinaus vollst√§ndige Pfade f√ºr eingehenden Datenverkehr erstellen, basierend auf der Annahme, dass der Cluster in Umgebungen wie GCP oder AWS ausgef√ºhrt wird, die die Konfiguration von Netzwerkressourcen √ºber die API unterst√ºtzen. <br><br><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: service-test spec: type: LoadBalancer selector:   app: service_test_pod ports: - port: 80   targetPort: http</code> </pre> <br>  Wenn wir den Dienst aus unserem Beispiel in der Google Kubernetes Engine l√∂schen und neu erstellen, k√∂nnen wir kurz darauf mit dem <code>kubectl get svc service-test</code> √ºberpr√ºfen, ob die externe IP zugewiesen ist. <br><br><pre> <code class="plaintext hljs">$ kubectl get svc service-test NAME      CLUSTER-IP      EXTERNAL-IP PORT(S)          AGE openvpn   10.3.241.52     35.184.97.156 80:32213/TCP     5m</code> </pre> <br>  Es wurde oben erw√§hnt, dass wir die Tatsache der Zuweisung einer externen IP-Adresse ‚Äûbald‚Äú √ºberpr√ºfen k√∂nnen, obwohl die Zuweisung einer externen IP-Adresse einige Minuten dauern kann, was angesichts der Menge an Ressourcen, die in einen gesunden Zustand gebracht werden m√ºssen, nicht √ºberraschend ist.  Auf der GCP-Plattform muss das System beispielsweise eine externe IP-Adresse, Verkehrsumleitungsregeln, einen Ziel-Proxy-Server, einen Back-End-Dienst und m√∂glicherweise eine Instanz einer Gruppe erstellen.  Nachdem Sie eine externe IP-Adresse zugewiesen haben, k√∂nnen Sie √ºber diese Adresse eine Verbindung zum Dienst herstellen, ihm einen Dom√§nennamen zuweisen und Clients informieren.  Bis der Dienst zerst√∂rt und neu erstellt wird (dazu selten, wenn es einen guten Grund gibt), √§ndert sich die IP-Adresse nicht. <br><br>  Dienste wie <code>LoadBalancer</code> haben einige Einschr√§nkungen.  Ein solcher Dienst kann nicht zum Entschl√ºsseln des HTTPS-Verkehrs konfiguriert werden.  Sie k√∂nnen keine virtuellen Hosts erstellen oder pfadbasiertes Routing konfigurieren. Daher k√∂nnen Sie in praktischen Konfigurationen keinen einzelnen Load Balancer mit vielen Diensten verwenden.  Diese Einschr√§nkungen f√ºhrten zur Einf√ºhrung von Kubernetes 1.1.  Eine spezielle Ressource zum Konfigurieren von Load Balancern.  Dies ist eine Ressource vom Typ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ingress</a> .  Dienste wie <code>LoadBalancer</code> zielen darauf ab, die Funktionen eines einzelnen Dienstes zur Unterst√ºtzung externer Clients zu erweitern.  Im Gegensatz dazu sind <code>Ingress</code> Ressourcen spezielle Ressourcen, mit denen Sie Load Balancer flexibel konfigurieren k√∂nnen.  Die Ingress-API unterst√ºtzt die Entschl√ºsselung von TLS-Verkehr, virtuellen Hosts und pfadbasiertem Routing.  Mit dieser API kann der Load Balancer einfach f√ºr die Arbeit mit mehreren Backend-Diensten konfiguriert werden. <br><br>  Die Ressourcen-API des <code>Ingress</code> Typs ist zu gro√ü, um ihre Funktionen hier zu erl√§utern. Dar√ºber hinaus hat dies keinen besonderen Einfluss auf die Funktionsweise der Ingress-Ressourcen auf Netzwerkebene.  Die Implementierung dieser Ressource folgt dem √ºblichen Kubernetes-Muster: Es gibt einen Ressourcentyp und einen Controller, um diesen Typ zu steuern.  Die Ressource in diesem Fall ist die <code>Ingress</code> Ressource, die Anforderungen an Netzwerkressourcen beschreibt.  So k√∂nnte die Beschreibung einer <code>Ingress</code> Ressource aussehen. <br><br><pre> <code class="plaintext hljs">apiVersion: extensions/v1beta1 kind: Ingress metadata: name: test-ingress annotations:   kubernetes.io/ingress.class: "gce" spec: tls:   - secretName: my-ssl-secret rules: - host: testhost.com   http:     paths:     - path: /*       backend:         serviceName: service-test         servicePort: 80</code> </pre> <br>  Der Ingress-Controller ist f√ºr die Ausf√ºhrung dieser Anforderungen verantwortlich, indem er andere Ressourcen in den gew√ºnschten Zustand versetzt.  Bei Verwendung von Ingress werden Dienste wie <code>NodePort</code> erstellt. <code>NodePort</code> der Ingress-Controller entscheiden, wie der Datenverkehr zu Knoten geleitet werden soll.  Es gibt eine Implementierung des Ingress-Controllers f√ºr GCE-Load-Balancer, f√ºr AWS-Balancer und f√ºr beliebte Proxy-Server wie Nginx und Haproxy.  Beachten Sie, dass das Mischen von Ingress-Ressourcen und -Diensten wie <code>LoadBalancer</code> in einigen Umgebungen zu geringf√ºgigen Problemen <code>LoadBalancer</code> kann.  Sie sind einfach zu handhaben, aber im Allgemeinen ist es am besten, Ingress auch nur f√ºr einfache Dienste zu verwenden. <br><br><h2>  <font color="#3AC1EF">HostPort und HostNetwork</font> </h2><br>  Wor√ºber wir jetzt sprechen werden, n√§mlich <code>HostPort</code> und <code>HostNetwork</code> , kann eher der Kategorie interessanter Rarit√§ten und nicht n√ºtzlichen Werkzeugen zugeschrieben werden.  Tats√§chlich verpflichte ich mich zu behaupten, dass in 99,99% der F√§lle ihre Verwendung als Anti-Pattern angesehen werden kann und jedes System, in dem sie verwendet werden, einer obligatorischen √úberpr√ºfung seiner Architektur unterzogen werden muss. <br><br>  Ich dachte, dass es sich √ºberhaupt nicht lohnt, √ºber sie zu sprechen, aber sie sind so etwas wie die Tools, die von Ingress-Ressourcen verwendet werden, um eingehenden Datenverkehr zu verarbeiten. Deshalb habe ich beschlossen, dass es sich lohnt, sie zumindest kurz zu erw√§hnen. <br><br>  <code>HostPort</code> √ºber <code>HostPort</code> sprechen.  Dies ist eine Containereigenschaft (in der <code>ContainerPort</code> Struktur deklariert).  Wenn eine bestimmte Portnummer darin geschrieben ist, f√ºhrt dies zur √ñffnung dieses Ports auf dem Knoten und zu seiner Umleitung direkt zum Container.  Es gibt keine Proxy-Mechanismen und der Port wird nur auf den Knoten ge√∂ffnet, auf denen der Container ausgef√ºhrt wird.  In den fr√ºhen Tagen der Plattform, bevor die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DaemonSet-</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">StatefulSet-</a> Mechanismen darin auftauchten, war <code>HostPort</code> ein Trick, mit dem nur ein Container eines bestimmten Typs auf einem Knoten gestartet werden konnte.  Ich habe dies beispielsweise einmal verwendet, um einen Elasticsearch-Cluster zu erstellen, indem <code>HostPort</code> auf <code>9200</code> und so viele Replikate angegeben wurden, wie Knoten vorhanden waren.       ,          Kubernetes,    -     <code>HostPort</code> . <br><br>   <code>NostNetwork</code> , ,   Kubernetes    ,  <code>HostPort</code> .       <code>true</code> ,       - <code>network=host</code>  <code>docker run</code> .    ,           .            <code>eth0</code>    .  ,             .      ,  ,  ,    Kubernetes,     - . <br><br><h2>  <font color="#3AC1EF">Zusammenfassung</font> </h2><br>        Kubernetes,   ,          Ingress. ,  ,    ,       Kubernetes. <br><br>  <b>Liebe Leser!</b>     Ingress? <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de442646/">https://habr.com/ru/post/de442646/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de442636/index.html">Bringen Sie dem Management schlechte Nachrichten?</a></li>
<li><a href="../de442638/index.html">Kubernetes Anwendungsskalierung basierend auf Metriken von Prometheus</a></li>
<li><a href="../de442640/index.html">Perfekter Fehler: Verwenden von Typverwirrung in Flash. Teil 1</a></li>
<li><a href="../de442642/index.html">Was Sie im M√§rz lesen sollten: 22 neue B√ºcher f√ºr Vermarkter, Manager, Entwickler und Designer</a></li>
<li><a href="../de442644/index.html">Die meisten Nicht-Programmierkenntnisse erh√∂hen den Entwicklerwert</a></li>
<li><a href="../de442648/index.html">Go-Zuweisungsmechanismen</a></li>
<li><a href="../de442650/index.html">Analyse und Optimierung von React-Anwendungen</a></li>
<li><a href="../de442652/index.html">Verwenden von Fastify und Preact zum schnellen Prototypen von Webanwendungen</a></li>
<li><a href="../de442654/index.html">Wechseln Sie zu Next.js und beschleunigen Sie das Laden der Manifold.co-Homepage um das 7,5-fache</a></li>
<li><a href="../de442658/index.html">8 Tricks f√ºr die Arbeit mit CSS: Parallaxe, Sticky Footer und andere</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>