<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>☠️ 🛶 🈳 Kubernetes Networks: Ingress 🦆 📑 🔣</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Heute veröffentlichen wir eine Übersetzung des dritten Teils des Kubernetes Networking Guide. Der erste Teil befasste sich mit Pods, der zweite mit Di...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Kubernetes Networks: Ingress</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/442646/">  Heute veröffentlichen wir eine Übersetzung des dritten Teils des Kubernetes Networking Guide.  Der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erste</a> Teil befasste sich mit Pods, der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zweite</a> mit Diensten, und heute werden wir über Lastausgleich und Kubernetes-Ressourcen vom Typ Ingress sprechen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/te/wp/ce/tewpcee5cggzqu97irog_mj_qgo.png"></div><a name="habracut"></a><h2>  <font color="#3AC1EF">Routing ist kein Lastausgleich</font> </h2><br>  Im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vorherigen</a> Artikel dieser Serie haben wir eine Konfiguration betrachtet, die aus zwei Herden und einem Dienst besteht, dem eine IP-Adresse zugewiesen wurde, die als "Cluster-IP" bezeichnet wird.  Anfragen für Herde wurden an diese Adresse gesendet.  Hier werden wir weiter an unserem Trainingssystem arbeiten, beginnend dort, wo wir das letzte Mal unseren Abschluss gemacht haben.  Denken Sie daran, dass die Cluster-IP-Adresse des Dienstes <code>10.3.241.152</code> zu einem Bereich von IP-Adressen gehört, der sich von dem im <code>10.3.241.152</code> und dem in dem Netzwerk verwendeten Netzwerk unterscheidet, in dem sich die Knoten befinden.  Ich habe das durch diesen Adressraum definierte Netzwerk als "Dienstnetzwerk" bezeichnet, obwohl es kaum einen besonderen Namen verdient, da keine Geräte mit diesem Netzwerk verbunden sind und sein Adressraum tatsächlich ausschließlich aus Routing-Regeln besteht.  Es wurde zuvor gezeigt, wie dieses Netzwerk auf der Basis der Kubernetes-Komponente namens <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kube-proxy implementiert wird</a> und mit dem Linux-Kernel- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Netfilter-</a> Modul interagiert, um den an den IP-Cluster gesendeten Datenverkehr abzufangen und umzuleiten, unter dem gearbeitet werden soll. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/615/35f/1ec/61535f1ec0169dbd13732aba4c9a5621.png"></div><br>  <i><font color="#999999">Netzwerkdiagramm</font></i> <br><br>  Bisher haben wir über „Verbindungen“ und „Anfragen“ gesprochen und sogar das schwer zu interpretierende Konzept des „Verkehrs“ verwendet. Um jedoch die Funktionen des Kubernetes Ingress-Mechanismus zu verstehen, müssen wir genauere Begriffe verwenden.  Verbindungen und Anforderungen funktionieren also auf der 4. Ebene des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OSI-Modells</a> (tcp) oder auf der 7. Ebene (http, rpc usw.).  Netfilter-Regeln sind Routing-Regeln und funktionieren mit IP-Paketen auf der dritten Ebene.  Alle Router, einschließlich Netfilter, treffen Entscheidungen mehr oder weniger nur auf der Grundlage der im Paket enthaltenen Informationen.  Im Allgemeinen interessieren sie sich dafür, woher das Paket kommt und wohin es geht.  Um dieses Verhalten in Bezug auf die dritte Ebene des OSI-Modells zu beschreiben, muss daher gesagt werden, dass jedes Paket, das für den um <code>10.3.241.152:80</code> Dienst <code>10.3.241.152:80</code> und an der Schnittstelle des <code>eth0</code> Knotens <code>eth0</code> , von netfilter verarbeitet wird und gemäß Die für unseren Service festgelegten Regeln werden an die IP-Adresse eines funktionsfähigen Herdes umgeleitet. <br><br>  Es liegt auf der Hand, dass jeder Mechanismus, mit dem externe Clients auf Pods zugreifen können, dieselbe Routing-Infrastruktur verwenden sollte.  Infolgedessen greifen diese externen Clients auf die IP-Adresse und den Port des Clusters zu, da sie der „Zugangspunkt“ zu allen Mechanismen sind, über die wir bisher gesprochen haben.  Sie erlauben uns, uns keine Gedanken darüber zu machen, wo genau es zu einem bestimmten Zeitpunkt ausgeführt wird.  Es ist jedoch keineswegs klar, wie alles funktioniert. <br><br>  Der Cluster-IP-Dienst ist nur über die Ethernet-Schnittstelle des Knotens erreichbar.  Nichts außerhalb des Clusters weiß, was mit Adressen aus dem Bereich zu tun ist, zu dem diese Adresse gehört.  Wie kann ich Datenverkehr von einer öffentlichen IP-Adresse zu einer Adresse umleiten, die nur erreichbar ist, wenn das Paket bereits beim Host angekommen ist? <br><br>  Wenn wir versuchen, eine Lösung für dieses Problem zu finden, können Sie bei der Suche nach einer Lösung unter anderem die Netzfilterregeln mit dem Dienstprogramm <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">iptables untersuchen</a> .  Wenn Sie dies tun, können Sie etwas entdecken, das auf den ersten Blick ungewöhnlich erscheint: Die Regeln für den Dienst sind nicht auf ein bestimmtes Quellnetzwerk beschränkt.  Dies bedeutet, dass alle Pakete, die irgendwo generiert werden und auf der Ethernet-Schnittstelle des Knotens ankommen und eine Zieladresse von <code>10.3.241.152:80</code> haben, als regelkonform erkannt und an das Sub weitergeleitet werden.  Können wir den Clients einfach einen IP-Cluster geben, indem wir ihn möglicherweise an einen geeigneten Domänennamen binden, und dann eine Route einrichten, mit der wir die Zustellung dieser Pakete an einen der Knoten organisieren können? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5f3/6ba/ea7/5f36baea7589e3559632de385f3f2bf6.png"></div><br>  <i><font color="#999999">Externer Client und Cluster</font></i> <br><br>  Wenn alles auf diese Weise eingerichtet ist, wird sich herausstellen, dass ein solches Design funktioniert.  Clients greifen auf die Cluster-IP zu, Pakete folgen der Route, die zum Host führt, und werden dann nach unten umgeleitet.  In diesem Moment scheint es Ihnen, dass eine solche Lösung begrenzt sein kann, aber sie leidet unter einigen ernsthaften Problemen.  Das erste ist, dass die Knoten, in der Tat das Konzept der Vergänglichkeit, sie unterscheiden sich in dieser Hinsicht nicht besonders von den Herden.  Sie sind der materiellen Welt natürlich etwas näher als Pods, aber sie können auf neue virtuelle Maschinen migrieren, Cluster können vergrößert oder verkleinert werden und so weiter.  Router arbeiten auf der dritten Ebene des OSI-Modells, und Pakete können nicht zwischen normal funktionierenden Diensten und solchen unterscheiden, die nicht ordnungsgemäß funktionieren.  Sie erwarten, dass der nächste Übergang auf der Route zugänglich und stabil sein wird.  Wenn der Knoten nicht erreichbar ist, wird die Route funktionsunfähig und bleibt in den meisten Fällen viel Zeit.  Selbst wenn die Route fehleranfällig ist, führt ein solches Schema dazu, dass der gesamte externe Verkehr über einen einzelnen Knoten geleitet wird, was wahrscheinlich nicht optimal ist. <br><br>  Unabhängig davon, wie wir den Clientverkehr zum System bringen, müssen wir dies tun, damit er nicht vom Status eines einzelnen Clusterknotens abhängt.  Tatsächlich gibt es keine zuverlässige Möglichkeit, dies nur mit Routing zu tun, ohne den Router aktiv zu verwalten.  Tatsächlich spielt kube-proxy in Bezug auf den Netzfilter genau diese Rolle, die Rolle des Steuerungssystems.  Die Ausweitung der Verantwortung von Kubernetes auf die Verwaltung eines externen Routers war für Systemarchitekten wahrscheinlich nicht sehr sinnvoll, zumal wir bereits bewährte Tools zur Verteilung des Clientverkehrs auf mehrere Server haben.  Sie werden als Load Balancer bezeichnet, und es ist nicht verwunderlich, dass sie die wirklich zuverlässige Lösung für Kubernetes Ingress sind.  Um genau zu verstehen, wie dies geschieht, müssen wir von der dritten OSI-Ebene aufstehen und erneut über Verbindungen sprechen. <br><br>  Um den Load Balancer zum Verteilen des Clientverkehrs zwischen Clusterknoten verwenden zu können, benötigen wir eine öffentliche IP-Adresse, mit der Clients eine Verbindung herstellen können, sowie die Adressen der Knoten selbst, an die der Load Balancer Anforderungen umleiten kann.  Aus den oben genannten Gründen können wir nicht einfach eine stabile statische Route zwischen dem Gateway-Router und den Knoten mithilfe eines dienstbasierten Netzwerks (IP-Cluster) erstellen. <br><br>  Unter den anderen Adressen, mit denen Sie arbeiten können, können nur die Adressen des Netzwerks notiert werden, mit dem die Ethernet-Schnittstellen der Knoten verbunden sind, in diesem Beispiel <code>10.100.0.0/24</code> .  Der Router weiß bereits, wie Pakete an diese Schnittstellen weitergeleitet werden, und die vom Load Balancer an den Router gesendeten Verbindungen werden dorthin geleitet, wo sie hingehört.  Wenn der Client jedoch über Port 80 eine Verbindung zu unserem Dienst herstellen möchte, können wir nicht einfach Pakete an diesen Port über die Netzwerkschnittstellen der Knoten senden. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4cd/c77/40b/4cdc7740be5ecfc3069a8b3fb157c605.png"></div><br>  <i><font color="#999999">Load Balancer, erfolgloser Versuch, auf Port 80 der Host-Netzwerkschnittstelle zuzugreifen</font></i> <br><br>  Der Grund, warum dies nicht möglich ist, liegt auf der Hand.  Wir sprechen nämlich über die Tatsache, dass um <code>10.100.0.3:80</code> kein Prozess auf Verbindungen wartet (und wenn <code>10.100.0.3:80</code> , dann ist dies definitiv nicht der gleiche Prozess), und über die Netfilter-Regeln, die, wie wir gehofft hatten, die Anforderung und abfangen würden Sie senden es ihm. Sie arbeiten nicht an dieser Zieladresse.  Sie antworten nur auf ein Cluster-IP-Netzwerk, das auf Diensten basiert, <code>10.3.241.152:80</code> auf die Adresse <code>10.3.241.152:80</code> .  Infolgedessen können diese Pakete bei ihrer Ankunft nicht an die Zieladresse <code>ECONNREFUSED</code> , und der Kernel gibt eine <code>ECONNREFUSED</code> Antwort aus.  Dies bringt uns in eine verwirrende Position: Es ist nicht einfach, mit einem Netzwerk für die Paketumleitung zu arbeiten, zu dem Netfilter konfiguriert ist, wenn Daten vom Gateway zu Knoten umgeleitet werden, und ein Netzwerk, für das das Routing einfach zu konfigurieren ist, ist nicht das Netzwerk, zu dem Netfilter Pakete umleitet.  Um dieses Problem zu lösen, können Sie eine Brücke zwischen diesen Netzwerken erstellen.  Genau das macht Kubernetes mit einem Dienst wie NodePort. <br><br><h2>  <font color="#3AC1EF">Dienste wie NodePort</font> </h2><br>  Dem Dienst, den wir beispielsweise im vorherigen Artikel erstellt haben, ist kein Typ zugewiesen, daher wurde der Standardtyp <code>ClusterIP</code> .  Es gibt zwei weitere Arten von Diensten, die sich in zusätzlichen Funktionen unterscheiden, und der, an dem wir <code>NodePort</code> interessiert sind, ist <code>NodePort</code> .  Hier ist ein Beispiel für eine Beschreibung eines Dienstes dieses Typs: <br><br><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: service-test spec: type: NodePort selector:   app: service_test_pod ports: - port: 80   targetPort: http</code> </pre> <br>  Dienste vom Typ <code>NodePort</code> sind Dienste vom Typ <code>ClusterIP</code> , die eine zusätzliche Möglichkeit bieten: Der Zugriff auf diese Dienste kann sowohl über die dem Host zugewiesene IP-Adresse als auch über die dem Cluster im <code>ClusterIP</code> zugewiesene Adresse erfolgen.  Dies wird auf relativ einfache Weise erreicht: Wenn Kubernetes einen NodePort-Dienst erstellt, weist kube-proxy einen Port im Bereich von 30000-32767 zu und öffnet diesen Port auf der <code>eth0</code> Schnittstelle jedes Knotens (daher der Name des <code>NodePort</code> - <code>NodePort</code> ).  Verbindungen zu diesem Port (wir werden solche <code>NodePort</code> Ports nennen) werden an die Cluster-IP des Dienstes umgeleitet.  Wenn wir den oben beschriebenen Dienst erstellen und den <code>kubectl get svc service-test</code> , können wir den ihm zugewiesenen Port sehen. <br><br><pre> <code class="plaintext hljs">$ kubectl get svc service-test NAME           CLUSTER-IP EXTERNAL-IP   PORT(S) AGE service-test   10.3.241.152 &lt;none&gt;        80:32213/TCP 1m</code> </pre> <br>  In diesem Fall wird dem Dienst NodePort 32213 zugewiesen.  Dies bedeutet, dass wir jetzt über einen beliebigen Knoten in unserem experimentellen Cluster um <code>10.100.0.2:32213</code> oder um <code>10.100.0.3:32213</code> Verbindung zum Dienst herstellen können.  In diesem Fall wird der Datenverkehr an den Dienst umgeleitet. <br><br>  Nachdem dieser Teil des Systems seinen Platz eingenommen hat, verfügen wir über alle Fragmente der Pipeline, um die durch Clientanforderungen verursachte Last auf alle Knoten des Clusters auszugleichen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/da5/78e/735/da578e735e468dce8077f8a1d27d8490.png"></div><br>  <i><font color="#999999">NodePort-Dienst</font></i> <br><br>  In der vorherigen Abbildung stellt der Client über eine öffentliche IP-Adresse eine Verbindung zum Load Balancer her. Der Load Balancer wählt den Knoten aus und stellt um <code>10.100.0.3:32213</code> Verbindung zu ihm her. Kube-proxy akzeptiert diese Verbindung und leitet sie an den Dienst weiter, auf den über Cluster IP <code>10.3.241.152:80</code> .  Hier wird die Anforderung gemäß den von netfilter festgelegten Regeln erfolgreich verarbeitet und unter der Adresse <code>10.0.2.2:8080</code> an den Server-Pod <code>10.0.2.2:8080</code> .  Vielleicht sieht das alles etwas kompliziert aus und ist es bis zu einem gewissen Grad auch, aber es ist nicht einfach, eine einfachere Lösung zu finden, die all die großartigen Funktionen unterstützt, die uns Pods und service-basierte Netzwerke bieten. <br><br>  Dieser Mechanismus ist jedoch nicht ohne eigene Probleme.  Durch die Verwendung von Diensten wie <code>NodePort</code> Kunden über einen nicht standardmäßigen Port auf Dienste zugreifen.  Oft ist dies kein Problem, da der Load Balancer ihnen einen regulären Port zur Verfügung stellen und <code>NodePort</code> vor Endbenutzern verbergen kann.  In einigen Szenarien, z. B. bei Verwendung eines externen Load Balancers für die Google Cloud-Plattform, kann es jedoch erforderlich sein, <code>NodePort</code> Clients <code>NodePort</code> .  Es sollte beachtet werden, dass solche Ports zusätzlich begrenzte Ressourcen darstellen, obwohl 2768 Ports wahrscheinlich selbst für die größten Cluster ausreichen.  In den meisten Fällen können Sie Kubernetes Portnummern zufällig auswählen lassen, diese jedoch bei Bedarf selbst festlegen.  Ein weiteres Problem sind einige Einschränkungen hinsichtlich der Speicherung von Quell-IP-Adressen in Anforderungen.  Informationen zur Lösung dieser Probleme finden Sie in der Kubernetes-Dokumentation. <br><br>  Ports <code>NodePorts</code> ist der grundlegende Mechanismus, mit dem der gesamte externe Datenverkehr in den Kubernetes-Cluster gelangt.  Sie selbst präsentieren uns jedoch keine vorgefertigte Lösung.  Aus den oben genannten Gründen ist vor dem Cluster immer eine Art Load Balancer erforderlich, unabhängig davon, ob es sich bei den Clients um interne oder externe Entitäten in einem öffentlichen Netzwerk handelt. <br><br>  Die Plattformarchitekten stellten dies fest und stellten zwei Möglichkeiten zur Verfügung, um den Load Balancer über die Kubernetes-Plattform selbst zu konfigurieren.  Lassen Sie uns das diskutieren. <br><br><h2>  <font color="#3AC1EF">Dienste wie LoadBalancer und Ressourcen vom Typ Ingress</font> </h2><br>  Dienste wie <code>LoadBalancer</code> und Ressourcen vom Typ <code>Ingress</code> gehören zu den komplexesten Kubernetes-Mechanismen.  Wir werden jedoch nicht zu viel Zeit mit ihnen verbringen, da ihre Verwendung nicht zu grundlegenden Änderungen in allem führt, worüber wir bisher gesprochen haben.  Der gesamte externe Datenverkehr gelangt nach wie vor über <code>NodePort</code> in den Cluster. <br><br>  Architekten könnten hier aufhören und denjenigen, die Cluster erstellen, erlauben, sich nur um öffentliche IP-Adressen und Load Balancer zu kümmern.  In bestimmten Situationen, z. B. beim Starten eines Clusters auf normalen Servern oder zu Hause, ist dies genau das, was sie tun.  In Umgebungen, die API-gesteuerte Netzwerkressourcenkonfigurationen unterstützen, können Sie mit Kubernetes alles, was Sie benötigen, an einem Ort konfigurieren. <br><br>  Der erste und einfachste Ansatz zur Lösung dieses Problems ist die Verwendung von Kubernetes-Diensten wie <code>LoadBalancer</code> .  Solche Dienste verfügen über alle Funktionen von Diensten wie <code>NodePort</code> und können darüber hinaus vollständige Pfade für eingehenden Datenverkehr erstellen, basierend auf der Annahme, dass der Cluster in Umgebungen wie GCP oder AWS ausgeführt wird, die die Konfiguration von Netzwerkressourcen über die API unterstützen. <br><br><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: service-test spec: type: LoadBalancer selector:   app: service_test_pod ports: - port: 80   targetPort: http</code> </pre> <br>  Wenn wir den Dienst aus unserem Beispiel in der Google Kubernetes Engine löschen und neu erstellen, können wir kurz darauf mit dem <code>kubectl get svc service-test</code> überprüfen, ob die externe IP zugewiesen ist. <br><br><pre> <code class="plaintext hljs">$ kubectl get svc service-test NAME      CLUSTER-IP      EXTERNAL-IP PORT(S)          AGE openvpn   10.3.241.52     35.184.97.156 80:32213/TCP     5m</code> </pre> <br>  Es wurde oben erwähnt, dass wir die Tatsache der Zuweisung einer externen IP-Adresse „bald“ überprüfen können, obwohl die Zuweisung einer externen IP-Adresse einige Minuten dauern kann, was angesichts der Menge an Ressourcen, die in einen gesunden Zustand gebracht werden müssen, nicht überraschend ist.  Auf der GCP-Plattform muss das System beispielsweise eine externe IP-Adresse, Verkehrsumleitungsregeln, einen Ziel-Proxy-Server, einen Back-End-Dienst und möglicherweise eine Instanz einer Gruppe erstellen.  Nachdem Sie eine externe IP-Adresse zugewiesen haben, können Sie über diese Adresse eine Verbindung zum Dienst herstellen, ihm einen Domänennamen zuweisen und Clients informieren.  Bis der Dienst zerstört und neu erstellt wird (dazu selten, wenn es einen guten Grund gibt), ändert sich die IP-Adresse nicht. <br><br>  Dienste wie <code>LoadBalancer</code> haben einige Einschränkungen.  Ein solcher Dienst kann nicht zum Entschlüsseln des HTTPS-Verkehrs konfiguriert werden.  Sie können keine virtuellen Hosts erstellen oder pfadbasiertes Routing konfigurieren. Daher können Sie in praktischen Konfigurationen keinen einzelnen Load Balancer mit vielen Diensten verwenden.  Diese Einschränkungen führten zur Einführung von Kubernetes 1.1.  Eine spezielle Ressource zum Konfigurieren von Load Balancern.  Dies ist eine Ressource vom Typ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ingress</a> .  Dienste wie <code>LoadBalancer</code> zielen darauf ab, die Funktionen eines einzelnen Dienstes zur Unterstützung externer Clients zu erweitern.  Im Gegensatz dazu sind <code>Ingress</code> Ressourcen spezielle Ressourcen, mit denen Sie Load Balancer flexibel konfigurieren können.  Die Ingress-API unterstützt die Entschlüsselung von TLS-Verkehr, virtuellen Hosts und pfadbasiertem Routing.  Mit dieser API kann der Load Balancer einfach für die Arbeit mit mehreren Backend-Diensten konfiguriert werden. <br><br>  Die Ressourcen-API des <code>Ingress</code> Typs ist zu groß, um ihre Funktionen hier zu erläutern. Darüber hinaus hat dies keinen besonderen Einfluss auf die Funktionsweise der Ingress-Ressourcen auf Netzwerkebene.  Die Implementierung dieser Ressource folgt dem üblichen Kubernetes-Muster: Es gibt einen Ressourcentyp und einen Controller, um diesen Typ zu steuern.  Die Ressource in diesem Fall ist die <code>Ingress</code> Ressource, die Anforderungen an Netzwerkressourcen beschreibt.  So könnte die Beschreibung einer <code>Ingress</code> Ressource aussehen. <br><br><pre> <code class="plaintext hljs">apiVersion: extensions/v1beta1 kind: Ingress metadata: name: test-ingress annotations:   kubernetes.io/ingress.class: "gce" spec: tls:   - secretName: my-ssl-secret rules: - host: testhost.com   http:     paths:     - path: /*       backend:         serviceName: service-test         servicePort: 80</code> </pre> <br>  Der Ingress-Controller ist für die Ausführung dieser Anforderungen verantwortlich, indem er andere Ressourcen in den gewünschten Zustand versetzt.  Bei Verwendung von Ingress werden Dienste wie <code>NodePort</code> erstellt. <code>NodePort</code> der Ingress-Controller entscheiden, wie der Datenverkehr zu Knoten geleitet werden soll.  Es gibt eine Implementierung des Ingress-Controllers für GCE-Load-Balancer, für AWS-Balancer und für beliebte Proxy-Server wie Nginx und Haproxy.  Beachten Sie, dass das Mischen von Ingress-Ressourcen und -Diensten wie <code>LoadBalancer</code> in einigen Umgebungen zu geringfügigen Problemen <code>LoadBalancer</code> kann.  Sie sind einfach zu handhaben, aber im Allgemeinen ist es am besten, Ingress auch nur für einfache Dienste zu verwenden. <br><br><h2>  <font color="#3AC1EF">HostPort und HostNetwork</font> </h2><br>  Worüber wir jetzt sprechen werden, nämlich <code>HostPort</code> und <code>HostNetwork</code> , kann eher der Kategorie interessanter Raritäten und nicht nützlichen Werkzeugen zugeschrieben werden.  Tatsächlich verpflichte ich mich zu behaupten, dass in 99,99% der Fälle ihre Verwendung als Anti-Pattern angesehen werden kann und jedes System, in dem sie verwendet werden, einer obligatorischen Überprüfung seiner Architektur unterzogen werden muss. <br><br>  Ich dachte, dass es sich überhaupt nicht lohnt, über sie zu sprechen, aber sie sind so etwas wie die Tools, die von Ingress-Ressourcen verwendet werden, um eingehenden Datenverkehr zu verarbeiten. Deshalb habe ich beschlossen, dass es sich lohnt, sie zumindest kurz zu erwähnen. <br><br>  <code>HostPort</code> über <code>HostPort</code> sprechen.  Dies ist eine Containereigenschaft (in der <code>ContainerPort</code> Struktur deklariert).  Wenn eine bestimmte Portnummer darin geschrieben ist, führt dies zur Öffnung dieses Ports auf dem Knoten und zu seiner Umleitung direkt zum Container.  Es gibt keine Proxy-Mechanismen und der Port wird nur auf den Knoten geöffnet, auf denen der Container ausgeführt wird.  In den frühen Tagen der Plattform, bevor die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DaemonSet-</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">StatefulSet-</a> Mechanismen darin auftauchten, war <code>HostPort</code> ein Trick, mit dem nur ein Container eines bestimmten Typs auf einem Knoten gestartet werden konnte.  Ich habe dies beispielsweise einmal verwendet, um einen Elasticsearch-Cluster zu erstellen, indem <code>HostPort</code> auf <code>9200</code> und so viele Replikate angegeben wurden, wie Knoten vorhanden waren.       ,          Kubernetes,    -     <code>HostPort</code> . <br><br>   <code>NostNetwork</code> , ,   Kubernetes    ,  <code>HostPort</code> .       <code>true</code> ,       - <code>network=host</code>  <code>docker run</code> .    ,           .            <code>eth0</code>    .  ,             .      ,  ,  ,    Kubernetes,     - . <br><br><h2>  <font color="#3AC1EF">Zusammenfassung</font> </h2><br>        Kubernetes,   ,          Ingress. ,  ,    ,       Kubernetes. <br><br>  <b>Liebe Leser!</b>     Ingress? <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de442646/">https://habr.com/ru/post/de442646/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de442636/index.html">Bringen Sie dem Management schlechte Nachrichten?</a></li>
<li><a href="../de442638/index.html">Kubernetes Anwendungsskalierung basierend auf Metriken von Prometheus</a></li>
<li><a href="../de442640/index.html">Perfekter Fehler: Verwenden von Typverwirrung in Flash. Teil 1</a></li>
<li><a href="../de442642/index.html">Was Sie im März lesen sollten: 22 neue Bücher für Vermarkter, Manager, Entwickler und Designer</a></li>
<li><a href="../de442644/index.html">Die meisten Nicht-Programmierkenntnisse erhöhen den Entwicklerwert</a></li>
<li><a href="../de442648/index.html">Go-Zuweisungsmechanismen</a></li>
<li><a href="../de442650/index.html">Analyse und Optimierung von React-Anwendungen</a></li>
<li><a href="../de442652/index.html">Verwenden von Fastify und Preact zum schnellen Prototypen von Webanwendungen</a></li>
<li><a href="../de442654/index.html">Wechseln Sie zu Next.js und beschleunigen Sie das Laden der Manifold.co-Homepage um das 7,5-fache</a></li>
<li><a href="../de442658/index.html">8 Tricks für die Arbeit mit CSS: Parallaxe, Sticky Footer und andere</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>