<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíò üåÇ üöà ACL 2019 Konferenznotizen ü§æüèΩ üëáüèæ üë©‚Äçüë©‚Äçüëß‚Äçüë¶</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die Jahrestagung der Vereinigung f√ºr Computerlinguistik (ACL) ist die wichtigste Konferenz zur Verarbeitung nat√ºrlicher Sprache. Es ist seit 1962 orga...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>ACL 2019 Konferenznotizen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/463241/"><img src="https://habrastorage.org/webt/to/6d/jy/to6djymeashzcthmzckiaiwlnsg.jpeg"><br><br>  Die Jahrestagung der Vereinigung f√ºr Computerlinguistik (ACL) ist die wichtigste Konferenz zur Verarbeitung nat√ºrlicher Sprache.  Es ist seit 1962 organisiert.  Nach Kanada und Australien kehrte sie nach Europa zur√ºck und marschierte in Florenz.  So war es in diesem Jahr bei europ√§ischen Forschern beliebter als EMNLP √§hnlich. <br><br>  In diesem Jahr wurden 660 Artikel von 2900 eingereichten Artikeln ver√∂ffentlicht.  Eine riesige Menge.  Es ist kaum m√∂glich, eine objektive √úberpr√ºfung der Konferenz vorzunehmen.  Deshalb werde ich meine subjektiven Gef√ºhle von diesem Ereignis erz√§hlen. <br><a name="habracut"></a><br>  Ich kam zur Konferenz, um in einer Postersession <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">unsere Entscheidung</a> des Kaggle-Wettbewerbs √ºber Googles <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Gendered Pronoun Resolution zu zeigen</a> .  Unsere L√∂sung st√ºtzte sich stark auf die Verwendung von vorgefertigten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">BERT-Modellen</a> .  Und wie sich herausstellte, waren wir damit nicht allein. <br><br><h2>  Bertologie </h2><br><img src="https://habrastorage.org/webt/ol/zh/ba/olzhbat3al984zylni9zvcrizqo.jpeg"><br>  Es gab so viele Arbeiten, die auf BERT basierten, seine Eigenschaften beschrieben und als Keller nutzten, dass sogar der Begriff Bertologie auftauchte.  In der Tat haben sich die BERT-Modelle als so erfolgreich erwiesen, dass selbst gro√üe Forschungsgruppen ihre Modelle mit dem BERT vergleichen. <br><br>  Anfang Juni erschien die Arbeit √ºber <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">XLNet</a> .  Und kurz vor der Konferenz - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ERNIE 2.0</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RoBERTa</a> <br><br><h4>  Facebook RoBERTa </h4><br>  Als das XLNet-Modell erstmals vorgestellt wurde, schlugen einige Forscher vor, dass es nicht nur aufgrund seiner Architektur und seiner Trainingsprinzipien bessere Ergebnisse erzielt.  Sie studierte auch an einem gr√∂√üeren K√∂rper (fast 10 Mal) als BERT und l√§nger (4 Mal mehr Iterationen). <br><br>  Forscher bei Facebook haben gezeigt, dass BERT sein Maximum noch nicht erreicht hat.  Sie pr√§sentierten einen optimierten Ansatz f√ºr die Vermittlung des BERT-Modells - RoBERTa (Robust optimierter BERT-Ansatz). <br><br>  Sie √§nderten nichts an der Architektur des Modells und √§nderten das Trainingsverfahren: <br><br><ol><li>  Wir haben den K√∂rper f√ºr das Training, die Gr√∂√üe der Charge, die L√§nge der Sequenz und die Zeit des Trainings erh√∂ht. </li><li>  Die Aufgabe, den n√§chsten Satz vorherzusagen, wurde aus dem Training genommen. </li><li>  Sie begannen, dynamisch MASK-Token zu generieren (Token, die das Modell w√§hrend des Vortrainings vorherzusagen versucht). </li></ol><br><h4>  ERNIE 2.0 von Baidu </h4><br>  Wie alle g√§ngigen neueren Modelle (BERT, GPT, XLM, RoBERTa, XLNet) basiert ERNIE auf dem Konzept eines Transformators mit Selbstaufmerksamkeitsmechanismus.  Was es von anderen Modellen unterscheidet, sind die Konzepte des Multi-Task-Lernens und des kontinuierlichen Lernens. <br><br>  ERNIE lernt in verschiedenen Aufgaben und aktualisiert st√§ndig die interne Darstellung seines Sprachmodells.  Diese Aufgaben haben wie andere Modelle selbstlernende (selbst√ºberwachte und schwach √ºberwachte) Ziele.  Beispiele f√ºr solche Aufgaben: <br><br><ul><li>  Stellen Sie die richtige Wortreihenfolge in einem Satz wieder her. </li><li>  Gro√üschreibung von W√∂rtern. </li><li>  Definition von maskierten W√∂rtern. </li></ul><br>  Bei diesen Aufgaben lernt das Modell nacheinander und kehrt zu den Aufgaben zur√ºck, f√ºr die es zuvor trainiert wurde. <br><br><h4>  RoBERTa gegen ERNIE </h4><br>  In Ver√∂ffentlichungen werden RoBERTa und ERNIE nicht miteinander verglichen, da sie fast gleichzeitig auftraten.  Sie werden mit BERT und XLNet verglichen.  Aber hier ist es nicht so einfach, einen Vergleich anzustellen.  Zum Beispiel wird <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GLUE</a> XLNet im beliebten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Benchmark</a> durch ein Ensemble von Modellen dargestellt.  Und Forscher aus Baidu sind mehr daran interessiert, einzelne Modelle zu vergleichen.  Da Baidu ein chinesisches Unternehmen ist, sind sie au√üerdem daran interessiert, die Ergebnisse der Arbeit mit der chinesischen Sprache zu vergleichen.  In j√ºngerer Zeit ist ein neuer Benchmark erschienen: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SuperGLUE</a> .  Es gibt noch nicht viele L√∂sungen, aber RoBERTa steht hier an erster Stelle. <br><br>  Insgesamt schneiden RoBERTa und ERNIE jedoch besser als XLNet und deutlich besser als BERT ab.  RoBERTa wiederum arbeitet etwas besser als ERNIE. <br><br><h2>  Diagramme des Wissens </h2><br>  Es wurde viel Arbeit darauf verwendet, zwei Ans√§tze zu kombinieren: vorgefertigte Netzwerke und die Verwendung von Regeln in Form von Wissensgraphen (Knowledge Graphs, KG). <br><br>  Zum Beispiel: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ERNIE: Verbesserte Sprachrepr√§sentation mit informativen Einheiten</a> .  In diesem Artikel wird die Verwendung von Wissensdiagrammen √ºber dem BERT-Sprachmodell hervorgehoben.  Auf diese Weise k√∂nnen Sie bessere Ergebnisse bei Aufgaben wie der Bestimmung des Entit√§tstyps erzielen ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Entit√§tstypisierung) und Beziehungsklassifizierung</a> . <br><br>  Im Allgemeinen f√ºhrt die Mode, Namen f√ºr Models anhand der Namen von Charakteren aus der Sesamstra√üe zu w√§hlen, zu lustigen Konsequenzen.  Zum Beispiel hat diese ERNIE nichts mit Baidus ERNIE 2.0 zu tun, √ºber das ich oben geschrieben habe. <br><br><img src="https://habrastorage.org/webt/um/6y/qp/um6yqpe7esqpdixrlodndhuf_pi.jpeg"><br><br>  Eine weitere interessante Arbeit zur Generierung neuen Wissens: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">COMET: Commonsense-Transformatoren f√ºr die automatische Erstellung von Wissensgraphen</a> .  Das Papier pr√ºft die M√∂glichkeit, neue Architekturen auf Basis von Transformatoren f√ºr das Training wissensbasierter Netzwerke zu verwenden.  Wissensbasen in vereinfachter Form sind viele Dreifache: Subjekt, Haltung, Objekt.  Sie nahmen zwei Wissensdatenbank-Datens√§tze: ATOMIC und ConceptNet.  Und sie trainierten ein Netzwerk, das auf dem GPT-Modell (Generative Pre-trained Transformer) basiert.  Das Subjekt und die Haltung wurden eingegeben und versucht, das Objekt vorherzusagen.  So erhielten sie ein Modell, das Objekte durch Eingabe von Subjekten und Beziehungen generiert. <br><br><h2>  Metriken </h2><br>  Ein weiteres interessantes Thema der Konferenz war die Auswahl von Metriken.  Es ist oft schwierig, die Qualit√§t eines Modells bei der Verarbeitung nat√ºrlicher Sprache zu bewerten, was den Fortschritt in diesem Bereich des maschinellen Lernens verlangsamt. <br><br>  In einem Artikel zur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bewertung von Zusammenfassungsbewertungsmetriken im</a> Artikel " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Angemessener</a> Bewertungsbereich" erl√§utert Maxim Peyar die Verwendung verschiedener Metriken in einem Textzusammenfassungsproblem.  Diese Metriken korrelieren nicht immer gut miteinander, was den objektiven Vergleich verschiedener Algorithmen st√∂rt. <br><br>  Oder hier ist ein interessanter Job: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Automatische Auswertung f√ºr Texte mit mehreren S√§tzen</a> .  Darin pr√§sentieren die Autoren eine Metrik, die BLEU und ROUGE bei Aufgaben ersetzen kann, bei denen Sie Texte aus mehreren S√§tzen bewerten m√ºssen. <br><br>  Die BLEU-Metrik kann als Pr√§zision dargestellt werden - wie viele W√∂rter (oder n-Gramm) aus der Antwort des Modells sind im Ziel enthalten.  ROUGE ist Recall - wie viele W√∂rter (oder n-Gramm) vom Ziel in der Antwort des Modells enthalten sind. <br><br>  Die im Artikel vorgeschlagene Metrik basiert auf der WMD-Metrik (Word Mover's Distance) - der Entfernung zwischen zwei Dokumenten.  Sie entspricht dem Mindestabstand zwischen W√∂rtern in zwei S√§tzen im Raum der Vektordarstellung dieser W√∂rter.  Weitere Informationen zu Massenvernichtungswaffen finden Sie im Tutorial, in dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Massenvernichtungswaffen von Word2Vec</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GloVe verwendet werden</a> . <br><br>  In ihrem Artikel bieten sie eine neue Metrik an: WMS (Word Mover's Similarity). <br><br><pre><code class="plaintext hljs">WMS(A, B) = exp(‚àíWMD(A, B))</code> </pre> <br>  Anschlie√üend definieren sie SMS (Satzbewegungs√§hnlichkeit).  Es wird ein √§hnlicher Ansatz wie bei WMS verwendet.  Als Vektordarstellung des Satzes nehmen sie den gemittelten Vektor von Satzw√∂rtern. <br><br>  Bei der Berechnung von WMS werden W√∂rter anhand ihrer H√§ufigkeit im Dokument normalisiert.  Bei der Berechnung werden SMS-S√§tze durch die Anzahl der W√∂rter im Satz normalisiert. <br><br>  Schlie√ülich ist die S + WMS-Metrik eine Kombination aus WMS und SMS.  In ihrem Artikel weisen sie darauf hin, dass ihre Metriken besser mit der manuellen Bewertung einer Person korrelieren. <br><br><h2>  Chatbots </h2><br>  Der n√ºtzlichste Teil der Konferenz waren meiner Meinung nach Postersessions.  Nicht alle Berichte waren interessant, aber wenn Sie anfingen, einige anzuh√∂ren, werden Sie in der Mitte des Berichts nicht zu einem anderen gehen.  Plakate sind eine andere Sache.  Es gibt mehrere Dutzend von ihnen bei der Postersession.  Sie w√§hlen die aus, die Ihnen gefallen, und k√∂nnen in der Regel direkt mit dem Entwickler √ºber technische Details sprechen.  √úbrigens gibt es eine interessante Seite mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Postern von Konferenzen</a> .  Zwar gibt es dort Poster von zwei Konferenzen, und es ist nicht bekannt, ob die Website aktualisiert wird. <br><br><img src="https://habrastorage.org/webt/tm/y9/z4/tmy9z4i7y1tzu2gxpk5kfheopqy.jpeg"><br><br>  In Postersessions pr√§sentierten gro√üe Unternehmen oft interessante Arbeiten.  Hier ist zum Beispiel ein Facebook-Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Lernen aus dem Dialog nach der Bereitstellung: F√ºttern Sie sich selbst, Chatbot!</a>  . <br><br>  Die Besonderheit ihres Systems ist die erweiterte Verwendung von Benutzerantworten.  Sie haben einen Klassifikator, der bewertet, wie zufrieden der Benutzer mit dem Dialog ist.  Sie verwenden diese Informationen f√ºr verschiedene Aufgaben: <br><br><ul><li>  Verwenden Sie ein Ma√ü f√ºr die Zufriedenheit als Ma√ü f√ºr die Qualit√§t. </li><li>  Sie trainieren das Modell und wenden so den Ansatz des kontinuierlichen Lernens (Continuous Learning) an. </li><li>  Verwenden Sie direkt im Dialog.  Dr√ºcken Sie eine menschliche Reaktion aus, wenn der Benutzer zufrieden ist.  Oder sie fragen, was falsch ist, wenn der Benutzer nicht zufrieden ist. </li></ul><br>  Aus den Berichten ging eine interessante Geschichte √ºber den chinesischen Chatbot von Microsoft hervor.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Das Design und die Implementierung von XiaoIce, einem einf√ºhlsamen sozialen Chatbot</a> <br><br>  China ist bereits f√ºhrend bei der Einf√ºhrung k√ºnstlicher Intelligenz.  Aber oft ist das, was in China passiert, in Europa nicht bekannt.  Und XiaoIce ist ein erstaunliches Projekt.  Es existiert bereits seit f√ºnf Jahren.  Derzeit arbeiten nicht viele Chatbots in diesem Alter.  Im Jahr 2018 hatte es bereits 660 Millionen Nutzer. <br><br>  Das System verf√ºgt sowohl √ºber einen Chit-Chat-Bot als auch √ºber ein Skill-System.  Der Bot hat bereits 230 Fertigkeiten, das hei√üt, sie f√ºgen ungef√§hr eine Fertigkeit pro Woche hinzu. <br><br>  Um die Qualit√§t des Chit-Chat-Bots zu beurteilen, verwenden sie die Dauer des Dialogs.  Und nicht in Minuten, wie es oft gemacht wird, sondern in der Anzahl der Replikate in einem Gespr√§ch.  Sie nennen diese Metrik Conversation-Turns Per Session (CPS) und schreiben, dass ihr Durchschnittswert derzeit 23 betr√§gt, was der beste Indikator unter √§hnlichen Systemen ist. <br><br>  Im Allgemeinen ist das Projekt in China sehr beliebt.  Neben dem Bot selbst schreibt das System Gedichte, zeichnet Bilder, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ver√∂ffentlicht eine Sammlung von Kleidern</a> und singt Lieder. <br><br><h2>  Maschinelle √úbersetzung </h2><br>  Von allen Reden, an denen ich teilnahm, war der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Simultan√ºbersetzungsbericht</a> von Liang Huang, der Baidu Research vertrat, der lebhafteste. <br><br>  Er sprach √ºber solche Schwierigkeiten bei der modernen Simultan√ºbersetzung: <br><br><ul><li>  Es gibt weltweit nur 3.000 zertifizierte Simultandolmetscher. </li><li>  √úbersetzer k√∂nnen nur 15 bis 20 Minuten ununterbrochen arbeiten. </li><li>  Nur etwa 60% des Quelltextes werden √ºbersetzt. </li></ul><br>  Die √úbersetzung ganzer S√§tze hat bereits ein gutes Niveau erreicht, aber f√ºr die Simultan√ºbersetzung gibt es noch Verbesserungspotenzial.  Als Beispiel f√ºhrte er ihr Simultan√ºbersetzungssystem an, das auf der Baidu-Weltkonferenz funktionierte.  Die Verz√∂gerung bei der √úbersetzung im Jahr 2018 gegen√ºber 2017 wurde von 10 auf 3 Sekunden reduziert. <br><br>  Dies tun nicht viele Teams, und es gibt nur wenige funktionierende Systeme.  Wenn Google beispielsweise die online geschriebene Phrase √ºbersetzt, wird die endg√ºltige Phrase st√§ndig neu erstellt.  Und dies ist keine Simultan√ºbersetzung, da wir bei Simultan√ºbersetzung die bereits gesprochenen W√∂rter nicht √§ndern k√∂nnen. <br><br><img src="https://habrastorage.org/webt/ko/z9/xg/koz9xgvrqfclne8wwzw02fxwrdy.png"><br><br>  In ihrem System verwenden sie die Pr√§fix√ºbersetzung - Teil einer Phrase.  Das hei√üt, sie warten ein paar Worte und beginnen zu √ºbersetzen, um zu erraten, was in der Quelle erscheinen wird.  Die Gr√∂√üe dieser Verschiebung wird in Worten gemessen und ist adaptiv.  Nach jedem Schritt entscheidet das System, ob sich das Warten lohnt oder ob es bereits √ºbersetzt werden kann.  Um diese Verz√∂gerung zu bewerten, f√ºhren sie die folgende Metrik ein: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Metrik der durchschnittlichen</a> Verz√∂gerung <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">(AL)</a> . <br><br>  Die Hauptschwierigkeit bei der Simultan√ºbersetzung ist die unterschiedliche Wortreihenfolge in Sprachen.  Und der Kontext hilft, dies zu bek√§mpfen.  Zum Beispiel m√ºssen Sie h√§ufig die Reden von Politikern √ºbersetzen, und sie sind ziemlich stereotyp.  Es gibt aber auch Probleme.  Dann scherzte der Sprecher √ºber Trump.  Also, sagt er, wenn Bush nach Moskau geflogen ist, dann ist es sehr wahrscheinlich, dass er sich mit Putin trifft.  Und wenn Trump geflogen ist, kann er sich treffen und Golf spielen.  Im Allgemeinen kommen die Leute beim √úbersetzen oft auf die Idee, etwas von sich selbst hinzuzuf√ºgen.  Nehmen wir an, wenn Sie eine Art Witz √ºbersetzen m√ºssen und sie es nicht sofort tun k√∂nnen, k√∂nnen sie sagen: "Hier wurde ein Witz gesagt, lachen Sie einfach." <br><br>  Es gab auch einen Artikel √ºber maschinelle √úbersetzung, der mit dem Preis ‚ÄûThe Best Long Paper‚Äú ausgezeichnet wurde: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">√úberbr√ºckung der L√ºcke zwischen Training und Inferenz f√ºr neuronale maschinelle √úbersetzung</a> . <br><br>  Es beschreibt ein solches Problem der maschinellen √úbersetzung.  W√§hrend des Lernprozesses generieren wir eine Wort-f√ºr-Wort-√úbersetzung basierend auf dem Kontext bekannter W√∂rter.  Bei der Verwendung des Modells st√ºtzen wir uns auf den Kontext der neu generierten W√∂rter.  Es besteht eine Diskrepanz zwischen dem Training des Modells und seiner Verwendung. <br><br>  Um diese Diskrepanz zu verringern, schlagen die Autoren vor, in der Phase des Trainings im Kontext die W√∂rter zu mischen, die vom Modell w√§hrend des Trainings vorhergesagt werden.  Der Artikel beschreibt die optimale Auswahl solcher generierten W√∂rter. <br><br><h2>  Fazit </h2><br>  Nat√ºrlich besteht eine Konferenz nicht nur aus Artikeln und Berichten.  Es ist auch Kommunikation, Dating und andere Vernetzung.  Dar√ºber hinaus versuchen Konferenzorganisatoren, die Teilnehmer irgendwie zu unterhalten.  In der ACL, auf der Hauptparty, gab es eine Auff√ºhrung von Ten√∂ren, schlie√ülich Italien.  Zusammenfassend gab es Ank√ºndigungen der Organisatoren anderer Konferenzen.  Die heftigste Reaktion unter den Teilnehmern wurde durch die Nachrichten der Organisatoren von EMNLP ausgel√∂st, dass die Hauptpartei in diesem Jahr in Hongkong Disneyland stattfinden wird und die Konferenz 2020 in der Dominikanischen Republik stattfinden wird. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de463241/">https://habr.com/ru/post/de463241/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de463229/index.html">Jetpacks in Kultur: Kino</a></li>
<li><a href="../de463231/index.html">Schulung Cisco 200-125 CCNA v3.0. Tag 14. VTP, Bereinigen und natives VLAN</a></li>
<li><a href="../de463233/index.html">Schulung Cisco 200-125 CCNA v3.0. Tag 15. Langsame Kommunikation und Port-Sicherheit</a></li>
<li><a href="../de463237/index.html">Wie wir Musik mit neuronalen Netzen v 2.0 gespielt haben</a></li>
<li><a href="../de463239/index.html">22. August - Alfa JS MeetUP SPb</a></li>
<li><a href="../de463243/index.html">Manipulation des Bewusstseins. Warum ist es so einfach?</a></li>
<li><a href="../de463245/index.html">Wie das DWH-Repository in TELE2 angeordnet wurde</a></li>
<li><a href="../de463247/index.html">Informationstools oder wie wir √ºber unsere Dienstleistungen und Prozesse sprechen</a></li>
<li><a href="../de463249/index.html">Game Dev Sim: Brettspiel √ºber Spieleentwicklung</a></li>
<li><a href="../de463251/index.html">So schneiden Sie die Stadtteilmenge (eine beliebige Beziehung) aus OSM-Daten</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>