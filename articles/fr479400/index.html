<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõµ üèâ üì† Comment faire d'un journaliste un r√©seau de neurones, ou "Les secrets de la r√©duction du texte sur Habr√© sans un mot" üë©üèæ‚Äçüé® üö∂üèΩ ‚ô¶Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ne soyez pas surpris, mais la deuxi√®me rubrique de cet article a g√©n√©r√© un r√©seau de neurones, ou plut√¥t l'algorithme de sammarisation. Et qu'est-ce q...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment faire d'un journaliste un r√©seau de neurones, ou "Les secrets de la r√©duction du texte sur Habr√© sans un mot"</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/abbyy/blog/479400/"><img src="https://habrastorage.org/webt/iq/f_/fg/iqf_fgkfyqb1rbtwvba0p4gbsiq.png" align="left">  Ne soyez pas surpris, mais la deuxi√®me rubrique de cet article a g√©n√©r√© un r√©seau de neurones, ou plut√¥t l'algorithme de sammarisation.  Et qu'est-ce que la sammarisation? <br><br>  C'est l'un des <a href="https://habr.com/ru/company/abbyy/blog/437008/">d√©fis</a> cl√©s et classiques <a href="https://habr.com/ru/company/abbyy/blog/437008/">du traitement automatique du langage naturel (NLP)</a> .  Il consiste √† cr√©er un algorithme qui prend du texte en entr√©e et en √©dite une version abr√©g√©e.  De plus, la structure correcte (correspondant aux normes de la langue) y est pr√©serv√©e et l'id√©e principale du texte est correctement transmise. <br><br>  De tels algorithmes sont largement utilis√©s dans l'industrie.  Par exemple, ils sont utiles pour les moteurs de recherche: en utilisant la r√©duction de texte, vous pouvez facilement comprendre si l'id√©e principale d'un site ou d'un document est en corr√©lation avec une requ√™te de recherche.  Ils sont utilis√©s pour rechercher des informations pertinentes dans un large flux de donn√©es multim√©dias et pour filtrer les informations inutiles.  La r√©duction de texte aide √† la recherche financi√®re, √† l'analyse des contrats juridiques, √† l'annotation des articles scientifiques et bien plus encore.  Soit dit en passant, l'algorithme de sammarisation a g√©n√©r√© toutes les sous-rubriques pour ce message. <br><br>  √Ä ma grande surprise, sur Habr√© il y avait tr√®s peu d'articles sur la sammarisation, j'ai donc d√©cid√© de partager mes recherches et r√©sultats dans ce sens.  Cette ann√©e, j'ai particip√© √† l'hippodrome lors de la conf√©rence <a href="http://www.dialog-21.ru/">Dialogue</a> et exp√©riment√© avec des g√©n√©rateurs de titres pour des informations et des po√®mes utilisant des r√©seaux de neurones.  Dans cet article, je vais d'abord bri√®vement passer en revue la partie th√©orique de la sammarisation, puis je donnerai des exemples avec la g√©n√©ration de titres, je vous dirai quelles difficult√©s les mod√®les ont lors de la r√©duction du texte et comment ces mod√®les peuvent √™tre am√©lior√©s pour obtenir de meilleurs titres. <br><a name="habracut"></a><br>  Vous trouverez ci-dessous un exemple de news et son titre de r√©f√©rence d'origine.  Les mod√®les dont je vais parler s'entra√Æneront √† g√©n√©rer des en-t√™tes avec cet exemple: <br><br><img src="https://habrastorage.org/webt/8n/yy/gc/8nyygcx_ymy38dwrqpbqdiylvsu.png" alt="image"><br><br><h2>  Secrets pour couper l'architecture de texte seq2seq </h2><br>  Il existe deux types de m√©thodes de r√©duction de texte: <br><br><ol><li>  <b>Extractive</b> .  Elle consiste √† trouver les parties les plus informatives du texte et √† en construire l'annotation correcte pour la langue donn√©e.  Ce groupe de m√©thodes utilise uniquement les mots qui se trouvent dans le texte source. </li><li>  <b>R√©sum√©</b>  Il consiste √† extraire des liens s√©mantiques du texte, tout en tenant compte des d√©pendances linguistiques.  Avec la sammarisation abstraite, les mots d'annotation ne sont pas s√©lectionn√©s dans le texte abr√©g√©, mais dans le dictionnaire (la liste des mots pour une langue donn√©e) - reformulant ainsi l'id√©e principale. </li></ol><br>  La seconde approche implique que l'algorithme doit prendre en compte les d√©pendances du langage, reformuler et g√©n√©raliser.  Il souhaite √©galement avoir une certaine connaissance du monde r√©el afin d'√©viter les erreurs factuelles.  Pendant longtemps, cela a √©t√© consid√©r√© comme une t√¢che difficile, et les chercheurs n'ont pas pu obtenir une solution de haute qualit√© - un texte grammaticalement correct tout en pr√©servant l'id√©e principale.  C'est pourquoi dans le pass√©, la plupart des algorithmes √©taient bas√©s sur une approche d'extraction, car la s√©lection de morceaux entiers de texte et leur transfert vers le r√©sultat vous permet de maintenir le m√™me niveau d'alphab√©tisation que la source. <br><br>  Mais c'√©tait avant le boom des r√©seaux de neurones et sa p√©n√©tration imminente dans la PNL.  En 2014, l'architecture <b>seq2seq a</b> √©t√© <b>introduite avec un m√©canisme d'attention</b> qui peut lire certaines s√©quences de texte et en g√©n√©rer d'autres (ce qui d√©pend de ce que le mod√®le a appris √† produire) ( <a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">article</a> de Sutskever et al.).  En 2016, une telle architecture a √©t√© appliqu√©e directement √† la solution du probl√®me de la sammarisation, r√©alisant ainsi une approche abstraite et obtenant un r√©sultat comparable √† ce qu'une personne comp√©tente pourrait √©crire ( <a href="https://arxiv.org/pdf/1602.06023.pdf">article</a> de Nallapati et al., 2016; <a href="https://arxiv.org/pdf/1509.00685.pdf">article</a> de Rush et al., 2015; )  Comment fonctionne cette architecture? <br><br><img src="https://habrastorage.org/webt/px/4x/wi/px4xwin577dd65z-lahsx7fvexs.png" alt="image"><br><br>  Seq2Seq se compose de deux parties: <br><br><ol><li>  <b>Encodeur</b> (Encoder) - un RNN bidirectionnel, qui est utilis√© pour lire la s√©quence d'entr√©e, c'est-√†-dire qui traite s√©quentiellement les √©l√©ments d'entr√©e simultan√©ment de gauche √† droite et de droite √† gauche pour mieux prendre en compte le contexte. </li><li>  <b>d√©codeur</b> (d√©codeur) - RNN unidirectionnel, qui produit s√©quentiellement et par √©l√©ment une s√©quence de sortie. </li></ol><br>  Premi√®rement, la s√©quence d'entr√©e est traduite en une s√©quence d'int√©gration (en bref, l'int√©gration est une repr√©sentation concise d'un mot en tant que vecteur).  Les plongements passent ensuite par le r√©seau r√©cursif de l'encodeur.  Ainsi, pour chaque mot, nous obtenons les √©tats cach√©s de l'encodeur ( <i>indiqu√©s par des rectangles rouges dans le diagramme</i> ), et ils contiennent des informations sur le jeton lui-m√™me et son contexte, ce qui nous permet de prendre en compte les connexions linguistiques entre les mots. <br><br>  Apr√®s avoir trait√© l'entr√©e, l'encodeur transf√®re son dernier √©tat cach√© (qui contient des informations compress√©es sur le texte entier) au d√©codeur, qui re√ßoit un jeton sp√©cial <img src="https://habrastorage.org/webt/qn/ud/38/qnud38u15vpvzie4zkne-doze7k.png" alt="image">  et cr√©e le premier mot de la s√©quence de sortie ( <i>dans l'image, c'est ¬´Allemagne¬ª</i> ).  Ensuite, il prend cycliquement sa sortie pr√©c√©dente, la transmet √† lui-m√™me et affiche √† nouveau l'√©l√©ment de sortie suivant ( <i>donc apr√®s ¬´Allemagne¬ª vient ¬´beat¬ª, et apr√®s ¬´beat¬ª vient le mot suivant, etc.</i> ).  Ceci est r√©p√©t√© jusqu'√† ce qu'un jeton sp√©cial soit √©mis <img src="https://habrastorage.org/webt/vw/kv/4g/vwkv4gs-ul7vlvnkjwxag8njicw.png" alt="image">  .  Cela signifie la fin de la g√©n√©ration. <br><br>  Pour afficher l'√©l√©ment suivant, le d√©codeur, tout comme l'encodeur, convertit le jeton d'entr√©e en int√©gration, fait un pas dans le r√©seau r√©cursif et re√ßoit le prochain √©tat cach√© du d√©codeur ( <i>rectangles jaunes dans le diagramme</i> ).  Ensuite, en utilisant une couche enti√®rement connect√©e, une distribution de probabilit√© est obtenue pour tous les mots √† partir d'un dictionnaire de mod√®les pr√©compil√©.  Les mots les plus probables seront d√©duits par le mod√®le. <br><br>  L'ajout d' <b>un m√©canisme d'attention</b> aide le d√©codeur √† mieux utiliser les informations d'entr√©e.  Le m√©canisme √† chaque √©tape de la g√©n√©ration d√©termine la <b>distribution</b> dite d' <b>attention</b> (les <i>rectangles bleus sur la figure sont l'ensemble des poids correspondant aux √©l√©ments de la s√©quence d'origine, la somme des poids est 1, tous les poids&gt; = 0</i> ), et √† partir de cela, il re√ßoit la somme pond√©r√©e de tous les √©tats cach√©s de l'encodeur, formant ainsi vecteur de contexte ( <i>le diagramme montre un rectangle rouge avec un trait bleu</i> ).  Ce vecteur concat√®ne avec l'incorporation du mot d'entr√©e du d√©codeur au stade du calcul de l'√©tat latent et avec l'√©tat latent lui-m√™me au stade de la d√©termination du mot suivant.  Ainsi, √† chaque √©tape de la sortie, le mod√®le peut d√©terminer quels √©tats du codeur sont les plus importants pour lui pour le moment.  En d'autres termes, il d√©cide du contexte dans lequel les mots d'entr√©e doivent √™tre le plus pris en compte (par exemple, dans l'image, affichant le mot "beat", le m√©canisme d'attention accorde un poids important aux jetons "victorieux" et "win", et les autres sont proches de z√©ro). <br><br>  √âtant donn√© que la g√©n√©ration d'en-t√™tes est √©galement l'une des t√¢ches de la sammarisation, uniquement avec le minimum de sortie possible (1-12 mots), j'ai d√©cid√© d'appliquer √©galement <b>seq2seq avec le m√©canisme d'attention</b> pour notre cas.  Nous formons un tel syst√®me sur des textes avec des titres, par exemple, sur l'actualit√©.  De plus, il est conseill√© au stade de la formation de soumettre au d√©codeur non pas sa propre sortie, mais les mots du vrai titre (for√ßage de l'enseignant), facilitant ainsi la vie de lui-m√™me et du mod√®le.  En tant que fonction d'erreur, nous utilisons la fonction de perte d'entropie crois√©e standard, montrant √† quel point les distributions de probabilit√© du mot de sortie et du mot de l'en-t√™te r√©el sont proches: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/3v/co/b9/3vcob9esvdhgsydjcnjfpzv2wbi.png"></div><br>  Lorsque vous utilisez le mod√®le entra√Æn√©, nous utilisons la recherche de rayons pour trouver une s√©quence de mots plus probable que l'utilisation de l'algorithme gourmand.  Pour ce faire, √† chaque √©tape de la g√©n√©ration, nous ne d√©rivons pas le mot le plus probable, mais en m√™me temps regardons la largeur de faisceau des s√©quences de mots les plus probables.  Quand ils se terminent (chacun se termine le <img src="https://habrastorage.org/webt/vw/kv/4g/vwkv4gs-ul7vlvnkjwxag8njicw.png" alt="image">  ), nous d√©rivons la s√©quence la plus probable. <br><br><img src="https://habrastorage.org/webt/t2/dt/ni/t2dtnicefjxn0hwd0elaeduekbk.png" alt="image"><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yx/ln/q-/yxlnq-7z2-lwuyylmz83pferopm.png"></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ey/81/9u/ey819uvi_jhx4wbwchr-wsixvgs.png"></div><div style="text-align:center;"><img src="https://habrastorage.org/webt/q1/tn/zu/q1tnzuopjtdslrltsmn8euup-kq.png"></div><br><br><h2>  Evolution du mod√®le </h2><br>  L'un des probl√®mes du mod√®le sur seq2seq est l'impossibilit√© de citer des mots qui ne sont pas dans le dictionnaire.  Par exemple, le mod√®le n'a aucune chance de d√©duire "obamacare" de l'article ci-dessus.  Il en va de m√™me pour: <br><br><ul><li>  noms et pr√©noms rares </li><li>  nouveaux termes </li><li>  des mots dans d'autres langues, </li><li>  diff√©rentes paires de mots reli√©s par un trait d'union (en tant que "s√©nateur r√©publicain") </li><li>  et d'autres mod√®les. </li></ul><br>  Bien s√ªr, vous pouvez √©tendre le dictionnaire, mais cela augmente consid√©rablement le nombre de param√®tres entra√Æn√©s.  De plus, il est n√©cessaire de fournir un grand nombre de documents dans lesquels se trouvent ces mots rares, afin que le g√©n√©rateur apprenne √† les utiliser de mani√®re qualitative. <br><br>  Une autre solution plus √©l√©gante √† ce probl√®me a √©t√© pr√©sent√©e dans un article de 2017 - ¬´ <a href="https://arxiv.org/pdf/1704.04368.pdf">Get to the Point: Summarization with Pointer-Generator Networks</a> ¬ª (Abigail See et al.).  Elle ajoute un nouveau m√©canisme √† notre mod√®le - <b>un m√©canisme de</b> pointeur, qui peut s√©lectionner des mots dans le texte source et les ins√©rer directement dans la s√©quence g√©n√©r√©e.  Si le texte contient OOV ( <i>hors vocabulaire - un mot qui n'est pas dans le dictionnaire</i> ), alors le mod√®le, s'il le juge n√©cessaire, peut isoler OOV et l'ins√©rer √† la sortie.  Un tel syst√®me est appel√© <b>¬´</b> pointeur-g√©n√©rateur¬ª (pointeur-g√©n√©rateur ou pg) et est une synth√®se de deux approches de la sammarisation.  Elle-m√™me peut d√©cider √† quelle √©tape elle doit √™tre abstraite et √† quelle √©tape - extraire.  Comment elle le fait, nous allons le d√©couvrir maintenant. <br><br><img src="https://habrastorage.org/webt/f-/9y/xw/f-9yxwborbgpjpalzwd5e_f74xi.png" alt="image"><br><br>  La principale diff√©rence par rapport au mod√®le seq2seq habituel est l'action suppl√©mentaire sur laquelle p <sub>gen</sub> est calcul√© - la probabilit√© de g√©n√©ration.  Cela se fait √† l'aide de l'√©tat cach√© du d√©codeur et du vecteur de contexte.  La signification de l'action suppl√©mentaire est simple.  Plus p <sub>gen est</sub> proche de 1, plus il est probable que le mod√®le √©mette un mot de son dictionnaire en utilisant la g√©n√©ration abstraite.  Plus p <sub>gen est</sub> proche de 0, plus il est probable que le g√©n√©rateur extrait le mot du texte, guid√© par la r√©partition de l'attention obtenue pr√©c√©demment.  La distribution de probabilit√© finale des r√©sultats du mot est la somme de la distribution de probabilit√© g√©n√©r√©e des mots (dans laquelle il n'y a pas d'OOV) multipli√©e par p <sub>gen</sub> et de la distribution de l'attention (dans laquelle OOV est, par exemple, ¬´2-0¬ª dans l'image) multipli√©e par (1 - p <sub>gen</sub> ). <br><br><img src="https://habrastorage.org/webt/iv/8f/8q/iv8f8qvol1j3bbyx79vt5zo7-oq.png" alt="image"><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/if/ew/s3/ifews3gqgojf5mclbxd0lpkz2q8.png"></div><br>  En plus du m√©canisme de pointage, l'article pr√©sente <b>un m√©canisme de couverture</b> , qui permet d'√©viter de r√©p√©ter des mots.  Je l'ai √©galement exp√©riment√©, mais je n'ai pas remarqu√© d'am√©lioration significative de la qualit√© des en-t√™tes - ce n'est pas vraiment n√©cessaire.  Tr√®s probablement, cela est d√ª aux sp√©cificit√©s de la t√¢che: comme il est n√©cessaire de sortir un petit nombre de mots, le g√©n√©rateur n'a tout simplement pas le temps de se r√©p√©ter.  Mais pour d'autres t√¢ches de sammarisation, par exemple, l'annotation, cela peut √™tre utile.  Si vous √™tes int√©ress√©, vous pouvez en lire plus dans l' <a href="https://arxiv.org/pdf/1704.04368.pdf">article</a> original. <br><br><img src="https://habrastorage.org/webt/4q/bx/op/4qbxopqt862cphikmbezyaoadlk.png" alt="image"><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/--/3m/mz/--3mmzahhdk8kelwoqyh9rdjg8e.png"></div><br><h2>  Grande vari√©t√© de mots russes </h2><br>  Une autre fa√ßon d'am√©liorer la qualit√© des en-t√™tes de sortie consiste √† pr√©traiter correctement la s√©quence d'entr√©e.  En plus de l'√©limination √©vidente des caract√®res majuscules, j'ai √©galement essay√© de convertir les mots du texte source en paires de styles et d'inflexions (c'est-√†-dire les fondations et les terminaisons).  Pour le fractionnement, utilisez le Porter Stemmer. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jy/j8/-y/jyj8-yonzpmvpyoye6mbygrlcjg.png"></div><div style="text-align:center;"><img src="https://habrastorage.org/webt/_k/__/vy/_k__vyhouqm_ctxpsp8ukcselwe.png"></div><br>  Nous marquons toutes les inflexions avec le symbole ¬´+¬ª au d√©but pour les distinguer des autres jetons.  Nous consid√©rons chaque sujet et inflexion comme un mot distinct et apprenons d'eux de la m√™me mani√®re que dans les mots.  Autrement dit, nous obtenons d'eux des plongements et en d√©rivons une s√©quence (√©galement d√©compos√©e en fondements et terminaisons) qui peut √™tre facilement transform√©e en mots. <br><br>  Une telle conversion est tr√®s utile lorsque vous travaillez avec des langues morphologiquement riches comme le russe.  Au lieu de compiler d'√©normes dictionnaires avec une grande vari√©t√© de formes de mots russes, vous pouvez vous limiter √† un grand nombre de tiges de ces mots (elles sont plusieurs fois plus petites que le nombre de formes de mots) et √† un tr√®s petit ensemble de terminaisons (j'ai eu beaucoup de 450 inflexions).  Ainsi, nous permettons au mod√®le de travailler plus facilement avec cette ¬´richesse¬ª et en m√™me temps nous n'augmentons pas la complexit√© de l'architecture et le nombre de param√®tres. <br><br><img src="https://habrastorage.org/webt/a4/es/s5/a4ess5qr3vaxprksv7avun3obl8.png" alt="image"><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/3d/rk/6h/3drk6hntzcow2i3soi-heul7ckq.png"></div><br>  J'ai √©galement essay√© d'utiliser la transformation lemme + gramme.  Autrement dit, √† partir de chaque mot avant le traitement, vous pouvez obtenir sa forme initiale et sa signification grammaticale en utilisant le package pymorphy (par exemple, "was" <img src="https://habrastorage.org/webt/95/wb/-a/95wb-aopfalycfvrshqfynxnwd4.png" alt="image">  "√ätre" et "VERBE | impf | pass√© | chanter | femn").  Ainsi, j'ai obtenu une paire de s√©quences parall√®les (dans l'une - les formes initiales, dans l'autre - les valeurs grammaticales).  Pour chaque type de s√©quence, j'ai compil√© mes plongements, que j'ai ensuite concat√©n√©s et soumis au pipeline d√©crit pr√©c√©demment.  Dans ce document, le d√©codeur n'a pas appris √† donner un mot, mais un lemme et des grammes.  Mais un tel syst√®me n'a pas apport√© d'am√©liorations visibles par rapport √† pg sur le sujet.  C'√©tait peut-√™tre une architecture trop simple pour travailler avec des valeurs grammaticales, et cela valait la peine de cr√©er un classificateur distinct pour chaque cat√©gorie grammaticale dans la sortie.  Mais je n'ai pas exp√©riment√© de mod√®les tels ou plus complexes. <br><br>  J'ai exp√©riment√© un autre ajout √† l'architecture originale du g√©n√©rateur de pointeur, qui, cependant, ne s'applique pas au pr√©traitement.  Il s'agit d'une augmentation du nombre de couches (jusqu'√† 3) des r√©seaux r√©cursifs du codeur et du d√©codeur.  L'augmentation de la profondeur du r√©seau r√©current peut am√©liorer la qualit√© de la sortie, car l'√©tat cach√© des derni√®res couches peut contenir des informations sur une sous-s√©quence d'entr√©e beaucoup plus longue que l'√©tat cach√© d'un RNN monocouche.  Cela permet de prendre en compte les connexions s√©mantiques √©tendues complexes entre les √©l√©ments de la s√©quence d'entr√©e.  Certes, cela co√ªte une augmentation significative du nombre de param√®tres du mod√®le et complique l'apprentissage. <br><br><img src="https://habrastorage.org/webt/cf/7g/ej/cf7gejf-hxd5pbscvqgnvuuviqy.png" alt="image"><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/4g/ww/zy/4gwwzytsrt506_xllgy5xcis6vk.png"></div><br><h2>  Exp√©riences de g√©n√©rateur d'en-t√™te </h2><br>  Toutes mes exp√©riences sur les g√©n√©rateurs de titres peuvent √™tre divis√©es en deux types: les exp√©riences avec des articles de presse et des versets.  Je vais vous en parler dans l'ordre. <br><br><h3>  Exp√©riences </h3><br>  Lorsque je travaillais avec des actualit√©s, j'utilisais des mod√®les tels que seq2seq, pg, pg avec des tiges et des inflexions - monocouche et trois couches.  J'ai √©galement consid√©r√© les mod√®les qui fonctionnent avec des grammes, mais tout ce que je voulais en dire, je l'ai d√©j√† d√©crit ci-dessus.  Je dois dire tout de suite que tous les pg d√©crits dans cette section utilisaient le m√©canisme de rev√™tement, bien que son influence sur le r√©sultat soit douteuse (car sans elle ce n'√©tait pas bien pire). <br><br>  Je me suis entra√Æn√© sur l'ensemble de donn√©es RIA Novosti, qui a √©t√© fourni par l'agence de presse Rossiya Segodnya pour mener une piste de g√©n√©ration de titres lors de la conf√©rence Dialog.  L'ensemble de donn√©es contient 1 003 869 articles de presse publi√©s de janvier 2010 √† d√©cembre 2014. <br><br>  Tous les mod√®les √©tudi√©s ont utilis√© les m√™mes plongements (128), vocabulaire (100k) et √©tats latents (256) et form√©s pour le m√™me nombre d'√©poques.  Par cons√©quent, seuls des changements qualitatifs dans l'architecture ou dans le pr√©traitement pourraient affecter le r√©sultat. <br><br>  Les mod√®les adapt√©s pour travailler avec du texte pr√©trait√© donnent de meilleurs r√©sultats que les mod√®les qui fonctionnent avec des mots.  Un pg √† trois couches qui utilise des informations sur les sujets et les inflexions fonctionne mieux.  Lorsque vous utilisez un pg, l'am√©lioration attendue de la qualit√© des en-t√™tes par rapport √† seq2seq appara√Æt √©galement, ce qui indique l'utilisation pr√©f√©r√©e du pointeur lors de la g√©n√©ration des en-t√™tes.  Voici un exemple de fonctionnement de tous les mod√®les: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6w/4u/0r/6w4u0ruudngxtt2szek31w0achq.png"></div><br>  En regardant les en-t√™tes g√©n√©r√©s, nous pouvons distinguer les probl√®mes suivants des mod√®les √† l'√©tude: <br><br><ol><li>  Les mod√®les utilisent souvent des formes de mots irr√©guli√®res.  Les mod√®les √† tiges (comme dans l'exemple ci-dessus) sont plus soulag√©s de cet inconv√©nient; </li><li>  Tous les mod√®les, √† l'exception de ceux qui fonctionnent avec des th√®mes, peuvent produire des en-t√™tes qui semblent incomplets ou des conceptions √©tranges qui ne sont pas dans le langage (comme dans l'exemple ci-dessus); </li><li>  Tous les mod√®les √©tudi√©s confondent souvent les personnes d√©crites, substituent des dates incorrectes ou utilisent des mots pas tout √† fait appropri√©s. </li></ol><br><img src="https://habrastorage.org/webt/9c/-b/ot/9c-bote3bwomvfu_3bwaixhqnks.png" alt="image"><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/e-/3c/px/e-3cpxuknykthyu-xvf-cp-rdsi.png"></div><br><img src="https://habrastorage.org/webt/lf/ry/ep/lfryepksl4kmcqxttb14ikgx4wg.png" alt="image"><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/b3/4m/fw/b34mfwpsffud2kaa93tccjvyja0.png"></div><br><h3>  Exp√©riences avec des versets </h3><br>  √âtant donn√© que la pg √† trois couches avec les th√®mes pr√©sente le moins d'inexactitudes dans les en-t√™tes g√©n√©r√©s, c'est le mod√®le que j'ai choisi pour les exp√©riences avec les versets.  Je lui ai enseign√© le cas, compos√© de 6 millions de po√®mes russes du site "stihi.ru".  Ils comprennent l'amour (environ la moiti√© des versets sont consacr√©s √† ce sujet), civique (environ un quart), la po√©sie urbaine et paysag√®re.  P√©riode d'√©criture: janvier 2014 - mai 2019. Je vais donner des exemples de rubriques g√©n√©r√©es pour les vers: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yv/mo/da/yvmodarkck7cgra-ymxigffwhik.png"></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/3u/mb/fn/3umbfnpzo1hwhqy0_glxyhxynu4.png"></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tp/tk/5e/tptk5eoq9wa1xrnj9w0gewjeayk.png"></div><br><br>  Le mod√®le s'est av√©r√© √™tre principalement extrait: presque tous les en-t√™tes sont une seule ligne, souvent extraite de la premi√®re ou de la derni√®re strophe.  Dans des cas exceptionnels, le mod√®le peut g√©n√©rer des mots qui ne sont pas dans le po√®me.  Cela est d√ª au fait qu'un tr√®s grand nombre de textes dans le cas ont vraiment une des lignes comme nom. <br><br>  En conclusion, je dirai que le g√©n√©rateur d'index, qui fonctionne sur les tiges et utilise un d√©codeur et un encodeur monocouche, a pris la deuxi√®me place sur la <a href="https://vk.com/wall-177402111_31">piste</a> de la <a href="https://vk.com/wall-177402111_31">comp√©tition</a> pour g√©n√©rer des titres pour des articles d'actualit√© √† la conf√©rence scientifique Dialogue sur la linguistique informatique.  L'organisateur principal de cette conf√©rence est ABBYY, la soci√©t√© est engag√©e dans la recherche dans presque tous les domaines modernes du traitement automatique du langage naturel. <br><br>  Enfin, je vous propose un peu interactif: envoyez des nouvelles dans les commentaires, et voyez quels en-t√™tes le r√©seau neuronal va g√©n√©rer pour eux. <br><br>  <i>Matvey, d√©veloppeur chez NLP Group chez ABBYY</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr479400/">https://habr.com/ru/post/fr479400/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr479384/index.html">Que fait le Big Data dans MegaFon et comment y arriver?</a></li>
<li><a href="../fr479388/index.html">Caract√©ristiques de la construction de centres de donn√©es nationaux, Mikhalych</a></li>
<li><a href="../fr479392/index.html">Pinebook Pro: plus de Chromebook</a></li>
<li><a href="../fr479394/index.html">Comment j'ai cherch√© un helpdesk parmi 15 solutions et ... n'a pas trouv√©</a></li>
<li><a href="../fr479398/index.html">Nous apportons l'√©quation de r√©gression lin√©aire sous forme de matrice</a></li>
<li><a href="../fr479402/index.html">Comment payer officiellement des services d'ind√©pendant √† l'√©tranger, payer 0% d'imp√¥ts et ne pas alimenter les syst√®mes de paiement</a></li>
<li><a href="../fr479404/index.html">Personnel pour le P√®re No√´l</a></li>
<li><a href="../fr479406/index.html">16 conseils de d√©veloppement pour Android dans Kotlin. Partie 1</a></li>
<li><a href="../fr479414/index.html">Fa√ßons de trouver le but. Le r√¥le du hasard</a></li>
<li><a href="../fr479416/index.html">Voyez o√π vous allez (vision p√©riph√©rique vs charge cognitive)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>