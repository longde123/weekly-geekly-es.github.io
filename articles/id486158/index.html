<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>😅 🍦 👨🏽‍🤝‍👨🏼 Memvisualisasikan terjemahan mesin saraf (model seq2seq dengan mekanisme perhatian) 🧑🏻 ✍🏼 💆</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Halo, Habr! Saya hadir untuk Anda terjemahan artikel "Visualisasi Model Terjemahan Mesin Neural (Mekanik Model Seq2seq Dengan Penuh Perhatian)" oleh J...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Memvisualisasikan terjemahan mesin saraf (model seq2seq dengan mekanisme perhatian)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/486158/"><p>  Halo, Habr!  Saya hadir untuk Anda terjemahan artikel <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="nofollow">"Visualisasi Model Terjemahan Mesin Neural (Mekanik Model Seq2seq Dengan Penuh Perhatian)"</a> oleh Jay Alammar. </p><br><p>  Model urutan-ke-urutan (seq2seq) adalah model pembelajaran mendalam yang telah mencapai sukses besar dalam tugas-tugas seperti terjemahan mesin, peringkasan teks, anotasi gambar, dll. Misalnya, pada akhir 2016, model serupa <a href="https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/" rel="nofollow">dibangun</a> ke dalam Google Translate.  Dasar-dasar model seq2seq diletakkan kembali pada tahun 2014 dengan merilis dua artikel - <a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="nofollow">Sutskever et al., 2014</a> , <a href="http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf" rel="nofollow">Cho et al., 2014</a> . </p><br><p> Agar cukup memahami dan kemudian menggunakan model ini, beberapa konsep harus diklarifikasi terlebih dahulu.  Visualisasi yang diusulkan dalam artikel ini akan menjadi pelengkap yang baik untuk artikel yang disebutkan di atas. </p><br><p>  Model urutan-ke-urutan adalah model yang menerima urutan input elemen (kata, huruf, atribut gambar, dll.) Dan mengembalikan urutan elemen lainnya.  Model yang terlatih bekerja sebagai berikut: </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="https://jalammar.github.io/images/seq2seq_1.mp4" type="video/mp4"></video></div></div></div><a name="habracut"></a><br><p>  Dalam terjemahan mesin saraf, urutan elemen adalah kumpulan kata yang diproses secara bergantian.  Kesimpulannya juga serangkaian kata-kata: </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="https://jalammar.github.io/images/seq2seq_2.mp4" type="video/mp4"></video></div></div></div><br><h1 id="zaglyanem-pod-kapot">  Lihatlah di bawah tenda </h1><br><p>  Di bawah tenda, model memiliki encoder dan decoder. </p><br><p>  Encoder memproses setiap elemen dari urutan input, menerjemahkan informasi yang diterima ke dalam vektor yang disebut konteks.  Setelah memproses seluruh urutan input, encoder mengirimkan konteks ke decoder, yang kemudian mulai menghasilkan elemen urutan output dengan elemen. </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="https://jalammar.github.io/images/seq2seq_3.mp4" type="video/mp4"></video></div></div></div><br><p>  Hal yang sama terjadi dengan terjemahan mesin. </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="https://jalammar.github.io/images/seq2seq_4.mp4" type="video/mp4"></video></div></div></div><br><p>  Untuk terjemahan mesin, konteksnya adalah vektor (array angka), dan encoder dan decoder, pada gilirannya, paling sering adalah jaringan saraf berulang (lihat pengantar untuk RNN ​​- <a href="https://www.youtube.com/watch%3Fv%3DUNmqTiOnRfg" rel="nofollow">Pengantar ramah untuk Jaringan Syaraf Berulang</a> ). </p><br><p><img src="https://habrastorage.org/webt/yr/ir/92/yrir92d8paf_xdm29bovaw109gu.png" alt="konteks"></p><br><p>  <em>Konteks adalah vektor angka floating point.</em>  <em>Lebih lanjut dalam artikel ini, vektor akan divisualisasikan dalam warna sehingga warna yang lebih terang sesuai dengan sel dengan nilai yang besar.</em> </p><br><p>  Saat melatih model, Anda dapat mengatur ukuran vektor konteks - jumlah neuron tersembunyi (unit tersembunyi) dalam encoder RNN.  Data visualisasi menunjukkan vektor 4-dimensi, tetapi dalam aplikasi nyata, vektor konteks akan memiliki dimensi urutan 256, 512, atau 1024. </p><br><p>  Secara default, pada setiap interval waktu, RNN menerima dua elemen untuk input: elemen input itu sendiri (dalam kasus encoder, satu kata dari kalimat asli) dan keadaan tersembunyi.  Namun, kata tersebut harus diwakili oleh vektor.  Untuk mengubah kata menjadi vektor, mereka menggunakan serangkaian algoritma yang disebut perkawinan kata.  Embeddings menerjemahkan kata ke dalam ruang vektor yang berisi informasi semantik dan semantik tentangnya (misalnya, <a href="http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html" rel="nofollow">"raja" - "pria" + "wanita" = "ratu"</a> ). </p><br><p><img src="https://habrastorage.org/webt/87/pp/s9/87pps99ndmgvp6gqxskkodf4zl4.png" alt="menanamkan"></p><br><p>  <em>Sebelum memproses kata, Anda harus mengonversinya menjadi vektor.</em>  <em>Transformasi ini dilakukan dengan menggunakan algoritma embedding kata.</em>  <em>Anda dapat menggunakan embeddings pra-terlatih dan embeddings pada set data Anda.</em>  <em>200-300 - dimensi khas dari vektor embedding;</em>  <em>artikel ini menggunakan dimensi 4 untuk kesederhanaan.</em> </p><br><p>  Sekarang setelah kami berkenalan dengan vektor / tensor utama kami, mari kita mengingat kembali mekanisme RNN dan membuat visualisasi untuk menggambarkannya: </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="https://jalammar.github.io/images/RNN_1.mp4" type="video/mp4"></video></div></div></div><br><p>  Pada langkah berikutnya, RNN mengambil vektor input kedua dan keadaan laten # 1 untuk membentuk output pada interval waktu ini.  Kemudian dalam artikel tersebut, animasi serupa digunakan untuk menggambarkan vektor di dalam model terjemahan mesin saraf. </p><br><p>  Dalam visualisasi berikut, setiap frame menjelaskan pemrosesan input oleh encoder dan generasi output oleh decoder dalam satu interval waktu.  Karena baik encoder dan decoder adalah RNN, pada setiap interval waktu, jaringan saraf sedang sibuk memproses dan memperbarui status tersembunyi berdasarkan arus dan semua input sebelumnya.  Dalam kasus ini, status tersembunyi terakhir pembuat enkode adalah konteks yang dikirimkan ke dekoder. </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="https://jalammar.github.io/images/seq2seq_5.mp4" type="video/mp4"></video></div></div></div><br><p>  Dekoder juga berisi status tersembunyi yang ditransfer dari satu slot waktu ke slot waktu lainnya.  (Ini tidak ada dalam visualisasi, hanya menggambarkan bagian utama dari model.) </p><br><p>  Kami sekarang beralih ke jenis visualisasi model urutan-ke-urutan lainnya.  Animasi ini akan membantu untuk memahami grafik statis yang menggambarkan model-model ini - yang disebut  tampilan tanpa gulungan, di mana alih-alih menunjukkan satu decoder, kami menunjukkan salinannya untuk setiap interval waktu.  Jadi kita bisa melihat elemen input dan output pada setiap interval waktu. </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="https://jalammar.github.io/images/seq2seq_6.mp4" type="video/mp4"></video></div></div></div><br><h1 id="obratite-vnimanie">  Perhatikan! </h1><br><p>  Vektor konteks adalah hambatan untuk model jenis ini, sehingga sulit bagi mereka untuk berurusan dengan kalimat yang panjang.  Solusi ini diusulkan dalam artikel oleh <a href="" rel="nofollow">Bahdanau et al., 2014</a> dan <a href="https://arxiv.org/abs/1508.04025" rel="nofollow">Luong et al., 2015</a> , yang menyajikan teknik yang disebut mekanisme perhatian.  Mekanisme ini secara signifikan meningkatkan kualitas sistem terjemahan mesin, memungkinkan model untuk berkonsentrasi pada bagian yang relevan dari urutan input. </p><br><p><img src="https://habrastorage.org/webt/b1/ru/kj/b1rukj6w-dgtyfncd3a62635zb0.png" alt="perhatian"></p><br><p>  <em>Dalam rentang waktu ke-7, mekanisme perhatian memungkinkan dekoder untuk fokus pada kata étudiant (siswa dalam bahasa Prancis) sebelum menghasilkan terjemahan ke dalam bahasa Inggris.</em>  <em>Kemampuan ini untuk memperkuat sinyal dari bagian yang relevan dari urutan input memungkinkan model berdasarkan mekanisme perhatian untuk mendapatkan hasil yang lebih baik dibandingkan dengan model lain.</em> </p><br><p>  Ketika mempertimbangkan model dengan mekanisme perhatian pada abstraksi tingkat tinggi, dua perbedaan utama dari model urutan-ke-urutan yang klasik dapat dibedakan. </p><br><p>  Pertama, encoder mentransfer lebih banyak data secara signifikan ke decoder: alih-alih hanya mentransmisikan status tersembunyi terakhir setelah tahap penyandian, encoder mengirimkan semua status tersembunyi ke dalamnya: </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="https://jalammar.github.io/images/seq2seq_7.mp4" type="video/mp4"></video></div></div></div><br><p>  Kedua, decoder melewati langkah tambahan sebelum menghasilkan output.  Untuk fokus pada bagian-bagian dari urutan input yang relevan untuk rentang waktu yang sesuai, decoder melakukan yang berikut: </p><br><ol><li>  Melihat seperangkat status laten yang diterima dari pembuat enkode - masing-masing status laten berkorelasi paling baik dengan salah satu kata dalam urutan input; </li><li>  Tetapkan penilaian tertentu untuk setiap keadaan laten (jangan abaikan sekarang bagaimana prosedur estimasi terjadi); </li><li>  Mengalikan setiap kondisi tersembunyi dengan fungsi evaluasi yang dikonversi softmax, sehingga menyoroti status tersembunyi dengan peringkat besar dan menurunkan status tersembunyi dengan yang kecil ke latar belakang. </li></ol><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="https://jalammar.github.io/images/attention_process.mp4" type="video/mp4"></video></div></div></div><br><p>  “Latihan penilaian” ini dilakukan pada dekoder pada setiap interval waktu. </p><br><p>  Jadi, meringkas semua hal di atas, kami mempertimbangkan proses model dengan mekanisme perhatian: </p><br><ol><li>  Pada dekoder, RNN menerima penyematan &lt;END&gt; token dan status tersembunyi awal. </li><li>  RNN memproses elemen input, menghasilkan output, dan vektor keadaan tersembunyi baru (h4).  Outputnya dibuang. </li><li>  Mekanisme perhatian menggunakan status tersembunyi pembuat enkode dan vektor h4 untuk menghitung vektor konteks (C4) pada interval waktu tertentu. </li><li>  Vektor h4 dan C4 digabungkan menjadi satu vektor. </li><li>  Vektor ini dilewatkan melalui feedforward neural network (FFN), dilatih bersama dengan model. </li><li>  Output dari jaringan FFN menunjukkan kata output pada interval waktu tertentu. </li><li>  Algoritma diulang untuk interval waktu berikutnya. </li></ol><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="https://jalammar.github.io/images/attention_tensor_dance.mp4" type="video/mp4"></video></div></div></div><br><p>  Cara lain untuk melihat bagian kalimat mana yang menjadi fokus model di setiap tahap dekoder: </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="https://jalammar.github.io/images/seq2seq_9.mp4" type="video/mp4"></video></div></div></div><br><p>  Perhatikan bahwa model tidak hanya menghubungkan kata pertama pada input dengan kata pertama pada output.  Dia benar-benar mengerti selama proses pelatihan bagaimana mencocokkan kata-kata dalam pasangan bahasa yang dianggap ini (dalam kasus kami, Prancis dan Inggris).  Contoh seberapa akurat mekanisme ini dapat bekerja dapat ditemukan di artikel tentang mekanisme perhatian yang disebutkan di atas. </p><br><p><img src="https://habrastorage.org/webt/cl/dv/9f/cldv9f7zdegsobuszn10hyy1nde.png" alt="attention_sentence"></p><br><p>  Jika Anda merasa siap mempelajari cara menerapkan model ini, lihat manual <a href="https://github.com/tensorflow/nmt" rel="nofollow">Terjemahan Mesin Saraf (seq2seq)</a> di TensorFlow. </p><br><h1 id="avtory">  Penulis </h1><br><ul><li>  <strong>Asli</strong> oleh <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="nofollow">Jay Alammar</a> </li><li>  <strong>Terjemahan</strong> - <a href="https://habr.com/ru/users/smekur/">Ekaterina Smirnova</a> </li><li>  <strong>Pengeditan dan tata letak</strong> - <a href="https://habr.com/ru/users/kouki_rus/">Shkarin Sergey</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id486158/">https://habr.com/ru/post/id486158/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../In144568/index.html">डेटा वेयरहाउस मेट्रिक्स</a></li>
<li><a href="../In144569/index.html">ड्रैगन स्पेसएक्स सफलतापूर्वक आईएसएस तक पहुंच गया</a></li>
<li><a href="../In144571/index.html">एक उत्तल बहुभुज में एक बिंदु का स्थानीयकरण</a></li>
<li><a href="../id486124/index.html">Impala vs Hive vs Spark SQL: Memilih mesin SQL yang tepat untuk bekerja dengan baik di Gudang Data Cloudera</a></li>
<li><a href="../id486128/index.html">Uji Solusi Arsitek: siapa itu dan kapan dibutuhkan</a></li>
<li><a href="../id486144/index.html">Mengapa altcoin mati dan apa yang dapat terjadi dengan cryptocurrency dalam waktu dekat?</a></li>
<li><a href="../id486150/index.html">Pengembangan IT-sphere di Slovakia. Manfaat kerja untuk profesional muda</a></li>
<li><a href="../id486156/index.html">Saat saya mengajar, dan kemudian menulis manual pelatihan dengan Python</a></li>
<li><a href="../id486164/index.html">Coronavirus 2019-nCoV. FAQ tentang Perlindungan Pernafasan dan Disinfeksi</a></li>
<li><a href="../id486174/index.html">Saya memiliki nol turnover</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>