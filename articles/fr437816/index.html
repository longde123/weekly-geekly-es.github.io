<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üê™ üëø ü§æüèº MPLS est partout. Comment est l'infrastructure r√©seau Yandex.Cloud üöñ üë©‚Äçüëß‚Äçüë¶ üç´</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Article pr√©par√© par: Alexander Virilin xscrew - auteur, chef du service d'infrastructure r√©seau, Leonid Klyuyev - √©diteur 

 Nous continuons de vous f...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>MPLS est partout. Comment est l'infrastructure r√©seau Yandex.Cloud</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/437816/"> <sup><i>Article pr√©par√© par: Alexander Virilin <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">xscrew</a> - auteur, chef du service d'infrastructure r√©seau, Leonid Klyuyev - √©diteur</i></sup> <br><br><img src="https://habrastorage.org/webt/td/uq/e-/tduqe-fvjvebbot1h11mdm0ri9g.png" align="right" width="400">  Nous continuons de vous familiariser avec la structure interne de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Yandex.Cloud</a> .  Aujourd'hui, nous parlerons des r√©seaux - nous vous expliquerons comment fonctionne l'infrastructure de r√©seau, pourquoi elle utilise le paradigme MPLS impopulaire pour les centres de donn√©es, quelles autres d√©cisions complexes nous avons d√ª prendre dans le processus de construction d'un r√©seau cloud, comment nous le g√©rons et quel type de surveillance nous utilisons. <br><br>  Le r√©seau dans le cloud se compose de trois couches.  La couche inf√©rieure est l'infrastructure d√©j√† mentionn√©e.  Il s'agit d'un r√©seau physique ¬´de fer¬ª √† l'int√©rieur des centres de donn√©es, entre les centres de donn√©es et dans les lieux de connexion aux r√©seaux externes.  Un r√©seau virtuel est construit au-dessus de l'infrastructure r√©seau et les services r√©seau sont construits au-dessus du r√©seau virtuel.  Cette structure n'est pas monolithique: les couches se croisent, le r√©seau virtuel et les services r√©seau interagissent directement avec l'infrastructure r√©seau.  √âtant donn√© que le r√©seau virtuel est souvent appel√© superposition, nous appelons g√©n√©ralement la sous-infrastructure de r√©seau. <br><a name="habracut"></a><br><img src="https://habrastorage.org/webt/lr/c_/kr/lrc_krqlbldrs_spninjomwdekm.png"><br><br>  D√©sormais, l'infrastructure Cloud est bas√©e dans la r√©gion centrale de la Russie et comprend trois zones d'acc√®s, c'est-√†-dire trois centres de donn√©es ind√©pendants r√©partis g√©ographiquement.  Ind√©pendant - ind√©pendant les uns des autres dans le contexte des r√©seaux, de l'ing√©nierie et des syst√®mes √©lectriques, etc. <br><br>  √Ä propos des caract√©ristiques.  La g√©ographie de l'emplacement des centres de donn√©es est telle que le temps d'aller-retour (RTT) du temps d'aller-retour entre eux est toujours de 6 √† 7 ms.  La capacit√© totale des canaux a d√©j√† d√©pass√© 10 t√©rabits et ne cesse de cro√Ætre, car Yandex dispose de son propre r√©seau de fibre optique entre les zones.  Comme nous ne louons pas de canaux de communication, nous pouvons rapidement augmenter la capacit√© de la bande entre les DC: chacun d'eux utilise un √©quipement de multiplexage spectral. <br><br>  Voici la repr√©sentation la plus sch√©matique des zones: <br><br><img src="https://habrastorage.org/webt/iw/tz/11/iwtz11yihyj7beyxar1pefzif-4.png"><br><br>  La r√©alit√©, quant √† elle, est l√©g√®rement diff√©rente: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ai/bf/fp/aibffptzbz0jtemoiczeiledcdm.png" width="500"></div><br>  Voici le r√©seau de base actuel de Yandex dans la r√©gion.  Tous les services Yandex fonctionnent en plus, une partie du r√©seau est utilis√©e par le Cloud.  (Il s'agit d'une image √† usage interne, par cons√©quent, les informations de service sont d√©lib√©r√©ment cach√©es. N√©anmoins, il est possible d'estimer le nombre de n≈ìuds et de connexions.) La d√©cision d'utiliser le r√©seau de base √©tait logique: nous ne pouvions rien inventer, mais r√©utiliser l'infrastructure actuelle - ¬´souffert¬ª au cours des ann√©es de d√©veloppement. <br><br>  Quelle est la diff√©rence entre la premi√®re photo et la seconde?  Tout d'abord, les zones d'acc√®s ne sont pas directement li√©es: des sites techniques sont situ√©s entre elles.  Les sites ne contiennent pas d'√©quipement serveur - seuls des p√©riph√©riques r√©seau pour assurer la connectivit√© y sont plac√©s.  Les points de pr√©sence o√π Yandex et Cloud se connectent au monde ext√©rieur sont connect√©s √† des sites techniques.  Tous les points de pr√©sence fonctionnent pour toute la r√©gion.  Soit dit en passant, il est important de noter que du point de vue de l'acc√®s externe √† partir d'Internet, toutes les zones d'acc√®s au Cloud sont √©quivalentes.  En d'autres termes, ils offrent la m√™me connectivit√©, c'est-√†-dire la m√™me vitesse et le m√™me d√©bit, ainsi que des latences tout aussi faibles. <br><br>  En outre, il existe des √©quipements sur les points de pr√©sence, auxquels - s'il existe des ressources sur site et une volont√© d'√©tendre l'infrastructure locale avec des installations cloud - les clients peuvent se connecter via un canal garanti.  Cela peut √™tre fait avec l'aide de partenaires ou par vous-m√™me. <br><br>  Le r√©seau central est utilis√© par le Cloud comme transport MPLS. <br><br><h2>  MPLS </h2><br><img src="https://habrastorage.org/webt/ul/kj/rv/ulkjrvkt7kk2igkl_sbjzivjfxk.png"><br><br>  La commutation d'√©tiquettes multiprotocole est une technologie largement utilis√©e dans notre industrie.  Par exemple, lorsqu'un paquet est transf√©r√© entre des zones d'acc√®s ou entre une zone d'acc√®s et Internet, les √©quipements de transport ne pr√™tent attention qu'√† l'√©tiquette sup√©rieure, sans ¬´penser¬ª √† ce qui se trouve en dessous.  De cette fa√ßon, MPLS vous permet de masquer la complexit√© du cloud √† la couche de transport.  En g√©n√©ral, nous, dans le Cloud, aimons beaucoup MPLS.  Nous l'avons m√™me int√©gr√© au niveau inf√©rieur et l'avons utilis√© directement dans l'usine de commutation du centre de donn√©es: <br><br><img src="https://habrastorage.org/webt/k-/iy/hg/k-iyhg3ru8bkto7rqrawug0ivra.png"><br><br>  (En fait, il existe de nombreux liens parall√®les entre les commutateurs Leaf et les √©pines.) <br><br><h4>  Pourquoi MPLS? </h4><br>  Certes, MPLS n'est pas souvent trouv√© dans les r√©seaux de centres de donn√©es.  Souvent, des technologies compl√®tement diff√©rentes sont utilis√©es. <br><br>  Nous utilisons MPLS pour plusieurs raisons.  Tout d'abord, nous avons trouv√© pratique d'unifier les technologies du plan de contr√¥le et du plan de donn√©es.  Autrement dit, au lieu de certains protocoles dans le r√©seau du centre de donn√©es, d'autres protocoles dans le r√©seau central et la jonction de ces protocoles - un seul MPLS.  Ainsi, nous avons unifi√© la pile technologique et r√©duit la complexit√© du r√©seau. <br><br>  Deuxi√®mement, dans le Cloud, nous utilisons diverses appliances r√©seau, telles que Cloud Gateway et Network Load Balancer.  Ils doivent communiquer entre eux, envoyer du trafic vers Internet et vice versa.  Ces appliances r√©seau peuvent √™tre mises √† l'√©chelle horizontalement avec une charge croissante, et puisque le Cloud est construit selon le mod√®le d'hyperconvergence, elles peuvent √™tre lanc√©es √† n'importe quel endroit du point de vue du r√©seau dans le centre de donn√©es, c'est-√†-dire dans un pool de ressources commun. <br><br>  Ainsi, ces appliances peuvent d√©marrer derri√®re n'importe quel port du commutateur rack o√π se trouve le serveur et commencer √† communiquer via MPLS avec le reste de l'infrastructure.  Le seul probl√®me dans la construction d'une telle architecture √©tait l'alarme. <br><br><h2>  Alarme </h2><br>  La pile de protocoles MPLS classique est assez complexe.  C'est d'ailleurs l'une des raisons de la non-prolif√©ration du MPLS dans les r√©seaux de centres de donn√©es. <br><br>  √Ä notre tour, nous n'avons utilis√© ni IGP (Interior Gateway Protocol), ni LDP (Label Distribution Protocol), ni d'autres protocoles de distribution d'√©tiquettes.  Seul BGP (Border Gateway Protocol) Label-Unicast est utilis√©.  Chaque appliance, qui s'ex√©cute, par exemple, comme une machine virtuelle, cr√©e une session BGP avant le commutateur Leaf mont√© en rack. <br><br><img src="https://habrastorage.org/webt/z7/-o/03/z7-o03kr2x9-v-yuawoom5xegq4.png"><br><br>  Une session BGP est construite √† une adresse pr√©-connue.  Il n'est pas n√©cessaire de configurer automatiquement le commutateur pour ex√©cuter chaque appliance.  Tous les commutateurs sont pr√©configur√©s et coh√©rents. <br><br>  Au sein d'une session BGP, chaque appliance envoie son propre bouclage et re√ßoit des bouclages du reste des appareils avec lesquels elle devra √©changer du trafic.  Des exemples de tels dispositifs sont plusieurs types de r√©flecteurs de route, de routeurs de bordure et d'autres appareils.  Par cons√©quent, des informations sur la fa√ßon de se rejoindre s'affichent sur les appareils.  √Ä partir de la passerelle Cloud via le commutateur Leaf, le commutateur Spine et le r√©seau jusqu'au routeur fronti√®re, un chemin de commutateur d'√©tiquette est cr√©√©.  Les commutateurs sont des commutateurs L3 qui se comportent comme un routeur de commutation d'√©tiquettes et ne connaissent pas la complexit√© qui les entoure. <br><br>  MPLS √† tous les niveaux de notre r√©seau, entre autres, nous a permis d'utiliser le concept de manger votre propre nourriture pour chien. <br><br><h2>  Mangez votre propre nourriture pour chien </h2><br>  D'un point de vue r√©seau, ce concept implique que nous vivons dans la m√™me infrastructure que nous fournissons √† l'utilisateur.  Voici des sch√©mas de racks dans les zones d'accessibilit√©: <br><br><img src="https://habrastorage.org/webt/tz/zq/o0/tzzqo08b1pv-whl9mqu9e_xnups.png"><br><br>  L'h√¥te cloud prend la charge de l'utilisateur, contient ses machines virtuelles.  Et litt√©ralement, un h√¥te voisin dans un rack peut supporter la charge d'infrastructure du point de vue du r√©seau, y compris les r√©flecteurs d'itin√©raire, les serveurs de gestion, de surveillance, etc. <br><br>  Pourquoi cela at-il √©t√© fait?  Il y avait une tentation d'ex√©cuter des r√©flecteurs de route et tous les √©l√©ments d'infrastructure dans un segment distinct tol√©rant aux pannes.  Ensuite, si le segment d'utilisateurs √©tait tomb√© en panne quelque part dans le centre de donn√©es, les serveurs d'infrastructure continueraient √† g√©rer l'ensemble de l'infrastructure r√©seau.  Mais cette approche nous a sembl√© vicieuse - si nous ne faisons pas confiance √† notre propre infrastructure, comment pouvons-nous la fournir √† nos clients?  Apr√®s tout, absolument tout le Cloud, tous les r√©seaux virtuels, les services utilisateur et cloud fonctionnent en plus. <br><br>  Par cons√©quent, nous avons abandonn√© un segment distinct.  Nos √©l√©ments d'infrastructure fonctionnent dans la m√™me topologie r√©seau et la m√™me connectivit√© r√©seau.  Naturellement, ils fonctionnent en triple instance - tout comme nos clients lancent leurs services dans le Cloud. <br><br><h2>  Usine IP / MPLS </h2><br>  Voici un exemple de diagramme de l'une des zones de disponibilit√©: <br><br><img src="https://habrastorage.org/webt/jn/xi/rw/jnxirwsmzj62bwzvldfnrq3n2_4.png"><br><br>  Dans chaque zone de disponibilit√©, il y a environ cinq modules et dans chaque module une centaine de racks.  Commutateurs mont√©s sur ch√¢ssis, ils sont connect√©s au sein de leur module par le niveau Spine, et la connectivit√© inter-modules est assur√©e via l'interconnexion r√©seau.  Il s'agit du niveau suivant, qui comprend les commutateurs dits Super-Spines et Edge, qui connectent d√©j√† les zones d'acc√®s.  Nous avons d√©lib√©r√©ment abandonn√© la L2, nous ne parlons que de la connectivit√© IP / MPLS L3.  BGP est utilis√© pour distribuer les informations de routage. <br><br>  En fait, il y a beaucoup plus de connexions parall√®les que dans l'image.  Un si grand nombre de connexions ECMP (multi-trajets √† co√ªt √©gal) impose des exigences de surveillance particuli√®res.  De plus, il existe, √† premi√®re vue, des limites inattendues dans l'√©quipement - par exemple, le nombre de groupes ECMP. <br><br><h2>  Connexion au serveur </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xf/ay/jo/xfayjofymnoqjc-1u1otd0axhgm.png"></div><br>  En raison d'investissements puissants, Yandex construit des services de telle mani√®re qu'une d√©faillance d'un serveur, d'un rack de serveur, d'un module ou m√™me d'un centre de donn√©es complet n'entra√Æne jamais un arr√™t complet du service.  Si nous avons des probl√®mes de r√©seau - supposons qu'un commutateur de montage en rack soit cass√© - les utilisateurs externes ne voient jamais cela. <br><br>  Yandex.Cloud est un cas particulier.  Nous ne pouvons pas dicter au client comment construire ses propres services, et nous avons d√©cid√© de niveler ce point de d√©faillance unique possible.  Par cons√©quent, tous les serveurs du cloud sont connect√©s √† deux commutateurs mont√©s en rack. <br><br>  Nous n'utilisons pas non plus de protocoles de redondance au niveau L2, mais avons imm√©diatement commenc√© √† utiliser uniquement L3 avec BGP - encore une fois, pour des raisons d'unification des protocoles.  Cette connexion fournit √† chaque service une connectivit√© IPv4 et IPv6: certains services fonctionnent sur IPv4 et certains services sur IPv6. <br><br>  Physiquement, chaque serveur est connect√© par deux interfaces de 25 gigabits.  Voici une photo du centre de donn√©es: <br><br><img src="https://habrastorage.org/webt/jz/sr/xj/jzsrxj39equkvhixj3bjoj5kn6a.png"><br><br>  Vous voyez ici deux commutateurs mont√©s en rack avec des ports de 100 gigabits.  Des c√¢bles de d√©rivation divergents sont visibles, divisant le port de 100 gigabits du commutateur en 4 ports de 25 gigabits par serveur.  Nous appelons ces c√¢bles "hydre". <br><br><h2>  Gestion des infrastructures </h2><br>  L'infrastructure de r√©seau Cloud ne contient aucune solution de gestion propri√©taire: tous les syst√®mes sont soit open source avec personnalisation pour le Cloud, soit enti√®rement auto-√©crits. <br><br><img src="https://habrastorage.org/webt/-g/if/qu/-gifqu8wgzw3xwehpuy5_ejfapc.png"><br><br>  Comment cette infrastructure est-elle g√©r√©e?  Ce n'est pas interdit dans le Cloud, mais il est fortement d√©conseill√© d'aller sur un p√©riph√©rique r√©seau et de faire des ajustements.  Il y a l'√©tat actuel du syst√®me, et nous devons appliquer les changements: arriver √† un nouvel √©tat cible.  "Ex√©cutez un script" √† travers toutes les glandes, changez quelque chose dans la configuration - vous ne devriez pas faire cela.  Au lieu de cela, nous apportons des modifications aux mod√®les, √† une seule source de syst√®me de v√©rit√©, et validons notre modification au syst√®me de contr√¥le de version.  C'est tr√®s pratique, car vous pouvez toujours effectuer une restauration, consulter l'historique, savoir qui est responsable du commit, etc. <br><br>  Lorsque nous avons effectu√© les modifications, des configurations sont g√©n√©r√©es et nous les d√©ployons dans la topologie de test de laboratoire.  Du point de vue du r√©seau, il s'agit d'un petit nuage qui r√©p√®te compl√®tement toute la production existante.  Nous verrons imm√©diatement si les changements souhait√©s cassent quelque chose: d'une part, par la surveillance, et d'autre part, par les retours de nos utilisateurs internes. <br><br>  Si la surveillance indique que tout est calme, alors nous continuons √† d√©ployer - mais n'appliquons le changement qu'√† une partie de la topologie (deux ou plusieurs accessibilit√©s ¬´n'ont pas le droit¬ª de s'arr√™ter pour la m√™me raison).  De plus, nous continuons de suivre de pr√®s le suivi.  Il s'agit d'un processus assez compliqu√©, dont nous parlerons ci-dessous. <br><br>  Apr√®s avoir v√©rifi√© que tout va bien, nous appliquons le changement √† toute la production.  √Ä tout moment, vous pouvez revenir en arri√®re et revenir √† l'√©tat pr√©c√©dent du r√©seau, suivre et r√©soudre rapidement le probl√®me. <br><br><h4>  Suivi </h4><br>  Nous avons besoin d'une surveillance diff√©rente.  L'un des plus recherch√©s est la surveillance de la connectivit√© de bout en bout.  √Ä tout moment, chaque serveur doit pouvoir communiquer avec n'importe quel autre serveur.  Le fait est que s'il y a un probl√®me quelque part, alors nous voulons savoir exactement o√π le plus t√¥t possible (c'est-√†-dire quels serveurs ont des probl√®mes d'acc√®s les uns aux autres).  Assurer la connectivit√© de bout en bout est notre principale pr√©occupation. <br><br>  Chaque serveur r√©pertorie un ensemble de tous les serveurs avec lesquels il devrait pouvoir communiquer √† tout moment.  Le serveur prend un sous-ensemble al√©atoire de cet ensemble et envoie des paquets ICMP, TCP et UDP √† toutes les machines s√©lectionn√©es.  Ceci v√©rifie s'il y a des pertes sur le r√©seau, si le retard a augment√©, etc. L'ensemble du r√©seau est ¬´appel√©¬ª dans l'une des zones d'acc√®s et entre elles.  Les r√©sultats sont envoy√©s √† un syst√®me centralis√© qui les visualise pour nous. <br><br>  Voici √† quoi ressemblent les r√©sultats quand tout n'est pas tr√®s bon: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yd/-1/bs/yd-1bs7mguqfcy-xhdko674rvu0.png"></div><br>  Ici, vous pouvez voir quels segments de r√©seau il y a un probl√®me entre (dans ce cas, A et B) et o√π tout va bien (A et D).  Des serveurs sp√©cifiques, des commutateurs mont√©s en rack, des modules et des zones de disponibilit√© enti√®res peuvent √™tre affich√©s ici.  Si l'un des √©l√©ments ci-dessus devient la source du probl√®me, nous le verrons en temps r√©el. <br><br>  De plus, il existe une surveillance des √©v√©nements.  Nous surveillons de pr√®s toutes les connexions, les niveaux de signal sur les √©metteurs-r√©cepteurs, les sessions BGP, etc. Supposons que trois sessions BGP soient construites √† partir d'un segment de r√©seau, dont l'un a √©t√© interrompu la nuit.  Si nous configurons la surveillance de sorte que la chute d'une session BGP ne soit pas critique pour nous et puisse attendre jusqu'au matin, alors la surveillance ne r√©veillera pas les ing√©nieurs r√©seau.  Mais si la deuxi√®me des trois sessions tombe, un ing√©nieur appelle automatiquement. <br><br>  En plus de la surveillance de bout en bout et des √©v√©nements, nous utilisons une collection centralis√©e de journaux, leur analyse en temps r√©el et leur analyse ult√©rieure.  Vous pouvez voir les corr√©lations, identifier les probl√®mes et d√©couvrir ce qui se passait sur l'√©quipement r√©seau. <br><br>  Le sujet de surveillance est assez grand, il y a une √©norme marge d'am√©lioration.  Je veux amener le syst√®me √† une plus grande automatisation et √† une v√©ritable auto-gu√©rison. <br><br><h2>  Et ensuite? </h2><br>  Nous avons de nombreux plans.  Il est n√©cessaire d'am√©liorer les syst√®mes de contr√¥le, la surveillance, la commutation des usines IP / MPLS et bien plus encore. <br><br>  Nous recherchons √©galement activement des commutateurs √† bo√Ætier blanc.  Il s'agit d'un appareil "fer" pr√™t √† l'emploi, un interrupteur sur lequel vous pouvez faire rouler votre logiciel.  Premi√®rement, si tout est fait correctement, il sera possible de ¬´traiter¬ª les commutateurs de la m√™me mani√®re que pour les serveurs, de cr√©er un processus CI / CD vraiment pratique, de d√©ployer progressivement des configurations, etc. <br><br>  Deuxi√®mement, s'il y a des probl√®mes, il est pr√©f√©rable de garder un groupe d'ing√©nieurs et de d√©veloppeurs qui r√©gleront ces probl√®mes plut√¥t que d'attendre longtemps une solution du fournisseur. <br><br>  Pour que tout fonctionne, le travail est en cours dans deux directions: <br><br><ul><li>  Nous avons consid√©rablement r√©duit la complexit√© de l'usine IP / MPLS.  D'une part, le niveau du r√©seau virtuel et les outils d'automatisation de celui-ci, au contraire, sont devenus un peu plus compliqu√©s.  D'un autre c√¥t√©, le r√©seau sous-jacent lui-m√™me est devenu plus facile.  En d'autres termes, il existe une certaine ¬´quantit√©¬ª de complexit√© qui ne peut pas √™tre sauvegard√©e.  Il peut √™tre "jet√©" d'un niveau √† un autre - par exemple, entre les niveaux du r√©seau ou du niveau du r√©seau au niveau de l'application.  Et vous pouvez distribuer correctement cette complexit√©, ce que nous essayons de faire. </li><li>  Et bien s√ªr, nous finalisons notre ensemble d'outils pour g√©rer l'ensemble de l'infrastructure. </li></ul><br>  C'est tout ce que nous voulions parler de notre infrastructure r√©seau.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Voici un lien</a> vers la cha√Æne Cloud Telegram avec des nouvelles et des conseils. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr437816/">https://habr.com/ru/post/fr437816/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr437806/index.html">EcmaScript 10 - JavaScript de cette ann√©e (ES2019)</a></li>
<li><a href="../fr437808/index.html">Perf et graphes de flamme</a></li>
<li><a href="../fr437810/index.html">R√©alit√© d'entreprise</a></li>
<li><a href="../fr437812/index.html">Xcode 10.2, macOS Mojave 10.14.4, iOS 12.1 et autres b√™tas</a></li>
<li><a href="../fr437814/index.html">Xcode 10.2, macOS Mojave 10.14.4, iOS 12.1 et autres versions b√™ta</a></li>
<li><a href="../fr437818/index.html">Nous enseignons √† un ordinateur √† distinguer les sons: se familiariser avec le concours DCASE et assembler votre classificateur audio en 30 minutes</a></li>
<li><a href="../fr437820/index.html">50 nuances de s√©curit√© Drupal</a></li>
<li><a href="../fr437824/index.html">Extension universelle 1C pour Google Sheets et Docs - prendre et utiliser</a></li>
<li><a href="../fr437826/index.html">Comment nous avons migr√© la base de donn√©es de Redis et Riak KV vers PostgreSQL. Partie 1: le processus</a></li>
<li><a href="../fr437828/index.html">Webinaire ouvert "SELECT ordre d'ex√©cution des requ√™tes et plan de requ√™te dans MS SQL Server"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>