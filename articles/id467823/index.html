<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🚼 🤳🏻 🦕 Parsing: OOM di Kubernetes 👨🏻 😰 🗄️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Masalah di lingkungan kerja selalu menjadi bencana. Itu terjadi ketika Anda pulang, dan alasannya selalu tampak bodoh. Baru-baru ini, kami kehabisan m...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Parsing: OOM di Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/467823/"><p><img src="https://habrastorage.org/webt/m8/of/zv/m8ofzvtlhfh8mkwo-i04c5ren38.jpeg"></p><br><p>  Masalah di lingkungan kerja selalu menjadi bencana.  Itu terjadi ketika Anda pulang, dan alasannya selalu tampak bodoh.  Baru-baru ini, kami kehabisan memori pada node di cluster Kubernetes, meskipun node segera pulih, tanpa gangguan yang terlihat.  Hari ini kita akan membahas tentang kasus ini, tentang kerusakan apa yang kita derita dan bagaimana kita bermaksud menghindari masalah yang sama di masa depan. </p><br><h2 id="sluchay-pervyy">  Kasus satu </h2><br><h3 id="subbota-15-iyunya-2019-g-1712">  Sabtu, 15 Juni 2019 5:12 malam </h3><a name="habracut"></a><br><p>  Blue Matador (ya, kami memantau diri kami sendiri!) Menghasilkan peringatan: acara di salah satu node di cluster produksi Kubernetes - SystemOOM. </p><br><h3 id="1716">  17:16 </h3><br><p>  Blue Matador menghasilkan peringatan: EBS Burst Balance rendah dalam volume root node - yang mana acara SystemOOM berlangsung.  Meskipun peringatan tentang Burst Balance muncul setelah pemberitahuan tentang SystemOOM, data CloudWatch aktual menunjukkan bahwa Burst Balance telah mencapai level minimum pada 17:02.  Alasan keterlambatannya adalah metrik EBS terus-menerus tertinggal 10-15 menit, dan sistem kami tidak menangkap semua peristiwa secara real time. </p><br><p><img src="https://habrastorage.org/webt/xw/gb/u3/xwgbu3jxukrpz_qmakfhvsb5bm4.png"></p><br><h3 id="1718">  17:18 </h3><br><p> Saat ini saya melihat peringatan dan peringatan.  Saya dengan cepat menjalankan <strong>kubectl mendapatkan pod</strong> untuk melihat kerusakan apa yang kami alami, dan saya terkejut menemukan bahwa pod dalam aplikasi mati tepat 0. Saya <strong>melakukan node top kubectl</strong> , tetapi pemeriksaan ini juga menunjukkan bahwa simpul yang dicurigai memiliki masalah memori;  Benar, sudah pulih dan menggunakan sekitar 60% dari memorinya.  Sekarang jam 5 sore, dan bir kerajinan sudah memanas.  Setelah memastikan bahwa node itu operasional dan tidak ada satu pod pun yang rusak, saya memutuskan bahwa telah terjadi kecelakaan.  Jika ada, saya akan mencari tahu pada hari Senin. </p><br><p>  Ini korespondensi kami dengan stasiun layanan di Slack malam itu: </p><br><p><img src="https://habrastorage.org/webt/ib/fp/f0/ibfpf05vnjd2uaukxr0rwjgcjj0.png"></p><br><h2 id="sluchay-vtoroy">  Kasus dua </h2><br><h3 id="subbota-16-iyunya-2019-g-1802">  Sabtu, 16 Juni 2019 6:02 malam </h3><br><p>  Blue Matador menghasilkan peringatan: acara sudah ada di node lain, juga SystemOOM.  Pasti bahwa stasiun layanan pada saat itu hanya melihat layar smartphone, karena itu menulis kepada saya dan membuat saya segera mengambil acara, saya sendiri tidak dapat menyalakan komputer (apakah sudah waktunya untuk menginstal ulang Windows lagi?).  Dan lagi, semuanya tampak normal.  Tidak ada satu pod pun yang terbunuh, dan node tersebut hampir tidak mengkonsumsi 70% dari memori. </p><br><h3 id="1806">  18:06 </h3><br><p>  Blue Matador menghasilkan peringatan lagi: EBS Burst Balance.  Kali kedua dalam sehari, yang artinya saya tidak bisa melepaskan masalah ini saat menginjak rem.  Dengan CloudWatch tidak berubah, Burst Balance menyimpang dari norma 2 jam atau lebih sebelum masalah teridentifikasi. </p><br><h3 id="1811">  18:11 </h3><br><p>  Saya pergi ke Datalog dan melihat data tentang konsumsi memori.  Saya melihat itu tepat sebelum acara SystemOOM, simpul yang dicurigai benar-benar mengambil banyak memori.  Jejak mengarah ke polong fluentd-sumologis kami. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/ae/s-/k-/aes-k-fwfx5qtsm7ehvhkgnpofe.png"></a> </p><br><p>  Anda dapat dengan jelas melihat penyimpangan tajam dalam konsumsi memori, pada waktu yang hampir bersamaan dengan peristiwa SystemOOM terjadi.  Kesimpulan saya: pod-pod inilah yang mengambil semua memori, dan ketika SystemOOM terjadi, Kubernetes menyadari bahwa pod-pod ini dapat dibunuh dan dimulai kembali untuk mengembalikan memori yang diperlukan tanpa mempengaruhi pod saya yang lain.  Bagus sekali, Kubernetes! </p><br><p>  Jadi mengapa saya tidak melihat ini pada hari Sabtu ketika saya menemukan pod mana yang direstart?  Faktanya adalah bahwa saya menjalankan polent fluentd-sumologic di namespace terpisah dan terburu-buru saya hanya tidak berpikir untuk melihatnya. </p><br><blockquote>  Kesimpulan 1: Saat mencari pod yang dimulai kembali, periksa semua ruang nama. </blockquote><p>  Setelah menerima data ini, saya menghitung bahwa pada hari berikutnya memori pada node lain tidak akan berakhir, namun, saya melanjutkan dan me-restart semua pod sumologi sehingga mereka mulai bekerja dengan konsumsi memori yang rendah.  Pagi berikutnya, saya berencana untuk menyiapkan cara mengintegrasikan pekerjaan pada masalah ke dalam rencana untuk minggu ini dan tidak memuat terlalu banyak Minggu malam. </p><br><h3 id="2300">  23:00 </h3><br><p>  Saya menonton seri "Cermin Hitam" berikutnya (omong-omong, saya suka Miley) dan memutuskan untuk melihat bagaimana kinerja cluster.  Konsumsi memori normal, jadi jangan ragu untuk meninggalkan semuanya karena untuk malam. </p><br><h2 id="pochinka">  Perbaiki </h2><br><p>  Pada hari Senin, saya menyediakan waktu untuk masalah ini.  Tidak ada ruginya berburu dengannya setiap malam.  Apa yang saya tahu saat ini: </p><br><ul><li>  Wadah-sumologi fluentd melahap satu ton memori; </li><li>  Acara SystemOOM diawali oleh aktivitas disk tinggi, tapi saya tidak tahu yang mana. </li></ul><br><p>  Awalnya saya berpikir bahwa wadah fluentd-sumologis diterima untuk memakan memori pada saat masuknya kayu gelondongan yang tiba-tiba.  Namun, setelah memeriksa Sumologic, saya melihat bahwa log digunakan secara stabil, dan pada saat yang sama ketika ada masalah, tidak ada peningkatan dalam log ini. </p><br><p>  Sedikit googling, saya menemukan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">tugas ini di github</a> , yang menyarankan menyesuaikan beberapa pengaturan Ruby - untuk mengurangi konsumsi memori.  Saya memutuskan untuk mencobanya, menambahkan variabel lingkungan ke spesifikasi pod dan menjalankannya: </p><br><pre><code class="plaintext hljs">env: - name: RUBY_GC_HEAP_OLDOBJECT_LIMIT_FACTOR value: "0.9"</code> </pre> <br><p>  Melihat melalui manifes fluentd-sumologic, saya perhatikan bahwa saya tidak mendefinisikan permintaan dan pembatasan sumber daya.  Saya mulai curiga bahwa perbaikan RUBY_GCP_HEAP akan melakukan semacam keajaiban, jadi sekarang masuk akal untuk menetapkan batas konsumsi memori.  Bahkan jika saya tidak memperbaiki masalah memori, paling tidak akan mungkin membatasi konsumsinya untuk set pod ini.  Menggunakan <strong>kubectl pod teratas |</strong>  <strong>grep fluentd-sumologic</strong> , saya sudah tahu berapa banyak sumber daya untuk diminta: </p><br><pre> <code class="plaintext hljs">resources: requests: memory: "128Mi" cpu: "100m" limits: memory: "1024Mi" cpu: "250m"</code> </pre> <br><blockquote>  Kesimpulan 2: Tetapkan batas sumber daya, terutama untuk aplikasi pihak ketiga. </blockquote><br><h2 id="proverka-ispolneniya">  Verifikasi eksekusi </h2><br><p>  Setelah beberapa hari, saya mengkonfirmasi bahwa metode di atas berfungsi.  Konsumsi memori stabil, dan - tidak ada masalah dengan komponen Kubernetes, EC2 dan EBS.  Sekarang jelas betapa pentingnya menentukan permintaan dan pembatasan sumber daya untuk semua pod yang saya jalankan, dan inilah yang perlu dilakukan: menerapkan kombinasi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">batas sumber daya default dan</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">kuota</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sumber daya</a> . </p><br><p>  Misteri yang belum terpecahkan terakhir adalah EBS Burst Balance, yang bertepatan dengan acara SystemOOM.  Saya tahu bahwa ketika ada sedikit memori, OS menggunakan ruang swap agar tidak dibiarkan sepenuhnya tanpa memori.  Tapi saya belum lahir kemarin dan saya sadar bahwa Kubernetes bahkan tidak akan mulai di server tempat file halaman diaktifkan.  Hanya ingin memastikan, saya naik ke node saya melalui SSH - untuk memeriksa apakah file halaman diaktifkan;  Saya menggunakan memori bebas dan yang ada di area swap.  File tidak diaktifkan. </p><br><p>  Dan karena swapping tidak berfungsi, saya memiliki lebih banyak petunjuk tentang apa yang menyebabkan pertumbuhan aliran I / O, itulah mengapa node hampir kehabisan memori, tidak.  Sebenarnya, saya punya firasat: pod fluentd-sumologic itu sendiri sedang menulis satu ton pesan log saat ini, bahkan mungkin pesan log yang terkait dengan pengaturan Ruby GC.  Ada juga kemungkinan bahwa ada sumber-sumber lain dari Kubernet atau pesan journald yang menjadi sangat produktif ketika ingatannya habis, dan saya menghilangkannya ketika mengatur fluentd.  Sayangnya, saya tidak lagi memiliki akses ke file log yang direkam sebelum kerusakan terjadi, dan sekarang tidak ada cara untuk menggali lebih dalam. </p><br><blockquote>  Kesimpulan 3: Meskipun ada peluang, gali lebih dalam saat menganalisis akar permasalahan, apa pun masalahnya. </blockquote><br><h2 id="zaklyuchenie">  Kesimpulan </h2><br><p>  Dan meskipun saya tidak sampai ke akar penyebabnya, saya yakin mereka tidak diperlukan untuk mencegah kerusakan yang sama di masa depan.  Waktu adalah uang, tetapi saya sudah terlalu lama sibuk, dan setelah itu saya juga menulis posting ini untuk Anda.  Dan karena kami menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Blue Matador</a> , kerusakan seperti itu ditangani dengan sangat rinci, jadi saya membiarkan diri saya melepaskan sesuatu pada rem, tanpa terganggu dari proyek utama. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id467823/">https://habr.com/ru/post/id467823/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id467811/index.html">Arsitektur dan pemrograman Fairchild Channel F</a></li>
<li><a href="../id467813/index.html">Tinjau perubahan dalam urutan 17 FSTEC</a></li>
<li><a href="../id467815/index.html">Media menimbulkan kepanikan bahwa "alamat IP kehabisan di Rusia." Bagaimana bisa?</a></li>
<li><a href="../id467817/index.html">Sedikit tentang pola desain generatif</a></li>
<li><a href="../id467821/index.html">Sederhanakan dan Potong yang Dibutuhkan: Wawancara dengan John Romero, pencipta Doom</a></li>
<li><a href="../id467825/index.html">Algoritma pembelajaran mesin harus dimiliki</a></li>
<li><a href="../id467827/index.html">Bagaimana kami melakukan Unity kecil kami dari awal</a></li>
<li><a href="../id467831/index.html">Jalan Berduri menuju Pemrograman</a></li>
<li><a href="../id467837/index.html">MCU tiga sen "Mengerikan" - tinjauan singkat mikrokontroler dengan biaya kurang dari $ 0,1</a></li>
<li><a href="../id467841/index.html">Buat lebih mudah untuk selesai: Wawancara dengan John Romero, pengembang Doom</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>