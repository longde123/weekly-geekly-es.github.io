<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘©ğŸ» â• ğŸšµğŸ» Konkurensi PostgreSQL: tidak berbentuk bola, bukan kuda, tidak dalam ruang hampa ğŸ» ğŸ§“ğŸ¾ â˜ ï¸</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Menskalakan DBMS adalah masa depan yang terus maju. DBMS meningkat dan skala lebih baik pada platform perangkat keras, sementara platform perangkat ke...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Konkurensi PostgreSQL: tidak berbentuk bola, bukan kuda, tidak dalam ruang hampa</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/postgrespro/blog/423685/"><img src="https://habrastorage.org/webt/b0/s3/rq/b0s3rqkffufh7baruji0jc_vbgq.jpeg"><br><br>  Menskalakan DBMS adalah masa depan yang terus maju.  DBMS meningkat dan skala lebih baik pada platform perangkat keras, sementara platform perangkat keras itu sendiri meningkatkan produktivitas, jumlah core, dan memori - Achilles mengejar ketinggalan dengan kura-kura, tetapi masih belum.  Masalah penskalaan DBMS sedang dalam ayunan penuh. <br><br>  Postgres Professional memiliki masalah dengan penskalaan tidak hanya secara teoritis, tetapi juga secara praktis: dengan pelanggannya.  Dan lebih dari sekali.  Salah satu kasus seperti itu akan dibahas dalam artikel ini. <br><br>  PostgreSQL memiliki skala yang baik pada sistem NUMA jika merupakan motherboard tunggal dengan banyak prosesor dan beberapa bus data.  Beberapa optimasi dapat dibaca di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> dan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> .  Namun, ada kelas lain dari sistem, mereka memiliki beberapa motherboard, pertukaran data di antaranya dilakukan dengan menggunakan interkoneksi, sementara satu instance dari OS bekerja pada mereka dan untuk pengguna desain ini terlihat seperti mesin tunggal.  Dan meskipun secara formal sistem tersebut juga dapat dikaitkan dengan NUMA, tetapi pada dasarnya mereka lebih dekat ke superkomputer, seperti  akses ke memori lokal dari node dan akses ke memori dari node tetangga berbeda secara radikal.  Komunitas PostgreSQL percaya bahwa satu-satunya instance Postgres yang berjalan pada arsitektur seperti itu adalah sumber masalah, dan belum ada pendekatan sistematis untuk menyelesaikannya. <br><a name="habracut"></a><br>  Ini karena arsitektur perangkat lunak yang menggunakan memori bersama pada dasarnya dirancang untuk fakta bahwa waktu akses berbagai proses ke memori mereka sendiri dan jarak jauh lebih atau kurang sebanding.  Dalam kasus ketika kita bekerja dengan banyak node, taruhan pada memori bersama sebagai saluran komunikasi cepat berhenti untuk membenarkan dirinya sendiri, karena karena latensi itu jauh "lebih murah" untuk mengirim permintaan untuk tindakan tertentu ke node (node) di mana ia berada data menarik daripada mengirim data ini di bus.  Oleh karena itu, untuk superkomputer dan, secara umum, sistem dengan banyak node, solusi cluster relevan. <br><br>  Ini tidak berarti bahwa kombinasi dari sistem multi-simpul dan arsitektur shared-memory khas Postgres harus diakhiri.  Lagi pula, jika proses postgres menghabiskan sebagian besar waktu mereka melakukan perhitungan kompleks secara lokal, maka arsitektur ini bahkan akan sangat efisien.  Dalam situasi kami, klien telah membeli server multi-node yang kuat, dan kami harus menyelesaikan masalah PostgreSQL di dalamnya. <br><br>  Tetapi masalahnya serius: permintaan penulisan paling sederhana (ubah beberapa nilai bidang dalam satu catatan) dieksekusi dalam periode beberapa menit hingga satu jam.  Seperti yang kemudian dikonfirmasikan, masalah-masalah ini memanifestasikan diri mereka dalam semua kemuliaan mereka justru karena banyaknya inti dan, karenanya, paralelisme radikal dalam pelaksanaan permintaan dengan pertukaran yang relatif lambat antara node. <br><br>  Karenanya, artikel tersebut akan berubah, seolah-olah, untuk tujuan ganda: <br><br><ul><li>  Berbagi pengalaman: apa yang harus dilakukan jika dalam sistem multi-simpul database melambat dengan sungguh-sungguh.  Mulai dari mana, cara mendiagnosis ke mana harus pindah. </li><li>  Jelaskan bagaimana masalah DBMS PostgreSQL itu sendiri dapat diselesaikan dengan konkurensi tingkat tinggi.  Termasuk bagaimana perubahan dalam algoritma untuk mengambil kunci mempengaruhi kinerja PostgreSQL. </li></ul><br><h3>  Server dan DB </h3><br>  Sistem ini terdiri dari 8 bilah dengan masing-masing 2 soket.  Secara total, lebih dari 300 core (tidak termasuk hypertreading).  Ban cepat (teknologi pabrikan berpemilik) menghubungkan bilah.  Bukan berarti itu superkomputer, tetapi untuk satu contoh DBMS, konfigurasi sangat mengesankan. <br>  Bebannya juga agak besar.  Data lebih dari 1 terabyte.  Sekitar 3000 transaksi per detik.  Lebih dari 1000 koneksi ke postgres. <br><br>  Setelah mulai berurusan dengan harapan perekaman per jam, hal pertama yang kami lakukan adalah menulis ke disk sebagai penyebab penundaan.  Segera setelah penundaan yang tidak dapat dipahami dimulai, tes mulai dilakukan secara eksklusif pada <code>tmpfs</code> .  Gambar tidak berubah.  Disk tidak ada hubungannya dengan itu. <br><br><h3>  Memulai dengan Diagnosis: Tampilan </h3><br>  Karena masalah muncul kemungkinan besar karena tingginya persaingan proses yang "mengetuk" objek yang sama, hal pertama yang perlu diperiksa adalah kunci.  Di PostgreSQL, ada tampilan <code>pg.catalog.pg_locks</code> dan <code>pg_stat_activity</code> untuk <code>pg_stat_activity</code> semacam itu.  Yang kedua, sudah dalam versi 9.6, menambahkan informasi tentang apa proses menunggu ( <i>Amit Kapila, Ildus Kurbangaliev</i> ) - <code>wait_event_type</code> .  Nilai yang mungkin untuk bidang ini dijelaskan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . <br><br>  Tapi pertama-tama, hitung saja: <br><br><pre> <code class="sql hljs">postgres=<span class="hljs-comment"><span class="hljs-comment"># SELECT COUNT(*) FROM pg_locks; count â€”---â€” 88453 (1 row) postgres=# SELECT COUNT(*) FROM pg_stat_activity; count â€”---â€” 1826 (1 row) postgres=# SELECT COUNT(*) FROM pg_stat_activity WHERE state ='active'; count â€”---â€” 1005</span></span></code> </pre> <br>  Ini adalah bilangan real.  Mencapai hingga 200.000 kunci. <br>  Pada saat yang sama, kunci seperti itu tergantung pada permintaan naas: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">COUNT</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">mode</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">mode</span></span> <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> pg_locks <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> pid =<span class="hljs-number"><span class="hljs-number">580707</span></span> <span class="hljs-keyword"><span class="hljs-keyword">GROUP</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> <span class="hljs-keyword"><span class="hljs-keyword">mode</span></span>; count | mode â€”<span class="hljs-comment"><span class="hljs-comment">-----+---------------â€” 93 | AccessShareLock 1 | ExclusiveLock</span></span></code> </pre> <br>  Saat membaca buffer, DBMS menggunakan kunci <code>share</code> , sambil menulis - <code>exclusive</code> .  Artinya, menulis kunci menyumbang kurang dari 1% dari semua permintaan. <br>  Dalam tampilan <code>pg_locks</code> , tipe kunci tidak selalu terlihat seperti yang dijelaskan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dalam dokumentasi</a> pengguna. <br><br>  Ini plat pertandingannya: <br><br><pre> <code class="plaintext hljs">AccessShareLock = LockTupleKeyShare RowShareLock = LockTupleShare ExclusiveLock = LockTupleNoKeyExclusive AccessExclusiveLock = LockTupleExclusive</code> </pre> <br>  Mode SELECT FROM permintaan pg_locks menunjukkan bahwa CREATE INDEX (tanpa CONCURRENTLY) akan menunggu 234 INSERT dan 390 INSERT untuk <code>buffer content lock</code> .  Solusi yang mungkin adalah "mengajar" INSERT dari sesi yang berbeda untuk memotong lebih sedikit buffer. <br><br><h3>  Saatnya menggunakan perf </h3><br>  Utilitas <b><code>perf</code></b> mengumpulkan banyak informasi diagnostik.  Dalam mode <code>record</code> ... ini menulis statistik peristiwa sistem ke file (secara default mereka berada di <code>./perf_data</code> ), dan dalam mode <code>report</code> ini menganalisis data yang dikumpulkan, misalnya, Anda dapat memfilter peristiwa yang hanya menyangkut <code>postgres</code> atau <code>pid</code> diberikan: <br><br><pre> <code class="plaintext hljs">$ perf record -u postgres  $ perf record -p 76876  ,  $ perf report &gt; ./my_results</code> </pre> <br>  Akibatnya, kita akan melihat sesuatu seperti <br><br><img src="https://habrastorage.org/webt/rn/ta/rv/rntarvj2jticq7glciiqockk3-y.jpeg"><br><br>  Cara menggunakan <code>perf</code> untuk mendiagnosis PostgreSQL dijelaskan, misalnya, di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> , serta di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">wiki pg</a> . <br><br>  Dalam kasus kami, bahkan mode paling sederhana pun memberikan informasi penting - <code>perf top</code> , yang berfungsi, tentu saja, dalam semangat sistem operasi <code>top</code> .  Dengan <code>perf top</code> kami melihat bahwa sebagian besar waktu yang dihabiskan prosesor dalam <code>PinBuffer()</code> inti, serta dalam fungsi <code>PinBuffer()</code> dan <code>LWLockAttemptLock().</code>  . <br><br>  <code>PinBuffer()</code> adalah fungsi yang meningkatkan counter referensi ke buffer (memetakan halaman data ke RAM), berkat proses postgres yang tahu buffer mana yang dapat dipaksa keluar dan mana yang tidak bisa. <br><br>  <code>LWLockAttemptLock()</code> - fungsi penangkapan <code>LWLock</code> .  <code>LWLock</code> adalah sejenis kunci dengan dua tingkat yang <code>shared</code> dan <code>exclusive</code> , tanpa menentukan <code>deadlock</code> , kunci telah dialokasikan sebelumnya ke <code>shared memory</code> , proses menunggu menunggu dalam antrian. <br><br>  Fungsi-fungsi ini telah dioptimalkan secara serius di PostgreSQL 9.5 dan 9.6.  Spinlocks di dalamnya digantikan oleh penggunaan langsung operasi atom. <br><br><h3>  Grafik nyala </h3><br>  Tidak mungkin tanpa mereka: bahkan jika mereka tidak berguna, masih layak diceritakan tentang mereka - mereka luar biasa cantik.  Tetapi mereka bermanfaat.  Ini adalah ilustrasi dari <code>github</code> , bukan dari kasus kami (kami maupun klien belum siap untuk mengungkapkan detailnya). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/75b/909/e8d/75b909e8de5177a48fa7d73f53eff437.svg"><br><br>  Gambar-gambar indah ini dengan sangat jelas menunjukkan apa yang dilakukan siklus prosesor.  <code>perf</code> sama dapat mengumpulkan data, tetapi <code>flame graph</code> cerdas memvisualisasikan data, dan membangun pohon berdasarkan tumpukan panggilan yang dikumpulkan.  Anda dapat membaca lebih lanjut tentang pembuatan profil dengan grafik nyala, misalnya, di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> , dan unduh semua yang Anda butuhkan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . <br><br>  Dalam kasus kami, sejumlah besar <code>nestloop</code> terlihat pada grafik nyala.  Rupanya, GABUNGAN dari sejumlah besar tabel dalam berbagai permintaan baca bersamaan menyebabkan sejumlah besar kunci <code>access share</code> . <br><br>  Statistik yang dikumpulkan oleh <code>perf</code> menunjukkan kemana siklus prosesor pergi.  Dan meskipun kami melihat bahwa sebagian besar waktu prosesor melewati kunci, kami tidak melihat apa yang sebenarnya mengarah pada harapan kunci yang lama, karena kami tidak melihat dengan tepat di mana harapan kunci terjadi, karena  Waktu CPU tidak sia-sia menunggu. <br><br>  Untuk melihat harapan itu sendiri, Anda dapat membuat permintaan ke tampilan sistem <code>pg_stat_activity</code> . <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> wait_event_type, wait_event, <span class="hljs-keyword"><span class="hljs-keyword">COUNT</span></span>(*) <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> pg_stat_activity <span class="hljs-keyword"><span class="hljs-keyword">GROUP</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> wait_event_type, wait_event;</code> </pre> <br>  mengungkapkan bahwa: <br><br><pre> <code class="plaintext hljs">LWLockTranche | buffer_content | UPDATE ************* LWLockTranche | buffer_content | INSERT INTO ******** LWLockTranche | buffer_content | \r | | insert into B4_MUTEX | | values (nextval('hib | | returning ID Lock | relation | INSERT INTO B4_***** LWLockTranche | buffer_content | UPDATE ************* Lock | relation | INSERT INTO ******** LWLockTranche | buffer_mapping | INSERT INTO ******** LWLockTranche | buffer_content | \r</code> </pre> <br>  (tanda bintang di sini hanya mengganti detail permintaan yang tidak kami ungkapkan). <br><br>  Anda dapat melihat nilai <code>buffer_content</code> (memblokir konten buffer) dan <code>buffer_mapping</code> (memblokir komponen dari plat hash <code>shared_buffers</code> ). <br><br><h3>  Untuk bantuan ke gdb </h3><br>  Tetapi mengapa ada begitu banyak harapan untuk jenis kunci ini?  Untuk informasi lebih rinci tentang harapan, saya harus menggunakan debugger <code>GDB</code> .  Dengan <code>GDB</code> kita bisa mendapatkan setumpuk panggilan proses tertentu.  Dengan menerapkan sampling, mis.  Setelah mengumpulkan sejumlah tumpukan panggilan acak, Anda bisa mendapatkan gagasan tumpukan mana yang memiliki harapan terlama. <br><br>  Pertimbangkan proses penyusunan statistik.  Kami akan mempertimbangkan kumpulan statistik "manual", meskipun dalam kehidupan nyata skrip khusus digunakan yang melakukan ini secara otomatis. <br><br>  Pertama, <code>gdb</code> harus dilampirkan ke proses PostgreSQL.  Untuk melakukan ini, cari <code>pid</code> proses server, katakan dari <br><br><pre> <code class="plaintext hljs">$ ps aux | grep postgres</code> </pre> <br>  Katakanlah kita menemukan: <br><br><pre> <code class="plaintext hljs">postgres 2025 0.0 0.1 172428 1240 pts/17  S   23  0:00 /usr/local/pgsql/bin/postgres -D /usr/local/pgsql/data</code> </pre> <br>  dan sekarang masukkan <code>pid</code> ke debugger: <br><br><pre> <code class="plaintext hljs">igor_le:~$gdb -p 2025</code> </pre> <br>  Setelah di dalam debugger, kami menulis <code>bt</code> [yaitu, <code>backtrace</code> ] atau di <code>where</code> .  Dan kami mendapatkan banyak informasi tentang jenis ini: <br><br><pre> <code class="plaintext hljs">(gdb) bt #0 0x00007fbb65d01cd0 in __write_nocancel () from /lib64/libc.so.6 #1 0x00000000007c92f4 in write_pipe_chunks ( data=0x110e6e8 "2018â€06â€01 15:35:38 MSK [524647]: [392â€1] db=bp,user=bp,app=[unknown],client=192.168.70.163 (http://192.168.70.163) LOG: relation 23554 new block 493: 248.389503\n2018â€06â€01 15:35:38 MSK [524647]: [393â€1] db=bp,user=bp,app=["..., len=409, dest=dest@entry=1) at elog.c:3123 #2 0x00000000007cc07b in send_message_to_server_log (edata=0xc6ee60 &lt;errordata&gt;) at elog.c:3024 #3 EmitErrorReport () at elog.c:1479</code> </pre> <br>  Setelah mengumpulkan statistik, termasuk tumpukan panggilan dari semua proses postgres, dikumpulkan berulang kali pada titik waktu yang berbeda, kami melihat bahwa <code>buffer partition lock</code> di dalam <code>relation extension lock</code> berlangsung 3706 detik (sekitar satu jam), yaitu, mengunci sepotong tabel hash buffer manajer, yang diperlukan untuk mengganti buffer lama, untuk kemudian menggantinya dengan yang baru yang sesuai dengan bagian tabel yang diperpanjang.  Sejumlah kunci <code>buffer content lock</code> juga terlihat, yang sesuai dengan harapan mengunci halaman indeks <code>B-tree</code> untuk dimasukkan. <br><br><img src="https://habrastorage.org/webt/6c/y6/cw/6cy6cwmfye29jw84dkzoabjlgvg.jpeg"><br><br>  Pada awalnya, dua penjelasan datang untuk waktu tunggu yang mengerikan: <br><br><ul><li>  Orang lain mengambil <code>LWLock</code> ini dan terjebak.  Tapi ini tidak mungkin.  Karena tidak ada yang rumit terjadi di dalam kunci partisi penyangga. </li><li>  Kami menemukan beberapa perilaku patologis <code>LWLock</code> .  Artinya, terlepas dari kenyataan bahwa tidak ada yang mengambil kunci terlalu lama, harapannya bertahan lama tidak masuk akal. </li></ul><br><h3>  Tambalan diagnostik dan perawatan pohon </h3><br>  Dengan mengurangi jumlah koneksi simultan, kami mungkin akan mengalirkan aliran permintaan ke kunci.  Tapi itu akan seperti menyerah.  Alih-alih, <i>Alexander Korotkov</i> , kepala arsitek Postgres Professional (tentu saja, ia membantu menyiapkan artikel ini), mengusulkan serangkaian tambalan. <br><br>  Pertama-tama, perlu untuk mendapatkan gambaran yang lebih rinci tentang bencana.  Tidak peduli sebagus apa pun alat yang digunakan, tambalan diagnostik buatan mereka sendiri juga akan bermanfaat. <br><br>  Patch ditulis yang menambahkan pencatatan terperinci waktu yang dihabiskan dalam <code>relation extension</code> , apa yang terjadi di dalam fungsi <code>RelationAddExtraBlocks()</code> . Jadi kami mencari tahu berapa waktu yang dihabiskan di dalam <code>RelationAddExtraBlocks().</code> <br><br>  Dan untuk mendukungnya, tambalan lain ditulis melaporkan dalam <code>pg_stat_activity</code> tentang apa yang kita lakukan sekarang dalam <code>relation extension</code> .  Itu dilakukan dengan cara ini: ketika <code>relation</code> berkembang, <code>application_name</code> menjadi <code>RelationAddExtraBlocks</code> .  Proses ini sekarang mudah dianalisis dengan detail maksimum menggunakan <code>gdb bt</code> dan <code>perf</code> . <br><br>  Sebenarnya tambalan medis (dan bukan diagnostik) ditulis dua.  Tambalan pertama mengubah perilaku kunci daun <code>Bâ€tree</code> : sebelumnya, ketika diminta untuk memasukkan, daun diblokir sebagai <code>share</code> , dan setelah itu menjadi <code>exclusive</code> .  Sekarang dia langsung menjadi <code>exclusive</code> .  Sekarang tambalan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ini telah dikomit</a> untuk <b>PostgreSQL 12</b> .  Untungnya, tahun ini <i>Alexander Korotkov</i> menerima <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">status committer</a> - committer PostgreSQL kedua di Rusia dan yang kedua di perusahaan. <br><br>  Nilai <code>NUM_BUFFER_PARTITIONS</code> juga ditingkatkan dari 128 menjadi 512 untuk mengurangi beban pada kunci pemetaan: tabel hash manajer buffer dibagi menjadi potongan-potongan yang lebih kecil, dengan harapan bahwa beban pada setiap bagian tertentu akan berkurang. <br><br>  Setelah menerapkan tambalan ini, kunci pada konten buffer hilang, tetapi meskipun ada peningkatan <code>NUM_BUFFER_PARTITIONS</code> , <code>buffer_mapping</code> tetap ada, yaitu, kami mengingatkan Anda tentang memblokir bagian dari tabel hash manajer buffer: <br><br><pre> <code class="plaintext hljs">locks_count | active_session | buffer_content | buffer_mapping ----â€â€â€--â€â€â€+â€------â€â€â€â€â€â€â€â€â€+â€â€â€------â€â€â€â€â€â€â€+â€â€------â€â€â€ 12549 | 1218 | 0 | 15</code> </pre> <br>  Dan itu pun tidak banyak.  B - tree tidak lagi menjadi hambatan.  Perpanjangan <code>heap-</code> datang ke depan. <br><br><h3>  Perawatan hati nurani </h3><br>  Selanjutnya, Alexander mengajukan hipotesis dan solusi berikut: <br><br>  Kami menunggu banyak waktu pada <code>buffer parittion lock</code> ketika <code>buffer parittion lock</code> buffer.  Mungkin pada <code>buffer parittion lock</code> sama ada beberapa halaman yang sangat dituntut, misalnya, root dari beberapa <code>Bâ€tree</code> .  Pada titik ini ada aliran permintaan terus menerus untuk <code>shared lock</code> dari permintaan membaca. <br><br>  <code>LWLock</code> di <code>LWLock</code> â€œtidak adil.â€  Karena <code>shared lock</code> dapat diambil sebanyak yang dibutuhkan sekaligus, maka jika <code>shared lock</code> sudah diambil, maka <code>shared lock</code> selanjutnya lewat tanpa antrian.  Dengan demikian, jika aliran kunci bersama adalah intensitas yang cukup sehingga tidak ada "jendela" di antara mereka, maka menunggu <code>exclusive lock</code> berjalan hampir hingga tak terbatas. <br><br>  Untuk memperbaikinya, Anda dapat mencoba menawarkan - sepetak kunci "sopan" dari kunci.  Ini membangkitkan hati nurani dari <code>shared locker</code> dan mereka jujur â€‹â€‹mengantri ketika sudah ada <code>exclusive lock</code> (yang menarik, kunci berat - <code>hwlock</code> - tidak memiliki masalah dengan hati nurani: mereka selalu jujur â€‹â€‹mengantri) <br><br><pre> <code class="plaintext hljs">locks_count | active_session | buffer_content | buffer_mapping | reladdextra | inserts&gt;30sec â€â€â€â€â€â€-â€â€â€â€â€+â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€+â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€+â€â€â€â€â€â€â€â€â€â€â€--â€-â€+â€â€â€â€â€â€-â€â€â€â€â€â€+â€â€â€â€------ 173985 | 1802 | 0 | 569 | 0 | 0</code> </pre> <br>  Semuanya baik-baik saja!  Tidak ada <code>insert</code> panjang.  Meskipun kunci pada potongan pelat hash tetap ada.  Tapi apa yang harus dilakukan, ini adalah sifat-sifat ban superkomputer kecil kami. <br><br>  Tambalan ini juga <a href="">ditawarkan kepada komunitas</a> .  Tetapi tidak peduli bagaimana nasib patch ini dalam komunitas berkembang, tidak ada yang menghalangi mereka untuk masuk ke versi <b>Postgres Pro Enterprise berikutnya</b> , yang dirancang khusus untuk pelanggan dengan sistem yang sarat muatan. <br><br><h3>  Akhlak </h3><br>  Kunci <code>share</code> ringan bermoral tinggi - blok <code>exclusive</code> melewatkan antrian - telah memecahkan masalah keterlambatan per jam dalam sistem multi-simpul.  Tag hash <code>buffer manager</code> tidak berfungsi karena terlalu banyak aliran <code>share lock</code> , yang tidak memberikan peluang bagi kunci yang diperlukan untuk mengganti buffer lama dan memuat yang baru.  Masalah dengan ekstensi buffer untuk tabel database hanya konsekuensi dari ini.  Sebelum ini, adalah mungkin untuk memperluas bottleneck dengan akses ke root <code>B-tree</code> . <br><br>  PostgreSQL tidak dirancang untuk arsitektur dan superkomputer NUMA.  Beradaptasi dengan arsitektur Postgres seperti itu adalah pekerjaan besar yang akan membutuhkan (dan mungkin memerlukan) upaya terkoordinasi dari banyak orang dan bahkan perusahaan.  Tetapi konsekuensi yang tidak menyenangkan dari masalah arsitektur ini dapat dikurangi.  Dan kita harus: jenis-jenis beban yang menyebabkan penundaan yang serupa dengan yang dijelaskan cukup tipikal, sinyal marabahaya yang serupa dari tempat lain terus mendatangi kita.  Masalah serupa muncul sebelumnya - pada sistem dengan core lebih sedikit, hanya konsekuensinya tidak begitu mengerikan, dan gejalanya diobati dengan metode lain dan patch lainnya.  Sekarang obat lain telah muncul - tidak universal, tetapi jelas bermanfaat. <br><br>  Jadi, ketika PostgreSQL bekerja dengan memori seluruh sistem sebagai lokal, tidak ada bus berkecepatan tinggi antara node yang dapat dibandingkan dengan waktu akses ke memori lokal.  Tugas muncul karena ini sulit, seringkali mendesak, tetapi menarik.  Dan pengalaman memecahkannya bermanfaat tidak hanya untuk yang menentukan, tetapi juga untuk seluruh komunitas. <br><br><img src="https://habrastorage.org/webt/od/n1/sf/odn1sf_id7l60ezlyo-padxymmi.jpeg"></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id423685/">https://habr.com/ru/post/id423685/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id423663/index.html">Kami menulis penerjemah sederhana dalam Lisp - III</a></li>
<li><a href="../id423667/index.html">Sejarah video game mikroprosesor pertama</a></li>
<li><a href="../id423677/index.html">Pilot Jetpack: Frankie West</a></li>
<li><a href="../id423679/index.html">Tugas dengan gedung pencakar langit dan telur - bukan tempat sampah Newton?</a></li>
<li><a href="../id423683/index.html">Berdasarkan akal sehat: menumbuhkan DevOps dari awal</a></li>
<li><a href="../id423687/index.html">HyperX Pulsefire FPS Pro - lebih cepat, lebih kejam, lebih terjangkau</a></li>
<li><a href="../id423689/index.html">RTOS MAX - gratis? Kami berencana untuk membuka lisensi untuk penggunaan komersial gratis</a></li>
<li><a href="../id423693/index.html">Cara lain untuk menggunakan Webpack 4 dan pemisahan kode</a></li>
<li><a href="../id423695/index.html">Cara pensiun sebelum 40 tahun dengan sejuta dolar di rekening bank</a></li>
<li><a href="../id423697/index.html">Memperkenalkan Data Musim Semi JDBC</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>