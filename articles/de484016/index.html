<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üï¥Ô∏è üë®üèΩ‚Äçüéì üéôÔ∏è Die Grundlagen des Deep Learning am Beispiel des Debug-Autoencoders, Teil 1 üçò ü§µüèº üêç</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wenn Sie das Training zu Auto-Encodern auf der keras.io-Website lesen, dann gibt es eine der ersten Meldungen in etwa: In der Praxis werden Auto-Encod...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Die Grundlagen des Deep Learning am Beispiel des Debug-Autoencoders, Teil 1</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/484016/"><p>  Wenn Sie das Training zu Auto-Encodern auf der keras.io-Website lesen, dann gibt es eine der ersten Meldungen in etwa: In der Praxis werden Auto-Encoder so gut wie nie verwendet, aber in Schulungen wird h√§ufig dar√ºber gesprochen, und die Leute kommen vorbei. </p><br><p>  <em>Ihr Hauptanspruch auf Ruhm beruht darauf, dass sie in vielen online verf√ºgbaren Einf√ºhrungskursen f√ºr maschinelles Lernen vorgestellt werden.</em>  <em>Infolgedessen lieben viele Neulinge auf dem Gebiet absolut Autoencoder und k√∂nnen nicht genug von ihnen bekommen.</em>  <em>Aus diesem Grund gibt es dieses Tutorial!</em> </p><br><p>  Nichtsdestotrotz ist eine der praktischen Aufgaben, f√ºr die sie auf sich selbst angewendet werden k√∂nnen, die Suche nach Anomalien, die ich pers√∂nlich im Rahmen des Abendprojekts ben√∂tigt habe. </p><br><p>  Im Internet gibt <strong>es</strong> viele Tutorials zu Auto-Encodern, wof√ºr noch eins schreiben?  Nun, um ehrlich zu sein, gab es mehrere Gr√ºnde daf√ºr: </p><br><ul><li>  Es bestand das Gef√ºhl, dass es sich bei den Tutorials tats√§chlich um 3 oder 4 handelte, der Rest wurde in eigenen Worten umgeschrieben. </li><li>  Fast alles - auf dem leidenden MNIST'e mit Bildern 28x28; </li><li>  Meiner bescheidenen Meinung nach entwickeln sie keine Intuition dar√ºber, wie dies alles funktionieren soll, sondern bieten einfach an, es zu wiederholen. </li><li>  Und der wichtigste Faktor - pers√∂nlich, als ich MNIST durch <strong>meinen eigenen Datensatz</strong> ersetzte <strong>- war, dass alles nicht mehr funktionierte</strong> . </li></ul><br><p>  Das Folgende beschreibt meinen Weg, auf dem Zapfen gestopft werden.  Wenn Sie eines der vorgeschlagenen flachen (nicht-konvolutionellen) Modelle aus der Masse der Tutorials nehmen und es dumm kopieren, funktioniert √ºberraschenderweise nichts.  Der Zweck des Artikels besteht darin, zu verstehen, warum und, wie mir scheint, eine Art intuitives Verst√§ndnis daf√ºr zu erlangen, wie dies alles funktioniert. </p><br><p>  Ich bin kein Spezialist f√ºr maschinelles Lernen und verwende die Ans√§tze, die ich in der t√§glichen Arbeit gewohnt bin.  F√ºr erfahrene Datenwissenschaftler wird dieser ganze Artikel wahrscheinlich wild sein, aber f√ºr Anf√§nger, so scheint es mir, k√∂nnte sich etwas Neues ergeben. </p><a name="habracut"></a><br><div class="spoiler">  <b class="spoiler_title">Was f√ºr ein Projekt</b> <div class="spoiler_text"><p>  In aller K√ºrze √ºber das Projekt, obwohl der Artikel nicht √ºber ihn ist.  Es gibt einen ADS-B-Empf√§nger, der Daten von vorbeifliegenden Flugzeugen auff√§ngt und diese, Flugzeuge, zur Basis koordiniert.  Manchmal verhalten sich Flugzeuge ungew√∂hnlich - sie kreisen umher, um vor der Landung Treibstoff zu verbrennen, oder einfach private Fl√ºge fliegen an Standardrouten (Korridoren) vorbei.  Es ist interessant, von ungef√§hr tausend Flugzeugen pro Tag diejenigen zu isolieren, die sich nicht wie die anderen verhalten haben.  Ich gebe uneingeschr√§nkt zu, dass grundlegende Abweichungen einfacher berechnet werden k√∂nnen, aber ich wollte es versuchen <del>  die Magie </del>  neuronale Netze. </p></div></div><br><p>  Fangen wir an.  Ich habe einen Datensatz von 4000 Schwarzwei√übildern (64 x 64 Pixel), der ungef√§hr so ‚Äã‚Äãaussieht: </p><br><p><img src="https://habrastorage.org/webt/vw/5r/mh/vw5rmhetruoniuc7p4ksjulshde.png"></p><br><p>  Nur einige Linien auf schwarzem Hintergrund, und im 64x64-Bild sind ca. 2% der Punkte ausgef√ºllt.  Wenn man sich viele Bilder ansieht, stellt sich nat√ºrlich heraus, dass die meisten Linien ziemlich √§hnlich sind. </p><br><p>  Ich werde nicht n√§her darauf eingehen, wie der Datensatz geladen und verarbeitet wurde, da der Zweck des Artikels wiederum nicht darin besteht.  Zeigen Sie einfach ein unheimliches St√ºck Code. </p><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># only for google colab %tensorflow_version 2.x import tensorflow as tf import numpy as np import matplotlib.pyplot as plt import os import zipfile import datetime import tensorflow_addons as tfa BATCH_SIZE = 128 AUTOTUNE=tf.data.experimental.AUTOTUNE def load_image(fpath): img_raw = tf.io.read_file(fpath) img = tf.io.decode_png(img_raw, channels=1, dtype=tf.uint8) return tf.image.convert_image_dtype(img, dtype=tf.float32) ## for splitting test/train def is_test(x, y): return x % 4 == 0 def is_train(x, y): return not is_test(x,y) ## for image augmentation def random_flip_flop(img): return tf.image.random_flip_left_right(img) def transform_aug(shift_val): def random_transform(img): return tfa.image.translate(img,tf.random.uniform([2], -1*shift_val, shift_val)) return random_transform def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000, transform=0, flip=False): if cache: if isinstance(cache, str): ds = ds.cache(cache) else: ds = ds.cache() ds = ds.shuffle(buffer_size=shuffle_buffer_size) if transform != 0: ds = ds.map(transform_aug(transform)) if flip: ds = ds.map(random_flip_flop) ds = ds.repeat() ds = ds.batch(BATCH_SIZE) ds = ds.prefetch(buffer_size=AUTOTUNE) return ds def prepare_input_output(x): return (x, x) list_ds = tf.data.Dataset.list_files("/content/planes64/*") imgs_df = list_ds.map(load_image) train = imgs_df.enumerate().filter(is_train).map(lambda x,y: y) train_ds = prepare_for_training(train, transform=10, flip=True) train_ds = train_ds.map(prepare_input_output) val = imgs_df.enumerate().filter(is_test).map(lambda x, y: y) val_ds = val.map(prepare_input_output).batch(BATCH_SIZE, drop_remainder=True)</span></span></code> </pre> </div></div><br><p>  Hier ist zum Beispiel das erste mit keras.io vorgeschlagene Modell, an dem sie gearbeitet und auf mnist trainiert haben: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># this is the size of our encoded representations encoding_dim = 32 # 32 floats -&gt; compression of factor 24.5, assuming the input is 784 floats # this is our input placeholder input_img = Input(shape=(784,)) # "encoded" is the encoded representation of the input encoded = Dense(encoding_dim, activation='relu')(input_img) # "decoded" is the lossy reconstruction of the input decoded = Dense(784, activation='sigmoid')(encoded)</span></span></code> </pre> <br><p>  In meinem Fall ist das Modell folgenderma√üen definiert: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>/<span class="hljs-number"><span class="hljs-number">10</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">"sigmoid"</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Es gibt geringf√ºgige Unterschiede, die ich direkt im Modell abflache und neu formiere und die ich nicht 25 Mal, sondern nur 10 Mal "komprimiere". Dies sollte nichts beeinflussen. </p><br><p>  Als Verlustfunktion - mittlerer quadratischer Fehler - ist der Optimierer nicht grundlegend, lassen Sie Adam.  Im Folgenden trainieren wir 20 Epochen, 100 Schritte pro Epoche. </p><br><p>  Wenn Sie sich die Metriken ansehen, steht alles in Flammen.  Genauigkeit == 0,993.  Wenn Sie sich die Trainingspl√§ne ansehen - alles ist ein bisschen trauriger, wir erreichen ein Plateau in der Region der dritten √Ñra. </p><br><p><img src="https://habrastorage.org/webt/fo/rq/r7/forqr7krelrj1xq1qis_jg2jeoq.png"></p><br><p>  Wenn Sie sich das Ergebnis des Encoders direkt ansehen, erhalten Sie ein allgemein trauriges Bild (das Original ist oben und das Ergebnis der Codierung-Decodierung ist unten): </p><br><p><img src="https://habrastorage.org/webt/xo/mn/oq/xomnoqmjj80uaal0echobmal9xm.png"></p><br><p>  Wenn Sie herausfinden m√∂chten, warum etwas nicht funktioniert, sollten Sie die gesamte Funktionalit√§t in gro√üe Bl√∂cke aufteilen und diese einzeln pr√ºfen.  Also lass es uns tun. </p><br><p>  Im Original des Tutorials werden flache Daten an den Modelleingang geliefert und am Ausgang abgenommen.  Warum √ºberpr√ºfe ich nicht meine Aktionen zum Abflachen und Umformen?  Hier ist ein solches No-Op-Modell: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Ergebnis: <br><img src="https://habrastorage.org/webt/vx/z0/aj/vxz0ajadu2qiktdq5ndzj5xje8w.png"></p><br><p>  Hier gibt es nichts zu lehren.  Gleichzeitig hat sich gezeigt, dass meine Visualisierungsfunktion auch funktioniert. </p><br><p>  Versuchen Sie als n√§chstes, das Modell nicht no-op, sondern so dumm wie m√∂glich zu machen - schneiden Sie einfach die Komprimierungsebene aus und lassen Sie eine Ebene so gro√ü wie die Eingabe.  Wie sie in allen Tutorials sagen, ist es sehr wichtig, dass Ihr Modell Funktionen lernt und nicht nur eine Identit√§tsfunktion.  Genau das wollen wir erreichen, indem wir das resultierende Bild einfach an die Ausgabe weitergeben. </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">"sigmoid"</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Sie lernt etwas, Genauigkeit == 0,995 und stolpert wieder in ein Plateau. <br><img src="https://habrastorage.org/webt/ro/t6/jk/rot6jkf2ertweb7weeh8d_layui.png"></p><br><p>  Aber im Allgemeinen ist klar, dass es nicht sehr gut funktioniert.  Wie auch immer - was Sie dort lernen sollten, passieren Sie den Eingang zum Ausgang und das wars. </p><br><p>  Wenn Sie die Keras-Dokumentation √ºber dichte Schichten lesen, wird beschrieben, was sie tun: <code>output = activation(dot(input, kernel) + bias)</code> <br>  Damit die Ausgabe mit der Eingabe √ºbereinstimmt, sind zwei einfache Dinge ausreichend - Bias = 0 und Kernel - die Identit√§tsmatrix (es ist wichtig, die Matrix nicht mit Einheiten gef√ºllt zu lassen - dies sind sehr unterschiedliche Dinge).  Gl√ºcklicherweise k√∂nnen sowohl dies als auch das ganz einfach aus der Dokumentation f√ºr dieselbe <code>Dense</code> . </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation = <span class="hljs-string"><span class="hljs-string">"sigmoid"</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, kernel_initializer = tf.keras.initializers.Identity())) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Weil  Wir stellen das Gewicht sofort ein, dann k√∂nnen Sie nichts lernen - sofort ist es gut: <br><img src="https://habrastorage.org/webt/dm/tk/5a/dmtk5ardo9xksg8c5v5febzikdo.png"></p><br><p>  Aber wenn Sie mit dem Training beginnen, dann beginnt es auf den ersten Blick √ºberraschend - das Modell beginnt mit einer Genauigkeit von == 1,0, aber es f√§llt schnell ab. <br>  Bewerten Sie das Ergebnis vor dem Training: <code>8/Unknown - 1s 140ms/step - loss: 0.2488 - accuracy: 1.0000[0.24875330179929733, 1.0]</code> .  Schulung: </p><br><pre> <code class="plaintext hljs">Epoch 1/20 100/100 [==============================] - 6s 56ms/step - loss: 0.1589 - accuracy: 0.9990 - val_loss: 0.0944 - val_accuracy: 0.9967 Epoch 2/20 100/100 [==============================] - 5s 51ms/step - loss: 0.0836 - accuracy: 0.9964 - val_loss: 0.0624 - val_accuracy: 0.9958 Epoch 3/20 100/100 [==============================] - 5s 50ms/step - loss: 0.0633 - accuracy: 0.9961 - val_loss: 0.0470 - val_accuracy: 0.9958 Epoch 4/20 100/100 [==============================] - 5s 48ms/step - loss: 0.0520 - accuracy: 0.9961 - val_loss: 0.0423 - val_accuracy: 0.9961 Epoch 5/20 100/100 [==============================] - 5s 48ms/step - loss: 0.0457 - accuracy: 0.9962 - val_loss: 0.0357 - val_accuracy: 0.9962</code> </pre> <br><p>  Ja, und es ist nicht sehr klar, wir haben bereits ein ideales Modell - das Bild kommt 1 zu 1 heraus und der Verlust (mittlerer quadratischer Fehler) zeigt fast 0,25. </p><br><p>  Dies ist √ºbrigens eine h√§ufige Frage in den Foren - der Verlust sinkt, aber die Genauigkeit steigt nicht. Wie kann das sein? <br>  Hier lohnt es sich, noch einmal die Definition der Dichten Ebene in Erinnerung zu rufen: <code>output = activation(dot(input, kernel) + bias)</code> und das darin erw√§hnte Wort Aktivierung, das ich oben so erfolgreich ignoriert habe.  Mit Gewichten aus der Identit√§tsmatrix und ohne Verzerrung erhalten wir <code>output = activation(input)</code> . </p><br><p>  Eigentlich ist die Aktivierungsfunktion in unserem Quellcode schon angegeben, Sigmoid, ich hab sie ziemlich doof kopiert und das wars.  In den Tutorials wird empfohlen, es √ºberall zu verwenden.  Aber du musst es herausfinden. </p><br><p>  F√ºr den Anfang k√∂nnen Sie in der Dokumentation lesen, was sie dar√ºber schreiben: <code>The sigmoid activation: (1.0 / (1.0 + exp(-x)))</code> .  Das sagt mir pers√∂nlich nichts, denn ich bin kein einziges Mal Phantomo, um solche Grafiken in meinem Kopf zu erstellen. <br>  Aber Sie k√∂nnen mit Stiften bauen: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.ticker <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plticker range_tensor = tf.range(<span class="hljs-number"><span class="hljs-number">-4</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, dtype=tf.float32) fig, ax = plt.subplots(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(range_tensor.numpy(), tf.keras.activations.sigmoid(range_tensor).numpy()) ax.grid(which=<span class="hljs-string"><span class="hljs-string">'major'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">'-'</span></span>, linewidth=<span class="hljs-string"><span class="hljs-string">'0.5'</span></span>, color=<span class="hljs-string"><span class="hljs-string">'red'</span></span>) ax.grid(which=<span class="hljs-string"><span class="hljs-string">'minor'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">':'</span></span>, linewidth=<span class="hljs-string"><span class="hljs-string">'0.5'</span></span>, color=<span class="hljs-string"><span class="hljs-string">'black'</span></span>) ax.yaxis.set_major_locator(plticker.MultipleLocator(base=<span class="hljs-number"><span class="hljs-number">0.5</span></span>) ) plt.minorticks_on()</code> </pre> <br><p><img src="https://habrastorage.org/webt/tp/bs/1b/tpbs1bjym9vqahdihjhm1aubrne.png"></p><br><p>  Und hier wird deutlich, dass das Sigma bei Null den Wert 0,5 annimmt und in der Einheit - etwa 0,73.  Und die Punkte, die wir haben, sind entweder schwarz (0,0) oder wei√ü (1,0).  Es zeigt sich also, dass der mittlere quadratische Fehler der Identit√§tsfunktion ungleich Null bleibt. </p><br><p>  Sie k√∂nnen sich sogar die Stifte ansehen, hier ist eine Zeile vom resultierenden Bild: </p><br><pre> <code class="python hljs">array([<span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.7310586</span></span>, <span class="hljs-number"><span class="hljs-number">0.7310586</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> ], dtype=float32)</code> </pre> <br><p>  Und das ist alles in der Tat sehr cool, weil mehrere Fragen gleichzeitig auftauchen: </p><br><ul><li>  Warum war dies in der obigen Visualisierung nicht sichtbar? </li><li>  warum dann genau == 1.0, weil die originalbilder 0 und 1 sind. </li></ul><br><p>  Mit der Visualisierung ist alles √ºberraschend einfach.  Um die Bilder anzuzeigen, habe ich matplotlib: <code>plt.imshow(res_imgs[i][:, :, 0])</code> .  Und wie √ºblich, wenn Sie zur Dokumentation gehen, wird alles dort geschrieben: <code>The Normalize instance used to scale scalar data to the [0, 1] range before mapping to colors using cmap. By default, a linear scaling mapping the lowest value to 0 and the highest to 1 is used.</code> <code>The Normalize instance used to scale scalar data to the [0, 1] range before mapping to colors using cmap. By default, a linear scaling mapping the lowest value to 0 and the highest to 1 is used.</code>  Das hei√üt  Die Bibliothek normalisierte meine 0,5 und 0,73 sorgf√§ltig im Bereich von 0 bis 1. √Ñndern Sie den Code: </p><br><pre> <code class="python hljs">plt.imshow(res_imgs[i][:, :, <span class="hljs-number"><span class="hljs-number">0</span></span>], norm=matplotlib.colors.Normalize(<span class="hljs-number"><span class="hljs-number">0.0</span></span>, <span class="hljs-number"><span class="hljs-number">1.0</span></span>))</code> </pre> <br><p><img src="https://habrastorage.org/webt/rn/qn/ta/rnqntag1dohhikail8y6myq7siy.png"></p><br><p>  Und hier ist die Frage mit Genauigkeit.  Zun√§chst gehen wir aus Gewohnheit in die Dokumentation, lesen f√ºr <code>tf.keras.metrics.Accuracy</code> und dort scheint es, dass sie verst√§ndlich schreiben: </p><br><pre> <code class="plaintext hljs">For example, if y_true is [1, 2, 3, 4] and y_pred is [0, 2, 3, 4] then the accuracy is 3/4 or .75.</code> </pre> <br><p>  In diesem Fall h√§tte unsere Genauigkeit 0 betragen m√ºssen. Ich habe mich daher in der Quelle vergraben, und mir ist klar: </p><br><pre> <code class="plaintext hljs"> When you pass the strings 'accuracy' or 'acc', we convert this to one of `tf.keras.metrics.BinaryAccuracy`, `tf.keras.metrics.CategoricalAccuracy`, `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well.</code> </pre> <br><p>  Dar√ºber hinaus ist in der Dokumentation auf der Website aus irgendeinem Grund dieser Absatz nicht in der Beschreibung von <code>.compile</code> . </p><br><p>  Hier ist ein Code von <a href="https://github.com/tensorflow/tensorflow/blob/66c48046f169f3565d12e5fea263f6d731f9bfd2/tensorflow/python/keras/engine/compile_utils.py">https://github.com/tensorflow/tensorflow/blob/66c48046f169f3565d12e5fea263f6d731f9bfd2/tensorflow/python/keras/engine/compile_utils.py</a> </p><br><pre> <code class="python hljs">y_t_rank = len(y_t.shape.as_list()) y_p_rank = len(y_p.shape.as_list()) y_t_last_dim = y_t.shape.as_list()[<span class="hljs-number"><span class="hljs-number">-1</span></span>] y_p_last_dim = y_p.shape.as_list()[<span class="hljs-number"><span class="hljs-number">-1</span></span>] is_binary = y_p_last_dim == <span class="hljs-number"><span class="hljs-number">1</span></span> is_sparse_categorical = ( y_t_rank &lt; y_p_rank <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> y_t_last_dim == <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> y_p_last_dim &gt; <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> metric <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> [<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>, <span class="hljs-string"><span class="hljs-string">'acc'</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> is_binary: metric_obj = metrics_mod.binary_accuracy <span class="hljs-keyword"><span class="hljs-keyword">elif</span></span> is_sparse_categorical: metric_obj = metrics_mod.sparse_categorical_accuracy <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: metric_obj = metrics_mod.categorical_accuracy</code> </pre> <br><p>  <code>y_t</code> ist y_true oder die erwartete Ausgabe, <code>y_p</code> ist y_predicted oder das vorhergesagte Ergebnis. <br>  Wir haben das Datenformat: <code>shape=(64,64,1)</code> , es stellt sich also heraus, dass Genauigkeit als bin√§re Genauigkeit betrachtet wird.  Interesse f√ºr die Art und Weise, wie es betrachtet wird: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">binary_accuracy</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(y_true, y_pred, threshold=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.5</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> threshold = math_ops.cast(threshold, y_pred.dtype) y_pred = math_ops.cast(y_pred &gt; threshold, y_pred.dtype) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> K.mean(math_ops.equal(y_true, y_pred), axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>)</code> </pre> <br><p>  Es ist lustig, dass wir hier nur Gl√ºck haben - standardm√§√üig wird alles als Einheit betrachtet, die gr√∂√üer als 0,5 und kleiner als 0,5 ist - Null.  Die Genauigkeit unseres Identit√§tsmodells ist also hundertprozentig, obwohl die Zahlen dort √ºberhaupt nicht gleich sind.  Nun, es ist klar, dass wir, wenn wir wirklich wollen, den Schwellenwert korrigieren und die Genauigkeit auf Null reduzieren k√∂nnen, nur dass dies nicht wirklich erforderlich ist.  Dies ist eine Metrik, die sich nicht auf das Training auswirkt. Sie m√ºssen lediglich verstehen, dass Sie sie auf tausend verschiedene Arten berechnen und ganz unterschiedliche Indikatoren erhalten k√∂nnen.  Sie k√∂nnen beispielsweise verschiedene Metriken mit Stiften abrufen und unsere Daten darauf √ºbertragen: </p><br><pre> <code class="python hljs">m = tf.keras.metrics.BinaryAccuracy() m.update_state(x_batch, res_imgs) print(m.result().numpy())</code> </pre> <br><p>  Wird uns <code>1.0</code> . </p><br><p>  Und hier </p><br><pre> <code class="python hljs">m = tf.keras.metrics.Accuracy() m.update_state(x_batch, res_imgs) print(m.result().numpy())</code> </pre> <br><p>  Gibt uns <code>0.0</code> f√ºr die gleichen Daten. </p><br><p>  √úbrigens kann derselbe Code verwendet werden, um mit Verlustfunktionen zu spielen und zu verstehen, wie sie funktionieren.  Wenn Sie die Tutorials zu Auto-Encodern lesen, empfehlen sie grunds√§tzlich die Verwendung einer von zwei Verlustfunktionen: entweder Mean Squared Error oder 'binary_crossentropy'.  Sie k√∂nnen sie auch gleichzeitig anzeigen. </p><br><p>  Ich erinnere Sie daran, dass ich f√ºr <code>mse</code> bereits <code>evaluate</code> : </p><br><pre> <code class="plaintext hljs">8/Unknown - 2s 221ms/step - loss: 0.2488 - accuracy: 1.0000[0.24876083992421627, 1.0]</code> </pre> <br><p>  Das hei√üt  Verlust == 0,2488.  Mal sehen, warum das so ist.  Mir pers√∂nlich scheint es das einfachste und verst√§ndlichste zu sein: Der Unterschied zwischen y_true und y_predict wird pixelweise subtrahiert, jedes Ergebnis wird quadriert und dann der Durchschnitt gesucht. </p><br><pre> <code class="python hljs">tf.keras.backend.mean(tf.math.squared_difference(x_batch[<span class="hljs-number"><span class="hljs-number">0</span></span>], res_imgs[<span class="hljs-number"><span class="hljs-number">0</span></span>]))</code> </pre> <br><p>  Und am Ausgang: </p><br><pre> <code class="plaintext hljs">&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.24826494&gt;</code> </pre> <br><p>  Hier ist die Intuition sehr einfach - die Mehrheit der leeren Pixel, das Modell erzeugt 0,5, sie erhalten 0,25 - Quadratdifferenz f√ºr sie. </p><br><p>  Bei der bin√§ren Crossenttrtopie sind die Dinge etwas komplizierter, und es gibt ganze Artikel dar√ºber, wie das funktioniert, aber pers√∂nlich war es f√ºr mich immer einfacher, die Quellen zu lesen, und dort sieht es ungef√§hr so ‚Äã‚Äãaus: </p><br><pre> <code class="python hljs"> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> from_logits: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> isinstance(output, (ops.EagerTensor, variables_module.Variable)): output = _backtrack_identity(output) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> output.op.type == <span class="hljs-string"><span class="hljs-string">'Sigmoid'</span></span>: <span class="hljs-comment"><span class="hljs-comment"># When sigmoid activation function is used for output operation, we # use logits from the sigmoid function directly to compute loss in order # to prevent collapsing zero when training. assert len(output.op.inputs) == 1 output = output.op.inputs[0] return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output) # Compute cross entropy from probabilities. bce = target * math_ops.log(output + epsilon()) bce += (1 - target) * math_ops.log(1 - output + epsilon()) return -bce</span></span></code> </pre> <br><p>  Um ehrlich zu sein, habe ich mir sehr lange den Kopf √ºber diese wenigen Codezeilen zerbrochen.  Zun√§chst ist sofort klar, dass zwei Implementierungen funktionieren k√∂nnen: Entweder wird <code>sigmoid_cross_entropy_with_logits</code> aufgerufen, oder das letzte Zeilenpaar funktioniert.  Der Unterschied besteht darin, dass <code>sigmoid_cross_entropy_with_logits</code> mit logits (wie der Name schon sagt, doh) und der Hauptcode mit Wahrscheinlichkeiten arbeitet. </p><br><p>  Wer sind Logs?  Wenn Sie eine Million verschiedener Artikel zu diesem Thema lesen, erw√§hnen sie mathematische Definitionen, Formeln und etwas anderes.  In der Praxis scheint alles √ºberraschend einfach (korrigiere mich, wenn ich falsch liege).  Die Rohausgabe der Vorhersage sind Logits.  Nun, oder logistische Quoten, die logarithmischen Quoten, die in logistischen und logistischen Papageien gemessen werden. </p><br><div class="spoiler">  <b class="spoiler_title">Es gibt einen kleinen Exkurs - warum gibt es Logarithmen?</b> <div class="spoiler_text"><p>  Quoten sind das Verh√§ltnis der Anzahl der Ereignisse, die wir ben√∂tigen, zur Anzahl der Ereignisse, die wir nicht ben√∂tigen (im Gegensatz zur Wahrscheinlichkeit, die das Verh√§ltnis der Ereignisse ist, die wir ben√∂tigen, zur Anzahl aller Ereignisse im Allgemeinen).  Zum Beispiel - die Anzahl der Siege unserer Mannschaft an die Anzahl ihrer Niederlagen.  Und es gibt ein Problem.  Wenn wir das Beispiel mit den Siegen der Teams fortsetzen, kann unser Team in der Mitte des Verlierers sein und die Chance haben, 1/2 (eins zu zwei) zu gewinnen, und vielleicht sogar extrem viel zu verlieren - und die Chance haben, 1/100 zu gewinnen.  Und in die entgegengesetzte Richtung - mittelhoch und 2/1, steiler als die h√∂chsten Berge - und dann 100/1.  Und es stellt sich heraus, dass die gesamte Bandbreite der Verliererteams durch Zahlen von 0 bis 1 und coole Teams - von 1 bis unendlich - beschrieben wird.  Infolgedessen ist es unbequem zu vergleichen, es gibt keine Symmetrie, damit zu arbeiten ist im Allgemeinen f√ºr alle unbequem, Mathematik ist h√§sslich.  Und wenn Sie den Logarithmus der Gewinnchancen nehmen, wird alles symmetrisch: </p><br><pre> <code class="plaintext hljs">ln(1/2) == -0.69 ln(2/1) == 0.69 ln(1/100) == -4.6 ln(100/1) == 4.6</code> </pre> </div></div><br><p>  Im Fall von Tensorflow ist dies eher willk√ºrlich, da die Ausgabe des Layers streng genommen keine logarithmischen Quoten ist, sondern bereits akzeptiert wird.  Wenn der Rohwert zwischen -‚àû und + ‚àû liegt, werden die Protokolle erstellt.  Dann k√∂nnen sie in Wahrscheinlichkeiten umgewandelt werden.  Hierf√ºr gibt es zwei M√∂glichkeiten: softmax und den Sonderfall Sigmoid.  Softmax - Nehmen Sie einen Vektor von Logs und wandeln Sie sie in einen Vektor von Wahrscheinlichkeiten um, und zwar so, dass die Summe der Wahrscheinlichkeit aller Ereignisse darin 1 ergibt. Sigmoid (im Fall von tf) nimmt auch einen Vektor von Logs, wandelt jedoch jedes von ihnen unabh√§ngig voneinander in Wahrscheinlichkeiten um vom Rest. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># 1+ln(0.5) == 0.30685281944 tf.math.softmax(tf.constant([0.30685281944, 1.0, 0.30685281944])) ## &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.25, 0.5 , 0.25], dtype=float32)&gt; tf.math.sigmoid(tf.constant([0.30685281944, 1.0, 0.30685281944])) ## &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.57611686, 0.7310586 , 0.57611686], dtype=float32)&gt;</span></span></code> </pre> <br><p>  Sie k√∂nnen es so sehen.  Es gibt Klassifizierungsaufgaben f√ºr mehrere Etiketten, es gibt Klassifizierungsaufgaben f√ºr mehrere Klassen.  Multiclass - Dies ist, wenn Sie die √Ñpfel auf dem Bild oder Orangen und vielleicht sogar Ananas bestimmen m√ºssen.  Und multilabel ist, wenn es eine Obstvase auf dem Bild geben kann und Sie sagen m√ºssen, dass sie √Ñpfel und Orangen enth√§lt, aber keine Ananas.  Wenn wir Multiklassen wollen, brauchen wir Softmax, wenn wir Multilabel wollen, brauchen wir Sigmoid. <br>  Hier haben wir den Fall des Multilabels - es ist notwendig, dass jedes einzelne Pixel (Klasse) angibt, ob es installiert ist. </p><br><p>  Zur√ºck zum Tensorflow und warum es in der bin√§ren Crossentropie (zumindest in anderen Crossentropiefunktionen ist es ungef√§hr gleich) zwei globale Zweige gibt.  Crossentropy funktioniert immer mit Wahrscheinlichkeiten. Wir werden sp√§ter dar√ºber sprechen.  Dann gibt es einfach zwei M√∂glichkeiten: Entweder gehen Wahrscheinlichkeiten bereits in die Eingabe ein oder es kommen Logs in die Eingabe - und dann wird Sigmoid zuerst auf sie angewendet, um die Wahrscheinlichkeit zu ermitteln.  Es hat sich herausgestellt, dass das Anwenden von Sigmoid und das Berechnen von Crossentropy besser ist als nur das Berechnen von Crossentropy aus Wahrscheinlichkeiten (die Quelle der Funktion <code>sigmoid_cross_entropy_with_logits</code> hat eine mathematische Schlussfolgerung, und f√ºr die Neugierigen k√∂nnen Sie Google 'Numerical Stability Cross Entropy' empfehlen, auch Tensorflow-Entwickler, die Wahrscheinlichkeit nicht zu √ºberschreiten Crossentropy-Funktionen eingeben und unformatierte Protokolle zur√ºckgeben.  Nun, direkt im Code werden die Verlustfunktionen √ºberpr√ºft, wenn die letzte Schicht Sigmoid ist. Dann werden sie abgeschnitten und die Aktivierungseingabe anstelle der Ausgabe zur Berechnung <code>sigmoid_cross_entropy_with_logits</code> , wobei alles <code>sigmoid_cross_entropy_with_logits</code> wird, was in <code>sigmoid_cross_entropy_with_logits</code> ber√ºcksichtigt werden <code>sigmoid_cross_entropy_with_logits</code> . </p><br><p>  Okay, hab's gekl√§rt, jetzt bin√§r_kreuztropie.  Es gibt zwei beliebte ‚Äûintuitive‚Äú Erkl√§rungen, die die Querentropie messen. </p><br><p>  Formaler: Stellen Sie sich vor, dass es ein bestimmtes Modell gibt, das f√ºr n Klassen die Wahrscheinlichkeit ihres Auftretens kennt (y <sub>0</sub> , y <sub>1</sub> , ..., y <sub>n</sub> ).  Und jetzt im Leben ist jede dieser Klassen k <sub>n-</sub> mal aufgetreten (k <sub>1</sub> , k <sub>1</sub> , ..., k <sub>n</sub> ).  Die Wahrscheinlichkeit eines solchen Ereignisses ist das Produkt der Wahrscheinlichkeit f√ºr jede einzelne Klasse - (y <sub>1</sub> ^ k <sub>1</sub> ) (y <sub>2</sub> ^ k <sub>2</sub> ) ... (y <sub>n</sub> ^ k <sub>n</sub> ).  Grunds√§tzlich - dies ist bereits eine normale Definition der Kreuzentropie - wird die Wahrscheinlichkeit eines Datensatzes als Wahrscheinlichkeit eines anderen Datensatzes ausgedr√ºckt.  Das Problem bei dieser Definition ist, dass sie sich als von 0 bis 1 herausstellt und oft sehr klein ist und es nicht bequem ist, solche Werte zu vergleichen. <br>  Nimmt man den Logarithmus daraus, so ergibt sich k <sub>1</sub> log (y <sub>1</sub> ) + k <sub>2</sub> log (y <sub>2</sub> ) und so weiter.  Der Wertebereich wird von -‚àû bis 0. Multiplizieren Sie dies alles mit -1 / n - und der Bereich von 0 bis + ‚àû ergibt sich au√üerdem aus  es wird als die Summe der Werte f√ºr jede Klasse ausgedr√ºckt, die √Ñnderung in jeder Klasse spiegelt sich auf sehr vorhersehbare Weise im Gesamtwert wider. </p><br><p>  Einfacher ausgedr√ºckt: Die Kreuzentropie zeigt, wie viele zus√§tzliche Bits erforderlich sind, um die Stichprobe in Bezug auf das urspr√ºngliche Modell auszudr√ºcken.  Wenn wir da w√§ren, um einen Logarithmus mit Basis 2 zu erstellen, w√ºrden wir direkt Bits gehen.  Wir verwenden √ºberall nat√ºrliche Logarithmen, daher zeigen sie die Anzahl der Nat's ( <a href="https://en.wikipedia.org/wiki/Nat_(unit">https://en.wikipedia.org/wiki/Nat_(unit</a> )), nicht Bits. </p><br><p>  Die bin√§re Kreuzentropie ist wiederum ein Sonderfall der gew√∂hnlichen Kreuzentropie, wenn die Anzahl der Klassen zwei betr√§gt.  Dann haben wir genug Wissen √ºber die Wahrscheinlichkeit des Auftretens einer Klasse - y <sub>1</sub> , und die Wahrscheinlichkeit des Auftretens der zweiten Klasse wird (1-y <sub>1</sub> ) sein. </p><br><p>  Aber es scheint mir, ein wenig rutschte mich.  Ich m√∂chte Sie daran erinnern, dass wir beim letzten Versuch, einen Identit√§ts-Auto-Encoder zu erstellen, ein wundersch√∂nes Bild und sogar eine Genauigkeit von 1,0 aufwiesen. Die Zahlen erwiesen sich jedoch als schrecklich.  Im Interesse des Experiments k√∂nnen Sie noch ein paar Tests durchf√ºhren: <br>  1) Die Aktivierung kann ganz entfernt werden, es wird eine saubere Identit√§t geben <br>  2) Sie k√∂nnen andere Aktivierungsfunktionen ausprobieren, zum Beispiel das gleiche Relu </p><br><p>  Ohne Aktivierung: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, kernel_initializer=tf.keras.initializers.Identity())) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Wir bekommen das perfekte Identit√§tsmodell: </p><br><pre> <code class="python hljs">model.evaluate(x=val.map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: (x,x)).batch(BATCH_SIZE, drop_remainder=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) <span class="hljs-comment"><span class="hljs-comment"># 8/Unknown - 1s 173ms/step - loss: 0.0000e+00 - accuracy: 1.0000[0.0, 1.0]</span></span></code> </pre> <br><p>  √úbrigens f√ºhrt Training zu nichts, denn Verlust == 0.0. </p><br><p>  Jetzt mit relu.  Sein Graph sieht so aus: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.ticker <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plticker range_tensor = tf.range(<span class="hljs-number"><span class="hljs-number">-4</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, dtype=tf.float32) fig, ax = plt.subplots(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(range_tensor.numpy(), tf.keras.activations.relu(range_tensor).numpy()) ax.grid(which=<span class="hljs-string"><span class="hljs-string">'major'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">'-'</span></span>, linewidth=<span class="hljs-string"><span class="hljs-string">'0.5'</span></span>, color=<span class="hljs-string"><span class="hljs-string">'red'</span></span>) ax.grid(which=<span class="hljs-string"><span class="hljs-string">'minor'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">':'</span></span>, linewidth=<span class="hljs-string"><span class="hljs-string">'0.5'</span></span>, color=<span class="hljs-string"><span class="hljs-string">'black'</span></span>) ax.yaxis.set_major_locator(plticker.MultipleLocator(base=<span class="hljs-number"><span class="hljs-number">1</span></span>) ) plt.minorticks_on()</code> </pre> <br><p><img src="https://habrastorage.org/webt/wq/ph/iw/wqphiwwmhtmxwfogld4nstyfp9w.png"></p><br><p>  Unter null - null, √ºber - y = x, d.h.  Theoretisch sollten wir den gleichen Effekt erzielen wie ohne Aktivierung - ein ideales Modell. </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, kernel_initializer=tf.keras.initializers.Identity())) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.compile(optimizer=<span class="hljs-string"><span class="hljs-string">"adam"</span></span>, loss=<span class="hljs-string"><span class="hljs-string">"binary_crossentropy"</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">"accuracy"</span></span>]) model.evaluate(x=val.map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: (x,x)).batch(BATCH_SIZE, drop_remainder=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) <span class="hljs-comment"><span class="hljs-comment"># 8/Unknown - 1s 158ms/step - loss: 0.0000e+00 - accuracy: 1.0000[0.0, 1.0]</span></span></code> </pre> <br><p>  Okay, wir haben das Identit√§tsmodell herausgefunden, auch wenn ein Teil der Theorie klarer wurde.  Versuchen wir nun, dasselbe Modell so zu trainieren, dass es zur Identit√§t wird. </p><br><p>  Zum Spa√ü werde ich dieses Experiment mit drei Aktivierungsfunktionen durchf√ºhren.  Zun√§chst - relu, weil es sich schon fr√ºher gezeigt hat (alles ist wie vorher, aber der kernel_initializer ist entfernt, also standardm√§√üig <code>glorot_uniform</code> ): </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Es lernt wunderbar: </p><br><p><img src="https://habrastorage.org/webt/hv/wz/oo/hvwzoodkp6dxopq7pjkopeeuorw.png"></p><br><p>  Das Ergebnis war ziemlich gut, Genauigkeit: 0,9999, Verlust (mse): 2e-04 nach 20 Epochen und Sie k√∂nnen weiter trainieren. </p><br><p><img src="https://habrastorage.org/webt/4y/io/jt/4yiojttsafqnf796ha6all0qmsy.png"></p><br><p>  Als n√§chstes versuchen Sie es mit Sigmoid: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Ich habe bereits zuvor etwas √Ñhnliches gelehrt, mit dem einzigen Unterschied, dass die Vorurteile hier deaktiviert sind.  Er studiert meeeely, geht auf ein Plateau in der Region der 50. √Ñra, Genauigkeit: 0,9970, Verlust: 0,01 nach 60 Epochen. </p><br><p>  Das Ergebnis ist wieder nicht beeindruckend: </p><br><p><img src="https://habrastorage.org/webt/_7/fl/o_/_7flo_xh5gkgh8oennqthe2ymhk.png"></p><br><p>  Nun, √ºberpr√ºfen Sie auch Tanh: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'tanh'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Das Ergebnis ist vergleichbar mit relu - Genauigkeit: 0,9999, Verlust: 6e-04 nach 20 Epochen, und Sie k√∂nnen weiter trainieren: </p><br><p><img src="https://habrastorage.org/webt/m4/ib/xc/m4ibxctlge5cxs7eqozjt5uwfsc.png"></p><br><p><img src="https://habrastorage.org/webt/sk/r3/p8/skr3p8etvlatcf-mnc6q9sabtfk.png"></p><br><p>  Tats√§chlich qu√§lt mich die Frage, ob etwas getan werden kann, um Sigmoid zu einem vergleichbaren Ergebnis zu bringen.  Ausschliesslich aus sportlichem Interesse. </p><br><p>  Sie k√∂nnen beispielsweise versuchen, BatchNormalization hinzuzuf√ºgen: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)) model.add(tf.keras.layers.BatchNormalization()) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Und dann passiert eine Art Magie.  In der 13. √Ñra Genauigkeit: 1,0.  Und die feurigen Ergebnisse: </p><br><p><img src="https://habrastorage.org/webt/6x/md/di/6xmddijppdnstc8ire1mxlcwkca.png"></p><br><p>  III ... auf diesem Klippenh√§nger beende ich den ersten Teil, weil der Text zu dumm ist und nicht klar ist, ob jemand ihn braucht oder nicht.  Im zweiten Teil werde ich verstehen, was Magie passiert ist, mit verschiedenen Optimierern experimentieren, versuchen, einen ehrlichen Encoder-Decoder zu bauen, meinen Kopf auf den Tisch zu schlagen.  Ich hoffe jemand war interessiert und hilfsbereit. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de484016/">https://habr.com/ru/post/de484016/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de484004/index.html">@ Pythonetc Dezember 2019</a></li>
<li><a href="../de484006/index.html">Tipps und Tricks aus meinem Telegrammkanal @pythonetc, Dezember 2019</a></li>
<li><a href="../de484008/index.html">Was ist es, ein Teamleiter zu sein?</a></li>
<li><a href="../de484012/index.html">Optimieren Sie den Schreibvorgang in einem Blocknotizbuch</a></li>
<li><a href="../de484014/index.html">10 SEO-Mythen, die 2020 aufgegeben werden m√ºssen</a></li>
<li><a href="../de484018/index.html">IT-technische Seite des Segelsports</a></li>
<li><a href="../de484020/index.html">Wen m√∂chten Sie mit Ihren Terminen beeindrucken?</a></li>
<li><a href="../de484026/index.html">Teil 6: MemTest86 + nach RISC-V portieren</a></li>
<li><a href="../de484028/index.html">Horseshoe Bend - Cabrio-Tablet mit Klappdisplay</a></li>
<li><a href="../de484034/index.html">Umsetzung des Arbeitsschemas der gezielten Warenlagerung auf Basis der Lagerabrechnungseinheit 1C Integrated Automation 2</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>