<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>📙 👩🏼‍🤝‍👨🏽 🦆 Monitoring dan Kubernetes (review dan laporan video) 🥖 👐🏾 👦</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pada tanggal 28 Mei, pada konferensi RootConf 2018, yang berlangsung sebagai bagian dari festival RIT ++ 2018, di bagian “Penebangan dan Pemantauan”, ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Monitoring dan Kubernetes (review dan laporan video)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/412901/">  Pada tanggal 28 Mei, pada konferensi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">RootConf</a> 2018, yang berlangsung sebagai bagian dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">festival</a> RIT ++ 2018, di bagian “Penebangan dan Pemantauan”, sebuah laporan “Pemantauan dan Kubernet” disampaikan.  Ini menceritakan tentang pengalaman pengaturan pemantauan dengan Prometheus, yang diperoleh oleh Flant sebagai hasil dari operasi puluhan proyek Kubernetes dalam produksi. <br><br><img src="https://habrastorage.org/webt/pm/o9/dm/pmo9dmnz9jf7b-yhej9shmir92q.jpeg"><br><br>  Secara tradisi, kami senang menyajikan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><b>video dengan laporan</b></a> (sekitar satu jam, <b>jauh lebih</b> informatif <b>daripada</b> artikel) dan penekanan utama dalam bentuk teks.  Ayo pergi! <a name="habracut"></a><br><br><h2>  Apa itu pemantauan? </h2><br>  Ada banyak sistem pemantauan: <br><br><img src="https://habrastorage.org/webt/pa/07/i0/pa07i0ojuohdqwxbk6lvf86lgls.png"><br><br>  Tampaknya mengambil dan menginstal salah satunya - itu saja, pertanyaannya sudah ditutup.  Tetapi latihan menunjukkan bahwa ini tidak benar.  Dan inilah alasannya: <br><br><ol><li>  <b>Speedometer menunjukkan kecepatan</b> .  Jika kita mengukur kecepatan satu menit sekali dengan speedometer, maka kecepatan rata-rata, yang kita hitung berdasarkan data ini, tidak akan bertepatan dengan data odometer.  Dan jika dalam kasus mobil ini jelas, maka ketika menyangkut banyak, banyak indikator untuk server, kita sering melupakannya. <br><img src="https://habrastorage.org/webt/s9/13/fv/s913fvrbguhqbp3iuuobwrmazt8.png"><br>  <i>Apa yang kami ukur dan bagaimana kami bepergian</i> </li><li>  <b>Lebih banyak pengukuran</b> .  Semakin banyak indikator yang kita dapatkan, diagnosis masalah akan lebih akurat ... tetapi hanya dengan syarat bahwa ini adalah indikator yang benar-benar bermanfaat, dan bukan hanya segala sesuatu yang Anda kumpulkan. </li><li>  <b>Lansiran</b> .  Tidak ada yang rumit dalam mengirim peringatan.  Namun, dua masalah umum: a) alarm palsu terjadi sangat sering sehingga kita berhenti merespons peringatan apa pun, b) peringatan datang pada saat terlambat (semuanya sudah meledak).  Dan untuk mencapai dalam memantau bahwa masalah ini tidak muncul adalah seni asli! </li></ol><br>  Pemantauan adalah kue tiga lapis, yang masing-masing sangat penting: <br><br><ol><li>  Pertama-tama, ini adalah sistem yang memungkinkan Anda untuk <b>mencegah kecelakaan</b> , <b>memberi tahu tentang kecelakaan</b> (jika tidak dapat dicegah) dan melakukan <b>diagnosis</b> masalah dengan <b>cepat</b> . </li><li>  Apa yang dibutuhkan untuk ini?  <b>Data akurat</b> , <b>bagan berguna</b> (lihat mereka dan pahami di mana masalahnya), <b>peringatan yang relevan</b> (tiba pada waktu yang tepat dan mengandung informasi yang jelas). </li><li>  Dan agar semua ini berfungsi, diperlukan <b>sistem pemantauan</b> . </li></ol><br>  Pengaturan yang tepat dari sistem pemantauan yang benar-benar berfungsi bukanlah tugas yang mudah, membutuhkan pendekatan yang bijaksana untuk implementasi bahkan tanpa Kubernetes.  Tapi apa yang terjadi dengan penampilannya? <br><br><h2>  Spesifik pemantauan Kubernetes </h2><br><h3>  1  Lebih besar dan lebih cepat </h3><br>  Kubernet berubah banyak karena infrastrukturnya semakin besar dan cepat.  Jika sebelumnya, dengan server besi biasa, jumlahnya sangat terbatas, dan proses penambahannya sangat lama (butuh berhari-hari atau berminggu-minggu), maka dengan mesin virtual jumlah entitas meningkat secara signifikan, dan waktu pengenalan mereka ke pertempuran berkurang menjadi detik. <br><br>  Dengan Kubernetes, jumlah entitas telah bertambah dengan urutan besarnya, penambahan mereka sepenuhnya otomatis (manajemen konfigurasi diperlukan, karena tanpa deskripsi pod baru tidak dapat dibuat), seluruh infrastruktur menjadi sangat dinamis (misalnya, pod dihapus dan dirilis setiap kali dibuat lagi). <br><br><img src="https://habrastorage.org/webt/01/fv/cf/01fvcfbc_i2roepw7nh0q6womsk.png"><br><br>  Apa yang berubah? <br><br><ol><li>  Pada prinsipnya, kita berhenti melihat pod atau wadah individu - sekarang kita <b>hanya</b> tertarik <b>pada kelompok objek</b> . </li><li>  <b>Service Discovery menjadi sangat wajib</b> , karena "kecepatan" sudah sedemikian sehingga, pada prinsipnya, kami tidak dapat secara manual memulai / menghapus entitas baru, seperti sebelumnya, ketika server baru dibeli. </li><li>  <b>Jumlah data tumbuh secara signifikan</b> .  Jika metrik sebelumnya dikumpulkan dari server atau mesin virtual, sekarang dari pod, jumlahnya jauh lebih besar. </li><li>  Perubahan paling menarik yang saya sebut " <b>aliran metadata</b> " dan saya akan memberi tahu Anda lebih banyak tentang hal itu. </li></ol><br>  Saya akan mulai dengan perbandingan ini: <br><br><ul><li>  Ketika Anda mengirim anak Anda ke taman kanak-kanak, ia akan diberi kotak pribadi, yang ditugaskan kepadanya untuk tahun berikutnya (atau lebih) dan di mana namanya disebutkan. </li><li>  Ketika Anda datang ke kolam, loker Anda tidak ditandatangani dan diberikan kepada Anda untuk satu "sesi". </li></ul><br>  Jadi <b>sistem pemantauan klasik berpikir bahwa mereka adalah taman kanak-kanak</b> , bukan kolam: mereka menganggap bahwa objek pemantauan datang kepada mereka selamanya atau untuk waktu yang lama, dan memberi mereka loker yang sesuai.  Tetapi kenyataan di Kubernet berbeda: polong datang ke kolam (mis. Diciptakan), berenang di dalamnya (sampai penyebaran baru) dan meninggalkan (dihancurkan) - semua ini terjadi dengan cepat dan teratur.  Dengan demikian, sistem pemantauan harus memahami bahwa objek yang dipantau hidup singkat, dan harus dapat sepenuhnya melupakannya pada waktu yang tepat. <br><br><h3>  2  Realitas paralel ada </h3><br>  Poin penting lainnya - dengan munculnya Kubernetes, kami secara bersamaan memiliki dua "realitas": <br><br><ol><li>  Dunia Kubernetes di mana ada ruang nama, penyebaran, pod, wadah.  Ini adalah dunia yang kompleks, tetapi logis, terstruktur. </li><li>  Dunia "fisik", terdiri dari banyak (secara harfiah - tumpukan) kontainer pada setiap simpul. </li></ol><br><img src="https://habrastorage.org/webt/1p/wc/xj/1pwcxjjt1xbgwqufldfm1upua10.png"><br>  <i>Satu dan wadah yang sama di Kubernet “realitas virtual” (di atas) dan dunia fisik simpul (di bawah)</i> <br><br>  Dan dalam proses pemantauan, kita perlu terus-menerus <b>membandingkan dunia fisik wadah dengan realitas Kubernet</b> .  Misalnya, ketika kita melihat beberapa namespace, kita ingin tahu di mana semua wadahnya (atau wadah salah satu perapiannya) berada.  Tanpa ini, peringatan tidak akan visual dan nyaman digunakan - karena penting bagi kita untuk memahami objek apa yang mereka laporkan. <br><br><img src="https://habrastorage.org/webt/2p/n7/3t/2pn73tojozunuxdnjiba8yqw7-y.png"><br>  <i>Berbagai jenis lansiran - yang terakhir lebih visual dan nyaman digunakan daripada yang lainnya</i> <br><br>  <b>Kesimpulan di</b> sini adalah: <br><br><ol><li>  Sistem pemantauan harus menggunakan primitif bawaan Kubernetes. </li><li>  Ada lebih dari satu kenyataan: sering kali masalah tidak terjadi dengan perapian, tetapi dengan simpul tertentu, dan kita perlu terus-menerus memahami seperti apa "realitas" yang mereka hadapi. </li><li>  Dalam satu cluster, sebagai aturan, ada beberapa lingkungan (selain produksi), yang berarti bahwa ini harus diperhitungkan (misalnya, tidak menerima peringatan di malam hari tentang masalah pada dev). </li></ol><br>  Jadi, kami memiliki tiga syarat yang diperlukan agar semuanya berjalan: <br><br><ol><li>  Kami sangat memahami apa itu pemantauan. </li><li>  Kami tahu tentang fitur-fiturnya, fitur yang muncul dengan Kubernetes. </li><li>  Kami mengadopsi Prometheus. </li></ol><br>  Jadi, untuk benar-benar berhasil, tetap hanya membuat <i>banyak</i> usaha!  Ngomong-ngomong, mengapa tepatnya Prometheus? .. <br><br><h2>  Prometheus </h2><br>  Ada dua cara untuk menjawab pertanyaan tentang memilih Prometheus: <br><br><ol><li>  Lihat siapa dan apa yang biasanya digunakan untuk memantau Kubernet. </li><li>  Pertimbangkan keunggulan teknisnya. </li></ol><br>  Untuk yang pertama, saya menggunakan data survei dari The New Stack (dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">The State of the Kubernetes Ecosystem</a> e-book), yang menurutnya Prometheus paling tidak lebih populer daripada solusi lain (baik Open Source, dan SaaS), dan jika Anda lihat, ini memiliki keunggulan statistik lima kali lipat. . <br><br>  Sekarang mari kita lihat bagaimana Prometheus bekerja, bersamaan dengan bagaimana kapabilitasnya bergabung dengan Kubernetes dan menyelesaikan tantangan terkait. <br><br><h2>  Bagaimana Prometheus terstruktur? </h2><br>  Prometheus ditulis dalam Go dan didistribusikan sebagai file biner tunggal, di mana semuanya built-in.  Algoritma dasar untuk operasinya adalah sebagai berikut: <br><br><img src="https://habrastorage.org/webt/nh/xt/hp/nhxthp-dm3wveymrgbejbanainw.png"><br><br><ul><li>  <b>Kolektor</b> membaca <b>tabel target</b> , mis.  daftar objek yang akan dipantau dan frekuensi pemungutan suaranya (secara default - 60 detik). </li><li>  Setelah itu, kolektor mengirimkan permintaan HTTP ke setiap pod yang Anda <b>butuhkan</b> dan menerima respons dengan serangkaian metrik - mungkin ada seratus, seribu, sepuluh ribu ... Setiap metrik memiliki nama, nilai, dan <b>label</b> . </li><li>  Respons yang diterima <b>disimpan</b> dalam basis data <b>TSDB</b> , di mana cap waktu tanda terima dan label objek dari mana ia diambil ditambahkan ke data metrik yang diterima. <br><br><div class="spoiler">  <b class="spoiler_title">Secara singkat tentang TSDB</b> <div class="spoiler_text">  <i>TSDB - basis data deret waktu (DB untuk deret waktu) on Go, yang memungkinkan Anda menyimpan data selama beberapa hari dan melakukannya dengan sangat efisien (dalam ukuran, memori, dan input / output).</i>  <i>Data disimpan hanya secara lokal, tanpa pengelompokan dan replikasi, yang merupakan nilai tambah (ini berfungsi dengan mudah dan terjamin) dan nilai minus (tidak ada penskalaan horizontal penyimpanan), tetapi dalam kasus pengabaian Prometheus dilakukan dengan baik, federasi - lebih lanjut tentang ini nanti.</i> <br></div></div></li><li>  Disajikan dalam skema, <b>Service Discovery</b> adalah mesin penemuan layanan yang dibangun di Prometheus yang memungkinkan Anda menerima data "dari kotak" (melalui API Kubernetes) untuk membuat tabel sasaran. </li></ul><br>  Seperti apa tabel ini?  Untuk setiap entri, ia menyimpan URL yang digunakan untuk mendapatkan metrik, frekuensi panggilan dan label. <br><br><img src="https://habrastorage.org/webt/xu/9m/c_/xu9mc_ikwk-sdzkrs_fjt6lsfj0.png"><br><br>  Label digunakan untuk penjajaran "dunia" Kubernetes dengan fisik.  Misalnya, untuk menemukan pod dengan Redis, kita perlu memiliki nilai namespace, layanan (digunakan alih-alih penyebaran karena fitur teknis untuk kasus tertentu) dan pod aktual.  Dengan demikian, 3 label ini disimpan dalam entri tabel tujuan untuk metrik Redis. <br><br>  Entri-entri dalam tabel ini dibentuk berdasarkan <code>scrape_configs</code> Prometheus di mana objek pemantauan dijelaskan: di bagian <code>scrape_configs</code> , <code>scrape_configs</code> ditentukan, yang menunjukkan label mana yang akan mencari objek untuk dipantau, cara memfilternya, dan label mana yang akan direkam. <br><br><h2>  Data apa yang dikumpulkan Kubernet? </h2><br><ul><li>  Pertama, <b>penyihir</b> di Kubernetes cukup rumit - dan sangat penting untuk memantau keadaan kerjanya (kube-apiserver, kube-controller-manager, kube-scheduler, kube-etcd3 ...), terlebih lagi, ia terikat pada node cluster. </li><li>  Kedua, penting untuk mengetahui apa yang sedang terjadi di <b>dalam Kubernetes</b> . Untuk melakukan ini, kami mendapatkan data dari: <br><ul><li>  <i>kubelet</i> - komponen Kubernetes ini berjalan di setiap node cluster (dan terhubung ke wizard K8s);  cAdvisor dibangun di dalamnya (semua metrik menurut wadah), dan juga menyimpan informasi tentang Persistent Volume yang terhubung; </li><li>  <i>kube-state-metrics</i> - sebenarnya, ini adalah Eksportir Prometheus untuk API Kubernetes (memungkinkan Anda untuk mendapatkan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">informasi tentang objek</a> yang disimpan di Kubernet: pod, layanan, penyebaran, dll.; misalnya, kami tidak akan tahu tanpanya wadah atau status perapian); </li><li>  <i>simpul-eksportir</i> - memberikan informasi tentang simpul itu sendiri, metrik dasar pada sistem Linux (cpu, diskstats, meminfo, dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sebagainya</a> ). </li></ul></li><li>  Berikutnya adalah <b>komponen Kubernetes</b> , seperti kube-dns, kube-prometheus-operator dan kube-prometheus, ingress-nginx-controller, dll. </li><li>  Kategori objek berikutnya untuk dipantau sebenarnya adalah <b>perangkat lunak yang</b> diluncurkan di Kubernetes.  Ini adalah layanan server yang khas seperti nginx, php-fpm, Redis, MongoDB, RabbitMQ ... Kami melakukannya sendiri sehingga ketika kami menambahkan label tertentu ke layanan, secara otomatis mulai mengumpulkan data yang diperlukan, yang menciptakan dasbor saat ini di Grafana. </li><li>  Akhirnya, kategori untuk semua yang lain adalah <b>kebiasaan</b> .  Alat Prometheus memungkinkan Anda untuk mengotomatiskan koleksi metrik sewenang-wenang (misalnya, jumlah pesanan) dengan hanya menambahkan satu label <code>prometheus-custom-target</code> ke deskripsi layanan. </li></ul><br><img src="https://habrastorage.org/webt/0s/e6/c3/0se6c3ygspys909x3ju6m9aq7uu.gif"><br><h2>  Grafik </h2><br>  Data yang diterima <i>(dijelaskan di atas)</i> digunakan untuk mengirim peringatan dan membuat grafik.  Kami menggambar grafik menggunakan <b>Grafana</b> .  Dan "detail" penting di sini adalah <b>PromQL</b> , bahasa permintaan Prometheus yang terintegrasi sempurna dengan Grafana. <br><br><img src="https://habrastorage.org/webt/_a/sl/7w/_asl7wbfscz1eyxstacdij_liio.png"><br><br>  Ini cukup sederhana dan nyaman untuk sebagian besar tugas <i>(tetapi, misalnya, bergabung dengan bergabung di dalamnya sudah tidak nyaman, tetapi Anda masih harus melakukannya)</i> .  PromQL memungkinkan Anda untuk menyelesaikan semua tugas yang diperlukan: dengan cepat memilih metrik yang diperlukan, membandingkan nilai, melakukan operasi aritmatika pada mereka, grup, bekerja dengan interval waktu dan banyak lagi.  Sebagai contoh: <br><br><img src="https://habrastorage.org/webt/3h/1d/0v/3h1d0vskm__dosoxpx1rzd5jis8.png"><br><br>  Selain itu, Prometheus memiliki <b>Evaluator</b> , yang, menggunakan PromQL yang sama, dapat mengakses TSDB dengan frekuensi yang ditentukan.  Kenapa ini?  Contoh: mulai mengirimkan peringatan jika kami memiliki, berdasarkan metrik yang tersedia, kesalahan 500 pada server web selama 5 menit terakhir.  Selain label yang ada dalam permintaan, Evaluator menambahkan yang tambahan (seperti yang kita konfigurasi) ke data untuk peringatan, setelah itu mereka dikirim dalam format JSON ke komponen Prometheus lain - <b>Alertmanager</b> . <br><br>  Prometheus secara berkala (sekali setiap 30 detik) mengirimkan peringatan kepada Alertmanager, yang menduplikasi mereka (setelah menerima peringatan pertama, itu akan mengirimnya, dan yang berikutnya tidak akan dikirim lagi). <br><br><img src="https://habrastorage.org/webt/ju/0e/lg/ju0elg1u57wtxfjopy6wz38vtfg.png"><br><br>  <i><b>Catatan</b> : Kami tidak menggunakan Alertmanager di rumah, tetapi mengirim data dari Prometheus langsung ke sistem kami, yang digunakan petugas kami untuk bekerja, tetapi ini tidak masalah dalam skema umum.</i> <br><br><h2>  Prometheus at Kubernetes: The Big Picture </h2><br>  Sekarang mari kita lihat bagaimana seluruh bundel Prometheus ini bekerja di dalam Kubernetes: <br><br><img src="https://habrastorage.org/webt/w-/gu/ue/w-guue_2b8q12romg-qv7uw2hpi.png"><br><br><ul><li>  Kubernetes memiliki namespace sendiri untuk Prometheus <i>(kami memiliki <code>kube-prometheus</code> dalam ilustrasi)</i> . </li><li>  Namespace ini meng-host pod dengan instalasi Prometheus, yang setiap 30 detik mengumpulkan metrik dari semua target yang diterima melalui Service Discovery di cluster. </li><li>  Ia juga menampung pod dengan Alertmanager, yang menerima data dari Prometheus dan mengirimkan peringatan <i>(ke surat, Slack, PagerDuty, WeChat, integrasi pihak ketiga, dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sebagainya</a> )</i> . </li><li>  Prometheus menghadapi penyeimbang beban - Layanan reguler di Kubernetes - dan Grafana mengakses Prometheus melalui itu.  Untuk <b>memastikan toleransi kesalahan, Prometheus</b> menggunakan beberapa pod dengan instalasi Prometheus, yang masing-masing mengumpulkan semua data dan menyimpannya dalam TSDB-nya.  Melalui penyeimbang, Grafana memukul salah satunya. </li><li>  Jumlah pod dengan Prometheus dikendalikan oleh pengaturan <i>StatefulSet</i> - kami biasanya membuat tidak lebih dari dua pod, tetapi Anda dapat menambah jumlah ini.  Demikian pula, melalui StatefulSet, seorang Alertmanager juga dikerahkan, untuk toleransi kesalahan yang membutuhkan setidaknya 3 polong (karena kuorum diperlukan untuk membuat keputusan tentang mengirimkan peringatan). <br></li></ul><br>  Apa yang hilang di sini? .. <br><br><h2>  Federasi untuk Prometheus </h2><br>  Ketika data dikumpulkan setiap 30 (atau 60) detik, tempat untuk menyimpannya dengan sangat cepat berakhir, dan bahkan lebih buruk, itu membutuhkan banyak sumber daya komputasi (ketika menerima dan memproses sejumlah besar titik dari TSDB).  Tetapi kami ingin menyimpan dan memiliki kemampuan untuk mengunduh informasi untuk <b>interval waktu besar <i>dan</i> e waktu</b> .  Bagaimana cara mencapai ini? <br><br>  Cukup menambahkan <b>satu instalasi lagi Prometheus</b> (kami menyebutnya <i>jangka panjang</i> ) ke skema umum, di mana Service Discovery dinonaktifkan, dan di tabel tujuan hanya ada catatan statis yang mengarah ke Prometheus <i>utama</i> ( <i>utama</i> ).  <b>Ini dimungkinkan berkat <a href="">federasi</a></b> : Prometheus memungkinkan Anda mengembalikan nilai terbaru semua metrik dalam satu kueri.  Dengan demikian, instalasi pertama Prometheus masih berfungsi (mengakses setiap 60 atau, misalnya, 30 detik) ke semua target di kluster Kubernetes, dan yang kedua - sekali setiap 5 menit, menerima data dari yang pertama dan menyimpannya untuk dapat menonton data untuk jangka waktu yang lama ( tetapi tanpa detail yang dalam). <br><br><img src="https://habrastorage.org/webt/zc/uj/k3/zcujk3s5oxler1lhwaaswadmiyy.png"><br>  <i>Instalasi Prometheus kedua tidak perlu Service Discovery, dan tabel sasaran akan terdiri dari satu baris</i> <br><br><img src="https://habrastorage.org/webt/od/zr/ow/odzrowlvdyq3qmhfldvy085naou.png"><br>  <i>Seluruh gambar dengan instalasi Prometheus dari dua jenis: utama (atas) dan jangka panjang</i> <br><br>  Sentuhan terakhir adalah <b>menghubungkan Grafana</b> ke instalasi Prometheus dan membuat dasbor dengan cara khusus sehingga Anda dapat beralih di antara sumber data ( <i>utama</i> atau <i>jangka panjang</i> ).  Untuk melakukan ini, menggunakan mesin templat, gantikan variabel <code>$prometheus</code> alih-alih sumber data di semua panel. <br><br><img src="https://habrastorage.org/webt/ju/v3/9u/juv39un0v2qdtwegrt07qpg-1yi.png"><br><br><h2>  Apa lagi yang penting dalam grafik? </h2><br>  Dua poin utama yang perlu dipertimbangkan ketika mengatur jadwal adalah dukungan untuk primitif Kubernetes dan kemampuan untuk dengan cepat menelusuri dari gambaran keseluruhan (atau "tampilan" yang lebih rendah) ke layanan tertentu dan sebaliknya. <br><br>  Dukungan untuk primitif (ruang nama, polong, dll.) Telah disebutkan - ini adalah kondisi yang diperlukan pada prinsipnya untuk pekerjaan yang nyaman dalam realitas Kubernetes.  Dan berikut adalah contoh tentang menelusuri: <br><br><ul><li>  Kami melihat grafik konsumsi sumber daya oleh tiga proyek (mis., Tiga ruang nama) - kami melihat bahwa bagian utama CPU (atau memori, atau jaringan, ...) jatuh pada proyek A. </li><li>  Kami melihat grafik yang sama, tetapi sudah untuk layanan Proyek A: yang mana dari mereka yang paling banyak mengkonsumsi CPU? </li><li>  Kami beralih ke bagan layanan yang diinginkan: pod mana yang “disalahkan”? </li><li>  Kita beralih ke grafik pod yang diinginkan: wadah mana yang harus "disalahkan"?  Inilah tujuan yang diinginkan! </li></ul><br><img src="https://habrastorage.org/webt/fq/vb/ly/fqvblyn0wdnoqqzqgwi0ublftjy.png"><br><h2>  Ringkasan </h2><br><ul><li>  Nyatakan secara akurat untuk diri Anda sendiri apa itu pemantauan.  <i>(Biarkan "kue tiga lapis" berfungsi sebagai pengingat ini ... serta fakta bahwa memanggang dengan kompeten tidak mudah bahkan tanpa Kubernetes!)</i> </li><li>  Ingat bahwa Kubernetes menambahkan spesifik wajib: pengelompokan target, penemuan layanan, sejumlah besar data, aliran metadata.  Selain itu: <br><ul><li>  ya, beberapa dari mereka secara ajaib ("out of the box") diselesaikan di Prometheus; </li><li>  namun, masih ada bagian lain yang perlu dipantau secara independen dan dipertimbangkan. </li></ul></li></ul><br>  Dan ingat bahwa <b>konten lebih penting daripada sistem</b> , mis.  bagan dan peringatan yang benar adalah yang utama, dan bukan Prometheus (atau perangkat lunak serupa lainnya). <br><br><img src="https://habrastorage.org/webt/ml/61/ou/ml61oub7wmavnyfdmjepkm7bd-y.png"><br><br><h2>  Video dan slide </h2><br>  Video dari kinerja (sekitar satu jam): <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/zj6SlzzBRaA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  Penyajian laporan: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h2>  PS </h2><br>  Laporan lain di blog kami: <br><br><ul><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Database dan Kubernetes</a> ";  <i>(Dmitry Stolyarov; 8 November 2018 di HighLoad ++)</i> ; </li><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Praktik CI / CD Terbaik dengan Kubernetes dan GitLab</a> ”;  <i>(Dmitry Stolyarov; 7 November 2017 di HighLoad ++)</i> ; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pengalaman kami dengan Kubernetes dalam proyek-proyek kecil</a> ";  <i>(Dmitry Stolyarov; 6 Juni 2017 di RootConf)</i> ; </li><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kami mengumpulkan gambar Docker untuk CI / CD dengan cepat dan nyaman dengan dapp</a> ” <i>(Dmitry Stolyarov; 8 November 2016 di HighLoad ++)</i> ; </li><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Praktik Pengiriman Berkelanjutan dengan Docker</a> ” <i>(Dmitry Stolyarov; 31 Mei 2016 di RootConf)</i> . </li></ul><br>  Anda mungkin juga tertarik dengan publikasi berikut: <br><br><ul><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Perangkat dan mekanisme operasi Operator Prometheus di Kubernetes</a> "; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Memantau dengan Prometheus di Kubernetes dalam 15 menit</a> "; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Infrastruktur dengan Kubernet sebagai layanan yang terjangkau</a> ." </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id412901/">https://habr.com/ru/post/id412901/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id412891/index.html">Citrix XenServer 7.0 I / O tidak dioptimalkan Agen Manajemen tidak diinstal</a></li>
<li><a href="../id412893/index.html">Untuk mencapai programmer senior dalam empat tahun: metode "Sekolah 21"</a></li>
<li><a href="../id412895/index.html">Vesta Matveeva: perang melawan kejahatan dunia maya adalah pilihan moral</a></li>
<li><a href="../id412897/index.html">Memantau Produk Atlassian dengan Prometheus</a></li>
<li><a href="../id412899/index.html">Weekend Reading: 30 materi tentang suara, sejarah merek audio dan industri film</a></li>
<li><a href="../id412903/index.html">Bagaimana kami melukis Habr</a></li>
<li><a href="../id412905/index.html">Tentang LL Parsing: Suatu Pendekatan untuk Parsing Melalui Konsep Pemotongan Tali</a></li>
<li><a href="../id412911/index.html">Pengembang berbicara tentang fitur yang terpotong dari game</a></li>
<li><a href="../id412913/index.html">"Baikal-T1" mulai dijual seharga 3990 rubel</a></li>
<li><a href="../id412915/index.html">Penentuan kerapatan gas dari hasil pengukuran tekanan dan suhu dengan sensor Arduino</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>