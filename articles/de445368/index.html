<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>✊🏾 🤑 🗓️ Laden Sie ein Spiel mit ein paar Hunderttausenden von virtuellen Benutzern 🧓🏿 💗 🙍🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Habr! 

 Ich arbeite für eine Spielefirma, die Online-Spiele entwickelt. Gegenwärtig sind alle unsere Spiele in viele "Märkte" unterteilt (ein "...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Laden Sie ein Spiel mit ein paar Hunderttausenden von virtuellen Benutzern</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/445368/">  Hallo Habr! <br><br>  Ich arbeite für eine Spielefirma, die Online-Spiele entwickelt.  Gegenwärtig sind alle unsere Spiele in viele "Märkte" unterteilt (ein "Markt" pro Land) und in jedem "Markt" gibt es ein Dutzend Welten, zwischen denen die Spieler während der Registrierung verteilt werden (oder manchmal können sie es selbst auswählen).  Jede Welt hat eine Datenbank und einen oder mehrere Web- / App-Server.  Auf diese Weise wird die Last nahezu gleichmäßig auf die Welten / Server verteilt und verteilt. Dadurch erhalten wir das Maximum von 6K-8K-Spielern online (dies ist das Maximum, meistens um ein Vielfaches weniger) und 200-300 Anfragen pro Hauptsendezeit pro Welt. <br><br>  Eine solche Struktur mit der Aufteilung der Akteure in Märkte und Welten wird obsolet, die Spieler wollen etwas Globales.  In den letzten Spielen haben wir aufgehört, die Menschen nach Ländern aufzuteilen, und nur ein oder zwei Märkte (Amerika und Europa) verlassen, aber immer noch viele Welten in jeder.  Der nächste Schritt wird die Entwicklung von Spielen mit einer neuen Architektur und die Vereinigung aller Spieler in einer einzigen Welt mit <b>einer Datenbank sein</b> . <br><br>  Heute wollte ich ein wenig darüber sprechen, wie ich damit beauftragt wurde zu überprüfen, ob das gesamte Online-Spiel (und das sind jeweils 50 bis 200.000 Benutzer) eines unserer beliebten Spiele "sendet", um das nächste Spiel zu spielen, das auf der neuen Architektur basiert, und ob Das gesamte System, insbesondere die Datenbank ( <b>PostgreSQL 11</b> ), kann einer solchen Belastung praktisch standhalten und, falls dies nicht möglich ist, herausfinden, wo unser Maximum liegt.  Ich werde Ihnen ein wenig über die aufgetretenen Probleme und die Entscheidungen zur Vorbereitung auf das Testen so vieler Benutzer, den Prozess selbst und ein wenig über die Ergebnisse erzählen. <br><a name="habracut"></a><br><h2>  Intro </h2><br>  In der Vergangenheit hat jedes <b>Innenteam</b> bei der <b>InnoGames GmbH</b> ein Spielprojekt nach seinem Geschmack und seiner Farbe erstellt, wobei häufig unterschiedliche Technologien, Programmiersprachen und Datenbanken verwendet wurden.  Darüber hinaus verfügen wir über viele externe Systeme, die für Zahlungen, das Versenden von Push-Benachrichtigungen, Marketing und mehr verantwortlich sind.  Um mit diesen Systemen arbeiten zu können, haben Entwickler auch ihre einzigartigen Schnittstellen so gut wie möglich erstellt. <br><br>  Derzeit im Mobile-Gaming-Geschäft viel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Geld</a> und dementsprechend viel Wettbewerb.  Es ist hier sehr wichtig, es von jedem Dollar zurückzugeben, der für Marketing ausgegeben wird, und ein bisschen mehr von oben. Daher schließen alle Spielefirmen Spiele sehr oft sogar im Stadium geschlossener Tests, wenn sie die analytischen Erwartungen nicht erfüllen.  Dementsprechend ist es unrentabel, Zeit für die Erfindung des nächsten Rads zu verlieren. Daher wurde beschlossen, eine einheitliche Plattform zu schaffen, die Entwicklern eine vorgefertigte Lösung für die Integration in alle externen Systeme, eine Datenbank mit Replikation und alle Best Practices bietet.  Alles, was Entwickler brauchen, ist, ein gutes Spiel zu entwickeln und darauf zu setzen und keine Zeit mit der Entwicklung zu verschwenden, die nicht mit dem Spiel selbst zusammenhängt. <br><br>  Diese Plattform heißt <b>GameStarter</b> : <br><br><img src="https://habrastorage.org/webt/fz/go/g3/fzgog3jsz4rzjqi0zvbwzysz-po.jpeg" alt="Bild"><br><br>  Also auf den Punkt.  Alle zukünftigen InnoGames-Spiele werden auf dieser Plattform erstellt, die über zwei Datenbanken verfügt - Master und Game (PostgreSQL 11).  Der Master speichert grundlegende Informationen über die Spieler (Login, Passwort usw.) und nimmt hauptsächlich nur am Anmelde- / Registrierungsprozess im Spiel selbst teil.  Spiel - die Datenbank des Spiels selbst, in der dementsprechend alle Spieldaten und -entitäten gespeichert sind. Dies ist der Kern des Spiels, in den die gesamte Last gehen wird. <br>  Daher stellte sich die Frage, ob diese gesamte Struktur einer solchen potenziellen Anzahl von Benutzern standhalten kann, die dem Maximum online eines unserer beliebtesten Spiele entspricht. <br><br><h2>  Herausforderung </h2><br>  Die Aufgabe selbst bestand darin, zu überprüfen, ob die Datenbank (PostgreSQL 11) mit aktivierter Replikation der Last standhält, die wir derzeit im am meisten geladenen Spiel haben, und über den gesamten PowerEdge M630-Hypervisor (HV) verfügt. <br>  Ich werde klarstellen, dass die Aufgabe im Moment <b>nur darin bestand</b> , anhand der vorhandenen Datenbankkonfigurationen zu überprüfen, die wir unter Berücksichtigung von Best Practices und unserer eigenen Erfahrung erstellt haben. <br><br>  Ich sage gleich die Datenbank, und das gesamte System hat sich mit Ausnahme einiger Punkte gut gezeigt.  Dieses spezielle Spielprojekt befand sich jedoch im Prototypenstadium und in Zukunft werden mit der Komplikation der Spielmechanik die Anforderungen an die Datenbank komplizierter und die Last selbst kann erheblich zunehmen und ihre Art kann sich ändern.  Um dies zu verhindern, muss das Projekt iterativ mit jedem mehr oder weniger wichtigen Meilenstein getestet werden.  Die Automatisierung der Fähigkeit, diese Art von Tests mit ein paar Hunderttausenden von Benutzern durchzuführen, ist in dieser Phase zur Hauptaufgabe geworden. <br><br><h2>  Profil </h2><br>  Wie bei jedem Lasttest beginnt alles mit einem Lastprofil. <br>  Unser potenzieller Wert CCU60 (CCU ist die maximale Anzahl von Benutzern für einen bestimmten Zeitraum, in diesem Fall 60 Minuten) wird mit <b>250.000</b> Benutzern angenommen.  Die Anzahl der wettbewerbsfähigen virtuellen Benutzer (VUs) ist geringer als die der CCU60, und Analysten haben vorgeschlagen, dass sie sicher in zwei Teile geteilt werden kann.  Runden Sie <b>150.000</b> wettbewerbsfähige VUs auf und akzeptieren Sie sie. <br><br>  Die Gesamtzahl der Anfragen pro Sekunde wurde einem ziemlich geladenen Spiel entnommen: <br><br><img src="https://habrastorage.org/webt/lv/te/69/lvte69rifceelurn7r3t7trbgs4.png"><br><br>  Somit beträgt unsere Ziellast ~ <b>20.000 Anfragen / s</b> bei <b>150.000</b> VU. <br><br><h2>  Struktur </h2><br><h3>  Eigenschaften des „Standes“ </h3><br>  In einem früheren <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel habe</a> ich bereits über die Automatisierung des gesamten Lasttestprozesses gesprochen.  Weiter möchte ich mich ein wenig wiederholen, aber ich werde Ihnen einige Punkte genauer erläutern. <br><br><img src="https://habrastorage.org/webt/zh/hz/eo/zhhzeorw5_gboyuocu9ajmb3ulo.png"><br><br>  In der Abbildung sind die blauen Quadrate unsere Hypervisoren (HV), eine Cloud, die aus vielen Servern besteht (Dell M620 - M640).  Auf jeder HV werden ein Dutzend virtueller Maschinen (VMs) über KVM gestartet (Web / App und Datenbank im Mix).  Beim Erstellen einer neuen VM erfolgt das Ausgleichen und Durchsuchen des Parametersatzes einer geeigneten HV, und es ist zunächst nicht bekannt, auf welchem ​​Server sie sich befinden wird. <br><br><h4>  Datenbank (Spiel-DB): </h4><br>  Für unseren db1-Zweck haben wir jedoch einen separaten HV <b>targer_hypervisor</b> reserviert, der auf dem M630 basiert. <br><br>  Kurze Eigenschaften von targer_hypervisor: <br><br>  Dell M_630 <br>  Modellname: Intel® Xeon® CPU E5-2680 v3 bei 2,50 GHz <br>  CPU (s): 48 <br>  Gewinde pro Kern: 2 <br>  Kern (e) pro Sockel: 12 <br>  Steckdose (n): 2 <br>  RAM: 128 GB <br>  Debian GNU / Linux 9 (Strecke) <br>  4.9.0-8-amd64 # 1 SMP Debian 4.9.130-2 (2018-10-27) <br><br><div class="spoiler">  <b class="spoiler_title">Detaillierte Spezifikationen</b> <div class="spoiler_text">  Debian GNU / Linux 9 (Strecke) <br>  4.9.0-8-amd64 # 1 SMP Debian 4.9.130-2 (2018-10-27) <br>  lscpu <br>  Architektur: x86_64 <br>  CPU-Betriebsmodus: 32-Bit, 64-Bit <br>  Bytereihenfolge: Little Endian <br>  CPU (s): 48 <br>  Liste der Online-CPUs: 0-47 <br>  Gewinde pro Kern: 2 <br>  Kern (e) pro Sockel: 12 <br>  Steckdose (n): 2 <br>  NUMA-Knoten: 2 <br>  Hersteller-ID: GenuineIntel <br>  CPU-Familie: 6 <br>  Modell: 63 <br>  Modellname: Intel® Xeon® CPU E5-2680 v3 bei 2,50 GHz <br>  Schritt: 2 <br>  CPU MHz: 1309,356 <br>  CPU max MHz: 3300.0000 <br>  CPU min MHz: 1200.0000 <br>  BogoMIPS: 4988,42 <br>  Virtualisierung: VT-x <br>  L1d-Cache: 32 KB <br>  L1i-Cache: 32 KB <br>  L2-Cache: 256 KB <br>  L3-Cache: 30720 KB <br>  NUMA-Knoten 0 CPU (s): 0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42 44,46 <br>  NUMA-Knoten1 CPU (s): 1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43 45,47 <br>  Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse SMX est tm2 SSSE3 SDBG fma CX16 xtpr pdcm PCID DCA sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes XSAVE AVX F16C rdrand lahf_lm abm EPB invpcid_single ssbd IBRS IbpB stibp Kaiser tpr_shadow vnmi Flexpriority ept VPID fsgsbase tsc_adjust Bmi1 AVX2 SMEP BMI2 erms invpcid CQM xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts flush_l1d <br><br>  / usr / bin / qemu-system-x86_64 --version <br>  QEMU-Emulator Version 2.8.1 (Debian 1: 2.8 + dfsg-6 + deb9u5) <br>  Copyright © 2003-2016 Fabrice Bellard und die QEMU-Projektentwickler <br></div></div><br>  Kurze Eigenschaften von db1: <br>  Architektur: x86_64 <br>  CPU (s): 48 <br>  RAM: 64 GB <br>  4.9.0-8-amd64 # 1 SMP Debian 4.9.144-3.1 (2019-02-19) x86_64 GNU / Linux <br>  Debian GNU / Linux 9 (Strecke) <br>  psql (PostgreSQL) 11.2 (Debian 11.2-1.pgdg90 + 1) <br><br><div class="spoiler">  <b class="spoiler_title">PostgreSQL-Konfiguration mit einigen Erklärungen</b> <div class="spoiler_text">  seq_page_cost = 1.0 <br>  random_page_cost = 1.1 # Wir haben SSD <br>  include '/etc/postgresql/11/main/extension.conf' <br>  log_line_prefix = '% t [% p-% l]% q% u @% h' <br>  log_checkpoints = on <br>  log_lock_waits = on <br>  log_statement = ddl <br>  log_min_duration_statement = 100 <br>  log_temp_files = 0 <br>  autovacuum_max_workers = 5 <br>  autovacuum_naptime = 10s <br>  autovacuum_vacuum_cost_delay = 20ms <br>  vacac_cost_limit = 2000 <br>  wartung_arbeit_mem = 128MB <br>  synchronous_commit = aus <br>  checkpoint_timeout = 30min <br>  listen_addresses = '*' <br>  work_mem = 32 MB <br>  effektive_cache_size = 26214MB # 50% des verfügbaren Speichers <br>  shared_buffers = 16384MB # 25% des verfügbaren Speichers <br>  max_wal_size = 15 GB <br>  min_wal_size = 80 MB <br>  wal_level = hot_standby <br>  max_wal_senders = 10 <br>  wal_compression = on <br>  archive_mode = on <br>  archive_command = '/ bin / true' <br>  archive_timeout = 1800 <br>  hot_standby = on <br>  wal_log_hints = on <br>  hot_standby_feedback = on <br></div></div><br>  <b>hot_standby_feedback ist</b> standardmäßig <b>deaktiviert</b> . Wir hatten es <b>aktiviert</b> , aber später musste es <b>deaktiviert</b> werden, um einen erfolgreichen Test durchzuführen.  Ich werde später erklären, warum. <br><br>  Die wichtigsten aktiven Tabellen in der Datenbank (Konstruktion, Produktion, game_entity, building, core_inventory_player_resource, Survivor) werden mithilfe eines Bash-Skripts mit Daten (ca. 80 GB) vorgefüllt. <br><br><div class="spoiler">  <b class="spoiler_title">db-fill-script.sh</b> <div class="spoiler_text"><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash --clean TRUNCATE TABLE production CASCADE; TRUNCATE TABLE construction CASCADE; TRUNCATE TABLE building CASCADE; TRUNCATE TABLE grid CASCADE; TRUNCATE TABLE core_inventory_player_resource CASCADE; TRUNCATE TABLE survivor CASCADE; TRUNCATE TABLE city CASCADE; TRUNCATE TABLE game_entity CASCADE; TRUNCATE TABLE player CASCADE; TRUNCATE TABLE core_player CASCADE; TRUNCATE TABLE core_client_device CASCADE; --core_client_device INSERT INTO core_client_device (id, creation_date, modification_date, device_model, device_name, locale, platform, user_agent, os_type, os_version, network_type, device_type) SELECT (1000000000+generate_series(0,999999)) AS id, now(), now(), 'device model', 'device name', 'en_DK', 'ios', 'ios user agent', 'android', '8.1', 'wlan', 'browser'; --core_player INSERT INTO core_player (id, guest, name, nickname, premium_points, soft_deleted, session_id, tracking_device_data_id) SELECT (1000000000+generate_series(0,999999)) AS id, true, 'guest0000000000000000000', null, 100, false, '00000000-0000-0000-0000-000000000000', (1000000000+generate_series(0,999999)) ; --player INSERT INTO player (id, creation_date, modification_date, core_player_id) SELECT (1000000000+generate_series(0,999999)) , now(), now(), (1000000000+generate_series(0,999999)) ; --city INSERT INTO game_entity (id, type, creation_date, modification_date) SELECT (1000000000+generate_series(0,999999)) , 'city', now(), now(); INSERT INTO city (id, game_design, player_id) SELECT (1000000000+generate_series(0,999999)) , 'city.default', (1000000000+generate_series(0,999999)) ; --survivor INSERT INTO game_entity (id, type, creation_date, modification_date) SELECT (1001000000+generate_series(0,999999)) , 'survivor', now(), now(); INSERT INTO survivor (id, game_design, owning_entity_id, type) SELECT (1001000000+generate_series(0,999999)) , 'survivor.prod_1', (1000000000+generate_series(0,999999)) , 'survivor'; --core_inventory_player_resource INSERT INTO core_inventory_player_resource (id, creation_date, modification_date, amount, player_id, resource_key) SELECT (1000000000+generate_series(0,1999999)) , NOW(), NOW(), 1000, (1000000000+generate_series(0,1999999)/2) , CONCAT('resource_', (1000000000+generate_series(0,1999999)) % 2); --grid DROP INDEX grid_area_idx; INSERT INTO grid (id, creation_date, modification_date, area, city_id) SELECT (1000000000+generate_series(0,19999999)) , NOW(), NOW(), BOX '0,0,4,4', (1000000000+generate_series(0,19999999)/20) ; create index on grid using gist (area box_ops); --building INSERT INTO game_entity (id, type, creation_date, modification_date) SELECT (1002000000+generate_series(0,99999999)) , 'building', now(), now(); INSERT INTO building (id, game_design, owning_entity_id, x, y, rotation, type) SELECT (1002000000+generate_series(0,99999999)) , 'building.building_prod_1', (1000000000+generate_series(0,99999999)/100) , 0, 0, 'DEGREES_0', 'building'; --construction INSERT INTO construction (id, creation_date, modification_date, definition, entity_id, start) SELECT (1000000000+generate_series(0,1999999)) , NOW(), NOW(), 'construction.building_prod_1-construction', (1002000000+generate_series(0,1999999)*50) , NOW(); --production INSERT INTO production (id, creation_date, modification_date, active, definition, entity_id, start_time) SELECT (1000000000+generate_series(0,49999999)) , NOW(), NOW(), true, 'production.building_prod_1_production_1', (1002000000+generate_series(0,49999999)*2) , NOW();</span></span></code> </pre> <br></div></div><br>  Replikation: <br><br><pre> <code class="plaintext hljs">SELECT * FROM pg_stat_replication; pid | usesysid | usename | application_name | client_addr | client_hostname | client_port | backend_start | backend_xmin | state | sent_lsn | write_lsn | flush_lsn | replay_lsn | write_lag | flush_lag | replay_lag | sync_priority | sync_state -----+----------+---------+---------------------+--------------+---------------------+-------------+-------------------------------+--------------+-----------+------------+------------+------------+------------+-----------------+-----------------+-----------------+---------------+------------ 759 | 17035 | repmgr | xl1db2 | xxxx | xl1db2 | 51142 | 2019-01-27 08:56:44.581758+00 | | streaming | 18/424A9F0 | 18/424A9F0 | 18/424A9F0 | 18/424A9F0 | 00:00:00.000393 | 00:00:00.001159 | 00:00:00.001313 | 0 | async 977 | 17035 | repmgr | xl1db3 |xxxxx | xl1db3 | 42888 | 2019-01-27 08:57:03.232969+00 | | streaming | 18/424A9F0 | 18/424A9F0 | 18/424A9F0 | 18/424A9F0 | 00:00:00.000373 | 00:00:00.000798 | 00:00:00.000919 | 0 | async</code> </pre><br><h4>  Anwendungsserver </h4><br>  Anschließend wurden auf einer produktiven HV (prod_hypervisors) mit verschiedenen Konfigurationen und Kapazitäten 15 App-Server gestartet: 8 Kerne, 4 GB.  Die Hauptsache, die gesagt werden kann: openjdk 11.0.1 2018-10-16, Frühling, Interaktion mit der Datenbank über <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hikari</a> (hikari.maximum-pool-size: 50) <br><br><h4>  Stresstest-Umgebung </h4><br>  Die gesamte Lasttestumgebung besteht aus einem Hauptserver <b>admin.loadtest</b> und mehreren <b>GeneratorN.loadtest-</b> Servern (in diesem Fall waren es 14). <br><br>  <b>generatorN.loadtest</b> - "nackte" VM Debian Linux 9 mit installiertem Java 8. 32 Kernel / 32 Gigabyte.  Sie befinden sich auf nicht produktiven HVs, um die Leistung wichtiger VMs nicht versehentlich zu beeinträchtigen. <br><br>  <b>admin.loadtest</b> - Debian Linux 9 <b>virtuelle Maschine</b> , 16 Kerne / 16 Gigs, auf der Jenkins, JLTC und andere zusätzliche unwichtige Software ausgeführt werden. <br><br>  JLTC - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Jmeter Lasttestzentrum</a> .  Ein System in Py / Django, das den Start von Tests sowie die Analyse von Ergebnissen steuert und automatisiert. <br><br><h3>  Teststartschema </h3><br><img src="https://habrastorage.org/webt/pb/f_/th/pbf_thx7mwuois96bffvbeehtxk.png"><br><br>  Der Prozess zum Ausführen des Tests sieht folgendermaßen aus: <br><br><ul><li>  Der Test wird von <b>Jenkins</b> aus gestartet.  Wählen Sie den gewünschten Job aus, und geben Sie die gewünschten Testparameter ein: <ul><li>  <b>DAUER</b> - <b>Testdauer</b> </li><li>  <b>RAMPUP</b> - Aufwärmzeit </li><li>  <b>THREAD_COUNT_TOTAL</b> - Die gewünschte Anzahl virtueller Benutzer (VU) oder Threads </li><li>  <b>TARGET_RESPONSE_TIME</b> ist ein wichtiger Parameter. <b>Um</b> das gesamte System nicht zu überlasten, stellen wir die gewünschte Antwortzeit ein. <b>Dementsprechend</b> hält der Test die Last auf einem Niveau, bei dem die Antwortzeit des gesamten Systems nicht mehr als die angegebene beträgt. </li></ul></li><li>  Starten </li><li>  Jenkins klont den Testplan von Gitlab und sendet ihn an JLTC. </li><li>  JLTC arbeitet ein wenig mit einem Testplan (fügt beispielsweise einen einfachen CSV-Writer ein). </li><li>  JLTC berechnet die erforderliche Anzahl von Jmeter-Servern, um die gewünschte Anzahl von VUs auszuführen (THREAD_COUNT_TOTAL). </li><li>  JLTC stellt eine Verbindung zu jedem LoadgeneratorN-Generator her und startet den jmeter-Server. </li></ul><br>  Während des Tests generiert der <b>JMeter-Client</b> eine CSV-Datei mit den Ergebnissen.  Während des Tests wächst die Datenmenge und die Größe dieser Datei in einem <b>wahnsinnigen</b> Tempo und kann nach dem Test nicht mehr für die Analyse verwendet werden. <b>Daemon wurde</b> erfunden (als Experiment) und analysiert es <i>„on the fly“</i> . <br><br><h3>  Testplan </h3><br>  Den Testplan können Sie hier herunterladen. <br><br>  Nach der Registrierung / Anmeldung arbeiten Benutzer im <b>Verhaltensmodul</b> , das aus mehreren <b>Durchsatz-Controllern besteht</b> , die die Wahrscheinlichkeit einer bestimmten Spielfunktion angeben.  In jedem Durchsatz-Controller gibt es einen <b>Modul-Controller</b> , der sich auf das entsprechende Modul bezieht, das die Funktion implementiert. <br><br><img src="https://habrastorage.org/webt/t0/ws/qp/t0wsqpbkgo-w6dt55gvxd6aw9pm.png"><br><br><h4>  Off-Topic </h4><br>  Während der Entwicklung des Skripts haben wir versucht, Groovy in vollem Umfang zu nutzen, und dank unseres Java-Programmierers habe ich ein paar Tricks für mich entdeckt (vielleicht ist es für jemanden nützlich): <br><br><ul><li>  Sie können eine Funktion irgendwo am Anfang des Testplans deklarieren und sie dann in anderen Pre-, Postprozessoren und Samplern verwenden.  Mehr <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Groovy Goodness: Verwandeln Sie Methoden in Abschlüsse</a> : <br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">//     - def sum(Integer x, Integer y) { return x + y } vars.putObject('sum', this.&amp;sum) //      closure.   . //     sampler`       def sum= vars.getObject('sum'); println sum(2, 2);</span></span></code> </pre> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">groovy.json.JsonSlurper</a> ist ein großartiger schneller JSON-Parser.  Zusammen mit groovy können Sie Daten elegant analysieren und verarbeiten: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> groovy.json.JsonSlurper def canBuild = vars.getObject(canBuild); <span class="hljs-comment"><span class="hljs-comment">// ""       def content = jsonSlurper.parseText(response).content def buildings = content[0].buildings //         //               def constructableBuildingDefs = buildings .collect { k,v -&gt; v } .grep{ it.definitions .grep { it2 -&gt; it2['@type'] == 'type.googleapis.com/ConstructionDefinitionDTO'} .grep { it2 -&gt; canBuild(it2) } //   .size() &gt; 0 } if (!constructableBuildingDefs) { return; } Collections.shuffle(constructableBuildingDefs) //       </span></span></code> </pre></li></ul><br><h3>  VU / Threads </h3><br>  Wenn ein Benutzer bei der Konfiguration des Jobs in Jenkins die gewünschte Anzahl von VUs mit dem Parameter THREAD_COUNT_TOTAL eingibt, muss die erforderliche Anzahl von Jmeter-Servern irgendwie gestartet und die endgültige Anzahl von VUs zwischen ihnen verteilt werden.  Dieser Teil liegt beim JLTC im Teil <b>Controller / Provision</b> . <br><br>  Im Wesentlichen lautet der Algorithmus wie folgt: <br><br><ul><li>  Wir teilen die gewünschte Anzahl von VU <b>threads_num</b> in 200-300 Threads auf und basierend auf der mehr oder weniger angemessenen Größe <b>-Xmsm -Xmxm</b> bestimmen <b>wir</b> den erforderlichen Speicherwert für einen <i>jmeter-Server</i> <b>required_memory_for_jri</b> (JRI - Ich nenne Jmeter-Remote-Instanz anstelle von Jmeter-Server). </li><li>  Aus threads_num und required_memory_for_jri ermitteln wir die Gesamtzahl der jmeter-Server: <b>target_amount_jri</b> und den Gesamtwert des erforderlichen Speichers: <b>required_memory_total</b> . </li><li>  Wir sortieren alle LoadgeneratorN-Generatoren nacheinander und starten die maximale Anzahl von Jmeter-Servern basierend auf dem verfügbaren Speicher.  Solange die Anzahl der laufenden current_amount_jri-Instanzen <b>nicht gleich</b> target_amount_jri ist. </li><li>  (Wenn die Anzahl der Generatoren und der Gesamtspeicher nicht ausreichen, fügen Sie dem Pool einen neuen hinzu.) </li><li>  Wir stellen mit <b>netstat eine</b> Verbindung zu jedem Generator her, wobei <b>wir uns</b> an alle ausgelasteten Ports erinnern, und führen auf zufälligen Ports (die nicht belegt sind) die erforderliche Anzahl von jmeter-Servern aus: <br><br><pre> <code class="python hljs"> netstat_cmd= <span class="hljs-string"><span class="hljs-string">'netstat -tulpn | grep LISTEN'</span></span> stdin, stdout, stderr = ssh.exec_command(cmd1) used_ports = [] netstat_output = str(stdout.readlines()) ports = re.findall(<span class="hljs-string"><span class="hljs-string">'\d+\.\d+\.\d+\.\d+\:(\d+)'</span></span>, netstat_output) ports_ipv6 = re.findall(<span class="hljs-string"><span class="hljs-string">'\:\:\:(\d+)'</span></span>, netstat_output) p.wait() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> port <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> ports: used_ports.append(int(port)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> port <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> ports_ipv6: used_ports.append(int(port)) ssh.close() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, possible_jris_on_host + <span class="hljs-number"><span class="hljs-number">1</span></span>): port = int(random.randint(<span class="hljs-number"><span class="hljs-number">10000</span></span>, <span class="hljs-number"><span class="hljs-number">20000</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> port <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> used_ports: port = int(random.randint(<span class="hljs-number"><span class="hljs-number">10000</span></span>, <span class="hljs-number"><span class="hljs-number">20000</span></span>)) <span class="hljs-comment"><span class="hljs-comment"># ...  Jmeter-    </span></span></code> </pre></li><li>  Wir sammeln alle laufenden jmeter-Server auf einmal in der Formatadresse: port, zum Beispiel <b>generator13: 15576, generator9: 14015, generator11: 19152, generator14: 12125, generator2: 17602</b> </li><li>  Die resultierende Liste und threads_per_host werden zu Beginn des Tests an den JMeter-Client gesendet: <br><br><pre> <code class="bash hljs">REMOTE_TESTING_FLAG=<span class="hljs-string"><span class="hljs-string">" -R </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$REMOTE_HOSTS_STRING</span></span></span><span class="hljs-string">"</span></span> java -jar -Xms7g -Xmx7g -Xss228k <span class="hljs-variable"><span class="hljs-variable">$JMETER_DIR</span></span>/bin/ApacheJMeter.jar -Jserver.rmi.ssl.disable=<span class="hljs-literal"><span class="hljs-literal">true</span></span> -n -t <span class="hljs-variable"><span class="hljs-variable">$TEST_PLAN</span></span> -j <span class="hljs-variable"><span class="hljs-variable">$WORKSPACE</span></span>/loadtest.log -GTHREAD_COUNT=<span class="hljs-variable"><span class="hljs-variable">$THREADS_PER_HOST</span></span> <span class="hljs-variable"><span class="hljs-variable">$OTHER_VARS</span></span> <span class="hljs-variable"><span class="hljs-variable">$REMOTE_TESTING_FLAG</span></span> -Jjmeter.save.saveservice.default_delimiter=,</code> </pre></li></ul><br>  In unserem Fall wurde der Test gleichzeitig von 300 Jmeter-Servern mit jeweils 500 Threads durchgeführt. Das Startformat eines Jmeter-Servers mit Java-Parametern sah folgendermaßen aus: <br><br><pre> <code class="bash hljs">nohup java -server -Xms1200m -Xmx1200m -Xss228k -XX:+DisableExplicitGC -XX:+CMSClassUnloadingEnabled -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 -XX:+ScavengeBeforeFullGC -XX:+CMSScavengeBeforeRemark -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -Djava.net.preferIPv6Addresses=<span class="hljs-literal"><span class="hljs-literal">true</span></span> -Djava.net.preferIPv4Stack=<span class="hljs-literal"><span class="hljs-literal">false</span></span> -jar <span class="hljs-string"><span class="hljs-string">"/tmp/jmeter-JwKse5nY/bin/ApacheJMeter.jar"</span></span> -Jserver.rmi.ssl.disable=<span class="hljs-literal"><span class="hljs-literal">true</span></span> <span class="hljs-string"><span class="hljs-string">"-Djava.rmi.server.hostname=generator12.loadtest.ig.local"</span></span> -Duser.dir=/tmp/jmeter-JwKse5nY/bin/ -Dserver_port=13114 -s -Jpoll=49 &gt; /dev/null 2&gt;&amp;1</code> </pre> <br><h3>  50ms </h3><br>  Die Aufgabe besteht darin, zu bestimmen, wie viel unsere Datenbank aushalten kann, anstatt sie und das gesamte System als Ganzes in einen kritischen Zustand zu versetzen.  Bei so vielen Jmeter-Servern müssen Sie die Last auf einem bestimmten Niveau halten und nicht das gesamte System töten.  Der beim Starten des Tests angegebene Parameter <b>TARGET_RESPONSE_TIME</b> ist dafür verantwortlich.  Wir waren uns einig, dass <b>50 ms</b> die optimale Reaktionszeit ist, für die das System verantwortlich sein sollte. <br><br>  In JMeter gibt es standardmäßig viele verschiedene Timer, mit denen Sie den Durchsatz steuern können. In unserem Fall ist jedoch nicht bekannt, wo Sie ihn erhalten können.  Es gibt jedoch einen <b>JSR223-Timer,</b> mit dem Sie anhand der <b>aktuellen</b> Systemantwortzeit etwas finden <b>können</b> .  Der Timer selbst befindet sich im Hauptblock <b>Verhalten</b> : <br><br><img src="https://habrastorage.org/webt/uv/o8/rb/uvo8rb9ph7mr1xhxkzxccsfa06a.png"><br><br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">//      = 0 vars.put('samples', '20'); vars.putObject('respAvg', ${TARGET_RESPONSE_TIME}.0); vars.putObject('sleep', 0.0); //  JSR223-Timer           "" double sleep = vars.getObject('sleep'); double respAvg = vars.getObject('respAvg'); double previous = sleep; double target = ${TARGET_RESPONSE_TIME}; if (respAvg &lt; target) { sleep /= 1.5; } if (respAvg &gt; target) { sleep *= 1.1; } sleep = Math.max(10, sleep); //      sleep = Math.min(20000, sleep); vars.putObject('sleep', sleep); return (int)sleep;</span></span></code> </pre><br><h3>  Analyse der Ergebnisse (Daemon) </h3><br>  Zusätzlich zu den Diagrammen in Grafana müssen auch aggregierte Testergebnisse vorliegen, damit die Tests anschließend in JLTC verglichen werden können. <br><br>  Ein solcher Test generiert 16.000 bis 20.000 Anfragen pro Sekunde. Es ist einfach zu berechnen, dass in 4 Stunden eine CSV-Datei mit einer Größe von einigen hundert GB generiert wird. Daher musste ein Job erstellt werden, der die Daten jede Minute analysiert, an die Datenbank sendet und die Hauptdatei bereinigt. <br><br><img src="https://habrastorage.org/webt/pb/mj/kc/pbmjkcwgjcxupnihgzpzmqd0nvq.png"><br><br>  Der Algorithmus ist wie folgt: <br><br><ul><li>  Wir lesen die Daten aus der vom jmeter-client generierten CSV-Datei <b>result.jtl</b> , speichern und bereinigen die Datei (Sie müssen sie korrekt bereinigen, sonst sieht die leere Datei wie die alte FD mit derselben Größe aus): <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(jmeter_results_file, <span class="hljs-string"><span class="hljs-string">'r+'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: rows = f.readlines() f.seek(<span class="hljs-number"><span class="hljs-number">0</span></span>) f.truncate(<span class="hljs-number"><span class="hljs-number">0</span></span>) f.writelines(rows[<span class="hljs-number"><span class="hljs-number">-1</span></span>])</code> </pre></li><li>  Wir schreiben die gelesenen Daten in die temporäre Datei <b>temp_result.jtl</b> : <br><br><pre> <code class="python hljs">rows_num = len(rows) open(temp_result_filename, <span class="hljs-string"><span class="hljs-string">'w'</span></span>).writelines(rows[<span class="hljs-number"><span class="hljs-number">0</span></span>:rows_num]) <span class="hljs-comment"><span class="hljs-comment"># avoid last line</span></span></code> </pre> </li><li>  Wir lesen die Datei <b>temp_result.jtl</b> .  Wir verteilen die gelesenen Daten "in Minuten": <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> r <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> f.readlines(): row = r.split(<span class="hljs-string"><span class="hljs-string">','</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(row[<span class="hljs-number"><span class="hljs-number">0</span></span>]) == <span class="hljs-number"><span class="hljs-number">13</span></span>: ts_c = int(row[<span class="hljs-number"><span class="hljs-number">0</span></span>]) dt_c = datetime.datetime.fromtimestamp(ts_c/<span class="hljs-number"><span class="hljs-number">1000</span></span>) minutes_data.setdefault(dt_c.strftime(<span class="hljs-string"><span class="hljs-string">'%Y_%m_%d_%H_%M'</span></span>), []).append(r)</code> </pre></li><li>  Die Daten für jede Minute von <b>minutendaten werden</b> in die entsprechende Datei im Ordner <b>to_parse / geschrieben</b> .  (Daher hat im Moment jede Minute des Tests eine eigene Datendatei. Während der Aggregation spielt <b>es</b> keine Rolle, in welcher Reihenfolge die Daten in jede Datei gelangen.) <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> key, value <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> minutes_data.iteritems(): <span class="hljs-comment"><span class="hljs-comment">#      timestamp (key) temp_ts_file = os.path.join(temp_to_parse_path, key) open(temp_ts_file, 'a+').writelines(value)</span></span></code> </pre></li><li>  Unterwegs analysieren wir die Dateien im Ordner to_parse. Wenn sich eine dieser Dateien nicht innerhalb einer Minute geändert hat, ist diese Datei ein Kandidat für die Datenanalyse, Aggregation und das Senden an die JLTC-Datenbank: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> filename <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> os.listdir(temp_to_parse_path): data_file = os.path.join(temp_to_parse_path, filename) file_mod_time = os.stat(data_file).st_mtime last_time = (time.time() - file_mod_time) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> last_time &gt; <span class="hljs-number"><span class="hljs-number">60</span></span>: logger.info(<span class="hljs-string"><span class="hljs-string">'[DAEMON] File {} was not modified since 1min, adding to parse list.'</span></span>.format(data_file)) files_to_parse.append(data_file)</code> </pre></li><li>  Wenn es solche Dateien gibt (eine oder mehrere), senden wir sie analysiert an die Funktion <b>parse_csv_data</b> (jede Datei parallel): <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> f <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> files_to_parse: logger.info(<span class="hljs-string"><span class="hljs-string">'[DAEMON THREAD] Parse {}.'</span></span>.format(f)) t = threading.Thread( target=parse_csv_data, args=( f, jmeter_results_file_fields, test, data_resolution)) t.start() threads.append(t) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> threads: t.join()</code> </pre></li></ul><br>  Daemon selbst in cron.d startet jede Minute: <br><br>  Daemon startet jede Minute mit cron.d: <br><br><pre> <code class="bash hljs">* * * * * root sleep 21 &amp;&amp; /usr/bin/python /var/lib/jltc/manage.py daemon</code> </pre> <br>  Somit schwillt die Datei mit den Ergebnissen nicht auf unvorstellbare Größen an, sondern wird im laufenden Betrieb analysiert und gelöscht. <br><br><h2>  Ergebnisse </h2><br><h3>  Die App </h3><br>  Unsere 150.000 virtuellen Spieler: <br><br><img src="https://habrastorage.org/webt/sv/ex/ep/svexepi9ikzy0unpxur96mty5q8.png"><br><br>  Der Test versucht, die Antwortzeit von 50 ms anzupassen, sodass die Last selbst ständig im Bereich zwischen 16.000 und 18.000 Anforderungen / c springt: <br><br><img src="https://habrastorage.org/webt/-z/98/oi/-z98oi-_8a41hkqbrmz2rvzmryk.png"><br><br>  Anwendungsserverlast (15 App).  Zwei Server haben "Pech", auf dem langsameren M620 zu sein: <br><br><img src="https://habrastorage.org/webt/nc/xy/et/ncxyetqyk_a8mbhjocndd-yxgkm.png"><br><br>  Datenbankantwortzeit (für App-Server): <br><br><img src="https://habrastorage.org/webt/zr/a-/gw/zra-gwtq_vqhtfhlrjzzy1osisg.png"><br><br><h3>  Datenbank </h3><br>  CPU-Auslastung auf db1 (VM): <br><br><img src="https://habrastorage.org/webt/ej/hn/in/ejhnin0jo_7rrzhlj7pqko6udnq.png"><br><br>  CPU-Auslastung auf dem Hypervisor: <br><br><img src="https://habrastorage.org/webt/uw/ik/tz/uwiktzvcdzydlsjaoexqfbr3tay.png"><br><br>  Die Belastung der virtuellen Maschine ist geringer, da davon ausgegangen wird, dass 48 reale Kerne zur Verfügung stehen. Auf dem Hypervisor befinden sich tatsächlich 24 <b>Hyperthreading-</b> Kerne. <br><br>  Es werden <b>maximal ~ 250.000 Abfragen / s</b> an die Datenbank <b>gesendet</b> , bestehend aus (83% Auswahlen, 3% Einfügungen, 11,6% Aktualisierungen (90% HEISS), 1,6% Löschungen): <br><br><img src="https://habrastorage.org/webt/lx/lu/bl/lxlublobhm4nm3c45g9jikcstc4.png"><br><br><img src="https://habrastorage.org/webt/18/jw/gp/18jwgpmebrkyot3ngvarsl_3ysu.png"><br><br>  Mit einem Standardwert von <b>autovacuum_vacuum_scale_factor</b> = 0,2 wuchs die Anzahl der toten Tupel mit dem Test (mit zunehmender Tabellengröße) sehr schnell, was mehrmals zu kurzen Datenbankleistungsproblemen führte, die den gesamten Test mehrmals ruinierten.  Ich musste dieses Wachstum für einige Tabellen „zähmen“, indem ich diesem Parameter autovacuum_vacuum_scale_factor persönliche Werte zuwies: <br><br><div class="spoiler">  <b class="spoiler_title">ALTER TABLE ... SET (autovacuum_vacuum_scale_factor = ...)</b> <div class="spoiler_text">  ALTER TABLE Konstruktion SET (autovacuum_vacuum_scale_factor = 0.10); <br>  ALTER TABLE Produktion SET (autovacuum_vacuum_scale_factor = 0.01); <br>  ALTER TABLE game_entity SET (autovacuum_vacuum_scale_factor = 0.01); <br>  ALTER TABLE game_entity SET (autovacuum_analyze_scale_factor = 0.01); <br>  ALTER TABLE building SET (autovacuum_vacuum_scale_factor = 0.01); <br>  ALTER TABLE building SET (autovacuum_analyze_scale_factor = 0.01); <br>  ALTER TABLE core_inventory_player_resource SET (autovacuum_vacuum_scale_factor = 0.10); <br>  ALTER TABLE Survivor SET (autovacuum_vacuum_scale_factor = 0.01); <br>  ALTER TABLE Survivor SET (autovacuum_analyze_scale_factor = 0.01); <br></div></div><br><img src="https://habrastorage.org/webt/0s/ja/1e/0sja1e1m3-2gitbmhh8j24t2ktu.png"><br><br>  Im Idealfall sollte rows_fetched in der Nähe von rows_returned liegen, was wir glücklicherweise beobachten: <br><br><img src="https://habrastorage.org/webt/e4/k6/zo/e4k6zobp25h5asrmxhmficoea3a.png"><br><br><h4>  hot_standby_feedback </h4><br>  Das Problem war der Parameter <b>hot_standby_feedback</b> , der die Leistung des <b>Hauptservers</b> erheblich beeinträchtigen kann, wenn seine <b>Standby-</b> Server keine Zeit haben, Änderungen an WAL-Dateien anzuwenden.  In der Dokumentation (https://postgrespro.ru/docs/postgrespro/11/runtime-config-replication) heißt es, dass "bestimmt wird, ob der Hot-Standby-Server den Master oder den übergeordneten Slave über die Anforderungen benachrichtigt, die er gerade ausführt".  Standardmäßig ist es deaktiviert, aber es wurde in unserer Konfiguration aktiviert.  Was zu traurigen Konsequenzen führte, wenn 2 Standby-Server vorhanden sind und die Replikationsverzögerung während des Ladens (aus verschiedenen Gründen) von Null abweicht, können Sie ein solches Bild beobachten, das zum Zusammenbruch des gesamten Tests führen kann: <br><br><img src="https://habrastorage.org/webt/vl/1l/jd/vl1ljdockxrjlfsoiybq751vo9y.png"><br><br><img src="https://habrastorage.org/webt/qh/4s/m_/qh4sm_hpnunb2w0axzhk90lmf6u.png"><br><br>  Dies liegt an der Tatsache, dass VACUUM bei aktiviertem hot_standby_feedback keine toten Tupel löschen möchte, wenn Standby-Server in ihrer Transaktions-ID zurückliegen, um Replikationskonflikte zu vermeiden.  Ausführlicher Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Was hot_standby_feedback in PostgreSQL wirklich tut</a> : <br><br><pre> <code class="plaintext hljs">xl1_game=# VACUUM VERBOSE core_inventory_player_resource; INFO: vacuuming "public.core_inventory_player_resource" INFO: scanned index "core_inventory_player_resource_pkey" to remove 62869 row versions DETAIL: CPU: user: 1.37 s, system: 0.58 s, elapsed: 4.20 s ………... INFO: "core_inventory_player_resource": found 13682 removable, 7257082 nonremovable row versions in 71842 out of 650753 pages &lt;b&gt;DETAIL: 3427824 dead row versions cannot be removed yet, oldest xmin: 3810193429&lt;/b&gt; There were 1920498 unused item pointers. Skipped 8 pages due to buffer pins, 520953 frozen pages. 0 pages are entirely empty. CPU: user: 4.55 s, system: 1.46 s, elapsed: 11.74 s.</code> </pre><br>  Eine so große Anzahl toter Tupel führt zu dem oben gezeigten Bild.  Hier sind zwei Tests, bei denen hot_standby_feedback ein- und ausgeschaltet ist: <br><br><img src="https://habrastorage.org/webt/8f/od/n6/8fodn60lohgzsu-twheqysldjzy.png"><br><br>  Und dies ist unsere Replikationsverzögerung während des Tests, mit der in Zukunft etwas unternommen werden muss: <br><br><img src="https://habrastorage.org/webt/et/s0/7h/ets07hkl8skkv5wexxjrgi-3x7m.png"><br><br><h2>  Fazit </h2><br>  Dieser Test hat glücklicherweise (oder leider für den Inhalt des Artikels) gezeigt, dass es in dieser Phase des Prototyps des Spiels durchaus möglich ist, die gewünschte Last seitens der Benutzer zu absorbieren, was ausreicht, um grünes Licht für weitere Prototypen und Entwicklungen zu geben.  In den nachfolgenden Entwicklungsphasen müssen die Grundregeln befolgt werden (um die Einfachheit der ausgeführten Abfragen zu gewährleisten, eine Überfülle an Indizes sowie nicht indizierte Messwerte usw. zu verhindern) und vor allem das Projekt in jeder wichtigen Entwicklungsphase testen, um Probleme zu finden und zu beheben kann früher sein.  Vielleicht schreibe ich bald einen Artikel, da wir bereits bestimmte Probleme gelöst haben. <br><br>  Viel Glück an alle! <br><br>  Unser <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GitHub</a> nur für den Fall;) </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de445368/">https://habr.com/ru/post/de445368/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de445356/index.html">Überblick über den Bereich Mobile bei DUMP-2019: Maximal angewendet und nützlich in der täglichen Arbeit</a></li>
<li><a href="../de445358/index.html">Organisation des Ereignissystems in Unity - mit den Augen eines Spieledesigners</a></li>
<li><a href="../de445360/index.html">5 typische Aufgaben für JavaScript-Interviews: Analyse und Lösungen</a></li>
<li><a href="../de445362/index.html">Das Buch "Distributed Systems. Entwurfsmuster</a></li>
<li><a href="../de445366/index.html">So beschleunigen Sie die Verschlüsselung gemäß GOST 28147-89 auf dem Baikal-T1-Prozessor aufgrund des SIMD-Blocks</a></li>
<li><a href="../de445370/index.html">TSDB-Analyse in Prometheus 2</a></li>
<li><a href="../de445372/index.html">Bildverarbeitung vs. menschliche Intuition: Algorithmen zur Unterbrechung des Betriebs von Objekterkennungsprogrammen</a></li>
<li><a href="../de445378/index.html">Labyrinthe: Klassifizierung, Erzeugung, Suche nach Lösungen</a></li>
<li><a href="../de445380/index.html">Modernes PHP ist schön und produktiv</a></li>
<li><a href="../de445384/index.html">Chang'e-4 Mission - wissenschaftliche Ausrüstung auf dem Landemodul und dem Repeater-Satelliten</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>