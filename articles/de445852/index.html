<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🕵🏿 🙅🏻 🕕 So erstellen Sie einen DAG-Trigger in Airflow mithilfe der experimentellen API 🖤 ✋🏻 👗</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bei der Vorbereitung unserer Bildungsprogramme stoßen wir regelmäßig auf Schwierigkeiten bei der Arbeit mit einigen Tools. Und in dem Moment, in dem w...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>So erstellen Sie einen DAG-Trigger in Airflow mithilfe der experimentellen API</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/newprolab/blog/445852/"><p>  Bei der Vorbereitung unserer Bildungsprogramme stoßen wir regelmäßig auf Schwierigkeiten bei der Arbeit mit einigen Tools.  Und in dem Moment, in dem wir ihnen begegnen, gibt es nicht immer genug Dokumentationen und Artikel, um dieses Problem zu lösen. </p><br><p>  Dies war beispielsweise 2015 der Fall, und wir haben im Rahmen des Big Data Specialist-Programms einen Hadoop-Cluster mit Spark für 35 gleichzeitige Benutzer verwendet.  Wie man es unter einem solchen Anwenderfall mit YARN kocht, war nicht klar.  Nachdem sie es herausgefunden und den Weg alleine gegangen waren, machten sie einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beitrag auf Habré</a> und traten auch beim <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Moscow Spark Meetup auf</a> . </p><br><h3 id="predystoriya">  Hintergrund </h3><br><p>  Dieses Mal werden wir über ein anderes Programm sprechen - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Data Engineer</a> .  Unsere Teilnehmer bauen zwei Arten von Architektur darauf: Lambda und Kappa.  In der Lamdba-Architektur wird Airflow im Rahmen der Stapelverarbeitung zum Übertragen von Protokollen von HDFS zu ClickHouse verwendet. </p><br><p>  Im Allgemeinen ist alles gut.  Lassen Sie sie ihre Pipelines bauen.  Es gibt jedoch ein „Aber“: Alle unsere Programme sind im Hinblick auf den Lernprozess selbst technologisch.  Um das Labor zu überprüfen, verwenden wir automatische Prüfer: Der Teilnehmer muss zu seinem persönlichen Konto gehen, auf die Schaltfläche „Prüfen“ klicken und nach einer Weile sieht er eine Art erweitertes Feedback zu dem, was er getan hat.  Und in diesem Moment beginnen wir, uns unserem Problem zu nähern. </p><a name="habracut"></a><br><p>  Die Überprüfung dieses Labors ist wie folgt organisiert: Wir senden ein Kontrolldatenpaket an Kafka des Teilnehmers, dann überträgt Gobblin das Datenpaket an HDFS, dann nimmt Airflow dieses Datenpaket und legt es in ClickHouse ab.  Der Trick ist, dass Airflow dies nicht in Echtzeit tun sollte, sondern nach einem Zeitplan: Alle 15 Minuten nimmt es eine Reihe von Dateien und wirft sie ein. </p><br><p> Es stellt sich heraus, dass wir auf Anfrage des Prüfers hier und jetzt irgendwie ihre DAG selbst auslösen müssen.  Beim Googeln haben wir herausgefunden, dass es für die späteren Versionen von Airflow die sogenannte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">experimentelle API gibt</a> .  Das Wort <code>experimental</code> klingt natürlich beängstigend, aber was zu tun ist ... Plötzlich wird es hochfliegen. </p><br><p>  Als Nächstes beschreiben wir den gesamten Weg: von der Installation von Airflow bis zur Generierung einer POST-Anforderung, die mithilfe der experimentellen API eine DAG auslöst.  Wir werden mit Ubuntu 16.04 arbeiten. </p><br><h3>  1. Installieren des Luftstroms </h3><br><p>  Lassen Sie uns überprüfen, ob wir Python 3 und virtualenv haben. </p><br><pre> <code class="python hljs">$ python3 --version Python <span class="hljs-number"><span class="hljs-number">3.6</span></span><span class="hljs-number"><span class="hljs-number">.6</span></span> $ virtualenv --version <span class="hljs-number"><span class="hljs-number">15.2</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span></code> </pre> <br><p>  Wenn etwas davon fehlt, installieren Sie es. </p><br><p>  Erstellen Sie nun ein Verzeichnis, in dem wir weiterhin mit Airflow arbeiten werden. </p><br><pre> <code class="bash hljs">$ mkdir &lt;your name of directory&gt; $ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /path/to/your/new/directory $ virtualenv -p <span class="hljs-built_in"><span class="hljs-built_in">which</span></span> python3 venv $ <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> venv/bin/activate (venv) $</code> </pre> <br><p>  Luftstrom installieren: </p><br><pre> <code class="bash hljs">(venv) $ pip install airflow</code> </pre> <br><p>  Die Version, an der wir gearbeitet haben: 1.10. </p><br><p>  Jetzt müssen wir das Verzeichnis <code>airflow_home</code> erstellen, in dem sich die DAG-Dateien und Airflow-Plugins befinden.  <code>AIRFLOW_HOME</code> nach dem Erstellen des Verzeichnisses die Umgebungsvariable <code>AIRFLOW_HOME</code> . </p><br><pre> <code class="bash hljs">(venv) $ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /path/to/my/airflow/workspace (venv) $ mkdir airflow_home (venv) $ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> AIRFLOW_HOME=&lt;path to airflow_home&gt;</code> </pre> <br><p>  Der nächste Schritt besteht darin, den Befehl auszuführen, mit dem die Datenstromdatenbank in SQLite erstellt und initialisiert wird: </p><br><pre> <code class="bash hljs">(venv) $ airflow initdb</code> </pre> <br><p>  Die Datenbank wird standardmäßig in <code>airflow.db</code> erstellt. </p><br><p>  Überprüfen Sie, ob Airflow installiert ist: </p><br><pre> <code class="bash hljs">$ airflow version [2018-11-26 19:38:19,607] {__init__.py:57} INFO - Using executor SequentialExecutor [2018-11-26 19:38:19,745] {driver.py:123} INFO - Generating grammar tables from /usr/lib/python3.6/lib2to3/Grammar.txt [2018-11-26 19:38:19,771] {driver.py:123} INFO - Generating grammar tables from /usr/lib/python3.6/lib2to3/PatternGrammar.txt ____________ _____________ ____ |__( )_________ __/__ /________ __ ____ /| |_ /__ ___/_ /_ __ /_ __ \_ | /| / / ___ ___ | / _ / _ __/ _ / / /_/ /_ |/ |/ / _/_/ |_/_/ /_/ /_/ /_/ \____/____/|__/ v1.10.0</code> </pre> <br><p>  Wenn der Befehl funktioniert hat, hat Airflow seine Konfigurationsdatei <code>airflow.cfg</code> in <code>AIRFLOW_HOME</code> : </p><br><pre> <code class="bash hljs">$ tree . ├── airflow.cfg └── unittests.cfg</code> </pre> <br><p>  Airflow verfügt über eine Webschnittstelle.  Es kann durch Ausführen des folgenden Befehls gestartet werden: </p><br><pre> <code class="bash hljs">(venv) $ airflow webserver --port 8081</code> </pre> <br><p>  Jetzt können Sie in einem Browser auf Port 8081 auf dem Host, auf dem Airflow gestartet wurde, auf die Weboberfläche zugreifen, z. B.: <code>&lt;hostname:8081&gt;</code> . </p><br><h3 id="2-rabota-s-experimental-api">  2. Arbeiten mit der experimentellen API </h3><br><p>  In diesem Fall ist Airflow konfiguriert und betriebsbereit.  Wir müssen jedoch auch die experimentelle API ausführen.  Unsere Prüfer sind in Python geschrieben, sodass alle Anfragen mithilfe der <code>requests</code> bearbeitet werden. </p><br><p>  Tatsächlich funktioniert die API bereits für einfache Abfragen.  Mit einer solchen Anforderung können Sie beispielsweise den Betrieb testen: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> requests &gt;&gt;&gt; host = &lt;your hostname&gt; &gt;&gt;&gt; airflow_port = <span class="hljs-number"><span class="hljs-number">8081</span></span> <span class="hljs-comment"><span class="hljs-comment">#   ,    8080 &gt;&gt;&gt; requests.get('http://{}:{}/{}'.format(host, airflow_port, 'api/experimental/test').text 'OK'</span></span></code> </pre> <br><p>  Wenn Sie eine solche Nachricht als Antwort erhalten haben, bedeutet dies, dass alles funktioniert. </p><br><p>  Wenn wir jedoch die DAG aktivieren möchten, werden wir feststellen, dass diese Art von Anfrage nicht ohne Authentifizierung gestellt werden kann. </p><br><p>  Dazu müssen Sie eine Reihe von Aktionen ausführen. </p><br><p>  Zunächst müssen Sie dies zur Konfiguration hinzufügen: </p><br><pre> <code class="plaintext hljs">[api] auth_backend = airflow.contrib.auth.backends.password_auth</code> </pre> <br><p>  Anschließend müssen Sie Ihren Benutzer mit Administratorrechten erstellen: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> airflow &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> models, settings &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow.contrib.auth.backends.password_auth <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PasswordUser &gt;&gt;&gt; user = PasswordUser(models.Admin()) &gt;&gt;&gt; user.username = <span class="hljs-string"><span class="hljs-string">'new_user_name'</span></span> &gt;&gt;&gt; user.password = <span class="hljs-string"><span class="hljs-string">'set_the_password'</span></span> &gt;&gt;&gt; session = settings.Session() &gt;&gt;&gt; session.add(user) &gt;&gt;&gt; session.commit() &gt;&gt;&gt; session.close() &gt;&gt;&gt; exit()</code> </pre> <br><p>  Anschließend müssen Sie einen Benutzer mit normalen Rechten erstellen, der einen DAG-Trigger ausführen darf. </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> airflow &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> models, settings &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow.contrib.auth.backends.password_auth <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PasswordUser &gt;&gt;&gt; user = PasswordUser(models.User()) &gt;&gt;&gt; user.username = <span class="hljs-string"><span class="hljs-string">'newprolab'</span></span> &gt;&gt;&gt; user.password = <span class="hljs-string"><span class="hljs-string">'Newprolab2019!'</span></span> &gt;&gt;&gt; session = settings.Session() &gt;&gt;&gt; session.add(user) &gt;&gt;&gt; session.commit() &gt;&gt;&gt; session.close() &gt;&gt;&gt; exit()</code> </pre> <br><p>  Jetzt ist alles fertig. </p><br><h3 id="3-zapusk-post-zaprosa">  3. Starten einer POST-Anforderung </h3><br><p>  Die POST-Anfrage selbst sieht folgendermaßen aus: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>dag_id = newprolab &gt;&gt;&gt; url = <span class="hljs-string"><span class="hljs-string">'http://{}:{}/{}/{}/{}'</span></span>.format(host, airflow_port, <span class="hljs-string"><span class="hljs-string">'api/experimental/dags'</span></span>, dag_id, <span class="hljs-string"><span class="hljs-string">'dag_runs'</span></span>) &gt;&gt;&gt; data = {<span class="hljs-string"><span class="hljs-string">"conf"</span></span>:<span class="hljs-string"><span class="hljs-string">"{\"key\":\"value\"}"</span></span>} &gt;&gt;&gt; headers = {<span class="hljs-string"><span class="hljs-string">'Content-type'</span></span>: <span class="hljs-string"><span class="hljs-string">'application/json'</span></span>} &gt;&gt;&gt; auth = (<span class="hljs-string"><span class="hljs-string">'newprolab'</span></span>, <span class="hljs-string"><span class="hljs-string">'Newprolab2019!'</span></span>) &gt;&gt;&gt; uri = requests.post(url, data=json.dumps(data), headers=headers, auth=auth) &gt;&gt;&gt; uri.text <span class="hljs-string"><span class="hljs-string">'{\n "message": "Created &lt;DagRun newprolab @ 2019-03-27 10:24:25+00:00: manual__2019-03-27T10:24:25+00:00, externally triggered: True&gt;"\n}\n'</span></span></code> </pre> <br><p>  Anfrage erfolgreich bearbeitet. </p><br><p>  Dementsprechend geben wir der DAG etwas Zeit für die Verarbeitung und stellen eine Anfrage an die ClickHouse-Tabelle, um zu versuchen, ein Steuerdatenpaket abzufangen. </p><br><p>  Überprüfung abgeschlossen. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de445852/">https://habr.com/ru/post/de445852/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de445834/index.html">Entwicklung der Fähigkeit zur Verwendung von Gruppierung und Datenvisualisierung in Python</a></li>
<li><a href="../de445838/index.html">Robotik für Kinder: Roboteraugen</a></li>
<li><a href="../de445844/index.html">GitLab 11.9 wurde mit Secrets Detection- und Multiple Marge Request Resolution-Regeln veröffentlicht</a></li>
<li><a href="../de445846/index.html">Professionelle Postgres</a></li>
<li><a href="../de445850/index.html">Reparieren Sie das Sharp Memowriter EL-7000 Note-Speicher- und Druckgerät nach Batterieverlust</a></li>
<li><a href="../de445854/index.html">Suchen eines JS-Frameworks für die UI-Generierung</a></li>
<li><a href="../de445856/index.html">Telegramm nach 5 Jahren</a></li>
<li><a href="../de445858/index.html">Antiquitäten: Als Telefone seltsam waren</a></li>
<li><a href="../de445860/index.html">Die geringe Volatilität von Bitcoin (BTC) führt zum nächsten Crypto Bull Run</a></li>
<li><a href="../de445862/index.html">JS von allen Seiten: Top 10 Berichte von HolyJS 2018 Moskau</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>