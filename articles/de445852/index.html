<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üïµüèø üôÖüèª üïï So erstellen Sie einen DAG-Trigger in Airflow mithilfe der experimentellen API üñ§ ‚úãüèª üëó</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bei der Vorbereitung unserer Bildungsprogramme sto√üen wir regelm√§√üig auf Schwierigkeiten bei der Arbeit mit einigen Tools. Und in dem Moment, in dem w...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>So erstellen Sie einen DAG-Trigger in Airflow mithilfe der experimentellen API</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/newprolab/blog/445852/"><p>  Bei der Vorbereitung unserer Bildungsprogramme sto√üen wir regelm√§√üig auf Schwierigkeiten bei der Arbeit mit einigen Tools.  Und in dem Moment, in dem wir ihnen begegnen, gibt es nicht immer genug Dokumentationen und Artikel, um dieses Problem zu l√∂sen. </p><br><p>  Dies war beispielsweise 2015 der Fall, und wir haben im Rahmen des Big Data Specialist-Programms einen Hadoop-Cluster mit Spark f√ºr 35 gleichzeitige Benutzer verwendet.  Wie man es unter einem solchen Anwenderfall mit YARN kocht, war nicht klar.  Nachdem sie es herausgefunden und den Weg alleine gegangen waren, machten sie einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beitrag auf Habr√©</a> und traten auch beim <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Moscow Spark Meetup auf</a> . </p><br><h3 id="predystoriya">  Hintergrund </h3><br><p>  Dieses Mal werden wir √ºber ein anderes Programm sprechen - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Data Engineer</a> .  Unsere Teilnehmer bauen zwei Arten von Architektur darauf: Lambda und Kappa.  In der Lamdba-Architektur wird Airflow im Rahmen der Stapelverarbeitung zum √úbertragen von Protokollen von HDFS zu ClickHouse verwendet. </p><br><p>  Im Allgemeinen ist alles gut.  Lassen Sie sie ihre Pipelines bauen.  Es gibt jedoch ein ‚ÄûAber‚Äú: Alle unsere Programme sind im Hinblick auf den Lernprozess selbst technologisch.  Um das Labor zu √ºberpr√ºfen, verwenden wir automatische Pr√ºfer: Der Teilnehmer muss zu seinem pers√∂nlichen Konto gehen, auf die Schaltfl√§che ‚ÄûPr√ºfen‚Äú klicken und nach einer Weile sieht er eine Art erweitertes Feedback zu dem, was er getan hat.  Und in diesem Moment beginnen wir, uns unserem Problem zu n√§hern. </p><a name="habracut"></a><br><p>  Die √úberpr√ºfung dieses Labors ist wie folgt organisiert: Wir senden ein Kontrolldatenpaket an Kafka des Teilnehmers, dann √ºbertr√§gt Gobblin das Datenpaket an HDFS, dann nimmt Airflow dieses Datenpaket und legt es in ClickHouse ab.  Der Trick ist, dass Airflow dies nicht in Echtzeit tun sollte, sondern nach einem Zeitplan: Alle 15 Minuten nimmt es eine Reihe von Dateien und wirft sie ein. </p><br><p> Es stellt sich heraus, dass wir auf Anfrage des Pr√ºfers hier und jetzt irgendwie ihre DAG selbst ausl√∂sen m√ºssen.  Beim Googeln haben wir herausgefunden, dass es f√ºr die sp√§teren Versionen von Airflow die sogenannte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">experimentelle API gibt</a> .  Das Wort <code>experimental</code> klingt nat√ºrlich be√§ngstigend, aber was zu tun ist ... Pl√∂tzlich wird es hochfliegen. </p><br><p>  Als N√§chstes beschreiben wir den gesamten Weg: von der Installation von Airflow bis zur Generierung einer POST-Anforderung, die mithilfe der experimentellen API eine DAG ausl√∂st.  Wir werden mit Ubuntu 16.04 arbeiten. </p><br><h3>  1. Installieren des Luftstroms </h3><br><p>  Lassen Sie uns √ºberpr√ºfen, ob wir Python 3 und virtualenv haben. </p><br><pre> <code class="python hljs">$ python3 --version Python <span class="hljs-number"><span class="hljs-number">3.6</span></span><span class="hljs-number"><span class="hljs-number">.6</span></span> $ virtualenv --version <span class="hljs-number"><span class="hljs-number">15.2</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span></code> </pre> <br><p>  Wenn etwas davon fehlt, installieren Sie es. </p><br><p>  Erstellen Sie nun ein Verzeichnis, in dem wir weiterhin mit Airflow arbeiten werden. </p><br><pre> <code class="bash hljs">$ mkdir &lt;your name of directory&gt; $ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /path/to/your/new/directory $ virtualenv -p <span class="hljs-built_in"><span class="hljs-built_in">which</span></span> python3 venv $ <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> venv/bin/activate (venv) $</code> </pre> <br><p>  Luftstrom installieren: </p><br><pre> <code class="bash hljs">(venv) $ pip install airflow</code> </pre> <br><p>  Die Version, an der wir gearbeitet haben: 1.10. </p><br><p>  Jetzt m√ºssen wir das Verzeichnis <code>airflow_home</code> erstellen, in dem sich die DAG-Dateien und Airflow-Plugins befinden.  <code>AIRFLOW_HOME</code> nach dem Erstellen des Verzeichnisses die Umgebungsvariable <code>AIRFLOW_HOME</code> . </p><br><pre> <code class="bash hljs">(venv) $ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /path/to/my/airflow/workspace (venv) $ mkdir airflow_home (venv) $ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> AIRFLOW_HOME=&lt;path to airflow_home&gt;</code> </pre> <br><p>  Der n√§chste Schritt besteht darin, den Befehl auszuf√ºhren, mit dem die Datenstromdatenbank in SQLite erstellt und initialisiert wird: </p><br><pre> <code class="bash hljs">(venv) $ airflow initdb</code> </pre> <br><p>  Die Datenbank wird standardm√§√üig in <code>airflow.db</code> erstellt. </p><br><p>  √úberpr√ºfen Sie, ob Airflow installiert ist: </p><br><pre> <code class="bash hljs">$ airflow version [2018-11-26 19:38:19,607] {__init__.py:57} INFO - Using executor SequentialExecutor [2018-11-26 19:38:19,745] {driver.py:123} INFO - Generating grammar tables from /usr/lib/python3.6/lib2to3/Grammar.txt [2018-11-26 19:38:19,771] {driver.py:123} INFO - Generating grammar tables from /usr/lib/python3.6/lib2to3/PatternGrammar.txt ____________ _____________ ____ |__( )_________ __/__ /________ __ ____ /| |_ /__ ___/_ /_ __ /_ __ \_ | /| / / ___ ___ | / _ / _ __/ _ / / /_/ /_ |/ |/ / _/_/ |_/_/ /_/ /_/ /_/ \____/____/|__/ v1.10.0</code> </pre> <br><p>  Wenn der Befehl funktioniert hat, hat Airflow seine Konfigurationsdatei <code>airflow.cfg</code> in <code>AIRFLOW_HOME</code> : </p><br><pre> <code class="bash hljs">$ tree . ‚îú‚îÄ‚îÄ airflow.cfg ‚îî‚îÄ‚îÄ unittests.cfg</code> </pre> <br><p>  Airflow verf√ºgt √ºber eine Webschnittstelle.  Es kann durch Ausf√ºhren des folgenden Befehls gestartet werden: </p><br><pre> <code class="bash hljs">(venv) $ airflow webserver --port 8081</code> </pre> <br><p>  Jetzt k√∂nnen Sie in einem Browser auf Port 8081 auf dem Host, auf dem Airflow gestartet wurde, auf die Weboberfl√§che zugreifen, z. B.: <code>&lt;hostname:8081&gt;</code> . </p><br><h3 id="2-rabota-s-experimental-api">  2. Arbeiten mit der experimentellen API </h3><br><p>  In diesem Fall ist Airflow konfiguriert und betriebsbereit.  Wir m√ºssen jedoch auch die experimentelle API ausf√ºhren.  Unsere Pr√ºfer sind in Python geschrieben, sodass alle Anfragen mithilfe der <code>requests</code> bearbeitet werden. </p><br><p>  Tats√§chlich funktioniert die API bereits f√ºr einfache Abfragen.  Mit einer solchen Anforderung k√∂nnen Sie beispielsweise den Betrieb testen: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> requests &gt;&gt;&gt; host = &lt;your hostname&gt; &gt;&gt;&gt; airflow_port = <span class="hljs-number"><span class="hljs-number">8081</span></span> <span class="hljs-comment"><span class="hljs-comment">#   ,    8080 &gt;&gt;&gt; requests.get('http://{}:{}/{}'.format(host, airflow_port, 'api/experimental/test').text 'OK'</span></span></code> </pre> <br><p>  Wenn Sie eine solche Nachricht als Antwort erhalten haben, bedeutet dies, dass alles funktioniert. </p><br><p>  Wenn wir jedoch die DAG aktivieren m√∂chten, werden wir feststellen, dass diese Art von Anfrage nicht ohne Authentifizierung gestellt werden kann. </p><br><p>  Dazu m√ºssen Sie eine Reihe von Aktionen ausf√ºhren. </p><br><p>  Zun√§chst m√ºssen Sie dies zur Konfiguration hinzuf√ºgen: </p><br><pre> <code class="plaintext hljs">[api] auth_backend = airflow.contrib.auth.backends.password_auth</code> </pre> <br><p>  Anschlie√üend m√ºssen Sie Ihren Benutzer mit Administratorrechten erstellen: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> airflow &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> models, settings &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow.contrib.auth.backends.password_auth <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PasswordUser &gt;&gt;&gt; user = PasswordUser(models.Admin()) &gt;&gt;&gt; user.username = <span class="hljs-string"><span class="hljs-string">'new_user_name'</span></span> &gt;&gt;&gt; user.password = <span class="hljs-string"><span class="hljs-string">'set_the_password'</span></span> &gt;&gt;&gt; session = settings.Session() &gt;&gt;&gt; session.add(user) &gt;&gt;&gt; session.commit() &gt;&gt;&gt; session.close() &gt;&gt;&gt; exit()</code> </pre> <br><p>  Anschlie√üend m√ºssen Sie einen Benutzer mit normalen Rechten erstellen, der einen DAG-Trigger ausf√ºhren darf. </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> airflow &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> models, settings &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow.contrib.auth.backends.password_auth <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PasswordUser &gt;&gt;&gt; user = PasswordUser(models.User()) &gt;&gt;&gt; user.username = <span class="hljs-string"><span class="hljs-string">'newprolab'</span></span> &gt;&gt;&gt; user.password = <span class="hljs-string"><span class="hljs-string">'Newprolab2019!'</span></span> &gt;&gt;&gt; session = settings.Session() &gt;&gt;&gt; session.add(user) &gt;&gt;&gt; session.commit() &gt;&gt;&gt; session.close() &gt;&gt;&gt; exit()</code> </pre> <br><p>  Jetzt ist alles fertig. </p><br><h3 id="3-zapusk-post-zaprosa">  3. Starten einer POST-Anforderung </h3><br><p>  Die POST-Anfrage selbst sieht folgenderma√üen aus: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>dag_id = newprolab &gt;&gt;&gt; url = <span class="hljs-string"><span class="hljs-string">'http://{}:{}/{}/{}/{}'</span></span>.format(host, airflow_port, <span class="hljs-string"><span class="hljs-string">'api/experimental/dags'</span></span>, dag_id, <span class="hljs-string"><span class="hljs-string">'dag_runs'</span></span>) &gt;&gt;&gt; data = {<span class="hljs-string"><span class="hljs-string">"conf"</span></span>:<span class="hljs-string"><span class="hljs-string">"{\"key\":\"value\"}"</span></span>} &gt;&gt;&gt; headers = {<span class="hljs-string"><span class="hljs-string">'Content-type'</span></span>: <span class="hljs-string"><span class="hljs-string">'application/json'</span></span>} &gt;&gt;&gt; auth = (<span class="hljs-string"><span class="hljs-string">'newprolab'</span></span>, <span class="hljs-string"><span class="hljs-string">'Newprolab2019!'</span></span>) &gt;&gt;&gt; uri = requests.post(url, data=json.dumps(data), headers=headers, auth=auth) &gt;&gt;&gt; uri.text <span class="hljs-string"><span class="hljs-string">'{\n "message": "Created &lt;DagRun newprolab @ 2019-03-27 10:24:25+00:00: manual__2019-03-27T10:24:25+00:00, externally triggered: True&gt;"\n}\n'</span></span></code> </pre> <br><p>  Anfrage erfolgreich bearbeitet. </p><br><p>  Dementsprechend geben wir der DAG etwas Zeit f√ºr die Verarbeitung und stellen eine Anfrage an die ClickHouse-Tabelle, um zu versuchen, ein Steuerdatenpaket abzufangen. </p><br><p>  √úberpr√ºfung abgeschlossen. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de445852/">https://habr.com/ru/post/de445852/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de445834/index.html">Entwicklung der F√§higkeit zur Verwendung von Gruppierung und Datenvisualisierung in Python</a></li>
<li><a href="../de445838/index.html">Robotik f√ºr Kinder: Roboteraugen</a></li>
<li><a href="../de445844/index.html">GitLab 11.9 wurde mit Secrets Detection- und Multiple Marge Request Resolution-Regeln ver√∂ffentlicht</a></li>
<li><a href="../de445846/index.html">Professionelle Postgres</a></li>
<li><a href="../de445850/index.html">Reparieren Sie das Sharp Memowriter EL-7000 Note-Speicher- und Druckger√§t nach Batterieverlust</a></li>
<li><a href="../de445854/index.html">Suchen eines JS-Frameworks f√ºr die UI-Generierung</a></li>
<li><a href="../de445856/index.html">Telegramm nach 5 Jahren</a></li>
<li><a href="../de445858/index.html">Antiquit√§ten: Als Telefone seltsam waren</a></li>
<li><a href="../de445860/index.html">Die geringe Volatilit√§t von Bitcoin (BTC) f√ºhrt zum n√§chsten Crypto Bull Run</a></li>
<li><a href="../de445862/index.html">JS von allen Seiten: Top 10 Berichte von HolyJS 2018 Moskau</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>