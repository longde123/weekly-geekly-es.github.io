<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèΩ‚Äçü§ù‚Äçüë®üèæ ‚ôøÔ∏è üÜô Intelig√™ncia artificial de uso geral. TK, status atual, perspectivas üÜñ üì° üè†</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Atualmente, as palavras "intelig√™ncia artificial" significam muitos sistemas diferentes - de uma rede neural para reconhecimento de imagens a um bot p...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Intelig√™ncia artificial de uso geral. TK, status atual, perspectivas</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/468379/">  Atualmente, as palavras "intelig√™ncia artificial" significam muitos sistemas diferentes - de uma rede neural para reconhecimento de imagens a um bot para jogar Quake.  A Wikipedia fornece uma defini√ß√£o maravilhosa de IA - essa √© "a propriedade de sistemas inteligentes para executar fun√ß√µes criativas que s√£o tradicionalmente consideradas prerrogativas do homem".  Ou seja, √© claramente visto a partir da defini√ß√£o - se uma determinada fun√ß√£o foi automatizada com sucesso, ela deixa de ser considerada intelig√™ncia artificial. <br><br>  No entanto, quando a tarefa de "criar intelig√™ncia artificial" foi definida pela primeira vez, a IA significava algo diferente.  Esse objetivo agora √© chamado de IA forte ou AI de uso geral. <br><a name="habracut"></a><br><h2>  Declara√ß√£o do problema </h2><br>  Agora, existem duas formula√ß√µes bem conhecidas do problema.  O primeiro √© IA forte.  O segundo √© uma IA de uso geral (tamb√©m conhecida como Artifical General Intelligence, AGI abreviada). <br>  Upd.  Nos coment√°rios, eles me dizem que essa diferen√ßa √© mais prov√°vel no n√≠vel do idioma.  Em russo, a palavra "intelig√™ncia" n√£o significa exatamente o que a palavra "intelig√™ncia" em ingl√™s <br><br>  <b>Uma IA forte</b> √© uma IA hipot√©tica que poderia fazer tudo o que uma pessoa poderia fazer.  Costuma-se mencionar que ele deve passar no teste de Turing no cen√°rio inicial (hum, as pessoas passam?), Estar ciente de si mesmo como uma pessoa separada e ser capaz de atingir seus objetivos. <br><br>  Ou seja, √© algo como uma pessoa artificial.  Na minha opini√£o, a utilidade de uma IA desse tipo √© principalmente pesquisa, porque as defini√ß√µes de uma IA forte n√£o dizem em lugar algum quais ser√£o seus objetivos. <br><br>  <b>AGI ou AI de uso geral</b> √© uma "m√°quina de resultados".  Ela recebe um determinado objetivo na entrada - e fornece algumas a√ß√µes de controle em motores / lasers / placas de rede / monitores.  E o objetivo √© alcan√ßado.  Ao mesmo tempo, a AGI inicialmente n√£o tem conhecimento sobre o meio ambiente - apenas sensores, atuadores e o canal atrav√©s do qual estabelece metas.  O sistema de gerenciamento ser√° considerado um AGI se conseguir atingir algum objetivo em qualquer ambiente.  N√≥s a colocamos para dirigir um carro e evitar acidentes - ela vai lidar com isso.  N√≥s a colocamos no controle de um reator nuclear para que haja mais energia, mas n√£o exploda - ela pode lidar com isso.  Vamos dar uma caixa de correio e instruir para vender aspiradores de p√≥ - tamb√©m vai lidar.  AGI √© um solucionador de "problemas inversos".  Verificar quantos aspiradores de p√≥ s√£o vendidos √© uma quest√£o simples.  Mas descobrir como convencer uma pessoa a comprar esse aspirador de p√≥ j√° √© uma tarefa para o intelecto. <br><br>  Neste artigo, vou falar sobre a AGI.  Sem testes de Turing, sem autoconsci√™ncia, sem personalidades artificiais - IA excepcionalmente pragm√°tica e operadores menos pragm√°ticos. <br><br><h2>  Situa√ß√£o atual </h2><br>  Agora, existe uma classe de sistemas como Aprendizado por Refor√ßo ou Aprendizado Refor√ßado.  Isso √© algo como AGI, apenas sem versatilidade.  Eles s√£o capazes de aprender e, devido a isso, alcan√ßar objetivos em uma variedade de ambientes.  Mas ainda est√£o muito longe de alcan√ßar metas em qualquer ambiente. <br><br>  Em geral, como s√£o organizados os sistemas de Aprendizado por Refor√ßo e quais s√£o seus problemas? <br><br><img src="https://habrastorage.org/webt/oj/3-/dl/oj3-dl6vkgtbhaxeilj7rkl0jxe.png"><br><br>  Qualquer RL √© organizado assim.  Existe um sistema de controle, alguns sinais sobre a realidade circundante entram atrav√©s dos sensores (estado) e atrav√©s dos √≥rg√£os de governo (a√ß√µes) que atuam na realidade circundante.  Recompensa √© um sinal de refor√ßo.  Nos sistemas RL, o refor√ßo √© formado de fora da unidade de controle e indica o qu√£o bem a IA lida com a consecu√ß√£o do objetivo.  Quantos aspiradores de p√≥ vendidos no √∫ltimo minuto, por exemplo. <br>  Em seguida, uma tabela √© formada de algo assim (vou cham√°-la de tabela SAR): <br><br><img src="https://habrastorage.org/webt/lp/oo/ek/lpooekbdpwktkkmvsilzsytsnbo.png"><br><br>  O eixo do tempo √© direcionado para baixo.  A tabela mostra tudo o que a IA fez, tudo o que ele viu e todos os sinais de refor√ßo.  Geralmente, para que RL fa√ßa algo significativo, ele primeiro precisa fazer movimentos aleat√≥rios por um tempo ou observar os movimentos de outra pessoa.  Em geral, o RL inicia quando j√° existem pelo menos algumas linhas na tabela SAR. <br>  O que acontece depois? <br><br><h3>  Sarsa </h3><br>  A forma mais simples de aprendizado por refor√ßo. <br><br>  Adotamos algum tipo de modelo de aprendizado de m√°quina e, usando uma combina√ß√£o de S e A (estado e a√ß√£o), prevemos o R total para os pr√≥ximos ciclos de clock.  Por exemplo, veremos que (com base na tabela acima) se voc√™ disser a uma mulher "seja homem, compre um aspirador de p√≥!", A recompensa ser√° baixa e, se voc√™ disser a mesma coisa a um homem, ser√° alta. <br><br>  Quais modelos espec√≠ficos podem ser usados ‚Äã‚Äã- descreverei mais adiante, por enquanto apenas direi que essas n√£o s√£o apenas redes neurais.  Voc√™ pode usar √°rvores de decis√£o ou at√© mesmo definir uma fun√ß√£o em forma de tabela. <br><br>  E ent√£o acontece o seguinte.  O AI recebe outra mensagem ou link para outro cliente.  Todos os dados do cliente s√£o inseridos na AI de fora - consideraremos a base de clientes e o contador de mensagens como parte do sistema do sensor.  Ou seja, resta atribuir um A (a√ß√£o) e aguardar refor√ßos.  A IA toma todas as a√ß√µes poss√≠veis e, por sua vez, prev√™ (usando o mesmo modelo de Machine Learning) - o que acontecer√° se eu fizer isso?  E se for?  E quanto refor√ßo ser√° para isso?  E ent√£o RL executa a a√ß√£o para a qual a recompensa m√°xima √© esperada. <br><br>  Eu introduzi um sistema t√£o simples e desajeitado em um dos meus jogos.  A SARSA contrata unidades no jogo e se adapta em caso de altera√ß√£o nas regras do jogo. <br><br>  Al√©m disso, em todos os tipos de treinamento refor√ßado, h√° um desconto de recompensas e um dilema de explorar / explorar. <br><br>  O desconto de pr√™mios √© uma abordagem desse tipo quando a RL tenta maximizar n√£o o valor da recompensa para os pr√≥ximos N movimentos, mas o valor ponderado de acordo com o princ√≠pio "100 rublos agora √© melhor que 110 em um ano".  Por exemplo, se o fator de desconto for 0,9 e o horizonte de planejamento for 3, treinaremos o modelo n√£o no R total para os pr√≥ximos 3 ciclos de rel√≥gio, mas em R1 * 0,9 + R2 * 0,81 + R3 * 0,729.  Por que isso √© necess√°rio?  Ent√£o, essa IA, criando lucro em algum lugar no infinito, n√£o precisamos.  Precisamos de uma IA que gere lucro aqui e agora. <br>  Explorar / explorar o dilema.  Se a RL fizer o que o modelo considera ideal, nunca saber√° se existem estrat√©gias melhores.  Explorar √© uma estrat√©gia na qual RL faz o que promete recompensas m√°ximas.  Explore √© uma estrat√©gia na qual a RL faz algo para explorar o ambiente em busca de melhores estrat√©gias.  Como implementar uma intelig√™ncia eficaz?  Por exemplo, voc√™ pode executar uma a√ß√£o aleat√≥ria a cada poucas medidas.  Ou voc√™ pode criar n√£o um modelo preditivo, mas v√°rios com configura√ß√µes ligeiramente diferentes.  Eles produzir√£o resultados diferentes.  Quanto maior a diferen√ßa, maior o grau de incerteza dessa op√ß√£o.  Voc√™ pode executar a a√ß√£o para que ela tenha o valor m√°ximo: M + k * std, em que M √© a previs√£o m√©dia de todos os modelos, std √© o desvio padr√£o das previs√µes e k √© o coeficiente de curiosidade. <br><br>  <b>Quais s√£o as desvantagens?</b> <br><br>  Digamos que temos op√ß√µes.  V√° para a meta (que fica a 10 km de dist√¢ncia e o caminho √© bom) de carro ou a p√©.  E ent√£o, ap√≥s essa escolha, temos op√ß√µes - mova-se com cuidado ou tente colidir com cada pilar. <br><br>  A pessoa dir√° imediatamente que geralmente √© melhor dirigir um carro e se comportar com cuidado. <br><br>  Mas SARSA ... Ele analisar√° o que a decis√£o de ir de carro levou antes.  Mas isso levou a isso.  No est√°gio do conjunto inicial de estat√≠sticas, a IA dirigiu de forma imprudente e caiu em algum lugar na metade dos casos.  Sim, ele pode dirigir bem.  Mas quando ele escolhe ir de carro, ele n√£o sabe o que escolher√° na pr√≥xima jogada.  Ele tem estat√≠sticas - ent√£o, na metade dos casos, ele escolheu a op√ß√£o apropriada e na metade - suicida.  Portanto, em m√©dia, √© melhor andar. <br><br>  A SARSA acredita que o agente seguir√° a mesma estrat√©gia usada para preencher a tabela.  E age nessa base.  Mas e se assumirmos o contr√°rio - que o agente aderir√° √† melhor estrat√©gia nos pr√≥ximos movimentos? <br><br><h3>  Q-learning </h3><br>  Este modelo calcula para cada estado a recompensa total m√°xima alcan√ß√°vel a partir dele.  E ele escreve em uma coluna especial Q. Ou seja, se do estado S voc√™ pode obter 2 pontos ou 1, dependendo da jogada, ent√£o Q (S) ser√° igual a 2 (com uma profundidade de previs√£o de 1).  Que recompensa pode ser obtida do estado S, aprendemos com o modelo preditivo Y (S, A).  (Estado S, a√ß√£o A). <br><br>  Em seguida, criamos um modelo preditivo Q (S, A) - ou seja, em que estado Q iremos se executarmos a a√ß√£o A de S. E criamos a pr√≥xima coluna na tabela - Q2.  Ou seja, o Q m√°ximo que pode ser obtido do estado S (classificamos todos os poss√≠veis A). <br><br>  Em seguida, criamos um modelo de regress√£o Q3 (S, A) - isto √©, para o estado em que Q2 iremos se executarmos a a√ß√£o A de S. <br><br>  E assim por diante  Assim, podemos alcan√ßar uma profundidade ilimitada de previs√£o. <br><br><img src="https://habrastorage.org/webt/ie/e_/gn/iee_gnm9ldz5uiv0dmyj4jni470.jpeg"><br><br>  Na figura, R √© o refor√ßo. <br><br>  E ent√£o, a cada movimento, selecionamos a a√ß√£o que promete o maior Qn.  Se aplic√°ssemos esse algoritmo ao xadrez, obter√≠amos algo como um minimax ideal.  Algo quase equivalente a calcular mal os movimentos para grandes profundidades. <br><br>  Um exemplo comum de comportamento de q-learning.  O ca√ßador tem uma lan√ßa, e ele vai com ele at√© o urso, por sua pr√≥pria iniciativa.  Ele sabe que a grande maioria de seus movimentos futuros tem uma recompensa negativa muito grande (h√° muito mais maneiras de perder do que maneiras de ganhar), ele sabe que existem movimentos com uma recompensa positiva.  O ca√ßador acredita que no futuro ele far√° as melhores jogadas (e n√£o se sabe quais s√£o as da SARSA) e, se ele fizer as melhores jogadas, derrotar√° o urso.  Ou seja, para ir ao urso, basta que ele seja capaz de criar todos os elementos necess√°rios √† ca√ßa, mas n√£o √© necess√°rio ter experi√™ncia de sucesso imediato. <br><br>  Se o ca√ßador agisse no estilo da SARSA, ele assumiria que suas a√ß√µes no futuro seriam as mesmas de antes (apesar do fato de que agora ele tem uma bagagem de conhecimento diferente), e ele s√≥ aceitaria o urso se ele j√° fosse para e ele venceu, por exemplo, em&gt; 50% dos casos (bem, ou se outros ca√ßadores venceram em mais da metade dos casos, se ele aprender com a experi√™ncia deles). <br><br>  <b>Quais s√£o as desvantagens?</b> <br><br><ol><li>  O modelo n√£o lida com a realidade em mudan√ßa.  Se toda a nossa vida nos foi premiada por pressionar o bot√£o vermelho, e agora eles est√£o nos punindo, e nenhuma mudan√ßa vis√≠vel ocorreu ... QL dominar√° esse padr√£o por muito tempo. </li><li>  Qn pode ser uma fun√ß√£o muito complexa.  Por exemplo, para calcul√°-lo, voc√™ precisa rolar um ciclo de N itera√ß√µes - e isso n√£o funcionar√° mais r√°pido.  Um modelo preditivo geralmente tem complexidade limitada - mesmo uma grande rede neural tem um limite de complexidade e quase nenhum modelo de aprendizado de m√°quina pode girar ciclos. </li><li>  A realidade geralmente tem vari√°veis ‚Äã‚Äãocultas.  Por exemplo, que horas s√£o agora?  √â f√°cil descobrir se olhamos para o rel√≥gio, mas assim que desviar o olhar, essa j√° √© uma vari√°vel oculta.  Para levar em conta esses valores n√£o observ√°veis, √© necess√°rio que o modelo leve em considera√ß√£o n√£o apenas o estado atual, mas tamb√©m algum tipo de hist√≥rico.  Na QL, voc√™ pode fazer isso - por exemplo, para alimentar n√£o apenas o S ‚Äã‚Äãatual, mas tamb√©m v√°rios anteriores no neur√¥nio ou o que temos l√°.  Isso √© feito no RL, que joga os jogos da Atari.  Al√©m disso, voc√™ pode usar uma rede neural recorrente para previs√£o - deixe que ela execute sequencialmente v√°rios quadros do hist√≥rico e calcule o Qn. </li></ol><br><h3>  Sistemas baseados em modelo </h3><br>  Mas e se predizermos n√£o apenas R ou Q, mas geralmente todos os dados sensoriais?  Teremos constantemente uma c√≥pia de bolso da realidade e poderemos verificar nossos planos.  Nesse caso, estamos muito menos preocupados com a dificuldade de calcular a fun√ß√£o Q.  Sim, √© preciso calcular muitos rel√≥gios - bem, de qualquer maneira, para cada plano, executaremos repetidamente o modelo de previs√£o.  Planejando 10 avan√ßa?  Lan√ßamos o modelo 10 vezes e cada vez que alimentamos suas sa√≠das com suas entradas. <br><br>  <b>Quais s√£o as desvantagens?</b> <br><br><ol><li>  Intensidade de recursos.  Suponha que precisamos escolher duas alternativas em cada medida.  Ent√£o, por 10 ciclos de rel√≥gio, teremos 2 ^ 10 = 1024 planos poss√≠veis.  Cada plano √© de 10 lan√ßamentos de modelos.  Se controlarmos um avi√£o com dezenas de √≥rg√£os de governo?  E simulamos a realidade com um per√≠odo de 0,1 segundos?  Deseja ter um horizonte de planejamento por pelo menos alguns minutos?  Teremos que executar o modelo muitas vezes, h√° muitos ciclos de clock do processador para uma solu√ß√£o.  Mesmo que voc√™ otimize de alguma forma a enumera√ß√£o de planos, mesmo assim, existem ordens de magnitude com mais c√°lculos do que na QL. </li><li>  O problema do caos.  Alguns sistemas s√£o projetados para que at√© uma pequena imprecis√£o da simula√ß√£o de entrada leve a um enorme erro de sa√≠da.  Para combater isso, voc√™ pode executar v√°rias simula√ß√µes da realidade - um pouco diferente.  Eles produzir√£o resultados muito diferentes e, a partir disso, ser√° poss√≠vel entender que estamos na zona dessa instabilidade. </li></ol><br><h2>  M√©todo de enumera√ß√£o da estrat√©gia </h2><br>  Se tivermos acesso ao ambiente de teste para IA, se o executarmos n√£o na realidade, mas em uma simula√ß√£o, poderemos anotar de alguma forma a estrat√©gia do comportamento de nosso agente.  E ent√£o escolha - com a evolu√ß√£o ou outra coisa - uma estrat√©gia que leve ao lucro m√°ximo. <br>  ‚ÄúEscolha uma estrat√©gia‚Äù significa que precisamos primeiro aprender a escrever uma estrat√©gia de tal maneira que ela possa ser inserida no algoritmo de evolu√ß√£o.  Ou seja, podemos escrever a estrat√©gia com o c√≥digo do programa, mas em alguns lugares deixamos os coeficientes e deixamos que a evolu√ß√£o os pegue.  Ou podemos escrever uma estrat√©gia com uma rede neural - e deixar a evolu√ß√£o pegar o peso de suas conex√µes. <br><br>  Ou seja, n√£o h√° previs√£o aqui.  Nenhuma tabela SAR.  Simplesmente selecionamos uma estrat√©gia e ela imediatamente distribui a√ß√µes. <br><br>  Esse √© um m√©todo poderoso e eficaz. Se voc√™ quiser experimentar o RL e n√£o souber por onde come√ßar, recomendo.  Esta √© uma maneira muito barata de "ver um milagre". <br><br>  <b>Quais s√£o as desvantagens?</b> <br><br><ol><li>  A capacidade de executar as mesmas experi√™ncias muitas vezes √© necess√°ria.  Ou seja, devemos ser capazes de retroceder a realidade ao ponto de partida - dezenas de milhares de vezes.  Para tentar uma nova estrat√©gia. <br><br>  A vida raramente oferece tais oportunidades.  Normalmente, se temos um modelo do processo em que estamos interessados, n√£o podemos criar uma estrat√©gia astuta - podemos simplesmente elaborar um plano, como em uma abordagem baseada em modelos, mesmo com for√ßa bruta. </li><li>  Intoler√¢ncia √† experi√™ncia.  Temos uma tabela SAR para anos de experi√™ncia?  Podemos esquecer, isso n√£o se encaixa no conceito. </li></ol><br><h3>  Um m√©todo de enumerar estrat√©gias, mas "ao vivo" </h3><br>  A mesma enumera√ß√£o de estrat√©gias, mas na realidade viva.  Tentamos 10 medidas de uma estrat√©gia.  Ent√£o 10 mede outro.  Ent√£o 10 medidas do terceiro.  Depois, selecionamos aquele em que houve mais refor√ßo. <br>  Os melhores resultados para caminhar human√≥ides foram obtidos por esse m√©todo. <br><br><img src="https://habrastorage.org/webt/zh/v_/sn/zhv_snutr8ma1aeenqr3itjojvc.png"><br><br>  Para mim, isso parece um tanto inesperado - parece que a abordagem baseada no modelo QL + √© matematicamente ideal.  Mas nada disso.  As vantagens da abordagem s√£o aproximadamente as mesmas que as anteriores - mas s√£o menos pronunciadas, j√° que as estrat√©gias n√£o s√£o testadas por muito tempo (bem, n√£o temos mil√™nios na evolu√ß√£o), o que significa que os resultados s√£o inst√°veis.  Al√©m disso, o n√∫mero de testes tamb√©m n√£o pode ser elevado ao infinito - o que significa que a estrat√©gia ter√° que ser buscada em um espa√ßo de op√ß√µes n√£o muito complicado.  Ela n√£o apenas ter√° "canetas" que podem ser "torcidas".  Bem, a intoler√¢ncia √† experi√™ncia n√£o foi cancelada.  E, comparados a QL ou baseados em modelo, esses modelos usam a experi√™ncia de maneira ineficiente.  Eles precisam de muito mais intera√ß√µes com a realidade do que abordagens que usam aprendizado de m√°quina. <br><br>  Como voc√™ pode ver, qualquer tentativa de criar uma AGI em teoria deve incluir aprendizado de m√°quina para pr√™mios de previs√£o ou alguma forma de nota√ß√£o param√©trica de uma estrat√©gia - para que voc√™ possa pegar essa estrat√©gia com algo como evolu√ß√£o. <br><br>  Este √© um forte ataque contra pessoas que se oferecem para criar IA com base em bancos de dados, l√≥gica e gr√°ficos conceituais.  Se voc√™, proponente da abordagem simb√≥lica, leia isso - seja bem-vindo aos coment√°rios, ficarei feliz em saber o que a AGI pode fazer sem a mec√¢nica descrita acima. <br><br><h2>  Modelos de aprendizado de m√°quina para RL </h2><br>  Quase qualquer modelo de ML pode ser usado para aprendizado refor√ßado.  As redes neurais s√£o, obviamente, boas.  Mas h√°, por exemplo, KNN.  Para cada par S e A, procuramos os mais semelhantes, mas no passado.  E estamos procurando o que ser√° R. Est√∫pido depois disso?  Sim, mas funciona.  Existem √°rvores decisivas - aqui √© melhor dar um passeio nas palavras-chave "aumento de gradiente" e "floresta decisiva".  As √°rvores s√£o pobres na captura de depend√™ncias complexas?  Use a engenharia de recursos.  Deseja que sua IA seja mais pr√≥xima do General?  Use FE autom√°tico!  Passe por v√°rias f√≥rmulas diferentes, envie-as como recursos para seu impulso, descarte as f√≥rmulas que aumentam o erro e deixe as f√≥rmulas que melhoram a precis√£o.  Em seguida, envie as melhores f√≥rmulas como argumentos para as novas f√≥rmulas e assim por diante. <br><br>  Voc√™ pode usar regress√µes simb√≥licas para previs√£o - ou seja, apenas classificando f√≥rmulas na tentativa de obter algo que se aproxime de Q ou R. √â poss√≠vel tentar classificar algoritmos - ent√£o voc√™ obt√©m uma coisa chamada indu√ß√£o de Solomonov, que √© teoricamente ideal, mas quase muito dif√≠cil de treinar. aproxima√ß√µes de fun√ß√µes. <br><br>  Mas redes neurais s√£o geralmente um compromisso entre expressividade e complexidade de aprendizado.  A regress√£o algor√≠tmica idealmente capta qualquer depend√™ncia - por centenas de anos.  A √°rvore de decis√£o funcionar√° muito rapidamente - mas n√£o poder√° extrapolar y = a + b.  Uma rede neural √© algo no meio. <br><br><h2>  Perspectivas de desenvolvimento </h2><br>  Quais s√£o as maneiras de fazer exatamente a AGI agora?  Pelo menos teoricamente. <br><br><h3>  Evolu√ß√£o </h3><br>  Podemos criar muitos ambientes de teste diferentes e iniciar a evolu√ß√£o de alguma rede neural.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As configura√ß√µes que obtiverem mais pontos no total em todas as tentativas ser√£o multiplicadas. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A rede neural deve ter mem√≥ria e seria desej√°vel ter pelo menos parte da mem√≥ria na forma de uma fita, como uma m√°quina de Turing ou como um disco r√≠gido. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O problema √© que, com a ajuda da evolu√ß√£o, voc√™ pode cultivar algo como RL, √© claro. </font><font style="vertical-align: inherit;">Mas como deve ser a linguagem na qual a RL parece compacta - para que a evolu√ß√£o a encontre - e ao mesmo tempo que a evolu√ß√£o n√£o encontrar solu√ß√µes como "mas vou criar um neur√¥nio para cento e cinquenta camadas, para que todos fiquem loucos enquanto eu o ensino!" . </font><font style="vertical-align: inherit;">A evolu√ß√£o √© como uma multid√£o de usu√°rios analfabetos - encontrar√° falhas no c√≥digo e abandonar√° todo o sistema.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Aixi </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Voc√™ pode criar um sistema baseado em modelo com base em um pacote de muitas regress√µes algor√≠tmicas. </font><font style="vertical-align: inherit;">O algoritmo √© garantido como Turing completo - o que significa que n√£o haver√° padr√µes que n√£o possam ser captados. </font><font style="vertical-align: inherit;">O algoritmo √© escrito em c√≥digo - o que significa que sua complexidade pode ser facilmente calculada. </font><font style="vertical-align: inherit;">Isso significa que √© poss√≠vel refinar matematicamente suas hip√≥teses sobre o dispositivo de complexidade do mundo. </font><font style="vertical-align: inherit;">Com redes neurais, por exemplo, esse truque n√£o funcionar√° - a√≠ a penalidade pela complexidade √© muito indireta e heur√≠stica. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Resta apenas aprender a treinar rapidamente regress√µes algor√≠tmicas. </font><font style="vertical-align: inherit;">At√© agora, o melhor para isso √© a evolu√ß√£o, e √© imperdo√°velmente longa.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Seed AI </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Seria legal criar uma IA que se aprimore. Melhore sua capacidade de resolver problemas. Isso pode parecer uma id√©ia estranha, mas esse problema </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j√° foi resolvido para sistemas de otimiza√ß√£o est√°tica, como a evolu√ß√£o</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Se voc√™ conseguir perceber isso ... Tudo o que o expositor sabe √©? Obteremos uma IA muito poderosa em muito pouco tempo. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como fazer isso? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Voc√™ pode tentar organizar que, na RL, algumas das a√ß√µes afetem as configura√ß√µes da pr√≥pria RL. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ou d√™ ao sistema RL alguma ferramenta para criar novos processadores de pr√© e p√≥s-dados para voc√™. Deixe a RL ser burra, mas ela poder√° criar calculadoras, notebooks e computadores para si mesma. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Outra op√ß√£o √© criar algum tipo de IA usando a evolu√ß√£o, na qual parte das a√ß√µes afetar√° seu dispositivo no n√≠vel do c√≥digo.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mas, no momento, n√£o vi op√ß√µes vi√°veis ‚Äã‚Äãpara o Seed AI - embora muito limitadas. </font><font style="vertical-align: inherit;">Os desenvolvedores est√£o se escondendo? </font><font style="vertical-align: inherit;">Ou essas op√ß√µes s√£o t√£o fracas que n√£o mereceram aten√ß√£o geral e passaram por mim? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No entanto, agora o Google e o DeepMind trabalham principalmente com arquiteturas de redes neurais. </font><font style="vertical-align: inherit;">Aparentemente, eles n√£o querem se envolver na enumera√ß√£o combinat√≥ria e tentam tornar suas id√©ias adequadas para o m√©todo de propaga√ß√£o reversa do erro. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Espero que este artigo de revis√£o seja √∫til =) Coment√°rios s√£o bem-vindos, especialmente coment√°rios como "Eu sei como melhorar a AGI"!</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt468379/">https://habr.com/ru/post/pt468379/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt468351/index.html">Microestrutura de mercado e sele√ß√£o adversa</a></li>
<li><a href="../pt468363/index.html">Minha magnum opus do mundo dos jogos para celular</a></li>
<li><a href="../pt468367/index.html">Amazon anuncia plano de aquecimento global</a></li>
<li><a href="../pt468369/index.html">Como eu criei o "WildMAN" - uma par√≥dia de muitos jogos de 8 bits e o portei recentemente para o Android</a></li>
<li><a href="../pt468377/index.html">8 hist√≥rias sobre o interior da China. O que n√£o √© mostrado aos estrangeiros</a></li>
<li><a href="../pt468381/index.html">De volta ao futuro? Borracha pendente Quantum</a></li>
<li><a href="../pt468383/index.html">Ruby meme generator para atrair interesse no idioma</a></li>
<li><a href="../pt468385/index.html">A √°rea de trabalho est√° morta, viva a √°rea de trabalho! Eu coleciono habrastatistiki</a></li>
<li><a href="../pt468387/index.html">O resumo de materiais interessantes para o desenvolvedor m√≥vel n¬∫ 316 (de 16 a 22 de setembro)</a></li>
<li><a href="../pt468389/index.html">Artyom Galonsky, STO Bureau do Bureau: ‚ÄúSou contra um engenheiro de DevOps‚Äù</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>