<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèº‚Äçüè´ üë®üèæ‚Äçü§ù‚Äçüë®üèº üïñ Starten Sie LDA in der realen Welt. Detaillierte Anleitung üîΩ üßëüèº üèôÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Vorwort 


 Im Internet gibt es viele Tutorials, in denen erkl√§rt wird, wie die LDA funktioniert (Latent Dirichlet Allocation) und wie sie in die Prax...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Starten Sie LDA in der realen Welt. Detaillierte Anleitung</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/417167/"><h2 id="predislovie">  Vorwort </h2><br><p>  Im Internet gibt es viele Tutorials, in denen erkl√§rt wird, wie die LDA funktioniert (Latent Dirichlet Allocation) und wie sie in die Praxis umgesetzt wird.  Beispiele f√ºr LDA-Schulungen werden h√§ufig anhand von ‚Äûbeispielhaften‚Äú Datens√§tzen demonstriert, z. B. dem ‚Äû20 Newsgroups-Datensatz‚Äú, der bei sklearn erh√§ltlich ist. </p><br><p>  Ein Merkmal des Trainings am Beispiel "beispielhafter" Datens√§tze ist, dass die Daten dort immer in Ordnung sind und bequem an einem Ort gestapelt werden.  Beim Training von Produktionsmodellen sind die Daten, die direkt aus realen Quellen stammen, normalerweise umgekehrt: </p><br><ul><li>  Viele Emissionen. </li><li>  Falsches Markup (falls vorhanden). </li><li>  Sehr starke Klassenungleichgewichte und h√§ssliche Verteilungen von Datensatzparametern. </li><li>  F√ºr Texte sind dies: Grammatikfehler, eine gro√üe Anzahl seltener und einzigartiger W√∂rter, Mehrsprachigkeit. </li><li>  Eine unbequeme Art, Daten zu speichern (verschiedene oder seltene Formate, die Notwendigkeit des Parsens) </li></ul><br><p>  Historisch gesehen versuche ich, aus Beispielen zu lernen, die den Realit√§ten der Produktionsrealit√§t so nahe wie m√∂glich kommen, weil man auf diese Weise die Problembereiche einer bestimmten Art von Aufgabe am besten erfassen kann.  So war es auch mit der LDA, und in diesem Artikel m√∂chte ich meine Erfahrungen teilen - wie man LDA von Grund auf mit vollst√§ndig rohen Daten ausf√ºhrt.  Ein Teil des Artikels befasst sich mit dem Abrufen dieser Daten, damit das Beispiel zu einem vollwertigen ‚Äûtechnischen Fall‚Äú wird. </p><a name="habracut"></a><br><h2 id="topic-modeling-i-lda">  Themenmodellierung und LDA. </h2><br><p>  √úberlegen Sie sich zun√§chst, was der LDA im Allgemeinen tut und welche Aufgaben er verwendet. <br>  Am h√§ufigsten wird LDA f√ºr Themenmodellierungsaufgaben verwendet.  Solche Aufgaben sind die Aufgaben des Gruppierens oder Klassifizierens von Texten - so, dass jede Klasse oder jeder Cluster Texte mit √§hnlichen Themen enth√§lt. </p><br><p>  Um LDA auf den Textdatensatz (im Folgenden als Textk√∂rper bezeichnet) anzuwenden, muss der K√∂rper in eine Termdokumentmatrix umgewandelt werden. </p><br><p>  Eine Termdokumentmatrix ist eine Matrix mit einer Gr√∂√üe <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mtext>&amp;#xA0;</mtext><mi>m</mi><mi>a</mi><mi>l</mi><mi>W</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="9.043ex" height="2.057ex" viewBox="0 -780.1 3893.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-4E" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-6D" x="1138" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-61" x="2017" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-6C" x="2546" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-57" x="2845" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi><mtext>&nbsp;</mtext><mi>m</mi><mi>a</mi><mi>l</mi><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-1"> N \ mal W </script>  wo <br>  N ist die Anzahl der Dokumente in dem Fall und W ist die Gr√∂√üe des W√∂rterbuchs des Falls, d.h.  die Anzahl der W√∂rter (eindeutig), die in unserem Korpus gefunden werden.  In der i-ten Zeile ist die j-te Spalte der Matrix eine Zahl - wie oft im i-ten Text das j-te Wort gefunden wurde. </p><br><p>  Die LDA erstellt f√ºr eine gegebene Term-Dokumentmatrix und T einer vorbestimmten Anzahl von Themen zwei Verteilungen: </p><br><ol><li>  Die Verteilung der Themen in den Texten. (In der Praxis durch die Gr√∂√üenmatrix gegeben <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mtext>&amp;#xA0;</mtext><mi>m</mi><mi>a</mi><mi>l</mi><mi>T</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="8.244ex" height="2.057ex" viewBox="0 -780.1 3549.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-4E" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-6D" x="1138" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-61" x="2017" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-6C" x="2546" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-54" x="2845" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi><mtext>&nbsp;</mtext><mi>m</mi><mi>a</mi><mi>l</mi><mi>T</mi></math></span></span><script type="math/tex" id="MathJax-Element-2"> N \ mal T </script>  ) </li><li>  Die Verteilung der W√∂rter nach Themen (Gr√∂√üenmatrix <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>T</mi><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>W</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.509ex" height="2.057ex" viewBox="0 -780.1 4524.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-54" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-74" x="954" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-69" x="1316" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-6D" x="1661" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-65" x="2540" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-73" x="3006" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-57" x="3476" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi><mtext>&nbsp;</mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-3"> T \ times W </script>  ) </li></ol><br><p>  Die Werte der Zellen dieser Matrizen sind jeweils die Wahrscheinlichkeiten, mit denen dieses Thema in diesem Dokument enthalten ist (oder der Anteil des Themas im Dokument, wenn wir das Dokument als eine Mischung verschiedener Themen betrachten) f√ºr die Matrix 'Verteilung von Themen in Texten'. </p><br><p>  F√ºr die Matrix 'Verteilung von W√∂rtern nach Themen' sind die Werte die Wahrscheinlichkeit, das Wort j im Text mit Thema i zu treffen. Qualitativ k√∂nnen wir diese Zahlen als Koeffizienten betrachten, die charakterisieren, wie dieses Wort f√ºr dieses Thema typisch ist. </p><br><p>  Es sollte gesagt werden, dass das Wort Thema keine ‚Äûallt√§gliche‚Äú Definition dieses Wortes ist.  Die LDA weist diesen T zu, aber welche Art von Themen dies sind und ob sie bekannten Themen von Texten entsprechen, wie z. B. "Sport", "Wissenschaft", "Politik", ist unbekannt.  In diesem Fall ist es besser, √ºber das Thema als eine Art abstrakte Einheit zu sprechen, die durch eine Linie in der Matrix der Wortverteilung nach Themen definiert ist und mit einiger Wahrscheinlichkeit diesem Text entspricht, wenn Sie es sich als eine Familie charakteristischer Wortgruppen vorstellen k√∂nnen, die sich mit entsprechenden Wahrscheinlichkeiten treffen (aus der Tabelle) in einem bestimmten Satz von Texten. </p><br><p>  Wenn Sie detaillierter und in Formeln studieren m√∂chten, wie die LDA geschult ist und funktioniert, finden Sie hier einige Materialien (die vom Autor verwendet wurden): </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Originalartikel</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Auf Englisch mit anschaulichen Beispielen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Details in russischer Sprache</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Informationen zur Python-Implementierung</a> </li></ul><br><h2 id="dobyvaem-dikie-dannye">  Wir bekommen wilde Daten </h2><br><p>  F√ºr unsere 'Laborarbeit' ben√∂tigen wir einen benutzerdefinierten Datensatz mit eigenen Fehlern und Merkmalen.  Sie k√∂nnen es an verschiedenen Orten erhalten: Laden Sie Rezensionen von Kinopoisk, Wikipedia-Artikel, Nachrichten von einem Nachrichtenportal herunter, wir werden eine etwas extremere Option w√§hlen - Beitr√§ge von VKontakte-Communities. </p><br><p>  Wir werden das so machen: </p><br><ol><li>  Wir w√§hlen einen VK-Benutzer aus. </li><li>  Wir bekommen eine Liste aller seiner Freunde. </li><li>  F√ºr jeden Freund nehmen wir seine gesamte Gemeinschaft. </li><li>  F√ºr jede Community jedes Freundes pumpen wir die ersten n (n = 100) Community-Beitr√§ge aus und kombinieren sie zu einem Community-Textinhalt. </li></ol><br><h4 id="instrumenty-i-stati">  Werkzeuge und Artikel </h4><br><p>  Um Beitr√§ge herunterzuladen, verwenden wir das vk-Modul, um mit der VKontakte-API f√ºr Python zu arbeiten.  Einer der kompliziertesten Momente beim Schreiben einer Anwendung mit der VKontakte-API ist die Autorisierung. Gl√ºcklicherweise ist der Code, der diese Arbeit ausf√ºhrt, bereits geschrieben und gemeinfrei. Mit Ausnahme von vk habe ich ein kleines Autorisierungsmodul verwendet - vkauth. </p><br><p>  Links zu den Modulen und Artikeln, die zum Studium der VKontakte-API verwendet wurden: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vkauth</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vkauth Tutorial</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vk Tutorial</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vk Tutorial Nummer 2</a> </li><li>  Offizielle Dokumentation der Vkontakte API </li></ul><br><h4 id="pishem-kod">  Einen Code schreiben </h4><br><p>  Melden Sie sich mit vkauth an: </p><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#authorization of app using modules imported. app_id = '6203169' perms = ['photos','friends','groups'] API_ver = '5.68' Auth = VKAuth(perms, app_id, API_ver) Auth.auth() token = Auth.get_token() user_id = Auth.get_user_id() #starting session session = vk.Session(access_token=token) api = vk.API(session)</span></span></code> </pre> <br><p>  Dabei wurde ein kleines Modul geschrieben, das alle Funktionen enth√§lt, die zum Herunterladen von Inhalten im entsprechenden Format erforderlich sind. Sie sind unten aufgef√ºhrt. Lassen Sie uns diese durchgehen: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_friends_ids</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(api, user_id)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' For a given API object and user_id returns a list of all his friends ids. '''</span></span> friends = api.friends.get(user_id=user_id, v = <span class="hljs-string"><span class="hljs-string">'5.68'</span></span>) friends_ids = friends[<span class="hljs-string"><span class="hljs-string">'items'</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> friends_ids <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_user_groups</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(api, user_id, moder=True, only_open=True)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' For a given API user_id returns list of all groups he subscribed to. Flag model to get only those groups where user is a moderator or an admin) Flag only_open to get only public(open) groups. '''</span></span> kwargs = {<span class="hljs-string"><span class="hljs-string">'user_id'</span></span> : user_id, <span class="hljs-string"><span class="hljs-string">'v'</span></span> : <span class="hljs-string"><span class="hljs-string">'5.68'</span></span> } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> moder == <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: kwargs[<span class="hljs-string"><span class="hljs-string">'filter'</span></span>] = <span class="hljs-string"><span class="hljs-string">'moder'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> only_open == <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: kwargs[<span class="hljs-string"><span class="hljs-string">'extended'</span></span>] = <span class="hljs-number"><span class="hljs-number">1</span></span> kwargs[<span class="hljs-string"><span class="hljs-string">'fields'</span></span>] = [<span class="hljs-string"><span class="hljs-string">'is_closed'</span></span>] groups = api.groups.get(**kwargs) groups_refined = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> group <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> groups[<span class="hljs-string"><span class="hljs-string">'items'</span></span>]: cond_check = (only_open <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> group[<span class="hljs-string"><span class="hljs-string">'is_closed'</span></span>] == <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> only_open <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> cond_check: refined = {} refined[<span class="hljs-string"><span class="hljs-string">'id'</span></span>] = group[<span class="hljs-string"><span class="hljs-string">'id'</span></span>] * (<span class="hljs-number"><span class="hljs-number">-1</span></span>) refined[<span class="hljs-string"><span class="hljs-string">'name'</span></span>] = group[<span class="hljs-string"><span class="hljs-string">'name'</span></span>] groups_refined.append(refined) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> groups_refined <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_n_posts_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(api, group_id, n_posts=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">50</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' For a given api and group_id returns first n_posts concatenated as one text. '''</span></span> wall_contents = api.wall.get(owner_id = group_id, count=n_posts, v = <span class="hljs-string"><span class="hljs-string">'5.68'</span></span>) wall_contents = wall_contents[<span class="hljs-string"><span class="hljs-string">'items'</span></span>] text = <span class="hljs-string"><span class="hljs-string">''</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> post <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> wall_contents: text += post[<span class="hljs-string"><span class="hljs-string">'text'</span></span>] + <span class="hljs-string"><span class="hljs-string">' '</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text</code> </pre> <br><p>  Die endg√ºltige Pipeline lautet wie folgt: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#id of user whose friends you gonna get, like: https://vk.com/id111111111 user_id = 111111111 friends_ids = vt.get_friends_ids(api, user_id) #collecting all groups groups = [] for i,friend in tqdm(enumerate(friends_ids)): if i % 3 == 0: sleep(1) friend_groups = vt.get_user_groups(api, friend, moder=False) groups += friend_groups #converting groups to dataFrame groups_df = pd.DataFrame(groups) groups_df.drop_duplicates(inplace=True) #reading content(content == first 100 posts) for i,group in tqdm(groups_df.iterrows()): name = group['name'] group_id = group['id'] #Different kinds of fails occures during scrapping #For examples there are names of groups with slashes #Like: 'The Kaaats / Indie-rock' try: content = vt.get_n_posts_text(api, group_id, n_posts=100) dst_path = join(data_path, name + '.txt') with open(dst_path, 'w+t') as f: f.write(content) except Exception as e: print('Error occured on group:', name) print(e) continue #need it because of requests limitaion in VK API. if i % 3 == 0: sleep(1)</span></span></code> </pre> <br><h4 id="fails">  Schl√§gt fehl </h4><br><p>  Im Allgemeinen ist das Herunterladen von Daten an sich nicht schwierig. Sie sollten nur zwei Punkte beachten: </p><br><ol><li>  Aufgrund der Privatsph√§re einiger Communitys erhalten Sie manchmal Zugriffsfehler, manchmal werden andere Fehler durch die Installation von try behoben, au√üer an der richtigen Stelle. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">VK hat eine Begrenzung</a> f√ºr die Anzahl der Anforderungen pro Sekunde. </li></ol><br><p>  Wenn Sie eine gro√üe Anzahl von Anforderungen stellen, beispielsweise in einer Schleife, werden auch Fehler abgefangen.  Dieses Problem kann auf verschiedene Arten gel√∂st werden: </p><br><ol><li>  Dumm und unverbl√ºmt: Bleiben Sie alle 3 Anfragen im Schlaf (einige).  Dies geschieht in einer Zeile und verlangsamt das Entladen erheblich, wenn das Datenvolumen nicht gro√ü ist und keine Zeit f√ºr komplexere Methoden bleibt - dies ist durchaus akzeptabel. (In diesem Artikel implementiert) </li><li>  Verstehen Sie die Arbeit von Long Poll-Anfragen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://vk.com/dev/using_longpoll</a> </li></ol><br><p>  In diesem Artikel wurde eine einfache und langsame Methode gew√§hlt. In Zukunft werde ich wahrscheinlich einen Mikroartikel dar√ºber schreiben, wie die Anzahl der Anfragen pro Sekunde umgangen oder die Einschr√§nkungen aufgehoben werden k√∂nnen. </p><br><h4 id="itog">  Zusammenfassung </h4><br><p>  Mit dem Startwert "einige" Benutzer mit ~ 150 Freunden gelang es ihnen, 4.679 Texte zu erhalten - jeder kennzeichnet eine bestimmte VK-Community.  Die Texte sind sehr unterschiedlich gro√ü und in vielen Sprachen verfasst - einige davon sind f√ºr unsere Zwecke nicht geeignet, aber wir werden etwas weiter darauf eingehen. </p><br><h3 id="osnovnaya-chast">  Hauptteil </h3><br><p><img src="https://habrastorage.org/webt/bj/to/hm/bjtohmsrvsxlcbs78u0thlawxky.png" alt="Bild"></p><br><p>  Lassen Sie uns alle Bl√∂cke unserer Pipeline durchgehen - zuerst das obligatorische (Ideal), dann den Rest - sie sind nur von gr√∂√ütem Interesse. </p><br><h4 id="countvectorizer">  Countvectorizer </h4><br><p>  Bevor wir LDA unterrichten, m√ºssen wir unsere Dokumente in Form einer Term-Dokumentmatrix pr√§sentieren.  Dies umfasst normalerweise Operationen wie: </p><br><ul><li>  Puttuctions / Zahlen / unn√∂tige Token entfernen. </li><li>  Tokenisierung (Pr√§sentation als Wortliste) </li><li>  W√∂rter z√§hlen, eine thermische Dokumentmatrix zusammenstellen. </li></ul><br><p>  Alle diese Aktionen in sklearn werden bequem im Rahmen einer Programmentit√§t implementiert - sklearn.feature_extraction.text.CountVectorizer. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentationslink</a> </p><br><p>  Alles was Sie tun m√ºssen ist: </p><br><pre> <code class="python hljs">count_vect = CountVectorizer(input=<span class="hljs-string"><span class="hljs-string">'filename'</span></span>, stop_words=stopwords, vocabulary=voc) dataset = count_vect.fit_transform(train_names)</code> </pre> <br><h4 id="lda">  Lda </h4><br><p>  √Ñhnlich wie bei CountVectorizer ist LDA in Sklearn und anderen Frameworks perfekt implementiert. Daher macht es in unserem rein praktischen Artikel nicht viel Sinn, viel Platz direkt f√ºr deren Implementierung zu verwenden. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentationslink</a> </p><br><p>  Alles was Sie brauchen, um LDA zu starten ist: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#training LDA lda = LDA(n_components = 60, max_iter=30, n_jobs=6, learning_method='batch', verbose=1) lda.fit(dataset)</span></span></code> </pre> <br><h4 id="preprocessing">  Vorverarbeitung </h4><br><p>  Wenn wir unsere Texte nur unmittelbar nach dem Herunterladen und Konvertieren in eine Term-Document-Matrix mit dem CountVectorizer und dem integrierten Standard-Tokenizer verwenden, erhalten wir eine Matrix mit der Gr√∂√üe 4679 x 769801 (f√ºr die von mir verwendeten Daten). </p><br><p>  Die Gr√∂√üe unseres W√∂rterbuchs wird 769801 betragen. Selbst wenn wir davon ausgehen, dass die meisten W√∂rter informativ sind, ist es unwahrscheinlich, dass wir eine gute LDA erhalten. Etwas wie ‚ÄûFl√ºche der Dimensionen‚Äú erwartet uns, ganz zu schweigen von fast jedem Computer. Wir werden nur den gesamten RAM verstopfen.  Tats√§chlich sind die meisten dieser W√∂rter v√∂llig uninformativ.  Die √ºberwiegende Mehrheit von ihnen sind: </p><br><ul><li>  Emoticons, Zeichen, Zahlen. </li><li>  Einzigartige oder sehr seltene W√∂rter (z. B. polnische W√∂rter aus einer Gruppe mit polnischen Memen, falsch geschriebene W√∂rter oder ‚Äûalbanisch‚Äú). </li><li>  Sehr h√§ufige Wortarten (z. B. Pr√§positionen und Pronomen). </li></ul><br><p>  Dar√ºber hinaus sind viele Gruppen in VK ausschlie√ülich auf Bilder spezialisiert - es gibt dort fast keine Textbeitr√§ge - die ihnen entsprechenden Texte sind entartet, in der thermischen Dokumentmatrix geben sie uns fast vollst√§ndig Nullzeilen. </p><br><p>  Und so, lasst uns alles kl√§ren! <br>  Wir tokenisieren alle Texte, entfernen Satzzeichen und Zahlen aus ihnen und betrachten das Histogramm der Verteilung von Texten nach der Anzahl der W√∂rter: <br><img src="https://habrastorage.org/webt/v4/qh/w0/v4qhw0mrgpizranmnptbz5lnivk.png" alt="Bild"></p><br><p>  Wir entfernen alle Texte, die kleiner als 100 W√∂rter sind (es gibt 525 davon). </p><br><p>  Nun das W√∂rterbuch: <br>  Das Entfernen aller Token (W√∂rter), die keine Buchstaben sind, im Rahmen unserer Aufgabe - dies ist durchaus akzeptabel.  Der CountVectorizer macht dies selbstst√§ndig, auch wenn dies nicht der Fall ist. Ich denke, hier m√ºssen keine Beispiele angegeben werden (sie sind in der Vollversion des Codes f√ºr den Artikel enthalten). </p><br><p>  Eines der h√§ufigsten Verfahren zum Reduzieren der Gr√∂√üe eines W√∂rterbuchs ist das Entfernen der sogenannten Stoppw√∂rter (Stoppw√∂rter) - W√∂rter, die keine semantische Last tragen und / oder keine thematische F√§rbung aufweisen (in unserem Fall Themenmodellierung).  Solche W√∂rter sind in unserem Fall zum Beispiel: </p><br><ul><li>  Pronomen und Pr√§positionen. </li><li>  Artikel - die, a. </li><li>  Allgemeine W√∂rter: "sein", "gut", "wahrscheinlich" usw. </li></ul><br><p>  Das nltk-Modul hat Listen von Stoppw√∂rtern in Russisch und Englisch erstellt, die jedoch eher schwach sind.  Im Internet k√∂nnen Sie auch Listen mit Stoppw√∂rtern f√ºr jede Sprache finden und zu denen in nltk hinzuf√ºgen.  Also werden wir es tun.  Nehmen Sie zus√§tzliche Stoppw√∂rter von hier: </p><br><ul><li>  <a href="">https://github.com/stopwords-iso/stopwords-ru/blob/master/stopwords-ru.json</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://gist.github.com/menzenski/7047705</a> </li></ul><br><p>  In der Praxis werden bei der L√∂sung spezifischer Probleme die Listen der Stoppw√∂rter schrittweise angepasst und erg√§nzt, wenn die Modelle trainiert werden, da f√ºr jeden spezifischen Datensatz und jedes Problem bestimmte "inkonsistente" W√∂rter vorhanden sind.  Nach dem Training unserer LDA der ersten Generation werden wir auch benutzerdefinierte Stoppw√∂rter verwenden. </p><br><p>  Das Verfahren zum Entfernen von Stoppw√∂rtern ist in CountVectorizer integriert - wir ben√∂tigen lediglich eine Liste davon. </p><br><p>  Ist das, was wir getan haben, genug? </p><br><p><img src="https://habrastorage.org/webt/ja/xd/6l/jaxd6lnbbnd_dmmmk6nog9a2ntm.png" alt="Bild"></p><br><p>  Die meisten W√∂rter in unserem W√∂rterbuch sind immer noch nicht zu informativ, um LDA zu lernen, und sie sind nicht in der Liste der Stoppw√∂rter enthalten.  Daher wenden wir eine andere Filtermethode auf unsere Daten an. </p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>i</mi><mi>d</mi><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>,</mo><mi>D</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>o</mi><mi>g</mi><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>D</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mspace linebreak=&quot;newline&quot; /><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mtext>&amp;#xA0;</mtext><mi>i</mi><mi>n</mi><mi>D</mi><mo>:</mo><mi>t</mi><mtext>&amp;#xA0;</mtext><mi>i</mi><mi>n</mi><mi>d</mi><mspace linebreak=&quot;newline&quot; /></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></mrow></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="37.218ex" height="8.202ex" viewBox="0 -832 16024.3 3531.4" role="img" focusable="false" style="vertical-align: -6.27ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-69" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-64" x="345" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-66" x="869" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMAIN-28" x="1419" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-74" x="1809" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMAIN-2C" x="2170" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-44" x="2615" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMAIN-29" x="3444" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMAIN-3D" x="4111" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-6C" x="5417" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-6F" x="5716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-67" x="6201" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-66" x="6932" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-72" x="7482" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-61" x="7934" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-63" x="8463" y="0"></use><g transform="translate(8897,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMAIN-7C" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-44" x="278" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMAIN-7C" x="1107" y="0"></use></g><g transform="translate(10282,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMAIN-7C" x="0" y="0"></use><g transform="translate(0,-1432)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-64" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-69" x="773" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-6E" x="1119" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-44" x="1719" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMAIN-3A" x="2825" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-74" x="3382" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-69" x="3993" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-6E" x="4339" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-64" x="4939" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMAIN-7C" x="5463" y="0"></use></g></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>i</mi><mi>d</mi><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>&nbsp;</mtext><mi>l</mi><mi>o</mi><mi>g</mi><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>D</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mrow><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mspace linebreak="newline"></mspace><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mtext>&nbsp;</mtext><mi>i</mi><mi>n</mi><mi>D</mi><mo>:</mo><mi>t</mi><mtext>&nbsp;</mtext><mi>i</mi><mi>n</mi><mi>d</mi><mspace linebreak="newline"></mspace></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-4"> idf (t, D) = \ log \ frac {| D |} {| \\ {d \ in D: t \ in d \\} |} </script></p><br><p>  wo <br>  t ist ein Wort aus dem W√∂rterbuch. <br>  D - Fall (viele Texte) <br>  d ist einer der K√∂rpertexte. <br>  Wir berechnen die IDF aller unserer W√∂rter und schneiden die W√∂rter mit der gr√∂√üten IDF (sehr selten) und mit der kleinsten (weit verbreiteten W√∂rter) ab. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#'training' (tf-)idf vectorizer. tf_idf = TfidfVectorizer(input='filename', stop_words=stopwords, smooth_idf=False ) tf_idf.fit(train_names) #getting idfs idfs = tf_idf.idf_ #sorting out too rare and too common words lower_thresh = 3. upper_thresh = 6. not_often = idfs &gt; lower_thresh not_rare = idfs &lt; upper_thresh mask = not_often * not_rare good_words = np.array(tf_idf.get_feature_names())[mask] #deleting punctuation as well. cleaned = [] for word in good_words: word = re.sub("^(\d+\w*$|_+)", "", word) if len(word) == 0: continue cleaned.append(word)</span></span></code> </pre> <br><p>  Das Erhalten nach den oben genannten Verfahren ist bereits f√ºr das LDA-Training gut geeignet, aber wir werden mehr Stemming betreiben - die gleichen W√∂rter werden h√§ufig in unserem Datensatz gefunden, jedoch in verschiedenen F√§llen.  Zum <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stemming wurde Pymystem3 verwendet</a> . </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Stemming m = Mystem() stemmed = set() voc_len = len(cleaned) for i in tqdm(range(voc_len)): word = cleaned.pop() stemmed_word = m.lemmatize(word)[0] stemmed.add(stemmed_word) stemmed = list(stemmed) print('After stemming: %d'%(len(stemmed)))</span></span></code> </pre> <br><p>  Nach Anwendung der obigen Filterung verringerte sich die W√∂rterbuchgr√∂√üe von 769801 auf <br>  13611 und bereits mit solchen Daten k√∂nnen Sie ein LDA-Modell von akzeptabler Qualit√§t erhalten. </p><br><h3 id="testirovanie-primenenie-i-tyuning-lda">  Testen, Anwenden und Einstellen von LDA </h3><br><p>  Nachdem wir nun den Datensatz, die Vorverarbeitung und die Modelle haben, die wir f√ºr den verarbeiteten Datensatz trainiert haben, w√§re es sch√∂n, die Angemessenheit unserer Modelle zu √ºberpr√ºfen und einige Anwendungen f√ºr sie zu erstellen. </p><br><p>  Betrachten Sie als Anwendung zun√§chst die Aufgabe, Schl√ºsselw√∂rter f√ºr einen bestimmten Text zu generieren.  Sie k√∂nnen dies auf ziemlich einfache Weise wie folgt tun: </p><br><ol><li>  Wir erhalten von der LDA die Verteilung der Themen f√ºr diesen Text. </li><li>  W√§hlen Sie n (zum Beispiel n = 2) der am st√§rksten ausgepr√§gten Themen. </li><li>  W√§hlen Sie f√ºr jedes Thema m (zum Beispiel m = 3) die charakteristischsten W√∂rter. </li><li>  Wir haben eine Reihe von n * m W√∂rtern, die einen bestimmten Text charakterisieren. </li></ol><br><p>  Wir werden eine einfache Schnittstellenklasse schreiben, die diese Methode zum Generieren von Schl√ºsselw√∂rtern implementiert: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Let\`s do simple interface class class TopicModeler(object): ''' Inteface object for CountVectorizer + LDA simple usage. ''' def __init__(self, count_vect, lda): ''' Args: count_vect - CountVectorizer object from sklearn. lda - LDA object from sklearn. ''' self.lda = lda self.count_vect = count_vect self.count_vect.input = 'content' def __call__(self, text): ''' Gives topics distribution for a given text Args: text - raw text via python string. returns: numpy array - topics distribution for a given text. ''' vectorized = self.count_vect.transform([text]) lda_topics = self.lda.transform(vectorized) return lda_topics def get_keywords(self, text, n_topics=3, n_keywords=5): ''' For a given text gives n top keywords for each of m top texts topics. Args: text - raw text via python string. n_topics - int how many top topics to use. n_keywords - how many top words of each topic to return. returns: list - of m*n keywords for a given text. ''' lda_topics = self(text) lda_topics = np.squeeze(lda_topics, axis=0) n_topics_indices = lda_topics.argsort()[-n_topics:][::-1] top_topics_words_dists = [] for i in n_topics_indices: top_topics_words_dists.append(self.lda.components_[i]) shape=(n_keywords*n_topics, self.lda.components_.shape[1]) keywords = np.zeros(shape=shape) for i,topic in enumerate(top_topics_words_dists): n_keywords_indices = topic.argsort()[-n_keywords:][::-1] for k,j in enumerate(n_keywords_indices): keywords[i * n_keywords + k, j] = 1 keywords = self.count_vect.inverse_transform(keywords) keywords = [keyword[0] for keyword in keywords] return keywords</span></span></code> </pre> <br><p>  Wir wenden unsere Methode auf mehrere Texte an und sehen, was passiert: <br>  Community <strong>:</strong> Reiseb√ºro "Farben der Welt" <br>  <strong>Schl√ºsselw√∂rter:</strong> ['Foto', 'sozial', 'Reisen', 'Gemeinschaft', 'Reisen', 'Euro', 'Unterkunft', 'Preis', 'Polen', 'Abreise'] <br>  <strong>Gemeinschaft:</strong> Food Gifs <br>  <strong>Schl√ºsselw√∂rter:</strong> ['Butter', 'St', 'Salz', 'PC', 'Teig', 'Kochen', 'Zwiebel', 'Pfeffer', 'Zucker', 'Gr'] </p><br><p>  Die obigen Ergebnisse sind nicht "Kirschpickel" und sehen v√∂llig angemessen aus.  Tats√§chlich sind dies die Ergebnisse eines bereits konfigurierten Modells.  Die ersten LDAs, die im Rahmen dieses Artikels geschult wurden, f√ºhrten zu deutlich schlechteren Ergebnissen, unter anderem bei h√§ufig verwendeten Keywords: </p><br><ol><li>  Zusammengesetzte Komponenten von Webadressen: www, http, ru, com ... </li><li>  Gemeinsame W√∂rter. </li><li>  Einheiten: cm, Meter, km ... </li></ol><br><p>  Das Tuning (Tuning) des Modells wurde wie folgt durchgef√ºhrt: </p><br><ol><li>  W√§hlen Sie f√ºr jedes Thema n (n = 5) charakteristischste W√∂rter aus. </li><li>  Wir betrachten sie je nach Schulungsfall als idf. </li><li>  Wir bringen 5-10% der am weitesten verbreiteten Keywords ein. </li></ol><br><p>  Eine solche ‚ÄûReinigung‚Äú sollte sorgf√§ltig durchgef√ºhrt werden, wobei genau diese 10% der W√∂rter vorab betrachtet werden.  Stattdessen sollten Kandidaten zum L√∂schen auf diese Weise ausgew√§hlt werden, und dann sollten W√∂rter, die gel√∂scht werden sollen, manuell aus ihnen ausgew√§hlt werden. </p><br><p>  Irgendwo in der 2-3-Generation von Modellen mit einer √§hnlichen Art der Auswahl von Stoppw√∂rtern f√ºr die Top 5% der weit verbreiteten Top-Wort-Verteilungen erhalten wir: <br>  ['beliebig', 'vollst√§ndig', 'richtig', 'einfach', 'weiter', 'Internet', 'klein', 'Weg', 'schwierig', 'Stimmung', 'so viel', 'eingestellt', ' Option ',' Name ',' Rede ',' Programm ',' Wettbewerb ',' Musik ',' Ziel ',' Film ',' Preis ',' Spiel ',' System ',' Spiel ',' Firma ' , 'nett'] </p><br><h3 id="esche-prilozheniya">  Weitere Anwendungen </h3><br><p>  Das erste, was mir speziell in den Sinn kommt, ist, die Verteilung von Themen im Text als "Einbettung" von Texten zu verwenden. In dieser Interpretation k√∂nnen Sie Visualisierungs- oder Clustering-Algorithmen auf sie anwenden und auf diese Weise nach den endg√ºltigen "effektiven" thematischen Clustern suchen. </p><br><p>  Machen wir das: </p><br><pre> <code class="python hljs">term_doc_matrix = count_vect.transform(names) embeddings = lda.transform(term_doc_matrix) kmeans = KMeans(n_clusters=<span class="hljs-number"><span class="hljs-number">30</span></span>) clust_labels = kmeans.fit_predict(embeddings) clust_centers = kmeans.cluster_centers_ embeddings_to_tsne = np.concatenate((embeddings,clust_centers), axis=<span class="hljs-number"><span class="hljs-number">0</span></span>) tSNE = TSNE(n_components=<span class="hljs-number"><span class="hljs-number">2</span></span>, perplexity=<span class="hljs-number"><span class="hljs-number">15</span></span>) tsne_embeddings = tSNE.fit_transform(embeddings_to_tsne) tsne_embeddings, centroids_embeddings = np.split(tsne_embeddings, [len(clust_labels)], axis=<span class="hljs-number"><span class="hljs-number">0</span></span>)</code> </pre> <br><p>  Am Ausgang erhalten wir folgendes Bild: <br><img src="https://habrastorage.org/webt/7j/2r/qv/7j2rqvpp2-blr9vfq44qjs75upc.png" alt="Bild"></p><br><p>  Kreuze sind die Schwerpunkte (Cenroide) von Clustern. </p><br><p>  Im tSNE-Bild von Einbettungen ist zu sehen, dass die mit KMeans ausgew√§hlten Cluster ziemlich zusammenh√§ngende und meist r√§umlich trennbare Mengen bilden. </p><br><p>  Alles andere bis zu dir. </p><br><p>  Link zu allen Codes: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://gitlab.com/Mozes/VK_LDA</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de417167/">https://habr.com/ru/post/de417167/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de417155/index.html">Linux-Kernel 4.18: Was bereitet sich auf die bevorstehende Ver√∂ffentlichung vor?</a></li>
<li><a href="../de417157/index.html">Die Singularit√§t r√ºckt n√§her: Die KI beginnt, Roboter zu steuern</a></li>
<li><a href="../de417161/index.html">Burger King: geheime √úberwachung, L√ºgen, Diebstahl von Bankkarten. Fortsetzung</a></li>
<li><a href="../de417163/index.html">Feine Commits</a></li>
<li><a href="../de417165/index.html">Was Burger King bedroht</a></li>
<li><a href="../de417171/index.html">Studie: Von Frauen betriebene Hedgefonds zeigen bessere Ergebnisse</a></li>
<li><a href="../de417173/index.html">‚ÄûOld New Vinyl‚Äú: 20 Materialien zur Geschichte und Produktion von Plattenspielern und Schallplatten</a></li>
<li><a href="../de417175/index.html">Acme Road Semaphor Restaurierung der ersten H√§lfte des 20. Jahrhunderts</a></li>
<li><a href="../de417177/index.html">Lokaler Webserver unter Linux mit automatischem Host-Raising und PHP-Versionswechsel</a></li>
<li><a href="../de417179/index.html">Einrichten einer Heimentwicklungsumgebung (Docker + Gitlab + DNS)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>