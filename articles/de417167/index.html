<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏼‍🏫 👨🏾‍🤝‍👨🏼 🕖 Starten Sie LDA in der realen Welt. Detaillierte Anleitung 🔽 🧑🏼 🏙️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Vorwort 


 Im Internet gibt es viele Tutorials, in denen erklärt wird, wie die LDA funktioniert (Latent Dirichlet Allocation) und wie sie in die Prax...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Starten Sie LDA in der realen Welt. Detaillierte Anleitung</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/417167/"><h2 id="predislovie">  Vorwort </h2><br><p>  Im Internet gibt es viele Tutorials, in denen erklärt wird, wie die LDA funktioniert (Latent Dirichlet Allocation) und wie sie in die Praxis umgesetzt wird.  Beispiele für LDA-Schulungen werden häufig anhand von „beispielhaften“ Datensätzen demonstriert, z. B. dem „20 Newsgroups-Datensatz“, der bei sklearn erhältlich ist. </p><br><p>  Ein Merkmal des Trainings am Beispiel "beispielhafter" Datensätze ist, dass die Daten dort immer in Ordnung sind und bequem an einem Ort gestapelt werden.  Beim Training von Produktionsmodellen sind die Daten, die direkt aus realen Quellen stammen, normalerweise umgekehrt: </p><br><ul><li>  Viele Emissionen. </li><li>  Falsches Markup (falls vorhanden). </li><li>  Sehr starke Klassenungleichgewichte und hässliche Verteilungen von Datensatzparametern. </li><li>  Für Texte sind dies: Grammatikfehler, eine große Anzahl seltener und einzigartiger Wörter, Mehrsprachigkeit. </li><li>  Eine unbequeme Art, Daten zu speichern (verschiedene oder seltene Formate, die Notwendigkeit des Parsens) </li></ul><br><p>  Historisch gesehen versuche ich, aus Beispielen zu lernen, die den Realitäten der Produktionsrealität so nahe wie möglich kommen, weil man auf diese Weise die Problembereiche einer bestimmten Art von Aufgabe am besten erfassen kann.  So war es auch mit der LDA, und in diesem Artikel möchte ich meine Erfahrungen teilen - wie man LDA von Grund auf mit vollständig rohen Daten ausführt.  Ein Teil des Artikels befasst sich mit dem Abrufen dieser Daten, damit das Beispiel zu einem vollwertigen „technischen Fall“ wird. </p><a name="habracut"></a><br><h2 id="topic-modeling-i-lda">  Themenmodellierung und LDA. </h2><br><p>  Überlegen Sie sich zunächst, was der LDA im Allgemeinen tut und welche Aufgaben er verwendet. <br>  Am häufigsten wird LDA für Themenmodellierungsaufgaben verwendet.  Solche Aufgaben sind die Aufgaben des Gruppierens oder Klassifizierens von Texten - so, dass jede Klasse oder jeder Cluster Texte mit ähnlichen Themen enthält. </p><br><p>  Um LDA auf den Textdatensatz (im Folgenden als Textkörper bezeichnet) anzuwenden, muss der Körper in eine Termdokumentmatrix umgewandelt werden. </p><br><p>  Eine Termdokumentmatrix ist eine Matrix mit einer Größe <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mtext>&amp;#xA0;</mtext><mi>m</mi><mi>a</mi><mi>l</mi><mi>W</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="9.043ex" height="2.057ex" viewBox="0 -780.1 3893.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-4E" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-6D" x="1138" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-61" x="2017" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-6C" x="2546" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-57" x="2845" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi><mtext>&nbsp;</mtext><mi>m</mi><mi>a</mi><mi>l</mi><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-1"> N \ mal W </script>  wo <br>  N ist die Anzahl der Dokumente in dem Fall und W ist die Größe des Wörterbuchs des Falls, d.h.  die Anzahl der Wörter (eindeutig), die in unserem Korpus gefunden werden.  In der i-ten Zeile ist die j-te Spalte der Matrix eine Zahl - wie oft im i-ten Text das j-te Wort gefunden wurde. </p><br><p>  Die LDA erstellt für eine gegebene Term-Dokumentmatrix und T einer vorbestimmten Anzahl von Themen zwei Verteilungen: </p><br><ol><li>  Die Verteilung der Themen in den Texten. (In der Praxis durch die Größenmatrix gegeben <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mtext>&amp;#xA0;</mtext><mi>m</mi><mi>a</mi><mi>l</mi><mi>T</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="8.244ex" height="2.057ex" viewBox="0 -780.1 3549.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-4E" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-6D" x="1138" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-61" x="2017" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-6C" x="2546" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-54" x="2845" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi><mtext>&nbsp;</mtext><mi>m</mi><mi>a</mi><mi>l</mi><mi>T</mi></math></span></span><script type="math/tex" id="MathJax-Element-2"> N \ mal T </script>  ) </li><li>  Die Verteilung der Wörter nach Themen (Größenmatrix <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>T</mi><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>W</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.509ex" height="2.057ex" viewBox="0 -780.1 4524.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-54" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-74" x="954" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-69" x="1316" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-6D" x="1661" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-65" x="2540" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-73" x="3006" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-57" x="3476" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi><mtext>&nbsp;</mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-3"> T \ times W </script>  ) </li></ol><br><p>  Die Werte der Zellen dieser Matrizen sind jeweils die Wahrscheinlichkeiten, mit denen dieses Thema in diesem Dokument enthalten ist (oder der Anteil des Themas im Dokument, wenn wir das Dokument als eine Mischung verschiedener Themen betrachten) für die Matrix 'Verteilung von Themen in Texten'. </p><br><p>  Für die Matrix 'Verteilung von Wörtern nach Themen' sind die Werte die Wahrscheinlichkeit, das Wort j im Text mit Thema i zu treffen. Qualitativ können wir diese Zahlen als Koeffizienten betrachten, die charakterisieren, wie dieses Wort für dieses Thema typisch ist. </p><br><p>  Es sollte gesagt werden, dass das Wort Thema keine „alltägliche“ Definition dieses Wortes ist.  Die LDA weist diesen T zu, aber welche Art von Themen dies sind und ob sie bekannten Themen von Texten entsprechen, wie z. B. "Sport", "Wissenschaft", "Politik", ist unbekannt.  In diesem Fall ist es besser, über das Thema als eine Art abstrakte Einheit zu sprechen, die durch eine Linie in der Matrix der Wortverteilung nach Themen definiert ist und mit einiger Wahrscheinlichkeit diesem Text entspricht, wenn Sie es sich als eine Familie charakteristischer Wortgruppen vorstellen können, die sich mit entsprechenden Wahrscheinlichkeiten treffen (aus der Tabelle) in einem bestimmten Satz von Texten. </p><br><p>  Wenn Sie detaillierter und in Formeln studieren möchten, wie die LDA geschult ist und funktioniert, finden Sie hier einige Materialien (die vom Autor verwendet wurden): </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Originalartikel</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Auf Englisch mit anschaulichen Beispielen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Details in russischer Sprache</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Informationen zur Python-Implementierung</a> </li></ul><br><h2 id="dobyvaem-dikie-dannye">  Wir bekommen wilde Daten </h2><br><p>  Für unsere 'Laborarbeit' benötigen wir einen benutzerdefinierten Datensatz mit eigenen Fehlern und Merkmalen.  Sie können es an verschiedenen Orten erhalten: Laden Sie Rezensionen von Kinopoisk, Wikipedia-Artikel, Nachrichten von einem Nachrichtenportal herunter, wir werden eine etwas extremere Option wählen - Beiträge von VKontakte-Communities. </p><br><p>  Wir werden das so machen: </p><br><ol><li>  Wir wählen einen VK-Benutzer aus. </li><li>  Wir bekommen eine Liste aller seiner Freunde. </li><li>  Für jeden Freund nehmen wir seine gesamte Gemeinschaft. </li><li>  Für jede Community jedes Freundes pumpen wir die ersten n (n = 100) Community-Beiträge aus und kombinieren sie zu einem Community-Textinhalt. </li></ol><br><h4 id="instrumenty-i-stati">  Werkzeuge und Artikel </h4><br><p>  Um Beiträge herunterzuladen, verwenden wir das vk-Modul, um mit der VKontakte-API für Python zu arbeiten.  Einer der kompliziertesten Momente beim Schreiben einer Anwendung mit der VKontakte-API ist die Autorisierung. Glücklicherweise ist der Code, der diese Arbeit ausführt, bereits geschrieben und gemeinfrei. Mit Ausnahme von vk habe ich ein kleines Autorisierungsmodul verwendet - vkauth. </p><br><p>  Links zu den Modulen und Artikeln, die zum Studium der VKontakte-API verwendet wurden: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vkauth</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vkauth Tutorial</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vk Tutorial</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vk Tutorial Nummer 2</a> </li><li>  Offizielle Dokumentation der Vkontakte API </li></ul><br><h4 id="pishem-kod">  Einen Code schreiben </h4><br><p>  Melden Sie sich mit vkauth an: </p><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#authorization of app using modules imported. app_id = '6203169' perms = ['photos','friends','groups'] API_ver = '5.68' Auth = VKAuth(perms, app_id, API_ver) Auth.auth() token = Auth.get_token() user_id = Auth.get_user_id() #starting session session = vk.Session(access_token=token) api = vk.API(session)</span></span></code> </pre> <br><p>  Dabei wurde ein kleines Modul geschrieben, das alle Funktionen enthält, die zum Herunterladen von Inhalten im entsprechenden Format erforderlich sind. Sie sind unten aufgeführt. Lassen Sie uns diese durchgehen: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_friends_ids</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(api, user_id)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' For a given API object and user_id returns a list of all his friends ids. '''</span></span> friends = api.friends.get(user_id=user_id, v = <span class="hljs-string"><span class="hljs-string">'5.68'</span></span>) friends_ids = friends[<span class="hljs-string"><span class="hljs-string">'items'</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> friends_ids <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_user_groups</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(api, user_id, moder=True, only_open=True)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' For a given API user_id returns list of all groups he subscribed to. Flag model to get only those groups where user is a moderator or an admin) Flag only_open to get only public(open) groups. '''</span></span> kwargs = {<span class="hljs-string"><span class="hljs-string">'user_id'</span></span> : user_id, <span class="hljs-string"><span class="hljs-string">'v'</span></span> : <span class="hljs-string"><span class="hljs-string">'5.68'</span></span> } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> moder == <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: kwargs[<span class="hljs-string"><span class="hljs-string">'filter'</span></span>] = <span class="hljs-string"><span class="hljs-string">'moder'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> only_open == <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: kwargs[<span class="hljs-string"><span class="hljs-string">'extended'</span></span>] = <span class="hljs-number"><span class="hljs-number">1</span></span> kwargs[<span class="hljs-string"><span class="hljs-string">'fields'</span></span>] = [<span class="hljs-string"><span class="hljs-string">'is_closed'</span></span>] groups = api.groups.get(**kwargs) groups_refined = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> group <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> groups[<span class="hljs-string"><span class="hljs-string">'items'</span></span>]: cond_check = (only_open <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> group[<span class="hljs-string"><span class="hljs-string">'is_closed'</span></span>] == <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> only_open <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> cond_check: refined = {} refined[<span class="hljs-string"><span class="hljs-string">'id'</span></span>] = group[<span class="hljs-string"><span class="hljs-string">'id'</span></span>] * (<span class="hljs-number"><span class="hljs-number">-1</span></span>) refined[<span class="hljs-string"><span class="hljs-string">'name'</span></span>] = group[<span class="hljs-string"><span class="hljs-string">'name'</span></span>] groups_refined.append(refined) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> groups_refined <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_n_posts_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(api, group_id, n_posts=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">50</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' For a given api and group_id returns first n_posts concatenated as one text. '''</span></span> wall_contents = api.wall.get(owner_id = group_id, count=n_posts, v = <span class="hljs-string"><span class="hljs-string">'5.68'</span></span>) wall_contents = wall_contents[<span class="hljs-string"><span class="hljs-string">'items'</span></span>] text = <span class="hljs-string"><span class="hljs-string">''</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> post <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> wall_contents: text += post[<span class="hljs-string"><span class="hljs-string">'text'</span></span>] + <span class="hljs-string"><span class="hljs-string">' '</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text</code> </pre> <br><p>  Die endgültige Pipeline lautet wie folgt: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#id of user whose friends you gonna get, like: https://vk.com/id111111111 user_id = 111111111 friends_ids = vt.get_friends_ids(api, user_id) #collecting all groups groups = [] for i,friend in tqdm(enumerate(friends_ids)): if i % 3 == 0: sleep(1) friend_groups = vt.get_user_groups(api, friend, moder=False) groups += friend_groups #converting groups to dataFrame groups_df = pd.DataFrame(groups) groups_df.drop_duplicates(inplace=True) #reading content(content == first 100 posts) for i,group in tqdm(groups_df.iterrows()): name = group['name'] group_id = group['id'] #Different kinds of fails occures during scrapping #For examples there are names of groups with slashes #Like: 'The Kaaats / Indie-rock' try: content = vt.get_n_posts_text(api, group_id, n_posts=100) dst_path = join(data_path, name + '.txt') with open(dst_path, 'w+t') as f: f.write(content) except Exception as e: print('Error occured on group:', name) print(e) continue #need it because of requests limitaion in VK API. if i % 3 == 0: sleep(1)</span></span></code> </pre> <br><h4 id="fails">  Schlägt fehl </h4><br><p>  Im Allgemeinen ist das Herunterladen von Daten an sich nicht schwierig. Sie sollten nur zwei Punkte beachten: </p><br><ol><li>  Aufgrund der Privatsphäre einiger Communitys erhalten Sie manchmal Zugriffsfehler, manchmal werden andere Fehler durch die Installation von try behoben, außer an der richtigen Stelle. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">VK hat eine Begrenzung</a> für die Anzahl der Anforderungen pro Sekunde. </li></ol><br><p>  Wenn Sie eine große Anzahl von Anforderungen stellen, beispielsweise in einer Schleife, werden auch Fehler abgefangen.  Dieses Problem kann auf verschiedene Arten gelöst werden: </p><br><ol><li>  Dumm und unverblümt: Bleiben Sie alle 3 Anfragen im Schlaf (einige).  Dies geschieht in einer Zeile und verlangsamt das Entladen erheblich, wenn das Datenvolumen nicht groß ist und keine Zeit für komplexere Methoden bleibt - dies ist durchaus akzeptabel. (In diesem Artikel implementiert) </li><li>  Verstehen Sie die Arbeit von Long Poll-Anfragen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://vk.com/dev/using_longpoll</a> </li></ol><br><p>  In diesem Artikel wurde eine einfache und langsame Methode gewählt. In Zukunft werde ich wahrscheinlich einen Mikroartikel darüber schreiben, wie die Anzahl der Anfragen pro Sekunde umgangen oder die Einschränkungen aufgehoben werden können. </p><br><h4 id="itog">  Zusammenfassung </h4><br><p>  Mit dem Startwert "einige" Benutzer mit ~ 150 Freunden gelang es ihnen, 4.679 Texte zu erhalten - jeder kennzeichnet eine bestimmte VK-Community.  Die Texte sind sehr unterschiedlich groß und in vielen Sprachen verfasst - einige davon sind für unsere Zwecke nicht geeignet, aber wir werden etwas weiter darauf eingehen. </p><br><h3 id="osnovnaya-chast">  Hauptteil </h3><br><p><img src="https://habrastorage.org/webt/bj/to/hm/bjtohmsrvsxlcbs78u0thlawxky.png" alt="Bild"></p><br><p>  Lassen Sie uns alle Blöcke unserer Pipeline durchgehen - zuerst das obligatorische (Ideal), dann den Rest - sie sind nur von größtem Interesse. </p><br><h4 id="countvectorizer">  Countvectorizer </h4><br><p>  Bevor wir LDA unterrichten, müssen wir unsere Dokumente in Form einer Term-Dokumentmatrix präsentieren.  Dies umfasst normalerweise Operationen wie: </p><br><ul><li>  Puttuctions / Zahlen / unnötige Token entfernen. </li><li>  Tokenisierung (Präsentation als Wortliste) </li><li>  Wörter zählen, eine thermische Dokumentmatrix zusammenstellen. </li></ul><br><p>  Alle diese Aktionen in sklearn werden bequem im Rahmen einer Programmentität implementiert - sklearn.feature_extraction.text.CountVectorizer. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentationslink</a> </p><br><p>  Alles was Sie tun müssen ist: </p><br><pre> <code class="python hljs">count_vect = CountVectorizer(input=<span class="hljs-string"><span class="hljs-string">'filename'</span></span>, stop_words=stopwords, vocabulary=voc) dataset = count_vect.fit_transform(train_names)</code> </pre> <br><h4 id="lda">  Lda </h4><br><p>  Ähnlich wie bei CountVectorizer ist LDA in Sklearn und anderen Frameworks perfekt implementiert. Daher macht es in unserem rein praktischen Artikel nicht viel Sinn, viel Platz direkt für deren Implementierung zu verwenden. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentationslink</a> </p><br><p>  Alles was Sie brauchen, um LDA zu starten ist: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#training LDA lda = LDA(n_components = 60, max_iter=30, n_jobs=6, learning_method='batch', verbose=1) lda.fit(dataset)</span></span></code> </pre> <br><h4 id="preprocessing">  Vorverarbeitung </h4><br><p>  Wenn wir unsere Texte nur unmittelbar nach dem Herunterladen und Konvertieren in eine Term-Document-Matrix mit dem CountVectorizer und dem integrierten Standard-Tokenizer verwenden, erhalten wir eine Matrix mit der Größe 4679 x 769801 (für die von mir verwendeten Daten). </p><br><p>  Die Größe unseres Wörterbuchs wird 769801 betragen. Selbst wenn wir davon ausgehen, dass die meisten Wörter informativ sind, ist es unwahrscheinlich, dass wir eine gute LDA erhalten. Etwas wie „Flüche der Dimensionen“ erwartet uns, ganz zu schweigen von fast jedem Computer. Wir werden nur den gesamten RAM verstopfen.  Tatsächlich sind die meisten dieser Wörter völlig uninformativ.  Die überwiegende Mehrheit von ihnen sind: </p><br><ul><li>  Emoticons, Zeichen, Zahlen. </li><li>  Einzigartige oder sehr seltene Wörter (z. B. polnische Wörter aus einer Gruppe mit polnischen Memen, falsch geschriebene Wörter oder „albanisch“). </li><li>  Sehr häufige Wortarten (z. B. Präpositionen und Pronomen). </li></ul><br><p>  Darüber hinaus sind viele Gruppen in VK ausschließlich auf Bilder spezialisiert - es gibt dort fast keine Textbeiträge - die ihnen entsprechenden Texte sind entartet, in der thermischen Dokumentmatrix geben sie uns fast vollständig Nullzeilen. </p><br><p>  Und so, lasst uns alles klären! <br>  Wir tokenisieren alle Texte, entfernen Satzzeichen und Zahlen aus ihnen und betrachten das Histogramm der Verteilung von Texten nach der Anzahl der Wörter: <br><img src="https://habrastorage.org/webt/v4/qh/w0/v4qhw0mrgpizranmnptbz5lnivk.png" alt="Bild"></p><br><p>  Wir entfernen alle Texte, die kleiner als 100 Wörter sind (es gibt 525 davon). </p><br><p>  Nun das Wörterbuch: <br>  Das Entfernen aller Token (Wörter), die keine Buchstaben sind, im Rahmen unserer Aufgabe - dies ist durchaus akzeptabel.  Der CountVectorizer macht dies selbstständig, auch wenn dies nicht der Fall ist. Ich denke, hier müssen keine Beispiele angegeben werden (sie sind in der Vollversion des Codes für den Artikel enthalten). </p><br><p>  Eines der häufigsten Verfahren zum Reduzieren der Größe eines Wörterbuchs ist das Entfernen der sogenannten Stoppwörter (Stoppwörter) - Wörter, die keine semantische Last tragen und / oder keine thematische Färbung aufweisen (in unserem Fall Themenmodellierung).  Solche Wörter sind in unserem Fall zum Beispiel: </p><br><ul><li>  Pronomen und Präpositionen. </li><li>  Artikel - die, a. </li><li>  Allgemeine Wörter: "sein", "gut", "wahrscheinlich" usw. </li></ul><br><p>  Das nltk-Modul hat Listen von Stoppwörtern in Russisch und Englisch erstellt, die jedoch eher schwach sind.  Im Internet können Sie auch Listen mit Stoppwörtern für jede Sprache finden und zu denen in nltk hinzufügen.  Also werden wir es tun.  Nehmen Sie zusätzliche Stoppwörter von hier: </p><br><ul><li>  <a href="">https://github.com/stopwords-iso/stopwords-ru/blob/master/stopwords-ru.json</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://gist.github.com/menzenski/7047705</a> </li></ul><br><p>  In der Praxis werden bei der Lösung spezifischer Probleme die Listen der Stoppwörter schrittweise angepasst und ergänzt, wenn die Modelle trainiert werden, da für jeden spezifischen Datensatz und jedes Problem bestimmte "inkonsistente" Wörter vorhanden sind.  Nach dem Training unserer LDA der ersten Generation werden wir auch benutzerdefinierte Stoppwörter verwenden. </p><br><p>  Das Verfahren zum Entfernen von Stoppwörtern ist in CountVectorizer integriert - wir benötigen lediglich eine Liste davon. </p><br><p>  Ist das, was wir getan haben, genug? </p><br><p><img src="https://habrastorage.org/webt/ja/xd/6l/jaxd6lnbbnd_dmmmk6nog9a2ntm.png" alt="Bild"></p><br><p>  Die meisten Wörter in unserem Wörterbuch sind immer noch nicht zu informativ, um LDA zu lernen, und sie sind nicht in der Liste der Stoppwörter enthalten.  Daher wenden wir eine andere Filtermethode auf unsere Daten an. </p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>i</mi><mi>d</mi><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>,</mo><mi>D</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>o</mi><mi>g</mi><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>D</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mspace linebreak=&quot;newline&quot; /><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mtext>&amp;#xA0;</mtext><mi>i</mi><mi>n</mi><mi>D</mi><mo>:</mo><mi>t</mi><mtext>&amp;#xA0;</mtext><mi>i</mi><mi>n</mi><mi>d</mi><mspace linebreak=&quot;newline&quot; /></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></mrow></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="37.218ex" height="8.202ex" viewBox="0 -832 16024.3 3531.4" role="img" focusable="false" style="vertical-align: -6.27ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-69" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-64" x="345" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-66" x="869" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMAIN-28" x="1419" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-74" x="1809" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMAIN-2C" x="2170" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-44" x="2615" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMAIN-29" x="3444" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMAIN-3D" x="4111" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-6C" x="5417" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-6F" x="5716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-67" x="6201" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-66" x="6932" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-72" x="7482" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-61" x="7934" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-63" x="8463" y="0"></use><g transform="translate(8897,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMAIN-7C" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-44" x="278" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMAIN-7C" x="1107" y="0"></use></g><g transform="translate(10282,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMAIN-7C" x="0" y="0"></use><g transform="translate(0,-1432)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-64" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-69" x="773" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-6E" x="1119" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-44" x="1719" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMAIN-3A" x="2825" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-74" x="3382" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-69" x="3993" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-6E" x="4339" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMATHI-64" x="4939" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhhhjLBA_MgUwfm6SluyksYfJeTqtw#MJMAIN-7C" x="5463" y="0"></use></g></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>i</mi><mi>d</mi><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>&nbsp;</mtext><mi>l</mi><mi>o</mi><mi>g</mi><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>D</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mrow><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mspace linebreak="newline"></mspace><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mtext>&nbsp;</mtext><mi>i</mi><mi>n</mi><mi>D</mi><mo>:</mo><mi>t</mi><mtext>&nbsp;</mtext><mi>i</mi><mi>n</mi><mi>d</mi><mspace linebreak="newline"></mspace></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-4"> idf (t, D) = \ log \ frac {| D |} {| \\ {d \ in D: t \ in d \\} |} </script></p><br><p>  wo <br>  t ist ein Wort aus dem Wörterbuch. <br>  D - Fall (viele Texte) <br>  d ist einer der Körpertexte. <br>  Wir berechnen die IDF aller unserer Wörter und schneiden die Wörter mit der größten IDF (sehr selten) und mit der kleinsten (weit verbreiteten Wörter) ab. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#'training' (tf-)idf vectorizer. tf_idf = TfidfVectorizer(input='filename', stop_words=stopwords, smooth_idf=False ) tf_idf.fit(train_names) #getting idfs idfs = tf_idf.idf_ #sorting out too rare and too common words lower_thresh = 3. upper_thresh = 6. not_often = idfs &gt; lower_thresh not_rare = idfs &lt; upper_thresh mask = not_often * not_rare good_words = np.array(tf_idf.get_feature_names())[mask] #deleting punctuation as well. cleaned = [] for word in good_words: word = re.sub("^(\d+\w*$|_+)", "", word) if len(word) == 0: continue cleaned.append(word)</span></span></code> </pre> <br><p>  Das Erhalten nach den oben genannten Verfahren ist bereits für das LDA-Training gut geeignet, aber wir werden mehr Stemming betreiben - die gleichen Wörter werden häufig in unserem Datensatz gefunden, jedoch in verschiedenen Fällen.  Zum <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stemming wurde Pymystem3 verwendet</a> . </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Stemming m = Mystem() stemmed = set() voc_len = len(cleaned) for i in tqdm(range(voc_len)): word = cleaned.pop() stemmed_word = m.lemmatize(word)[0] stemmed.add(stemmed_word) stemmed = list(stemmed) print('After stemming: %d'%(len(stemmed)))</span></span></code> </pre> <br><p>  Nach Anwendung der obigen Filterung verringerte sich die Wörterbuchgröße von 769801 auf <br>  13611 und bereits mit solchen Daten können Sie ein LDA-Modell von akzeptabler Qualität erhalten. </p><br><h3 id="testirovanie-primenenie-i-tyuning-lda">  Testen, Anwenden und Einstellen von LDA </h3><br><p>  Nachdem wir nun den Datensatz, die Vorverarbeitung und die Modelle haben, die wir für den verarbeiteten Datensatz trainiert haben, wäre es schön, die Angemessenheit unserer Modelle zu überprüfen und einige Anwendungen für sie zu erstellen. </p><br><p>  Betrachten Sie als Anwendung zunächst die Aufgabe, Schlüsselwörter für einen bestimmten Text zu generieren.  Sie können dies auf ziemlich einfache Weise wie folgt tun: </p><br><ol><li>  Wir erhalten von der LDA die Verteilung der Themen für diesen Text. </li><li>  Wählen Sie n (zum Beispiel n = 2) der am stärksten ausgeprägten Themen. </li><li>  Wählen Sie für jedes Thema m (zum Beispiel m = 3) die charakteristischsten Wörter. </li><li>  Wir haben eine Reihe von n * m Wörtern, die einen bestimmten Text charakterisieren. </li></ol><br><p>  Wir werden eine einfache Schnittstellenklasse schreiben, die diese Methode zum Generieren von Schlüsselwörtern implementiert: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Let\`s do simple interface class class TopicModeler(object): ''' Inteface object for CountVectorizer + LDA simple usage. ''' def __init__(self, count_vect, lda): ''' Args: count_vect - CountVectorizer object from sklearn. lda - LDA object from sklearn. ''' self.lda = lda self.count_vect = count_vect self.count_vect.input = 'content' def __call__(self, text): ''' Gives topics distribution for a given text Args: text - raw text via python string. returns: numpy array - topics distribution for a given text. ''' vectorized = self.count_vect.transform([text]) lda_topics = self.lda.transform(vectorized) return lda_topics def get_keywords(self, text, n_topics=3, n_keywords=5): ''' For a given text gives n top keywords for each of m top texts topics. Args: text - raw text via python string. n_topics - int how many top topics to use. n_keywords - how many top words of each topic to return. returns: list - of m*n keywords for a given text. ''' lda_topics = self(text) lda_topics = np.squeeze(lda_topics, axis=0) n_topics_indices = lda_topics.argsort()[-n_topics:][::-1] top_topics_words_dists = [] for i in n_topics_indices: top_topics_words_dists.append(self.lda.components_[i]) shape=(n_keywords*n_topics, self.lda.components_.shape[1]) keywords = np.zeros(shape=shape) for i,topic in enumerate(top_topics_words_dists): n_keywords_indices = topic.argsort()[-n_keywords:][::-1] for k,j in enumerate(n_keywords_indices): keywords[i * n_keywords + k, j] = 1 keywords = self.count_vect.inverse_transform(keywords) keywords = [keyword[0] for keyword in keywords] return keywords</span></span></code> </pre> <br><p>  Wir wenden unsere Methode auf mehrere Texte an und sehen, was passiert: <br>  Community <strong>:</strong> Reisebüro "Farben der Welt" <br>  <strong>Schlüsselwörter:</strong> ['Foto', 'sozial', 'Reisen', 'Gemeinschaft', 'Reisen', 'Euro', 'Unterkunft', 'Preis', 'Polen', 'Abreise'] <br>  <strong>Gemeinschaft:</strong> Food Gifs <br>  <strong>Schlüsselwörter:</strong> ['Butter', 'St', 'Salz', 'PC', 'Teig', 'Kochen', 'Zwiebel', 'Pfeffer', 'Zucker', 'Gr'] </p><br><p>  Die obigen Ergebnisse sind nicht "Kirschpickel" und sehen völlig angemessen aus.  Tatsächlich sind dies die Ergebnisse eines bereits konfigurierten Modells.  Die ersten LDAs, die im Rahmen dieses Artikels geschult wurden, führten zu deutlich schlechteren Ergebnissen, unter anderem bei häufig verwendeten Keywords: </p><br><ol><li>  Zusammengesetzte Komponenten von Webadressen: www, http, ru, com ... </li><li>  Gemeinsame Wörter. </li><li>  Einheiten: cm, Meter, km ... </li></ol><br><p>  Das Tuning (Tuning) des Modells wurde wie folgt durchgeführt: </p><br><ol><li>  Wählen Sie für jedes Thema n (n = 5) charakteristischste Wörter aus. </li><li>  Wir betrachten sie je nach Schulungsfall als idf. </li><li>  Wir bringen 5-10% der am weitesten verbreiteten Keywords ein. </li></ol><br><p>  Eine solche „Reinigung“ sollte sorgfältig durchgeführt werden, wobei genau diese 10% der Wörter vorab betrachtet werden.  Stattdessen sollten Kandidaten zum Löschen auf diese Weise ausgewählt werden, und dann sollten Wörter, die gelöscht werden sollen, manuell aus ihnen ausgewählt werden. </p><br><p>  Irgendwo in der 2-3-Generation von Modellen mit einer ähnlichen Art der Auswahl von Stoppwörtern für die Top 5% der weit verbreiteten Top-Wort-Verteilungen erhalten wir: <br>  ['beliebig', 'vollständig', 'richtig', 'einfach', 'weiter', 'Internet', 'klein', 'Weg', 'schwierig', 'Stimmung', 'so viel', 'eingestellt', ' Option ',' Name ',' Rede ',' Programm ',' Wettbewerb ',' Musik ',' Ziel ',' Film ',' Preis ',' Spiel ',' System ',' Spiel ',' Firma ' , 'nett'] </p><br><h3 id="esche-prilozheniya">  Weitere Anwendungen </h3><br><p>  Das erste, was mir speziell in den Sinn kommt, ist, die Verteilung von Themen im Text als "Einbettung" von Texten zu verwenden. In dieser Interpretation können Sie Visualisierungs- oder Clustering-Algorithmen auf sie anwenden und auf diese Weise nach den endgültigen "effektiven" thematischen Clustern suchen. </p><br><p>  Machen wir das: </p><br><pre> <code class="python hljs">term_doc_matrix = count_vect.transform(names) embeddings = lda.transform(term_doc_matrix) kmeans = KMeans(n_clusters=<span class="hljs-number"><span class="hljs-number">30</span></span>) clust_labels = kmeans.fit_predict(embeddings) clust_centers = kmeans.cluster_centers_ embeddings_to_tsne = np.concatenate((embeddings,clust_centers), axis=<span class="hljs-number"><span class="hljs-number">0</span></span>) tSNE = TSNE(n_components=<span class="hljs-number"><span class="hljs-number">2</span></span>, perplexity=<span class="hljs-number"><span class="hljs-number">15</span></span>) tsne_embeddings = tSNE.fit_transform(embeddings_to_tsne) tsne_embeddings, centroids_embeddings = np.split(tsne_embeddings, [len(clust_labels)], axis=<span class="hljs-number"><span class="hljs-number">0</span></span>)</code> </pre> <br><p>  Am Ausgang erhalten wir folgendes Bild: <br><img src="https://habrastorage.org/webt/7j/2r/qv/7j2rqvpp2-blr9vfq44qjs75upc.png" alt="Bild"></p><br><p>  Kreuze sind die Schwerpunkte (Cenroide) von Clustern. </p><br><p>  Im tSNE-Bild von Einbettungen ist zu sehen, dass die mit KMeans ausgewählten Cluster ziemlich zusammenhängende und meist räumlich trennbare Mengen bilden. </p><br><p>  Alles andere bis zu dir. </p><br><p>  Link zu allen Codes: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://gitlab.com/Mozes/VK_LDA</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de417167/">https://habr.com/ru/post/de417167/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de417155/index.html">Linux-Kernel 4.18: Was bereitet sich auf die bevorstehende Veröffentlichung vor?</a></li>
<li><a href="../de417157/index.html">Die Singularität rückt näher: Die KI beginnt, Roboter zu steuern</a></li>
<li><a href="../de417161/index.html">Burger King: geheime Überwachung, Lügen, Diebstahl von Bankkarten. Fortsetzung</a></li>
<li><a href="../de417163/index.html">Feine Commits</a></li>
<li><a href="../de417165/index.html">Was Burger King bedroht</a></li>
<li><a href="../de417171/index.html">Studie: Von Frauen betriebene Hedgefonds zeigen bessere Ergebnisse</a></li>
<li><a href="../de417173/index.html">„Old New Vinyl“: 20 Materialien zur Geschichte und Produktion von Plattenspielern und Schallplatten</a></li>
<li><a href="../de417175/index.html">Acme Road Semaphor Restaurierung der ersten Hälfte des 20. Jahrhunderts</a></li>
<li><a href="../de417177/index.html">Lokaler Webserver unter Linux mit automatischem Host-Raising und PHP-Versionswechsel</a></li>
<li><a href="../de417179/index.html">Einrichten einer Heimentwicklungsumgebung (Docker + Gitlab + DNS)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>