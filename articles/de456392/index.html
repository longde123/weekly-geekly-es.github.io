<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🅱️ 🗻 👨🏻 Parsim 25 TB mit AWK und R. ☦️ 🏼 👨🏽‍🚀</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wie man diesen Artikel liest : Ich entschuldige mich dafür, dass der Text so lang und chaotisch geworden ist. Um Ihnen Zeit zu sparen, beginne ich jed...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Parsim 25 TB mit AWK und R.</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/456392/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/9d/3y/lc/9d3ylcjuqiv6r7vrv6p52apvmne.jpeg"></div><br>  <i><b>Wie man diesen Artikel liest</b> : Ich entschuldige mich dafür, dass der Text so lang und chaotisch geworden ist.</i>  <i>Um Ihnen Zeit zu sparen, beginne ich jedes Kapitel mit der Einführung von „Was ich gelernt habe“, in der ich die Essenz des Kapitels in ein oder zwei Sätzen erkläre.</i> <i><br><br></i>  <i><b>"Zeigen Sie einfach die Lösung!"</b></i>  <i>Wenn Sie nur sehen möchten, wozu ich gekommen bin, lesen Sie das Kapitel "Erfinderischer werden". Ich finde es jedoch interessanter und nützlicher, über Fehler zu lesen.</i> <br><br>  Kürzlich wurde ich angewiesen, ein Verfahren zur Verarbeitung eines großen Volumens der ursprünglichen DNA-Sequenzen einzurichten (technisch gesehen ist dies ein SNP-Chip).  Es war notwendig, schnell Daten über einen bestimmten genetischen Ort (SNP genannt) für die nachfolgende Modellierung und andere Aufgaben zu erhalten.  Mit Hilfe von R und AWK konnte ich die Daten auf natürliche Weise bereinigen und organisieren und so die Bearbeitung von Anfragen erheblich beschleunigen.  Dies war für mich nicht einfach und erforderte zahlreiche Iterationen.  Dieser Artikel wird Ihnen helfen, einige meiner Fehler zu vermeiden und zu demonstrieren, was ich am Ende getan habe. <br><a name="habracut"></a><br>  Zunächst einige einleitende Erklärungen. <br><br><h2>  Daten </h2><br>  Unser Genetic Information Processing Center der Universität hat uns 25 TB TSV-Daten zur Verfügung gestellt.  Ich habe sie in 5 von Gzip komprimierte Pakete aufgeteilt, von denen jedes ungefähr 240 Vier-Gigabyte-Dateien enthielt.  Jede Zeile enthielt Daten für einen SNP einer Person.  Insgesamt wurden Daten zu ~ 2,5 Millionen SNPs und ~ 60.000 Menschen übertragen.  Zusätzlich zu den SNP-Informationen gab es in den Dateien zahlreiche Spalten mit Zahlen, die verschiedene Merkmale wie Leseintensität, Häufigkeit verschiedener Allele usw. widerspiegeln.  Es gab ungefähr 30 Spalten mit eindeutigen Werten. <br><br><h4>  Zweck </h4><br>  Wie bei jedem Datenverwaltungsprojekt war es am wichtigsten, zu bestimmen, wie die Daten verwendet werden sollen.  In diesem Fall werden <b>wir zum größten Teil Modelle und Workflows für SNP basierend auf SNP auswählen</b> .  Das heißt, gleichzeitig benötigen wir Daten für nur einen SNP.  Ich musste lernen, wie man alle Datensätze, die sich auf einen der 2,5 Millionen SNPs beziehen, so einfach wie möglich, schneller und billiger extrahiert. <br><br><h1>  Wie man es nicht macht </h1><br>  Ich werde ein passendes Klischee zitieren: <br><br><blockquote>  Ich habe nicht tausendmal versagt, sondern nur tausend Möglichkeiten entdeckt, eine Reihe von Daten nicht in einem für Abfragen geeigneten Format zu analysieren. </blockquote><br>
<h2>  Erster Versuch </h2><br>  <b>Was ich gelernt habe</b> : Es gibt keine billige Möglichkeit, 25 TB gleichzeitig zu analysieren. <br><br>  Nachdem ich mir an der Vanderbilt University das Thema „Advanced Big Data Processing Methods“ angehört hatte, war ich mir sicher, dass es ein Hut war.  Möglicherweise dauert es ein oder zwei Stunden, um den Hive-Server so zu konfigurieren, dass er alle Daten durchläuft und über das Ergebnis berichtet.  Da unsere Daten in AWS S3 gespeichert sind, habe ich den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Athena-</a> Dienst verwendet, mit dem Sie Hive SQL-Abfragen auf S3-Daten anwenden können.  Sie müssen den Hive-Cluster nicht konfigurieren / erhöhen und müssen nur für die gesuchten Daten bezahlen. <br><br>  Nachdem ich Athena meine Daten und ihr Format gezeigt hatte, führte ich einige Tests mit ähnlichen Abfragen durch: <br><br><pre><code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">select</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> intensityData <span class="hljs-keyword"><span class="hljs-keyword">limit</span></span> <span class="hljs-number"><span class="hljs-number">10</span></span>;</code> </pre> <br>  Und schnell gut strukturierte Ergebnisse erhalten.  Fertig. <br><br>  Bis wir versuchten, die Daten in der Arbeit zu verwenden ... <br><br>  Ich wurde gebeten, alle SNP-Informationen abzurufen, um das Modell darauf zu testen.  Ich habe eine Abfrage ausgeführt: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">select</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> intensityData <span class="hljs-keyword"><span class="hljs-keyword">where</span></span> snp = <span class="hljs-string"><span class="hljs-string">'rs123456'</span></span>;</code> </pre> <br>  ... und wartete.  Nach acht Minuten und mehr als 4 TB der angeforderten Daten erhielt ich das Ergebnis.  Athena berechnet eine Gebühr für die gefundene Datenmenge von 5 USD pro Terabyte.  Diese einzelne Anfrage kostete also 20 US-Dollar und acht Minuten Wartezeit.  Um das Modell nach allen Daten laufen zu lassen, musste man 38 Jahre warten und 50 Millionen Dollar zahlen. Offensichtlich passte dies nicht zu uns. <br><br><h2>  Es war notwendig, Parkett zu verwenden ... </h2><br>  <b>Was ich gelernt habe</b> : Seien Sie vorsichtig mit der Größe Ihrer Parkettdateien und ihrer Organisation. <br><br>  Zuerst habe ich versucht, die Situation zu korrigieren, indem ich alle TSVs in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Parkettdateien konvertiert habe</a> .  Sie eignen sich für die Arbeit mit großen Datenmengen, da die darin enthaltenen Informationen in Spaltenform gespeichert sind: Im Gegensatz zu Textdateien, in denen Zeilen Elemente jeder Spalte enthalten, befindet sich jede Spalte in einem eigenen Speicher- / Festplattensegment.  Und wenn Sie etwas finden müssen, lesen Sie einfach die erforderliche Spalte.  Darüber hinaus wird in jeder Datei in einer Spalte ein Wertebereich gespeichert. Wenn der gewünschte Wert nicht im Spaltenbereich liegt, verschwendet Spark keine Zeit mit dem Scannen der gesamten Datei. <br><br>  Ich habe eine einfache <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AWS Glue-</a> Aufgabe ausgeführt, um unsere TSVs in Parkett umzuwandeln, und neue Dateien in Athena abgelegt.  Es dauerte ungefähr 5 Stunden.  Aber als ich die Anfrage startete, dauerte es ungefähr die gleiche Zeit und etwas weniger Geld, um sie abzuschließen.  Tatsache ist, dass Spark beim Versuch, die Aufgabe zu optimieren, einfach einen TSV-Block entpackt und in einen eigenen Parkettblock gelegt hat.  Und da jeder Block groß genug war und die vollständigen Aufzeichnungen vieler Personen enthielt, wurden alle SNPs in jeder Datei gespeichert, sodass Spark alle Dateien öffnen musste, um die erforderlichen Informationen zu extrahieren. <br><br>  Seltsamerweise ist der Standardkomprimierungstyp (und der empfohlene Komprimierungstyp) in Parkett - bissig - nicht aufteilbar.  Daher blieb jeder Executor bei der Aufgabe, den gesamten 3,5-GB-Datensatz zu entpacken und herunterzuladen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f42/584/fb3/f42584fb3e65319eef46f117c11525f3.png"><br><h2>  Wir verstehen das Problem </h2><br>  <b>Was ich gelernt habe</b> : Das Sortieren ist schwierig, besonders wenn die Daten verteilt sind. <br><br>  Es schien mir, dass ich jetzt die Essenz des Problems verstand.  Ich musste die Daten nur nach SNP-Spalten sortieren, nicht nach Personen.  Dann werden mehrere SNPs in einem separaten Datenblock gespeichert, und dann manifestiert sich die intelligente Parkettfunktion „Nur öffnen, wenn der Wert im Bereich liegt“ in ihrer ganzen Pracht.  Leider hat sich das Aussortieren von Milliarden von Zeilen, die über einen Cluster verteilt sind, als entmutigende Aufgabe erwiesen. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-0" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1105127759318319105"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  AWS möchte das Geld sicherlich nicht zurückgeben, weil "ich ein zerstreuter Student bin".  Nachdem ich mit dem Sortieren auf Amazon Glue begonnen hatte, funktionierte es 2 Tage lang und stürzte ab. <br><br><h2>  Was ist mit Partitionierung? </h2><br>  <b>Was ich gelernt habe</b> : Partitionen in Spark sollten ausgeglichen sein. <br><br>  Dann kam mir die Idee, die Daten auf den Chromosomen zu partitionieren.  Es gibt 23 von ihnen (und einige weitere angesichts mitochondrialer DNA und nicht kartierter Bereiche). <br>  Auf diese Weise können Sie die Daten in kleinere Teile aufteilen.  Wenn Sie der Spark-Exportfunktion im Glue-Skript nur eine Zeile <code>partition_by = "chr"</code> hinzufügen, sollten die Daten in Buckets sortiert werden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/652/f42/3dc/652f423dc8806401b6638a3cf8c1480b.png"><br>  <i>Das Genom besteht aus zahlreichen Fragmenten, die als Chromosomen bezeichnet werden.</i> <br><br>  Dies hat leider nicht funktioniert.  Chromosomen haben unterschiedliche Größen und daher unterschiedliche Informationsmengen.  Dies bedeutet, dass die Aufgaben, die Spark an die Mitarbeiter gesendet hat, nicht ausgeglichen und langsam ausgeführt wurden, da einige Knoten früher beendet wurden und inaktiv waren.  Die Aufgaben wurden jedoch abgeschlossen.  Bei der Anforderung eines SNP verursachte das Ungleichgewicht jedoch erneut Probleme.  Die Kosten für die Verarbeitung von SNPs auf größeren Chromosomen (dh woher wir die Daten beziehen möchten) haben sich nur um das Zehnfache verringert.  Viel, aber nicht genug. <br><br><h2>  Und wenn Sie sich in noch kleinere Partitionen aufteilen? </h2><br>  <b>Was ich gelernt habe</b> : Versuchen Sie niemals, 2,5 Millionen Partitionen zu erstellen. <br><br>  Ich entschied mich für einen Spaziergang und teilte jeden SNP auf.  Dies garantierte die gleiche Größe von Partitionen.  <b>SCHLECHT WAR EINE IDEE</b> .  Ich nutzte Glue und fügte die unschuldige <code>partition_by = 'snp'</code> .  Die Aufgabe wurde gestartet und ausgeführt.  Einen Tag später überprüfte ich, ob in S3 bisher nichts geschrieben war, und beendete die Aufgabe.  Es sieht so aus, als hätte Glue Zwischendateien an einen versteckten Ort in S3 geschrieben, und viele Dateien, vielleicht ein paar Millionen.  Infolgedessen kostete mein Fehler mehr als tausend Dollar und gefiel meinem Mentor nicht. <br><br><h2>  Partitionieren + Sortieren </h2><br>  <b>Was ich gelernt habe</b> : Das Sortieren ist immer noch schwierig, ebenso wie das Einrichten von Spark. <br><br>  Der letzte Versuch der Partitionierung war, dass ich die Chromosomen partitionierte und dann jede Partition sortierte.  Theoretisch würde dies jede Anforderung beschleunigen, da die gewünschten SNP-Daten innerhalb mehrerer Parkettblöcke innerhalb eines bestimmten Bereichs liegen sollten.  Leider hat sich das Sortieren selbst partitionierter Daten als schwierige Aufgabe erwiesen.  Infolgedessen wechselte ich für einen benutzerdefinierten Cluster zu EMR und verwendete acht leistungsstarke Instanzen (C5.4xl) und Sparklyr, um einen flexibleren Workflow zu erstellen ... <br><br><pre> <code class="scala hljs"># <span class="hljs-type"><span class="hljs-type">Sparklyr</span></span> snippet to partition by chr and sort w/in partition # <span class="hljs-type"><span class="hljs-type">Join</span></span> the raw data <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> the snp bins raw_data group_by(chr) %&gt;% arrange(<span class="hljs-type"><span class="hljs-type">Position</span></span>) %&gt;% <span class="hljs-type"><span class="hljs-type">Spark_write_Parquet</span></span>( path = <span class="hljs-type"><span class="hljs-type">DUMP_LOC</span></span>, mode = <span class="hljs-symbol"><span class="hljs-symbol">'overwrit</span></span>e', partition_by = c(<span class="hljs-symbol"><span class="hljs-symbol">'ch</span></span>r') )</code> </pre> <br>  ... die Aufgabe wurde jedoch noch nicht erledigt.  Ich habe in jeder Hinsicht optimiert: Ich habe die Speicherzuordnung für jeden Abfrage-Executor erhöht, Knoten mit einer großen Speichermenge verwendet, Broadcast-Variablen verwendet, aber jedes Mal stellte sich heraus, dass es sich um halbe Sachen handelte, und nach und nach versagten die Darsteller, bis alles aufhörte. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-1" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1128703858610450434"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br><h1>  Ich werde erfinderischer </h1><br>  <b>Was ich gelernt habe</b> : Manchmal erfordern spezielle Daten spezielle Lösungen. <br><br>  Jeder SNP hat einen Positionswert.  Dies ist die Anzahl, die der Anzahl der Basen entspricht, die entlang des Chromosoms liegen.  Dies ist eine gute und natürliche Art, unsere Daten zu organisieren.  Zuerst wollte ich jedes Chromosom nach Regionen aufteilen.  Zum Beispiel die Positionen 1 - 2000, 2001 - 4000 usw.  Das Problem ist jedoch, dass SNPs nicht gleichmäßig über die Chromosomen verteilt sind, weshalb die Größe der Gruppen stark variiert. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f46/a8e/17b/f46a8e17b9af8d2ae9777c47017764c6.png"><br><br>  Infolgedessen wurde ich in Kategorien (Rang) Positionen eingeteilt.  Gemäß den bereits heruntergeladenen Daten habe ich eine Liste mit eindeutigen SNPs, deren Positionen und Chromosomen angefordert.  Dann sortierte er die Daten in jedem Chromosom und sammelte SNP in Gruppen (bin) einer bestimmten Größe.  Sagen Sie jeweils 1000 SNP.  Dies gab mir eine SNP-Beziehung zu einer Gruppe im Chromosom. <br><br>  Am Ende habe ich Gruppen (bin) auf 75 SNP erstellt, ich werde den Grund unten erklären. <br><br><pre> <code class="bash hljs">snp_to_bin &lt;- unique_snps %&gt;% group_by(chr) %&gt;% arrange(position) %&gt;% mutate( rank = 1:n() bin = floor(rank/snps_per_bin) ) %&gt;% ungroup()</code> </pre> <br><h2>  Versuchen Sie es zuerst mit Spark </h2><br>  <b>Was ich gelernt habe</b> : Die Spark-Integration ist schnell, aber die Partitionierung ist immer noch teuer. <br><br>  Ich wollte diesen kleinen Datenrahmen (2,5 Millionen Zeilen) in Spark lesen, ihn mit Rohdaten kombinieren und dann durch die neu hinzugefügte <code>bin</code> Spalte partitionieren. <br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment"># Join the raw data with the snp bins data_w_bin &lt;- raw_data %&gt;% left_join(sdf_broadcast(snp_to_bin), by ='snp_name') %&gt;% group_by(chr_bin) %&gt;% arrange(Position) %&gt;% Spark_write_Parquet( path = DUMP_LOC, mode = 'overwrite', partition_by = c('chr_bin') )</span></span></code> </pre> <br>  Ich habe <code>sdf_broadcast()</code> , damit Spark herausfindet, dass ein <code>sdf_broadcast()</code> an alle Knoten <code>sdf_broadcast()</code> werden soll.  Dies ist nützlich, wenn die Daten klein sind und für alle Aufgaben benötigt werden.  Andernfalls versucht Spark, intelligent zu sein und Daten nach Bedarf zu verteilen, was zu Bremsen führen kann. <br><br>  Und wieder funktionierte meine Idee nicht: Die Aufgaben funktionierten eine Weile, schlossen die Fusion ab und begannen dann, wie die durch Partitionierung gestarteten Executoren, zu scheitern. <br><br><h2>  AWK hinzufügen </h2><br>  <b>Was ich gelernt habe</b> : Schlafen Sie nicht, wenn die Grundlagen es Ihnen beibringen.  Sicherlich hat jemand Ihr Problem bereits in den 1980er Jahren gelöst. <br><br>  Bis zu diesem Zeitpunkt war die Ursache all meiner Fehler mit Spark die Verwirrung der Daten im Cluster.  Vielleicht kann die Situation durch Vorverarbeitung verbessert werden.  Ich beschloss, die Rohtextdaten in Chromosomenspalten aufzuteilen, und hoffte, Spark mit „vorpartitionierten“ Daten versorgen zu können. <br><br>  Ich habe in StackOverflow nach Möglichkeiten zum Aufteilen von Spaltenwerten gesucht und eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">so gute Antwort gefunden.</a>  Mit AWK können Sie eine Textdatei in Spaltenwerte aufteilen, indem Sie in das Skript schreiben, anstatt die Ergebnisse an <code>stdout</code> senden. <br><br>  Zum Testen habe ich ein Bash-Skript geschrieben.  Ich habe einen der gepackten TSVs heruntergeladen, ihn dann mit <code>gzip</code> entpackt und an <code>awk</code> gesendet. <br><br><pre> <code class="bash hljs">gzip -dc path/to/chunk/file.gz | awk -F <span class="hljs-string"><span class="hljs-string">'\t'</span></span> \ <span class="hljs-string"><span class="hljs-string">'{print $1",..."$30"&gt;"chunked/"$chr"_chr"$15".csv"}'</span></span></code> </pre> <br>  Es hat funktioniert! <br><br><h2>  Kernfüllung </h2><br>  <b>Was ich gelernt habe</b> : <code>gnu parallel</code> ist eine magische Sache, jeder sollte sie benutzen. <br><br>  Die Trennung war ziemlich langsam, und als ich <code>htop</code> startete, um die Verwendung einer leistungsstarken (und teuren) EC2-Instanz zu testen, stellte sich heraus, dass ich nur einen Kern und ungefähr 200 MB Speicher verwendete.  Um das Problem zu lösen und nicht viel Geld zu verlieren, musste herausgefunden werden, wie die Arbeit parallelisiert werden kann.  Glücklicherweise fand ich in Jeron Janssens 'beeindruckendem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Data Science at the Command Line-</a> Buch ein Kapitel über Parallelisierung.  Daraus lernte ich <code>gnu parallel</code> , eine sehr flexible Methode zur Implementierung von Multithreading unter Unix. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/835/7c0/e45/8357c0e45f4162d53ca1c3da0c78444a.png" width="300"></div><br>  Als ich die Partition mit einem neuen Prozess startete, war alles in Ordnung, aber es gab einen Engpass - das Herunterladen von S3-Objekten auf die Festplatte war nicht zu schnell und nicht vollständig parallelisiert.  Um dies zu beheben, habe ich Folgendes getan: <br><br><ol><li>  Ich fand heraus, dass es möglich ist, den S3-Download-Schritt direkt in der Pipeline zu implementieren, wodurch der Zwischenspeicher auf der Festplatte vollständig entfällt.  Dies bedeutet, dass ich das Schreiben von Rohdaten auf die Festplatte vermeiden und noch kleineren und daher günstigeren Speicher auf AWS verwenden kann. <br></li><li>  Der Befehlssatz <code>aws configure set default.s3.max_concurrent_requests 50</code> hat die Anzahl der von der AWS CLI verwendeten Threads erheblich erhöht (standardmäßig sind es 10). <br></li><li>  Ich wechselte zu der für die Netzwerkgeschwindigkeit optimierten EC2-Instanz mit dem Buchstaben n im Namen.  Ich fand heraus, dass der Verlust an Rechenleistung bei Verwendung von n-Instanzen durch eine Erhöhung der Download-Geschwindigkeit mehr als ausgeglichen wird.  Für die meisten Aufgaben habe ich c5n.4xl verwendet. <br></li><li>  Ich habe <code>gzip</code> in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><code>pigz</code></a> geändert. Dies ist ein gzip-Tool, das coole Dinge tun kann, um die anfangs beispiellose Aufgabe des Entpackens von Dateien zu parallelisieren (dies hat am wenigsten geholfen). <br></li></ol><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Let S3 use as many threads as it wants aws configure set default.s3.max_concurrent_requests 50 for chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do aws s3 cp s3://$batch_loc$chunk_file - | pigz -dc | parallel --block 100M --pipe \ "awk -F '\t' '{print \$1\",...\"$30\"&gt;\"chunked/{#}_chr\"\$15\".csv\"}'" # Combine all the parallel process chunks to single files ls chunked/ | cut -d '_' -f 2 | sort -u | parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}' # Clean up intermediate data rm chunked/* done</span></span></code> </pre> <br>  Diese Schritte werden miteinander kombiniert, so dass alles sehr schnell funktioniert.  Dank der erhöhten Download-Geschwindigkeit und der Ablehnung des Schreibens auf die Festplatte konnte ich jetzt ein 5-Terabyte-Paket in nur wenigen Stunden verarbeiten. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-2" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1129416944233226240"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  Dieser Tweet sollte "TSV" erwähnen.  Leider. <br><br><h2>  Neu analysierte Daten verwenden </h2><br>  <b>Was ich gelernt habe</b> : Spark liebt unkomprimierte Daten und kombiniert keine Partitionen. <br><br>  Jetzt waren die Daten in S3 in einem entpackten (gelesenen, freigegebenen) und halb geordneten Format, und ich konnte wieder zu Spark zurückkehren.  Eine Überraschung erwartete mich: Ich habe wieder nicht das Gewünschte erreicht!  Es war sehr schwierig, Spark genau zu sagen, wie die Daten partitioniert wurden.  Und selbst als ich dies tat, stellte sich heraus, dass es zu viele Partitionen gab (95.000), und als ich ihre Anzahl mit <code>coalesce</code> auf kohärente Grenzen reduzierte, ruinierte dies meine Partitionierung.  Ich bin sicher, dass dies behoben werden kann, aber in ein paar Tagen der Suche konnte ich keine Lösung finden.  Am Ende habe ich alle Aufgaben in Spark erledigt, obwohl es einige Zeit gedauert hat, und meine geteilten Parkettdateien waren nicht sehr klein (~ 200 Kb).  Die Daten waren jedoch dort, wo sie benötigt wurden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ae5/43b/236/ae543b236b8d37d4a6794aa63d9ada94.png"><br>  <i>Zu klein und anders, wunderbar!</i> <br><br><h2>  Testen lokaler Spark-Anforderungen </h2><br>  <b>Was ich gelernt habe</b> : Spark hat zu viel Aufwand bei der Lösung einfacher Probleme. <br><br>  Durch das Herunterladen der Daten in einem intelligenten Format konnte ich die Geschwindigkeit testen.  Ich habe ein Skript auf R eingerichtet, um den lokalen Spark-Server zu starten, und dann den Spark-Datenrahmen aus dem angegebenen Repository der Parkettgruppen (bin) geladen.  Ich habe versucht, alle Daten zu laden, konnte Sparklyr jedoch nicht dazu bringen, die Partitionierung zu erkennen. <br><br><pre> <code class="scala hljs">sc &lt;- <span class="hljs-type"><span class="hljs-type">Spark_connect</span></span>(master = <span class="hljs-string"><span class="hljs-string">"local"</span></span>) desired_snp &lt;- <span class="hljs-symbol"><span class="hljs-symbol">'rs3477173</span></span>9' # <span class="hljs-type"><span class="hljs-type">Start</span></span> a timer start_time &lt;- <span class="hljs-type"><span class="hljs-type">Sys</span></span>.time() # <span class="hljs-type"><span class="hljs-type">Load</span></span> the desired bin into <span class="hljs-type"><span class="hljs-type">Spark</span></span> intensity_data &lt;- sc %&gt;% <span class="hljs-type"><span class="hljs-type">Spark_read_Parquet</span></span>( name = <span class="hljs-symbol"><span class="hljs-symbol">'intensity_dat</span></span>a', path = get_snp_location(desired_snp), memory = <span class="hljs-type"><span class="hljs-type">FALSE</span></span> ) # <span class="hljs-type"><span class="hljs-type">Subset</span></span> bin to snp and then collect to local test_subset &lt;- intensity_data %&gt;% filter(<span class="hljs-type"><span class="hljs-type">SNP_Name</span></span> == desired_snp) %&gt;% collect() print(<span class="hljs-type"><span class="hljs-type">Sys</span></span>.time() - start_time)</code> </pre> <br>  Die Ausführung dauerte 29.415 Sekunden.  Viel besser, aber nicht zu gut für Massentests.  Außerdem konnte ich die Arbeit mit dem Caching nicht beschleunigen, da Spark beim Versuch, den Datenrahmen im Speicher zwischenzuspeichern, immer abstürzte, selbst wenn ich mehr als 50 GB Speicher für ein Dataset mit einem Gewicht von weniger als 15 zugewiesen habe. <br><br><h2>  Kehre zu AWK zurück </h2><br>  <b>Was ich gelernt habe</b> : AWK assoziative Arrays sind sehr effizient. <br><br>  Ich verstand, dass ich eine höhere Geschwindigkeit erreichen konnte.  Ich erinnerte mich, dass ich in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bruce Barnetts</a> ausgezeichnetem AWK-Handbuch über eine coole Funktion namens " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Assoziative Arrays</a> " gelesen habe.  Tatsächlich handelt es sich hierbei um Schlüssel-Wert-Paare, die aus irgendeinem Grund in AWK anders bezeichnet wurden, und deshalb habe ich sie irgendwie nicht besonders erwähnt.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Roman Cheplyaka</a> erinnerte daran, dass der Begriff „assoziative Arrays“ viel älter ist als der Begriff „Schlüssel-Wert-Paar“.  Selbst wenn Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">in Google Ngram nach Schlüsselwerten suchen</a> , wird dieser Begriff dort nicht <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">angezeigt</a> , aber Sie finden assoziative Arrays!  Darüber hinaus wird das Schlüssel-Wert-Paar am häufigsten mit Datenbanken verknüpft, sodass ein Vergleich mit Hashmap viel logischer ist.  Ich erkannte, dass ich diese assoziativen Arrays verwenden konnte, um meine SNPs mit der Bin-Tabelle und den Rohdaten zu verbinden, ohne Spark zu verwenden. <br><br>  Dafür habe ich im AWK-Skript den <code>BEGIN</code> Block verwendet.  Dies ist ein Code, der ausgeführt wird, bevor die erste Datenzeile an den Hauptteil des Skripts übertragen wird. <br><br><pre> <code class="cpp hljs">join_data.awk BEGIN { FS=<span class="hljs-string"><span class="hljs-string">","</span></span>; batch_num=substr(chunk,<span class="hljs-number"><span class="hljs-number">7</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>); chunk_id=substr(chunk,<span class="hljs-number"><span class="hljs-number">15</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">while</span></span>(getline &lt; <span class="hljs-string"><span class="hljs-string">"snp_to_bin.csv"</span></span>) {bin[$<span class="hljs-number"><span class="hljs-number">1</span></span>] = $<span class="hljs-number"><span class="hljs-number">2</span></span>} } { print $<span class="hljs-number"><span class="hljs-number">0</span></span> &gt; <span class="hljs-string"><span class="hljs-string">"chunked/chr_"</span></span>chr<span class="hljs-string"><span class="hljs-string">"_bin_"</span></span>bin[$<span class="hljs-number"><span class="hljs-number">1</span></span>]<span class="hljs-string"><span class="hljs-string">"_"</span></span>batch_num<span class="hljs-string"><span class="hljs-string">"_"</span></span>chunk_id<span class="hljs-string"><span class="hljs-string">".csv"</span></span> }</code> </pre> <br>  Mit dem Befehl <code>while(getline...)</code> alle Zeilen aus der CSV-Gruppe (bin) geladen, die erste Spalte (SNP-Name) als Schlüssel für das assoziative Array <code>bin</code> und der zweite Wert (group) als Wert festgelegt.  Dann wird in dem Block <code>{</code> <code>}</code> , der auf alle Zeilen der Hauptdatei angewendet wird, jede Zeile an die Ausgabedatei gesendet, die abhängig von ihrer Gruppe (bin) einen eindeutigen Namen erhält: <code>..._bin_"bin[$1]"_...</code> <br><br>  Die <code>chunk_id</code> <code>batch_num</code> und <code>chunk_id</code> entsprachen den von der Pipeline bereitgestellten Daten, wodurch der Race-Status vermieden wurde, und jeder <code>parallel</code> gestartete Ausführungsthread schrieb in seine eigene eindeutige Datei. <br><br>  Da ich alle Rohdaten in Ordnern auf den Chromosomen verteilt habe, die nach meinem vorherigen Experiment mit AWK übrig geblieben waren, konnte ich jetzt ein weiteres Bash-Skript schreiben, um es gleichzeitig auf dem Chromosom zu verarbeiten und S3 tiefer partitionierte Daten zu geben. <br><br><pre> <code class="bash hljs">DESIRED_CHR=<span class="hljs-string"><span class="hljs-string">'13'</span></span> <span class="hljs-comment"><span class="hljs-comment"># Download chromosome data from s3 and split into bins aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv' | parallel "echo 'reading {}'; aws s3 cp "$DATA_LOC"{} - | awk -v chr=\""$DESIRED_CHR"\" -v chunk=\"{}\" -f split_on_chr_bin.awk" # Combine all the parallel process chunks to single files and upload to rds using R ls chunked/ | cut -d '_' -f 4 | sort -u | parallel "echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds" rm chunked/*</span></span></code> </pre> <br>  Das Skript besteht aus zwei <code>parallel</code> Abschnitten. <br><br>  Im ersten Abschnitt werden Daten aus allen Dateien gelesen, die Informationen zum gewünschten Chromosom enthalten. Anschließend werden diese Daten auf Streams verteilt, die Dateien in die entsprechenden Gruppen (bin) verteilen.  Um zu verhindern, dass Rennbedingungen auftreten, wenn mehrere Streams in eine einzelne Datei geschrieben werden, überträgt AWK die Dateinamen zum Schreiben von Daten an verschiedene Stellen, z. B. <code>chr_10_bin_52_batch_2_aa.csv</code> .  Infolgedessen werden viele kleine Dateien auf der Festplatte erstellt (dafür habe ich Terabyte-EBS-Volumes verwendet). <br><br>  Die Pipeline aus dem zweiten <code>parallel</code> Abschnitt durchläuft die Gruppen (bin) und kombiniert ihre einzelnen Dateien zu gemeinsamen CSVs mit <code>cat</code> und sendet sie dann zum Export. <br><br><h2>  Sendung an R? </h2><br>  <b>Was ich gelernt habe</b> : Sie können über ein R-Skript auf <code>stdin</code> und <code>stdout</code> zugreifen und es daher in der Pipeline verwenden. <br><br>  Im Bash-Skript stellen Sie möglicherweise folgende Zeile fest: <code>...cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R...</code>  <code>...cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R...</code>  Es übersetzt alle verketteten Gruppendateien (bin) in das unten stehende R-Skript.  <code>{}</code> ist eine spezielle <code>parallel</code> Technik, bei der alle von ihr gesendeten Daten in den angegebenen Stream direkt in den Befehl selbst eingefügt werden.  Die Option <code>{#}</code> bietet eine eindeutige Thread-ID, und <code>{%}</code> für die Job-Slot-Nummer (wiederholt, jedoch niemals gleichzeitig).  Eine Liste aller Optionen finden Sie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation.</a> <br><br><pre> <code class="lisp hljs"><span class="hljs-meta"><span class="hljs-meta">#!/usr/bin/env Rscript library(readr) library(aws.s3) # Read first command line argument data_destination &lt;- commandArgs(trailingOnly = TRUE)[1] data_cols &lt;- list(SNP_Name = 'c', ...) s3saveRDS( read_csv( file("stdin"), col_names = names(data_cols), col_types = data_cols ), object = data_destination )</span></span></code> </pre> <br>  Wenn die <code>readr::read_csv</code> <code>file("stdin")</code> an <code>readr::read_csv</code> , werden die in das R-Skript übersetzten Daten in den Frame geladen, der dann mit <code>aws.s3</code> als <code>.rds</code> Datei direkt in S3 <code>aws.s3</code> . <br><br>  RDS ist ein bisschen wie eine jüngere Version von Parkett, ohne den Schnickschnack der Säulenlagerung. <br><br>  Nach Abschluss des Bash-Skripts erhielt ich eine <code>.rds</code> Dateien in S3, mit denen ich effiziente Komprimierung und integrierte Typen verwenden konnte. <br><br>  Trotz der Verwendung der Bremse R funktionierte alles sehr schnell.  Es ist nicht überraschend, dass die Fragmente auf R, die für das Lesen und Schreiben von Daten verantwortlich sind, gut optimiert sind.  Nach dem Testen auf einem mittelgroßen Chromosom war die Aufgabe auf der C5n.4xl-Instanz in etwa zwei Stunden abgeschlossen. <br><br><h2>  S3 Einschränkungen </h2><br>  <b>Was ich gelernt habe</b> : Dank der intelligenten Implementierung von Pfaden kann S3 viele Dateien verarbeiten. <br><br>  Ich war besorgt, ob S3 viele darauf übertragene Dateien verarbeiten könnte.  Ich könnte die Dateinamen aussagekräftig machen, aber wie wird S3 danach suchen? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/841/0dc/c34/8410dcc34a563c683dd7602dc66d884a.png"><br> <i>  S3    ,        <code>/</code> . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> FAQ- S3.</a></i> <br><br> , S3            -      .  (bucket)   ,   —    . <br><br>          Amazon, ,    «-----»  .    :       get-,       . ,      20 . bin-. ,   ,      (,      ,      ).          . <br><br><h2>    ? </h2><br>   :     —     . <br><br>       : «    ?»      ( gzip CSV-   7  )      .     ,  R     Parquet ( Arrow)     Spark.       R,         ,         ,       . <br><br><h2>   </h2><br> <b>  </b> :     ,    . <br><br>       ,      . <br>     EC2  ,                 ( ,  Spark    ).  ,          ,    AWS-      10 . <br><br>      R      . <br><br>   S3 ,       . <br><br><pre> <code class="bash hljs">library(aws.s3) library(tidyverse) chr_sizes &lt;- get_bucket_df( bucket = <span class="hljs-string"><span class="hljs-string">'...'</span></span>, prefix = <span class="hljs-string"><span class="hljs-string">'...'</span></span>, max = Inf ) %&gt;% mutate(Size = as.numeric(Size)) %&gt;% filter(Size != 0) %&gt;% mutate( <span class="hljs-comment"><span class="hljs-comment"># Extract chromosome from the file name chr = str_extract(Key, 'chr.{1,4}\\.csv') %&gt;% str_remove_all('chr|\\.csv') ) %&gt;% group_by(chr) %&gt;% summarise(total_size = sum(Size)/1e+9) # Divide to get value in GB # A tibble: 27 x 2 chr total_size &lt;chr&gt; &lt;dbl&gt; 1 0 163. 2 1 967. 3 10 541. 4 11 611. 5 12 542. 6 13 364. 7 14 375. 8 15 372. 9 16 434. 10 17 443. # … with 17 more rows</span></span></code> </pre> <br>    ,    ,   ,     <code>num_jobs</code>  ,       . <br><br><pre> <code class="bash hljs">num_jobs &lt;- 7 <span class="hljs-comment"><span class="hljs-comment"># How big would each job be if perfectly split? job_size &lt;- sum(chr_sizes$total_size)/7 shuffle_job &lt;- function(i){ chr_sizes %&gt;% sample_frac() %&gt;% mutate( cum_size = cumsum(total_size), job_num = ceiling(cum_size/job_size) ) %&gt;% group_by(job_num) %&gt;% summarise( job_chrs = paste(chr, collapse = ','), total_job_size = sum(total_size) ) %&gt;% mutate(sd = sd(total_job_size)) %&gt;% nest(-sd) } shuffle_job(1) # A tibble: 1 x 2 sd data &lt;dbl&gt; &lt;list&gt; 1 153. &lt;tibble [7 × 3]&gt;</span></span></code> </pre> <br>      purrr     . <br><br><pre> <code class="bash hljs">1:1000 %&gt;% map_df(shuffle_job) %&gt;% filter(sd == min(sd)) %&gt;% pull(data) %&gt;% pluck(1)</code> </pre> <br>     ,    .       Bash-    <code>for</code> .       10 .    ,             .  ,        . <br><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> DESIRED_CHR <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-string"><span class="hljs-string">"16"</span></span> <span class="hljs-string"><span class="hljs-string">"9"</span></span> <span class="hljs-string"><span class="hljs-string">"7"</span></span> <span class="hljs-string"><span class="hljs-string">"21"</span></span> <span class="hljs-string"><span class="hljs-string">"MT"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-comment"><span class="hljs-comment"># Code for processing a single chromosome fi</span></span></code> </pre> <br>     : <br><br><pre> <code class="bash hljs">sudo shutdown -h now</code> </pre> <br> …   !   AWS CLI       <code>user_data</code>   Bash-    .     ,         . <br><br><pre> <code class="bash hljs">aws ec2 run-instances ...\ --tag-specifications <span class="hljs-string"><span class="hljs-string">"ResourceType=instance,Tags=[{Key=Name,Value=&lt;&lt;job_name&gt;&gt;}]"</span></span> \ --user-data file://&lt;&lt;job_script_loc&gt;&gt;</code> </pre> <br><h1> ! </h1><br> <b>  </b> : API        . <br><br> -        .      ,     .     API   .        <code>.rds</code>  Parquet-,       ,    .       R-. <br><br>      ,        ,    <code>get_snp</code> .       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">pkgdown</a> ,        . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a75/afb/f3a/a75afbf3a2c7c8ef5fa2a873f8ba50b9.png"><br><br><h2>   </h2><br> <b>  </b> :     ,   ! <br><br>          SNP      ,     (binning)   .     SNP,          (bin).      ( )    . <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Part of get_snp() ... # Test if our current snp data has the desired snp. already_have_snp &lt;- desired_snp %in% prev_snp_results$snps_in_bin if(!already_have_snp){ # Grab info on the bin of the desired snp snp_results &lt;- get_snp_bin(desired_snp) # Download the snp's bin data snp_results$bin_data &lt;- aws.s3::s3readRDS(object = snp_results$data_loc) } else { # The previous snp data contained the right bin so just use it snp_results &lt;- prev_snp_results } ...</span></span></code> </pre> <br>       ,       .    ,      . , <code>dplyr::filter</code>           ,           ,    . <br><br>  ,   <code>prev_snp_results</code>   <code>snps_in_bin</code> .     SNP   (bin),   ,       .        SNP   (bin)    : <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Get bin-mates snps_in_bin &lt;- my_snp_results$snps_in_bin for(current_snp in snps_in_bin){ my_snp_results &lt;- get_snp(current_snp, my_snp_results) # Do something with results }</span></span></code> </pre> <br><h1>  Ergebnisse </h1><br>    (  )    ,   .  ,           .      . <br><br>       ,       ,     ,     … <br><br>   .       .       (  ),  ,   (bin)   ,    SNP     0,1 ,     ,     S3 . <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-3" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1134151057385369600"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br><h2>  Fazit </h2><br>   —   .   ,     . ,    . ,   ,         ,     .  ,       ,  ,        ,    .  ,       ,    ,        ,      -     . <br><br>     .     ,        ,  «»  ,    .          . <br><br><h3>   : </h3><br><ul><li>      25   ; <br></li><li>      Parquet-   ; <br></li><li>   Spark   ; <br></li><li>      2,5  ; <br></li><li>    ,    Spark; <br></li><li>      ; <br></li><li>   Spark  ,      ; <br></li><li>  ,    ,  -       1980-; <br></li><li> <code>gnu parallel</code> —   ,    ; <br></li><li> Spark        ; <br></li><li>  Spark        ; <br></li><li>    AWK  ; <br></li><li>    <code>stdin</code>  <code>stdout</code>  R-,       ; <br></li><li>     S3    ; <br></li><li>     —     ; <br></li><li>     ,    ; <br></li><li> API        ; <br></li><li>     ,   ! <br></li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de456392/">https://habr.com/ru/post/de456392/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de456376/index.html">Not One Spring Boot: Ein Überblick über Alternativen</a></li>
<li><a href="../de456380/index.html">Tag der offenen Tür der Fakultät für Programmierung in der Netologie</a></li>
<li><a href="../de456382/index.html">Zusammenarbeit und Automatisierung im Frontend. Was wir aus 13 Schulen gelernt haben</a></li>
<li><a href="../de456386/index.html">Öffnen Sie Bibliotheken zur Visualisierung von Audioinhalten</a></li>
<li><a href="../de456388/index.html">Diagnoseentwicklungsdiagramm in PVS-Studio</a></li>
<li><a href="../de456394/index.html">Erstellen des allgegenwärtigen Begrüßungsbildschirms unter iOS</a></li>
<li><a href="../de456398/index.html">Vue-Cli-Plugins, die mit komplexen Daten und eigenschaftsbasierten Tests arbeiten - Ankündigung des Panda-Meetup-Frontends</a></li>
<li><a href="../de456400/index.html">Warum Wettkämpfe besser sind als Pauken: Unsere Erfahrung mit Gamification</a></li>
<li><a href="../de456402/index.html">Zähne der Weisheit: Pull-Pull</a></li>
<li><a href="../de456404/index.html">Looper - Plugin für Sketch</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>