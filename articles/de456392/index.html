<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üÖ±Ô∏è üóª üë®üèª Parsim 25 TB mit AWK und R. ‚ò¶Ô∏è üèº üë®üèΩ‚ÄçüöÄ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wie man diesen Artikel liest : Ich entschuldige mich daf√ºr, dass der Text so lang und chaotisch geworden ist. Um Ihnen Zeit zu sparen, beginne ich jed...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Parsim 25 TB mit AWK und R.</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/456392/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/9d/3y/lc/9d3ylcjuqiv6r7vrv6p52apvmne.jpeg"></div><br>  <i><b>Wie man diesen Artikel liest</b> : Ich entschuldige mich daf√ºr, dass der Text so lang und chaotisch geworden ist.</i>  <i>Um Ihnen Zeit zu sparen, beginne ich jedes Kapitel mit der Einf√ºhrung von ‚ÄûWas ich gelernt habe‚Äú, in der ich die Essenz des Kapitels in ein oder zwei S√§tzen erkl√§re.</i> <i><br><br></i>  <i><b>"Zeigen Sie einfach die L√∂sung!"</b></i>  <i>Wenn Sie nur sehen m√∂chten, wozu ich gekommen bin, lesen Sie das Kapitel "Erfinderischer werden". Ich finde es jedoch interessanter und n√ºtzlicher, √ºber Fehler zu lesen.</i> <br><br>  K√ºrzlich wurde ich angewiesen, ein Verfahren zur Verarbeitung eines gro√üen Volumens der urspr√ºnglichen DNA-Sequenzen einzurichten (technisch gesehen ist dies ein SNP-Chip).  Es war notwendig, schnell Daten √ºber einen bestimmten genetischen Ort (SNP genannt) f√ºr die nachfolgende Modellierung und andere Aufgaben zu erhalten.  Mit Hilfe von R und AWK konnte ich die Daten auf nat√ºrliche Weise bereinigen und organisieren und so die Bearbeitung von Anfragen erheblich beschleunigen.  Dies war f√ºr mich nicht einfach und erforderte zahlreiche Iterationen.  Dieser Artikel wird Ihnen helfen, einige meiner Fehler zu vermeiden und zu demonstrieren, was ich am Ende getan habe. <br><a name="habracut"></a><br>  Zun√§chst einige einleitende Erkl√§rungen. <br><br><h2>  Daten </h2><br>  Unser Genetic Information Processing Center der Universit√§t hat uns 25 TB TSV-Daten zur Verf√ºgung gestellt.  Ich habe sie in 5 von Gzip komprimierte Pakete aufgeteilt, von denen jedes ungef√§hr 240 Vier-Gigabyte-Dateien enthielt.  Jede Zeile enthielt Daten f√ºr einen SNP einer Person.  Insgesamt wurden Daten zu ~ 2,5 Millionen SNPs und ~ 60.000 Menschen √ºbertragen.  Zus√§tzlich zu den SNP-Informationen gab es in den Dateien zahlreiche Spalten mit Zahlen, die verschiedene Merkmale wie Leseintensit√§t, H√§ufigkeit verschiedener Allele usw. widerspiegeln.  Es gab ungef√§hr 30 Spalten mit eindeutigen Werten. <br><br><h4>  Zweck </h4><br>  Wie bei jedem Datenverwaltungsprojekt war es am wichtigsten, zu bestimmen, wie die Daten verwendet werden sollen.  In diesem Fall werden <b>wir zum gr√∂√üten Teil Modelle und Workflows f√ºr SNP basierend auf SNP ausw√§hlen</b> .  Das hei√üt, gleichzeitig ben√∂tigen wir Daten f√ºr nur einen SNP.  Ich musste lernen, wie man alle Datens√§tze, die sich auf einen der 2,5 Millionen SNPs beziehen, so einfach wie m√∂glich, schneller und billiger extrahiert. <br><br><h1>  Wie man es nicht macht </h1><br>  Ich werde ein passendes Klischee zitieren: <br><br><blockquote>  Ich habe nicht tausendmal versagt, sondern nur tausend M√∂glichkeiten entdeckt, eine Reihe von Daten nicht in einem f√ºr Abfragen geeigneten Format zu analysieren. </blockquote><br>
<h2>  Erster Versuch </h2><br>  <b>Was ich gelernt habe</b> : Es gibt keine billige M√∂glichkeit, 25 TB gleichzeitig zu analysieren. <br><br>  Nachdem ich mir an der Vanderbilt University das Thema ‚ÄûAdvanced Big Data Processing Methods‚Äú angeh√∂rt hatte, war ich mir sicher, dass es ein Hut war.  M√∂glicherweise dauert es ein oder zwei Stunden, um den Hive-Server so zu konfigurieren, dass er alle Daten durchl√§uft und √ºber das Ergebnis berichtet.  Da unsere Daten in AWS S3 gespeichert sind, habe ich den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Athena-</a> Dienst verwendet, mit dem Sie Hive SQL-Abfragen auf S3-Daten anwenden k√∂nnen.  Sie m√ºssen den Hive-Cluster nicht konfigurieren / erh√∂hen und m√ºssen nur f√ºr die gesuchten Daten bezahlen. <br><br>  Nachdem ich Athena meine Daten und ihr Format gezeigt hatte, f√ºhrte ich einige Tests mit √§hnlichen Abfragen durch: <br><br><pre><code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">select</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> intensityData <span class="hljs-keyword"><span class="hljs-keyword">limit</span></span> <span class="hljs-number"><span class="hljs-number">10</span></span>;</code> </pre> <br>  Und schnell gut strukturierte Ergebnisse erhalten.  Fertig. <br><br>  Bis wir versuchten, die Daten in der Arbeit zu verwenden ... <br><br>  Ich wurde gebeten, alle SNP-Informationen abzurufen, um das Modell darauf zu testen.  Ich habe eine Abfrage ausgef√ºhrt: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">select</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> intensityData <span class="hljs-keyword"><span class="hljs-keyword">where</span></span> snp = <span class="hljs-string"><span class="hljs-string">'rs123456'</span></span>;</code> </pre> <br>  ... und wartete.  Nach acht Minuten und mehr als 4 TB der angeforderten Daten erhielt ich das Ergebnis.  Athena berechnet eine Geb√ºhr f√ºr die gefundene Datenmenge von 5 USD pro Terabyte.  Diese einzelne Anfrage kostete also 20 US-Dollar und acht Minuten Wartezeit.  Um das Modell nach allen Daten laufen zu lassen, musste man 38 Jahre warten und 50 Millionen Dollar zahlen. Offensichtlich passte dies nicht zu uns. <br><br><h2>  Es war notwendig, Parkett zu verwenden ... </h2><br>  <b>Was ich gelernt habe</b> : Seien Sie vorsichtig mit der Gr√∂√üe Ihrer Parkettdateien und ihrer Organisation. <br><br>  Zuerst habe ich versucht, die Situation zu korrigieren, indem ich alle TSVs in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Parkettdateien konvertiert habe</a> .  Sie eignen sich f√ºr die Arbeit mit gro√üen Datenmengen, da die darin enthaltenen Informationen in Spaltenform gespeichert sind: Im Gegensatz zu Textdateien, in denen Zeilen Elemente jeder Spalte enthalten, befindet sich jede Spalte in einem eigenen Speicher- / Festplattensegment.  Und wenn Sie etwas finden m√ºssen, lesen Sie einfach die erforderliche Spalte.  Dar√ºber hinaus wird in jeder Datei in einer Spalte ein Wertebereich gespeichert. Wenn der gew√ºnschte Wert nicht im Spaltenbereich liegt, verschwendet Spark keine Zeit mit dem Scannen der gesamten Datei. <br><br>  Ich habe eine einfache <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AWS Glue-</a> Aufgabe ausgef√ºhrt, um unsere TSVs in Parkett umzuwandeln, und neue Dateien in Athena abgelegt.  Es dauerte ungef√§hr 5 Stunden.  Aber als ich die Anfrage startete, dauerte es ungef√§hr die gleiche Zeit und etwas weniger Geld, um sie abzuschlie√üen.  Tatsache ist, dass Spark beim Versuch, die Aufgabe zu optimieren, einfach einen TSV-Block entpackt und in einen eigenen Parkettblock gelegt hat.  Und da jeder Block gro√ü genug war und die vollst√§ndigen Aufzeichnungen vieler Personen enthielt, wurden alle SNPs in jeder Datei gespeichert, sodass Spark alle Dateien √∂ffnen musste, um die erforderlichen Informationen zu extrahieren. <br><br>  Seltsamerweise ist der Standardkomprimierungstyp (und der empfohlene Komprimierungstyp) in Parkett - bissig - nicht aufteilbar.  Daher blieb jeder Executor bei der Aufgabe, den gesamten 3,5-GB-Datensatz zu entpacken und herunterzuladen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f42/584/fb3/f42584fb3e65319eef46f117c11525f3.png"><br><h2>  Wir verstehen das Problem </h2><br>  <b>Was ich gelernt habe</b> : Das Sortieren ist schwierig, besonders wenn die Daten verteilt sind. <br><br>  Es schien mir, dass ich jetzt die Essenz des Problems verstand.  Ich musste die Daten nur nach SNP-Spalten sortieren, nicht nach Personen.  Dann werden mehrere SNPs in einem separaten Datenblock gespeichert, und dann manifestiert sich die intelligente Parkettfunktion ‚ÄûNur √∂ffnen, wenn der Wert im Bereich liegt‚Äú in ihrer ganzen Pracht.  Leider hat sich das Aussortieren von Milliarden von Zeilen, die √ºber einen Cluster verteilt sind, als entmutigende Aufgabe erwiesen. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-0" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1105127759318319105"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  AWS m√∂chte das Geld sicherlich nicht zur√ºckgeben, weil "ich ein zerstreuter Student bin".  Nachdem ich mit dem Sortieren auf Amazon Glue begonnen hatte, funktionierte es 2 Tage lang und st√ºrzte ab. <br><br><h2>  Was ist mit Partitionierung? </h2><br>  <b>Was ich gelernt habe</b> : Partitionen in Spark sollten ausgeglichen sein. <br><br>  Dann kam mir die Idee, die Daten auf den Chromosomen zu partitionieren.  Es gibt 23 von ihnen (und einige weitere angesichts mitochondrialer DNA und nicht kartierter Bereiche). <br>  Auf diese Weise k√∂nnen Sie die Daten in kleinere Teile aufteilen.  Wenn Sie der Spark-Exportfunktion im Glue-Skript nur eine Zeile <code>partition_by = "chr"</code> hinzuf√ºgen, sollten die Daten in Buckets sortiert werden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/652/f42/3dc/652f423dc8806401b6638a3cf8c1480b.png"><br>  <i>Das Genom besteht aus zahlreichen Fragmenten, die als Chromosomen bezeichnet werden.</i> <br><br>  Dies hat leider nicht funktioniert.  Chromosomen haben unterschiedliche Gr√∂√üen und daher unterschiedliche Informationsmengen.  Dies bedeutet, dass die Aufgaben, die Spark an die Mitarbeiter gesendet hat, nicht ausgeglichen und langsam ausgef√ºhrt wurden, da einige Knoten fr√ºher beendet wurden und inaktiv waren.  Die Aufgaben wurden jedoch abgeschlossen.  Bei der Anforderung eines SNP verursachte das Ungleichgewicht jedoch erneut Probleme.  Die Kosten f√ºr die Verarbeitung von SNPs auf gr√∂√üeren Chromosomen (dh woher wir die Daten beziehen m√∂chten) haben sich nur um das Zehnfache verringert.  Viel, aber nicht genug. <br><br><h2>  Und wenn Sie sich in noch kleinere Partitionen aufteilen? </h2><br>  <b>Was ich gelernt habe</b> : Versuchen Sie niemals, 2,5 Millionen Partitionen zu erstellen. <br><br>  Ich entschied mich f√ºr einen Spaziergang und teilte jeden SNP auf.  Dies garantierte die gleiche Gr√∂√üe von Partitionen.  <b>SCHLECHT WAR EINE IDEE</b> .  Ich nutzte Glue und f√ºgte die unschuldige <code>partition_by = 'snp'</code> .  Die Aufgabe wurde gestartet und ausgef√ºhrt.  Einen Tag sp√§ter √ºberpr√ºfte ich, ob in S3 bisher nichts geschrieben war, und beendete die Aufgabe.  Es sieht so aus, als h√§tte Glue Zwischendateien an einen versteckten Ort in S3 geschrieben, und viele Dateien, vielleicht ein paar Millionen.  Infolgedessen kostete mein Fehler mehr als tausend Dollar und gefiel meinem Mentor nicht. <br><br><h2>  Partitionieren + Sortieren </h2><br>  <b>Was ich gelernt habe</b> : Das Sortieren ist immer noch schwierig, ebenso wie das Einrichten von Spark. <br><br>  Der letzte Versuch der Partitionierung war, dass ich die Chromosomen partitionierte und dann jede Partition sortierte.  Theoretisch w√ºrde dies jede Anforderung beschleunigen, da die gew√ºnschten SNP-Daten innerhalb mehrerer Parkettbl√∂cke innerhalb eines bestimmten Bereichs liegen sollten.  Leider hat sich das Sortieren selbst partitionierter Daten als schwierige Aufgabe erwiesen.  Infolgedessen wechselte ich f√ºr einen benutzerdefinierten Cluster zu EMR und verwendete acht leistungsstarke Instanzen (C5.4xl) und Sparklyr, um einen flexibleren Workflow zu erstellen ... <br><br><pre> <code class="scala hljs"># <span class="hljs-type"><span class="hljs-type">Sparklyr</span></span> snippet to partition by chr and sort w/in partition # <span class="hljs-type"><span class="hljs-type">Join</span></span> the raw data <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> the snp bins raw_data group_by(chr) %&gt;% arrange(<span class="hljs-type"><span class="hljs-type">Position</span></span>) %&gt;% <span class="hljs-type"><span class="hljs-type">Spark_write_Parquet</span></span>( path = <span class="hljs-type"><span class="hljs-type">DUMP_LOC</span></span>, mode = <span class="hljs-symbol"><span class="hljs-symbol">'overwrit</span></span>e', partition_by = c(<span class="hljs-symbol"><span class="hljs-symbol">'ch</span></span>r') )</code> </pre> <br>  ... die Aufgabe wurde jedoch noch nicht erledigt.  Ich habe in jeder Hinsicht optimiert: Ich habe die Speicherzuordnung f√ºr jeden Abfrage-Executor erh√∂ht, Knoten mit einer gro√üen Speichermenge verwendet, Broadcast-Variablen verwendet, aber jedes Mal stellte sich heraus, dass es sich um halbe Sachen handelte, und nach und nach versagten die Darsteller, bis alles aufh√∂rte. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-1" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1128703858610450434"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br><h1>  Ich werde erfinderischer </h1><br>  <b>Was ich gelernt habe</b> : Manchmal erfordern spezielle Daten spezielle L√∂sungen. <br><br>  Jeder SNP hat einen Positionswert.  Dies ist die Anzahl, die der Anzahl der Basen entspricht, die entlang des Chromosoms liegen.  Dies ist eine gute und nat√ºrliche Art, unsere Daten zu organisieren.  Zuerst wollte ich jedes Chromosom nach Regionen aufteilen.  Zum Beispiel die Positionen 1 - 2000, 2001 - 4000 usw.  Das Problem ist jedoch, dass SNPs nicht gleichm√§√üig √ºber die Chromosomen verteilt sind, weshalb die Gr√∂√üe der Gruppen stark variiert. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f46/a8e/17b/f46a8e17b9af8d2ae9777c47017764c6.png"><br><br>  Infolgedessen wurde ich in Kategorien (Rang) Positionen eingeteilt.  Gem√§√ü den bereits heruntergeladenen Daten habe ich eine Liste mit eindeutigen SNPs, deren Positionen und Chromosomen angefordert.  Dann sortierte er die Daten in jedem Chromosom und sammelte SNP in Gruppen (bin) einer bestimmten Gr√∂√üe.  Sagen Sie jeweils 1000 SNP.  Dies gab mir eine SNP-Beziehung zu einer Gruppe im Chromosom. <br><br>  Am Ende habe ich Gruppen (bin) auf 75 SNP erstellt, ich werde den Grund unten erkl√§ren. <br><br><pre> <code class="bash hljs">snp_to_bin &lt;- unique_snps %&gt;% group_by(chr) %&gt;% arrange(position) %&gt;% mutate( rank = 1:n() bin = floor(rank/snps_per_bin) ) %&gt;% ungroup()</code> </pre> <br><h2>  Versuchen Sie es zuerst mit Spark </h2><br>  <b>Was ich gelernt habe</b> : Die Spark-Integration ist schnell, aber die Partitionierung ist immer noch teuer. <br><br>  Ich wollte diesen kleinen Datenrahmen (2,5 Millionen Zeilen) in Spark lesen, ihn mit Rohdaten kombinieren und dann durch die neu hinzugef√ºgte <code>bin</code> Spalte partitionieren. <br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment"># Join the raw data with the snp bins data_w_bin &lt;- raw_data %&gt;% left_join(sdf_broadcast(snp_to_bin), by ='snp_name') %&gt;% group_by(chr_bin) %&gt;% arrange(Position) %&gt;% Spark_write_Parquet( path = DUMP_LOC, mode = 'overwrite', partition_by = c('chr_bin') )</span></span></code> </pre> <br>  Ich habe <code>sdf_broadcast()</code> , damit Spark herausfindet, dass ein <code>sdf_broadcast()</code> an alle Knoten <code>sdf_broadcast()</code> werden soll.  Dies ist n√ºtzlich, wenn die Daten klein sind und f√ºr alle Aufgaben ben√∂tigt werden.  Andernfalls versucht Spark, intelligent zu sein und Daten nach Bedarf zu verteilen, was zu Bremsen f√ºhren kann. <br><br>  Und wieder funktionierte meine Idee nicht: Die Aufgaben funktionierten eine Weile, schlossen die Fusion ab und begannen dann, wie die durch Partitionierung gestarteten Executoren, zu scheitern. <br><br><h2>  AWK hinzuf√ºgen </h2><br>  <b>Was ich gelernt habe</b> : Schlafen Sie nicht, wenn die Grundlagen es Ihnen beibringen.  Sicherlich hat jemand Ihr Problem bereits in den 1980er Jahren gel√∂st. <br><br>  Bis zu diesem Zeitpunkt war die Ursache all meiner Fehler mit Spark die Verwirrung der Daten im Cluster.  Vielleicht kann die Situation durch Vorverarbeitung verbessert werden.  Ich beschloss, die Rohtextdaten in Chromosomenspalten aufzuteilen, und hoffte, Spark mit ‚Äûvorpartitionierten‚Äú Daten versorgen zu k√∂nnen. <br><br>  Ich habe in StackOverflow nach M√∂glichkeiten zum Aufteilen von Spaltenwerten gesucht und eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">so gute Antwort gefunden.</a>  Mit AWK k√∂nnen Sie eine Textdatei in Spaltenwerte aufteilen, indem Sie in das Skript schreiben, anstatt die Ergebnisse an <code>stdout</code> senden. <br><br>  Zum Testen habe ich ein Bash-Skript geschrieben.  Ich habe einen der gepackten TSVs heruntergeladen, ihn dann mit <code>gzip</code> entpackt und an <code>awk</code> gesendet. <br><br><pre> <code class="bash hljs">gzip -dc path/to/chunk/file.gz | awk -F <span class="hljs-string"><span class="hljs-string">'\t'</span></span> \ <span class="hljs-string"><span class="hljs-string">'{print $1",..."$30"&gt;"chunked/"$chr"_chr"$15".csv"}'</span></span></code> </pre> <br>  Es hat funktioniert! <br><br><h2>  Kernf√ºllung </h2><br>  <b>Was ich gelernt habe</b> : <code>gnu parallel</code> ist eine magische Sache, jeder sollte sie benutzen. <br><br>  Die Trennung war ziemlich langsam, und als ich <code>htop</code> startete, um die Verwendung einer leistungsstarken (und teuren) EC2-Instanz zu testen, stellte sich heraus, dass ich nur einen Kern und ungef√§hr 200 MB Speicher verwendete.  Um das Problem zu l√∂sen und nicht viel Geld zu verlieren, musste herausgefunden werden, wie die Arbeit parallelisiert werden kann.  Gl√ºcklicherweise fand ich in Jeron Janssens 'beeindruckendem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Data Science at the Command Line-</a> Buch ein Kapitel √ºber Parallelisierung.  Daraus lernte ich <code>gnu parallel</code> , eine sehr flexible Methode zur Implementierung von Multithreading unter Unix. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/835/7c0/e45/8357c0e45f4162d53ca1c3da0c78444a.png" width="300"></div><br>  Als ich die Partition mit einem neuen Prozess startete, war alles in Ordnung, aber es gab einen Engpass - das Herunterladen von S3-Objekten auf die Festplatte war nicht zu schnell und nicht vollst√§ndig parallelisiert.  Um dies zu beheben, habe ich Folgendes getan: <br><br><ol><li>  Ich fand heraus, dass es m√∂glich ist, den S3-Download-Schritt direkt in der Pipeline zu implementieren, wodurch der Zwischenspeicher auf der Festplatte vollst√§ndig entf√§llt.  Dies bedeutet, dass ich das Schreiben von Rohdaten auf die Festplatte vermeiden und noch kleineren und daher g√ºnstigeren Speicher auf AWS verwenden kann. <br></li><li>  Der Befehlssatz <code>aws configure set default.s3.max_concurrent_requests 50</code> hat die Anzahl der von der AWS CLI verwendeten Threads erheblich erh√∂ht (standardm√§√üig sind es 10). <br></li><li>  Ich wechselte zu der f√ºr die Netzwerkgeschwindigkeit optimierten EC2-Instanz mit dem Buchstaben n im Namen.  Ich fand heraus, dass der Verlust an Rechenleistung bei Verwendung von n-Instanzen durch eine Erh√∂hung der Download-Geschwindigkeit mehr als ausgeglichen wird.  F√ºr die meisten Aufgaben habe ich c5n.4xl verwendet. <br></li><li>  Ich habe <code>gzip</code> in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><code>pigz</code></a> ge√§ndert. Dies ist ein gzip-Tool, das coole Dinge tun kann, um die anfangs beispiellose Aufgabe des Entpackens von Dateien zu parallelisieren (dies hat am wenigsten geholfen). <br></li></ol><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Let S3 use as many threads as it wants aws configure set default.s3.max_concurrent_requests 50 for chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do aws s3 cp s3://$batch_loc$chunk_file - | pigz -dc | parallel --block 100M --pipe \ "awk -F '\t' '{print \$1\",...\"$30\"&gt;\"chunked/{#}_chr\"\$15\".csv\"}'" # Combine all the parallel process chunks to single files ls chunked/ | cut -d '_' -f 2 | sort -u | parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}' # Clean up intermediate data rm chunked/* done</span></span></code> </pre> <br>  Diese Schritte werden miteinander kombiniert, so dass alles sehr schnell funktioniert.  Dank der erh√∂hten Download-Geschwindigkeit und der Ablehnung des Schreibens auf die Festplatte konnte ich jetzt ein 5-Terabyte-Paket in nur wenigen Stunden verarbeiten. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-2" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1129416944233226240"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  Dieser Tweet sollte "TSV" erw√§hnen.  Leider. <br><br><h2>  Neu analysierte Daten verwenden </h2><br>  <b>Was ich gelernt habe</b> : Spark liebt unkomprimierte Daten und kombiniert keine Partitionen. <br><br>  Jetzt waren die Daten in S3 in einem entpackten (gelesenen, freigegebenen) und halb geordneten Format, und ich konnte wieder zu Spark zur√ºckkehren.  Eine √úberraschung erwartete mich: Ich habe wieder nicht das Gew√ºnschte erreicht!  Es war sehr schwierig, Spark genau zu sagen, wie die Daten partitioniert wurden.  Und selbst als ich dies tat, stellte sich heraus, dass es zu viele Partitionen gab (95.000), und als ich ihre Anzahl mit <code>coalesce</code> auf koh√§rente Grenzen reduzierte, ruinierte dies meine Partitionierung.  Ich bin sicher, dass dies behoben werden kann, aber in ein paar Tagen der Suche konnte ich keine L√∂sung finden.  Am Ende habe ich alle Aufgaben in Spark erledigt, obwohl es einige Zeit gedauert hat, und meine geteilten Parkettdateien waren nicht sehr klein (~ 200 Kb).  Die Daten waren jedoch dort, wo sie ben√∂tigt wurden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ae5/43b/236/ae543b236b8d37d4a6794aa63d9ada94.png"><br>  <i>Zu klein und anders, wunderbar!</i> <br><br><h2>  Testen lokaler Spark-Anforderungen </h2><br>  <b>Was ich gelernt habe</b> : Spark hat zu viel Aufwand bei der L√∂sung einfacher Probleme. <br><br>  Durch das Herunterladen der Daten in einem intelligenten Format konnte ich die Geschwindigkeit testen.  Ich habe ein Skript auf R eingerichtet, um den lokalen Spark-Server zu starten, und dann den Spark-Datenrahmen aus dem angegebenen Repository der Parkettgruppen (bin) geladen.  Ich habe versucht, alle Daten zu laden, konnte Sparklyr jedoch nicht dazu bringen, die Partitionierung zu erkennen. <br><br><pre> <code class="scala hljs">sc &lt;- <span class="hljs-type"><span class="hljs-type">Spark_connect</span></span>(master = <span class="hljs-string"><span class="hljs-string">"local"</span></span>) desired_snp &lt;- <span class="hljs-symbol"><span class="hljs-symbol">'rs3477173</span></span>9' # <span class="hljs-type"><span class="hljs-type">Start</span></span> a timer start_time &lt;- <span class="hljs-type"><span class="hljs-type">Sys</span></span>.time() # <span class="hljs-type"><span class="hljs-type">Load</span></span> the desired bin into <span class="hljs-type"><span class="hljs-type">Spark</span></span> intensity_data &lt;- sc %&gt;% <span class="hljs-type"><span class="hljs-type">Spark_read_Parquet</span></span>( name = <span class="hljs-symbol"><span class="hljs-symbol">'intensity_dat</span></span>a', path = get_snp_location(desired_snp), memory = <span class="hljs-type"><span class="hljs-type">FALSE</span></span> ) # <span class="hljs-type"><span class="hljs-type">Subset</span></span> bin to snp and then collect to local test_subset &lt;- intensity_data %&gt;% filter(<span class="hljs-type"><span class="hljs-type">SNP_Name</span></span> == desired_snp) %&gt;% collect() print(<span class="hljs-type"><span class="hljs-type">Sys</span></span>.time() - start_time)</code> </pre> <br>  Die Ausf√ºhrung dauerte 29.415 Sekunden.  Viel besser, aber nicht zu gut f√ºr Massentests.  Au√üerdem konnte ich die Arbeit mit dem Caching nicht beschleunigen, da Spark beim Versuch, den Datenrahmen im Speicher zwischenzuspeichern, immer abst√ºrzte, selbst wenn ich mehr als 50 GB Speicher f√ºr ein Dataset mit einem Gewicht von weniger als 15 zugewiesen habe. <br><br><h2>  Kehre zu AWK zur√ºck </h2><br>  <b>Was ich gelernt habe</b> : AWK assoziative Arrays sind sehr effizient. <br><br>  Ich verstand, dass ich eine h√∂here Geschwindigkeit erreichen konnte.  Ich erinnerte mich, dass ich in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bruce Barnetts</a> ausgezeichnetem AWK-Handbuch √ºber eine coole Funktion namens " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Assoziative Arrays</a> " gelesen habe.  Tats√§chlich handelt es sich hierbei um Schl√ºssel-Wert-Paare, die aus irgendeinem Grund in AWK anders bezeichnet wurden, und deshalb habe ich sie irgendwie nicht besonders erw√§hnt.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Roman Cheplyaka</a> erinnerte daran, dass der Begriff ‚Äûassoziative Arrays‚Äú viel √§lter ist als der Begriff ‚ÄûSchl√ºssel-Wert-Paar‚Äú.  Selbst wenn Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">in Google Ngram nach Schl√ºsselwerten suchen</a> , wird dieser Begriff dort nicht <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">angezeigt</a> , aber Sie finden assoziative Arrays!  Dar√ºber hinaus wird das Schl√ºssel-Wert-Paar am h√§ufigsten mit Datenbanken verkn√ºpft, sodass ein Vergleich mit Hashmap viel logischer ist.  Ich erkannte, dass ich diese assoziativen Arrays verwenden konnte, um meine SNPs mit der Bin-Tabelle und den Rohdaten zu verbinden, ohne Spark zu verwenden. <br><br>  Daf√ºr habe ich im AWK-Skript den <code>BEGIN</code> Block verwendet.  Dies ist ein Code, der ausgef√ºhrt wird, bevor die erste Datenzeile an den Hauptteil des Skripts √ºbertragen wird. <br><br><pre> <code class="cpp hljs">join_data.awk BEGIN { FS=<span class="hljs-string"><span class="hljs-string">","</span></span>; batch_num=substr(chunk,<span class="hljs-number"><span class="hljs-number">7</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>); chunk_id=substr(chunk,<span class="hljs-number"><span class="hljs-number">15</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">while</span></span>(getline &lt; <span class="hljs-string"><span class="hljs-string">"snp_to_bin.csv"</span></span>) {bin[$<span class="hljs-number"><span class="hljs-number">1</span></span>] = $<span class="hljs-number"><span class="hljs-number">2</span></span>} } { print $<span class="hljs-number"><span class="hljs-number">0</span></span> &gt; <span class="hljs-string"><span class="hljs-string">"chunked/chr_"</span></span>chr<span class="hljs-string"><span class="hljs-string">"_bin_"</span></span>bin[$<span class="hljs-number"><span class="hljs-number">1</span></span>]<span class="hljs-string"><span class="hljs-string">"_"</span></span>batch_num<span class="hljs-string"><span class="hljs-string">"_"</span></span>chunk_id<span class="hljs-string"><span class="hljs-string">".csv"</span></span> }</code> </pre> <br>  Mit dem Befehl <code>while(getline...)</code> alle Zeilen aus der CSV-Gruppe (bin) geladen, die erste Spalte (SNP-Name) als Schl√ºssel f√ºr das assoziative Array <code>bin</code> und der zweite Wert (group) als Wert festgelegt.  Dann wird in dem Block <code>{</code> <code>}</code> , der auf alle Zeilen der Hauptdatei angewendet wird, jede Zeile an die Ausgabedatei gesendet, die abh√§ngig von ihrer Gruppe (bin) einen eindeutigen Namen erh√§lt: <code>..._bin_"bin[$1]"_...</code> <br><br>  Die <code>chunk_id</code> <code>batch_num</code> und <code>chunk_id</code> entsprachen den von der Pipeline bereitgestellten Daten, wodurch der Race-Status vermieden wurde, und jeder <code>parallel</code> gestartete Ausf√ºhrungsthread schrieb in seine eigene eindeutige Datei. <br><br>  Da ich alle Rohdaten in Ordnern auf den Chromosomen verteilt habe, die nach meinem vorherigen Experiment mit AWK √ºbrig geblieben waren, konnte ich jetzt ein weiteres Bash-Skript schreiben, um es gleichzeitig auf dem Chromosom zu verarbeiten und S3 tiefer partitionierte Daten zu geben. <br><br><pre> <code class="bash hljs">DESIRED_CHR=<span class="hljs-string"><span class="hljs-string">'13'</span></span> <span class="hljs-comment"><span class="hljs-comment"># Download chromosome data from s3 and split into bins aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv' | parallel "echo 'reading {}'; aws s3 cp "$DATA_LOC"{} - | awk -v chr=\""$DESIRED_CHR"\" -v chunk=\"{}\" -f split_on_chr_bin.awk" # Combine all the parallel process chunks to single files and upload to rds using R ls chunked/ | cut -d '_' -f 4 | sort -u | parallel "echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds" rm chunked/*</span></span></code> </pre> <br>  Das Skript besteht aus zwei <code>parallel</code> Abschnitten. <br><br>  Im ersten Abschnitt werden Daten aus allen Dateien gelesen, die Informationen zum gew√ºnschten Chromosom enthalten. Anschlie√üend werden diese Daten auf Streams verteilt, die Dateien in die entsprechenden Gruppen (bin) verteilen.  Um zu verhindern, dass Rennbedingungen auftreten, wenn mehrere Streams in eine einzelne Datei geschrieben werden, √ºbertr√§gt AWK die Dateinamen zum Schreiben von Daten an verschiedene Stellen, z. B. <code>chr_10_bin_52_batch_2_aa.csv</code> .  Infolgedessen werden viele kleine Dateien auf der Festplatte erstellt (daf√ºr habe ich Terabyte-EBS-Volumes verwendet). <br><br>  Die Pipeline aus dem zweiten <code>parallel</code> Abschnitt durchl√§uft die Gruppen (bin) und kombiniert ihre einzelnen Dateien zu gemeinsamen CSVs mit <code>cat</code> und sendet sie dann zum Export. <br><br><h2>  Sendung an R? </h2><br>  <b>Was ich gelernt habe</b> : Sie k√∂nnen √ºber ein R-Skript auf <code>stdin</code> und <code>stdout</code> zugreifen und es daher in der Pipeline verwenden. <br><br>  Im Bash-Skript stellen Sie m√∂glicherweise folgende Zeile fest: <code>...cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R...</code>  <code>...cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R...</code>  Es √ºbersetzt alle verketteten Gruppendateien (bin) in das unten stehende R-Skript.  <code>{}</code> ist eine spezielle <code>parallel</code> Technik, bei der alle von ihr gesendeten Daten in den angegebenen Stream direkt in den Befehl selbst eingef√ºgt werden.  Die Option <code>{#}</code> bietet eine eindeutige Thread-ID, und <code>{%}</code> f√ºr die Job-Slot-Nummer (wiederholt, jedoch niemals gleichzeitig).  Eine Liste aller Optionen finden Sie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation.</a> <br><br><pre> <code class="lisp hljs"><span class="hljs-meta"><span class="hljs-meta">#!/usr/bin/env Rscript library(readr) library(aws.s3) # Read first command line argument data_destination &lt;- commandArgs(trailingOnly = TRUE)[1] data_cols &lt;- list(SNP_Name = 'c', ...) s3saveRDS( read_csv( file("stdin"), col_names = names(data_cols), col_types = data_cols ), object = data_destination )</span></span></code> </pre> <br>  Wenn die <code>readr::read_csv</code> <code>file("stdin")</code> an <code>readr::read_csv</code> , werden die in das R-Skript √ºbersetzten Daten in den Frame geladen, der dann mit <code>aws.s3</code> als <code>.rds</code> Datei direkt in S3 <code>aws.s3</code> . <br><br>  RDS ist ein bisschen wie eine j√ºngere Version von Parkett, ohne den Schnickschnack der S√§ulenlagerung. <br><br>  Nach Abschluss des Bash-Skripts erhielt ich eine <code>.rds</code> Dateien in S3, mit denen ich effiziente Komprimierung und integrierte Typen verwenden konnte. <br><br>  Trotz der Verwendung der Bremse R funktionierte alles sehr schnell.  Es ist nicht √ºberraschend, dass die Fragmente auf R, die f√ºr das Lesen und Schreiben von Daten verantwortlich sind, gut optimiert sind.  Nach dem Testen auf einem mittelgro√üen Chromosom war die Aufgabe auf der C5n.4xl-Instanz in etwa zwei Stunden abgeschlossen. <br><br><h2>  S3 Einschr√§nkungen </h2><br>  <b>Was ich gelernt habe</b> : Dank der intelligenten Implementierung von Pfaden kann S3 viele Dateien verarbeiten. <br><br>  Ich war besorgt, ob S3 viele darauf √ºbertragene Dateien verarbeiten k√∂nnte.  Ich k√∂nnte die Dateinamen aussagekr√§ftig machen, aber wie wird S3 danach suchen? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/841/0dc/c34/8410dcc34a563c683dd7602dc66d884a.png"><br> <i>  S3    ,        <code>/</code> . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> FAQ- S3.</a></i> <br><br> , S3            -      .  (bucket)   ,   ‚Äî    . <br><br>          Amazon, ,    ¬´-----¬ª  .    :       get-,       . ,      20 . bin-. ,   ,      (,      ,      ).          . <br><br><h2>    ? </h2><br>   :     ‚Äî     . <br><br>       : ¬´    ?¬ª      ( gzip CSV-   7  )      .     ,  R     Parquet ( Arrow)     Spark.       R,         ,         ,       . <br><br><h2>   </h2><br> <b>  </b> :     ,    . <br><br>       ,      . <br>     EC2  ,                 ( ,  Spark    ).  ,          ,    AWS-      10 . <br><br>      R      . <br><br>   S3 ,       . <br><br><pre> <code class="bash hljs">library(aws.s3) library(tidyverse) chr_sizes &lt;- get_bucket_df( bucket = <span class="hljs-string"><span class="hljs-string">'...'</span></span>, prefix = <span class="hljs-string"><span class="hljs-string">'...'</span></span>, max = Inf ) %&gt;% mutate(Size = as.numeric(Size)) %&gt;% filter(Size != 0) %&gt;% mutate( <span class="hljs-comment"><span class="hljs-comment"># Extract chromosome from the file name chr = str_extract(Key, 'chr.{1,4}\\.csv') %&gt;% str_remove_all('chr|\\.csv') ) %&gt;% group_by(chr) %&gt;% summarise(total_size = sum(Size)/1e+9) # Divide to get value in GB # A tibble: 27 x 2 chr total_size &lt;chr&gt; &lt;dbl&gt; 1 0 163. 2 1 967. 3 10 541. 4 11 611. 5 12 542. 6 13 364. 7 14 375. 8 15 372. 9 16 434. 10 17 443. # ‚Ä¶ with 17 more rows</span></span></code> </pre> <br>    ,    ,   ,     <code>num_jobs</code>  ,       . <br><br><pre> <code class="bash hljs">num_jobs &lt;- 7 <span class="hljs-comment"><span class="hljs-comment"># How big would each job be if perfectly split? job_size &lt;- sum(chr_sizes$total_size)/7 shuffle_job &lt;- function(i){ chr_sizes %&gt;% sample_frac() %&gt;% mutate( cum_size = cumsum(total_size), job_num = ceiling(cum_size/job_size) ) %&gt;% group_by(job_num) %&gt;% summarise( job_chrs = paste(chr, collapse = ','), total_job_size = sum(total_size) ) %&gt;% mutate(sd = sd(total_job_size)) %&gt;% nest(-sd) } shuffle_job(1) # A tibble: 1 x 2 sd data &lt;dbl&gt; &lt;list&gt; 1 153. &lt;tibble [7 √ó 3]&gt;</span></span></code> </pre> <br>      purrr     . <br><br><pre> <code class="bash hljs">1:1000 %&gt;% map_df(shuffle_job) %&gt;% filter(sd == min(sd)) %&gt;% pull(data) %&gt;% pluck(1)</code> </pre> <br>     ,    .       Bash-    <code>for</code> .       10 .    ,             .  ,        . <br><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> DESIRED_CHR <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-string"><span class="hljs-string">"16"</span></span> <span class="hljs-string"><span class="hljs-string">"9"</span></span> <span class="hljs-string"><span class="hljs-string">"7"</span></span> <span class="hljs-string"><span class="hljs-string">"21"</span></span> <span class="hljs-string"><span class="hljs-string">"MT"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-comment"><span class="hljs-comment"># Code for processing a single chromosome fi</span></span></code> </pre> <br>     : <br><br><pre> <code class="bash hljs">sudo shutdown -h now</code> </pre> <br> ‚Ä¶   !   AWS CLI       <code>user_data</code>   Bash-    .     ,         . <br><br><pre> <code class="bash hljs">aws ec2 run-instances ...\ --tag-specifications <span class="hljs-string"><span class="hljs-string">"ResourceType=instance,Tags=[{Key=Name,Value=&lt;&lt;job_name&gt;&gt;}]"</span></span> \ --user-data file://&lt;&lt;job_script_loc&gt;&gt;</code> </pre> <br><h1> ! </h1><br> <b>  </b> : API        . <br><br> -        .      ,     .     API   .        <code>.rds</code>  Parquet-,       ,    .       R-. <br><br>      ,        ,    <code>get_snp</code> .       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">pkgdown</a> ,        . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a75/afb/f3a/a75afbf3a2c7c8ef5fa2a873f8ba50b9.png"><br><br><h2>   </h2><br> <b>  </b> :     ,   ! <br><br>          SNP      ,     (binning)   .     SNP,          (bin).      ( )    . <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Part of get_snp() ... # Test if our current snp data has the desired snp. already_have_snp &lt;- desired_snp %in% prev_snp_results$snps_in_bin if(!already_have_snp){ # Grab info on the bin of the desired snp snp_results &lt;- get_snp_bin(desired_snp) # Download the snp's bin data snp_results$bin_data &lt;- aws.s3::s3readRDS(object = snp_results$data_loc) } else { # The previous snp data contained the right bin so just use it snp_results &lt;- prev_snp_results } ...</span></span></code> </pre> <br>       ,       .    ,      . , <code>dplyr::filter</code>           ,           ,    . <br><br>  ,   <code>prev_snp_results</code>   <code>snps_in_bin</code> .     SNP   (bin),   ,       .        SNP   (bin)    : <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Get bin-mates snps_in_bin &lt;- my_snp_results$snps_in_bin for(current_snp in snps_in_bin){ my_snp_results &lt;- get_snp(current_snp, my_snp_results) # Do something with results }</span></span></code> </pre> <br><h1>  Ergebnisse </h1><br>    (  )    ,   .  ,           .      . <br><br>       ,       ,     ,     ‚Ä¶ <br><br>   .       .       (  ),  ,   (bin)   ,    SNP     0,1 ,     ,     S3 . <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-3" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1134151057385369600"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br><h2>  Fazit </h2><br>   ‚Äî   .   ,     . ,    . ,   ,         ,     .  ,       ,  ,        ,    .  ,       ,    ,        ,      -     . <br><br>     .     ,        ,  ¬´¬ª  ,    .          . <br><br><h3>   : </h3><br><ul><li>      25   ; <br></li><li>      Parquet-   ; <br></li><li>   Spark   ; <br></li><li>      2,5  ; <br></li><li>    ,    Spark; <br></li><li>      ; <br></li><li>   Spark  ,      ; <br></li><li>  ,    ,  -       1980-; <br></li><li> <code>gnu parallel</code> ‚Äî   ,    ; <br></li><li> Spark        ; <br></li><li>  Spark        ; <br></li><li>    AWK  ; <br></li><li>    <code>stdin</code>  <code>stdout</code>  R-,       ; <br></li><li>     S3    ; <br></li><li>     ‚Äî     ; <br></li><li>     ,    ; <br></li><li> API        ; <br></li><li>     ,   ! <br></li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de456392/">https://habr.com/ru/post/de456392/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de456376/index.html">Not One Spring Boot: Ein √úberblick √ºber Alternativen</a></li>
<li><a href="../de456380/index.html">Tag der offenen T√ºr der Fakult√§t f√ºr Programmierung in der Netologie</a></li>
<li><a href="../de456382/index.html">Zusammenarbeit und Automatisierung im Frontend. Was wir aus 13 Schulen gelernt haben</a></li>
<li><a href="../de456386/index.html">√ñffnen Sie Bibliotheken zur Visualisierung von Audioinhalten</a></li>
<li><a href="../de456388/index.html">Diagnoseentwicklungsdiagramm in PVS-Studio</a></li>
<li><a href="../de456394/index.html">Erstellen des allgegenw√§rtigen Begr√º√üungsbildschirms unter iOS</a></li>
<li><a href="../de456398/index.html">Vue-Cli-Plugins, die mit komplexen Daten und eigenschaftsbasierten Tests arbeiten - Ank√ºndigung des Panda-Meetup-Frontends</a></li>
<li><a href="../de456400/index.html">Warum Wettk√§mpfe besser sind als Pauken: Unsere Erfahrung mit Gamification</a></li>
<li><a href="../de456402/index.html">Z√§hne der Weisheit: Pull-Pull</a></li>
<li><a href="../de456404/index.html">Looper - Plugin f√ºr Sketch</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>