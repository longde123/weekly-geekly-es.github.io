<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßóüèæ üßëüèæ‚Äçü§ù‚ÄçüßëüèΩ ‚ôêÔ∏è O Pixel 3 aprende como determinar a profundidade nas fotos ü¶î üî® üèÇüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="O modo retrato nos smartphones Pixel permite tirar fotos com apar√™ncia profissional que chamam a aten√ß√£o para o assunto com a desfocagem do fundo. No ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>O Pixel 3 aprende como determinar a profundidade nas fotos</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/433600/"> O modo retrato nos smartphones Pixel permite tirar fotos com apar√™ncia profissional que chamam a aten√ß√£o para o assunto com a desfocagem do fundo.  No ano passado, descrevemos como calculamos a profundidade usando uma √∫nica c√¢mera e foco autom√°tico com detec√ß√£o de fase (foco autom√°tico com detec√ß√£o de fase, PDAF), tamb√©m conhecido como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">foco autom√°tico com dois pixels</a> .  Esse processo usou um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">algoritmo est√©reo tradicional</a> sem treinamento.  Este ano, no Pixel 3, adotamos o aprendizado de m√°quina para melhorar a avalia√ß√£o de profundidade e produzir resultados ainda melhores no modo retrato. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/505/531/899/505531899e63adc78fbd74d94f1c3a3a.gif"><br>  <i>Esquerda: A imagem original capturada em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">HDR +</a> .</i>  <i>√Ä direita, h√° uma compara√ß√£o dos resultados da fotografia no modo retrato usando a profundidade do est√©reo tradicional e do aprendizado de m√°quina.</i>  <i>Os resultados da aprendizagem produzem menos erros.</i>  <i>No resultado est√©reo tradicional, a profundidade de muitas linhas horizontais atr√°s do homem √© incorretamente estimada igual √† profundidade do pr√≥prio homem, como resultado das quais elas permanecem n√≠tidas.</i> <br><a name="habracut"></a><br><h2>  Uma breve excurs√£o ao material anterior </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">No ano passado,</a> descrevemos que o modo retrato usa uma rede neural para separar os pixels pertencentes √†s imagens das pessoas e uma imagem de fundo e complementa essa m√°scara de dois n√≠veis com informa√ß√µes de profundidade derivadas dos pixels do PDAF.  Tudo isso foi feito para obter o desfoque, dependendo da profundidade, pr√≥ximo ao que uma c√¢mera profissional pode oferecer. <br><br>  Para trabalhar, o PDAF tira duas fotos ligeiramente diferentes da cena.  Alternando entre imagens, voc√™ pode ver que a pessoa n√£o est√° se movendo e o fundo est√° se movendo horizontalmente - esse efeito √© chamado de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">paralaxe</a> .  Como a paralaxe √© uma fun√ß√£o da dist√¢ncia de um ponto da c√¢mera e da dist√¢ncia entre dois pontos de vista, podemos determinar a profundidade comparando cada ponto em uma imagem com o ponto correspondente em outra. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e15/41c/044/e1541c044baa5454ddee71ef45c9a96c.gif"><br>  <i>As imagens do PDAF √† esquerda e no meio parecem semelhantes, mas a paralaxe pode ser vista no fragmento ampliado √† direita.</i>  <i>√â mais f√°cil notar pela estrutura redonda no centro de amplia√ß√£o.</i> <br><br>  No entanto, encontrar essas correspond√™ncias nas imagens PDAF (esse m√©todo √© chamado de profundidade est√©reo) √© uma tarefa extremamente dif√≠cil, pois os pontos entre as fotos se movem muito fracamente.  Al√©m disso, todas as tecnologias est√©reo sofrem de problemas de abertura.  Se voc√™ observar a cena atrav√©s de uma pequena abertura, n√£o ser√° poss√≠vel encontrar a correspond√™ncia de pontos para linhas paralelas √† linha de base est√©reo, ou seja, a linha que liga as duas c√¢meras.  Em outras palavras, ao estudar linhas horizontais na foto apresentada (ou linhas verticais nas fotos com orienta√ß√£o retrato), todas as mudan√ßas em uma imagem em rela√ß√£o √† outra parecem aproximadamente iguais.  No modo retrato do ano passado, todos esses fatores podem levar a erros na determina√ß√£o da profundidade e na apar√™ncia de artefatos desagrad√°veis. <br><br><h2>  Melhorando a avalia√ß√£o da profundidade </h2><br>  Com o modo retrato Pixel 3, corrigimos esses erros usando o fato de o paralaxe de fotos est√©reo ser apenas uma das muitas pistas nas imagens.  Por exemplo, pontos que est√£o longe do plano de foco parecem menos n√≠tidos e isso ser√° uma dica da profundidade desfocada.  Al√©m disso, mesmo ao visualizar uma imagem em uma tela plana, podemos estimar facilmente a dist√¢ncia dos objetos, pois sabemos o tamanho aproximado dos objetos do dia a dia (ou seja, voc√™ pode usar o n√∫mero de pixels que representam o rosto de uma pessoa para estimar a sua localiza√ß√£o).  Esta ser√° uma pista sem√¢ntica. <br><br>  O desenvolvimento manual de um algoritmo que combina essas dicas √© extremamente dif√≠cil, mas usando o MO, podemos fazer isso enquanto melhoramos o desempenho das dicas de paralaxe do PDAF.  Especificamente, treinamos uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">rede neural convolucional</a> escrita em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">TensorFlow</a> , que recebe pixels do PDAF como entrada e aprendemos a prever a profundidade.  Esse novo m√©todo aprimorado para estimar a profundidade com base no MO √© usado no modo retrato Pixel 3. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7db/0ff/5e3/7db0ff5e3063425f703aaf2b3f30fd77.png"><br>  <i>Nossa rede neural convolucional recebe imagens de PDAF e fornece um mapa de profundidade.</i>  <i>A rede usa uma arquitetura no estilo codificador-decodificador com <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">conex√µes de salto</a> adicionais e blocos residuais.</i> <br><br><h2>  Treinamento em redes neurais </h2><br>  Para treinar a rede, precisamos de muitas imagens PDAF e mapas de profundidade de alta qualidade correspondentes.  E como precisamos que previs√µes de profundidade sejam √∫teis no modo retrato, precisamos de dados de treinamento semelhantes √†s fotos que os usu√°rios tiram com smartphones. <br><br>  Para isso, projetamos um dispositivo Frankenfon especial, no qual combinamos cinco telefones Pixel 3 e estabelecemos uma conex√£o WiFi entre eles, o que nos permitiu tirar fotos simultaneamente de todos os telefones (com uma diferen√ßa de n√£o mais que 2 ms).  Com este dispositivo, calculamos mapas de profundidade de alta qualidade com base em fotografias, usando movimento e est√©reo de v√°rios √¢ngulos. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/995/827/05b/99582705b2c447fa6faf95c5c114d20f.gif"><br>  <i>Esquerda: Um dispositivo para coletar dados de treinamento.</i>  <i>No meio: um exemplo de altern√¢ncia entre cinco fotografias.</i>  <i>A sincroniza√ß√£o da c√¢mera garante a capacidade de calcular a profundidade em cenas din√¢micas.</i>  <i>Direita: profundidade total.</i>  <i>Pontos com baixa confian√ßa, onde a compara√ß√£o de pixels em fotos diferentes era incerta devido √† fraqueza das texturas, s√£o pintados de preto e n√£o s√£o usados ‚Äã‚Äãem treinamento.</i> <br><br>  Os dados obtidos com este dispositivo foram ideais para treinar a rede pelos seguintes motivos: <br><br><ul><li>  Cinco pontos de vista garantem a presen√ßa de paralaxe em v√°rias dire√ß√µes, o que nos salva do problema da abertura. </li><li>  A localiza√ß√£o das c√¢meras garante que qualquer ponto da imagem seja repetido em pelo menos duas fotografias, o que reduz o n√∫mero de pontos que n√£o podem ser correspondidos. </li><li>  A linha de base, ou seja, a dist√¢ncia entre as c√¢meras, √© maior que a do PDAF, o que garante uma estimativa mais precisa da profundidade. </li><li>  A sincroniza√ß√£o da c√¢mera garante a capacidade de calcular a profundidade em cenas din√¢micas. </li><li>  A portabilidade do dispositivo garante a possibilidade de tirar fotos na natureza, simulando fotos que os usu√°rios tiram usando smartphones. </li></ul><br>  No entanto, apesar da idealidade dos dados obtidos usando este dispositivo, ainda √© extremamente dif√≠cil prever a profundidade absoluta dos objetos de cena - qualquer par PDAF pode corresponder a v√°rios mapas de profundidade (tudo depende das caracter√≠sticas das lentes, dist√¢ncia focal etc.).  Para levar tudo isso em considera√ß√£o, estimamos a profundidade relativa dos objetos da cena, o que √© suficiente para obter resultados satisfat√≥rios no modo retrato. <br><br><h2>  Combinamos tudo isso </h2><br>  A estimativa da profundidade usando MOs no Pixel 3 deve funcionar rapidamente, para que os usu√°rios n√£o precisem esperar muito tempo pelos resultados de retrato.  No entanto, para obter boas estimativas de profundidade usando pequenos desfocagens e paralaxe, √© necess√°rio alimentar as redes neurais da foto em resolu√ß√£o total.  Para garantir resultados r√°pidos, usamos o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">TensorFlow Lite</a> , uma solu√ß√£o de plataforma cruzada para o lan√ßamento de modelos MO em dispositivos m√≥veis e incorporados, bem como uma poderosa GPU Pixel 3, que permite calcular rapidamente a profundidade de dados de entrada incomumente grandes.  Em seguida, combinamos as estimativas de profundidade obtidas com as m√°scaras da nossa rede neural, que distingue as pessoas, para obter os mais belos resultados da captura no modo retrato. <br><br><h2>  Experimente voc√™ mesmo </h2><br>  No Google Camera App vers√£o 6.1 e superior, nossos mapas de profundidade s√£o incorporados nas imagens no modo retrato.  Isso significa que podemos usar o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Editor de profundidade</a> do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Google Fotos</a> para alterar o grau de desfoque e o ponto de foco depois de tirar uma foto.  Voc√™ tamb√©m pode usar programas de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">terceiros</a> para extrair mapas de profundidade do jpeg e estud√°-los voc√™ mesmo.  Voc√™ tamb√©m pode tirar um √°lbum <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">do link</a> , mostrando mapas de profundidade relativa e imagens correspondentes no modo retrato, para comparar a abordagem est√©reo e MO tradicional. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt433600/">https://habr.com/ru/post/pt433600/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt433586/index.html">Como n√£o vencemos o hackathon</a></li>
<li><a href="../pt433588/index.html">Desempenho surpreendente de algoritmos paralelos C ++ 17. Mito ou Realidade?</a></li>
<li><a href="../pt433592/index.html">Informa√ß√µes: Yandex.Phone</a></li>
<li><a href="../pt433596/index.html">Erro de Magalh√£es: satura√ß√£o de buffer ou expedi√ß√£o ao redor do mundo usando SQLite FTS</a></li>
<li><a href="../pt433598/index.html">Como o LLVM otimiza a fun√ß√£o</a></li>
<li><a href="../pt433602/index.html">A simplicidade matem√°tica pode estar na base da velocidade da evolu√ß√£o.</a></li>
<li><a href="../pt433604/index.html">Trabalho confort√°vel com o Android Studio</a></li>
<li><a href="../pt433606/index.html">Profundidades do SIEM: correla√ß√µes prontas para uso. Parte 3.2 Metodologia de Normaliza√ß√£o de Eventos</a></li>
<li><a href="../pt433608/index.html">O carro do futuro. Telas em vez de vidro autom√°tico?</a></li>
<li><a href="../pt433610/index.html">Notas de um fitoqu√≠mico. Caqui</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>