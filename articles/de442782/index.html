<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üéç üë®üèæ‚Äçüé® üë®‚Äç‚ù§Ô∏è‚Äçüíã‚Äçüë® VShard - horizontale Skalierung in Tarantool ü•û üíΩ ‚èÆÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo, mein Name ist Vladislav und ich bin Mitglied des Tarantool- Entwicklungsteams. Tarantool ist ein DBMS und ein Anwendungsserver in einem. Heute ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>VShard - horizontale Skalierung in Tarantool</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/442782/"><img src="https://habrastorage.org/webt/4p/e8/fo/4pe8foryc_t_l5joliydwpislhm.png"><br><br>  Hallo, mein Name ist Vladislav und ich bin Mitglied des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tarantool-</a> Entwicklungsteams.  Tarantool ist ein DBMS und ein Anwendungsserver in einem.  Heute werde ich die Geschichte erz√§hlen, wie wir die horizontale Skalierung in Tarantool mithilfe des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">VShard-</a> Moduls <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">implementiert haben</a> . <br><br>  Einige Grundkenntnisse zuerst. <br><br>  Es gibt zwei Arten der Skalierung: horizontal und vertikal.  Es gibt zwei Arten der horizontalen Skalierung: Replikation und Sharding.  Die Replikation stellt die rechnerische Skalierung sicher, w√§hrend das Sharding f√ºr die Datenskalierung verwendet wird. <br><br>  Sharding wird ebenfalls in zwei Typen unterteilt: Range-basiertes Sharding und Hash-basiertes Sharding. <br><br>  Bereichsbasiertes Sharding impliziert, dass f√ºr jeden Clusterdatensatz ein Shard-Schl√ºssel berechnet wird.  Die Shard-Schl√ºssel werden auf eine gerade Linie projiziert, die in Bereiche unterteilt und verschiedenen physischen Knoten zugeordnet ist. <br><br>  Hash-basiertes Sharding ist weniger kompliziert: F√ºr jeden Datensatz in einem Cluster wird eine Hash-Funktion berechnet.  Datens√§tze mit derselben Hash-Funktion werden demselben physischen Knoten zugewiesen. <br><br>  Ich werde mich auf die horizontale Skalierung mit Hash-basiertem Sharding konzentrieren. <br><a name="habracut"></a><br><h2>  √Ñltere Implementierung </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tarantool Shard</a> war unser urspr√ºngliches Modul f√ºr die horizontale Skalierung.  Es wurden einfaches Hash-basiertes Sharding und berechnete Shard-Schl√ºssel nach Prim√§rschl√ºssel f√ºr alle Datens√§tze in einem Cluster verwendet. <br><br><pre><code class="plaintext hljs">function shard_function(primary_key) return guava(crc32(primary_key), shard_count) end</code> </pre> <br>  Aber schlie√ülich wurde Tarantool Shard nicht mehr in der Lage, neue Aufgaben zu bew√§ltigen. <br><br>  Erstens wurde eine unserer m√∂glichen Anforderungen die garantierte <b>Lokalit√§t logisch zusammengeh√∂riger Daten</b> .  Mit anderen Worten, wenn wir logisch zusammenh√§ngende Daten haben, m√∂chten wir diese immer auf einem einzelnen physischen Knoten speichern, unabh√§ngig von der Clustertopologie und dem Ausgleich von √Ñnderungen.  Tarantool Shard kann dies nicht garantieren.  Es wurden Hashes nur mit Prim√§rschl√ºsseln berechnet, sodass ein erneutes Ausgleichen die vor√ºbergehende Trennung von Datens√§tzen mit demselben Hash verursachen kann, da die √Ñnderungen nicht atomar ausgef√ºhrt werden. <br><br>  Dieser Mangel an Datenlokalit√§t war das Hauptproblem f√ºr uns.  Hier ist ein Beispiel.  Angenommen, es gibt eine Bank, bei der ein Kunde ein Konto er√∂ffnet hat.  Die Informationen √ºber das Konto und den Kunden sollten physisch zusammen gespeichert werden, damit sie in einer einzelnen Anfrage abgerufen oder in einer einzelnen Transaktion ge√§ndert werden k√∂nnen, z. B. w√§hrend einer Geld√ºberweisung.  Wenn wir das traditionelle Sharding von Tarantool Shard verwenden, gibt es unterschiedliche Hash-Funktionswerte f√ºr Konten und Kunden.  Die Daten k√∂nnten auf separaten physischen Knoten landen.  Dies erschwert das Lesen und die Abwicklung von Kundendaten erheblich. <br><br><pre> <code class="plaintext hljs">format = {{'id', 'unsigned'}, {'email', 'string'}} box.schema.create_space('customer', {format = format}) format = {{'id', 'unsigned'}, {'customer_id', 'unsigned'}, {'balance', 'number'}} box.schema.create_space('account', {format = format})</code> </pre> <br>  Im obigen Beispiel k√∂nnen die ID-Felder der Konten und des Kunden inkonsistent sein.  Sie sind durch das Feld customer_id des Kontos und das Feld id des Kunden verbunden.  Das gleiche ID-Feld w√ºrde die Eindeutigkeitsbeschr√§nkung des Konto-Prim√§rschl√ºssels verletzen.  Und Shard kann kein Sharding auf andere Weise durchf√ºhren. <br><br>  Ein weiteres Problem war das <b>langsame Resharding</b> , das das grundlegende Problem aller Hash-Shards ist.  Unter dem Strich √§ndert sich beim √Ñndern von Clusterkomponenten die Shard-Funktion, da sie normalerweise von der Anzahl der Knoten abh√§ngt.  Wenn sich die Funktion √§ndert, m√ºssen alle Datens√§tze im Cluster durchsucht und die Funktion neu berechnet werden.  Es kann auch erforderlich sein, einige Datens√§tze zu √ºbertragen.  Und w√§hrend der Daten√ºbertragung wissen wir nicht einmal, ob der erforderliche Datensatz?  In der Anfrage wurden bereits Daten √ºbertragen oder werden gerade √ºbertragen.  W√§hrend des Resharding ist es daher erforderlich, Leseanforderungen sowohl mit alten als auch mit neuen Shard-Funktionen zu stellen.  Anfragen werden zweimal langsamer bearbeitet, was nicht akzeptabel ist. <br><br>  Ein weiteres Problem mit Tarantool Shard war die geringe Verf√ºgbarkeit von Lesevorg√§ngen bei einem Knotenausfall in einem Replikatsatz. <br><br><h2>  Neue L√∂sung </h2><br>  Wir haben <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tarantool VShard entwickelt</a> , um die drei oben genannten Probleme zu l√∂sen.  Der Hauptunterschied besteht darin, dass seine Datenspeicherebene virtualisiert ist, d. H. In physischen Speichern werden virtuelle Speicher gehostet, und Datens√§tze werden den virtuellen Speichern zugewiesen.  Diese Speicher werden <i>Eimer genannt</i> .  Der Benutzer muss sich keine Gedanken dar√ºber machen, was sich auf einem bestimmten physischen Knoten befindet.  Ein Bucket ist eine atomare unteilbare Dateneinheit, wie ein Tupel beim herk√∂mmlichen Sharding.  VShard speichert immer einen gesamten Bucket auf einem physischen Knoten und migriert w√§hrend des Resharding alle Daten atomar von einem Bucket.  Diese Methode stellt die Datenlokalit√§t sicher.  Wir legen die Daten einfach in einen Bucket und k√∂nnen immer sicher sein, dass sie bei Clusterwechseln nicht getrennt werden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/42e/a4f/87b/42ea4f87b5c0f0b05bdf0e0c75b356fe.png"><br><br>  Wie packen wir Daten in einen Bucket?  F√ºgen wir der Tabelle f√ºr unseren Bankkunden ein neues Bucket-ID-Feld hinzu.  Wenn dieser Feldwert f√ºr verwandte Daten identisch ist, befinden sich alle Datens√§tze in einem Bucket.  Der Vorteil ist, dass wir Datens√§tze mit derselben Bucket-ID in verschiedenen R√§umen und sogar in verschiedenen Engines speichern k√∂nnen.  Die Datenlokalit√§t basierend auf der Bucket-ID ist unabh√§ngig von der Speichermethode garantiert. <br><br><pre> <code class="plaintext hljs">format = {{'id', 'unsigned'}, {'email', 'string'}, {'bucket_id', 'unsigned'}} box.schema.create_space('customer', {format = format}) format = {{'id', 'unsigned'}, {'customer_id', 'unsigned'}, {'balance', 'number'}, {'bucket_id', 'unsigned'}} box.schema.create_space('account', {format = format})</code> </pre> <br>  Warum ist das so wichtig?  Bei Verwendung von herk√∂mmlichem Sharding w√ºrden sich die Daten auf verschiedene vorhandene physische Speicher erstrecken.  In unserem Bankbeispiel m√ºssten wir jeden Knoten kontaktieren, wenn wir alle Konten f√ºr einen bestimmten Kunden anfordern.  Wir erhalten also eine Lesekomplexit√§t O (N), wobei N die Anzahl der physischen Speicher ist.  Es ist unglaublich langsam. <br><br>  Durch die Verwendung von Buckets und der Lokalit√§t nach Bucket-ID k√∂nnen die erforderlichen Daten von einem Knoten mit einer Anforderung gelesen werden - unabh√§ngig von der Clustergr√∂√üe. <br><br><img src="https://habrastorage.org/webt/t7/_r/fm/t7_rfmxoroosmaoqbe8cskpsr0k.png"><br><br>  In VShard berechnen Sie Ihre Bucket-ID und weisen sie zu.  F√ºr einige Menschen ist dies ein Vorteil, w√§hrend andere dies als Nachteil betrachten.  Ich glaube, dass die M√∂glichkeit, Ihre eigene Funktion f√ºr die Bucket-ID-Berechnung auszuw√§hlen, von Vorteil ist. <br><br>  Was ist der Hauptunterschied zwischen herk√∂mmlichem Sharding und virtuellem Sharding mit Eimern? <br><br>  Im ersteren Fall haben wir beim √Ñndern von Clusterkomponenten zwei Zust√§nde: den aktuellen (alten) und den neuen, der implementiert werden soll.  W√§hrend des √úbergangsprozesses m√ºssen nicht nur Daten migriert, sondern auch die Hash-Funktion f√ºr jeden Datensatz neu berechnet werden.  Dies ist nicht sehr praktisch, da wir zu einem bestimmten Zeitpunkt nicht wissen, ob die erforderlichen Daten bereits migriert wurden oder nicht.  Dar√ºber hinaus ist diese Methode nicht zuverl√§ssig und die √Ñnderungen sind nicht atomar, da die atomare Migration des Datensatzes mit demselben Hash-Funktionswert eine dauerhafte Speicherung des Migrationsstatus erfordern w√ºrde, falls eine Wiederherstellung erforderlich ist.  Infolgedessen gibt es Konflikte und Fehler, und der Vorgang muss mehrmals neu gestartet werden. <br><br>  Virtuelles Sharding ist viel einfacher.  Wir haben keine zwei verschiedenen Clusterzust√§nde.  Wir haben nur Bucket State.  Der Cluster ist flexibler und wechselt reibungslos von einem Status in einen anderen.  Es gibt jetzt mehr als zwei Staaten?  (unklar).  Mit dem reibungslosen √úbergang ist es m√∂glich, das Auswuchten im laufenden Betrieb zu √§ndern oder neu hinzugef√ºgte Speicher zu entfernen.  Das hei√üt, die Auswuchtkontrolle hat stark zugenommen und ist granularer geworden. <br><br><h2>  Verwendung </h2><br>  Angenommen, wir haben eine Funktion f√ºr unsere Bucket-ID ausgew√§hlt und so viele Daten in den Cluster hochgeladen, dass kein Speicherplatz mehr vorhanden ist.  Jetzt m√∂chten wir einige Knoten hinzuf√ºgen und Daten automatisch auf diese verschieben.  So machen wir es in VShard: Zuerst starten wir neue Knoten und f√ºhren dort Tarantool aus, dann aktualisieren wir unsere VShard-Konfiguration.  Es enth√§lt Informationen zu jeder Clusterkomponente, jedem Replikat, Replikats√§tzen, Mastern, zugewiesenen URIs und vielem mehr.  Jetzt f√ºgen wir unsere neuen Knoten zur Konfigurationsdatei hinzu und wenden sie mit VShard.storage.cfg auf alle Clusterknoten an. <br><br><pre> <code class="plaintext hljs">function create_user(email) local customer_id = next_id() local bucket_id = crc32(customer_id) box.space.customer:insert(customer_id, email, bucket_id) end function add_account(customer_id) local id = next_id() local bucket_id = crc32(customer_id) box.space.account:insert(id, customer_id, 0, bucket_id) end</code> </pre> <br>  Wie Sie sich vielleicht erinnern, √§ndert sich beim √Ñndern der Anzahl der Knoten beim herk√∂mmlichen Sharding auch die Shard-Funktion selbst.  Dies ist in VShard nicht der Fall.  Hier haben wir eine feste Anzahl von virtuellen Speichern oder Eimern.  Dies ist eine Konstante, die Sie beim Starten des Clusters ausw√§hlen.  Es mag den Anschein haben, dass die Skalierbarkeit eingeschr√§nkt ist, aber das ist es wirklich nicht.  Sie k√∂nnen eine gro√üe Anzahl von Eimern angeben, Zehntausende und Hunderttausende.  Es ist wichtig zu wissen, dass mindestens zwei Gr√∂√üenordnungen mehr Buckets vorhanden sein sollten als die maximale Anzahl von Replikats√§tzen, die Sie jemals im Cluster haben werden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/422/499/979/422499979e5b8c5728c3df2b967cf599.gif"><br><br>  Da sich die Anzahl der virtuellen Speicher nicht √§ndert und die Shard-Funktion nur von diesem Wert abh√§ngt, k√∂nnen wir so viele physische Speicher hinzuf√ºgen, wie wir m√∂chten, ohne die Shard-Funktion neu zu berechnen. <br><br>  Wie sind die Eimer den physischen Lagern zugeordnet?  Wenn VShard.storage.cfg aufgerufen wird, wird ein Neuausgleichsprozess auf einem der Knoten aktiviert.  Dies ist ein Analyseprozess, der das perfekte Gleichgewicht f√ºr den Cluster berechnet.  Der Prozess geht zu jedem physischen Knoten und ruft seine Anzahl von Buckets ab und erstellt dann Routen ihrer Bewegungen, um die Zuordnung auszugleichen.  Dann sendet der Rebalancer die Routen an die √ºberlasteten Speicher, die wiederum Eimer senden.  Etwas sp√§ter ist der Cluster ausgeglichen. <br><br>  In realen Projekten ist ein perfektes Gleichgewicht m√∂glicherweise nicht so einfach zu erreichen.  Beispielsweise k√∂nnte ein Replikatsatz weniger Daten enthalten als der andere, da er weniger Speicherkapazit√§t hat.  In diesem Fall k√∂nnte VShard denken, dass alles ausgeglichen ist, aber tats√§chlich steht der erste Speicher kurz vor einer √úberlastung.  Um dem entgegenzuwirken, haben wir einen Mechanismus zur Korrektur der Ausgleichsregeln mittels Gewichten bereitgestellt.  Jedem Replikatsatz oder Speicher kann eine Gewichtung zugewiesen werden.  Wenn der Ausgleicher entscheidet, wie viele Eimer und wohin gesendet werden sollen, ber√ºcksichtigt er die <b>Beziehungen</b> aller Gewichtspaare. <br><br>  Wenn beispielsweise ein Speicher 100 und der andere 200 wiegt, speichert der zweite doppelt so viele Eimer wie der erste.  Bitte beachten Sie, dass ich speziell √ºber Gewichtsbeziehungen spreche.  Absolutwerte haben keinerlei Einfluss.  Sie w√§hlen Gewichte basierend auf einer 100% igen Verteilung in einem Cluster: 30% f√ºr einen Speicher ergeben also 70% f√ºr den anderen.  Sie k√∂nnen die Speicherkapazit√§t in Gigabyte als Basis nehmen oder das Gewicht in der Anzahl der Buckets messen.  Das Wichtigste ist, das notwendige Verh√§ltnis einzuhalten. <br><br><img src="https://habrastorage.org/webt/sz/0v/gi/sz0vgicyunfvpamx3ic8enwsl58.png"><br><br>  Diese Methode hat einen interessanten Nebeneffekt: Wenn einem Speicher das Gewicht Null zugewiesen wird, veranlasst der Neuausgleicher, dass dieser Speicher alle seine Buckets neu verteilt.  Danach k√∂nnen Sie den gesamten Replikatsatz aus der Konfiguration entfernen. <br><br><h2>  Atomic Bucket Migration </h2><br>  Wir haben einen Eimer;  Es akzeptiert einige Lese- und Schreibvorg√§nge, und zu einem bestimmten Zeitpunkt fordert der Neuausgleich die Migration in einen anderen Speicher an.  Der Bucket akzeptiert keine Schreibanforderungen mehr, andernfalls wird er w√§hrend der Migration aktualisiert und dann w√§hrend der Update-Migration erneut aktualisiert. Anschlie√üend wird das Update aktualisiert und so weiter.  Daher werden Schreibanforderungen blockiert, aber das Lesen aus dem Bucket ist weiterhin m√∂glich.  Die Daten werden jetzt an den neuen Speicherort migriert.  Nach Abschluss der Migration nimmt der Bucket erneut Anforderungen an.  Es ist an der alten Stelle noch vorhanden, aber es ist als M√ºll markiert, und sp√§ter l√∂scht der M√ºllsammler es St√ºck f√ºr St√ºck. <br><br>  Auf der Festplatte, die jedem Bucket zugeordnet ist, sind einige Metadaten physisch gespeichert.  Alle oben beschriebenen Schritte werden auf der Festplatte gespeichert. Unabh√§ngig davon, was mit dem Speicher passiert, wird der Bucket-Status automatisch wiederhergestellt. <br><br>  M√∂glicherweise haben Sie folgende Fragen: <br><br><ul><li>  <b>Was passiert mit den Anforderungen, die mit dem Bucket arbeiten, wenn die Migration beginnt?</b> <br><br>  In den Metadaten jedes Buckets gibt es zwei Arten von Referenzen: RO und RW.  Wenn ein Benutzer eine Anfrage an einen Bucket stellt, gibt er an, ob die Arbeit schreibgesch√ºtzt oder schreibgesch√ºtzt sein soll.  F√ºr jede Anforderung wird der entsprechende Referenzz√§hler erh√∂ht. <br><br>  Warum ben√∂tigen wir Referenzz√§hler f√ºr Schreibanforderungen?  Angenommen, ein Bucket wird migriert, und der Garbage Collector m√∂chte ihn pl√∂tzlich l√∂schen.  Der Garbage Collector erkennt, dass der Referenzz√§hler √ºber Null liegt, sodass der Bucket nicht gel√∂scht wird.  Wenn alle Anforderungen abgeschlossen sind, kann der Garbage Collector seine Arbeit erledigen. <br><br>  Der Referenzz√§hler f√ºr Schreibvorg√§nge stellt au√üerdem sicher, dass die Migration des Buckets nicht gestartet wird, wenn mindestens eine Schreibanforderung in Bearbeitung ist.  Andererseits k√∂nnten Schreibanforderungen nacheinander eingehen, und der Bucket w√ºrde niemals migriert.  Wenn der Neuausgleicher den Bucket verschieben m√∂chte, blockiert das System neue Schreibanforderungen, w√§hrend es darauf wartet, dass die aktuellen Anforderungen w√§hrend eines bestimmten Zeitlimits abgeschlossen werden.  Wenn die Anforderungen nicht innerhalb des angegebenen Zeitlimits abgeschlossen werden, akzeptiert das System erneut neue Schreibanforderungen, w√§hrend die Bucket-Migration verschoben wird.  Auf diese Weise versucht der Rebalancer, den Bucket zu migrieren, bis die Migration erfolgreich ist. <br><br>  VShard verf√ºgt √ºber eine Bucket_Ref-API auf niedriger Ebene, falls Sie mehr als nur Funktionen auf hoher Ebene ben√∂tigen.  Wenn Sie wirklich etwas selbst tun m√∂chten, lesen Sie bitte diese API. </li><li>  <b>Ist es m√∂glich, die Datens√§tze freizugeben?</b> <br><br>  Nein, nein.  Wenn der Bucket wichtige Daten enth√§lt und permanenten Schreibzugriff erfordert, m√ºssen Sie die Migration vollst√§ndig blockieren.  Wir haben eine Bucket_Pin-Funktion, um genau das zu tun.  Der Bucket wird an den aktuellen Replikatsatz angeheftet, sodass der Rebalancer den Bucket nicht migrieren kann.  In diesem Fall k√∂nnen sich benachbarte Eimer jedoch ohne Einschr√§nkungen bewegen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b6a/848/fa7/b6a848fa775b0066ac6f69b73d97ed76.png"><br><br>  Eine Replikatsatzsperre ist ein noch st√§rkeres Werkzeug als Bucket_pin.  Dies geschieht nicht mehr im Code, sondern in der Konfiguration.  Eine Replikatsatzsperre deaktiviert die Migration eines Buckets innerhalb / au√üerhalb des Replikatsatzes.  So sind alle Daten permanent f√ºr Schreibvorg√§nge verf√ºgbar. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/65b/744/39c/65b74439c5b5743eda1168bdb320f8f4.png"></li></ul><br><h2>  VShard.router </h2><br>  VShard besteht aus zwei Submodulen: VShard.storage und VShard.router.  Wir k√∂nnen diese unabh√§ngig voneinander in einer einzigen Instanz erstellen und skalieren.  Wenn wir einen Cluster anfordern, wissen wir nicht, wo sich ein bestimmter Bucket befindet, und VShard.router durchsucht ihn nach der Bucket-ID f√ºr uns. <br><br>  Lassen Sie uns auf unser Beispiel zur√ºckblicken, den Bankcluster mit Kundenkonten.  Ich m√∂chte in der Lage sein, alle Konten eines bestimmten Kunden aus dem Cluster abzurufen.  Dies erfordert eine Standardfunktion f√ºr die lokale Suche: <br><br><img src="https://habrastorage.org/webt/q4/om/pp/q4omppscsww5c-cshatc6bnvgky.png"><br><br>  Es sucht nach allen Konten des Kunden anhand seiner ID.  Jetzt muss ich mich entscheiden, wo ich die Funktion ausf√ºhren soll.  Zu diesem Zweck berechne ich die Bucket-ID anhand der Kundenkennung in meiner Anfrage und fordere VShard.router auf, die Funktion in dem Speicher aufzurufen, in dem sich der Bucket mit der Ziel-Bucket-ID befindet.  Das Submodul verf√ºgt √ºber eine Routing-Tabelle, die die Positionen der Buckets in den Replikats√§tzen beschreibt.  VShard.router leitet meine Anfrage weiter. <br><br>  Es kann durchaus vorkommen, dass das Splittern genau in diesem Moment beginnt und sich die Eimer in Bewegung setzen.  Der Router im Hintergrund aktualisiert die Tabelle schrittweise in gro√üen Bl√∂cken, indem er aktuelle Bucket-Tabellen von den Speichern anfordert. <br><br>  M√∂glicherweise fordern wir sogar einen k√ºrzlich migrierten Bucket an, wobei der Router seine Routing-Tabelle noch nicht aktualisiert hat.  In diesem Fall wird der alte Speicher angefordert, der entweder den Router zu einem anderen Speicher umleitet oder einfach antwortet, dass er nicht √ºber die erforderlichen Daten verf√ºgt.  Dann durchsucht der Router jeden Speicher auf der Suche nach dem erforderlichen Bucket.  Und wir werden nicht einmal einen Fehler in der Routing-Tabelle bemerken. <br><br><h2>  Failover lesen </h2><br>  Erinnern wir uns an unsere anf√§nglichen Probleme: <br><br><ul><li>  Keine Datenlokalit√§t.  Mit Eimern gel√∂st. </li><li>  Resharding-Prozess blockiert und h√§lt alles zur√ºck.  Wir haben die atomare Daten√ºbertragung mithilfe von Buckets implementiert und die Neuberechnung der Shard-Funktion beseitigt. </li><li>  Failover lesen. </li></ul><br>  Das letzte Problem wird von VShard.router behoben, das vom automatischen Lesefailover-Subsystem unterst√ºtzt wird. <br><br>  Von Zeit zu Zeit pingt der Router die in der Konfiguration angegebenen Speicher an.  Angenommen, der Router kann keinen von ihnen anpingen.  Der Router verf√ºgt √ºber eine Hot-Backup-Verbindung zu jedem Replikat. Wenn das aktuelle Replikat nicht reagiert, wechselt es einfach zu einem anderen.  Die Leseanforderungen werden normal verarbeitet, da wir die Replikate lesen (aber nicht schreiben) k√∂nnen.  Und wir k√∂nnen die Priorit√§t f√ºr Replikate als Faktor angeben, damit der Router das Failover f√ºr Lesevorg√§nge ausw√§hlt.  Dies erfolgt mittels Zoning. <br><br><img src="https://habrastorage.org/webt/aw/iz/ry/awizryylhzk9h2rct_kxo1-jvpc.png"><br><br>  Wir weisen jedem Replikat und jedem Router eine Zonennummer zu und geben eine Tabelle an, in der wir den Abstand zwischen jedem Zonenpaar angeben.  Wenn der Router entscheidet, wohin er eine Leseanforderung senden soll, w√§hlt er ein Replikat in der n√§chsten Zone aus. <br><br>  So sieht es in der Konfiguration aus: <br><br><img src="https://habrastorage.org/webt/2w/jx/cu/2wjxcuidtcghobukd3mxu02ms2y.png"><br><br>  Im Allgemeinen k√∂nnen Sie jedes Replikat anfordern. Wenn der Cluster jedoch gro√ü, komplex und stark verteilt ist, kann das Zoning sehr n√ºtzlich sein.  Verschiedene Server-Racks k√∂nnen als Zonen ausgew√§hlt werden, damit das Netzwerk nicht durch Datenverkehr √ºberlastet wird.  Alternativ k√∂nnen geografisch isolierte Punkte ausgew√§hlt werden. <br><br>  Das Zoning hilft auch, wenn Replikate unterschiedliche Verhaltensweisen aufweisen.  Beispielsweise verf√ºgt jeder Replikatsatz √ºber ein Sicherungsreplikat, das keine Anforderungen akzeptieren, sondern nur eine Kopie der Daten speichern soll.  In diesem Fall platzieren wir es in einer Zone weit entfernt von allen Routern in der Tabelle, sodass der Router dieses Replikat nur dann adressiert, wenn dies unbedingt erforderlich ist. <br><br><h2>  Failover schreiben </h2><br>  Wir haben bereits √ºber Lese-Failover gesprochen.  Was ist mit Schreibfailover beim Wechsel des Masters?  In VShard ist das Bild nicht mehr so ‚Äã‚Äãrosig wie zuvor: Die Masterauswahl ist nicht implementiert, daher m√ºssen wir es selbst tun.  Wenn wir irgendwie einen Master benannt haben, sollte die bezeichnete Instanz nun als Master √ºbernehmen.  Anschlie√üend aktualisieren wir die Konfiguration, indem wir f√ºr den alten Master master = false und f√ºr den neuen master = true angeben, die Konfiguration mithilfe von VShard.storage.cfg anwenden und f√ºr jeden Speicher freigeben.  Alles andere wird automatisch erledigt.  Der alte Master akzeptiert keine Schreibanforderungen mehr und beginnt mit der Synchronisierung mit dem neuen, da m√∂glicherweise Daten vorhanden sind, die bereits auf den alten Master angewendet wurden, jedoch nicht auf den neuen.  Danach ist der neue Master verantwortlich und beginnt, Anforderungen anzunehmen, und der alte Master ist eine Replik.  So funktioniert Write Failover in VShard. <br><br><pre> <code class="plaintext hljs">replicas = new_cfg.sharding[uud].replicas replicas[old_master_uuid].master = false replicas[new_master_uuid].master = true vshard.storage.cfg(new_cfg)</code> </pre> <br><br><h2>  Wie verfolgen wir diese verschiedenen Ereignisse? </h2><br>  VShard.storage.info und VShard.router.info reichen aus. <br><br>  VShard.storage.info zeigt Informationen in mehreren Abschnitten an. <br><br><pre> <code class="plaintext hljs">vshard.storage.info() --- - replicasets: &lt;replicaset_2&gt;: uuid: &lt;replicaset_2&gt; master: uri: storage@127.0.0.1:3303 &lt;replicaset_1&gt;: uuid: &lt;replicaset_1&gt; master: missing bucket: receiving: 0 active: 0 total: 0 garbage: 0 pinned: 0 sending: 0 status: 2 replication: status: slave Alerts: - ['MISSING_MASTER', 'Master is not configured for ''replicaset &lt;replicaset_1&gt;']</code> </pre> <br>  Der erste Abschnitt dient der Replikation.  Hier sehen Sie den Status des Replikatsatzes, in dem die Funktion aufgerufen wird: die Replikationsverz√∂gerung, die verf√ºgbaren und nicht verf√ºgbaren Verbindungen, die Hauptkonfiguration usw. <br><br>  Im Bucket-Bereich k√∂nnen Sie in Echtzeit die Anzahl der Buckets anzeigen, die zum / vom aktuellen Replikatsatz migriert werden, die Anzahl der im regul√§ren Modus arbeitenden Buckets, die Anzahl der als M√ºll markierten Buckets und die Anzahl der angehefteten Buckets. <br><br>  Im Abschnitt "Warnungen" werden die Probleme angezeigt, die VShard selbst feststellen konnte: "Der Master ist nicht konfiguriert", "Es ist nicht gen√ºgend Redundanz vorhanden", "Der Master ist vorhanden, aber alle Replikate sind fehlgeschlagen" usw. <br><br>  Und der letzte Abschnitt (q: ist das "Status"?) Ist ein Licht, das rot wird, wenn alles schief geht.  Es ist eine Zahl von null bis drei, wobei eine h√∂here Zahl schlechter ist. <br><br>  VShard.router.info hat die gleichen Abschnitte, aber ihre Bedeutung ist etwas anders. <br><br><pre> <code class="plaintext hljs">vshard.router.info() --- - replicasets: &lt;replicaset_2&gt;: replica: &amp;0 status: available uri: storage@127.0.0.1:3303 uuid: 1e02ae8a-afc0-4e91-ba34-843a356b8ed7 bucket: available_rw: 500 uuid: &lt;replicaset_2&gt; master: *0 &lt;replicaset_1&gt;: replica: &amp;1 status: available uri: storage@127.0.0.1:3301 uuid: 8a274925-a26d-47fc-9e1b-af88ce939412 bucket: available_rw: 400 uuid: &lt;replicaset_1&gt; master: *1 bucket: unreachable: 0 available_ro: 800 unknown: 200 available_rw: 700 status: 1 alerts: - ['UNKNOWN_BUCKETS', '200 buckets are not discovered']</code> </pre> <br>  Der erste Abschnitt befasst sich mit der Replikation, enth√§lt jedoch keine Informationen zu Replikationsverz√∂gerungen, sondern Informationen zur Verf√ºgbarkeit: Router-Verbindungen zu einem Replikatsatz;  Hot-Verbindung und Backup-Verbindung f√ºr den Fall, dass der Master ausf√§llt;  der ausgew√§hlte Master;  und die Anzahl der verf√ºgbaren RW-Buckets und RO-Buckets auf jedem Replikatsatz. <br><br>  Im Bucket-Bereich wird die Gesamtzahl der Lese- / Schreib- und Nur-Lese-Buckets angezeigt, die derzeit f√ºr diesen Router verf√ºgbar sind.  die Anzahl der Eimer mit unbekanntem Standort;  und die Anzahl der Buckets mit einem bekannten Standort, jedoch ohne Verbindung zum erforderlichen Replikatsatz. <br><br>  Der Abschnitt "Warnungen" beschreibt haupts√§chlich Verbindungen, Failover-Ereignisse und nicht identifizierte Buckets. <br><br>  Schlie√ülich gibt es auch den einfachen Status?  Anzeige von null bis drei. <br><br><h2>  Was brauchst du, um VShard zu benutzen? </h2><br>  Zuerst m√ºssen Sie eine konstante Anzahl von Eimern ausw√§hlen.  Warum nicht einfach auf int32_max setzen?  Da Metadaten zusammen mit jedem Bucket gespeichert werden, werden 30 Bytes und 16 Bytes auf dem Router gespeichert.  Je mehr Buckets Sie haben, desto mehr Speicherplatz wird von den Metadaten belegt.  Gleichzeitig wird die Bucket-Gr√∂√üe jedoch kleiner, was eine h√∂here Cluster-Granularit√§t und eine h√∂here Migrationsgeschwindigkeit pro Bucket bedeutet.  Sie m√ºssen also ausw√§hlen, was f√ºr Sie wichtiger ist und welche Skalierbarkeit erforderlich ist. <br><br>  Zweitens m√ºssen Sie eine Shard-Funktion zur Berechnung der Bucket-ID ausw√§hlen.  Die Regeln sind die gleichen wie bei der Auswahl einer Shard-Funktion beim herk√∂mmlichen Sharding, da ein Bucket hier der festen Anzahl von Speichern beim traditionellen Sharding entspricht.  Die Funktion sollte die Ausgabewerte gleichm√§√üig verteilen, da sonst das Wachstum der Schaufelgr√∂√üe nicht ausgeglichen wird und VShard nur mit der Anzahl der Schaufeln arbeitet.  Wenn Sie Ihre Shard-Funktion nicht ausgleichen, m√ºssen Sie die Daten von einem Bucket in einen anderen migrieren und die Shard-Funktion √§ndern.  Daher sollten Sie sorgf√§ltig ausw√§hlen. <br><br><h2>  Zusammenfassung </h2><br>  VShard sorgt daf√ºr: <br><br><ul><li>  Datenlokalit√§t </li><li>  atomares Resharding </li><li>  h√∂here Clusterflexibilit√§t </li><li>  automatisches Lesefailover </li><li>  mehrere Bucket-Controller. </li></ul><br>  VShard befindet sich in der aktiven Entwicklung.  Einige geplante Aufgaben werden bereits umgesetzt.  Die erste Aufgabe ist der <b>Lastausgleich des Routers</b> .  Bei starken Leseanforderungen wird nicht immer empfohlen, diese an den Master zu richten.  Der Router sollte Anforderungen f√ºr verschiedene Lesereplikate selbst ausgleichen. <br><br>  Die zweite Aufgabe ist die <b>sperrfreie Bucket-Migration</b> .  Es wurde bereits ein Algorithmus implementiert, mit dem die Buckets auch w√§hrend der Migration nicht blockiert werden.  Der Bucket wird erst am Ende blockiert, um die Migration selbst zu dokumentieren. <br><br>  Die dritte Aufgabe ist die <b>atomare Anwendung der Konfiguration</b> .  Es ist nicht bequem oder atomar, die Konfiguration separat anzuwenden, da m√∂glicherweise kein Speicher verf√ºgbar ist. Was tun wir als N√§chstes, wenn die Konfiguration nicht angewendet wird?  Aus diesem Grund arbeiten wir an einem Mechanismus f√ºr die automatische Konfigurations√ºbertragung. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de442782/">https://habr.com/ru/post/de442782/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de442770/index.html">√úbersicht √ºber JavaScript-Barcode-Scanner</a></li>
<li><a href="../de442772/index.html">Mathematik f√ºr Data Scientist: Notwendige Abschnitte</a></li>
<li><a href="../de442776/index.html">Indizes in PostgreSQL - 3 (Hash)</a></li>
<li><a href="../de442778/index.html">Learning Go: Eine Auswahl von Videoberichten</a></li>
<li><a href="../de442780/index.html">Die h√§ufigsten Missverst√§ndnisse in der Popul√§rphysik</a></li>
<li><a href="../de442784/index.html">BGP-Hijacking durch Hinzuf√ºgen des AS des Opfers zum AS-SET des Angreifers</a></li>
<li><a href="../de442786/index.html">7 N√ºtzliche Tipps zur Raumnutzung</a></li>
<li><a href="../de442788/index.html">Warum brauchen wir ein √úberwachungssystem auf einem Chip?</a></li>
<li><a href="../de442790/index.html">Die Registrierung f√ºr das Allure Server Meetup in St. Petersburg ist offen</a></li>
<li><a href="../de442794/index.html">Wir laden Sie zur Konferenz ‚Äû(IT) Architekt in IT-Projekten und -Organisationen‚Äú ein.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>