<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§∏üèæ üè¥Û†ÅßÛ†Å¢Û†Å∑Û†Å¨Û†Å≥Û†Åø ‚èØÔ∏è Q-Learning verstehen, das Problem ‚ÄûAuf einem Felsen gehen‚Äú ü§° üöä üìë</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Habr! Ich mache Sie auf eine √úbersetzung des Artikels "Q-Learning verstehen, das Cliff Walking-Problem" von Lucas Vazquez aufmerksam . 




 Im ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Q-Learning verstehen, das Problem ‚ÄûAuf einem Felsen gehen‚Äú</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/443240/">  Hallo Habr!  Ich mache Sie auf eine √úbersetzung des Artikels <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">"Q-Learning verstehen, das Cliff Walking-Problem"</a> von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Lucas Vazquez aufmerksam</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ez/dl/ol/ezdlol2oj19smcejxtdlt4aon68.jpeg"></div><br><p> Im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">letzten Beitrag haben</a> wir das Problem ‚ÄûAuf einem Felsen gehen‚Äú vorgestellt und uns f√ºr einen schrecklichen Algorithmus entschieden, der keinen Sinn ergab.  Dieses Mal werden wir die Geheimnisse dieser grauen Box enth√ºllen und sehen, dass sie √ºberhaupt nicht so be√§ngstigend ist. </p><br><h3>  Zusammenfassung </h3><br><p>  Wir sind zu dem Schluss gekommen, dass wir durch die Maximierung der Anzahl zuk√ºnftiger Belohnungen auch den schnellsten Weg zum Ziel finden. Unser Ziel ist es nun, einen Weg zu finden, dies zu tun! </p><br><a name="habracut"></a><br><h3>  Einf√ºhrung in Q-Learning </h3><br><ul><li>  Beginnen wir mit der Erstellung einer Tabelle, die misst, wie gut eine bestimmte Aktion in einem beliebigen Zustand ausgef√ºhrt wird (wir k√∂nnen sie mit einem einfachen Skalarwert messen. Je gr√∂√üer der Wert, desto besser die Aktion). </li><li>  Diese Tabelle enth√§lt eine Zeile f√ºr jeden Status und eine Spalte f√ºr jede Aktion.  In unserer Welt hat das Raster 48 Zust√§nde (4 mal Y mal 12 mal X) und 4 Aktionen sind zul√§ssig (oben, unten, links, rechts), sodass die Tabelle 48 x 4 ist. </li><li>  Die in dieser Tabelle gespeicherten Werte werden als "Q-Werte" bezeichnet. </li><li>  Dies sind Sch√§tzungen der H√∂he zuk√ºnftiger Belohnungen.  Mit anderen Worten, sie sch√§tzen, wie viel mehr Belohnungen wir vor dem Ende des Spiels erhalten k√∂nnen, wenn wir uns in Zustand S befinden und Aktion A ausf√ºhren. </li><li>  Wir initialisieren die Tabelle mit zuf√§lligen Werten (oder einer Konstanten, zum Beispiel allen Nullen). </li></ul><br><p>  Die optimale ‚ÄûQ-Tabelle‚Äú hat Werte, die es uns erm√∂glichen, in jedem Zustand die besten Ma√ünahmen zu ergreifen, um am Ende den besten Weg zu finden, um zu gewinnen.  Das Problem ist gel√∂st, Prost, Robot Lords :). </p><br><img src="https://habrastorage.org/webt/5a/ii/wl/5aiiwljmx4igtrsrhrc3qoymoge.png"><br>  <i>Q-Werte der ersten f√ºnf Zust√§nde.</i> <br><br><h4>  Q-Learning </h4><br><ul><li>  Q-Learning ist ein Algorithmus, der diese Werte ‚Äûlernt‚Äú. </li><li>  Mit jedem Schritt erhalten wir mehr Informationen √ºber die Welt. </li><li>  Diese Informationen werden verwendet, um die Werte in der Tabelle zu aktualisieren. </li></ul><br><p>  Nehmen wir zum Beispiel an, wir sind einen Schritt vom Ziel entfernt (Quadrat [2, 11]), und wenn wir uns entscheiden, nach unten zu gehen, erhalten wir eine Belohnung von 0 anstelle von -1. <br>  Wir k√∂nnen diese Informationen verwenden, um den Wert des Status-Aktions-Paares in unserer Tabelle zu aktualisieren. Wenn wir es das n√§chste Mal besuchen, wissen wir bereits, dass wir nach unten eine Belohnung von 0 erhalten. </p><br><img src="https://habrastorage.org/webt/7a/iq/u3/7aiqu3ttrz1qnypctrqvlgyd93e.png"><br><p>  Jetzt k√∂nnen wir diese Informationen noch weiter verbreiten!  Da wir jetzt den Weg zum Ziel vom Quadrat aus kennen [2, 11], ist auch jede Aktion, die uns zum Quadrat f√ºhrt [2, 11], gut. Deshalb aktualisieren wir den Q-Wert des Quadrats, der uns zu [2, 11] f√ºhrt. n√§her an 0 sein. </p><br><p>  <b>Und das, meine Damen und Herren, ist die Essenz des Q-Lernens!</b> </p><br><p>  Bitte beachten Sie, dass wir jedes Mal, wenn wir das Ziel erreichen, unsere ‚ÄûKarte‚Äú zum Erreichen des Ziels um ein Quadrat erh√∂hen. Nach einer ausreichenden Anzahl von Iterationen erhalten wir eine vollst√§ndige Karte, die uns zeigt, wie wir aus jedem Zustand zum Ziel gelangen. </p><br><img src="https://habrastorage.org/webt/mj/q0/sh/mjq0shtkn3u37zlhdpptbh2oppe.gif"><br>  <i>Ein Pfad wird generiert, indem in jedem Zustand die beste Aktion ausgef√ºhrt wird.</i>  <i>Die gr√ºne Taste steht f√ºr die Bedeutung einer besseren Aktion, die ges√§ttigten Tasten f√ºr einen h√∂heren Wert.</i>  <i>Der Text stellt einen Wert f√ºr jede Aktion dar (nach oben, unten, rechts, links).</i> <br><br><h3>  Bellman-Gleichung </h3><br><p>  Bevor wir √ºber Code sprechen, sprechen wir √ºber Mathematik: das Grundkonzept des Q-Lernens, die Bellman-Gleichung. </p><br><img src="https://habrastorage.org/webt/i2/_u/gx/i2_ugxlinshqmsyzkawlbmirxri.png"><br><br><ul><li>  Vergessen wir zuerst Œ≥ in dieser Gleichung </li><li>  Die Gleichung besagt, dass der Q-Wert f√ºr ein bestimmtes Zustands-Aktions-Paar die Belohnung sein sollte, die beim √úbergang in einen neuen Zustand (durch Ausf√ºhren dieser Aktion) erhalten wird, addiert zum Wert der besten Aktion im n√§chsten Zustand. </li></ul><br><p>  <b>Mit anderen Worten, wir verbreiten Schritt f√ºr Schritt Informationen √ºber die Werte von Aktionen!</b> </p><br><p>  Wir k√∂nnen jedoch entscheiden, dass der Erhalt einer Belohnung im Moment wertvoller ist als der Erhalt einer Belohnung in der Zukunft. Daher haben wir Œ≥, eine Zahl von 0 bis 1 (normalerweise von 0,9 bis 0,99), die in Zukunft mit einer Belohnung multipliziert wird. zuk√ºnftige Belohnungen rabattieren. </p><br><p>  Wenn also Œ≥ = 0,9 gegeben ist und dies auf einige Zust√§nde unserer Welt (Gitter) angewendet wird, haben wir: </p><br><img src="https://habrastorage.org/webt/e7/yp/gi/e7ypginhzc-cetmrcbqa6ar3byg.png"><br><br><p>  Wir k√∂nnen diese Werte mit den oben genannten in GIF vergleichen und feststellen, dass sie gleich sind. </p><br><br><h3>  Implementierung </h3><br><p>  Nachdem wir ein intuitives Verst√§ndnis der Funktionsweise von Q-Learning haben, k√∂nnen wir √ºber die Implementierung all dessen nachdenken und den Q-Learning-Pseudocode aus <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Suttons Buch</a> als Leitfaden verwenden. </p><br><img src="https://habrastorage.org/webt/wf/6x/fi/wf6xfiyazgu0echvfsw8d9-oly4.png"><br>  <i>Pseudocode aus Suttons Buch.</i> <br><br><p>  Code: </p><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Initialize Q arbitrarily, in this case a table full of zeros q_values = np.zeros((num_states, num_actions)) # Iterate over 500 episodes for _ in range(500): state = env.reset() done = False # While episode is not over while not done: # Choose action action = egreedy_policy(q_values, state, epsilon=0.1) # Do the action next_state, reward, done = env.step(action) # Update q_values td_target = reward + gamma * np.max(q_values[next_state]) td_error = td_target - q_values[state][action] q_values[state][action] += learning_rate * td_error # Update state state = next_state</span></span></code> </pre> <br><ul><li>  Zun√§chst sagen wir: ‚ÄûF√ºr alle Zust√§nde und Aktionen initialisieren wir Q (s, a) willk√ºrlich‚Äú. Dies bedeutet, dass wir unsere Tabelle mit Q-Werten mit beliebigen Werten erstellen k√∂nnen. Sie k√∂nnen zuf√§llig sein, sie k√∂nnen es spielt keine Rolle, dauerhaft zu sein.  Wir sehen, dass wir in Zeile 2 eine Tabelle voller Nullen erstellen. </li></ul><br><p>  <b>Wir sagen auch: "Der Wert von Q f√ºr die Endzust√§nde ist Null", wir k√∂nnen in den Endzust√§nden keine Aktion ausf√ºhren, daher betrachten wir den Wert f√ºr alle Aktionen in diesem Zustand als Null.</b> </p><br><ul><li>  F√ºr jede Episode m√ºssen wir "S initialisieren". Dies ist nur eine ausgefallene Art, "Spiel neu starten" zu sagen. In unserem Fall bedeutet dies, den Spieler in die Ausgangsposition zu bringen.  In unserer Welt gibt es eine Methode, die genau das tut, und wir nennen sie in Zeile 6. </li><li>  F√ºr jeden Zeitschritt (jedes Mal, wenn wir handeln m√ºssen) m√ºssen wir die aus Q erhaltene Aktion ausw√§hlen. </li></ul><br><p>  Denken Sie daran, ich sagte: ‚ÄûErgreifen wir die Ma√ünahmen, die unter allen Bedingungen am wertvollsten sind? </p><br><p>  Wenn wir dies tun, verwenden wir unsere Q-Werte, um die Richtlinie zu erstellen.  In diesem Fall wird es eine gierige Politik sein, weil wir immer Ma√ünahmen ergreifen, die unserer Meinung nach in jedem Staat am besten sind. Deshalb wird gesagt, dass wir gierig handeln. </p><br><br><h3>  Junk </h3><br><p>  Bei diesem Ansatz gibt es jedoch ein Problem: Stellen Sie sich vor, wir befinden uns in einem Labyrinth mit zwei Belohnungen, von denen eine +1 und die andere +100 betr√§gt (und jedes Mal, wenn wir eine davon finden, endet das Spiel).  Da wir immer Ma√ünahmen ergreifen, die wir als die besten betrachten, bleiben wir bei der ersten gefundenen Auszeichnung h√§ngen und kehren immer wieder zu dieser zur√ºck. Wenn wir also zuerst die + 1-Auszeichnung anerkennen, werden wir die gro√üe + 100-Auszeichnung verpassen. </p><br><br><h3>  L√∂sung </h3><br><p>  Wir m√ºssen sicherstellen, dass wir unsere Welt ausreichend studiert haben (dies ist eine erstaunlich schwierige Aufgabe).  Hier kommt Œµ ins Spiel.  Œµ im gierigen Algorithmus bedeutet, dass wir eifrig handeln m√ºssen, ABER zuf√§llige Aktionen als Prozentsatz von Œµ im Laufe der Zeit ausf√ºhren. Bei einer unendlichen Anzahl von Versuchen m√ºssen wir also alle Zust√§nde untersuchen. </p><br><p>  Die Aktion wird gem√§√ü dieser Strategie in Zeile 12 mit epsilon = 0,1 ausgew√§hlt, was bedeutet, dass wir 10% der Zeit auf der Welt forschen.  Die Umsetzung der Richtlinie erfolgt wie folgt: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">egreedy_policy</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(q_values, state, epsilon=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># Get a random number from a uniform distribution between 0 and 1, # if the number is lower than epsilon choose a random action if np.random.random() &lt; epsilon: return np.random.choice(4) # Else choose the action with the highest value else: return np.argmax(q_values[state])</span></span></code> </pre><br><p>  In Zeile 14 in der ersten Liste nennen wir die Schrittmethode, um die Aktion auszuf√ºhren. Die Welt gibt uns den n√§chsten Status, die Belohnung und Informationen √ºber das Ende des Spiels zur√ºck. </p><br><p>  Zur√ºck zur Mathematik: </p><br><p>  Wir haben eine lange Gleichung, lassen Sie uns dar√ºber nachdenken: </p><br><img src="https://habrastorage.org/webt/9v/bn/ws/9vbnws8g-1gclwefuvtpjv_yqpm.png"><br><br><p>  Wenn wir Œ± = 1 nehmen: </p><br><img src="https://habrastorage.org/webt/7r/aw/er/7rawertkpcilxbfzrorhpygtrok.png"><br><br><p>  Was genau der Bellman-Gleichung entspricht, die wir vor einigen Abs√§tzen gesehen haben!  Wir wissen also bereits, dass dies die Leitung ist, die f√ºr die Verbreitung von Informationen √ºber Zustandswerte verantwortlich ist. </p><br><p>  Aber normalerweise ist Œ± (meistens als Lerngeschwindigkeit bekannt) viel kleiner als 1, sein Hauptziel ist es, gro√üe √Ñnderungen in einem Update zu vermeiden. Anstatt ins Ziel zu fliegen, n√§hern wir uns ihm langsam.  In unserem tabellarischen Ansatz verursacht das Setzen von Œ± = 1 keine Probleme, aber bei der Arbeit mit neuronalen Netzen (mehr dazu in den folgenden Artikeln) kann alles leicht au√üer Kontrolle geraten. </p><br><p>  Wenn wir uns den Code ansehen, sehen wir, dass in Zeile 16 in der ersten Auflistung, die wir td_target definiert haben, dies der Wert ist, dem wir n√§her kommen sollten, aber anstatt direkt zu diesem Wert in Zeile 17 zu gehen, berechnen wir td_error und verwenden diesen Wert in Kombination mit der Geschwindigkeit lernen, sich langsam dem Ziel zu n√§hern. </p><br><p>  <b>Denken Sie daran, dass diese Gleichung eine Q-Learning-Einheit ist.</b> </p><br><p>  Jetzt m√ºssen wir nur noch unseren Zustand aktualisieren und alles ist fertig, dies ist Zeile 20. Wir wiederholen diesen Vorgang, bis wir das Ende der Episode erreichen, in den Felsen sterben oder das Ziel erreichen. </p><br><br><h3>  Fazit </h3><br><p>  Jetzt verstehen und verstehen wir intuitiv, wie man Q-Learning codiert (zumindest eine tabellarische Version). √úberpr√ºfen Sie unbedingt den gesamten Code, der f√ºr diesen Beitrag verwendet wird und auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GitHub</a> verf√ºgbar ist. </p><br><p>  Visualisierung des Lernprozesstests: </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Vto8n9C7DSQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  Bitte beachten Sie, dass alle Aktionen mit einem Wert beginnen, der den Endwert √ºberschreitet. Dies ist ein Trick, um die Erforschung der Welt anzuregen. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de443240/">https://habr.com/ru/post/de443240/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de443224/index.html">Matte mit einem Elefanten und einem Pferd. TWIX-Methode</a></li>
<li><a href="../de443232/index.html">Myonenkatalyse in Bezug auf die Quantenchemie. Teil I: gew√∂hnlicher Wasserstoff vs. Myon Wasserstoff</a></li>
<li><a href="../de443234/index.html">Was Ingenieure bei Apple und Intel im B√ºro tun: ein karriereorientierter Online-Kurs in moderner Mikroelektronik f√ºr Studenten</a></li>
<li><a href="../de443236/index.html">Entmystifizieren Sie Faltungs-Neuronale Netze</a></li>
<li><a href="../de443238/index.html">Wie man sich nicht in eine Libelle verwandelt, wenn man viele verschiedene Datenbanken hat</a></li>
<li><a href="../de443242/index.html">Quarkus ist ein subatomares √úberschall-Java. Ein kurzer √úberblick √ºber das Framework</a></li>
<li><a href="../de443244/index.html">Nachbesprechungsaufgaben. Beanpoisk_1</a></li>
<li><a href="../de443246/index.html">Wie wir Askozia IP PBX neu erfunden haben, nachdem das Projekt vom Entwickler verkauft und geschlossen wurde</a></li>
<li><a href="../de443248/index.html">Protokolle zur nahtlosen Reservierung von PRP und HSR</a></li>
<li><a href="../de443252/index.html">Modulare Ant Bots mit Speicher</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>