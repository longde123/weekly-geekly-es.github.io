<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üóíÔ∏è üëåüèø üâê Curiosidade e procrastina√ß√£o no aprendizado de m√°quina üë©üèª‚Äç‚úàÔ∏è üë®üèº‚Äçüîß üõ∞Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="O Aprendizado Refor√ßado (RL) √© uma das t√©cnicas de aprendizado de m√°quina mais promissoras que est√° sendo ativamente desenvolvida. Aqui, o agente de I...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Curiosidade e procrastina√ß√£o no aprendizado de m√°quina</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/427847/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O Aprendizado Refor√ßado</a> (RL) √© uma das t√©cnicas de aprendizado de m√°quina mais promissoras que est√° sendo ativamente desenvolvida.  Aqui, o agente de IA recebe uma recompensa positiva pelas a√ß√µes corretas e uma recompensa negativa pelas a√ß√µes erradas.  Este m√©todo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cenoura e palito</a> √© simples e universal.  Com ele, o DeepMind ensinou o algoritmo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">DQN</a> a jogar videogames antigos da Atari e o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">AlphaGoZero</a> a jogar o antigo jogo Go.  Ent√£o a OpenAI ensinou o algoritmo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">OpenAI-Five</a> a jogar o moderno videogame Dota, e o Google ensinou m√£os rob√≥ticas para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">capturar novos objetos</a> .  Apesar do sucesso da RL, ainda existem muitos problemas que reduzem a efic√°cia dessa t√©cnica. <br><br>  Os algoritmos de RL <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">acham dif√≠cil trabalhar</a> em um ambiente em que o agente raramente recebe feedback.  Mas isso √© t√≠pico do mundo real.  Como exemplo, imagine procurar seu queijo favorito em um labirinto grande, como um supermercado.  Voc√™ est√° procurando e procurando um departamento com queijos, mas n√£o consegue encontr√°-lo.  Se a cada passo voc√™ n√£o recebe um "pau" ou uma "cenoura", √© imposs√≠vel dizer se voc√™ est√° se movendo na dire√ß√£o certa.  Na aus√™ncia de uma recompensa, o que o impede de perambular para sempre?  Nada al√©m de sua curiosidade.  Isso motiva a mudan√ßa para o departamento de compras, que parece desconhecido. <br><a name="habracut"></a><br>  O trabalho cient√≠fico, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">"Curiosidade epis√≥dica atrav√©s da acessibilidade",</a> √© o resultado de uma colabora√ß√£o entre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">a equipe do Google Brain</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">DeepMind,</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">a Escola T√©cnica Superior Su√≠√ßa de Zurique</a> .  Oferecemos um novo modelo de recompensa RL baseada em mem√≥ria epis√≥dica.  Ela parece uma curiosidade que permite explorar o meio ambiente.  Como o agente n√£o deve apenas estudar o ambiente, mas tamb√©m resolver o problema inicial, nosso modelo adiciona um b√¥nus √† recompensa inicialmente esparsa.  A recompensa combinada n√£o √© mais escassa, o que permite que os algoritmos RL padr√£o aprendam com ela.  Assim, nosso m√©todo de curiosidade expande a gama de tarefas que podem ser resolvidas usando RL. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4e9/462/46c/4e946246c97aafab60f3dbe954ca6ed2.png"><br>  <i><font color="gray">Curiosidade ocasional atrav√©s da acessibilidade: dados de observa√ß√£o s√£o adicionados √† mem√≥ria, a recompensa √© calculada com base na dist√¢ncia que a observa√ß√£o atual est√° de observa√ß√µes semelhantes na mem√≥ria.</font></i>  <i><font color="gray">O agente recebe uma recompensa maior por observa√ß√µes que ainda n√£o s√£o apresentadas na mem√≥ria.</font></i> <br><br>  A id√©ia principal do m√©todo √© armazenar as observa√ß√µes do ambiente do agente na mem√≥ria epis√≥dica, al√©m de recompensar o agente por visualizar as observa√ß√µes ainda n√£o apresentadas na mem√≥ria.  "Falta de mem√≥ria" √© a defini√ß√£o de novidade em nosso m√©todo.  A busca por tais observa√ß√µes significa a busca de um estranho.  Tal desejo de procurar um estranho levar√° o agente da IA ‚Äã‚Äãa novos locais, evitando assim vaguear em um c√≠rculo e, finalmente, ajud√°-lo a trope√ßar no alvo.  Como discutiremos mais adiante, nossa reda√ß√£o pode impedir o agente de comportamentos indesej√°veis ‚Äã‚Äãaos quais outras palavras est√£o sujeitas.  Para nossa grande surpresa, esse comportamento tem algumas semelhan√ßas com o que um leigo chamaria de "procrastina√ß√£o". <br><br><h4>  Curiosidade anterior </h4><br>  Embora tenha havido muitas tentativas de formular curiosidade no passado <sup>[1] [2] [3] [4]</sup> , neste artigo, focaremos em uma abordagem natural e muito popular: curiosidade por surpresa com base em previs√£o.  Essa t√©cnica √© descrita em um artigo recente, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">‚ÄúInvestigando um ambiente usando a curiosidade, prevendo sob seu pr√≥prio controle‚Äù</a> (geralmente chamado de ICM).  Para ilustrar a conex√£o entre surpresa e curiosidade, usamos novamente a analogia de encontrar queijo em um supermercado. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b93/003/a3c/b93003a3c6790968f3922777926a494a.jpg"><br>  <i><font color="gray">Ilustra√ß√£o de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Indira Pasko</a> , licenciada sob <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CC BY-NC-ND 4.0</a></font></i> <br><br>  Vagando pela loja, voc√™ est√° tentando prever o futuro ( <i>"Agora estou no departamento de carnes, ent√£o acho que o departamento ao virar da esquina √© o departamento de peixes, eles geralmente est√£o nas proximidades dessa cadeia de supermercados"</i> ).  Se a previs√£o estiver incorreta, voc√™ ficar√° surpreso ( <i>"Na verdade, existe um departamento de vegetais. Eu n√£o esperava isso!"</i> ) - e dessa maneira voc√™ recebe uma recompensa.  Isso aumenta a motiva√ß√£o no futuro para olhar ao virar da esquina novamente, explorando novos lugares apenas para verificar se suas expectativas s√£o verdadeiras (e possivelmente trope√ßam no queijo). <br><br>  Da mesma forma, o m√©todo ICM constr√≥i um modelo preditivo da din√¢mica do mundo e recompensa o agente se o modelo falhar em fazer boas previs√µes - um marcador de surpresa ou novidade.  Observe que explorar novos lugares n√£o est√° diretamente articulado na curiosidade do ICM.  Para o m√©todo ICM, comparecer a eles √© apenas uma maneira de obter mais "surpresas" e, assim, maximizar sua recompensa geral.  Como se v√™, em alguns ambientes, pode haver outras maneiras de se surpreender, o que leva a resultados inesperados. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a56/253/b56/a56253b56c9a991ce20b2c744f42d5a9.gif"></div><br>  <i><font color="gray">Um agente com um sistema de curiosidade baseado em surpresas congela ao se reunir com uma TV.</font></i>  <i><font color="gray">Anima√ß√£o do v√≠deo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Deepak Patak</a> , licenciado sob <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CC BY 2.0</a></font></i> <br><br><h4>  O perigo da procrastina√ß√£o </h4><br>  No artigo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">‚ÄúUm estudo em larga escala da aprendizagem baseada na curiosidade‚Äù, os</a> autores do m√©todo ICM, juntamente com os pesquisadores da OpenAI, mostram um perigo oculto de maximizar a surpresa: os agentes podem aprender a se entregar √† procrastina√ß√£o, em vez de fazer algo √∫til para a tarefa.  Para entender por que isso acontece, considere um experimento mental que os autores chamam de "problema de ru√≠do na televis√£o".  Aqui o agente √© colocado em um labirinto com a tarefa de encontrar um item muito √∫til (como "queijo" em nosso exemplo).  O ambiente possui uma TV e o agente possui um controle remoto.  H√° um n√∫mero limitado de canais (cada canal possui uma transmiss√£o separada) e cada press√£o no controle remoto muda a TV para um canal aleat√≥rio.  Como um agente atuar√° nesse ambiente? <br><br>  Se a curiosidade for formada com base na surpresa, uma mudan√ßa de canal dar√° mais recompensas, pois cada mudan√ßa √© imprevis√≠vel e inesperada.  √â importante observar que, mesmo ap√≥s uma varredura c√≠clica de todos os canais dispon√≠veis, uma sele√ß√£o aleat√≥ria de um canal garante que cada nova altera√ß√£o ainda seja inesperada - o agente faz uma previs√£o de que exibir√° TV ap√≥s alternar o canal e, muito provavelmente, a previs√£o ficar√° incorreta, o que causar√° surpresa.  √â importante observar que, mesmo que o agente j√° tenha visto cada transmiss√£o em cada canal, a altera√ß√£o ainda √© imprevis√≠vel.  Por isso, o agente, em vez de procurar um item muito √∫til, permanecer√° na frente da TV - semelhante √† procrastina√ß√£o.  Como mudar a express√£o da curiosidade para evitar esse comportamento? <br><br><h4>  Curiosidade epis√≥dica </h4><br>  No artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">‚ÄúCuriosidade epis√≥dica atrav√©s de alcan√ßabilidade‚Äù,</a> examinamos um modelo de curiosidade epis√≥dica baseada em mem√≥ria que √© menos propenso a prazer instant√¢neo.  Porque  Se tomarmos o exemplo acima, depois de algum tempo trocando de canal, todas as transmiss√µes acabar√£o na mem√≥ria.  Assim, a TV perde seu apelo: mesmo que a ordem em que os programas apare√ßam na tela seja aleat√≥ria e imprevis√≠vel, eles est√£o todos na mem√≥ria!  Essa √© a principal diferen√ßa do m√©todo baseado na surpresa: nosso m√©todo nem tenta prever o futuro, √© dif√≠cil de prever (ou mesmo imposs√≠vel).  Em vez disso, o agente examina o passado e verifica se h√° alguma observa√ß√£o na mem√≥ria <i>como a</i> atual.  Assim, nosso agente n√£o √© propenso a prazeres instant√¢neos, o que gera um "ru√≠do na televis√£o".  O agente precisar√° explorar o mundo fora da TV para obter mais recompensas. <br><br>  Mas como decidimos se um agente v√™ a mesma coisa que √© armazenada na mem√≥ria?  A verifica√ß√£o exata da correspond√™ncia √© in√∫til: em um ambiente real, um agente raramente v√™ a mesma coisa duas vezes.  Por exemplo, mesmo que o agente retorne √† mesma sala, ele ainda a ver√° de um √¢ngulo diferente. <br><br>  Em vez de verificar se h√° correspond√™ncias exatas, usamos uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">rede neural profunda</a> treinada para medir como duas experi√™ncias s√£o semelhantes.  Para treinar essa rede, devemos adivinhar o quanto as observa√ß√µes ocorreram no tempo.  A proximidade no tempo √© um bom indicador de se duas observa√ß√µes devem ser consideradas parte da mesma.  Esse aprendizado leva a um conceito geral de novidade atrav√©s da acessibilidade, que √© ilustrado abaixo. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/52c/f43/629/52cf436298b7bbf44550ba9770e84f1b.png"><br>  <i><font color="gray">O gr√°fico de alcan√ßabilidade define a novidade.</font></i>  <i><font color="gray">Na pr√°tica, este gr√°fico n√£o est√° dispon√≠vel - portanto, treinamos o aproximador da rede neural para estimar o n√∫mero de etapas entre as observa√ß√µes</font></i> <br><br><h4>  Resultados experimentais </h4><br>  Para comparar o desempenho de diferentes abordagens para descrever a curiosidade, as testamos em dois ambientes 3D visualmente ricos: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ViZDoom</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">DMLab</a> .  Nessas condi√ß√µes, o agente recebeu v√°rias tarefas, como encontrar um alvo no labirinto, colecionar objetos bons e evitar objetos ruins.  No ambiente DMLab, o agente √© equipado por padr√£o com um gadget fant√°stico como um laser, mas se o gadget n√£o for necess√°rio para uma tarefa espec√≠fica, o agente n√£o poder√° us√°-lo livremente.  Curiosamente, com base na surpresa, o agente do ICM realmente usou o laser com muita frequ√™ncia, mesmo que fosse in√∫til concluir a tarefa!  Como no caso da TV, em vez de procurar um item valioso no labirinto, ele preferia passar um tempo atirando nas paredes, porque isso dava muitas recompensas na forma de surpresa.  Teoricamente, o resultado de um tiro na parede deve ser previs√≠vel, mas na pr√°tica √© muito dif√≠cil de prever.  Provavelmente, isso requer um conhecimento mais profundo da f√≠sica do que o dispon√≠vel para o agente de IA padr√£o. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/497/4e6/be3/4974e6be39718b8ab6dbd9cdfcdab273.gif"></div><br>  <i><font color="gray">O agente ICM surpreso constantemente atira contra a parede em vez de explorar o labirinto</font></i> <br><br>  Ao contr√°rio dele, nosso agente dominou um comportamento razo√°vel para estudar o ambiente.  Isso aconteceu porque ele n√£o est√° tentando prever o resultado de suas a√ß√µes, mas procura observa√ß√µes que est√£o "mais longe" daquelas que est√£o na mem√≥ria epis√≥dica.  Em outras palavras, o agente persegue implicitamente objetivos que exigem mais esfor√ßo do que um simples tiro na parede. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b5a/002/381/b5a00238194970be7a6d6cf8969ac7f7.gif"></div><br>  <i><font color="gray">Nosso m√©todo demonstra um comportamento inteligente de explora√ß√£o ambiental.</font></i> <br><br>  √â interessante observar como nossa abordagem de recompensa pune um agente que circula em c√≠rculo, porque ap√≥s a conclus√£o do primeiro c√≠rculo, o agente n√£o encontra novas observa√ß√µes e, portanto, n√£o recebe nenhuma recompensa: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/489/ef2/a8b/489ef2a8b67c8c087cff940bdfeeee9f.gif"></div><br>  <i><font color="gray">Visualiza√ß√£o de recompensa: vermelho corresponde a recompensa negativa, verde a positivo.</font></i>  <i><font color="gray">Da esquerda para a direita: cart√£o-pr√™mio, mapa com localiza√ß√µes na mem√≥ria, vis√£o em primeira pessoa</font></i> <br><br>  Ao mesmo tempo, nosso m√©todo contribui para um bom estudo do meio ambiente: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/075/ae3/952/075ae39528e77477099fad13610d2e4a.gif"></div><br>  <i><font color="gray">Visualiza√ß√£o de recompensa: vermelho corresponde a recompensa negativa, verde a positivo.</font></i>  <i><font color="gray">Da esquerda para a direita: cart√£o-pr√™mio, mapa com localiza√ß√µes na mem√≥ria, vis√£o em primeira pessoa</font></i> <br><br>  Esperamos que nosso trabalho contribua para uma nova onda de pesquisa que vai al√©m do escopo da t√©cnica da surpresa, a fim de educar os agentes sobre comportamentos mais inteligentes.  Para uma an√°lise aprofundada do nosso m√©todo, consulte a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pr√©</a> - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">impress√£o do trabalho cient√≠fico</a> . <br><br><h4>  Agradecimentos: </h4><br>  Este projeto √© o resultado de uma colabora√ß√£o entre a equipe do Google Brain, DeepMind, e a Escola T√©cnica Superior Su√≠√ßa de Zurique.  Grupo principal de pesquisa: Nikolay Savinov, Anton Raichuk, Rafael Marinier, Damien Vincent, Mark Pollefeys, Timothy Lillirap e Sylvain Geli.  Gostar√≠amos de agradecer a Olivier Pietkin, Carlos Riquelme, Charles Blundell e Sergey Levine pela discuss√£o deste documento.  Agradecemos a Indira Pasco pela ajuda com as ilustra√ß√µes. <br><br><h4>  Refer√™ncias √† literatura: </h4><br>  [1] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">‚ÄúO estudo do meio ambiente baseado na contagem com modelos de densidade neural‚Äù</a> , Georg Ostrovsky, Mark G. Bellemar, Aaron Van den Oord, Remy Munoz <br>  [2] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">‚ÄúAmbientes de aprendizagem baseados em</a> contagem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">para aprendizado profundo com refor√ßo‚Äù</a> , Khaoran Tan, Rain Huthuft, Davis Foot, Adam Knock, Si Chen, Yan Duan, John Schulman, Philip de Turk, Peter Abbel <br>  [3] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">‚ÄúAprendendo sem um professor para localizar metas para pesquisas motivadas internamente‚Äù,</a> Alexander Pere, Sebastien Forestier, Olivier Sigot, Pierre-Yves Udaye <br>  [4] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">‚ÄúVIME: intelig√™ncia para maximizar as altera√ß√µes de informa√ß√µes‚Äù,</a> Rein Huthuft, Xi Chen, Yan Duan, John Schulman, Philippe de Turk, Peter Abbel </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt427847/">https://habr.com/ru/post/pt427847/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt427837/index.html">Os primeiros dias na equipe de desenvolvimento - como acontece conosco</a></li>
<li><a href="../pt427839/index.html">Autoriza√ß√£o de usu√°rio no Django atrav√©s do GSSAPI e delega√ß√£o de direitos de usu√°rio no servidor</a></li>
<li><a href="../pt427841/index.html">Golpe de salto m√°gico</a></li>
<li><a href="../pt427843/index.html">Como dormir certo e errado</a></li>
<li><a href="../pt427845/index.html">Como encaixar um milh√£o de estrelas em um iPhone</a></li>
<li><a href="../pt427849/index.html">Linha reta com TM. v3.0</a></li>
<li><a href="../pt427853/index.html">Reflex√µes sobre TDD. Por que essa metodologia n√£o √© amplamente reconhecida</a></li>
<li><a href="../pt427855/index.html">Mitaps do MOSDROID no FunCorp</a></li>
<li><a href="../pt427857/index.html">Quest√µes fiscais e legais para freelancers iniciantes</a></li>
<li><a href="../pt427859/index.html">Por que habilidades t√©cnicas para o gerente de projeto: explique os casos</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>