<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëñ üë©‚Äçüé§ üë®üèΩ‚ÄçüöÄ Parsim 25TB con AWK y R üïõ üêê üïë</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="C√≥mo leer este art√≠culo : Pido disculpas por el hecho de que el texto result√≥ tan largo y ca√≥tico. Para ahorrarle tiempo, comienzo cada cap√≠tulo con l...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Parsim 25TB con AWK y R</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/456392/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/9d/3y/lc/9d3ylcjuqiv6r7vrv6p52apvmne.jpeg"></div><br>  <i><b>C√≥mo leer este art√≠culo</b> : Pido disculpas por el hecho de que el texto result√≥ tan largo y ca√≥tico.</i>  <i>Para ahorrarle tiempo, comienzo cada cap√≠tulo con la introducci√≥n de "Lo que aprend√≠", en el que explico la esencia del cap√≠tulo en una o dos oraciones.</i> <i><br><br></i>  <i><b>"¬°Solo muestra la soluci√≥n!"</b></i>  <i>Si solo quiere ver a qu√© he venido, vaya al cap√≠tulo "Sea m√°s inventivo", pero creo que es m√°s interesante y √∫til leer sobre los fracasos.</i> <br><br>  Recientemente, me dieron instrucciones de configurar un proceso para procesar un gran volumen de las secuencias de ADN originales (t√©cnicamente, este es un chip SNP).  Era necesario obtener r√°pidamente datos sobre una ubicaci√≥n gen√©tica dada (llamada SNP) para el modelado posterior y otras tareas.  Con la ayuda de R y AWK, pude limpiar y organizar los datos de forma natural, acelerando enormemente el procesamiento de las solicitudes.  Esto no fue f√°cil para m√≠ y requiri√≥ numerosas iteraciones.  Este art√≠culo te ayudar√° a evitar algunos de mis errores y a demostrar lo que hice al final. <br><a name="habracut"></a><br>  Primero, algunas explicaciones introductorias. <br><br><h2>  Datos </h2><br>  Nuestro Centro de Procesamiento de Informaci√≥n Gen√©tica de la Universidad nos ha proporcionado 25 TB de datos TSV.  Los divid√≠ en 5 paquetes comprimidos por Gzip, cada uno de los cuales conten√≠a unos 240 archivos de cuatro gigabytes.  Cada fila conten√≠a datos para un SNP de una persona.  En total, se transmitieron datos sobre ~ 2.5 millones de SNP y ~ 60 mil personas.  Adem√°s de la informaci√≥n de SNP, hab√≠a numerosas columnas en los archivos con n√∫meros que reflejaban diversas caracter√≠sticas, como la intensidad de lectura, la frecuencia de diferentes alelos, etc.  Hab√≠a alrededor de 30 columnas con valores √∫nicos. <br><br><h4>  Prop√≥sito </h4><br>  Al igual que con cualquier proyecto de gesti√≥n de datos, lo m√°s importante era determinar c√≥mo se utilizar√≠an los datos.  En este caso, <b>en su mayor parte, seleccionaremos modelos y flujos de trabajo para SNP basados ‚Äã‚Äãen SNP</b> .  Es decir, al mismo tiempo necesitaremos datos para un solo SNP.  Tuve que aprender a extraer todos los registros relacionados con uno de los 2.5 millones de SNP de la manera m√°s simple, r√°pida y econ√≥mica. <br><br><h1>  Como no hacerlo </h1><br>  Citar√© un clich√© adecuado: <br><br><blockquote>  No fall√© mil veces, solo descubr√≠ mil maneras de no analizar un mont√≥n de datos en un formato conveniente para consultas. </blockquote><br><h2>  Primer intento </h2><br>  <b>Lo que aprend√≠</b> : no hay una forma barata de analizar 25 TB a la vez. <br><br>  Despu√©s de escuchar el tema "M√©todos avanzados de procesamiento de Big Data" en la Universidad de Vanderbilt, estaba seguro de que era un sombrero.  Quiz√°s llevar√° una o dos horas configurar el servidor Hive para que ejecute todos los datos e informe sobre el resultado.  Dado que nuestros datos se almacenan en AWS S3, utilic√© el servicio <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Athena</a> , que le permite aplicar consultas Hive SQL a los datos S3.  No es necesario configurar / elevar el cl√∫ster Hive, e incluso pagar solo por los datos que est√° buscando. <br><br>  Despu√©s de mostrarle a Athena mis datos y su formato, realic√© algunas pruebas con consultas similares: <br><br><pre><code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">select</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> intensityData <span class="hljs-keyword"><span class="hljs-keyword">limit</span></span> <span class="hljs-number"><span class="hljs-number">10</span></span>;</code> </pre> <br>  Y r√°pidamente obtuvo resultados bien estructurados.  Listo <br><br>  Hasta que intentamos usar los datos en el trabajo ... <br><br>  Me pidieron que extrajera toda la informaci√≥n de SNP para probar el modelo.  Ejecut√© una consulta: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">select</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> intensityData <span class="hljs-keyword"><span class="hljs-keyword">where</span></span> snp = <span class="hljs-string"><span class="hljs-string">'rs123456'</span></span>;</code> </pre> <br>  ... y esper√©.  Despu√©s de ocho minutos y m√°s de 4 TB de los datos solicitados, obtuve el resultado.  Athena cobra una tarifa por la cantidad de datos encontrados, a $ 5 por terabyte.  Entonces, esta √∫nica solicitud cuesta $ 20 y ocho minutos de espera.  Para ejecutar el modelo de acuerdo con todos los datos, era necesario esperar 38 a√±os y pagar $ 50 millones. Obviamente, esto no nos conven√≠a. <br><br><h2>  Era necesario usar Parquet ... </h2><br>  <b>Lo que aprend√≠</b> : tenga cuidado con el tama√±o de sus archivos de Parquet y su organizaci√≥n. <br><br>  Al principio trat√© de corregir la situaci√≥n convirtiendo todos los TSV en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">archivos de Parquet</a> .  Son convenientes para trabajar con grandes conjuntos de datos, porque la informaci√≥n en ellos se almacena en forma de columnas: cada columna se encuentra en su propio segmento de memoria / disco, a diferencia de los archivos de texto en los que las l√≠neas contienen elementos de cada columna.  Y si necesita encontrar algo, simplemente lea la columna necesaria.  Adem√°s, se almacena un rango de valores en cada archivo en una columna, por lo que si el valor deseado no est√° en el rango de la columna, Spark no perder√° tiempo escaneando todo el archivo. <br><br>  Ejecut√© una simple tarea de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AWS Glue</a> para convertir nuestros TSV a Parquet y solt√© nuevos archivos en Athena.  Tom√≥ alrededor de 5 horas.  Pero cuando lanc√© la solicitud, me llev√≥ casi el mismo tiempo y un poco menos de dinero completarla.  El hecho es que Spark, tratando de optimizar la tarea, simplemente desempac√≥ un trozo de TSV y lo coloc√≥ en su propio trozo de Parquet.  Y dado que cada fragmento era lo suficientemente grande y conten√≠a los registros completos de muchas personas, todos los SNP se almacenaban en cada archivo, por lo que Spark tuvo que abrir todos los archivos para extraer la informaci√≥n necesaria. <br><br>  Curiosamente, el tipo de compresi√≥n predeterminado (y recomendado) en Parquet (r√°pido) no es divisible.  Por lo tanto, cada ejecutor se aferr√≥ a la tarea de desempaquetar y descargar el conjunto de datos completo de 3.5 GB. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f42/584/fb3/f42584fb3e65319eef46f117c11525f3.png"><br><h2>  Entendemos el problema </h2><br>  <b>Lo que aprend√≠</b> : la clasificaci√≥n es dif√≠cil, especialmente si los datos se distribuyen. <br><br>  Me pareci√≥ que ahora entend√≠a la esencia del problema.  Todo lo que ten√≠a que hacer era ordenar los datos por columna SNP, no por personas.  Luego, se almacenar√°n varios SNP en una porci√≥n de datos separada, y luego la funci√≥n inteligente Parquet "abrir√° solo si el valor est√° en el rango" se manifestar√° en todo su esplendor.  Desafortunadamente, clasificar miles de millones de filas dispersas en un cl√∫ster ha demostrado ser una tarea desalentadora. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-0" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1105127759318319105"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  AWS ciertamente no quiere devolver el dinero porque "soy un estudiante distra√≠do".  Despu√©s de que comenc√© a ordenar en Amazon Glue, funcion√≥ durante 2 d√≠as y se bloque√≥. <br><br><h2>  ¬øQu√© pasa con la partici√≥n? </h2><br>  <b>Lo que aprend√≠</b> : las particiones en Spark deben estar equilibradas. <br><br>  Entonces se me ocurri√≥ la idea de particionar los datos en los cromosomas.  Hay 23 de ellos (y algunos m√°s, dado el ADN mitocondrial y las √°reas no mapeadas). <br>  Esto le permitir√° dividir los datos en porciones m√°s peque√±as.  Si agrega una sola l√≠nea de <code>partition_by = "chr"</code> a la funci√≥n de exportaci√≥n de Spark en la secuencia de comandos de Glue, los datos deben clasificarse en cubos. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/652/f42/3dc/652f423dc8806401b6638a3cf8c1480b.png"><br>  <i>El genoma consta de numerosos fragmentos llamados cromosomas.</i> <br><br>  Lamentablemente, esto no funcion√≥.  Los cromosomas tienen diferentes tama√±os y, por lo tanto, una cantidad diferente de informaci√≥n.  Esto significa que las tareas que Spark envi√≥ a los trabajadores no estaban equilibradas y se realizaban lentamente, porque algunos nodos finalizaron antes y estaban inactivos.  Sin embargo, las tareas se completaron.  Pero al solicitar un SNP, el desequilibrio nuevamente caus√≥ problemas.  El costo de procesar SNP en cromosomas m√°s grandes (es decir, de d√≥nde queremos obtener los datos) disminuy√≥ solo unas 10 veces.  Mucho, pero no lo suficiente. <br><br><h2>  ¬øY si te divides en particiones a√∫n m√°s peque√±as? </h2><br>  <b>Lo que aprend√≠</b> : nunca intente hacer 2.5 millones de particiones. <br><br>  Decid√≠ dar un paseo y particion√© cada SNP.  Esto garantiz√≥ el mismo tama√±o de particiones.  <b>Mal fue una idea</b> .  Aprovech√© Glue y agregu√© la inocente <code>partition_by = 'snp'</code> .  La tarea comenz√≥ y comenz√≥ a ejecutarse.  Un d√≠a despu√©s, verifiqu√© y vi que hasta ahora no se hab√≠a escrito nada en S3, as√≠ que elimin√© la tarea.  Parece que Glue estaba escribiendo archivos intermedios en un lugar oculto en S3, y muchos archivos, tal vez un par de millones.  Como resultado, mi error cost√≥ m√°s de mil d√≥lares y no agrad√≥ a mi mentor. <br><br><h2>  Particionamiento + clasificaci√≥n </h2><br>  <b>Lo que aprend√≠</b> : la clasificaci√≥n sigue siendo dif√≠cil, como lo es configurar Spark. <br><br>  El √∫ltimo intento de particionamiento fue que particion√© los cromosomas y luego clasifiqu√© cada partici√≥n.  En teor√≠a, esto acelerar√≠a cada solicitud, porque los datos SNP deseados deber√≠an estar dentro de varios fragmentos de Parquet dentro de un rango determinado.  Por desgracia, ordenar incluso datos particionados ha resultado ser una tarea dif√≠cil.  Como resultado, cambi√© a EMR para un cl√∫ster personalizado y us√© ocho instancias potentes (C5.4xl) y Sparklyr para crear un flujo de trabajo m√°s flexible ... <br><br><pre> <code class="scala hljs"># <span class="hljs-type"><span class="hljs-type">Sparklyr</span></span> snippet to partition by chr and sort w/in partition # <span class="hljs-type"><span class="hljs-type">Join</span></span> the raw data <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> the snp bins raw_data group_by(chr) %&gt;% arrange(<span class="hljs-type"><span class="hljs-type">Position</span></span>) %&gt;% <span class="hljs-type"><span class="hljs-type">Spark_write_Parquet</span></span>( path = <span class="hljs-type"><span class="hljs-type">DUMP_LOC</span></span>, mode = <span class="hljs-symbol"><span class="hljs-symbol">'overwrit</span></span>e', partition_by = c(<span class="hljs-symbol"><span class="hljs-symbol">'ch</span></span>r') )</code> </pre> <br>  ... sin embargo, la tarea a√∫n no se complet√≥.  Me sintonic√© en todos los sentidos: aument√© la asignaci√≥n de memoria para cada ejecutor de consultas, us√© nodos con una gran cantidad de memoria, utilic√© variables de transmisi√≥n, pero cada vez result√≥ ser medias tintas, y gradualmente los artistas comenzaron a fallar, hasta que todo se detuvo. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-1" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1128703858610450434"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br><h1>  Me estoy volviendo m√°s inventivo </h1><br>  <b>Lo que aprend√≠</b> : a veces los datos especiales requieren soluciones especiales. <br><br>  Cada SNP tiene un valor de posici√≥n.  Este es el n√∫mero correspondiente al n√∫mero de bases que se encuentran a lo largo de su cromosoma.  Esta es una forma buena y natural de organizar nuestros datos.  Al principio quer√≠a particionar por regi√≥n de cada cromosoma.  Por ejemplo, puestos 1 - 2000, 2001 - 4000, etc.  Pero el problema es que los SNP no se distribuyen uniformemente entre los cromosomas, por lo que el tama√±o de los grupos variar√° enormemente. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f46/a8e/17b/f46a8e17b9af8d2ae9777c47017764c6.png"><br><br>  Como resultado, llegu√© a dividirme en posiciones de categor√≠as (rango).  Seg√∫n los datos ya descargados, solicit√© una lista de SNP √∫nicos, sus posiciones y cromosomas.  Luego clasific√≥ los datos dentro de cada cromosoma y recolect√≥ SNP en grupos (bin) de un tama√±o dado.  Diga 1000 SNP cada uno.  Esto me dio una relaci√≥n SNP con un grupo en cromosoma. <br><br>  Al final, hice grupos (bin) en 75 SNP, explicar√© la raz√≥n a continuaci√≥n. <br><br><pre> <code class="bash hljs">snp_to_bin &lt;- unique_snps %&gt;% group_by(chr) %&gt;% arrange(position) %&gt;% mutate( rank = 1:n() bin = floor(rank/snps_per_bin) ) %&gt;% ungroup()</code> </pre> <br><h2>  Primero prueba con Spark </h2><br>  <b>Lo que aprend√≠</b> : la integraci√≥n de Spark es r√°pida, pero la partici√≥n sigue siendo costosa. <br><br>  Quer√≠a leer este peque√±o marco de datos (2,5 millones de l√≠neas) en Spark, combinarlo con datos sin procesar y luego particionar por la columna <code>bin</code> reci√©n agregada. <br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment"># Join the raw data with the snp bins data_w_bin &lt;- raw_data %&gt;% left_join(sdf_broadcast(snp_to_bin), by ='snp_name') %&gt;% group_by(chr_bin) %&gt;% arrange(Position) %&gt;% Spark_write_Parquet( path = DUMP_LOC, mode = 'overwrite', partition_by = c('chr_bin') )</span></span></code> </pre> <br>  <code>sdf_broadcast()</code> , por lo que Spark descubre que deber√≠a enviar un marco de datos a todos los nodos.  Esto es √∫til si los datos son peque√±os y necesarios para todas las tareas.  De lo contrario, Spark intenta ser inteligente y distribuye los datos seg√∫n sea necesario, lo que puede causar frenos. <br><br>  Y nuevamente, mi idea no funcion√≥: las tareas funcionaron por un tiempo, completaron la fusi√≥n y luego, como los ejecutores iniciados por la partici√≥n, comenzaron a fallar. <br><br><h2>  A√±adir AWK </h2><br>  <b>Lo que aprend√≠</b> : no duermas cuando lo b√°sico te ense√±a.  Seguramente alguien ya resolvi√≥ su problema en la d√©cada de 1980. <br><br>  Hasta este punto, la causa de todas mis fallas con Spark fue la confusi√≥n de datos en el cl√∫ster.  Quiz√°s la situaci√≥n pueda mejorarse mediante el preprocesamiento.  Decid√≠ tratar de dividir los datos de texto sin procesar en columnas cromos√≥micas, por lo que esperaba proporcionar a Spark datos "pre-particionados". <br><br>  Busqu√© en StackOverflow c√≥mo desglosar los valores de columna y encontr√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">una gran respuesta.</a>  Con AWK, puede dividir un archivo de texto en valores de columna escribiendo en el script, en lugar de enviar los resultados a <code>stdout</code> . <br><br>  Para probar, escrib√≠ un script Bash.  Descargu√© uno de los TSV empaquetados, luego lo descomprim√≠ con <code>gzip</code> y lo envi√© a <code>awk</code> . <br><br><pre> <code class="bash hljs">gzip -dc path/to/chunk/file.gz | awk -F <span class="hljs-string"><span class="hljs-string">'\t'</span></span> \ <span class="hljs-string"><span class="hljs-string">'{print $1",..."$30"&gt;"chunked/"$chr"_chr"$15".csv"}'</span></span></code> </pre> <br>  Funcion√≥! <br><br><h2>  Relleno del n√∫cleo </h2><br>  <b>Lo que aprend√≠</b> : <code>gnu parallel</code> es algo m√°gico, todos deber√≠an usarlo. <br><br>  La separaci√≥n fue bastante lenta, y cuando comenc√© a probar para usar una instancia EC2 potente (y costosa), result√≥ que estaba usando solo un n√∫cleo y aproximadamente 200 MB de memoria.  Para resolver el problema y no perder mucho dinero, fue necesario descubrir c√≥mo paralelizar el trabajo.  Afortunadamente, en el sorprendente <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Data Science de</a> Jeron Janssens <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">en el</a> libro de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">L√≠nea de Comando</a> , encontr√© un cap√≠tulo sobre paralelizaci√≥n.  De √©l aprend√≠ sobre <code>gnu parallel</code> , un m√©todo muy flexible para implementar multihilo en Unix. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/835/7c0/e45/8357c0e45f4162d53ca1c3da0c78444a.png" width="300"></div><br>  Cuando comenc√© la partici√≥n usando un nuevo proceso, todo estaba bien, pero hab√≠a un cuello de botella: la descarga de objetos S3 al disco no era demasiado r√°pida y no estaba completamente paralela.  Para solucionar esto, hice esto: <br><br><ol><li>  Descubr√≠ que es posible implementar el paso de descarga S3 directamente en la tuber√≠a, eliminando por completo el almacenamiento intermedio en el disco.  Esto significa que puedo evitar escribir datos en bruto en el disco y usar un almacenamiento a√∫n m√°s peque√±o y, por lo tanto, m√°s barato en AWS. <br></li><li>  El comando <code>aws configure set default.s3.max_concurrent_requests 50</code> aument√≥ en gran medida el n√∫mero de subprocesos que utiliza la AWS CLI (hay 10 por defecto). <br></li><li>  Cambi√© a la instancia EC2 optimizada para la velocidad de la red, con la letra n en el nombre.  Descubr√≠ que la p√©rdida de potencia inform√°tica cuando se usan n-instancias est√° m√°s que compensada por un aumento en la velocidad de descarga.  Para la mayor√≠a de las tareas, us√© c5n.4xl. <br></li><li>  Cambi√© <code>gzip</code> a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><code>pigz</code></a> , esta es una herramienta gzip que puede hacer cosas geniales para paralelizar la tarea inicialmente incomparable de desempaquetar archivos (esto ayud√≥ menos). <br></li></ol><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Let S3 use as many threads as it wants aws configure set default.s3.max_concurrent_requests 50 for chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do aws s3 cp s3://$batch_loc$chunk_file - | pigz -dc | parallel --block 100M --pipe \ "awk -F '\t' '{print \$1\",...\"$30\"&gt;\"chunked/{#}_chr\"\$15\".csv\"}'" # Combine all the parallel process chunks to single files ls chunked/ | cut -d '_' -f 2 | sort -u | parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}' # Clean up intermediate data rm chunked/* done</span></span></code> </pre> <br>  Estos pasos se combinan entre s√≠ para que todo funcione muy r√°pidamente.  Gracias a la mayor velocidad de descarga y al rechazo de la escritura en el disco, ahora podr√≠a procesar un paquete de 5 terabytes en solo unas pocas horas. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-2" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1129416944233226240"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  Se supon√≠a que este tweet mencionaba 'TSV'.  Por desgracia <br><br><h2>  Usar datos analizados nuevamente </h2><br>  <b>Lo que aprend√≠</b> : a Spark le encantan los datos sin comprimir y no le gusta combinar particiones. <br><br>  Ahora los datos estaban en S3 en un formato desempaquetado (le√≠do, compartido) y semi-ordenado, y pude volver a Spark nuevamente.  Me esperaba una sorpresa: ¬°otra vez no pude lograr lo deseado!  Fue muy dif√≠cil decirle a Spark exactamente c√≥mo se dividieron los datos.  E incluso cuando hice esto, result√≥ que hab√≠a demasiadas particiones (95 mil), y cuando reduje su n√∫mero a l√≠mites coherentes con la <code>coalesce</code> , arruin√≥ mi partici√≥n.  Estoy seguro de que esto se puede solucionar, pero en un par de d√≠as de b√∫squeda, no pude encontrar una soluci√≥n.  Al final, complet√© todas las tareas en Spark, aunque me llev√≥ algo de tiempo, y mis archivos de Parquet divididos no eran muy peque√±os (~ 200 Kb).  Sin embargo, los datos estaban donde se necesitaban. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ae5/43b/236/ae543b236b8d37d4a6794aa63d9ada94.png"><br>  <i>Demasiado peque√±o y diferente, maravilloso!</i> <br><br><h2>  Probar solicitudes locales de Spark </h2><br>  <b>Lo que aprend√≠</b> : Spark tiene demasiados gastos generales para resolver problemas simples. <br><br>  Al descargar los datos en un formato inteligente, pude probar la velocidad.  Configur√© un script en R para iniciar el servidor local de Spark, y luego cargu√© el marco de datos de Spark desde el repositorio especificado de los grupos de Parquet (bin).  Trat√© de cargar todos los datos, pero no pude hacer que Sparklyr reconociera la partici√≥n. <br><br><pre> <code class="scala hljs">sc &lt;- <span class="hljs-type"><span class="hljs-type">Spark_connect</span></span>(master = <span class="hljs-string"><span class="hljs-string">"local"</span></span>) desired_snp &lt;- <span class="hljs-symbol"><span class="hljs-symbol">'rs3477173</span></span>9' # <span class="hljs-type"><span class="hljs-type">Start</span></span> a timer start_time &lt;- <span class="hljs-type"><span class="hljs-type">Sys</span></span>.time() # <span class="hljs-type"><span class="hljs-type">Load</span></span> the desired bin into <span class="hljs-type"><span class="hljs-type">Spark</span></span> intensity_data &lt;- sc %&gt;% <span class="hljs-type"><span class="hljs-type">Spark_read_Parquet</span></span>( name = <span class="hljs-symbol"><span class="hljs-symbol">'intensity_dat</span></span>a', path = get_snp_location(desired_snp), memory = <span class="hljs-type"><span class="hljs-type">FALSE</span></span> ) # <span class="hljs-type"><span class="hljs-type">Subset</span></span> bin to snp and then collect to local test_subset &lt;- intensity_data %&gt;% filter(<span class="hljs-type"><span class="hljs-type">SNP_Name</span></span> == desired_snp) %&gt;% collect() print(<span class="hljs-type"><span class="hljs-type">Sys</span></span>.time() - start_time)</code> </pre> <br>  La ejecuci√≥n tom√≥ 29.415 segundos.  Mucho mejor, pero no demasiado bueno para probar algo en masa.  Adem√°s, no pude acelerar el trabajo con el almacenamiento en cach√©, porque cuando intent√© almacenar en cach√© el marco de datos en la memoria, Spark siempre se bloqueaba, incluso cuando asignaba m√°s de 50 GB de memoria para un conjunto de datos que pesaba menos de 15. <br><br><h2>  Regresar a AWK </h2><br>  <b>Lo que aprend√≠</b> : los arrays asociativos de AWK son muy eficientes. <br><br>  Comprend√≠ que pod√≠a lograr una mayor velocidad.  Record√© que en la excelente gu√≠a AWK de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Bruce Barnett</a> , le√≠ sobre una caracter√≠stica interesante llamada " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">matrices asociativas</a> ".  De hecho, estos son pares clave-valor, que por alguna raz√≥n se llamaron de manera diferente en AWK y, por lo tanto, de alguna manera no los mencion√© en particular.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Roman Cheplyaka</a> record√≥ que el t√©rmino "matrices asociativas" es mucho m√°s antiguo que el t√©rmino "par clave-valor".  Incluso si <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">busca valor-clave en Google Ngram</a> , no ver√° este t√©rmino all√≠, ¬°pero encontrar√° matrices asociativas!  Adem√°s, el par clave-valor se asocia con mayor frecuencia a las bases de datos, por lo que es mucho m√°s l√≥gico compararlo con el hashmap.  Me di cuenta de que pod√≠a usar estas matrices asociativas para conectar mis SNP a la tabla bin y los datos sin procesar sin usar Spark. <br><br>  Para esto, en el script AWK, us√© el bloque <code>BEGIN</code> .  Este es un fragmento de c√≥digo que se ejecuta antes de que la primera l√≠nea de datos se transfiera al cuerpo principal del script. <br><br><pre> <code class="cpp hljs">join_data.awk BEGIN { FS=<span class="hljs-string"><span class="hljs-string">","</span></span>; batch_num=substr(chunk,<span class="hljs-number"><span class="hljs-number">7</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>); chunk_id=substr(chunk,<span class="hljs-number"><span class="hljs-number">15</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">while</span></span>(getline &lt; <span class="hljs-string"><span class="hljs-string">"snp_to_bin.csv"</span></span>) {bin[$<span class="hljs-number"><span class="hljs-number">1</span></span>] = $<span class="hljs-number"><span class="hljs-number">2</span></span>} } { print $<span class="hljs-number"><span class="hljs-number">0</span></span> &gt; <span class="hljs-string"><span class="hljs-string">"chunked/chr_"</span></span>chr<span class="hljs-string"><span class="hljs-string">"_bin_"</span></span>bin[$<span class="hljs-number"><span class="hljs-number">1</span></span>]<span class="hljs-string"><span class="hljs-string">"_"</span></span>batch_num<span class="hljs-string"><span class="hljs-string">"_"</span></span>chunk_id<span class="hljs-string"><span class="hljs-string">".csv"</span></span> }</code> </pre> <br>  El comando <code>while(getline...)</code> carg√≥ todas las l√≠neas del grupo CSV (bin), estableci√≥ la primera columna (nombre SNP) como la clave para la matriz asociativa <code>bin</code> y el segundo valor (grupo) como el valor.  Luego, en el bloque <code>{</code> <code>}</code> , que se aplica a todas las l√≠neas del archivo principal, cada l√≠nea se env√≠a al archivo de salida, que obtiene un nombre √∫nico seg√∫n su grupo (bin): <code>..._bin_"bin[$1]"_...</code> <br><br>  Las <code>chunk_id</code> <code>batch_num</code> y <code>chunk_id</code> correspondieron a los datos proporcionados por la canalizaci√≥n, lo que evit√≥ el estado de la carrera, y cada hilo de ejecuci√≥n lanzado por <code>parallel</code> escribi√≥ en su propio archivo √∫nico. <br><br>  Dado que dispers√© todos los datos sin procesar en carpetas en los cromosomas que quedaron despu√©s de mi experimento anterior con AWK, ahora podr√≠a escribir otro script Bash para procesar en el cromosoma a la vez y dar datos particionados m√°s profundos a S3. <br><br><pre> <code class="bash hljs">DESIRED_CHR=<span class="hljs-string"><span class="hljs-string">'13'</span></span> <span class="hljs-comment"><span class="hljs-comment"># Download chromosome data from s3 and split into bins aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv' | parallel "echo 'reading {}'; aws s3 cp "$DATA_LOC"{} - | awk -v chr=\""$DESIRED_CHR"\" -v chunk=\"{}\" -f split_on_chr_bin.awk" # Combine all the parallel process chunks to single files and upload to rds using R ls chunked/ | cut -d '_' -f 4 | sort -u | parallel "echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds" rm chunked/*</span></span></code> </pre> <br>  El gui√≥n tiene dos secciones <code>parallel</code> . <br><br>  La primera secci√≥n lee los datos de todos los archivos que contienen informaci√≥n sobre el cromosoma deseado, luego estos datos se distribuyen a trav√©s de secuencias que dispersan los archivos en los grupos correspondientes (bin).  Para evitar que se produzcan condiciones de carrera cuando se escriben varias transmisiones en el mismo archivo, AWK transfiere los nombres de archivo para escribir datos en diferentes lugares, por ejemplo, <code>chr_10_bin_52_batch_2_aa.csv</code> .  Como resultado, se crean muchos archivos peque√±os en el disco (para esto utilic√© vol√∫menes EBS de terabytes). <br><br>  La canalizaci√≥n de la segunda secci√≥n <code>parallel</code> pasa por los grupos (bin) y combina sus archivos individuales en CSV comunes con <code>cat</code> , y luego los env√≠a para su exportaci√≥n. <br><br><h2>  Transmitir a R? </h2><br>  <b>Lo que aprend√≠</b> : puede acceder a <code>stdin</code> y <code>stdout</code> desde un script R y, por lo tanto, usarlo en la tuber√≠a. <br><br>  En la secuencia de comandos Bash, puede observar esta l√≠nea: <code>...cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R...</code>  <code>...cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R...</code>  Traduce todos los archivos de grupo concatenados (bin) en el script R a continuaci√≥n.  <code>{}</code> es una t√©cnica <code>parallel</code> especial que inserta cualquier dato enviado por √©l en la secuencia especificada directamente en el comando mismo.  La opci√≥n <code>{#}</code> proporciona una ID de subproceso √∫nica y <code>{%}</code> representa el n√∫mero de espacio de trabajo (repetido, pero nunca al mismo tiempo).  Se puede encontrar una lista de todas las opciones en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentaci√≥n.</a> <br><br><pre> <code class="lisp hljs"><span class="hljs-meta"><span class="hljs-meta">#!/usr/bin/env Rscript library(readr) library(aws.s3) # Read first command line argument data_destination &lt;- commandArgs(trailingOnly = TRUE)[1] data_cols &lt;- list(SNP_Name = 'c', ...) s3saveRDS( read_csv( file("stdin"), col_names = names(data_cols), col_types = data_cols ), object = data_destination )</span></span></code> </pre> <br>  Cuando la variable del <code>file("stdin")</code> se pasa a <code>readr::read_csv</code> , los datos traducidos en el script R se cargan en el marco, que luego se escribe directamente en S3 como un archivo <code>aws.s3</code> usando <code>aws.s3</code> . <br><br>  RDS es un poco como una versi√≥n m√°s joven de Parquet, sin los lujos del almacenamiento de columnas. <br><br>  Despu√©s de completar el script Bash, recib√≠ un <code>.rds</code> archivos <code>.rds</code> en S3, lo que me permiti√≥ usar una compresi√≥n eficiente y tipos incorporados. <br><br>  A pesar de usar el freno R, todo funcion√≥ muy r√°pido.  No es sorprendente que los fragmentos en R que son responsables de leer y escribir datos est√©n bien optimizados.  Despu√©s de probar en un cromosoma de tama√±o mediano, la tarea se complet√≥ en la instancia C5n.4xl en aproximadamente dos horas. <br><br><h2>  Limitaciones de S3 </h2><br>  <b>Lo que aprend√≠</b> : gracias a la implementaci√≥n inteligente de rutas, S3 puede procesar muchos archivos. <br><br>  Me preocupaba si S3 podr√≠a manejar muchos archivos transferidos a √©l.  Podr√≠a hacer que los nombres de los archivos tengan sentido, pero ¬øc√≥mo los buscar√° S3? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/841/0dc/c34/8410dcc34a563c683dd7602dc66d884a.png"><br> <i>  S3    ,        <code>/</code> . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> FAQ- S3.</a></i> <br><br> , S3            -      .  (bucket)   ,   ‚Äî    . <br><br>          Amazon, ,    ¬´-----¬ª  .    :       get-,       . ,      20 . bin-. ,   ,      (,      ,      ).          . <br><br><h2>    ? </h2><br>   :     ‚Äî     . <br><br>       : ¬´    ?¬ª      ( gzip CSV-   7  )      .     ,  R     Parquet ( Arrow)     Spark.       R,         ,         ,       . <br><br><h2>   </h2><br> <b>  </b> :     ,    . <br><br>       ,      . <br>     EC2  ,                 ( ,  Spark    ).  ,          ,    AWS-      10 . <br><br>      R      . <br><br>   S3 ,       . <br><br><pre> <code class="bash hljs">library(aws.s3) library(tidyverse) chr_sizes &lt;- get_bucket_df( bucket = <span class="hljs-string"><span class="hljs-string">'...'</span></span>, prefix = <span class="hljs-string"><span class="hljs-string">'...'</span></span>, max = Inf ) %&gt;% mutate(Size = as.numeric(Size)) %&gt;% filter(Size != 0) %&gt;% mutate( <span class="hljs-comment"><span class="hljs-comment"># Extract chromosome from the file name chr = str_extract(Key, 'chr.{1,4}\\.csv') %&gt;% str_remove_all('chr|\\.csv') ) %&gt;% group_by(chr) %&gt;% summarise(total_size = sum(Size)/1e+9) # Divide to get value in GB # A tibble: 27 x 2 chr total_size &lt;chr&gt; &lt;dbl&gt; 1 0 163. 2 1 967. 3 10 541. 4 11 611. 5 12 542. 6 13 364. 7 14 375. 8 15 372. 9 16 434. 10 17 443. # ‚Ä¶ with 17 more rows</span></span></code> </pre> <br>    ,    ,   ,     <code>num_jobs</code>  ,       . <br><br><pre> <code class="bash hljs">num_jobs &lt;- 7 <span class="hljs-comment"><span class="hljs-comment"># How big would each job be if perfectly split? job_size &lt;- sum(chr_sizes$total_size)/7 shuffle_job &lt;- function(i){ chr_sizes %&gt;% sample_frac() %&gt;% mutate( cum_size = cumsum(total_size), job_num = ceiling(cum_size/job_size) ) %&gt;% group_by(job_num) %&gt;% summarise( job_chrs = paste(chr, collapse = ','), total_job_size = sum(total_size) ) %&gt;% mutate(sd = sd(total_job_size)) %&gt;% nest(-sd) } shuffle_job(1) # A tibble: 1 x 2 sd data &lt;dbl&gt; &lt;list&gt; 1 153. &lt;tibble [7 √ó 3]&gt;</span></span></code> </pre> <br>      purrr     . <br><br><pre> <code class="bash hljs">1:1000 %&gt;% map_df(shuffle_job) %&gt;% filter(sd == min(sd)) %&gt;% pull(data) %&gt;% pluck(1)</code> </pre> <br>     ,    .       Bash-    <code>for</code> .       10 .    ,             .  ,        . <br><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> DESIRED_CHR <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-string"><span class="hljs-string">"16"</span></span> <span class="hljs-string"><span class="hljs-string">"9"</span></span> <span class="hljs-string"><span class="hljs-string">"7"</span></span> <span class="hljs-string"><span class="hljs-string">"21"</span></span> <span class="hljs-string"><span class="hljs-string">"MT"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-comment"><span class="hljs-comment"># Code for processing a single chromosome fi</span></span></code> </pre> <br>     : <br><br><pre> <code class="bash hljs">sudo shutdown -h now</code> </pre> <br> ‚Ä¶   !   AWS CLI       <code>user_data</code>   Bash-    .     ,         . <br><br><pre> <code class="bash hljs">aws ec2 run-instances ...\ --tag-specifications <span class="hljs-string"><span class="hljs-string">"ResourceType=instance,Tags=[{Key=Name,Value=&lt;&lt;job_name&gt;&gt;}]"</span></span> \ --user-data file://&lt;&lt;job_script_loc&gt;&gt;</code> </pre> <br><h1> ! </h1><br> <b>  </b> : API        . <br><br> -        .      ,     .     API   .        <code>.rds</code>  Parquet-,       ,    .       R-. <br><br>      ,        ,    <code>get_snp</code> .       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">pkgdown</a> ,        . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a75/afb/f3a/a75afbf3a2c7c8ef5fa2a873f8ba50b9.png"><br><br><h2>   </h2><br> <b>  </b> :     ,   ! <br><br>          SNP      ,     (binning)   .     SNP,          (bin).      ( )    . <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Part of get_snp() ... # Test if our current snp data has the desired snp. already_have_snp &lt;- desired_snp %in% prev_snp_results$snps_in_bin if(!already_have_snp){ # Grab info on the bin of the desired snp snp_results &lt;- get_snp_bin(desired_snp) # Download the snp's bin data snp_results$bin_data &lt;- aws.s3::s3readRDS(object = snp_results$data_loc) } else { # The previous snp data contained the right bin so just use it snp_results &lt;- prev_snp_results } ...</span></span></code> </pre> <br>       ,       .    ,      . , <code>dplyr::filter</code>           ,           ,    . <br><br>  ,   <code>prev_snp_results</code>   <code>snps_in_bin</code> .     SNP   (bin),   ,       .        SNP   (bin)    : <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Get bin-mates snps_in_bin &lt;- my_snp_results$snps_in_bin for(current_snp in snps_in_bin){ my_snp_results &lt;- get_snp(current_snp, my_snp_results) # Do something with results }</span></span></code> </pre> <br><h1>  Resultados </h1><br>    (  )    ,   .  ,           .      . <br><br>       ,       ,     ,     ‚Ä¶ <br><br>   .       .       (  ),  ,   (bin)   ,    SNP     0,1 ,     ,     S3 . <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-3" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1134151057385369600"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br><h2>  Conclusi√≥n </h2><br>   ‚Äî   .   ,     . ,    . ,   ,         ,     .  ,       ,  ,        ,    .  ,       ,    ,        ,      -     . <br><br>     .     ,        ,  ¬´¬ª  ,    .          . <br><br><h3>   : </h3><br><ul><li>      25   ; <br></li><li>      Parquet-   ; <br></li><li>   Spark   ; <br></li><li>      2,5  ; <br></li><li>    ,    Spark; <br></li><li>      ; <br></li><li>   Spark  ,      ; <br></li><li>  ,    ,  -       1980-; <br></li><li> <code>gnu parallel</code> ‚Äî   ,    ; <br></li><li> Spark        ; <br></li><li>  Spark        ; <br></li><li>    AWK  ; <br></li><li>    <code>stdin</code>  <code>stdout</code>  R-,       ; <br></li><li>     S3    ; <br></li><li>     ‚Äî     ; <br></li><li>     ,    ; <br></li><li> API        ; <br></li><li>     ,   ! <br></li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/456392/">https://habr.com/ru/post/456392/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../456380/index.html">Jornada de puertas abiertas de la Facultad de Programaci√≥n en Netolog√≠a</a></li>
<li><a href="../456382/index.html">Colaboraci√≥n y automatizaci√≥n en la interfaz. Lo que aprendimos de 13 escuelas</a></li>
<li><a href="../456384/index.html">PVS-Studio Gr√°fico de desarrollo de habilidades de diagn√≥stico</a></li>
<li><a href="../456386/index.html">Bibliotecas abiertas para la visualizaci√≥n de contenido de audio.</a></li>
<li><a href="../456388/index.html">Cuadro de desarrollo de diagn√≥stico en PVS-Studio</a></li>
<li><a href="../456394/index.html">Hacer la ubicua pantalla de bienvenida en iOS</a></li>
<li><a href="../456398/index.html">Complementos Vue-cli, trabajando con datos complejos y pruebas basadas en propiedades - anuncio Frontend de Panda-Meetup</a></li>
<li><a href="../456400/index.html">Por qu√© competir es mejor que abarrotar: nuestra experiencia de aprendizaje de gamificaci√≥n</a></li>
<li><a href="../456402/index.html">Dientes de Sabidur√≠a: Pull-Pull</a></li>
<li><a href="../456404/index.html">Looper - Plugin para Sketch</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>