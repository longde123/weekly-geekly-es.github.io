<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚õ≤Ô∏è üë®üèø‚Äçüé® üèûÔ∏è L'√©volution de l'interaction de cluster. Comment nous avons impl√©ment√© ActiveMQ et Hazelcast ü§æüèΩ üòπ üì™</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Au cours des 7 derni√®res ann√©es, avec mon √©quipe, j'ai soutenu et d√©velopp√© le c≈ìur du produit Miro (ex-RealtimeBoard): interaction client-serveur et ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>L'√©volution de l'interaction de cluster. Comment nous avons impl√©ment√© ActiveMQ et Hazelcast</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/miro/blog/441590/">  Au cours des 7 derni√®res ann√©es, avec mon √©quipe, j'ai soutenu et d√©velopp√© le c≈ìur du produit Miro (ex-RealtimeBoard): interaction client-serveur et cluster, en collaboration avec la base de donn√©es. <br><br>  Nous avons Java avec diff√©rentes biblioth√®ques √† bord.  Tout est lanc√© en dehors du conteneur, via le plugin Maven.  Il s'appuie sur la plateforme de nos partenaires, ce qui nous permet de travailler avec la base de donn√©es et les flux, de g√©rer l'interaction client-serveur, etc.  DB - Redis et PostgreSQL (mon coll√®gue a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">√©crit sur la fa√ßon dont nous passons d'une base de donn√©es √† une autre</a> ). <br><br>  En termes de logique m√©tier, l'application contient: <br><br><ul><li>  travailler avec des tableaux personnalis√©s et leur contenu; </li><li>  fonctionnalit√© d'enregistrement des utilisateurs, de cr√©ation et de gestion de tableaux; </li><li>  g√©n√©rateur de ressources personnalis√©.  Par exemple, il optimise les grandes images t√©l√©charg√©es vers l'application afin qu'elles ne ralentissent pas sur nos clients; </li><li> de nombreuses int√©grations avec des services tiers. </li></ul><br>  En 2011, alors que nous venions de commencer, tout Miro √©tait sur le m√™me serveur.  Tout y √©tait: Nginx sur lequel tournait php pour un site, une application Java et des bases de donn√©es. <br><br>  Le produit d√©velopp√©, le nombre d'utilisateurs et le contenu qu'ils ont ajout√© aux cartes ont augment√©, de sorte que la charge sur le serveur a √©galement augment√©.  En raison du grand nombre d'applications sur notre serveur, √† ce moment-l√†, nous ne pouvions pas comprendre ce qui donne exactement la charge et, par cons√©quent, nous ne pouvions pas l'optimiser. Pour r√©soudre ce probl√®me, nous avons tout divis√© en diff√©rents serveurs, et nous avons obtenu un serveur Web, un serveur avec notre serveur d'applications et de bases de donn√©es. <br><br>  Malheureusement, apr√®s un certain temps, des probl√®mes sont survenus √† nouveau, car la charge sur l'application a continu√© de cro√Ætre.  Nous avons ensuite r√©fl√©chi √† la mani√®re de dimensionner l'infrastructure. <br><br><img src="https://habrastorage.org/webt/_5/zq/_3/_5zq_3c16pydjklapiqamfzyxcg.png"><br><br>  Ensuite, je parlerai des difficult√©s que nous avons rencontr√©es dans le d√©veloppement de clusters et la mise √† l'√©chelle des applications et de l'infrastructure Java. <a name="habracut"></a><br><br><h2>  Mettre l'infrastructure √† l'√©chelle horizontalement </h2><br>  Nous avons commenc√© par collecter des m√©triques: l'utilisation de la m√©moire et du CPU, le temps n√©cessaire pour ex√©cuter les requ√™tes des utilisateurs, l'utilisation des ressources syst√®me et le travail avec la base de donn√©es.  D'apr√®s les mesures, il √©tait clair que la g√©n√©ration de ressources utilisateur √©tait un processus impr√©visible.  Nous pouvons charger le processeur √† 100% et attendre des dizaines de secondes jusqu'√† ce que tout soit fait.  Les demandes des utilisateurs pour les tableaux donnaient parfois une charge inattendue.  Par exemple, lorsqu'un utilisateur s√©lectionne un millier de widgets et commence √† les d√©placer spontan√©ment. <br><br>  Nous avons commenc√© √† r√©fl√©chir √† la mani√®re de mettre √† l'√©chelle ces parties du syst√®me et sommes parvenus √† des solutions √©videntes. <br><br>  <b>Travail √† grande √©chelle avec des tableaux et du contenu</b> .  L'utilisateur ouvre la carte comme ceci: l'utilisateur ouvre le client ‚Üí indique la carte qu'il veut ouvrir ‚Üí se connecte au serveur ‚Üí un flux est cr√©√© sur le serveur ‚Üí tous les utilisateurs de cette carte se connectent √† un flux ‚Üí toute modification ou cr√©ation du widget se produit dans ce flux.  Il s'av√®re que tout travail avec la carte est strictement limit√© par le flux, ce qui signifie que nous pouvons r√©partir ces flux entre les serveurs. <br><br>  <b>Faites √©voluer la g√©n√©ration de ressources utilisateur</b> .  Nous pouvons supprimer le serveur pour g√©n√©rer des ressources s√©par√©ment, et il recevra des messages pour la g√©n√©ration, puis r√©pondra que tout est g√©n√©r√©. <br><br>  Tout semble simple.  Mais d√®s que nous avons commenc√© √† √©tudier ce sujet plus en profondeur, il s'est av√©r√© que nous devions en outre r√©soudre certains probl√®mes indirects.  Par exemple, si les utilisateurs expirent un abonnement payant, nous devons les en informer, quel que soit le forum sur lequel ils se trouvent.  Ou, si l'utilisateur a mis √† jour la version de la ressource, vous devez vous assurer que le cache est correctement vid√© sur tous les serveurs et que nous fournissons la bonne version. <br><br>  Nous avons identifi√© la configuration syst√®me requise.  L'√©tape suivante consiste √† comprendre comment mettre cela en pratique.  En fait, nous avions besoin d'un syst√®me qui permettrait aux serveurs du cluster de communiquer entre eux et sur la base duquel nous r√©aliserions toutes nos id√©es. <br><br><h2>  Le premier cluster hors de la bo√Æte </h2><br>  Nous n'avons pas s√©lectionn√© la premi√®re version du syst√®me, car elle √©tait d√©j√† partiellement impl√©ment√©e dans la plateforme partenaire que nous avons utilis√©e.  Dans ce document, tous les serveurs √©taient connect√©s les uns aux autres via TCP, et en utilisant cette connexion, nous pouvions envoyer des messages RPC √† un ou √† tous les serveurs √† la fois. <br><br>  Par exemple, nous avons trois serveurs, ils sont connect√©s les uns aux autres via TCP, et dans Redis, nous avons une liste de ces serveurs.  Nous d√©marrons un nouveau serveur dans le cluster ‚Üí il s'ajoute √† la liste dans Redis ‚Üí lit la liste pour conna√Ætre tous les serveurs du cluster ‚Üí se connecte √† tous. <br><br><img src="https://habrastorage.org/webt/yj/9c/hv/yj9chvfavcbrixqnn_12jho2k7k.png"><br><br>  Bas√© sur RPC, la prise en charge du vidage du cache et de la redirection des utilisateurs vers le serveur souhait√© a d√©j√† √©t√© mise en ≈ìuvre.  Nous avons d√ª faire une g√©n√©ration de ressources utilisateur et informer les utilisateurs que quelque chose s'√©tait pass√© (par exemple, un compte avait expir√©).  Pour g√©n√©rer des ressources, nous avons choisi un serveur arbitraire et lui avons envoy√© une demande de g√©n√©ration, et pour les notifications concernant l'expiration d'un abonnement, nous avons envoy√© une commande √† tous les serveurs dans l'espoir que le message atteindrait l'objectif. <br><br><h3>  Le serveur lui-m√™me d√©termine √† qui envoyer le message. </h3><br>  Cela ressemble √† une fonctionnalit√©, pas √† un probl√®me.  Mais le serveur se concentre uniquement sur la connexion √† un autre serveur.  S'il y a des connexions, alors il y a un candidat pour envoyer un message. <br><br>  Le probl√®me est que le serveur num√©ro 1 ne sait pas que le serveur num√©ro 4 est actuellement sous forte charge et ne peut pas y r√©pondre assez rapidement.  Par cons√©quent, les demandes du serveur n ¬∞ 1 sont trait√©es plus lentement qu'elles ne le pourraient. <br><br><img src="https://habrastorage.org/webt/g7/mw/ez/g7mwezzba78vsgcvx8mpa_fzdou.png"><br><br><h3>  Le serveur ne sait pas que le deuxi√®me serveur est fig√© </h3><br>  Mais que se passe-t-il si le serveur n'est pas seulement lourdement charg√©, mais se bloque g√©n√©ralement?  De plus, il se bloque pour ne plus prendre vie.  Par exemple, j'ai √©puis√© toute la m√©moire disponible. <br><br>  Dans ce cas, le serveur n ¬∞ 1 ne conna√Æt pas le probl√®me, il continue donc d'attendre une r√©ponse.  Les serveurs restants du cluster ne connaissent pas non plus la situation avec le serveur n ¬∞ 4, ils enverront donc beaucoup de messages au serveur n ¬∞ 4 et attendront une r√©ponse.  Il en sera ainsi jusqu'√† la mort du serveur num√©ro 4. <br><br><img src="https://habrastorage.org/webt/5y/et/pg/5yetpgodx1zi38he2nchwnnniiq.png"><br><br>  Que faire  Nous pouvons ajouter ind√©pendamment une v√©rification de l'√©tat du serveur au syst√®me.  Ou nous pouvons rediriger les messages des serveurs ¬´malades¬ª vers des serveurs ¬´sains¬ª.  Tout cela prendra trop de temps aux d√©veloppeurs.  En 2012, nous avions peu d'exp√©rience dans ce domaine, nous avons donc commenc√© √† chercher des solutions toutes faites √† tous nos probl√®mes √† la fois. <br><br><h2>  Courtier de messages.  Activemq </h2><br>  Nous avons d√©cid√© d'aller dans le sens de Message Broker pour configurer correctement la communication entre les serveurs.  Ils ont choisi ActiveMQ en raison de la possibilit√© de configurer la r√©ception de messages sur le consommateur √† un certain moment.  Certes, nous n'avons jamais saisi cette opportunit√©, nous avons donc pu choisir RabbitMQ, par exemple. <br><br>  En cons√©quence, nous avons transf√©r√© l'int√©gralit√© de notre syst√®me de cluster vers ActiveMQ.  Qu'est-ce que cela a donn√©: <br><br><ol><li>  Le serveur ne d√©termine plus lui-m√™me √† qui le message est envoy√©, car tous les messages transitent par la file d'attente. </li><li>  Tol√©rance aux pannes configur√©e.  Pour lire la file d'attente, vous pouvez ex√©cuter non pas un, mais plusieurs serveurs.  M√™me si l'un d'eux tombe, le syst√®me continuera de fonctionner. </li><li>  Les serveurs sont apparus des r√¥les, ce qui a permis de diviser le serveur par type de charge.  Par exemple, un g√©n√©rateur de ressources peut uniquement se connecter √† une file d'attente pour lire des messages afin de g√©n√©rer des ressources, et un serveur avec des cartes peut se connecter √† une file d'attente pour ouvrir des cartes. </li><li>  La communication RPC, c'est-√†-dire  chaque serveur a sa propre file d'attente priv√©e, o√π d'autres serveurs lui envoient des √©v√©nements. </li><li>  Vous pouvez envoyer des messages √† tous les serveurs via Topic, que nous utilisons pour r√©initialiser les abonnements. </li></ol><br><br>  Le sch√©ma semble simple: tous les serveurs sont connect√©s au courtier et il g√®re la communication entre eux.  Tout fonctionne, les messages sont envoy√©s et re√ßus, des ressources sont cr√©√©es.  Mais il y a de nouveaux probl√®mes. <br><br><h3>  Que faire lorsque tous les serveurs n√©cessaires reposent? </h3><br>  Supposons que le serveur n ¬∞ 3 veuille envoyer un message pour g√©n√©rer des ressources dans une file d'attente.  Il s'attend √† ce que son message soit trait√©.  Mais il ne sait pas que pour une raison quelconque, il n'y a pas un seul destinataire du message.  Par exemple, les destinataires ont plant√© en raison d'une erreur. <br><br>  Pour tout le temps d'attente, le serveur envoie beaucoup de messages avec une requ√™te, c'est pourquoi une file d'attente de messages appara√Æt.  Par cons√©quent, lorsque des serveurs qui fonctionnent apparaissent, ils sont oblig√©s de traiter d'abord la file d'attente accumul√©e, ce qui prend du temps.  Du c√¥t√© de l'utilisateur, cela conduit au fait que l'image t√©l√©charg√©e par lui n'appara√Æt pas imm√©diatement.  Il n'est pas pr√™t √† attendre, alors il quitte le tableau. <br><br>  En cons√©quence, nous d√©pensons la capacit√© du serveur pour la g√©n√©ration de ressources, et personne n'a besoin du r√©sultat. <br><br><img src="https://habrastorage.org/webt/oq/p8/fd/oqp8fd0mdctqarlauh84jdjn8rc.png"><br><br>  Comment puis-je r√©soudre le probl√®me?  Nous pouvons mettre en place une surveillance qui vous informera de ce qui se passe.  Mais √† partir du moment o√π la surveillance rapporte quelque chose, jusqu'au moment o√π nous comprenons que nos serveurs sont mauvais, le temps passera.  Cela ne nous convient pas. <br><br>  Une autre option consiste √† ex√©cuter Service Discovery, ou un registre de services qui saura quels serveurs avec quels r√¥les s'ex√©cutent.  Dans ce cas, nous recevrons imm√©diatement un message d'erreur s'il n'y a pas de serveurs gratuits. <br><br><h3>  Certains services ne peuvent pas √™tre mis √† l'√©chelle horizontalement </h3><br>  Il s'agit d'un probl√®me de notre premier code, pas d'ActiveMQ.  Permettez-moi de vous montrer un exemple: <br><br><pre><code class="plaintext hljs">Permission ownerPermission = service.getOwnerPermission(board); Permission permission = service.getPermission(board,user); ownerPermission.setRole(EDITOR); permission.setRole(OWNER);</code> </pre> <br>  Nous avons un service pour travailler avec les droits des utilisateurs sur la carte: l'utilisateur peut √™tre le propri√©taire de la carte ou son √©diteur.  Il ne peut y avoir qu'un seul propri√©taire au conseil d'administration.  Supposons que nous ayons un sc√©nario o√π nous voulons transf√©rer la propri√©t√© d'une carte d'un utilisateur √† un autre.  Sur la premi√®re ligne, nous obtenons le propri√©taire actuel de la carte, sur la seconde - nous prenons l'utilisateur qui √©tait l'√©diteur, et devient maintenant le propri√©taire.  De plus, le propri√©taire actuel nous a mis le r√¥le d'√âDITEUR, et l'ancien √©diteur - le r√¥le de PROPRI√âTAIRE. <br><br>  Voyons comment cela fonctionnera dans un environnement multi-thread.  Lorsque le premier thread √©tablit le r√¥le EDITOR et que le second thread essaie de prendre le propri√©taire actuel, il peut arriver que OWNER n'existe pas, mais il y a deux EDITOR. <br><br>  La raison en est le manque de synchronisation.  Nous pouvons r√©soudre le probl√®me en ajoutant un bloc de synchronisation sur la carte. <br><br><pre> <code class="plaintext hljs">synchronized (board) { Permission ownerPermission = service.getOwnerPermission(board); Permission permission = service.getPermission(board,user); ownerPermission.setRole(EDITOR); permission.setRole(OWNER); }</code> </pre><br>  Cette solution ne fonctionnera pas dans le cluster.  La base de donn√©es SQL pourrait nous aider avec cela √† l'aide de transactions.  Mais nous avons Redis. <br><br>  Une autre solution consiste √† ajouter des verrous distribu√©s au cluster afin que la synchronisation se fasse √† l'int√©rieur de l'ensemble du cluster, et pas seulement d'un serveur. <br><br><h2>  Un seul point d'√©chec lors de l'entr√©e dans la planche </h2><br>  Le mod√®le d'interaction entre le client et le serveur est dynamique.  Il faut donc stocker l'√©tat de la carte sur le serveur.  Par cons√©quent, nous avons cr√©√© un r√¥le distinct pour les serveurs - BoardServer, qui g√®re les demandes des utilisateurs li√©es aux cartes. <br><br>  Imaginez que nous ayons trois BoardServer, dont l'un est le principal.  L'utilisateur lui envoie une requ√™te ¬´Ouvrez-moi la carte avec id = 123¬ª ‚Üí le serveur recherche dans sa base de donn√©es si la carte est ouverte et sur quel serveur elle se trouve.  Dans cet exemple, la carte est ouverte. <br><br><img src="https://habrastorage.org/webt/ej/kf/sd/ejkfsdptym30e-gdvkycc225zpw.png"><br><br>  Le serveur principal r√©pond que vous devez vous connecter au serveur n ¬∞ 1 ‚Üí l'utilisateur se connecte.  √âvidemment, si le serveur principal meurt, l'utilisateur ne pourra plus acc√©der √† de nouvelles cartes. <br><br>  Alors pourquoi avons-nous besoin d'un serveur qui sait o√π les cartes sont ouvertes?  Nous avons donc un seul point de d√©cision.  Si quelque chose arrive aux serveurs, nous devons comprendre si la carte est r√©ellement disponible afin de la retirer du registre ou de la rouvrir ailleurs.  Il serait possible d'organiser cela √† l'aide d'un quorum, lorsque plusieurs serveurs r√©solvent un probl√®me similaire, mais √† cette √©poque, nous n'avions pas les connaissances n√©cessaires pour impl√©menter le quorum de mani√®re ind√©pendante. <br><br><h2>  Passer √† Hazelcast </h2><br>  D'une mani√®re ou d'une autre, nous avons r√©gl√© les probl√®mes qui se sont pos√©s, mais ce n'est peut-√™tre pas la plus belle des fa√ßons.  Maintenant, nous devions comprendre comment les r√©soudre correctement, nous avons donc formul√© une liste d'exigences pour une nouvelle solution de cluster: <br><br><ol><li>  Nous avons besoin de quelque chose qui surveillera l'√©tat de tous les serveurs et leurs r√¥les.  Appelez-le Service Discovery. </li><li>  Nous avons besoin de verrous de cluster qui aideront √† garantir la coh√©rence lors de l'ex√©cution de requ√™tes dangereuses. </li><li>  Nous avons besoin d'une structure de donn√©es distribu√©e qui garantira que les cartes se trouvent sur certains serveurs et informera en cas de probl√®me. </li></ol><br>  C'√©tait l'ann√©e 2015.  Nous avons opt√© pour Hazelcast - In-Memory Data Grid, un syst√®me de cluster pour stocker des informations dans la RAM.  Nous avons alors pens√© que nous avions trouv√© une solution miracle, le Saint Graal du monde de l'interaction de cluster, un cadre miracle qui peut tout faire et combine des structures de donn√©es distribu√©es, des verrous, des messages RPC et des files d'attente. <br><br><img src="https://habrastorage.org/webt/ce/ws/c9/cewsc9gdgzsmtebczs9jxbs4j2e.png"><br><br>  Comme avec ActiveMQ, nous avons transf√©r√© presque tout √† Hazelcast: <br><br><ul><li>  g√©n√©ration de ressources utilisateur via ExecutorService; </li><li>  verrou distribu√© lorsque les droits sont modifi√©s; </li><li>  r√¥les et attributs des serveurs (Service Discovery); </li><li>  un registre unique de tableaux ouverts, etc. </li></ul><br><h3>  Topologies Hazelcast </h3><br>  Hazelcast peut √™tre configur√© dans deux topologies.  La premi√®re option est Client-Serveur, lorsque les membres sont situ√©s s√©par√©ment de l'application principale, ils forment eux-m√™mes un cluster et toutes les applications s'y connectent en tant que base de donn√©es. <br><br><img src="https://habrastorage.org/webt/r4/lg/vm/r4lgvmm7ni0dmyb6yp60cueklwm.png"><br><br>  La deuxi√®me topologie est Embedded, lorsque les membres Hazelcast sont int√©gr√©s dans l'application elle-m√™me.  Dans ce cas, nous pouvons utiliser moins d'instances, l'acc√®s aux donn√©es est plus rapide, car les donn√©es et la logique m√©tier elle-m√™me sont au m√™me endroit. <br><br><img src="https://habrastorage.org/webt/gq/rz/fa/gqrzfappt3yspdlfpfe5sm3mhyg.png"><br><br>  Nous avons choisi la deuxi√®me solution car nous l'avons jug√©e plus efficace et √©conomique √† mettre en ≈ìuvre.  Efficace, car la vitesse d'acc√®s aux donn√©es Hazelcast sera plus faible, car  peut-√™tre que ces donn√©es sont sur le serveur actuel.  √âconomique, car nous n'avons pas besoin de d√©penser d'argent pour des instances suppl√©mentaires. <br><br><h3>  Le cluster se bloque lorsque le membre se bloque </h3><br>  Quelques semaines apr√®s avoir activ√© Hazelcast, des probl√®mes sont apparus sur la prod. <br><br>  Au d√©but, notre surveillance a montr√© que l'un des serveurs commen√ßait √† surcharger progressivement la m√©moire.  Pendant que nous regardions ce serveur, les autres serveurs ont √©galement commenc√© √† se charger: le CPU a augment√©, puis la RAM, et apr√®s cinq minutes, tous les serveurs ont utilis√© toute la m√©moire disponible. <br><br>  √Ä ce stade des consoles, nous avons vu ces messages: <br><br><pre> <code class="java hljs"><span class="hljs-number"><span class="hljs-number">2015</span></span>-<span class="hljs-number"><span class="hljs-number">07</span></span>-<span class="hljs-number"><span class="hljs-number">15</span></span> <span class="hljs-number"><span class="hljs-number">15</span></span>:<span class="hljs-number"><span class="hljs-number">35</span></span>:<span class="hljs-number"><span class="hljs-number">51</span></span>,<span class="hljs-number"><span class="hljs-number">466</span></span> [WARN] (cached18) com.hazelcast.spi.impl.operationservice.impl.Invocation: [my.host.address.com]:<span class="hljs-number"><span class="hljs-number">5701</span></span> [dev] [<span class="hljs-number"><span class="hljs-number">3.5</span></span>] Asking ifoperation execution has been started: com.hazelcast.spi.impl.operationservice.impl.IsStillRunningService$InvokeIsStillRunningOperationRunnable@<span class="hljs-number"><span class="hljs-number">6</span></span>d4274d7 <span class="hljs-number"><span class="hljs-number">2015</span></span>-<span class="hljs-number"><span class="hljs-number">07</span></span>-<span class="hljs-number"><span class="hljs-number">15</span></span> <span class="hljs-number"><span class="hljs-number">15</span></span>:<span class="hljs-number"><span class="hljs-number">35</span></span>:<span class="hljs-number"><span class="hljs-number">51</span></span>,<span class="hljs-number"><span class="hljs-number">467</span></span> [WARN] (hz._hzInstance_1_dev.async.thread-<span class="hljs-number"><span class="hljs-number">3</span></span>) com.hazelcast.spi.impl.operationservice.impl.Invocation:[my.host.address.com]:<span class="hljs-number"><span class="hljs-number">5701</span></span> [dev] [<span class="hljs-number"><span class="hljs-number">3.5</span></span>] <span class="hljs-string"><span class="hljs-string">'is-executing'</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">true</span></span> -&gt; Invocation{ serviceName=<span class="hljs-string"><span class="hljs-string">'hz:impl:executorService'</span></span>, op=com.hazelcast.executor.impl.operations.MemberCallableTaskOperation{serviceName=<span class="hljs-string"><span class="hljs-string">'null'</span></span>, partitionId=-<span class="hljs-number"><span class="hljs-number">1</span></span>, callId=<span class="hljs-number"><span class="hljs-number">18062</span></span>, invocationTime=<span class="hljs-number"><span class="hljs-number">1436974430783</span></span>, waitTimeout=-<span class="hljs-number"><span class="hljs-number">1</span></span>,callTimeout=<span class="hljs-number"><span class="hljs-number">60000</span></span>}, partitionId=-<span class="hljs-number"><span class="hljs-number">1</span></span>, replicaIndex=<span class="hljs-number"><span class="hljs-number">0</span></span>, tryCount=<span class="hljs-number"><span class="hljs-number">250</span></span>, tryPauseMillis=<span class="hljs-number"><span class="hljs-number">500</span></span>, invokeCount=<span class="hljs-number"><span class="hljs-number">1</span></span>, callTimeout=<span class="hljs-number"><span class="hljs-number">60000</span></span>,target=Address[my.host2.address.com]:<span class="hljs-number"><span class="hljs-number">5701</span></span>, backupsExpected=<span class="hljs-number"><span class="hljs-number">0</span></span>, backupsCompleted=<span class="hljs-number"><span class="hljs-number">0</span></span>}</code> </pre><br>  Ici, Hazelcast v√©rifie si l'op√©ration envoy√©e au premier serveur ¬´mourant¬ª est en cours.  Hazelcast a essay√© de se tenir au courant et a v√©rifi√© l'√©tat de l'op√©ration plusieurs fois par seconde.  En cons√©quence, il a spamm√© tous les autres serveurs avec cette op√©ration, et apr√®s quelques minutes, ils ont vol√© hors de la m√©moire, et nous avons collect√© plusieurs Go de journaux de chacun d'eux. <br><br>  La situation s'est r√©p√©t√©e plusieurs fois.  Il s'est av√©r√© qu'il s'agit d'une erreur dans Hazelcast version 3.5, dans laquelle le m√©canisme de pulsation a √©t√© impl√©ment√©, qui v√©rifie l'√©tat des demandes.  Il n'a pas v√©rifi√© certains des cas limites que nous avons rencontr√©s.  J'ai d√ª optimiser l'application pour ne pas tomber dans ces cas, et apr√®s quelques semaines Hazelcast a corrig√© l'erreur √† la maison. <br><br><h3>  Ajout et suppression fr√©quents de membres de Hazelcast </h3><br>  Le prochain probl√®me que nous avons d√©couvert est l'ajout et la suppression de membres de Hazelcast. <br><br>  Tout d'abord, je vais d√©crire bri√®vement comment Hazelcast fonctionne avec les partitions.  Par exemple, il y a quatre serveurs, et chacun stocke une partie des donn√©es (dans la figure, elles sont de couleurs diff√©rentes).  L'unit√© est la partition principale, le diable est la partition secondaire, c'est-√†-dire  sauvegarde de la partition principale. <br><br><img src="https://habrastorage.org/webt/ex/qz/vj/exqzvjxs9rxlmfgssghnrnqxnn8.png"><br><br>  Lorsqu'un serveur est √©teint, les partitions sont envoy√©es √† d'autres serveurs.  Si le serveur meurt, les partitions ne sont pas transf√©r√©es depuis celui-ci, mais depuis les serveurs qui sont toujours en vie et qui contiennent une sauvegarde de ces partitions. <br><br><img src="https://habrastorage.org/webt/eu/ds/-0/euds-0xurnqjlbhisjoj8k9ucis.png"><br><br>  Il s'agit d'un m√©canisme fiable.  Le probl√®me est que nous allumons et √©teignons souvent les serveurs pour √©quilibrer la charge, et le r√©√©quilibrage des partitions prend √©galement du temps.  Et plus il y a de serveurs en cours d'ex√©cution et plus nous stockons de donn√©es dans Hazelcast, plus il faut de temps pour r√©√©quilibrer les partitions. <br><br>  Bien s√ªr, nous pouvons r√©duire le nombre de sauvegardes, c'est-√†-dire  partitions secondaires.  Mais ce n'est pas s√ªr, car quelque chose va mal tourner. <br><br>  Une autre solution consiste √† basculer vers la topologie client-serveur afin que l'activation et la d√©sactivation des serveurs n'affectent pas le cluster Hazelcast principal.  Nous avons essay√© de le faire et il s'est av√©r√© que les demandes RPC ne pouvaient pas √™tre effectu√©es sur les clients.  Voyons pourquoi. <br><br>  Pour ce faire, consid√©rez l'exemple d'envoi d'une demande RPC √† un autre serveur.  Nous prenons le ExecutorService, qui vous permet d'envoyer des messages RPC, et de soumettre avec une nouvelle t√¢che. <br><br><pre> <code class="plaintext hljs">hazelcastInstance .getExecutorService(...) .submit(new Task(), ...);</code> </pre><br>  La t√¢che elle-m√™me ressemble √† une classe Java standard qui impl√©mente Callable. <br><pre> <code class="plaintext hljs">public class Task implements Callable&lt;Long&gt; { @Override public Long call() { return 42; } }</code> </pre><br>  Le probl√®me est que les clients Hazelcast peuvent √™tre non seulement des applications Java, mais aussi des applications C ++, .NET et autres.  Naturellement, nous ne pouvons pas g√©n√©rer et convertir notre classe Java vers une autre plate-forme. <br><br>  Une option consiste √† passer √† l'utilisation des requ√™tes http au cas o√π nous voudrions envoyer quelque chose d'un serveur √† un autre et obtenir une r√©ponse.  Mais nous devrons alors abandonner partiellement Hazelcast. <br><br>  Par cons√©quent, comme solution, nous avons choisi d'utiliser des files d'attente au lieu de ExecutorService.  Pour ce faire, nous avons mis en ≈ìuvre ind√©pendamment un m√©canisme pour attendre qu'un √©l√©ment soit ex√©cut√© dans la file d'attente, qui traite les cas limites et renvoie le r√©sultat au serveur demandeur. <br><br><h2>  Qu'avons-nous appris </h2><br>  <b>Faites preuve de flexibilit√© dans le syst√®me.</b>  L'avenir change constamment, il n'y a donc pas de solutions parfaites.  Pour bien faire, ¬´bien¬ª ne fonctionne pas, mais vous pouvez essayer d'√™tre flexible et de l'int√©grer dans le syst√®me.  Cela nous a permis de reporter d'importantes d√©cisions architecturales jusqu'au moment o√π il n'est plus impossible de les accepter. <br><br>  Robert Martin dans Clean Architecture √©crit sur ce principe: <br><blockquote>  ¬´Le but de l'architecte est de cr√©er une forme pour le syst√®me qui fera de la politique l'√©l√©ment le plus important et les d√©tails non li√©s √† la politique.  Cela retardera et retardera les d√©cisions sur les d√©tails. ¬ª </blockquote><br><br>  <b>Il n'existe pas d'outils et de solutions universels.</b>  S'il vous semble qu'un cadre r√©sout tous vos probl√®mes, ce n'est probablement pas le cas.  Par cons√©quent, lors de la mise en ≈ìuvre d'un cadre, il est important de comprendre non seulement les probl√®mes qu'il r√©soudra, mais aussi ceux qu'il entra√Ænera. <br><br>  <b>Ne r√©√©crivez pas tout de suite.</b>  Si vous √™tes confront√© √† un probl√®me d'architecture et qu'il semble que la seule bonne solution est de tout √©crire √† partir de z√©ro, attendez.  Si le probl√®me est vraiment grave, trouvez une solution rapide et regardez comment le syst√®me fonctionnera √† l'avenir.  Tr√®s probablement, ce ne sera pas le seul probl√®me en architecture, avec le temps vous en trouverez plus.  Et ce n'est que lorsque vous s√©lectionnez un nombre suffisant de probl√®mes que vous pouvez commencer √† refactoriser.  Seulement dans ce cas, il y aura plus d'avantages que sa valeur. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr441590/">https://habr.com/ru/post/fr441590/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr441578/index.html">Tutoriel React, partie 19: M√©thodes du cycle de vie des composants</a></li>
<li><a href="../fr441580/index.html">Tutoriel React, partie 20: Premi√®re le√ßon de rendu conditionnel</a></li>
<li><a href="../fr441582/index.html">Optimisation du syst√®me de contr√¥le LQR</a></li>
<li><a href="../fr441584/index.html">PHP Digest n ¬∞ 150 (11-25 f√©vrier 2019)</a></li>
<li><a href="../fr441586/index.html">Comment recommander une musique que presque personne n'a √©cout√©e. Rapport Yandex</a></li>
<li><a href="../fr441594/index.html">Napalm d'entreprise</a></li>
<li><a href="../fr441596/index.html">Le premier port spatial priv√© sera construit en Russie</a></li>
<li><a href="../fr441598/index.html">Mission lunaire "Bereshit" - un portail en ligne avec un simulateur de trajectoire et la surveillance des param√®tres de vol actuels</a></li>
<li><a href="../fr441600/index.html">Interface utilisateur faible, programmeur faible</a></li>
<li><a href="../fr441602/index.html">Pourquoi une voiture automatique classique est impossible et n'a aucune perspective commerciale</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>