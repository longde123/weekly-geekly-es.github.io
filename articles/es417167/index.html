<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë£ üåÑ ‚òéÔ∏è Lanzar LDA en el mundo real. Gu√≠a detallada üï∫üèæ üîÆ üë®üèæ‚Äçüíª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pr√≥logo 


 Hay muchos tutoriales en Internet que explican c√≥mo funciona el LDA (Asignaci√≥n de Dirichlet Latente) y c√≥mo ponerlo en pr√°ctica. Los ejem...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Lanzar LDA en el mundo real. Gu√≠a detallada</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/417167/"><h2 id="predislovie">  Pr√≥logo </h2><br><p>  Hay muchos tutoriales en Internet que explican c√≥mo funciona el LDA (Asignaci√≥n de Dirichlet Latente) y c√≥mo ponerlo en pr√°ctica.  Los ejemplos de capacitaci√≥n en LDA a menudo se demuestran en conjuntos de datos "ejemplares", como el "conjunto de datos de 20 grupos de noticias", que est√° disponible en sklearn. </p><br><p>  Una caracter√≠stica de la capacitaci√≥n sobre el ejemplo de conjuntos de datos "ejemplares" es que los datos est√°n siempre en orden y convenientemente apilados en un solo lugar.  Al entrenar modelos de producci√≥n, los datos obtenidos directamente de fuentes reales suelen ser lo contrario: </p><br><ul><li>  Muchas emisiones. </li><li>  Marcado incorrecto (si lo hay). </li><li>  Desequilibrios de clase muy fuertes y distribuciones feas de cualquier par√°metro del conjunto de datos. </li><li>  Para los textos, estos son: errores gramaticales, una gran cantidad de palabras raras y √∫nicas, multiling√ºismo. </li><li>  Una forma inconveniente de almacenar datos (formatos diferentes o raros, la necesidad de analizar) </li></ul><br><p>  Hist√≥ricamente, trato de aprender de ejemplos que est√°n lo m√°s cerca posible de las realidades de la realidad de producci√≥n porque es de esta manera que uno puede sentir las √°reas problem√°ticas de un tipo particular de tarea.  As√≠ fue con la LDA, y en este art√≠culo quiero compartir mi experiencia: c√≥mo ejecutar LDA desde cero, en datos completamente sin procesar.  Parte del art√≠culo se dedicar√° a obtener estos mismos datos, de modo que el ejemplo se convierta en un "caso de ingenier√≠a" completo. </p><a name="habracut"></a><br><h2 id="topic-modeling-i-lda">  Modelado de temas y LDA. </h2><br><p>  Para comenzar, considere qu√© hace el LDA en general y qu√© tareas usa. <br>  Muy a menudo, LDA se utiliza para tareas de modelado de temas.  Tales tareas significan las tareas de agrupar o clasificar textos, de tal manera que cada clase o grupo contenga textos con temas similares. </p><br><p>  Para aplicar LDA al conjunto de datos de textos (en lo sucesivo, el cuerpo del texto), es necesario transformar el cuerpo en una matriz de documento de t√©rmino. </p><br><p>  Un t√©rmino matriz de documento es una matriz que tiene un tama√±o <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mtext>&amp;#xA0;</mtext><mi>v</mi><mi>e</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>W</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.471ex" height="2.057ex" viewBox="0 -780.1 4508.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-4E" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-76" x="1138" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-65" x="1624" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-63" x="2090" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-65" x="2524" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-73" x="2990" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-57" x="3460" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi><mtext>&nbsp;</mtext><mi>v</mi><mi>e</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-1"> N \ veces W </script>  donde <br>  N es el n√∫mero de documentos en el caso, y W es el tama√±o del diccionario del caso, es decir  El n√∫mero de palabras (√∫nicas) que se encuentran en nuestro corpus.  En la fila i-√©sima, la columna j-√©sima de la matriz es un n√∫mero: cu√°ntas veces en el texto i-√©simo se encontr√≥ la palabra j-√©sima. </p><br><p>  El LDA construye, para una matriz de documento de T√©rmino dado y T de un n√∫mero predeterminado de temas, dos distribuciones: </p><br><ol><li>  La distribuci√≥n de temas en los textos (en la pr√°ctica, dada por la matriz de tama√±o <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mtext>&amp;#xA0;</mtext><mi>v</mi><mi>e</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>T</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="9.672ex" height="2.057ex" viewBox="0 -780.1 4164.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-4E" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-76" x="1138" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-65" x="1624" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-63" x="2090" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-65" x="2524" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-73" x="2990" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-54" x="3460" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi><mtext>&nbsp;</mtext><mi>v</mi><mi>e</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>T</mi></math></span></span><script type="math/tex" id="MathJax-Element-2"> N \ veces T </script>  ) </li><li>  La distribuci√≥n de palabras por tema. (Matriz de tama√±o <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>T</mi><mtext>&amp;#xA0;</mtext><mi>v</mi><mi>e</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>W</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.044ex" height="2.057ex" viewBox="0 -780.1 4324.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-54" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-76" x="954" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-65" x="1440" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-63" x="1906" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-65" x="2340" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-73" x="2806" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-57" x="3276" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi><mtext>&nbsp;</mtext><mi>v</mi><mi>e</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-3"> T \ veces W </script>  ) </li></ol><br><p>  Los valores de las celdas de estas matrices son, respectivamente, las probabilidades de que este tema est√© contenido en este documento (o la proporci√≥n del tema en el documento, si consideramos el documento como una mezcla de diferentes temas) para la matriz 'Distribuci√≥n de temas en textos'. </p><br><p>  Para la matriz 'Distribuci√≥n de palabras por temas', los valores son la probabilidad de encontrar la palabra j en el texto con el tema i, cualitativamente, podemos considerar estos n√∫meros como coeficientes que caracterizan c√≥mo esta palabra es t√≠pica para este tema. </p><br><p>  Debe decirse que la palabra tema no es una definici√≥n "cotidiana" de esta palabra.  La LDA asigna T a esos, pero se desconoce qu√© tipo de temas son estos y si corresponden a alg√∫n tema de texto conocido, como: 'Deporte', 'Ciencia', 'Pol√≠tica'.  En este caso, es m√°s apropiado hablar sobre el tema como una especie de entidad abstracta, que se define por una l√≠nea en la matriz de distribuci√≥n de palabras por tema y con cierta probabilidad corresponde a este texto, si puede imaginarlo como una familia de conjuntos de palabras caracter√≠sticas que se juntan con las probabilidades correspondientes (de la tabla) en cierto conjunto de textos. </p><br><p>  Si est√° interesado en estudiar con m√°s detalle y 'en f√≥rmulas' c√≥mo se capacita y trabaja el LDA, aqu√≠ hay algunos materiales (que fueron utilizados por el autor): </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Art√≠culo original</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">En ingl√©s, con ejemplos ilustrativos.</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Detalles en ruso</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Acerca de la implementaci√≥n de Python</a> </li></ul><br><h2 id="dobyvaem-dikie-dannye">  Obtenemos datos salvajes </h2><br><p>  Para nuestro 'trabajo de laboratorio', necesitamos un conjunto de datos personalizado con sus propios defectos y caracter√≠sticas.  Puede obtenerlo en diferentes lugares: descargue rese√±as de Kinopoisk, art√≠culos de Wikipedia, noticias de alg√∫n portal de noticias, tomaremos una opci√≥n un poco m√°s extrema: publicaciones de las comunidades VKontakte. </p><br><p>  Haremos esto as√≠: </p><br><ol><li>  Seleccionamos alg√∫n usuario de VK. </li><li>  Tenemos una lista de todos sus amigos. </li><li>  Para cada amigo, tomamos toda su comunidad. </li><li>  Para cada comunidad de cada amigo, extraemos las primeras n (n = 100) publicaciones de la comunidad y las combinamos en un contenido de texto de la comunidad. </li></ol><br><h4 id="instrumenty-i-stati">  Herramientas y articulos </h4><br><p>  Para descargar publicaciones, utilizaremos el m√≥dulo vk para trabajar con la API VKontakte, para Python.  Uno de los momentos m√°s intrincados al escribir una aplicaci√≥n usando la API de VKontakte es la autorizaci√≥n, afortunadamente, el c√≥digo que realiza este trabajo ya ha sido escrito y est√° en el dominio p√∫blico, excepto vk, utilic√© un peque√±o m√≥dulo para autorizaci√≥n: vkauth. </p><br><p>  Enlaces a los m√≥dulos y art√≠culos utilizados para estudiar la API de VKontakte: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">vkauth</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tutorial de vkauth</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">vk tutorial</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">vk tutorial n√∫mero 2</a> </li><li>  Documentaci√≥n oficial de la API de Vkontakte </li></ul><br><h4 id="pishem-kod">  Escribir un c√≥digo </h4><br><p>  Y as√≠, usando vkauth, inicie sesi√≥n: </p><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#authorization of app using modules imported. app_id = '6203169' perms = ['photos','friends','groups'] API_ver = '5.68' Auth = VKAuth(perms, app_id, API_ver) Auth.auth() token = Auth.get_token() user_id = Auth.get_user_id() #starting session session = vk.Session(access_token=token) api = vk.API(session)</span></span></code> </pre> <br><p>  En el proceso, se escribi√≥ un peque√±o m√≥dulo que contiene todas las funciones necesarias para descargar contenido en el formato apropiado, se enumeran a continuaci√≥n, veamos: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_friends_ids</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(api, user_id)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' For a given API object and user_id returns a list of all his friends ids. '''</span></span> friends = api.friends.get(user_id=user_id, v = <span class="hljs-string"><span class="hljs-string">'5.68'</span></span>) friends_ids = friends[<span class="hljs-string"><span class="hljs-string">'items'</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> friends_ids <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_user_groups</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(api, user_id, moder=True, only_open=True)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' For a given API user_id returns list of all groups he subscribed to. Flag model to get only those groups where user is a moderator or an admin) Flag only_open to get only public(open) groups. '''</span></span> kwargs = {<span class="hljs-string"><span class="hljs-string">'user_id'</span></span> : user_id, <span class="hljs-string"><span class="hljs-string">'v'</span></span> : <span class="hljs-string"><span class="hljs-string">'5.68'</span></span> } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> moder == <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: kwargs[<span class="hljs-string"><span class="hljs-string">'filter'</span></span>] = <span class="hljs-string"><span class="hljs-string">'moder'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> only_open == <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: kwargs[<span class="hljs-string"><span class="hljs-string">'extended'</span></span>] = <span class="hljs-number"><span class="hljs-number">1</span></span> kwargs[<span class="hljs-string"><span class="hljs-string">'fields'</span></span>] = [<span class="hljs-string"><span class="hljs-string">'is_closed'</span></span>] groups = api.groups.get(**kwargs) groups_refined = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> group <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> groups[<span class="hljs-string"><span class="hljs-string">'items'</span></span>]: cond_check = (only_open <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> group[<span class="hljs-string"><span class="hljs-string">'is_closed'</span></span>] == <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> only_open <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> cond_check: refined = {} refined[<span class="hljs-string"><span class="hljs-string">'id'</span></span>] = group[<span class="hljs-string"><span class="hljs-string">'id'</span></span>] * (<span class="hljs-number"><span class="hljs-number">-1</span></span>) refined[<span class="hljs-string"><span class="hljs-string">'name'</span></span>] = group[<span class="hljs-string"><span class="hljs-string">'name'</span></span>] groups_refined.append(refined) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> groups_refined <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_n_posts_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(api, group_id, n_posts=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">50</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' For a given api and group_id returns first n_posts concatenated as one text. '''</span></span> wall_contents = api.wall.get(owner_id = group_id, count=n_posts, v = <span class="hljs-string"><span class="hljs-string">'5.68'</span></span>) wall_contents = wall_contents[<span class="hljs-string"><span class="hljs-string">'items'</span></span>] text = <span class="hljs-string"><span class="hljs-string">''</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> post <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> wall_contents: text += post[<span class="hljs-string"><span class="hljs-string">'text'</span></span>] + <span class="hljs-string"><span class="hljs-string">' '</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text</code> </pre> <br><p>  La tuber√≠a final es la siguiente: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#id of user whose friends you gonna get, like: https://vk.com/id111111111 user_id = 111111111 friends_ids = vt.get_friends_ids(api, user_id) #collecting all groups groups = [] for i,friend in tqdm(enumerate(friends_ids)): if i % 3 == 0: sleep(1) friend_groups = vt.get_user_groups(api, friend, moder=False) groups += friend_groups #converting groups to dataFrame groups_df = pd.DataFrame(groups) groups_df.drop_duplicates(inplace=True) #reading content(content == first 100 posts) for i,group in tqdm(groups_df.iterrows()): name = group['name'] group_id = group['id'] #Different kinds of fails occures during scrapping #For examples there are names of groups with slashes #Like: 'The Kaaats / Indie-rock' try: content = vt.get_n_posts_text(api, group_id, n_posts=100) dst_path = join(data_path, name + '.txt') with open(dst_path, 'w+t') as f: f.write(content) except Exception as e: print('Error occured on group:', name) print(e) continue #need it because of requests limitaion in VK API. if i % 3 == 0: sleep(1)</span></span></code> </pre> <br><h4 id="fails">  Falla </h4><br><p>  En general, el proceso de descarga de datos no es dif√≠cil en s√≠ mismo; debe prestar atenci√≥n solo a dos puntos: </p><br><ol><li>  A veces, debido a la privacidad de algunas comunidades, recibir√° errores de acceso, a veces otros errores se resolver√°n instalando try, excepto en el lugar correcto. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">VK tiene un l√≠mite</a> en el n√∫mero de solicitudes por segundo. </li></ol><br><p>  Al realizar una gran cantidad de solicitudes, por ejemplo en un bucle, tambi√©n detectaremos errores.  Este problema se puede resolver de varias maneras: </p><br><ol><li>  Est√∫pidamente y sin rodeos: Stick sleep (algunos) cada 3 solicitudes.  Se realiza en una l√≠nea y ralentiza enormemente la descarga, en situaciones en las que los vol√∫menes de datos no son grandes y no hay tiempo para m√©todos m√°s sofisticados, esto es bastante aceptable (implementado en este art√≠culo) </li><li>  Comprender el trabajo de las solicitudes de Long Poll <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://vk.com/dev/using_longpoll</a> </li></ol><br><p>  En este documento, se eligi√≥ un m√©todo simple y lento, en el futuro, probablemente escribir√© un micro art√≠culo sobre formas de evitar o aliviar las restricciones en el n√∫mero de solicitudes por segundo. </p><br><h4 id="itog">  Resumen </h4><br><p>  Con el usuario inicial "algunos" que tiene ~ 150 amigos, lograron obtener 4,679 textos, cada uno de los cuales caracteriza una determinada comunidad de VK.  Los textos var√≠an mucho en tama√±o y est√°n escritos en muchos idiomas; algunos de ellos no son adecuados para nuestros prop√≥sitos, pero hablaremos un poco m√°s sobre esto. </p><br><h3 id="osnovnaya-chast">  Cuerpo principal </h3><br><p><img src="https://habrastorage.org/webt/bj/to/hm/bjtohmsrvsxlcbs78u0thlawxky.png" alt="imagen"></p><br><p>  Repasemos todos los bloques de nuestra tuber√≠a, primero, en el obligatorio (Ideal), luego en el resto, son de gran inter√©s. </p><br><h4 id="countvectorizer">  Countvectorizer </h4><br><p>  Antes de ense√±ar LDA, debemos presentar nuestros documentos en forma de matriz de documentos a plazo.  Esto generalmente incluye operaciones como: </p><br><ul><li>  Eliminar puttucciones / n√∫meros / fichas innecesarias. </li><li>  Tokenizaci√≥n (presentaci√≥n como una lista de palabras) </li><li>  Contando palabras, compilando una matriz de documentos t√©rmicos. </li></ul><br><p>  Todas estas acciones en sklearn se implementan convenientemente en el marco de una entidad de programa: sklearn.feature_extraction.text.CountVectorizer. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Enlace de documentaci√≥n</a> </p><br><p>  Todo lo que necesitas hacer es: </p><br><pre> <code class="python hljs">count_vect = CountVectorizer(input=<span class="hljs-string"><span class="hljs-string">'filename'</span></span>, stop_words=stopwords, vocabulary=voc) dataset = count_vect.fit_transform(train_names)</code> </pre> <br><h4 id="lda">  Lda </h4><br><p>  De manera similar con CountVectorizer, LDA se implementa perfectamente en Sklearn y otros marcos, por lo tanto, no tiene mucho sentido dedicar mucho espacio directamente a sus implementaciones, en nuestro art√≠culo puramente pr√°ctico. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Enlace de documentaci√≥n</a> </p><br><p>  Todo lo que necesitas para iniciar LDA es: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#training LDA lda = LDA(n_components = 60, max_iter=30, n_jobs=6, learning_method='batch', verbose=1) lda.fit(dataset)</span></span></code> </pre> <br><h4 id="preprocessing">  Preprocesamiento </h4><br><p>  Si solo tomamos nuestros textos inmediatamente despu√©s de descargarlos y convertirlos en una matriz de documentos a plazo utilizando el CountVectorizer, con el tokenizador predeterminado incorporado, obtendremos una matriz de tama√±o 4679x769801 (en los datos que uso). </p><br><p>  El tama√±o de nuestro diccionario ser√° de 769801. Incluso si suponemos que la mayor√≠a de las palabras son informativas, es poco probable que obtengamos una buena LDA, encontraremos algo as√≠ como 'Maldiciones de dimensiones', sin mencionar que para casi cualquier computadora, simplemente obstruiremos toda la RAM.  De hecho, la mayor√≠a de estas palabras son completamente informativas.  La gran mayor√≠a de ellos son: </p><br><ul><li>  Emoticones, personajes, n√∫meros. </li><li>  Palabras √∫nicas o muy raras (por ejemplo, palabras polacas de un grupo con memes polacos, palabras escritas incorrectamente o en 'alban√©s'). </li><li>  Partes del habla muy frecuentes (por ejemplo, preposiciones y pronombres). </li></ul><br><p>  Adem√°s, muchos grupos en VK se especializan exclusivamente en im√°genes (casi no hay mensajes de texto all√≠), los textos correspondientes a ellos son degenerados, en la matriz de documentos t√©rmicos nos dar√°n casi cero l√≠neas. </p><br><p>  Y as√≠, ¬°solucion√©moslo todo! <br>  Tokenizamos todos los textos, eliminamos la puntuaci√≥n y los n√∫meros de ellos, miramos el histograma de la distribuci√≥n de textos por el n√∫mero de palabras: <br><img src="https://habrastorage.org/webt/v4/qh/w0/v4qhw0mrgpizranmnptbz5lnivk.png" alt="imagen"></p><br><p>  Eliminamos todos los textos de menos de 100 palabras (hay 525 de ellos) </p><br><p>  Ahora el diccionario: <br>  Eliminar todos los tokens (palabras) que no son letras, en el marco de nuestra tarea, esto es bastante aceptable.  CountVectorizer hace esto por s√≠ solo, incluso si no es as√≠, creo que no hay necesidad de dar ejemplos (est√°n en la versi√≥n completa del c√≥digo del art√≠culo). </p><br><p>  Uno de los procedimientos m√°s comunes para reducir el tama√±o de un diccionario es eliminar las llamadas palabras vac√≠as (palabras vac√≠as), palabras que no llevan una carga sem√°ntica y / o no tienen color tem√°tico (en nuestro caso, Modelado de temas).  Tales palabras en nuestro caso son, por ejemplo: </p><br><ul><li>  Pronombres y preposiciones. </li><li>  Art√≠culos - el, a. </li><li>  Palabras comunes: 'ser', 'bueno', 'probablemente', etc. </li></ul><br><p>  El m√≥dulo nltk ha formado listas de palabras vac√≠as en ruso e ingl√©s, pero son bastante d√©biles.  En Internet, tambi√©n puede encontrar listas de palabras vac√≠as para cualquier idioma y agregarlas a las de nltk.  Entonces lo haremos.  Tome pausas adicionales desde aqu√≠: </p><br><ul><li>  <a href="">https://github.com/stopwords-iso/stopwords-ru/blob/master/stopwords-ru.json</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://gist.github.com/menzenski/7047705</a> </li></ul><br><p>  En la pr√°ctica, al resolver problemas espec√≠ficos, las listas de palabras vac√≠as se ajustan gradualmente y se complementan a medida que se entrena a los modelos, ya que para cada conjunto de datos y problema espec√≠ficos hay palabras espec√≠ficas "inconsistentes".  Tambi√©n recogeremos palabras clave personalizadas despu√©s de entrenar a nuestra LDA de primera generaci√≥n. </p><br><p>  Por s√≠ solo, el procedimiento para eliminar las palabras vac√≠as est√° integrado en CountVectorizer; solo necesitamos una lista de ellas. </p><br><p>  ¬øEs suficiente lo que hemos hecho? </p><br><p><img src="https://habrastorage.org/webt/ja/xd/6l/jaxd6lnbbnd_dmmmk6nog9a2ntm.png" alt="imagen"></p><br><p>  La mayor√≠a de las palabras que est√°n en nuestro diccionario todav√≠a no son demasiado informativas para aprender LDA sobre ellas y no est√°n en la lista de palabras clave.  Por lo tanto, aplicamos otro m√©todo de filtrado a nuestros datos. </p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>i</mi><mi>d</mi><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>,</mo><mi>D</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>o</mi><mi>g</mi><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>D</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mspace linebreak=&quot;newline&quot; /><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mtext>&amp;#xA0;</mtext><mi>i</mi><mi>n</mi><mi>D</mi><mo>:</mo><mi>t</mi><mtext>&amp;#xA0;</mtext><mi>i</mi><mi>n</mi><mi>d</mi><mspace linebreak=&quot;newline&quot; /></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></mrow></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="37.218ex" height="8.202ex" viewBox="0 -832 16024.3 3531.4" role="img" focusable="false" style="vertical-align: -6.27ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-69" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-64" x="345" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-66" x="869" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMAIN-28" x="1419" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-74" x="1809" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMAIN-2C" x="2170" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-44" x="2615" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMAIN-29" x="3444" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMAIN-3D" x="4111" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-6C" x="5417" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-6F" x="5716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-67" x="6201" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-66" x="6932" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-72" x="7482" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-61" x="7934" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-63" x="8463" y="0"></use><g transform="translate(8897,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMAIN-7C" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-44" x="278" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMAIN-7C" x="1107" y="0"></use></g><g transform="translate(10282,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMAIN-7C" x="0" y="0"></use><g transform="translate(0,-1432)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-64" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-69" x="773" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-6E" x="1119" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-44" x="1719" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMAIN-3A" x="2825" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-74" x="3382" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-69" x="3993" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-6E" x="4339" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMATHI-64" x="4939" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/417167/&amp;usg=ALkJrhi2BzTnjC2NppF6Lk94tphcGDzIpA#MJMAIN-7C" x="5463" y="0"></use></g></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>i</mi><mi>d</mi><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>&nbsp;</mtext><mi>l</mi><mi>o</mi><mi>g</mi><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>D</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mrow><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mspace linebreak="newline"></mspace><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mtext>&nbsp;</mtext><mi>i</mi><mi>n</mi><mi>D</mi><mo>:</mo><mi>t</mi><mtext>&nbsp;</mtext><mi>i</mi><mi>n</mi><mi>d</mi><mspace linebreak="newline"></mspace></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-4"> idf (t, D) = \ log \ frac {| D |} {| \\ {d \ in D: t \ in d \\} |} </script></p><br><p>  donde <br>  t es una palabra del diccionario. <br>  D - caso (muchos textos) <br>  d es uno de los textos del cuerpo. <br>  Calculamos el IDF de todas nuestras palabras, y cortamos las palabras con el idf m√°s grande (muy raro) y con el m√°s peque√±o (palabras extendidas). </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#'training' (tf-)idf vectorizer. tf_idf = TfidfVectorizer(input='filename', stop_words=stopwords, smooth_idf=False ) tf_idf.fit(train_names) #getting idfs idfs = tf_idf.idf_ #sorting out too rare and too common words lower_thresh = 3. upper_thresh = 6. not_often = idfs &gt; lower_thresh not_rare = idfs &lt; upper_thresh mask = not_often * not_rare good_words = np.array(tf_idf.get_feature_names())[mask] #deleting punctuation as well. cleaned = [] for word in good_words: word = re.sub("^(\d+\w*$|_+)", "", word) if len(word) == 0: continue cleaned.append(word)</span></span></code> </pre> <br><p>  Obtenido despu√©s de los procedimientos anteriores, ya es bastante adecuado para la capacitaci√≥n de LDA, pero haremos m√°s derivaciones: las mismas palabras se encuentran a menudo en nuestro conjunto de datos, pero en diferentes casos.  Para la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">derivaci√≥n, se utiliz√≥ pymystem3</a> . </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Stemming m = Mystem() stemmed = set() voc_len = len(cleaned) for i in tqdm(range(voc_len)): word = cleaned.pop() stemmed_word = m.lemmatize(word)[0] stemmed.add(stemmed_word) stemmed = list(stemmed) print('After stemming: %d'%(len(stemmed)))</span></span></code> </pre> <br><p>  Despu√©s de aplicar el filtro anterior, el tama√±o del diccionario disminuy√≥ de 769801 a <br>  13611 y ya con dichos datos, puede obtener un modelo LDA de calidad aceptable. </p><br><h3 id="testirovanie-primenenie-i-tyuning-lda">  Probar, aplicar y ajustar LDA </h3><br><p>  Ahora que tenemos el conjunto de datos, el preprocesamiento y los modelos que capacitamos en el conjunto de datos procesados, ser√≠a bueno verificar la idoneidad de nuestros modelos, as√≠ como crear algunas aplicaciones para ellos. </p><br><p>  Como aplicaci√≥n, para empezar, considere la tarea de generar palabras clave para un texto dado.  Puede hacer esto de una manera bastante simple de la siguiente manera: </p><br><ol><li>  Obtenemos de LDA la distribuci√≥n de temas para este texto. </li><li>  Elija n (por ejemplo, n = 2) de los temas m√°s pronunciados. </li><li>  Para cada tema, elija m (por ejemplo m = 3) las palabras m√°s caracter√≠sticas. </li><li>  Tenemos un conjunto de n * m palabras que caracterizan un texto dado. </li></ol><br><p>  Escribiremos una clase de interfaz simple que implementar√° este m√©todo de generaci√≥n de palabras clave: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Let\`s do simple interface class class TopicModeler(object): ''' Inteface object for CountVectorizer + LDA simple usage. ''' def __init__(self, count_vect, lda): ''' Args: count_vect - CountVectorizer object from sklearn. lda - LDA object from sklearn. ''' self.lda = lda self.count_vect = count_vect self.count_vect.input = 'content' def __call__(self, text): ''' Gives topics distribution for a given text Args: text - raw text via python string. returns: numpy array - topics distribution for a given text. ''' vectorized = self.count_vect.transform([text]) lda_topics = self.lda.transform(vectorized) return lda_topics def get_keywords(self, text, n_topics=3, n_keywords=5): ''' For a given text gives n top keywords for each of m top texts topics. Args: text - raw text via python string. n_topics - int how many top topics to use. n_keywords - how many top words of each topic to return. returns: list - of m*n keywords for a given text. ''' lda_topics = self(text) lda_topics = np.squeeze(lda_topics, axis=0) n_topics_indices = lda_topics.argsort()[-n_topics:][::-1] top_topics_words_dists = [] for i in n_topics_indices: top_topics_words_dists.append(self.lda.components_[i]) shape=(n_keywords*n_topics, self.lda.components_.shape[1]) keywords = np.zeros(shape=shape) for i,topic in enumerate(top_topics_words_dists): n_keywords_indices = topic.argsort()[-n_keywords:][::-1] for k,j in enumerate(n_keywords_indices): keywords[i * n_keywords + k, j] = 1 keywords = self.count_vect.inverse_transform(keywords) keywords = [keyword[0] for keyword in keywords] return keywords</span></span></code> </pre> <br><p>  Aplicamos nuestro m√©todo a varios textos y vemos qu√© sucede: <br>  Comunidad <strong>:</strong> Agencia de viajes "Colores del mundo" <br>  <strong>Palabras clave:</strong> ['foto', 'social', 'viaje', 'comunidad', 'viaje', 'euro', 'alojamiento', 'precio', 'Polonia', 'salida'] <br>  <strong>Comunidad:</strong> Gifs de comida <br>  <strong>Palabras clave:</strong> ['mantequilla', 'st', 'sal', 'pc', 'masa', 'cocci√≥n', 'cebolla', 'pimienta', 'az√∫car', 'gr'] </p><br><p>  Los resultados anteriores no son 'selecci√≥n de cereza' y parecen bastante adecuados.  De hecho, estos son los resultados de un modelo ya configurado.  Los primeros LDA que se formaron como parte de este art√≠culo produjeron resultados significativamente peores, entre las palabras clave que a menudo se pueden ver, por ejemplo: </p><br><ol><li>  Componentes compuestos de direcciones web: www, http, ru, com ... </li><li>  Palabras comunes </li><li>  unidades: cm, metro, km ... </li></ol><br><p>  El ajuste (ajuste) del modelo se realiz√≥ de la siguiente manera: </p><br><ol><li>  Para cada tema, seleccione n (n = 5) palabras m√°s caracter√≠sticas. </li><li>  Los consideramos idf, seg√∫n el caso de capacitaci√≥n. </li><li>  Traemos palabras clave del 5 al 10% de las m√°s difundidas. </li></ol><br><p>  Tal "limpieza" debe llevarse a cabo con cuidado, visualizando previamente el 10% de las palabras.  Por el contrario, los candidatos para la eliminaci√≥n deben elegirse de esta manera, y luego las palabras que deben eliminarse deben seleccionarse manualmente de ellos. </p><br><p>  En alg√∫n lugar de la generaci√≥n 2-3 de modelos, con una forma similar de seleccionar palabras vac√≠as, para el 5% superior de las distribuciones de palabras principales generalizadas, obtenemos: <br>  ['any', 'completamente', 'right', 'easy', 'next', 'internet', 'small', 'way', 'difficult', 'mood', 'so much', 'set', ' opci√≥n ',' nombre ',' discurso ',' programa ',' competencia ',' m√∫sica ',' objetivo ',' pel√≠cula ',' precio ',' juego ',' sistema ',' juego ',' compa√±√≠a ' "agradable"] </p><br><h3 id="esche-prilozheniya">  M√°s aplicaciones </h3><br><p>  Lo primero que me viene a la mente espec√≠ficamente es usar la distribuci√≥n de temas en el texto como 'incrustaciones' de textos, en esta interpretaci√≥n puede aplicarles algoritmos de visualizaci√≥n o agrupaci√≥n, y buscar los grupos tem√°ticos 'efectivos' finales de esta manera. </p><br><p>  Hagamos esto: </p><br><pre> <code class="python hljs">term_doc_matrix = count_vect.transform(names) embeddings = lda.transform(term_doc_matrix) kmeans = KMeans(n_clusters=<span class="hljs-number"><span class="hljs-number">30</span></span>) clust_labels = kmeans.fit_predict(embeddings) clust_centers = kmeans.cluster_centers_ embeddings_to_tsne = np.concatenate((embeddings,clust_centers), axis=<span class="hljs-number"><span class="hljs-number">0</span></span>) tSNE = TSNE(n_components=<span class="hljs-number"><span class="hljs-number">2</span></span>, perplexity=<span class="hljs-number"><span class="hljs-number">15</span></span>) tsne_embeddings = tSNE.fit_transform(embeddings_to_tsne) tsne_embeddings, centroids_embeddings = np.split(tsne_embeddings, [len(clust_labels)], axis=<span class="hljs-number"><span class="hljs-number">0</span></span>)</code> </pre> <br><p>  En la salida, obtenemos la siguiente imagen: <br><img src="https://habrastorage.org/webt/7j/2r/qv/7j2rqvpp2-blr9vfq44qjs75upc.png" alt="imagen"></p><br><p>  Las cruces son los centros de gravedad (cenroides) de los grupos. </p><br><p>  En la imagen tSNE de incrustaciones, se puede ver que los cl√∫steres seleccionados usando KMeans forman conjuntos bastante conectados y m√°s a menudo separables espacialmente. </p><br><p>  Todo lo dem√°s, depende de ti. </p><br><p>  Enlace a todo el c√≥digo: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://gitlab.com/Mozes/VK_LDA</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es417167/">https://habr.com/ru/post/es417167/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es417155/index.html">Linux kernel 4.18: qu√© se est√° preparando para el pr√≥ximo lanzamiento</a></li>
<li><a href="../es417157/index.html">La singularidad se acerca: la IA comienza a controlar los robots</a></li>
<li><a href="../es417161/index.html">Burger King: vigilancia secreta, mentiras, robo de tarjetas bancarias. Continuaci√≥n</a></li>
<li><a href="../es417163/index.html">Compromisos finos</a></li>
<li><a href="../es417165/index.html">Lo que amenaza a Burger King</a></li>
<li><a href="../es417171/index.html">Estudio: los fondos de cobertura administrados por mujeres muestran mejores resultados</a></li>
<li><a href="../es417173/index.html">"Old New Vinyl": 20 materiales sobre la historia y la producci√≥n de tocadiscos y discos</a></li>
<li><a href="../es417175/index.html">Restauraci√≥n de sem√°foros de carretera de Acme de la primera mitad del siglo XX</a></li>
<li><a href="../es417177/index.html">Servidor web local bajo Linux, con aumento autom√°tico de host y cambio de versi√≥n de PHP</a></li>
<li><a href="../es417179/index.html">Configuraci√≥n de un entorno de desarrollo dom√©stico (docker + gitlab + DNS)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>