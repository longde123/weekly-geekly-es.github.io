<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§ü üåø üñ±Ô∏è Glusterfs + erasure coding: saat Anda membutuhkan banyak, murah dan dapat diandalkan üë®üèæ‚Äçüéì ü•ë üèì</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hanya sedikit orang yang memiliki Glaster di Rusia, dan pengalaman apa pun menarik. Kami memilikinya besar dan industri dan, dilihat dari diskusi di p...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Glusterfs + erasure coding: saat Anda membutuhkan banyak, murah dan dapat diandalkan</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/croccloudteam/blog/417475/">  Hanya sedikit orang yang memiliki Glaster di Rusia, dan pengalaman apa pun menarik.  Kami memilikinya besar dan industri dan, dilihat dari diskusi di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">posting terakhir</a> , laris.  Saya berbicara tentang awal pengalaman memigrasikan cadangan dari penyimpanan Enterprise ke Glusterfs. <br><br>  Ini tidak cukup hardcore.  Kami tidak berhenti dan memutuskan untuk mengumpulkan sesuatu yang lebih serius.  Oleh karena itu, di sini kita akan berbicara tentang hal-hal seperti pengkodean penghapusan, pengabaian, penyeimbangan kembali dan pelambatannya, pengujian tekanan dan sebagainya. <br><br><img src="https://habrastorage.org/webt/8t/ol/2d/8tol2dsnki7fr_jfcvhxwldsxdk.jpeg"><br><br><ul><li>  Lebih banyak teori volum / subwolum </li><li>  cadangan panas </li><li>  menyembuhkan / menyembuhkan penuh / menyeimbangkan kembali </li><li>  Kesimpulan setelah reboot 3 node (tidak pernah melakukan ini) </li><li>  Bagaimana cara merekam dengan kecepatan berbeda dari VM yang berbeda dan on / off shard mempengaruhi beban subvolume? </li><li>  menyeimbangkan kembali setelah kepergian disk </li><li>  penyeimbangan kembali cepat </li></ul><br><a name="habracut"></a><h3>  Apa yang kamu inginkan </h3><br>  <b>Tugasnya sederhana:</b> untuk mengumpulkan toko yang murah tapi dapat diandalkan.  Murah mungkin, dapat diandalkan - sehingga tidak menakutkan untuk menyimpan file kita sendiri untuk dijual.  Sampai jumpa.  Kemudian, setelah tes panjang dan cadangan ke sistem penyimpanan lain - juga yang klien. <br><br>  <b>Aplikasi (IO berurutan)</b> : <br><br>  - Cadangan <br>  - Uji infrastruktur <br>  - Uji penyimpanan untuk file media berat. <br>  Kita disini <br>  - File pertempuran dan infrastruktur pengujian serius <br>  - Penyimpanan untuk data penting. <br><br>  Seperti terakhir kali, persyaratan utama adalah kecepatan jaringan antara instance Glaster.  10G pada awalnya baik-baik saja. <br><br><h3>  Teori: apa volume terdispersi? </h3><br>  Volume yang didistribusikan didasarkan pada teknologi erasure coding (EC), yang memberikan perlindungan yang cukup efektif terhadap kegagalan disk atau server.  Ini seperti RAID 5 atau 6, tetapi tidak juga.  Ia menyimpan fragmen yang disandikan dari file untuk setiap bata sedemikian rupa sehingga hanya sebagian dari fragmen yang disimpan dalam briks yang tersisa diperlukan untuk mengembalikan file.  Jumlah batu bata yang mungkin tidak tersedia tanpa kehilangan akses ke data dikonfigurasikan oleh administrator selama pembuatan volume. <br><br><img src="https://habrastorage.org/webt/rt/br/t1/rtbrt12s-0oyc9avxlsp32qzsus.png"><br><br><h3>  Apa itu subvolume? </h3><br>  Esensi dari subvolume dalam terminologi GlusterFS dimanifestasikan bersama dengan volume yang didistribusikan.  Dalam coding penghapusan terdistribusi-disperced akan bekerja hanya dalam kerangka subwoofer.  Dan dalam hal ini, misalnya, dengan data terdistribusi-direplikasi akan direplikasi dalam kerangka subwoofer. <br>  Masing-masing didistribusikan pada server yang berbeda, yang memungkinkan mereka untuk secara bebas kehilangan atau menyinkronkan output.  Pada gambar, server (fisik) ditandai dengan warna hijau, subwolves bertitik.  Masing-masing disajikan sebagai disk (volume) ke server aplikasi: <br><br><img src="https://habrastorage.org/webt/w-/fp/dj/w-fpdjqguwigusvhtprq_6su3z8.png"><br><br>  Diputuskan bahwa konfigurasi 4 + 2 yang terdistribusi tersebar pada 6 node terlihat cukup andal, kita bisa kehilangan 2 server atau 2 disk dalam setiap subwoofer, sambil terus memiliki akses ke data. <br><br>  Kami memiliki 6 DELL PowerEdge R510 tua dengan 12 slot disk dan 48x2TB 3,5 drive SATA.  Pada prinsipnya, jika ada server dengan 12 slot disk, dan memiliki hingga 12 TB drive di pasaran, kami dapat mengumpulkan penyimpanan hingga 576 TB ruang yang dapat digunakan.  Tetapi jangan lupa bahwa meskipun ukuran HDD maksimum terus tumbuh dari tahun ke tahun, kinerjanya tetap diam dan pembangunan kembali disk 10-12TB dapat membawa Anda seminggu. <br><br><img src="https://habrastorage.org/webt/xo/ud/6f/xoud6fl7sdtknj7vrbn_5fgslpu.png"><br><br>  <b>Pembuatan volume:</b> <br>  Penjelasan terperinci tentang cara menyiapkan batu bata, dapat Anda baca di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">posting</a> saya <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sebelumnya</a> <br><br><pre><code class="bash hljs">gluster volume create freezer disperse-data 4 redundancy 2 transport tcp \ $(<span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> {0..7} ; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> {sl051s,sl052s,sl053s,sl064s,sl075s,sl078s}:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick<span class="hljs-variable"><span class="hljs-variable">$i</span></span>/freezer ; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span>)</code> </pre> <br>  Kami membuat, tetapi kami tidak terburu-buru untuk meluncurkan dan memasang, karena kami masih harus menerapkan beberapa parameter penting. <br><br>  <b>Apa yang kita dapatkan:</b> <br><br><img src="https://habrastorage.org/webt/8j/il/e9/8jile9swj-qws3hgdjo3gtht3oo.png"><br><br>  Semuanya terlihat cukup normal, tetapi ada satu peringatan. <br><br>  <b>Terdiri dari rekaman volume seperti itu pada batu bata:</b> <br>  File ditempatkan satu per satu di subwolves, dan tidak tersebar merata di mereka, oleh karena itu, cepat atau lambat kita akan lari ke ukurannya, dan bukan ukuran seluruh volume.  Ukuran file maksimum yang dapat kita tempatkan dalam repositori ini adalah ukuran subwoofer yang dapat digunakan dikurangi ruang yang sudah ditempati di dalamnya.  Dalam kasus saya, ini &lt;8 Tb. <br><br>  <b>Apa yang harus dilakukan</b>  <b>Bagaimana menjadi?</b> <br>  Masalah ini diselesaikan dengan volume beling atau garis, tetapi, seperti yang telah ditunjukkan oleh praktik, garis bekerja sangat buruk. <br><br>  Karena itu, kami akan mencoba sharding. <br><br>  <b>Apa yang sharding, secara detail di</b> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . <br><br>  <b>Singkatnya, apa yang sharding</b> : <br>  Setiap file yang Anda masukkan ke dalam volume akan dibagi menjadi beberapa bagian (pecahan), yang secara relatif diatur dalam subwolves.  Ukuran beling ditentukan oleh administrator, nilai standarnya adalah 4 MB. <br><br>  <b>Hidupkan sharding setelah membuat volume, tetapi sebelum dimulai</b> : <br><br><pre> <code class="bash hljs">gluster volume <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> freezer features.shard on</code> </pre> <br>  <b>Kami mengatur ukuran beling (yang optimal? Dudes dari oVirt merekomendasikan 512MB)</b> : <br><br><pre> <code class="bash hljs">gluster volume <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> freezer features.shard-block-size 512MB</code> </pre> <br>  Secara empiris, ternyata ukuran sebenarnya dari pecahan dalam batu bata ketika menggunakan volume terdispersi 4 + 2 sama dengan ukuran blok-shard / 4, dalam kasus kami 512M / 4 = 128M. <br><br>  Setiap pecahan sesuai dengan logika pengkodean erasure didekomposisi sesuai dengan batu bata dalam kerangka dunia bawah dengan potongan-potongan ini: 4 * 128M + 2 * 128M <br><br>  <b>Gambar kasus kegagalan yang gluster bertahan dengan konfigurasi ini:</b> <br>  Dalam konfigurasi ini, kita dapat selamat dari jatuhnya 2 node atau 2 disk apa pun dalam subvolume yang sama. <br><br>  Untuk pengujian, kami memutuskan untuk menyelipkan penyimpanan yang dihasilkan di bawah cloud kami dan menjalankan fio dari mesin virtual. <br><br>  Kami mengaktifkan rekaman berurutan dari 15 VM dan melakukan hal berikut. <br><br>  <b>Reboot dari node 1:</b> <br>  17:09 <br>  Itu terlihat tidak kritis (~ 5 detik tidak tersedianya parameter ping.timeout). <br><br>  17:19 <br>  Diluncurkan menyembuhkan penuh. <br>  Jumlah entri penyembuhan hanya bertambah, mungkin karena tingginya tingkat penulisan ke cluster. <br><br>  17:32 <br>  Diputuskan untuk mematikan rekaman dari VM. <br>  Jumlah entri penyembuhan mulai menurun. <br><br>  17:50 <br>  sembuh sudah selesai. <br><br>  <b>Mulai ulang 2 node:</b> <br><br>  <i>Hasil yang sama diamati seperti pada simpul ke-1.</i> <br><br>  <b>Mulai ulang 3 node:</b> <br>  <i>Titik akhir yang dikeluarkan Titik akhir transportasi tidak terhubung, VM menerima ioerror.</i> <i><br></i>  <i>Setelah menyalakan node, Glaster memulihkan diri, tanpa gangguan dari pihak kami, dan proses perawatan dimulai.</i> <br><br>  Tapi 4 dari 15 VM tidak bisa naik.  Saya melihat kesalahan pada hypervisor: <br><br><pre> <code class="bash hljs">2018.04.27 13:21:32.719 ( volumes.py:0029): I: Attaching volume vol-BA3A1BE1 (/GLU/volumes/33/33e3bc8c-b53e-4230-b9be-b120079c0ce1) with attach <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> generic... 2018.04.27 13:21:32.721 ( qmp.py:0166): D: Querying QEMU: __com.redhat_drive_add({<span class="hljs-string"><span class="hljs-string">'file'</span></span>: u<span class="hljs-string"><span class="hljs-string">'/GLU/volumes/33/33e3bc8c-b53e-4230-b9be-b120079c0ce1'</span></span>, <span class="hljs-string"><span class="hljs-string">'iops_rd'</span></span>: 400, <span class="hljs-string"><span class="hljs-string">'media'</span></span>: <span class="hljs-string"><span class="hljs-string">'disk'</span></span>, <span class="hljs-string"><span class="hljs-string">'format'</span></span>: <span class="hljs-string"><span class="hljs-string">'qcow2'</span></span>, <span class="hljs-string"><span class="hljs-string">'cache'</span></span>: <span class="hljs-string"><span class="hljs-string">'none'</span></span>, <span class="hljs-string"><span class="hljs-string">'detect-zeroes'</span></span>: <span class="hljs-string"><span class="hljs-string">'unmap'</span></span>, <span class="hljs-string"><span class="hljs-string">'id'</span></span>: <span class="hljs-string"><span class="hljs-string">'qdev_1k7EzY85TIWm6-gTBorE3Q'</span></span>, <span class="hljs-string"><span class="hljs-string">'iops_wr'</span></span>: 400, <span class="hljs-string"><span class="hljs-string">'discard'</span></span>: <span class="hljs-string"><span class="hljs-string">'unmap'</span></span>})... 2018.04.27 13:21:32.784 ( instance.py:0298): E: Failed to attach volume vol-BA3A1BE1 to the instance: Device <span class="hljs-string"><span class="hljs-string">'qdev_1k7EzY85TIWm6-gTBorE3Q'</span></span> could not be initialized Traceback (most recent call last): File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/ic/instance.py"</span></span>, line 292, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> emulation_started c2.qemu.volumes.attach(controller.qemu(), device) File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/c2/qemu/volumes.py"</span></span>, line 36, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> attach c2.qemu.query(qemu, drive_meth, drive_args) File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/c2/qemu/_init_.py"</span></span>, line 247, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> query <span class="hljs-built_in"><span class="hljs-built_in">return</span></span> c2.qemu.qmp.query(qemu.pending_messages, qemu.qmp_socket, <span class="hljs-built_in"><span class="hljs-built_in">command</span></span>, args, suppress_logging) File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/c2/qemu/qmp.py"</span></span>, line 194, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> query message[<span class="hljs-string"><span class="hljs-string">"error"</span></span>].get(<span class="hljs-string"><span class="hljs-string">"desc"</span></span>, <span class="hljs-string"><span class="hljs-string">"Unknown error"</span></span>) QmpError: Device <span class="hljs-string"><span class="hljs-string">'qdev_1k7EzY85TIWm6-gTBorE3Q'</span></span> could not be initialized qemu-img: Could not open <span class="hljs-string"><span class="hljs-string">'/GLU/volumes/33/33e3bc8c-b53e-4230-b9be-b120079c0ce1'</span></span>: Could not <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> image <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> determining its format: Input/output error</code> </pre><br>  <b>Sulit melunasi 3 node dengan sharding dimatikan</b> <br><br><pre> <code class="bash hljs">Transport endpoint is not connected (107) /GLU/volumes/e0/e0bf9a42-8915-48f7-b509-2f6dd3f17549: ERROR: cannot <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> (Input/output error)</code> </pre> <br>  Kami juga kehilangan data, tidak mungkin untuk memulihkan. <br><br>  <b>Bayar 3 node dengan sharding dengan lembut, apakah akan ada kerusakan data?</b> <br>  Ada, tetapi jauh lebih sedikit (kebetulan?), Saya kehilangan 3 dari 30 drive. <br><br>  <b>Kesimpulan:</b> <br><br><ol><li>  Sembuhkan file-file ini hang tanpa henti, penyeimbangan kembali tidak membantu.  Kami menyimpulkan bahwa file yang merekam aktif terjadi ketika simpul ke-3 dimatikan hilang selamanya. </li><li>  Jangan pernah memuat ulang lebih dari 2 node dalam konfigurasi 4 + 2 dalam produksi! </li><li>  Bagaimana tidak kehilangan data jika Anda benar-benar ingin me-reboot 3+ node?  P Berhenti merekam pada titik pemasangan dan / atau menghentikan volume. </li><li>  Node atau bata harus diganti sesegera mungkin.  Untuk ini, sangat diinginkan untuk memiliki, misalnya, 1-2 a la batu bata cadangan panas di setiap simpul untuk penggantian cepat.  Dan satu lagi node cadangan dengan batu bata dalam kasus dump node. </li></ol><br><img src="https://habrastorage.org/webt/-m/rj/ti/-mrjtikde2imxdydeka4w-hvjjk.png"><br><br>  Penting juga untuk menguji kasus penggantian drive <br><br>  <b>Keberangkatan briks (disk):</b> <b><br></b>  <b>17:20</b> <br>  Kami merobohkan bata: <br><br><pre> <code class="bash hljs">/dev/sdh 1.9T 598G 1.3T 33% /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick6</code> </pre> <br>  <b>17:22</b> <br><pre> <code class="bash hljs">gluster volume replace-brick freezer sl051s:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick_spare_1/freezer sl051s:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick2/freezer commit force</code> </pre> <br>  Anda dapat melihat penarikan seperti itu pada saat mengganti bata (catatan dari 1 sumber): <br><br><img src="https://habrastorage.org/webt/jk/96/ns/jk96ns2kzhy0radczfpxlfpodso.png"><br><br>  Proses penggantian cukup lama, dengan tingkat perekaman kecil per kluster dan pengaturan default 1 TB, dibutuhkan sekitar satu hari untuk pulih. <br><br>  <b>Parameter yang dapat disesuaikan untuk perawatan:</b> <br><br><pre> <code class="bash hljs">gluster volume <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> cluster.background-self-heal-count 20 <span class="hljs-comment"><span class="hljs-comment"># Default Value: 8 # Description: This specifies the number of per client self-heal jobs that can perform parallel heals in the background. gluster volume set cluster.heal-timeout 500 # Default Value: 600 # Description: time interval for checking the need to self-heal in self-heal-daemon gluster volume set cluster.self-heal-window-size 2 # Default Value: 1 # Description: Maximum number blocks per file for which self-heal process would be applied simultaneously. gluster volume set cluster.data-self-heal-algorithm diff # Default Value: (null) # Description: Select between "full", "diff". The "full" algorithm copies the entire file from source to # sink. The "diff" algorithm copies to sink only those blocks whose checksums don't match with those of # source. If no option is configured the option is chosen dynamically as follows: If the file does not exist # on one of the sinks or empty file exists or if the source file size is about the same as page size the # entire file will be read and written ie "full" algo, otherwise "diff" algo is chosen. gluster volume set cluster.self-heal-readdir-size 2KB # Default Value: 1KB # Description: readdirp size for performing entry self-heal</span></span></code> </pre> <br>  <i>Opsi: disperse.background-heals</i> <i><br></i>  <i>Nilai Default: 8</i> <i><br></i>  <i>Deskripsi: Opsi ini dapat digunakan untuk mengontrol jumlah penyembuhan paralel</i> <i><br><br></i>  <i>Opsi: disperse.heal-wait-qlength</i> <i><br></i>  <i>Nilai Default: 128</i> <i><br></i>  <i>Deskripsi: Opsi ini dapat digunakan untuk mengontrol jumlah penyembuhan yang bisa menunggu</i> <i><br><br></i>  <i>Opsi: disperse.shd-max-uts</i> <i><br></i>  <i>Nilai Default: 1</i> <i><br></i>  <i>Deskripsi: Jumlah maksimum penyembuhan paralel yang bisa dilakukan SHD per bata lokal.</i>  <i>Ini secara substansial dapat menurunkan waktu penyembuhan, tetapi juga dapat menghancurkan batu bata Anda jika Anda tidak memiliki perangkat keras penyimpanan untuk mendukung ini.</i> <i><br><br></i>  <i>Opsi: disperse.shd-wait-qlength</i> <i><br></i>  <i>Nilai Default: 1024</i> <i><br></i>  <i>Deskripsi: Opsi ini dapat digunakan untuk mengontrol jumlah penyembuhan yang bisa menunggu dalam SHD per subvolume</i> <i><br><br></i>  <i>Opsi: disperse.cpu-extensions</i> <i><br></i>  <i>Nilai Default: otomatis</i> <i><br></i>  <i>Deskripsi: memaksa ekstensi cpu untuk digunakan untuk mempercepat perhitungan bidang galois.</i> <i><br><br></i>  <i>Opsi: disperse.self-heal-window-size</i> <i><br></i>  <i>Nilai Default: 1</i> <i><br></i>  <i>Deskripsi: Blok angka maksimum (128KB) per file yang proses penyembuhan sendiri akan diterapkan secara bersamaan.</i> <br><br>  Berdiri: <br><br><pre> <code class="bash hljs">disperse.shd-max-threads: 6 disperse.self-heal-window-size: 4 cluster.self-heal-readdir-size: 2KB cluster.data-self-heal-algorithm: diff cluster.self-heal-window-size: 2 cluster.heal-timeout: 500 cluster.background-self-heal-count: 20 cluster.disperse-self-heal-daemon: <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> disperse.background-heals: 18</code> </pre> <br>  Dengan parameter baru, 1 TB data selesai dalam 8 jam (3 kali lebih cepat!) <br><br>  <b>Momen yang tidak menyenangkan adalah bahwa hasilnya adalah brik yang lebih besar daripada sebelumnya</b> <br><br>  <b>adalah:</b> <pre> <code class="bash hljs">Filesystem Size Used Avail Use% Mounted on /dev/sdd 1.9T 645G 1.2T 35% /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick2</code> </pre> <br>  <b>menjadi:</b> <pre> <code class="bash hljs">Filesystem Size Used Avail Use% Mounted on /dev/sdj 1.9T 1019G 843G 55% /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/hot_spare_brick_0</code> </pre> <br>  Ini perlu dipahami.  Mungkin masalahnya adalah menggembungkan cakram tipis.  Dengan penggantian berikutnya dari peningkatan bata, ukurannya tetap sama. <br><br>  <b>Penyeimbangan ulang:</b> <br>  <i>Setelah memperluas atau mengecilkan (tanpa memigrasi data) volume (menggunakan perintah add-brick dan remove-brick masing-masing), Anda perlu menyeimbangkan kembali data di antara server.</i>  <i>Dalam volume non-direplikasi, semua batu bata harus bangun untuk melakukan operasi penggantian bata (opsi mulai).</i>  <i>Dalam volume yang direplikasi, setidaknya salah satu bata di replika harus naik.</i> <br><br>  <b>Membentuk penyeimbangan kembali:</b> <br><br>  <i>Opsi: cluster.rebal-throttle</i> <i><br></i>  <i>Nilai Default: normal</i> <i><br></i>  <i>Deskripsi: Menetapkan jumlah maksimum migrasi file paralel yang diizinkan pada sebuah node selama operasi penyeimbangan kembali.</i>  <i>Nilai default adalah normal dan memungkinkan maksimal file [($ (unit pemrosesan) - 4) / 2), 2] ke</i> <i><br></i>  <i>Kami bermigrasi pada satu waktu.</i>  <i>Malas hanya akan memungkinkan satu file untuk dimigrasi pada satu waktu dan agresif akan memungkinkan maksimum [($ (unit pemrosesan) - 4) / 2), 4]</i> <br><br>  <i>Opsi: cluster.lock-migrasi</i> <i><br></i>  <i>Nilai Default: tidak aktif</i> <i><br></i>  <i>Deskripsi: Jika diaktifkan, fitur ini akan memigrasi kunci posix yang terkait dengan file selama penyeimbangan kembali</i> <br><br>  <i>Opsi: cluster.weighted-rebalance</i> <i><br></i>  <i>Nilai Default: pada</i> <i><br></i>  <i>Deskripsi: Saat diaktifkan, file akan dialokasikan ke batu bata dengan probabilitas yang sebanding dengan ukurannya.</i>  <i>Kalau tidak, semua batu bata akan memiliki probabilitas yang sama (perilaku warisan).</i> <br><br>  <b>Perbandingan penulisan, dan kemudian membaca parameter fio yang sama (hasil tes kinerja lebih rinci - dalam PM):</b> <br><br><pre> <code class="bash hljs">fio --fallocate=keep --ioengine=libaio --direct=1 --buffered=0 --iodepth=1 --bs=64k --name=<span class="hljs-built_in"><span class="hljs-built_in">test</span></span> --rw=write/<span class="hljs-built_in"><span class="hljs-built_in">read</span></span> --filename=/dev/vdb --runtime=6000</code> </pre><br><img src="https://habrastorage.org/webt/fw/up/j0/fwupj0gj9m6vn25bepslaaox01e.png"><br><br><img src="https://habrastorage.org/webt/xi/yf/pd/xiyfpdsecqbfc52fudoz4nwklty.png"><br><br><img src="https://habrastorage.org/webt/nh/xp/es/nhxpestbf-gfjkcwogumbbqabc4.jpeg"><br><br>  <b>Jika menarik, bandingkan kecepatan rsync dengan lalu lintas ke node Glaster:</b> <br><br><img src="https://habrastorage.org/webt/6i/cz/ox/6iczoxword1qaauuhkwm3vfkk-q.png"><br><br><img src="https://habrastorage.org/webt/42/ka/eq/42kaeqkdbcuzhq8rwdrqybgkc5u.png"><br><br>  <i>Dapat dilihat bahwa sekitar 170 MB / s / traffic hingga 110 MB / s / payload.</i>  <i>Ternyata ini adalah 33% dari lalu lintas tambahan, serta 1/3 dari redundansi Coding Erasure.</i> <br><br>  <b>Konsumsi memori di sisi server dengan dan tanpa memuat tidak berubah:</b> <br><br><img src="https://habrastorage.org/webt/nz/sz/zc/nzszzcu0eqrck4gpmhvazhm8x7i.png"><br><img src="https://habrastorage.org/webt/nz/sz/zc/nzszzcu0eqrck4gpmhvazhm8x7i.png"><br><br>  <b>Beban pada host cluster dengan beban maksimum pada volume:</b> <br><br><img src="https://habrastorage.org/webt/vb/u3/3c/vbu33cgi-rmgjn7c2w1guz5ps3c.png"></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id417475/">https://habr.com/ru/post/id417475/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../hi486156/index.html">‡§ú‡•à‡§∏‡§æ ‡§ï‡§ø ‡§Æ‡•à‡§Ç‡§®‡•á ‡§™‡§¢‡§º‡§æ‡§Ø‡§æ, ‡§î‡§∞ ‡§´‡§ø‡§∞ ‡§™‡§æ‡§Ø‡§•‡§® ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§Æ‡•à‡§®‡•Å‡§Ö‡§≤ ‡§≤‡§ø‡§ñ‡§æ</a></li>
<li><a href="../hi486158/index.html">‡§µ‡§ø‡§ú‡§º‡•Å‡§Ö‡§≤‡§æ‡§á‡§ú‡§º‡§ø‡§Ç‡§ó ‡§®‡•ç‡§Ø‡•Ç‡§∞‡§≤ ‡§Æ‡§∂‡•Ä‡§® ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§≤‡•á‡§∂‡§® (seq2seq ‡§Æ‡•â‡§°‡§≤ ‡§µ‡§ø‡§• ‡§Ö‡§ü‡•á‡§Ç‡§∂‡§® ‡§Æ‡•à‡§ï‡•á‡§®‡§ø‡§ú‡§º‡•ç‡§Æ)</a></li>
<li><a href="../hi486164/index.html">‡§ï‡•ã‡§∞‡•ã‡§®‡§æ‡§µ‡§æ‡§Ø‡§∞‡§∏ 2019-nCoV‡•§ ‡§∂‡•ç‡§µ‡§∏‡§® ‡§∏‡§Ç‡§∞‡§ï‡•ç‡§∑‡§£ ‡§î‡§∞ ‡§ï‡•Ä‡§ü‡§æ‡§£‡•Å‡§∂‡•ã‡§ß‡§® ‡§™‡§∞ ‡§Ö‡§ï‡•ç‡§∏‡§∞ ‡§™‡•Ç‡§õ‡•á ‡§ú‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§™‡•ç‡§∞‡§∂‡•ç‡§®</a></li>
<li><a href="../hi486174/index.html">‡§Æ‡•á‡§∞‡§æ ‡§ú‡•Ä‡§∞‡•ã ‡§ü‡§∞‡•ç‡§®‡§ì‡§µ‡§∞ ‡§π‡•à</a></li>
<li><a href="../id417473/index.html">Penyimpanan Tepercaya dengan DRBD9 dan Proxmox (Bagian 1: NFS)</a></li>
<li><a href="../id417477/index.html">Meja panas</a></li>
<li><a href="../id417479/index.html">Penggabungan string do-it-yourself yang lebih cepat di Go</a></li>
<li><a href="../id417481/index.html">Tentang generator di JavaScript ES6, dan mengapa itu opsional untuk mempelajarinya</a></li>
<li><a href="../id417483/index.html">Perbandingan kerangka JS: React, Vue, dan Hyperapp</a></li>
<li><a href="../id417485/index.html">[bookmark] Lembar contekan administrator sistem untuk alat jaringan Linux</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>