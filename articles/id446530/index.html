<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ˜¡ â• ğŸ‘¨â€ğŸš€ Word2vec dalam gambar ğŸ¤ŸğŸ¾ ğŸ•“ â›¹ğŸ»</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="â€œ Setiap hal menyembunyikan pola yang merupakan bagian dari alam semesta. Ini memiliki simetri, keanggunan dan keindahan - kualitas yang pertama-tama ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Word2vec dalam gambar</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/446530/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/85d/ad8/627/85dad8627ae6845b62f5bb965c291b19.png"></div><br><br><blockquote>  <font color="gray">â€œ <b>Setiap hal menyembunyikan pola yang merupakan bagian dari alam semesta.</b></font>  <font color="gray"><b>Ini memiliki simetri, keanggunan dan keindahan</b> - kualitas yang pertama-tama dipahami oleh setiap seniman sejati yang menangkap dunia.</font>  <font color="gray">Pola ini dapat ditangkap dalam pergantian musim, dalam bagaimana pasir mengalir di sepanjang lereng, dalam cabang-cabang kusut semak kreosote, dalam pola daunnya.</font> <font color="gray"><br><br></font>  <font color="gray">Kami mencoba untuk meniru pola ini dalam kehidupan kami dan masyarakat kami dan oleh karena itu kami menyukai ritme, lagu, tarian, berbagai bentuk yang membuat kami bahagia dan menghibur kami.</font>  <font color="gray">Namun, orang juga dapat melihat bahaya yang mengintai dalam mencari kesempurnaan absolut, karena jelas bahwa pola yang sempurna tidak berubah.</font>  <font color="gray">Dan, mendekati kesempurnaan, semuanya akan mati â€- <i>Dune</i> (1965)</font> </blockquote><br>  Saya percaya konsep embeddings adalah salah satu ide paling luar biasa dalam pembelajaran mesin.  Jika Anda pernah menggunakan Siri, Google Assistant, Alexa, Google Translate atau bahkan keyboard ponsel cerdas dengan prediksi kata berikutnya, maka Anda telah bekerja dengan model pemrosesan bahasa alami berbasis lampiran.  Selama beberapa dekade terakhir, konsep ini telah berevolusi secara signifikan untuk model saraf (perkembangan terakhir termasuk embeddings kata kontekstual dalam model-model canggih seperti BERT dan GPT2). <br><a name="habracut"></a><br>  Word2vec adalah metode pembuatan investasi yang efektif yang dikembangkan pada 2013.  Selain bekerja dengan kata-kata, beberapa konsepnya ternyata efektif dalam mengembangkan mekanisme rekomendasi dan memberikan makna pada data bahkan dalam tugas komersial, non-linguistik.  Teknologi ini telah digunakan oleh perusahaan seperti <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Airbnb</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Alibaba</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Spotify,</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Anghami</a> di mesin rekomendasinya. <br><br>  Pada artikel ini, kita akan melihat konsep dan mekanisme menghasilkan lampiran menggunakan word2vec.  Mari kita mulai dengan contoh untuk membiasakan diri dengan cara merepresentasikan objek dalam bentuk vektor.  Apakah Anda tahu berapa banyak daftar lima angka (vektor) dapat katakan tentang kepribadian Anda? <br><br><h1>  Personalisasi: apa kamu? </h1><br><blockquote>  <font color="gray">"Aku memberimu Desert Chameleon;</font>  <font color="gray">kemampuannya untuk bergabung dengan pasir akan memberi tahu Anda segala yang perlu Anda ketahui tentang akar ekologi dan alasan untuk menjaga kepribadian Anda. "</font>  <font color="gray">- <i>Anak</i> - <i>anak Dune</i></font> </blockquote><br>  Pada skala 0 hingga 100, apakah Anda memiliki tipe kepribadian introvert atau ekstrovert (di mana 0 adalah tipe paling introvert, dan 100 adalah tipe paling ekstrovert)?  Pernahkah Anda lulus tes kepribadian: misalnya, MBTI, atau lebih baik <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">, Lima Besar</a> ?  Anda diberikan daftar pertanyaan dan kemudian dievaluasi pada beberapa sumbu, termasuk introversi / ekstroversi. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/79f/11e/e22/79f11ee220ebf9d6f52f51a5b780b090.png"></div><br>  <i><font color="gray">Contoh hasil tes Lima Besar.</font></i>  <i><font color="gray">Dia benar-benar mengatakan banyak tentang kepribadian dan mampu memprediksi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">keberhasilan</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">akademik</a> , <a href="">pribadi</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">profesional</a> .</font></i>  <i><font color="gray">Misalnya, di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> Anda dapat melewatinya.</font></i> <br><br>  Misalkan saya mendapat nilai 38 dari 100 untuk mengevaluasi introversi / extraversion.  Ini dapat direpresentasikan sebagai berikut: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e56/729/35d/e5672935d7de17d41e78354d3742e6bc.png"></div><br><br>  Atau pada skala âˆ’1 hingga +1: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b39/e23/ce1/b39e23ce1c036b11763e3c45c3659a3e.png"></div><br><br>  Seberapa baik kita mengenali seseorang hanya dari penilaian ini?  Tidak juga.  Manusia adalah makhluk yang kompleks.  Oleh karena itu, kami menambahkan satu dimensi lagi: satu lagi karakteristik dari tes. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2aa/ab2/ebc/2aaab2ebc1ff30f1fd832e5cf5bf9cb1.png"></div><br>  <i><font color="gray">Anda dapat membayangkan dua dimensi ini sebagai titik pada grafik, atau, bahkan lebih baik, sebagai vektor dari titik asal ke titik ini.</font></i>  <i><font color="gray">Ada alat vektor hebat yang berguna segera.</font></i> <br><br>  Saya tidak menunjukkan sifat kepribadian apa yang kami tempatkan pada bagan sehingga Anda tidak menjadi terikat pada sifat-sifat tertentu, tetapi segera memahami representasi vektor kepribadian orang tersebut secara keseluruhan. <br><br>  Sekarang kita dapat mengatakan bahwa vektor ini sebagian mencerminkan kepribadian saya.  Ini adalah deskripsi yang berguna ketika membandingkan orang yang berbeda.  Misalkan saya ditabrak bus merah, dan Anda perlu mengganti saya dengan orang yang sama.  Manakah dari dua orang di tabel berikut yang lebih mirip dengan saya? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/de5/380/b84/de5380b84dc9fec4bb8b52ebe6519e15.png"></div><br><br>  Ketika bekerja dengan vektor, kesamaan biasanya dihitung dengan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">koefisien Otiai</a> (koefisien geometris): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/640/e59/7dd/640e597dd741a28bcec986454633e31d.png"></div><br>  <i><font color="green">Orang No. 1</font> <font color="gray">lebih seperti karakter saya.</font></i>  <i><font color="gray">Vektor dalam satu arah (panjang juga penting) memberikan koefisien Otiai yang lebih besar</font></i> <br><br>  Sekali lagi, dua dimensi tidak cukup untuk mengevaluasi orang.  Dekade perkembangan ilmu psikologi telah mengarah pada penciptaan tes untuk lima karakteristik kepribadian dasar (dengan banyak yang tambahan).  Jadi, mari kita gunakan semua lima dimensi: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/df5/3ae/d7b/df53aed7b1e439561a01e69b3f765487.png"></div><br><br>  Masalah dengan lima dimensi adalah bahwa tidak mungkin lagi menggambar panah yang rapi dalam 2D.  Ini adalah masalah umum dalam pembelajaran mesin, di mana Anda sering harus bekerja dalam ruang multidimensi.  Baik bahwa koefisien geometrik bekerja dengan sejumlah pengukuran: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/521/ab3/bf1/521ab3bf1374c5b37115441b7c2d27cc.png"></div><br>  <i><font color="gray">Koefisien geometrik bekerja untuk sejumlah pengukuran.</font></i>  <i><font color="gray">Dalam lima dimensi, hasilnya jauh lebih akurat.</font></i> <br><br>  Di akhir bab ini saya ingin mengulangi dua ide utama: <br><br><ol><li>  Orang (dan objek lain) dapat direpresentasikan sebagai vektor numerik (yang bagus untuk mobil!). <br></li><li>  Kita dapat dengan mudah menghitung seberapa mirip vektor-vektor itu. </li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/195/73d/16d/19573d16de1150ac1874640c79e0b381.png"></div><br><br><h1>  Penyisipan kata </h1><br><blockquote>  <font color="gray">"Karunia kata-kata adalah karunia penipuan dan ilusi."</font>  <font color="gray">- <i>Anak</i> - <i>anak Dune</i></font> </blockquote><br>  Dengan pemahaman ini, kita akan beralih ke representasi vektor dari kata-kata yang diperoleh sebagai hasil pelatihan (mereka juga disebut lampiran) dan melihat sifat-sifatnya yang menarik. <br><br>  Berikut adalah lampiran untuk kata "raja" (vektor GloVe, dilatih di Wikipedia): <br><br> <code>[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]</code> <br> <br>  Kami melihat daftar 50 angka, tetapi sulit untuk mengatakan sesuatu.  Mari kita visualisasikan untuk dibandingkan dengan vektor lain.  Masukkan angka dalam satu baris: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/516/c90/5ac/516c905ac831fe8688db73f0a63d325b.png"></div><br><br>  Warnai sel dengan nilainya (merah untuk mendekati 2, putih untuk dekat dengan 0, biru untuk dekat dengan âˆ’2): <br><br><div style="text-align:center;"> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/46f/7cb/1d5/46f7cb1d5adc32bd16368b2681ab26a4.png"></a> </div><br><br>  Sekarang lupakan angkanya, dan hanya dengan warna kita kontraskan "raja" dengan kata lain: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1c8/6b2/909/1c86b290963e8a42b375cb6a71245185.png"></div><br><br>  Anda lihat bahwa "pria" dan "wanita" lebih dekat satu sama lain daripada dengan "raja"?  Itu mengatakan sesuatu.  Representasi vektor menangkap banyak informasi / makna / asosiasi kata-kata ini. <br><br>  Berikut daftar contoh lainnya (bandingkan kolom dengan warna yang sama): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d61/30b/d45/d6130bd4502710631a8c812923230f47.png"></div><br><br>  Ada beberapa hal yang perlu diperhatikan: <br><br><ol><li>  Melalui semua kata berjalan satu kolom merah.  Artinya, kata-kata ini serupa dalam dimensi khusus ini (dan kita tidak tahu apa yang dikodekan di dalamnya). <br></li><li>  Anda dapat melihat bahwa "wanita" dan "gadis" sangat mirip.  Hal yang sama dengan "pria" dan "anak laki-laki". <br></li><li>  "Anak laki-laki" dan "anak perempuan" juga serupa dalam beberapa dimensi, tetapi berbeda dari "wanita" dan "pria".  Mungkinkah ini gagasan samar kode pemuda?  Mungkin <br></li><li>  Semuanya kecuali kata terakhir adalah ide orang.  Saya menambahkan objek (air) untuk menunjukkan perbedaan antara kategori.  Misalnya, Anda dapat melihat bagaimana kolom biru turun dan berhenti di depan vektor air. <br></li><li>  Ada dimensi yang jelas di mana "raja" dan "ratu" mirip satu sama lain dan berbeda dari orang lain.  Mungkin konsep royalti yang samar dikodekan di sana? </li></ol><br><h1>  Analogi </h1><br><blockquote>  <font color="gray">â€œKata-kata menanggung beban apa pun yang kita inginkan.</font>  <font color="gray">Yang diperlukan hanyalah kesepakatan tentang tradisi, yang dengannya kami membangun konsep. â€</font>  <font color="gray">- <i>Dewa Kaisar Dune</i></font> </blockquote><br>  Contoh terkenal yang menunjukkan sifat luar biasa dari investasi adalah konsep analogi.  Kita dapat menambah dan mengurangi vektor kata, mendapatkan hasil yang menarik.  Contoh paling terkenal adalah formula "raja - laki-laki + perempuan": <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c23/71f/ead/c2371feadc58f2f2a1236c94b6b05eff.png"></div><br>  <i><font color="gray">Menggunakan pustaka <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Gensim</a> dalam python, kita dapat menambah dan mengurangi vektor kata, dan pustaka akan menemukan kata-kata yang paling dekat dengan vektor yang dihasilkan.</font></i>  <i><font color="gray">Gambar menunjukkan daftar kata-kata yang paling mirip, masing-masing dengan koefisien kesamaan geometris</font></i> <br><br>  Kami memvisualisasikan analogi ini seperti sebelumnya: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a19/84b/fea/a1984bfeab5a597c6fb6300f7d694901.png"></div><br>  <i><font color="gray">Vektor yang dihasilkan dari perhitungan "raja - laki-laki + perempuan" tidak cukup sama dengan "ratu", tetapi ini adalah hasil terdekat dari 400.000 lampiran kata dalam kumpulan data</font></i> <br><br>  Setelah mempertimbangkan keterikatan kata-kata, mari kita belajar bagaimana pembelajaran terjadi.  Tetapi sebelum beralih ke word2vec, Anda perlu melihat leluhur konseptual kata embedding: model bahasa saraf. <br><br><h1>  Model bahasa </h1><br><blockquote>  <font color="gray">â€œNabi tidak tunduk pada ilusi masa lalu, sekarang atau masa depan.</font>  <font color="gray"><b>Fixity bentuk linguistik menentukan perbedaan linear tersebut.</b></font>  <font color="gray">Para nabi memegang kunci untuk mengunci lidah.</font>  <font color="gray">Bagi mereka, citra fisik tetap hanya citra fisik dan tidak lebih.</font> <font color="gray"><br><br></font>  <font color="gray">Alam semesta mereka tidak memiliki sifat-sifat alam semesta mekanis.</font>  <font color="gray">Urutan linear kejadian diasumsikan oleh pengamat.</font>  <font color="gray">Penyebab dan akibat?</font>  <font color="gray">Ini masalah yang sangat berbeda.</font>  <font color="gray">Nabi mengucapkan kata-kata yang menentukan.</font>  <font color="gray">Anda melihat sekilas peristiwa yang seharusnya terjadi "sesuai dengan logika segala sesuatu."</font>  <font color="gray">Tetapi sang nabi secara instan melepaskan energi dari kekuatan ajaib yang tak terbatas.</font>  <font color="gray">Alam semesta sedang mengalami perubahan spiritual. â€</font>  <font color="gray">- <i>Dewa Kaisar Dune</i></font> </blockquote><br>  Salah satu contoh NLP (Natural Language Processing) adalah fungsi prediksi kata berikutnya pada keyboard smartphone.  Miliaran orang menggunakannya ratusan kali sehari. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ca4/d48/a13/ca4d48a133d58fe3c4c11e0933ea218e.png"></div><br><br>  Memprediksi kata berikutnya adalah tugas yang cocok untuk <i>model bahasa</i> .  Dia dapat mengambil daftar kata-kata (katakanlah, dua kata) dan mencoba memprediksi yang berikut ini. <br><br>  Pada tangkapan layar di atas, model mengambil dua kata hijau ini ( <code>thou shalt</code> ) dan mengembalikan daftar opsi (kemungkinan besar untuk kata <code>not</code> ): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5a7/0fc/492/5a70fc49208b501202ed188f24ad1f2c.png"></div><br><br>  Kita bisa membayangkan model sebagai kotak hitam: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/164/72f/a83/16472fa83e5eadf58f4bb05b50075654.png"></div><br><br>  Namun dalam praktiknya, model tersebut menghasilkan lebih dari satu kata.  Ini memperoleh perkiraan probabilitas untuk hampir semua kata yang dikenal ("kamus" model bervariasi dari beberapa ribu hingga lebih dari satu juta kata).  Aplikasi keyboard kemudian menemukan kata-kata dengan skor tertinggi dan menunjukkannya kepada pengguna. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7a7/eda/ad6/7a7edaad67dd51240d90426de0b198c2.png"></div><br>  <i><font color="gray">Model bahasa saraf memberikan kemungkinan semua kata diketahui.</font></i>  <i><font color="gray">Kami menunjukkan probabilitas sebagai persentase, tetapi dalam vektor yang dihasilkan 40% akan direpresentasikan sebagai 0,4</font></i> <br><br>  Setelah pelatihan, model saraf pertama ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bengio 2003</a> ) menghitung prognosis dalam tiga tahap: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/27b/082/4f8/27b0824f81962e2863d6d4dcccabfdd2.png"></div><br><br>  Langkah pertama bagi kami adalah yang paling relevan, karena kami membahas investasi.  Sebagai hasil dari pelatihan, sebuah matriks dibuat dengan lampiran dari semua kata dalam kamus kami.  Untuk mendapatkan hasilnya, kita cukup mencari embeddings dari kata-kata input dan menjalankan prediksi: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1d1/34b/5ac/1d134b5ac32406ea363944887ce5fc53.png"></div><br><br>  Sekarang mari kita lihat proses pembelajaran dan cari tahu bagaimana matriks investasi ini dibuat. <br><br><h1>  Pelatihan model bahasa </h1><br><blockquote>  <font color="gray">â€œProsesnya tidak bisa dipahami dengan mengakhirinya.</font>  <font color="gray">Pemahaman harus bergerak seiring dengan proses, bergabung dengan alirannya dan mengalir bersamanya â€- <i>Dune</i></font> </blockquote><br>  Model bahasa memiliki keunggulan besar dibandingkan sebagian besar model pembelajaran mesin lainnya: mereka dapat dilatih tentang teks-teks yang kita miliki dengan berlimpah.  Pikirkan semua buku, artikel, bahan Wikipedia, dan bentuk data tekstual lain yang kita miliki.  Bandingkan dengan model pembelajaran mesin lainnya yang membutuhkan kerja manual dan data yang dikumpulkan secara khusus. <br><br><blockquote>  <b>"Anda harus belajar kata dari perusahaannya" - J. R. Furs</b> </blockquote><br>  Lampiran untuk kata-kata dihitung berdasarkan kata-kata di sekitarnya, yang lebih sering muncul di dekatnya.  Mekaniknya adalah sebagai berikut: <br><br><ol><li>  Kami mendapatkan banyak data teks (katakanlah, semua artikel Wikipedia) <br></li><li>  Atur jendela (misalnya, dari tiga kata) yang meluncur di seluruh teks. <br></li><li>  Jendela geser menghasilkan pola untuk melatih model kita. </li></ol><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a31/fc4/626/a31fc4626de165a21c2c91844b21e7ab.png"></div><br><br>  Ketika jendela ini melayang di atas teks, kita (sebenarnya) menghasilkan kumpulan data, yang kemudian kita gunakan untuk melatih model.  Untuk memahami, mari kita lihat bagaimana jendela geser menangani frasa ini: <br><br><blockquote>  <b>"Semoga kamu tidak membangun mesin yang memiliki kesamaan pikiran manusia" - <i>Dune</i></b> </blockquote><br>  Ketika kita mulai, jendela terletak pada tiga kata pertama dari kalimat: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/81c/c51/a04/81cc51a0478e1655c8f3f85641cf1e4e.png"></div><br><br>  Kami mengambil dua kata pertama untuk tanda, dan kata ketiga untuk label: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/097/981/086/0979810868ca398fdcad3066294055f5.png"></div><br>  <i><font color="gray">Kami menghasilkan sampel pertama dalam dataset yang kemudian dapat digunakan untuk mengajarkan model bahasa</font></i> <br><br>  Kemudian kami memindahkan jendela ke posisi berikutnya dan membuat sampel kedua: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/98c/0b3/f98/98c0b3f98ebf4790890fd2f66cf86ce9.png"></div><br><br>  Dan segera, kami mengumpulkan kumpulan data yang lebih besar: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4e7/3ce/50d/4e73ce50d863e1cbfde92a3b595dbaa3.png"></div><br><br>  Dalam praktiknya, model biasanya dilatih langsung dalam proses memindahkan jendela geser.  Namun secara logis, fase â€œpembuatan kumpulan dataâ€ terpisah dari fase pelatihan.  Selain pendekatan jaringan saraf, metode N-gram sering digunakan sebelumnya untuk model pengajaran bahasa (lihat bab ketiga buku <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">"Pidato dan Pemrosesan Bahasa"</a> ).  Untuk melihat perbedaan ketika beralih dari N-gram ke model neural dalam produk nyata, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">inilah posting 2015 di blog Swiftkey</a> , pengembang keyboard Android favorit saya, yang menyajikan model bahasa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">neuralnya</a> dan membandingkannya dengan model N-gram sebelumnya.  Saya suka contoh ini karena ini menunjukkan bagaimana properti algoritmik investasi dapat dijelaskan dalam bahasa pemasaran. <br><br><h1>  Kami melihat dua arah </h1><br><blockquote>  <font color="gray">â€œParadoks adalah tanda bahwa kita harus mencoba mempertimbangkan apa yang ada di baliknya.</font>  <font color="gray">Jika paradoks itu membuat Anda prihatin, itu berarti Anda berjuang untuk yang absolut.</font>  <font color="gray">Relativis memandang paradoks hanya sebagai pemikiran yang menarik, mungkin lucu, terkadang menakutkan, tetapi pemikiran yang sangat instruktif. â€</font>  <font color="gray"><i>Kaisar Dewa Dune</i></font> </blockquote><br>  Berdasarkan hal tersebut di atas, isi celah: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/680/613/871/680613871307e53415ab86fab022276a.png"></div><br><br>  Sebagai konteks, ada lima kata sebelumnya (dan referensi sebelumnya untuk "bus").  Saya yakin sebagian besar dari Anda menduga bahwa seharusnya ada "bus".  Tetapi jika saya memberi Anda kata lain setelah ruang, apakah ini akan mengubah jawaban Anda? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/45f/1fa/af0/45f1faaf0cdd4f57ac1699d87861934a.png"></div><br><br>  Ini sepenuhnya mengubah situasi: sekarang kata yang hilang kemungkinan besar "merah".  Jelas, kata-kata memiliki nilai informasi sebelum dan sesudah spasi.  Ternyata akuntansi di kedua arah (kiri dan kanan) memungkinkan Anda untuk menghitung investasi yang lebih baik.  Mari kita lihat bagaimana mengkonfigurasi pelatihan model dalam situasi seperti itu. <br><br><h1>  Lewati gram </h1><br><blockquote>  <font color="gray">â€œKetika pilihan yang benar-benar tidak salah tidak diketahui, intelijen mendapat kesempatan untuk bekerja dengan data yang terbatas di arena, di mana kesalahan tidak hanya mungkin terjadi, tetapi juga perlu.</font>  <font color="gray">- <i>Capitul Dunes</i></font> </blockquote><br>  Selain dua kata sebelum target, Anda dapat memperhitungkan dua kata lagi setelahnya. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e2b/1f6/1a1/e2b1f61a179e7d6835b47c7149a47486.png"></div><br><br>  Kemudian kumpulan data untuk model pelatihan akan terlihat seperti ini: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6ff/729/ed4/6ff729ed4ce86722dc9c3aa689614195.png"></div><br><br>  Ini disebut arsitektur CBOW (Continuous Bag of Words) dan dijelaskan dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">salah satu dokumen word2vec</a> [pdf].  Ada arsitektur lain, yang juga menunjukkan hasil yang sangat baik, tetapi disusun sedikit berbeda: ia mencoba menebak kata-kata tetangga dengan kata saat ini.  Jendela geser terlihat seperti ini: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc9/72d/baa/dc972dbaa78b592ba91b76e950ec56e0.png"></div><br>  <i><font color="gray">Di slot hijau adalah kata input, dan setiap bidang pink mewakili kemungkinan keluar</font></i> <br><br>  Persegi panjang merah muda memiliki nuansa yang berbeda karena jendela geser ini sebenarnya menciptakan empat pola terpisah dalam dataset pelatihan kami: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/709/8ac/dde/7098acddea8266d1efd5663ed98e6303.png"></div><br><br>  Metode ini disebut arsitektur <b>skip-gram</b> .  Anda dapat memvisualisasikan jendela geser sebagai berikut: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ee2/1d8/508/ee21d850835bde9e3f14250d267d88b1.png"></div><br><br>  Empat sampel berikut ditambahkan ke dataset pelatihan: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a40/871/f1c/a40871f1c1c7b48723d3737c05fc6284.png"></div><br><br>  Kemudian kami memindahkan jendela ke posisi berikut: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/14a/429/c7b/14a429c7b2ae6ba7383d6d39be9e3031.png"></div><br><br>  Yang menghasilkan empat contoh lagi: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e9b/b3c/89a/e9bb3c89a00306b3fd18eb86d8f2160b.png"></div><br><br>  Kami akan segera memiliki lebih banyak sampel: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6bb/749/096/6bb749096d3329712a7c00727b4d3cff.png"></div><br><br><h1>  Review Belajar </h1><br><blockquote>  <font color="gray">â€œMuad'Dib adalah pembelajar yang cepat karena dia terutama diajari cara belajar.</font>  <font color="gray">Tetapi pelajaran pertama adalah asimilasi keyakinan bahwa ia dapat belajar, dan itu adalah dasar dari segalanya.</font>  <font color="gray">Sungguh menakjubkan betapa banyak orang tidak percaya bahwa mereka dapat belajar dan belajar, dan berapa banyak lagi orang yang berpikir bahwa belajar itu sangat sulit. "</font>  <font color="gray">- <i>Dune</i></font> </blockquote><br>  Sekarang setelah kita memiliki set lompat-gram, kita menggunakannya untuk melatih model saraf dasar bahasa yang memprediksi kata tetangga. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/944/fb7/70d/944fb770d3aff38f1befa40dfaa7402a.png"></div><br><br>  Mari kita mulai dengan sampel pertama dalam kumpulan data kami.  Kami mengambil tanda dan mengirimkannya ke model yang tidak terlatih dengan permintaan untuk memprediksi kata berikutnya. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/abb/5cb/9a3/abb5cb9a38d29f1a54176206637131dc.png"></div><br><br>  Model melewati tiga langkah dan menampilkan vektor prediksi (dengan probabilitas untuk setiap kata dalam kamus).  Karena model ini tidak terlatih, pada tahap ini perkiraannya mungkin salah.  Tapi itu bukan apa-apa.  Kami tahu kata yang diprediksinya - ini adalah sel yang dihasilkan di baris yang saat ini kami gunakan untuk melatih model: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8af/4fd/c3d/8af4fdc3d3cc86ec1c81fdb3d2715529.png"></div><br>  <i><font color="gray">â€œVektor targetâ€ adalah kata di mana kata target memiliki probabilitas 1, dan semua kata lain memiliki probabilitas 0</font></i> <br><br>  Bagaimana model yang salah?  Kurangi vektor perkiraan dari target dan dapatkan vektor kesalahan: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e6d/b3e/395/e6db3e39593e9c8639d94ef4caccde58.png"></div><br><br>  Vektor kesalahan ini sekarang dapat digunakan untuk memperbarui model, jadi lain kali kemungkinan besar akan memberikan hasil yang akurat pada data input yang sama. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7d3/6c0/476/7d36c047604b937c907a4ef38ceaaeb7.png"></div><br><br>  Di sini tahap pertama pelatihan berakhir.  Kami terus melakukan hal yang sama dengan sampel berikutnya dalam kumpulan data, dan kemudian dengan yang berikutnya, sampai kami memeriksa semua sampel.  Ini adalah akhir dari era pembelajaran pertama.  Kami mengulang semuanya berulang-ulang untuk beberapa era, dan sebagai hasilnya kami mendapatkan model yang terlatih: dari sana Anda dapat mengekstrak matriks investasi dan menggunakannya dalam aplikasi apa pun. <br><br>  Meskipun kami belajar banyak, tetapi untuk sepenuhnya memahami bagaimana word2vec benar-benar belajar, beberapa ide kunci hilang. <br><br><h1>  Seleksi negatif </h1><br><blockquote>  <font color="gray">"Berusaha memahami Muad'Dib tanpa memahami musuh bebuyutannya - Harkonnenov - sama dengan mencoba memahami Kebenaran tanpa memahami apa itu Kepalsuan.</font>  <font color="gray">Ini adalah upaya untuk mengetahui Cahaya tanpa mengetahui Kegelapan.</font>  <font color="gray">Ini tidak mungkin. "</font>  <font color="gray">- <i>Dune</i></font> </blockquote><br>  Ingat tiga langkah bagaimana model saraf menghitung perkiraan: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8dd/fe1/4ca/8ddfe14ca387bd4d16c77eb9de8ce98f.png"></div><br><br>  Langkah ketiga sangat mahal dari sudut pandang komputasi, terutama jika Anda melakukannya untuk setiap sampel dalam kumpulan data (puluhan juta kali).  Penting untuk meningkatkan produktivitas. <br><br>  Salah satu caranya adalah dengan membagi tujuan menjadi dua tahap: <br><br><ol><li>  Buat lampiran kata berkualitas tinggi (tanpa memprediksi kata berikutnya). <br></li><li>  Gunakan investasi berkualitas tinggi ini untuk mengajar model bahasa (untuk perkiraan). </li></ol><br>  Artikel ini akan fokus pada langkah pertama.  Untuk meningkatkan produktivitas, Anda dapat beralih dari memprediksi kata tetangga ... <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/77d/0a8/c17/77d0a8c17587248a0f790155809798fe.png"></div><br><br>  ... dan beralih ke model yang mengambil input dan output kata-kata dan menghitung probabilitas kedekatannya (dari 0 ke 1). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/220/0e3/e06/2200e3e063f3119099d1615e59538d2a.png"></div><br><br>  Transisi sederhana seperti itu menggantikan jaringan saraf dengan model regresi logistik - dengan demikian, perhitungan menjadi lebih sederhana dan lebih cepat. <br><br>  Pada saat yang sama, kita perlu memperbaiki struktur set data kami: label sekarang menjadi kolom baru dengan nilai 0 atau 1. Dalam tabel kami, unit ada di mana-mana, karena kami menambahkan tetangga di sana. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc2/d1e/874/dc2d1e87438b2492dc9b6e4b1c72162e.png"></div><br><br>  Model seperti itu dihitung dengan kecepatan luar biasa: jutaan sampel dalam hitungan menit.  Tetapi Anda perlu menutup satu celah.  Jika semua contoh kami positif (sasaran: 1), maka model rumit dapat membentuk yang selalu menghasilkan 1, menunjukkan akurasi 100%, tetapi tidak mempelajari apa pun dan menghasilkan investasi sampah. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1ba/aa2/2d0/1baaa22d0b0c06be5398f896fa7a4c4b.png"></div><br><br>  Untuk mengatasi masalah ini, Anda harus memasukkan <i>pola negatif</i> ke dalam kumpulan data - kata-kata yang jelas bukan tetangga.  Bagi mereka, model harus kembali 0. Sekarang model harus bekerja keras, tetapi perhitungan tetap berjalan dengan sangat cepat. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f4c/194/0d8/f4c1940d80c5203620196907a1478431.png"></div><br>  <i><font color="gray">Untuk setiap sampel dalam dataset, tambahkan contoh negatif berlabel 0</font></i> <br><br>  Tapi apa yang harus diperkenalkan sebagai kata-kata keluaran?  Pilih kata-kata secara sewenang-wenang: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/84e/b22/06f/84eb2206f26b053f1ea8ec4e1b76c5b6.png"></div><br><br>  Ide ini lahir di bawah pengaruh metode <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">perbandingan kebisingan</a> [pdf].  Kami mencocokkan sinyal aktual (contoh positif dari kata-kata tetangga) dengan noise (kata-kata yang dipilih secara acak yang bukan tetangga).  Ini memberikan kompromi yang sangat baik antara kinerja dan kinerja statistik. <br><br><h1>  Sampel Negatif Lewati-gram (SGNS) </h1><br>  Kami melihat dua konsep utama dari word2vec: bersama-sama mereka disebut "melewatkan-gram dengan pengambilan sampel negatif". <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/873/720/fae/873720fae559ce7d6020be66ccb6c397.png"></div><br><br><h1>  Belajar word2vec </h1><br><blockquote>  <font color="gray">â€œMesin tidak dapat melihat setiap masalah yang penting bagi orang yang hidup.</font>  <font color="gray">Ada perbedaan besar antara ruang diskrit dan kontinum kontinu.</font>  <font color="gray">Kami tinggal di satu ruang, dan mesin ada di ruang lain. â€</font>  <font color="gray">- <i>Dewa Kaisar Dune</i></font> </blockquote><br>  Setelah memeriksa ide-ide dasar pengambilan sampel skip-gram dan negatif, kita dapat melanjutkan untuk melihat lebih dekat pada proses pembelajaran word2vec. <br><br>  Pertama, kami melakukan pra-proses teks di mana kami melatih model.  Tentukan ukuran kamus (kami akan menyebutnya <code>vocab_size</code> ), katakanlah, dalam 10.000 lampiran dan parameter dari kata-kata dalam kamus. <br><br>  Di awal pelatihan, kami membuat dua matriks: <code>Embedding</code> dan <code>Context</code> .  Lampiran untuk setiap kata disimpan dalam matriks ini dalam kamus kami (oleh karena itu <code>vocab_size</code> adalah salah satu parameternya).  Parameter kedua adalah dimensi lampiran (biasanya <code>embedding_size</code> diatur ke 300, tetapi sebelumnya kita melihat contoh dengan 50 dimensi). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1b7/8f8/018/1b78f8018d20fd36d0c5aef37d87a249.png"></div><br><br>  Pertama, kami menginisialisasi matriks ini dengan nilai acak.  Kemudian kami memulai proses pembelajaran.  Pada setiap tahap, kami mengambil satu contoh positif dan yang negatif yang terkait dengannya.  Inilah grup pertama kami: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2d2/261/806/2d22618069aa9e3f8a820cb431c6c014.png"></div><br><br>  Kami sekarang memiliki empat kata: kata input <code>not</code> dan kata <code>not</code> kata output / kontekstual <code>thou</code> (tetangga sebenarnya), <code>aaron</code> dan <code>taco</code> (contoh negatif).  Kami mulai mencari lampiran mereka dalam matriks <code>Embedding</code> (untuk kata input) dan <code>Context</code> (untuk kata konteks), meskipun kedua matriks berisi lampiran untuk semua kata dari kamus kami. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/23e/ff0/691/23eff069128db956ce358ae758c0b8bb.png"></div><br><br>  Kemudian kami menghitung produk skalar dari lampiran input dengan masing-masing lampiran kontekstual.  Dalam setiap kasus, diperoleh angka yang menunjukkan kesamaan input data dan lampiran kontekstual. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/615/319/e4a/615319e4accc7235c28fc8c76dca09f6.png"></div><br><br>  Sekarang kita perlu cara untuk mengubah estimasi ini menjadi semacam kemungkinan: semuanya harus angka positif antara 0 dan 1. Ini adalah tugas yang sangat baik untuk persamaan logistik <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sigmoid</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/98b/125/8b2/98b1258b21744917c993e617e0844ad8.png"></div><br><br>  Hasil perhitungan sigmoid dapat dianggap sebagai output dari model untuk sampel ini.  Seperti yang Anda lihat, <code>taco</code> skor tertinggi, sementara <code>aaron</code> masih memiliki skor terendah, baik sebelum dan sesudah sigmoid. <br><br>  Ketika model yang tidak terlatih membuat perkiraan dan memiliki tanda target nyata untuk perbandingan, mari kita hitung berapa banyak kesalahan dalam perkiraan model.  Untuk melakukan ini, cukup kurangi skor sigmoid dari label target. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d49/458/333/d49458333c28225c596014df3c6fcedb.png"></div><br>  <i><font color="gray"><code>error</code> = <code>target</code> - <code>sigmoid_scores</code></font></i> <br><br>  Di sinilah fase "belajar" dari istilah "pembelajaran mesin" dimulai.  Sekarang kita dapat menggunakan estimasi kesalahan ini untuk menyesuaikan investasi <code>not</code> , <code>thou</code> , <code>aaron</code> dan <code>taco</code> , sehingga hasilnya nanti lebih dekat dengan perkiraan target. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a5d/e51/cd3/a5de51cd3a86a1ed0784a709cb979bdc.png"></div><br><br>  Ini menyelesaikan satu tahap pelatihan.  Kami sedikit memperbaiki keterikatan beberapa kata ( <code>not</code> , <code>aaron</code> , dan <code>taco</code> ).  Sekarang kita beralih ke tahap berikutnya (sampel positif berikutnya dan yang negatif yang terkait dengannya) dan ulangi prosesnya. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/637/2ab/c3a/6372abc3a3f6b623d5b2cbab02953030.png"></div><br><br>  Lampiran terus meningkat saat kami menggilir seluruh set data beberapa kali.  Anda kemudian dapat menghentikan proses, mengesampingkan matriks <code>Context</code> , dan menggunakan matriks <code>Embeddings</code> terlatih untuk tugas berikutnya. <br><br><h1>  Ukuran jendela dan jumlah sampel negatif </h1><br>  Dalam proses belajar word2vec, dua hiperparameter kunci adalah ukuran jendela dan jumlah sampel negatif. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9fe/437/447/9fe4374479f547c1b324c7471cd61cbd.png"></div><br><br>  Ukuran jendela yang berbeda cocok untuk tugas yang berbeda.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Telah diperhatikan</a> bahwa ukuran jendela yang lebih kecil (2â€“15) menghasilkan lampiran yang <i>dapat dipertukarkan</i> dengan indeks yang sama (perhatikan bahwa antonim seringkali dapat dipertukarkan ketika melihat kata-kata di sekitarnya: misalnya, kata "baik" dan "buruk" sering disebutkan dalam konteks yang sama).  Ukuran jendela yang lebih besar (15-50 atau bahkan lebih) menghasilkan lampiran <i>terkait</i> dengan indeks yang serupa.  Dalam praktiknya, Anda sering harus memberikan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">anotasi</a> untuk persamaan semantik yang berguna dalam tugas Anda.  Di Gensim, ukuran jendela default adalah 5 (dua kata kiri dan kanan, di samping kata input itu sendiri). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4f4/b0a/45a/4f4b0a45a8552d6c19c7c9459302ac48.png"></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jumlah sampel negatif adalah faktor lain dalam proses pembelajaran. </font><font style="vertical-align: inherit;">Dokumen asli merekomendasikan 5-20. </font><font style="vertical-align: inherit;">Ia juga mengatakan bahwa 2-5 sampel tampaknya cukup ketika Anda memiliki dataset yang cukup besar. </font><font style="vertical-align: inherit;">Dalam Gensim, nilai default adalah 5 pola negatif.</font></font><br><br><h1>  Kesimpulan </h1><br><blockquote> <font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Jika perilakumu melebihi standarmu, maka kau adalah orang yang hidup, bukan automaton" - </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dewa-Kaisar Dune</font></font></i></font> </blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Saya harap Anda sekarang memahami embedding kata-kata dan esensi dari algoritma word2vec. </font><font style="vertical-align: inherit;">Saya juga berharap bahwa sekarang Anda akan lebih memahami artikel-artikel yang menyebutkan konsep "melewatkan-gram dengan pengambilan sampel negatif" (SGNS), seperti dalam sistem rekomendasi yang disebutkan di atas.</font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Referensi dan Bacaan Lebih Lanjut </font></font></h1><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">â€œRepresentasi kata dan frasa yang didistribusikan serta komposisinyaâ€</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> [pdf]</font></font></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Â«      Â»</a> [pdf] </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Â«   Â»</a> [pdf] </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Â«   Â»</a>      â€”    NLP. Word2vec    . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Â«      Â»</a> by <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> </a> â€”      . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> </a>        Word2vec.        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Â« word2vecÂ»</a> </li><li>   ?   : <ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> word2vec  Python</a>  Gensim </li><li>  <a href="">   C</a> ,    <a href="">       </a> </li></ul></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">    </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Â«  Â»</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> 2</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Â«Â»</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id446530/">https://habr.com/ru/post/id446530/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id446512/index.html">Pekerja Inti NET sebagai Layanan Windows</a></li>
<li><a href="../id446516/index.html">Visualisasi waktu kelahiran Roshan</a></li>
<li><a href="../id446518/index.html">Firewall aplikasi web</a></li>
<li><a href="../id446520/index.html">Bagaimana semuanya dimulai: kisah drone terbang</a></li>
<li><a href="../id446522/index.html">Swift 5.1 - apa yang baru?</a></li>
<li><a href="../id446532/index.html">Upwork memperkenalkan biaya untuk hak menulis kepada pelanggan potensial</a></li>
<li><a href="../id446534/index.html">Visual Studio 2019 Dirilis</a></li>
<li><a href="../id446536/index.html">Antrian dan JMeter: Pertukaran dengan Penerbit dan Pelanggan</a></li>
<li><a href="../id446538/index.html">PhotoGuru beralih ke "sisi gelap" dan "lebih bijaksana"</a></li>
<li><a href="../id446544/index.html">Microsoft memperluas Program Azure IP Advantage dengan manfaat IP baru untuk inovator dan startup Azure IoT</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>