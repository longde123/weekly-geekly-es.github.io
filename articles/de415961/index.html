<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üöª üêÜ üëÉüèΩ Russischer verteilter Speicher. Wie es funktioniert üç± üë©‚Äçüë©‚Äçüëß ‚õ∑Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In diesem Fr√ºhjahr hat das Reydiks-Team die erste Version der Software f√ºr die Erstellung verteilter Blockspeichersysteme vorbereitet und ver√∂ffentlic...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Russischer verteilter Speicher. Wie es funktioniert</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/raidix/blog/415961/"><img src="https://habrastorage.org/webt/wn/-j/zy/wn-jzyaxkasgaeb_mcf8xmjhhri.jpeg"><br><br>  In diesem Fr√ºhjahr hat das Reydiks-Team die erste Version der Software f√ºr die Erstellung verteilter Blockspeichersysteme vorbereitet und ver√∂ffentlicht, die auf Elbrus-4.4-Serverplattformen auf Basis von Elbrus-4C-Mikroprozessoren ausgef√ºhrt werden. <br><br>  Die N√ºtzlichkeit einer solchen Symbiose ist mit blo√üem Auge sichtbar - die Montage von Speichersystemen auf der Basis von inl√§ndischem Eisen und dem inl√§ndischen Betriebssystem wird zu einem attraktiven Produkt des heimischen Marktes, insbesondere f√ºr Kunden mit Schwerpunkt auf Importsubstitution. <br><a name="habracut"></a><br>  Das Potenzial des entwickelten Betriebssystems ist jedoch nicht auf russische Serverplattformen beschr√§nkt.  Derzeit wird die Kompatibilit√§t mit Standard-x86-64-Servern getestet, die auf dem Markt weit verbreitet sind.  Dar√ºber hinaus ist das Produkt mit der gew√ºnschten Funktionalit√§t "fertig", was seine Implementierung au√üerhalb des russischen Marktes erm√∂glicht. <br><br>  Im Folgenden wird eine kleine Diskussion √ºber die Anordnung der Softwarel√∂sung (RAIDIX RAIN) vorgestellt, mit der lokale Servermedien zu einem einzigen fehlertoleranten Speichercluster mit zentraler Verwaltung sowie horizontalen und vertikalen Skalierungsfunktionen kombiniert werden k√∂nnen. <br><br><h2>  Verteilte Speicherfunktionen </h2><br>  Herk√∂mmliche Speichersysteme, die in Form eines einzelnen Hardware- und Softwarekomplexes hergestellt werden, haben ein h√§ufiges Problem im Zusammenhang mit der Skalierung: Die Systemleistung beruht auf Controllern, ihre Anzahl ist begrenzt, die Kapazit√§tserweiterung durch Hinzuf√ºgen von Erweiterungsregalen mit Netzbetreibern erh√∂ht die Produktivit√§t nicht. <br><br>  Mit diesem Ansatz wird die Gesamtleistung des Speichersystems sinken, da mit zunehmender Kapazit√§t die vorherige Anzahl von Controllern mehr Zugriffsvorg√§nge auf das erh√∂hte Datenvolumen verarbeiten muss. <br><br>  RAIDIX RAIN unterst√ºtzt die horizontale Blockskalierung. Im Gegensatz zu herk√∂mmlichen L√∂sungen f√ºhrt das Erh√∂hen der Knoten (Serverbl√∂cke) des Systems zu einer linearen Erh√∂hung nicht nur der Kapazit√§t, sondern auch der Systemleistung.  Dies ist m√∂glich, da jeder RAIDIX RAIN-Knoten nicht nur Medien, sondern auch Rechenressourcen f√ºr E / A und Datenverarbeitung enth√§lt. <br><br><h2>  Anwendungsszenarien </h2><br>  RAIDIX RAIN umfasst die Implementierung aller Hauptanwendungsszenarien f√ºr verteilten Blockspeicher: Cloud-Speicherinfrastruktur, hoch ausgelastete Datenbanken und Big Data-Analysespeicher.  RAIDIX RAIN kann auch mit herk√∂mmlichen Speichersystemen mit ausreichend hohem Datenvolumen und entsprechenden finanziellen M√∂glichkeiten des Kunden konkurrieren. <br><br><h3>  √ñffentliche und private Clouds </h3><br>  Die L√∂sung bietet die flexible Skalierbarkeit, die f√ºr die Bereitstellung einer Cloud-Infrastruktur erforderlich ist: Leistung, Durchsatz und Speicherkapazit√§t erh√∂hen sich mit jedem dem System hinzugef√ºgten Knoten. <br><br><h3>  Datenbanken </h3><br>  Der RAIDIX RAIN-Cluster in einer All-Flash-Konfiguration ist eine effiziente L√∂sung f√ºr die Wartung hoch geladener Datenbanken.  Die L√∂sung wird eine kosteng√ºnstige Alternative zu Oracle Exadata-Produkten f√ºr Oracle RAC sein. <br><br><h3>  Big Data Analytics </h3><br>  Zusammen mit zus√§tzlicher Software ist es m√∂glich, eine L√∂sung zur Durchf√ºhrung von Big-Data-Analysen zu verwenden.  RAIDIX RAIN bietet im Vergleich zu einem HDFS-Cluster eine deutlich h√∂here Leistung und Wartungsfreundlichkeit. <br><br><h2>  L√∂sungsarchitektur </h2><br>  RAIDIX RAIN unterst√ºtzt zwei Bereitstellungsoptionen: dediziert (extern oder konvergent) und hyperkonvergiert (HCI, hyperkonvergierte Infrastruktur). <br><br><h3>  Spezielle Bereitstellungsoption </h3><br>  In der ausgew√§hlten Version ist der RAIDIX RAIN-Cluster ein klassischer Softwarespeicher.  Die L√∂sung wird auf der erforderlichen Anzahl dedizierter Serverknoten bereitgestellt (mindestens 3, die Anzahl ist von oben praktisch unbegrenzt), deren Ressourcen vollst√§ndig f√ºr Speicheraufgaben verwendet werden. <br><img src="https://habrastorage.org/webt/jr/zz/wd/jrzzwd0nqs1ykrpbv5nousfxfh0.png"><br>  <i><font color="#99999">Abb.</font></i>  <i><font color="#99999">1. Spezielle Bereitstellungsoption</font></i> <br><br>  Die RAIDIX RAIN-Software wird direkt auf Bare Metal installiert.  Anwendungen, Dienste und Computerressourcen, die RAIN zum Speichern von Informationen verwenden, werden auf externen Hosts gehostet und √ºber ein Speichernetzwerk (klassische Rechenzentrumsarchitektur) mit diesem verbunden. <br><br><h3>  Hyperkonvergierte Bereitstellungsoption </h3><br>  Die hyperkonvergente Option umfasst die gemeinsame Platzierung von Rechenleistung (Hypervisor und Produktions-VMs) und Speicherressourcen (Softwarespeicher) des Rechenzentrums auf einem Knotensatz. Dies gilt haupts√§chlich f√ºr virtuelle Infrastrukturen.  Bei diesem Ansatz wird die RAIN-Software auf jedem Host (Knoten) der Infrastruktur (HCI) in Form einer virtuellen Maschine installiert. <br><img src="https://habrastorage.org/webt/rc/6g/s4/rc6gs4fegn69jy4fpfy7idz45nw.png"><br>  <i><font color="#99999">Abb.</font></i>  <i><font color="#99999">2. Hyperkonvergierte Bereitstellungsoption</font></i> <br><br>  Die Interaktion der RAIN-Clusterknoten untereinander und mit den Endbenutzern von Speicherressourcen (Server, Anwendungen) erfolgt √ºber die Protokolle iSCSI (IP, IPoIB), iSER (RoCE, RDMA) oder NVMeOF. <br><br>  Die hyperkonvergente Bereitstellungsoption bietet die folgenden Vorteile: <br><br><ul><li>  Konsolidierung von Computer- und Speicherressourcen (keine Notwendigkeit, einen dedizierten externen Speicher zu implementieren und zu warten). </li><li>  Gemeinsame horizontale Blockskalierung von Computer- und Speicherressourcen. </li><li>  Einfache Implementierung und Wartung. </li><li>  Zentrales Management. </li><li>  Sparen Sie Rack-Kapazit√§t und Stromverbrauch. </li></ul><br>  In Bezug auf die verwendeten Medien unterst√ºtzt RAIDIX RAIN drei Konfigurationen: <br><br><ul><li>  All-Flash - Clusterknoten werden nur mit Flash-Medien (NVMe, SSD) geliefert. </li><li>  HDD - Clusterknoten werden nur mit HDD-Tr√§gern geliefert. </li><li>  Hybrid - zwei unabh√§ngige Speicherebenen auf Festplatte und SSD. </li></ul><br><br><h2>  Produktive Ausfallsicherheit </h2><br>  <b>Der Kernwert von RAIDIX RAIN</b> ist das optimale Gleichgewicht zwischen Leistung, Fehlertoleranz und effizienter Nutzung der Speicherkapazit√§t. <br><br>  Als Teil der Client-IT-Infrastruktur ist RAIDIX RAIN auch insofern attraktiv, als wir am Ausgang einen ‚Äûehrlichen‚Äú Blockzugriff haben, der die L√∂sung von den meisten Marktanaloga unterscheidet. <br><br>  Derzeit weisen die meisten Konkurrenzprodukte nur bei Verwendung der Spiegelung eine hohe Leistung auf.  Gleichzeitig wird die n√ºtzliche Speicherkapazit√§t um das Zweifache oder mehr reduziert: Einzelne Datenreplikation (Spiegelung) - 50% Redundanz, doppelte Datenreplikation (doppelte Spiegelung) - 66,6% Redundanz. <br><br>  Die Verwendung von Speicheroptimierungstechnologien wie EC (Erasure Coding - Noiseless Coding), Deduplizierung und Komprimierung in verteilten Speichersystemen f√ºhrt zu einer erheblichen Verschlechterung der Speicherleistung, was f√ºr verz√∂gerungsempfindliche Anwendungen nicht akzeptabel ist. <br><br>  In der Praxis sind solche L√∂sungen daher normalerweise gezwungen, ohne den Einsatz dieser Technologien zu arbeiten oder sie nur f√ºr ‚Äûkalte‚Äú Daten einzuschlie√üen. <br><br><h3>  Failover-Anforderungen </h3><br>  Urspr√ºnglich wurde RAIDIX RAIN mit einer Reihe klarer Anfangsanforderungen f√ºr die Ausfallsicherheit und Verf√ºgbarkeit des Systems entwickelt: <br><br><ul><li>  Der Cluster muss einen Ausfall von mindestens zwei Knoten √ºberleben, wobei die Anzahl der Knoten streng gr√∂√üer als 4 ist. F√ºr drei und vier ist ein Ausfall eines Knotens garantiert. </li><li>  Ein Knoten muss einen Ausfall von mindestens zwei Festplatten in jedem Knoten √ºberleben, wenn sich mindestens 5 Festplatten in einem Knoten befinden. </li><li>  Der Redundanzgrad von Laufwerken in einem typischen Cluster (von 16 Knoten) sollte 30% nicht √ºberschreiten </li><li>  Die Datenverf√ºgbarkeit muss mindestens 99,999% betragen </li></ul><br>  Dies hat die bestehende Produktarchitektur stark beeinflusst. <br><br><h3>  L√∂schcodierungsfunktionen im verteilten Speicher </h3><br>  Der prim√§re RAIDIX RAIN-Fehlertoleranzansatz ist die Verwendung einzigartiger Erasure Coding-Technologien.  EC-Unternehmen, die f√ºr ihr Flaggschiffprodukt bekannt sind, werden auch im verteilten Speicher eingesetzt, was eine Leistung erm√∂glicht, die mit gespiegelten Konfigurationen vergleichbar ist.  Dies gilt sowohl f√ºr zuf√§llige als auch f√ºr sequentielle Lasten.  Gleichzeitig wird ein vorbestimmtes Ma√ü an Fehlertoleranz sichergestellt und die Nutzkapazit√§t erheblich erh√∂ht, und die Gemeinkosten machen nicht mehr als 30% der Rohspeicherkapazit√§t aus. <br><br>  Hochleistungs-EC-RAIDIX ist bei sequentiellen Vorg√§ngen, insbesondere bei Verwendung von SATA-Festplatten mit gro√üer Kapazit√§t, gesondert zu erw√§hnen. <br><br>  Im Allgemeinen bietet RAIDIX RAIN drei fehlerkorrigierende Codierungsoptionen: <br><br><ul><li>  F√ºr 3 Knoten ist die Verwendung von RAID 1 optimal. </li><li>  f√ºr 4 Knoten optimale Nutzung von RAID 5; </li><li>  F√ºr einen Speichersubcluster von 5 bis 20 Knoten besteht der optimale Ansatz darin, Netzwerk-RAID 6 zu verwenden. </li></ul><br><img src="https://habrastorage.org/webt/ci/55/pd/ci55pdydidxbcjdkpzbhn5bwmzs.png"><br>  <i><font color="#99999">Abb.</font></i>  <i><font color="#99999">3. Optionen zur fehlerkorrigierenden Codierung</font></i> <br><br>  Alle Optionen setzen eine gleichm√§√üige Verteilung der Daten auf alle Knoten des Clusters voraus, wobei Redundanz in Form von Pr√ºfsummen (oder Korrekturcodes) hinzugef√ºgt wird.  Dies erm√∂glicht es uns, Parallelen zu den Reed-Solomon-Codes zu ziehen, die in Standard-RAID-Arrays (RAID-6) verwendet werden, und ein Failover von bis zu 2 Tr√§gern zu erm√∂glichen.  Netzwerk-RAID-6 funktioniert √§hnlich wie ein festplattenbasiertes RAID-6, verteilt jedoch Daten auf die Knoten des Clusters und erm√∂glicht ein Failover von 2 Knoten. <br><br>  Wenn in RAID 6 1-2 Netzbetreiber innerhalb eines Knotens ausfallen, werden sie lokal ohne Verwendung verteilter Pr√ºfsummen wiederhergestellt, wodurch die wiederhergestellte Datenmenge, die Netzwerklast und die allgemeine Systemverschlechterung minimiert werden. <br><br><h3>  Fehlerdom√§nen </h3><br>  RAIN unterst√ºtzt das Konzept von Fehlerdom√§nen oder Verf√ºgbarkeitsdom√§nen.  Auf diese Weise k√∂nnen Sie den Ausfall nicht nur einzelner Knoten, sondern auch ganzer Server-Racks oder -K√∂rbe ermitteln, deren Knoten logisch in Fehlerdom√§nen gruppiert sind.  Diese M√∂glichkeit wird durch die Verteilung von Daten erreicht, um deren Fehlertoleranz nicht auf der Ebene einzelner Knoten, sondern auf Dom√§nenebene sicherzustellen, wodurch der Ausfall aller darin gruppierten Knoten (z. B. eines gesamten Server-Racks) √ºberlebt werden kann.  Bei diesem Ansatz wird der Cluster in unabh√§ngige Untergruppen (Subcluster) unterteilt.  Die Anzahl der Knoten in einer Untergruppe betr√§gt nicht mehr als 20, was die Voraussetzung f√ºr Fehlertoleranz und Verf√ºgbarkeit darstellt.  Dar√ºber hinaus ist die Anzahl der Untergruppen nicht begrenzt. <br><img src="https://habrastorage.org/webt/mb/y6/u2/mby6u20ud-fmtvc2jiaghsjmr8u.png"><br>  <i><font color="#99999">Abb.</font></i>  <i><font color="#99999">4. Fehlerdom√§nen</font></i> <br><br>  Der Ausfall von Fehlern (Festplatten, Knoten oder Netzwerk) wird automatisch ausgef√ºhrt, ohne das System anzuhalten. <br><br>  Dar√ºber hinaus sind alle RAIDIX RAIN-Clusterger√§te durch Anschlie√üen an unterbrechungsfreie Stromversorgungen (USV) vor Stromausfall gesch√ºtzt.  Ger√§te, die an dieselbe USV angeschlossen sind, werden als Stromausfallgruppe bezeichnet. <br><br><h2>  Merkmale und Funktionalit√§t </h2><br>  Ber√ºcksichtigen Sie die wichtigsten Funktionsmerkmale von RAIDIX RAIN. <br>  <i><font color="#99999">Tabelle 1. Grundlegende RAIDIX RAIN-Funktionen</font></i> <br><table><tbody><tr><th>  Betriebsmerkmale </th><th>  Wert </th></tr><tr><td>  Unterst√ºtzte Knotentypen </td><td>  Inl√§ndische Serverplattformen basierend auf Elbrus-4C-Prozessoren <br>  Standard x86-64 Server (Perspektive) </td></tr><tr><td>  Unterst√ºtzte Medientypen </td><td>  SATA- und SAS-Festplatte, SATA- und SAS-SSD, NVMe </td></tr><tr><td>  Maximale Speicherkapazit√§t </td><td>  16 EB </td></tr><tr><td>  Maximale Clustergr√∂√üe </td><td>  1.024 Knoten </td></tr><tr><td>  Grundfunktionalit√§t </td><td>  Hot Volume Expansion <br>  Hot-Hinzuf√ºgen von Knoten zum Cluster <br>  Cluster-Neuausrichtung <br>  Failover ohne Ausfallzeit </td></tr><tr><td>  Resiliency Technologies </td><td>  Ausfall von Knoten, Medien, Netzwerk. <br>  L√∂schcodierung, verteilt auf die Clusterknoten: Netzwerk-RAID 0/1/5/6. <br>  Korrekturcodes auf der Ebene lokaler Host-Carrier (lokales RAID 6) <br>  Fehlerdom√§nen </td></tr></tbody></table><br>  Als wichtiges Funktionsmerkmal von RAIDIX RAIN ist zu beachten, dass Dienste wie <b>Initialisierung, Rekonstruktion und</b> Neuverteilung <b>(Skalierung) im Hintergrund stehen und auf einen Priorit√§tsparameter gesetzt werden k√∂nnen</b> . <br><br>  Mit der Priorit√§tseinstellung kann der Benutzer die Last im System unabh√§ngig anpassen und so die Arbeit dieser Dienste beschleunigen oder verlangsamen.  Zum Beispiel bedeutet Priorit√§t 0, dass Dienste nur funktionieren, wenn keine Last von Clientanwendungen geladen wird. <br><br><h3>  Skalierungsoptionen </h3><br>  Das Erweitern eines RAIDIX RAIN-Clusters ist so einfach und automatisiert wie m√∂glich. Das System verteilt Daten im Hintergrund unabh√§ngig unter Ber√ºcksichtigung der Kapazit√§t neuer Knoten neu, die Last wird ausgeglichen und gleichm√§√üig, die Gesamtleistung und die Speicherkapazit√§t werden proportional erh√∂ht.  Der Prozess der horizontalen Skalierung verl√§uft ohne Ausfallzeiten ‚Äûhei√ü‚Äú und erfordert kein Stoppen von Anwendungen und Diensten. <br><img src="https://habrastorage.org/webt/jp/93/5s/jp935sen4sbae6usigcpgmytu2e.png"><br>  <i><font color="#99999">Abb.</font></i>  <i><font color="#99999">5. Schema des Skalierungsprozesses</font></i> <br><br><h3>  Flexibilit√§t der Architektur </h3><br>  RAIDIX RAIN ist ein Softwareprodukt und nicht auf eine bestimmte Hardwareplattform beschr√§nkt. Das Konzept legt nahe, dass die Installation auf jeder kompatiblen Serverhardware m√∂glich ist. <br><br>  Basierend auf den Besonderheiten seiner Infrastruktur und Anwendungen w√§hlt jeder Kunde die beste Bereitstellungsoption: dediziert oder hyperkonvergiert. <br><br>  Durch die Unterst√ºtzung verschiedener Medientypen k√∂nnen Sie basierend auf RAIDIX RAIN basierend auf dem Budget und den zu l√∂senden Aufgaben erstellen: <br>  1. verteilter All-Flash-Speicher mit beispielloser hoher Leistung und garantierter geringer Latenz; <br>  2. wirtschaftliche Hybridsysteme, die die meisten Grundlasten erf√ºllen. <br><br><h2>  Leistungsindikatoren </h2><br>  Abschlie√üend zeigen wir einige Zahlen, die beim Testen von RAIDIX RAIN zur Konfiguration eines NVMe-Clusters mit 6 Knoten erhalten wurden.  Wir stellen erneut fest, dass bei einer solchen Assembly (mit x86-64-Servern) das Produkt noch finalisiert wird und diese Zahlen nicht endg√ºltig sind. <br><br><h3>  Testumgebung </h3><br><ul><li>  6 Knoten auf 2 Festplatten NVMe HGST SN100 </li><li>  IB-Karte Mellanox MT27700-Familie [ConnectX-4] </li><li>  Linux Kernel 4.11.6-1.el7.elrepo.x86_64 </li><li>  MLNX_OFED_LINUX-4.3-1.0.1.0-rhel7.4-x86_64 </li><li>  Lokaler √úberfall - √úberfall 0 </li><li>  Externer √úberfall - √úberfall 6 </li><li>  Benchmark zum Testen von FIO 3.1 </li></ul><br><br>  <b>UPD: Das</b> Laden wurde in 4K-Bl√∂cken durchgef√ºhrt, sequentiell - 1 MB, Warteschlangentiefe 32. Das Laden wurde gleichzeitig auf allen Knoten des Clusters gestartet und die Tabelle zeigt das Gesamtergebnis.  Verz√∂gerungen √ºberschreiten 1 ms (99,9 Perzentil) nicht. <br><br>  <i><font color="#99999">Tabelle 2. Testergebnisse</font></i> <br><table><tbody><tr><th>  Lasttyp </th><th>  Wert </th></tr><tr><td>  Zuf√§lliges Lesen 100% </td><td>  4.098.000 IOps </td></tr><tr><td>  Zuf√§lliges Schreiben 100% </td><td>  517.000 IOps </td></tr><tr><td>  Sequentielles Lesen 100% </td><td>  33,8 GB / s </td></tr><tr><td>  Sequentielles Schreiben 100% </td><td>  12 GB / s </td></tr><tr><td>  Zuf√§lliges Lesen 70% / zuf√§lliges Schreiben 30% </td><td>  1.000.000 IOps / 530.000 IOps </td></tr><tr><td>  Zuf√§lliges Lesen 50% / zuf√§lliges Schreiben 50% </td><td>  530.000 IOps / 530.000 IOps </td></tr><tr><td>  Zuf√§lliges Lesen 30% / zuf√§lliges Schreiben 70% </td><td>  187.000 IOps / 438.000 IOps </td></tr></tbody></table></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de415961/">https://habr.com/ru/post/de415961/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de415949/index.html">High Mining: Die neueste Option zum Schutz der PoW-Blockchain vor einem ‚Äû51% -Angriff‚Äú.</a></li>
<li><a href="../de415951/index.html">Mitap Sberbank und IBM auf HyperLedger Fabric</a></li>
<li><a href="../de415953/index.html">Wie blutiges Unternehmen Open Source gewinnt: der Kampf um BPMS</a></li>
<li><a href="../de415957/index.html">Wir brauchen mehr Rucks√§cke: Bobby XL von XD Design</a></li>
<li><a href="../de415959/index.html">Wie wir den Netzwerkcode des mobilen PvP-Shooters geschrieben haben: Player-Synchronisation auf dem Client</a></li>
<li><a href="../de415963/index.html">Naive Bayes oder wie Sie mit Mathematik Spam filtern k√∂nnen</a></li>
<li><a href="../de415965/index.html">Was im Juli zu lesen ist: 19 neue B√ºcher f√ºr Digitalprofis</a></li>
<li><a href="../de415967/index.html">SolidFire - Speicher f√ºr diejenigen, die Hassspeicher haben</a></li>
<li><a href="../de415969/index.html">HyperX Pulsefire Surge RGB - ein nat√ºrlich geborener Killer</a></li>
<li><a href="../de415973/index.html">So brechen Sie den Apache Ignite-Cluster nicht von Anfang an</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>