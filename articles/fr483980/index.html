<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>📽️ 👊🏿 ⛸️ Création d'une infrastructure informatique tolérante aux pannes. Partie 1 - Préparation du déploiement du cluster oVirt 4.3 🔡 🍏 👨🏿‍🤝‍👨🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Les lecteurs sont invités à se familiariser avec les principes de construction d'une infrastructure tolérante aux pannes d'une petite entreprise au se...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Création d'une infrastructure informatique tolérante aux pannes. Partie 1 - Préparation du déploiement du cluster oVirt 4.3</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/lenvendo/blog/483980/"><p>  Les lecteurs sont invités à se familiariser avec les principes de construction d'une infrastructure tolérante aux pannes d'une petite entreprise au sein d'un centre de données, qui seront examinés en détail dans une courte série d'articles. </p><a name="habracut"></a><br><h2 id="vvodnaya-chast">  Présentation </h2><br><p>  Sous le <strong>DPC</strong> (Data Processing Center) on peut comprendre: </p><br><ul><li>  propre rack dans sa «salle de serveurs» sur le territoire de l'entreprise qui répond aux exigences minimales pour fournir des équipements d'alimentation et de refroidissement, ainsi que d'avoir accès à Internet via deux fournisseurs indépendants; </li><li>  rack loué avec son propre équipement situé dans ce centre de données - le soi-disant  la colocalisation, conforme à la norme Tier III ou IV, qui garantit une alimentation électrique, un refroidissement et un accès Internet à tolérance de pannes fiables; </li><li>  équipement entièrement loué dans un centre de données de niveau III ou IV. </li></ul><br><p>  Quelle option d'hébergement choisir - dans chaque cas, tout est individuel et dépend généralement de plusieurs facteurs principaux: </p><br><ul><li>  pourquoi l'entreprise a-t-elle sa propre infrastructure informatique; </li><li>  ce que l'entreprise attend exactement de l'infrastructure informatique (fiabilité, évolutivité, facilité de gestion, etc.); </li><li>  le montant de l'investissement initial dans l'infrastructure informatique, ainsi que le type de coûts qu'il s'agit - en capital (ce qui signifie que vous achetez votre équipement) ou en fonctionnement (l'équipement est généralement loué); </li><li>  Horizon de planification de l'entreprise elle-même. </li></ul><br><p>  Beaucoup de choses peuvent être écrites sur les facteurs qui influencent la décision de l'entreprise de créer et d'utiliser son infrastructure informatique, mais notre objectif est de montrer dans la pratique comment créer cette infrastructure afin qu'elle soit à la fois tolérante aux pannes et qu'il soit possible d'économiser de l'argent - réduire le coût d'acquisition de logiciels commerciaux, voire les éviter. </p><br><p>  Comme le montre la longue pratique, cela ne vaut pas la peine d'économiser sur le matériel, car l'avare paie deux fois, et bien plus encore.  Mais là encore - bon matériel, ce n'est qu'une recommandation, et finalement quoi acheter exactement et pour combien dépend des capacités de l'entreprise et de la «cupidité» de sa gestion.  En outre, le mot «cupidité» doit être compris dans le bon sens du terme, car il vaut mieux investir dans le fer au stade initial, de sorte que plus tard, il n'y a pas de problèmes sérieux dans son soutien et sa mise à l'échelle, car une planification initialement incorrecte et des économies excessives peuvent conduire à l'avenir. plus cher que lors du démarrage d'un projet. </p><br><p>  Ainsi, les données initiales du projet: </p><br><ul><li>  il y a une entreprise qui a décidé de créer son propre portail Web et de mettre ses activités sur Internet; </li><li>  la société a décidé de louer un rack pour placer son équipement dans un bon centre de données certifié selon la norme Tier III; </li><li>  l'entreprise a décidé de ne pas économiser beaucoup sur le matériel et a donc acheté l'équipement suivant avec des garanties et un support étendus: </li></ul><br><div class="spoiler">  <b class="spoiler_title">Liste des équipements</b> <div class="spoiler_text"><blockquote><ul><li>  deux serveurs physiques Dell PowerEdge R640 comme suit: </li><li>  <em>deux processeurs Intel Xeon Gold 5120</em> </li><li>  <em>512 Go de RAM</em> </li><li>  <em>deux disques SAS en RAID1, pour l'installation du système d'exploitation</em> </li><li>  <em>carte réseau 1G 4 ports intégrée</em> </li><li>  <em>deux cartes réseau 10G à 2 ports</em> </li><li>  <em>un FC HBA 16G à 2 ports.</em> </li><li>  Système de stockage à 2 contrôleurs Dell MD3820f, connecté via FC 16G directement aux hôtes Dell; </li><li>  deux commutateurs du deuxième niveau - Cisco WS-C2960RX-48FPS-L empilés; </li><li> deux commutateurs de couche 3 - Cisco WS-C3850-24T-E, empilés; </li><li>  Rack, UPS, PDU, serveurs de console - fournis par le centre de données. </li></ul><br></blockquote></div></div><br><p>  Comme nous pouvons le voir, l'équipement existant a de bonnes perspectives de mise à l'échelle horizontale et verticale, si l'entreprise peut concurrencer d'autres entreprises de profil similaire sur Internet et commence à réaliser un profit qui peut être investi dans l'expansion des ressources pour une concurrence accrue et une croissance des bénéfices. </p><br><p>  Quel équipement pouvons-nous ajouter si l'entreprise décide d'augmenter les performances de notre cluster informatique: </p><br><ul><li>  nous avons une grande réserve pour le nombre de ports sur les commutateurs 2960X, ce qui signifie que vous pouvez ajouter plus de serveurs matériels; </li><li>  acheter deux commutateurs FC pour y connecter des systèmes de stockage et des serveurs supplémentaires; </li><li>  les serveurs déjà existants peuvent être mis à niveau - ajoutez de la mémoire, remplacez les processeurs par des plus efficaces, connectez les adaptateurs réseau existants à un réseau 10G; </li><li>  Vous pouvez ajouter des étagères de disques supplémentaires au stockage avec le type de disques nécessaire - SAS, SATA ou SSD, en fonction de la charge prévue; </li><li>  après avoir ajouté des commutateurs FC, vous pouvez acheter un autre système de stockage pour ajouter encore plus de capacité de disque, et si vous lui achetez l'option spéciale de réplication à distance, vous pouvez configurer la réplication des données entre les systèmes de stockage à la fois dans le même centre de données et entre les centres de données (mais cela dépasse déjà la portée de l'article); </li><li>  il existe également des commutateurs de troisième niveau - le Cisco 3850, qui peut être utilisé comme cœur tolérant aux pannes du réseau pour le routage à grande vitesse entre les réseaux internes.  Cela contribuera grandement à l'avenir, à mesure que l'infrastructure interne se développera.  Le 3850 possède également des ports 10G qui peuvent être activés ultérieurement lors de la mise à niveau de l'équipement réseau vers 10G. </li></ul><br><p>  Puisqu'il n'y a plus nulle part sans virtualisation, alors bien sûr nous serons à la mode, d'autant plus que c'est un excellent moyen de réduire le coût d'achat de serveurs coûteux pour certains éléments d'infrastructure (serveurs web, bases de données, etc.), qui ne sont pas toujours optimaux utilisé en cas de faible charge, et c'est exactement ce qui sera au début du lancement du projet. </p><br><p>  De plus, la virtualisation présente de nombreux autres avantages qui peuvent nous être très utiles: tolérance aux pannes de VM suite à une défaillance du serveur matériel, migration en direct entre les nœuds matériels du cluster pour leur maintenance, équilibrage de charge manuel ou automatique entre les nœuds du cluster, etc. </p><br><p>  Pour le matériel acquis par l'entreprise, le déploiement du cluster VMware vSphere hautement accessible se suggère, mais comme tout logiciel de VMware est connu pour ses prix à cheval, nous utiliserons un logiciel de gestion de la virtualisation absolument gratuit - <a href="https://ru.wikipedia.org/wiki/OVirt"><strong>oVirt</strong></a> , sur la base duquel le célèbre mais produit déjà commercial - <a href="https://ru.bmstu.wiki/RHEV_(Red_Hat_Enterprise_Virtualization)"><strong>RHEV</strong></a> . </p><br><p>  Le logiciel <strong>oVirt</strong> est nécessaire pour combiner tous les éléments de l'infrastructure afin de pouvoir travailler facilement avec des machines virtuelles hautement accessibles - ce sont des bases de données, des applications Web, des proxys, des équilibreurs, des serveurs pour collecter des journaux et des analyses, etc. n., c'est-à-dire en quoi consiste le portail Web de notre entreprise. </p><br><p>  Pour résumer cette introduction, les articles suivants nous attendent, qui en pratique montreront comment déployer l'intégralité de l'infrastructure matérielle et logicielle de l'entreprise: </p><br><div class="spoiler">  <b class="spoiler_title">Liste d'articles</b> <div class="spoiler_text"><ul><li>  <strong>Partie 1.</strong> Préparation du déploiement du cluster oVirt 4.3. </li><li>  <strong>Partie 2.</strong> Installation et configuration du cluster oVirt 4.3. </li><li>  <strong>Partie 3.</strong> Organisation du routage tolérant aux pannes sur les routeurs virtuels VyOS. </li><li>  <strong>Partie 4.</strong> Configuration de la pile Cisco 3850, organisation du routage intranet. </li></ul></div></div><br><h2 id="chast-1-podgotovka-k-razvyortyvaniyu-klastera-ovirt-43">  Partie 1. Préparation du déploiement du cluster oVirt 4.3 </h2><br><h3 id="bazovaya-nastroyka-hostov">  Configuration de base de l'hôte </h3><br><p>  L'installation et la configuration du système d'exploitation est l'étape la plus simple.  Il y a beaucoup d'articles sur la façon d'installer et de configurer correctement le système d'exploitation, donc cela n'a aucun sens d'essayer de donner quelque chose d'exclusif à ce sujet. </p><br><p>  Nous avons donc deux hôtes Dell PowerEdge R640, sur lesquels vous devez installer le système d'exploitation et effectuer des préconfigurations, pour les utiliser comme hyperviseurs pour exécuter des machines virtuelles dans le cluster oVirt 4.3. </p><br><p>  Comme nous prévoyons d'utiliser le logiciel gratuit non commercial oVirt, <strong>CentOS 7.7 a été</strong> choisi pour le déploiement des hôtes, bien que d'autres systèmes d'exploitation puissent également être installés sur les hôtes pour oVirt: </p><br><ul><li>  construction spéciale basée sur RHEL, la soi-disant  <a href="https://www.ovirt.org/documentation/admin-guide/chap-Hosts.html"><strong>oVirt Node</strong></a> ; </li><li>  OS Oracle Linux, à l'été 2019, le support pour le fonctionnement de oVirt sur celui-ci a <a href="https://blogs.oracle.com/virtualization/announcing-oracle-linux-virtualization-manager">été annoncé</a> . </li></ul><br><p>  Avant d'installer le système d'exploitation, il est recommandé: </p><br><ul><li>  Configurer l'interface réseau iDRAC sur les deux hôtes </li><li>  mettre à jour le firmware du BIOS et de l'iDRAC vers les dernières versions; </li><li>  il est souhaitable de configurer le serveur System Profile en mode Performance; </li><li>  configurer le RAID à partir de disques locaux (RAID1 recommandé) pour installer le système d'exploitation sur le serveur. </li></ul><br><p>  Ensuite, nous installons le système d'exploitation sur le disque créé précédemment via iDRAC - le processus d'installation est normal, il n'y a pas de moments spéciaux.  L'accès à la console du serveur pour commencer l'installation du système d'exploitation peut également être obtenu via iDRAC, bien que rien ne vous empêche de connecter le moniteur, le clavier et la souris directement au serveur et d'installer le système d'exploitation à partir d'un lecteur flash. </p><br><p>  Après avoir installé le système d'exploitation, effectuez ses réglages initiaux: </p><br><pre><code class="plaintext hljs">systemctl enable network.service systemctl start network.service systemctl status network.service</code> </pre> <br><pre> <code class="plaintext hljs">systemctl stop NetworkManager systemctl disable NetworkManager systemctl status NetworkManager</code> </pre> <br><pre> <code class="plaintext hljs">yum install -y ntp systemctl enable ntpd.service systemctl start ntpd.service</code> </pre> <br><pre> <code class="plaintext hljs">cat /etc/sysconfig/selinux SELINUX=disabled SELINUXTYPE=targeted</code> </pre> <br><pre> <code class="plaintext hljs">cat /etc/security/limits.conf * soft nofile 65536 * hard nofile 65536</code> </pre> <br><pre> <code class="plaintext hljs">cat /etc/sysctl.conf vm.max_map_count = 262144 vm.swappiness = 1</code> </pre> <br><p>  <u>Installer l'ensemble de logiciels de base</u> </p><br><p>  Pour la configuration initiale du système d'exploitation, vous devez configurer n'importe quelle interface réseau sur le serveur afin de pouvoir accéder à Internet, mettre à jour le système d'exploitation et installer les packages logiciels nécessaires.  Cela peut être fait à la fois pendant l'installation du système d'exploitation et après. </p><br><pre> <code class="plaintext hljs">yum -y install epel-release yum update yum -y install bind-utils yum-utils net-tools git htop iotop nmon pciutils sysfsutils sysstat mc nc rsync wget traceroute gzip unzip telnet</code> </pre> <br><p>  Tous les paramètres ci-dessus et un ensemble de logiciels sont une question de préférence personnelle, et cet ensemble n'est qu'une recommandation. </p><br><p>  Puisque notre hébergeur jouera le rôle d'un hyperviseur, nous activerons le profil de performance souhaité: </p><br><pre> <code class="plaintext hljs">systemctl enable tuned systemctl start tuned systemctl status tuned</code> </pre> <br><pre> <code class="plaintext hljs">tuned-adm profile tuned-adm profile virtual-host</code> </pre> <br><p>  Vous pouvez en savoir plus sur le profil de performance ici: " <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_tuning_and_optimization_guide/chap-virtualization_tuning_optimization_guide-tuned">Chapitre 4. tuned et tuned-adm</a> ". </p><br><p>  Après avoir installé le système d'exploitation, nous passons à la partie suivante - la configuration des interfaces réseau sur les hôtes et la pile de commutateurs Cisco 2960X. </p><br><h3 id="nastroyka-steka-kommutatorov-cisco-2960x">  Configuration de la pile de commutateurs Cisco 2960X </h3><br><p>  Notre projet utilisera les nombres suivants de VLAN - ou domaines de diffusion isolés les uns des autres, afin de séparer différents types de trafic: </p><br><p>  <strong>VLAN 10</strong> - Internet <br>  <strong>VLAN 17</strong> - Gestion (iDRAC, stockage, gestion des commutateurs) <br>  <strong>VLAN 32</strong> - Réseau de production de VM <br>  <strong>VLAN 33</strong> - réseau d'interconnexion (à des sous-traitants externes) <br>  <strong>VLAN 34</strong> - Réseau de test VM <br>  <strong>VLAN 35</strong> - Réseau de développeurs VM <br>  <strong>VLAN 40</strong> - Réseau de surveillance </p><br><p>  Avant de commencer les travaux, nous présentons un schéma au niveau L2, auquel nous devrions enfin arriver: </p><br><p><img src="https://habrastorage.org/webt/in/ak/8a/inak8aoyty8yujo2zagwdehdytg.jpeg"></p><br><p>  Pour l'interaction réseau entre les hôtes oVirt et les machines virtuelles entre eux, ainsi que pour gérer notre stockage, vous devez configurer la pile de commutateurs Cisco 2960X. </p><br><p>  Les hôtes Dell ont des cartes réseau à 4 ports intégrées, il est donc conseillé d'organiser leur connexion au Cisco 2960X à l'aide d'une connexion réseau à tolérance de pannes, en utilisant le regroupement des ports réseau physiques dans une interface logique et le protocole LACP (802.3ad): </p><br><ul><li>  les deux premiers ports de l'hôte sont configurés en mode de liaison et connectés au commutateur 2960X - sur cette interface logique, un <strong><em>pont</em></strong> avec une adresse pour gérer l'hôte, la surveillance, la communication avec les autres hôtes du cluster oVirt sera configuré, il sera également utilisé pour la migration en direct des machines virtuelles; </li><li>  les deux seconds ports de l'hôte sont également configurés en mode liaison et connectés au 2960X - sur cette interface logique utilisant oVirt, des ponts ultérieurs seront créés (dans les VLAN correspondants) auxquels les machines virtuelles seront connectées. </li><li>  les deux ports réseau, au sein de la même interface logique, seront actifs, c'est-à-dire  Le trafic sur eux peut être transmis simultanément, en mode d'équilibrage. </li><li>  les paramètres réseau sur les nœuds du cluster doivent être absolument identiques, à l'exception des adresses IP. </li></ul><br><p>  <u>Configuration de base de la <strong>pile de</strong> commutateurs <strong>2960X</strong> et de ses ports</u> </p><br><p>  Auparavant, nos commutateurs devaient être: </p><br><ul><li>  monté dans un rack; </li><li>  connectés par deux câbles spéciaux de la longueur souhaitée, par exemple CAB-STK-E-1M; </li><li>  connecté à l'alimentation; </li><li>  connecté au poste de travail de l'administrateur via le port console, pour leur configuration initiale. </li></ul><br><p>  Les conseils nécessaires à cet effet sont disponibles sur <a href="https://www.cisco.com/c/en/us/support/switches/catalyst-2960-x-series-switches/products-installation-guides-list.html">la page officielle</a> du fabricant. </p><br><p>  Après avoir effectué les étapes ci-dessus, configurez les commutateurs. <br>  Ce que signifie chaque équipe n'est pas censé être décrypté dans le cadre de cet article; si nécessaire, toutes les informations peuvent être trouvées indépendamment. <br>  Notre objectif est de configurer la pile de commutateurs le plus rapidement possible et d'y connecter les hôtes et les interfaces de gestion du stockage. </p><br><p>  1) Connectez-vous au commutateur principal, passez en mode privilégié, puis passez en mode de configuration et effectuez les réglages de base. </p><br><div class="spoiler">  <b class="spoiler_title">Configuration de commutation de base:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs"> enable configure terminal hostname 2960X no service pad service timestamps debug datetime msec service timestamps log datetime localtime show-timezone msec no service password-encryption service sequence-numbers switch 1 priority 15 switch 2 priority 14 stack-mac persistent timer 0 clock timezone MSK 3 vtp mode transparent ip subnet-zero vlan 17 name Management vlan 32 name PROD vlan 33 name Interconnect vlan 34 name Test vlan 35 name Dev vlan 40 name Monitoring spanning-tree mode rapid-pvst spanning-tree etherchannel guard misconfig spanning-tree portfast bpduguard default spanning-tree extend system-id spanning-tree vlan 1-40 root primary spanning-tree loopguard default vlan internal allocation policy ascending port-channel load-balance src-dst-ip errdisable recovery cause loopback errdisable recovery cause bpduguard errdisable recovery interval 60 line con 0 session-timeout 60 exec-timeout 60 0 logging synchronous line vty 5 15 session-timeout 60 exec-timeout 60 0 logging synchronous ip http server ip http secure-server no vstack interface Vlan1 no ip address shutdown exit</code> </pre></div></div><br><p>  Nous enregistrons la configuration avec la commande <strong>wr mem</strong> et rechargeons la pile de commutateurs avec la commande <strong>reload</strong> sur le commutateur principal 1. </p><br><p>  2) Configurez les ports réseau du commutateur en mode d'accès dans VLAN 17, pour connecter les interfaces de gestion des serveurs de stockage et iDRAC. </p><br><div class="spoiler">  <b class="spoiler_title">Paramètres du port de gestion:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">interface GigabitEthernet1/0/5 description iDRAC - host1 switchport access vlan 17 switchport mode access spanning-tree portfast edge interface GigabitEthernet1/0/6 description Storage1 - Cntr0/Eth0 switchport access vlan 17 switchport mode access spanning-tree portfast edge interface GigabitEthernet2/0/5 description iDRAC - host2 switchport access vlan 17 switchport mode access spanning-tree portfast edge interface GigabitEthernet2/0/6 description Storage1 – Cntr1/Eth0 switchport access vlan 17 switchport mode access spanning-tree portfast edge exit</code> </pre> </div></div><br><p>  3) Après avoir redémarré la pile, vérifiez qu'elle fonctionne correctement: </p><br><div class="spoiler">  <b class="spoiler_title">Vérification du fonctionnement de la pile:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">2960X#show switch stack-ring speed Stack Ring Speed : 20G Stack Ring Configuration: Full Stack Ring Protocol : FlexStack 2960X#show switch stack-ports Switch # Port 1 Port 2 -------- ------ ------ 1 Ok Ok 2 Ok Ok 2960X#show switch neighbors Switch # Port 1 Port 2 -------- ------ ------ 1 2 2 2 1 1 2960X#show switch detail Switch/Stack Mac Address : 0cd0.f8e4. Mac persistency wait time: Indefinite H/W Current Switch# Role Mac Address Priority Version State ---------------------------------------------------------- *1 Master 0cd0.f8e4. 15 4 Ready 2 Member 0029.c251. 14 4 Ready Stack Port Status Neighbors Switch# Port 1 Port 2 Port 1 Port 2 -------------------------------------------------------- 1 Ok Ok 2 2 2 Ok Ok 1 1</code> </pre> </div></div><br><p>  4) Configuration de l'accès SSH à la pile 2960X </p><br><p>  Pour la gestion à distance de la pile via SSH, nous utiliserons IP 172.20.1.10 configuré sur SVI (switch virtual interface) <strong>VLAN17</strong> . </p><br><p>  Bien qu'à des fins de gestion, il est conseillé d'utiliser un port dédié spécial sur le commutateur, mais c'est une question de préférence personnelle et d'opportunité. </p><br><div class="spoiler">  <b class="spoiler_title">Configuration de l'accès SSH à la pile de commutateurs:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">ip default-gateway 172.20.1.2 interface vlan 17 ip address 172.20.1.10 255.255.255.0 hostname 2960X ip domain-name hw.home-lab.ru no ip domain-lookup clock set 12:47:04 06 Dec 2019 crypto key generate rsa ip ssh version 2 ip ssh time-out 90 line vty 0 4 session-timeout 60 exec-timeout 60 0 privilege level 15 logging synchronous transport input ssh line vty 5 15 session-timeout 60 exec-timeout 60 0 privilege level 15 logging synchronous transport input ssh aaa new-model aaa authentication login default local username cisco privilege 15 secret my_ssh_password</code> </pre> </div></div><br><p>  Configurez un mot de passe pour entrer en mode privilégié: </p><br><pre> <code class="plaintext hljs">enable secret *myenablepassword* service password-encryption</code> </pre> <br><p>  Configurer NTP: </p><br><pre> <code class="plaintext hljs">ntp server 85.21.78.8 prefer ntp server 89.221.207.113 ntp server 185.22.60.71 ntp server 192.36.143.130 ntp server 185.209.85.222 show ntp status show ntp associations show clock detail</code> </pre> <br><p>  5) Configurez les interfaces logiques Etherchannel et les ports physiques connectés aux hôtes.  Pour faciliter la configuration, tous les VLAN disponibles seront autorisés sur toutes les interfaces logiques, mais il est généralement recommandé de configurer uniquement ce dont vous avez besoin: </p><br><div class="spoiler">  <b class="spoiler_title">Configurer les interfaces Etherchannel:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">interface Port-channel1 description EtherChannel with Host1-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk spanning-tree portfast edge trunk interface Port-channel2 description EtherChannel with Host2-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk spanning-tree portfast edge trunk interface Port-channel3 description EtherChannel with Host1-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk spanning-tree portfast edge trunk interface Port-channel4 description EtherChannel with Host2-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk spanning-tree portfast edge trunk interface GigabitEthernet1/0/1 description Host1-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 1 mode active interface GigabitEthernet1/0/2 description Host2-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 2 mode active interface GigabitEthernet1/0/3 description Host1-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 3 mode active interface GigabitEthernet1/0/4 description Host2-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 4 mode active interface GigabitEthernet2/0/1 description Host1-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 1 mode active interface GigabitEthernet2/0/2 description Host2-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 2 mode active interface GigabitEthernet2/0/3 description Host1-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 3 mode active interface GigabitEthernet2/0/4 description Host2-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 4 mode active</code> </pre> </div></div><br><p>  <u>Configuration initiale des interfaces réseau pour les machines virtuelles sur <strong>Host1</strong> et <strong>Host2</strong></u> </p><br><p>  Nous vérifions la disponibilité des modules nécessaires au collage dans le système, installons le module de gestion des ponts: </p><br><pre> <code class="plaintext hljs">modinfo bonding modinfo 8021q yum install bridge-utils</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Configuration de l'interface logique BOND1 pour les machines virtuelles sur les hôtes et ses interfaces physiques:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /etc/sysconfig/network-scripts/ifcfg-bond1 #DESCRIPTION - management DEVICE=bond1 NAME=bond1 TYPE=Bond IPV6INIT=no ONBOOT=yes USERCTL=no NM_CONTROLLED=no BOOTPROTO=none BONDING_OPTS='mode=4 lacp_rate=1 xmit_hash_policy=2' cat /etc/sysconfig/network-scripts/ifcfg-em2 #DESCRIPTION - management DEVICE=em2 TYPE=Ethernet BOOTPROTO=none ONBOOT=yes MASTER=bond1 SLAVE=yes USERCTL=no NM_CONTROLLED=no cat /etc/sysconfig/network-scripts/ifcfg-em3 #DESCRIPTION - management DEVICE=em3 TYPE=Ethernet BOOTPROTO=none ONBOOT=yes MASTER=bond1 SLAVE=yes USERCTL=no NM_CONTROLLED=no</code> </pre> </div></div><br><p>  Après avoir terminé les réglages sur la pile et les hôtes <strong>2960X</strong> , nous redémarrons le réseau sur les hôtes et vérifions l'interface logique. </p><br><ul><li>  sur l'hôte: </li></ul><br><pre> <code class="plaintext hljs">systemctl restart network cat /proc/net/bonding/bond1 Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011) Bonding Mode: IEEE 802.3ad Dynamic link aggregation Transmit Hash Policy: layer2+3 (2) MII Status: up MII Polling Interval (ms): 100 Up Delay (ms): 0 Down Delay (ms): 0 ... 802.3ad info LACP rate: fast Min links: 0 Aggregator selection policy (ad_select): stable System priority: 65535 ... Slave Interface: em2 MII Status: up Speed: 1000 Mbps Duplex: full ... Slave Interface: em3 MII Status: up Speed: 1000 Mbps Duplex: full</code> </pre> <br><ul><li>  sur la <strong>pile de</strong> commutateurs <strong>2960X</strong> : </li></ul><br><pre> <code class="plaintext hljs">2960X#show lacp internal Flags: S - Device is requesting Slow LACPDUs F - Device is requesting Fast LACPDUs A - Device is in Active mode P - Device is in Passive mode Channel group 1 LACP port Admin Oper Port Port Port Flags State Priority Key Key Number State Gi1/0/1 SA bndl 32768 0x1 0x1 0x102 0x3D Gi2/0/1 SA bndl 32768 0x1 0x1 0x202 0x3D 2960X#sh etherchannel summary Flags: D - down P - bundled in port-channel I - stand-alone s - suspended H - Hot-standby (LACP only) R - Layer3 S - Layer2 U - in use N - not in use, no aggregation f - failed to allocate aggregator M - not in use, minimum links not met m - not in use, port not aggregated due to minimum links not met u - unsuitable for bundling w - waiting to be aggregated d - default port A - formed by Auto LAG Number of channel-groups in use: 11 Number of aggregators: 11 Group Port-channel Protocol Ports ------+-------------+-----------+----------------------------------------------- 1 Po1(SU) LACP Gi1/0/1(P) Gi2/0/1(P)</code> </pre> <br><p>  Configuration initiale des interfaces réseau pour la gestion des ressources de cluster sur <strong>Host1</strong> et <strong>Host2</strong> </p><br><div class="spoiler">  <b class="spoiler_title">Configuration de l'interface logique BOND1 pour la gestion et ses interfaces physiques sur les hôtes:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /etc/sysconfig/network-scripts/ifcfg-bond0 #DESCRIPTION - management DEVICE=bond0 NAME=bond0 TYPE=Bond BONDING_MASTER=yes IPV6INIT=no ONBOOT=yes USERCTL=no NM_CONTROLLED=no BOOTPROTO=none BONDING_OPTS='mode=4 lacp_rate=1 xmit_hash_policy=2' cat /etc/sysconfig/network-scripts/ifcfg-em0 #DESCRIPTION - management DEVICE=em0 TYPE=Ethernet BOOTPROTO=none ONBOOT=yes MASTER=bond0 SLAVE=yes USERCTL=no NM_CONTROLLED=no cat /etc/sysconfig/network-scripts/ifcfg-em1 #DESCRIPTION - management DEVICE=em1 TYPE=Ethernet BOOTPROTO=none ONBOOT=yes MASTER=bond0 SLAVE=yes USERCTL=no NM_CONTROLLED=no</code> </pre> </div></div><br><p>  Après avoir terminé les réglages sur la pile et les hôtes <strong>2960X</strong> , nous redémarrons le réseau sur les hôtes et vérifions l'interface logique. </p><br><pre> <code class="plaintext hljs">systemctl restart network cat /proc/net/bonding/bond1 2960X#show lacp internal 2960X#sh etherchannel summary</code> </pre> <br><p>  Nous configurons l'interface réseau de contrôle sur chaque hôte du <strong>VLAN 17</strong> et la lions à l'interface logique BOND1: </p><br><div class="spoiler">  <b class="spoiler_title">Configuration de VLAN17 sur Host1:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /etc/sysconfig/network-scripts/ifcfg-bond1.17 DEVICE=bond1.17 NAME=bond1-vlan17 BOOTPROTO=none ONBOOT=yes USERCTL=no NM_CONTROLLED=no VLAN=yes MTU=1500 IPV4_FAILURE_FATAL=yes IPV6INIT=no IPADDR=172.20.1.163 NETMASK=255.255.255.0 GATEWAY=172.20.1.2 DEFROUTE=yes DNS1=172.20.1.8 DNS2=172.20.1.9 ZONE=public</code> </pre> </div></div><br><div class="spoiler">  <b class="spoiler_title">Configuration de VLAN17 sur Host2:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /etc/sysconfig/network-scripts/ifcfg-bond1.17 DEVICE=bond1.17 NAME=bond1-vlan17 BOOTPROTO=none ONBOOT=yes USERCTL=no NM_CONTROLLED=no VLAN=yes MTU=1500 IPV4_FAILURE_FATAL=yes IPV6INIT=no IPADDR=172.20.1.164 NETMASK=255.255.255.0 GATEWAY=172.20.1.2 DEFROUTE=yes DNS1=172.20.1.8 DNS2=172.20.1.9 ZONE=public</code> </pre> </div></div><br><p>  Nous redémarrons le réseau sur les hôtes et vérifions leur visibilité entre eux. </p><br><p>  Ceci termine la configuration de la pile de commutateurs Cisco 2960X, et si tout a été fait correctement, nous avons maintenant une connexion réseau de tous les éléments d'infrastructure les uns aux autres au niveau L2. </p><br><h3 id="nastroyka-shd-dell-md3820f">  Configurer le stockage Dell MD3820f </h3><br><p>  Avant de commencer les travaux de configuration du système de stockage, il doit déjà être connecté à la pile de commutateurs Cisco <strong>2960X</strong> par les interfaces de gestion, ainsi qu'aux <strong>hôtes</strong> <strong>Host1</strong> et <strong>Host2</strong> via FC. </p><br><p>  Le schéma général de la façon dont le stockage doit être connecté à la pile de commutateurs a été donné dans le chapitre précédent. </p><br><p>  Le schéma de connexion du stockage sur FC aux hôtes devrait ressembler à ceci: </p><br><p><img src="https://habrastorage.org/webt/el/k8/5a/elk85aqc6ilmxeiowmh6acrkwri.jpeg"></p><br><p>  Pendant la connexion, il est nécessaire d'enregistrer les adresses WWPN pour les hôtes HBA FC connectés aux ports FC du système de stockage - cela sera nécessaire pour la configuration ultérieure de la liaison des hôtes aux LUN sur le système de stockage. </p><br><p>  Sur le poste de travail de l'administrateur, téléchargez et installez l'utilitaire de gestion du stockage Dell MD3820f - <strong>PowerVault Modular Disk Storage Manager</strong> ( <strong>MDSM</strong> ). <br>  Nous nous y connectons via ses adresses IP par défaut, puis configurons nos adresses à partir de <strong>VLAN17</strong> pour contrôler les contrôleurs via TCP / IP: </p><br><p>  <strong>Stockage1</strong> : </p><br><pre> <code class="plaintext hljs">ControllerA IP - 172.20.1.13, MASK - 255.255.255.0, Gateway - 172.20.1.2 ControllerB IP - 172.20.1.14, MASK - 255.255.255.0, Gateway - 172.20.1.2</code> </pre> <br><p>  Après avoir défini les adresses, accédez à l'interface de gestion du stockage et définissez un mot de passe, définissez l'heure, mettez à jour le micrologiciel des contrôleurs et des disques, si nécessaire, etc. <br>  La procédure à suivre est décrite dans <a href="https://www.dell.com/support/manuals/ru/ru/rubsdc/powervault-md3820f/mdseriesagpub/introduction%3Fguid%3Dguid-51208376-0df9-48a8-be66-63c1b06512ae%26lang%3Den-us">le guide d'administration du</a> stockage. </p><br><p>  Après avoir terminé les paramètres ci-dessus, nous devrons effectuer quelques actions: </p><br><ol><li>  Configurez les <strong>identificateurs</strong> FC <strong>hôte</strong> . </li><li>  Créez un groupe d'hôtes - Groupe d' <strong>hôtes</strong> et ajoutez-y nos deux hôtes Dell. </li><li>  Créez-y un groupe de disques et des disques virtuels (ou LUN) qui seront présentés aux hôtes. </li><li>  Configurez la présentation des disques virtuels (ou LUN) pour les hôtes. </li></ol><br><p>  L'ajout de nouveaux hôtes et la liaison des identifiants des ports FC hôtes se fait via le menu - <strong>Mappages d'hôtes</strong> -&gt; <strong>Définir</strong> -&gt; <strong>Hôtes ...</strong> <br>  Les adresses WWPN des hôtes FC HBA peuvent être trouvées, par exemple, sur un serveur iDRAC. </p><br><p>  En conséquence, nous devrions obtenir quelque chose comme ceci: </p><br><p><img src="https://habrastorage.org/webt/p_/uk/u4/p_uku4o-ewgqpyi0xudxqjesfjc.png"></p><br><p>  L'ajout d'un nouveau groupe d'hôtes et la liaison d'hôtes à celui-ci se fait via le menu - <strong>Host Mappings</strong> -&gt; <strong>Define</strong> -&gt; <strong>Host Group ...</strong> <br>  Pour les hôtes, sélectionnez le type de système d'exploitation - <strong><em>Linux (DM-MP)</em></strong> . </p><br><p>  Après avoir créé le groupe d'hôtes, via l'onglet <strong>Storage &amp; Copy Services</strong> , créez un groupe de <strong>disques</strong> - <strong>Disk Group</strong> , dont le type dépend des exigences de tolérance aux pannes, par exemple, RAID10, et des disques virtuels de la bonne taille: </p><br><p><img src="https://habrastorage.org/webt/ue/g2/kn/ueg2kn0m3usv1kb4tygvx2vajz0.png"></p><br><p>  Et enfin, la dernière étape est la présentation des disques virtuels (ou LUN) pour les hôtes. <br>  Pour ce faire, via le menu - <strong>Mappages d'hôtes</strong> -&gt; <strong>Mappage</strong> <strong>lunaire</strong> -&gt; <strong>Ajouter ...</strong> nous faisons la liaison des disques virtuels aux hôtes, en leur attribuant des numéros. </p><br><p>  Tout devrait se passer comme dans cette capture d'écran: </p><br><p><img src="https://habrastorage.org/webt/db/a-/xq/dba-xqbacgrjxrutupvhvmua7ro.png"></p><br><p>  Nous en avons terminé avec la configuration des systèmes de stockage, et si tout a été fait correctement, les hôtes devraient voir les LUN qui leur sont présentés via leurs HBA FC. <br>  Demandez au système de mettre à jour les informations sur les lecteurs mappés: </p><br><pre> <code class="plaintext hljs">ls -la /sys/class/scsi_host/ echo "- - -" &gt; /sys/class/scsi_host/host[0-9]/scan</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Voyons quels appareils sont visibles sur nos serveurs:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /proc/scsi/scsi Attached devices: Host: scsi0 Channel: 02 Id: 00 Lun: 00 Vendor: DELL Model: PERC H330 Mini Rev: 4.29 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 00 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 01 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 04 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 11 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 31 Vendor: DELL Model: Universal Xport Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 00 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 01 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 04 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 11 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 31 Vendor: DELL Model: Universal Xport Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 lsscsi [0:2:0:0] disk DELL PERC H330 Mini 4.29 /dev/sda [15:0:0:0] disk DELL MD38xxf 0825 - [15:0:0:1] disk DELL MD38xxf 0825 /dev/sdb [15:0:0:4] disk DELL MD38xxf 0825 /dev/sdc [15:0:0:11] disk DELL MD38xxf 0825 /dev/sdd [15:0:0:31] disk DELL Universal Xport 0825 - [18:0:0:0] disk DELL MD38xxf 0825 - [18:0:0:1] disk DELL MD38xxf 0825 /dev/sdi [18:0:0:4] disk DELL MD38xxf 0825 /dev/sdj [18:0:0:11] disk DELL MD38xxf 0825 /dev/sdk [18:0:0:31] disk DELL Universal Xport 0825 -</code> </pre> </div></div><br><p>  Vous pouvez également configurer le <strong>multichemin</strong> sur les hôtes, et bien que lors de l'installation de oVirt, il puisse le faire lui-même, il est préférable de vérifier le MP par vous-même à l'avance. </p><br><p>  <u>Installer et configurer DM Multipath</u> </p><br><pre> <code class="plaintext hljs">yum install device-mapper-multipath mpathconf --enable --user_friendly_names y cat /etc/multipath.conf | egrep -v "^\s*(#|$)" defaults { user_friendly_names yes find_multipaths yes } blacklist { wwid 26353900f02796769 devnode "^(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]*" devnode "^hd[az]" }</code> </pre> <br><p>  Installez le service MP en démarrage automatique et exécutez-le: </p><br><pre> <code class="plaintext hljs">systemctl enable multipathd &amp;&amp; systemctl restart multipathd</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Vérification des informations sur les modules chargés pour le fonctionnement MP:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">lsmod | grep dm_multipath dm_multipath 27792 6 dm_service_time dm_mod 124407 139 dm_multipath,dm_log,dm_mirror modinfo dm_multipath filename: /lib/modules/3.10.0-957.12.2.el7.x86_64/kernel/drivers/md/dm-multipath.ko.xz license: GPL author: Sistina Software &lt;dm-devel@redhat.com&gt; description: device-mapper multipath target retpoline: Y rhelversion: 7.6 srcversion: 985A03DCAF053D4910E53EE depends: dm-mod intree: Y vermagic: 3.10.0-957.12.2.el7.x86_64 SMP mod_unload modversions signer: CentOS Linux kernel signing key sig_key: A3:2D:39:46:F2:D3:58:EA:52:30:1F:63:37:8A:37:A5:54:03:00:45 sig_hashalgo: sha256</code> </pre> </div></div><br><p>  Voir le résumé de la configuration multichemin existante: </p><br><pre> <code class="plaintext hljs">mpathconf multipath is enabled find_multipaths is disabled user_friendly_names is disabled dm_multipath module is loaded multipathd is running</code> </pre> <br><p>  Après avoir ajouté un nouveau LUN au stockage et l'avoir présenté à l'hôte, il est nécessaire de le scanner connecté à l'hôte HBA. </p><br><pre> <code class="plaintext hljs">systemctl reload multipathd multipath -v2</code> </pre> <br><p>  Et enfin, nous vérifions si tous les LUN ont été présentés sur le stockage pour les hôtes, et si tous ont deux chemins. </p><br><div class="spoiler">  <b class="spoiler_title">Vérifier le fonctionnement du MP:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">multipath -ll 3600a098000e4b4b3000003175cec1840 dm-2 DELL ,MD38xxf size=2.0T features='3 queue_if_no_path pg_init_retries 50' hwhandler='1 rdac' wp=rw |-+- policy='service-time 0' prio=14 status=active | `- 15:0:0:1 sdb 8:16 active ready running `-+- policy='service-time 0' prio=9 status=enabled `- 18:0:0:1 sdi 8:128 active ready running 3600a098000e4b48f000002ab5cec1921 dm-6 DELL ,MD38xxf size=10T features='3 queue_if_no_path pg_init_retries 50' hwhandler='1 rdac' wp=rw |-+- policy='service-time 0' prio=14 status=active | `- 18:0:0:11 sdk 8:160 active ready running `-+- policy='service-time 0' prio=9 status=enabled `- 15:0:0:11 sdd 8:48 active ready running 3600a098000e4b4b3000003c95d171065 dm-3 DELL ,MD38xxf size=150G features='3 queue_if_no_path pg_init_retries 50' hwhandler='1 rdac' wp=rw |-+- policy='service-time 0' prio=14 status=active | `- 15:0:0:4 sdc 8:32 active ready running `-+- policy='service-time 0' prio=9 status=enabled `- 18:0:0:4 sdj 8:144 active ready running</code> </pre> </div></div><br><p>  Comme vous pouvez le voir, les trois disques virtuels du système de stockage sont visibles de deux manières.  Ainsi, tous les travaux préparatoires ont été achevés, ce qui signifie que nous pouvons passer à la partie principale - la mise en place du cluster oVirt, qui sera discutée dans le prochain article. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr483980/">https://habr.com/ru/post/fr483980/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr483964/index.html">N'ayez pas peur de JSON ou de votre première application API</a></li>
<li><a href="../fr483972/index.html">Comment utiliser Quora pour promouvoir votre entreprise</a></li>
<li><a href="../fr483974/index.html">Ceph via iSCSI - ou skier debout dans un hamac</a></li>
<li><a href="../fr483976/index.html">Cybersécurité et menaces 2020: ce qui nous attend après les vacances</a></li>
<li><a href="../fr483978/index.html">Comprendre le concept de développement d'applications Web modernes en 2020</a></li>
<li><a href="../fr483986/index.html">Contrôle des pensées des robots avec Emotiv Insight</a></li>
<li><a href="../fr483988/index.html">MicroSPA, ou comment inventer une roue carrée</a></li>
<li><a href="../fr483992/index.html">Pourquoi certaines planètes mangent leur ciel</a></li>
<li><a href="../fr483994/index.html">Déménagement informatique sur un yacht. De la Suède à l'Espagne</a></li>
<li><a href="../fr484004/index.html">@Pythonetc décembre 2019</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>