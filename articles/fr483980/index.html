<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üìΩÔ∏è üëäüèø ‚õ∏Ô∏è Cr√©ation d'une infrastructure informatique tol√©rante aux pannes. Partie 1 - Pr√©paration du d√©ploiement du cluster oVirt 4.3 üî° üçè üë®üèø‚Äçü§ù‚Äçüë®üèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Les lecteurs sont invit√©s √† se familiariser avec les principes de construction d'une infrastructure tol√©rante aux pannes d'une petite entreprise au se...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cr√©ation d'une infrastructure informatique tol√©rante aux pannes. Partie 1 - Pr√©paration du d√©ploiement du cluster oVirt 4.3</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/lenvendo/blog/483980/"><p>  Les lecteurs sont invit√©s √† se familiariser avec les principes de construction d'une infrastructure tol√©rante aux pannes d'une petite entreprise au sein d'un centre de donn√©es, qui seront examin√©s en d√©tail dans une courte s√©rie d'articles. </p><a name="habracut"></a><br><h2 id="vvodnaya-chast">  Pr√©sentation </h2><br><p>  Sous le <strong>DPC</strong> (Data Processing Center) on peut comprendre: </p><br><ul><li>  propre rack dans sa ¬´salle de serveurs¬ª sur le territoire de l'entreprise qui r√©pond aux exigences minimales pour fournir des √©quipements d'alimentation et de refroidissement, ainsi que d'avoir acc√®s √† Internet via deux fournisseurs ind√©pendants; </li><li>  rack lou√© avec son propre √©quipement situ√© dans ce centre de donn√©es - le soi-disant  la colocalisation, conforme √† la norme Tier III ou IV, qui garantit une alimentation √©lectrique, un refroidissement et un acc√®s Internet √† tol√©rance de pannes fiables; </li><li>  √©quipement enti√®rement lou√© dans un centre de donn√©es de niveau III ou IV. </li></ul><br><p>  Quelle option d'h√©bergement choisir - dans chaque cas, tout est individuel et d√©pend g√©n√©ralement de plusieurs facteurs principaux: </p><br><ul><li>  pourquoi l'entreprise a-t-elle sa propre infrastructure informatique; </li><li>  ce que l'entreprise attend exactement de l'infrastructure informatique (fiabilit√©, √©volutivit√©, facilit√© de gestion, etc.); </li><li>  le montant de l'investissement initial dans l'infrastructure informatique, ainsi que le type de co√ªts qu'il s'agit - en capital (ce qui signifie que vous achetez votre √©quipement) ou en fonctionnement (l'√©quipement est g√©n√©ralement lou√©); </li><li>  Horizon de planification de l'entreprise elle-m√™me. </li></ul><br><p>  Beaucoup de choses peuvent √™tre √©crites sur les facteurs qui influencent la d√©cision de l'entreprise de cr√©er et d'utiliser son infrastructure informatique, mais notre objectif est de montrer dans la pratique comment cr√©er cette infrastructure afin qu'elle soit √† la fois tol√©rante aux pannes et qu'il soit possible d'√©conomiser de l'argent - r√©duire le co√ªt d'acquisition de logiciels commerciaux, voire les √©viter. </p><br><p>  Comme le montre la longue pratique, cela ne vaut pas la peine d'√©conomiser sur le mat√©riel, car l'avare paie deux fois, et bien plus encore.  Mais l√† encore - bon mat√©riel, ce n'est qu'une recommandation, et finalement quoi acheter exactement et pour combien d√©pend des capacit√©s de l'entreprise et de la ¬´cupidit√©¬ª de sa gestion.  En outre, le mot ¬´cupidit√©¬ª doit √™tre compris dans le bon sens du terme, car il vaut mieux investir dans le fer au stade initial, de sorte que plus tard, il n'y a pas de probl√®mes s√©rieux dans son soutien et sa mise √† l'√©chelle, car une planification initialement incorrecte et des √©conomies excessives peuvent conduire √† l'avenir. plus cher que lors du d√©marrage d'un projet. </p><br><p>  Ainsi, les donn√©es initiales du projet: </p><br><ul><li>  il y a une entreprise qui a d√©cid√© de cr√©er son propre portail Web et de mettre ses activit√©s sur Internet; </li><li>  la soci√©t√© a d√©cid√© de louer un rack pour placer son √©quipement dans un bon centre de donn√©es certifi√© selon la norme Tier III; </li><li>  l'entreprise a d√©cid√© de ne pas √©conomiser beaucoup sur le mat√©riel et a donc achet√© l'√©quipement suivant avec des garanties et un support √©tendus: </li></ul><br><div class="spoiler">  <b class="spoiler_title">Liste des √©quipements</b> <div class="spoiler_text"><blockquote><ul><li>  deux serveurs physiques Dell PowerEdge R640 comme suit: </li><li>  <em>deux processeurs Intel Xeon Gold 5120</em> </li><li>  <em>512 Go de RAM</em> </li><li>  <em>deux disques SAS en RAID1, pour l'installation du syst√®me d'exploitation</em> </li><li>  <em>carte r√©seau 1G 4 ports int√©gr√©e</em> </li><li>  <em>deux cartes r√©seau 10G √† 2 ports</em> </li><li>  <em>un FC HBA 16G √† 2 ports.</em> </li><li>  Syst√®me de stockage √† 2 contr√¥leurs Dell MD3820f, connect√© via FC 16G directement aux h√¥tes Dell; </li><li>  deux commutateurs du deuxi√®me niveau - Cisco WS-C2960RX-48FPS-L empil√©s; </li><li> deux commutateurs de couche 3 - Cisco WS-C3850-24T-E, empil√©s; </li><li>  Rack, UPS, PDU, serveurs de console - fournis par le centre de donn√©es. </li></ul><br></blockquote></div></div><br><p>  Comme nous pouvons le voir, l'√©quipement existant a de bonnes perspectives de mise √† l'√©chelle horizontale et verticale, si l'entreprise peut concurrencer d'autres entreprises de profil similaire sur Internet et commence √† r√©aliser un profit qui peut √™tre investi dans l'expansion des ressources pour une concurrence accrue et une croissance des b√©n√©fices. </p><br><p>  Quel √©quipement pouvons-nous ajouter si l'entreprise d√©cide d'augmenter les performances de notre cluster informatique: </p><br><ul><li>  nous avons une grande r√©serve pour le nombre de ports sur les commutateurs 2960X, ce qui signifie que vous pouvez ajouter plus de serveurs mat√©riels; </li><li>  acheter deux commutateurs FC pour y connecter des syst√®mes de stockage et des serveurs suppl√©mentaires; </li><li>  les serveurs d√©j√† existants peuvent √™tre mis √† niveau - ajoutez de la m√©moire, remplacez les processeurs par des plus efficaces, connectez les adaptateurs r√©seau existants √† un r√©seau 10G; </li><li>  Vous pouvez ajouter des √©tag√®res de disques suppl√©mentaires au stockage avec le type de disques n√©cessaire - SAS, SATA ou SSD, en fonction de la charge pr√©vue; </li><li>  apr√®s avoir ajout√© des commutateurs FC, vous pouvez acheter un autre syst√®me de stockage pour ajouter encore plus de capacit√© de disque, et si vous lui achetez l'option sp√©ciale de r√©plication √† distance, vous pouvez configurer la r√©plication des donn√©es entre les syst√®mes de stockage √† la fois dans le m√™me centre de donn√©es et entre les centres de donn√©es (mais cela d√©passe d√©j√† la port√©e de l'article); </li><li>  il existe √©galement des commutateurs de troisi√®me niveau - le Cisco 3850, qui peut √™tre utilis√© comme c≈ìur tol√©rant aux pannes du r√©seau pour le routage √† grande vitesse entre les r√©seaux internes.  Cela contribuera grandement √† l'avenir, √† mesure que l'infrastructure interne se d√©veloppera.  Le 3850 poss√®de √©galement des ports 10G qui peuvent √™tre activ√©s ult√©rieurement lors de la mise √† niveau de l'√©quipement r√©seau vers 10G. </li></ul><br><p>  Puisqu'il n'y a plus nulle part sans virtualisation, alors bien s√ªr nous serons √† la mode, d'autant plus que c'est un excellent moyen de r√©duire le co√ªt d'achat de serveurs co√ªteux pour certains √©l√©ments d'infrastructure (serveurs web, bases de donn√©es, etc.), qui ne sont pas toujours optimaux utilis√© en cas de faible charge, et c'est exactement ce qui sera au d√©but du lancement du projet. </p><br><p>  De plus, la virtualisation pr√©sente de nombreux autres avantages qui peuvent nous √™tre tr√®s utiles: tol√©rance aux pannes de VM suite √† une d√©faillance du serveur mat√©riel, migration en direct entre les n≈ìuds mat√©riels du cluster pour leur maintenance, √©quilibrage de charge manuel ou automatique entre les n≈ìuds du cluster, etc. </p><br><p>  Pour le mat√©riel acquis par l'entreprise, le d√©ploiement du cluster VMware vSphere hautement accessible se sugg√®re, mais comme tout logiciel de VMware est connu pour ses prix √† cheval, nous utiliserons un logiciel de gestion de la virtualisation absolument gratuit - <a href="https://ru.wikipedia.org/wiki/OVirt"><strong>oVirt</strong></a> , sur la base duquel le c√©l√®bre mais produit d√©j√† commercial - <a href="https://ru.bmstu.wiki/RHEV_(Red_Hat_Enterprise_Virtualization)"><strong>RHEV</strong></a> . </p><br><p>  Le logiciel <strong>oVirt</strong> est n√©cessaire pour combiner tous les √©l√©ments de l'infrastructure afin de pouvoir travailler facilement avec des machines virtuelles hautement accessibles - ce sont des bases de donn√©es, des applications Web, des proxys, des √©quilibreurs, des serveurs pour collecter des journaux et des analyses, etc. n., c'est-√†-dire en quoi consiste le portail Web de notre entreprise. </p><br><p>  Pour r√©sumer cette introduction, les articles suivants nous attendent, qui en pratique montreront comment d√©ployer l'int√©gralit√© de l'infrastructure mat√©rielle et logicielle de l'entreprise: </p><br><div class="spoiler">  <b class="spoiler_title">Liste d'articles</b> <div class="spoiler_text"><ul><li>  <strong>Partie 1.</strong> Pr√©paration du d√©ploiement du cluster oVirt 4.3. </li><li>  <strong>Partie 2.</strong> Installation et configuration du cluster oVirt 4.3. </li><li>  <strong>Partie 3.</strong> Organisation du routage tol√©rant aux pannes sur les routeurs virtuels VyOS. </li><li>  <strong>Partie 4.</strong> Configuration de la pile Cisco 3850, organisation du routage intranet. </li></ul></div></div><br><h2 id="chast-1-podgotovka-k-razvyortyvaniyu-klastera-ovirt-43">  Partie 1. Pr√©paration du d√©ploiement du cluster oVirt 4.3 </h2><br><h3 id="bazovaya-nastroyka-hostov">  Configuration de base de l'h√¥te </h3><br><p>  L'installation et la configuration du syst√®me d'exploitation est l'√©tape la plus simple.  Il y a beaucoup d'articles sur la fa√ßon d'installer et de configurer correctement le syst√®me d'exploitation, donc cela n'a aucun sens d'essayer de donner quelque chose d'exclusif √† ce sujet. </p><br><p>  Nous avons donc deux h√¥tes Dell PowerEdge R640, sur lesquels vous devez installer le syst√®me d'exploitation et effectuer des pr√©configurations, pour les utiliser comme hyperviseurs pour ex√©cuter des machines virtuelles dans le cluster oVirt 4.3. </p><br><p>  Comme nous pr√©voyons d'utiliser le logiciel gratuit non commercial oVirt, <strong>CentOS 7.7 a √©t√©</strong> choisi pour le d√©ploiement des h√¥tes, bien que d'autres syst√®mes d'exploitation puissent √©galement √™tre install√©s sur les h√¥tes pour oVirt: </p><br><ul><li>  construction sp√©ciale bas√©e sur RHEL, la soi-disant  <a href="https://www.ovirt.org/documentation/admin-guide/chap-Hosts.html"><strong>oVirt Node</strong></a> ; </li><li>  OS Oracle Linux, √† l'√©t√© 2019, le support pour le fonctionnement de oVirt sur celui-ci a <a href="https://blogs.oracle.com/virtualization/announcing-oracle-linux-virtualization-manager">√©t√© annonc√©</a> . </li></ul><br><p>  Avant d'installer le syst√®me d'exploitation, il est recommand√©: </p><br><ul><li>  Configurer l'interface r√©seau iDRAC sur les deux h√¥tes </li><li>  mettre √† jour le firmware du BIOS et de l'iDRAC vers les derni√®res versions; </li><li>  il est souhaitable de configurer le serveur System Profile en mode Performance; </li><li>  configurer le RAID √† partir de disques locaux (RAID1 recommand√©) pour installer le syst√®me d'exploitation sur le serveur. </li></ul><br><p>  Ensuite, nous installons le syst√®me d'exploitation sur le disque cr√©√© pr√©c√©demment via iDRAC - le processus d'installation est normal, il n'y a pas de moments sp√©ciaux.  L'acc√®s √† la console du serveur pour commencer l'installation du syst√®me d'exploitation peut √©galement √™tre obtenu via iDRAC, bien que rien ne vous emp√™che de connecter le moniteur, le clavier et la souris directement au serveur et d'installer le syst√®me d'exploitation √† partir d'un lecteur flash. </p><br><p>  Apr√®s avoir install√© le syst√®me d'exploitation, effectuez ses r√©glages initiaux: </p><br><pre><code class="plaintext hljs">systemctl enable network.service systemctl start network.service systemctl status network.service</code> </pre> <br><pre> <code class="plaintext hljs">systemctl stop NetworkManager systemctl disable NetworkManager systemctl status NetworkManager</code> </pre> <br><pre> <code class="plaintext hljs">yum install -y ntp systemctl enable ntpd.service systemctl start ntpd.service</code> </pre> <br><pre> <code class="plaintext hljs">cat /etc/sysconfig/selinux SELINUX=disabled SELINUXTYPE=targeted</code> </pre> <br><pre> <code class="plaintext hljs">cat /etc/security/limits.conf * soft nofile 65536 * hard nofile 65536</code> </pre> <br><pre> <code class="plaintext hljs">cat /etc/sysctl.conf vm.max_map_count = 262144 vm.swappiness = 1</code> </pre> <br><p>  <u>Installer l'ensemble de logiciels de base</u> </p><br><p>  Pour la configuration initiale du syst√®me d'exploitation, vous devez configurer n'importe quelle interface r√©seau sur le serveur afin de pouvoir acc√©der √† Internet, mettre √† jour le syst√®me d'exploitation et installer les packages logiciels n√©cessaires.  Cela peut √™tre fait √† la fois pendant l'installation du syst√®me d'exploitation et apr√®s. </p><br><pre> <code class="plaintext hljs">yum -y install epel-release yum update yum -y install bind-utils yum-utils net-tools git htop iotop nmon pciutils sysfsutils sysstat mc nc rsync wget traceroute gzip unzip telnet</code> </pre> <br><p>  Tous les param√®tres ci-dessus et un ensemble de logiciels sont une question de pr√©f√©rence personnelle, et cet ensemble n'est qu'une recommandation. </p><br><p>  Puisque notre h√©bergeur jouera le r√¥le d'un hyperviseur, nous activerons le profil de performance souhait√©: </p><br><pre> <code class="plaintext hljs">systemctl enable tuned systemctl start tuned systemctl status tuned</code> </pre> <br><pre> <code class="plaintext hljs">tuned-adm profile tuned-adm profile virtual-host</code> </pre> <br><p>  Vous pouvez en savoir plus sur le profil de performance ici: " <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_tuning_and_optimization_guide/chap-virtualization_tuning_optimization_guide-tuned">Chapitre 4. tuned et tuned-adm</a> ". </p><br><p>  Apr√®s avoir install√© le syst√®me d'exploitation, nous passons √† la partie suivante - la configuration des interfaces r√©seau sur les h√¥tes et la pile de commutateurs Cisco 2960X. </p><br><h3 id="nastroyka-steka-kommutatorov-cisco-2960x">  Configuration de la pile de commutateurs Cisco 2960X </h3><br><p>  Notre projet utilisera les nombres suivants de VLAN - ou domaines de diffusion isol√©s les uns des autres, afin de s√©parer diff√©rents types de trafic: </p><br><p>  <strong>VLAN 10</strong> - Internet <br>  <strong>VLAN 17</strong> - Gestion (iDRAC, stockage, gestion des commutateurs) <br>  <strong>VLAN 32</strong> - R√©seau de production de VM <br>  <strong>VLAN 33</strong> - r√©seau d'interconnexion (√† des sous-traitants externes) <br>  <strong>VLAN 34</strong> - R√©seau de test VM <br>  <strong>VLAN 35</strong> - R√©seau de d√©veloppeurs VM <br>  <strong>VLAN 40</strong> - R√©seau de surveillance </p><br><p>  Avant de commencer les travaux, nous pr√©sentons un sch√©ma au niveau L2, auquel nous devrions enfin arriver: </p><br><p><img src="https://habrastorage.org/webt/in/ak/8a/inak8aoyty8yujo2zagwdehdytg.jpeg"></p><br><p>  Pour l'interaction r√©seau entre les h√¥tes oVirt et les machines virtuelles entre eux, ainsi que pour g√©rer notre stockage, vous devez configurer la pile de commutateurs Cisco 2960X. </p><br><p>  Les h√¥tes Dell ont des cartes r√©seau √† 4 ports int√©gr√©es, il est donc conseill√© d'organiser leur connexion au Cisco 2960X √† l'aide d'une connexion r√©seau √† tol√©rance de pannes, en utilisant le regroupement des ports r√©seau physiques dans une interface logique et le protocole LACP (802.3ad): </p><br><ul><li>  les deux premiers ports de l'h√¥te sont configur√©s en mode de liaison et connect√©s au commutateur 2960X - sur cette interface logique, un <strong><em>pont</em></strong> avec une adresse pour g√©rer l'h√¥te, la surveillance, la communication avec les autres h√¥tes du cluster oVirt sera configur√©, il sera √©galement utilis√© pour la migration en direct des machines virtuelles; </li><li>  les deux seconds ports de l'h√¥te sont √©galement configur√©s en mode liaison et connect√©s au 2960X - sur cette interface logique utilisant oVirt, des ponts ult√©rieurs seront cr√©√©s (dans les VLAN correspondants) auxquels les machines virtuelles seront connect√©es. </li><li>  les deux ports r√©seau, au sein de la m√™me interface logique, seront actifs, c'est-√†-dire  Le trafic sur eux peut √™tre transmis simultan√©ment, en mode d'√©quilibrage. </li><li>  les param√®tres r√©seau sur les n≈ìuds du cluster doivent √™tre absolument identiques, √† l'exception des adresses IP. </li></ul><br><p>  <u>Configuration de base de la <strong>pile de</strong> commutateurs <strong>2960X</strong> et de ses ports</u> </p><br><p>  Auparavant, nos commutateurs devaient √™tre: </p><br><ul><li>  mont√© dans un rack; </li><li>  connect√©s par deux c√¢bles sp√©ciaux de la longueur souhait√©e, par exemple CAB-STK-E-1M; </li><li>  connect√© √† l'alimentation; </li><li>  connect√© au poste de travail de l'administrateur via le port console, pour leur configuration initiale. </li></ul><br><p>  Les conseils n√©cessaires √† cet effet sont disponibles sur <a href="https://www.cisco.com/c/en/us/support/switches/catalyst-2960-x-series-switches/products-installation-guides-list.html">la page officielle</a> du fabricant. </p><br><p>  Apr√®s avoir effectu√© les √©tapes ci-dessus, configurez les commutateurs. <br>  Ce que signifie chaque √©quipe n'est pas cens√© √™tre d√©crypt√© dans le cadre de cet article; si n√©cessaire, toutes les informations peuvent √™tre trouv√©es ind√©pendamment. <br>  Notre objectif est de configurer la pile de commutateurs le plus rapidement possible et d'y connecter les h√¥tes et les interfaces de gestion du stockage. </p><br><p>  1) Connectez-vous au commutateur principal, passez en mode privil√©gi√©, puis passez en mode de configuration et effectuez les r√©glages de base. </p><br><div class="spoiler">  <b class="spoiler_title">Configuration de commutation de base:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs"> enable configure terminal hostname 2960X no service pad service timestamps debug datetime msec service timestamps log datetime localtime show-timezone msec no service password-encryption service sequence-numbers switch 1 priority 15 switch 2 priority 14 stack-mac persistent timer 0 clock timezone MSK 3 vtp mode transparent ip subnet-zero vlan 17 name Management vlan 32 name PROD vlan 33 name Interconnect vlan 34 name Test vlan 35 name Dev vlan 40 name Monitoring spanning-tree mode rapid-pvst spanning-tree etherchannel guard misconfig spanning-tree portfast bpduguard default spanning-tree extend system-id spanning-tree vlan 1-40 root primary spanning-tree loopguard default vlan internal allocation policy ascending port-channel load-balance src-dst-ip errdisable recovery cause loopback errdisable recovery cause bpduguard errdisable recovery interval 60 line con 0 session-timeout 60 exec-timeout 60 0 logging synchronous line vty 5 15 session-timeout 60 exec-timeout 60 0 logging synchronous ip http server ip http secure-server no vstack interface Vlan1 no ip address shutdown exit</code> </pre></div></div><br><p>  Nous enregistrons la configuration avec la commande <strong>wr mem</strong> et rechargeons la pile de commutateurs avec la commande <strong>reload</strong> sur le commutateur principal 1. </p><br><p>  2) Configurez les ports r√©seau du commutateur en mode d'acc√®s dans VLAN 17, pour connecter les interfaces de gestion des serveurs de stockage et iDRAC. </p><br><div class="spoiler">  <b class="spoiler_title">Param√®tres du port de gestion:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">interface GigabitEthernet1/0/5 description iDRAC - host1 switchport access vlan 17 switchport mode access spanning-tree portfast edge interface GigabitEthernet1/0/6 description Storage1 - Cntr0/Eth0 switchport access vlan 17 switchport mode access spanning-tree portfast edge interface GigabitEthernet2/0/5 description iDRAC - host2 switchport access vlan 17 switchport mode access spanning-tree portfast edge interface GigabitEthernet2/0/6 description Storage1 ‚Äì Cntr1/Eth0 switchport access vlan 17 switchport mode access spanning-tree portfast edge exit</code> </pre> </div></div><br><p>  3) Apr√®s avoir red√©marr√© la pile, v√©rifiez qu'elle fonctionne correctement: </p><br><div class="spoiler">  <b class="spoiler_title">V√©rification du fonctionnement de la pile:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">2960X#show switch stack-ring speed Stack Ring Speed : 20G Stack Ring Configuration: Full Stack Ring Protocol : FlexStack 2960X#show switch stack-ports Switch # Port 1 Port 2 -------- ------ ------ 1 Ok Ok 2 Ok Ok 2960X#show switch neighbors Switch # Port 1 Port 2 -------- ------ ------ 1 2 2 2 1 1 2960X#show switch detail Switch/Stack Mac Address : 0cd0.f8e4. Mac persistency wait time: Indefinite H/W Current Switch# Role Mac Address Priority Version State ---------------------------------------------------------- *1 Master 0cd0.f8e4. 15 4 Ready 2 Member 0029.c251. 14 4 Ready Stack Port Status Neighbors Switch# Port 1 Port 2 Port 1 Port 2 -------------------------------------------------------- 1 Ok Ok 2 2 2 Ok Ok 1 1</code> </pre> </div></div><br><p>  4) Configuration de l'acc√®s SSH √† la pile 2960X </p><br><p>  Pour la gestion √† distance de la pile via SSH, nous utiliserons IP 172.20.1.10 configur√© sur SVI (switch virtual interface) <strong>VLAN17</strong> . </p><br><p>  Bien qu'√† des fins de gestion, il est conseill√© d'utiliser un port d√©di√© sp√©cial sur le commutateur, mais c'est une question de pr√©f√©rence personnelle et d'opportunit√©. </p><br><div class="spoiler">  <b class="spoiler_title">Configuration de l'acc√®s SSH √† la pile de commutateurs:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">ip default-gateway 172.20.1.2 interface vlan 17 ip address 172.20.1.10 255.255.255.0 hostname 2960X ip domain-name hw.home-lab.ru no ip domain-lookup clock set 12:47:04 06 Dec 2019 crypto key generate rsa ip ssh version 2 ip ssh time-out 90 line vty 0 4 session-timeout 60 exec-timeout 60 0 privilege level 15 logging synchronous transport input ssh line vty 5 15 session-timeout 60 exec-timeout 60 0 privilege level 15 logging synchronous transport input ssh aaa new-model aaa authentication login default local username cisco privilege 15 secret my_ssh_password</code> </pre> </div></div><br><p>  Configurez un mot de passe pour entrer en mode privil√©gi√©: </p><br><pre> <code class="plaintext hljs">enable secret *myenablepassword* service password-encryption</code> </pre> <br><p>  Configurer NTP: </p><br><pre> <code class="plaintext hljs">ntp server 85.21.78.8 prefer ntp server 89.221.207.113 ntp server 185.22.60.71 ntp server 192.36.143.130 ntp server 185.209.85.222 show ntp status show ntp associations show clock detail</code> </pre> <br><p>  5) Configurez les interfaces logiques Etherchannel et les ports physiques connect√©s aux h√¥tes.  Pour faciliter la configuration, tous les VLAN disponibles seront autoris√©s sur toutes les interfaces logiques, mais il est g√©n√©ralement recommand√© de configurer uniquement ce dont vous avez besoin: </p><br><div class="spoiler">  <b class="spoiler_title">Configurer les interfaces Etherchannel:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">interface Port-channel1 description EtherChannel with Host1-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk spanning-tree portfast edge trunk interface Port-channel2 description EtherChannel with Host2-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk spanning-tree portfast edge trunk interface Port-channel3 description EtherChannel with Host1-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk spanning-tree portfast edge trunk interface Port-channel4 description EtherChannel with Host2-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk spanning-tree portfast edge trunk interface GigabitEthernet1/0/1 description Host1-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 1 mode active interface GigabitEthernet1/0/2 description Host2-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 2 mode active interface GigabitEthernet1/0/3 description Host1-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 3 mode active interface GigabitEthernet1/0/4 description Host2-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 4 mode active interface GigabitEthernet2/0/1 description Host1-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 1 mode active interface GigabitEthernet2/0/2 description Host2-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 2 mode active interface GigabitEthernet2/0/3 description Host1-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 3 mode active interface GigabitEthernet2/0/4 description Host2-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 4 mode active</code> </pre> </div></div><br><p>  <u>Configuration initiale des interfaces r√©seau pour les machines virtuelles sur <strong>Host1</strong> et <strong>Host2</strong></u> </p><br><p>  Nous v√©rifions la disponibilit√© des modules n√©cessaires au collage dans le syst√®me, installons le module de gestion des ponts: </p><br><pre> <code class="plaintext hljs">modinfo bonding modinfo 8021q yum install bridge-utils</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Configuration de l'interface logique BOND1 pour les machines virtuelles sur les h√¥tes et ses interfaces physiques:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /etc/sysconfig/network-scripts/ifcfg-bond1 #DESCRIPTION - management DEVICE=bond1 NAME=bond1 TYPE=Bond IPV6INIT=no ONBOOT=yes USERCTL=no NM_CONTROLLED=no BOOTPROTO=none BONDING_OPTS='mode=4 lacp_rate=1 xmit_hash_policy=2' cat /etc/sysconfig/network-scripts/ifcfg-em2 #DESCRIPTION - management DEVICE=em2 TYPE=Ethernet BOOTPROTO=none ONBOOT=yes MASTER=bond1 SLAVE=yes USERCTL=no NM_CONTROLLED=no cat /etc/sysconfig/network-scripts/ifcfg-em3 #DESCRIPTION - management DEVICE=em3 TYPE=Ethernet BOOTPROTO=none ONBOOT=yes MASTER=bond1 SLAVE=yes USERCTL=no NM_CONTROLLED=no</code> </pre> </div></div><br><p>  Apr√®s avoir termin√© les r√©glages sur la pile et les h√¥tes <strong>2960X</strong> , nous red√©marrons le r√©seau sur les h√¥tes et v√©rifions l'interface logique. </p><br><ul><li>  sur l'h√¥te: </li></ul><br><pre> <code class="plaintext hljs">systemctl restart network cat /proc/net/bonding/bond1 Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011) Bonding Mode: IEEE 802.3ad Dynamic link aggregation Transmit Hash Policy: layer2+3 (2) MII Status: up MII Polling Interval (ms): 100 Up Delay (ms): 0 Down Delay (ms): 0 ... 802.3ad info LACP rate: fast Min links: 0 Aggregator selection policy (ad_select): stable System priority: 65535 ... Slave Interface: em2 MII Status: up Speed: 1000 Mbps Duplex: full ... Slave Interface: em3 MII Status: up Speed: 1000 Mbps Duplex: full</code> </pre> <br><ul><li>  sur la <strong>pile de</strong> commutateurs <strong>2960X</strong> : </li></ul><br><pre> <code class="plaintext hljs">2960X#show lacp internal Flags: S - Device is requesting Slow LACPDUs F - Device is requesting Fast LACPDUs A - Device is in Active mode P - Device is in Passive mode Channel group 1 LACP port Admin Oper Port Port Port Flags State Priority Key Key Number State Gi1/0/1 SA bndl 32768 0x1 0x1 0x102 0x3D Gi2/0/1 SA bndl 32768 0x1 0x1 0x202 0x3D 2960X#sh etherchannel summary Flags: D - down P - bundled in port-channel I - stand-alone s - suspended H - Hot-standby (LACP only) R - Layer3 S - Layer2 U - in use N - not in use, no aggregation f - failed to allocate aggregator M - not in use, minimum links not met m - not in use, port not aggregated due to minimum links not met u - unsuitable for bundling w - waiting to be aggregated d - default port A - formed by Auto LAG Number of channel-groups in use: 11 Number of aggregators: 11 Group Port-channel Protocol Ports ------+-------------+-----------+----------------------------------------------- 1 Po1(SU) LACP Gi1/0/1(P) Gi2/0/1(P)</code> </pre> <br><p>  Configuration initiale des interfaces r√©seau pour la gestion des ressources de cluster sur <strong>Host1</strong> et <strong>Host2</strong> </p><br><div class="spoiler">  <b class="spoiler_title">Configuration de l'interface logique BOND1 pour la gestion et ses interfaces physiques sur les h√¥tes:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /etc/sysconfig/network-scripts/ifcfg-bond0 #DESCRIPTION - management DEVICE=bond0 NAME=bond0 TYPE=Bond BONDING_MASTER=yes IPV6INIT=no ONBOOT=yes USERCTL=no NM_CONTROLLED=no BOOTPROTO=none BONDING_OPTS='mode=4 lacp_rate=1 xmit_hash_policy=2' cat /etc/sysconfig/network-scripts/ifcfg-em0 #DESCRIPTION - management DEVICE=em0 TYPE=Ethernet BOOTPROTO=none ONBOOT=yes MASTER=bond0 SLAVE=yes USERCTL=no NM_CONTROLLED=no cat /etc/sysconfig/network-scripts/ifcfg-em1 #DESCRIPTION - management DEVICE=em1 TYPE=Ethernet BOOTPROTO=none ONBOOT=yes MASTER=bond0 SLAVE=yes USERCTL=no NM_CONTROLLED=no</code> </pre> </div></div><br><p>  Apr√®s avoir termin√© les r√©glages sur la pile et les h√¥tes <strong>2960X</strong> , nous red√©marrons le r√©seau sur les h√¥tes et v√©rifions l'interface logique. </p><br><pre> <code class="plaintext hljs">systemctl restart network cat /proc/net/bonding/bond1 2960X#show lacp internal 2960X#sh etherchannel summary</code> </pre> <br><p>  Nous configurons l'interface r√©seau de contr√¥le sur chaque h√¥te du <strong>VLAN 17</strong> et la lions √† l'interface logique BOND1: </p><br><div class="spoiler">  <b class="spoiler_title">Configuration de VLAN17 sur Host1:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /etc/sysconfig/network-scripts/ifcfg-bond1.17 DEVICE=bond1.17 NAME=bond1-vlan17 BOOTPROTO=none ONBOOT=yes USERCTL=no NM_CONTROLLED=no VLAN=yes MTU=1500 IPV4_FAILURE_FATAL=yes IPV6INIT=no IPADDR=172.20.1.163 NETMASK=255.255.255.0 GATEWAY=172.20.1.2 DEFROUTE=yes DNS1=172.20.1.8 DNS2=172.20.1.9 ZONE=public</code> </pre> </div></div><br><div class="spoiler">  <b class="spoiler_title">Configuration de VLAN17 sur Host2:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /etc/sysconfig/network-scripts/ifcfg-bond1.17 DEVICE=bond1.17 NAME=bond1-vlan17 BOOTPROTO=none ONBOOT=yes USERCTL=no NM_CONTROLLED=no VLAN=yes MTU=1500 IPV4_FAILURE_FATAL=yes IPV6INIT=no IPADDR=172.20.1.164 NETMASK=255.255.255.0 GATEWAY=172.20.1.2 DEFROUTE=yes DNS1=172.20.1.8 DNS2=172.20.1.9 ZONE=public</code> </pre> </div></div><br><p>  Nous red√©marrons le r√©seau sur les h√¥tes et v√©rifions leur visibilit√© entre eux. </p><br><p>  Ceci termine la configuration de la pile de commutateurs Cisco 2960X, et si tout a √©t√© fait correctement, nous avons maintenant une connexion r√©seau de tous les √©l√©ments d'infrastructure les uns aux autres au niveau L2. </p><br><h3 id="nastroyka-shd-dell-md3820f">  Configurer le stockage Dell MD3820f </h3><br><p>  Avant de commencer les travaux de configuration du syst√®me de stockage, il doit d√©j√† √™tre connect√© √† la pile de commutateurs Cisco <strong>2960X</strong> par les interfaces de gestion, ainsi qu'aux <strong>h√¥tes</strong> <strong>Host1</strong> et <strong>Host2</strong> via FC. </p><br><p>  Le sch√©ma g√©n√©ral de la fa√ßon dont le stockage doit √™tre connect√© √† la pile de commutateurs a √©t√© donn√© dans le chapitre pr√©c√©dent. </p><br><p>  Le sch√©ma de connexion du stockage sur FC aux h√¥tes devrait ressembler √† ceci: </p><br><p><img src="https://habrastorage.org/webt/el/k8/5a/elk85aqc6ilmxeiowmh6acrkwri.jpeg"></p><br><p>  Pendant la connexion, il est n√©cessaire d'enregistrer les adresses WWPN pour les h√¥tes HBA FC connect√©s aux ports FC du syst√®me de stockage - cela sera n√©cessaire pour la configuration ult√©rieure de la liaison des h√¥tes aux LUN sur le syst√®me de stockage. </p><br><p>  Sur le poste de travail de l'administrateur, t√©l√©chargez et installez l'utilitaire de gestion du stockage Dell MD3820f - <strong>PowerVault Modular Disk Storage Manager</strong> ( <strong>MDSM</strong> ). <br>  Nous nous y connectons via ses adresses IP par d√©faut, puis configurons nos adresses √† partir de <strong>VLAN17</strong> pour contr√¥ler les contr√¥leurs via TCP / IP: </p><br><p>  <strong>Stockage1</strong> : </p><br><pre> <code class="plaintext hljs">ControllerA IP - 172.20.1.13, MASK - 255.255.255.0, Gateway - 172.20.1.2 ControllerB IP - 172.20.1.14, MASK - 255.255.255.0, Gateway - 172.20.1.2</code> </pre> <br><p>  Apr√®s avoir d√©fini les adresses, acc√©dez √† l'interface de gestion du stockage et d√©finissez un mot de passe, d√©finissez l'heure, mettez √† jour le micrologiciel des contr√¥leurs et des disques, si n√©cessaire, etc. <br>  La proc√©dure √† suivre est d√©crite dans <a href="https://www.dell.com/support/manuals/ru/ru/rubsdc/powervault-md3820f/mdseriesagpub/introduction%3Fguid%3Dguid-51208376-0df9-48a8-be66-63c1b06512ae%26lang%3Den-us">le guide d'administration du</a> stockage. </p><br><p>  Apr√®s avoir termin√© les param√®tres ci-dessus, nous devrons effectuer quelques actions: </p><br><ol><li>  Configurez les <strong>identificateurs</strong> FC <strong>h√¥te</strong> . </li><li>  Cr√©ez un groupe d'h√¥tes - Groupe d' <strong>h√¥tes</strong> et ajoutez-y nos deux h√¥tes Dell. </li><li>  Cr√©ez-y un groupe de disques et des disques virtuels (ou LUN) qui seront pr√©sent√©s aux h√¥tes. </li><li>  Configurez la pr√©sentation des disques virtuels (ou LUN) pour les h√¥tes. </li></ol><br><p>  L'ajout de nouveaux h√¥tes et la liaison des identifiants des ports FC h√¥tes se fait via le menu - <strong>Mappages d'h√¥tes</strong> -&gt; <strong>D√©finir</strong> -&gt; <strong>H√¥tes ...</strong> <br>  Les adresses WWPN des h√¥tes FC HBA peuvent √™tre trouv√©es, par exemple, sur un serveur iDRAC. </p><br><p>  En cons√©quence, nous devrions obtenir quelque chose comme ceci: </p><br><p><img src="https://habrastorage.org/webt/p_/uk/u4/p_uku4o-ewgqpyi0xudxqjesfjc.png"></p><br><p>  L'ajout d'un nouveau groupe d'h√¥tes et la liaison d'h√¥tes √† celui-ci se fait via le menu - <strong>Host Mappings</strong> -&gt; <strong>Define</strong> -&gt; <strong>Host Group ...</strong> <br>  Pour les h√¥tes, s√©lectionnez le type de syst√®me d'exploitation - <strong><em>Linux (DM-MP)</em></strong> . </p><br><p>  Apr√®s avoir cr√©√© le groupe d'h√¥tes, via l'onglet <strong>Storage &amp; Copy Services</strong> , cr√©ez un groupe de <strong>disques</strong> - <strong>Disk Group</strong> , dont le type d√©pend des exigences de tol√©rance aux pannes, par exemple, RAID10, et des disques virtuels de la bonne taille: </p><br><p><img src="https://habrastorage.org/webt/ue/g2/kn/ueg2kn0m3usv1kb4tygvx2vajz0.png"></p><br><p>  Et enfin, la derni√®re √©tape est la pr√©sentation des disques virtuels (ou LUN) pour les h√¥tes. <br>  Pour ce faire, via le menu - <strong>Mappages d'h√¥tes</strong> -&gt; <strong>Mappage</strong> <strong>lunaire</strong> -&gt; <strong>Ajouter ...</strong> nous faisons la liaison des disques virtuels aux h√¥tes, en leur attribuant des num√©ros. </p><br><p>  Tout devrait se passer comme dans cette capture d'√©cran: </p><br><p><img src="https://habrastorage.org/webt/db/a-/xq/dba-xqbacgrjxrutupvhvmua7ro.png"></p><br><p>  Nous en avons termin√© avec la configuration des syst√®mes de stockage, et si tout a √©t√© fait correctement, les h√¥tes devraient voir les LUN qui leur sont pr√©sent√©s via leurs HBA FC. <br>  Demandez au syst√®me de mettre √† jour les informations sur les lecteurs mapp√©s: </p><br><pre> <code class="plaintext hljs">ls -la /sys/class/scsi_host/ echo "- - -" &gt; /sys/class/scsi_host/host[0-9]/scan</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Voyons quels appareils sont visibles sur nos serveurs:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /proc/scsi/scsi Attached devices: Host: scsi0 Channel: 02 Id: 00 Lun: 00 Vendor: DELL Model: PERC H330 Mini Rev: 4.29 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 00 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 01 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 04 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 11 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 31 Vendor: DELL Model: Universal Xport Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 00 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 01 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 04 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 11 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 31 Vendor: DELL Model: Universal Xport Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 lsscsi [0:2:0:0] disk DELL PERC H330 Mini 4.29 /dev/sda [15:0:0:0] disk DELL MD38xxf 0825 - [15:0:0:1] disk DELL MD38xxf 0825 /dev/sdb [15:0:0:4] disk DELL MD38xxf 0825 /dev/sdc [15:0:0:11] disk DELL MD38xxf 0825 /dev/sdd [15:0:0:31] disk DELL Universal Xport 0825 - [18:0:0:0] disk DELL MD38xxf 0825 - [18:0:0:1] disk DELL MD38xxf 0825 /dev/sdi [18:0:0:4] disk DELL MD38xxf 0825 /dev/sdj [18:0:0:11] disk DELL MD38xxf 0825 /dev/sdk [18:0:0:31] disk DELL Universal Xport 0825 -</code> </pre> </div></div><br><p>  Vous pouvez √©galement configurer le <strong>multichemin</strong> sur les h√¥tes, et bien que lors de l'installation de oVirt, il puisse le faire lui-m√™me, il est pr√©f√©rable de v√©rifier le MP par vous-m√™me √† l'avance. </p><br><p>  <u>Installer et configurer DM Multipath</u> </p><br><pre> <code class="plaintext hljs">yum install device-mapper-multipath mpathconf --enable --user_friendly_names y cat /etc/multipath.conf | egrep -v "^\s*(#|$)" defaults { user_friendly_names yes find_multipaths yes } blacklist { wwid 26353900f02796769 devnode "^(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]*" devnode "^hd[az]" }</code> </pre> <br><p>  Installez le service MP en d√©marrage automatique et ex√©cutez-le: </p><br><pre> <code class="plaintext hljs">systemctl enable multipathd &amp;&amp; systemctl restart multipathd</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">V√©rification des informations sur les modules charg√©s pour le fonctionnement MP:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">lsmod | grep dm_multipath dm_multipath 27792 6 dm_service_time dm_mod 124407 139 dm_multipath,dm_log,dm_mirror modinfo dm_multipath filename: /lib/modules/3.10.0-957.12.2.el7.x86_64/kernel/drivers/md/dm-multipath.ko.xz license: GPL author: Sistina Software &lt;dm-devel@redhat.com&gt; description: device-mapper multipath target retpoline: Y rhelversion: 7.6 srcversion: 985A03DCAF053D4910E53EE depends: dm-mod intree: Y vermagic: 3.10.0-957.12.2.el7.x86_64 SMP mod_unload modversions signer: CentOS Linux kernel signing key sig_key: A3:2D:39:46:F2:D3:58:EA:52:30:1F:63:37:8A:37:A5:54:03:00:45 sig_hashalgo: sha256</code> </pre> </div></div><br><p>  Voir le r√©sum√© de la configuration multichemin existante: </p><br><pre> <code class="plaintext hljs">mpathconf multipath is enabled find_multipaths is disabled user_friendly_names is disabled dm_multipath module is loaded multipathd is running</code> </pre> <br><p>  Apr√®s avoir ajout√© un nouveau LUN au stockage et l'avoir pr√©sent√© √† l'h√¥te, il est n√©cessaire de le scanner connect√© √† l'h√¥te HBA. </p><br><pre> <code class="plaintext hljs">systemctl reload multipathd multipath -v2</code> </pre> <br><p>  Et enfin, nous v√©rifions si tous les LUN ont √©t√© pr√©sent√©s sur le stockage pour les h√¥tes, et si tous ont deux chemins. </p><br><div class="spoiler">  <b class="spoiler_title">V√©rifier le fonctionnement du MP:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">multipath -ll 3600a098000e4b4b3000003175cec1840 dm-2 DELL ,MD38xxf size=2.0T features='3 queue_if_no_path pg_init_retries 50' hwhandler='1 rdac' wp=rw |-+- policy='service-time 0' prio=14 status=active | `- 15:0:0:1 sdb 8:16 active ready running `-+- policy='service-time 0' prio=9 status=enabled `- 18:0:0:1 sdi 8:128 active ready running 3600a098000e4b48f000002ab5cec1921 dm-6 DELL ,MD38xxf size=10T features='3 queue_if_no_path pg_init_retries 50' hwhandler='1 rdac' wp=rw |-+- policy='service-time 0' prio=14 status=active | `- 18:0:0:11 sdk 8:160 active ready running `-+- policy='service-time 0' prio=9 status=enabled `- 15:0:0:11 sdd 8:48 active ready running 3600a098000e4b4b3000003c95d171065 dm-3 DELL ,MD38xxf size=150G features='3 queue_if_no_path pg_init_retries 50' hwhandler='1 rdac' wp=rw |-+- policy='service-time 0' prio=14 status=active | `- 15:0:0:4 sdc 8:32 active ready running `-+- policy='service-time 0' prio=9 status=enabled `- 18:0:0:4 sdj 8:144 active ready running</code> </pre> </div></div><br><p>  Comme vous pouvez le voir, les trois disques virtuels du syst√®me de stockage sont visibles de deux mani√®res.  Ainsi, tous les travaux pr√©paratoires ont √©t√© achev√©s, ce qui signifie que nous pouvons passer √† la partie principale - la mise en place du cluster oVirt, qui sera discut√©e dans le prochain article. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr483980/">https://habr.com/ru/post/fr483980/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr483964/index.html">N'ayez pas peur de JSON ou de votre premi√®re application API</a></li>
<li><a href="../fr483972/index.html">Comment utiliser Quora pour promouvoir votre entreprise</a></li>
<li><a href="../fr483974/index.html">Ceph via iSCSI - ou skier debout dans un hamac</a></li>
<li><a href="../fr483976/index.html">Cybers√©curit√© et menaces 2020: ce qui nous attend apr√®s les vacances</a></li>
<li><a href="../fr483978/index.html">Comprendre le concept de d√©veloppement d'applications Web modernes en 2020</a></li>
<li><a href="../fr483986/index.html">Contr√¥le des pens√©es des robots avec Emotiv Insight</a></li>
<li><a href="../fr483988/index.html">MicroSPA, ou comment inventer une roue carr√©e</a></li>
<li><a href="../fr483992/index.html">Pourquoi certaines plan√®tes mangent leur ciel</a></li>
<li><a href="../fr483994/index.html">D√©m√©nagement informatique sur un yacht. De la Su√®de √† l'Espagne</a></li>
<li><a href="../fr484004/index.html">@Pythonetc d√©cembre 2019</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>