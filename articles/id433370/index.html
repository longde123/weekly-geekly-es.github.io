<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>💅🏼 🌄 🕦 Teori beling 👧🏻 ☝🏼 📽️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Tampaknya kita begitu terbenam di dalam hutan pembangunan yang sangat tinggi sehingga kita tidak memikirkan masalah mendasarnya. Ambil, misalnya, shar...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Teori beling</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/433370/">  Tampaknya kita begitu terbenam di dalam hutan pembangunan yang sangat tinggi sehingga kita tidak memikirkan masalah mendasarnya.  Ambil, misalnya, sharding.  Apa yang harus dipahami jika dimungkinkan untuk menulis pecahan bersyarat = n dalam pengaturan basis data dan semuanya akan dilakukan dengan sendirinya.  Begitulah, tetapi jika, ketika terjadi kesalahan, sumber daya mulai benar-benar langka, saya ingin memahami apa alasannya dan bagaimana cara memperbaikinya. <br><br>  Singkatnya, jika Anda berkontribusi implementasi hash alternatif Anda di Cassandra, maka hampir tidak ada wahyu untuk Anda.  Tetapi jika beban pada layanan Anda sudah tiba, dan pengetahuan sistem tidak mengikutinya, maka Anda dipersilakan.  <strong>Andrei Aksyonov</strong> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=" class="user_link">shodan</a> ) yang hebat dan mengerikan dengan cara yang biasa akan mengatakan bahwa <strong>sharding itu buruk, tidak sharding juga buruk</strong> , dan bagaimana hal itu diatur di dalam.  Dan secara tidak sengaja, salah satu bagian dari cerita tentang sharding sebenarnya bukan tentang sharding, tetapi iblis tahu tentang apa - bagaimana memetakan objek ke pecahan. <br><img src="https://habrastorage.org/webt/c9/ju/s6/c9jus6tadexnz4aih4q95bl7ega.jpeg"><br>  Foto anjing laut (walaupun secara tidak sengaja ternyata adalah anak anjing) sepertinya sudah menjawab pertanyaan mengapa ini semua, tapi mari kita mulai secara berurutan. <br><a name="habracut"></a><br><h2>  Apa itu sharding? <br></h2><br>  Jika Anda terus-menerus google, ternyata ada batas yang agak kabur antara yang disebut partisi dan yang disebut sharding.  Semua orang menyebut semua yang dia inginkan daripada yang dia inginkan.  Beberapa orang membedakan antara partisi horizontal dan sharding.  Yang lain mengatakan bahwa sharding adalah jenis partisi horizontal tertentu. <br><br>  Saya tidak menemukan standar terminologis tunggal yang akan disetujui oleh para pendiri dan disertifikasi dalam ISO.  Keyakinan batin pribadi adalah sesuatu seperti ini: <strong>Partisi</strong> rata-rata adalah "memotong pangkalan menjadi berkeping-keping" dengan cara yang sewenang-wenang. <br><br><ul><li>  Partisi <strong>vertikal</strong>  Misalnya, ada meja raksasa dengan beberapa miliar entri dalam 60 kolom.  Alih-alih memegang satu tabel raksasa seperti itu, kami menyimpan 60 tabel raksasa juga dengan masing-masing 2 miliar rekaman - dan ini bukan basis paruh waktu, tetapi partisi vertikal (sebagai contoh terminologi). <br></li><li>  Partisi <strong>horizontal</strong> - kami memotong baris demi baris, mungkin di dalam server. <br></li></ul><br>  Momen canggung di sini adalah perbedaan halus antara partisi horizontal dan sharding.  Anda dapat memotong saya berkeping-keping, tetapi saya tidak akan memberi tahu Anda dengan pasti apa itu terdiri.  Ada perasaan bahwa sharding dan partisi horizontal adalah tentang hal yang sama. <br><br>  Sharding secara umum ketika sebuah tabel besar dalam hal basis data atau kumpulan dokumen, objek, jika Anda tidak memiliki database, tetapi toko dokumen, dipotong khusus untuk objek.  Artinya, potongan dari 2 miliar objek dipilih, tidak peduli berapa ukurannya.  Objek dengan sendirinya di dalam setiap objek tidak dipotong-potong, kita tidak membusuk menjadi kolom terpisah, yaitu, kita meletakkan bundel di tempat yang berbeda. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/xx_Lv1P_X_I" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Tautkan</a> ke presentasi untuk kelengkapan.</i> <br><br>  Perbedaan terminologis yang halus telah berlangsung.  Sebagai contoh, secara relatif, pengembang Postgres dapat mengatakan bahwa partisi horizontal adalah ketika semua tabel di mana tabel utama dibagi berada dalam skema yang sama, dan ketika pada mesin yang berbeda itu sharding. <br><br>  Dalam arti umum, tanpa terikat dengan terminologi database tertentu dan sistem manajemen data tertentu, ada perasaan bahwa sharding hanya mengiris dokumen baris demi baris dan sebagainya - dan hanya itu: <br><br><blockquote>  Sharding (~ =, \ in ...) Horizontal Partitioning == adalah tipikal. <br></blockquote><br>  Saya menekankan, biasanya.  Dalam arti bahwa kita melakukan semua ini bukan hanya untuk memotong 2 miliar dokumen menjadi 20 tabel, yang masing-masing akan lebih mudah dikelola, tetapi untuk mendistribusikannya ke banyak core, banyak disk atau banyak server fisik atau virtual yang berbeda . <br><br>  Dipahami bahwa kita melakukan ini sehingga setiap beling - setiap data shatka - direplikasi berkali-kali.  Tapi sebenarnya tidak. <br><br><pre><code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> docs00 <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> documents <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">id</span></span>%<span class="hljs-number"><span class="hljs-number">16</span></span>)=<span class="hljs-number"><span class="hljs-number">0</span></span> ... <span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> docs15 <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> documents <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">id</span></span>%<span class="hljs-number"><span class="hljs-number">16</span></span>)=<span class="hljs-number"><span class="hljs-number">15</span></span></code> </pre> <br>  Faktanya, jika Anda membuat irisan data seperti itu, dan dari satu tabel SQL raksasa di MySQL, Anda akan menghasilkan 16 tabel kecil pada laptop Anda yang gagah berani, tanpa melampaui satu laptop, bukan skema tunggal, bukan database tunggal, dll.  dll.  - Segalanya, Anda sudah memiliki pecahan. <br><br>  Mengingat ilustrasi dengan anak-anak anjing, ini mengarah pada yang berikut: <br><br><ul><li>  Bandwidth meningkat. <br></li><li>  Latensi tidak berubah, yaitu masing-masing, sehingga dapat dikatakan, pekerja atau konsumen dalam hal ini, mendapatkan miliknya sendiri.  Tidak diketahui apa yang didapat anak anjing dalam gambar, tetapi permintaan disajikan dalam waktu yang hampir bersamaan, seolah-olah anak anjing itu sendirian. </li><li>  Atau keduanya itu, dan yang lain, dan ketersediaan masih tinggi (replikasi). <br></li></ul><br>  <strong>Mengapa bandwidth?</strong>  Kadang-kadang kita mungkin memiliki volume data yang tidak sesuai - tidak jelas di mana, tetapi tidak cocok - oleh 1 {core |  drive |  server |  ...}.  Hanya sumber daya tidak cukup dan hanya itu.  Agar dapat bekerja dengan dataset besar ini, Anda perlu memotongnya. <br><br>  <strong>Mengapa latensi?</strong>  Pada satu inti, pemindaian tabel 2 miliar baris 20 kali lebih lambat dari pemindaian 20 tabel pada 20 kernel, melakukan hal ini secara paralel.  Data sedang diproses terlalu lambat pada satu sumber daya. <br><br>  <strong>Mengapa ketersediaan tinggi?</strong>  Atau kami memotong data untuk melakukan satu dan yang lain pada saat yang sama, dan pada saat yang sama beberapa salinan dari setiap pecahan - replikasi menyediakan ketersediaan tinggi. <br><br><h2>  Contoh sederhana "bagaimana melakukannya dengan tangan Anda" <br></h2><br>  Sharding bersyarat dapat dipotong menggunakan tabel tes test.dokumen untuk 32 dokumen, dan dengan menghasilkan dari tabel ini 16 tabel uji untuk sekitar 2 dokumen test.docs00, 01, 02, ..., 15 masing-masing. <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> docs00 <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> documents <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">id</span></span>%<span class="hljs-number"><span class="hljs-number">16</span></span>)=<span class="hljs-number"><span class="hljs-number">0</span></span> ... <span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> docs15 <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> documents <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">id</span></span>%<span class="hljs-number"><span class="hljs-number">16</span></span>)=<span class="hljs-number"><span class="hljs-number">15</span></span></code> </pre><br>  Mengapa tentang?  Karena apriori kita tidak tahu bagaimana id didistribusikan, jika dari 1 hingga 32 inklusif, maka masing-masing akan ada 2 dokumen, jika tidak. <br><br>  <strong>Kami melakukan ini untuk apa.</strong>  Setelah kita melakukan 16 tabel, kita dapat "menangkap" 16 dari apa yang kita butuhkan.  Terlepas dari apa yang kita sandarkan, kita dapat memparalelkan sumber daya ini.  Misalnya, jika tidak ada cukup ruang disk, masuk akal untuk menguraikan tabel ini menjadi disk terpisah. <br><br>  Semua ini, sayangnya, tidak gratis.  Saya menduga bahwa dalam kasus standar SQL kanonik (saya belum membaca ulang standar SQL untuk waktu yang lama, mungkin belum diperbarui untuk waktu yang lama), tidak ada sintaks standar resmi untuk mengatakan ke server SQL mana pun: “Server SQL yang terhormat, buatkan saya 32 pecahan dan menaruhnya di 4 disc. "  Tetapi dalam implementasi individu, sering ada sintaks tertentu untuk melakukan hal yang sama secara prinsip.  PostgreSQL memiliki mekanisme untuk mempartisi, MySQL MariaDB memilikinya, Oracle mungkin telah melakukan semua ini sejak lama. <br><br>  Namun demikian, jika kita melakukan ini dengan tangan, tanpa dukungan basis data dan dalam kerangka kerja standar, maka kita <strong>membayar secara kondisional kompleksitas akses ke data</strong> .  Di mana ada SELECT * FROM dokumen WHERE sederhana id = 123, sekarang 16 x SELECT * FROM docsXX.  Dan yah, jika kami mencoba untuk mendapatkan catatan dengan kunci.  Secara signifikan lebih menarik jika kami mencoba untuk mendapatkan rentang catatan awal.  Sekarang (jika, saya tekankan, seolah-olah bodoh, dan tetap dalam standar), hasil dari 16 SELECT * FROM ini harus digabungkan dalam aplikasi. <br><br>  <strong>Perubahan kinerja apa yang diharapkan?</strong> <br><br><ul><li>  Linier yang intuitif. </li><li>  Secara teoritis - sublinear, karena <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">hukum Amdahl</a> . </li><li>  Dalam praktiknya - mungkin hampir linear, mungkin tidak. </li></ul><br>  Faktanya, jawaban yang benar tidak diketahui.  Dengan aplikasi cerdas dari teknik sharding, Anda dapat mencapai kemunduran super-linier yang signifikan dalam pengoperasian aplikasi Anda, dan bahkan DBA akan berjalan dengan poker panas. <br><br>  Mari kita lihat bagaimana ini bisa dicapai.  Jelas bahwa hanya pengaturan pengaturan ke PostgreSQL beling = 16, dan kemudian lepas sendiri - ini tidak menarik.  Mari kita pikirkan tentang bagaimana kita dapat mencapai hal itu sehingga <em>kita akan melambat menjadi 32 kali dari sharding</em> , yang menarik dari sudut pandang bagaimana tidak melakukan ini. <br><br>  Upaya kami untuk mempercepat atau memperlambat akan selalu bertumpu pada klasik - hukum Amdahl lama yang baik, yang mengatakan bahwa tidak ada paralelisasi sempurna dari permintaan apa pun, selalu ada beberapa bagian yang konsisten. <br><br><h2>  Hukum Amdahl <br></h2><br><blockquote>  <strong><em>Selalu</em></strong> ada bagian serial. <br></blockquote><br>  Selalu ada bagian dari eksekusi permintaan yang paralel, dan selalu ada bagian yang tidak paralel.  Bahkan jika menurut Anda itu adalah kueri paralel sempurna, setidaknya mengumpulkan deretan hasil yang akan Anda kirim ke klien, dari baris yang diterima dari setiap beling, selalu ada, dan selalu konsisten. <br><br>  Selalu ada semacam bagian berurutan.  Ini bisa sangat kecil, benar-benar tidak terlihat dengan latar belakang umum, itu bisa sangat besar dan, karenanya, sangat mempengaruhi paralelisasi, tetapi selalu ada. <br><br>  Selain itu, pengaruhnya <strong><em>berubah</em></strong> dan dapat tumbuh secara signifikan, misalnya, jika kita memotong tabel kita - mari kita tingkatkan - dari 64 catatan menjadi 16 tabel dari 4 catatan, bagian ini akan berubah.  Tentu saja, dilihat dari jumlah data sebesar itu, kami bekerja pada ponsel dan prosesor 86 MHz, kami tidak memiliki cukup file yang dapat tetap dibuka pada saat bersamaan.  Rupanya, dengan input seperti itu, kami membuka satu file sekaligus. <br><br><ul><li>  Itu <strong>Total =</strong> <strong>Serial +</strong> <strong>Paralel</strong> .  Di mana, misalnya, paralel adalah semua pekerjaan di dalam DB, dan serial mengirim hasilnya ke klien. <br></li><li>  Itu menjadi <strong>Total2 = Serial + Parallel / N + Xserial.</strong>  Misalnya, ketika ORDER BY umum, Xserial&gt; 0. <br></li></ul><br>  Dengan contoh sederhana ini, saya mencoba menunjukkan bahwa beberapa Xserial muncul.  Selain fakta bahwa selalu ada bagian serial, dan fakta bahwa kami mencoba untuk bekerja dengan data secara paralel, bagian tambahan muncul untuk memastikan data ini diiris.  Secara kasar, kita mungkin perlu: <br><br><ul><li>  temukan 16 tabel ini di kamus basis data internal; </li><li>  buka file; </li><li>  mengalokasikan memori; </li><li>  pindah memori; </li><li>  noda hasil; </li><li>  sinkronisasi antara inti; </li></ul><br>  Efek tidak sinkron selalu muncul.  Mereka mungkin tidak signifikan dan menempati sepersejuta dari total waktu, tetapi mereka selalu nol dan selalu ada.  Dengan bantuan mereka, kita dapat secara dramatis kehilangan produktivitas setelah sharding. <br><br><img src="https://habrastorage.org/webt/fh/mx/yh/fhmxyh9tozfrbd4yszxj2va9a1g.jpeg"><br><br>  Ini adalah gambar standar tentang hukum Amdahl.  Ini tidak mudah dibaca, tetapi penting bahwa garis-garis, yang idealnya harus lurus dan tumbuh secara linear, berbatasan dengan asimtot.  Tetapi karena grafik dari Internet tidak dapat dibaca, saya membuat, menurut pendapat saya, lebih banyak tabel visual dengan angka. <br><br>  Misalkan kita memiliki beberapa bagian serial dari pemrosesan permintaan, yang hanya membutuhkan 5%: <strong>serial = 0,05 = 1/20.</strong> <br><br>  Secara intuitif, akan terlihat bahwa dengan bagian serial, yang hanya membutuhkan 1/20 dari pemrosesan permintaan, jika kita memparalelkan pemrosesan permintaan menjadi 20 core, maka akan menjadi sekitar 20, dalam kasus terburuk, 18 kali lebih cepat. <br><br>  Sebenarnya, <b>matematika adalah hal yang tidak berperasaan</b> : <br><br> <code>wall = 0.05 + 0.95/num_cores, speedup = 1 / (0.05 + 0.95/num_cores)</code> <br> <br>  Ternyata jika Anda hati-hati menghitung, dengan bagian serial 5%, akselerasi akan 10 kali (10,3), dan ini adalah 51% dibandingkan dengan ideal teoretis. <br><br><table><tbody><tr><td>  8 core </td><td>  = 5.9 </td><td>  <font color="#c45911">= 74%</font> </td></tr><tr><td>  10 core </td><td>  = 6.9 </td><td>  <font color="#c45911">= 69%</font> </td></tr><tr><td>  <strong>20 core</strong> </td><td>  <strong>= 10.3</strong> </td><td>  <strong><font color="#c45911">= 51%</font></strong> </td></tr><tr><td>  40 core </td><td>  = 13.6 </td><td>  <font color="#ff0000">= 34%</font> </td></tr><tr><td>  128 core </td><td>  = 17,4 </td><td>  <font color="#ff0000">= 14%</font> </td></tr></tbody></table><br>  Menggunakan 20 core (20 disk, jika Anda suka) untuk tugas yang telah dikerjakan sebelumnya, kami secara teoritis tidak akan pernah mendapatkan akselerasi lebih dari 20 kali, tetapi secara praktis jauh lebih sedikit.  Selain itu, dengan peningkatan jumlah paralel, inefisiensi tumbuh dengan cepat. <br><br>  Ketika hanya 1% dari pekerjaan berseri yang tersisa, dan 99% diparalelkan, nilai akselerasi agak ditingkatkan: <br><br><table><tbody><tr><td>  8 core </td><td>  = 7,5 </td><td>  <font color="#538135">= 93%</font> </td></tr><tr><td>  16 core </td><td>  = 13.9 </td><td>  <font color="#538135">= 87%</font> </td></tr><tr><td>  32 core </td><td>  = 24,4 </td><td>  <font color="#c45911">= 76%</font> </td></tr><tr><td>  64 core </td><td>  = 39,3 </td><td>  <font color="#c45911">= 61%</font> </td></tr></tbody></table><br>  Untuk permintaan yang sepenuhnya termonuklir, yang secara alami berjalan selama berjam-jam, dan pekerjaan persiapan dan merakit hasilnya membutuhkan waktu sangat sedikit (serial = 0,001), kita akan melihat efisiensi yang baik: <br><br><table><tbody><tr><td>  8 core </td><td>  = 7.94 </td><td>  <font color="#538135">= 99%</font> </td></tr><tr><td>  16 core </td><td>  = 15,76 </td><td>  <font color="#538135">= 99%</font> </td></tr><tr><td>  32 core </td><td>  = 31,04 </td><td>  <font color="#538135">= 97%</font> </td></tr><tr><td>  64 core </td><td>  = 60,20 </td><td>  <font color="#538135">= 94%</font> </td></tr></tbody></table><br>  Harap dicatat bahwa <strong>kami tidak akan pernah melihat 100%</strong> .  Dalam kasus yang sangat baik, Anda dapat melihat, misalnya, 99,999%, tetapi tidak persis 100%. <br><br><h2>  Bagaimana cara mengocok dan memecahkan dalam N kali? <br></h2><br>  Anda dapat mengocok dan membobol tepat N kali: <br><br><ol><li>  Kirim docs00 ... permintaan docs15 <strong>secara berurutan</strong> , tidak paralel. </li><li>  Dalam pertanyaan sederhana, jangan pilih <strong>dengan kunci</strong> , MANA sesuatu = 234. </li></ol><br>  Dalam hal ini, bagian berseri (serial) menempati tidak 1% dan bukan 5%, tetapi sekitar 20% dalam database modern.  Anda bisa mendapatkan 50% dari bagian serial jika Anda mengakses database menggunakan protokol biner yang sangat efisien atau menautkannya sebagai pustaka dinamis ke skrip Python. <br><br>  Sisa waktu pemrosesan untuk permintaan sederhana akan ditempati oleh operasi parsing permintaan yang tidak diparalelkan, persiapan rencana, dll.  Artinya, melambat tidak membaca catatan. <br><br>  Jika kita memecah data menjadi 16 tabel dan menjalankannya secara berurutan, seperti kebiasaan dalam bahasa pemrograman PHP, misalnya, (ia tidak tahu cara menjalankan proses asinkron dengan sangat baik), maka kami hanya mendapatkan penurunan 16 kali.  Dan, mungkin, bahkan lebih, karena perjalanan pulang-pergi jaringan juga akan ditambahkan. <br><br><blockquote>  Tiba-tiba saat sharding, pilihan bahasa pemrograman menjadi penting. <br></blockquote><br>  Kami ingat tentang pilihan bahasa pemrograman, karena jika Anda mengirim permintaan ke database (atau server pencarian) secara berurutan, lalu dari mana akselerasi itu berasal?  Melainkan, pelambatan akan muncul. <br><br><h3>  Sepeda dari kehidupan <br></h3><br>  Jika Anda memilih C ++, <strong>tulis ke Thread POSIX</strong> , bukan Boost I / O.  Saya melihat perpustakaan yang sangat baik dari pengembang berpengalaman dari Oracle dan MySQL sendiri, yang menulis komunikasi dengan server MySQL di Boost.  Rupanya, mereka dipaksa untuk menulis dalam bahasa C murni di tempat kerja, tetapi kemudian mereka berhasil berbalik, mengambil Boost dengan I / O yang tidak sinkron, dll.  Satu masalah - I / O asinkron ini, yang secara teoritis seharusnya mendorong 10 permintaan secara paralel, karena beberapa alasan memiliki titik sinkronisasi yang tidak terlihat di dalam.  Ketika memulai 10 permintaan secara paralel, mereka dieksekusi tepat 20 kali lebih lambat dari satu, karena 10 kali untuk permintaan itu sendiri dan satu kali ke titik sinkronisasi. <br><br>  <strong>Kesimpulan:</strong> tulis dalam bahasa yang mengimplementasikan running paralel dan menunggu permintaan berbeda dengan baik.  Saya tidak tahu, jujur ​​saja, apa sebenarnya yang disarankan selain Go.  Bukan hanya karena saya benar-benar mencintai Go, tetapi karena saya tidak tahu apa pun yang lebih cocok. <br><br>  <strong>Jangan menulis dalam bahasa yang tidak cocok</strong> di mana Anda tidak dapat menjalankan 20 pertanyaan paralel ke database.  Atau di setiap kesempatan, jangan lakukan semuanya dengan tangan Anda - pahami cara kerjanya, tapi jangan lakukan secara manual. <br><br><h2>  Sepeda uji A / B <br></h2><br>  Kadang-kadang Anda dapat memperlambat karena Anda terbiasa dengan fakta bahwa semuanya bekerja, dan Anda tidak memperhatikan bahwa bagian bersambung, pertama, adalah yang besar. <br><br><ul><li>  Segera ~ 60 pecahan indeks pencarian, kategori </li><li>  Ini adalah pecahan yang benar dan benar, di bawah area subjek. </li><li>  Ada hingga 1000 dokumen, dan ada 50.000 dokumen. </li></ul><br>  Ini adalah sepeda produksi, ketika permintaan pencarian sedikit berubah dan mereka mulai memilih lebih banyak dokumen dari 60 pecahan indeks pencarian.  Semuanya bekerja dengan cepat dan berdasarkan prinsip: "Berhasil - jangan menyentuhnya", mereka semua lupa, yang sebenarnya ada di dalam 60 pecahan.  Kami meningkatkan batas pengambilan sampel untuk setiap pecahan dari seribu hingga 50 ribu dokumen.  Tiba-tiba, itu mulai melambat dan paralelisme berhenti.  Permintaan sendiri, yang dieksekusi sesuai dengan pecahan, terbang cukup baik, dan panggung diperlambat, ketika 50 ribu dokumen dikumpulkan dari 60 pecahan.  3 juta dokumen akhir ini pada satu inti digabung bersama, disortir, bagian atas 3 juta dipilih dan diberikan kepada klien.  Bagian serial yang sama melambat, hukum Amdal yang sama kejamnya bekerja. <br><br>  <em>Jadi mungkin Anda sebaiknya tidak melakukan sharding dengan tangan Anda, tetapi hanya secara manusiawi</em> <em><br></em>  <em>beri tahu database: "Lakukan!"</em> <em><br></em> <br>  <strong>Penafian:</strong> Saya tidak benar-benar tahu bagaimana melakukan sesuatu dengan benar.  Saya seperti dari lantai yang salah !!! <br><br>  Saya telah mempromosikan agama yang disebut "fundamentalisme algoritmik" di sepanjang kehidupan sadar saya.  Ini dirumuskan secara singkat dengan sangat sederhana: <br><br><blockquote>  Anda tidak benar-benar ingin melakukan apa pun dengan tangan Anda, tetapi sangat berguna untuk mengetahui bagaimana itu diatur di dalam.  Sehingga pada saat ketika terjadi kesalahan dalam database, Anda setidaknya mengerti apa yang salah di sana, bagaimana hal itu diatur di dalam dan kira-kira bagaimana hal itu dapat diperbaiki. <br></blockquote><br>  Mari kita lihat opsinya: <br><br><ol><li>  <strong>"Tangan</strong> . <strong>"</strong>  Sebelumnya, kami membagi data secara manual menjadi 16 tabel virtual, dan menulis ulang semua pertanyaan dengan tangan kami - ini sangat tidak nyaman.  <strong>Jika ada kesempatan untuk tidak mengocok tangan - jangan mengocok tangan!</strong>  Tetapi kadang-kadang ini tidak mungkin, misalnya, Anda memiliki MySQL 3.23, dan kemudian Anda harus melakukannya. </li><li>  <strong>"Otomatis".</strong>  Kebetulan Anda dapat mengocok secara otomatis atau hampir secara otomatis, ketika database dapat mendistribusikan data itu sendiri, Anda hanya perlu menulis secara kasar di suatu tempat dengan pengaturan tertentu.  Ada banyak pangkalan, dan mereka memiliki banyak pengaturan yang berbeda.  Saya yakin bahwa di setiap basis data di mana dimungkinkan untuk menulis pecahan = 16 (apa pun sintaksisnya), banyak pengaturan lain terpaku pada case ini oleh mesin. </li><li>  <strong>"Semi-otomatis"</strong> - mode yang sepenuhnya kosmik, menurut saya, dan brutal.  Artinya, basis itu sendiri tampaknya tidak mampu, tetapi ada tambalan tambahan eksternal. </li></ol><br>  Sulit untuk mengatakan sesuatu tentang mesin, kecuali mengirimnya ke dokumentasi pada database yang sesuai (MongoDB, Elastic, Cassandra, ... secara umum, yang disebut NoSQL).  Jika Anda beruntung, maka Anda cukup menarik sakelar “make me 16 shards” dan semuanya akan berhasil.  Pada saat itu, ketika itu tidak berhasil, sisa artikel mungkin diperlukan. <br><br><h2>  Tentang perangkat semi otomatis <br></h2><br>  Di beberapa tempat, teknologi informasi canggih menginspirasi horor chthonic.  Sebagai contoh, MySQL out of the box tidak memiliki implementasi sharding ke versi tertentu untuk memastikan, namun, ukuran pangkalan yang dioperasikan dalam pertempuran tumbuh menjadi nilai tidak senonoh. <br><br>  Menderita kemanusiaan di hadapan masing-masing DBA telah disiksa selama bertahun-tahun dan menulis beberapa solusi buruk yang dibuat tanpa alasan.  Setelah itu, satu atau lebih solusi sharding yang layak disebut ProxySQL (MariaDB / Spider, PG / pg_shard / Citus, ...).  Ini adalah contoh terkenal dari mantel yang sama ini. <br><br>  ProxySQL secara keseluruhan, tentu saja, adalah solusi kelas enterprise lengkap untuk open source, untuk routing dan banyak lagi.  Tetapi salah satu tugas yang harus dipecahkan adalah sharding untuk database, yang dengan sendirinya tidak tahu cara shard secara manusiawi.  Anda lihat, tidak ada saklar "beling = 16", baik Anda harus menulis ulang setiap permintaan dalam aplikasi, dan ada banyak dari mereka, atau meletakkan lapisan menengah antara aplikasi dan database yang terlihat: "Hmm ... PILIH * DARI dokumen?  Ya, itu harus dibagi menjadi 16 SELECT * FROM server1.document1 kecil, SELECT * FROM server2.document2 - ke server ini dengan nama pengguna / kata sandi, untuk ini dengan yang lain.  Jika seseorang tidak menjawab, maka ... "dll. <br><br>  Tepatnya ini bisa dilakukan dengan tambalan menengah.  Mereka sedikit kurang dari untuk semua database.  Untuk PostgreSQL, seperti yang saya pahami, pada saat yang sama ada beberapa solusi bawaan (PostgresForeign Data Wrappers, menurut pendapat saya, dibangun ke dalam PostgreSQL itu sendiri), ada tambalan eksternal. <br><br>  Mengkonfigurasi setiap tambalan spesifik adalah topik raksasa terpisah yang tidak akan cocok dalam satu laporan, jadi kami hanya akan membahas konsep dasar. <br><br>  Lebih baik kita bicara sedikit tentang teori buzz. <br><br><h2>  Otomatisasi sempurna mutlak? <br></h2><br>  Seluruh teori buzz dalam hal sharding dalam huruf F () ini, prinsip dasarnya <strong>selalu</strong> sama mentah: <code>shard_id = F(object).</code> <br><br>  Sharding umumnya tentang apa?  Kami memiliki 2 miliar catatan (atau 64).  Kami ingin membaginya menjadi beberapa bagian.  Pertanyaan tak terduga muncul - bagaimana?  Dengan prinsip apa saya harus menyebarkan 2 milyar catatan saya (atau 64) ke 16 server yang tersedia untuk saya? <br><br>  Ahli matematika laten dalam diri kita harus menyarankan bahwa pada akhirnya selalu ada fungsi sihir tertentu yang, untuk setiap dokumen (objek, garis, dll.), Akan menentukan bagian mana yang akan diletakkan. <br><br>  Jika kita masuk lebih dalam ke matematika, fungsi ini selalu tergantung tidak hanya pada objek itu sendiri (garis itu sendiri), tetapi juga pada pengaturan eksternal seperti jumlah total pecahan.  Fungsi, yang untuk setiap objek harus mengatakan di mana harus meletakkannya, tidak dapat mengembalikan nilai lebih dari server di sistem.  Dan fungsinya sedikit berbeda: <br><br><ul><li>  shard_func = <strong>F1</strong> (objek); <br></li><li>  shard_id = <strong>F2</strong> (shard_func, ...); </li><li>  shard_id = <strong>F2</strong> ( <strong>F1</strong> (objek), current_num_shards, ...). </li></ul><br>  Tetapi lebih jauh lagi kita tidak akan menggali ke dalam hutan fungsi individu ini, kita hanya berbicara tentang apa fungsi sihir F (). <br><br><h2>  Apa itu F ()? <br></h2><br>  Mereka dapat menghasilkan banyak mekanisme implementasi yang berbeda dan banyak berbeda.  Ringkasan sampel: <br><br><ul><li>  F = <strong>rand</strong> ()% nums_shards </li><li>  F = <strong>somehash</strong> (object.id)% num_shards </li><li>  F = object.date% num_shards </li><li>  F = object.user_id% num_shards </li><li>  ... </li><li>  F = shard_table [somehash () | ... object.date | ...] </li></ul><br>  Fakta menarik - Anda dapat secara alami menyebarkan semua data secara acak - kami melemparkan catatan berikutnya pada server yang arbitrer, pada kernel yang arbitrer, dalam tabel arbitrer.  Tidak akan ada banyak kebahagiaan dalam hal ini, tetapi itu akan berhasil. <br><br>  Ada metode scamming yang sedikit lebih cerdas untuk fungsi hash yang dapat direproduksi atau bahkan konsisten, atau scamming untuk beberapa atribut.  Mari kita pergi melalui setiap metode. <br><br><h3>  F = rand () <br></h3><br>  Menyebarkan sekitar bukanlah metode yang sangat benar.  Satu masalah: kami menyebarkan 2 miliar catatan kami per seribu server secara acak, dan kami tidak tahu di mana catatan itu berada.  Kami perlu menarik pengguna_1, tetapi kami tidak tahu di mana itu.  Kami pergi ke seribu server dan memilah-milah segalanya - entah bagaimana itu tidak efisien. <br><br><h3>  F = somehash () <br></h3><br>  Mari menyebarkan pengguna dengan cara dewasa: membaca fungsi hash yang direproduksi dari user_id, ambil sisa divisi dengan jumlah server dan akses server yang diinginkan segera. <br><br>  <em>Kenapa kita melakukan ini?</em>  <em>Lalu, kami memiliki beban tinggi dan kami tidak mendapatkan apa pun ke dalam satu server.</em>  <em>Jika disela, hidup akan sangat sederhana.</em> <br><br>  Nah, situasinya sudah membaik, untuk mendapatkan satu catatan, kami pergi ke satu server yang terkenal.  Tetapi jika kita memiliki rentang kunci, maka dalam semua rentang ini kita perlu memilah-milah semua nilai kunci dan dalam batasnya pergi ke banyak pecahan karena kita memiliki kunci dalam jangkauan, atau untuk setiap server secara umum.  Situasi, tentu saja, telah membaik, tetapi tidak untuk semua permintaan.  Beberapa permintaan telah terpengaruh. <br><br><h3>  Sharding alami (F = object.date% num_shards) <br></h3><br>  Kadang-kadang, itu sering kali, 95% dari lalu lintas dan 95% dari beban adalah permintaan yang memiliki semacam sharding alami. , 95%  -       1 , 3 , 7 ,   5%     .  95% ,  ,    ,        . <br><br>        , ,   ,         -           . <br><br>   —        ,      .       ,    , , ,    .        5 %  . <br><br>       ,    : <br><br><ol><li>      ,  95%     . </li><li>  95%    ,       ,     .   ,           .     ,     . </li></ol><br>  ,      —    ,         - . <br><br>   ,   ,         ,     ,         .       «   -      ». <br><br> <strong>     «».</strong> ,            . <br><br><h3> 1.  :   <br></h3><br>    ,      ,  . <br><br><ul><li>    ,   ! </li><li> <strong><em></em></strong>  () . </li></ul><br>   , /  , ,  , PM    (       ,  PM   ),     .     . <br><br>  ,    .      ,       ,    100   .        . <br><br>   ,  ,   ,            ,    - . <br><br><h3> 2. «» : , join <br></h3><br>   ,             ? <br><br><ul><li>  «» … WHERE randcol BETWEEN aaa AND bbb? <br></li><li>  «» … users_32shards JOIN posts_1024 shards? </li></ul><br>  : , ! <br><br>           ,    ,       ,           .      .       (, , document store    ),     ,     . <br><br>   — <strong>-       </strong> .     .  ,          .     ,       ,    ,   .       - , ,         ,   ,         —    . <br><br>       ,             . <br><br><h3> 3. / :  <br></h3><br> :         ,          . <br><br><blockquote>    ,   . <br></blockquote><br>      ,  , ,  .     ,     ,   ,    10 , -        30,       100   .    .          —       ,  -   —  , -  . <br><br> ,      :  16 -,  32. ,   17,  23 —    .      ,  ,    -  ? <br><br>  : ,    ,     . <br><br>  ,    «»,   « ». <br><br><h4>   #1.   <br></h4><br><ul><li>     NewF(object),    . </li><li>   NewF()=OldF() . </li><li>   <strong> .</strong> </li><li>  Aduh </li></ul><br>  ,    2       ,  ,  .   :  17 ,  6   ,  2  ,    17   23 .   10  , ,    .      . <br><br><h4>   #2.   <br></h4><br>    —       —  17    23,     16   32 !         ,        . <br><br><ul><li>     NewF(object),    . </li><li> <strong>  2^N,   2^(N+1) .</strong> </li><li>   NewF()=OldF()  0,5. </li><li>   50% . </li><li> ,   <strong>   .</strong> </li></ul><br>  ,  ,         .   ,   ,  . <br><br>  ,            .   ,  16     16,      —    . <br><br> ,        —     . <br><br><h4>  #3. Consistent hashing <br></h4><br> ,       consistent hashing <br><img src="https://habrastorage.org/webt/il/ml/rt/ilmlrt9xy-c3wuyfaafntagufay.jpeg"><br><br>   «consistent hashing»,    ,    . <br><br> :    ()   ,      .    ,     ,  ,      (  ,     ), . <br><br><ul><li>   :  <strong><em> </em></strong> ,   2 «»,    1/n. <br></li><li>   :    ,   .  . </li></ul><br>          ,         .  ,      ,      ,     :     ,          . <br><br>        .  ,        .  ,   ..,    .  ,   - , ,        . <br><br>       ,  , ,  Cassandra   .  ,         , ,      , ,  . <br><br>   ,        —     /    ,   ,    . <br><br> , :    ?       ? — ,  ! <br><br><h4>  #4. Rendezvous/HRW <br></h4><br>    (  ,   ): <strong>shard_id = arg max hash(object_id, shard_id).</strong> <br><br>    Rendezvous hashing,   ,  ,    Highest Random Weight.      : <br><img src="https://habrastorage.org/webt/0t/dt/rm/0tdtrm0iftxxb5ors5a2wxcex8s.jpeg"><br>   , , 16 .    (),   - ,  16 ,      .      -,   . <br><br>    HRW-hashing,   Rendezvous hashing.       , -,        ,   . <br><br>    ,       .  ,        - -        .      . <br><br>   ,       . <br><br><h4>  #5.   <br></h4><br> ,        Google    -   : <br><br><ul><li> Jump Hash — Google '2014. </li><li> Multi Probe —Google '2015. </li><li> Maglev — Google '2016. </li></ul><br>    ,    .      ,   ,    , -,       .      . <br><br><h4>  #6.  <br></h4><br>      —  .     ?   ,     2  ,          object_id  2  ,     . <br><br>  ,       ?    ? <br><br>     . ,   -     ,   ,  .  ,      , ,  ,     . <br><br> : <br><br><ul><li>  1  . </li><li>      /  /  /       : min/max_id =&gt; shard_id. </li><li>    8    4    (4      !) —  20    . </li><li>      -   ,        20  —     . </li><li> 20  —                 . </li></ul><br>     2     -    16  —   100   -   .       : ,         ,   —  1 .     ,  ,   . <br><br> ,    ,     ,    - ,     . <br><br><h1>  Kesimpulan <br></h1><br>            : «  ,   !».       ,     20 . <br><br>   ,   ,     .   ,  <strong>   </strong> —   .     100$        ,     .          -,    .     —   . <br><br> <strong>    </strong> , ,  «» (, DFS, ...)   .   ,   , highload   -   .  ,        ,     - .     — <strong> ,    </strong> . <br><br>     <strong> </strong> <strong>F()</strong> ,   , ,  ..  , ,    2   <strong>     </strong> . <br><br><h2>   <br></h2><br> ,      ,        .       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> HighLoad++</a> ,  ,     —Sphinx—highload  ,   . <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/qpGljUyIht8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h2>    <br></h2><br>         Highload User Group.  ,    . <br><br>  , ,     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">HighLoad++</a>     .         , ,  .  ,            , .     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> </a>   highload-,   . <br><br>        ,  ,     ,  . ,           , ,        . <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> </a> <strong>24   -</strong>      «», « ».  ,        .      ,     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="></a> . <br><br><blockquote>         , ,  <strong>8  9   -  </strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><strong>HighLoad++</strong></a>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> </a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="></a> early bird . <br></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id433370/">https://habr.com/ru/post/id433370/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id433360/index.html">Para ilmuwan telah mencoba memprediksi kapan pesawat listrik akan menjadi kenyataan</a></li>
<li><a href="../id433362/index.html">9 prinsip kecantikan, kesederhanaan dan perawatan di UX</a></li>
<li><a href="../id433364/index.html">LDraw + Unity. Bagaimana saya menghasilkan Lego</a></li>
<li><a href="../id433366/index.html">Bekerja dengan sumber daya eksternal di Unity3D</a></li>
<li><a href="../id433368/index.html">Cara mengaplikasikan pemikiran belanjaan ke dunia: contoh kaus</a></li>
<li><a href="../id433372/index.html">Sepeda mobil</a></li>
<li><a href="../id433376/index.html">Kursus MIT "Keamanan Sistem Komputer". Kuliah 21: Pelacakan Data, Bagian 1</a></li>
<li><a href="../id433378/index.html">Kursus MIT "Keamanan Sistem Komputer". Kuliah 21: Pelacakan Data, Bagian 2</a></li>
<li><a href="../id433380/index.html">Kursus MIT "Keamanan Sistem Komputer". Kuliah 21: Pelacakan Data, Bagian 3</a></li>
<li><a href="../id433382/index.html">[Ilustrasi] Panduan jaringan di Kubernetes. Bagian 3</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>