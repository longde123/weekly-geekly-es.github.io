<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üîè ‚úîÔ∏è üí∏ Algoritmos de aprendizaje autom√°tico imprescindibles üÖæÔ∏è ü¶è ‚Ü™Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Habr, hola. 

 Esta publicaci√≥n es una breve descripci√≥n de los algoritmos generales de aprendizaje autom√°tico. Cada uno est√° acompa√±ado de una breve ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Algoritmos de aprendizaje autom√°tico imprescindibles</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/467825/">  Habr, hola. <br><br>  Esta publicaci√≥n es una breve descripci√≥n de los algoritmos generales de aprendizaje autom√°tico.  Cada uno est√° acompa√±ado de una breve descripci√≥n, gu√≠as y enlaces √∫tiles. <br><br><h2>  M√©todo de componente principal (PCA) / SVD </h2><br>  Este es uno de los algoritmos b√°sicos de aprendizaje autom√°tico.  Le permite reducir la dimensionalidad de los datos, perdiendo la menor cantidad de informaci√≥n.  Se utiliza en muchos campos, como el reconocimiento de objetos, la visi√≥n por computadora, la compresi√≥n de datos, etc. El c√°lculo de los componentes principales se reduce al c√°lculo de los vectores propios y los valores propios de la matriz de covarianza de los datos de origen o a la descomposici√≥n singular de la matriz de datos. <br><br><img src="https://habrastorage.org/webt/q1/s1/jh/q1s1jh5xtwmuvbklcbapvgcmk4a.png" alt="imagen"><br><br>  SVD es una forma de calcular componentes ordenados. <br><br>  Enlaces utiles: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">scipy.linalg.svd</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sklearn.decomposition.pca</a> </li></ul><br>  Gu√≠a introductoria <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Tutorial de an√°lisis de componentes b√°sicos</a> </li></ul><a name="habracut"></a><br><h2>  M√©todo de m√≠nimos cuadrados </h2><br>  El m√©todo de m√≠nimos cuadrados es un m√©todo matem√°tico utilizado para resolver varios problemas, basado en minimizar la suma de los cuadrados de las desviaciones de algunas funciones de las variables deseadas.  Se puede usar para "resolver" sistemas de ecuaciones sobredeterminados (cuando el n√∫mero de ecuaciones excede el n√∫mero de inc√≥gnitas), para encontrar una soluci√≥n en el caso de sistemas de ecuaciones no lineales ordinarios (no redefinidos) y tambi√©n para aproximar los valores de puntos de una funci√≥n. <br><br><img src="https://habrastorage.org/webt/7s/uw/cm/7suwcmx0ilzbfou_eqzltg4-hsm.jpeg" alt="imagen"><br><br>  Use este algoritmo para ajustar curvas / regresiones simples. <br><br>  Enlaces utiles: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">numpy.linalg.lstsq</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Numpy</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">numpy.polyfit</a> </li></ul><br>  Gu√≠a introductoria <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Regresi√≥n lineal de Stanford (PDF)</a> </li></ul><br><h2>  Regresi√≥n lineal limitada </h2><br>  El m√©todo de m√≠nimos cuadrados puede confundir valores at√≠picos, campos falsos, etc. Se necesitan restricciones para reducir la varianza de la l√≠nea que colocamos en el conjunto de datos.  La soluci√≥n correcta es ajustar un modelo de regresi√≥n lineal que garantice que los pesos no se comporten "mal".  Los modelos pueden tener la norma L1 (LASSO) o L2 (Regresi√≥n de cresta) o ambas (regresi√≥n el√°stica). <br><br><img src="https://habrastorage.org/webt/rn/a7/wk/rna7wkkbva6w6qrbq_lk0ayxshg.jpeg" alt="imagen"><br><br>  Use este algoritmo para hacer coincidir las l√≠neas de regresi√≥n restringidas, evitando la anulaci√≥n. <br><br>  Enlace √∫til: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Modelos lineales generalizados</a> </li></ul><br>  Gu√≠as introductorias: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Regresi√≥n de cresta</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Regresi√≥n LASSO</a> </li></ul><br><h2>  M√©todo K-means </h2><br>  El algoritmo de agrupamiento incontrolado favorito de todos.  Dado un conjunto de datos en forma de vectores, podemos crear grupos de puntos basados ‚Äã‚Äãen las distancias entre ellos.  Este es uno de los algoritmos de aprendizaje autom√°tico que mueve secuencialmente los centros de los grupos y luego agrupa los puntos con cada centro del grupo.  La entrada es el n√∫mero de cl√∫steres que se crear√°n y el n√∫mero de iteraciones. <br><br><img src="https://habrastorage.org/webt/2u/pa/9z/2upa9z52ro49nolljotlbb9sfgg.png" alt="imagen"><br><br>  Enlace √∫til: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sklearn.cluster.KMeans</a> </li></ul><br>  Gu√≠as introductorias: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Video de agrupamiento</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Introducci√≥n a la agrupaci√≥n</a> </li></ul><br><h2>  Regresi√≥n log√≠stica </h2><br>  La regresi√≥n log√≠stica est√° limitada por la regresi√≥n lineal con no linealidad (principalmente utilizando la funci√≥n sigmoide o tanh) despu√©s de aplicar pesos, por lo tanto, la limitaci√≥n de salida est√° cerca de las clases +/- (que es 1 y 0 en el caso de un sigmoide).  Las funciones de p√©rdida de entrop√≠a cruzada se optimizan utilizando el m√©todo de descenso de gradiente. <br><br>  Nota para principiantes: la regresi√≥n log√≠stica se utiliza para la clasificaci√≥n, no para la regresi√≥n.  En general, es similar a una red neuronal de capa √∫nica.  Entrenado utilizando t√©cnicas de optimizaci√≥n como el descenso de gradiente o L-BFGS.  Los desarrolladores de PNL a menudo lo usan, llam√°ndolo "clasificaci√≥n de entrop√≠a m√°xima". <br><br><img src="https://habrastorage.org/webt/gc/yv/ne/gcyvnenl933eapskav1qili-vde.jpeg" alt="imagen"><br><br>  Use LR para entrenar clasificadores simples pero muy "fuertes". <br><br>  Enlace √∫til: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sklearn.linear_model.LogisticRegression</a> </li></ul><br>  Gu√≠a introductoria <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Regresi√≥n log√≠stica |</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">clasificaci√≥n</a> </li></ul><br><h2>  SVM (M√©todo de vector de soporte) </h2><br>  SVM es un modelo lineal como la regresi√≥n lineal / log√≠stica.  La diferencia es que tiene una funci√≥n de p√©rdida basada en el margen.  Puede optimizar la funci√≥n de p√©rdida utilizando m√©todos de optimizaci√≥n como L-BFGS o SGD. <br><br><img src="https://habrastorage.org/webt/ic/0b/n6/ic0bn6jtbl4eyukyrbfbysgiazi.jpeg" alt="imagen"><br><br>  Una cosa √∫nica que SVM puede hacer es aprender clasificadores de clases. <br><br>  SVM puede usarse para entrenar clasificadores (incluso regresores). <br><br>  Enlace √∫til: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sklearn.svm.SVC</a> </li></ul><br>  Gu√≠as introductorias: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Soporte de m√°quina de vectores</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sklearn.linear_model.SGDClassifier</a> </li></ul><br><h2>  Redes neuronales de distribuci√≥n directa. </h2><br>  B√°sicamente, estos son clasificadores multinivel de regresi√≥n log√≠stica.  Muchas capas de pesos est√°n separadas por no linealidades (sigmoide, tanh, relu + softmax y cool selu nuevo).  Tambi√©n se les llama perceptrones multicapa.  Los FFNN se pueden utilizar para la clasificaci√≥n y la "capacitaci√≥n sin maestros" como codificadores autom√°ticos. <br><br><img src="https://habrastorage.org/webt/t0/zf/km/t0zfkm_ouawuvewy-ypy4uamh0e.jpeg" alt="imagen"><br><br>  FFNN se puede utilizar para entrenar el clasificador o extraer funciones como codificadores autom√°ticos. <br><br>  Enlaces utiles: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sklearn.neural_network.MLPClassifier</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sklearn.neural_network.MLPRegressor</a> </li></ul><br>  Gu√≠as introductorias: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Ffnn</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Codificadores autom√°ticos</a> </li></ul><br><h2>  Redes neuronales convolucionales </h2><br>  Casi todos los logros modernos en el campo del aprendizaje autom√°tico se lograron utilizando redes neuronales convolucionales.  Se utilizan para clasificar im√°genes, detectar objetos o incluso segmentar im√°genes.  Inventado por Jan Lekun a principios de los 90, las redes tienen capas convolucionales que act√∫an como extractores jer√°rquicos de objetos.  Puede usarlos para trabajar con texto (e incluso para trabajar con gr√°ficos). <br><br><img src="https://habrastorage.org/webt/rw/2c/jh/rw2cjhlifo_p6xwl2bpdjftfaxy.png" alt="imagen"><br><br>  Enlaces utiles: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Sistema de aprendizaje interactivo GPU con aprendizaje profundo</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">TorchCV: PyTorch Vision Library imita a ChainerCV</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ChainerCV: biblioteca para aprendizaje profundo y visi√≥n por computadora</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Documentaci√≥n de Keras</a> </li></ul><br>  Gu√≠as introductorias: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CNN para reconocimiento visual</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Gu√≠a para principiantes de CNN</a> </li></ul><br><h2>  Redes neuronales recurrentes (RNN) </h2><br>  Las secuencias de modelo de RNNs aplican el mismo conjunto de pesos de forma recursiva al estado del agregador en el tiempo ty la entrada en el tiempo t.  Los RNN puros rara vez se usan ahora, pero sus contrapartes, como LSTM y GRU, son los m√°s avanzados en la mayor√≠a de las tareas de modelado de secuencias.  LSTM, que se utiliza en lugar de una simple capa densa en RNN puro. <br><br><img src="https://habrastorage.org/webt/qn/g3/-b/qng3-bibabexlnufcac1i3edeaa.png" alt="imagen"><br><br>  Use RNN para cualquier tarea de clasificaci√≥n de texto, traducci√≥n autom√°tica, modelado de idiomas. <br><br>  Enlaces utiles: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Modelos y ejemplos creados con TensorFlow</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Referencia de clasificaci√≥n de texto en PyTorch</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Sistema de traducci√≥n de c√≥digo abierto</a> </li></ul><br>  Gu√≠as introductorias: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Aprendizaje profundo para el procesamiento del lenguaje de Stanford</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Art√≠culos RNN</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Entendiendo LSTM</a> </li></ul><br><h2>  Campos aleatorios condicionales (CRF) </h2><br>  Se usan para el modelado de secuencias, como los RNN, y se pueden usar en combinaci√≥n con los RNN.  Tambi√©n se pueden usar en otras tareas de pron√≥stico estructurado, por ejemplo, en la segmentaci√≥n de im√°genes.  El CRF modela cada elemento de la secuencia (por ejemplo, una oraci√≥n), de modo que los vecinos influyan en la etiqueta del componente en la secuencia, y no todas las etiquetas que sean independientes entre s√≠. <br><br>  Use CRF para vincular secuencias (en texto, imagen, series de tiempo, ADN, etc.). <br><br>  Enlace √∫til: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sklearn-crfsuite</a> </li></ul><br>  Gu√≠as introductorias: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Introducci√≥n a los campos aleatorios condicionales</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Lista de reproducci√≥n de YouTube para campos aleatorios condicionales</a> </li></ul><br><h2>  √Årboles de decisi√≥n y bosques al azar </h2><br>  Uno de los algoritmos de aprendizaje autom√°tico m√°s comunes.  Utilizado en estad√≠sticas y an√°lisis de datos para modelos de pron√≥stico.  La estructura es "hojas" y "ramas".  Los atributos de los que depende la funci√≥n objetivo se registran en las "ramas" del √°rbol de decisi√≥n, los valores de la funci√≥n objetivo se escriben en las "hojas", y los atributos que distinguen los casos se registran en los nodos restantes. <br><br>  Para clasificar un nuevo caso, debe bajar del √°rbol a la hoja y emitir el valor correspondiente.  El objetivo es crear un modelo que prediga el valor de la variable objetivo en funci√≥n de varias variables de entrada. <br><br>  Enlaces utiles: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sklearn.ensemble.RandomForestClassifier</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sklearn.ensemble.GradientBoostingClassifier</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Documentaci√≥n de XGBoost</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Documentaci√≥n CatBoost</a> </li></ul><br>  Gu√≠as introductorias: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Introducci√≥n a los √°rboles de decisi√≥n</a> </li><li>  <a href="">Comprensi√≥n de los bosques aleatorios: de la teor√≠a a la pr√°ctica.</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">XGBoost en Python</a> </li></ul><br>  Aprender√° m√°s informaci√≥n sobre aprendizaje autom√°tico y ciencia de datos suscribi√©ndose a mi cuenta en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Habr√©</a> y el canal de Telegram <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Neuron</a> .  No te saltes futuros art√≠culos. <br><br>  Todo el conocimiento! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/467825/">https://habr.com/ru/post/467825/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../467813/index.html">Revisi√≥n de los cambios en el orden 17 del FSTEC</a></li>
<li><a href="../467815/index.html">Los medios de comunicaci√≥n generaron p√°nico porque "las direcciones IP se est√°n agotando en Rusia". Como realmente</a></li>
<li><a href="../467817/index.html">Un poco sobre patrones de dise√±o generativo</a></li>
<li><a href="../467821/index.html">Simplifique y recorte lo necesario: Entrevista con John Romero, creador de Doom</a></li>
<li><a href="../467823/index.html">Parsing: OOM en Kubernetes</a></li>
<li><a href="../467827/index.html">C√≥mo hicimos nuestra peque√±a Unidad desde cero</a></li>
<li><a href="../467831/index.html">El camino espinoso a la programaci√≥n</a></li>
<li><a href="../467837/index.html">MCU de tres centavos "terrible": una breve descripci√≥n de los microcontroladores que cuestan menos de $ 0.1</a></li>
<li><a href="../467841/index.html">Facilite terminar: Entrevista con John Romero, desarrollador de Doom</a></li>
<li><a href="../467843/index.html">¬øC√≥mo ahorrar hasta medio mill√≥n de d√≥lares en AWS?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>