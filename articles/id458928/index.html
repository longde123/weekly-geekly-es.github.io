<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘©ğŸ¿â€ğŸ¤â€ğŸ‘©ğŸ¼ â˜£ï¸ â” XLNet vs BERT ğŸ‘‡ğŸ¾ ğŸ¤º ğŸ’·</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pada akhir Juni, sebuah tim dari Universitas Carnegie Mellon menunjukkan kepada kami XLNet, segera menyusun publikasi , kode , dan model yang sudah ja...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>XLNet vs BERT</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/458928/"><img src="https://habrastorage.org/webt/py/g0/es/pyg0es7u25w7xb0cc8z49aczcls.png"><br><br>  Pada akhir Juni, sebuah tim dari Universitas Carnegie Mellon menunjukkan kepada kami XLNet, segera menyusun <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">publikasi</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">kode</a> , dan model yang sudah jadi ( <a href="">XLNet-Large</a> , Cased: 24-layer, 1024-hidden, 16-heads).  Ini adalah model pra-terlatih untuk menyelesaikan berbagai masalah pemrosesan bahasa alami. <br><br>  Dalam publikasi, mereka segera menunjukkan perbandingan model mereka dengan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">BERT</a> Google.  Mereka menulis bahwa XLNet lebih unggul daripada BERT dalam banyak tugas.  Dan menunjukkan hasil dalam 18 tugas canggih. <br><a name="habracut"></a><br><h2>  BERT, XLNet, dan Transformer </h2><br>  Salah satu tren terbaru dalam pembelajaran yang mendalam adalah Transfer Learning.  Kami melatih model untuk memecahkan masalah sederhana pada sejumlah besar data, dan kemudian kami menggunakan model pra-terlatih ini, tetapi untuk memecahkan masalah lain yang lebih spesifik.  BERT dan XLNet hanyalah jaringan pra-terlatih yang dapat digunakan untuk memecahkan masalah pemrosesan bahasa alami. <br><br>  Model-model ini mengembangkan ide <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">transformer</a> - pendekatan yang saat ini dominan untuk membangun model untuk bekerja dengan urutan.  Sangat detail dan dengan contoh kode pada transformer dan mekanisme Attention ditulis dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">The Annotated Transformer</a> . <br><br>  Jika Anda melihat <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">leaderboard benchmark General Language Understanding Evaluation (GLUE)</a> , maka dari atas Anda dapat melihat banyak model berdasarkan transformator.  Termasuk kedua model yang menunjukkan hasil lebih baik daripada manusia.  Kita dapat mengatakan bahwa dengan transformer kita menyaksikan revolusi kecil dalam pemrosesan bahasa alami. <br><br><h2>  Kerugian BERT </h2><br>  BERT adalah auto-encoder (autoencoder, AE).  Dia menyembunyikan dan merusak beberapa kata dalam urutan dan mencoba mengembalikan urutan kata asli dari konteks. <br><br>  Ini menyebabkan kelemahan model: <br><br><ul><li>  Setiap kata yang tersembunyi diprediksi secara individual.  Kami kehilangan informasi tentang kemungkinan hubungan antara kata-kata yang di-mask.  Artikel tersebut memberikan contoh yang disebut "New York."  Jika kami mencoba secara independen memprediksi kata-kata ini dalam konteks, kami tidak akan memperhitungkan hubungan di antara mereka. </li><li>  Ketidakkonsistenan antara fase pelatihan model BERT dan penggunaan model BERT yang telah dilatih sebelumnya.  Ketika kami melatih model - kami memiliki kata-kata tersembunyi (token) [MASKER], ketika kami menggunakan model yang sudah dilatih sebelumnya, kami belum menyediakan token tersebut ke input. </li></ul><br>  Namun, terlepas dari masalah ini, BERT menunjukkan hasil yang canggih pada banyak tugas pemrosesan bahasa alami. <br><br><h2>  Fitur XLNet </h2><br>  XLNet adalah pemodelan bahasa autoregresif, AR LM.  Dia mencoba memprediksi token berikutnya dari urutan yang sebelumnya.  Dalam model autoregresif klasik, urutan kontekstual ini diambil secara independen dari dua arah string asli. <br><br>  XLNet menggeneralisasikan metode ini dan membentuk konteks dari berbagai tempat dalam urutan sumber.  Bagaimana dia melakukannya?  Dia mengambil semua (dalam teori) kemungkinan permutasi dari urutan asli dan memprediksi setiap token dalam urutan dari yang sebelumnya. <br><br>  Berikut ini adalah contoh dari artikel bagaimana x3 token dari berbagai permutasi dari urutan asli diprediksi. <br><br><img src="https://habrastorage.org/webt/yq/mb/fa/yqmbfas9mcnfkciq6pmew_-4hh8.png"><br><br>  Selain itu, konteks bukanlah kantong kata-kata.  Informasi tentang urutan awal token juga diberikan ke model. <br><br>  Jika kita menggambar analogi dengan BERT, ternyata kita tidak menyembunyikan token terlebih dahulu, melainkan menggunakan set token tersembunyi yang berbeda untuk permutasi yang berbeda.  Pada saat yang sama, masalah kedua BERT menghilang - kurangnya token tersembunyi saat menggunakan model pra-terlatih.  Dalam kasus XLNet, seluruh rangkaian, tanpa topeng, sudah dimasukkan. <br><br>  Dari mana XL berasal dari namanya.  XL - karena XLNet menggunakan mekanisme Perhatian dan ide-ide dari model Transformer-XL.  Meskipun bahasa jahat mengklaim bahwa XL mengisyaratkan jumlah sumber daya yang dibutuhkan untuk melatih jaringan. <br><br><img src="https://habrastorage.org/webt/hs/fb/u-/hsfbu-ufj-9e-me1agkauoa389c.png"><br><br>  Dan tentang sumber daya.  Di Twitter, mereka memposting <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">perhitungan</a> berapa biaya untuk melatih jaringan dengan parameter dari artikel.  Ternyata 245.000 dolar.  Benar, kemudian seorang insinyur dari Google datang dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">mengoreksi</a> bahwa artikel tersebut menyebutkan 512 chip TPU, empat di antaranya ada di perangkat.  Artinya, biayanya sudah 62.440 dolar, atau bahkan 32.720 dolar, mengingat 512 core, yang juga disebutkan dalam artikel. <br><br><h2>  XLNet vs BERT </h2><br>  Sejauh ini, hanya satu model pra-terlatih untuk Bahasa Inggris yang telah disiapkan untuk artikel (XLNet-Large, Cased).  Tetapi artikel itu juga menyebutkan percobaan dengan model yang lebih kecil.  Dan dalam banyak tugas, model XLNet menunjukkan hasil yang lebih baik dibandingkan dengan model BERT serupa. <br><br><img src="https://habrastorage.org/webt/ac/p_/th/acp_thyxqwgcuyhvfvkkixhwj_y.png"><br><br>  Munculnya BERT dan terutama model pra-pelatihan menarik banyak perhatian peneliti dan menyebabkan sejumlah besar karya terkait.  Sekarang di sini adalah XLNet.  Sangat menarik untuk melihat apakah akan menjadi standar de facto dalam NLP untuk beberapa waktu, atau sebaliknya akan memacu para peneliti dalam mencari arsitektur baru dan pendekatan untuk memproses bahasa alami. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id458928/">https://habr.com/ru/post/id458928/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id458918/index.html">Apa yang bisa Anda pelajari dari desain gim kasual</a></li>
<li><a href="../id458920/index.html">Konferensi untuk penggemar DevOps</a></li>
<li><a href="../id458922/index.html">Cara pindah dari ESXi ke KVM / LXD dan tidak kehilangan akal</a></li>
<li><a href="../id458924/index.html">Kecelakaan membantu Anda belajar</a></li>
<li><a href="../id458926/index.html">Tragedi tidak datang sendiri</a></li>
<li><a href="../id458930/index.html">Bagaimana siswa dari Perm berhasil mencapai final kejuaraan penambangan data internasional, Piala Penambangan Data 2019</a></li>
<li><a href="../id458932/index.html">Yota - atau bagaimana Anda bisa mengetahui semuanya</a></li>
<li><a href="../id458934/index.html">Menyebarkan aplikasi pada beberapa kluster Kubernet dengan Helm</a></li>
<li><a href="../id458936/index.html">"Lebih mudah untuk menjawab daripada diam" - sebuah wawancara besar dengan ayah dari memori transaksional, Maurice Herlichi</a></li>
<li><a href="../id458938/index.html">C ++ 20 dibundel, C ++ 23 dimulai. Hasil pertemuan di Cologne</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>