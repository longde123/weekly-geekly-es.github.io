<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üéÖüèΩ üë©‚Äçüç≥ üîª Apache Kafka und Streaming mit Spark Streaming üèõÔ∏è üë≤üèª üë®üèø‚Äçüè´</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Habr! Heute werden wir ein System erstellen, das Apark Kafka verwendet, um Nachrichtenstr√∂me mit Spark Streaming zu verarbeiten und das Verarbei...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Apache Kafka und Streaming mit Spark Streaming</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/451160/">  Hallo Habr!  Heute werden wir ein System erstellen, das Apark Kafka verwendet, um Nachrichtenstr√∂me mit Spark Streaming zu verarbeiten und das Verarbeitungsergebnis in die AWS RDS-Cloud-Datenbank zu schreiben. <br><br>  Stellen Sie sich vor, ein bestimmtes Kreditinstitut hat uns die Aufgabe gestellt, eingehende Transaktionen in allen Filialen im laufenden Betrieb zu verarbeiten.  Dies kann erfolgen, um schnell die offene W√§hrungsposition f√ºr das Treasury, Limits oder Finanzergebnisse f√ºr Transaktionen usw. zu berechnen. <br><br>  Wie man diesen Fall ohne den Einsatz von Magie und Zauberspr√ºchen umsetzt - wir lesen unter dem Schnitt!  Lass uns gehen! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/5w/sb/8v/5wsb8vvncrzhysct-pd6oqraqky.jpeg"></div><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">(Bildquelle)</a> <br><a name="habracut"></a><br><h2>  Einf√ºhrung </h2><br>  Nat√ºrlich bietet die Verarbeitung eines gro√üen Datenarrays in Echtzeit zahlreiche M√∂glichkeiten f√ºr den Einsatz in modernen Systemen.  Eine der beliebtesten Kombinationen hierf√ºr ist das Apache Kafka- und Spark Streaming-Tandem, bei dem Kafka einen Stream eingehender Nachrichtenpakete erstellt und Spark Streaming diese Pakete in einem bestimmten Zeitintervall verarbeitet. <br><br>  Um die Fehlertoleranz der Anwendung zu erh√∂hen, verwenden wir Checkpoints - Checkpoints.  Wenn das Spark-Streaming-Modul mithilfe dieses Mechanismus verlorene Daten wiederherstellen muss, muss es nur zum letzten Kontrollpunkt zur√ºckkehren und die Berechnungen von diesem fortsetzen. <br><br><h2>  Architektur des in Entwicklung befindlichen Systems </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/od/ef/zc/odefzciug8ckvim4-ei6pdg49tw.png"></div><br><br>  Verwendete Komponenten: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><b>Apache Kafka</b></a> ist ein verteiltes Nachrichtensystem mit Publish und Subscribe.  Geeignet f√ºr den Offline- und Online-Nachrichtenverbrauch.  Um Datenverlust zu vermeiden, werden Kafka-Nachrichten auf der Festplatte gespeichert und im Cluster repliziert.  Das Kafka-System basiert auf dem ZooKeeper-Synchronisierungsdienst. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><b>Apache Spark Streaming</b></a> - Spark-Komponente zur Verarbeitung von Streaming-Daten.  Das Spark-Streaming-Modul wird unter Verwendung der Micro-Batch-Architektur erstellt, wenn der Datenstrom als fortlaufende Folge kleiner Datenpakete interpretiert wird.  Spark Streaming empf√§ngt Daten aus verschiedenen Quellen und kombiniert sie zu kleinen Paketen.  In regelm√§√üigen Abst√§nden werden neue Pakete erstellt.  Zu Beginn jedes Zeitintervalls wird ein neues Paket erstellt, und alle w√§hrend dieses Intervalls empfangenen Daten werden in das Paket aufgenommen.  Am Ende des Intervalls stoppt das Paketwachstum.  Die Gr√∂√üe des Intervalls wird durch einen Parameter bestimmt, der als Stapelintervall bezeichnet wird. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><b>Apache Spark SQL</b></a> - Kombiniert relationale Verarbeitung mit funktionsf√§higer Spark-Programmierung.  Strukturierte Daten beziehen sich auf Daten mit einem Schema, dh einem einzelnen Satz von Feldern f√ºr alle Datens√§tze.  Spark SQL unterst√ºtzt Eingaben aus einer Vielzahl strukturierter Datenquellen und kann dank der Verf√ºgbarkeit von Schemainformationen nur die erforderlichen Datensatzfelder effizient abrufen und bietet au√üerdem DataFrame-APIs </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><b>AWS RDS</b></a> ist eine relativ kosteng√ºnstige Cloud-basierte relationale Datenbank, ein Webdienst, der Konfiguration, Betrieb und Skalierung vereinfacht und direkt von Amazon verwaltet wird. </li></ul><br><h2>  Installieren und starten Sie den Kafka-Server </h2><br>  Bevor Sie Kafka direkt verwenden, m√ºssen Sie sicherstellen, dass Java verf√ºgbar ist  JVM wird f√ºr die Arbeit verwendet: <br><br><pre><code class="bash hljs">sudo apt-get update sudo apt-get install default-jre java -version</code> </pre> <br>  Erstellen Sie einen neuen Benutzer f√ºr die Arbeit mit Kafka: <br><br><pre> <code class="bash hljs">sudo useradd kafka -m sudo passwd kafka sudo adduser kafka sudo</code> </pre><br>  Laden Sie als N√§chstes die Distribution von der offiziellen Apache Kafka-Website herunter: <br><br><pre> <code class="bash hljs">wget -P /YOUR_PATH <span class="hljs-string"><span class="hljs-string">"http://apache-mirror.rbc.ru/pub/apache/kafka/2.2.0/kafka_2.12-2.2.0.tgz"</span></span></code> </pre> <br>  Entpacken Sie das heruntergeladene Archiv: <br><pre> <code class="bash hljs">tar -xvzf /YOUR_PATH/kafka_2.12-2.2.0.tgz ln -s /YOUR_PATH/kafka_2.12-2.2.0 kafka</code> </pre><br>  Der n√§chste Schritt ist optional.  Tatsache ist, dass die Standardeinstellungen nicht die vollst√§ndige Nutzung aller Funktionen von Apache Kafka erm√∂glichen.  L√∂schen Sie beispielsweise ein Thema, eine Kategorie oder eine Gruppe, f√ºr die Nachrichten ver√∂ffentlicht werden k√∂nnen.  Um dies zu √§ndern, bearbeiten Sie die Konfigurationsdatei: <br><br><pre> <code class="bash hljs">vim ~/kafka/config/server.properties</code> </pre> <br>  F√ºgen Sie am Ende der Datei Folgendes hinzu: <br><br><pre> <code class="bash hljs">delete.topic.enable = <span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br>  Bevor Sie den Kafka-Server starten, m√ºssen Sie den ZooKeeper-Server starten. Wir verwenden das Hilfsskript, das mit der Kafka-Distribution geliefert wird: <br><br><pre> <code class="bash hljs">Cd ~/kafka bin/zookeeper-server-start.sh config/zookeeper.properties</code> </pre><br>  Nachdem ZooKeeper erfolgreich gestartet wurde, starten wir in einem separaten Terminal den Kafka-Server: <br><br><pre> <code class="bash hljs">bin/kafka-server-start.sh config/server.properties</code> </pre> <br>  Erstellen Sie ein neues Thema mit dem Namen "Transaktion": <br><br><pre> <code class="bash hljs">bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 3 --topic transaction</code> </pre> <br>  Stellen Sie sicher, dass das Thema mit der richtigen Anzahl von Partitionen und Replikationen erstellt wurde: <br><br><pre> <code class="bash hljs">bin/kafka-topics.sh --describe --zookeeper localhost:2181</code> </pre> <br><img src="https://habrastorage.org/webt/s5/gh/bu/s5ghbuswhb0dcc0pmlvu_uloes4.png"><br><br>  Wir werden die Momente verpassen, in denen der Produzent und der Konsument auf das neu geschaffene Thema getestet werden.  Weitere Informationen zum Testen des Sendens und Empfangens von Nachrichten finden Sie in der offiziellen Dokumentation - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Senden einiger Nachrichten</a> .  Nun schreiben wir einen Produzenten in Python mit der KafkaProducer-API. <br><br><h2>  Produzent schreiben </h2><br>  Der Produzent generiert zuf√§llige Daten - 100 Nachrichten pro Sekunde.  Mit zuf√§lligen Daten meinen wir ein W√∂rterbuch, das aus drei Feldern besteht: <br><br><ul><li>  <b>Filiale</b> - Name der Verkaufsstelle des Kreditinstituts; </li><li>  <b>W√§hrung</b> - Transaktionsw√§hrung; </li><li>  <b>Betrag</b> - Transaktionsbetrag.  Der Betrag ist eine positive Zahl, wenn es sich um einen Kauf einer W√§hrung durch die Bank handelt, und eine negative Zahl, wenn es sich um einen Verkauf handelt. </li></ul><br>  Der Code f√ºr den Hersteller lautet wie folgt: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> numpy.random <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> choice, randint <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_random_value</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> new_dict = {} branch_list = [<span class="hljs-string"><span class="hljs-string">'Kazan'</span></span>, <span class="hljs-string"><span class="hljs-string">'SPB'</span></span>, <span class="hljs-string"><span class="hljs-string">'Novosibirsk'</span></span>, <span class="hljs-string"><span class="hljs-string">'Surgut'</span></span>] currency_list = [<span class="hljs-string"><span class="hljs-string">'RUB'</span></span>, <span class="hljs-string"><span class="hljs-string">'USD'</span></span>, <span class="hljs-string"><span class="hljs-string">'EUR'</span></span>, <span class="hljs-string"><span class="hljs-string">'GBP'</span></span>] new_dict[<span class="hljs-string"><span class="hljs-string">'branch'</span></span>] = choice(branch_list) new_dict[<span class="hljs-string"><span class="hljs-string">'currency'</span></span>] = choice(currency_list) new_dict[<span class="hljs-string"><span class="hljs-string">'amount'</span></span>] = randint(<span class="hljs-number"><span class="hljs-number">-100</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> new_dict</code> </pre><br>  Als N√§chstes senden wir mithilfe der send-Methode eine Nachricht im gew√ºnschten Thema im JSON-Format an den Server: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> kafka <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> KafkaProducer producer = KafkaProducer(bootstrap_servers=[<span class="hljs-string"><span class="hljs-string">'localhost:9092'</span></span>], value_serializer=<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x:dumps(x).encode(<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>), compression_type=<span class="hljs-string"><span class="hljs-string">'gzip'</span></span>) my_topic = <span class="hljs-string"><span class="hljs-string">'transaction'</span></span> data = get_random_value() <span class="hljs-keyword"><span class="hljs-keyword">try</span></span>: future = producer.send(topic = my_topic, value = data) record_metadata = future.get(timeout=<span class="hljs-number"><span class="hljs-number">10</span></span>) print(<span class="hljs-string"><span class="hljs-string">'--&gt; The message has been sent to a topic: \ {}, partition: {}, offset: {}'</span></span> \ .format(record_metadata.topic, record_metadata.partition, record_metadata.offset )) <span class="hljs-keyword"><span class="hljs-keyword">except</span></span> Exception <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> e: print(<span class="hljs-string"><span class="hljs-string">'--&gt; It seems an Error occurred: {}'</span></span>.format(e)) <span class="hljs-keyword"><span class="hljs-keyword">finally</span></span>: producer.flush()</code> </pre><br>  Beim Ausf√ºhren des Skripts erhalten wir folgende Meldungen im Terminal: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/_e/3g/zj/_e3gzjrmsycjb8ntjmur6ztaspw.png"></div><br>  Dies bedeutet, dass alles so funktioniert, wie wir es wollten - der Produzent generiert und sendet Nachrichten an das Thema, das wir ben√∂tigen. <br><br>  Der n√§chste Schritt besteht darin, Spark zu installieren und diesen Nachrichtenfluss zu verarbeiten. <br><br><h2>  Installieren Sie Apache Spark </h2><br>  <b>Apache Spark</b> ist eine vielseitige und leistungsstarke Cluster-Computing-Plattform. <br><br>  In Bezug auf die Leistung √ºbertrifft Spark die g√§ngigen Implementierungen des MapReduce-Modells und bietet gleichzeitig Unterst√ºtzung f√ºr eine gr√∂√üere Bandbreite von Berechnungstypen, einschlie√ülich interaktiver Abfragen und Stream-Verarbeitung.  Geschwindigkeit spielt eine wichtige Rolle bei der Verarbeitung gro√üer Datenmengen, da Sie mit dieser Geschwindigkeit interaktiv arbeiten k√∂nnen, ohne Minuten oder Stunden warten zu m√ºssen.  Eine der gr√∂√üten St√§rken von Spark bei einer so hohen Geschwindigkeit ist die F√§higkeit, In-Memory-Berechnungen durchzuf√ºhren. <br><br>  Dieses Framework ist in Scala geschrieben, daher m√ºssen Sie es zuerst installieren: <br><br><pre> <code class="bash hljs">sudo apt-get install scala</code> </pre> <br>  Laden Sie die Spark-Distribution von der offiziellen Website herunter: <br><br><pre> <code class="bash hljs">wget <span class="hljs-string"><span class="hljs-string">"http://mirror.linux-ia64.org/apache/spark/spark-2.4.2/spark-2.4.2-bin-hadoop2.7.tgz"</span></span></code> </pre> <br>  Packen Sie das Archiv aus: <br><br><pre> <code class="bash hljs">sudo tar xvf spark-2.4.2/spark-2.4.2-bin-hadoop2.7.tgz -C /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/spark</code> </pre> <br>  F√ºgen Sie den Pfad zum Spark in der Bash-Datei hinzu: <br><br><pre> <code class="bash hljs">vim ~/.bashrc</code> </pre> <br>  F√ºgen Sie die folgenden Zeilen √ºber den Editor hinzu: <br><br><pre> <code class="bash hljs">SPARK_HOME=/usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/spark <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> PATH=<span class="hljs-variable"><span class="hljs-variable">$SPARK_HOME</span></span>/bin:<span class="hljs-variable"><span class="hljs-variable">$PATH</span></span></code> </pre><br>  F√ºhren Sie den folgenden Befehl aus, nachdem Sie √Ñnderungen an bashrc vorgenommen haben: <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">source</span></span> ~/.bashrc</code> </pre> <br><h2>  AWS PostgreSQL-Bereitstellung </h2><br>  Es bleibt die Bereitstellung der Datenbank, in die die verarbeiteten Informationen aus den Streams hochgeladen werden.  Hierf√ºr verwenden wir den AWS RDS-Service. <br><br>  Gehen Sie zur Konsole AWS -&gt; AWS RDS -&gt; Datenbanken -&gt; Datenbank erstellen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/dg/os/m7/dgosm7dwnh3fr-uksjdt_xpltsk.png"></div><br>  W√§hlen Sie PostgreSQL und klicken Sie auf die Schaltfl√§che Weiter: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/3y/d_/8r/3yd_8rsz2swfgaxaafpkyizthac.png"></div><br>  Weil  Dieses Beispiel dient ausschlie√ülich Bildungszwecken. Wir verwenden einen kostenlosen Server "mindestens" (Free Tier): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fn/6p/5b/fn6p5bjyitndy_ozs2cdcw_ssi0.png"></div><br>  Setzen Sie als N√§chstes ein H√§kchen in den Free Tier-Block, und danach wird uns automatisch eine Instanz der t2.micro-Klasse angeboten - obwohl schwach, ist sie kostenlos und f√ºr unsere Aufgabe gut geeignet: <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mj/jh/wg/mjjhwg3cknoehrq8wyxk3uw5v74.png"></div><br>  Es folgen sehr wichtige Dinge: der Name der Datenbankinstanz, der Name des Hauptbenutzers und sein Passwort.  Nennen wir die Instanz: myHabrTest, den <b>Hauptbenutzer</b> : <b>habr</b> , das Passwort: <b>habr12345</b> und klicken Sie auf die Schaltfl√§che Weiter: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lg/jt/mf/lgjtmfdfst0pvqthojb_bdpeohc.png"></div><br><br>  Die n√§chste Seite enth√§lt die Parameter, die f√ºr die Verf√ºgbarkeit unseres Datenbankservers von au√üen (√∂ffentliche Zug√§nglichkeit) und die Verf√ºgbarkeit von Ports verantwortlich sind: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/40/z9/q7/40z9q7owar5kpnimyzrdj5laqgs.png"></div><br>  Erstellen wir eine neue Konfiguration f√ºr die VPC-Sicherheitsgruppe, mit der wir von au√üen √ºber Port 5432 (PostgreSQL) auf unseren Datenbankserver zugreifen k√∂nnen. <br><br>  Wechseln Sie in einem separaten Browserfenster zur AWS-Konsole im Abschnitt VPC-Dashboard -&gt; Sicherheitsgruppen -&gt; Sicherheitsgruppe erstellen: <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fl/2i/ne/fl2inejlgnghwsh3itdrlcywdsu.png"></div><br>  Legen Sie den Namen f√ºr die Sicherheitsgruppe fest - PostgreSQL, eine Beschreibung, geben Sie an, welcher VPC diese Gruppe zugeordnet werden soll, und klicken Sie auf die Schaltfl√§che Erstellen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/js/8r/tv/js8rtvp8tudwjtpgso6xota5h-g.png"></div><br>  F√ºllen Sie die neu erstellte Gruppe Eingehende Regeln f√ºr Port 5432 aus (siehe Abbildung unten).  Sie m√ºssen keinen manuellen Port angeben, sondern w√§hlen PostgreSQL aus der Dropdown-Liste Typ aus. <br><br>  Streng genommen bedeutet der Wert :: / 0 die Verf√ºgbarkeit von eingehendem Datenverkehr f√ºr einen Server aus der ganzen Welt, was kanonisch nicht ganz richtig ist, aber um das Beispiel zu analysieren, verwenden wir diesen Ansatz: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ge/8j/bn/ge8jbntssnooajc8so36h0tjo80.png"></div><br>  Wir kehren zur Browserseite zur√ºck, auf der "Erweiterte Einstellungen konfigurieren" ge√∂ffnet ist und w√§hlen Sie im Abschnitt VPC-Sicherheitsgruppen -&gt; Vorhandene VPC-Sicherheitsgruppen ausw√§hlen -&gt; PostgreSQL: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nk/ae/-s/nkae-ste1tp3wgvmyilicvwlk8e.png"></div><br>  Als n√§chstes im Abschnitt Datenbankoptionen -&gt; Datenbankname -&gt; den Namen <b>festlegen</b> - <b>habrDB</b> . <br><br>  Wir k√∂nnen den Rest der Parameter belassen, mit Ausnahme des Deaktivierens der Sicherung (Sicherungsaufbewahrungszeitraum - 0 Tage), der √úberwachung und von Performance Insights.  Klicken Sie auf die Schaltfl√§che <b>Datenbank erstellen</b> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ex/1p/po/ex1ppogq_vdsk3nnvywm7l8vq8i.png"></div><br><h2>  Stream-Handler </h2><br>  Der letzte Schritt wird die Entwicklung von Spark-Jobs sein, die alle zwei Sekunden neue Daten von Kafka verarbeiten und das Ergebnis in die Datenbank eingeben. <br><br>  Wie oben erw√§hnt, sind Pr√ºfpunkte der Hauptmechanismus in SparkStreaming, der konfiguriert werden muss, um Fehlertoleranz bereitzustellen.  Wir werden Kontrollpunkte verwenden, und im Falle eines Prozedurabbruchs muss das Spark-Streaming-Modul nur zum letzten Kontrollpunkt zur√ºckkehren und die Berechnungen von diesem fortsetzen, um die verlorenen Daten wiederherzustellen. <br><br>  Sie k√∂nnen den Haltepunkt aktivieren, indem Sie das Verzeichnis in einem fehlertoleranten, zuverl√§ssigen Dateisystem (z. B. HDFS, S3 usw.) festlegen, in dem die Haltepunktinformationen gespeichert werden.  Dies geschieht zum Beispiel mit: <br><br><pre> <code class="python hljs">streamingContext.checkpoint(checkpointDirectory)</code> </pre> <br>  In unserem Beispiel verwenden wir den folgenden Ansatz: Wenn checkpointDirectory vorhanden ist, wird der Kontext aus den Kontrollpunktdaten neu erstellt.  Wenn das Verzeichnis nicht vorhanden ist (d. H. Zum ersten Mal ausgef√ºhrt wird), wird die Funktion functionToCreateContext aufgerufen, um einen neuen Kontext zu erstellen und DStreams zu konfigurieren: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyspark.streaming <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> StreamingContext context = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext)</code> </pre><br>  Erstellen Sie ein DirectStream-Objekt, um mithilfe der createDirectStream-Methode der KafkaUtils-Bibliothek eine Verbindung zum Thema "Transaktion" herzustellen: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyspark.streaming.kafka <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> KafkaUtils sc = SparkContext(conf=conf) ssc = StreamingContext(sc, <span class="hljs-number"><span class="hljs-number">2</span></span>) broker_list = <span class="hljs-string"><span class="hljs-string">'localhost:9092'</span></span> topic = <span class="hljs-string"><span class="hljs-string">'transaction'</span></span> directKafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {<span class="hljs-string"><span class="hljs-string">"metadata.broker.list"</span></span>: broker_list})</code> </pre><br>  Analysieren eingehender Daten im JSON-Format: <br><br><pre> <code class="python hljs">rowRdd = rdd.map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> w: Row(branch=w[<span class="hljs-string"><span class="hljs-string">'branch'</span></span>], currency=w[<span class="hljs-string"><span class="hljs-string">'currency'</span></span>], amount=w[<span class="hljs-string"><span class="hljs-string">'amount'</span></span>])) testDataFrame = spark.createDataFrame(rowRdd) testDataFrame.createOrReplaceTempView(<span class="hljs-string"><span class="hljs-string">"treasury_stream"</span></span>)</code> </pre><br>  Mit Spark SQL erstellen wir eine einfache Gruppierung und drucken das Ergebnis auf der Konsole aus: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">select</span></span> from_unixtime(<span class="hljs-keyword"><span class="hljs-keyword">unix_timestamp</span></span>()) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> curr_time, t.branch <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> branch_name, t.currency <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> currency_code, <span class="hljs-keyword"><span class="hljs-keyword">sum</span></span>(amount) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> batch_value <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> treasury_stream t <span class="hljs-keyword"><span class="hljs-keyword">group</span></span> <span class="hljs-keyword"><span class="hljs-keyword">by</span></span> t.branch, t.currency</code> </pre><br>  Abfragetext abrufen und √ºber Spark SQL ausf√ºhren: <br><br><pre> <code class="python hljs">sql_query = get_sql_query() testResultDataFrame = spark.sql(sql_query) testResultDataFrame.show(n=<span class="hljs-number"><span class="hljs-number">5</span></span>)</code> </pre><br>  Anschlie√üend speichern wir die empfangenen aggregierten Daten in einer Tabelle in AWS RDS.  Um die Aggregationsergebnisse in einer Datenbanktabelle zu speichern, verwenden wir die Schreibmethode des DataFrame-Objekts: <br><br><pre> <code class="python hljs">testResultDataFrame.write \ .format(<span class="hljs-string"><span class="hljs-string">"jdbc"</span></span>) \ .mode(<span class="hljs-string"><span class="hljs-string">"append"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"driver"</span></span>, <span class="hljs-string"><span class="hljs-string">'org.postgresql.Driver'</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"url"</span></span>,<span class="hljs-string"><span class="hljs-string">"jdbc:postgresql://myhabrtest.ciny8bykwxeg.us-east-1.rds.amazonaws.com:5432/habrDB"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"dbtable"</span></span>, <span class="hljs-string"><span class="hljs-string">"transaction_flow"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"user"</span></span>, <span class="hljs-string"><span class="hljs-string">"habr"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"password"</span></span>, <span class="hljs-string"><span class="hljs-string">"habr12345"</span></span>) \ .save()</code> </pre><br><blockquote>  Ein paar Worte zum Einrichten einer Verbindung zu AWS RDS.  Wir haben den Benutzer und das Kennwort daf√ºr im Schritt "Bereitstellen von AWS PostgreSQL" erstellt.  Verwenden Sie f√ºr die Datenbankserver-URL Endpoint, der im Abschnitt Konnektivit√§t und Sicherheit angezeigt wird: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9n/sj/jd/9nsjjdun0hdy5qtwqub0xhvzunk.png"></div></blockquote><br>  Um Spark und Kafka korrekt zu verbinden, sollten Sie den Job √ºber smark-submit mit dem <b>Artefakt spark-Streaming-kafka-0-8_2.11 ausf√ºhren</b> .  Dar√ºber hinaus wenden wir das Artefakt auch auf die Interaktion mit der PostgreSQL-Datenbank an und √ºbertragen es √ºber --packages. <br><br>  F√ºr die Flexibilit√§t des Skripts nehmen wir auch den Namen des Nachrichtenservers und das Thema heraus, von dem wir Daten als Eingabeparameter empfangen m√∂chten. <br><br>  Es ist also Zeit, das System zu starten und zu testen: <br><br><pre> <code class="bash hljs">spark-submit \ --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.2,\ org.postgresql:postgresql:9.4.1207 \ spark_job.py localhost:9092 transaction</code> </pre><br>  Alles hat geklappt!  Wie Sie in der Abbildung unten sehen k√∂nnen, werden w√§hrend des Betriebs der Anwendung alle 2 Sekunden neue Aggregationsergebnisse angezeigt, da wir das Stapelintervall beim Erstellen des StreamingContext-Objekts auf 2 Sekunden festgelegt haben: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/cf/q1/25/cfq125zpzkyldktsuvdo175fazy.png"></div><br>  Als N√§chstes f√ºhren wir eine einfache Abfrage an die Datenbank durch, um nach Datens√§tzen in der <b>Transaktion_Fluss-</b> Tabelle zu suchen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7j/j9/qm/7jj9qmf4zpter3jkbblrmiqni2s.png"></div><br><h2>  Fazit </h2><br>  In diesem Artikel wurde ein Beispiel f√ºr die Verarbeitung von Streaming-Informationen mithilfe von Spark Streaming in Verbindung mit Apache Kafka und PostgreSQL untersucht.  Mit dem Wachstum von Daten aus verschiedenen Quellen ist es schwierig, den praktischen Wert von Spark Streaming f√ºr die Erstellung von Streaming-Anwendungen und Anwendungen, die in Echtzeit arbeiten, zu √ºbersch√§tzen. <br><br>  Den vollst√§ndigen Quellcode finden Sie in meinem Repository auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GitHub</a> . <br><br>  Ich bin bereit, diesen Artikel gerne zu diskutieren, freue mich auf Ihre Kommentare und hoffe auf konstruktive Kritik aller betroffenen Leser. <br><br>  Ich w√ºnsche Ihnen viel Erfolg! <br><br>  <b>PS</b> Es war urspr√ºnglich geplant, eine lokale PostgreSQL-Datenbank zu verwenden, aber aufgrund meiner Vorliebe f√ºr AWS habe ich beschlossen, die Datenbank in die Cloud zu stellen.  Im n√§chsten Artikel zu diesem Thema werde ich zeigen, wie das gesamte oben in AWS beschriebene System mit AWS Kinesis und AWS EMR implementiert wird.  Folgen Sie den Nachrichten! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de451160/">https://habr.com/ru/post/de451160/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de451148/index.html">Objektorientierte Programmierung in Grafiksprachen</a></li>
<li><a href="../de451150/index.html">Fang mich, wenn du kannst. Manager-Version</a></li>
<li><a href="../de451152/index.html">Der Widerstand in der Gate-Schaltung oder wie man es richtig macht</a></li>
<li><a href="../de451154/index.html">Lokales autonomes Datenerfassungssystem (Fortsetzung)</a></li>
<li><a href="../de451158/index.html">Stromkreise. Schaltungstypen</a></li>
<li><a href="../de451162/index.html">Fehlerkorrektur - Physikalische Konstanten in der gegenw√§rtigen und neuen Version des Internationalen Einheitensystems (SI)</a></li>
<li><a href="../de451164/index.html">Auf der Suche nach freien Parkpl√§tzen mit Python</a></li>
<li><a href="../de451166/index.html">Was bieten die neuen Repositories f√ºr AI- und MO-Systeme?</a></li>
<li><a href="../de451170/index.html">Jeff Bezos k√ºndigte Pl√§ne an, den Mond zu erobern</a></li>
<li><a href="../de451172/index.html">Julia: Funktionen und Strukturen als Funktionen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>