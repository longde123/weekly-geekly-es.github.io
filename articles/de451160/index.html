<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🎅🏽 👩‍🍳 🔻 Apache Kafka und Streaming mit Spark Streaming 🏛️ 👲🏻 👨🏿‍🏫</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Habr! Heute werden wir ein System erstellen, das Apark Kafka verwendet, um Nachrichtenströme mit Spark Streaming zu verarbeiten und das Verarbei...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Apache Kafka und Streaming mit Spark Streaming</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/451160/">  Hallo Habr!  Heute werden wir ein System erstellen, das Apark Kafka verwendet, um Nachrichtenströme mit Spark Streaming zu verarbeiten und das Verarbeitungsergebnis in die AWS RDS-Cloud-Datenbank zu schreiben. <br><br>  Stellen Sie sich vor, ein bestimmtes Kreditinstitut hat uns die Aufgabe gestellt, eingehende Transaktionen in allen Filialen im laufenden Betrieb zu verarbeiten.  Dies kann erfolgen, um schnell die offene Währungsposition für das Treasury, Limits oder Finanzergebnisse für Transaktionen usw. zu berechnen. <br><br>  Wie man diesen Fall ohne den Einsatz von Magie und Zaubersprüchen umsetzt - wir lesen unter dem Schnitt!  Lass uns gehen! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/5w/sb/8v/5wsb8vvncrzhysct-pd6oqraqky.jpeg"></div><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">(Bildquelle)</a> <br><a name="habracut"></a><br><h2>  Einführung </h2><br>  Natürlich bietet die Verarbeitung eines großen Datenarrays in Echtzeit zahlreiche Möglichkeiten für den Einsatz in modernen Systemen.  Eine der beliebtesten Kombinationen hierfür ist das Apache Kafka- und Spark Streaming-Tandem, bei dem Kafka einen Stream eingehender Nachrichtenpakete erstellt und Spark Streaming diese Pakete in einem bestimmten Zeitintervall verarbeitet. <br><br>  Um die Fehlertoleranz der Anwendung zu erhöhen, verwenden wir Checkpoints - Checkpoints.  Wenn das Spark-Streaming-Modul mithilfe dieses Mechanismus verlorene Daten wiederherstellen muss, muss es nur zum letzten Kontrollpunkt zurückkehren und die Berechnungen von diesem fortsetzen. <br><br><h2>  Architektur des in Entwicklung befindlichen Systems </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/od/ef/zc/odefzciug8ckvim4-ei6pdg49tw.png"></div><br><br>  Verwendete Komponenten: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><b>Apache Kafka</b></a> ist ein verteiltes Nachrichtensystem mit Publish und Subscribe.  Geeignet für den Offline- und Online-Nachrichtenverbrauch.  Um Datenverlust zu vermeiden, werden Kafka-Nachrichten auf der Festplatte gespeichert und im Cluster repliziert.  Das Kafka-System basiert auf dem ZooKeeper-Synchronisierungsdienst. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><b>Apache Spark Streaming</b></a> - Spark-Komponente zur Verarbeitung von Streaming-Daten.  Das Spark-Streaming-Modul wird unter Verwendung der Micro-Batch-Architektur erstellt, wenn der Datenstrom als fortlaufende Folge kleiner Datenpakete interpretiert wird.  Spark Streaming empfängt Daten aus verschiedenen Quellen und kombiniert sie zu kleinen Paketen.  In regelmäßigen Abständen werden neue Pakete erstellt.  Zu Beginn jedes Zeitintervalls wird ein neues Paket erstellt, und alle während dieses Intervalls empfangenen Daten werden in das Paket aufgenommen.  Am Ende des Intervalls stoppt das Paketwachstum.  Die Größe des Intervalls wird durch einen Parameter bestimmt, der als Stapelintervall bezeichnet wird. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><b>Apache Spark SQL</b></a> - Kombiniert relationale Verarbeitung mit funktionsfähiger Spark-Programmierung.  Strukturierte Daten beziehen sich auf Daten mit einem Schema, dh einem einzelnen Satz von Feldern für alle Datensätze.  Spark SQL unterstützt Eingaben aus einer Vielzahl strukturierter Datenquellen und kann dank der Verfügbarkeit von Schemainformationen nur die erforderlichen Datensatzfelder effizient abrufen und bietet außerdem DataFrame-APIs </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><b>AWS RDS</b></a> ist eine relativ kostengünstige Cloud-basierte relationale Datenbank, ein Webdienst, der Konfiguration, Betrieb und Skalierung vereinfacht und direkt von Amazon verwaltet wird. </li></ul><br><h2>  Installieren und starten Sie den Kafka-Server </h2><br>  Bevor Sie Kafka direkt verwenden, müssen Sie sicherstellen, dass Java verfügbar ist  JVM wird für die Arbeit verwendet: <br><br><pre><code class="bash hljs">sudo apt-get update sudo apt-get install default-jre java -version</code> </pre> <br>  Erstellen Sie einen neuen Benutzer für die Arbeit mit Kafka: <br><br><pre> <code class="bash hljs">sudo useradd kafka -m sudo passwd kafka sudo adduser kafka sudo</code> </pre><br>  Laden Sie als Nächstes die Distribution von der offiziellen Apache Kafka-Website herunter: <br><br><pre> <code class="bash hljs">wget -P /YOUR_PATH <span class="hljs-string"><span class="hljs-string">"http://apache-mirror.rbc.ru/pub/apache/kafka/2.2.0/kafka_2.12-2.2.0.tgz"</span></span></code> </pre> <br>  Entpacken Sie das heruntergeladene Archiv: <br><pre> <code class="bash hljs">tar -xvzf /YOUR_PATH/kafka_2.12-2.2.0.tgz ln -s /YOUR_PATH/kafka_2.12-2.2.0 kafka</code> </pre><br>  Der nächste Schritt ist optional.  Tatsache ist, dass die Standardeinstellungen nicht die vollständige Nutzung aller Funktionen von Apache Kafka ermöglichen.  Löschen Sie beispielsweise ein Thema, eine Kategorie oder eine Gruppe, für die Nachrichten veröffentlicht werden können.  Um dies zu ändern, bearbeiten Sie die Konfigurationsdatei: <br><br><pre> <code class="bash hljs">vim ~/kafka/config/server.properties</code> </pre> <br>  Fügen Sie am Ende der Datei Folgendes hinzu: <br><br><pre> <code class="bash hljs">delete.topic.enable = <span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br>  Bevor Sie den Kafka-Server starten, müssen Sie den ZooKeeper-Server starten. Wir verwenden das Hilfsskript, das mit der Kafka-Distribution geliefert wird: <br><br><pre> <code class="bash hljs">Cd ~/kafka bin/zookeeper-server-start.sh config/zookeeper.properties</code> </pre><br>  Nachdem ZooKeeper erfolgreich gestartet wurde, starten wir in einem separaten Terminal den Kafka-Server: <br><br><pre> <code class="bash hljs">bin/kafka-server-start.sh config/server.properties</code> </pre> <br>  Erstellen Sie ein neues Thema mit dem Namen "Transaktion": <br><br><pre> <code class="bash hljs">bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 3 --topic transaction</code> </pre> <br>  Stellen Sie sicher, dass das Thema mit der richtigen Anzahl von Partitionen und Replikationen erstellt wurde: <br><br><pre> <code class="bash hljs">bin/kafka-topics.sh --describe --zookeeper localhost:2181</code> </pre> <br><img src="https://habrastorage.org/webt/s5/gh/bu/s5ghbuswhb0dcc0pmlvu_uloes4.png"><br><br>  Wir werden die Momente verpassen, in denen der Produzent und der Konsument auf das neu geschaffene Thema getestet werden.  Weitere Informationen zum Testen des Sendens und Empfangens von Nachrichten finden Sie in der offiziellen Dokumentation - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Senden einiger Nachrichten</a> .  Nun schreiben wir einen Produzenten in Python mit der KafkaProducer-API. <br><br><h2>  Produzent schreiben </h2><br>  Der Produzent generiert zufällige Daten - 100 Nachrichten pro Sekunde.  Mit zufälligen Daten meinen wir ein Wörterbuch, das aus drei Feldern besteht: <br><br><ul><li>  <b>Filiale</b> - Name der Verkaufsstelle des Kreditinstituts; </li><li>  <b>Währung</b> - Transaktionswährung; </li><li>  <b>Betrag</b> - Transaktionsbetrag.  Der Betrag ist eine positive Zahl, wenn es sich um einen Kauf einer Währung durch die Bank handelt, und eine negative Zahl, wenn es sich um einen Verkauf handelt. </li></ul><br>  Der Code für den Hersteller lautet wie folgt: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> numpy.random <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> choice, randint <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_random_value</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> new_dict = {} branch_list = [<span class="hljs-string"><span class="hljs-string">'Kazan'</span></span>, <span class="hljs-string"><span class="hljs-string">'SPB'</span></span>, <span class="hljs-string"><span class="hljs-string">'Novosibirsk'</span></span>, <span class="hljs-string"><span class="hljs-string">'Surgut'</span></span>] currency_list = [<span class="hljs-string"><span class="hljs-string">'RUB'</span></span>, <span class="hljs-string"><span class="hljs-string">'USD'</span></span>, <span class="hljs-string"><span class="hljs-string">'EUR'</span></span>, <span class="hljs-string"><span class="hljs-string">'GBP'</span></span>] new_dict[<span class="hljs-string"><span class="hljs-string">'branch'</span></span>] = choice(branch_list) new_dict[<span class="hljs-string"><span class="hljs-string">'currency'</span></span>] = choice(currency_list) new_dict[<span class="hljs-string"><span class="hljs-string">'amount'</span></span>] = randint(<span class="hljs-number"><span class="hljs-number">-100</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> new_dict</code> </pre><br>  Als Nächstes senden wir mithilfe der send-Methode eine Nachricht im gewünschten Thema im JSON-Format an den Server: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> kafka <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> KafkaProducer producer = KafkaProducer(bootstrap_servers=[<span class="hljs-string"><span class="hljs-string">'localhost:9092'</span></span>], value_serializer=<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x:dumps(x).encode(<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>), compression_type=<span class="hljs-string"><span class="hljs-string">'gzip'</span></span>) my_topic = <span class="hljs-string"><span class="hljs-string">'transaction'</span></span> data = get_random_value() <span class="hljs-keyword"><span class="hljs-keyword">try</span></span>: future = producer.send(topic = my_topic, value = data) record_metadata = future.get(timeout=<span class="hljs-number"><span class="hljs-number">10</span></span>) print(<span class="hljs-string"><span class="hljs-string">'--&gt; The message has been sent to a topic: \ {}, partition: {}, offset: {}'</span></span> \ .format(record_metadata.topic, record_metadata.partition, record_metadata.offset )) <span class="hljs-keyword"><span class="hljs-keyword">except</span></span> Exception <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> e: print(<span class="hljs-string"><span class="hljs-string">'--&gt; It seems an Error occurred: {}'</span></span>.format(e)) <span class="hljs-keyword"><span class="hljs-keyword">finally</span></span>: producer.flush()</code> </pre><br>  Beim Ausführen des Skripts erhalten wir folgende Meldungen im Terminal: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/_e/3g/zj/_e3gzjrmsycjb8ntjmur6ztaspw.png"></div><br>  Dies bedeutet, dass alles so funktioniert, wie wir es wollten - der Produzent generiert und sendet Nachrichten an das Thema, das wir benötigen. <br><br>  Der nächste Schritt besteht darin, Spark zu installieren und diesen Nachrichtenfluss zu verarbeiten. <br><br><h2>  Installieren Sie Apache Spark </h2><br>  <b>Apache Spark</b> ist eine vielseitige und leistungsstarke Cluster-Computing-Plattform. <br><br>  In Bezug auf die Leistung übertrifft Spark die gängigen Implementierungen des MapReduce-Modells und bietet gleichzeitig Unterstützung für eine größere Bandbreite von Berechnungstypen, einschließlich interaktiver Abfragen und Stream-Verarbeitung.  Geschwindigkeit spielt eine wichtige Rolle bei der Verarbeitung großer Datenmengen, da Sie mit dieser Geschwindigkeit interaktiv arbeiten können, ohne Minuten oder Stunden warten zu müssen.  Eine der größten Stärken von Spark bei einer so hohen Geschwindigkeit ist die Fähigkeit, In-Memory-Berechnungen durchzuführen. <br><br>  Dieses Framework ist in Scala geschrieben, daher müssen Sie es zuerst installieren: <br><br><pre> <code class="bash hljs">sudo apt-get install scala</code> </pre> <br>  Laden Sie die Spark-Distribution von der offiziellen Website herunter: <br><br><pre> <code class="bash hljs">wget <span class="hljs-string"><span class="hljs-string">"http://mirror.linux-ia64.org/apache/spark/spark-2.4.2/spark-2.4.2-bin-hadoop2.7.tgz"</span></span></code> </pre> <br>  Packen Sie das Archiv aus: <br><br><pre> <code class="bash hljs">sudo tar xvf spark-2.4.2/spark-2.4.2-bin-hadoop2.7.tgz -C /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/spark</code> </pre> <br>  Fügen Sie den Pfad zum Spark in der Bash-Datei hinzu: <br><br><pre> <code class="bash hljs">vim ~/.bashrc</code> </pre> <br>  Fügen Sie die folgenden Zeilen über den Editor hinzu: <br><br><pre> <code class="bash hljs">SPARK_HOME=/usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/spark <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> PATH=<span class="hljs-variable"><span class="hljs-variable">$SPARK_HOME</span></span>/bin:<span class="hljs-variable"><span class="hljs-variable">$PATH</span></span></code> </pre><br>  Führen Sie den folgenden Befehl aus, nachdem Sie Änderungen an bashrc vorgenommen haben: <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">source</span></span> ~/.bashrc</code> </pre> <br><h2>  AWS PostgreSQL-Bereitstellung </h2><br>  Es bleibt die Bereitstellung der Datenbank, in die die verarbeiteten Informationen aus den Streams hochgeladen werden.  Hierfür verwenden wir den AWS RDS-Service. <br><br>  Gehen Sie zur Konsole AWS -&gt; AWS RDS -&gt; Datenbanken -&gt; Datenbank erstellen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/dg/os/m7/dgosm7dwnh3fr-uksjdt_xpltsk.png"></div><br>  Wählen Sie PostgreSQL und klicken Sie auf die Schaltfläche Weiter: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/3y/d_/8r/3yd_8rsz2swfgaxaafpkyizthac.png"></div><br>  Weil  Dieses Beispiel dient ausschließlich Bildungszwecken. Wir verwenden einen kostenlosen Server "mindestens" (Free Tier): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fn/6p/5b/fn6p5bjyitndy_ozs2cdcw_ssi0.png"></div><br>  Setzen Sie als Nächstes ein Häkchen in den Free Tier-Block, und danach wird uns automatisch eine Instanz der t2.micro-Klasse angeboten - obwohl schwach, ist sie kostenlos und für unsere Aufgabe gut geeignet: <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mj/jh/wg/mjjhwg3cknoehrq8wyxk3uw5v74.png"></div><br>  Es folgen sehr wichtige Dinge: der Name der Datenbankinstanz, der Name des Hauptbenutzers und sein Passwort.  Nennen wir die Instanz: myHabrTest, den <b>Hauptbenutzer</b> : <b>habr</b> , das Passwort: <b>habr12345</b> und klicken Sie auf die Schaltfläche Weiter: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lg/jt/mf/lgjtmfdfst0pvqthojb_bdpeohc.png"></div><br><br>  Die nächste Seite enthält die Parameter, die für die Verfügbarkeit unseres Datenbankservers von außen (öffentliche Zugänglichkeit) und die Verfügbarkeit von Ports verantwortlich sind: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/40/z9/q7/40z9q7owar5kpnimyzrdj5laqgs.png"></div><br>  Erstellen wir eine neue Konfiguration für die VPC-Sicherheitsgruppe, mit der wir von außen über Port 5432 (PostgreSQL) auf unseren Datenbankserver zugreifen können. <br><br>  Wechseln Sie in einem separaten Browserfenster zur AWS-Konsole im Abschnitt VPC-Dashboard -&gt; Sicherheitsgruppen -&gt; Sicherheitsgruppe erstellen: <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fl/2i/ne/fl2inejlgnghwsh3itdrlcywdsu.png"></div><br>  Legen Sie den Namen für die Sicherheitsgruppe fest - PostgreSQL, eine Beschreibung, geben Sie an, welcher VPC diese Gruppe zugeordnet werden soll, und klicken Sie auf die Schaltfläche Erstellen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/js/8r/tv/js8rtvp8tudwjtpgso6xota5h-g.png"></div><br>  Füllen Sie die neu erstellte Gruppe Eingehende Regeln für Port 5432 aus (siehe Abbildung unten).  Sie müssen keinen manuellen Port angeben, sondern wählen PostgreSQL aus der Dropdown-Liste Typ aus. <br><br>  Streng genommen bedeutet der Wert :: / 0 die Verfügbarkeit von eingehendem Datenverkehr für einen Server aus der ganzen Welt, was kanonisch nicht ganz richtig ist, aber um das Beispiel zu analysieren, verwenden wir diesen Ansatz: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ge/8j/bn/ge8jbntssnooajc8so36h0tjo80.png"></div><br>  Wir kehren zur Browserseite zurück, auf der "Erweiterte Einstellungen konfigurieren" geöffnet ist und wählen Sie im Abschnitt VPC-Sicherheitsgruppen -&gt; Vorhandene VPC-Sicherheitsgruppen auswählen -&gt; PostgreSQL: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nk/ae/-s/nkae-ste1tp3wgvmyilicvwlk8e.png"></div><br>  Als nächstes im Abschnitt Datenbankoptionen -&gt; Datenbankname -&gt; den Namen <b>festlegen</b> - <b>habrDB</b> . <br><br>  Wir können den Rest der Parameter belassen, mit Ausnahme des Deaktivierens der Sicherung (Sicherungsaufbewahrungszeitraum - 0 Tage), der Überwachung und von Performance Insights.  Klicken Sie auf die Schaltfläche <b>Datenbank erstellen</b> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ex/1p/po/ex1ppogq_vdsk3nnvywm7l8vq8i.png"></div><br><h2>  Stream-Handler </h2><br>  Der letzte Schritt wird die Entwicklung von Spark-Jobs sein, die alle zwei Sekunden neue Daten von Kafka verarbeiten und das Ergebnis in die Datenbank eingeben. <br><br>  Wie oben erwähnt, sind Prüfpunkte der Hauptmechanismus in SparkStreaming, der konfiguriert werden muss, um Fehlertoleranz bereitzustellen.  Wir werden Kontrollpunkte verwenden, und im Falle eines Prozedurabbruchs muss das Spark-Streaming-Modul nur zum letzten Kontrollpunkt zurückkehren und die Berechnungen von diesem fortsetzen, um die verlorenen Daten wiederherzustellen. <br><br>  Sie können den Haltepunkt aktivieren, indem Sie das Verzeichnis in einem fehlertoleranten, zuverlässigen Dateisystem (z. B. HDFS, S3 usw.) festlegen, in dem die Haltepunktinformationen gespeichert werden.  Dies geschieht zum Beispiel mit: <br><br><pre> <code class="python hljs">streamingContext.checkpoint(checkpointDirectory)</code> </pre> <br>  In unserem Beispiel verwenden wir den folgenden Ansatz: Wenn checkpointDirectory vorhanden ist, wird der Kontext aus den Kontrollpunktdaten neu erstellt.  Wenn das Verzeichnis nicht vorhanden ist (d. H. Zum ersten Mal ausgeführt wird), wird die Funktion functionToCreateContext aufgerufen, um einen neuen Kontext zu erstellen und DStreams zu konfigurieren: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyspark.streaming <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> StreamingContext context = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext)</code> </pre><br>  Erstellen Sie ein DirectStream-Objekt, um mithilfe der createDirectStream-Methode der KafkaUtils-Bibliothek eine Verbindung zum Thema "Transaktion" herzustellen: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyspark.streaming.kafka <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> KafkaUtils sc = SparkContext(conf=conf) ssc = StreamingContext(sc, <span class="hljs-number"><span class="hljs-number">2</span></span>) broker_list = <span class="hljs-string"><span class="hljs-string">'localhost:9092'</span></span> topic = <span class="hljs-string"><span class="hljs-string">'transaction'</span></span> directKafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {<span class="hljs-string"><span class="hljs-string">"metadata.broker.list"</span></span>: broker_list})</code> </pre><br>  Analysieren eingehender Daten im JSON-Format: <br><br><pre> <code class="python hljs">rowRdd = rdd.map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> w: Row(branch=w[<span class="hljs-string"><span class="hljs-string">'branch'</span></span>], currency=w[<span class="hljs-string"><span class="hljs-string">'currency'</span></span>], amount=w[<span class="hljs-string"><span class="hljs-string">'amount'</span></span>])) testDataFrame = spark.createDataFrame(rowRdd) testDataFrame.createOrReplaceTempView(<span class="hljs-string"><span class="hljs-string">"treasury_stream"</span></span>)</code> </pre><br>  Mit Spark SQL erstellen wir eine einfache Gruppierung und drucken das Ergebnis auf der Konsole aus: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">select</span></span> from_unixtime(<span class="hljs-keyword"><span class="hljs-keyword">unix_timestamp</span></span>()) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> curr_time, t.branch <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> branch_name, t.currency <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> currency_code, <span class="hljs-keyword"><span class="hljs-keyword">sum</span></span>(amount) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> batch_value <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> treasury_stream t <span class="hljs-keyword"><span class="hljs-keyword">group</span></span> <span class="hljs-keyword"><span class="hljs-keyword">by</span></span> t.branch, t.currency</code> </pre><br>  Abfragetext abrufen und über Spark SQL ausführen: <br><br><pre> <code class="python hljs">sql_query = get_sql_query() testResultDataFrame = spark.sql(sql_query) testResultDataFrame.show(n=<span class="hljs-number"><span class="hljs-number">5</span></span>)</code> </pre><br>  Anschließend speichern wir die empfangenen aggregierten Daten in einer Tabelle in AWS RDS.  Um die Aggregationsergebnisse in einer Datenbanktabelle zu speichern, verwenden wir die Schreibmethode des DataFrame-Objekts: <br><br><pre> <code class="python hljs">testResultDataFrame.write \ .format(<span class="hljs-string"><span class="hljs-string">"jdbc"</span></span>) \ .mode(<span class="hljs-string"><span class="hljs-string">"append"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"driver"</span></span>, <span class="hljs-string"><span class="hljs-string">'org.postgresql.Driver'</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"url"</span></span>,<span class="hljs-string"><span class="hljs-string">"jdbc:postgresql://myhabrtest.ciny8bykwxeg.us-east-1.rds.amazonaws.com:5432/habrDB"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"dbtable"</span></span>, <span class="hljs-string"><span class="hljs-string">"transaction_flow"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"user"</span></span>, <span class="hljs-string"><span class="hljs-string">"habr"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"password"</span></span>, <span class="hljs-string"><span class="hljs-string">"habr12345"</span></span>) \ .save()</code> </pre><br><blockquote>  Ein paar Worte zum Einrichten einer Verbindung zu AWS RDS.  Wir haben den Benutzer und das Kennwort dafür im Schritt "Bereitstellen von AWS PostgreSQL" erstellt.  Verwenden Sie für die Datenbankserver-URL Endpoint, der im Abschnitt Konnektivität und Sicherheit angezeigt wird: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9n/sj/jd/9nsjjdun0hdy5qtwqub0xhvzunk.png"></div></blockquote><br>  Um Spark und Kafka korrekt zu verbinden, sollten Sie den Job über smark-submit mit dem <b>Artefakt spark-Streaming-kafka-0-8_2.11 ausführen</b> .  Darüber hinaus wenden wir das Artefakt auch auf die Interaktion mit der PostgreSQL-Datenbank an und übertragen es über --packages. <br><br>  Für die Flexibilität des Skripts nehmen wir auch den Namen des Nachrichtenservers und das Thema heraus, von dem wir Daten als Eingabeparameter empfangen möchten. <br><br>  Es ist also Zeit, das System zu starten und zu testen: <br><br><pre> <code class="bash hljs">spark-submit \ --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.2,\ org.postgresql:postgresql:9.4.1207 \ spark_job.py localhost:9092 transaction</code> </pre><br>  Alles hat geklappt!  Wie Sie in der Abbildung unten sehen können, werden während des Betriebs der Anwendung alle 2 Sekunden neue Aggregationsergebnisse angezeigt, da wir das Stapelintervall beim Erstellen des StreamingContext-Objekts auf 2 Sekunden festgelegt haben: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/cf/q1/25/cfq125zpzkyldktsuvdo175fazy.png"></div><br>  Als Nächstes führen wir eine einfache Abfrage an die Datenbank durch, um nach Datensätzen in der <b>Transaktion_Fluss-</b> Tabelle zu suchen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7j/j9/qm/7jj9qmf4zpter3jkbblrmiqni2s.png"></div><br><h2>  Fazit </h2><br>  In diesem Artikel wurde ein Beispiel für die Verarbeitung von Streaming-Informationen mithilfe von Spark Streaming in Verbindung mit Apache Kafka und PostgreSQL untersucht.  Mit dem Wachstum von Daten aus verschiedenen Quellen ist es schwierig, den praktischen Wert von Spark Streaming für die Erstellung von Streaming-Anwendungen und Anwendungen, die in Echtzeit arbeiten, zu überschätzen. <br><br>  Den vollständigen Quellcode finden Sie in meinem Repository auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GitHub</a> . <br><br>  Ich bin bereit, diesen Artikel gerne zu diskutieren, freue mich auf Ihre Kommentare und hoffe auf konstruktive Kritik aller betroffenen Leser. <br><br>  Ich wünsche Ihnen viel Erfolg! <br><br>  <b>PS</b> Es war ursprünglich geplant, eine lokale PostgreSQL-Datenbank zu verwenden, aber aufgrund meiner Vorliebe für AWS habe ich beschlossen, die Datenbank in die Cloud zu stellen.  Im nächsten Artikel zu diesem Thema werde ich zeigen, wie das gesamte oben in AWS beschriebene System mit AWS Kinesis und AWS EMR implementiert wird.  Folgen Sie den Nachrichten! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de451160/">https://habr.com/ru/post/de451160/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de451148/index.html">Objektorientierte Programmierung in Grafiksprachen</a></li>
<li><a href="../de451150/index.html">Fang mich, wenn du kannst. Manager-Version</a></li>
<li><a href="../de451152/index.html">Der Widerstand in der Gate-Schaltung oder wie man es richtig macht</a></li>
<li><a href="../de451154/index.html">Lokales autonomes Datenerfassungssystem (Fortsetzung)</a></li>
<li><a href="../de451158/index.html">Stromkreise. Schaltungstypen</a></li>
<li><a href="../de451162/index.html">Fehlerkorrektur - Physikalische Konstanten in der gegenwärtigen und neuen Version des Internationalen Einheitensystems (SI)</a></li>
<li><a href="../de451164/index.html">Auf der Suche nach freien Parkplätzen mit Python</a></li>
<li><a href="../de451166/index.html">Was bieten die neuen Repositories für AI- und MO-Systeme?</a></li>
<li><a href="../de451170/index.html">Jeff Bezos kündigte Pläne an, den Mond zu erobern</a></li>
<li><a href="../de451172/index.html">Julia: Funktionen und Strukturen als Funktionen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>