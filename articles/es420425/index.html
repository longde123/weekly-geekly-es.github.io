<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>游둤 游뱢 游뚺游낖 Teor칤a y pr치ctica del uso de HBase 游냏 游녤游낖 游늮</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Buenas tardes Mi nombre es Danil Lipova, nuestro equipo en Sbertech comenz칩 a usar HBase como dep칩sito de datos operativos. Durante su estudio, se gan...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Teor칤a y pr치ctica del uso de HBase</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/sberbank/blog/420425/">  Buenas tardes  Mi nombre es Danil Lipova, nuestro equipo en Sbertech comenz칩 a usar HBase como dep칩sito de datos operativos.  Durante su estudio, se gan칩 experiencia que quer칤a sistematizar y describir (esperamos que sea 칰til para muchos).  Todos los experimentos a continuaci칩n se realizaron con versiones de HBase 1.2.0-cdh5.14.2 y 2.0.0-cdh6.0.0-beta1. <br><br><ol><li>  Arquitectura general </li><li>  Escribir datos en HBASE </li><li>  Lectura de datos de HBASE </li><li>  Almacenamiento en cach칠 de datos </li><li>  Procesamiento por lotes MultiGet / MultiPut </li><li>  Estrategia para dividir tablas en regiones (derrames) </li><li>  Tolerancia a fallos, compactaci칩n y localidad de datos </li><li>  Configuraciones y rendimiento </li><li>  Prueba de carga </li><li>  Conclusiones </li></ol><a name="habracut"></a><br><h2>  1. Arquitectura general </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/y9/wa/vl/y9wavltynzhs9r_7v5sn1d8ff68.png"></div><br>  El maestro en espera escucha los latidos activos en el nodo ZooKeeper y, en caso de desaparici칩n, se hace cargo de las funciones del maestro. <br><br><h2>  2. Escribir datos en HBASE </h2><br>  Primero, considere el caso m치s simple: escribir un objeto clave-valor en una tabla determinada usando put (rowkey).  El cliente primero debe averiguar d칩nde se encuentra el servidor de regi칩n ra칤z (RRS) que almacena la hbase: meta table.  Recibe esta informaci칩n de ZooKeeper.  Luego recurre a RRS y lee la tabla hbase: meta, de la que recupera la informaci칩n que RegionServer (RS) es responsable de almacenar los datos para la clave de fila dada en la tabla que le interesa.  Para uso futuro, el cliente almacena en cach칠 la metatabla y, por lo tanto, las llamadas posteriores van m치s r치pido, directamente a RS. <br><br>  Luego, RS, despu칠s de recibir la solicitud, primero la escribe en WriteAheadLog (WAL), que es necesaria para la recuperaci칩n en caso de un bloqueo.  Luego guarda los datos en MemStore.  Este es un b칰fer en la memoria que contiene un conjunto ordenado de claves para una regi칩n determinada.  La tabla se puede dividir en regiones (particiones), cada una de las cuales contiene un conjunto disuelto de claves.  Esto permite colocar regiones en diferentes servidores para obtener un mayor rendimiento.  Sin embargo, a pesar de lo obvio de esta declaraci칩n, veremos m치s adelante que esto no funciona en todos los casos. <br><br>  Despu칠s de colocar el registro en MemStore, el cliente recibe una respuesta de que el registro se guard칩 correctamente.  Al mismo tiempo, realmente se almacena solo en el b칰fer y llega al disco solo despu칠s de un cierto per칤odo de tiempo o cuando se llena con nuevos datos. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xt/xi/p7/xtxip7moylyjdgqggsqiu8j_cm4.png"></div><br>  Cuando se realiza la operaci칩n "Eliminar", no se produce la eliminaci칩n de datos f칤sicos.  Simplemente se marcan como eliminados, y la destrucci칩n en s칤 ocurre cuando se llama a la funci칩n compacta principal, que se describe con m치s detalle en la Secci칩n 7. <br><br>  Los archivos en formato HFile se acumulan en HDFS y, de vez en cuando, se inicia el proceso compacto menor, que simplemente pega archivos peque침os en archivos m치s grandes sin eliminar nada.  Con el tiempo, esto se convierte en un problema que se manifiesta solo al leer datos (volveremos a esto m치s adelante). <br><br>  Adem치s del proceso de arranque descrito anteriormente, existe un procedimiento mucho m치s eficiente, que es probablemente el lado m치s poderoso de esta base de datos: BulkLoad.  Consiste en el hecho de que creamos HFiles de forma independiente y lo colocamos en el disco, lo que nos permite escalar perfectamente y alcanzar velocidades muy decentes.  De hecho, la limitaci칩n aqu칤 no es HBase, sino las posibilidades de hierro.  A continuaci칩n se muestran los resultados de la carga en un cl칰ster que consta de 16 RegionServers y 16 NodeManager YARN (CPU Xeon E5-2680 v4 @ 2.40GHz * 64 hilos), versi칩n HBase 1.2.0-cdh5.14.2. <br><br><img src="https://habrastorage.org/webt/ro/bu/hf/robuhfegpwqyed6gmwg7he2bmfk.png"><br><br>  Se puede ver que al aumentar el n칰mero de particiones (regiones) en la tabla, as칤 como los ejecutables Spark, obtenemos un aumento en la velocidad de descarga.  Adem치s, la velocidad depende de la cantidad de grabaci칩n.  Los bloques grandes aumentan la medici칩n de MB / seg, los peque침os en el n칰mero de registros insertados por unidad de tiempo, todas las dem치s cosas son iguales. <br><br>  Tambi칠n puede comenzar a cargar en dos tablas al mismo tiempo y duplicar la velocidad.  Se puede ver a continuaci칩n que los bloques de 10 KB se escriben en dos tablas a la vez a una velocidad de aproximadamente 600 Mb / s cada una (un total de 1275 Mb / s), lo que coincide con la velocidad de escritura de 623 MB / s en una tabla (ver No. 11 arriba) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gr/05/vp/gr05vpmhauzclwmn310erbgj22u.png"></div><br>  Pero el segundo lanzamiento con registros de 50 KB muestra que la velocidad de descarga ya est치 creciendo ligeramente, lo que indica una aproximaci칩n a los valores l칤mite.  Debe tenerse en cuenta que pr치cticamente no hay carga en HBASE en s칤, todo lo que se requiere de 칠l es primero proporcionar los datos de hbase: meta, y despu칠s de alinear HFiles, vaciar los datos de BlockCache y guardar el b칰fer MemStore en el disco si no es as칤. Vac칤o <br><br><h2>  3. Lectura de datos de HBASE </h2><br>  Si suponemos que toda la informaci칩n de hbase: meta ya tiene un cliente (consulte la secci칩n 2), la solicitud va inmediatamente al RS donde se almacena la clave deseada.  Primero la b칰squeda se realiza en MemCache.  Independientemente de si hay datos all칤 o no, la b칰squeda tambi칠n se lleva a cabo en el b칰fer BlockCache y, si es necesario, en HFiles.  Si los datos se encontraron en un archivo, se colocan en BlockCache y se devolver치n m치s r치pido en la pr칩xima solicitud.  Las b칰squedas de archivos H son relativamente r치pidas debido al uso del filtro Bloom, es decir  Despu칠s de leer una peque침a cantidad de datos, determina de inmediato si este archivo contiene la clave deseada y, de lo contrario, pasa a la siguiente. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/8-/zz/wz/8-zzwzcoed7obxmlgzxl3ihrzzw.png"></div><br>  Habiendo recibido datos de estas tres fuentes, RS forma una respuesta.  En particular, puede transferir varias versiones del objeto encontrado a la vez si el cliente solicit칩 el control de versiones. <br><br><h2>  4. Almacenamiento en cach칠 de datos </h2><br>  Las memorias intermedias MemStore y BlockCache ocupan hasta el 80% de la memoria RS asignada en el mont칩n (el resto est치 reservado para tareas de servicio RS).  Si el modo de uso t칤pico es tal que los procesos escriben e inmediatamente leen los mismos datos, entonces tiene sentido reducir BlockCache y aumentar MemStore, porque  cuando la escritura de datos en la memoria cach칠 de lectura no cae, entonces el uso de BlockCache ocurrir치 con menos frecuencia.  El b칰fer BlockCache consta de dos partes: LruBlockCache (siempre en el mont칩n) y BucketCache (generalmente fuera del mont칩n o en SSD).  BucketCache debe usarse cuando hay muchas solicitudes de lectura y no encajan en LruBlockCache, lo que conduce al trabajo activo de Garbage Collector.  Al mismo tiempo, no debe esperar un aumento radical en el rendimiento del uso de la memoria cach칠 de lectura, pero volveremos a esto en la Secci칩n 8 <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/rq/oe/nw/rqoenwgqtngb-a37gsof7sqbgn0.png"></div><br>  BlockCache es uno para todo el RS, y MemStore tiene el suyo para cada tabla (uno para cada familia de columnas). <br><br>  Como se <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">describe</a> en teor칤a, cuando la escritura de datos no ingresa en la memoria cach칠, y de hecho, dichos par치metros CACHE_DATA_ON_WRITE para la tabla y "Datos de cach칠 en escritura" para RS se establecen en falso.  Sin embargo, en la pr치ctica, si escribe datos en MemStore, luego los vac칤a en el disco (limpi치ndolo de esta manera), luego elimina el archivo resultante y luego, al ejecutar una solicitud de obtenci칩n, recibiremos los datos con 칠xito.  E incluso si deshabilita completamente BlockCache y llena la tabla con nuevos datos, luego obtenga MemStore en el disco, elim칤nelos y solicite de otra sesi칩n, a칰n se obtendr치n de alguna parte.  Entonces HBase almacena no solo datos, sino tambi칠n misteriosos rompecabezas. <br><br><pre><code class="bash hljs">hbase(main):001:0&gt; create <span class="hljs-string"><span class="hljs-string">'ns:magic'</span></span>, <span class="hljs-string"><span class="hljs-string">'cf'</span></span> Created table ns:magic Took 1.1533 seconds hbase(main):002:0&gt; put <span class="hljs-string"><span class="hljs-string">'ns:magic'</span></span>, <span class="hljs-string"><span class="hljs-string">'key1'</span></span>, <span class="hljs-string"><span class="hljs-string">'cf:c'</span></span>, <span class="hljs-string"><span class="hljs-string">'try_to_delete_me'</span></span> Took 0.2610 seconds hbase(main):003:0&gt; flush <span class="hljs-string"><span class="hljs-string">'ns:magic'</span></span> Took 0.6161 seconds hdfs dfs -mv /data/hbase/data/ns/magic/* /tmp/trash hbase(main):002:0&gt; get <span class="hljs-string"><span class="hljs-string">'ns:magic'</span></span>, <span class="hljs-string"><span class="hljs-string">'key1'</span></span> cf:c timestamp=1534440690218, value=try_to_delete_me</code> </pre> <br>  La DATOS de la cach칠 en lectura se establece en falso.  Si tiene alguna idea, bienvenido a discutir esto en los comentarios. <br><br><h2>  5. Procesamiento por lotes de datos MultiGet / MultiPut </h2><br>  El procesamiento de solicitudes individuales (Get / Put / Delete) es una operaci칩n bastante costosa, por lo que debe combinarlas tanto como sea posible en una Lista o Lista, lo que le permite obtener un aumento significativo del rendimiento.  Esto es especialmente cierto en la operaci칩n de escritura, pero cuando se lee hay el siguiente escollo.  El siguiente gr치fico muestra el tiempo de lectura de 50,000 registros de MemStore.  La lectura se realiz칩 en una secuencia y el eje horizontal muestra el n칰mero de claves en la solicitud.  Se puede ver que cuando aumenta a mil claves en una solicitud, el tiempo de ejecuci칩n disminuye, es decir  la velocidad aumenta  Sin embargo, cuando el modo MSLAB est치 activado de forma predeterminada, despu칠s de este umbral, comienza una ca칤da dram치tica en el rendimiento, y cuanto mayor es la cantidad de datos en el registro, mayor es el tiempo de ejecuci칩n. <br><br><img src="https://habrastorage.org/webt/1k/ic/hj/1kichjm1xdpxskbx7avzppmlqty.png"><br><br>  Las pruebas se realizaron en una m치quina virtual, 8 n칰cleos, HBase versi칩n 2.0.0-cdh6.0.0-beta1. <br><br>  El modo MSLAB est치 dise침ado para reducir la fragmentaci칩n del almacenamiento din치mico, que ocurre debido a la mezcla de datos de generaci칩n nueva y antigua.  Como soluci칩n al problema cuando MSLAB est치 habilitado, los datos se colocan en celdas relativamente peque침as (trozos) y se procesan en lotes.  Como resultado, cuando el volumen en el paquete de datos solicitado excede el tama침o asignado, el rendimiento cae bruscamente.  Por otro lado, desactivar este modo tampoco es aconsejable, ya que provocar치 paradas debido a GC durante los momentos de trabajo intensivo con datos.  Una buena salida es aumentar el volumen de la celda, en el caso de la escritura activa a trav칠s de la colocaci칩n simult치nea con la lectura.  Vale la pena se침alar que el problema no ocurre si, despu칠s de la grabaci칩n, ejecuta el comando flush que descarga MemStore en el disco o si la carga se realiza utilizando BulkLoad.  La siguiente tabla muestra que las consultas de los datos de MemStore de un volumen mayor (y la misma cantidad) conducen a una desaceleraci칩n.  Sin embargo, aumentar el tama침o del fragmento vuelve el tiempo de procesamiento a la normalidad. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/zb/jq/s3/zbjqs3tou2ywnnzc16p93fttbva.png"></div><br>  Adem치s de aumentar el tama침o del fragmento, la fragmentaci칩n de datos por regi칩n ayuda, es decir  Mesa dividida.  Esto lleva al hecho de que llegan menos solicitudes a cada regi칩n y si se colocan en una celda, la respuesta sigue siendo buena. <br><br><h2>  6. La estrategia de dividir tablas en regiones (corte) </h2><br>  Dado que HBase es un almacenamiento de valor clave y la partici칩n se realiza por clave, es extremadamente importante compartir datos de manera uniforme en todas las regiones.  Por ejemplo, dividir dicha tabla en tres partes dar치 como resultado que los datos se dividan en tres regiones: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/rg/4c/9d/rg4c9dm-gbtodx0dqr3quc9h6we.png"></div><br>  Sucede que esto conduce a una fuerte desaceleraci칩n si los datos cargados en el futuro se ver치n, por ejemplo, como valores largos, la mayor칤a de los cuales comienzan con el mismo d칤gito, por ejemplo: <br><br>  1000001 <br>  1000002 <br>  ... <br>  1100003 <br><br>  Dado que las claves se almacenan como una matriz de bytes, todas ellas comenzar치n de la misma manera y pertenecer치n a la misma regi칩n # 1 que almacena este rango de claves.  Hay varias estrategias divididas: <br><br>  HexStringSplit: convierte la clave en una cadena con codificaci칩n hexadecimal en el rango "00000000" =&gt; "FFFFFFFF" y la llena con ceros a la izquierda. <br><br>  UniformSplit: convierte una clave en una matriz de bytes con codificaci칩n hexadecimal en el rango "00" =&gt; "FF" y la llena con ceros a la derecha. <br><br>  Adem치s, puede especificar cualquier rango o conjunto de teclas para dividir y configurar la divisi칩n autom치tica.  Sin embargo, uno de los enfoques m치s simples y efectivos es UniformSplit y el uso de concatenaci칩n de hash, por ejemplo, un alto par de bytes para ejecutar una clave a trav칠s de la funci칩n CRC32 (rowkey) y rowkey en s칤: <br><br>  hash + rowkey <br><br>  Luego, todos los datos se distribuir치n de manera uniforme en todas las regiones.  Al leer, los primeros dos bytes simplemente se descartan y la clave original permanece.  RS tambi칠n controla la cantidad de datos y claves en la regi칩n y cuando se exceden los l칤mites, se divide autom치ticamente en pedazos. <br><br><h2>  7. Tolerancia a fallos y localidad de datos </h2><br>  Dado que solo una regi칩n es responsable de cada conjunto de claves, la soluci칩n a los problemas asociados con fallas de RS o desmantelamiento es almacenar todos los datos necesarios en HDFS.  Cuando RS falla, el maestro detecta esto a trav칠s de la ausencia de un latido en el nodo ZooKeeper.  Luego asigna la regi칩n servida a otro RS y, dado que los archivos H se almacenan en un sistema de archivos distribuido, el nuevo host los lee y contin칰a sirviendo los datos.  Sin embargo, dado que algunos de los datos pueden estar en MemStore y no tuvieron tiempo de ingresar a HFiles, los WAL, que tambi칠n se almacenan en HDFS, se utilizan para restaurar el historial de operaciones.  Despu칠s de la transferencia de los cambios, RS puede responder a las solicitudes, sin embargo, el movimiento lleva al hecho de que parte de los datos y sus procesos est치n en nodos diferentes, es decir.  Disminuci칩n de la localidad. <br><br>  La soluci칩n al problema es una compactaci칩n importante: este procedimiento mueve los archivos a los nodos que son responsables de ellos (donde se encuentran sus regiones), como resultado de lo cual la carga en la red y los discos aumenta considerablemente durante este procedimiento.  Sin embargo, en el futuro, el acceso a los datos se acelera notablemente.  Adem치s, major_compaction combina todos los archivos H en un archivo dentro de la regi칩n, y tambi칠n limpia los datos seg칰n la configuraci칩n de la tabla.  Por ejemplo, puede especificar el n칰mero de versiones de un objeto que desea guardar o su duraci칩n, despu칠s de lo cual el objeto se elimina f칤sicamente. <br><br>  Este procedimiento puede tener un efecto muy positivo en HBase.  La siguiente imagen muestra c칩mo se degrad칩 el rendimiento como resultado de la grabaci칩n activa de datos.  Aqu칤 puede ver c칩mo se escribieron 40 secuencias en una tabla y 40 secuencias leyeron datos al mismo tiempo.  Las secuencias de escritura forman cada vez m치s archivos H, que otras secuencias leen.  Como resultado, es necesario eliminar m치s y m치s datos de la memoria y al final el GC comienza a funcionar, lo que pr치cticamente paraliza todo el trabajo.  El lanzamiento de una gran compactaci칩n condujo a la limpieza de los bloqueos resultantes y la restauraci칩n del rendimiento. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/x2/ha/ga/x2haga1cohdfilxz5ffu_vatfzy.png"></div><br>  La prueba se realiz칩 en 3 DataNode y 4 RS (CPU Xeon E5-2680 v4 @ 2.40GHz * 64 hilos).  HBase Versi칩n 1.2.0-cdh5.14.2 <br><br>  Vale la pena se침alar que el lanzamiento de la compactaci칩n mayor se realiz칩 en una tabla "en vivo", en la que los datos se escribieron y leyeron activamente.  Hubo una declaraci칩n en la red de que esto podr칤a conducir a una respuesta incorrecta al leer los datos.  Para verificar, se lanz칩 un proceso que gener칩 nuevos datos y los escribi칩 en la tabla.  Despu칠s de lo cual le칤 inmediatamente y verifiqu칠 si el valor obtenido coincid칤a con lo que se registr칩.  Durante este proceso, se lanz칩 una compactaci칩n importante unas 200 veces y no se registr칩 una sola falla.  Quiz치s el problema aparece raramente y solo durante una carga alta, por lo que es m치s seguro detener de forma programada los procesos de escritura y lectura y realizar la limpieza sin permitir tales reducciones de GC. <br><br>  Adem치s, la compactaci칩n principal no afecta el estado de MemStore, para vaciarlo en el disco y compactar, debe usar flush (connection.getAdmin (). Flush (TableName.valueOf (tblName))). <br><br><h2>  8. Configuraci칩n y rendimiento </h2><br>  Como ya se mencion칩, HBase muestra el mayor 칠xito donde no necesita hacer nada al ejecutar BulkLoad.  Sin embargo, esto se aplica a la mayor칤a de los sistemas y personas.  Sin embargo, esta herramienta es m치s adecuada para el apilamiento masivo de datos en bloques grandes, mientras que si el proceso requiere muchas solicitudes de lectura y escritura de la competencia, se utilizan los comandos Get y Put descritos anteriormente.  Para determinar los par치metros 칩ptimos, se realizaron lanzamientos con varias combinaciones de par치metros y configuraciones de la tabla: <br><br><ul><li>  Se iniciaron 10 hilos al mismo tiempo 3 veces seguidas (llam칠moslo un bloque de hilos). </li><li>  El tiempo de operaci칩n de todos los flujos en el bloque se promedi칩 y fue el resultado final de la operaci칩n del bloque. </li><li>  Todos los hilos trabajaron con la misma tabla. </li><li>  Antes de cada inicio del bloque de subprocesos, se ejecut칩 una compactaci칩n importante. </li><li>  Cada bloque realiz칩 solo una de las siguientes operaciones: </li></ul><br>  - Poner <br>  - obtener <br>  - Obtener + poner <br><br><ul><li>  Cada bloque realiz칩 50,000 repeticiones de su operaci칩n. </li><li>  El tama침o del registro en el bloque es de 100 bytes, 1000 bytes o 10000 bytes (aleatorio). </li><li>  Los bloques se lanzaron con un n칰mero diferente de claves solicitadas (una clave o 10). </li><li>  Los bloques se lanzaron en varias configuraciones de tabla.  Par치metros cambiados: </li></ul><br>  - BlockCache = activado o desactivado <br>  - Tama침o de bloque = 65 Kb o 16 Kb <br>  - Particiones = 1, 5 o 30 <br>  - MSLAB = encendido o apagado <br><br>  Por lo tanto, el bloque se ve as칤: <br><br>  a.  Modo MSLAB activado / desactivado. <br>  b.  Se cre칩 una tabla para la cual se establecieron los siguientes par치metros: BlockCache = true / none, BlockSize = 65/16 Kb, Partitions = 1/5/30. <br>  c.  Establecer compresi칩n GZ. <br>  d.  Se lanzaron 10 subprocesos simult치neamente haciendo 1/10 de las operaciones put / get / get + put en esta tabla con registros de 100/1000/10000 bytes, ejecutando 50,000 consultas seguidas (claves aleatorias). <br>  e.  El punto d se repiti칩 tres veces. <br>  f.  Se promedi칩 el tiempo de funcionamiento de todos los hilos. <br><br>  Se verificaron todas las combinaciones posibles.  Es predecible que a medida que aumenta el tama침o de la grabaci칩n, la velocidad disminuir치 o que la desactivaci칩n del almacenamiento en cach칠 disminuir치.  Sin embargo, el objetivo era comprender el grado y la importancia de la influencia de cada par치metro, por lo tanto, los datos recopilados se alimentaron a la entrada de la funci칩n de regresi칩n lineal, lo que permite evaluar la confiabilidad utilizando estad칤sticas t.  A continuaci칩n se muestran los resultados de los bloques que realizan operaciones Put.  Un conjunto completo de combinaciones 2 * 2 * 3 * 2 * 3 = 144 opciones + 72 desde  algunos se realizaron dos veces.  Por lo tanto, un total de 216 lanzamientos: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ai/5u/wj/ai5uwj0fvmyo9hyqkjg4-cigceq.png"></div><br>  Las pruebas se llevaron a cabo en un mini-cluster que consta de 3 DataNode y 4 RS (CPU Xeon E5-2680 v4 @ 2.40GHz * 64 flujos).  HBase versi칩n 1.2.0-cdh5.14.2. <br><br>  La velocidad de inserci칩n m치s alta de 3.7 segundos se obtuvo cuando el modo MSLAB estaba apagado, en una tabla con una partici칩n, con BlockCache habilitado, BlockSize = 16, registros de 100 bytes de 10 piezas por paquete. <br>  La velocidad de inserci칩n m치s baja de 82.8 segundos se obtuvo cuando se habilit칩 el modo MSLAB, en una tabla con una partici칩n, con BlockCache habilitado, BlockSize = 16, registros de 10,000 bytes cada uno. <br><br>  Ahora veamos el modelo.  Vemos un modelo de buena calidad para R2, pero est치 claro que la extrapolaci칩n est치 contraindicada aqu칤.  El comportamiento real del sistema al cambiar los par치metros no ser치 lineal, este modelo no es necesario para los pron칩sticos, sino para comprender lo que sucedi칩 dentro de los par치metros dados.  Por ejemplo, aqu칤 vemos seg칰n el criterio de Student que para la operaci칩n Put, los par치metros BlockSize y BlockCache no importan (lo que generalmente es predecible): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/aq/vg/xt/aqvgxt9uyfs_l4m4crnm3adhliy.png"></div><br>  Pero el hecho de que un aumento en el n칰mero de particiones conduzca a una disminuci칩n en el rendimiento es algo inesperado (ya hemos visto el efecto positivo de un aumento en el n칰mero de particiones con BulkLoad), aunque es comprensible.  Primero, para el procesamiento, es necesario formar consultas en 30 regiones en lugar de una, y la cantidad de datos no es tal que proporcione una ganancia.  En segundo lugar, el tiempo de operaci칩n total est치 determinado por el RS m치s lento, y dado que el n칰mero de DataNode es menor que el n칰mero de RS, algunas regiones tienen cero localidades.  Bueno, echemos un vistazo a los cinco primeros: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bx/md/iu/bxmdiuzfdzqlfe1l8_s_2ktecb0.png"></div><br>  Ahora vamos a evaluar los resultados de la ejecuci칩n de los bloques Get: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/zl/2b/no/zl2bnojdyx-byfpty6yr9poebzg.png"></div><br>  El n칰mero de particiones ha perdido importancia, lo que probablemente se deba al hecho de que los datos est치n bien almacenados en cach칠 y el cach칠 de lectura es el par치metro m치s significativo (estad칤sticamente).  Naturalmente, aumentar el n칰mero de mensajes en una solicitud tambi칠n es muy 칰til para el rendimiento.  Los mejores resultados: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/f0/6d/pu/f06dpurnzlck4po4jw1xpyrphl8.png"></div><br>  Bueno, finalmente, mire el modelo del bloque que ejecut칩 primero, y luego ponga: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ae/fc/23/aefc23q9mcfbtbumdc5qjmw_qrg.png"></div><br>  Aqu칤 todos los par치metros son significativos.  Y los resultados de los l칤deres: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/q0/vo/th/q0vothoybhc8k6m1ysefviz3tco.png"></div><br><h2>  9. Prueba de carga </h2><br>  Bueno, finalmente, lanzaremos una carga m치s o menos decente, pero siempre es m치s interesante cuando hay algo para comparar.  El sitio de DataStax, un desarrollador clave de Cassandra, tiene los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">resultados de</a> NT de varios repositorios NoSQL, incluida HBase versi칩n 0.98.6-1.  La carga se realiz칩 mediante 40 flujos, tama침o de datos de 100 bytes, discos SSD.  El resultado de probar las operaciones de Lectura-Modificaci칩n-Escritura mostr칩 estos resultados. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ha/7b/bw/ha7bbwydc612f04jtwyqvdvg_ae.png"></div><br>  Seg칰n tengo entendido, la lectura se realiz칩 en bloques de 100 registros y para 16 nodos HBase, la prueba DataStax mostr칩 un rendimiento de 10 mil operaciones por segundo. <br><br>  Es una suerte que nuestro cl칰ster tambi칠n tenga 16 nodos, pero no es muy "afortunado" que cada uno tenga 64 n칰cleos (subprocesos), mientras que en la prueba DataStax solo tiene 4. Por otro lado, tienen discos SSD, y tenemos HDD y m치s La nueva versi칩n de HBase y la utilizaci칩n de la CPU durante la carga pr치cticamente no aument칩 significativamente (visualmente en un 5-10 por ciento).  Sin embargo, intentaremos comenzar con esta configuraci칩n.  Configuraci칩n de la tabla de forma predeterminada, la lectura se realiza en un rango de teclas de 0 a 50 millones al azar (es decir, cada vez que hay una nueva).  En la tabla, 50 millones de entradas se dividen en 64 particiones.  Las claves son crc32 hash.  La configuraci칩n de la tabla est치 predeterminada, MSLAB est치 habilitado.  Comenzando 40 subprocesos, cada subproceso lee un conjunto de 100 claves aleatorias e inmediatamente escribe los 100 bytes generados en estas claves. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/po/sd/el/posdel66zx7quvvo3kvrjb6uif8.png"></div><br>  Soporte: 16 DataNode y 16 RS (CPU Xeon E5-2680 v4 @ 2.40GHz * 64 flujos).  HBase versi칩n 1.2.0-cdh5.14.2. <br><br>  El resultado promedio es m치s cercano a 40 mil operaciones por segundo, lo cual es significativamente mejor que en la prueba DataStax.  Sin embargo, para los fines del experimento, las condiciones pueden cambiar ligeramente.  Es bastante improbable que todo el trabajo se realice exclusivamente con una tabla, y solo con claves 칰nicas.  Suponga que hay un cierto conjunto de teclas "activas" que genera la carga principal.  Por lo tanto, trataremos de crear una carga con registros m치s grandes (10 KB), tambi칠n en paquetes de 100 cada uno, en 4 tablas diferentes y limitando el rango de claves solicitadas a 50 mil. El siguiente gr치fico muestra el inicio de 40 hilos, cada secuencia lee un conjunto de 100 claves y escribe inmediatamente 10 al azar. KB en estas teclas de nuevo. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/f7/ec/mr/f7ecmrebgulvlcyru05c7jzytyi.png"></div><br>  Soporte: 16 DataNode y 16 RS (CPU Xeon E5-2680 v4 @ 2.40GHz * 64 flujos).  HBase versi칩n 1.2.0-cdh5.14.2. <br><br>  Durante la carga, se lanz칩 una compactaci칩n importante varias veces, como se muestra arriba sin este procedimiento, el rendimiento se degradar치 gradualmente, sin embargo, tambi칠n se produce una carga adicional durante la ejecuci칩n.  Las reducciones son causadas por varias razones.  Algunas veces los subprocesos terminaron y mientras se reiniciaron hubo una pausa, a veces las aplicaciones de terceros crearon una carga en el cl칰ster. <br><br>  Leer y escribir de inmediato es uno de los escenarios de trabajo m치s dif칤ciles para HBase.  Si solo coloca solicitudes de colocaci칩n de un tama침o peque침o, por ejemplo, 100 bytes cada una, combin치ndolas en lotes de 10-50 mil piezas, puede obtener cientos de miles de operaciones por segundo y la situaci칩n es similar con las solicitudes de solo lectura.  Vale la pena se침alar que los resultados son radicalmente mejores que los que se obtuvieron en DataStax, sobre todo debido a las solicitudes en bloques de 50 mil. <br><br><img src="https://habrastorage.org/webt/kv/_j/bv/kv_jbvizskwbod1nxapokckg9s8.png"><br>  Soporte: 16 DataNode y 16 RS (CPU Xeon E5-2680 v4 @ 2.40GHz * 64 flujos).  HBase versi칩n 1.2.0-cdh5.14.2. <br><br><h2>  10. Conclusiones </h2><br>  Este sistema es lo suficientemente flexible como para configurarlo, pero a칰n se desconoce el efecto de una gran cantidad de par치metros.  Algunos de ellos fueron probados, pero no se incluyeron en el conjunto de pruebas resultante.  Por ejemplo, los experimentos preliminares mostraron la insignificancia de un par치metro como DATA_BLOCK_ENCODING, que codifica la informaci칩n utilizando valores de celdas vecinas, lo cual es bastante comprensible para los datos generados aleatoriamente.  En el caso de utilizar una gran cantidad de objetos repetidos, la ganancia puede ser significativa.  En general, podemos decir que HBase da la impresi칩n de una base de datos bastante seria y bien pensada, que puede ser bastante productiva cuando se trata de grandes bloques de datos.  Especialmente si es posible difundir los procesos de lectura y escritura a tiempo. <br><br>  Si algo en su opini칩n no se divulga lo suficiente, estoy listo para contarle m치s.  Sugerimos compartir su experiencia o debatir si no est치 de acuerdo con algo. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es420425/">https://habr.com/ru/post/es420425/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es420409/index.html">Aprende OpenGL. Lecci칩n 5.7 - HDR</a></li>
<li><a href="../es420413/index.html">SQLite y NW.js: instrucciones paso a paso para crear fuertes amistades</a></li>
<li><a href="../es420415/index.html">Todo lo que quer칤a saber sobre probar adaptadores de Wi-Fi, pero ten칤a miedo de preguntar</a></li>
<li><a href="../es420419/index.html">Corredores para aquellos a quienes les gusta la humillaci칩n o c칩mo cambiamos y modificamos PixJam</a></li>
<li><a href="../es420423/index.html">Problemas de interfaz de cruce de tierra</a></li>
<li><a href="../es420429/index.html">USE, RED, PgBouncer, su configuraci칩n y monitoreo</a></li>
<li><a href="../es420431/index.html">Marte Gu칤a pr치ctica de terraformaci칩n para amas de casa</a></li>
<li><a href="../es420433/index.html">"Formato del viernes": caminos musicales: qu칠 es y por qu칠 no est치n en Rusia</a></li>
<li><a href="../es420435/index.html">Inicio r치pido con ARM Mbed: desarrollo en microcontroladores modernos para principiantes</a></li>
<li><a href="../es420437/index.html">Una introducci칩n pr치ctica al administrador de paquetes para Kubernetes - Helm</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>