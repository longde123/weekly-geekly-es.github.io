<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßû üìç ü•ò Identificador de raza de perro: desarrollo de ciclo completo del programa Keras a la aplicaci√≥n de Android. en el mercado de juego üë®‚Äçüíº üë©üèø‚Äçüíª üö∏</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Con el progreso reciente en las redes neuronales en general y el reconocimiento de im√°genes en particular, podr√≠a parecer que crear una aplicaci√≥n bas...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Identificador de raza de perro: desarrollo de ciclo completo del programa Keras a la aplicaci√≥n de Android. en el mercado de juego</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/447732/">  Con el progreso reciente en las redes neuronales en general y el reconocimiento de im√°genes en particular, podr√≠a parecer que crear una aplicaci√≥n basada en NN para el reconocimiento de im√°genes es una operaci√≥n de rutina simple.  Bueno, hasta cierto punto es cierto: si puedes imaginar una aplicaci√≥n de reconocimiento de im√°genes, entonces es muy probable que alguien ya haya hecho algo similar.  Todo lo que necesitas hacer es buscarlo en Google y repetir. <br><br>  Sin embargo, todav√≠a hay innumerables peque√±os detalles que ... no son insolubles, no.  Simplemente toman demasiado tiempo, especialmente si eres un principiante.  Lo que ser√≠a de ayuda es un proyecto paso a paso, hecho frente a usted, de principio a fin.  Un proyecto que no contiene "esta parte es obvia, as√≠ que saltemos" las declaraciones.  Bueno, casi :) <br><br>  En este tutorial veremos un identificador de raza de perro: crearemos y ense√±aremos una red neuronal, luego la trasladaremos a Java para Android y la publicaremos en Google Play. <br><br>  Para aquellos de ustedes que desean ver un resultado final, aqu√≠ est√° el enlace a la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aplicaci√≥n NeuroDog</a> en Google Play. <br><br>  Sitio web con mi rob√≥tica: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">robotics.snowcron.com</a> . <br>  Sitio web con: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Gu√≠a del usuario de NeuroDog</a> . <br><br>  Aqu√≠ hay una captura de pantalla del programa: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/186/b91/457/186b914572170b01446ed1d722bce200.png" alt="imagen"><br><br><a name="habracut"></a><br><br><h3>  Una visi√≥n general </h3><br><br>  Vamos a utilizar Keras: la biblioteca de Google para trabajar con redes neuronales.  Es de alto nivel, lo que significa que la curva de aprendizaje ser√° empinada, definitivamente m√°s r√°pida que con otras bibliotecas que conozco.  Familiar√≠cese con √©l: hay muchos tutoriales de alta calidad en l√≠nea. <br><br>  Utilizaremos CNNs - Redes neuronales convolucionales.  Las CNN (y las redes m√°s avanzadas basadas en ellas) son el est√°ndar de facto en el reconocimiento de im√°genes.  Sin embargo, ense√±ar uno correctamente puede convertirse en una tarea formidable: la estructura de la red, los par√°metros de aprendizaje (todas esas tasas de aprendizaje, √≠mpetu, L1 y L2, etc.) deben ajustarse cuidadosamente, y como la tarea requiere muchos recursos computacionales, nosotros No puede simplemente probar todas las combinaciones posibles. <br><br>  Esta es una de las pocas razones por las cuales en la mayor√≠a de los casos preferimos usar el "conocimiento de transferencia" al llamado enfoque "vainilla".  Transfer Knowlege utiliza una red neuronal entrenada por otra persona (piense en Google) para alguna otra tarea.  Luego eliminamos las √∫ltimas capas, agregamos capas propias ... y funciona milagros. <br><br>  Puede sonar extra√±o: tomamos la red de Google entrenada para reconocer gatos, flores y muebles, ¬°y ahora identifica la raza de perros!  Para comprender c√≥mo funciona, echemos un vistazo a la forma en que funcionan las redes neuronales profundas, incluidas las utilizadas para el reconocimiento de im√°genes. <br><br>  Le damos una imagen como entrada.  La primera capa de una red analiza la imagen en busca de patrones simples, como "l√≠nea horizontal corta", "un arco", etc.  La siguiente capa toma estos patrones (y d√≥nde est√°n ubicados en la imagen) y produce patrones de nivel superior, como "pelaje", "esquina de un ojo", etc.  Al final, tenemos un rompecabezas que se puede combinar en una descripci√≥n de un perro: pelaje, dos ojos, pierna humana en la boca, etc. <br><br>  Ahora, todo esto fue hecho por un conjunto de capas pre-entrenadas que obtuvimos (de Google o de alg√∫n otro gran jugador).  Finalmente, agregamos nuestras propias capas encima y le ense√±amos a trabajar con esos patrones para reconocer las razas de perros.  Suena l√≥gico <br><br>  Para resumir, en este tutorial vamos a crear CNN "vainilla" y un par de redes de "aprendizaje de transferencia" de diferentes tipos.  En cuanto a "vainilla": solo lo usar√© como un ejemplo de c√≥mo se puede hacer, pero no voy a ajustarlo, ya que las redes "pre-entrenadas" son mucho m√°s f√°ciles de usar.  Keras viene con pocas redes pre-entrenadas, elegir√© un par de configuraciones y las comparar√©. <br><br>  Como queremos que nuestra red neuronal pueda reconocer las razas de perros, necesitamos "mostrar" im√°genes de muestra de diferentes razas.  Afortunadamente, hay un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">gran conjunto de datos</a> creado para una tarea similar ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">original aqu√≠</a> ).  En este art√≠culo, voy a usar la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">versi√≥n de Kaggle</a> <br><br>  Luego voy a portar el "ganador" a Android.  Portar Keras NN a Android es relativamente f√°cil, y vamos a recorrer todos los pasos necesarios. <br><br>  Luego lo publicaremos en Google Play.  Como era de esperar, Google no va a cooperar, por lo que se requerir√°n pocos trucos adicionales.  Por ejemplo, nuestra red neuronal supera el tama√±o permitido de Android APK: tendremos que usar el paquete.  Adem√°s, Google no mostrar√° nuestra aplicaci√≥n en los resultados de b√∫squeda, a menos que hagamos ciertas cosas m√°gicas. <br><br>  Al final, tendremos una aplicaci√≥n Android "NN" totalmente comercial (entre comillas, ya que es gratuita pero lista para el mercado). <br><br><h3>  Entorno de desarrollo </h3><br><br>  Existen pocos enfoques diferentes para la programaci√≥n de Keras, seg√∫n el sistema operativo que use (se recomienda Ubuntu), la tarjeta de video que tenga (o no), etc.  No hay nada de malo en configurar el entorno de desarrollo en su computadora local e instalar todas las bibliotecas necesarias, etc.  Excepto ... hay una manera m√°s f√°cil. <br><br>  Primero, la instalaci√≥n y configuraci√≥n de m√∫ltiples herramientas de desarrollo lleva tiempo y tendr√° que pasar tiempo nuevamente cuando las nuevas versiones est√©n disponibles.  En segundo lugar, entrenar redes neuronales requiere mucha potencia de c√°lculo.  Puede acelerar su computadora utilizando GPU ... en el momento de escribir este art√≠culo, una GPU superior para los c√°lculos relacionados con NN cuesta 2000 - 7000 d√≥lares.  Y configurarlo tambi√©n lleva tiempo. <br><br>  Entonces vamos a utilizar un enfoque diferente.  Vea, Google permite a las personas usar sus GPU de forma gratuita para los c√°lculos relacionados con NN, tambi√©n ha creado un entorno totalmente configurado;  todos juntos se llama Google Colab.  El servicio le otorga acceso a un Jupiter Notebook con Python, Keras y toneladas de bibliotecas adicionales ya instaladas.  Todo lo que necesita hacer es obtener una cuenta de Google (obtenga una cuenta de Gmail, y tendr√° acceso a todo lo dem√°s) y eso es todo. <br><br>  En el momento de escribir este art√≠culo, se puede acceder a Colab <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">mediante este enlace</a> , pero puede cambiar.  Simplemente busque en Google "Google Colab". <br><br>  Un problema obvio con Colab es que es un servicio WEB.  ¬øC√≥mo vas a acceder a TUS archivos desde all√≠?  ¬øGuardar redes neuronales despu√©s de completar el entrenamiento, cargar datos espec√≠ficos para su tarea, etc.? <br><br>  Hay pocos (en el momento de escribir esto: tres) enfoques diferentes;  vamos a usar lo que creo que es lo mejor: usar Google Drive. <br><br>  Google Drive es un almacenamiento en la nube que funciona m√°s o menos como un disco duro, y se puede asignar a Google Colab (consulte el c√≥digo a continuaci√≥n).  Luego trabajas con √©l como lo har√≠as con un disco duro local.  Entonces, por ejemplo, si desea acceder a fotos de perros desde la Red neuronal que cre√≥ en Colab, debe cargar esas fotos en su Google Drive, eso es todo. <br><br><h2>  Creando y entrenando el NN </h2><br><br>  A continuaci√≥n, voy a recorrer el c√≥digo de Python, un bloque de c√≥digo de Jupiter Notebook tras otro.  Puede copiar ese c√≥digo en su computadora port√°til y ejecutarlo, ya que los bloques se pueden ejecutar independientemente uno del otro. <br><br><h3>  Inicializaci√≥n </h3><br><br>  En primer lugar, montemos Google Drive.  Solo dos l√≠neas de c√≥digo.  Ese c√≥digo debe ejecutarse solo una vez por sesi√≥n de Colab (por ejemplo, una vez cada seis horas de trabajo).  Si lo ejecuta la segunda vez, se omitir√° ya que la unidad ya est√° montada. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> google.colab <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> drive drive.mount(<span class="hljs-string"><span class="hljs-string">'/content/drive/'</span></span>)</code> </pre> <br><br>  La primera vez se le pedir√° que confirme el montaje, nada complicado aqu√≠.  Se ve as√≠: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>Go to this URL <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> a browser: ... &gt;&gt;&gt; Enter your authorization code: &gt;&gt;&gt; ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑ &gt;&gt;&gt; Mounted at /content/drive/</code> </pre><br><br>  Una bonita secci√≥n est√°ndar <i>incluida</i> ;  Lo m√°s probable es que algunas de las inclusiones no sean necesarias.  Adem√°s, como voy a probar diferentes configuraciones de NN, tendr√° que comentar / descomentar algunas de ellas para un tipo particular de NN: por ejemplo, para usar InceptionV3 tipo NN, descomentar InceptionV3 y comentar, por ejemplo, ResNet50.  O no: puede mantener esos comentarios sin comentar, usar√° m√°s memoria, pero eso es todo. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> datetime <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> dt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> warnings <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> backend <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> K <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> regularizers <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense, Dropout, Activation <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Flatten, Conv2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MaxPooling2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BatchNormalization, Input <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dropout, GlobalAveragePooling2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Callback, EarlyStopping <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ReduceLROnPlateau <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ModelCheckpoint <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> shutil <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.vgg16 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocess_input <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> image <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.image <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ImageDataGenerator <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> load_model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ResNet50 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocess_input <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> decode_predictions <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> inception_v3 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.inception_v3 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> InceptionV3 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.inception_v3 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocess_input <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> inception_v3_preprocessor <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.mobilenetv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MobileNetV2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.nasnet <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> NASNetMobile</code> </pre><br><br>  En Google Drive, vamos a crear una carpeta para nuestros archivos.  La segunda l√≠nea muestra su contenido: <br><br><pre> <code class="python hljs">working_path = <span class="hljs-string"><span class="hljs-string">"/content/drive/My Drive/DeepDogBreed/data/"</span></span> !ls <span class="hljs-string"><span class="hljs-string">"/content/drive/My Drive/DeepDogBreed/data"</span></span> &gt;&gt;&gt; all_images labels.csv models test train valid</code> </pre><br><br>  Como puede ver, las fotos de perros (las copiadas del conjunto de datos de Stanford (ver arriba) a Google Drive) se almacenan inicialmente en la carpeta <i>all_images</i> . M√°s adelante las <i>copiaremos a las</i> carpetas de <i>entrenamiento, validez</i> y <i>prueba</i> . Vamos a guardar modelos entrenados en la carpeta de <i>modelos</i> . En cuanto al archivo labels.csv, es parte de un conjunto de datos, asigna archivos de imagen a razas de perros. <br><br>  Hay muchas pruebas que puede ejecutar para descubrir qu√© es lo que tiene, ejecutemos solo una: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Is GPU Working? import tensorflow as tf tf.test.gpu_device_name() &gt;&gt;&gt; '/device:GPU:0'</span></span></code> </pre><br><br>  Ok, la GPU est√° conectada.  De lo contrario, b√∫squelo en la configuraci√≥n de Jupiter Notebook y act√≠velo. <br><br>  Ahora necesitamos declarar algunas constantes que vamos a utilizar, como el tama√±o de una imagen que la Red Neural deber√≠a esperar, etc.  Tenga en cuenta que usamos una imagen de 256x256, ya que es lo suficientemente grande en un lado y cabe en la memoria en el otro.  Sin embargo, algunos tipos de redes neuronales que estamos a punto de usar esperan una imagen de 224x224.  Para manejar esto, cuando sea necesario, comente el tama√±o de la imagen anterior y descomente una nueva. <br><br>  El mismo enfoque (comentario uno - descomentar el otro) se aplica a los nombres de los modelos que guardamos, simplemente porque no queremos sobrescribir el resultado de una prueba anterior cuando intentamos una nueva configuraci√≥n. <br><pre> <code class="python hljs">warnings.filterwarnings(<span class="hljs-string"><span class="hljs-string">"ignore"</span></span>) os.environ[<span class="hljs-string"><span class="hljs-string">'TF_CPP_MIN_LOG_LEVEL'</span></span>] = <span class="hljs-string"><span class="hljs-string">'2'</span></span> np.random.seed(<span class="hljs-number"><span class="hljs-number">7</span></span>) start = dt.datetime.now() BATCH_SIZE = <span class="hljs-number"><span class="hljs-number">16</span></span> EPOCHS = <span class="hljs-number"><span class="hljs-number">15</span></span> TESTING_SPLIT=<span class="hljs-number"><span class="hljs-number">0.3</span></span> <span class="hljs-comment"><span class="hljs-comment"># 70/30 % NUM_CLASSES = 120 IMAGE_SIZE = 256 #strModelFileName = "models/ResNet50.h5" # strModelFileName = "models/InceptionV3.h5" strModelFileName = "models/InceptionV3_Sgd.h5" #IMAGE_SIZE = 224 #strModelFileName = "models/MobileNetV2.h5" #IMAGE_SIZE = 224 #strModelFileName = "models/NASNetMobileSgd.h5"</span></span></code> </pre><br><br><h3>  Cargando datos </h3><br><br>  Primero, <i>carguemos el</i> archivo <i>labels.csv</i> y dividamos su contenido en partes de capacitaci√≥n y validaci√≥n.  Tenga en cuenta que todav√≠a no hay una parte de prueba, ya que voy a hacer un poco de trampa, para obtener m√°s datos para el entrenamiento. <br><br><pre> <code class="python hljs">labels = pd.read_csv(working_path + <span class="hljs-string"><span class="hljs-string">'labels.csv'</span></span>) print(labels.head()) train_ids, valid_ids = train_test_split(labels, test_size = TESTING_SPLIT) print(len(train_ids), <span class="hljs-string"><span class="hljs-string">'train ids'</span></span>, len(valid_ids), <span class="hljs-string"><span class="hljs-string">'validation ids'</span></span>) print(<span class="hljs-string"><span class="hljs-string">'Total'</span></span>, len(labels), <span class="hljs-string"><span class="hljs-string">'testing images'</span></span>) &gt;&gt;&gt; id breed &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">000</span></span>bec180eb18c7604dcecc8fe0dba07 boston_bull &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">001513</span></span>dfcb2ffafc82cccf4d8bbaba97 dingo &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">001</span></span>cdf01b096e06d78e9e5112d419397 pekinese &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">00214</span></span>f311d5d2247d5dfe4fe24b2303d bluetick &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">0021</span></span>f9ceb3235effd7fcde7f7538ed62 golden_retriever &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">7155</span></span> train ids <span class="hljs-number"><span class="hljs-number">3067</span></span> validation ids &gt;&gt;&gt; Total <span class="hljs-number"><span class="hljs-number">10222</span></span> testing images</code> </pre><br><br>  Lo siguiente, tenemos que copiar los archivos de imagen reales a las carpetas de entrenamiento / validaci√≥n / prueba, de acuerdo con la matriz de nombres de archivos que pasamos.  La siguiente funci√≥n copia archivos con nombres proporcionados a una carpeta especificada. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">copyFileSet</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(strDirFrom, strDirTo, arrFileNames)</span></span></span><span class="hljs-function">:</span></span> arrBreeds = np.asarray(arrFileNames[<span class="hljs-string"><span class="hljs-string">'breed'</span></span>]) arrFileNames = np.asarray(arrFileNames[<span class="hljs-string"><span class="hljs-string">'id'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> os.path.exists(strDirTo): os.makedirs(strDirTo) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tqdm(range(len(arrFileNames))): strFileNameFrom = strDirFrom + arrFileNames[i] + <span class="hljs-string"><span class="hljs-string">".jpg"</span></span> strFileNameTo = strDirTo + arrBreeds[i] + <span class="hljs-string"><span class="hljs-string">"/"</span></span> + arrFileNames[i] + <span class="hljs-string"><span class="hljs-string">".jpg"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> os.path.exists(strDirTo + arrBreeds[i] + <span class="hljs-string"><span class="hljs-string">"/"</span></span>): os.makedirs(strDirTo + arrBreeds[i] + <span class="hljs-string"><span class="hljs-string">"/"</span></span>) <span class="hljs-comment"><span class="hljs-comment"># As a new breed dir is created, copy 1st file # to "test" under name of that breed if not os.path.exists(working_path + "test/"): os.makedirs(working_path + "test/") strFileNameTo = working_path + "test/" + arrBreeds[i] + ".jpg" shutil.copy(strFileNameFrom, strFileNameTo) shutil.copy(strFileNameFrom, strFileNameTo)</span></span></code> </pre><br><br>  Como puede ver, solo copiamos un archivo para cada raza de perros a una carpeta de <i>prueba</i> .  A medida que copiamos archivos, tambi√©n creamos subcarpetas, una subcarpeta por cada raza de perros.  Las im√°genes para cada raza en particular se copian en su subcarpeta. <br><br>  La raz√≥n es que Keras puede trabajar con una estructura de directorios organizada de esta manera, cargando archivos de imagen seg√∫n sea necesario, ahorrando memoria.  Ser√≠a una muy mala idea cargar todas las 15,000 im√°genes en la memoria a la vez. <br><br>  Llamar a esta funci√≥n cada vez que ejecutamos nuestro c√≥digo ser√≠a una exageraci√≥n: las im√°genes ya est√°n copiadas, ¬øpor qu√© deber√≠amos copiarlas nuevamente?  Entonces, com√©ntalo despu√©s del primer uso: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Move the data in subfolders so we can # use the Keras ImageDataGenerator. # This way we can also later use Keras # Data augmentation features. # --- Uncomment once, to copy files --- #copyFileSet(working_path + "all_images/", # working_path + "train/", train_ids) #copyFileSet(working_path + "all_images/", # working_path + "valid/", valid_ids)</span></span></code> </pre><br><br>  Adem√°s, necesitamos una lista de razas de perros: <br><br><pre> <code class="python hljs">breeds = np.unique(labels[<span class="hljs-string"><span class="hljs-string">'breed'</span></span>]) map_characters = {} <span class="hljs-comment"><span class="hljs-comment">#{0:'none'} for i in range(len(breeds)): map_characters[i] = breeds[i] print("&lt;item&gt;" + breeds[i] + "&lt;/item&gt;") &gt;&gt;&gt; &lt;item&gt;affenpinscher&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;afghan_hound&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;african_hunting_dog&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;airedale&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;american_staffordshire_terrier&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;appenzeller&lt;/item&gt;</span></span></code> </pre><br><br><h3>  Procesando im√°genes </h3><br><br>  Vamos a utilizar la caracter√≠stica de Keras llamada ImageDataGenerators.  ImageDataGenerator puede procesar una imagen, cambiar su tama√±o, rotar, etc.  Tambi√©n puede tomar una funci√≥n de <i>procesamiento</i> que realiza manipulaciones de im√°genes personalizadas. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocess</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(img)</span></span></span><span class="hljs-function">:</span></span> img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) <span class="hljs-comment"><span class="hljs-comment"># or use ImageDataGenerator( rescale=1./255... img_1 = image.img_to_array(img) img_1 = cv2.resize(img_1, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) img_1 = np.expand_dims(img_1, axis=0) / 255. #img = cv2.blur(img,(5,5)) return img_1[0]</span></span></code> </pre><br><br>  Tenga en cuenta la siguiente l√≠nea: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># or use ImageDataGenerator( rescale=1./255...</span></span></code> </pre><br><br>  Podemos realizar la normalizaci√≥n (ajustando el rango de 0-255 del canal de imagen a 0-1) en ImageDataGenerator.  Entonces, ¬øpor qu√© necesitar√≠amos un preprocesador?  Como ejemplo, he proporcionado la funci√≥n de <i>desenfoque</i> (comentado): que es una manipulaci√≥n de imagen personalizada.  Puede usar cualquier cosa, desde afilar hasta HDR aqu√≠. <br><br>  Vamos a utilizar dos ImageDataGenerators diferentes, uno para capacitaci√≥n y otro para validaci√≥n.  La diferencia es que necesitamos rotaciones y zoom para el entrenamiento, para que las im√°genes sean m√°s "diversas", pero no lo necesitamos para la validaci√≥n (no en esta tarea). <br><br><pre> <code class="python hljs">train_datagen = ImageDataGenerator( preprocessing_function=preprocess, <span class="hljs-comment"><span class="hljs-comment">#rescale=1./255, # done in preprocess() # randomly rotate images (degrees, 0 to 30) rotation_range=30, # randomly shift images horizontally # (fraction of total width) width_shift_range=0.3, height_shift_range=0.3, # randomly flip images horizontal_flip=True, ,vertical_flip=False, zoom_range=0.3) val_datagen = ImageDataGenerator( preprocessing_function=preprocess) train_gen = train_datagen.flow_from_directory( working_path + "train/", batch_size=BATCH_SIZE, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, class_mode="categorical") val_gen = val_datagen.flow_from_directory( working_path + "valid/", batch_size=BATCH_SIZE, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, class_mode="categorical")</span></span></code> </pre><br><br><h3>  Crear red neuronal </h3><br><br>  Como se mencion√≥ anteriormente, vamos a crear algunos tipos de redes neuronales.  Cada vez que usamos una funci√≥n diferente, se incluye una biblioteca diferente y, en algunos casos, diferentes tama√±os de imagen.  Por lo tanto, para cambiar de un tipo de red neuronal a otra, debe comentar / descomentar el c√≥digo correspondiente. <br><br>  Primero, creemos la CNN "vainilla".  Funciona mal, ya que no lo he optimizado, pero al menos proporciona un marco que podr√≠a usar para crear una red propia (en general, es una mala idea, ya que hay redes previamente capacitadas disponibles). <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelVanilla</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> model = Sequential() <span class="hljs-comment"><span class="hljs-comment"># Note the (7, 7) here. This is one of technics # used to reduce memory use by the NN: we scan # the image in a larger steps. # Also note regularizers.l2: this technic is # used to prevent overfitting. The "0.001" here # is an empirical value and can be optimized. model.add(Conv2D(16, (7, 7), padding='same', use_bias=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), kernel_regularizer=regularizers.l2(0.001))) # Note the use of a standard CNN building blocks: # Conv2D - BatchNormalization - Activation # MaxPooling2D - Dropout # The last two are used to avoid overfitting, also, # MaxPooling2D reduces memory use. model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(16, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(32, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(32, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(64, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(64, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(128, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(128, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(256, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(256, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) # This is the end on "convolutional" part of CNN. # Now we need to transform multidementional # data into one-dim. array for a fully-connected # classifier: model.add(Flatten()) # And two layers of classifier itself (plus an # Activation layer in between): model.add(Dense(NUM_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(0.01))) model.add(Activation("relu")) model.add(Dense(NUM_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(0.01))) # We need to compile the resulting network. # Note that there are few parameters we can # try here: the best performing one is uncommented, # the rest is commented out for your reference. #model.compile(optimizer='rmsprop', # loss='categorical_crossentropy', # metrics=['accuracy']) #model.compile( # optimizer=keras.optimizers.RMSprop(lr=0.0005), # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) #model.compile(optimizer='adadelta', # loss='categorical_crossentropy', # metrics=['accuracy']) #opt = keras.optimizers.Adadelta(lr=1.0, # rho=0.95, epsilon=0.01, decay=0.01) #model.compile(optimizer=opt, # loss='categorical_crossentropy', # metrics=['accuracy']) #opt = keras.optimizers.RMSprop(lr=0.0005, # rho=0.9, epsilon=None, decay=0.0001) #model.compile(optimizer=opt, # loss='categorical_crossentropy', # metrics=['accuracy']) # model.summary() return(model)</span></span></code> </pre><br><br>  Cuando creamos una red neuronal usando el <i>aprendizaje de transferencia</i> , el procedimiento cambia: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelMobileNetV2</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># First, create the NN and load pre-trained # weights for it ('imagenet') # Note that we are not loading last layers of # the network (include_top=False), as we are # going to add layers of our own: base_model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)) # Then attach our layers at the end. These are # to build "classifier" that makes sense of # the patterns previous layers provide: x = base_model.output x = Dense(512)(x) x = Activation('relu')(x) x = Dropout(0.5)(x) predictions = Dense(NUM_CLASSES, activation='softmax')(x) # Create a model model = Model(inputs=base_model.input, outputs=predictions) # We need to make sure that pre-trained # layers are not changed when we train # our classifier: # Either this: #model.layers[0].trainable = False # or that: for layer in base_model.layers: layer.trainable = False # As always, there are different possible # settings, I tried few and chose the best: # model.compile(optimizer='adam', # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br>  Crear otros tipos de NN pre-entrenados es muy similar: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelResNet50</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> base_model = ResNet50(weights=<span class="hljs-string"><span class="hljs-string">'imagenet'</span></span>, include_top=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, pooling=<span class="hljs-string"><span class="hljs-string">'avg'</span></span>, input_shape=(IMAGE_SIZE, IMAGE_SIZE, <span class="hljs-number"><span class="hljs-number">3</span></span>)) x = base_model.output x = Dense(<span class="hljs-number"><span class="hljs-number">512</span></span>)(x) x = Activation(<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) x = Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)(x) predictions = Dense(NUM_CLASSES, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)(x) model = Model(inputs=base_model.input, outputs=predictions) <span class="hljs-comment"><span class="hljs-comment">#model.layers[0].trainable = False # model.compile(loss='categorical_crossentropy', # optimizer='adam', metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br>  A la atenci√≥n de: el ganador!  Este NN demostr√≥ los mejores resultados: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelInceptionV3</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># model.layers[0].trainable = False # model.compile(optimizer='sgd', # loss='categorical_crossentropy', # metrics=['accuracy']) base_model = InceptionV3(weights = 'imagenet', include_top = False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)) x = base_model.output x = GlobalAveragePooling2D()(x) x = Dense(512, activation='relu')(x) predictions = Dense(NUM_CLASSES, activation='softmax')(x) model = Model(inputs = base_model.input, outputs = predictions) for layer in base_model.layers: layer.trainable = False # model.compile(optimizer='adam', # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br>  Uno mas: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelNASNetMobile</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># model.layers[0].trainable = False # model.compile(optimizer='sgd', # loss='categorical_crossentropy', # metrics=['accuracy']) base_model = NASNetMobile(weights = 'imagenet', include_top = False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)) x = base_model.output x = GlobalAveragePooling2D()(x) x = Dense(512, activation='relu')(x) predictions = Dense(NUM_CLASSES, activation='softmax')(x) model = Model(inputs = base_model.input, outputs = predictions) for layer in base_model.layers: layer.trainable = False # model.compile(optimizer='adam', # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br>  Se utilizan diferentes tipos de NN en diferentes situaciones.  Adem√°s de los problemas de precisi√≥n, el tama√±o importa (el NN m√≥vil es 5 veces m√°s peque√±o que el inicio uno) y la velocidad (si necesitamos un an√°lisis en tiempo real de una transmisi√≥n de video, podr√≠amos tener que sacrificar la precisi√≥n). <br><br><h3>  Entrenando la red neuronal </h3><br><br>  En primer lugar, estamos <i>experimentando</i> , por lo que debemos poder eliminar los NN que hemos guardado anteriormente, pero que ya no necesitamos.  La siguiente funci√≥n elimina NN si el archivo existe: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Make sure that previous "best network" is deleted. def deleteSavedNet(best_weights_filepath): if(os.path.isfile(best_weights_filepath)): os.remove(best_weights_filepath) print("deleteSavedNet():File removed") else: print("deleteSavedNet():No file to remove")</span></span></code> </pre><br><br>  La forma en que creamos y eliminamos NN es sencilla.  Primero, lo eliminamos.  Ahora, si no desea llamar a <i>eliminar</i> , solo recuerde que Jupiter Notebook tiene una funci√≥n de "ejecutar selecci√≥n": seleccione solo lo que necesita y ejec√∫telo. <br><br>  Luego creamos el NN si su archivo no existe o lo <i>cargamos</i> si el archivo existe: por supuesto, no podemos llamar "eliminar" y luego esperar que exista el NN, por lo que para usar la red previamente guardada, no llame a <i>eliminar</i> . <br><br>  En otras palabras, podemos crear un nuevo NN o usar uno existente, dependiendo de lo que estemos experimentando en este momento.  Un escenario simple: hemos entrenado a la NN, luego nos fuimos de vacaciones.  Google nos desconect√≥, por lo que debemos volver a cargar el NN: comentar la parte "eliminar" y descomentar la parte "cargar". <br><br><pre> <code class="python hljs">deleteSavedNet(working_path + strModelFileName) <span class="hljs-comment"><span class="hljs-comment">#if not os.path.exists(working_path + "models"): # os.makedirs(working_path + "models") # #if not os.path.exists(working_path + # strModelFileName): # model = createModelResNet50() model = createModelInceptionV3() # model = createModelMobileNetV2() # model = createModelNASNetMobile() #else: # model = load_model(working_path + strModelFileName)</span></span></code> </pre><br><br>  <b>Los puntos de control</b> son muy importantes cuando se ense√±an las NN.  Puede crear una serie de funciones que se llamar√°n al final de cada √©poca de entrenamiento, por ejemplo, puede guardar el NN si muestra mejores resultados que el √∫ltimo guardado. <br><br><pre> <code class="python hljs">checkpoint = ModelCheckpoint(working_path + strModelFileName, monitor=<span class="hljs-string"><span class="hljs-string">'val_acc'</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">1</span></span>, save_best_only=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'auto'</span></span>, save_weights_only=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) callbacks_list = [ checkpoint ]</code> </pre><br><br>  Finalmente, ense√±aremos nuestro NN usando el conjunto de entrenamiento: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Calculate sizes of training and validation sets STEP_SIZE_TRAIN=train_gen.n//train_gen.batch_size STEP_SIZE_VALID=val_gen.n//val_gen.batch_size # Set to False if we are experimenting with # some other part of code, use history that # was calculated before (and is still in # memory bDoTraining = True if bDoTraining == True: # model.fit_generator does the actual training # Note the use of generators and callbacks # that were defined earlier history = model.fit_generator(generator=train_gen, steps_per_epoch=STEP_SIZE_TRAIN, validation_data=val_gen, validation_steps=STEP_SIZE_VALID, epochs=EPOCHS, callbacks=callbacks_list) # --- After fitting, load the best model # This is important as otherwise we'll # have the LAST model loaded, not necessarily # the best one. model.load_weights(working_path + strModelFileName) # --- Presentation part # summarize history for accuracy plt.plot(history.history['acc']) plt.plot(history.history['val_acc']) plt.title('model accuracy') plt.ylabel('accuracy') plt.xlabel('epoch') plt.legend(['acc', 'val_acc'], loc='upper left') plt.show() # summarize history for loss plt.plot(history.history['loss']) plt.plot(history.history['val_loss']) plt.title('model loss') plt.ylabel('loss') plt.xlabel('epoch') plt.legend(['loss', 'val_loss'], loc='upper left') plt.show() # As grid optimization of NN would take too long, # I did just few tests with different parameters. # Below I keep results, commented out, in the same # code. As you can see, Inception shows the best # results: # Inception: # adam: val_acc 0.79393 # sgd: val_acc 0.80892 # Mobile: # adam: val_acc 0.65290 # sgd: Epoch 00015: val_acc improved from 0.67584 to 0.68469 # sgd-30 epochs: 0.68 # NASNetMobile, adam: val_acc did not improve from 0.78335 # NASNetMobile, sgd: 0.8</span></span></code> </pre><br><br>  Aqu√≠ hay tablas de precisi√≥n y p√©rdida para el ganador NN: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f0e/97d/9cc/f0e97d9ccdc8f8ed9e44ddba02cf1f8d.png"><br><img src="https://habrastorage.org/getpro/habr/post_images/612/e09/8b0/612e098b088979768d1cc66c2f6972bc.png"><br><br>  Como puede ver, la red aprende bien. <br><br><h3>  Probar la red neuronal </h3><br><br>  Una vez completada la fase de capacitaci√≥n, debemos realizar pruebas;  para hacerlo, NN se presenta con im√°genes que nunca vio.  Como recordar√°, hemos reservado una imagen para cada especie de perro. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># --- Test j = 0 # Final cycle performs testing on the entire # testing set. for file_name in os.listdir( working_path + "test/"): img = image.load_img(working_path + "test/" + file_name); img_1 = image.img_to_array(img) img_1 = cv2.resize(img_1, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) img_1 = np.expand_dims(img_1, axis=0) / 255. y_pred = model.predict_on_batch(img_1) # get 5 best predictions y_pred_ids = y_pred[0].argsort()[-5:][::-1] print(file_name) for i in range(len(y_pred_ids)): print("\n\t" + map_characters[y_pred_ids[i]] + " (" + str(y_pred[0][y_pred_ids[i]]) + ")") print("--------------------\n") j = j + 1</span></span></code> </pre><br><br><h3>  Exportando NN a Java </h3><br><br>  Primero, necesitamos cargar el NN.  La raz√≥n es que exportar es un bloque de c√≥digo separado, por lo que es probable que lo ejecutemos por separado, sin volver a entrenar el NN.  Cuando usa mi c√≥digo, realmente no le importa, pero si hiciera su propio desarrollo, tratar√≠a de evitar volver a entrenar <i>la misma</i> red una y otra vez. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Test: load and run model = load_model(working_path + strModelFileName)</span></span></code> </pre><br><br>  Por la misma raz√≥n, esto es de alguna manera un bloque de c√≥digo separado, estamos usando inclusiones adicionales aqu√≠.  Nada nos impide subirlos, por supuesto: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> load_model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf</code> </pre><br><br>  Un poco de prueba, solo para asegurarnos de que hemos cargado todo bien: <br><br><pre> <code class="python hljs">img = image.load_img(working_path + <span class="hljs-string"><span class="hljs-string">"test/affenpinscher.jpg"</span></span>) <span class="hljs-comment"><span class="hljs-comment">#basset.jpg") img_1 = image.img_to_array(img) img_1 = cv2.resize(img_1, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) img_1 = np.expand_dims(img_1, axis=0) / 255. y_pred = model.predict(img_1) Y_pred_classes = np.argmax(y_pred,axis = 1) # print(y_pred) fig, ax = plt.subplots() ax.imshow(img) ax.axis('off') ax.set_title(map_characters[Y_pred_classes[0]]) plt.show()</span></span></code> </pre><br><br><img src="https://habrastorage.org/getpro/habr/post_images/05c/032/846/05c03284674e4337a2e5a3ba617634dd.png" alt="imagen"><br><br>  Lo siguiente, necesitamos obtener nombres de las capas de entrada y salida de nuestra red (a menos que usemos el par√°metro "nombre" al crear la red, lo cual no hicimos). <br><br><pre> <code class="python hljs">model.summary() &gt;&gt;&gt; Layer (type) &gt;&gt;&gt; ====================== &gt;&gt;&gt; input_7 (InputLayer) &gt;&gt;&gt; ______________________ &gt;&gt;&gt; conv2d_283 (Conv2D) &gt;&gt;&gt; ______________________ &gt;&gt;&gt; ... &gt;&gt;&gt; dense_14 (Dense) &gt;&gt;&gt; ====================== &gt;&gt;&gt; Total params: <span class="hljs-number"><span class="hljs-number">22</span></span>,<span class="hljs-number"><span class="hljs-number">913</span></span>,<span class="hljs-number"><span class="hljs-number">432</span></span> &gt;&gt;&gt; Trainable params: <span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">110</span></span>,<span class="hljs-number"><span class="hljs-number">648</span></span> &gt;&gt;&gt; Non-trainable params: <span class="hljs-number"><span class="hljs-number">21</span></span>,<span class="hljs-number"><span class="hljs-number">802</span></span>,<span class="hljs-number"><span class="hljs-number">784</span></span></code> </pre><br><br>  Usaremos nombres de la capa de entrada y salida m√°s adelante, al importar el NN en la aplicaci√≥n Java de Android. <br><br>  Tambi√©n podemos usar el siguiente c√≥digo para obtener esta informaci√≥n: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">print_graph_nodes</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> g = tf.GraphDef() g.ParseFromString(open(filename, <span class="hljs-string"><span class="hljs-string">'rb'</span></span>).read()) print() print(filename) print(<span class="hljs-string"><span class="hljs-string">"=======================INPUT==================="</span></span>) print([n <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> g.node <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n.name.find(<span class="hljs-string"><span class="hljs-string">'input'</span></span>) != <span class="hljs-number"><span class="hljs-number">-1</span></span>]) print(<span class="hljs-string"><span class="hljs-string">"=======================OUTPUT=================="</span></span>) print([n <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> g.node <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n.name.find(<span class="hljs-string"><span class="hljs-string">'output'</span></span>) != <span class="hljs-number"><span class="hljs-number">-1</span></span>]) print(<span class="hljs-string"><span class="hljs-string">"===================KERAS_LEARNING=============="</span></span>) print([n <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> g.node <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n.name.find(<span class="hljs-string"><span class="hljs-string">'keras_learning_phase'</span></span>) != <span class="hljs-number"><span class="hljs-number">-1</span></span>]) print(<span class="hljs-string"><span class="hljs-string">"==============================================="</span></span>) print() <span class="hljs-comment"><span class="hljs-comment">#def get_script_path(): # return os.path.dirname(os.path.realpath(sys.argv[0]))</span></span></code> </pre><br><br>  Sin embargo, se prefiere el primer enfoque. <br><br>  La siguiente funci√≥n exporta Keras Neural Network a formato <i>pb</i> , una que vamos a usar en Android. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">keras_to_tensorflow</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(keras_model, output_dir, model_name,out_prefix=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">"output_"</span></span></span></span><span class="hljs-function"><span class="hljs-params">, log_tensorboard=True)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> os.path.exists(output_dir) == <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>: os.mkdir(output_dir) out_nodes = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(keras_model.outputs)): out_nodes.append(out_prefix + str(i + <span class="hljs-number"><span class="hljs-number">1</span></span>)) tf.identity(keras_model.output[i], out_prefix + str(i + <span class="hljs-number"><span class="hljs-number">1</span></span>)) sess = K.get_session() <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.python.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> graph_util <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.python.framework graph_io init_graph = sess.graph.as_graph_def() main_graph = graph_util.convert_variables_to_constants( sess, init_graph, out_nodes) graph_io.write_graph(main_graph, output_dir, name=model_name, as_text=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> log_tensorboard: <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.python.tools <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> import_pb_to_tensorboard import_pb_to_tensorboard.import_to_tensorboard( os.path.join(output_dir, model_name), output_dir)</code> </pre><br><br><p>  Usemos estas funciones para crear un NN exportado: <br><br></p><pre> <code class="python hljs">model = load_model(working_path + strModelFileName) keras_to_tensorflow(model, output_dir=working_path + strModelFileName, model_name=working_path + <span class="hljs-string"><span class="hljs-string">"models/dogs.pb"</span></span>) print_graph_nodes(working_path + <span class="hljs-string"><span class="hljs-string">"models/dogs.pb"</span></span>)</code> </pre><br><br>  La √∫ltima l√≠nea imprime la estructura de nuestro NN. <br><br><h2>  Crear una aplicaci√≥n de Android con NN </h2><br><br>  Exportando NN a la aplicaci√≥n de Android.  est√° bien formalizado y no debe presentar ninguna dificultad.  Hay, como siempre, m√°s de una forma de hacerlo;  vamos a usar los m√°s populares (al menos por el momento). <br><br>  En primer lugar, use Android Studio para crear un nuevo proyecto.  Vamos a cortar un poco las esquinas, por lo que solo contendr√° una sola actividad. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6b3/76e/997/6b376e997b34f45359c46923f6613d60.png" alt="imagen"><br><br>  Como puede ver, hemos agregado la carpeta "activos" y copiamos nuestro archivo de red neuronal all√≠. <br><br><h3>  Archivo Gradle </h3><br><br>  Hay un par de cambios que debemos hacer para gradle file.  En primer lugar, tenemos que importar la biblioteca <i>tensorflow-android</i> .  Se utiliza para manejar Tensorflow (y Keras, en consecuencia) desde Java: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a16/091/fab/a16091fab2166f834827812611142d26.png" alt="imagen"><br><br>  Como detalle adicional "dif√≠cil de encontrar", tenga en cuenta las versiones: <i>versionCode</i> y <i>versionName</i> .  Mientras trabaja en su aplicaci√≥n, deber√° cargar nuevas versiones a Google Play.  Sin actualizar las versiones (algo as√≠ como 1 -&gt; 2 -&gt; 3 ...) no podr√° hacerlo. <br><br><h3>  Manifiesto </h3><br><br>  En primer lugar, nuestra aplicaci√≥n.  va a ser "pesado": una red neuronal de 100 Mb cabe f√°cilmente en la memoria de los tel√©fonos modernos, pero abrir una instancia por separado cada vez que el usuario "comparte" una imagen de Facebook definitivamente no es una buena idea. <br><br>  As√≠ que nos aseguraremos de que solo haya una instancia de nuestra aplicaci√≥n: <br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">activity</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">".MainActivity"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:launchMode</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"singleTask"</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre><br><br>  Al agregar <i>android: launchMode = "singleTask"</i> a MainActivity, le decimos a Android que abra una aplicaci√≥n existente, en lugar de lanzar otra instancia. <br><br>  Luego nos aseguramos de nuestra aplicaci√≥n.  aparece en una lista de aplicaciones capaces de manejar im√°genes <i>compartidas</i> : <br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">intent-filter</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-comment"><span class="hljs-comment">&lt;!-- Send action required to display activity in share list --&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">action</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.intent.action.SEND"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-comment"><span class="hljs-comment">&lt;!-- Make activity default to launch --&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">category</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.intent.category.DEFAULT"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-comment"><span class="hljs-comment">&lt;!-- Mime type ie what can be shared with this activity only image and text --&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">data</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:mimeType</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"image/*"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">intent-filter</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre><br><br>  Finalmente, necesitamos solicitar caracter√≠sticas y permisos, para que la aplicaci√≥n pueda acceder a la funcionalidad del sistema que requiere: <br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">uses-feature</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.hardware.camera"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:required</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"true"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">uses-permission</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">= </span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.permission.WRITE_EXTERNAL_STORAGE"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">uses-permission</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.permission.READ_PHONE_STATE"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">tools:node</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"remove"</span></span></span><span class="hljs-tag"> /&gt;</span></span></code> </pre><br><br>  Si est√° familiarizado con la programaci√≥n de Android, esta parte no debe plantear preguntas. <br><br><h3>  Dise√±o de la aplicaci√≥n. </h3><br><br>  Vamos a crear dos dise√±os, uno para el modo vertical y otro para el modo horizontal.  Aqu√≠ est√° el <a href="">dise√±o</a> del <a href="">retrato</a> . <br><br>  Lo que tenemos aqu√≠: una vista grande para mostrar una imagen, una lista bastante molesta de comerciales (que se muestra cuando se presiona el bot√≥n "hueso"), botones "Ayuda", botones para cargar una imagen desde Archivo / Galer√≠a y desde C√°mara, y finalmente, un bot√≥n (inicialmente oculto) "Proceso". <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f71/882/81f/f7188281ff581965c20c7e818cb0fd77.png" alt="imagen"><br><br>  En la actividad en s√≠, implementaremos algunos botones l√≥gicos que muestran / ocultan y habilitan / deshabilitan dependiendo del estado de la aplicaci√≥n. <br><br><h3>  Actividad principal </h3><br><br>  La actividad extiende una actividad est√°ndar de Android: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">MainActivity</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Activity</span></span></span></span></code> </pre><br><br>  Echemos un vistazo al c√≥digo responsable de las operaciones NN. <br><br>  En primer lugar, NN acepta un mapa de bits.  Originalmente es un mapa de bits grande desde un archivo o c√°mara (m_bitmap), luego lo transformamos en un mapa de bits est√°ndar de 256x256 (m_bitmapForNn).  Tambi√©n mantenemos las dimensiones de la imagen (256) en una constante: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">static</span></span> Bitmap m_bitmap = <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> Bitmap m_bitmapForNn = <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> m_nImageSize = <span class="hljs-number"><span class="hljs-number">256</span></span>;</code> </pre><br><br>  Necesitamos decirle al NN cu√°les son los nombres de las capas de entrada y salida;  Si consulta la lista anterior, encontrar√° que los nombres son (¬°en nuestro caso! ¬°Su caso puede ser diferente!): <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> String INPUT_NAME = <span class="hljs-string"><span class="hljs-string">"input_7_1"</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> String OUTPUT_NAME = <span class="hljs-string"><span class="hljs-string">"output_1"</span></span>;</code> </pre><br><br>  Luego declaramos la variable para contener el objeto TensofFlow.  Adem√°s, almacenamos la ruta al archivo NN en los activos: <br><br><p></p><pre> private TensorFlowInferenceInterface tf;
 Cadena privada MODEL_PATH = 
	 "archivo: ///android_asset/dogs.pb";
</pre><br><br>  Razas de perros, para presentar al usuario una informaci√≥n significativa, en lugar de √≠ndices en la matriz: <br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> String[] m_arrBreedsArray;</code> </pre><br><br>  Inicialmente, cargamos un mapa de bits.  Sin embargo, NN en s√≠ mismo espera un conjunto de valores RGB, y su salida es un conjunto de probabilidades de que la imagen presentada sea una raza particular.  Por lo tanto, debemos agregar dos matrices m√°s (tenga en cuenta que 120 es el n√∫mero de razas en nuestro conjunto de datos de entrenamiento): <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>[] m_arrPrediction = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>[<span class="hljs-number"><span class="hljs-number">120</span></span>]; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>[] m_arrInput = <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>;</code> </pre><br><br>  Cargue la biblioteca de inferencia de tensorflow <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">static</span></span> { System.loadLibrary(<span class="hljs-string"><span class="hljs-string">"tensorflow_inference"</span></span>); }</code> </pre><br><br>  Como la operaci√≥n de NN es larga, necesitamos realizarla en un hilo separado, de lo contrario hay una buena posibilidad de llegar a la aplicaci√≥n del sistema.  advertencia "no responde", sin mencionar que arruina la experiencia del usuario. <br><br><pre> <code class="java hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">PredictionTask</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">AsyncTask</span></span></span><span class="hljs-class">&lt;</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Void</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Void</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Void</span></span></span><span class="hljs-class">&gt; </span></span>{ <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">protected</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onPreExecute</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">super</span></span>.onPreExecute(); } <span class="hljs-comment"><span class="hljs-comment">// --- @Override protected Void doInBackground(Void... params) { try { # We get RGB values packed in integers # from the Bitmap, then break those # integers into individual triplets m_arrInput = new float[ m_nImageSize * m_nImageSize * 3]; int[] intValues = new int[ m_nImageSize * m_nImageSize]; m_bitmapForNn.getPixels(intValues, 0, m_nImageSize, 0, 0, m_nImageSize, m_nImageSize); for (int i = 0; i &lt; intValues.length; i++) { int val = intValues[i]; m_arrInput[i * 3 + 0] = ((val &gt;&gt; 16) &amp; 0xFF) / 255f; m_arrInput[i * 3 + 1] = ((val &gt;&gt; 8) &amp; 0xFF) / 255f; m_arrInput[i * 3 + 2] = (val &amp; 0xFF) / 255f; } // --- tf = new TensorFlowInferenceInterface( getAssets(), MODEL_PATH); //Pass input into the tensorflow tf.feed(INPUT_NAME, m_arrInput, 1, m_nImageSize, m_nImageSize, 3); //compute predictions tf.run(new String[]{OUTPUT_NAME}, false); //copy output into PREDICTIONS array tf.fetch(OUTPUT_NAME, m_arrPrediction); } catch (Exception e) { e.getMessage(); } return null; } // --- @Override protected void onPostExecute(Void result) { super.onPostExecute(result); // --- enableControls(true); // --- tf = null; m_arrInput = null; # strResult contains 5 lines of text # with most probable dog breeds and # their probabilities m_strResult = ""; # What we do below is sorting the array # by probabilities (using map) # and getting in reverse order) the # first five entries TreeMap&lt;Float, Integer&gt; map = new TreeMap&lt;Float, Integer&gt;( Collections.reverseOrder()); for(int i = 0; i &lt; m_arrPrediction.length; i++) map.put(m_arrPrediction[i], i); int i = 0; for (TreeMap.Entry&lt;Float, Integer&gt; pair : map.entrySet()) { float key = pair.getKey(); int idx = pair.getValue(); String strBreed = m_arrBreedsArray[idx]; m_strResult += strBreed + ": " + String.format("%.6f", key) + "\n"; i++; if (i &gt; 5) break; } m_txtViewBreed.setVisibility(View.VISIBLE); m_txtViewBreed.setText(m_strResult); } }</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> En onCreate () de MainActivity, necesitamos agregar onClickListener para el bot√≥n "Proceso": </font></font><br><br><pre> <code class="java hljs">m_btn_process.setOnClickListener(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> View.OnClickListener() { <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onClick</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(View v)</span></span></span><span class="hljs-function"> </span></span>{ processImage(); } });</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Lo que hace processImage () es simplemente llamar al hilo que vimos arriba: </font></font><br><br><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">processImage</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> { enableControls(<span class="hljs-keyword"><span class="hljs-keyword">false</span></span>); <span class="hljs-comment"><span class="hljs-comment">// --- PredictionTask prediction_task = new PredictionTask(); prediction_task.execute(); } catch (Exception e) { e.printStackTrace(); } }</span></span></code> </pre><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Detalles adicionales </font></font></h3><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No vamos a discutir el c√≥digo relacionado con la interfaz de usuario en este tutorial, ya que es trivial y definitivamente no es parte de la tarea "portar NN". </font><font style="vertical-align: inherit;">Sin embargo, hay pocas cosas que deber√≠an aclararse. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cuando prevenimos nuestra aplicaci√≥n. </font><font style="vertical-align: inherit;">desde el lanzamiento de varias instancias, hemos evitado, al mismo tiempo, un flujo normal de control: si comparte una imagen de Facebook y luego comparte otra, la aplicaci√≥n no se reiniciar√°. </font><font style="vertical-align: inherit;">Significa que la forma "tradicional" de manejar datos compartidos al capturarlos en onCreate no es suficiente en nuestro caso, ya que onCreate no se llama en un escenario que acabamos de crear. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aqu√≠ hay una manera de manejar la situaci√≥n: </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1. En onCreate of MainActivity, llame a la funci√≥n onSharedIntent:</font></font><br><br><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">protected</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onCreate</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( Bundle savedInstanceState)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">super</span></span>.onCreate(savedInstanceState); .... onSharedIntent(); ....</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Adem√°s, agregue un controlador para onNewIntent: </font></font><br><br><pre> <code class="java hljs"><span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">protected</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onNewIntent</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(Intent intent)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">super</span></span>.onNewIntent(intent); setIntent(intent); onSharedIntent(); }</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> La funci√≥n onSharedIntent en s√≠ misma: </font></font><br><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onSharedIntent</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ Intent receivedIntent = getIntent(); String receivedAction = receivedIntent.getAction(); String receivedType = receivedIntent.getType(); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (receivedAction.equals(Intent.ACTION_SEND)) { <span class="hljs-comment"><span class="hljs-comment">// If mime type is equal to image if (receivedType.startsWith("image/")) { m_txtViewBreed.setText(""); m_strResult = ""; Uri receivedUri = receivedIntent.getParcelableExtra( Intent.EXTRA_STREAM); if (receivedUri != null) { try { Bitmap bitmap = MediaStore.Images.Media.getBitmap( this.getContentResolver(), receivedUri); if(bitmap != null) { m_bitmap = bitmap; m_picView.setImageBitmap(m_bitmap); storeBitmap(); enableControls(true); } } catch (Exception e) { e.printStackTrace(); } } } } }</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ahora manejamos la imagen compartida desde onCreate (si la aplicaci√≥n se acaba de iniciar) o desde onNewIntent si se encontr√≥ una instancia en la memoria. </font></font><br><br><br><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Buena suerte </font><font style="vertical-align: inherit;">Si te gusta este art√≠culo, por favor "dale me gusta" en las redes sociales, tambi√©n hay botones sociales en un </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sitio en</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> s√≠.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/447732/">https://habr.com/ru/post/447732/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../447718/index.html">Todo ir√° de acuerdo al plan</a></li>
<li><a href="../447720/index.html">Seguridad de IoT. Problema 2. Hogar inteligente</a></li>
<li><a href="../447724/index.html">C√≥mo surgen las ciudades inteligentes</a></li>
<li><a href="../447728/index.html">Calculamos el presupuesto de energ√≠a de una l√≠nea de radio para un sat√©lite en formato CubeSat</a></li>
<li><a href="../447730/index.html">La evoluci√≥n del marketing por correo electr√≥nico: de QWERTYUIOP a GDPR</a></li>
<li><a href="../447734/index.html">Por qu√© el front-end deber√≠a comprender los principios de la interfaz de usuario</a></li>
<li><a href="../447736/index.html">Video de drones: una nueva tendencia en las redes sociales</a></li>
<li><a href="../447738/index.html">Julian Assange arrestado por la polic√≠a del Reino Unido</a></li>
<li><a href="../447742/index.html">¬øQu√© es la metodolog√≠a DevOps y qui√©n la necesita?</a></li>
<li><a href="../447744/index.html">Climbing Elbrus - Reconocimiento en batalla. Parte t√©cnica 2. Interrupciones, excepciones, temporizador del sistema.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>