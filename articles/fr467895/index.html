<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üîä ‚úåüèø üì≤ Quels mod√®les les r√©seaux de neurones trouvent-ils? üë©‚Äç‚ù§Ô∏è‚Äçüë® üòß ü•©</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dans cet article, je veux parler des mod√®les que les r√©seaux de neurones peuvent trouver. De nombreux guides pour d√©butants se concentrent sur la tech...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Quels mod√®les les r√©seaux de neurones trouvent-ils?</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/467895/">  Dans cet article, je veux parler des mod√®les que les r√©seaux de neurones peuvent trouver.  De nombreux guides pour d√©butants se concentrent sur la technique d'√©criture de code pour les r√©seaux de neurones, tandis que les questions de ¬´logique¬ª (que peuvent faire les r√©seaux de neurones? Quelles architectures sont mieux adapt√©es √† quelles t√¢ches et pourquoi?) Restent souvent en marge.  J'esp√®re que mon article aidera les d√©butants √† mieux comprendre les capacit√©s des r√©seaux de neurones.  Pour ce faire, nous allons essayer de voir comment ils g√®rent certaines t√¢ches du mod√®le.  Un exemple de code sera fourni en python √† l'aide de la biblioth√®que keras. <br><br>  <b>T√¢che 1.</b> Commen√ßons par une simple.  Nous construisons un r√©seau neuronal se rapprochant du sinus. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_X_y</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n)</span></span></span><span class="hljs-function">:</span></span> X = np.random.uniform(<span class="hljs-number"><span class="hljs-number">0</span></span>, np.pi, n) y = np.sin(X) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X, y n = <span class="hljs-number"><span class="hljs-number">40</span></span> X, y = get_X_y(n) print(<span class="hljs-string"><span class="hljs-string">"X shape:"</span></span>, X.shape) model = Sequential() model.add(Dense(<span class="hljs-number"><span class="hljs-number">6</span></span>, input_dim=<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">4</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>]) model.fit(X, y, epochs=<span class="hljs-number"><span class="hljs-number">1000</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">4</span></span>) X_test = np.linspace(start=<span class="hljs-number"><span class="hljs-number">0</span></span>, stop=np.pi, num=<span class="hljs-number"><span class="hljs-number">500</span></span>) print(<span class="hljs-string"><span class="hljs-string">"X test shape:"</span></span>, X_test.shape) y_test = model.predict(X_test) font = {<span class="hljs-string"><span class="hljs-string">'weight'</span></span>: <span class="hljs-string"><span class="hljs-string">'bold'</span></span>, <span class="hljs-string"><span class="hljs-string">'size'</span></span>: <span class="hljs-number"><span class="hljs-number">25</span></span>} matplotlib.rc(<span class="hljs-string"><span class="hljs-string">'font'</span></span>, **font) axes = plt.gca() axes.set_ylim(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(X_test, y_test, c=<span class="hljs-string"><span class="hljs-string">'green'</span></span>, marker=<span class="hljs-string"><span class="hljs-string">'o'</span></span>, markersize=<span class="hljs-number"><span class="hljs-number">5</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">"Sinus approximated by neural network"</span></span>) plt.yticks(np.arange(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>)) plt.grid() plt.show()</code> </pre> <br>  Nous obtenons le graphique suivant: <br><br><img src="https://habrastorage.org/webt/t3/xv/_o/t3xv_ocq-o9m8yupmxxdrfxqgai.png" width="500" height="500"><br><br>  Comme vous pouvez le voir, le r√©seau neuronal a r√©ussi √† faire face √† la t√¢che d'approximation d'une fonction simple. <br><a name="habracut"></a><br>  <b>T√¢che 2.</b> Voyons comment le r√©seau neuronal fera face √† une t√¢che plus complexe.  Nous entrerons x valeurs uniform√©ment r√©parties sur l'intervalle [0, 1], et y sera d√©fini de mani√®re al√©atoire: pour x &lt;0,6, y sera une variable al√©atoire prenant la valeur 0 avec une probabilit√© de 0,75 et 1 avec une probabilit√© de 0,25 (c'est-√†-dire une valeur al√©atoire binomiale avec p = 0,25).  Pour x&gt; 0,6, y sera une variable al√©atoire prenant la valeur 0 avec probabilit√© 0,3 et la valeur 1 avec probabilit√© 0,7.  En tant que fonction optimis√©e, nous prenons l'erreur standard. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_X_y</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n)</span></span></span><span class="hljs-function">:</span></span> X = np.random.uniform(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, n) y0 = np.random.binomial(size=n, n=<span class="hljs-number"><span class="hljs-number">1</span></span>, p=<span class="hljs-number"><span class="hljs-number">0.25</span></span>) y1 = np.random.binomial(size=n, n=<span class="hljs-number"><span class="hljs-number">1</span></span>, p=<span class="hljs-number"><span class="hljs-number">0.7</span></span>) y = np.where(X &lt; <span class="hljs-number"><span class="hljs-number">0.6</span></span>, y0, y1) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X, y n_inputs = <span class="hljs-number"><span class="hljs-number">1</span></span> n_hidden1 = <span class="hljs-number"><span class="hljs-number">100</span></span> n_hidden2 = <span class="hljs-number"><span class="hljs-number">50</span></span> n_outputs = <span class="hljs-number"><span class="hljs-number">1</span></span> n = <span class="hljs-number"><span class="hljs-number">2000</span></span> X, y = get_X_y(n) print(<span class="hljs-string"><span class="hljs-string">"X shape:"</span></span>, X.shape) model = Sequential() model.add(Dense(n_hidden1, input_dim=<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(Dense(n_hidden2, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>]) model.fit(X, y, epochs=<span class="hljs-number"><span class="hljs-number">200</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">100</span></span>) X_test = np.linspace(start=<span class="hljs-number"><span class="hljs-number">0</span></span>, stop=<span class="hljs-number"><span class="hljs-number">1</span></span>, num=<span class="hljs-number"><span class="hljs-number">100</span></span>) print(<span class="hljs-string"><span class="hljs-string">"X test shape:"</span></span>, X_test.shape) y_test = model.predict(X_test) font = {<span class="hljs-string"><span class="hljs-string">'weight'</span></span>: <span class="hljs-string"><span class="hljs-string">'bold'</span></span>, <span class="hljs-string"><span class="hljs-string">'size'</span></span>: <span class="hljs-number"><span class="hljs-number">25</span></span>} matplotlib.rc(<span class="hljs-string"><span class="hljs-string">'font'</span></span>, **font) axes = plt.gca() axes.set_ylim(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(X_test, y_test, c=<span class="hljs-string"><span class="hljs-string">'green'</span></span>, marker=<span class="hljs-string"><span class="hljs-string">'o'</span></span>, markersize=<span class="hljs-number"><span class="hljs-number">5</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">"Binomial distribution approximated by neural network"</span></span>) plt.yticks(np.arange(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>)) plt.grid() plt.show()</code> </pre><br>  Nous obtenons le graphique suivant d'un r√©seau neuronal √† fonction approximative: <br><br><img src="https://habrastorage.org/webt/y_/rp/lo/y_rplovhyaxioena0tb8iueed9m.png" width="500" height="500"><br><br>  Comme vous pouvez le voir, le r√©seau neuronal a approch√© l'attente math√©matique de notre variable al√©atoire y.  Ainsi, les r√©seaux de neurones peuvent (en principe) approcher les valeurs moyennes des variables al√©atoires qui d√©pendent des param√®tres.  Par exemple, nous pouvons nous attendre √† ce qu'ils r√©solvent le probl√®me suivant: les personnes dont le revenu ne d√©passe pas 1 000 $ sont en moyenne m√©contentes et les personnes dont le revenu est sup√©rieur √† 1 000 $ sont en moyenne satisfaites;  il faut apprendre √† pr√©dire le "niveau de bonheur" en fonction des revenus.  Le r√©seau de neurones pourra trouver la d√©pendance du niveau moyen de bonheur sur le revenu, malgr√© le fait que parmi les personnes avec n'importe quel niveau de revenu, il y a √† la fois heureux et malheureux. <br><br>  <b>Probl√®me 3.</b> Passons maintenant √† la pr√©diction des s√©quences.  Nous consid√©rerons les s√©quences de 0 et 1 donn√©es par la r√®gle suivante: 10 membres - √©quitablement 0 ou 1, et le onzi√®me est √©gal √† 1 si le terme pr√©c√©dent est 0, et √©galement probable 0 ou 1 si le terme pr√©c√©dent 1. Nous g√©n√©rerons de telles s√©quences de longueur 11 (entr√©e 10 s√©quence des membres et un, le dernier, nous pr√©disons) et les former sur notre r√©seau neuronal r√©current.  Et apr√®s l'entra√Ænement, v√©rifions comment elle g√®re la pr√©diction sur les nouvelles s√©quences (√©galement de longueur 11). <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LSTM, Dense <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_X_y</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(m, n)</span></span></span><span class="hljs-function">:</span></span> X = np.random.binomial(size=(m,n), n=<span class="hljs-number"><span class="hljs-number">1</span></span>, p=<span class="hljs-number"><span class="hljs-number">0.5</span></span>) y0 = np.ones(m) y1 = np.random.binomial(size=m, n=<span class="hljs-number"><span class="hljs-number">1</span></span>, p=<span class="hljs-number"><span class="hljs-number">0.5</span></span>) y = np.where(X[:, n<span class="hljs-number"><span class="hljs-number">-1</span></span>]==<span class="hljs-number"><span class="hljs-number">0</span></span>, y0, y1) X = np.reshape(X, (X.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], X.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X, y model = Sequential() model.add(LSTM(units=<span class="hljs-number"><span class="hljs-number">50</span></span>)) model.add(Dense(units=<span class="hljs-number"><span class="hljs-number">1</span></span>)) model.compile(optimizer = <span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss = <span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>) X_train, y_train = get_X_y(<span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>) model.fit(X_train, y_train, epochs = <span class="hljs-number"><span class="hljs-number">20</span></span>, batch_size = <span class="hljs-number"><span class="hljs-number">32</span></span>) m_test = <span class="hljs-number"><span class="hljs-number">12</span></span> n_test = <span class="hljs-number"><span class="hljs-number">10</span></span> X_test, y_test = get_X_y(m_test, n_test) y_predicted = model.predict(X_test) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(m_test): print(<span class="hljs-string"><span class="hljs-string">"x_last="</span></span>, X_test[i, n_test<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-string"><span class="hljs-string">"y_predicted="</span></span>, y_predicted[i, <span class="hljs-number"><span class="hljs-number">0</span></span>])</code> </pre><br>  Voyons quelles pr√©visions notre r√©seau de neurones donne sur les s√©quences test√©es (vos r√©sultats seront diff√©rents, car ici l'al√©atoire est pr√©sent √† la fois dans le choix des s√©quences et dans l'entra√Ænement du r√©seau de neurones). <br><br><div class="scrollable-table"><table><tbody><tr><th>  Num√©ro de s√©quence </th><th>  Avant-dernier membre de la s√©quence </th><th>  Valeur pr√©dite </th></tr><tr><td>  1 </td><td>  0 </td><td>  0,96 </td></tr><tr><td>  2 </td><td>  0 </td><td>  0,95 </td></tr><tr><td>  3 </td><td>  0 </td><td>  0,97 </td></tr><tr><td>  4 </td><td>  0 </td><td>  0,96 </td></tr><tr><td>  5 </td><td>  0 </td><td>  0,96 </td></tr><tr><td>  6 </td><td>  1 </td><td>  0,45 </td></tr><tr><td>  7 </td><td>  0 </td><td>  0,94 </td></tr><tr><td>  8 </td><td>  1 </td><td>  0,50 </td></tr><tr><td>  9 </td><td>  0 </td><td>  0,96 </td></tr><tr><td>  10 </td><td>  1 </td><td>  0,42 </td></tr><tr><td>  11 </td><td>  1 </td><td>  0,44 </td></tr><tr><td>  12 </td><td>  0 </td><td>  0,92 </td></tr></tbody></table></div><br><br>  Comme vous pouvez le voir, si l'avant-dernier membre de la s√©quence est 0, alors le r√©seau neuronal pr√©dit une valeur proche de 1, et s'il est 1, alors une valeur proche de 0,5.  Ceci est proche de la pr√©vision optimale.  Un exemple similaire de la "vie" pourrait ressembler √† ceci: "si je vais au cin√©ma aujourd'hui, alors demain je d√©jeunerai dans un restaurant;  si je vais au th√©√¢tre aujourd'hui, alors demain je d√©jeunerai n'importe o√π. "  Comme nous l'avons vu, un r√©seau de neurones peut capturer des mod√®les de ce type et pr√©dire un voyage dans un restaurant en allant au cin√©ma (et en allant au th√©√¢tre pour pr√©dire ¬´quelque chose entre les deux¬ª). <br><br>  <b>T√¢che 4.</b> Nous compliquons la t√¢che du r√©seau neuronal.  Que tout soit comme dans l'exemple pr√©c√©dent, seul le onzi√®me membre de la s√©quence sera d√©termin√© non pas par le pr√©c√©dent, mais par le deuxi√®me membre de la s√©quence (par la m√™me r√®gle).  Nous ne donnerons pas le code ici, car il ne diff√®re pratiquement pas du pr√©c√©dent.  Mon exp√©rience a montr√© que le r√©seau neuronal trouve toujours un sch√©ma, mais pour plus de temps (j'ai d√ª utiliser 100 √©poques au lieu de 20 pour l'entra√Ænement). <br>  Ainsi, les r√©seaux de neurones peuvent (√† nouveau, en principe, clarifier) ‚Äã‚Äãattraper des d√©pendances √† assez long terme (dans notre ¬´exemple de vie¬ª, ils peuvent attraper des mod√®les comme ¬´Je vais au restaurant aujourd'hui si j'√©tais dans un film il y a une semaine¬ª). <br><br>  <b>T√¢che 5.</b> Voyons comment le r√©seau neuronal utilise les informations disponibles pour la pr√©vision. <br>  Pour ce faire, nous effectuerons des entra√Ænements sur des s√©quences de longueur 4. Au total, nous aurons 3 s√©quences diff√©rentes √©galement probables: <br><br> <code>0, 0, 1, 1 <br> 0, 1, 0, 1 <br> 0, 1, 1, 0</code> <br> <br>  Ainsi, apr√®s la combinaison initiale de 0, 0, nous rencontrons toujours deux unit√©s, apr√®s la combinaison de 0, 1, nous sommes √©galement susceptibles de rencontrer 0 ou 1, mais nous conna√Ætrons certainement le dernier nombre.  Nous allons maintenant demander √† notre r√©seau de neurones de renvoyer des s√©quences en d√©finissant return_sequences = True.  Comme les s√©quences pr√©dites, nous prenons nos m√™mes s√©quences d√©cal√©es d'un pas et compl√©t√©es par z√©ro √† droite.  Maintenant, nous pouvons d√©j√† supposer ce qui va se passer: √† la premi√®re √©tape, le r√©seau neuronal produira un nombre proche de 2/3 (car avec une probabilit√© de 2/3 le deuxi√®me terme est 1), puis pour une combinaison de 0, 0, il produira deux nombres proches de unit√©, et pour 0, 1, il donnera d'abord un nombre proche de 0,5, puis il donnera un nombre proche de 0 ou 1, selon que nous avons obtenu la s√©quence 0, 1, 0 ou 0, 1, 1. √Ä la fin du r√©seau neuronal produira toujours un nombre proche de 0. La v√©rification avec le code suivant montre que nos hypoth√®ses sont correctes. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LSTM, Dense <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_X_y</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n)</span></span></span><span class="hljs-function">:</span></span> X = np.zeros((n, <span class="hljs-number"><span class="hljs-number">4</span></span>)) z = np.array([random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n)]) X[z == <span class="hljs-number"><span class="hljs-number">0</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>] X[z == <span class="hljs-number"><span class="hljs-number">1</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>] X[z == <span class="hljs-number"><span class="hljs-number">2</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>] y = np.zeros((n, <span class="hljs-number"><span class="hljs-number">4</span></span>)) y[:, :<span class="hljs-number"><span class="hljs-number">3</span></span>] = X[:, <span class="hljs-number"><span class="hljs-number">1</span></span>:] X = np.reshape(X, (X.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], X.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>)) y = np.reshape(y, (y.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], y.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X, y model = Sequential() model.add(LSTM(units=<span class="hljs-number"><span class="hljs-number">20</span></span>, return_sequences=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) model.add(Dense(units=<span class="hljs-number"><span class="hljs-number">1</span></span>)) model.compile(optimizer = <span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss = <span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>) X_train, y_train = get_X_y(<span class="hljs-number"><span class="hljs-number">1000</span></span>) model.fit(X_train, y_train, epochs = <span class="hljs-number"><span class="hljs-number">100</span></span>, batch_size = <span class="hljs-number"><span class="hljs-number">32</span></span>) X_test = np.zeros((<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>)) X_test[<span class="hljs-number"><span class="hljs-number">0</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>] X_test[<span class="hljs-number"><span class="hljs-number">1</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>] X_test[<span class="hljs-number"><span class="hljs-number">2</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>] X_test = np.reshape(X_test, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) y_predicted = model.predict(X_test) print(y_predicted)</code> </pre><br><br>  De cet exemple, nous voyons que le r√©seau de neurones peut changer dynamiquement la pr√©vision en fonction des informations re√ßues.  Nous ferions de m√™me, en essayant de pr√©dire une certaine s√©quence: lorsque les informations disponibles nous permettent d'estimer les probabilit√©s de r√©sultats √† l'√©tape suivante, nous pr√©disons sur la base de ces informations;  mais lorsque nous trouvons des informations suppl√©mentaires √† l'√©tape suivante, nous modifions les pr√©visions en fonction de celles-ci. <br>  Donc, si nous voyons que quelqu'un vient de nous du noir, alors nous disons "c'est une personne, nous ne le savons pas plus en d√©tail";  quand on commence √† distinguer les cheveux longs dans le noir, on se dit "c'est probablement une femme".  Mais si apr√®s cela nous consid√©rons qu'une personne a une moustache, alors nous disons que c'est probablement un homme (quoique avec des cheveux longs).  Comme nous l'avons vu, un r√©seau de neurones agit de la m√™me mani√®re, utilisant l'int√©gralit√© des informations actuellement disponibles pour la pr√©vision. <br><br>  Nous avons donc examin√© des exemples simples du fonctionnement des r√©seaux de neurones et des mod√®les qu'ils peuvent trouver.  En g√©n√©ral, nous avons vu que les r√©seaux de neurones se comportent souvent de mani√®re ¬´raisonnablement¬ª, faisant des pr√©dictions proches de celles qu'une personne ferait.  Bien que, il convient de noter que pour saisir des mod√®les simples, ils ont besoin de beaucoup plus de donn√©es que les personnes. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr467895/">https://habr.com/ru/post/fr467895/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr467883/index.html">Kubernetes 1.16 - comment mettre √† niveau et ne rien casser</a></li>
<li><a href="../fr467885/index.html">Mines de produits et de segments</a></li>
<li><a href="../fr467887/index.html">R√©flexions caustiques r√©alistes</a></li>
<li><a href="../fr467891/index.html">FAQ sur la signature [√©lectronique] du cloud</a></li>
<li><a href="../fr467893/index.html">Juste un autre wrapper Qt pour gRPC et protobuf</a></li>
<li><a href="../fr467897/index.html">Outils de test automatique, int√©gration de Yandex Mapkit 3, design sympa et approche d'interface utilisateur pilot√©e par le serveur - Annonce Mitap Android</a></li>
<li><a href="../fr467901/index.html">R√©futez quatre st√©r√©otypes sur le langage de programmation Rust</a></li>
<li><a href="../fr467905/index.html">Comment nous avons fait une reconnaissance historique dans Cloud Mail.ru, et pourquoi</a></li>
<li><a href="../fr467909/index.html">Chat sur iOS: √† l'aide de sockets</a></li>
<li><a href="../fr467913/index.html">Comment am√©liorer le ¬´min√©ral b√¢tard¬ª, ou la nouvelle interface pour le panneau solaire</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>