<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤞🏾 👨🏼‍🏭 👩‍🚀 Bitrix24: "Schnell angehoben gilt nicht als gefallen" 👩🏾‍💻 👩🏿‍⚖️ 🐌</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bis heute hat der Bitrix24-Dienst nicht Hunderte von Gigabit Verkehr, es gibt keine riesige Flotte von Servern (obwohl es natürlich viele gibt). Für v...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bitrix24: "Schnell angehoben gilt nicht als gefallen"</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/itsumma/blog/455112/">  Bis heute hat der Bitrix24-Dienst nicht Hunderte von Gigabit Verkehr, es gibt keine riesige Flotte von Servern (obwohl es natürlich viele gibt).  Für viele Kunden ist es jedoch das Hauptwerkzeug für die Arbeit im Unternehmen. Es ist eine echte geschäftskritische Anwendung.  Deshalb fallen - na ja, auf keinen Fall.  Aber was ist, wenn der Sturz passiert ist, aber der Dienst so schnell „rebelliert“ hat, dass niemand etwas bemerkt hat?  Und wie schaffen Sie es, ein Failover zu implementieren, ohne die Arbeitsqualität und die Anzahl der Kunden zu verlieren?  Alexander Demidov, Director von Bitrix24 Cloud Services, berichtete in unserem Blog über die Entwicklung des Backup-Systems in den sieben Jahren seines Bestehens. <br><br><img src="https://habrastorage.org/webt/xp/iz/9_/xpiz9_ri1tdppyelfb8nu9tzeg8.jpeg"><br><a name="habracut"></a><br>  „Mit SaaS haben wir Bitrix24 vor 7 Jahren eingeführt.  Die Hauptschwierigkeit war wahrscheinlich die folgende: Vor dem öffentlichen Start in Form von SaaS existierte dieses Produkt einfach im Format einer Box-Lösung.  Kunden haben es bei uns gekauft, auf ihren Servern platziert, ein Unternehmensportal eingerichtet - eine gängige Lösung für Mitarbeiterkommunikation, Dateispeicherung, Aufgabenverwaltung, CRM - das ist alles.  Und bis 2012 haben wir beschlossen, es als SaaS zu starten, es selbst zu verwalten und Fehlertoleranz und Zuverlässigkeit zu gewährleisten.  Wir haben dabei Erfahrungen gesammelt, weil wir sie bis dahin einfach nicht hatten - wir waren nur Softwarehersteller, keine Dienstleister. <br><br>  Beim Starten des Dienstes haben wir verstanden, dass das Wichtigste darin besteht, die Fehlertoleranz, Zuverlässigkeit und ständige Verfügbarkeit des Dienstes sicherzustellen, denn wenn Sie eine einfache reguläre Website haben, zum Beispiel ein Geschäft, das von Ihnen gefallen ist und eine Stunde liegt - nur Sie selbst leiden, verlieren Sie Aufträge Sie verlieren Kunden, aber für Ihren Kunden selbst - für ihn ist dies nicht sehr kritisch.  Er war natürlich verärgert, ging aber und kaufte auf einer anderen Seite.  Und wenn dies eine Anwendung ist, an die alle Arbeiten innerhalb des Unternehmens, der Kommunikation und der Lösungen gebunden sind, ist es das Wichtigste, das Vertrauen der Benutzer zu gewinnen, dh sie nicht im Stich zu lassen und nicht zu fallen.  Weil die ganze Arbeit aufstehen kann, wenn etwas im Inneren nicht funktioniert. <br><br><h4>  Bitrix. 24 als SaaS </h4><br>  Der erste Prototyp, den wir ein Jahr vor dem öffentlichen Start im Jahr 2011 zusammengebaut haben.  In ungefähr einer Woche versammelt, geschaut, verdreht - er arbeitete sogar.  Das heißt, es war möglich, in das Formular zu gehen, dort den Namen des Portals einzugeben, ein neues Portal wurde entfaltet, eine Benutzerbasis wurde eingerichtet.  Wir haben es uns angesehen, das Produkt im Prinzip bewertet, ausgeschaltet und ein Jahr später fertiggestellt.  Weil wir eine große Aufgabe hatten: Wir wollten nicht zwei verschiedene Codebasen erstellen, wir wollten kein separates Box-Produkt, keine separaten Cloud-Lösungen unterstützen - wir wollten dies alles innerhalb desselben Codes tun. <br><br><img src="https://habrastorage.org/webt/lw/zu/fe/lwzufe1w3lchcx7iqtwwh04gqfu.jpeg"><br><br>  Eine typische Webanwendung zu dieser Zeit ist ein Server, auf dem PHP-Code ausgeführt wird, die MySQL-Basis, Dateien werden heruntergeladen, Dokumente und Bilder werden in den Upload-Daddy gestellt - nun, alles funktioniert.  Leider ist es unmöglich, einen kritisch nachhaltigen Webdienst zu betreiben.  Der verteilte Cache wird dort nicht unterstützt, die Datenbankreplikation wird nicht unterstützt. <br><br>  Wir haben die Anforderungen formuliert: Diese Fähigkeit, sich an verschiedenen Standorten zu befinden, die Replikation zu unterstützen, idealerweise in verschiedenen geografisch verteilten Rechenzentren.  Trennen Sie die Logik des Produkts und in der Tat die Speicherung von Daten.  Dynamisch in der Lage sein, entsprechend der Last zu skalieren, machen in der Regel die Statik.  Aufgrund dieser Überlegungen gab es tatsächlich Anforderungen an das Produkt, das wir gerade im Laufe des Jahres entwickelt haben.  Während dieser Zeit haben wir auf einer Plattform, die sich als einheitlich herausstellte - für Boxed-Lösungen, für unseren eigenen Service - Unterstützung für die Dinge geleistet, die wir brauchten.  Unterstützung für die MySQL-Replikation auf der Ebene des Produkts selbst: Das heißt, der Entwickler, der den Code schreibt, denkt nicht darüber nach, wie seine Anforderungen verteilt werden, er verwendet unsere API, und wir können Schreib- und Leseanforderungen korrekt zwischen Mastern und Slaves verteilen. <br><br>  Wir haben Unterstützung auf Produktebene für verschiedene Cloud-Objektspeicher bereitgestellt: Google Storage, Amazon S3, - Plus, Unterstützung für Open Stack Swift.  Daher war es sowohl für uns als Service als auch für Entwickler, die mit einer Boxed-Lösung arbeiten, praktisch: Wenn sie nur unsere API für die Arbeit verwenden, denken sie nicht, wo die Datei gespeichert wird, weder lokal im Dateisystem noch im Objektdateispeicher . <br><br>  Infolgedessen haben wir sofort beschlossen, auf der Ebene eines gesamten Rechenzentrums zu reservieren.  2012 haben wir vollständig in Amazon AWS gestartet, da wir bereits Erfahrung mit dieser Plattform hatten - unsere eigene Website wurde dort gehostet.  Wir waren von der Tatsache angezogen, dass es in jeder Region in Amazon mehrere Zugangszonen gibt - tatsächlich (in ihrer Terminologie) mehrere Rechenzentren, die mehr oder weniger unabhängig voneinander sind und es uns ermöglichen, auf der Ebene eines gesamten Rechenzentrums zu reservieren: if es schlägt plötzlich fehl, die Master-Master-Datenbanken werden repliziert, die Webanwendungsserver werden reserviert und die statische Daten werden in den s3-Objektspeicher verschoben.  Die Last ist ausgeglichen - zu dieser Zeit der Amazonas-Elb, aber wenig später kamen wir zu unseren eigenen Balancern, weil wir eine komplexere Logik brauchten. <br><br><h4>  Was sie wollten, haben sie bekommen ... </h4><br>  Alle grundlegenden Dinge, die wir bereitstellen wollten - die Fehlertoleranz der Server selbst, Webanwendungen, Datenbanken - alles hat gut funktioniert.  Das einfachste Szenario: Wenn einige der Webanwendungen fehlschlagen, ist alles einfach - sie werden aus dem Gleichgewicht genommen. <br><br><img src="https://habrastorage.org/webt/0y/vn/7h/0yvn7ho3syy9rpwkz8wo7govryu.jpeg"><br><br>  Der Maschinenausgleicher (damals war es ein Amazonas-Elb), der die Maschine selbst als ungesund eingestuft hat, hat die Lastverteilung auf ihnen ausgeschaltet.  Die automatische Skalierung im Amazonasgebiet funktionierte: Als die Last zunahm, wurden der Autoskalierungsgruppe neue Autos hinzugefügt, die Ladung wurde auf neue Autos verteilt - alles war in Ordnung.  Bei unseren Balancern ist die Logik ungefähr dieselbe: Wenn dem Anwendungsserver etwas passiert, entfernen wir Anforderungen von ihm, werfen diese Computer aus, starten neue und arbeiten weiter.  Das Schema für all diese Jahre hat sich ein wenig geändert, funktioniert aber weiterhin: Es ist einfach, verständlich und es gibt keine Schwierigkeiten damit. <br><br>  Wir arbeiten auf der ganzen Welt, die Spitzenlast der Kunden ist völlig anders, und wir sollten in der Lage sein, jederzeit bestimmte Wartungsarbeiten mit allen Komponenten unseres Systems durchzuführen - unsichtbar für die Kunden.  Daher haben wir die Möglichkeit, die Datenbank von der Arbeit herunterzufahren und die Last im zweiten Rechenzentrum neu zu verteilen. <br><br>  Wie funktioniert das alles?  - Wir schalten den Datenverkehr auf ein funktionierendes Rechenzentrum um. Wenn es sich um einen Unfall in einem Rechenzentrum handelt. Wenn es sich um unsere geplante Arbeit mit einer Basis handelt, werden wir Teil des Datenverkehrs, der diese Clients bedient, in ein zweites Rechenzentrum umschalten Replikation.  Wenn Sie neue Maschinen für Webanwendungen benötigen, werden diese automatisch gestartet, da die Belastung des zweiten Rechenzentrums gestiegen ist.  Wir beenden die Arbeit, die Replikation wird wiederhergestellt und wir geben die gesamte Last zurück.  Wenn wir einige Arbeiten im zweiten Domänencontroller spiegeln müssen, z. B. Systemaktualisierungen installieren oder die Einstellungen in der zweiten Datenbank ändern müssen, wiederholen wir im Allgemeinen dasselbe, nur in die andere Richtung.  Und wenn dies ein Unfall ist, dann machen wir alles auf einfache Weise: Im Überwachungssystem verwenden wir den Event-Handler-Mechanismus.  Wenn mehrere Überprüfungen für uns funktionieren und der Status kritisch wird, wird dieser Handler gestartet, ein Handler, der diese oder jene Logik ausführen kann.  Für jede Datenbank haben wir registriert, welcher Server für sie ein Failover ist und wo Sie den Datenverkehr wechseln müssen, wenn er nicht verfügbar ist.  Wir - wie es sich historisch entwickelt hat - verwenden in der einen oder anderen Form Nagios oder eine seiner Gabeln.  Im Prinzip gibt es ähnliche Mechanismen in fast jedem Überwachungssystem. Wir verwenden noch nichts Komplizierteres, aber vielleicht werden wir es eines Tages tun.  Jetzt wird die Überwachung durch Unzugänglichkeit ausgelöst und kann etwas umschalten. <br><br><h4>  Haben wir alles reserviert? </h4><br>  Wir haben viele Kunden aus den USA, viele Kunden aus Europa, viele Kunden, die näher am Osten liegen - Japan, Singapur und so weiter.  Natürlich ein großer Teil der Kunden in Russland.  Das heißt, Arbeit ist weit davon entfernt, in einer Region zu sein.  Benutzer möchten eine schnelle Antwort, es gibt Anforderungen für die Einhaltung verschiedener lokaler Gesetze, und innerhalb jeder Region reservieren wir zwei Rechenzentren. Außerdem gibt es einige zusätzliche Dienste, die wiederum bequem in einer Region platziert werden können - für Kunden, die sich in dieser Region befinden Region Arbeit.  REST-Handler, Autorisierungsserver, sie sind für den gesamten Client weniger kritisch. Sie können mit einer kleinen akzeptablen Verzögerung zwischen ihnen wechseln, möchten jedoch keine Fahrräder erfinden, wie sie überwacht werden und was mit ihnen zu tun ist.  Daher versuchen wir maximal, vorhandene Lösungen zu nutzen und keine Kompetenz für zusätzliche Produkte zu entwickeln.  Und irgendwo verwenden wir trivialerweise das Umschalten auf DNS-Ebene und bestimmen die Lebendigkeit des Dienstes mit denselben DNS.  Amazon verfügt über einen Route 53-Dienst, der jedoch nicht nur DNS-Daten enthält, sondern auch viel flexibler und bequemer ist.  Mithilfe dieser Funktion können Sie geoverteilte Dienste mit Geolokalisierung erstellen, wenn Sie damit bestimmen, woher der Client stammt, und ihnen bestimmte Datensätze geben. Mit ihm können Sie Failover-Architekturen erstellen.  Dieselben Integritätsprüfungen werden in Route 53 selbst konfiguriert. Sie geben überwachte Endpunkte an, legen Metriken fest und geben an, welche Protokolle die Lebensdauer des Dienstes bestimmen - tcp, http, https;  Legen Sie die Häufigkeit der Überprüfungen fest, mit denen festgestellt wird, ob der Dienst aktiv ist oder nicht.  Und in der DNS selbst schreiben Sie vor, was primär sein soll, was sekundär sein soll, wo gewechselt werden soll, wenn die Integritätsprüfung innerhalb der Route 53 ausgelöst wird. All dies kann mit einigen anderen Tools durchgeführt werden, aber was ist bequemer - wir haben es einmal eingerichtet und denken dann nicht darüber nach wie wir prüfen, wie wir wechseln: alles funktioniert von selbst. <br><br>  <b>Das erste „aber“</b> : Wie und wie reserviert man die Route 53 selbst?  Passiert es, wenn ihm etwas passiert?  Glücklicherweise sind wir noch nie auf diesen Rechen getreten, aber wieder werde ich vor mir eine Geschichte haben, warum wir dachten, dass wir noch reservieren müssen.  Hier legen wir den Strohhalm im Voraus.  Mehrmals am Tag entladen wir alle Zonen auf Route 53 vollständig.  Mit der Amazon-API können sie sicher an JSON gesendet werden. Wir haben mehrere redundante Server eingerichtet, auf denen wir sie konvertieren, in Form von Konfigurationen hochladen und grob gesagt eine Sicherungskonfiguration haben.  In diesem Fall können wir es schnell manuell bereitstellen, ohne dass die DNS-Einstellungsdaten verloren gehen. <br><br>  <b>Das zweite "aber"</b> : Was ist auf diesem Bild nicht reserviert?  Der Balancer selbst!  Wir haben die Verteilung der Kunden nach Regionen sehr einfach gemacht.  Wir haben die Domänen bitrix24.ru, bitrix24.com und .de - jetzt gibt es 13 verschiedene Domänen, die in sehr unterschiedlichen Zonen funktionieren.  Wir sind zu Folgendem gekommen: Jede Region hat ihre eigenen Balancer.  Die Verteilung nach Regionen ist bequemer, je nachdem, wo sich die Spitzenlast im Netzwerk befindet.  Wenn dies ein Fehler auf der Ebene eines Balancers ist, wird er einfach außer Betrieb genommen und aus DNS entfernt.  Wenn bei einer Gruppe von Balancern ein Problem auftritt, sind diese an anderen Standorten reserviert, und das Umschalten zwischen ihnen erfolgt auf derselben Route53, da aufgrund eines kurzen ttl das Umschalten maximal 2, 3, 5 Minuten dauert. <br><br>  <b>Das dritte "aber"</b> : Was wurde noch nicht reserviert?  S3, richtig.  Wir haben die Dateien, die von Benutzern in s3 gespeichert werden, platziert und aufrichtig geglaubt, dass sie panzerbrechend sind und dass dort nichts reserviert werden muss.  Aber die Geschichte zeigt, was anders passiert.  Im Allgemeinen beschreibt Amazon S3 als einen grundlegenden Dienst, da Amazon selbst S3 zum Speichern von Bildern von Maschinen, Konfigurationen, AMI-Bildern, Schnappschüssen ... verwendet. Und wenn s3 abstürzt, wie es in diesen 7 Jahren einmal passiert ist, wie viel bitrix24 wir verwendet haben, folgt ein Fan zieht eine Menge von allem - Unzugänglichkeit beim Starten von virtuellen Maschinen, Fehlfunktionen der API und so weiter. <br><br>  Und S3 kann fallen - es ist einmal passiert.  Daher kamen wir zu folgendem Schema: Vor einigen Jahren gab es in Russland keine ernsthaften öffentlichen Lager für Objekte, und wir überlegten, ob wir etwas Eigenes tun könnten ... Glücklicherweise haben wir damit nicht begonnen, weil wir uns mit dieser Prüfung befassen würden, die wir nicht durchgeführt haben besitzen, und hätte es wahrscheinlich getan.  Jetzt hat Mail.ru s3-kompatible Speicher, Yandex hat es und eine Reihe von Anbietern haben es noch.  Als Ergebnis kamen wir zu dem Schluss, dass wir erstens ein Backup und zweitens die Möglichkeit haben möchten, mit lokalen Kopien zu arbeiten.  Für eine bestimmte russische Region verwenden wir den Mail.ru Hotbox-Dienst, der mit s3 kompatibel ist.  Wir brauchten keine ernsthaften Änderungen am Code in der Anwendung und haben den folgenden Mechanismus vorgenommen: In s3 gibt es Trigger, die beim Erstellen / Löschen von Objekten funktionieren. Amazon hat einen Dienst wie Lambda - dies ist serverloser laufender Code, der nur ausgeführt wird wenn bestimmte Trigger ausgelöst werden. <br><br><img src="https://habrastorage.org/webt/yv/y5/cp/yvy5cp7xsn82ryeocsgos7ns7ss.jpeg"><br><br>  Wir haben es ganz einfach gemacht: Wenn unser Trigger ausgelöst wird, führen wir den Code aus, der das Objekt in das Mail.ru-Repository kopiert.  Um vollständig mit lokalen Kopien von Daten arbeiten zu können, benötigen wir auch eine umgekehrte Synchronisierung, damit Clients im russischen Segment mit einem Speicher arbeiten können, der näher an ihnen liegt.  Mail ist dabei, die Trigger in seinem Repository zu vervollständigen - es wird möglich sein, eine umgekehrte Synchronisation bereits auf Infrastrukturebene durchzuführen, aber im Moment tun wir dies auf der Ebene unseres eigenen Codes.  Wenn wir sehen, dass der Client eine Art Datei abgelegt hat, stellen wir das Ereignis auf Codeebene in die Warteschlange, verarbeiten es und führen die umgekehrte Replikation durch.  Warum ist es so schlimm: Wenn wir mit unseren Objekten außerhalb unseres Produkts arbeiten, dh auf externe Weise, werden wir dies nicht berücksichtigen.  Daher warten wir bis zum Ende, wenn Trigger auf Speicherebene angezeigt werden, sodass das zu uns gekommene Objekt unabhängig davon, von wo aus wir den Code ausführen, in die andere Richtung kopiert wird. <br><br>  Auf Codeebene werden für jeden Client beide Repositorys registriert: eines wird als Hauptrepository betrachtet, das andere als Backup.  Wenn alles in Ordnung ist, arbeiten wir mit dem Speicher, der näher bei uns liegt: Das heißt, unsere Kunden bei Amazon arbeiten mit S3, und diejenigen, die in Russland arbeiten, arbeiten mit Hotbox.  Wenn das Kontrollkästchen funktioniert, sollte das Failover eine Verbindung zu uns herstellen, und wir werden die Clients auf einen anderen Speicher umstellen.  Wir können dieses Flag unabhängig nach Region setzen und es hin und her schalten.  In der Praxis haben wir dies noch nicht verwendet, aber wir haben uns diesen Mechanismus vorgestellt und wir denken, dass wir eines Tages genau diesen Schalter brauchen und verwenden werden.  Sobald es schon passiert ist. <br><br><h4>  Oh, und dein Amazon ist entkommen ... </h4><br>  Dieser April ist der Jahrestag des Starts der Telegrammsperren in Russland.  Der am stärksten betroffene Anbieter, der darunter fiel, ist Amazon.  Und leider litten russische Unternehmen, die auf der ganzen Welt tätig waren, mehr. <br><br>  Wenn das Unternehmen global ist und Russland ein sehr kleines Segment ist, 3-5% - auf die eine oder andere Weise können Sie sie spenden. <br><br>  Wenn es sich um ein rein russisches Unternehmen handelt - ich bin sicher, dass Sie es vor Ort suchen müssen -, ist es nur so, dass die Benutzer selbst bequem und komfortabel sind und weniger Risiken bestehen. <br><br>  Und wenn dies ein Unternehmen ist, das global arbeitet und ungefähr den gleichen Anteil an Kunden aus Russland und irgendwo auf der Welt hat?  Die Konnektivität der Segmente ist wichtig und sie müssen trotzdem miteinander arbeiten. <br><br>  Ende März 2018 sandte Roskomnadzor einen Brief an die größten Betreiber, in dem er erklärte, dass sie planen, mehrere Millionen IP-Amazon zu blockieren, um ... den Zello-Messenger zu blockieren.  Dank dieser Anbieter haben sie den Brief erfolgreich an alle weitergegeben, und es bestand Einigkeit darüber, dass die Konnektivität mit Amazon auseinanderfallen könnte.  Es war Freitag, wir rannten panisch zu den Kollegen von servers.ru mit den Worten: „Freunde, wir brauchen mehrere Server, die nicht in Russland, nicht in Amazon, sondern zum Beispiel irgendwo in Amsterdam sein werden.“ Um zumindest irgendwie unseren eigenen VPN und Proxy für einige Endpunkte dort zu platzieren, die wir überhaupt nicht beeinflussen können, zum Beispiel Endponts desselben S3 - wir können nicht versuchen, einen neuen Dienst zu eröffnen und eine andere IP zu erhalten Sie müssen noch dorthin gelangen.  In wenigen Tagen haben wir diese Server eingerichtet, erhöht und im Allgemeinen auf den Start der Sperren vorbereitet.  Es ist merkwürdig, dass der ILV angesichts des Hype und der erhöhten Panik sagte: "Nein, wir werden jetzt nichts blockieren."  (Dies ist jedoch genau bis zu dem Moment, als sie begannen, Telegramme zu blockieren.) Nachdem wir die Bypass-Optionen eingerichtet und festgestellt hatten, dass sie das Schloss nicht betreten hatten, haben wir das Ganze dennoch nicht abgebaut.  Also nur für den Fall. <br><br><img src="https://habrastorage.org/webt/la/o1/c1/lao1c1iqnqljjdv76qb1s9hg-qo.jpeg"><br><br>  Und im Jahr 2019 leben wir immer noch unter den Bedingungen von Schlössern.  Ich habe letzte Nacht nachgesehen: Ungefähr eine Million IP sind weiterhin blockiert.  Zwar hat Amazon fast vollständig entsperrt, in der Spitze wurden 20 Millionen Adressen erreicht ... Im Allgemeinen ist die Realität, dass Konnektivität, gute Konnektivität - möglicherweise nicht.  Auf einmal.  Es kann nicht aus technischen Gründen sein - Brände, Bagger, all das.  Oder, wie wir gesehen haben, nicht ganz technisch.  Daher kann jemand, der groß und groß ist, mit seinem eigenen AS-Kami es wahrscheinlich auf andere Weise steuern - direkte Verbindung und andere Dinge befinden sich bereits auf der l2-Ebene.  Aber in einer einfachen Version, genau wie wir oder noch kleiner, können Sie für alle Fälle Redundanz auf der Ebene von Servern haben, die an anderer Stelle angehoben wurden, im Voraus vpn, Proxy konfiguriert, mit der Möglichkeit, Konfigurationen in den Segmenten, in denen Sie über kritische Konnektivität verfügen, schnell zu wechseln .  Dies war für uns mehr als einmal nützlich, als Amazon-Sperren gestartet wurden, haben wir im schlimmsten Fall den S3-Verkehr durchgelassen, aber nach und nach ging alles schief. <br><br><h4>  Und wie reserviert man ... den ganzen Anbieter? </h4><br>  Jetzt haben wir kein Szenario für den Fall eines Ausfalls des gesamten Amazonas.  Wir haben ein ähnliches Szenario für Russland.  Wir in Russland wurden von einem Anbieter gehostet, von dem wir uns für mehrere Standorte entschieden haben.  Und vor einem Jahr sind wir auf ein Problem gestoßen: Obwohl es sich um zwei Rechenzentren handelt, kann es bereits Probleme auf der Ebene der Netzwerkkonfiguration des Anbieters geben, die beide Rechenzentren ohnehin betreffen.  Und wir können an beiden Standorten unzugänglich werden.  Das ist natürlich passiert.  Wir haben schließlich die Architektur im Inneren neu definiert.  Es hat sich nicht viel geändert, aber für Russland haben wir jetzt zwei Standorte, die nicht ein Anbieter, sondern zwei verschiedene sind.  Wenn einer von ihnen ausfällt, können wir zu einem anderen wechseln. <br><br>  Hypothetisch erwägen wir, dass Amazon auf der Ebene eines anderen Anbieters reserviert.  Vielleicht Google, vielleicht jemand anderes ... Aber bisher haben wir in der Praxis beobachtet, dass Abstürze auf der Ebene einer ganzen Region ziemlich selten sind, wenn Amazon auf derselben Ebene der Verfügbarkeitszone abstürzt.  Daher haben wir theoretisch die Idee, dass wir vielleicht eine Reservierung machen werden "Amazon ist nicht Amazon", aber in der Praxis existiert dies noch nicht. <br><br><h4>  Ein paar Worte zur Automatisierung </h4><br>  Benötigen Sie immer Automatisierung?  Es ist angebracht, an den Mahn-Krüger-Effekt zu erinnern.  Auf der x-Achse unser Wissen und unsere Erfahrung, die wir gewinnen, und auf der y-Achse das Vertrauen in unser Handeln.  Zuerst wissen wir nichts und sind uns überhaupt nicht sicher.  Dann wissen wir ein wenig und werden mega-selbstbewusst - dies ist der sogenannte "Höhepunkt der Dummheit", der durch das Bild "Demenz und Mut" gut veranschaulicht wird.  Weiter haben wir schon ein wenig gelernt und sind bereit, in die Schlacht zu ziehen.  Dann treten wir auf einen mega-ernsten Rechen und fallen in ein Tal der Verzweiflung, wenn wir etwas zu wissen scheinen, aber tatsächlich wissen wir nicht viel.  Wenn Sie dann Erfahrungen sammeln, werden wir selbstbewusster. <br><br><img src="https://habrastorage.org/webt/ha/-x/uy/ha-xuyf7ttz6wyojkkh5idgrli4.jpeg"><br><br>  Unsere Logik über verschiedene automatische Umschaltungen auf den einen oder anderen Unfall wird in dieser Grafik sehr gut beschrieben.   —    ,     .   ,       , ,  .      -:    false positive,    - , , -,    . ,     - —     .     ,       .       ,    .  Aber!     ,        ,  ,  , ,   ,     … <br><br><h4>  Fazit </h4><br>  7      , ,  - , —  -,  ,    ,   ,   —   — .    - ,     ,   ,   .      —         ,    ,           —    .     ,  -        —    s3,     ,   .         ,      ,  - - .     .     ,      , —  :  ,      —            ? , -           ,      ,     -   «,   ». <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ein vernünftiger Kompromiss zwischen Perfektionismus und realen Kräften, Zeit und Geld, die Sie für das Programm ausgeben können, das Sie letztendlich haben werden. </font></font><br><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dieser Text ist eine ergänzende und erweiterte Version des Berichts von Alexander Demidov auf der Konferenz </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Uptime Day 4</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de455112/">https://habr.com/ru/post/de455112/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de455098/index.html">Interview mit Alexander Makarov, Yii-Kernteam</a></li>
<li><a href="../de455100/index.html">Wie man Kaffee im Büro organisiert</a></li>
<li><a href="../de455102/index.html">Serielle Schnittstellen über IP verbinden</a></li>
<li><a href="../de455106/index.html">Softwareentwicklung. 2019 Trends</a></li>
<li><a href="../de455110/index.html">Hängt die Zufriedenheit der Mitarbeiter von interessanten Aufgaben ab? Erzählen Sie Badoo, SKB Kontur, Dodo Pizza, Staply und Alternativa Games</a></li>
<li><a href="../de455114/index.html">Implementierung eines Integer-Typs in CPython</a></li>
<li><a href="../de455120/index.html">Wi-Fi ist nicht jedermanns Sache. Wie kann man Ausländer im Netzwerk gesetzlich autorisieren?</a></li>
<li><a href="../de455122/index.html">Schnellere Java-Reflexionsalternative</a></li>
<li><a href="../de455126/index.html">Intelligente Kleidung der Zukunft: Gibt es Potenzial?</a></li>
<li><a href="../de455128/index.html">Wie wir Anzeigen moderieren</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>