<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ¤ğŸ¾ ğŸ‘¨ğŸ¼â€ğŸ­ ğŸ‘©â€ğŸš€ Bitrix24: "Schnell angehoben gilt nicht als gefallen" ğŸ‘©ğŸ¾â€ğŸ’» ğŸ‘©ğŸ¿â€âš–ï¸ ğŸŒ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bis heute hat der Bitrix24-Dienst nicht Hunderte von Gigabit Verkehr, es gibt keine riesige Flotte von Servern (obwohl es natÃ¼rlich viele gibt). FÃ¼r v...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bitrix24: "Schnell angehoben gilt nicht als gefallen"</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/itsumma/blog/455112/">  Bis heute hat der Bitrix24-Dienst nicht Hunderte von Gigabit Verkehr, es gibt keine riesige Flotte von Servern (obwohl es natÃ¼rlich viele gibt).  FÃ¼r viele Kunden ist es jedoch das Hauptwerkzeug fÃ¼r die Arbeit im Unternehmen. Es ist eine echte geschÃ¤ftskritische Anwendung.  Deshalb fallen - na ja, auf keinen Fall.  Aber was ist, wenn der Sturz passiert ist, aber der Dienst so schnell â€rebelliertâ€œ hat, dass niemand etwas bemerkt hat?  Und wie schaffen Sie es, ein Failover zu implementieren, ohne die ArbeitsqualitÃ¤t und die Anzahl der Kunden zu verlieren?  Alexander Demidov, Director von Bitrix24 Cloud Services, berichtete in unserem Blog Ã¼ber die Entwicklung des Backup-Systems in den sieben Jahren seines Bestehens. <br><br><img src="https://habrastorage.org/webt/xp/iz/9_/xpiz9_ri1tdppyelfb8nu9tzeg8.jpeg"><br><a name="habracut"></a><br>  â€Mit SaaS haben wir Bitrix24 vor 7 Jahren eingefÃ¼hrt.  Die Hauptschwierigkeit war wahrscheinlich die folgende: Vor dem Ã¶ffentlichen Start in Form von SaaS existierte dieses Produkt einfach im Format einer Box-LÃ¶sung.  Kunden haben es bei uns gekauft, auf ihren Servern platziert, ein Unternehmensportal eingerichtet - eine gÃ¤ngige LÃ¶sung fÃ¼r Mitarbeiterkommunikation, Dateispeicherung, Aufgabenverwaltung, CRM - das ist alles.  Und bis 2012 haben wir beschlossen, es als SaaS zu starten, es selbst zu verwalten und Fehlertoleranz und ZuverlÃ¤ssigkeit zu gewÃ¤hrleisten.  Wir haben dabei Erfahrungen gesammelt, weil wir sie bis dahin einfach nicht hatten - wir waren nur Softwarehersteller, keine Dienstleister. <br><br>  Beim Starten des Dienstes haben wir verstanden, dass das Wichtigste darin besteht, die Fehlertoleranz, ZuverlÃ¤ssigkeit und stÃ¤ndige VerfÃ¼gbarkeit des Dienstes sicherzustellen, denn wenn Sie eine einfache regulÃ¤re Website haben, zum Beispiel ein GeschÃ¤ft, das von Ihnen gefallen ist und eine Stunde liegt - nur Sie selbst leiden, verlieren Sie AuftrÃ¤ge Sie verlieren Kunden, aber fÃ¼r Ihren Kunden selbst - fÃ¼r ihn ist dies nicht sehr kritisch.  Er war natÃ¼rlich verÃ¤rgert, ging aber und kaufte auf einer anderen Seite.  Und wenn dies eine Anwendung ist, an die alle Arbeiten innerhalb des Unternehmens, der Kommunikation und der LÃ¶sungen gebunden sind, ist es das Wichtigste, das Vertrauen der Benutzer zu gewinnen, dh sie nicht im Stich zu lassen und nicht zu fallen.  Weil die ganze Arbeit aufstehen kann, wenn etwas im Inneren nicht funktioniert. <br><br><h4>  Bitrix. 24 als SaaS </h4><br>  Der erste Prototyp, den wir ein Jahr vor dem Ã¶ffentlichen Start im Jahr 2011 zusammengebaut haben.  In ungefÃ¤hr einer Woche versammelt, geschaut, verdreht - er arbeitete sogar.  Das heiÃŸt, es war mÃ¶glich, in das Formular zu gehen, dort den Namen des Portals einzugeben, ein neues Portal wurde entfaltet, eine Benutzerbasis wurde eingerichtet.  Wir haben es uns angesehen, das Produkt im Prinzip bewertet, ausgeschaltet und ein Jahr spÃ¤ter fertiggestellt.  Weil wir eine groÃŸe Aufgabe hatten: Wir wollten nicht zwei verschiedene Codebasen erstellen, wir wollten kein separates Box-Produkt, keine separaten Cloud-LÃ¶sungen unterstÃ¼tzen - wir wollten dies alles innerhalb desselben Codes tun. <br><br><img src="https://habrastorage.org/webt/lw/zu/fe/lwzufe1w3lchcx7iqtwwh04gqfu.jpeg"><br><br>  Eine typische Webanwendung zu dieser Zeit ist ein Server, auf dem PHP-Code ausgefÃ¼hrt wird, die MySQL-Basis, Dateien werden heruntergeladen, Dokumente und Bilder werden in den Upload-Daddy gestellt - nun, alles funktioniert.  Leider ist es unmÃ¶glich, einen kritisch nachhaltigen Webdienst zu betreiben.  Der verteilte Cache wird dort nicht unterstÃ¼tzt, die Datenbankreplikation wird nicht unterstÃ¼tzt. <br><br>  Wir haben die Anforderungen formuliert: Diese FÃ¤higkeit, sich an verschiedenen Standorten zu befinden, die Replikation zu unterstÃ¼tzen, idealerweise in verschiedenen geografisch verteilten Rechenzentren.  Trennen Sie die Logik des Produkts und in der Tat die Speicherung von Daten.  Dynamisch in der Lage sein, entsprechend der Last zu skalieren, machen in der Regel die Statik.  Aufgrund dieser Ãœberlegungen gab es tatsÃ¤chlich Anforderungen an das Produkt, das wir gerade im Laufe des Jahres entwickelt haben.  WÃ¤hrend dieser Zeit haben wir auf einer Plattform, die sich als einheitlich herausstellte - fÃ¼r Boxed-LÃ¶sungen, fÃ¼r unseren eigenen Service - UnterstÃ¼tzung fÃ¼r die Dinge geleistet, die wir brauchten.  UnterstÃ¼tzung fÃ¼r die MySQL-Replikation auf der Ebene des Produkts selbst: Das heiÃŸt, der Entwickler, der den Code schreibt, denkt nicht darÃ¼ber nach, wie seine Anforderungen verteilt werden, er verwendet unsere API, und wir kÃ¶nnen Schreib- und Leseanforderungen korrekt zwischen Mastern und Slaves verteilen. <br><br>  Wir haben UnterstÃ¼tzung auf Produktebene fÃ¼r verschiedene Cloud-Objektspeicher bereitgestellt: Google Storage, Amazon S3, - Plus, UnterstÃ¼tzung fÃ¼r Open Stack Swift.  Daher war es sowohl fÃ¼r uns als Service als auch fÃ¼r Entwickler, die mit einer Boxed-LÃ¶sung arbeiten, praktisch: Wenn sie nur unsere API fÃ¼r die Arbeit verwenden, denken sie nicht, wo die Datei gespeichert wird, weder lokal im Dateisystem noch im Objektdateispeicher . <br><br>  Infolgedessen haben wir sofort beschlossen, auf der Ebene eines gesamten Rechenzentrums zu reservieren.  2012 haben wir vollstÃ¤ndig in Amazon AWS gestartet, da wir bereits Erfahrung mit dieser Plattform hatten - unsere eigene Website wurde dort gehostet.  Wir waren von der Tatsache angezogen, dass es in jeder Region in Amazon mehrere Zugangszonen gibt - tatsÃ¤chlich (in ihrer Terminologie) mehrere Rechenzentren, die mehr oder weniger unabhÃ¤ngig voneinander sind und es uns ermÃ¶glichen, auf der Ebene eines gesamten Rechenzentrums zu reservieren: if es schlÃ¤gt plÃ¶tzlich fehl, die Master-Master-Datenbanken werden repliziert, die Webanwendungsserver werden reserviert und die statische Daten werden in den s3-Objektspeicher verschoben.  Die Last ist ausgeglichen - zu dieser Zeit der Amazonas-Elb, aber wenig spÃ¤ter kamen wir zu unseren eigenen Balancern, weil wir eine komplexere Logik brauchten. <br><br><h4>  Was sie wollten, haben sie bekommen ... </h4><br>  Alle grundlegenden Dinge, die wir bereitstellen wollten - die Fehlertoleranz der Server selbst, Webanwendungen, Datenbanken - alles hat gut funktioniert.  Das einfachste Szenario: Wenn einige der Webanwendungen fehlschlagen, ist alles einfach - sie werden aus dem Gleichgewicht genommen. <br><br><img src="https://habrastorage.org/webt/0y/vn/7h/0yvn7ho3syy9rpwkz8wo7govryu.jpeg"><br><br>  Der Maschinenausgleicher (damals war es ein Amazonas-Elb), der die Maschine selbst als ungesund eingestuft hat, hat die Lastverteilung auf ihnen ausgeschaltet.  Die automatische Skalierung im Amazonasgebiet funktionierte: Als die Last zunahm, wurden der Autoskalierungsgruppe neue Autos hinzugefÃ¼gt, die Ladung wurde auf neue Autos verteilt - alles war in Ordnung.  Bei unseren Balancern ist die Logik ungefÃ¤hr dieselbe: Wenn dem Anwendungsserver etwas passiert, entfernen wir Anforderungen von ihm, werfen diese Computer aus, starten neue und arbeiten weiter.  Das Schema fÃ¼r all diese Jahre hat sich ein wenig geÃ¤ndert, funktioniert aber weiterhin: Es ist einfach, verstÃ¤ndlich und es gibt keine Schwierigkeiten damit. <br><br>  Wir arbeiten auf der ganzen Welt, die Spitzenlast der Kunden ist vÃ¶llig anders, und wir sollten in der Lage sein, jederzeit bestimmte Wartungsarbeiten mit allen Komponenten unseres Systems durchzufÃ¼hren - unsichtbar fÃ¼r die Kunden.  Daher haben wir die MÃ¶glichkeit, die Datenbank von der Arbeit herunterzufahren und die Last im zweiten Rechenzentrum neu zu verteilen. <br><br>  Wie funktioniert das alles?  - Wir schalten den Datenverkehr auf ein funktionierendes Rechenzentrum um. Wenn es sich um einen Unfall in einem Rechenzentrum handelt. Wenn es sich um unsere geplante Arbeit mit einer Basis handelt, werden wir Teil des Datenverkehrs, der diese Clients bedient, in ein zweites Rechenzentrum umschalten Replikation.  Wenn Sie neue Maschinen fÃ¼r Webanwendungen benÃ¶tigen, werden diese automatisch gestartet, da die Belastung des zweiten Rechenzentrums gestiegen ist.  Wir beenden die Arbeit, die Replikation wird wiederhergestellt und wir geben die gesamte Last zurÃ¼ck.  Wenn wir einige Arbeiten im zweiten DomÃ¤nencontroller spiegeln mÃ¼ssen, z. B. Systemaktualisierungen installieren oder die Einstellungen in der zweiten Datenbank Ã¤ndern mÃ¼ssen, wiederholen wir im Allgemeinen dasselbe, nur in die andere Richtung.  Und wenn dies ein Unfall ist, dann machen wir alles auf einfache Weise: Im Ãœberwachungssystem verwenden wir den Event-Handler-Mechanismus.  Wenn mehrere ÃœberprÃ¼fungen fÃ¼r uns funktionieren und der Status kritisch wird, wird dieser Handler gestartet, ein Handler, der diese oder jene Logik ausfÃ¼hren kann.  FÃ¼r jede Datenbank haben wir registriert, welcher Server fÃ¼r sie ein Failover ist und wo Sie den Datenverkehr wechseln mÃ¼ssen, wenn er nicht verfÃ¼gbar ist.  Wir - wie es sich historisch entwickelt hat - verwenden in der einen oder anderen Form Nagios oder eine seiner Gabeln.  Im Prinzip gibt es Ã¤hnliche Mechanismen in fast jedem Ãœberwachungssystem. Wir verwenden noch nichts Komplizierteres, aber vielleicht werden wir es eines Tages tun.  Jetzt wird die Ãœberwachung durch UnzugÃ¤nglichkeit ausgelÃ¶st und kann etwas umschalten. <br><br><h4>  Haben wir alles reserviert? </h4><br>  Wir haben viele Kunden aus den USA, viele Kunden aus Europa, viele Kunden, die nÃ¤her am Osten liegen - Japan, Singapur und so weiter.  NatÃ¼rlich ein groÃŸer Teil der Kunden in Russland.  Das heiÃŸt, Arbeit ist weit davon entfernt, in einer Region zu sein.  Benutzer mÃ¶chten eine schnelle Antwort, es gibt Anforderungen fÃ¼r die Einhaltung verschiedener lokaler Gesetze, und innerhalb jeder Region reservieren wir zwei Rechenzentren. AuÃŸerdem gibt es einige zusÃ¤tzliche Dienste, die wiederum bequem in einer Region platziert werden kÃ¶nnen - fÃ¼r Kunden, die sich in dieser Region befinden Region Arbeit.  REST-Handler, Autorisierungsserver, sie sind fÃ¼r den gesamten Client weniger kritisch. Sie kÃ¶nnen mit einer kleinen akzeptablen VerzÃ¶gerung zwischen ihnen wechseln, mÃ¶chten jedoch keine FahrrÃ¤der erfinden, wie sie Ã¼berwacht werden und was mit ihnen zu tun ist.  Daher versuchen wir maximal, vorhandene LÃ¶sungen zu nutzen und keine Kompetenz fÃ¼r zusÃ¤tzliche Produkte zu entwickeln.  Und irgendwo verwenden wir trivialerweise das Umschalten auf DNS-Ebene und bestimmen die Lebendigkeit des Dienstes mit denselben DNS.  Amazon verfÃ¼gt Ã¼ber einen Route 53-Dienst, der jedoch nicht nur DNS-Daten enthÃ¤lt, sondern auch viel flexibler und bequemer ist.  Mithilfe dieser Funktion kÃ¶nnen Sie geoverteilte Dienste mit Geolokalisierung erstellen, wenn Sie damit bestimmen, woher der Client stammt, und ihnen bestimmte DatensÃ¤tze geben. Mit ihm kÃ¶nnen Sie Failover-Architekturen erstellen.  Dieselben IntegritÃ¤tsprÃ¼fungen werden in Route 53 selbst konfiguriert. Sie geben Ã¼berwachte Endpunkte an, legen Metriken fest und geben an, welche Protokolle die Lebensdauer des Dienstes bestimmen - tcp, http, https;  Legen Sie die HÃ¤ufigkeit der ÃœberprÃ¼fungen fest, mit denen festgestellt wird, ob der Dienst aktiv ist oder nicht.  Und in der DNS selbst schreiben Sie vor, was primÃ¤r sein soll, was sekundÃ¤r sein soll, wo gewechselt werden soll, wenn die IntegritÃ¤tsprÃ¼fung innerhalb der Route 53 ausgelÃ¶st wird. All dies kann mit einigen anderen Tools durchgefÃ¼hrt werden, aber was ist bequemer - wir haben es einmal eingerichtet und denken dann nicht darÃ¼ber nach wie wir prÃ¼fen, wie wir wechseln: alles funktioniert von selbst. <br><br>  <b>Das erste â€aberâ€œ</b> : Wie und wie reserviert man die Route 53 selbst?  Passiert es, wenn ihm etwas passiert?  GlÃ¼cklicherweise sind wir noch nie auf diesen Rechen getreten, aber wieder werde ich vor mir eine Geschichte haben, warum wir dachten, dass wir noch reservieren mÃ¼ssen.  Hier legen wir den Strohhalm im Voraus.  Mehrmals am Tag entladen wir alle Zonen auf Route 53 vollstÃ¤ndig.  Mit der Amazon-API kÃ¶nnen sie sicher an JSON gesendet werden. Wir haben mehrere redundante Server eingerichtet, auf denen wir sie konvertieren, in Form von Konfigurationen hochladen und grob gesagt eine Sicherungskonfiguration haben.  In diesem Fall kÃ¶nnen wir es schnell manuell bereitstellen, ohne dass die DNS-Einstellungsdaten verloren gehen. <br><br>  <b>Das zweite "aber"</b> : Was ist auf diesem Bild nicht reserviert?  Der Balancer selbst!  Wir haben die Verteilung der Kunden nach Regionen sehr einfach gemacht.  Wir haben die DomÃ¤nen bitrix24.ru, bitrix24.com und .de - jetzt gibt es 13 verschiedene DomÃ¤nen, die in sehr unterschiedlichen Zonen funktionieren.  Wir sind zu Folgendem gekommen: Jede Region hat ihre eigenen Balancer.  Die Verteilung nach Regionen ist bequemer, je nachdem, wo sich die Spitzenlast im Netzwerk befindet.  Wenn dies ein Fehler auf der Ebene eines Balancers ist, wird er einfach auÃŸer Betrieb genommen und aus DNS entfernt.  Wenn bei einer Gruppe von Balancern ein Problem auftritt, sind diese an anderen Standorten reserviert, und das Umschalten zwischen ihnen erfolgt auf derselben Route53, da aufgrund eines kurzen ttl das Umschalten maximal 2, 3, 5 Minuten dauert. <br><br>  <b>Das dritte "aber"</b> : Was wurde noch nicht reserviert?  S3, richtig.  Wir haben die Dateien, die von Benutzern in s3 gespeichert werden, platziert und aufrichtig geglaubt, dass sie panzerbrechend sind und dass dort nichts reserviert werden muss.  Aber die Geschichte zeigt, was anders passiert.  Im Allgemeinen beschreibt Amazon S3 als einen grundlegenden Dienst, da Amazon selbst S3 zum Speichern von Bildern von Maschinen, Konfigurationen, AMI-Bildern, SchnappschÃ¼ssen ... verwendet. Und wenn s3 abstÃ¼rzt, wie es in diesen 7 Jahren einmal passiert ist, wie viel bitrix24 wir verwendet haben, folgt ein Fan zieht eine Menge von allem - UnzugÃ¤nglichkeit beim Starten von virtuellen Maschinen, Fehlfunktionen der API und so weiter. <br><br>  Und S3 kann fallen - es ist einmal passiert.  Daher kamen wir zu folgendem Schema: Vor einigen Jahren gab es in Russland keine ernsthaften Ã¶ffentlichen Lager fÃ¼r Objekte, und wir Ã¼berlegten, ob wir etwas Eigenes tun kÃ¶nnten ... GlÃ¼cklicherweise haben wir damit nicht begonnen, weil wir uns mit dieser PrÃ¼fung befassen wÃ¼rden, die wir nicht durchgefÃ¼hrt haben besitzen, und hÃ¤tte es wahrscheinlich getan.  Jetzt hat Mail.ru s3-kompatible Speicher, Yandex hat es und eine Reihe von Anbietern haben es noch.  Als Ergebnis kamen wir zu dem Schluss, dass wir erstens ein Backup und zweitens die MÃ¶glichkeit haben mÃ¶chten, mit lokalen Kopien zu arbeiten.  FÃ¼r eine bestimmte russische Region verwenden wir den Mail.ru Hotbox-Dienst, der mit s3 kompatibel ist.  Wir brauchten keine ernsthaften Ã„nderungen am Code in der Anwendung und haben den folgenden Mechanismus vorgenommen: In s3 gibt es Trigger, die beim Erstellen / LÃ¶schen von Objekten funktionieren. Amazon hat einen Dienst wie Lambda - dies ist serverloser laufender Code, der nur ausgefÃ¼hrt wird wenn bestimmte Trigger ausgelÃ¶st werden. <br><br><img src="https://habrastorage.org/webt/yv/y5/cp/yvy5cp7xsn82ryeocsgos7ns7ss.jpeg"><br><br>  Wir haben es ganz einfach gemacht: Wenn unser Trigger ausgelÃ¶st wird, fÃ¼hren wir den Code aus, der das Objekt in das Mail.ru-Repository kopiert.  Um vollstÃ¤ndig mit lokalen Kopien von Daten arbeiten zu kÃ¶nnen, benÃ¶tigen wir auch eine umgekehrte Synchronisierung, damit Clients im russischen Segment mit einem Speicher arbeiten kÃ¶nnen, der nÃ¤her an ihnen liegt.  Mail ist dabei, die Trigger in seinem Repository zu vervollstÃ¤ndigen - es wird mÃ¶glich sein, eine umgekehrte Synchronisation bereits auf Infrastrukturebene durchzufÃ¼hren, aber im Moment tun wir dies auf der Ebene unseres eigenen Codes.  Wenn wir sehen, dass der Client eine Art Datei abgelegt hat, stellen wir das Ereignis auf Codeebene in die Warteschlange, verarbeiten es und fÃ¼hren die umgekehrte Replikation durch.  Warum ist es so schlimm: Wenn wir mit unseren Objekten auÃŸerhalb unseres Produkts arbeiten, dh auf externe Weise, werden wir dies nicht berÃ¼cksichtigen.  Daher warten wir bis zum Ende, wenn Trigger auf Speicherebene angezeigt werden, sodass das zu uns gekommene Objekt unabhÃ¤ngig davon, von wo aus wir den Code ausfÃ¼hren, in die andere Richtung kopiert wird. <br><br>  Auf Codeebene werden fÃ¼r jeden Client beide Repositorys registriert: eines wird als Hauptrepository betrachtet, das andere als Backup.  Wenn alles in Ordnung ist, arbeiten wir mit dem Speicher, der nÃ¤her bei uns liegt: Das heiÃŸt, unsere Kunden bei Amazon arbeiten mit S3, und diejenigen, die in Russland arbeiten, arbeiten mit Hotbox.  Wenn das KontrollkÃ¤stchen funktioniert, sollte das Failover eine Verbindung zu uns herstellen, und wir werden die Clients auf einen anderen Speicher umstellen.  Wir kÃ¶nnen dieses Flag unabhÃ¤ngig nach Region setzen und es hin und her schalten.  In der Praxis haben wir dies noch nicht verwendet, aber wir haben uns diesen Mechanismus vorgestellt und wir denken, dass wir eines Tages genau diesen Schalter brauchen und verwenden werden.  Sobald es schon passiert ist. <br><br><h4>  Oh, und dein Amazon ist entkommen ... </h4><br>  Dieser April ist der Jahrestag des Starts der Telegrammsperren in Russland.  Der am stÃ¤rksten betroffene Anbieter, der darunter fiel, ist Amazon.  Und leider litten russische Unternehmen, die auf der ganzen Welt tÃ¤tig waren, mehr. <br><br>  Wenn das Unternehmen global ist und Russland ein sehr kleines Segment ist, 3-5% - auf die eine oder andere Weise kÃ¶nnen Sie sie spenden. <br><br>  Wenn es sich um ein rein russisches Unternehmen handelt - ich bin sicher, dass Sie es vor Ort suchen mÃ¼ssen -, ist es nur so, dass die Benutzer selbst bequem und komfortabel sind und weniger Risiken bestehen. <br><br>  Und wenn dies ein Unternehmen ist, das global arbeitet und ungefÃ¤hr den gleichen Anteil an Kunden aus Russland und irgendwo auf der Welt hat?  Die KonnektivitÃ¤t der Segmente ist wichtig und sie mÃ¼ssen trotzdem miteinander arbeiten. <br><br>  Ende MÃ¤rz 2018 sandte Roskomnadzor einen Brief an die grÃ¶ÃŸten Betreiber, in dem er erklÃ¤rte, dass sie planen, mehrere Millionen IP-Amazon zu blockieren, um ... den Zello-Messenger zu blockieren.  Dank dieser Anbieter haben sie den Brief erfolgreich an alle weitergegeben, und es bestand Einigkeit darÃ¼ber, dass die KonnektivitÃ¤t mit Amazon auseinanderfallen kÃ¶nnte.  Es war Freitag, wir rannten panisch zu den Kollegen von servers.ru mit den Worten: â€Freunde, wir brauchen mehrere Server, die nicht in Russland, nicht in Amazon, sondern zum Beispiel irgendwo in Amsterdam sein werden.â€œ Um zumindest irgendwie unseren eigenen VPN und Proxy fÃ¼r einige Endpunkte dort zu platzieren, die wir Ã¼berhaupt nicht beeinflussen kÃ¶nnen, zum Beispiel Endponts desselben S3 - wir kÃ¶nnen nicht versuchen, einen neuen Dienst zu erÃ¶ffnen und eine andere IP zu erhalten Sie mÃ¼ssen noch dorthin gelangen.  In wenigen Tagen haben wir diese Server eingerichtet, erhÃ¶ht und im Allgemeinen auf den Start der Sperren vorbereitet.  Es ist merkwÃ¼rdig, dass der ILV angesichts des Hype und der erhÃ¶hten Panik sagte: "Nein, wir werden jetzt nichts blockieren."  (Dies ist jedoch genau bis zu dem Moment, als sie begannen, Telegramme zu blockieren.) Nachdem wir die Bypass-Optionen eingerichtet und festgestellt hatten, dass sie das Schloss nicht betreten hatten, haben wir das Ganze dennoch nicht abgebaut.  Also nur fÃ¼r den Fall. <br><br><img src="https://habrastorage.org/webt/la/o1/c1/lao1c1iqnqljjdv76qb1s9hg-qo.jpeg"><br><br>  Und im Jahr 2019 leben wir immer noch unter den Bedingungen von SchlÃ¶ssern.  Ich habe letzte Nacht nachgesehen: UngefÃ¤hr eine Million IP sind weiterhin blockiert.  Zwar hat Amazon fast vollstÃ¤ndig entsperrt, in der Spitze wurden 20 Millionen Adressen erreicht ... Im Allgemeinen ist die RealitÃ¤t, dass KonnektivitÃ¤t, gute KonnektivitÃ¤t - mÃ¶glicherweise nicht.  Auf einmal.  Es kann nicht aus technischen GrÃ¼nden sein - BrÃ¤nde, Bagger, all das.  Oder, wie wir gesehen haben, nicht ganz technisch.  Daher kann jemand, der groÃŸ und groÃŸ ist, mit seinem eigenen AS-Kami es wahrscheinlich auf andere Weise steuern - direkte Verbindung und andere Dinge befinden sich bereits auf der l2-Ebene.  Aber in einer einfachen Version, genau wie wir oder noch kleiner, kÃ¶nnen Sie fÃ¼r alle FÃ¤lle Redundanz auf der Ebene von Servern haben, die an anderer Stelle angehoben wurden, im Voraus vpn, Proxy konfiguriert, mit der MÃ¶glichkeit, Konfigurationen in den Segmenten, in denen Sie Ã¼ber kritische KonnektivitÃ¤t verfÃ¼gen, schnell zu wechseln .  Dies war fÃ¼r uns mehr als einmal nÃ¼tzlich, als Amazon-Sperren gestartet wurden, haben wir im schlimmsten Fall den S3-Verkehr durchgelassen, aber nach und nach ging alles schief. <br><br><h4>  Und wie reserviert man ... den ganzen Anbieter? </h4><br>  Jetzt haben wir kein Szenario fÃ¼r den Fall eines Ausfalls des gesamten Amazonas.  Wir haben ein Ã¤hnliches Szenario fÃ¼r Russland.  Wir in Russland wurden von einem Anbieter gehostet, von dem wir uns fÃ¼r mehrere Standorte entschieden haben.  Und vor einem Jahr sind wir auf ein Problem gestoÃŸen: Obwohl es sich um zwei Rechenzentren handelt, kann es bereits Probleme auf der Ebene der Netzwerkkonfiguration des Anbieters geben, die beide Rechenzentren ohnehin betreffen.  Und wir kÃ¶nnen an beiden Standorten unzugÃ¤nglich werden.  Das ist natÃ¼rlich passiert.  Wir haben schlieÃŸlich die Architektur im Inneren neu definiert.  Es hat sich nicht viel geÃ¤ndert, aber fÃ¼r Russland haben wir jetzt zwei Standorte, die nicht ein Anbieter, sondern zwei verschiedene sind.  Wenn einer von ihnen ausfÃ¤llt, kÃ¶nnen wir zu einem anderen wechseln. <br><br>  Hypothetisch erwÃ¤gen wir, dass Amazon auf der Ebene eines anderen Anbieters reserviert.  Vielleicht Google, vielleicht jemand anderes ... Aber bisher haben wir in der Praxis beobachtet, dass AbstÃ¼rze auf der Ebene einer ganzen Region ziemlich selten sind, wenn Amazon auf derselben Ebene der VerfÃ¼gbarkeitszone abstÃ¼rzt.  Daher haben wir theoretisch die Idee, dass wir vielleicht eine Reservierung machen werden "Amazon ist nicht Amazon", aber in der Praxis existiert dies noch nicht. <br><br><h4>  Ein paar Worte zur Automatisierung </h4><br>  BenÃ¶tigen Sie immer Automatisierung?  Es ist angebracht, an den Mahn-KrÃ¼ger-Effekt zu erinnern.  Auf der x-Achse unser Wissen und unsere Erfahrung, die wir gewinnen, und auf der y-Achse das Vertrauen in unser Handeln.  Zuerst wissen wir nichts und sind uns Ã¼berhaupt nicht sicher.  Dann wissen wir ein wenig und werden mega-selbstbewusst - dies ist der sogenannte "HÃ¶hepunkt der Dummheit", der durch das Bild "Demenz und Mut" gut veranschaulicht wird.  Weiter haben wir schon ein wenig gelernt und sind bereit, in die Schlacht zu ziehen.  Dann treten wir auf einen mega-ernsten Rechen und fallen in ein Tal der Verzweiflung, wenn wir etwas zu wissen scheinen, aber tatsÃ¤chlich wissen wir nicht viel.  Wenn Sie dann Erfahrungen sammeln, werden wir selbstbewusster. <br><br><img src="https://habrastorage.org/webt/ha/-x/uy/ha-xuyf7ttz6wyojkkh5idgrli4.jpeg"><br><br>  Unsere Logik Ã¼ber verschiedene automatische Umschaltungen auf den einen oder anderen Unfall wird in dieser Grafik sehr gut beschrieben.   â€”    ,     .   ,       , ,  .      -:    false positive,    - , , -,    . ,     - â€”     .     ,       .       ,    .  Aber!     ,        ,  ,  , ,   ,     â€¦ <br><br><h4>  Fazit </h4><br>  7      , ,  - , â€”  -,  ,    ,   ,   â€”   â€” .    - ,     ,   ,   .      â€”         ,    ,           â€”    .     ,  -        â€”    s3,     ,   .         ,      ,  - - .     .     ,      , â€”  :  ,      â€”            ? , -           ,      ,     -   Â«,   Â». <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ein vernÃ¼nftiger Kompromiss zwischen Perfektionismus und realen KrÃ¤ften, Zeit und Geld, die Sie fÃ¼r das Programm ausgeben kÃ¶nnen, das Sie letztendlich haben werden. </font></font><br><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dieser Text ist eine ergÃ¤nzende und erweiterte Version des Berichts von Alexander Demidov auf der Konferenz </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Uptime Day 4</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de455112/">https://habr.com/ru/post/de455112/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de455098/index.html">Interview mit Alexander Makarov, Yii-Kernteam</a></li>
<li><a href="../de455100/index.html">Wie man Kaffee im BÃ¼ro organisiert</a></li>
<li><a href="../de455102/index.html">Serielle Schnittstellen Ã¼ber IP verbinden</a></li>
<li><a href="../de455106/index.html">Softwareentwicklung. 2019 Trends</a></li>
<li><a href="../de455110/index.html">HÃ¤ngt die Zufriedenheit der Mitarbeiter von interessanten Aufgaben ab? ErzÃ¤hlen Sie Badoo, SKB Kontur, Dodo Pizza, Staply und Alternativa Games</a></li>
<li><a href="../de455114/index.html">Implementierung eines Integer-Typs in CPython</a></li>
<li><a href="../de455120/index.html">Wi-Fi ist nicht jedermanns Sache. Wie kann man AuslÃ¤nder im Netzwerk gesetzlich autorisieren?</a></li>
<li><a href="../de455122/index.html">Schnellere Java-Reflexionsalternative</a></li>
<li><a href="../de455126/index.html">Intelligente Kleidung der Zukunft: Gibt es Potenzial?</a></li>
<li><a href="../de455128/index.html">Wie wir Anzeigen moderieren</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>