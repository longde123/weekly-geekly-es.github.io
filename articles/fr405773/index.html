<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘ŒğŸ¿ ğŸ§”ğŸ¼ ğŸ² Comment tromper un ordinateur: la science insidieuse de tromper l'intelligence artificielle ğŸ‘©â€ğŸ‘§â€ğŸ‘§ ğŸ‘¨ğŸ¾â€ğŸ¤â€ğŸ‘¨ğŸ½ â¬‡ï¸</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Au dÃ©but du XXe siÃ¨cle, Wilhelm von Austin, entraÃ®neur de chevaux et mathÃ©maticien allemand, a annoncÃ© au monde qu'il avait appris Ã  compter sur un ch...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment tromper un ordinateur: la science insidieuse de tromper l'intelligence artificielle</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/405773/"><img src="https://habrastorage.org/web/c38/08f/4a7/c3808f4a70d1426ca51c7b7f9abb7627.jpg"><br><br>  Au dÃ©but du XXe siÃ¨cle, Wilhelm von Austin, entraÃ®neur de chevaux et mathÃ©maticien allemand, a annoncÃ© au monde qu'il avait appris Ã  compter sur un cheval.  Pendant des annÃ©es, von Austin a voyagÃ© Ã  travers l'Allemagne avec une dÃ©monstration de ce phÃ©nomÃ¨ne.  Il a demandÃ© Ã  son cheval, surnommÃ© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Clever Hans</a> (race <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Orlov trotter</a> ), de calculer les rÃ©sultats d'Ã©quations simples.  RÃ©pondit Hans en tapant du sabot.  Deux plus deux?  Quatre coups sÃ»rs. <br><br>  Mais les scientifiques ne pensaient pas que Hans Ã©tait aussi intelligent que von Austin le prÃ©tendait.  Le psychologue <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Karl Stumpf a</a> menÃ© une enquÃªte approfondie, surnommÃ©e le Â«comitÃ© HansÂ».  Il a dÃ©couvert que Smart Hans ne rÃ©sout pas les Ã©quations, mais rÃ©pond aux signaux visuels.  Hans a tapotÃ© son sabot jusqu'Ã  ce qu'il obtienne la bonne rÃ©ponse, aprÃ¨s quoi son entraÃ®neur et une foule enthousiaste ont Ã©clatÃ© en cris.  Et puis il s'est juste arrÃªtÃ©.  Quand il n'a pas vu ces rÃ©actions, il a continuÃ© Ã  frapper. <br><a name="habracut"></a><br>  L'informatique peut apprendre beaucoup de Hans.  L'accÃ©lÃ©ration du rythme de dÃ©veloppement dans ce domaine suggÃ¨re que la plupart de l'IA que nous avons crÃ©Ã©e s'est suffisamment formÃ©e pour fournir les bonnes rÃ©ponses, mais ne comprend pas vraiment les informations.  Et c'est facile de duper. <br><br>  Les algorithmes d'apprentissage automatique se sont rapidement transformÃ©s en bergers qui voient tout du troupeau humain.  Le logiciel nous connecte Ã  Internet, surveille le spam et le contenu malveillant dans notre courrier, et conduira bientÃ´t nos voitures.  Leur dÃ©ception dÃ©place le fondement tectonique de l'Internet et menace notre sÃ©curitÃ© Ã  l'avenir. <br><br>  De petits groupes de recherche - de la Pennsylvania State University, de Google, de l'armÃ©e amÃ©ricaine - Ã©laborent des plans pour se protÃ©ger contre les attaques potentielles contre l'IA.  Les thÃ©ories avancÃ©es dans l'Ã©tude disent qu'un attaquant peut changer ce que Â«voitÂ» un robot.  Ou activez la reconnaissance vocale sur le tÃ©lÃ©phone et forcez-le Ã  accÃ©der Ã  un site Web malveillant en utilisant des sons qui ne seront que du bruit pour une personne.  Ou laissez le virus s'infiltrer Ã  travers le pare-feu du rÃ©seau. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/0e0/75b/0eb/0e075b0eb595f3aaeafaa8233e4df1b2.jpg" alt="image"><br>  <i>Ã€ gauche, l'image du bÃ¢timent, Ã  droite, l'image modifiÃ©e, que le rÃ©seau neuronal profond concerne les autruches.</i>  <i>Au milieu, toutes les modifications appliquÃ©es Ã  l'image principale sont affichÃ©es.</i> <br><br>  Au lieu de prendre le contrÃ´le du contrÃ´le d'un robot, cette mÃ©thode lui montre quelque chose comme une hallucination - une image qui n'existe pas rÃ©ellement. <br><br>  De telles attaques utilisent des images avec une astuce [exemples contradictoires - il n'y a pas de terme russe Ã©tabli, mot pour mot il s'avÃ¨re quelque chose comme Â«exemples avec contrasteÂ» ou Â«exemples rivauxÂ» - env.  trad.]: images, sons, textes qui semblent normaux aux gens mais qui sont perÃ§us par une machine complÃ¨tement diffÃ©rente.  Les petits changements apportÃ©s par les attaquants peuvent amener le rÃ©seau neuronal profond Ã  tirer des conclusions erronÃ©es sur ce qu'il montre. <br><br>  Â«Tout systÃ¨me qui utilise l'apprentissage automatique pour prendre des dÃ©cisions critiques en matiÃ¨re de sÃ©curitÃ© est potentiellement vulnÃ©rable Ã  ce type d'attaqueÂ», a dÃ©clarÃ© Alex Kanchelyan, chercheur Ã  l'UniversitÃ© de Berkeley qui Ã©tudie les attaques d'apprentissage automatique Ã  l'aide d'images usurpÃ©es. <br><br>  ConnaÃ®tre ces nuances aux premiers stades du dÃ©veloppement de l'IA donne aux chercheurs un outil pour comprendre comment corriger ces lacunes.  Certains l'ont dÃ©jÃ  compris et disent que leurs algorithmes sont devenus de plus en plus efficaces Ã  cause de cela. <br><br>  La majeure partie du courant principal de la recherche sur l'IA est basÃ©e sur des rÃ©seaux de neurones profonds, qui Ã  leur tour sont basÃ©s sur un domaine plus large de l'apprentissage automatique.  Les technologies MoD utilisent le calcul diffÃ©rentiel et intÃ©gral et les statistiques pour crÃ©er des logiciels utilisÃ©s par la plupart d'entre nous, tels que les filtres anti-spam dans le courrier ou la recherche sur Internet.  Au cours des 20 derniÃ¨res annÃ©es, les chercheurs ont commencÃ© Ã  appliquer ces techniques Ã  une nouvelle idÃ©e, les rÃ©seaux de neurones - des structures logicielles qui imitent la fonction cÃ©rÃ©brale.  L'idÃ©e est de dÃ©centraliser les calculs sur des milliers de petites Ã©quations (Â«neuronesÂ») qui reÃ§oivent les donnÃ©es, les traitent et les transmettent plus loin, Ã  la couche suivante de milliers de petites Ã©quations. <br><br>  Ces algorithmes d'IA sont entraÃ®nÃ©s de la mÃªme maniÃ¨re que dans le cas de MO, qui, Ã  son tour, copie le processus d'apprentissage d'une personne.  On leur montre des exemples de diffÃ©rentes choses et leurs balises associÃ©es.  Montrez Ã  l'ordinateur (ou Ã  l'enfant) l'image d'un chat, dites que le chat ressemble Ã  ceci, et l'algorithme apprendra Ã  reconnaÃ®tre les chats.  Mais pour cela, l'ordinateur devra visualiser des milliers et des millions d'images de chats et de chats. <br><br>  Les chercheurs ont dÃ©couvert que ces systÃ¨mes peuvent Ãªtre attaquÃ©s avec des donnÃ©es trompeuses spÃ©cialement sÃ©lectionnÃ©es, qu'ils ont appelÃ©es Â«exemples contradictoiresÂ». <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/862/7d4/14c/8627d414c9dbe2c4d35b06ca93e1fe0e.jpg" alt="image"><br>  <i>Dans un article de 2015, des chercheurs de Google ont montrÃ© que les rÃ©seaux de neurones profonds pouvaient Ãªtre obligÃ©s d'attribuer cette image d'un panda aux gibbons.</i> <br><br>  "Nous vous montrons une photo qui montre clairement le bus scolaire et vous fait penser que c'est une autruche", a dÃ©clarÃ© Ian Goodfellow, un chercheur de Google qui travaille activement sur de telles attaques contre les rÃ©seaux de neurones. <br><br>  En ne modifiant les images fournies aux rÃ©seaux de neurones que de 4%, les chercheurs ont pu les inciter Ã  commettre des erreurs de classification dans 97% des cas.  MÃªme s'ils ne savaient pas exactement comment le rÃ©seau neuronal traite les images, ils pourraient le tromper dans 85% des cas.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">La derniÃ¨re variante de la</a> fraude sans donnÃ©es sur l'architecture du rÃ©seau est appelÃ©e Â«attaque de boÃ®te noireÂ».  Il s'agit du premier cas documentÃ© d'une attaque fonctionnelle de ce type sur un rÃ©seau neuronal profond, et son importance est qu'environ dans ce scÃ©nario, des attaques dans le monde rÃ©el peuvent avoir lieu. <br><br>  Dans l'Ã©tude, des chercheurs de la Pennsylvania State University, de Google et du US Navy Research Laboratory ont attaquÃ© un rÃ©seau de neurones qui classe les images prises en charge par le projet MetaMind et sert d'outil en ligne pour les dÃ©veloppeurs.  L'Ã©quipe a construit et formÃ© le rÃ©seau attaquÃ©, mais leur algorithme d'attaque a fonctionnÃ© indÃ©pendamment de l'architecture.  Avec un tel algorithme, ils ont pu tromper le rÃ©seau neuronal de la boÃ®te noire avec une prÃ©cision de 84,24%. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/7b1/dc3/1f4/7b1dc31f4d7eb5712d1b230f7fdabf64.jpg" alt="image"><br>  <i>La rangÃ©e supÃ©rieure de photos et de caractÃ¨res - reconnaissance correcte des caractÃ¨res.</i> <i><br></i>  <i>RangÃ©e du bas - le rÃ©seau a Ã©tÃ© forcÃ© de reconnaÃ®tre des signes complÃ¨tement faux.</i> <br><br>  Fournir des donnÃ©es inexactes aux machines n'est pas une idÃ©e nouvelle, mais Doug Tygar, professeur Ã  l'UniversitÃ© de Berkeley, qui Ã©tudie l'apprentissage machine depuis 10 ans en revanche, dit que cette technologie d'attaque a Ã©voluÃ© d'un simple MO Ã  des rÃ©seaux complexes de neurones profonds.  Les pirates malveillants utilisent cette technique sur les filtres anti-spam depuis des annÃ©es. <br><br>  Les recherches de Tiger proviennent de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ses travaux de 2006</a> sur des attaques de ce type sur un rÃ©seau avec le ministÃ¨re de la DÃ©fense, qu'il a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Ã©largi</a> en 2011 avec l'aide de chercheurs de UC Berkeley et de Microsoft Research.  L'Ã©quipe Google, la premiÃ¨re Ã  utiliser les rÃ©seaux de neurones profonds, a publiÃ© ses <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">premiers travaux</a> en 2014, deux ans aprÃ¨s avoir dÃ©couvert la possibilitÃ© de telles attaques.  Ils voulaient s'assurer que ce n'Ã©tait pas une sorte d'anomalie, mais une rÃ©elle possibilitÃ©.  En 2015, ils ont publiÃ© un autre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ouvrage</a> dans lequel ils ont dÃ©crit un moyen de protÃ©ger les rÃ©seaux et d'augmenter leur efficacitÃ©, et Ian Goodfellow a depuis donnÃ© des conseils sur d'autres travaux scientifiques dans ce domaine, y compris <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">l'attaque de la boÃ®te noire</a> . <br><br>  Les chercheurs appellent l'idÃ©e plus gÃ©nÃ©rale d'informations peu fiables Â«donnÃ©es byzantinesÂ» et, grÃ¢ce aux progrÃ¨s de la recherche, ils en sont venus Ã  un apprentissage en profondeur.  Le terme vient de la fameuse Â« <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tÃ¢che des gÃ©nÃ©raux byzantins</a> Â», une expÃ©rience de pensÃ©e dans le domaine de l'informatique, dans laquelle un groupe de gÃ©nÃ©raux doit coordonner leurs actions avec l'aide de messagers, sans avoir la certitude que l'un d'eux est un traÃ®tre.  Ils ne peuvent pas faire confiance aux informations reÃ§ues de leurs collÃ¨gues. <br><br>  Â«Ces algorithmes sont conÃ§us pour gÃ©rer le bruit alÃ©atoire, mais pas les donnÃ©es byzantinesÂ», explique Taigar.  Pour comprendre le fonctionnement de telles attaques, Goodfello suggÃ¨re d'imaginer un rÃ©seau neuronal sous la forme d'un diagramme de dispersion. <br><br>  Chaque point du diagramme reprÃ©sente un pixel de l'image traitÃ©e par le rÃ©seau neuronal.  En rÃ¨gle gÃ©nÃ©rale, le rÃ©seau essaie de tracer une ligne Ã  travers les donnÃ©es qui correspond le mieux Ã  l'ensemble de tous les points.  En pratique, c'est un peu plus compliquÃ©, car diffÃ©rents pixels ont des valeurs diffÃ©rentes pour le rÃ©seau.  En rÃ©alitÃ©, il s'agit d'un graphe multidimensionnel complexe traitÃ© par un ordinateur. <br><br>  Mais dans notre simple analogie d'un nuage de points, la forme de la ligne tracÃ©e Ã  travers les donnÃ©es dÃ©termine ce que le rÃ©seau pense voir.  Pour une attaque rÃ©ussie sur de tels systÃ¨mes, les chercheurs doivent modifier seulement une petite partie de ces points et faire en sorte que le rÃ©seau prenne une dÃ©cision qui n'existe pas.  Dans l'exemple d'un bus qui ressemble Ã  une autruche, la photo du bus scolaire est parsemÃ©e de pixels disposÃ©s selon le motif associÃ© aux caractÃ©ristiques uniques des photos d'autruches familiÃ¨res au rÃ©seau.  Il s'agit d'un contour invisible pour l'Å“il, mais lorsque l'algorithme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">traite et simplifie les donnÃ©es</a> , les points de donnÃ©es extrÃªmes pour l'autruche lui semblent une option de classification appropriÃ©e.  Dans la version boÃ®te noire, les chercheurs ont testÃ© l'utilisation de diffÃ©rentes donnÃ©es d'entrÃ©e pour dÃ©terminer comment l'algorithme voit certains objets. <br><br>  En donnant au classificateur d'objets de fausses informations et en Ã©tudiant les dÃ©cisions prises par la machine, les chercheurs ont pu restaurer l'algorithme pour tromper le systÃ¨me de reconnaissance d'image.  Potentiellement, un tel systÃ¨me dans les robomobiles dans ce cas peut voir le signe Â«cÃ©der le passageÂ» au lieu du panneau d'arrÃªt.  Lorsqu'ils ont compris le fonctionnement du rÃ©seau, ils ont pu faire voir quoi que ce soit Ã  la machine. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/d91/d4a/f16/d91d4af16d2c1644444f58a67dd42e8e.jpg" alt="image"><br>  <i>Un exemple de la faÃ§on dont le classificateur d'images dessine diffÃ©rentes lignes en fonction des diffÃ©rents objets de l'image.</i>  <i>Les faux exemples peuvent Ãªtre considÃ©rÃ©s comme des valeurs extrÃªmes sur le graphique.</i> <br><br>  Les chercheurs disent qu'une telle attaque peut Ãªtre entrÃ©e directement dans le systÃ¨me de traitement d'image, en contournant la camÃ©ra, ou que ces manipulations peuvent Ãªtre effectuÃ©es avec un vrai signe. <br><br>  Mais la spÃ©cialiste de la sÃ©curitÃ© de l'UniversitÃ© Columbia, Alison Bishop, a dÃ©clarÃ© qu'une telle prÃ©vision est irrÃ©aliste et dÃ©pend du systÃ¨me utilisÃ© dans le robot.  Si les attaquants ont dÃ©jÃ  accÃ¨s au flux de donnÃ©es de la camÃ©ra, ils peuvent dÃ©jÃ  lui donner n'importe quelle entrÃ©e. <br><br>  Â«S'ils peuvent arriver Ã  l'entrÃ©e de la camÃ©ra, de telles difficultÃ©s ne sont pas nÃ©cessairesÂ», dit-elle.  "Vous pouvez simplement lui montrer le panneau d'arrÃªt." <br><br>  D'autres mÃ©thodes d'attaque, outre le contournement de la camÃ©ra - par exemple, dessiner des marques visuelles sur un signe rÃ©el, semblent Ã  Bishop peu probables.  Elle doute que les camÃ©ras basse rÃ©solution utilisÃ©es sur les robots soient gÃ©nÃ©ralement capables de distinguer les petits changements dans l'enseigne. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/9ef/d47/a6e/9efd47a6ee9f74748cdb79c976da84bf.jpg" alt="image"><br>  <i>L'image vierge Ã  gauche est classÃ©e comme un autobus scolaire.</i>  <i>CorrigÃ© Ã  droite - comme une autruche.</i>  <i>Au milieu - l'image change.</i> <br><br>  Deux groupes, l'un Ã  l'UniversitÃ© de Berkeley et l'autre Ã  l'UniversitÃ© de Georgetown, ont dÃ©veloppÃ© avec succÃ¨s des algorithmes qui peuvent Ã©mettre des commandes vocales Ã  des assistants numÃ©riques comme Siri et Google Now, qui sonnent comme un bruit inaudible.  Pour une personne, de telles commandes ressembleront Ã  du bruit alÃ©atoire, mais en mÃªme temps, elles peuvent donner des commandes Ã  des appareils comme Alexa, non prÃ©vus par leur propriÃ©taire. <br><br>  Nicholas Carlini, l'un des chercheurs dans les attaques audio byzantines, dit que dans leurs tests, ils ont pu activer des programmes de reconnaissance audio open source, Siri et Google Now, avec une prÃ©cision de plus de 90%. <br><br>  Le bruit est comme une sorte de nÃ©gociation extraterrestre de science-fiction.  C'est un mÃ©lange de bruit blanc et de voix humaine, mais ce n'est pas du tout comme une commande vocale. <br><br>  Selon Carlini, dans une telle attaque, toute personne qui a entendu un bruit de tÃ©lÃ©phone (alors qu'il est nÃ©cessaire de planifier sÃ©parÃ©ment les attaques sur iOS et Android) peut Ãªtre obligÃ©e de se rendre sur une page Web qui fait Ã©galement du bruit, ce qui infectera les tÃ©lÃ©phones situÃ©s Ã  proximitÃ©.  Ou cette page peut tÃ©lÃ©charger tranquillement un programme malveillant.  Il est Ã©galement possible que de tels bruits disparaissent Ã  la radio et qu'ils soient cachÃ©s dans un bruit blanc ou en parallÃ¨le avec d'autres informations audio. <br><br>  De telles attaques peuvent se produire car la machine est formÃ©e pour garantir que presque toutes les donnÃ©es contiennent des donnÃ©es importantes, ainsi qu'une chose est plus courante que l'autre, comme l'explique Goodfello. <br><br>  Tromper le rÃ©seau, le forÃ§ant Ã  croire qu'il voit un objet commun, est plus facile, car il pense qu'il devrait voir ces objets plus souvent.  Par consÃ©quent, Goodfellow et un autre groupe de l'UniversitÃ© du Wyoming ont rÃ©ussi Ã  classer le rÃ©seau des images qui n'existaient pas du tout - il a identifiÃ© des objets dans le bruit blanc, crÃ©Ã© au hasard des pixels noirs et blancs. <br><br>  Dans une Ã©tude Goodfellow, un bruit blanc alÃ©atoire passant par un rÃ©seau a Ã©tÃ© classÃ© par elle comme un cheval.  Par coÃ¯ncidence, cela nous ramÃ¨ne Ã  l'histoire de Clever Hans, un cheval peu douÃ© en mathÃ©matiques. <br><br>  Goodfellow dit que les rÃ©seaux de neurones, comme Smart Hans, n'apprennent rÃ©ellement aucune idÃ©e, mais apprennent seulement Ã  dÃ©couvrir quand ils trouvent la bonne idÃ©e.  La diffÃ©rence est petite mais importante.  Le manque de connaissances fondamentales facilite les tentatives malveillantes de recrÃ©er l'apparence de trouver les Â«bonsÂ» rÃ©sultats d'algorithme, qui s'avÃ¨rent en fait faux.  Pour comprendre ce qu'est quelque chose, une machine doit Ã©galement comprendre ce qu'elle n'est pas. <br><br>  Goodfello, aprÃ¨s avoir formÃ© le rÃ©seau Ã  trier les images Ã  la fois sur les images naturelles et sur les images (fausses) traitÃ©es, a constatÃ© qu'il pouvait non seulement rÃ©duire l'efficacitÃ© de ces attaques de 90%, mais aussi permettre au rÃ©seau de mieux faire face Ã  la tÃ¢che initiale. <br><br>  Â«En permettant d'expliquer de fausses images vraiment inhabituelles, vous pouvez obtenir une explication encore plus fiable des concepts sous-jacentsÂ», explique Goodfellow. <br><br>  Deux groupes de chercheurs audio ont utilisÃ© une approche similaire Ã  celle de l'Ã©quipe Google, protÃ©geant leurs rÃ©seaux neuronaux de leurs propres attaques par un surentraÃ®nement.  Ils ont Ã©galement obtenu des succÃ¨s similaires, rÃ©duisant leur efficacitÃ© d'attaque de plus de 90%. <br><br>  Il n'est pas surprenant que ce domaine de recherche ait intÃ©ressÃ© l'armÃ©e amÃ©ricaine.  Le Laboratoire de recherche de l'armÃ©e a mÃªme parrainÃ© deux des plus rÃ©cents travaux sur ce sujet, y compris l'attaque de la boÃ®te noire.  Et bien que l'agence finance la recherche, cela ne signifie pas que la technologie sera utilisÃ©e pendant la guerre.  Selon le reprÃ©sentant du dÃ©partement, jusqu'Ã  10 ans peuvent passer de la recherche aux technologies adaptÃ©es Ã  l'usage d'un soldat. <br><br>  Ananthram Swami, chercheur au US Army Laboratory, a participÃ© Ã  plusieurs travaux rÃ©cents sur la tromperie de l'IA.  L'armÃ©e s'intÃ©resse Ã  la question de la dÃ©tection et de l'arrÃªt des donnÃ©es frauduleuses dans notre monde, oÃ¹ toutes les sources d'information ne peuvent pas Ãªtre soigneusement vÃ©rifiÃ©es.  Swami pointe vers un ensemble de donnÃ©es obtenues Ã  partir de capteurs publics situÃ©s dans les universitÃ©s et travaillant dans des projets open source. <br><br>  Â«Nous ne contrÃ´lons pas toujours toutes les donnÃ©es.  Il est assez facile pour notre adversaire de nous tromper Â», dit Swami.  Â«Dans certains cas, les consÃ©quences d'une telle fraude peuvent Ãªtre frivoles, dans certains cas, le contraire.Â» <br><br>  Il dit Ã©galement que l'armÃ©e s'intÃ©resse aux robots autonomes, aux chars et Ã  d'autres vÃ©hicules, donc le but de ces recherches est Ã©vident.  En Ã©tudiant ces questions, l'armÃ©e sera en mesure de prendre une longueur d'avance dans le dÃ©veloppement de systÃ¨mes qui ne sont pas susceptibles d'attaques de ce type. <br><br>  Mais tout groupe utilisant un rÃ©seau de neurones devrait s'inquiÃ©ter du potentiel d'attaques par usurpation d'IA.  L'apprentissage automatique et l'IA en sont Ã  leurs balbutiements, et les failles de sÃ©curitÃ© peuvent avoir des consÃ©quences dÃ©sastreuses pour le moment.  De nombreuses entreprises confient des informations hautement sensibles Ã  des systÃ¨mes d'IA qui n'ont pas passÃ© l'Ã©preuve du temps.  Nos rÃ©seaux de neurones sont encore trop jeunes pour que nous sachions tout ce dont nous avons besoin Ã  leur sujet. <br><br>  Un oubli similaire a conduit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le bot Twitter de Microsoft, Tay</a> , Ã  devenir rapidement un raciste avec un penchant pour le gÃ©nocide.  Le flux de donnÃ©es malveillantes et la fonction Â«rÃ©pÃ©ter aprÃ¨s moiÂ» ont conduit Ã  ce que Tay s'Ã©cartait considÃ©rablement du chemin prÃ©vu.  Le bot a Ã©tÃ© trompÃ© par une entrÃ©e de qualitÃ© infÃ©rieure, et cela sert d'exemple pratique d'une mauvaise mise en Å“uvre de l'apprentissage automatique. <br><br>  Kanchelyan dit qu'il ne croit pas que les possibilitÃ©s de telles attaques aient Ã©tÃ© Ã©puisÃ©es aprÃ¨s des recherches rÃ©ussies par l'Ã©quipe de Google. <br><br>  Â«Dans le domaine de la sÃ©curitÃ© informatique, les attaquants sont toujours en avance sur nousÂ», explique Kanchelyan.  Â«Il sera plutÃ´t dangereux de prÃ©tendre que nous avons rÃ©solu tous les problÃ¨mes de tromperie des rÃ©seaux de neurones grÃ¢ce Ã  leur entraÃ®nement rÃ©pÃ©tÃ©.Â» </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr405773/">https://habr.com/ru/post/fr405773/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr405763/index.html">Kingston Duo 3C - bouÃ©e de sauvetage pour cartes mÃ©moire microSD</a></li>
<li><a href="../fr405765/index.html">Maximisez vos Ã©conomies avec Canon MAXIFY: imprimantes Ã  jet d'encre compactes pour les groupes de travail de taille moyenne et les bureaux Ã  domicile</a></li>
<li><a href="../fr405767/index.html">Bitcoin Cash: Genie sorti de la bouteille</a></li>
<li><a href="../fr405769/index.html">A la recherche d'argent perdu: les administrateurs de la BTC-E ont annoncÃ© le retour du contrÃ´le de la base d'Ã©change. Mais ce n'est pas sÃ»r</a></li>
<li><a href="../fr405771/index.html">Cinq ans sur Mars</a></li>
<li><a href="../fr405775/index.html">Seul Kaspersky Anti-Virus bloque l'utilitaire CIA</a></li>
<li><a href="../fr405777/index.html">Le changement climatique pourrait rendre une partie de l'Asie du Sud inhabitÃ©e</a></li>
<li><a href="../fr405779/index.html">Les civilisations avancÃ©es peuvent construire l'Internet galactique Ã  l'aide de revues planÃ©taires</a></li>
<li><a href="../fr405781/index.html">La vie avec une Ã©toile - Partie 2: MÃ©tÃ©o spatiale</a></li>
<li><a href="../fr405783/index.html">Le sombre avenir d'Internet: inÃ©galitÃ©s et manque de libertÃ©</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>