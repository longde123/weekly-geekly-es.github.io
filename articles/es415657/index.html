<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèæ‚Äçüé® üèûÔ∏è üåä Visi√≥n artificial para el comercio minorista. C√≥mo leer etiquetas de precios en una tienda üòá üí° ü¶í</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La visi√≥n artificial es un tema muy candente en estos d√≠as. Para resolver el problema de reconocer etiquetas de tienda utilizando redes neuronales, el...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Visi√≥n artificial para el comercio minorista. C√≥mo leer etiquetas de precios en una tienda</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/sap/blog/415657/"> La visi√≥n artificial es un tema muy candente en estos d√≠as.  Para resolver el problema de reconocer etiquetas de tienda utilizando redes neuronales, elegimos el marco TensorFlow. <br><br>  El art√≠culo discutir√° exactamente c√≥mo usarlo para localizar e identificar varios objetos en la misma etiqueta de precio de la tienda, as√≠ como para reconocer su contenido.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Una tarea similar de reconocer las etiquetas de precio de IKEA ya se ha resuelto en Habr√©</a> utilizando las herramientas de procesamiento de im√°genes cl√°sicas disponibles en la biblioteca OpenCV. <br><br>  Por separado, me gustar√≠a se√±alar que la soluci√≥n puede funcionar tanto en la plataforma SAP HANA junto con Tensorflow Serving como en la plataforma SAP Cloud Platform. <br><br>  La tarea de reconocer el precio de los bienes es relevante para los compradores que desean "manipular" los precios entre ellos y elegir una tienda para compras, y para los minoristas: quieren conocer los precios de los competidores en tiempo real. <br><br>  Enough lyrics - ve a la t√©cnica! <br><a name="habracut"></a><br>  <b>Kit de herramientas</b> <br><br>  Para la detecci√≥n y clasificaci√≥n de im√°genes, utilizamos redes neuronales convolucionales implementadas en la biblioteca TensorFlow y disponibles para el control a trav√©s de la API de detecci√≥n de objetos. <br>  La API de detecci√≥n de objetos TensorFlow es un metaframe de c√≥digo abierto basado en TensorFlow que simplifica la creaci√≥n, capacitaci√≥n y despliegue de modelos para la detecci√≥n de objetos. <br><br>  Despu√©s de detectar el objeto deseado, el reconocimiento de texto se llev√≥ a cabo utilizando Tesseract, una biblioteca para el reconocimiento de caracteres.  Desde 2006, Tesseract se considera una de las bibliotecas de OCR m√°s precisas disponibles en c√≥digo abierto. <br><br>  Es posible que haga una pregunta: ¬øpor qu√© no todo el procesamiento se realiza en TF?  La respuesta es muy simple: requerir√≠a mucho m√°s tiempo para la implementaci√≥n, pero no hubo mucho de todos modos.  Era m√°s f√°cil sacrificar la velocidad de procesamiento y ensamblar un prototipo terminado que molestarse con un OCR casero. <br><br>  <b>Creaci√≥n y preparaci√≥n de un conjunto de datos.</b> <br><br>  Para empezar, era necesario recolectar materiales para el trabajo.  Visitamos 3 tiendas y tomamos alrededor de 400 fotos de diferentes etiquetas de precios en una c√°mara de tel√©fono m√≥vil en modo autom√°tico <br><br>  <i>Fotos de muestra:</i> <br><br><img src="https://habrastorage.org/webt/bp/re/ql/bpreqlti9nccwxxkuu-ubgg5e7s.png"><br>  <i>Fig.</i>  <i>1. Ejemplo de una imagen de etiqueta de precio</i> <br><br><img src="https://habrastorage.org/webt/4u/cn/9f/4ucn9fztbym8gbhpwum28rgmigg.png"><br>  <i>Fig.</i>  <i>2. Ejemplo de imagen de etiqueta de precio</i> <br><br>  Despu√©s de eso, debe procesar y marcar todas las fotos de las etiquetas de precio.  En el proceso de recopilaci√≥n de im√°genes, tratamos de recopilar im√°genes de alta calidad (sin artefactos): etiquetas de precio de aproximadamente el mismo formato, sin desenfoque, rotaciones significativas, etc.  Esto se hizo para facilitar una mayor comparaci√≥n del contenido en la etiqueta de precio real y su imagen digital.  Sin embargo, si entrenamos la red neuronal solo en las im√°genes de alta calidad disponibles, esto conducir√° muy naturalmente al hecho de que la confianza del modelo en la identificaci√≥n de ejemplos distorsionados disminuir√° significativamente.  Con el fin de entrenar a la red neuronal para que sea resistente a tales situaciones, utilizamos el conocido procedimiento para expandir el conjunto de entrenamiento con versiones distorsionadas de im√°genes (aumento).  Para complementar la muestra de entrenamiento, aplicamos algoritmos de la biblioteca Imgaug: turnos, giros peque√±os, desenfoque gaussiano, ruido.  Se agregaron im√°genes distorsionadas a la muestra, lo que la aument√≥ en aproximadamente 5 veces (de 300 a 1.500 im√°genes). <br><br>  Para marcar la imagen y seleccionar objetos, se utiliz√≥ el programa LabelImg, que est√° disponible en el dominio p√∫blico.  Le permite seleccionar los objetos necesarios en la imagen con un rect√°ngulo y asignar cada clase al cuadro delimitador.  Todas las coordenadas y etiquetas de los marcos creados para cada foto se guardan en un archivo XML separado. <br><br>  Los siguientes objetos se destacaron en cada foto: etiqueta de precio del producto, precio del producto, nombre del producto y c√≥digo de barras del producto en la etiqueta del precio.  En algunos ejemplos de im√°genes, donde estaba l√≥gicamente justificado, las √°reas estaban marcadas con superposici√≥n. <br><br><img src="https://habrastorage.org/webt/bp/ib/k-/bpibk-12lzq1m0jpaf4pxs0omak.png"><br>  <i>Fig.</i>  <i>3. Un ejemplo de una fotograf√≠a de un par de etiquetas de precio marcadas en LabelImg.</i>  <i>Se resaltan las √°reas con descripci√≥n del producto, precio y c√≥digo de barras.</i> <br><br><img src="https://habrastorage.org/webt/yl/nx/sl/ylnxslebkaqu778rxyomixqdvby.png"><br>  <i>Fig.</i>  <i>4. Un ejemplo de una fotograf√≠a de una etiqueta de precio marcada en LabelImg.</i>  <i>Se resaltan las √°reas con descripci√≥n del producto, precio y c√≥digo de barras.</i> <br><br>  Despu√©s de que todas las fotos hayan sido procesadas y marcadas, preparamos el conjunto de datos con la separaci√≥n de todas las fotos y archivos de etiquetas en una muestra de entrenamiento y prueba.  Por lo general, tome el 80% de la muestra de entrenamiento al 20% de la muestra de prueba y mezcle al azar. <br><br>  Luego, en la m√°quina donde se entrenar√° el modelo, es necesario instalar todas las bibliotecas necesarias.  En primer lugar, instalamos la biblioteca de aprendizaje autom√°tico TensorFlow.  Dependiendo del tipo de su sistema y necesita instalar una biblioteca adicional para computar en la GPU.  A continuaci√≥n, instale la biblioteca de la API de detecci√≥n de objetos de Tensorflow y bibliotecas adicionales para trabajar con im√°genes y gr√°ficos.  A continuaci√≥n hay una lista de bibliotecas que utilizamos en nuestro trabajo: <br><br>  <i>TensorFlow-GPU v1.5, CUDA v9.0, cuDNN v7.0</i> <i><br></i>  <i>Protobuf 3+, Python-tk, Pillow 1.0, lxml, tf Slim, cuaderno Jupyter, Matplotlib</i> <i><br></i>  <i>Tensorflow, Cython, Cocoapi;</i>  <i>Opencv-python;</i>  <i>Pandas</i> <br><br>  Cuando se completen todos los pasos de instalaci√≥n, puede proceder a preparar los datos y establecer los par√°metros de aprendizaje. <br><br>  <b>Entrenamiento modelo</b> <br><br>  Para resolver nuestro problema, utilizamos dos versiones de la red neuronal pre-entrenada MobileNet V2 y Faster-RCNN V2 en el conjunto de datos de coco como extractores de propiedades de imagen.  Los modelos fueron reentrenados en 4 nuevas clases: etiqueta de precio, descripci√≥n del producto, precio, c√≥digo de barras.  Como el principal, elegimos MobileNet V2, que es un modelo relativamente simple que nos permite proporcionar una calidad aceptable a una velocidad agradable.  MobileNet V2 le permite implementar el reconocimiento de im√°genes incluso en un dispositivo m√≥vil. <br><br>  Primero, debe informar a la biblioteca de la API de detecci√≥n de objetos de Tensorflow el n√∫mero de etiquetas, as√≠ como los nombres de estas etiquetas. <br><br>  Lo √∫ltimo que debe hacer antes de entrenar es crear un mapa de acceso directo y editar el archivo de configuraci√≥n.  El mapa de etiquetas informa el modelo y asigna los nombres de clase a los n√∫meros de identificador de clase para cada objeto. <br><br><img src="https://habrastorage.org/webt/xp/4x/xu/xp4xxucl6kfbkukwqwkmqrahwbk.png"><br><br>  Finalmente, debe configurar las fuentes de aprendizaje para la detecci√≥n de objetos para determinar qu√© modelo y qu√© par√°metros se utilizar√°n para la capacitaci√≥n.  Este es el √∫ltimo paso antes de comenzar a entrenar. <br><br><img src="https://habrastorage.org/webt/ci/of/zi/ciofzi9v6s53lb-tusznaisw5ss.png"><br><br>  El procedimiento de entrenamiento se inicia con el comando: <br><br><pre><code class="hljs powershell">python train.py -<span class="hljs-literal"><span class="hljs-literal">-logtostderr</span></span> -<span class="hljs-literal"><span class="hljs-literal">-train_dir</span></span>=training/ -<span class="hljs-literal"><span class="hljs-literal">-pipeline_config_path</span></span>=training/mobilenet.config</code> </pre> <br>  Si todo est√° configurado correctamente, TensorFlow inicializa el reentrenamiento de la red neuronal.  La inicializaci√≥n puede tardar hasta 30 segundos antes de que comience el entrenamiento real.  A medida que la red neuronal se vuelve a entrenar en cada paso, se mostrar√° el valor de la funci√≥n de error del algoritmo (p√©rdida).  Para MobileNet V2, el valor inicial de la funci√≥n de p√©rdida es de aproximadamente 20. El modelo debe entrenarse hasta que la funci√≥n de p√©rdida caiga a un valor de aproximadamente 2. Para visualizar el proceso de aprendizaje de la red neuronal, puede usar la pr√°ctica utilidad TensorBoard. <br><br><pre> <code class="hljs pgsql">: tensorboard <span class="hljs-comment"><span class="hljs-comment">--logdir=training</span></span></code> </pre> <br>  El comando inicializa la interfaz web en la m√°quina local, que estar√° disponible en localhost: 6006.  Despu√©s de detenerse, el procedimiento de entrenamiento se puede reanudar m√°s tarde utilizando puntos de control que se guardan cada 5 minutos. <br><br>  <b>Reconocimiento de etiquetas de precio y sus elementos.</b> <br><br>  Cuando se completa el entrenamiento, el √∫ltimo paso es crear un gr√°fico de red neuronal.  Esto se realiza mediante el comando de consola, donde debajo de los asteriscos debe especificar la mayor cantidad de archivos cpkt existentes en el directorio de capacitaci√≥n. <br><br><pre> <code class="hljs powershell">python export_inference_graph.py -<span class="hljs-literal"><span class="hljs-literal">-input_type</span></span> image_tensor -<span class="hljs-literal"><span class="hljs-literal">-pipeline_config_path</span></span> training/faster_rcnn_inception_v2.config -<span class="hljs-literal"><span class="hljs-literal">-trained_checkpoint_prefix</span></span> training/model.ckpt-**** -<span class="hljs-literal"><span class="hljs-literal">-output_directory</span></span> inference_graph</code> </pre> <br>  Despu√©s de este procedimiento, el clasificador de detecci√≥n de objetos est√° listo para funcionar.  Para verificar el reconocimiento de im√°genes, es suficiente ejecutar un script que viene con la biblioteca de detecci√≥n de objetos de Tensorflow que indica el modelo que se entren√≥ previamente y las fotos para el reconocimiento.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Aqu√≠</a> se proporciona un ejemplo de script Python est√°ndar. <br><br>  En nuestro ejemplo, lleva aproximadamente 1,5 segundos reconocer una foto usando el modelo ssd mobilenet en una computadora port√°til simple. <br><br><img src="https://habrastorage.org/webt/nd/uw/ev/nduwevuhylmr-xtfs9wcr6nmvwa.png"><br>  <i>Fig.</i>  <i>5. El resultado del reconocimiento de im√°genes con etiquetas de precio en la muestra de prueba</i> <br><br><img src="https://habrastorage.org/webt/or/-4/w-/or-4w-dq93_tjn2oj4y4f0icpag.png"><br>  <i>Fig.</i>  <i>6. El resultado del reconocimiento de im√°genes con etiquetas de precio en la muestra de prueba</i> <br><br>  Cuando estamos convencidos de que las etiquetas de precios se detectan normalmente, es necesario ense√±ar al modelo a leer informaci√≥n de elementos individuales: el precio de los productos, el nombre de los productos y un c√≥digo de barras.  Para esto, hay bibliotecas disponibles en Python para reconocer caracteres y c√≥digos de barras en fotograf√≠as: Pyzbar y Tesseract. <br><br>  Antes de comenzar a reconocer caracteres y c√≥digos de barras en una foto, debe cortar esta foto en los elementos que necesitamos, para aumentar la velocidad y no reconocer informaci√≥n innecesaria que no est√° incluida en el precio.  Tambi√©n es necesario "extraer" las coordenadas de los objetos que el modelo reconoci√≥ junto con sus clases. <br><br><img src="https://habrastorage.org/webt/hc/dk/g6/hcdkg6ngmc1l8no5paqiwm9ewga.png"><br><br>  Luego usamos estas coordenadas para cortar nuestra foto en partes para reconocer solo el √°rea necesaria. <br><br><img src="https://habrastorage.org/webt/tq/2c/q7/tq2cq7ri8ecarbn2i0qmgssexug.png"><br><img src="https://habrastorage.org/webt/hg/vi/ss/hgvissbwiljgcity6q51rt0wgg8.png"><br><img src="https://habrastorage.org/webt/db/gb/lt/dbgbltacl6uwrhswvqlw196h7rm.png"><br>  <i>Fig.</i>  <i>7. Un ejemplo de partes resaltadas de la etiqueta de precio</i> <br><br>  A continuaci√≥n, transferimos todas las √°reas recortadas a las bibliotecas: el nombre del producto y el precio del producto se transfieren a tesseract, y el c√≥digo de barras a pyzbar, y obtenemos el resultado del reconocimiento. <br><br><img src="https://habrastorage.org/webt/pn/oo/sc/pnooscbtqlhzdbbrkza9wzim_0a.png"><br><img src="https://habrastorage.org/webt/ib/gk/wb/ibgkwblzbbfmxzwie-5p4zr-4-0.png"><br>  <i>Fig.</i>  <i>8. Un ejemplo de contenido reconocido es el √°rea de etiqueta de precio.</i> <br><br>  En este punto, el reconocimiento de texto y c√≥digo de barras puede causar problemas si la imagen original estaba en baja resoluci√≥n o borrosa.  Si el precio puede reconocerse normalmente debido a los grandes n√∫meros en la etiqueta de precio, entonces el nombre del producto y el c√≥digo de barras estar√°n mal definidos o no estar√°n definidos en absoluto.  Para hacer esto, se recomienda no usar fotos peque√±as para el reconocimiento, y tambi√©n subir im√°genes sin ruido y una fuerte distorsi√≥n, por ejemplo, sin la falta de enfoque adecuado. <br><br>  Ejemplo de mal reconocimiento de imagen: <br><br><img src="https://habrastorage.org/webt/gs/gr/qm/gsgrqms2iy2x3j0ipckqhxvd3x4.png"><br><img src="https://habrastorage.org/webt/kc/od/bn/kcodbnaech2u4gr_2qp4j1qorlk.png"><br><img src="https://habrastorage.org/webt/sg/tv/ve/sgtvvedm0i1ssvdhuzmypadxb-0.png"><br><img src="https://habrastorage.org/webt/d7/cf/kj/d7cfkjdswbwqelwrbs1hcd_3yrc.png"><br>  <i>Fig.</i>  <i>9. Un ejemplo de partes resaltadas de una etiqueta de precio borrosa y contenido reconocido</i> <br><br>  En este ejemplo, puede ver que si el precio de los productos se reconoce m√°s o menos correctamente en la imagen de mala calidad, la biblioteca no podr√≠a hacer frente al nombre de los productos.  Y el c√≥digo de barras no est√° sujeto a reconocimiento en absoluto. <br><br>  El mismo texto en buena calidad. <br><br><img src="https://habrastorage.org/webt/yr/q0/kv/yrq0kvrbtwcyj7sbg_jknpwfgyg.png"><br><img src="https://habrastorage.org/webt/wh/cj/2m/whcj2mkjm-lafollpostyctv_lg.png"><br>  <i>Fig.</i>  <i>10. Ejemplo de partes destacadas de etiquetas de precio y contenido reconocido</i> <br><br>  <b>Conclusiones</b> <br><br>  Al final, logramos obtener un modelo de calidad aceptable con un bajo porcentaje de errores y un alto porcentaje de detecci√≥n de objetos relevantes.  Faster-RCNN Inception V2 tiene una mejor calidad de reconocimiento que MobileNet SSD V2, pero tiene un orden de magnitud inferior en velocidad, lo cual es una limitaci√≥n significativa. <br><br>  La precisi√≥n obtenida del reconocimiento de etiqueta de precio en una muestra retrasada de 50 im√°genes es del 100%, es decir, todas las etiquetas de precio se identificaron con √©xito en todas las fotos.  La precisi√≥n de reconocimiento de √°reas con c√≥digo de barras y precio fue del 90%.  La precisi√≥n del reconocimiento del √°rea de texto es del 85%.  La precisi√≥n de la lectura de precios fue de aproximadamente el 95%, y el texto - 80-85%.  Adem√°s, como experimento, presentamos el resultado del reconocimiento de la etiqueta de precio, que es completamente diferente de las etiquetas de precio en la muestra de capacitaci√≥n. <br><br><img src="https://habrastorage.org/webt/_c/8d/ez/_c8dezdcd7wqlpkjycogphibwau.png"><br>  <i>Fig.</i>  <i>11. Un ejemplo de reconocimiento de etiquetas de precio at√≠picas que no est√°n en el conjunto de capacitaci√≥n.</i> <br><br>  Como puede ver, incluso con etiquetas de precio que son significativamente diferentes de las etiquetas de precio de capacitaci√≥n, los modelos no est√°n exentos de errores, pero se pueden reconocer objetos significativos en la etiqueta de precio. <br><br>  <b>¬øQu√© m√°s se puede hacer?</b> <br><br>  1) Recientemente se ha publicado un art√≠culo interesante sobre el aumento autom√°tico, cuyo enfoque se puede utilizar <br>  2) El modelo entrenado terminado puede y debe estar sustancialmente comprimido <br>  3) Ejemplos de publicaci√≥n de servicios terminados en SCP y TFS <br><br>  <i>Al preparar el prototipo y este art√≠culo, se utilizaron los siguientes materiales:</i> <br><br>  1. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Trayendo Machine Learning (TensorFlow) a la empresa con SAP HANA</a> <br>  2. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SAP Leonardo ML Foundation - Traiga su propio modelo (BYOM)</a> <br>  3. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">TensorFlow Object Detection Repositorio de GitHub</a> <br>  4. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Art√≠culo de reconocimiento de cheques de IKEA</a> <br>  5. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Art√≠culo sobre los beneficios de MobileNet</a> <br>  6. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Art√≠culo de detecci√≥n de objetos TensorFlow</a> <br><br>  <i>El art√≠culo fue preparado por:</i> <i><br></i>  <i>Sergey Abdurakipov, Dmitry Buslov, Alexey Khristenko</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es415657/">https://habr.com/ru/post/es415657/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es415645/index.html">Una mirada a Highload ++ de Siberia</a></li>
<li><a href="../es415647/index.html">A partir del 1 de julio, se requieren servicios de Internet para almacenar mensajes de usuarios rusos durante 6 meses.</a></li>
<li><a href="../es415649/index.html">5G vs Wi-Fi: expectativa y realidad</a></li>
<li><a href="../es415651/index.html">Mol√©culas org√°nicas complejas descubiertas en el sat√©lite de Saturno</a></li>
<li><a href="../es415655/index.html">Principiante o experimentado? C√≥mo contratar un desarrollador m√≥vil para iOS que realmente sepa c√≥mo</a></li>
<li><a href="../es415659/index.html">La pr√°ctica de trabajar con hilos en Node.js 10.5.0</a></li>
<li><a href="../es415661/index.html">Los informes de rendimiento de los empleados son una p√©rdida de tiempo.</a></li>
<li><a href="../es415663/index.html">P√°ginas de la historia de Intel. 1101 - el primer MOS con un obturador de silicio</a></li>
<li><a href="../es415665/index.html">Proporcionando un trabajo r√°pido en el sitio como parte de la tuber√≠a de desarrollo</a></li>
<li><a href="../es415667/index.html">La ecuaci√≥n de Drake no funciona, y aqu√≠ se explica c√≥mo solucionarla</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>