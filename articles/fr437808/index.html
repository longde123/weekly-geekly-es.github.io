<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>⏹️ 👩‍💼 👩🏻‍🤝‍👨🏿 Perf et graphes de flamme 🔚 👨🏽‍✈️ 🙁</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Le sujet de l'amélioration des performances des systèmes d'exploitation et de la recherche de goulots d'étranglement gagne en popularité. Dans cet art...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Perf et graphes de flamme</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/selectel/blog/437808/"><img src="https://habrastorage.org/webt/pa/ue/x8/pauex8un6--wep6-1ehqvmciieg.png"><br><br>  Le sujet de l'amélioration des performances des systèmes d'exploitation et de la recherche de goulots d'étranglement gagne en popularité.  Dans cet article, nous parlerons d'un outil pour trouver ces endroits mêmes en utilisant l'exemple de la pile de blocs sous Linux et d'un cas de dépannage d'un hôte. <br><br><h2>  Exemple 1. Test </h2><br><h3>  Rien ne marche </h3><br>  Les tests dans notre département sont des synthétiques sur le matériel du produit et des tests de logiciels d'application ultérieurs.  Nous avons reçu un lecteur Intel Optane pour les tests.  Nous avons déjà écrit sur le test des disques Optane <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dans notre blog</a> . <br><br>  Le disque a été installé sur un serveur standard construit pendant une période relativement longue sous l'un des projets cloud. <br><a name="habracut"></a><br>  Pendant le test, le disque ne s'est pas montré de la meilleure façon: pendant le test avec une profondeur de file d'attente de 1 requête pour 1 flux, en blocs de 4 Ko environ ~ 70 Ko.  Et cela signifie que le temps de réponse est énorme: environ 13 microsecondes par requête! <br><br>  C'est étrange, car la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">spécification</a> promet "Latence - Lire 10 µs", et nous avons obtenu 30% de plus, la différence est assez significative.  Le disque a été réorganisé sur une autre plate-forme, un assemblage plus «frais» utilisé dans un autre projet. <br><br><h3>  Pourquoi ça marche? </h3><br>  C'est drôle, mais le lecteur sur la nouvelle plate-forme a fonctionné comme il se doit.  Performances augmentées, latence diminuée, CPU par étagère, 1 flux par requête, blocs de 4 octets, ~ 106 Ko à ~ 9 microsecondes par requête. <br><br>  Et puis il est temps de <s>comparer les paramètres</s> pour obtenir des <b>performances</b> de <b>jambes</b> larges.  Après tout, nous nous demandons pourquoi?  Avec <b>perf,</b> vous pouvez: <br><br><ul><li>  Prenez des relevés de compteur matériel: le nombre d'appels d'instructions, de ratés de cache, de branches incorrectement prédites, etc.  (Événements PMU) </li><li>  Supprimer les informations des points de vente statiques, le nombre d'occurrences </li><li>  Effectuer un traçage dynamique </li></ul><br>  Pour la vérification, nous avons utilisé l'échantillonnage du processeur. <br><br>  L'essentiel est que <b>perf</b> puisse compiler la trace de pile entière d'un programme en cours d'exécution.  Naturellement, l'exécution de <b>perf</b> introduira un retard dans le fonctionnement de l'ensemble du système.  Mais nous avons le drapeau <i>-F #</i> , où <i>#</i> est la fréquence d'échantillonnage, mesurée en Hz. <br><br>  Il est important de comprendre que plus la fréquence d'échantillonnage est élevée, plus il est probable qu'il intercepte un appel à une fonction particulière, mais plus le profileur apporte de freins au système.  Plus la fréquence est basse, plus il est probable que nous ne verrons pas une partie de la pile. <br><br>  Lorsque vous choisissez une fréquence, vous devez être guidé par le bon sens et une astuce - essayez de ne pas définir une fréquence paire, afin de ne pas vous retrouver dans une situation où un travail exécuté sur une minuterie avec cette fréquence pénètre dans les échantillons. <br><br>  Un autre point qui est trompeur au départ - le logiciel doit être compilé avec le drapeau <i>-fno-omit-frame-pointer</i> , si cela est bien sûr possible.  Sinon, dans la trace, au lieu des noms de fonction, nous verrons <i>des</i> valeurs <i>inconnues</i> solides.  Pour certains logiciels, les symboles de débogage sont fournis dans un package séparé, par exemple <i>someutil-dbg</i> .  Il est recommandé de les installer avant d'exécuter <b>perf</b> . <br><br>  Nous avons effectué les actions suivantes: <br><br><ul><li>  Extrait de fio de git: //git.kernel.dk/fio.git, tag fio-3.9 </li><li>  Ajout de l' <em>option -fno-omit-frame-pointer</em> à CPPFLAGS dans Makefile </li><li>  Lancement de <em>make -j8</em> </li></ul><br><pre><code class="bash hljs">perf record -g ~/fio/fio --name=<span class="hljs-built_in"><span class="hljs-built_in">test</span></span> --rw=randread --bs=4k --ioengine=pvsync2 --filename=/dev/nvme0n1 --direct=1 --hipri --filesize=1G</code> </pre> <br>  L'option -g est nécessaire pour capturer la pile de traces. <br><br>  Vous pouvez voir le résultat par la commande: <br><br><pre> <code class="bash hljs">perf report -g fractal</code> </pre> <br>  L'option <i>-g fractal</i> est nécessaire pour que les pourcentages reflétant le nombre d'échantillons avec cette fonction et montrés par <b>perf</b> soient relatifs à la fonction appelante, dont le nombre d'appels est pris à 100%. <br><br>  Vers la fin de la longue pile d'appels fio sur la plate-forme «nouvelle construction», nous verrons: <br><br><img src="https://habrastorage.org/webt/_y/pn/jb/_ypnjb3xkf3urq140p0qssevtku.png"><br><br>  Et sur la plateforme «old build»: <br><br><img src="https://habrastorage.org/webt/gq/kx/ul/gqkxulpyxspbmfudoxhh7ysdv1e.png"><br><br>  Super!  Mais je veux de beaux flammes. <br><br><h3>  Construction de flammes </h3><br>  Pour être belle, il existe deux outils: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Graphique de flamme</a> relativement plus statique </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Flamescope</a> , qui permet de sélectionner une période de temps spécifique parmi les échantillons collectés.  Ceci est très utile lorsque le code de recherche charge le CPU avec de courtes rafales. </li></ul><br>  Ces utilitaires acceptent <b>perf script&gt; result</b> comme entrée. <br><br>  Téléchargez le <i>résultat</i> et envoyez-le via des tuyaux à <i>svg</i> : <br><br><pre> <code class="bash hljs">FlameGraph/stackcollapse-perf.pl ./result | FlameGraph/flamegraph.pl &gt; ./result.svg</code> </pre> <br>  Ouvrez dans un navigateur et profitez d'une image cliquable. <br><br>  Vous pouvez utiliser une autre méthode: <br><br><ol><li>  Ajouter le <i>résultat</i> au flamescope / exemple / </li><li>  Exécutez python ./run.py </li><li>  On passe par le navigateur vers le port 5000 de l'hôte local </li></ol><br><h3>  Que voyons-nous finalement? </h3><br>  Un bon fio passe beaucoup de temps à <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">voter</a> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/o1/zg/wy/o1zgwy-l6idzwcxniq16ndbskvo.png"></div><br>  Un mauvais fio passe du temps n'importe où, mais pas dans les sondages: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/3z/er/bz/3zerbzvtrpwznzewdteyf6bexfq.png"></div><br>  À première vue, il semble que l'interrogation ne fonctionne pas sur l'ancien hôte, mais partout le noyau 4.15 est du même assemblage et l'interrogation est activée par défaut sur les disques NVMe.  Vérifiez si l'interrogation est activée dans <b>sysfs</b> : <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cat /sys/class/block/nvme0n1/queue/io_poll 1</span></span></code> </pre> <br>  Pendant les tests, des appels <i>preadv2</i> avec l'indicateur <i>RWF_HIPRI</i> sont <i>utilisés</i> - une condition nécessaire pour que l'interrogation fonctionne.  Et, si vous étudiez attentivement le graphique de la flamme (ou la capture d'écran précédente de la sortie du <b>rapport de</b> performance), vous pouvez le trouver, mais cela prend très peu de temps. <br><br>  La deuxième chose visible est la pile d'appels différente pour la fonction submit_bio () et le manque d'appels à io_schedule ().  Examinons de plus près la différence à l'intérieur de submit_bio (). <br><br>  Plate-forme lente "ancienne construction": <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/sd/ba/ew/sdbaewdxmxq2qqy7w6xwlkmpuia.png"></div><br>  Plateforme rapide "fraîche": <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/_a/d2/_w/_ad2_wholhggbeewyuoxvqdpuas.png"></div><br>  Il semble que sur une plate-forme lente, la demande va un long chemin vers l'appareil, tout en entrant dans le <b>planificateur kyber</b> .  Vous pouvez en savoir plus sur les planificateurs d'E / S dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">notre article</a> . <br><br>  Une fois le <b>kyber</b> éteint, le même test fio a montré une latence moyenne d'environ 10 microsecondes, comme indiqué dans la spécification.  Super! <br><br>  Mais d'où vient la différence dans une autre microseconde? <br><br><h3>  Et si un peu plus profond? </h3><br>  Comme déjà mentionné, <b>perf</b> vous permet de collecter des statistiques à partir de compteurs matériels.  Essayons de voir le nombre d'échecs de cache et d'instructions par cycle: <br><br><pre> <code class="bash hljs">perf <span class="hljs-built_in"><span class="hljs-built_in">stat</span></span> -e cycles,instructions,cache-references,cache-misses,bus-cycles /root/fio/fio --clocksource=cpu --name=<span class="hljs-built_in"><span class="hljs-built_in">test</span></span> --bs=4k --filename=/dev/nvme0n1p4 --direct=1 --ioengine=pvsync2 --hipri --rw=randread --filesize=4G --loops=10</code> </pre> <br><img src="https://habrastorage.org/webt/ue/pc/fo/uepcfo8up5ehpvqb1ophotqjzb8.png"><br><br><img src="https://habrastorage.org/webt/p5/ln/al/p5lnalg0u05xtvc5792ghegtv34.png"><br><br>  Les résultats montrent qu'une plate-forme rapide exécute plus d'instructions pour le cycle du processeur et présente un pourcentage plus faible de ratés de cache pendant l'exécution.  Bien entendu, nous n'entrerons pas dans les détails du fonctionnement des différentes plateformes matérielles dans le cadre de cet article. <br><br><h2>  Exemple 2. Épicerie </h2><br><h3>  Quelque chose ne va pas </h3><br>  Dans le travail d'un système de stockage distribué, une augmentation de la charge sur le CPU sur l'un des hôtes a été observée avec une augmentation du trafic entrant.  Les hôtes sont des pairs, des pairs et ont un matériel et des logiciels identiques. <br><br>  Regardons la charge CPU: <br><br><pre> <code class="bash hljs">~<span class="hljs-comment"><span class="hljs-comment"># pidstat -p 1441734 1 Linux 3.13.0-96-generic (lol) 10/10/2018 _x86_64_ (24 CPU) 09:23:30 PM UID PID %usr %system %guest %CPU CPU Command 09:23:44 PM 0 1441734 23.00 1.00 0.00 24.00 4 ceph-osd 09:23:45 PM 0 1441734 85.00 34.00 0.00 119.00 4 ceph-osd 09:23:46 PM 0 1441734 0.00 130.00 0.00 130.00 4 ceph-osd 09:23:47 PM 0 1441734 121.00 0.00 0.00 121.00 4 ceph-osd 09:23:48 PM 0 1441734 28.00 82.00 0.00 110.00 4 ceph-osd 09:23:49 PM 0 1441734 4.00 13.00 0.00 17.00 4 ceph-osd 09:23:50 PM 0 1441734 1.00 6.00 0.00 7.00 4 ceph-osd</span></span></code> </pre> <br>  Le problème est survenu à 09:23:46 et nous voyons que le processus a fonctionné dans l'espace du noyau exclusivement pendant toute la seconde.  Voyons ce qui se passait à l'intérieur. <br><br><h3>  Pourquoi si lent? </h3><br>  Dans ce cas, nous avons prélevé des échantillons de l'ensemble du système: <br><br><pre> <code class="bash hljs">perf record -a -g -- sleep 22 perf script &gt; perf.results</code> </pre> <br>  L'option <i>-a</i> est nécessaire ici pour que <b>perf</b> supprime les traces de tous les CPU. <br><br>  Ouvrez <b>perf.results</b> avec <b>flamescope</b> pour suivre le moment de l'augmentation de la charge du processeur. <br><br><div class="spoiler">  <b class="spoiler_title">Heatmap</b> <div class="spoiler_text"><div style="text-align:center;"><img src="https://habrastorage.org/webt/ao/db/hq/aodbhqbwotkcwaq99bvmbznkbrg.png"></div><br></div></div><br>  Nous avons devant nous une "carte thermique" dont les deux axes (X et Y) représentent le temps. <br><br>  Sur l'axe X, l'espace est divisé en secondes, et sur l'axe Y, en segments de 20 millisecondes en X secondes. Le temps passe de bas en haut et de gauche à droite.  Les carrés les plus brillants ont le plus grand nombre d'échantillons.  Autrement dit, le processeur à ce moment-là a fonctionné le plus activement. <br><br>  En fait, nous nous intéressons à la tache rouge au milieu.  Sélectionnez-le avec la souris, cliquez et voyez ce qu'il cache: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gv/kk/ko/gvkkkomg9vl7u1ylpwx7h8pceqc.png"></div><br>  En général, il est déjà évident que le problème est le <i>fonctionnement</i> lent <i>tcp_recvmsg</i> et <i>skb_copy_datagram_iovec</i> . <br><br>  Pour plus de clarté, comparez avec des exemples d'un autre hôte sur lequel la même quantité de trafic entrant ne pose pas de problème: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/v6/_j/72/v6_j72zqvscfolhkaipeoyx9lg8.png"></div><br>  Sur la base du fait que nous avons la même quantité de trafic entrant, des plates-formes identiques qui fonctionnent depuis longtemps sans s'arrêter, nous pouvons supposer que les problèmes sont survenus du côté du fer.  La fonction <i>skb_copy_datagram_iovec</i> copie les données de la structure du noyau vers la structure dans l'espace utilisateur pour les transmettre à l'application.  Il y a probablement des problèmes de mémoire hôte.  Dans le même temps, il n'y a aucune erreur dans les journaux. <br><br>  Nous redémarrons la plateforme.  Lors du chargement du BIOS, nous voyons un message concernant une barre de mémoire cassée.  Remplacement, l'hôte démarre et le problème avec un CPU surchargé n'est plus reproduit. <br><br><h2>  Postscript </h2><br><h3>  Performances du système avec perf </h3><br>  De manière générale, sur un système occupé, l'exécution de <b>perf</b> peut <b>entraîner</b> un retard dans le traitement des demandes.  La taille de ces retards dépend également de la charge sur le serveur. <br><br>  Essayons de trouver ce retard: <br><br><pre> <code class="bash hljs">~<span class="hljs-comment"><span class="hljs-comment"># /root/fio/fio --clocksource=cpu --name=test --bs=4k --filename=/dev/nvme0n1p4 --direct=1 --ioengine=pvsync2 --hipri --rw=randread --filesize=4G --loops=1 test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=pvsync2, iodepth=1 fio-3.9-dirty Starting 1 process Jobs: 1 (f=1): [r(1)][100.0%][r=413MiB/s][r=106k IOPS][eta 00m:00s] test: (groupid=0, jobs=1): err= 0: pid=109786: Wed Dec 12 17:25:56 2018 read: IOPS=106k, BW=414MiB/s (434MB/s)(4096MiB/9903msec) clat (nsec): min=8161, max=84768, avg=9092.68, stdev=1866.73 lat (nsec): min=8195, max=92651, avg=9127.03, stdev=1867.13 … ~# perf record /root/fio/fio --clocksource=cpu --name=test --bs=4k --filename=/dev/nvme0n1p4 --direct=1 --ioengine=pvsync2 --hipri --rw=randread --filesize=4G --loops=1 test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=pvsync2, iodepth=1 fio-3.9-dirty Starting 1 process Jobs: 1 (f=1): [r(1)][100.0%][r=413MiB/s][r=106k IOPS][eta 00m:00s] test: (groupid=0, jobs=1): err= 0: pid=109839: Wed Dec 12 17:27:50 2018 read: IOPS=106k, BW=413MiB/s (433MB/s)(4096MiB/9916msec) clat (nsec): min=8259, max=55066, avg=9102.88, stdev=1903.37 lat (nsec): min=8293, max=55096, avg=9135.43, stdev=1904.01</span></span></code> </pre> <br>  La différence n'est pas très visible, seulement environ 8 nanosecondes. <br><br>  Voyons ce qui se passe si vous augmentez la charge: <br><br><pre> <code class="bash hljs">~<span class="hljs-comment"><span class="hljs-comment"># /root/fio/fio --clocksource=cpu --name=test --numjobs=4 --bs=4k --filename=/dev/nvme0n1p4 --direct=1 --ioengine=pvsync2 --hipri --rw=randread --filesize=4G --loops=1 test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=pvsync2, iodepth=1 ... fio-3.9-dirty Starting 4 processes Jobs: 4 (f=4): [r(4)][100.0%][r=1608MiB/s][r=412k IOPS][eta 00m:00s] ~# perf record /root/fio/fio --clocksource=cpu --name=test --numjobs=4 --bs=4k --filename=/dev/nvme0n1p4 --direct=1 --ioengine=pvsync2 --hipri --rw=randread --filesize=4G --loops=1 test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=pvsync2, iodepth=1 ... fio-3.9-dirty Starting 4 processes Jobs: 4 (f=4): [r(4)][100.0%][r=1584MiB/s][r=405k IOPS][eta 00m:00s]</span></span></code> </pre> <br>  Ici, la différence devient déjà perceptible.  On peut dire que le système a ralenti de moins de 1%, mais essentiellement perdre environ 7 kiops sur un système lourdement chargé peut entraîner des problèmes. <br><br>  Il est clair que cet exemple est synthétique, néanmoins il est très révélateur. <br><br>  Essayons d'exécuter un autre test synthétique qui calcule les nombres premiers - <i>sysbench</i> : <br><br><pre> <code class="bash hljs">~<span class="hljs-comment"><span class="hljs-comment"># sysbench --max-time=10 --test=cpu run --num-threads=10 --cpu-max-prime=100000 ... Test execution summary: total time: 10.0140s total number of events: 3540 total time taken by event execution: 100.1248 per-request statistics: min: 28.26ms avg: 28.28ms max: 28.53ms approx. 95 percentile: 28.31ms Threads fairness: events (avg/stddev): 354.0000/0.00 execution time (avg/stddev): 10.0125/0.00 ~# perf record sysbench --max-time=10 --test=cpu run --num-threads=10 --cpu-max-prime=100000 … Test execution summary: total time: 10.0284s total number of events: 3498 total time taken by event execution: 100.2164 per-request statistics: min: 28.53ms avg: 28.65ms max: 28.89ms approx. 95 percentile: 28.67ms Threads fairness: events (avg/stddev): 349.8000/0.40 execution time (avg/stddev): 10.0216/0.01</span></span></code> </pre> <br>  Ici, vous pouvez voir que même le temps de traitement minimum a augmenté de 270 microsecondes. <br><br><h3>  Au lieu d'une conclusion </h3><br>  <b>Perf</b> est un outil très puissant pour analyser les performances du système et le débogage.  Cependant, comme pour tout autre outil, vous devez garder le contrôle et vous rappeler que tout système chargé sous surveillance étroite fonctionne moins bien. <br><br>  Liens connexes: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Exemples d'une seule ligne avec perf</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Perf wiki</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr437808/">https://habr.com/ru/post/fr437808/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr437796/index.html">AlphaStar a-t-il implémenté la vitesse surhumaine en tant que patch pour l'erreur d'entraînement à la simulation?</a></li>
<li><a href="../fr437800/index.html">ScrumBut dans l'équipe d'analyse: avant le décollage</a></li>
<li><a href="../fr437802/index.html">Innovate Cloud Technology: Catastrophic Cloud</a></li>
<li><a href="../fr437804/index.html">Puis-je utiliser Redux sur un serveur?</a></li>
<li><a href="../fr437806/index.html">EcmaScript 10 - JavaScript de cette année (ES2019)</a></li>
<li><a href="../fr437810/index.html">Réalité d'entreprise</a></li>
<li><a href="../fr437812/index.html">Xcode 10.2, macOS Mojave 10.14.4, iOS 12.1 et autres bêtas</a></li>
<li><a href="../fr437814/index.html">Xcode 10.2, macOS Mojave 10.14.4, iOS 12.1 et autres versions bêta</a></li>
<li><a href="../fr437816/index.html">MPLS est partout. Comment est l'infrastructure réseau Yandex.Cloud</a></li>
<li><a href="../fr437818/index.html">Nous enseignons à un ordinateur à distinguer les sons: se familiariser avec le concours DCASE et assembler votre classificateur audio en 30 minutes</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>