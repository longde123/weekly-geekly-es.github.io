<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘ŠğŸ¿ ğŸ’‰ ğŸ”œ BERT adalah model bahasa tercanggih untuk 104 bahasa. Tutorial untuk meluncurkan BERT secara lokal dan di Google Colab ğŸš• ğŸ˜« ğŸ‘°ğŸ¼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="BERT adalah jaringan saraf dari Google, yang ditunjukkan oleh hasil margin yang canggih pada sejumlah tugas. Menggunakan BERT, Anda dapat membuat prog...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>BERT adalah model bahasa tercanggih untuk 104 bahasa. Tutorial untuk meluncurkan BERT secara lokal dan di Google Colab</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/436878/"><p><img src="https://habrastorage.org/getpro/habr/post_images/2bd/0ba/1c4/2bd0ba1c4fb80fe4d771f555168c9ff0.png" alt="gambar"></p><br><p>  BERT adalah jaringan saraf dari Google, yang ditunjukkan oleh hasil margin yang canggih pada sejumlah tugas.  Menggunakan BERT, Anda dapat membuat program AI untuk memproses bahasa alami: jawab pertanyaan yang diajukan dalam bentuk apa pun, buat bot obrolan, penerjemah otomatis, analisis teks, dan sebagainya. </p><br><p>  Google telah memposting model BERT yang sudah dilatih sebelumnya, tetapi seperti biasanya dengan Machine Learning, mereka mengalami kekurangan dokumentasi.  Oleh karena itu, dalam tutorial ini kita akan belajar cara menjalankan jaringan saraf BERT di komputer lokal, serta pada server GPU gratis di Google Colab. </p><a name="habracut"></a><br><h2 id="zachem-eto-voobsche-nuzhno">  Mengapa itu perlu sama sekali </h2><br><p>  Untuk mengirimkan teks ke input jaringan saraf, Anda harus menyajikannya dalam bentuk angka.  Paling mudah untuk melakukan surat ini dengan surat, menerapkan satu huruf untuk setiap pintu masuk jaringan saraf.  Kemudian setiap huruf akan dikodekan dengan angka dari 0 hingga 32 (ditambah beberapa jenis margin untuk tanda baca).  Ini yang disebut level karakter. </p><br><p>  Tetapi hasil yang jauh lebih baik diperoleh jika kami menyajikan proposal tidak dengan satu huruf, tetapi dengan mengirimkan ke setiap input jaringan saraf segera seluruh kata (atau setidaknya suku kata).  Itu sudah akan menjadi level kata.  Opsi termudah adalah mengkompilasi kamus dengan semua kata yang ada, dan memberi makan jaringan jumlah kata dalam kamus ini.  Misalnya, jika kata "anjing" ada di kamus ini di tempat 1678, maka kita memasukkan angka 1678 untuk input dari jaringan saraf untuk kata ini. </p><br><p>  Tetapi hanya dalam bahasa alami, dengan kata "anjing", banyak asosiasi muncul sekaligus dalam diri seseorang: "lembut", "jahat", "teman seseorang".  Mungkinkah untuk menyandikan fitur pemikiran kita ini dalam presentasi untuk jaringan saraf?  Ternyata kamu bisa.  Untuk melakukan ini, cukup dengan menyusun ulang angka kata sehingga kata-kata yang dekat artinya berdiri berdampingan.  Misalnya, untuk "anjing" angka 1678, dan untuk kata "mengembang" angka 1680. Dan untuk kata "teko" jumlahnya 9000. Seperti yang Anda lihat, angka 1678 dan 1680 lebih dekat satu sama lain daripada angka 9000. </p><br><p>  Dalam praktiknya, setiap kata diberikan bukan satu angka, tetapi beberapa - satu vektor, katakanlah, dari 32 angka.  Dan jarak diukur sebagai jarak antara titik-titik yang ditunjukkan oleh vektor-vektor ini dalam ruang dimensi yang sesuai (untuk vektor 32 digit panjangnya, ini adalah ruang dengan 32 dimensi, atau dengan 32 sumbu).  Ini memungkinkan Anda untuk membandingkan satu kata sekaligus dengan beberapa kata yang hampir memiliki makna (tergantung pada sumbu mana yang harus dihitung).  Selain itu, operasi aritmatika dapat dilakukan dengan vektor.  Contoh klasik: jika Anda mengurangi vektor "pria" dari vektor yang menunjukkan kata "raja" dan menambahkan vektor untuk kata "wanita", Anda mendapatkan vektor hasil tertentu.  Dan dia secara ajaib akan sesuai dengan kata "ratu".  Dan memang, "raja adalah pria + wanita = ratu."  Keajaiban!  Dan ini bukan contoh abstrak, tetapi itu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">benar</a> - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">benar terjadi</a> .  Mempertimbangkan bahwa jaringan saraf diadaptasi dengan baik untuk transformasi matematis atas input mereka, ini tampaknya memberikan efisiensi yang tinggi dari metode ini. </p><br><p>  Pendekatan ini disebut Embeddings.  Semua paket pembelajaran mesin (TensorFlow, PyTorch) memungkinkan lapisan pertama dari jaringan saraf untuk menempatkan lapisan khusus dari Lapisan Penanaman, yang melakukan ini secara otomatis.  Artinya, pada input dari jaringan saraf kita mengirimkan nomor kata yang biasa dalam kamus, dan Embedding Layer, belajar mandiri, menerjemahkan setiap kata ke dalam vektor dengan panjang yang ditentukan, katakanlah, 32 angka. </p><br><p>  Tetapi mereka segera menyadari bahwa jauh lebih menguntungkan untuk melakukan pra-pelatihan representasi vektor kata-kata seperti itu pada beberapa kumpulan besar teks, misalnya, di seluruh Wikipedia, dan menggunakan vektor kata yang sudah jadi dalam jaringan saraf tertentu, daripada mengajar mereka lagi setiap kali. </p><br><p>  Ada beberapa cara untuk mewakili kata sebagai vektor, mereka berevolusi secara bertahap: word2vec, GloVe, Elmo. </p><br><p>  Pada musim panas 2018, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">OpenAI memperhatikan</a> bahwa jika Anda melakukan pra-latih jaringan saraf pada arsitektur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Transformer</a> pada volume teks yang besar, maka secara tak terduga dan dengan margin yang besar menunjukkan hasil yang sangat baik dalam berbagai jenis tugas pemrosesan bahasa alami.  Bahkan, jaringan saraf seperti itu pada keluarannya menciptakan representasi vektor untuk kata-kata, dan bahkan seluruh frasa.  Dan dengan menggantung di atas model bahasa semacam itu satu blok kecil sepasang lapisan neuron tambahan, Anda dapat melatih jaringan saraf ini untuk tugas apa pun. </p><br><p>  BERT dari Google adalah jaringan IPK lanjutan dari OpenAI (dua arah bukan satu arah, dll.), Juga berdasarkan pada arsitektur Transformer.  Saat ini, BERT canggih di hampir semua tolok ukur NLP populer. </p><br><h2 id="kak-oni-eto-sdelali">  Bagaimana mereka melakukannya </h2><br><p>  Gagasan di balik BERT sangat sederhana: mari memberi makan di jaringan saraf dengan frasa di mana kita mengganti 15% dari kata-kata dengan [MASK], dan melatih jaringan saraf untuk memprediksi kata-kata bertopeng ini. </p><br><p>  Sebagai contoh, jika kita mengirim frasa â€œAku datang ke [MASK] dan membeli [MASK]â€ ke input dari jaringan saraf, itu akan menunjukkan kata â€œsimpanâ€ dan â€œsusuâ€ di output.  Ini adalah contoh yang disederhanakan dari halaman BERT resmi, pada kalimat yang lebih panjang, kisaran opsi yang mungkin menjadi lebih kecil, dan respon dari jaringan saraf tidak ambigu. </p><br><p>  Dan agar jaringan saraf untuk belajar memahami hubungan antara kalimat yang berbeda, kami juga akan melatihnya untuk memprediksi apakah kalimat kedua adalah kelanjutan logis dari kalimat pertama.  Atau apakah itu frase acak yang tidak ada hubungannya dengan yang pertama. </p><br><p>  Jadi, untuk dua kalimat: "Saya pergi ke toko."  dan "Dan membeli susu di sana.", jaringan saraf harus menjawab bahwa ini logis.  Dan jika frasa kedua adalah "Crucian sky Pluto", maka saya harus menjawab bahwa proposal ini tidak ada hubungannya dengan yang pertama.  Kami akan bermain-main dengan kedua mode BERT di bawah ini. </p><br><p>  Setelah melatih jaringan saraf pada badan teks dari Wikipedia dan koleksi buku BookCorpus selama 4 hari di 16 TPU, kami mendapat BERT. </p><br><h2 id="ustanovka-i-nastroyka">  Instalasi dan pengaturan </h2><br><p>  <em><strong>Catatan</strong> : di bagian ini kami akan meluncurkan dan bermain dengan BERT di komputer lokal.</em>  <em>Untuk menjalankan jaringan saraf ini pada GPU lokal, Anda membutuhkan NVidia GTX 970 dengan memori video 4 GB atau lebih tinggi.</em>  <em>Jika Anda hanya ingin menjalankan BERT di peramban (Anda bahkan tidak memerlukan GPU di komputer Anda untuk ini), kemudian buka bagian Google Colab.</em> </p><br><p>  Pertama instal TensorFlow, jika Anda belum memilikinya, ikuti instruksi dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">https://www.tensorflow.org/install</a> .  Untuk mendukung GPU, Anda harus terlebih dahulu menginstal CUDA Toolkit 9.0, lalu cuDNN SDK 7.2, dan hanya kemudian TensorFlow dengan dukungan GPU: </p><br><pre><code class="dos hljs">pip install tensorflow-gpu</code> </pre> <br><p>  Pada dasarnya, ini cukup untuk menjalankan BERT.  Tetapi tidak ada instruksi seperti itu, Anda dapat menulis sendiri dengan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">menyortir</a> sumber dalam file <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">run_classifier.py</a> (situasi yang biasa dalam Machine Learning adalah ketika Anda harus pergi ke sumber bukan dokumentasi).  Tetapi kita akan melakukannya dengan lebih mudah dan menggunakan shell <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">BERT Keras</a> (ini juga dapat berguna untuk fine-tuning jaringan nanti, karena menyediakan antarmuka Keras yang nyaman). </p><br><p>  Untuk melakukan ini, instal Keras itu sendiri: </p><br><pre> <code class="dos hljs">pip install keras</code> </pre> <br><p>  Dan setelah Keras BERT: </p><br><pre> <code class="dos hljs">pip install keras-bert</code> </pre> <br><p>  Kita juga memerlukan file <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">tokenization.py</a> dari BERT github yang asli.  Klik tombol Raw dan simpan ke folder dengan skrip yang akan datang, atau unduh seluruh repositori dan ambil file dari sana, atau ambil salinan dari repositori dengan kode ini <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">https://github.com/blade1780/bert</a> . </p><br><p>  Sekarang saatnya untuk mengunduh jaringan saraf pra-terlatih.  Ada beberapa opsi untuk BERT, semuanya terdaftar di halaman resmi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">github.com/google-research/bert</a> .  Kami akan mengambil multibahasa universal "BERT-Base, Multilingual Cased", untuk 104 bahasa.  Unduh <a href="">multi_cased_L-12_H-768_A-12.zip</a> file (632 Mb) dan unzip ke folder dengan skrip yang akan datang. </p><br><p>  Semuanya siap, buat file BERT.py, maka akan ada sedikit kode. </p><br><p>  Impor pustaka yang diperlukan dan setel jalur </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># coding: utf-8 import sys import codecs import numpy as np from keras_bert import load_trained_model_from_checkpoint import tokenization # ,     BERT folder = 'multi_cased_L-12_H-768_A-12' config_path = folder+'/bert_config.json' checkpoint_path = folder+'/bert_model.ckpt' vocab_path = folder+'/vocab.txt'</span></span></code> </pre> <br><p>  Karena kita harus menerjemahkan garis teks biasa ke dalam format token khusus, kita akan membuat objek khusus untuk ini.  Perhatikan do_lower_case = Salah, karena kami menggunakan model Cased BERT, yang peka terhadap huruf besar-kecil. </p><br><pre> <code class="python hljs">tokenizer = tokenization.FullTokenizer(vocab_file=vocab_path, do_lower_case=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)</code> </pre> <br><p>  Memuat model </p><br><pre> <code class="python hljs">model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) model.summary()</code> </pre> <br><p>  BERT dapat bekerja dalam dua mode: menebak kata yang terlewat dalam frasa, atau menebak apakah frasa kedua masuk akal setelah yang pertama.  Kami akan melakukan kedua opsi. </p><br><p>  Untuk mode pertama, Anda perlu mengirimkan frasa dalam format: </p><br><pre> <code class="dos hljs">[<span class="hljs-built_in"><span class="hljs-built_in">CLS</span></span>]    [MASK]   [MASK]. [SEP]</code> </pre> <br><p>  Jaringan saraf seharusnya mengembalikan kalimat lengkap dengan kata-kata yang terisi sebagai ganti topeng: "Saya datang ke toko dan membeli susu." </p><br><p>  Untuk mode kedua, kedua frasa yang dipisahkan oleh pemisah harus diumpankan ke input jaringan saraf: </p><br><pre> <code class="dos hljs">[<span class="hljs-built_in"><span class="hljs-built_in">CLS</span></span>]    . [SEP]   . [SEP]</code> </pre> <br><p>  Jaringan saraf harus menjawab apakah frasa kedua merupakan kelanjutan logis dari frasa pertama.  Atau apakah itu frase acak yang tidak ada hubungannya dengan yang pertama. </p><br><p>  Agar BERT berfungsi, Anda harus menyiapkan tiga vektor, masing-masing dengan panjang 512 angka: token_input, seg_input, dan mask_input. </p><br><p>  <strong>Token_input</strong> akan menyimpan kode sumber kami yang diterjemahkan ke dalam token menggunakan tokenizer.  Frasa dalam bentuk indeks dalam kamus akan berada di awal vektor ini, dan sisanya akan diisi dengan nol. </p><br><p>  Dalam <strong>mask_input,</strong> kita harus meletakkan 1 untuk semua posisi di mana topeng [MASK] berada, dan isi sisanya dengan nol. </p><br><p>  Dalam <strong>seg_input,</strong> kita harus <strong>menunjukkan</strong> frasa pertama (termasuk CLS awal dan pemisah SEP) sebagai 0, frasa kedua (termasuk SEP akhir) sebagai 1, dan mengisi sisanya ke ujung vektor dengan nol. </p><br><p>  BERT tidak menggunakan kamus seluruh kata, tetapi lebih dari suku kata yang paling umum.  Meskipun juga memiliki kata-kata utuh.  Anda dapat membuka file vocab.txt di jaringan saraf yang diunduh dan melihat kata-kata apa yang digunakan jaringan saraf pada inputnya.  Ada banyak kata seperti Prancis.  Tetapi sebagian besar kata-kata Rusia perlu dipecah menjadi suku kata.  Jadi, kata "datang" harus dipecah menjadi "dengan" dan "## pergi".  Untuk membantu mengonversi baris teks biasa ke format yang diperlukan oleh BERT, kami menggunakan modul tokenization.py. </p><br><h2 id="rezhim-1-predskazanie-slov-zakrytyh-tokenom-mask-v-fraze">  Mode 1: Prediksi Kata-Kata Ditutup oleh Token [MASK] dalam Frasa </h2><br><p>  Frasa input yang diumpankan ke input jaringan saraf </p><br><pre> <code class="python hljs">sentence = <span class="hljs-string"><span class="hljs-string">'   [MASK]   [MASK].'</span></span> print(sentence)</code> </pre> <br><p>  Konversikan ke token.  Masalahnya adalah tokenizer tidak dapat memproses tanda layanan seperti [CLS] dan [MASK], meskipun vocab.txt memasukkannya ke dalam kamus.  Oleh karena itu, kita harus mematahkan baris kita secara manual dengan spidol [MASK] dan memilih potongan teks biasa dari sana untuk mengubahnya menjadi token BERT menggunakan tokenizer.  Tambahkan juga [CLS] di awal dan [SEP] di akhir frasa. </p><br><pre> <code class="python hljs">sentence = sentence.replace(<span class="hljs-string"><span class="hljs-string">' [MASK] '</span></span>,<span class="hljs-string"><span class="hljs-string">'[MASK]'</span></span>); sentence = sentence.replace(<span class="hljs-string"><span class="hljs-string">'[MASK] '</span></span>,<span class="hljs-string"><span class="hljs-string">'[MASK]'</span></span>); sentence = sentence.replace(<span class="hljs-string"><span class="hljs-string">' [MASK]'</span></span>,<span class="hljs-string"><span class="hljs-string">'[MASK]'</span></span>) <span class="hljs-comment"><span class="hljs-comment">#    sentence = sentence.split('[MASK]') #     tokens = ['[CLS]'] #      [CLS] #        tokenizer.tokenize(),    [MASK] for i in range(len(sentence)): if i == 0: tokens = tokens + tokenizer.tokenize(sentence[i]) else: tokens = tokens + ['[MASK]'] + tokenizer.tokenize(sentence[i]) tokens = tokens + ['[SEP]'] #      [SEP]</span></span></code> </pre> <br><p>  Token sekarang memiliki token yang dijamin akan dikonversi menjadi indeks dalam kamus.  Mari kita lakukan: </p><br><pre> <code class="python hljs">token_input = tokenizer.convert_tokens_to_ids(tokens)</code> </pre> <br><p>  Sekarang di token_input ada serangkaian angka (nomor kata dalam kamus vocab.txt) yang perlu dimasukkan ke input jaringan saraf.  Tetap hanya untuk memperpanjang vektor ini ke panjang 512 elemen.  Konstruksi Python [0] * panjang menciptakan array dengan panjang, diisi dengan nol.  Cukup tambahkan ke token kami, yang dengan python menggabungkan dua array menjadi satu. </p><br><pre> <code class="python hljs">token_input = token_input + [<span class="hljs-number"><span class="hljs-number">0</span></span>] * (<span class="hljs-number"><span class="hljs-number">512</span></span> - len(token_input))</code> </pre> <br><p>  Sekarang buat topeng mask dengan panjang 512, menempatkan 1 di mana-mana, di mana angka 103 muncul di token (yang sesuai dengan penanda [MASK] dalam kamus vocab.txt), dan mengisi sisanya dengan 0: </p><br><pre> <code class="python hljs">mask_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">512</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(mask_input)): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> token_input[i] == <span class="hljs-number"><span class="hljs-number">103</span></span>: mask_input[i] = <span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br><p>  Untuk mode operasi BERT pertama, seg_input harus sepenuhnya diisi dengan nol: </p><br><pre> <code class="python hljs">seg_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">512</span></span></code> </pre> <br><p>  Langkah terakhir, Anda perlu mengubah array python menjadi array numpy dengan bentuk (1.512), yang kami letakkan dalam subarray []: </p><br><pre> <code class="python hljs">token_input = np.asarray([token_input]) mask_input = np.asarray([mask_input]) seg_input = np.asarray([seg_input])</code> </pre> <br><p>  Oke, sudah selesai.  Sekarang jalankan prediksi jaringan saraf! </p><br><pre> <code class="python hljs">predicts = model.predict([token_input, seg_input, mask_input])[<span class="hljs-number"><span class="hljs-number">0</span></span>] predicts = np.argmax(predicts, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) predicts = predicts[<span class="hljs-number"><span class="hljs-number">0</span></span>][:len(tokens)] <span class="hljs-comment"><span class="hljs-comment">#   ,    ,       </span></span></code> </pre> <br><p>  Sekarang format hasil dari token kembali ke string yang dipisahkan oleh spasi </p><br><pre> <code class="python hljs">out = [] <span class="hljs-comment"><span class="hljs-comment">#   out     [MASK],    1  mask_input for i in range(len(mask_input[0])): if mask_input[0][i] == 1: # [0][i], ..   batch   (1,512),       out.append(predicts[i]) out = tokenizer.convert_ids_to_tokens(out) #     out = ' '.join(out) #       out = tokenization.printable_text(out) #    out = out.replace(' ##','') #   : " ##" -&gt; ""</span></span></code> </pre> <br><p>  Dan output hasilnya: </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">'Result:'</span></span>, out)</code> </pre> <br><p>  Dalam contoh kami, untuk frasa "Saya datang ke [MASK] dan membeli [MASK]."  jaringan saraf menghasilkan "rumah" dan "itu": "Saya datang ke rumah dan membelinya."  Yah, tidak terlalu buruk untuk pertama kalinya.  Membeli rumah pasti lebih baik daripada susu). </p><br><div class="spoiler">  <b class="spoiler_title">Contoh lain (saya tidak memberikan yang gagal, ada lebih dari yang sukses. Dalam kebanyakan kasus, jaringan memberikan jawaban kosong):</b> <div class="spoiler_text"><p>  Bumi adalah [MASKER] ketiga dari Matahari <br>  Hasil: Bintang </p><br><p>  sandwich terbaik [MASK] dengan mentega <br>  Hasil: Bertemu </p><br><p>  setelah [MASKER] makan siang seharusnya tidur <br>  Hasil: dari ini </p><br><p>  menjauh dari [MASKER] <br>  Hasil: ## oh - apakah itu semacam kutukan?  ) </p><br><p>  [MASKER] dari pintu <br>  Hasil: tampilan </p><br><p>  Dengan [MASKER] palu dan paku bisa membuat lemari <br>  Hasil: bantuan </p><br><p>  Dan jika besok tidak?  Hari ini, misalnya, ini bukan [MASK]! <br>  Hasil: akan </p><br><p>  Bagaimana Anda bisa bosan mengabaikan [MASK]? <br>  Hasil: dia </p><br><p>  Ada logika sehari-hari, ada logika wanita, tetapi tidak ada yang diketahui tentang laki-laki [MASK] <br>  Hasil: Filsafat </p><br><p>  Pada wanita, pada usia tiga puluh, gambar pangeran terbentuk, yang cocok dengan [MASK]. <br>  Hasil: man </p><br><p>  Dengan suara mayoritas, Putri Salju dan tujuh Kurcaci memilih [MASK], dengan satu suara menentang. <br>  Hasil: desa - huruf pertama sudah benar </p><br><p>  Nilai kebosanan Anda pada skala 10 poin: [MASK] poin <br>  Hasil: 10 </p><br><p>  [MASKER] Anda, [MASKER] dan [MASKER]! <br>  Hasil: mencintaiku aku - tidak, BERT, aku tidak bersungguh-sungguh sama sekali </p></div></div><br><p>  Anda dapat memasukkan frasa bahasa Inggris (dan apa saja dalam 104 bahasa, daftar yang <a href="">ada di sini</a> ) </p><br><p>  [MASK] harus terus! <br>  Hasil: I </p><br><h2 id="rezhim-2-proverka-logichnosti-dvuh-fraz">  Mode 2: memeriksa konsistensi dua frasa </h2><br><p>  Kami menetapkan dua frasa berurutan yang akan dimasukkan ke input jaringan saraf </p><br><pre> <code class="python hljs">sentence_1 = <span class="hljs-string"><span class="hljs-string">'   .'</span></span> sentence_2 = <span class="hljs-string"><span class="hljs-string">'  .'</span></span> print(sentence_1, <span class="hljs-string"><span class="hljs-string">'-&gt;'</span></span>, sentence_2)</code> </pre> <br><p>  Kami akan membuat token dalam format [CLS] phrase_1 [SEP] phrase_2 [SEP], mengonversi teks biasa menjadi token menggunakan tokenizer: </p><br><pre> <code class="python hljs">tokens_sen_1 = tokenizer.tokenize(sentence_1) tokens_sen_2 = tokenizer.tokenize(sentence_2) tokens = [<span class="hljs-string"><span class="hljs-string">'[CLS]'</span></span>] + tokens_sen_1 + [<span class="hljs-string"><span class="hljs-string">'[SEP]'</span></span>] + tokens_sen_2 + [<span class="hljs-string"><span class="hljs-string">'[SEP]'</span></span>]</code> </pre> <br><p>  Kami mengonversi token string ke indeks numerik (angka kata dalam kamus vocab.txt) dan memperluas vektor ke 512: </p><br><pre> <code class="python hljs">token_input = tokenizer.convert_tokens_to_ids(tokens) token_input = token_input + [<span class="hljs-number"><span class="hljs-number">0</span></span>] * (<span class="hljs-number"><span class="hljs-number">512</span></span> - len(token_input))</code> </pre> <br><p>  Topeng kata dalam kasus ini sepenuhnya diisi dengan nol </p><br><pre> <code class="python hljs">mask_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>] * <span class="hljs-number"><span class="hljs-number">512</span></span></code> </pre> <br><p>  Tetapi topeng proposal harus diisi di bawah frase kedua (termasuk SEP akhir) dengan unit, dan segala sesuatu yang lain dengan nol: </p><br><pre> <code class="python hljs">seg_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">512</span></span> len_1 = len(tokens_sen_1) + <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-comment"><span class="hljs-comment">#   , +2 -   CLS   SEP for i in range(len(tokens_sen_2)+1): # +1, ..   SEP seg_input[len_1 + i] = 1 #   ,   SEP,  #   numpy   (1,) -&gt; (1,512) token_input = np.asarray([token_input]) mask_input = np.asarray([mask_input]) seg_input = np.asarray([seg_input])</span></span></code> </pre> <br><p>  Kami melewati frasa melalui jaringan saraf (kali ini hasilnya di [1], dan tidak di [0], seperti di atas) </p><br><pre> <code class="python hljs">predicts = model.predict([token_input, seg_input, mask_input])[<span class="hljs-number"><span class="hljs-number">1</span></span>]</code> </pre> <br><p>  Dan kami memperoleh probabilitas bahwa frasa kedua adalah normal, dan bukan kumpulan kata-kata acak </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">'Sentence is okey:'</span></span>, int(round(predicts[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">100</span></span>)), <span class="hljs-string"><span class="hljs-string">'%'</span></span>)</code> </pre> <br><p>  Ke dalam dua frasa: </p><br><p>  Saya datang ke toko.  -&gt; Dan membeli susu. </p><br><p>  Respon jaringan saraf: </p><br><p>  Kalimat okey: 99% </p><br><p>  Dan jika frasa kedua adalah "Crucian sky Pluto", maka jawabannya adalah: </p><br><p>  Kalimat okey: 4% </p><br><h2 id="google-colab">  Google colab </h2><br><p>  Google menyediakan GPU server Tesla K80 gratis dengan memori video 12 Gb (TPU kini tersedia, tetapi konfigurasinya sedikit lebih rumit).  Semua kode untuk Colab harus dirancang sebagai notebook jupyter.  Untuk meluncurkan BERT di browser, cukup buka tautan </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">http://colab.research.google.com/github/blade1780/bert/blob/master/BERT.ipynb</a> </p><br><p>  Dari menu <strong>Runtime</strong> , pilih <strong>Run All</strong> , sehingga untuk pertama kalinya semua sel memulai, unduhan model dan pustaka yang diperlukan terhubung.  Setuju untuk mengatur ulang semua Runtime jika perlu. </p><br><div class="spoiler">  <b class="spoiler_title">Jika ada yang salah ...</b> <div class="spoiler_text"><p>  Pastikan GPU dan Python 3 dipilih di menu Runtime -&gt; Change runtime type </p><br><p>  Jika tombol hubungkan tidak aktif, klik untuk terhubung. </p></div></div><br><p>  Sekarang ubah <strong>kalimat</strong> baris input, <strong>kalimat_1</strong> dan <strong>kalimat_2</strong> , dan klik ikon Putar di sebelah kiri untuk memulai hanya sel saat ini.  Menjalankan seluruh notebook tidak lagi diperlukan. </p><br><p>  Anda dapat menjalankan BERT di Google Colab bahkan dari smartphone, tetapi jika itu tidak terbuka, Anda mungkin perlu mengaktifkan kotak centang Versi lengkap di pengaturan browser Anda. </p><br><h2 id="chto-dalshe">  Apa selanjutnya </h2><br><p>  Untuk melatih BERT untuk tugas tertentu, Anda perlu menambahkan satu atau dua lapisan jaringan Feed Forward yang sederhana di atasnya, dan hanya latihlah tanpa menyentuh jaringan BERT utama.  Ini dapat dilakukan pada TensorFlow telanjang atau melalui shell Keras BERT.  Pelatihan tambahan semacam itu untuk domain tertentu terjadi dengan sangat cepat dan sangat mirip dengan Fine Tuning di jaringan konvolusi.  Jadi, untuk tugas SQuAD, Anda dapat melatih jaringan saraf pada satu TPU hanya dalam 30 menit (dibandingkan dengan 4 hari pada 16 TPU untuk melatih BERT itu sendiri). </p><br><p>  Untuk melakukan ini, Anda harus mempelajari bagaimana lapisan terakhir diwakili dalam BERT, dan juga memiliki dataset yang sesuai.  Pada halaman BERT resmi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">https://github.com/google-research/bert</a> ada beberapa contoh untuk tugas yang berbeda, serta instruksi tentang cara memulai pelatihan ulang pada cloud TPUs.  Dan yang lainnya harus mencari di sumber di file <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">run_classifier.py</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">extract_features.py</a> . </p><br><h3 id="ps">  PS </h3><br><p>  Kode dan notebook jupyter untuk Google Colab yang disajikan di sini <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><strong>dihosting di repositori</strong></a> . </p><br><p>  Mukjizat seharusnya tidak diharapkan.  Jangan berharap BERT berbicara seperti orang lain.  Status state-of-the-art sama sekali tidak berarti bahwa kemajuan dalam NLP telah mencapai tingkat yang dapat diterima.  Ini hanya berarti bahwa BERT lebih baik daripada model sebelumnya, yang bahkan lebih buruk.  AI percakapan yang kuat masih sangat jauh.  Selain itu, BERT terutama merupakan model bahasa, bukan bot obrolan yang sudah jadi, sehingga menunjukkan hasil yang baik hanya setelah pelatihan ulang untuk tugas tertentu. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id436878/">https://habr.com/ru/post/id436878/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id436866/index.html">Cara mengenali proyek Agile palsu</a></li>
<li><a href="../id436868/index.html">Sematkan analisis statis ke dalam proses, bukan cari bug dengan itu</a></li>
<li><a href="../id436872/index.html">PGConf.Russia 2019 Segera Hadir</a></li>
<li><a href="../id436874/index.html">Tarian Tahun Baru di sekitar adaptor FC atau kisah tentang seberapa jauh penyebab masalah berasal dari gejala</a></li>
<li><a href="../id436876/index.html">[SAP] SAPUI5 untuk boneka bagian 1: Latihan selangkah demi selangkah yang lengkap</a></li>
<li><a href="../id436880/index.html">C ++ Dasar-dasar Templat: Templat Fungsi</a></li>
<li><a href="../id436884/index.html">Kami menguasai async / menunggu contoh nyata</a></li>
<li><a href="../id436886/index.html">Menggunakan Babel dan Webpack untuk mengatur proyek Bereaksi dari awal</a></li>
<li><a href="../id436888/index.html">Cerita tentang cara mendesain API</a></li>
<li><a href="../id436890/index.html">React Tutorial Bagian 10: Workshop Bekerja dengan Properti Komponen dan Gaya</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>