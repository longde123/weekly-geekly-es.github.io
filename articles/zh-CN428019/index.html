<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🏳️‍🌈 🍥 🙏 使用TensorFlow.js玩真人快打 🌒 🙌🏿 🏹</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="通过对Guess.js预测模型的改进进行试验，我开始密切关注深度学习：递归神经网络（RNN），尤其是LSTM，因为它们在Guess.js工作的领域具有“不合理的有效性” 。 同时，我开始研究卷积神经网络（CNN），它也经常用于时间序列。 CNN通常用于分类，识别和检测图像。 


 使用Tensor...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>使用TensorFlow.js玩真人快打</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/428019/"> 通过对<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Guess.js</a>预测<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">模型的</a>改进进行试验，我开始密切关注深度学习：递归神经网络（RNN），尤其是LSTM，因为它们在Guess.js工作的领域具有<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">“不合理的有效性”</a> 。 同时，我开始研究卷积神经网络（CNN），它也经常用于时间序列。  CNN通常用于分类，识别和检测图像。 <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1fb/9be/edc/1fb9beedcad00d1c0dcdc7bbef67e6d9.png"><br>  <i><font color="gray">使用TensorFlow.js管理<a href="">MK.js</a></font></i> <br><br><blockquote>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">本文</a>和<a href="">MK.js</a>的源代码在我的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">GitHub上</a> 。 我还没有发布训练数据集，但是您可以按照以下说明构建自己的模型并训练模型！ </blockquote><a name="habracut"></a><br> 在玩了CNN之后，我还记得几年前浏览器开发人员发布<code>getUserMedia</code> API时进行的一项<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">实验</a> 。 在其中，用户的相机充当了播放Mortal Kombat 3的小型JavaScript克隆的控制器。您可以在<a href="">GitHub存储库中</a>找到该游戏。 作为实验的一部分，我实现了一种基本的定位算法，该算法将图像分为以下几类： <br><br><ul><li> 左拳或右拳 </li><li> 左踢或右踢 </li><li> 左右步骤 </li><li> 下蹲 </li><li> 以上都不是 </li></ul><br> 该算法非常简单，我可以用几句话来解释它： <br><br><blockquote> 该算法拍摄背景。 一旦用户出现在框架中，该算法就会计算出与用户之间的背景和当前框架之间的差异。 因此，它确定了用户图形的位置。 下一步是以黑底白字显示用户的身体。 之后，建立垂直和水平直方图，将每个像素的值相加。 基于此计算，算法将确定身体的当前位置。 </blockquote><br> 视频显示了程序的工作方式。  <a href="">GitHub</a>源代码。 <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/0_yfU_iNUYo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br> 尽管微型MK克隆成功运行，但该算法还远非完美。 需要带有背景的框架。 为了正确操作，在程序执行期间，背景必须为相同的颜色。 这样的限制意味着光线，阴影和其他事物的变化会干扰并给出不准确的结果。 最后，该算法无法识别该动作。 他只将新框架分类为预定义集合中身体的位置。 <br><br> 现在，由于Web API（即WebGL）的进步，我决定通过应用TensorFlow.js返回此任务。 <br><br><h1> 引言 </h1><br> 在本文中，我将分享我使用TensorFlow.js和MobileNet创建用于对身体位置进行分类的算法的经验。 请考虑以下主题： <br><br><ul><li> 收集训练数据以进行图像分类 </li><li> 使用<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">imgaug进行</a>数据增强 </li><li> 使用MobileNet转移学习 </li><li> 二元分类和N元分类 </li><li> 在Node.js中训练TensorFlow.js图像分类模型并在浏览器中使用它 </li><li> 关于使用LSTM对动作进行分类的几句话 </li></ul><br> 在本文中，与通过一系列帧识别动作相反，我们将减少基于一个帧确定身体位置的问题。 我们将与老师一起开发深度学习模型，该模型基于用户网络摄像头中的图像来确定人的动作：踢腿，踢腿或不做任何动作。 <br><br> 到本文结尾，我们将能够构建一个播放<a href="">MK.js</a>的模型： <br><br><img src="https://habrastorage.org/webt/2u/0e/g6/2u0eg6ng2p4kwxosmut1koa751g.gif"><br><br> 为了更好地理解本文，读者应该熟悉编程和JavaScript的基本概念。 对深度学习的基本理解也是有用的，但不是必需的。 <br><br><h1> 资料收集 </h1><br> 深度学习模型的准确性高度依赖于数据质量。 与生产中一样，我们需要努力收集广泛的数据集。 <br><br> 我们的模型应该能够识别出拳和踢脚。 这意味着我们必须收集三个类别的图像： <br><br><ul><li> 踢球 </li><li> 踢球 </li><li> 其他 </li></ul><br> 在此实验中，两名志愿者（ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">@lili_vs</a>和<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">@gsamokovarov</a> ）帮助我收集了照片。 我们在MacBook Pro上录制了5个QuickTime视频，每个视频包含2-4个脚和2-4个脚。 <br><br> 然后，我们使用ffmpeg从视频中提取单个帧并将其另存为<code>jpg</code>图像： <br><br> <code>ffmpeg -i video.mov $filename%03d.jpg</code> <br> <br> 要执行上述命令，首先需要在计算机上<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">安装</a> <code>ffmpeg</code> 。 <br><br> 如果要训练模型，则必须提供输入数据和相应的输出数据，但是在此阶段，我们只有一堆包含三个人的不同姿势的图像。 要构建数据，您需要将框架分为三类：拳，踢和其他。 对于每个类别，将创建一个单独的目录，所有对应的图像都将移动到该目录中。 <br><br> 因此，在每个目录中应该有大约200张与以下图像相似的图像： <br><br><img src="https://habrastorage.org/getpro/habr/post_images/798/e9a/908/798e9a9083a1f5dfa5811fbb7de3bcc9.jpg"><br><br> 请注意，“其他”目录中将有更多的图像，因为相对较少的帧中包含有拳打脚踢的照片，而其余的帧中则是人们走动，转身或控制视频。 如果我们在一类的图像上有太多的图像，则冒着教导偏向该特定类的模型的风险。 在这种情况下，当对具有冲击力的图像进行分类时，神经网络仍可以确定类别“其他”。 为了减少这种偏见，您可以从“其他”目录中删除一些照片，并在每个类别上使用相同数量的图像训练模型。 <br><br> 为方便起见，我们在目录编号中分配的编号从<code>1</code>到<code>190</code> ，因此第一个图像为<code>1.jpg</code> ，第二个图像为<code>1.jpg</code> ，依此<code>2.jpg</code> 。 <br><br> 如果仅在同一环境下由同一个人拍摄600张照片中的模型，我们将无法获得很高的准确性。 为了充分利用我们的数据，最好使用数据增强来生成一些额外的样本。 <br><br><h1> 数据扩充 </h1><br> 数据增强是一种通过从现有集中合成新点来增加数据点数量的技术。 通常，增强用于增加训练集的大小和多样性。 我们将原始图像转移到创建新图像的转换管道中。 您不能太过激进地进行转换：应该从打孔器中生成其他手动打孔器。 <br><br> 可接受的转换包括旋转，颜色反转，模糊等。有出色的开源工具可用于数据增强。 在用JavaScript撰写本文时，没有太多选择，因此我使用了在Python中实现的库<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">-imgaug</a> 。 它具有一组可以概率应用的增强器。 <br><br> 这是此实验的数据扩充逻辑： <br><br><pre> <code class="python hljs">np.random.seed(<span class="hljs-number"><span class="hljs-number">44</span></span>) ia.seed(<span class="hljs-number"><span class="hljs-number">44</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">main</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">191</span></span>): draw_single_sequential_images(str(i), <span class="hljs-string"><span class="hljs-string">"others"</span></span>, <span class="hljs-string"><span class="hljs-string">"others-aug"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">191</span></span>): draw_single_sequential_images(str(i), <span class="hljs-string"><span class="hljs-string">"hits"</span></span>, <span class="hljs-string"><span class="hljs-string">"hits-aug"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">191</span></span>): draw_single_sequential_images(str(i), <span class="hljs-string"><span class="hljs-string">"kicks"</span></span>, <span class="hljs-string"><span class="hljs-string">"kicks-aug"</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">draw_single_sequential_images</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename, path, aug_path)</span></span></span><span class="hljs-function">:</span></span> image = misc.imresize(ndimage.imread(path + <span class="hljs-string"><span class="hljs-string">"/"</span></span> + filename + <span class="hljs-string"><span class="hljs-string">".jpg"</span></span>), (<span class="hljs-number"><span class="hljs-number">56</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>)) sometimes = <span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> aug: iaa.Sometimes(<span class="hljs-number"><span class="hljs-number">0.5</span></span>, aug) seq = iaa.Sequential( [ iaa.Fliplr(<span class="hljs-number"><span class="hljs-number">0.5</span></span>), <span class="hljs-comment"><span class="hljs-comment"># horizontally flip 50% of all images # crop images by -5% to 10% of their height/width sometimes(iaa.CropAndPad( percent=(-0.05, 0.1), pad_mode=ia.ALL, pad_cval=(0, 255) )), sometimes(iaa.Affine( scale={"x": (0.8, 1.2), "y": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis translate_percent={"x": (-0.1, 0.1), "y": (-0.1, 0.1)}, # translate by -10 to +10 percent (per axis) rotate=(-5, 5), shear=(-5, 5), # shear by -5 to +5 degrees order=[0, 1], # use nearest neighbour or bilinear interpolation (fast) cval=(0, 255), # if mode is constant, use a cval between 0 and 255 mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples) )), iaa.Grayscale(alpha=(0.0, 1.0)), iaa.Invert(0.05, per_channel=False), # invert color channels # execute 0 to 5 of the following (less important) augmenters per image # don't execute all of them, as that would often be way too strong iaa.SomeOf((0, 5), [ iaa.OneOf([ iaa.GaussianBlur((0, 2.0)), # blur images with a sigma between 0 and 2.0 iaa.AverageBlur(k=(2, 5)), # blur image using local means with kernel sizes between 2 and 5 iaa.MedianBlur(k=(3, 5)), # blur image using local medians with kernel sizes between 3 and 5 ]), iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.01*255), per_channel=0.5), # add gaussian noise to images iaa.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value) iaa.AddToHueAndSaturation((-20, 20)), # change hue and saturation # either change the brightness of the whole image (sometimes # per channel) or change the brightness of subareas iaa.OneOf([ iaa.Multiply((0.9, 1.1), per_channel=0.5), iaa.FrequencyNoiseAlpha( exponent=(-2, 0), first=iaa.Multiply((0.9, 1.1), per_channel=True), second=iaa.ContrastNormalization((0.9, 1.1)) ) ]), iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5), # improve or worsen the contrast ], random_order=True ) ], random_order=True ) im = np.zeros((16, 56, 100, 3), dtype=np.uint8) for c in range(0, 16): im[c] = image for im in range(len(grid)): misc.imsave(aug_path + "/" + filename + "_" + str(im) + ".jpg", grid[im])</span></span></code> </pre> <br> 该脚本使用具有三个<code>for</code>循环的<code>main</code>方法-每个图像类别一个。 在每次迭代中，在每个循环中，我们都调用<code>draw_single_sequential_images</code>方法：第一个参数是文件名，第二个是路径，第三个是保存结果的目录。 <br><br> 之后，我们从磁盘读取映像，并对其进行一系列转换。 我已经在上面的代码片段中记录了大多数转换，因此我们将不再重复。 <br><br> 对于每个图像，还将创建其他16张图片。 这是它们的外观示例： <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/759/ad9/43d/759ad943d7aa07dbccee4a6f26a1d920.jpg"></a> <br><br> 请注意，在上述脚本中，我们将图像缩放到<code>100x56</code>像素。 我们这样做是为了减少数据量，从而减少我们的模型在训练和评估期间执行的计算数量。 <br><br><h1> 模型制作 </h1><br> 现在建立分类模型！ <br><br> 由于我们正在处理图像，因此我们使用卷积神经网络（CNN）。 已知该网络体系结构适用于图像识别，对象检测和分类。 <br><br><h3> 学习转移 </h3><br> 下图显示了流行的CNN VGG-16，用于对图像进行分类。 <br><br><img src="https://habrastorage.org/webt/7t/0u/zk/7t0uzk4kdf4pbesgvlojn5nal18.png"><br><br>  VGG-16神经网络可识别1000种图像类别。 它有16层（不计算池和输出层）。 这样的多层网络在实践中很难训练。 这将需要大量的数据集和许多小时的培训。 <br><br> 被训练的CNN的隐藏层从边缘开始识别训练集中的图像的各种元素，然后移动到更复杂的元素，例如形状，单个对象等。  VGG-16风格的经过训练的CNN（用于识别大量图像）必须具有隐藏层，这些隐藏层已从训练集中学习了很多功能。 这些功能对于大多数图像来说是通用的，因此可以在不同的任务中重复使用。 <br><br> 学习转移使您可以重用现有且经过培训的网络。 我们可以从现有网络的任何层获取输出，并将其作为输入传递到新的神经网络。 因此，通过教导新创建的神经网络，随着时间的推移，可以教导人们识别更高级别的新功能，并从原始模型从未见过的类中正确分类图像。 <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7n/cc/a7/7ncca7e5ne2ammearn2sqnk4by0.png"></div><br><br> 为了我们的目的，请从<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">@ tensorflow-models / mobilenet软件包中</a>获取MobileNet神经网络。  MobileNet的功能与VGG-16一样强大，但体积却小得多，从而可以加快直接分发速度，即网络传播（正向传播）的速度，并减少浏览器中的下载时间。  MobileNet接受<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">ILSVRC-2012-CLS</a>图像分类<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">数据集的</a>培训。 <br><br> 在开发具有学习转移的模型时，我们有两种选择： <br><br><ol><li> 源模型的哪一层用作目标模型的输入的输出。 </li><li> 我们将训练目标模型中的多少层（如果有）。 </li></ol><br> 第一点非常重要。 根据所选层的不同，我们将以较低或较高的抽象水平获得要素，作为对神经网络的输入。 <br><br> 我们不会训练MobileNet的任何层。 我们从<code>global_average_pooling2d_1</code>输出，并将其作为输入传递到我们的微型模型。 为什么选择此特定图层？ 根据经验。 我做了一些测试，这一层工作得很好。 <br><br><h3> 型号定义 </h3><br> 最初的任务是将图像分为三类：手，脚和其他运动。 首先，让我们解决较小的问题：我们将确定框架中是否有手触。 这是一个典型的二进制分类问题。 为此，我们可以定义以下模型： <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> <span class="hljs-string"><span class="hljs-string">'@tensorflow/tfjs'</span></span>; const model = tf.sequential(); model.add(tf.layers.inputLayer({ inputShape: [<span class="hljs-number"><span class="hljs-number">1024</span></span>] })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">1024</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'relu'</span></span> })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">1</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span> })); model.compile({ optimizer: tf.train.adam(<span class="hljs-number"><span class="hljs-number">1e-6</span></span>), loss: tf.losses.sigmoidCrossEntropy, metrics: [<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>] });</code> </pre> <br> 这样的代码定义了一个简单的模型，一个具有<code>1024</code>单元的层和<code>ReLU</code>激活，以及一个通过<code>sigmoid</code>激活<code>sigmoid</code>输出单元。 后者给出一个从<code>0</code>到<code>1</code> ，具体取决于该帧中手部打击的可能性。 <br><br> 为什么我选择<code>1024</code>单元作为第二级且训练速度为<code>1e-6</code> ？ 好吧，我尝试了几种不同的选择，并且发现这种选择效果最好。  Spear方法似乎并不是最好的方法，但是在很大程度上，这就是深度学习中超参数设置的工作方式-基于对模型的理解，我们使用直觉来更新正交参数并凭经验验证模型的工作方式。 <br><br>  <code>compile</code>方法将各层编译在一起，为训练和评估准备模型。 在这里，我们宣布我们要使用<code>adam</code>优化算法。 我们还声明我们将根据交叉熵计算损失（损失），并表明我们要评估模型的准确性。 然后TensorFlow.js使用以下公式计算准确性： <br><br> <code>Accuracy = (True Positives + True Negatives) / (Positives + Negatives)</code> <br> <br> 如果您从原始MobileNet模型转移培训，则必须首先下载它。 由于在浏览器中在超过3,000张图像上训练模型是不切实际的，因此我们将使用Node.js并从文件中加载神经网络。 <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">在此处</a>下载MobileNet。 目录包含文件<code>model.json</code> ，其中包含模型的体系结构-层，激活等。 其余文件包含模型参数。 您可以使用以下代码从文件中加载模型： <br><br><pre> <code class="python hljs">export const loadModel = <span class="hljs-keyword"><span class="hljs-keyword">async</span></span> () =&gt; { const mn = new mobilenet.MobileNet(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>); mn.path = `file://PATH/TO/model.json`; <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> mn.load(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (input): tf.Tensor1D =&gt; mn.infer(input, <span class="hljs-string"><span class="hljs-string">'global_average_pooling2d_1'</span></span>) .reshape([<span class="hljs-number"><span class="hljs-number">1024</span></span>]); };</code> </pre> <br> 请注意，在<code>loadModel</code>方法中<code>loadModel</code>我们返回一个函数，该函数接受一维张量作为输入并返回<code>mn.infer(input, Layer)</code> 。  <code>infer</code>方法采用张量和层作为参数。 该层确定我们要从哪个隐藏层输出。 如果打开<a href="">model.json</a>并<code>global_average_pooling2d_1</code> ，则会在其中一层上找到这样的名称。 <br><br> 现在，您需要创建用于训练模型的数据集。 为此，我们必须将所有图像传递给MobileNet中的<code>infer</code>方法，并为其分配标签： <code>1</code>表示带有笔划的图像， <code>0</code>表示没有笔划的图像： <br><br><pre> <code class="python hljs">const punches = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Punches) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Punches}/${f}`); const others = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Others) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Others}/${f}`); const ys = tf.tensor1d( new Array(punches.length).fill(<span class="hljs-number"><span class="hljs-number">1</span></span>) .concat(new Array(others.length).fill(<span class="hljs-number"><span class="hljs-number">0</span></span>))); const xs: tf.Tensor2D = tf.stack( punches .map((path: string) =&gt; mobileNet(readInput(path))) .concat(others.map((path: string) =&gt; mobileNet(readInput(path)))) ) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor2D;</code> </pre> <br> 在上面的代码中，我们首先读取带有或不带有匹配项的目录中的文件。 然后我们确定包含输出标签的一维张量。 如果我们有<code>n</code>带有笔触的图像和<code>m</code>其他图像，则张量将包含<code>n</code>元素的值为1， <code>m</code>元素的值为0。 <br><br> 在<code>xs</code>我们为单个图像添加了调用<code>infer</code>方法的结果。 请注意，对于每个图像，我们都调用<code>readInput</code>方法。 这是它的实现： <br><br><pre> <code class="python hljs">export const readInput = img =&gt; imageToInput(readImage(img), TotalChannels); const readImage = path =&gt; jpeg.decode(fs.readFileSync(path), true); const imageToInput = image =&gt; { const values = serializeImage(image); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> tf.tensor3d(values, [image.height, image.width, <span class="hljs-number"><span class="hljs-number">3</span></span>], <span class="hljs-string"><span class="hljs-string">'int32'</span></span>); }; const serializeImage = image =&gt; { const totalPixels = image.width * image.height; const result = new Int32Array(totalPixels * <span class="hljs-number"><span class="hljs-number">3</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (let i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; totalPixels; i++) { result[i * <span class="hljs-number"><span class="hljs-number">3</span></span> + <span class="hljs-number"><span class="hljs-number">0</span></span>] = image.data[i * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">0</span></span>]; result[i * <span class="hljs-number"><span class="hljs-number">3</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>] = image.data[i * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>]; result[i * <span class="hljs-number"><span class="hljs-number">3</span></span> + <span class="hljs-number"><span class="hljs-number">2</span></span>] = image.data[i * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">2</span></span>]; } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> result; };</code> </pre> <br>  <code>readInput</code>首先调用<code>readImage</code>函数，然后将其调用委托给<code>imageToInput</code> 。  <code>readImage</code>函数从磁盘读取图像，然后使用<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">jpeg-js</a>包从缓冲区解码jpg。 在<code>imageToInput</code>我们将图像转换为三维张量。 <br><br> 结果，对于每个从<code>0</code>到<code>TotalImages</code> <code>ys[i]</code> ，如果<code>xs[i]</code>对应于具有点击的图像，则<code>ys[i]</code>等于<code>1</code> ，否则为<code>0</code> 。 <br><br><h1> 模型训练 </h1><br> 现在该模型已准备好进行训练！ 调用<code>fit</code>方法： <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">await</span></span> model.fit(xs, ys, { epochs: Epochs, batchSize: parseInt(((punches.length + others.length) * BatchSize).toFixed(<span class="hljs-number"><span class="hljs-number">0</span></span>)), callbacks: { onBatchEnd: <span class="hljs-keyword"><span class="hljs-keyword">async</span></span> (_, logs) =&gt; { console.log(<span class="hljs-string"><span class="hljs-string">'Cost: %s, accuracy: %s'</span></span>, logs.loss.toFixed(<span class="hljs-number"><span class="hljs-number">5</span></span>), logs.acc.toFixed(<span class="hljs-number"><span class="hljs-number">5</span></span>)); <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> tf.nextFrame(); } } });</code> </pre> <br> 上面的代码调用包含三个参数： <code>xs</code> ，ys和配置对象。 在配置对象中，我们将设置训练模型的数量，数据包大小以及TensorFlow.js在处理每个数据包后将生成的回调的次数。 <br><br> 数据包大小决定了<code>xs</code>和<code>ys</code>以便在一个时代中训练模型。 对于每个时代，TensorFlow.js将从<code>ys</code>选择<code>xs</code>的子集和相应的元素，执行直接分配，以<code>sigmoid</code>激活方式接收层的输出，然后基于损失，使用<code>adam</code>算法执行优化。 <br><br> 启动训练脚本后，您将看到类似于以下结果： <br><br><pre> 成本：0.84212，精度：1.00000
 eta = 0.3&gt; ---------- acc = 1.00损失= 0.84成本：0.79740，准确性：1.00000
 eta = 0.2 =&gt; --------- acc = 1.00损失= 0.80成本：0.81533，准确性：1.00000
 eta = 0.2 ==&gt; -------- acc = 1.00损失= 0.82成本：0.64303，准确性：0.50000
 eta = 0.2 ===&gt; ------- acc = 0.50损失= 0.64成本：0.51377，准确性：0.00000
 eta = 0.2 ====&gt; ------ acc = 0.00损失= 0.51成本：0.46473，准确性：0.50000
 eta = 0.1 =====&gt; ----- acc = 0.50损失= 0.46成本：0.50872，准确性：0.00000
 eta = 0.1 ======&gt; acc = 0.00损失= 0.51成本：0.62556，准确性：1.00000
 eta = 0.1 =======&gt; --- acc = 1.00损失= 0.63成本：0.65133，准确性：0.50000
 eta = 0.1 =======&gt;-acc = 0.50损失= 0.65成本：0.63824，准确性：0.50000
 eta = 0.0 ===========&gt;
 293ms 14675us /步-acc = 0.60损耗= 0.65
时代3/50
费用：0.44661，准确性：1.000000
 eta = 0.3&gt; ---------- acc = 1.00损失= 0.45成本：0.78060，准确性：1.00000
 eta = 0.3 =&gt; --------- acc = 1.00损失= 0.78成本：0.79208，准确性：1.00000
 eta = 0.3 ==&gt; -------- acc = 1.00损失= 0.79成本：0.49072，准确性：0.50000
 eta = 0.2 ===&gt; ------- acc = 0.50损失= 0.49成本：0.62232，准确性：1.00000
 eta = 0.2 ====&gt; ------ acc = 1.00损失= 0.62成本：0.82899，准确性：1.00000
 eta = 0.2 =====&gt; ----- acc = 1.00损失= 0.83成本：0.67629，准确性：0.50000
 eta = 0.1 ======&gt; ---- acc = 0.50损失= 0.68成本：0.62621，准确性：0.50000
 eta = 0.1 =======&gt; --- acc = 0.50损失= 0.63成本：0.46077，准确性：1.00000
 eta = 0.1 =======&gt;-acc = 1.00损失= 0.46成本：0.62076，准确性：1.000000
 eta = 0.0 ===========&gt;
 304ms 15221us / step-acc = 0.85损耗= 0.63 </pre><br> 请注意，随着时间的流逝，精度如何提高，而损耗却如何降低。 <br><br> 在我的数据集上，训练后的模型显示出92％的准确性。 请记住，由于训练数据集很少，因此准确性可能不是很高。 <br><br><h1> 在浏览器中运行模型 </h1><br> 在上一节中，我们训练了二进制分类模型。 现在，在浏览器中运行它并连接到<a href="">MK.js</a> ！ <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> video = <span class="hljs-built_in"><span class="hljs-built_in">document</span></span>.getElementById(<span class="hljs-string"><span class="hljs-string">'cam'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> Layer = <span class="hljs-string"><span class="hljs-string">'global_average_pooling2d_1'</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> mobilenetInfer = <span class="hljs-function"><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">m</span></span></span><span class="hljs-function"> =&gt;</span></span> (p): tf.Tensor&lt;tf.Rank&gt; =&gt; m.infer(p, Layer); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> canvas = <span class="hljs-built_in"><span class="hljs-built_in">document</span></span>.getElementById(<span class="hljs-string"><span class="hljs-string">'canvas'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> scale = <span class="hljs-built_in"><span class="hljs-built_in">document</span></span>.getElementById(<span class="hljs-string"><span class="hljs-string">'crop'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> ImageSize = { <span class="hljs-attr"><span class="hljs-attr">Width</span></span>: <span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-attr"><span class="hljs-attr">Height</span></span>: <span class="hljs-number"><span class="hljs-number">56</span></span> }; navigator.mediaDevices .getUserMedia({ <span class="hljs-attr"><span class="hljs-attr">video</span></span>: <span class="hljs-literal"><span class="hljs-literal">true</span></span>, <span class="hljs-attr"><span class="hljs-attr">audio</span></span>: <span class="hljs-literal"><span class="hljs-literal">false</span></span> }) .then(<span class="hljs-function"><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">stream</span></span></span><span class="hljs-function"> =&gt;</span></span> { video.srcObject = stream; });</code> </pre> <br> 上面的代码中有几个声明： <br><br><ul><li> <code>video</code>     <code>HTML5 video</code>   </li><li> <code>Layer</code>     MobileNet,                  </li><li> <code>mobilenetInfer</code> — ,    MobileNet    .              MobileNet </li><li> <code>canvas</code>    <code>HTML5 canvas</code> ,          </li><li> <code>scale</code> —   <code>canvas</code> ,       </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">之后，我们从用户的摄像头获取视频流，并将其设置为element的源</font></font><code>video</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">下一步是实现一个接受</font></font><code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">并转换其内容</font><font style="vertical-align: inherit;">的灰度滤镜</font><font style="vertical-align: inherit;">：</font></font><br><br><pre> <code class="python hljs">const grayscale = (canvas: HTMLCanvasElement) =&gt; { const imageData = canvas.getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>).getImageData(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, canvas.width, canvas.height); const data = imageData.data; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (let i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; data.length; i += <span class="hljs-number"><span class="hljs-number">4</span></span>) { const avg = (data[i] + data[i + <span class="hljs-number"><span class="hljs-number">1</span></span>] + data[i + <span class="hljs-number"><span class="hljs-number">2</span></span>]) / <span class="hljs-number"><span class="hljs-number">3</span></span>; data[i] = avg; data[i + <span class="hljs-number"><span class="hljs-number">1</span></span>] = avg; data[i + <span class="hljs-number"><span class="hljs-number">2</span></span>] = avg; } canvas.getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>).putImageData(imageData, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>); };</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 下一步，我们将模型与MK.js连接： </font></font><br><br><pre> <code class="python hljs">let mobilenet: (p: any) =&gt; tf.Tensor&lt;tf.Rank&gt;; tf.loadModel(<span class="hljs-string"><span class="hljs-string">'http://localhost:5000/model.json'</span></span>).then(model =&gt; { mobileNet .load() .then((mn: any) =&gt; mobilenet = mobilenetInfer(mn)) .then(startInterval(mobilenet, model)); });</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">在上面的代码中，我们首先加载上面训练的模型，然后下载MobileNet。</font><font style="vertical-align: inherit;">我们将MobileNet传递给该方法，</font></font><code>mobilenetInfer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">以获得从隐藏网络层计算输出的方法。</font><font style="vertical-align: inherit;">之后，我们</font></font><code>startInterval</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">以两个网络作为参数</font><font style="vertical-align: inherit;">调用该方法</font><font style="vertical-align: inherit;">。</font></font><br><br><pre> <code class="python hljs">const startInterval = (mobilenet, model) =&gt; () =&gt; { setInterval(() =&gt; { canvas.getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>).drawImage(video, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>); grayscale(scale .getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>) .drawImage( canvas, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, canvas.width, canvas.width / (ImageSize.Width / ImageSize.Height), <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, ImageSize.Width, ImageSize.Height )); const [punching] = Array.<span class="hljs-keyword"><span class="hljs-keyword">from</span></span>(( model.predict(mobilenet(tf.fromPixels(scale))) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor1D) .dataSync() <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> Float32Array); const detect = (window <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> any).Detect; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (punching &gt;= <span class="hljs-number"><span class="hljs-number">0.4</span></span>) detect &amp;&amp; detect.onPunch(); }, <span class="hljs-number"><span class="hljs-number">100</span></span>); };</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">最有趣的部分始于方法</font></font><code>startInterval</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">！首先，我们运行一个间隔，每个人都</font></font><code>100ms</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">调用一个匿名函数。在其中，</font></font><code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">具有当前帧</font><font style="vertical-align: inherit;">的</font><font style="vertical-align: inherit;">视频</font><font style="vertical-align: inherit;">首先在其顶部</font><font style="vertical-align: inherit;">呈现。然后，我们将帧尺寸减小到，</font></font><code>100x56</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">并对其应用灰度滤镜。</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">下一步是将帧传输到MobileNet，从所需的隐藏层获取输出，并将其作为输入传输到</font></font><code>predict</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">我们的模型</font><font style="vertical-align: inherit;">方法中</font><font style="vertical-align: inherit;">。这将返回具有一个元素的张量。使用，</font></font><code>dataSync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">我们从张量获得值并将其分配给一个常量</font></font><code>punching</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">最后，我们检查：如果触击的可能性超过</font></font><code>0.4</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，则我们调用</font></font><code>onPunch</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">global object </font><font style="vertical-align: inherit;">method </font></font><code>Detect</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。 MK.js为全局对象提供了三种方法：</font></font><code>onKick</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，</font></font><code>onPunch</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">而且</font></font><code>onStand</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">我们可以将其用于控制的人物之一。</font></font><br><br> 做完了！<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 结果就是这里！ </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/83e/05c/e0e/83e05ce0e9304865bb6aee072204902b.gif"><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 具有N分类的脚踢和手臂识别 </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">在下一部分中，我们将创建一个更智能的模型：识别出拳，脚踢和其他图像的神经网络。</font><font style="vertical-align: inherit;">这次，让我们从准备训练集开始：</font></font><br><br><pre> <code class="python hljs">const punches = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Punches) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Punches}/${f}`); const kicks = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Kicks) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Kicks}/${f}`); const others = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Others) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Others}/${f}`); const ys = tf.tensor2d( new Array(punches.length) .fill([<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>]) .concat(new Array(kicks.length).fill([<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>])) .concat(new Array(others.length).fill([<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>])), [punches.length + kicks.length + others.length, <span class="hljs-number"><span class="hljs-number">3</span></span>] ); const xs: tf.Tensor2D = tf.stack( punches .map((path: string) =&gt; mobileNet(readInput(path))) .concat(kicks.map((path: string) =&gt; mobileNet(readInput(path)))) .concat(others.map((path: string) =&gt; mobileNet(readInput(path)))) ) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor2D;</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">和以前一样，我们首先阅读带有手，脚和其他图像的打孔图像的目录。此后，与上次不同，我们以二维张量而不是一维的形式形成预期结果。如果我们有</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ñ</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">图片包含有冲头，</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">米</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">图像与踢和</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ķ</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">其他图像，张量</font></font><code>ys</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">将是</font></font><code>n</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">的值的元件</font></font><code>[1, 0, 0]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，</font></font><code>m</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">与该值的元素</font></font><code>[0, 1, 0]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">和</font></font><code>k</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">项具有值</font></font><code>[0, 0, 1]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">一个</font></font><code>n</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">元素</font><font style="vertical-align: inherit;">向量，</font><font style="vertical-align: inherit;">其中有</font></font><code>n - 1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">一个具有值的元素和一个具有值的</font></font><code>0</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">元素</font></font><code>1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，我们称为a矢量（one-hot vector）。</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">之后，我们形成输入张量</font></font><code>xs</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">堆叠来自MobileNet的每个图像的输出。</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">在这里，您必须更新模型定义：</font></font><br><br><pre> <code class="python hljs">const model = tf.sequential(); model.add(tf.layers.inputLayer({ inputShape: [<span class="hljs-number"><span class="hljs-number">1024</span></span>] })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">1024</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'relu'</span></span> })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">3</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'softmax'</span></span> })); <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> model.compile({ optimizer: tf.train.adam(<span class="hljs-number"><span class="hljs-number">1e-6</span></span>), loss: tf.losses.sigmoidCrossEntropy, metrics: [<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>] });</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 与以前的模型仅有的两个区别是： </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 输出层中的单位数 </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 输出层中的激活 </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 输出层中有三个单位，因为我们有三种不同的图像类别： </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 罢工 </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 踢 </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 其他 </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">在这三个单元上触发激活</font></font><code>softmax</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，这会将它们的参数转换为具有三个值的张量。为什么输出层需要三个单位？为三类的三个值中的每一个可以由两个比特表示：</font></font><code>00</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，</font></font><code>01</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，</font></font><code>10</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。创建的张量值的总和</font></font><code>softmax</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">为1，也就是说，我们永远不会得到00，因此我们将无法对其中一个类别的图像进行分类。</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">经过多年的训练</font></font><code>500</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，我的准确率达到了约92％！这还不错，但是请不要忘记训练是在一个小的数据集上进行的。</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">下一步是在浏览器中运行模型！由于逻辑与运行用于二进制分类的模型非常相似，因此请看最后一步，其中根据模型的输出选择操作：</font></font><br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> [punch, kick, nothing] = <span class="hljs-built_in"><span class="hljs-built_in">Array</span></span>.from((model.predict( mobilenet(tf.fromPixels(scaled)) ) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor1D).dataSync() <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> <span class="hljs-built_in"><span class="hljs-built_in">Float32Array</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> detect = (<span class="hljs-built_in"><span class="hljs-built_in">window</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> any).Detect; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (nothing &gt;= <span class="hljs-number"><span class="hljs-number">0.4</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (kick &gt; punch &amp;&amp; kick &gt;= <span class="hljs-number"><span class="hljs-number">0.35</span></span>) { detect.onKick(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span>; } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (punch &gt; kick &amp;&amp; punch &gt;= <span class="hljs-number"><span class="hljs-number">0.35</span></span>) detect.onPunch();</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">首先，我们将MobileNet的框架缩小为灰色阴影，然后再传递经过训练的模型的结果。</font><font style="vertical-align: inherit;">模型返回一维张量，我们将其转换为</font></font><code>Float32Array</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">c </font></font><code>dataSync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">下一步，我们将</font></font><code>Array.from</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">类型化数组转换为JavaScript数组。</font><font style="vertical-align: inherit;">然后，我们提取帧中存在用手，脚踢或不发球的概率。</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">如果第三个结果的概率超过</font></font><code>0.4</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，我们将返回。</font><font style="vertical-align: inherit;">否则，如果发生踢的可能性更高</font></font><code>0.32</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，我们会向MK.js发送一个踢命令。</font><font style="vertical-align: inherit;">如果踢</font></font><code>0.32</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">的可能性越来越大，则我们发送踢的动作。</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">一般来说，仅此而已！</font><font style="vertical-align: inherit;">结果如下所示：</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/168/f71/f3d/168f71f3df8d267bec3e0791d5857c64.gif"><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 动作识别 </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">如果您收集了大量有关手脚搏动的人的数据，则可以建立一个适用于各个框架的模型。但是够了吗？如果我们想走得更远并区分两种不同的踢法，该怎么办：转身和后卫（后踢）。</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">从下面的框架中可以看出，在某个角度从某个角度的某个时间点，两个笔画看起来是相同的：</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/6c1/567/5bf/6c15675bf7b8c238e7ce9d5aaefeea80.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/a60/e3c/dba/a60e3cdba0eb3ecbc8730c39bc6c95b2.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">但是如果您查看性能，则动作是完全不同的：</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/e72/28b/fe8/e7228bfe8cfe9bbe73f9011d94778a7a.gif"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">如何训练神经网络来分析框架的序列，而不仅仅是一个框架？</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">为此，我们可以探索另一类神经网络，称为递归神经网络（RNN）。例如，RNN非常适合处理时间序列：</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 自然语言处理（NLP），其中每个单词都取决于前一个和后一个 </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 根据您的浏览历史预测下一页 </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 帧识别 </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 实现这种模型不在本文讨论范围之内，但是让我们看一下示例架构，以了解所有这些如何协同工作。 </font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> RNN的力量 </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">下图显示了动作识别模型：</font></font><br><br><img src="https://habrastorage.org/webt/kz/oq/ie/kzoqieod8t9nhs_taapnhpr_y0c.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">我们</font></font><code>n</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">从视频中</font><font style="vertical-align: inherit;">获取最后一</font><font style="vertical-align: inherit;">帧并将其传输到CNN。</font><font style="vertical-align: inherit;">每个帧的CNN输出作为输入RNN传输。</font><font style="vertical-align: inherit;">循环神经网络将确定各个框架之间的关系，并识别它们对应的动作。</font></font><br><br><h1> 结论 </h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">在本文中，我们开发了图像分类模型。为此，我们收集了一个数据集：我们提取了视频帧并将其手动分为三类。然后</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;">imgaug</font></a><font style="vertical-align: inherit;">添加图像来</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;">增强</font></a><font style="vertical-align: inherit;">数据</font><font style="vertical-align: inherit;">。</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">之后，我们解释了什么是学习转移，并使用了</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">@ tensorflow-models / mobilenet</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">软件包中训练有素的MobileNet模型来达到我们的目的</font><font style="vertical-align: inherit;">。我们从Node.js进程中的文件加载了MobileNet，并训练了一个额外的密集层，该层从隐藏的MobileNet层馈送数据。经过培训，我们的准确率达到了90％以上！</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">为了在浏览器中使用该模型，我们将其与MobileNet一起下载，并开始每100毫秒对用户网络摄像头中的帧进行分类。我们将模型与游戏连接</font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MK.js</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">并使用模型输出来控制字符之一。</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">最后，我们研究了如何通过将模型与递归神经网络结合以识别动作来改进模型。</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">希望您能像我一样喜欢这个小项目！</font><font style="vertical-align: inherit;">‍</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN428019/">https://habr.com/ru/post/zh-CN428019/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN428003/index.html">响应式设计：保持标记元素的形状</a></li>
<li><a href="../zh-CN428005/index.html">加剧公关灾难的三种有效方法</a></li>
<li><a href="../zh-CN428007/index.html">已经不是便携式计算机，还不是笔记本：笔记本电脑TOSHIBA T3100 / 20</a></li>
<li><a href="../zh-CN428009/index.html">Equifax：最大的数据泄漏发生一年后</a></li>
<li><a href="../zh-CN428011/index.html">太空僵尸歌曲</a></li>
<li><a href="../zh-CN428021/index.html">密封神经网络。 或选择并运行神经网络以识别Raspberry Zero上的对象</a></li>
<li><a href="../zh-CN428023/index.html">电子设备设计中的电气安全基础</a></li>
<li><a href="../zh-CN428025/index.html">使用外部SSD作为系统时，在MAC OS X中连接交换文件（SWAP）</a></li>
<li><a href="../zh-CN428027/index.html">我如何尝试制作GLSL静态分析器（以及出了什么问题）</a></li>
<li><a href="../zh-CN428029/index.html">10月29日至11月4日在莫斯科举行的数字活动</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>