<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßõ üêØ üôåüèº Postgres-Tuesday # 5: ¬´PostgreSQL et Kubernetes. CI / CD. Automatisation des tests ¬ª üë∞üèª üßòüèø üßìüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="√Ä la fin de l'ann√©e derni√®re, une autre diffusion en direct de la communaut√© russe PostgreSQL #RuPostgres a eu lieu , au cours de laquelle son co-fond...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Postgres-Tuesday # 5: ¬´PostgreSQL et Kubernetes. CI / CD. Automatisation des tests ¬ª</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/479438/"><img src="https://habrastorage.org/webt/qm/rm/ln/qmrmlnm8gjj_8gih4dzhy2ybrvy.jpeg"><br><br>  √Ä la fin de l'ann√©e derni√®re, une autre diffusion en direct de la communaut√© russe PostgreSQL <a href="https://www.meetup.com/postgresqlrussia/">#RuPostgres a eu lieu</a> , au cours de laquelle son co-fondateur Nikolai Samokhvalov a parl√© avec le directeur technique de Flanta Dmitry Stolyarov de ce SGBD dans le contexte de Kubernetes. <br><br>  Nous publions une transcription du corps principal de cette discussion, et une vid√©o compl√®te a √©t√© publi√©e <a href="https://www.youtube.com/channel/UC0SBGSNmBLrTZIkbN-lJHnw">sur la cha√Æne YouTube de la communaut√©</a> : <a name="habracut"></a><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/qXc9VTr4TFc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h2>  Bases de donn√©es et Kubernetes </h2><br>  <i><b>NS</b> : Nous ne parlerons pas de vide et de points de contr√¥le aujourd'hui.</i>  <i>Nous voulons parler de Kubernetes.</i>  <i>Je sais que vous avez de nombreuses ann√©es d'exp√©rience.</i>  <i>J'ai regard√© vos vid√©os et m√™me revu certaines des pi√®ces ... Allons-y tout de suite: pourquoi Postgres ou MySQL dans K8s?</i> <br><br>  <b>DS</b> : Il n'y a pas de r√©ponse unique √† cette question et elle ne peut pas l'√™tre.  Mais en g√©n√©ral, c'est la simplicit√© et la commodit√© ... le potentiel.  Apr√®s tout, tout le monde veut des services g√©r√©s. <br><br>  <i><b>NS</b> : Pour aimer <a href="https://aws.amazon.com/rds/">RDS</a> , uniquement √† la maison?</i> <br><br>  <b>DS</b> : Oui: pour aimer RDS, seulement n'importe o√π. <br><br>  <i><b>NS</b> : ¬´N'importe o√π¬ª est un bon point.</i>  <i>Dans les grandes entreprises, tout est situ√© √† diff√©rents endroits.</i>  <i>Et pourquoi alors, s'il s'agit d'une grande entreprise, ne prenez pas une solution toute faite?</i>  <i>Par exemple, Nutanix a ses propres d√©veloppements, tandis que d'autres soci√©t√©s (VMware ...) ont le m√™me ¬´RDS, uniquement √† domicile¬ª.</i> <br><br>  <b>DS</b> : Mais nous parlons d'une impl√©mentation unique qui ne fonctionnera que sous certaines conditions.  Et si nous parlons de Kubernetes, il existe une grande vari√©t√© d'infrastructures (qui peuvent √™tre dans les K8).  C'est essentiellement la norme pour l'API vers le cloud ... <br><br>  <i><b>NS</b> : C'est gratuit aussi!</i> <br><br>  <b>DS</b> : Ce n'est pas si important.  La gratuit√© n'est pas importante pour un tr√®s grand segment du march√©.  Une autre chose est importante ... Vous vous souvenez probablement du rapport " <a href="https://habr.com/ru/company/flant/blog/431500/">Bases de donn√©es et Kubernetes</a> "? <br><br>  <i><b>NS</b> : Oui.</i> <br><br>  <b>DS</b> : J'ai r√©alis√© qu'il √©tait per√ßu de mani√®re tr√®s ambigu√´.  Certaines personnes pensaient que je disais: "Les gars, nous sommes all√©s toutes les bases de donn√©es √† Kubernetes!", Tandis que d'autres ont d√©cid√© que ce sont tous des v√©los terribles.  Et je voulais dire autre chose: ¬´Regardez ce qui se passe, quels sont les probl√®mes et comment ils peuvent √™tre r√©solus.  Allez maintenant des bases √† Kubernetes?  La production?  Eh bien, seulement si vous aimez ... faire certaines choses.  Mais pour les d√©veloppeurs, je peux dire que je le recommande.  Pour les d√©veloppeurs, la cr√©ation / suppression dynamique d'environnements est tr√®s importante. ¬ª <br><br>  <i>NS: Par dev, voulez-vous dire tous les environnements qui ne sont pas prod?</i>  <i>Mise en sc√®ne, QA ...</i> <br><br>  <b>DS</b> : Si nous parlons de stands de perf, ce n'est probablement pas d√©j√† le cas, car les exigences y sont sp√©cifiques.  Si nous parlons de cas particuliers o√π la mise en sc√®ne a besoin d'une tr√®s grande base de donn√©es, alors probablement pas non plus ... S'il s'agit d'un environnement statique, de longue dur√©e, alors quel est l'avantage d'avoir la base situ√©e dans les K8? <br><br>  <i><b>NS</b> : Aucun.</i>  <i>Mais o√π voit-on des environnements statiques?</i>  <i>L'environnement statique est d√©pass√© demain.</i> <br><br>  <b>DS</b> : la mise en sc√®ne peut √™tre statique.  Nous avons des clients ... <br><br>  <i><b>NS</b> : Oui, je l'ai aussi.</i>  <i>Le gros probl√®me est que si vous avez une base de 10 To et une mise en sc√®ne - 200 Go ...</i> <br><br>  <b>DS</b> : J'ai un √©tui tr√®s cool!  Lors de la mise en sc√®ne, il existe une base prod'ovy dans laquelle des modifications sont apport√©es.  Et un bouton est fourni: "d√©ploiement en production".  Ces changements - deltas - sont ajout√©s (il semble, ils sont juste synchronis√©s par les API) en production.  C'est une option tr√®s exotique. <br><br>  <i><b>NS</b> : J'ai vu des startups dans la vall√©e qui sont encore assis dans RDS ou m√™me dans Heroku - ce sont des histoires d'il y a 2-3 ans - et ils t√©l√©chargent le vidage sur leur ordinateur portable.</i>  <i>Parce que la base ne fait que 80 Go jusqu'√† pr√©sent, et qu'il y a une place sur l'ordinateur portable.</i>  <i>Ensuite, ils ach√®tent des disques pour tout le monde, afin qu'ils aient 3 bases, afin qu'ils puissent effectuer diff√©rents d√©veloppements.</i>  <i>Cela arrive aussi.</i>  <i>J'ai √©galement vu qu'ils n'avaient pas peur de copier la prod dans la mise en sc√®ne - cela d√©pend beaucoup de l'entreprise.</i>  <i>Mais il a vu qu'ils avaient tr√®s peur et que souvent ils n'avaient pas assez de temps et de mains.</i>  <i>Mais avant de passer √† ce sujet, je veux entendre parler de Kubernetes.</i>  <i>Je comprends bien que dans prod'e jusqu'ici personne?</i> <br><br>  <b>DS</b> : Nous avons de petites bases en prod.  Nous parlons de volumes de dizaines de gigaoctets et de services non critiques, pour lesquels il √©tait trop paresseux pour faire des r√©pliques (et il n'y a pas un tel besoin).  Et √† condition que sous Kubernetes il y ait un stockage normal.  Cette base de donn√©es fonctionnait dans une machine virtuelle - conditionnellement dans VMware, en plus du stockage.  Nous l'avons plac√© en <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PV</a> et maintenant nous pouvons le transf√©rer de voiture en voiture. <br><br>  <i><b>NS</b> : Des bases de cette taille, jusqu'√† 100 Go, sur de bons disques et avec un bon r√©seau peuvent √™tre d√©ploy√©es en quelques minutes, non?</i>  <i>Une vitesse de 1 Go par seconde n'est plus exotique.</i> <br><br>  <b>DS</b> : Oui, pour une op√©ration lin√©aire ce n'est pas un probl√®me. <br><br>  <i><b>NS</b> : D'accord, nous ne devrions penser qu'√† prod.</i>  <i>Et si nous consid√©rons Kubernetes pour les environnements non prod - comment le faire?</i>  <i>Je vois qu'√† Zalando <a href="https://github.com/zalando/postgres-operator">ils font un op√©rateur</a> , √† Crunchy ils <a href="https://github.com/CrunchyData/postgres-operator">scient</a> , il y a d'autres options.</i>  <i>Et il y a <a href="https://ongres.com/">OnGres</a> - c'est notre bon ami espagnol Alvaro: ils ne sont pas seulement un <a href="https://habr.com/ru/company/flant/blog/326414/">op√©rateur</a> , mais une distribution enti√®re ( <a href="https://gitlab.com/ongresinc/stackgres">StackGres</a> ), dans laquelle, en plus de Postgres lui-m√™me, ils ont √©galement d√©cid√© de bourrer la sauvegarde, le proxy Envoy ...</i> <br><br>  <b>DS</b> : Envoy√© pour quoi?  L'√©quilibrage du trafic Postgres exactement? <br><br>  <i><b>NS</b> : Oui.</i>  <i>Autrement dit, ils le voient comme: si vous prenez la distribution Linux et le noyau, alors le PostgreSQL habituel est le noyau, et ils veulent faire une distribution qui est compatible avec le cloud et fonctionne sur Kubernetes.</i>  <i>Ils ancrent les composants (sauvegardes, etc.) et d√©boguent pour qu'ils fonctionnent bien.</i> <br><br>  <b>DS</b> : Tr√®s cool!  En substance, c'est un logiciel pour cr√©er vos Postgres g√©r√©s. <br><br>  <i><b>NS</b> : les distributions Linux ont des probl√®mes √©ternels: comment cr√©er des pilotes pour que tout le mat√©riel soit pris en charge.</i>  <i>Et ils ont l'id√©e de travailler chez Kubernetes.</i>  <i>Je sais que chez l'op√©rateur Zalando, nous avons r√©cemment vu les globes oculaires sur AWS et ce n'est pas tr√®s bon.</i>  <i>Il ne devrait pas y avoir de liens avec une infrastructure sp√©cifique - √† quoi bon alors?</i> <br><br>  <b>DS</b> : Je ne sais pas dans quelle situation sp√©cifique Zalando s'est impliqu√©, mais dans Kubernetes, le stockage est d√©sormais r√©alis√© de telle mani√®re qu'il est impossible de supprimer une sauvegarde de disque de mani√®re g√©n√©rique.  R√©cemment, la norme - dans la derni√®re version de <a href="https://habr.com/ru/company/flant/blog/465417/">la sp√©cification CSI</a> - a rendu possible les instantan√©s, mais o√π est-elle impl√©ment√©e?  Honn√™tement, c'est toujours aussi brut ... Nous essayons CSI par-dessus AWS, GCE, Azure, vSphere, mais nous commen√ßons √† l'utiliser un peu, car vous pouvez voir qu'il n'est pas encore pr√™t. <br><br>  <i><b>NS</b> : Par cons√©quent, il faut parfois se rattacher √† l'infrastructure.</i>  <i>Je pense que ce n'est encore qu'un stade pr√©coce - des probl√®mes de croissance.</i>  <i>Question: que recommanderiez-vous aux d√©butants qui souhaitent essayer PgSQL dans les K8?</i>  <i>Quel op√©rateur peut-√™tre?</i> <br><br>  <b>DS</b> : Le probl√®me est que pour nous, Postgres est de 3%.  Nous avons toujours une tr√®s grande liste de diff√©rents logiciels dans Kubernetes, je ne vais m√™me pas tout √©num√©rer.  Par exemple, Elasticsearch.  Il y a beaucoup d'op√©rateurs: certains se d√©veloppent activement, d'autres non.  Pour nous, nous avons fait des exigences qui devraient √™tre dans l'op√©rateur, afin que nous le prenions au s√©rieux.  L'op√©rateur est sp√©cifiquement pour Kubernetes - pas "l'op√©rateur pour faire quelque chose dans les conditions d'Amazon ..." En fait, nous utilisons un seul op√©rateur assez largement (= pour presque tous les clients) - <a href="https://github.com/spotahome/redis-operator">pour Redis</a> <i>(nous publierons bient√¥t un article √† ce sujet)</i> . <br><br>  <i><b>NS</b> : Mais pour MySQL aussi?</i>  <i>Je sais que Percona ... puisqu'ils sont maintenant impliqu√©s dans MySQL, MongoDB et Postgres, ils devront avoir une sorte d'entaille universelle: pour toutes les bases de donn√©es, pour tous les fournisseurs de cloud.</i> <br><br>  <b>DS</b> : Nous n'avons pas eu le temps de regarder les d√©clarations pour MySQL.  Pour nous, ce n'est pas l'objectif principal maintenant.  MySQL fonctionne tr√®s bien en autonome.  Pourquoi un op√©rateur, si vous pouvez simplement d√©marrer la base de donn√©es ... Vous pouvez d√©marrer le conteneur Docker avec Postrges, ou vous pouvez le d√©marrer de mani√®re simple. <br><br>  <i><b>NS</b> : C'√©tait aussi une question.</i>  <i>Pas d'op√©rateur du tout?</i> <br><br>  <b>DS</b> : Oui, 100% d'entre nous ont PostgreSQL fonctionnant sans op√©rateur.  Jusqu'√† pr√©sent.  Nous utilisons activement l'op√©rateur pour Prometheus, pour Redis.  Nous avons l'intention de trouver un op√©rateur pour Elasticsearch - il br√ªle le plus parce que nous voulons l'installer dans 100% des cas √† Kubernetes.  Tout comme nous voulons nous assurer que MongoDB est toujours √©galement install√© dans Kubernetes.  Certaines listes de souhaits apparaissent ici - on a le sentiment que dans ces cas, quelque chose peut √™tre fait.  Et √† propos de Postgres, nous n'avons m√™me pas regard√©.  Bien s√ªr, nous connaissons l'existence de diff√©rentes options, mais en fait, nous en avons une autonome. <br><br><h2>  Tester la base de donn√©es dans Kubernetes </h2><br>  <i><b>NS</b> : Passons au sujet des tests.</i>  <i>Comment d√©ployer les modifications dans la base de donn√©es - du point de vue de la perspective DevOps.</i>  <i>Il y a des microservices, de nombreuses bases de donn√©es, tout le temps quelque part quelque chose change.</i>  <i>Comment garantir un CI / CD normal afin que tout soit en ordre √† partir de la position du SGBD.</i>  <i>Quelle est votre approche?</i> <br><br>  <b>DS</b> : Il ne peut y avoir qu'une seule r√©ponse.  Il existe plusieurs options.  Le premier est la taille de la base que nous voulons d√©ployer.  Vous avez vous-m√™me mentionn√© que les entreprises ont une attitude diff√©rente √† l'id√©e d'avoir une copie de la base de production sur le d√©veloppement et la sc√®ne. <br><br>  <i><b>NS</b> : Et en termes de RGPD, je pense qu'ils sont de plus en plus soign√©s ... Je peux dire qu'en Europe, ils ont d√©j√† commenc√© √† aller bien.</i> <br><br>  <b>DS</b> : Mais vous pouvez souvent √©crire des logiciels qui d√©chargent la production et la brouillent.  Il s'av√®re que les donn√©es prod'ovye (snapshot, dump, copie binaire ...), mais elles sont anonymes.  Au lieu de cela, il peut y avoir des scripts de g√©n√©ration: il peut s'agir de fixtures ou simplement d'un script qui g√©n√®re une grande base de donn√©es.  Le probl√®me est quoi: combien de temps prend l'image de base pour √™tre cr√©√©e?  Et combien de temps pour le d√©ployer sur le bon environnement? <br><br>  Nous sommes arriv√©s au sch√©ma: si le client a un ensemble de donn√©es de fixture (version minimale de la base de donn√©es), alors par d√©faut nous les utilisons.  Si nous parlons d'environnements d'examen, lorsque nous avons cr√©√© une branche, nous avons d√©ploy√© une instance d'application - nous y d√©ployons une petite base de donn√©es.  Mais l' <a href="https://habr.com/ru/company/flant/blog/417509/">option</a> s'est av√©r√©e bien, lorsque nous supprimons le vidage de la production une fois par jour (la nuit) et collectons sur sa base un conteneur Docker avec PostgreSQL et MySQL avec ces donn√©es charg√©es.  Si vous devez d√©ployer la base 50 fois √† partir de cette image, cela se fait assez simplement et rapidement. <br><br>  <i><b>NS</b> : Copie simple?</i> <br><br>  <b>DS</b> : les donn√©es sont stock√©es directement dans l'image Docker.  C'est-√†-dire  nous avons une image pr√™te √† l'emploi, bien que 100 Go.  Gr√¢ce aux couches de Docker, nous pouvons rapidement d√©ployer cette image autant de fois que n√©cessaire.  La m√©thode est stupide, mais elle fonctionne plut√¥t bien. <br><br>  <i><b>NS</b> : De plus, lors des tests, cela change directement √† l'int√©rieur du Docker, non?</i>  <i>Copie sur √©criture dans Docker - jetez-le et recommencez, tout va bien.</i>  <i>Classe!</i>  <i>Et vous l'utilisez d√©j√† avec might et main?</i> <br><br>  <b>DS</b> : Pendant longtemps. <br><br>  <i><b>NS</b> : Nous faisons des choses tr√®s similaires.</i>  <i>Seulement, nous n'utilisons pas la copie sur √©criture de Docker, mais quelques autres.</i> <br><br>  <b>JS</b> : Il n'est pas g√©n√©rique.  Et Docker'ny fonctionne partout. <br><br>  <i><b>NS</b> : En th√©orie, oui.</i>  <i>Mais nous avons √©galement des modules l√†-bas, vous pouvez cr√©er diff√©rents modules et travailler avec diff√©rents syst√®mes de fichiers.</i>  <i>Quel moment.</i>  <i>De Postgres, nous regardons tout cela diff√©remment.</i>  <i>Maintenant, j'ai regard√© du c√¥t√© de Docker et j'ai vu que tout fonctionnait pour vous.</i>  <i>Mais si la base de donn√©es est √©norme, par exemple, 1 To, alors c'est tout long: √† la fois les op√©rations de nuit et tout bourrer dans Docker ... Et si 5 To sont bourr√©s dans Docker ... Ou est-ce que tout est normal?</i> <br><br>  <b>DS</b> : Quelle diff√©rence cela fait-il: ce sont des blobs, juste des bits et des octets. <br><br>  <i><b>NS</b> : La diff√©rence est la suivante: faites-vous cela via le vidage et la restauration?</i> <br><br>  <b>DS</b> : Pas du tout n√©cessaire.  Les m√©thodes de g√©n√©ration de cette image peuvent √™tre diff√©rentes. <br><br>  <i><b>NS</b> : Pour certains clients, nous avons fait en sorte qu'au lieu de g√©n√©rer r√©guli√®rement une image de base, nous la maintenions constamment √† jour.</i>  <i>Il s'agit essentiellement d'une r√©plique, mais les donn√©es ne sont pas re√ßues directement du ma√Ætre, mais via l'archive.</i>  <i>L'archive binaire o√π les WAL sont roul√©s tous les jours, les sauvegardes y sont √©galement supprim√©es ... Ces WAL volent ensuite - avec un l√©ger retard (litt√©ralement 1-2 secondes) - vers l'image de base.</i>  <i>Nous le clonons de quelque fa√ßon que ce soit - nous avons maintenant ZFS par d√©faut.</i> <br><br>  <b>DS</b> : Mais avec ZFS, vous √™tes limit√© √† un seul n≈ìud. <br><br>  <i><b>NS</b> : Oui.</i>  <i>Mais ZFS a aussi un <a href="https://docs.oracle.com/cd/E18752_01/html/819-5461/gbchx.html">envoi</a> magique: vous pouvez envoyer un instantan√© avec lui et m√™me (je ne l'ai pas encore vraiment test√©, mais ...) vous pouvez envoyer un delta entre deux <code>PGDATA</code> .</i>  <i>En fait, nous avons un autre outil que nous n'avons pas particuli√®rement pris en compte pour de telles t√¢ches.</i>  <i>PostgreSQL a <a href="https://www.postgresql.org/docs/12/app-pgrewind.html">pg_rewind</a> , qui fonctionne comme une rsync ¬´intelligente¬ª, sautant beaucoup de choses que vous n‚Äôavez pas √† regarder, car rien n‚Äôa chang√© √† coup s√ªr.</i>  <i>Nous pouvons effectuer une synchronisation rapide entre les deux serveurs et rembobiner exactement de la m√™me mani√®re.</i> <br><br>  <i>Donc, nous essayons sur ce point, plus DBA'noy, de cr√©er un outil qui vous permet de faire la m√™me chose que vous avez dit: nous avons une base, mais nous voulons tester quelque chose 50 fois, presque en m√™me temps.</i> <br><br>  <b>DS</b> : 50 fois signifie que vous devez commander 50 instances Spot. <br><br>  <i><b>NS</b> : Non, nous faisons tout sur une seule machine.</i> <br><br>  <b>DS</b> : Mais comment d√©ployez-vous 50 fois si cette base est, disons, un t√©raoctet.  Tr√®s probablement, elle a besoin conditionnellement de 256 Go de RAM? <br><br>  <i><b>NS</b> : Oui, parfois beaucoup de m√©moire est n√©cessaire - c'est normal.</i>  <i>Mais un tel exemple de la vie.</i>  <i>La machine de production a 96 c≈ìurs et 600 Go.</i>  <i>Dans le m√™me temps, 32 c≈ìurs sont utilis√©s pour la base de donn√©es (m√™me 16 c≈ìurs sont parfois d√©sormais utilis√©s) et 100 √† 120 Go de m√©moire.</i> <br><br>  <b>DS</b> : Et 50 exemplaires y entrent? <br><br>  <i><b>NS</b> : Il n'y a donc qu'une seule copie, puis la copie sur √©criture (ZFS'ny) fonctionne ... Je vais vous en dire plus.</i> <br><br>  <i>Par exemple, nous avons une base de 10 To.</i>  <i>Ils ont fait un disque pour cela, ZFS a encore r√©duit sa taille en pourcentage de 30 √† 40.</i>  <i>Comme nous ne faisons pas de test de charge, le temps de r√©ponse exact n'est pas important pour nous: laissez-le √™tre jusqu'√† 2 fois plus lent - c'est correct.</i> <br><br>  <i>Nous permettons aux programmeurs, QA, DBA, etc.</i>  <i>Effectuez des tests dans 1-2 threads.</i>  <i>Par exemple, ils peuvent d√©marrer une sorte de migration.</i>  <i>Il ne n√©cessite pas 10 c≈ìurs √† la fois - il a besoin de 1 backend Postgres, 1 core.</i>  <i>La migration commencera - peut-√™tre que le <a href="https://www.postgresql.org/docs/12/routine-vacuuming.html">vide automatique</a> d√©marrera toujours, puis le deuxi√®me noyau sera activ√©.</i>  <i>Nous avons allou√© 16 √† 32 c≈ìurs, donc 10 personnes peuvent travailler simultan√©ment, il n'y a aucun probl√®me.</i> <br><br>  <i>√âtant donn√© que <code>PGDATA</code> physiquement le m√™me, il s'av√®re que nous trompons r√©ellement Postgres.</i>  <i>L'astuce est la suivante: elle d√©marre, par exemple, 10 Postgres en m√™me temps.</i>  <i>Quel probl√®me est g√©n√©ralement quoi?</i>  <i>Ils ont <a href="https://www.postgresql.org/docs/current/runtime-config-resource.html">mis les tampons partag√©s</a> , disons, √† 25%.</i>  <i>En cons√©quence, cela fait 200 Go.</i>  <i>Vous n'en d√©marrerez pas plus de trois, car la m√©moire se terminera.</i> <br><br>  <i>Mais √† un moment donn√©, nous avons r√©alis√© que ce n'√©tait pas n√©cessaire: nous avons d√©fini shared_buffers sur 2 Go.</i>  <i>PostgreSQL a <a href="https://www.postgresql.org/docs/current/runtime-config-query.html">effective_cache_size</a> , et en r√©alit√©, cela n'affecte que les <a href="https://en.wikipedia.org/wiki/Query_plan">plans</a> .</i>  <i>Nous le mettons √† 0,5 To.</i>  <i>Et peu importe qu‚Äôils ne soient pas vraiment l√†: il fait des plans comme si c‚Äô√©tait le cas.</i> <br><br>  <i>En cons√©quence, lorsque nous testons une sorte de migration, nous pouvons collecter tous les plans - nous verrons comment cela se produira en production.</i>  <i>Les secondes seront diff√©rentes (plus lentes), mais les donn√©es que nous lisons r√©ellement et les plans eux-m√™mes (quel type de JOIN, etc.) sont obtenus exactement de la m√™me mani√®re qu'en production.</i>  <i>Et en parall√®le, vous pouvez ex√©cuter plusieurs de ces v√©rifications sur une seule machine.</i> <br><br>  <b>DS</b> : Pensez-vous qu'il y a plusieurs probl√®mes?  La premi√®re est une solution qui ne fonctionne que sur PostgreSQL.  Cette approche est tr√®s priv√©e, elle n'est pas g√©n√©rique.  La seconde - Kubernetes (et c'est l√† que le cloud va maintenant) implique de nombreux n≈ìuds, et ces n≈ìuds sont √©ph√©m√®res.  Et dans votre cas, il s'agit d'un n≈ìud persistant avec √©tat.  Ces choses me contredisent. <br><br>  <i><b>NS</b> : Tout d'abord - je suis d'accord, c'est une histoire purement Postgres.</i>  <i>Je pense que si nous avons des E / S directes et un pool de m√©moire tampon pour presque toute la m√©moire, cette approche ne fonctionnera pas - il y aura des plans diff√©rents.</i>  <i>Mais nous ne travaillons avec Postgres que pour le moment, nous ne pensons pas aux autres.</i> <br><br>  <i>√Ä propos de Kubernetes.</i>  <i>Vous dites vous-m√™me toujours que nous avons une base persistante.</i>  <i>Si l'instance se bloque, l'essentiel est de sauvegarder le disque.</i>  <i>Ici, nous avons √©galement la plate-forme enti√®re dans Kubernetes, et le composant avec Postgres est s√©par√© (bien qu'il soit l√† un jour).</i>  <i>Par cons√©quent, tout est ainsi: l'instance est tomb√©e, mais nous l'avons enregistr√©e PV et nous nous sommes juste connect√©s √† une autre (nouvelle) instance, comme si rien ne s'√©tait pass√©.</i> <br><br>  <b>DS</b> : De mon point de vue, nous cr√©ons des pods dans Kubernetes.  K8 - √©lastique: les composants sont command√©s seuls selon les besoins.  La t√¢che consiste simplement √† cr√©er un pod et √† dire qu'il a besoin de ressources X, puis les K8 le d√©couvriront.  Mais le support de stockage dans Kubernetes est toujours instable: en <a href="https://habr.com/ru/company/flant/blog/467477/">1.16</a> , en <a href="https://habr.com/ru/company/flant/blog/476998/">1.17</a> (cette version est sortie il y a des <i>semaines</i> ), ces fonctionnalit√©s ne deviennent que b√™ta. <br><br>  Six mois ou un an s'√©couleront - il deviendra plus ou moins stable, ou du moins sera d√©clar√© comme tel.  Ensuite, la possibilit√© d'instantan√©s et de redimensionner r√©sout d√©j√† compl√®tement votre probl√®me.  Parce que vous avez une base.  Oui, ce n'est peut-√™tre pas tr√®s rapide, mais la vitesse d√©pend de ce qui est ¬´sous le capot¬ª, car certaines impl√©mentations peuvent copier et copier-√©crire au niveau du sous-syst√®me de disque. <br><br>  <i><b>NS</b> : Il est √©galement n√©cessaire que tous les moteurs (Amazon, Google ...) commencent √† prendre en charge cette version - cela prend √©galement un certain temps.</i> <br><br>  <b>DS</b> : Bien que nous ne les utilisions pas.  Nous utilisons le n√¥tre. <br><br><h2>  D√©veloppement local sous Kubernetes </h2><br>  <i><b>NS</b> : Avez-vous rencontr√© une telle liste de souhaits lorsque vous devez lever tous les pods sur une seule machine et faire un si petit test.</i>  <i>Afin d'obtenir rapidement une preuve de concept, v√©rifiez que l'application fonctionne dans Kubernetes, sans lui allouer un tas de machines.</i>  <i>Y a-t-il un Minikube, non?</i> <br><br>  <b>DS</b> : Il me semble que ce cas - d√©ployer sur un n≈ìud - concerne exclusivement le d√©veloppement local.  Ou certaines manifestations d'un tel sch√©ma.  Il y a <a href="https://habr.com/ru/company/flant/blog/333470/">Minikube</a> , il y a des <a href="https://k3s.io/">k3</a> , <a href="https://github.com/kubernetes-sigs/kind">KIND</a> .  Nous allons utiliser Kubernetes IN Docker.  Maintenant, ils ont commenc√© √† travailler avec lui pour des tests. <br><br>  <i><b>NS</b> : Je pensais que c'√©tait une tentative d'envelopper tous les pods dans une seule image Docker.</i>  <i>Mais il s'est av√©r√© qu'il s'agissait d'autre chose.</i>  <i>Quoi qu'il en soit, il y a des conteneurs s√©par√©s, des pods s√©par√©s - juste dans le Docker.</i> <br><br>  <b>DS</b> : Oui.  Et l√†, une imitation assez dr√¥le est faite, mais le fait est ... Nous avons un outil de d√©ploiement - <a href="https://werf.io/">werf</a> .  Nous voulons y faire un mode - <code>werf up</code> conditionnellement: ¬´√âlevez-moi un Kubernetes local¬ª.  Et puis ex√©cutez le <code>werf follow</code> conditionnel <code>werf follow</code> .  Ensuite, le d√©veloppeur pourra √©diter dans l'IDE, et un processus est lanc√© dans le syst√®me qui voit les changements et rassemble les images, les remod√®le dans les K8 locaux.  Nous voulons donc essayer de r√©soudre le probl√®me du d√©veloppement local. <br><br><h2>  Instantan√©s et clonage de base de donn√©es dans la r√©alit√© des K8 </h2><br>  <i><b>NS</b> : Si vous revenez √† la copie sur √©criture.</i>  <i>J'ai remarqu√© que les nuages ‚Äã‚Äãont √©galement des instantan√©s.</i>  <i>Ils fonctionnent diff√©remment.</i>  <i>Par exemple, dans GCP: vous avez une instance de plusieurs t√©raoctets sur la c√¥te est des √âtats-Unis.</i>  <i>Vous effectuez r√©guli√®rement des instantan√©s.</i>  <i>Vous prenez une copie du disque sur la c√¥te ouest √† partir d'un instantan√© - en quelques minutes tout est pr√™t, cela fonctionne tr√®s rapidement, seul le cache doit √™tre rempli en m√©moire.</i>  <i>Mais ces clones (instantan√©s) - afin de ¬´provisionner¬ª un nouveau volume.</i>  <i>C'est formidable lorsque vous devez cr√©er de nombreuses instances.</i> <br><br>  <i>Mais pour les tests, il me semble, les instantan√©s dont vous parlez dans Docker ou dont je parle dans ZFS, btrfs et m√™me LVM ... - ils vous permettent de ne pas faire vraiment de nouvelles donn√©es sur la m√™me machine.</i>  <i>Dans le cloud, vous devez toujours les payer √† chaque fois et attendre non pas des minutes, mais des minutes (et dans le cas d'un <a href="https://aws.amazon.com/about-aws/whats-new/2019/11/amazon-ebs-fast-snapshot-restore-eliminates-need-for-prewarming-data-into-volumes-created-snapshots/">chargement paresseux</a> , c'est probablement des heures).</i> <br><br>  <i>Au lieu de cela, vous pouvez obtenir ces donn√©es en une seconde ou deux, effectuer le test et le jeter.</i>  <i>Ces instantan√©s r√©solvent diff√©rents probl√®mes.</i>  <i>Dans le premier cas - pour √©voluer et obtenir de nouvelles r√©pliques, et dans le second - pour les tests.</i> <br><br>  <b>DS</b> : Je ne suis pas d'accord.  Le clonage de volumes est normalement la t√¢che du cloud.  Je n'ai pas regard√© leur mise en ≈ìuvre, mais je sais comment nous le faisons sur le mat√©riel.  Nous avons Ceph, vous pouvez dire √† n'importe quel volume physique ( <a href="https://docs.ceph.com/docs/master/rbd/">RBD</a> ) de <i>cloner</i> et obtenir un deuxi√®me volume avec les m√™mes caract√©ristiques, <a href="https://en.wikipedia.org/wiki/IOPS">IOPS</a> , etc., en dizaines de millisecondes.  Vous devez comprendre qu'il y a une copie sur √©criture d√©licate √† l'int√©rieur.  Pourquoi le cloud ne fait-il pas de m√™me?  Je suis s√ªr qu'ils essaient en quelque sorte de le faire. <br><br>  <i><b>NS</b> : Mais il leur faudra encore des secondes, des dizaines de secondes pour soulever l'instance, amener Docker l√†-bas, etc.</i> <br><br>  <b>DS</b> : Pourquoi est-il n√©cessaire de g√©n√©rer une instance enti√®re?  Mais nous avons une instance pour 32 c≈ìurs, pour 16 ... et il s'int√®gre en quelque sorte - par exemple, quatre.  Lorsque nous commandons le cinqui√®me, l'instance augmentera, puis elle sera supprim√©e. <br><br>  <i><b>NS</b> : Oui, fait int√©ressant, Kubernetes a une histoire diff√©rente.</i>  <i>Notre base de donn√©es n'est pas en K8 et une seule instance.</i>  <i>Mais le clonage d'une base de donn√©es de plusieurs t√©raoctets ne prend pas plus de deux secondes.</i> <br><br>  <b>DS</b> : C'est cool.  Mais mon message initial est que ce n'est pas une solution g√©n√©rique.  Oui, c'est cool, mais seul Postgres convient et sur un seul n≈ìud. <br><br>  <i><b>NS</b> : Il ne convient pas seulement √† Postgres: ces plans, comme je l'ai d√©crit, ne fonctionneront que de cette mani√®re.</i>  <i>Mais si vous ne vous emb√™tez pas avec les plans, mais nous avons juste besoin de toutes les donn√©es pour les tests fonctionnels, alors cela convient √† n'importe quel SGBD.</i> <br><br>  <b>DS</b> : Il y a de nombreuses ann√©es, nous l'avons fait sur des instantan√©s LVM.  Ceci est un classique.  Cette approche a √©t√© utilis√©e tr√®s activement.  Seuls les n≈ìuds avec √©tat sont une douleur.  Parce qu'ils n'ont pas besoin d'√™tre abandonn√©s, souvenez-vous toujours d'eux ... <br><br>  <i><b>NS</b> : Voyez-vous une possibilit√© hybride ici?</i>  <i>Disons que stateful est une sorte de pod, il fonctionne pour plusieurs personnes (de nombreux testeurs).</i>  <i>Nous avons un volume, mais gr√¢ce au syst√®me de fichiers, les clones sont locaux.</i>  <i>Si le pod tombe, le disque reste - le pod monte, il consid√®re les informations sur tous les clones, reprend tout et dit: "Voici vos clones sur ces ports, commencez √† travailler avec eux."</i> <br><br>  <b>DS</b> : Techniquement, cela signifie qu'au sein de Kubernetes, il s'agit d'un seul pod, √† l'int√©rieur duquel nous ex√©cutons de nombreux Postgres. <br><br>  <i><b>NS</b> : Oui.</i>  <i>Il a une limite: supposons, en m√™me temps, pas plus de 10 personnes travaillent avec lui.</i>  <i>Si vous en avez besoin de 20, ex√©cutez le deuxi√®me module de ce type.</i>  <i>De fa√ßon r√©aliste, clonez-le, apr√®s avoir re√ßu le deuxi√®me volume complet, il aura les m√™mes 10 clones ¬´fins¬ª.</i>  <i>Vous ne voyez pas une telle opportunit√©?</i> <br><br>  <b>DS</b> : Nous devons ajouter des probl√®mes de s√©curit√© ici.  Une telle option d'organisation implique que ce pod a des capacit√©s √©lev√©es car il peut effectuer des op√©rations non standard sur le syst√®me de fichiers ... Mais je r√©p√®te: je pense qu'√† moyen terme, le stockage sera fix√© dans Kubernetes, toute l'histoire avec des volumes sera fix√©e dans les nuages - tout fonctionnera "simplement".  Il va redimensionner, cloner ... Il y a un volume - nous disons: ¬´Cr√©ez-en un nouveau sur la base de cela¬ª - et apr√®s une seconde et demie nous obtenons ce dont nous avons besoin. <br><br>  <i><b>NS</b> : Je ne crois pas en une seconde et demie pour plusieurs t√©raoctets.</i>  <i>Chez Ceph, vous le faites vous-m√™me et vous parlez de nuages.</i>  <i>Acc√©dez au cloud, sur EC2, faites un clone du volume EBS de plusieurs t√©raoctets et voyez quelles seront les performances.</i>  <i>Cela ne prend pas quelques secondes.</i>  <i>Je suis tr√®s int√©ress√© quand ils atteignent un tel indicateur.</i>  <i>Je comprends de quoi vous parlez, mais je ne suis pas d'accord.</i> <br><br>  <b>DS</b> : D'accord, mais je l'ai dit √† moyen terme, pas √† court terme.  Depuis plusieurs ann√©es. <br><br><h2>  Op√©rateur Pro pour PostgreSQL de Zalando </h2><br>  Au milieu de cette r√©union, Alexey Klyukin, un ancien d√©veloppeur de Zalando, qui a parl√© de l'histoire de l'op√©rateur PostgreSQL, l'a √©galement rejoint: <br><br><blockquote>  C'est formidable qu'en g√©n√©ral, ce sujet ait √©t√© abord√©: Postgres et Kubernetes.  Lorsque nous avons commenc√© √† le faire √† Zalando en 2017, c'√©tait un sujet que tout le monde voulait faire, mais personne ne l'a fait.  Tout le monde avait d√©j√† Kubernetes, mais lorsqu'on leur a demand√© quoi faire avec les bases de donn√©es, m√™me des gens comme <a href="https://github.com/kelseyhightower">Kelsey Hightower</a> qui ont pr√™ch√© les K8 ont dit quelque chose comme ceci: <br><br>  <i>¬´Acc√©dez aux services g√©r√©s et utilisez-les, ne d√©marrez pas la base de donn√©es dans Kubernetes.</i>  <i>Sinon, vos K8 d√©cideront, par exemple, de mettre √† niveau, d'√©teindre tous les n≈ìuds et vos donn√©es voleront tr√®s loin. "</i> <br><br>  Nous avons d√©cid√© de faire un op√©rateur qui, contrairement √† ce conseil, lancera la base de donn√©es Postgres √† Kubernetes.  Et nous avions une bonne base - <a href="https://github.com/zalando/patroni">Patroni</a> .  Il s'agit d'un basculement automatique pour PostgreSQL, effectu√© correctement, c'est-√†-dire  en utilisant etcd, consul ou ZooKeeper comme r√©f√©rentiel pour les informations de cluster.  Un tel r√©f√©rentiel qui sera remis √† tous ceux qui demanderont, par exemple, quel leader est maintenant, les m√™mes informations - malgr√© le fait que nous ayons tout distribu√© - pour qu'il n'y ait pas de cerveau divis√©.  De plus, nous avions une <a href="https://github.com/zalando/patroni/tree/master/docker">image Docker</a> pour lui. <br><br>  En g√©n√©ral, le besoin de basculement automatique dans l'entreprise est apparu apr√®s la migration du centre de donn√©es Iron vers le cloud.  Le cloud √©tait bas√© sur une solution propri√©taire PaaS (Platform-as-a-Service).  C'est de l'Open Source, mais pour le d√©velopper, il a fallu travailler dur.  Cela s'appelait <a href="https://stups.io/">STUPS</a> . <br><br>  Au d√©part, il n'y avait pas de Kubernetes.  Plus pr√©cis√©ment, lorsque sa propre solution a √©t√© d√©ploy√©e, les K8 l'√©taient d√©j√†, mais si grossiers qu'ils n'√©taient pas adapt√©s √† la production.  C'√©tait, √† mon avis, 2015 ou 2016.  En 2017, Kubernetes est devenu plus ou moins mature - il fallait y migrer. <br><br>  Et nous avions d√©j√† un conteneur docker.  Il y avait PaaS qui utilisait Docker.  Pourquoi ne pas essayer les K8?  Pourquoi ne pas √©crire votre propre d√©claration?  Murat Kabilov, qui nous est venu d'Avito, a commenc√© cela comme un projet de sa propre initiative - ¬´jouer¬ª - et le projet ¬´a d√©coll√©¬ª. <br><br>  Mais en g√©n√©ral, je voulais parler d'AWS.  Pourquoi y avait-il un code historiquement li√© √† AWS ... <br><br>  Lorsque vous ex√©cutez quelque chose dans Kubernetes, vous devez comprendre que K8s est un tel travail en cours.  Il se d√©veloppe constamment, s'am√©liore et se brise m√™me p√©riodiquement.  Vous devez surveiller attentivement tous les changements dans Kubernetes, vous devez √™tre pr√™t √† vous y plonger et √† d√©couvrir comment cela fonctionne en d√©tail - peut-√™tre plus que vous ne le souhaiteriez.  Il s'agit, en principe, de toute plateforme sur laquelle vous ex√©cutez vos bases de donn√©es ... <br><br>  Ainsi, lorsque nous avons fait la d√©claration, nous avions Postgres, qui fonctionnait avec un volume externe (dans ce cas, EBS, puisque nous travaillions dans AWS).  La base de donn√©es grandissait, √† un moment donn√©, il a fallu la redimensionner: par exemple, la taille d'origine de l'EBS est de 100 To, la base de donn√©es a grandi, maintenant nous voulons faire de l'EBS en 200 To.  Comment?  Supposons que vous puissiez vider / restaurer vers une nouvelle instance, mais cela est long et avec des temps d'arr√™t. <br><br>  Par cons√©quent, je voulais un redimensionnement qui √©tendrait la partition EBS, puis indiquerait au syst√®me de fichiers d'utiliser le nouvel espace.  Et nous l'avons fait, mais √† ce moment-l√†, Kubernetes n'avait pas d'API pour l'op√©ration de redimensionnement.  Depuis que nous avons travaill√© sur AWS, nous avons √©crit du code pour son API. <br><br>  Personne ne prend la peine de faire de m√™me pour les autres plates-formes.  Il n'y a aucune complication dans la d√©claration selon laquelle il ne peut √™tre ex√©cut√© que sur AWS, et il ne fonctionnera pas sur tout le reste.  Il s'agit en g√©n√©ral d'un projet Open Source: si quelqu'un veut acc√©l√©rer l'√©mergence de l'utilisation de la nouvelle API, nous sommes les bienvenus.  Il y a <a href="https://github.com/zalando/postgres-operator">GitHub</a> , pull-requests - l'√©quipe Zalando essaie de leur r√©pondre rapidement et de promouvoir l'op√©rateur.  Pour autant que je sache, le projet a <a href="https://summerofcode.withgoogle.com/archive/2019/organizations/6187982082539520/">particip√©</a> √† Google Summer of Code et √† d'autres initiatives similaires.  Zalando y est tr√®s actif. <br></blockquote><br><h2>  Bonus PS! </h2><br>  Si vous √™tes int√©ress√© par le sujet de PostgreSQL et Kubernetes, alors nous attirons √©galement l'attention sur le fait que la semaine derni√®re, le prochain Postgres a eu lieu, o√π <b>Alexander Kukushkin de Zalando a</b> parl√© avec Nikolai.  La vid√©o est disponible <a href="https://www.youtube.com/watch%3Fv%3DFE0xi7SBqsg">ici</a> . <br><br><h2>  PPS </h2><br>  Lisez aussi dans notre blog: <br><br><ul><li>  ¬´ <a href="https://habr.com/ru/company/flant/blog/431500/">Bases de donn√©es et Kubernetes (revue et reportage vid√©o)</a> ¬ª; </li><li>  ¬´ <a href="https://habr.com/ru/company/flant/blog/475036/">Migration de Cassandra vers Kubernetes: fonctionnalit√©s et solutions</a> ¬ª; </li><li>  ¬´ <a href="https://habr.com/ru/company/flant/blog/461149/">Migration sans entrave de MongoDB vers Kubernetes</a> ¬ª; </li><li>  " <a href="https://habr.com/ru/company/flant/blog/450662/">Migration sans obstacle de RabbitMQ vers Kubernetes</a> ." </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr479438/">https://habr.com/ru/post/fr479438/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr479422/index.html">[Animation vid√©o] Monde filaire: comment en 35 ans un r√©seau de c√¢bles sous-marins a envelopp√© le globe</a></li>
<li><a href="../fr479426/index.html">Semaine de la s√©curit√© 50: Attaques d'homme au milieu dans Confluence et Linux</a></li>
<li><a href="../fr479428/index.html">√âv√©nements num√©riques √† Moscou du 9 au 15 d√©cembre</a></li>
<li><a href="../fr479430/index.html">√âv√©nements num√©riques √† Saint-P√©tersbourg du 9 au 15 d√©cembre</a></li>
<li><a href="../fr479432/index.html">Yandex.Maps: Je suis all√© au contr√¥leur de carte - j'ai imm√©diatement obtenu la position de l'utilisateur (d'accord, maintenant s√©rieusement)</a></li>
<li><a href="../fr479442/index.html">Alexey Savvateev: Mod√®le de schisme social de la th√©orie des jeux (+ enqu√™te nginx)</a></li>
<li><a href="../fr479446/index.html">Les voitures ont d√©j√† une longueur d'avance sur les tests de lecture; mais comprennent-ils ce qu'ils lisent?</a></li>
<li><a href="../fr479450/index.html">AppCode 2019.3: fonctionne plus rapidement, comprend mieux Swift, conna√Æt Mac Catalyst et affiche facilement les messages d'assemblage</a></li>
<li><a href="../fr479452/index.html">Comment le syst√®me de noms de domaine s'est d√©velopp√©: l'√®re ARPANET</a></li>
<li><a href="../fr479458/index.html">Beaut√© ou praticit√© dans la salle des serveurs</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>