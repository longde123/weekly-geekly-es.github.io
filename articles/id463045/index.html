<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🚶 💿 🦓 Secara otomatis mendeteksi emosi dalam percakapan teks menggunakan jaringan saraf 👩‍👩‍👧‍👦 🤳 🚙</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Salah satu tugas utama sistem dialog adalah tidak hanya menyediakan informasi yang dibutuhkan pengguna, tetapi juga menghasilkan sebanyak mungkin jawa...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Secara otomatis mendeteksi emosi dalam percakapan teks menggunakan jaringan saraf</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/463045/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/t6/sr/jr/t6srjrmjjmm6qn8gpld9emy4txu.gif"></div><br>  Salah satu tugas utama sistem dialog adalah tidak hanya menyediakan informasi yang dibutuhkan pengguna, tetapi juga menghasilkan sebanyak mungkin jawaban manusia.  Dan pengenalan emosi lawan bicaranya bukan lagi fitur keren, itu adalah kebutuhan vital.  Dalam artikel ini, kita akan melihat <b>arsitektur jaringan saraf berulang untuk menentukan emosi dalam percakapan teks</b> , yang mengambil bagian dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">SemEval-2019 Tugas 3 "EmoContext"</a> , kompetisi tahunan dalam linguistik komputer.  Tugasnya adalah untuk mengklasifikasikan emosi ("bahagia", "sedih", "marah" dan "lain-lain") dalam percakapan tiga komentar, di mana sebuah obrolan bot dan seseorang berpartisipasi. <br><br>  Pada bagian pertama artikel kami akan mempertimbangkan tugas yang diatur dalam EmoContext dan data yang disediakan oleh penyelenggara.  Di bagian kedua dan ketiga, kami menganalisis pemrosesan awal teks dan cara-cara representasi vektor kata-kata.  Pada bagian keempat, kami menggambarkan arsitektur LSTM yang kami gunakan dalam kompetisi.  Kode ditulis dalam Python menggunakan perpustakaan Keras. <br><a name="habracut"></a><br><h2>  1. Data pelatihan </h2><br>  Lagu "EmoContext" di SemEval-2019 didedikasikan untuk definisi emosi dalam percakapan teks, dengan mempertimbangkan konteks korespondensi.  Konteks dalam kasus ini adalah beberapa komentar berturut-turut dari peserta dialog.  Ada dua peserta dalam percakapan: pengguna anonim (ia memiliki replika pertama dan ketiga) dan bot obrolan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Ruuh</a> (ia memiliki replika kedua).  Berdasarkan tiga replika, perlu untuk menentukan emosi apa yang dialami pengguna saat menulis jawaban ke chatbot (Tabel 1).  Secara total, markup dataset berisi empat emosi: "bahagia", "sedih", "marah" atau "lain-lain" (Tabel 1).  Penjelasan terperinci disajikan di sini: ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Chatterjee et al., 2019</a> ). <br><br>  <i>Tabel 1. Contoh dari dataset EmoContext ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Chatterjee et al., 2019</a> )</i> <br><div class="scrollable-table"><table><tbody><tr><th>  Pengguna (Tahap-1) </th><th>  Robot Interaktif (Tahap-1) </th><th>  Pengguna (Tahap 2) </th><th>  Kelas yang benar </th></tr><tr><td>  Saya hanya memenuhi syarat untuk magang Nabard </td><td>  WOOT!  Itu berita bagus.  Selamat! </td><td>  Saya mulai menangis </td><td>  Kebahagiaan </td></tr><tr><td>  Beraninya kau menampar anakku </td><td>  Jika Anda merusak mobil saya, saya akan melakukannya untuk Anda juga </td><td>  Coba lakukan itu sekali saja </td><td>  Amarah </td></tr><tr><td>  Aku lebih disakiti olehmu </td><td>  Anda tidak bersungguh-sungguh. </td><td>  katakan aku cinta aku </td><td>  Kesedihan </td></tr><tr><td>  Saya akan melakukan malam. </td><td>  Baiklah  Buat saya tetap di dalam lingkaran. </td><td>  Tidak memberi WhatsApp no. </td><td>  Lainnya </td></tr></tbody></table></div><br>  Selama kompetisi, panitia menyediakan beberapa set data.  Dataset pelatihan (Kereta) terdiri dari 30.160 teks yang ditandai secara manual.  Dalam teks-teks ini ada sekitar 5000 objek milik kelas "bahagia", "sedih" dan "marah", serta 15000 teks dari kelas "lain-lain" (Tabel 2). <br><br>  Panitia juga menyediakan set data untuk pengembangan (Dev) dan pengujian (Tes), di mana, tidak seperti dataset pelatihan, distribusi berdasarkan kelas emosi sesuai dengan kehidupan nyata: sekitar 4% untuk masing-masing kelas "bahagia", "sedih" dan " marah ", dan sisanya adalah kelas" orang lain ".  Data yang disediakan oleh Microsoft, Anda dapat mengunduhnya di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">grup resmi di LinkedIn</a> . <br><br>  <i>Tabel 2. Distribusi label kelas emosi dalam dataset ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Chatterjee et al., 2019</a> ).</i> <br><div class="scrollable-table"><table><tbody><tr><th>  Datacet </th><th>  Kebahagiaan </th><th>  Kesedihan </th><th>  Amarah </th><th>  Lainnya </th><th>  Total </th></tr><tr><td> Pelatihan <br></td><td>  14,07% <br></td><td>  18.11% <br></td><td>  18,26% <br></td><td>  49,56% <br></td><td>  30 160 <br></td></tr><tr><td>  Untuk berkembang <br></td><td>  5,15% <br></td><td>  4,54% <br></td><td>  5,45% <br></td><td>  84,86% <br></td><td>  2755 <br></td></tr><tr><td>  Tes <br></td><td>  5,16% <br></td><td>  4,54% <br></td><td>  5,41% <br></td><td>  84,90% <br></td><td>  5509 <br></td></tr><tr><td>  Jauh <br></td><td>  33,33% <br></td><td>  33,33% <br></td><td>  33,33% <br></td><td>  0% <br></td><td>  900 ribu <br></td></tr></tbody></table></div><br>  Selain data ini, kami mengumpulkan 900 ribu pesan berbahasa Inggris dari Twitter untuk membuat dataset Jauh (300 ribu tweet untuk setiap emosi).  Saat membuatnya, kami mengikuti strategi Go et al.  (2009), dalam kerangka yang pesannya hanya dikaitkan dengan kehadiran kata-kata yang berhubungan dengan emosi, seperti #angry, #annoyed, #happy, #sad, #surprised, dan sebagainya.  Daftar istilah didasarkan pada ketentuan dari SemEval-2018 AIT DISC ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Duppada et al., 2018</a> ). <br><br>  Metrik kualitas utama dalam kompetisi EmoContext adalah ukuran rata-rata F1 untuk tiga kelas emosi, yaitu, untuk kelas "senang", "sedih" dan "marah". <br><br><pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocessData</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(dataFilePath, mode)</span></span></span><span class="hljs-function">:</span></span> conversations = [] labels = [] <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> io.open(dataFilePath, encoding=<span class="hljs-string"><span class="hljs-string">"utf8"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> finput: finput.readline() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> finput: line = line.strip().split(<span class="hljs-string"><span class="hljs-string">'\t'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>): line[i] = tokenize(line[i]) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> mode == <span class="hljs-string"><span class="hljs-string">"train"</span></span>: labels.append(emotion2label[line[<span class="hljs-number"><span class="hljs-number">4</span></span>]]) conv = line[<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">4</span></span>] conversations.append(conv) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> mode == <span class="hljs-string"><span class="hljs-string">"train"</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.array(conversations), np.array(labels) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.array(conversations) texts_train, labels_train = preprocessData(<span class="hljs-string"><span class="hljs-string">'./starterkitdata/train.txt'</span></span>, mode=<span class="hljs-string"><span class="hljs-string">"train"</span></span>) texts_dev, labels_dev = preprocessData(<span class="hljs-string"><span class="hljs-string">'./starterkitdata/dev.txt'</span></span>, mode=<span class="hljs-string"><span class="hljs-string">"train"</span></span>) texts_test, labels_test = preprocessData(<span class="hljs-string"><span class="hljs-string">'./starterkitdata/test.txt'</span></span>, mode=<span class="hljs-string"><span class="hljs-string">"train"</span></span>)</code> </pre> <br><h2>  2. Pra-pemrosesan teks </h2><br>  Sebelum pelatihan, kami memproses teks menggunakan alat Ekphrasis (Baziotis et al., 2017).  Ini membantu untuk memperbaiki ejaan, menormalkan kata-kata, segmen, dan juga menentukan token mana yang harus dibuang, dinormalisasi atau dijelaskan menggunakan tag khusus.  Pada tahap pra-pemrosesan, kami melakukan hal berikut: <br><br><ul><li>  URL dan surat, tanggal dan waktu, nama panggilan, persentase, mata uang dan angka diganti dengan tag yang sesuai. </li><li>  Istilah huruf besar berulang, disensor, memanjang kami disertai dengan label yang sesuai. </li><li>  Kata-kata memanjang telah diperbaiki secara otomatis. </li></ul><br>  Selain itu, Penekanan mengandung tokenizer yang dapat mengidentifikasi sebagian besar emoji, emotikon dan ekspresi kompleks, serta tanggal, waktu, mata uang, dan akronim. <br><br>  <i>Tabel 3. Contoh preprocessing teks.</i> <br><div class="scrollable-table"><table><tbody><tr><th>  Sumber teks </th><th>  Teks pra-diproses </th></tr><tr><td>  AKU MERASA ANDA ... Saya membobol jutaan keping <img src="https://habrastorage.org/webt/2n/p4/l5/2np4l5uym3fkohcwlijjcma8eaw.png" width="100"></td><td>  &lt;allcaps&gt; saya merasa Anda &lt;/allcaps&gt;.  [Diulang] saya membobol jutaan keping <img src="https://habrastorage.org/webt/2n/p4/l5/2np4l5uym3fkohcwlijjcma8eaw.png" width="100"></td></tr><tr><td>  lelah dan aku juga merindukanmu :–( </td><td>  lelah dan aku juga merindukanmu &lt;sad&gt; </td></tr><tr><td>  Anda harus liiiiii mendengarkan ini: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">www.youtube.com/watch?v=99myH1orbs4</a> </td><td>  Anda harus mendengarkan &lt;elongated&gt; untuk ini: &lt;url&gt; </td></tr><tr><td>  Apartemen saya yang mengurusnya.  Sewa saya sekitar $ 650. </td><td>  apartemen saya mengurusnya.  sewa saya sekitar &lt;money&gt;. </td></tr></tbody></table></div><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ekphrasis.classes.preprocessor <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> TextPreProcessor <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ekphrasis.classes.tokenizer <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SocialTokenizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ekphrasis.dicts.emoticons <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> emoticons <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> io label2emotion = {<span class="hljs-number"><span class="hljs-number">0</span></span>: <span class="hljs-string"><span class="hljs-string">"others"</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>: <span class="hljs-string"><span class="hljs-string">"happy"</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>: <span class="hljs-string"><span class="hljs-string">"sad"</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>: <span class="hljs-string"><span class="hljs-string">"angry"</span></span>} emotion2label = {<span class="hljs-string"><span class="hljs-string">"others"</span></span>: <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-string"><span class="hljs-string">"happy"</span></span>: <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-string"><span class="hljs-string">"sad"</span></span>: <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-string"><span class="hljs-string">"angry"</span></span>: <span class="hljs-number"><span class="hljs-number">3</span></span>} emoticons_additional = { <span class="hljs-string"><span class="hljs-string">'(^・^)'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">':‑c'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;sad&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">'=‑d'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">":'‑)"</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">':‑d'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;laugh&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">':‑('</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;sad&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">';‑)'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">':‑)'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">':\\/'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;sad&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">'d=&lt;'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;annoyed&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">':‑/'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;annoyed&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">';‑]'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">'(^ ^)'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">'angru'</span></span>: <span class="hljs-string"><span class="hljs-string">'angry'</span></span>, <span class="hljs-string"><span class="hljs-string">"d‑':"</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;annoyed&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">":'‑("</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;sad&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">":‑["</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;annoyed&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">'( ? )'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">'x‑d'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;laugh&gt;'</span></span>, } text_processor = TextPreProcessor( <span class="hljs-comment"><span class="hljs-comment"># terms that will be normalized normalize=['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'url', 'date', 'number'], # terms that will be annotated annotate={"hashtag", "allcaps", "elongated", "repeated", 'emphasis', 'censored'}, fix_html=True, # fix HTML tokens # corpus from which the word statistics are going to be used # for word segmentation segmenter="twitter", # corpus from which the word statistics are going to be used # for spell correction corrector="twitter", unpack_hashtags=True, # perform word segmentation on hashtags unpack_contractions=True, # Unpack contractions (can't -&gt; can not) spell_correct_elong=True, # spell correction for elongated words # select a tokenizer. You can use SocialTokenizer, or pass your own # the tokenizer, should take as input a string and return a list of tokens tokenizer=SocialTokenizer(lowercase=True).tokenize, # list of dictionaries, for replacing tokens extracted from the text, # with other expressions. You can pass more than one dictionaries. dicts=[emoticons, emoticons_additional] ) def tokenize(text): text = " ".join(text_processor.pre_process_doc(text)) return text</span></span></code> </pre><br><h2>  3. Representasi vektor kata-kata </h2><br>  Representasi vektor telah menjadi bagian integral dari sebagian besar pendekatan untuk penciptaan sistem NLP menggunakan pembelajaran yang mendalam.  Untuk menentukan model pemetaan vektor yang paling cocok, kami mencoba Word2Vec ( <a href="">Mikolov et al., 2013</a> ), GloVe ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pennington et al., 2014</a> ) dan FastText ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Joulin et al., 2017</a> ), serta vektor DataStories yang telah dilatih sebelumnya ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Baziotis et al. ., 2017</a> ).  Word2Vec menemukan hubungan antara kata-kata dengan mengasumsikan bahwa kata-kata yang berhubungan secara semantik ditemukan dalam konteks yang sama.  Word2Vec mencoba untuk memprediksi kata target (arsitektur CBOW) atau konteks (arsitektur Skip-Gram), yaitu, meminimalkan fungsi kerugian, dan GloVe menghitung vektor kata, mengurangi dimensi dari matriks adjacency.  Logika FastText mirip dengan logika Word2Vec, kecuali bahwa ia menggunakan n-gram simbolis untuk membangun vektor kata, dan sebagai hasilnya, ia dapat memecahkan masalah kata-kata yang tidak dikenal. <br><br>  Untuk semua model yang disebutkan, kami menggunakan parameter pelatihan default yang disediakan oleh penulis.  Kami melatih model LSTM sederhana (dim = 64) berdasarkan masing-masing representasi vektor ini dan membandingkan efisiensi klasifikasi menggunakan cross-validation.  Hasil terbaik dalam tindakan F1 ditunjukkan oleh vektor DataStories yang telah dilatih sebelumnya. <br><br>  Untuk memperkaya pemetaan vektor yang dipilih dengan pewarnaan emosional kata-kata, kami memutuskan untuk menyempurnakan vektor menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dataset</a> Jauh yang berlabel secara otomatis ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Deriu et al., 2017</a> ).  Kami menggunakan dataset Jauh untuk melatih jaringan LSTM sederhana untuk mengklasifikasikan pesan "jahat", "sedih" dan "bahagia".  Lapisan embedding dibekukan selama iterasi pertama pelatihan untuk menghindari perubahan kuat dalam bobot vektor, dan untuk lima iterasi berikutnya lapisan dicairkan.  Setelah pelatihan, vektor "tertunda" disimpan untuk digunakan nanti dalam jaringan saraf, serta <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dibagikan</a> . <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getEmbeddings</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(file)</span></span></span><span class="hljs-function">:</span></span> embeddingsIndex = {} dim = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> io.open(file, encoding=<span class="hljs-string"><span class="hljs-string">"utf8"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> f: values = line.split() word = values[<span class="hljs-number"><span class="hljs-number">0</span></span>] embeddingVector = np.asarray(values[<span class="hljs-number"><span class="hljs-number">1</span></span>:], dtype=<span class="hljs-string"><span class="hljs-string">'float32'</span></span>) embeddingsIndex[word] = embeddingVector dim = len(embeddingVector) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> embeddingsIndex, dim <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getEmbeddingMatrix</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(wordIndex, embeddings, dim)</span></span></span><span class="hljs-function">:</span></span> embeddingMatrix = np.zeros((len(wordIndex) + <span class="hljs-number"><span class="hljs-number">1</span></span>, dim)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word, i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> wordIndex.items(): embeddingMatrix[i] = embeddings.get(word) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> embeddingMatrix <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.text <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Tokenizer embeddings, dim = getEmbeddings(<span class="hljs-string"><span class="hljs-string">'emosense.300d.txt'</span></span>) tokenizer = Tokenizer(filters=<span class="hljs-string"><span class="hljs-string">''</span></span>) tokenizer.fit_on_texts([<span class="hljs-string"><span class="hljs-string">' '</span></span>.join(list(embeddings.keys()))]) wordIndex = tokenizer.word_index print(<span class="hljs-string"><span class="hljs-string">"Found %s unique tokens."</span></span> % len(wordIndex)) embeddings_matrix = getEmbeddingMatrix(wordIndex, embeddings, dim)</code> </pre><br><h2>  4. Arsitektur jaringan saraf </h2><br>  Recurrent Neural Networks (RNNs) adalah keluarga jaringan saraf yang berspesialisasi dalam memproses serangkaian acara.  Tidak seperti jaringan saraf tradisional, RNN dirancang untuk bekerja dengan urutan menggunakan keseimbangan internal.  Untuk ini, grafik komputasi RNN berisi siklus yang mencerminkan pengaruh informasi sebelumnya dari urutan peristiwa pada saat ini.  Jaringan saraf LSTM (Memori Jangka Pendek Panjang) diperkenalkan sebagai perpanjangan RNN pada tahun 1997 ( <a href="">Hochreiter dan Schmidhuber, 1997</a> ).  Sel rekurensi LSTM terhubung untuk menghindari masalah meledak dan pudar.  LSTM tradisional hanya menyimpan informasi masa lalu ketika memproses urutan dalam satu arah.  LSTM dua arah yang beroperasi di kedua arah menggabungkan output dari dua lapisan LSTM tersembunyi yang mentransmisikan informasi dalam arah yang berlawanan - satu dalam perjalanan waktu dan yang lainnya terhadap - dengan demikian secara bersamaan menerima data dari kondisi masa lalu dan masa depan ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Schuster dan Paliwal, 1997</a> ). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bdf/d46/a41/bdfd46a41a20ba916382a57bb7c17e19.png"><br>  <i>Gambar 1: Versi arsitektur yang dikurangi.</i>  <i>Modul LSTM menggunakan bobot yang sama untuk tahap pertama dan ketiga.</i> <br><br>  Representasi yang disederhanakan dari pendekatan yang dijelaskan disajikan pada Gambar 1. Arsitektur jaringan saraf terdiri dari lapisan embedding dan dua modul LTSM dua arah (redup = 64).  Modul LTSM pertama menganalisis kata-kata pengguna pertama (mis., Replika pertama dan ketiga dari percakapan), dan modul kedua menganalisis kata-kata pengguna kedua (replika kedua).  Pada tahap pertama, kata-kata setiap pengguna yang menggunakan representasi vektor pra-dilatih dimasukkan ke dalam modul LTSM dua arah yang sesuai.  Kemudian tiga peta fitur yang dihasilkan digabungkan menjadi vektor fitur datar, dan kemudian ditransfer ke lapisan tersembunyi yang sepenuhnya terhubung (dim = 30), yang menganalisis interaksi antara fitur yang diekstraksi.  Akhirnya, karakteristik ini diproses dalam lapisan keluaran menggunakan fungsi aktivasi softmax untuk menentukan label kelas akhir.  Untuk mengurangi overfitting, setelah lapisan representasi vektor, lapisan regularisasi dengan noise Gaussian ditambahkan, dan lapisan dropout ditambahkan ke setiap modul LTSM (p = 0,2) dan lapisan yang terhubung sepenuhnya tersembunyi (p = 0,1) ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Srivastava et al., 2014</a> ) <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Input, Dense, Embedding, Concatenate, Activation, \ Dropout, LSTM, Bidirectional, GlobalMaxPooling1D, GaussianNoise <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">buildModel</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(embeddings_matrix, sequence_length, lstm_dim, hidden_layer_dim, num_classes, noise=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.1</span></span></span></span><span class="hljs-function"><span class="hljs-params">, dropout_lstm=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.2</span></span></span></span><span class="hljs-function"><span class="hljs-params">, dropout=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.2</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> turn1_input = Input(shape=(sequence_length,), dtype=<span class="hljs-string"><span class="hljs-string">'int32'</span></span>) turn2_input = Input(shape=(sequence_length,), dtype=<span class="hljs-string"><span class="hljs-string">'int32'</span></span>) turn3_input = Input(shape=(sequence_length,), dtype=<span class="hljs-string"><span class="hljs-string">'int32'</span></span>) embedding_dim = embeddings_matrix.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>] embeddingLayer = Embedding(embeddings_matrix.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], embedding_dim, weights=[embeddings_matrix], input_length=sequence_length, trainable=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) turn1_branch = embeddingLayer(turn1_input) turn2_branch = embeddingLayer(turn2_input) turn3_branch = embeddingLayer(turn3_input) turn1_branch = GaussianNoise(noise, input_shape=(<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, sequence_length, embedding_dim))(turn1_branch) turn2_branch = GaussianNoise(noise, input_shape=(<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, sequence_length, embedding_dim))(turn2_branch) turn3_branch = GaussianNoise(noise, input_shape=(<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, sequence_length, embedding_dim))(turn3_branch) lstm1 = Bidirectional(LSTM(lstm_dim, dropout=dropout_lstm)) lstm2 = Bidirectional(LSTM(lstm_dim, dropout=dropout_lstm)) turn1_branch = lstm1(turn1_branch) turn2_branch = lstm2(turn2_branch) turn3_branch = lstm1(turn3_branch) x = Concatenate(axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>)([turn1_branch, turn2_branch, turn3_branch]) x = Dropout(dropout)(x) x = Dense(hidden_layer_dim, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) output = Dense(num_classes, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)(x) model = Model(inputs=[turn1_input, turn2_input, turn3_input], outputs=output) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'categorical_crossentropy'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'acc'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model model = buildModel(embeddings_matrix, MAX_SEQUENCE_LENGTH, lstm_dim=<span class="hljs-number"><span class="hljs-number">64</span></span>, hidden_layer_dim=<span class="hljs-number"><span class="hljs-number">30</span></span>, num_classes=<span class="hljs-number"><span class="hljs-number">4</span></span>)</code> </pre> <br><h2>  5. Hasil </h2><br>  Dalam mencari arsitektur optimal, kami bereksperimen tidak hanya dengan jumlah neuron di lapisan, fungsi aktivasi dan parameter regularisasi, tetapi juga dengan arsitektur jaringan saraf itu sendiri.  Ini dijelaskan lebih detail dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">karya aslinya</a> . <br><br>  Arsitektur yang dijelaskan pada bagian sebelumnya menunjukkan hasil terbaik saat pelatihan tentang dataset Kereta Api dan validasi pada dataset Dev, sehingga digunakan pada tahap akhir kompetisi.  Pada dataset tes terakhir, model menunjukkan ukuran F1 mikro rata-rata 72,59%, dan hasil maksimum yang dicapai di antara semua peserta adalah 79,59%.  Namun demikian, hasil kami jauh lebih tinggi dari nilai dasar 58,68% yang ditetapkan oleh penyelenggara. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kode sumber untuk model dan representasi kata-kata vektor</a> tersedia di GitHub. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Versi lengkap artikel</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">bekerja dengan deskripsi tugas</a> ada di situs web ACL Anthology. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Dataset pelatihan</a> dapat diunduh dari grup LinkedIn resmi. <br><br>  Mengutip: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@inproceedings{smetanin-2019-emosense, title = "{E}mo{S}ense at {S}em{E}val-2019 Task 3: Bidirectional {LSTM} Network for Contextual Emotion Detection in Textual Conversations", author = "Smetanin, Sergey", booktitle = "Proceedings of the 13th International Workshop on Semantic Evaluation", year = "2019", address = "Minneapolis, Minnesota, USA", publisher = "Association for Computational Linguistics", url = "https://www.aclweb.org/anthology/S19-2034", pages = "210--214", }</span></span></code> </pre> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id463045/">https://habr.com/ru/post/id463045/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id463031/index.html">Hidup dan belajar. Bagian 3. Pendidikan berkelanjutan atau usia siswa abadi</a></li>
<li><a href="../id463035/index.html">Berita dari dunia OpenStreetMap No. 471 (07.23.2019-29.07.2019)</a></li>
<li><a href="../id463037/index.html">Mencari inspirasi, atau Cara keluar dari F</a></li>
<li><a href="../id463039/index.html">Pembersih debu manicure sendiri</a></li>
<li><a href="../id463041/index.html">Didefinisikan atau Tidak Ditentukan? Nuansa membuat array dalam JavaScript</a></li>
<li><a href="../id463055/index.html">Tentang admin, devops, kebingungan tanpa akhir dan transformasi DevOps dalam perusahaan</a></li>
<li><a href="../id463057/index.html">Kerangka kerja Yii 2 hak khusus</a></li>
<li><a href="../id463059/index.html">Tiga tinggal di IT dan bukan hanya</a></li>
<li><a href="../id463061/index.html">Aturan untuk menyiapkan tata letak dalam Figma</a></li>
<li><a href="../id463063/index.html">Kami berurusan dengan antarmuka di Go</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>