<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ùå ü¶í üí™üèø KI, praktischer Kurs. Einrichten des Modells und der Hyperparameter zum Erkennen von Emotionen in Bildern üåø üìó ü§õüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In den vorherigen Artikeln dieser Trainingsreihe wurden m√∂gliche Optionen zum Aufbereiten von Daten beschrieben. Vorverarbeitung und Hinzuf√ºgen von Da...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>KI, praktischer Kurs. Einrichten des Modells und der Hyperparameter zum Erkennen von Emotionen in Bildern</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/intel/blog/421367/"><img src="https://habrastorage.org/webt/zq/7s/el/zq7selxswjsrmplg_pxw2xilqi4.jpeg"><br><br>  In den vorherigen Artikeln dieser Trainingsreihe wurden m√∂gliche Optionen zum Aufbereiten von Daten beschrieben. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vorverarbeitung und Hinzuf√ºgen von Daten mit Bildern</a> ; in diesen Artikeln wurde auch das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Basismodell zum Erkennen von Emotionen</a> basierend auf Bildern eines Faltungsnetzwerks konstruiert. <br>  In diesem Artikel werden wir ein verbessertes Faltungsmodell f√ºr neuronale Netze zum Erkennen von Emotionen in Bildern mithilfe einer als <i>induktives Lernen bezeichneten</i> Technik erstellen. <br><a name="habracut"></a><br>  Zun√§chst m√ºssen Sie sich mit dem Artikel √ºber das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grundmodell zum Erkennen von Emotionen in Bildern</a> vertraut machen. Sie k√∂nnen auch beim Lesen darauf verweisen, da einige Abschnitte, einschlie√ülich des Studierens der Quelldaten und der Beschreibung von Netzwerkindikatoren, hier nicht im Detail aufgef√ºhrt werden. <br><br><h2>  <font color="#0071c5">Daten</font> </h2><br>  Der Datensatz enth√§lt 1630 Bilder mit Emotionen aus zwei Klassen: <i>Negativ</i> (Klasse 0) und <i>Positiv</i> (Klasse 1).  Einige Beispiele f√ºr solche Bilder sind unten angegeben. <br><br>  <b>Negativ</b> <br><img src="https://habrastorage.org/webt/zr/pf/ki/zrpfkiqvgcxw6kpsv777v9d1t6w.jpeg"><br><br><img src="https://habrastorage.org/webt/52/8x/s6/528xs6k7jnimfgvhagwolru8mi4.jpeg"><br><br><img src="https://habrastorage.org/webt/no/p0/vb/nop0vbapr4ep7o5dps1wvpkncoa.jpeg"><br><br>  <b>Positiv</b> <br><img src="https://habrastorage.org/webt/uc/ua/oh/ucuaohklcwcgotqf4uryopnt_qg.jpeg"><br><br><img src="https://habrastorage.org/webt/u5/hp/rb/u5hprb1cjig28b4xk8urdljb8lm.jpeg"><br><br><img src="https://habrastorage.org/webt/ru/_a/xo/ru_axoahw4h4xytx_-yphxk56ii.jpeg"><br><br>  Einige der Beispiele enthalten offensichtliche positive oder negative Emotionen, w√§hrend andere m√∂glicherweise nicht kategorisiert werden - selbst bei menschlicher Beteiligung.  Basierend auf einer Sichtpr√ºfung solcher F√§lle sch√§tzen wir, dass die maximal m√∂gliche Genauigkeit bei etwa 80 Prozent liegen sollte.  Beachten Sie, dass ein Zufallsklassifizierer aufgrund eines kleinen Ungleichgewichts in den Klassen eine Genauigkeit von ungef√§hr 53 Prozent bietet. <br><br>  Um das Modell zu trainieren, verwenden wir die Technik <i>, einen Teil der Proben beizubehalten</i> und den Anfangsdatensatz in zwei Teile zu teilen, von denen einer (20 Prozent des Anfangssatzes) von uns zur √úberpr√ºfung verwendet wird.  Die Partitionierung erfolgt mithilfe der <i>Schichtung</i> : Dies bedeutet, dass das Gleichgewicht zwischen den Klassen in den Trainings- und Tests√§tzen erhalten bleibt. <br><br><h2>  <font color="#0071c5">Beheben von Datenmangel</font> </h2><br>  Das Grundmodell zeigte Ergebnisse, die nur geringf√ºgig besser waren als zuf√§llige Vorhersagen der Bildklasse.  Es kann viele m√∂gliche Gr√ºnde f√ºr dieses Verhalten geben.  Wir glauben, dass der Hauptgrund darin besteht, dass die verf√ºgbare Datenmenge f√ºr ein solches Training des Faltungsteils des Netzwerks, das es erm√∂glichen w√ºrde, charakteristische Merkmale basierend auf dem Eingabebild zu erhalten, entschieden unzureichend ist. <br>  Es gibt viele verschiedene M√∂glichkeiten, um das Problem der Datenschw√§che zu l√∂sen.  Hier sind einige davon: <br><br><ul><li>  <b>Wiederholen</b> .  Die Idee der Methode ist es, die Verteilung von Daten zu bewerten und <i>neue Beispiele</i> aus dieser Verteilung auszuw√§hlen. </li><li>  <b>Lernen ohne Lehrer</b> .  Jeder kann gro√üe Datenmengen finden, die der Art der markierten Beispiele in einem bestimmten Datensatz entsprechen.  Beispielsweise k√∂nnen es Filme zur Videoerkennung oder H√∂rb√ºcher zur Spracherkennung sein.  Der n√§chste Schritt auf diesem Weg besteht darin, diese Daten f√ºr das Vortraining des Modells zu verwenden (z. B. mithilfe von Auto-Encodern). </li><li>  <b>Datenerweiterung</b> .  W√§hrend dieses Prozesses werden Probendaten unter Verwendung eines gegebenen Satzes von Transformationen zuf√§llig modifiziert. </li><li>  <b>Induktives Lernen</b> .  Dieses Thema ist f√ºr uns von gro√üem Interesse. Lassen Sie uns es n√§her kennenlernen. </li></ul><br><h2>  <font color="#0071c5">Induktives Lernen</font> </h2><br>  Der Begriff <i>induktives Training</i> bezieht sich auf eine Reihe von Techniken unter Verwendung von Modellen (oft sehr gro√ü), die auf verschiedenen Datens√§tzen ungef√§hr derselben Art trainiert wurden. <br><br><img src="https://habrastorage.org/webt/wl/jb/qi/wljbqidbfmpj1yfddtvjlkqt9ma.png"><br><br><img src="https://habrastorage.org/webt/bq/ji/ah/bqjiahabshkakrv2jcilp5pa1y8.png"><br><br>  Vergleich traditioneller maschineller und induktiver Lernmethoden.  Bild aus S. Ruders Blogeintrag <i>"Was ist induktives Lernen?"</i>  . <br>  Es gibt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">drei</a> Hauptszenarien f√ºr die Verwendung von induktivem Lernen: <br><br><ul><li>  <b>Vorgefertigte Modelle</b> .  Jeder Benutzer kann einfach ein von einer anderen Person geschultes Modell nehmen und es f√ºr seine Aufgaben verwenden.  Ein solches Szenario ist m√∂glich, wenn die Aufgaben sehr √§hnlich sind. </li><li>  <b>Blockieren Sie die Auswahl der Zeichen</b> .  An diesem Punkt wissen wir, dass die Architektur des Modells in zwei Hauptteile unterteilt werden kann: die <i>Merkmalsextraktionseinheit</i> , die f√ºr das Extrahieren von Merkmalen aus den Eingabedaten verantwortlich ist, und das <i>Klassifizierungsmodul</i> , das Beispiele basierend auf den empfangenen Merkmalen klassifiziert.  In der Regel ist der Feature-Extraktionsblock der Hauptteil des Modells.  Die Idee des Verfahrens besteht darin, einen Block zur Unterscheidung von Merkmalen von einem in einem anderen Problem trainierten Modell zu nehmen, seine Gewichtskoeffizienten festzulegen (sie nicht trainiert zu machen) und dann auf seiner Basis neue Klassifizierungsmodule f√ºr das betrachtete Problem aufzubauen.  Das Klassifizierungsmodul ist normalerweise nicht sehr tief und besteht aus mehreren vollst√§ndig verbundenen Schichten, sodass dieses Modell viel einfacher zu trainieren ist. </li><li>  <b>Pr√§zise und tiefe Abstimmung</b> .  Diese Methode √§hnelt einem Szenario mit einem Feature-Extraktionsblock.  Dieselben Aktionen werden ausgef√ºhrt, mit Ausnahme des "Einfrierens" des Feature-Extraktionsblocks.  Sie k√∂nnen beispielsweise das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">VGG-</a> Netzwerk als Merkmalsextraktionsblock verwenden und nur die ersten drei (von vier) Faltungsbl√∂cke darin "einfrieren".  In diesem Fall kann sich die Merkmalsextraktionseinheit besser an die aktuelle Aufgabe anpassen.  Weitere Informationen finden Sie im Blog-Beitrag von F. Chollet. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erstellen Sie leistungsstarke Bildklassifizierungsmodelle mit einer sehr kleinen Datenmenge</a> . </li></ul><br>  Eine detaillierte Beschreibung der Szenarien f√ºr die Verwendung des induktiven Lernens finden Sie im Kurs <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CS231n</a> der Stanford University <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zur Faltung neuronaler Netze zur visuellen Erkennung</a> durch Fei-Fei Li und in Blogeintr√§gen von S. Ruder. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Induktives Lernen ist die n√§chste Grenze in der Entwicklung maschinelles Lernen</a> (Themen umfassender diskutiert). <br><br>  M√∂glicherweise haben Sie Fragen: Warum werden all diese Methoden ben√∂tigt und warum k√∂nnen sie funktionieren?  Wir werden versuchen, sie zu beantworten. <br><br><ul><li>  Vorteile der Verwendung gro√üer Datenmengen.  Zum Beispiel k√∂nnen wir den Feature-Extraktionsblock aus einem Modell √ºbernehmen, das auf 14 Millionen Bildern trainiert wurde, die im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ImageNet-</a> Wettbewerbsdatensatz enthalten sind.  Diese Modelle sind komplex genug, um <i>sehr hochwertige Merkmale</i> aus den Eingabedaten zu <i>extrahieren</i> . </li><li>  √úberlegungen zur Zeit.  Das Training gro√üer Modelle kann Wochen oder sogar Monate dauern.  In diesem Fall kann jeder <i>viel Zeit und Rechenressourcen sparen</i> . </li><li>  Eine gewichtige Annahme, die zugrunde liegt, warum all dies funktionieren kann, lautet wie folgt: Die Attribute, die durch das Training in einer Aufgabe erhalten werden, k√∂nnen n√ºtzlich und f√ºr eine andere Aufgabe geeignet sein.  Mit anderen Worten, Merkmale haben die Eigenschaft der Invarianz in Bezug auf das Problem.  Beachten Sie, dass die <i>Dom√§ne der</i> neuen Aufgabe der Dom√§ne der urspr√ºnglichen Aufgabe √§hnlich sein muss.  Andernfalls kann die Merkmalsextraktionseinheit die Ergebnisse sogar verschlechtern. </li></ul><br><h2>  <font color="#0071c5">Erweiterte Modellarchitektur</font> </h2><br>  Jetzt kennen wir das Konzept des induktiven Lernens.  Wir wissen auch, dass ImageNet ein wichtiges Ereignis ist, bei dem fast alle modernen fortschrittlichen Faltungsarchitekturen f√ºr neuronale Netze getestet wurden.  Versuchen wir, den Feature-Extraktionsblock aus einem dieser Netzwerke zu √ºbernehmen. <br><br>  Gl√ºcklicherweise bietet uns die Keras-Bibliothek <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">mehrere</a> vorab trainierte (√ºber ImageNet) Modelle, die innerhalb dieser Plattform erstellt wurden.  Wir importieren und verwenden eines dieser Modelle. <br><br><img src="https://habrastorage.org/webt/jz/3t/ry/jz3trysk11d88hbmbxvlnoungxy.png"><br><br>  In diesem Fall verwenden wir ein Netzwerk mit VGG-Architektur.  Um nur die Merkmalsextraktionseinheit auszuw√§hlen, l√∂schen wir das Klassifizierungsmodul (die drei obersten vollst√§ndig verbundenen Schichten) des Netzwerks, indem <i>wir den</i> Parameter <i>include_top</i> auf <i>False setzen</i> .  Wir m√∂chten unser Netzwerk auch mit den Gewichten des in ImageNet trainierten Netzwerks initialisieren.  Der letzte Parameter ist die Gr√∂√üe der Eingabe. <br><br>  Bitte beachten Sie, dass die Gr√∂√üe der Originalbilder im ImageNet-Wettbewerb (224, 224, 3) betr√§gt, w√§hrend unsere Bilder (400, 500, 3) gro√ü sind.  Wir verwenden jedoch Faltungsschichten - dies bedeutet, dass die Netzwerkgewichte die Gewichte der sich bewegenden Kernel in der Faltungsoperation sind.  Zusammen mit der Eigenschaft der Parametertrennung (eine Diskussion hierzu finden Sie in unserem theoretischen Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">√úbersicht √ºber Faltungsnetzwerke zur Klassifizierung von Bildern</a> ) f√ºhrt dies dazu, dass die Gr√∂√üe der Eingabedaten nahezu beliebig sein kann, da die Faltung √ºber ein Schiebefenster durchgef√ºhrt wird und dieses Fenster entlang gleiten kann Bild jeder Gr√∂√üe.  Die einzige Einschr√§nkung besteht darin, dass die Gr√∂√üe der Eingabedaten gro√ü genug sein muss, damit sie in einer Zwischenschicht nicht auf einen Punkt (r√§umliche Messungen) kollabieren, da sonst keine weiteren Berechnungen m√∂glich sind. <br><br>  Ein weiterer Trick, den wir verwenden, ist das <i>Caching</i> .  VGG ist ein sehr gro√ües Netzwerk.  Ein direkter Durchgang f√ºr alle Bilder (1630 Beispiele) durch die Merkmalsextraktionseinheit dauert ungef√§hr 50 Sekunden.  Es sollte jedoch beachtet werden, dass die Gewichte der Merkmalsextraktionseinheit fest sind und ein direkter Durchgang immer das gleiche Ergebnis f√ºr das gleiche Bild liefert.  Wir k√∂nnen diese Tatsache verwenden, um einen direkten Durchlauf durch die Merkmalsextraktionseinheit nur <i>einmal</i> durchzuf√ºhren und dann die Ergebnisse in einem Zwischenarray zwischenzuspeichern.  Um dieses Szenario zu implementieren, erstellen wir zun√§chst eine Instanz der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ImageDataGenerator-</a> Klasse, um Dateien direkt von der Festplatte zu laden (weitere Informationen finden Sie im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Basisartikel</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grundmodell zum Erkennen von Emotionen in Bildern</a> ). <br><br><img src="https://habrastorage.org/webt/o-/wo/7h/o-wo7hel-_kgurcvwcs2v2smecc.png"><br><br>  In der n√§chsten Phase verwenden wir im Vorhersagemodus den zuvor erstellten Merkmalsextraktionsblock als Teil des Modells, um Bildmerkmale zu erhalten. <br><br><img src="https://habrastorage.org/webt/gl/no/p7/glnop717aagtytzitgpdtbl_11c.png"><br><br>  Es dauert ungef√§hr 50 Sekunden.  Jetzt k√∂nnen wir die Ergebnisse f√ºr ein sehr schnelles Training des oberen Klassifizierungsteils des Modells verwenden - eine √Ñra dauert f√ºr uns ungef√§hr 1 Sekunde.  Stellen Sie sich jetzt vor, dass jede √Ñra 50 Sekunden l√§nger dauert.  Mit dieser einfachen Caching-Technik konnten wir den Prozess des Netzwerktrainings um das 50-fache beschleunigen!  In diesem Szenario speichern wir alle Zeichen f√ºr alle Beispiele im RAM, da das Volumen daf√ºr ausreicht.  Wenn Sie einen gr√∂√üeren Datensatz verwenden, k√∂nnen Sie die Eigenschaften berechnen, auf die Festplatte schreiben und sie dann mit demselben Ansatz lesen, der der Generatorklasse zugeordnet ist. <br><br>  Betrachten Sie abschlie√üend die Architektur des Klassifizierungsteils des Modells: <br><br><img src="https://habrastorage.org/webt/6x/hq/qb/6xhqqbgjol6dfxyn47rufbuc_ag.png"><br><br><img src="https://habrastorage.org/webt/ss/4v/3t/ss4v3to-rinafik2lthu5ecszt8.png"><br><br>  Denken Sie daran, dass am Ausgang des Merkmalsextraktionsblocks des Faltungsnetzwerks ein vierdimensionaler Tensor (Beispiele, H√∂he, Breite und Kan√§le) ausgegeben wird und eine vollst√§ndig verbundene Schicht zur Klassifizierung einen zweidimensionalen Tensor (Beispiele, Merkmale) verwendet.  Eine M√∂glichkeit, einen vierdimensionalen Tensor mit Merkmalen zu transformieren, besteht darin, ihn einfach um die letzten drei Achsen auszurichten (wir haben im Basismodell eine √§hnliche Technik verwendet).  In diesem Szenario verwenden wir einen anderen Ansatz, der als <i>Global Mean Value Sub-Sampling</i> (GAP) bezeichnet wird.  Anstatt die vierdimensionalen Vektoren auszurichten, nehmen wir den Durchschnittswert basierend auf zwei r√§umlichen Dimensionen.  Tats√§chlich nehmen wir eine Karte mit Attributen und mitteln einfach alle darin enthaltenen Werte.  Die GAP-Methode wurde erstmals in der hervorragenden Arbeit des Min Lin- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Netzwerks im Internet eingef√ºhrt</a> (dieses Buch ist es wirklich wert, es kennenzulernen, da es einige wichtige Konzepte behandelt - zum Beispiel 1 √ó 1-Windungen).  Ein offensichtlicher Vorteil des GAP-Ansatzes ist eine signifikante Reduzierung der Anzahl der Parameter.  Bei Verwendung von GAP erhalten wir f√ºr jedes Beispiel nur 512 Features. Beim Ausrichten der Rohdaten betr√§gt die Anzahl der Features 15 √ó 12 √ó 512 = 92 160. Dies kann zu einem erheblichen Overhead f√ºhren, da in diesem Fall der Klassifizierungsteil des Modells etwa 50 hat Millionen Parameter!  Weitere Elemente des Klassifizierungsteils des Modells, z. B. vollst√§ndig verbundene Ebenen und Ebenen, die die Ausschlussmethode implementieren, werden im Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grundmodell zum Erkennen von Emotionen in Bildern</a> ausf√ºhrlich erl√§utert. <br><br><h2>  <font color="#0071c5">Einstellungen und Trainingsoptionen</font> </h2><br>  Nachdem wir die Architektur unseres Modells mit Keras vorbereitet haben, m√ºssen Sie das gesamte Modell f√ºr das Training mithilfe der Kompilierungsmethode konfigurieren. <br><br><img src="https://habrastorage.org/webt/dl/x5/by/dlx5byabpmih_ocdviw_8ngvatw.png"><br><br>  In diesem Fall verwenden wir Einstellungen, die den Einstellungen des Basismodells fast √§hnlich sind, mit Ausnahme der Auswahl des Optimierers.  Um das Lernen zu optimieren, wird die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">bin√§re Kreuzentropie</a> als Verlustfunktion verwendet und zus√§tzlich eine Genauigkeitsmetrik verfolgt.  Wir verwenden die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Adam-</a> Methode als Optimierer.  Adam ist eine Art stochastischer Gradientenabstiegsalgorithmus mit einem Moment und einer adaptiven <i>Lerngeschwindigkeit</i> (weitere Informationen finden Sie im Blogeintrag von S. Ruder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">√úbersicht √ºber Algorithmen zur Optimierung des Gradientenabstiegs</a> ). <br><br>  Die Lerngeschwindigkeit ist ein Optimierungshyperparameter, der konfiguriert werden muss, um sicherzustellen, dass das Modell betriebsbereit ist.  Denken Sie daran, dass die Formel f√ºr den Gradientenabstieg ‚ÄûVanille‚Äú keine zus√§tzlichen Funktionen enth√§lt: <br><br><img src="https://habrastorage.org/webt/qy/iu/ny/qyiunyjwn2bjmvzkkd_jg5050ay.png"><br><br>  Œò ist der Vektor der Modellparameter (in unserem Fall sind dies die Gewichtungskoeffizienten des neuronalen Netzwerks), - ist die Zielfunktion, ‚àá ist der Gradientenoperator (berechnet unter Verwendung des Algorithmus zur Fehlerr√ºckausbreitung), Œ± ist die Lerngeschwindigkeit.  Somit repr√§sentiert der Gradient der Zielfunktion die Richtung des Optimierungsschritts im Parameterraum, und die Lerngeschwindigkeit ist seine Gr√∂√üe.  Bei Verwendung einer unangemessen hohen Lerngeschwindigkeit besteht die M√∂glichkeit eines konstanten Verrutschens des optimalen Punktes aufgrund der zu gro√üen Schrittgr√∂√üe.  Wenn andererseits die Lerngeschwindigkeit zu niedrig ist, nimmt die Optimierung zu viel Zeit in Anspruch und kann die Konvergenz nur zu lokalen Minima geringer Qualit√§t anstelle eines globalen Extremums sicherstellen.  Daher ist es in jeder spezifischen Situation notwendig, einen geeigneten Kompromiss zu suchen.  Die Verwendung der Standardeinstellungen f√ºr den Adam-Algorithmus ist ein guter Ausgangspunkt f√ºr den Einstieg. <br><br>  Bei dieser Aufgabe zeigen die Standardeinstellungen von Adam jedoch schlechte Ergebnisse.  Wir m√ºssen die anf√§ngliche Lernrate auf 0,0001 reduzieren.  Andernfalls kann die Schulung keine Konvergenz gew√§hrleisten. <br><br>  Letztendlich k√∂nnen wir √ºber 100 Epochen lernen und dann das Modell selbst und die Geschichte des Lernens speichern.  Der Befehl <i>% time</i> ist ein magischer Ipython * -Befehl, mit dem Sie die Ausf√ºhrungszeit von Code messen k√∂nnen. <br><br><img src="https://habrastorage.org/webt/tp/qr/h3/tpqrh3zk0u7oxnxg77cbsmigxc8.png"><br><br><h2>  <font color="#0071c5">Bewertung</font> </h2><br><br><img src="https://habrastorage.org/webt/ij/w6/a-/ijw6a-rdyl9fd5rhlsuybmu24vm.png"><br><br>  Lassen Sie uns die Wirksamkeit des Modells w√§hrend des Trainings bewerten.  In unserem Fall betr√§gt die √úberpr√ºfungsgenauigkeit 73 Prozent (im Vergleich zu 55 Prozent beim Basismodell).  Dieses Ergebnis ist viel besser als das Ergebnis des Basismodells. <br><br>  Betrachten wir auch die Fehlerverteilung anhand der Matrix der Ungenauigkeiten.  Fehler werden fast gleichm√§√üig zwischen Klassen verteilt, wobei eine leichte Tendenz zu falsch klassifizierten negativen Beispielen besteht (obere linke Zelle der Matrix der Ungenauigkeiten).  Dies kann durch ein <i>kleines Ungleichgewicht im Datensatz</i> gegen√ºber der positiven Klasse erkl√§rt werden. <br><br>  Eine weitere Metrik, die wir verfolgen, ist die Empf√§ngerleistungskurve (ROC-Kurve) und die Fl√§che unter dieser Kurve (AUC).  Eine ausf√ºhrliche Beschreibung dieser Metriken finden Sie im Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grundmodell zum Erkennen von Emotionen in Bildern</a> . <br><br><img src="https://habrastorage.org/webt/if/lx/cf/iflxcf-2pxzdcuicg8im4us-9yg.png"><br><br>  Je n√§her die ROC-Kurve am oberen linken Punkt des Diagramms liegt und je gr√∂√üer die Fl√§che darunter ist (AUC-Metrik), desto besser funktioniert der Klassifikator.  Diese Abbildung zeigt deutlich, dass ein verbessertes und vorab trainiertes Modell bessere Ergebnisse zeigt als das von Grund auf neu erstellte Basismodell.  Der AUC-Wert f√ºr das vorab trainierte Modell betr√§gt 0,82, was ein gutes Ergebnis ist. <br><br><img src="https://habrastorage.org/webt/7o/dy/hu/7odyhuvi5j09ajzquknecv_ch2s.png"><br><br><h2>  <font color="#0071c5">Fazit</font> </h2><br>  In diesem Artikel haben wir eine leistungsstarke Technik kennengelernt - induktives Lernen.  Wir haben auch einen Faltungsklassifizierer f√ºr neuronale Netze unter Verwendung einer vorab trainierten Merkmalsextraktionseinheit konstruiert, die auf der VGG-Architektur basiert.  Dieser Klassifikator √ºbertraf in seinen Leistungsmerkmalen das von Grund auf trainierte grundlegende Faltungsmodell.  Die Erh√∂hung der Genauigkeit betrug 18 Prozent, und die Erh√∂hung der AUC-Metrik betrug 0,25, was eine sehr signifikante Verbesserung der Qualit√§t des Systems zeigt. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de421367/">https://habr.com/ru/post/de421367/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de421355/index.html">Go 1.11 gestartet - WebAssembly und native Module</a></li>
<li><a href="../de421357/index.html">Auf die Frage nach dem Unm√∂glichen. Teil 3</a></li>
<li><a href="../de421359/index.html">Das Festival ist wie ein Spiel. Taxonomie von IT-Mitarbeitern</a></li>
<li><a href="../de421361/index.html">AMD hat den Quellcode f√ºr V-EZ ge√∂ffnet, eine plattform√ºbergreifende Vulkan-API auf niedriger Ebene</a></li>
<li><a href="../de421365/index.html">Die Entwicklung eines Startups. Agil von Yaytselov bis Chiken Invaders</a></li>
<li><a href="../de421369/index.html">Was Auszubildende bei ABBYY eigentlich tun</a></li>
<li><a href="../de421371/index.html">Unsichtbare Nadeln: Wissenschaftler haben einen Weg entwickelt, Nanosensoren f√ºr Optik und Biomedizin zu maskieren</a></li>
<li><a href="../de421373/index.html">Python stellt die Programmierung einem breiten Publikum zur Verf√ºgung</a></li>
<li><a href="../de421375/index.html">Wie Unsicherheit den Handel t√∂tet</a></li>
<li><a href="../de421377/index.html">7 Missverst√§ndnisse eines unerfahrenen Projektmanagers in Gamedev</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>