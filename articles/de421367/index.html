<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>❌ 🦒 💪🏿 KI, praktischer Kurs. Einrichten des Modells und der Hyperparameter zum Erkennen von Emotionen in Bildern 🌿 📗 🤛🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In den vorherigen Artikeln dieser Trainingsreihe wurden mögliche Optionen zum Aufbereiten von Daten beschrieben. Vorverarbeitung und Hinzufügen von Da...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>KI, praktischer Kurs. Einrichten des Modells und der Hyperparameter zum Erkennen von Emotionen in Bildern</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/intel/blog/421367/"><img src="https://habrastorage.org/webt/zq/7s/el/zq7selxswjsrmplg_pxw2xilqi4.jpeg"><br><br>  In den vorherigen Artikeln dieser Trainingsreihe wurden mögliche Optionen zum Aufbereiten von Daten beschrieben. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vorverarbeitung und Hinzufügen von Daten mit Bildern</a> ; in diesen Artikeln wurde auch das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Basismodell zum Erkennen von Emotionen</a> basierend auf Bildern eines Faltungsnetzwerks konstruiert. <br>  In diesem Artikel werden wir ein verbessertes Faltungsmodell für neuronale Netze zum Erkennen von Emotionen in Bildern mithilfe einer als <i>induktives Lernen bezeichneten</i> Technik erstellen. <br><a name="habracut"></a><br>  Zunächst müssen Sie sich mit dem Artikel über das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grundmodell zum Erkennen von Emotionen in Bildern</a> vertraut machen. Sie können auch beim Lesen darauf verweisen, da einige Abschnitte, einschließlich des Studierens der Quelldaten und der Beschreibung von Netzwerkindikatoren, hier nicht im Detail aufgeführt werden. <br><br><h2>  <font color="#0071c5">Daten</font> </h2><br>  Der Datensatz enthält 1630 Bilder mit Emotionen aus zwei Klassen: <i>Negativ</i> (Klasse 0) und <i>Positiv</i> (Klasse 1).  Einige Beispiele für solche Bilder sind unten angegeben. <br><br>  <b>Negativ</b> <br><img src="https://habrastorage.org/webt/zr/pf/ki/zrpfkiqvgcxw6kpsv777v9d1t6w.jpeg"><br><br><img src="https://habrastorage.org/webt/52/8x/s6/528xs6k7jnimfgvhagwolru8mi4.jpeg"><br><br><img src="https://habrastorage.org/webt/no/p0/vb/nop0vbapr4ep7o5dps1wvpkncoa.jpeg"><br><br>  <b>Positiv</b> <br><img src="https://habrastorage.org/webt/uc/ua/oh/ucuaohklcwcgotqf4uryopnt_qg.jpeg"><br><br><img src="https://habrastorage.org/webt/u5/hp/rb/u5hprb1cjig28b4xk8urdljb8lm.jpeg"><br><br><img src="https://habrastorage.org/webt/ru/_a/xo/ru_axoahw4h4xytx_-yphxk56ii.jpeg"><br><br>  Einige der Beispiele enthalten offensichtliche positive oder negative Emotionen, während andere möglicherweise nicht kategorisiert werden - selbst bei menschlicher Beteiligung.  Basierend auf einer Sichtprüfung solcher Fälle schätzen wir, dass die maximal mögliche Genauigkeit bei etwa 80 Prozent liegen sollte.  Beachten Sie, dass ein Zufallsklassifizierer aufgrund eines kleinen Ungleichgewichts in den Klassen eine Genauigkeit von ungefähr 53 Prozent bietet. <br><br>  Um das Modell zu trainieren, verwenden wir die Technik <i>, einen Teil der Proben beizubehalten</i> und den Anfangsdatensatz in zwei Teile zu teilen, von denen einer (20 Prozent des Anfangssatzes) von uns zur Überprüfung verwendet wird.  Die Partitionierung erfolgt mithilfe der <i>Schichtung</i> : Dies bedeutet, dass das Gleichgewicht zwischen den Klassen in den Trainings- und Testsätzen erhalten bleibt. <br><br><h2>  <font color="#0071c5">Beheben von Datenmangel</font> </h2><br>  Das Grundmodell zeigte Ergebnisse, die nur geringfügig besser waren als zufällige Vorhersagen der Bildklasse.  Es kann viele mögliche Gründe für dieses Verhalten geben.  Wir glauben, dass der Hauptgrund darin besteht, dass die verfügbare Datenmenge für ein solches Training des Faltungsteils des Netzwerks, das es ermöglichen würde, charakteristische Merkmale basierend auf dem Eingabebild zu erhalten, entschieden unzureichend ist. <br>  Es gibt viele verschiedene Möglichkeiten, um das Problem der Datenschwäche zu lösen.  Hier sind einige davon: <br><br><ul><li>  <b>Wiederholen</b> .  Die Idee der Methode ist es, die Verteilung von Daten zu bewerten und <i>neue Beispiele</i> aus dieser Verteilung auszuwählen. </li><li>  <b>Lernen ohne Lehrer</b> .  Jeder kann große Datenmengen finden, die der Art der markierten Beispiele in einem bestimmten Datensatz entsprechen.  Beispielsweise können es Filme zur Videoerkennung oder Hörbücher zur Spracherkennung sein.  Der nächste Schritt auf diesem Weg besteht darin, diese Daten für das Vortraining des Modells zu verwenden (z. B. mithilfe von Auto-Encodern). </li><li>  <b>Datenerweiterung</b> .  Während dieses Prozesses werden Probendaten unter Verwendung eines gegebenen Satzes von Transformationen zufällig modifiziert. </li><li>  <b>Induktives Lernen</b> .  Dieses Thema ist für uns von großem Interesse. Lassen Sie uns es näher kennenlernen. </li></ul><br><h2>  <font color="#0071c5">Induktives Lernen</font> </h2><br>  Der Begriff <i>induktives Training</i> bezieht sich auf eine Reihe von Techniken unter Verwendung von Modellen (oft sehr groß), die auf verschiedenen Datensätzen ungefähr derselben Art trainiert wurden. <br><br><img src="https://habrastorage.org/webt/wl/jb/qi/wljbqidbfmpj1yfddtvjlkqt9ma.png"><br><br><img src="https://habrastorage.org/webt/bq/ji/ah/bqjiahabshkakrv2jcilp5pa1y8.png"><br><br>  Vergleich traditioneller maschineller und induktiver Lernmethoden.  Bild aus S. Ruders Blogeintrag <i>"Was ist induktives Lernen?"</i>  . <br>  Es gibt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">drei</a> Hauptszenarien für die Verwendung von induktivem Lernen: <br><br><ul><li>  <b>Vorgefertigte Modelle</b> .  Jeder Benutzer kann einfach ein von einer anderen Person geschultes Modell nehmen und es für seine Aufgaben verwenden.  Ein solches Szenario ist möglich, wenn die Aufgaben sehr ähnlich sind. </li><li>  <b>Blockieren Sie die Auswahl der Zeichen</b> .  An diesem Punkt wissen wir, dass die Architektur des Modells in zwei Hauptteile unterteilt werden kann: die <i>Merkmalsextraktionseinheit</i> , die für das Extrahieren von Merkmalen aus den Eingabedaten verantwortlich ist, und das <i>Klassifizierungsmodul</i> , das Beispiele basierend auf den empfangenen Merkmalen klassifiziert.  In der Regel ist der Feature-Extraktionsblock der Hauptteil des Modells.  Die Idee des Verfahrens besteht darin, einen Block zur Unterscheidung von Merkmalen von einem in einem anderen Problem trainierten Modell zu nehmen, seine Gewichtskoeffizienten festzulegen (sie nicht trainiert zu machen) und dann auf seiner Basis neue Klassifizierungsmodule für das betrachtete Problem aufzubauen.  Das Klassifizierungsmodul ist normalerweise nicht sehr tief und besteht aus mehreren vollständig verbundenen Schichten, sodass dieses Modell viel einfacher zu trainieren ist. </li><li>  <b>Präzise und tiefe Abstimmung</b> .  Diese Methode ähnelt einem Szenario mit einem Feature-Extraktionsblock.  Dieselben Aktionen werden ausgeführt, mit Ausnahme des "Einfrierens" des Feature-Extraktionsblocks.  Sie können beispielsweise das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">VGG-</a> Netzwerk als Merkmalsextraktionsblock verwenden und nur die ersten drei (von vier) Faltungsblöcke darin "einfrieren".  In diesem Fall kann sich die Merkmalsextraktionseinheit besser an die aktuelle Aufgabe anpassen.  Weitere Informationen finden Sie im Blog-Beitrag von F. Chollet. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erstellen Sie leistungsstarke Bildklassifizierungsmodelle mit einer sehr kleinen Datenmenge</a> . </li></ul><br>  Eine detaillierte Beschreibung der Szenarien für die Verwendung des induktiven Lernens finden Sie im Kurs <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CS231n</a> der Stanford University <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zur Faltung neuronaler Netze zur visuellen Erkennung</a> durch Fei-Fei Li und in Blogeinträgen von S. Ruder. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Induktives Lernen ist die nächste Grenze in der Entwicklung maschinelles Lernen</a> (Themen umfassender diskutiert). <br><br>  Möglicherweise haben Sie Fragen: Warum werden all diese Methoden benötigt und warum können sie funktionieren?  Wir werden versuchen, sie zu beantworten. <br><br><ul><li>  Vorteile der Verwendung großer Datenmengen.  Zum Beispiel können wir den Feature-Extraktionsblock aus einem Modell übernehmen, das auf 14 Millionen Bildern trainiert wurde, die im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ImageNet-</a> Wettbewerbsdatensatz enthalten sind.  Diese Modelle sind komplex genug, um <i>sehr hochwertige Merkmale</i> aus den Eingabedaten zu <i>extrahieren</i> . </li><li>  Überlegungen zur Zeit.  Das Training großer Modelle kann Wochen oder sogar Monate dauern.  In diesem Fall kann jeder <i>viel Zeit und Rechenressourcen sparen</i> . </li><li>  Eine gewichtige Annahme, die zugrunde liegt, warum all dies funktionieren kann, lautet wie folgt: Die Attribute, die durch das Training in einer Aufgabe erhalten werden, können nützlich und für eine andere Aufgabe geeignet sein.  Mit anderen Worten, Merkmale haben die Eigenschaft der Invarianz in Bezug auf das Problem.  Beachten Sie, dass die <i>Domäne der</i> neuen Aufgabe der Domäne der ursprünglichen Aufgabe ähnlich sein muss.  Andernfalls kann die Merkmalsextraktionseinheit die Ergebnisse sogar verschlechtern. </li></ul><br><h2>  <font color="#0071c5">Erweiterte Modellarchitektur</font> </h2><br>  Jetzt kennen wir das Konzept des induktiven Lernens.  Wir wissen auch, dass ImageNet ein wichtiges Ereignis ist, bei dem fast alle modernen fortschrittlichen Faltungsarchitekturen für neuronale Netze getestet wurden.  Versuchen wir, den Feature-Extraktionsblock aus einem dieser Netzwerke zu übernehmen. <br><br>  Glücklicherweise bietet uns die Keras-Bibliothek <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">mehrere</a> vorab trainierte (über ImageNet) Modelle, die innerhalb dieser Plattform erstellt wurden.  Wir importieren und verwenden eines dieser Modelle. <br><br><img src="https://habrastorage.org/webt/jz/3t/ry/jz3trysk11d88hbmbxvlnoungxy.png"><br><br>  In diesem Fall verwenden wir ein Netzwerk mit VGG-Architektur.  Um nur die Merkmalsextraktionseinheit auszuwählen, löschen wir das Klassifizierungsmodul (die drei obersten vollständig verbundenen Schichten) des Netzwerks, indem <i>wir den</i> Parameter <i>include_top</i> auf <i>False setzen</i> .  Wir möchten unser Netzwerk auch mit den Gewichten des in ImageNet trainierten Netzwerks initialisieren.  Der letzte Parameter ist die Größe der Eingabe. <br><br>  Bitte beachten Sie, dass die Größe der Originalbilder im ImageNet-Wettbewerb (224, 224, 3) beträgt, während unsere Bilder (400, 500, 3) groß sind.  Wir verwenden jedoch Faltungsschichten - dies bedeutet, dass die Netzwerkgewichte die Gewichte der sich bewegenden Kernel in der Faltungsoperation sind.  Zusammen mit der Eigenschaft der Parametertrennung (eine Diskussion hierzu finden Sie in unserem theoretischen Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Übersicht über Faltungsnetzwerke zur Klassifizierung von Bildern</a> ) führt dies dazu, dass die Größe der Eingabedaten nahezu beliebig sein kann, da die Faltung über ein Schiebefenster durchgeführt wird und dieses Fenster entlang gleiten kann Bild jeder Größe.  Die einzige Einschränkung besteht darin, dass die Größe der Eingabedaten groß genug sein muss, damit sie in einer Zwischenschicht nicht auf einen Punkt (räumliche Messungen) kollabieren, da sonst keine weiteren Berechnungen möglich sind. <br><br>  Ein weiterer Trick, den wir verwenden, ist das <i>Caching</i> .  VGG ist ein sehr großes Netzwerk.  Ein direkter Durchgang für alle Bilder (1630 Beispiele) durch die Merkmalsextraktionseinheit dauert ungefähr 50 Sekunden.  Es sollte jedoch beachtet werden, dass die Gewichte der Merkmalsextraktionseinheit fest sind und ein direkter Durchgang immer das gleiche Ergebnis für das gleiche Bild liefert.  Wir können diese Tatsache verwenden, um einen direkten Durchlauf durch die Merkmalsextraktionseinheit nur <i>einmal</i> durchzuführen und dann die Ergebnisse in einem Zwischenarray zwischenzuspeichern.  Um dieses Szenario zu implementieren, erstellen wir zunächst eine Instanz der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ImageDataGenerator-</a> Klasse, um Dateien direkt von der Festplatte zu laden (weitere Informationen finden Sie im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Basisartikel</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grundmodell zum Erkennen von Emotionen in Bildern</a> ). <br><br><img src="https://habrastorage.org/webt/o-/wo/7h/o-wo7hel-_kgurcvwcs2v2smecc.png"><br><br>  In der nächsten Phase verwenden wir im Vorhersagemodus den zuvor erstellten Merkmalsextraktionsblock als Teil des Modells, um Bildmerkmale zu erhalten. <br><br><img src="https://habrastorage.org/webt/gl/no/p7/glnop717aagtytzitgpdtbl_11c.png"><br><br>  Es dauert ungefähr 50 Sekunden.  Jetzt können wir die Ergebnisse für ein sehr schnelles Training des oberen Klassifizierungsteils des Modells verwenden - eine Ära dauert für uns ungefähr 1 Sekunde.  Stellen Sie sich jetzt vor, dass jede Ära 50 Sekunden länger dauert.  Mit dieser einfachen Caching-Technik konnten wir den Prozess des Netzwerktrainings um das 50-fache beschleunigen!  In diesem Szenario speichern wir alle Zeichen für alle Beispiele im RAM, da das Volumen dafür ausreicht.  Wenn Sie einen größeren Datensatz verwenden, können Sie die Eigenschaften berechnen, auf die Festplatte schreiben und sie dann mit demselben Ansatz lesen, der der Generatorklasse zugeordnet ist. <br><br>  Betrachten Sie abschließend die Architektur des Klassifizierungsteils des Modells: <br><br><img src="https://habrastorage.org/webt/6x/hq/qb/6xhqqbgjol6dfxyn47rufbuc_ag.png"><br><br><img src="https://habrastorage.org/webt/ss/4v/3t/ss4v3to-rinafik2lthu5ecszt8.png"><br><br>  Denken Sie daran, dass am Ausgang des Merkmalsextraktionsblocks des Faltungsnetzwerks ein vierdimensionaler Tensor (Beispiele, Höhe, Breite und Kanäle) ausgegeben wird und eine vollständig verbundene Schicht zur Klassifizierung einen zweidimensionalen Tensor (Beispiele, Merkmale) verwendet.  Eine Möglichkeit, einen vierdimensionalen Tensor mit Merkmalen zu transformieren, besteht darin, ihn einfach um die letzten drei Achsen auszurichten (wir haben im Basismodell eine ähnliche Technik verwendet).  In diesem Szenario verwenden wir einen anderen Ansatz, der als <i>Global Mean Value Sub-Sampling</i> (GAP) bezeichnet wird.  Anstatt die vierdimensionalen Vektoren auszurichten, nehmen wir den Durchschnittswert basierend auf zwei räumlichen Dimensionen.  Tatsächlich nehmen wir eine Karte mit Attributen und mitteln einfach alle darin enthaltenen Werte.  Die GAP-Methode wurde erstmals in der hervorragenden Arbeit des Min Lin- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Netzwerks im Internet eingeführt</a> (dieses Buch ist es wirklich wert, es kennenzulernen, da es einige wichtige Konzepte behandelt - zum Beispiel 1 × 1-Windungen).  Ein offensichtlicher Vorteil des GAP-Ansatzes ist eine signifikante Reduzierung der Anzahl der Parameter.  Bei Verwendung von GAP erhalten wir für jedes Beispiel nur 512 Features. Beim Ausrichten der Rohdaten beträgt die Anzahl der Features 15 × 12 × 512 = 92 160. Dies kann zu einem erheblichen Overhead führen, da in diesem Fall der Klassifizierungsteil des Modells etwa 50 hat Millionen Parameter!  Weitere Elemente des Klassifizierungsteils des Modells, z. B. vollständig verbundene Ebenen und Ebenen, die die Ausschlussmethode implementieren, werden im Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grundmodell zum Erkennen von Emotionen in Bildern</a> ausführlich erläutert. <br><br><h2>  <font color="#0071c5">Einstellungen und Trainingsoptionen</font> </h2><br>  Nachdem wir die Architektur unseres Modells mit Keras vorbereitet haben, müssen Sie das gesamte Modell für das Training mithilfe der Kompilierungsmethode konfigurieren. <br><br><img src="https://habrastorage.org/webt/dl/x5/by/dlx5byabpmih_ocdviw_8ngvatw.png"><br><br>  In diesem Fall verwenden wir Einstellungen, die den Einstellungen des Basismodells fast ähnlich sind, mit Ausnahme der Auswahl des Optimierers.  Um das Lernen zu optimieren, wird die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">binäre Kreuzentropie</a> als Verlustfunktion verwendet und zusätzlich eine Genauigkeitsmetrik verfolgt.  Wir verwenden die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Adam-</a> Methode als Optimierer.  Adam ist eine Art stochastischer Gradientenabstiegsalgorithmus mit einem Moment und einer adaptiven <i>Lerngeschwindigkeit</i> (weitere Informationen finden Sie im Blogeintrag von S. Ruder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Übersicht über Algorithmen zur Optimierung des Gradientenabstiegs</a> ). <br><br>  Die Lerngeschwindigkeit ist ein Optimierungshyperparameter, der konfiguriert werden muss, um sicherzustellen, dass das Modell betriebsbereit ist.  Denken Sie daran, dass die Formel für den Gradientenabstieg „Vanille“ keine zusätzlichen Funktionen enthält: <br><br><img src="https://habrastorage.org/webt/qy/iu/ny/qyiunyjwn2bjmvzkkd_jg5050ay.png"><br><br>  Θ ist der Vektor der Modellparameter (in unserem Fall sind dies die Gewichtungskoeffizienten des neuronalen Netzwerks), - ist die Zielfunktion, ∇ ist der Gradientenoperator (berechnet unter Verwendung des Algorithmus zur Fehlerrückausbreitung), α ist die Lerngeschwindigkeit.  Somit repräsentiert der Gradient der Zielfunktion die Richtung des Optimierungsschritts im Parameterraum, und die Lerngeschwindigkeit ist seine Größe.  Bei Verwendung einer unangemessen hohen Lerngeschwindigkeit besteht die Möglichkeit eines konstanten Verrutschens des optimalen Punktes aufgrund der zu großen Schrittgröße.  Wenn andererseits die Lerngeschwindigkeit zu niedrig ist, nimmt die Optimierung zu viel Zeit in Anspruch und kann die Konvergenz nur zu lokalen Minima geringer Qualität anstelle eines globalen Extremums sicherstellen.  Daher ist es in jeder spezifischen Situation notwendig, einen geeigneten Kompromiss zu suchen.  Die Verwendung der Standardeinstellungen für den Adam-Algorithmus ist ein guter Ausgangspunkt für den Einstieg. <br><br>  Bei dieser Aufgabe zeigen die Standardeinstellungen von Adam jedoch schlechte Ergebnisse.  Wir müssen die anfängliche Lernrate auf 0,0001 reduzieren.  Andernfalls kann die Schulung keine Konvergenz gewährleisten. <br><br>  Letztendlich können wir über 100 Epochen lernen und dann das Modell selbst und die Geschichte des Lernens speichern.  Der Befehl <i>% time</i> ist ein magischer Ipython * -Befehl, mit dem Sie die Ausführungszeit von Code messen können. <br><br><img src="https://habrastorage.org/webt/tp/qr/h3/tpqrh3zk0u7oxnxg77cbsmigxc8.png"><br><br><h2>  <font color="#0071c5">Bewertung</font> </h2><br><br><img src="https://habrastorage.org/webt/ij/w6/a-/ijw6a-rdyl9fd5rhlsuybmu24vm.png"><br><br>  Lassen Sie uns die Wirksamkeit des Modells während des Trainings bewerten.  In unserem Fall beträgt die Überprüfungsgenauigkeit 73 Prozent (im Vergleich zu 55 Prozent beim Basismodell).  Dieses Ergebnis ist viel besser als das Ergebnis des Basismodells. <br><br>  Betrachten wir auch die Fehlerverteilung anhand der Matrix der Ungenauigkeiten.  Fehler werden fast gleichmäßig zwischen Klassen verteilt, wobei eine leichte Tendenz zu falsch klassifizierten negativen Beispielen besteht (obere linke Zelle der Matrix der Ungenauigkeiten).  Dies kann durch ein <i>kleines Ungleichgewicht im Datensatz</i> gegenüber der positiven Klasse erklärt werden. <br><br>  Eine weitere Metrik, die wir verfolgen, ist die Empfängerleistungskurve (ROC-Kurve) und die Fläche unter dieser Kurve (AUC).  Eine ausführliche Beschreibung dieser Metriken finden Sie im Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grundmodell zum Erkennen von Emotionen in Bildern</a> . <br><br><img src="https://habrastorage.org/webt/if/lx/cf/iflxcf-2pxzdcuicg8im4us-9yg.png"><br><br>  Je näher die ROC-Kurve am oberen linken Punkt des Diagramms liegt und je größer die Fläche darunter ist (AUC-Metrik), desto besser funktioniert der Klassifikator.  Diese Abbildung zeigt deutlich, dass ein verbessertes und vorab trainiertes Modell bessere Ergebnisse zeigt als das von Grund auf neu erstellte Basismodell.  Der AUC-Wert für das vorab trainierte Modell beträgt 0,82, was ein gutes Ergebnis ist. <br><br><img src="https://habrastorage.org/webt/7o/dy/hu/7odyhuvi5j09ajzquknecv_ch2s.png"><br><br><h2>  <font color="#0071c5">Fazit</font> </h2><br>  In diesem Artikel haben wir eine leistungsstarke Technik kennengelernt - induktives Lernen.  Wir haben auch einen Faltungsklassifizierer für neuronale Netze unter Verwendung einer vorab trainierten Merkmalsextraktionseinheit konstruiert, die auf der VGG-Architektur basiert.  Dieser Klassifikator übertraf in seinen Leistungsmerkmalen das von Grund auf trainierte grundlegende Faltungsmodell.  Die Erhöhung der Genauigkeit betrug 18 Prozent, und die Erhöhung der AUC-Metrik betrug 0,25, was eine sehr signifikante Verbesserung der Qualität des Systems zeigt. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de421367/">https://habr.com/ru/post/de421367/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de421355/index.html">Go 1.11 gestartet - WebAssembly und native Module</a></li>
<li><a href="../de421357/index.html">Auf die Frage nach dem Unmöglichen. Teil 3</a></li>
<li><a href="../de421359/index.html">Das Festival ist wie ein Spiel. Taxonomie von IT-Mitarbeitern</a></li>
<li><a href="../de421361/index.html">AMD hat den Quellcode für V-EZ geöffnet, eine plattformübergreifende Vulkan-API auf niedriger Ebene</a></li>
<li><a href="../de421365/index.html">Die Entwicklung eines Startups. Agil von Yaytselov bis Chiken Invaders</a></li>
<li><a href="../de421369/index.html">Was Auszubildende bei ABBYY eigentlich tun</a></li>
<li><a href="../de421371/index.html">Unsichtbare Nadeln: Wissenschaftler haben einen Weg entwickelt, Nanosensoren für Optik und Biomedizin zu maskieren</a></li>
<li><a href="../de421373/index.html">Python stellt die Programmierung einem breiten Publikum zur Verfügung</a></li>
<li><a href="../de421375/index.html">Wie Unsicherheit den Handel tötet</a></li>
<li><a href="../de421377/index.html">7 Missverständnisse eines unerfahrenen Projektmanagers in Gamedev</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>