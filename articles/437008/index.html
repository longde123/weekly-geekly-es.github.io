<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üí≥ üëºüèº üéà PNL. Lo basico. T√©cnicas Autodesarrollo. Parte 1 ‚ö´Ô∏è üé® üéÄ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Contenido   PNL. Lo basico. T√©cnicas Autodesarrollo. Parte 2: NER 
 
 Hola Mi nombre es Ivan Smurov y lidero el grupo de investigaci√≥n de PNL en ABBYY...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>PNL. Lo basico. T√©cnicas Autodesarrollo. Parte 1</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/abbyy/blog/437008/"><div class="spoiler">  <b class="spoiler_title">Contenido</b> <div class="spoiler_text"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/webt/nf/p0/ws/nfp0wsz4wap5qwx33ulwimmaid8.png" alt="imagen"></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">PNL.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Lo basico.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">T√©cnicas</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Autodesarrollo.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 2: NER</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><br></a> </div></div><br>  Hola  Mi nombre es Ivan Smurov y lidero el grupo de investigaci√≥n de PNL en ABBYY.  Puedes leer sobre lo que nuestro grupo est√° haciendo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> .  Recientemente di una conferencia sobre Procesamiento del Lenguaje Natural (PNL) en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Escuela de Aprendizaje Profundo</a> ; este es un grupo en la Escuela de Matem√°ticas Aplicadas y Ciencias de la Computaci√≥n PhysTech en MIPT para estudiantes de √∫ltimo a√±o interesados ‚Äã‚Äãen la programaci√≥n y las matem√°ticas.  Tal vez las tesis de mi conferencia sean √∫tiles para alguien, as√≠ que las compartir√© con Habr. <br><br>  Como no se puede entender todo al mismo tiempo, dividiremos el art√≠culo en dos partes.  Hoy hablar√© sobre c√≥mo se usan las redes neuronales (o el aprendizaje profundo) en PNL.  En la segunda parte del art√≠culo, nos centraremos en una de las tareas de PNL m√°s comunes: la tarea de extraer entidades con nombre (reconocimiento de entidad con nombre, NER) y analizar en detalle la arquitectura de sus soluciones. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/cn/5o/dx/cn5odxj0tvmrhaf4jstaz7ptars.png"></div><br><a name="habracut"></a><br><h2>  ¬øQu√© es la PNL? <br></h2><br>  Esta es una amplia gama de tareas para procesar textos en un lenguaje natural (es decir, el idioma que las personas hablan y escriben).  Hay un conjunto de tareas cl√°sicas de PNL, cuya soluci√≥n es de uso pr√°ctico. <br><br><ul><li>  La primera y m√°s importante tarea hist√≥rica es la traducci√≥n autom√°tica.  Se ha practicado durante mucho tiempo y hay un progreso tremendo.  Pero la tarea de obtener una traducci√≥n totalmente autom√°tica de alta calidad (FAHQMT) sigue sin resolverse.  En cierto modo, este es el motor de PNL, una de las tareas m√°s grandes que puede hacer. <br><br><img src="https://habrastorage.org/webt/bj/r0/og/bjr0ogf9-tarmd10sntfy-c_mnq.png" alt="imagen"><br></li><li>  La segunda tarea es la clasificaci√≥n de los textos.  Se proporciona un conjunto de textos, y la tarea es clasificar estos textos en categor√≠as.  Cual?  Esta es una pregunta para el cuerpo. <br><br>  La primera y una de las formas m√°s pr√°cticas de aplicarlo desde un punto de vista pr√°ctico es la clasificaci√≥n de las letras en spam y boor (no spam). <br><br>  Otra opci√≥n cl√°sica es la clasificaci√≥n multiclase de noticias en categor√≠as (rubricaci√≥n): pol√≠tica exterior, deportes, carpa, etc. O, digamos, recibe cartas y desea separar los pedidos de la tienda en l√≠nea de los boletos a√©reos y las reservas de hotel. <br><br>  La tercera aplicaci√≥n cl√°sica del problema de clasificaci√≥n de texto es el an√°lisis sentimental.  Por ejemplo, la clasificaci√≥n de las revisiones como positivas, negativas y neutrales. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/my/c1/co/myc1cop8u7adcvhhh0qgw-dmtse.png"></div><br>  Dado que hay tantas categor√≠as posibles en las que puede dividir textos, la clasificaci√≥n de texto es una de las tareas pr√°cticas m√°s populares de la PNL. </li><li>  La tercera tarea es recuperar entidades nombradas, NER.  Seleccionamos en las secciones de texto que corresponden a un conjunto preseleccionado de entidades, por ejemplo, necesitamos encontrar todas las ubicaciones, personas y organizaciones en el texto.  En el texto "Ostap Bender - Director de la oficina de" Horns and Hooves "debe comprender que Ostap Bender es una persona y" Horns and Hooves "es una organizaci√≥n.  Por qu√© esta tarea es necesaria en la pr√°ctica y c√≥mo resolverla, hablaremos en la segunda parte de nuestro art√≠culo. </li><li><img src="https://habrastorage.org/webt/op/d2/lv/opd2lvphnvm4j1e7o6s6-u34pie.png" align="right">  La cuarta tarea est√° relacionada con la tercera: la tarea de extraer hechos y relaciones (extracci√≥n de relaciones).  Por ejemplo, hay una actitud de trabajo (Ocupaci√≥n).  Del texto "Ostap Bender - Director de la oficina" Horns and Hooves "est√° claro que nuestro h√©roe est√° conectado con las relaciones profesionales con" Horns and Hooves ".  Lo mismo puede decirse de muchas otras maneras: "La oficina de Ostap Bender est√° encabezada por la oficina de" Horns and Hooves ", u" Ostap Bender ha pasado del simple hijo del teniente Schmidt al jefe de la oficina de "Horns and Hooves" ".  Estas oraciones difieren no solo en el predicado, sino tambi√©n en la estructura. <br><br>  Ejemplos de otras relaciones que a menudo se destacan son Compra y Venta, Propiedad, el hecho de nacer con atributos como fecha, lugar, etc. (Nacimiento) y algunos otros. <br><br>  La tarea parece no tener una aplicaci√≥n pr√°ctica obvia, pero, sin embargo, se utiliza en la estructuraci√≥n de informaci√≥n no estructurada.  Adem√°s, esto es importante en los sistemas de preguntas y respuestas y de di√°logo, en los motores de b√∫squeda, siempre que necesite analizar una pregunta y comprender con qu√© tipo se relaciona, as√≠ como las restricciones que hay en la respuesta. <br><br><img src="https://habrastorage.org/webt/eh/y7/tl/ehy7tl9hbhhxlsy57j21q8xpa4w.png" alt="imagen"><br></li><li>  Las siguientes dos tareas son probablemente las m√°s exageradas.  Estos son sistemas de preguntas y respuestas (chat bots).  Amazon Alexa, Alice son ejemplos cl√°sicos de sistemas de conversaci√≥n.  Para que funcionen correctamente, se deben resolver muchas tareas de PNL.  Por ejemplo, la clasificaci√≥n de texto ayuda a determinar si caemos en uno de los escenarios de chatbot orientados a objetivos.  Supongamos, "la cuesti√≥n de los tipos de cambio".  La extracci√≥n de la relaci√≥n es necesaria para identificar marcadores de posici√≥n para la plantilla de secuencia de comandos, y la tarea de llevar a cabo un di√°logo sobre temas comunes ("conversadores") nos ayudar√° en una situaci√≥n en la que no hemos ca√≠do en ninguno de los escenarios. <br><br>  Los sistemas de preguntas y respuestas tambi√©n son algo comprensible y √∫til.  Usted hace una pregunta a un autom√≥vil, el autom√≥vil est√° buscando una respuesta en una base de datos o cuerpo de texto.  Ejemplos de tales sistemas son IBM Watson o Wolfram Alpha. </li><li>  Otro ejemplo del cl√°sico problema de PNL es la sammarizaci√≥n.  El enunciado del problema es simple: el sistema de entrada acepta un texto grande y el resultado es un texto m√°s peque√±o, que de alguna manera refleja el contenido de uno grande.  Por ejemplo, se requiere una m√°quina para generar un recuento de un texto, su nombre o anotaci√≥n. <br><br><img src="https://habrastorage.org/webt/m-/mj/uq/m-mjuqawoktinjvjcrkcmlnt5xi.png" alt="imagen"><br></li><li>  Otra tarea popular es la miner√≠a de argumentos, la b√∫squeda de justificaci√≥n en el texto.  Se le da un hecho y un texto, necesita encontrar una justificaci√≥n para este hecho en el texto. </li></ul><br>  De ninguna manera es la lista completa de tareas de PNL.  Hay docenas de ellos.  En general, todo lo que se puede hacer con texto en un lenguaje natural se puede atribuir a las tareas de PNL, solo los temas enumerados son de o√≠do y tienen las aplicaciones pr√°cticas m√°s obvias. <br><br><h2>  ¬øPor qu√© es dif√≠cil resolver tareas de PNL? <br></h2><br>  La redacci√≥n de las tareas no es muy complicada, pero las tareas en s√≠ no son simples, porque trabajamos con el lenguaje natural.  Los fen√≥menos de la polisemia (las palabras polis√©micas tienen un significado inicial com√∫n) y la homonimia (las palabras con diferentes significados se pronuncian y escriben igual) son caracter√≠sticas de cualquier lenguaje natural.  Y si un hablante nativo de ruso entiende bien que la <i>recepci√≥n c√°lida tiene</i> poco en com√∫n con la <i>t√©cnica de lucha</i> , por un lado, y la <i>cerveza tibia</i> , por otro, el sistema autom√°tico tiene que aprender esto durante mucho tiempo.  Por qu√© es mejor traducir " <i>Presione la barra espaciadora para continuar</i> " al aburrido " <i>Para continuar, presione la barra espaciadora</i> " que "La <i>barra espaciadora continuar√° funcionando</i> ". <br><br><ul><li>  Polisemia: parada (proceso o construcci√≥n), mesa (organizaci√≥n u objeto), p√°jaro carpintero (p√°jaro o persona). </li><li>  Homonimia: llave, arco, cerradura, estufa. <br><br><img src="https://habrastorage.org/webt/5i/-h/c1/5i-hc1p3hrtjf1duq5zxwajegyy.png" alt="imagen"><br></li><li>  Otro ejemplo cl√°sico de complejidad del lenguaje es el pronombre an√°fora.  Por ejemplo, perm√≠tanos recibir el texto " <i>Conserje dos horas de nieve, estaba insatisfecho</i> ".  El pronombre "√©l" puede referirse tanto al conserje como a la nieve.  Por contexto, entendemos f√°cilmente que es un conserje, no nieve.  Pero lograr que la computadora tambi√©n haya entendido esto f√°cilmente no es f√°cil.  El problema del pronombre an√°fora a√∫n no est√° muy bien resuelto; los intentos activos por mejorar la calidad de las decisiones contin√∫an. </li><li>  Otra complejidad a√±adida es la elipsis.  Por ejemplo, " <i>Petia comi√≥ una manzana verde y Masha comi√≥ una roja</i> ".  Entendemos que Masha comi√≥ una manzana roja.  Sin embargo, hacer que la m√°quina entienda esto tampoco es f√°cil.  Ahora la tarea de restaurar los puntos suspensivos se est√° resolviendo en casos peque√±os (varios cientos de oraciones), y en ellos la calidad de la restauraci√≥n completa es francamente d√©bil (del orden de 0,5).  Est√° claro que para aplicaciones pr√°cticas, tal calidad no es buena. </li></ul><br>  Por cierto, este a√±o en la conferencia de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Di√°logo</a> , las pistas se llevar√°n a cabo tanto en la an√°fora como en la separaci√≥n (un tipo de elipse) para el idioma ruso.  Para ambas tareas, los casos se ensamblaron con un volumen varias veces mayor que los vol√∫menes de los edificios existentes en la actualidad (adem√°s, para la separaci√≥n, el volumen del caso es un orden de magnitud mayor que los vol√∫menes de los casos, no solo para el ruso, sino para todos los idiomas en general).  Si desea participar en concursos en estos edificios, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">haga clic aqu√≠ (con registro, pero sin SMS)</a> . <br><br><h2>  C√≥mo se resuelven las tareas de PNL <br></h2><br>  A diferencia del procesamiento de im√°genes, a√∫n puede encontrar art√≠culos sobre PNL que describen soluciones que utilizan algoritmos cl√°sicos como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SVM</a> o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Xgboost</a> , no redes neuronales, y que muestran resultados que no son demasiado inferiores a las soluciones de vanguardia. <br><br>  Sin embargo, hace varios a√±os, las redes neuronales comenzaron a derrotar a los modelos cl√°sicos.  Es importante tener en cuenta que, para la mayor√≠a de las tareas, las soluciones basadas en m√©todos cl√°sicos eran √∫nicas, por regla general, no similares a la resoluci√≥n de otros problemas tanto en la arquitectura como en la forma en que se produce la recopilaci√≥n y el procesamiento de los atributos. <br><br>  Sin embargo, las arquitecturas de redes neuronales son mucho m√°s generales.  La arquitectura de la red en s√≠ misma, muy probablemente, tambi√©n es diferente, pero mucho m√°s peque√±a, hay una tendencia hacia la universalizaci√≥n total.  Sin embargo, con qu√© caracter√≠sticas y c√≥mo trabajamos exactamente, ya es casi lo mismo para la mayor√≠a de las tareas de PNL.  Solo las √∫ltimas capas de redes neuronales difieren.  Por lo tanto, podemos suponer que se ha formado una sola tuber√≠a de PNL.  Sobre c√≥mo est√° organizado, ahora le diremos m√°s. <br><br><h2>  Pipeline nlp </h2><br>  Esta forma de trabajar con signos, que es m√°s o menos igual para todas las tareas. <br><br>  Cuando se trata de lenguaje, la unidad b√°sica con la que trabajamos es la palabra.  O m√°s formalmente una "ficha".  Usamos este t√©rmino porque no est√° muy claro qu√© es 2128506, ¬øes una palabra o no?  La respuesta no es obvia.  El token generalmente est√° separado de otros tokens por espacios o signos de puntuaci√≥n.  Y como puede comprender por las dificultades que describimos anteriormente, el contexto de cada token es muy importante.  Existen diferentes enfoques, pero en el 95% de los casos, el contexto que se considera durante el trabajo del modelo es una propuesta que incluye el token inicial. <br><br>  Muchas tareas generalmente se resuelven a nivel de propuesta.  Por ejemplo, traducci√≥n autom√°tica.  La mayor√≠a de las veces, simplemente traducimos una oraci√≥n y no utilizamos un contexto m√°s amplio.  Hay tareas en las que este no es el caso, por ejemplo, los sistemas de di√°logo.  Es importante recordar sobre qu√© se le pregunt√≥ antes al sistema para que pueda responder preguntas.  Sin embargo, la oferta tambi√©n es la unidad principal con la que trabajamos. <br><br>  Por lo tanto, los primeros dos pasos de la tuber√≠a que se realizan para resolver casi cualquier tarea son la segmentaci√≥n (divisi√≥n de texto en oraciones) y la tokenizaci√≥n (divisi√≥n de oraciones en tokens, es decir, palabras individuales).  Esto se hace con algoritmos simples. <br><br>  A continuaci√≥n, debe calcular las caracter√≠sticas de cada token.  Como regla, esto sucede en dos etapas.  El primero es calcular atributos de token independientes del contexto.  Este es un conjunto de signos que de ninguna manera dependen de otras palabras que rodean nuestro token.  Los atributos comunes independientes del contexto son: <br><br><ul><li>  incrustaciones </li><li>  signos simb√≥licos </li><li>  caracter√≠sticas adicionales espec√≠ficas de una tarea o idioma en particular </li></ul><br>  Hablaremos sobre incrustaciones y signos simb√≥licos con m√°s detalle a continuaci√≥n (sobre signos simb√≥licos, no hoy, sino en la segunda parte de nuestro art√≠culo), pero por ahora vamos a dar posibles ejemplos de signos adicionales. <br><br>  Una de las caracter√≠sticas m√°s utilizadas es la parte del discurso o la etiqueta POS (parte del discurso).  Dichas caracter√≠sticas pueden ser importantes para resolver muchos problemas, por ejemplo, analizar tareas.  Para los idiomas con morfolog√≠a compleja, como el idioma ruso, los caracteres morfol√≥gicos tambi√©n son importantes: por ejemplo, en cuyo caso es el sustantivo, qu√© tipo de adjetivo.  De esto podemos sacar diferentes conclusiones sobre la estructura de la propuesta.  Adem√°s, la morfolog√≠a es necesaria para la lematizaci√≥n (reducci√≥n de palabras a formas iniciales), con la ayuda de la cual podemos reducir la dimensi√≥n del espacio de caracter√≠sticas y, por lo tanto, el an√°lisis morfol√≥gico se usa activamente para la mayor√≠a de los problemas de PNL. <br><br>  Cuando resolvemos un problema en el que la interacci√≥n entre diferentes objetos es importante (por ejemplo, en la tarea de extracci√≥n de relaciones o al crear un sistema de preguntas y respuestas), necesitamos saber mucho sobre la estructura de la propuesta.  Esto requiere an√°lisis.  En la escuela, todos analizaron una oraci√≥n para un tema, predicado, suma, etc. El an√°lisis sint√°ctico es algo en este esp√≠ritu, pero m√°s complicado. <br><br>  Otro ejemplo de una caracter√≠stica adicional es la posici√≥n del token en el texto.  Podemos saber a priori que alguna entidad se encuentra con mayor frecuencia al comienzo del texto o viceversa al final. <br><br>  Todos juntos (incrustaciones, signos simb√≥licos y adicionales) forman un vector de signos simb√≥licos que no depende del contexto. <br><br><h2>  Caracter√≠sticas sensibles al contexto </h2><br>  Los signos de token sensibles al contexto son un conjunto de signos que contienen informaci√≥n no solo sobre el token en s√≠, sino tambi√©n sobre sus vecinos.  Hay diferentes formas de calcular estos s√≠ntomas.  En los algoritmos cl√°sicos, la gente a menudo simplemente caminaba por la "ventana": tomaron varias (por ejemplo, tres) fichas al original y varias fichas despu√©s, y luego calcularon todos los signos en dicha ventana.  Este enfoque no es confiable, ya que la informaci√≥n importante para el an√°lisis puede estar a una distancia mayor que la ventana, respectivamente, podemos perder algo. <br><br>  Por lo tanto, ahora todas las caracter√≠sticas sensibles al contexto se calculan a nivel de propuesta de una manera est√°ndar: utilizando redes neuronales recurrentes bidireccionales LSTM o GRU.  Para obtener los atributos de token sensibles al contexto de los atributos independientes del contexto de todos los tokens de oferta se env√≠an al RNN bidireccional (de una o varias capas).  La salida del RNN bidireccional en el i-√©simo momento en el tiempo es un signo sensible al contexto del i-token, que contiene informaci√≥n sobre ambos tokens anteriores (ya que esta informaci√≥n est√° contenida en el i-√©simo valor del RNN directo) y sobre los siguientes (t .k. esta informaci√≥n est√° contenida en el valor correspondiente del RNN inverso). <br><br>  Adem√°s, para cada tarea individual, hacemos algo diferente, pero las primeras capas, hasta RNN bidireccional, se pueden usar para casi cualquier tarea. <br><br>  Este m√©todo de obtenci√≥n de caracter√≠sticas se denomina canalizaci√≥n de PNL. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mn/sz/0h/mnsz0hwhtiects6ksgru_y9hw2o.png"></div><br><br>  Vale la pena se√±alar que en los √∫ltimos 2 a√±os, los investigadores han estado tratando activamente de mejorar la tuber√≠a de PNL, tanto en t√©rminos de velocidad (por ejemplo, transformador, una arquitectura basada en la auto atenci√≥n que no contiene RNN y, por lo tanto, puede aprender y aplicar m√°s r√°pido), y con punto de vista de los signos utilizados (ahora est√°n utilizando activamente signos basados ‚Äã‚Äãen modelos de lenguaje previamente entrenados, por ejemplo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ELMo</a> , o est√°n utilizando las primeras capas del modelo de lenguaje previamente entrenado y los vuelven a capacitar en el caso disponible para la tarea: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ULMFit</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">BERT</a> ). <br><br><h2>  Incrustaciones de forma de palabra <br></h2><br>  Echemos un vistazo m√°s de cerca a lo que es incrustar.  En t√©rminos generales, incrustar es una representaci√≥n concisa del contexto de una palabra.  ¬øPor qu√© es importante conocer el contexto de una palabra?  Porque creemos en una hip√≥tesis de distribuci√≥n: que las palabras que tienen un significado similar se usan en contextos similares. <br><br>  Tratemos ahora de dar una definici√≥n rigurosa de incrustaci√≥n.  La incrustaci√≥n es un mapeo de un vector discreto de caracter√≠sticas categ√≥ricas en un vector continuo con una dimensi√≥n predeterminada. <br><br>  Un ejemplo can√≥nico de incrustaci√≥n es la incrustaci√≥n de palabras (incrustaci√≥n de forma de palabra). <br><br>  ¬øQu√© suele actuar como un vector de caracter√≠sticas discretas?  Un vector booleano que corresponde a todos los valores posibles de una determinada categor√≠a (por ejemplo, todas las partes posibles del discurso o todas las palabras posibles de alg√∫n diccionario limitado). <br><br>  Para incrustaciones de forma de palabra, esta categor√≠a suele ser el √≠ndice de la palabra en el diccionario.  Digamos que hay un diccionario con una dimensi√≥n de 100 mil.  En consecuencia, cada palabra tiene un vector discreto de signos: un vector booleano de dimensi√≥n 100 mil, donde en un lugar (el √≠ndice de la palabra en nuestro diccionario) es uno, y el resto son ceros. <br><br>  ¬øPor qu√© queremos asignar nuestros vectores de caracter√≠sticas discretas a dimensiones continuas dadas?  Debido a que los vectores con una dimensi√≥n de 100 mil no son muy convenientes de usar para los c√°lculos, los vectores de enteros de dimensiones 100, 200 o, por ejemplo, 300, son mucho m√°s convenientes. <br><br>  En principio, no podemos tratar de imponer restricciones adicionales a tal mapeo.  Pero como estamos construyendo un mapeo de este tipo, intentemos asegurarnos de que los vectores de palabras con significado similar tambi√©n est√©n cerca en alg√∫n sentido.  Esto se hace usando una simple red neuronal de retroalimentaci√≥n. <br><br><h2>  Entrenamiento de incrustaci√≥n <br></h2><br>  ¬øC√≥mo se entrenan las incrustaciones?  Estamos tratando de resolver el problema de restaurar una palabra por contexto (o viceversa, restaurar un contexto por palabra).  En el caso m√°s simple, obtenemos el √≠ndice en el diccionario de la palabra anterior (el vector booleano de la dimensi√≥n del diccionario) como entrada y tratamos de determinar el √≠ndice en el diccionario de nuestra palabra.  Esto se hace usando una cuadr√≠cula con una arquitectura extremadamente simple: dos capas completamente conectadas.  Primero viene una capa completamente conectada del vector booleano de la dimensi√≥n del diccionario a la capa oculta de la dimensi√≥n de incrustaci√≥n (es decir, simplemente multiplicando el vector booleano por la matriz de la dimensi√≥n deseada).  Y luego viceversa, una capa totalmente conectada con softmax de una capa oculta de dimensi√≥n incrustada en un vector de dimensi√≥n de diccionario.  Gracias a la funci√≥n de activaci√≥n softmax, obtenemos la distribuci√≥n de probabilidad de nuestra palabra y podemos elegir la opci√≥n m√°s probable. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/vn/fu/ld/vnfuldlmtgik5rihivtpcivmnla.png"></div><br>  <i>La incrustaci√≥n de la i-√©sima palabra es simplemente la i-√©sima fila en la matriz de transici√≥n W.</i> <br><br>  En los modelos utilizados en la pr√°ctica, la arquitectura es m√°s compleja, pero no por mucho.  La principal diferencia es que no utilizamos un vector del contexto para definir nuestra palabra, sino varios (por ejemplo, todo en una ventana de tama√±o 3).  Una opci√≥n un poco m√°s popular es cuando tratamos de predecir no una palabra por contexto, sino un contexto por palabra.  Este enfoque se llama Skip-gram. <br><br>  Pongamos un ejemplo de la aplicaci√≥n de una tarea que se resuelve durante el entrenamiento de incrustaciones (en la variante CBOW, predicciones de palabras por contexto).  Por ejemplo, supongamos que un contexto de token consta de 2 palabras anteriores.                ‚Äú ‚Äù, ,  ,       ‚Äú‚Äù. <br><br>   ,          (    ),       ,      . <br><br>      ,    ,      ,        (,        ,        ).   ‚Äî      . <br><br>  ,  ,         .      ,      ,    ,   . <br><br>         ,   ‚Äî ELMo, ULMFit, BERT.       ,         (  , , ,    ). <br><br><h2>   ? <br></h2><br>    ,     2  . <br><br><ul><li> -,     ,           ,   -   100 .     ‚Äì   :    ,    ,     . </li><li> -,      .      -.        .        .    ,        ,    .  ,    ,    .          .         ,       ,    . </li></ul><br><img src="https://habrastorage.org/webt/dh/w6/w2/dhw6w2s41xbc8y08jgszsupa-z8.png" alt="imagen"><br><br>    ,      .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> ,    ,  ,        ,    ,       .       ,     ,    ,     ,     . <br><br>          NER.    ,    ,            .     ,        ,       ,    ,    . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/437008/">https://habr.com/ru/post/437008/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../436998/index.html">Protecci√≥n de microchips contra ingenier√≠a inversa y entrada no autorizada</a></li>
<li><a href="../437000/index.html">C√≥mo ense√±ar a las personas a usar git</a></li>
<li><a href="../437002/index.html">ASP.NET Core v√°lido</a></li>
<li><a href="../437004/index.html">¬øLos programadores de YML sue√±an con pruebas ansibles?</a></li>
<li><a href="../437006/index.html">Rese√±a de la impresora 3D Wanhao Duplicator 10</a></li>
<li><a href="../437010/index.html">Ecos del pasado: la experiencia de Young en la base del nuevo m√©todo de espectroscop√≠a de rayos X</a></li>
<li><a href="../437014/index.html">La tarea de N cuerpos o c√≥mo volar una galaxia sin salir de la cocina</a></li>
<li><a href="../437018/index.html">Algunas trampas de tipeo est√°tico en Python</a></li>
<li><a href="../437020/index.html">¬øQu√© tiene de malo el aprendizaje por refuerzo?</a></li>
<li><a href="../437022/index.html">Noise Security Bit 0x22 (Ataques de inyecci√≥n de falla, 35C3 y Wallet.fail)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>