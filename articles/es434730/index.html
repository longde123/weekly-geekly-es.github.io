<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèæ‚Äç‚öñÔ∏è üî¨ ü§±üèº Bases de datos en memoria: aplicaci√≥n, escalado y adiciones importantes üêº ü§úüèΩ üëçüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Continuamos experimentando con los formatos de los mitaps. Recientemente, en un ring de boxeo, chocamos con un bus de datos centralizado y Service Mes...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bases de datos en memoria: aplicaci√≥n, escalado y adiciones importantes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/psb/blog/434730/">  Continuamos experimentando con los formatos de los mitaps.  Recientemente, en un ring de boxeo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">chocamos con un</a> bus de datos centralizado y Service Mesh.  Esta vez decidimos probar algo m√°s pac√≠fico: StandUp, es decir, un micr√≥fono abierto.  El tema fue elegido en la base de datos en memoria. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ffd/790/486/ffd790486ff5792ffd1bbebb3cc69be0.png"><br><br>  ¬øEn qu√© casos debo cambiar a memoria?  ¬øC√≥mo y por qu√© escalar?  ¬øY a qu√© vale la pena prestarle atenci√≥n?  Las respuestas est√°n en los discursos de los oradores, que cubriremos en esta publicaci√≥n. <br><a name="habracut"></a><br>  Pero primero, imagine los altavoces: <br><br><ul><li>  Andrey Trushkin, Jefe del Centro de Innovaci√≥n y Tecnolog√≠as Avanzadas de Promsvyazbank <br></li><li>  Vladislav Shpileva, desarrollador de Tarantool <br></li><li>  Artyom Shitov, arquitecto de soluciones de GridGain <br></li></ul><br><h2>  Cambiar a memoria </h2><br>  Las tendencias actuales en el mercado financiero imponen requisitos mucho m√°s estrictos sobre el tiempo de respuesta y la operaci√≥n de la automatizaci√≥n de procesos en general.  Adem√°s, casi todas las instituciones financieras m√°s grandes de hoy buscan construir sus propios ecosistemas. <br><br>  En este sentido, vemos por nosotros mismos dos aplicaciones principales de soluciones en memoria.  El primero es el almacenamiento en cach√© de datos de integraci√≥n.  Seg√∫n el escenario cl√°sico, en las grandes empresas existen varios sistemas automatizados que proporcionan datos a solicitud del usuario.  O un sistema externo, pero en este caso, el iniciador en la mayor√≠a de los casos es el usuario.  Tradicionalmente, estos sistemas almacenaban datos estructurados de cierta manera en la base de datos, accediendo a ellos a pedido. <br><br>  Hoy, tales sistemas ya no cumplen con los requisitos en t√©rminos de carga.  Aqu√≠ no debemos olvidar las llamadas remotas de estos sistemas por parte de los sistemas de consumo.  Esto implica la necesidad de revisar los enfoques para el almacenamiento y la presentaci√≥n de datos a usuarios, sistemas automatizados o servicios individuales.  Salida l√≥gica: almacenamiento de datos relevantes utilizados por los servicios en el nivel de capa en memoria;  Hay muchos casos exitosos similares en el mercado. <br><br>  Este fue el primer caso.  El segundo es efectivo, desde un punto de vista t√©cnico, la gesti√≥n de procesos de negocio.  Los sistemas BPM tradicionales automatizan la ejecuci√≥n de ciertas operaciones de acuerdo con un algoritmo predefinido.  Y en muchos casos surgen preguntas: ¬øpor qu√© estos sistemas no son lo suficientemente eficientes y r√°pidos? <br><br>  T√≠picamente, tales sistemas escriben cada paso (o un peque√±o conjunto de pasos, dise√±ado como una transacci√≥n comercial) en la base de datos.  Por lo tanto, est√°n vinculados al tiempo de respuesta y la interacci√≥n con estos sistemas.  Ahora, el n√∫mero de instancias de procesos de negocio que se ejecutan simult√°neamente en tiempo real es de √≥rdenes de magnitud hace m√°s de 10 a√±os.  Por lo tanto, los sistemas modernos de gesti√≥n de procesos comerciales deber√≠an tener un rendimiento significativamente mayor y garantizar la ejecuci√≥n de aplicaciones descentralizadas.  Adem√°s, hoy todas las empresas est√°n avanzando hacia la formaci√≥n de un gran entorno de microservicios.  El desaf√≠o es que diferentes instancias de procesos de negocios pueden compartir y usar eficientemente datos operativos.  Dentro del marco de la orquestaci√≥n, tiene sentido almacenarlos en una soluci√≥n en memoria. <br><br><h2>  Problema de reconciliaci√≥n </h2><br>  Supongamos que tenemos una gran cantidad de nodos y servicios, que se llevan a cabo una serie de procesos comerciales, cuyas acciones se implementan en forma de microservicios.  Para mejorar el rendimiento, cada uno de ellos comienza a escribir su estado en una instancia de memoria local.  Obtenemos una gran cantidad de instancias locales.  ¬øC√≥mo garantizar la relevancia y la coherencia para todos? <br><br>  Utilizamos zonas de zonas en memoria.  Por ejemplo, dependiendo del dominio comercial.  Cuando cortamos un dominio comercial, determinamos que ciertos microservicios / procesos comerciales funcionan solo dentro del marco de la zona que es responsable del dominio correspondiente.  De esta forma podemos acelerar la actualizaci√≥n de la memoria cach√© y la soluci√≥n completa en memoria. <br><br>  Al mismo tiempo, el cach√© responsable del dominio opera en modo de replicaci√≥n completa: el n√∫mero limitado de nodos debido a la distribuci√≥n entre dominios garantiza la velocidad y la correcci√≥n de la soluci√≥n en este modo.  La zonificaci√≥n y la fragmentaci√≥n m√°xima ayudan a resolver los problemas de sincronizaci√≥n, operaci√≥n del cl√∫ster, etc.  en un gran n√∫mero total de nodos. <br><br>  Naturalmente, a menudo surgen preguntas sobre la fiabilidad de las soluciones en memoria.  S√≠, no todo se puede poner all√≠.  Para garantizar la fiabilidad, siempre tenemos bases de datos al lado de la memoria.  Por ejemplo, para problemas importantes con los informes que deben reunirse, lo que puede ser dif√≠cil en una gran cantidad de nodos.  Entonces, ¬øcu√°l es nuestra visi√≥n hoy: la <i>sinergia de los dos enfoques</i> . <br><br>  Tambi√©n vale la pena se√±alar que estos dos enfoques tampoco son del todo correctos solo para contrastar.  Y al mismo tiempo, conc√©ntrate en ellos.  Los fabricantes y colaboradores de sistemas avanzados de virtualizaci√≥n en contenedores, como Kubernetes, ya nos brindan opciones para un almacenamiento confiable a largo plazo.  Ya han aparecido buenos casos industriales para implementar soluciones, en los que el almacenamiento se lleva a cabo en un formato virtualizado. <br><br>  Uno de los peri√≥dicos m√°s grandes de EE. UU. Ofrece a sus lectores la oportunidad de recibir cualquier n√∫mero en l√≠nea que se haya publicado desde el comienzo de la publicaci√≥n de este peri√≥dico en el siglo XIX.  Podemos imaginar el nivel de carga.  El almacenamiento lo implementan a trav√©s de la plataforma Apache Kafka, implementada en Kubernetes.  Aqu√≠ hay otra opci√≥n para almacenar informaci√≥n y proporcionar acceso a una gran cantidad de clientes bajo una gran carga.  Al dise√±ar nuevas soluciones, tambi√©n vale la pena prestar atenci√≥n a esta opci√≥n. <br><br><h2>  Escalado de bases de datos en memoria con Tarantool </h2><br>  Supongamos que tenemos un servidor.  Acepta solicitudes, almacena datos.  De repente hay m√°s solicitudes y datos, el servidor deja de hacer frente a la carga.  Puede cargar m√°s hardware en el servidor y aceptar√° m√°s solicitudes.  Pero este es un callej√≥n sin salida por tres razones a la vez: alto costo, capacidades t√©cnicas limitadas y problemas con la tolerancia a fallas.  En cambio, hay una escala horizontal: los "amigos" acuden al servidor para ayudarlo a completar las tareas.  Los dos tipos principales de escala horizontal son la replicaci√≥n y el fragmentaci√≥n. <br><br>  La replicaci√≥n se produce cuando hay muchos servidores, todos almacenan los mismos datos y las solicitudes de los clientes se encuentran dispersas en todos estos servidores.  As√≠ es como escala la inform√°tica, no los datos.  Esto funciona cuando los datos se colocan en un nodo, pero hay tantas solicitudes de clientes que un servidor no puede manejarlos.  Adem√°s, aqu√≠ se mejora mucho la tolerancia a fallos. <br><br>  El fragmentaci√≥n se utiliza para escalar datos: se crean muchos servidores y almacenan datos diferentes.  Entonces escala tanto los c√°lculos como los datos.  Pero la tolerancia a fallas en este caso es baja.  Si falla un servidor, se perder√° parte de los datos. <br><br>  Hay un tercer enfoque: combinarlos.  Dividimos el cl√∫ster en subgrupos, los llamamos conjuntos de r√©plica.  Cada uno de ellos almacena los mismos datos, y los datos no se cruzan entre conjuntos de r√©plicas.  El resultado es escalar datos, computaci√≥n y tolerancia a fallas. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/afe/63c/657/afe63c657f8ffb83527f4a5b298425a1.png"><br><br><h2>  Replicaci√≥n </h2><br>  La replicaci√≥n puede ser de dos tipos: as√≠ncrona y sincr√≥nica.  As√≠ncrono es cuando las solicitudes del cliente no esperan hasta que los datos se dispersen por las r√©plicas: escribir en una r√©plica es suficiente.  Tan pronto como los datos llegan al disco, al registro, la transacci√≥n tiene √©xito y alg√∫n d√≠a en el fondo estos datos se replican.  Sincr√≥nico: cuando una transacci√≥n se divide en 2 fases: preparar y confirmar.  Commit no devolver√° el √©xito hasta que los datos se repliquen en alg√∫n qu√≥rum de r√©plicas. <br><br>  La replicaci√≥n asincr√≥nica es obviamente m√°s r√°pida porque nada descansa en la red.  Los datos se enviar√°n a la red en segundo plano y la transacci√≥n en s√≠, tal como est√° registrada en el registro, se ha completado.  Pero hay un problema: las r√©plicas pueden retrasarse entre s√≠, aparecen sincronizadas. <br>  La replicaci√≥n sincr√≥nica es m√°s confiable, pero mucho m√°s lenta y m√°s dif√≠cil de implementar.  Hay protocolos complejos.  En Tarantool, puede elegir cualquiera de estos tipos de replicaciones, seg√∫n la tarea. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/32f/ca3/bd2/32fca3bd2ad0289626e48a5efc6a8e8d.png"><br><br>  El retraso de las r√©plicas da lugar no solo a la desincronizaci√≥n, sino tambi√©n al problema de ignorancia del maestro: no sabe c√≥mo pasar sus cambios a la r√©plica.  Los cambios generalmente se dan de forma incremental: se aplican y, de la misma forma, se alejan volando hacia la r√©plica.  Pero, ¬øqu√© hacer con ellos si la r√©plica no est√° disponible?  Por ejemplo, todo se puede configurar en Tarantool, y el asistente se vuelve muy flexible. <br><br>  Otro desaf√≠o: ¬øc√≥mo hacer que la topolog√≠a sea compleja?  Mail.ru, por ejemplo, tiene una topolog√≠a con cientos de Tarantool.  Tiene un n√∫cleo de tarantool al que las tar√°ntulas de r√©plica para copias de seguridad est√°n vinculadas en un c√≠rculo.  En Tarantool, puede hacer topolog√≠as completamente arbitrarias, la replicaci√≥n con esta vive perfectamente. <br><br><h2>  Sharding </h2><br>  Ahora pasemos a la escala de datos: fragmentaci√≥n.  Puede ser de dos tipos: rangos y hashes.  La divisi√≥n de rango es cuando todos los datos se ordenan por alguna clave de divisi√≥n, y esta secuencia grande se divide en rangos para que cada rango tenga aproximadamente la misma cantidad de datos.  Y cada rango se almacena por completo en cualquier nodo f√≠sico.  Pero por lo general, no se necesita tal fragmentaci√≥n.  Adem√°s, siempre es muy complicado. <br><br>  Tambi√©n hay fragmentaci√≥n con hashes.  Se acaba de presentar en Tarantool.  Es mucho m√°s f√°cil de implementar, usar y casi siempre es adecuado en lugar de rangos de fragmentaci√≥n.  Funciona as√≠: consideramos la funci√≥n hash del registro y devuelve el n√∫mero del nodo f√≠sico en el que almacenar.  Hay problemas: en primer lugar, es dif√≠cil completar r√°pidamente una consulta compleja. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/dcc/eb9/f36/dcceb9f36f82f045eb0c461f0f934459.png"><br><br>  En segundo lugar, est√° el problema de la reorganizaci√≥n.  Hay alg√∫n tipo de funci√≥n de fragmento que devuelve el n√∫mero del fragmento f√≠sico en el que se debe guardar la clave.  Y cuando cambia el n√∫mero de nodos, la funci√≥n de fragmento tambi√©n cambia.  Esto significa que para todos los datos que est√°n en el cl√∫ster, deber√° volver a calcularse y verificarse nuevamente.  Adem√°s, en la divisi√≥n cl√°sica, algunos datos no se transferir√°n a un nuevo nodo, sino que simplemente se barajar√°n entre los nodos antiguos.  Las transferencias in√∫tiles no se pueden reducir a cero en el fragmentaci√≥n cl√°sica. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/479/b1f/4e4/479b1f4e48b552723c900d49a4104412.png"><br><br>  Tarantool utiliza fragmentaci√≥n virtual: los datos se distribuyen no en nodos f√≠sicos, sino virtuales.  Cubo virtual en un cl√∫ster virtual.  Y las historias virtuales se presentan en las f√≠sicas.  Y ya est√° garantizado que cada piso virtual se encuentra completamente en un piso f√≠sico. <br><br>  ¬øC√≥mo resuelve esto el problema de la reventa?  El hecho es que el n√∫mero de dep√≥sitos es fijo y excede seriamente el n√∫mero de nodos f√≠sicos.  Por lo tanto, no importa cu√°nto escale f√≠sicamente su cl√∫ster, el dep√≥sito siempre ser√° suficiente para almacenar datos y distribuirlos de manera uniforme.  Y debido a que la funci√≥n de fragmento no ha cambiado, no tendr√° que volver a calcularla cuando cambie la composici√≥n del cl√∫ster. <br><br>  Como resultado, obtenemos <i>tres tipos de fragmentaci√≥n: rangos, hashes y cubos virtuales</i> .  En el caso de rangos y dep√≥sitos, existe un problema de b√∫squeda f√≠sica. <br><br>  ¬øC√≥mo solucionarlo?  La primera forma: simplemente prohibir el re-compartir.  Luego, para volver a compartir, tendr√° que crear un nuevo cl√∫ster y transferir todo all√≠.  La segunda forma: ir siempre a todos los nodos.  Pero esto no tiene sentido, porque necesita escalar, y los c√°lculos no escalan as√≠.  Tercera opci√≥n: un m√≥dulo proxy, que sirve como un tipo de enrutador para cubos.  Lo inicia, env√≠a una solicitud all√≠, indicando el n√∫mero del dep√≥sito, y enviar√° su solicitud como proxy al nodo f√≠sico deseado. <br><br><h2>  Avanzado en memoria con el ejemplo de plataforma GridGain </h2><br>  El negocio tiene requisitos de base de datos adicionales.  √âl quiere que todo esto sea tolerante a fallas y catastr√≥fico.  Quiere alta disponibilidad: para que nunca se pierda nada, para que pueda recuperarse r√°pidamente.  Tambi√©n se necesita una escalabilidad f√°cil y econ√≥mica, soporte sin complicaciones, confianza en la plataforma y mecanismos de acceso eficientes. <br><br>  Todas estas ideas no son nuevas.  Muchas de estas cosas se implementan, en un grado u otro, en los DBMS cl√°sicos, en particular, la replicaci√≥n entre centros de datos. <br><br>  In-Memory ya no es una tecnolog√≠a de inicio, son productos maduros que se utilizan en las empresas m√°s grandes del mundo (Barclays, Citi Group, Microsoft, etc.).  Se supone que all√≠ se cumplen todos estos requisitos. <br><br>  Entonces, si una cat√°strofe sucedi√≥ de repente, deber√≠a haber una oportunidad para recuperarse de la copia de seguridad.  Y si estamos hablando de una organizaci√≥n financiera, es importante que esta copia de seguridad sea coherente y no solo una copia de todas las unidades.  Para que no haya una situaci√≥n en la que en algunas partes de los nodos los datos se restauraron en el momento X, y en la otra parte en el tiempo Y. Es muy importante tener la recuperaci√≥n en un punto en el tiempo, de modo que incluso en una situaci√≥n de corrupci√≥n de datos o un accidente particularmente grave, minimice la cantidad de p√©rdida. <br><br>  Es importante poder enviar datos al disco.  Para que el cl√∫ster no se sobrecargue y contin√∫e funcionando a√∫n m√°s lentamente.  Y para levantarse r√°pidamente del disco, y luego ya bombe√≥ los datos a la memoria. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/042/edc/328/042edc32872f7b8687bf5677818c712e.png"><br>  <i>Respuesta en memoria a fallas con y sin componentes de tolerancia a fallas GridGain</i> <br><br>  Un cl√∫ster de conmutaci√≥n por error debe escalar f√°cilmente horizontal y verticalmente.  No tengo ganas de pagar mi servidor y ver c√≥mo la mitad de los recursos est√°n inactivos.  No quiero tener el infierno de cientos de procesos que necesitan ser gestionados.  Quiero un sistema simple desde el punto de vista del soporte, con entrada y salida f√°cil de nodos del cl√∫ster y un sistema de monitoreo desarrollado y maduro. <br><br>  Considere MongoDB en esta perspectiva.  Todos los que han trabajado con MongoDB conocen una gran cantidad de procesos.  Si tenemos un MongoDB sombreado de 5 fragmentos, cada fragmento tendr√° un conjunto de r√©plica de tres procesos (con una relaci√≥n de redundancia de 3).  Y esto son 15 procesos solo en los datos en s√≠.  El almacenamiento de configuraci√≥n de cl√∫ster es otro m√°s 3 procesos, en total obtiene 18, y esto no incluye enrutadores.  Si quieres 20 fragmentos, bienvenido a m√°s de 63 procesos (por ejemplo, otros 8, un total de 71) procesos. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/015/389/b64/015389b641f5babfe44815fd85f258cc.png"><br><br>  Comp√°ralo con Cassandra.  Tomamos los mismos 5 fragmentos: estos son 5 procesos y 5 nodos con la misma relaci√≥n de redundancia de 3, que es mucho m√°s simple en t√©rminos de control.  Quiero 20 fragmentos: estos son 20 procesos.  Puedo escalar mi cl√∫ster a cualquier n√∫mero de nodos, no necesariamente un m√∫ltiplo de 3 (u otro valor del coeficiente de redundancia).  Mucho m√°s f√°cil y econ√≥mico de implementar y mantener que los conjuntos de r√©plica. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bcf/70d/4a4/bcf70d4a4883f9fdc951c871b99c57be.png"><br><br>  Adem√°s, debe confiar en el sistema, para comprender qu√© respaldan las personas cada producto individual.  Idealmente, la licencia debe ser de c√≥digo abierto o n√∫cleo abierto.  Para que, en caso de fallecimiento del vendedor, se pueda hacer algo.  Tambi√©n es bueno si el c√≥digo fuente es administrado por una comunidad independiente: todos recordamos c√≥mo MongoDB y Redis cambiaron las licencias a pedido de la compa√±√≠a administradora.  C√≥mo Aerospike introdujo restricciones en la edici√≥n comunitaria de "c√≥digo abierto" a principios de a√±o. <br><br>  Necesita acceso efectivo a los datos.  Casi todos tienen un lenguaje de consulta estructurado de una forma u otra.  La mayor√≠a de las veces usan SQL, es necesario que la adaptaci√≥n con este lenguaje sea lo m√°s f√°cil posible.  Esto ayudar√° a la ejecuci√≥n de consultas distribuidas, cuando no necesite enviar una solicitud por separado a cada nodo, pero puede comunicarse con el cl√∫ster como con una "ventana √∫nica".  Sin pensar desde el punto de vista de la API, este es un conjunto de nodos (recuerde lo dif√≠cil que es trabajar con Memcache en grandes vol√∫menes incluso en el nivel m√°s simple de poner / obtener, sin consultas SQL potencialmente complejas), garant√≠as distribuidas de DDL y ACID. <br><br>  Y finalmente, apoyo.  Si algo de repente no funciona, entonces la compa√±√≠a simplemente pierde dinero.  Para algunas √°reas esto no es cr√≠tico, pero a menudo es importante que alguien asuma la responsabilidad del producto y su trabajo.  Que era posible en cualquier momento hacer un reclamo, y se resolvi√≥ r√°pidamente. <br><br>  Con esta publicaci√≥n estamos completando el a√±o de Promsvyazbank en Habr√©.  Recolectamos los deseos de A√±o Nuevo para los residentes de Khabrovsk en un video corto: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/yqp6V3Wqniw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es434730/">https://habr.com/ru/post/es434730/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es434720/index.html">Reverso del conocimiento cero: una puerta trasera en zk-SNARK que no se puede detectar</a></li>
<li><a href="../es434722/index.html">Dolor, pastillas y dos ambulancias, o c√≥mo todos subimos al quinto lugar IronStar 226 en Sochi</a></li>
<li><a href="../es434724/index.html">Los agricultores chinos hacen transmisi√≥n en vivo</a></li>
<li><a href="../es434726/index.html">Errores de identificar un dispositivo Android</a></li>
<li><a href="../es434728/index.html">Personas y procesos: ¬øpor qu√© udalenka no es adecuado para todas las empresas?</a></li>
<li><a href="../es434732/index.html">Vida a 6200 DPI. Revisi√≥n de HyperX Pulsefire Core</a></li>
<li><a href="../es434734/index.html">Transformada de Fourier. El r√°pido y el furioso</a></li>
<li><a href="../es434736/index.html">Usando la base de datos de registro de Mikrotik para suprimir la fuerza bruta</a></li>
<li><a href="../es434738/index.html">Aprendizaje de refuerzo en Python</a></li>
<li><a href="../es434740/index.html">Red neuronal ense√±ada a detectar paneles solares en im√°genes satelitales y predecir el nivel de su distribuci√≥n</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>