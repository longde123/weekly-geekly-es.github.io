<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üó®Ô∏è ü§ú üçÑ Verbesserung der Qualit√§t der Textklassifizierung durch Verbindung von Wikipedia ‚ÅâÔ∏è ‚ùï üôéüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wir verwenden eine gro√üe strukturierte Quelle mehrsprachiger Texte - Wikipedia, um die Klassifizierung von Texten zu verbessern. Der Ansatz ist gut mi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Verbesserung der Qualit√§t der Textklassifizierung durch Verbindung von Wikipedia</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/446228/"> Wir verwenden eine gro√üe strukturierte Quelle mehrsprachiger Texte - Wikipedia, um die Klassifizierung von Texten zu verbessern.  Der Ansatz ist gut mit einem hohen Ma√ü an Automatismus und Unabh√§ngigkeit, von dem ein bestimmtes Klassifizierungsproblem gel√∂st wird.  Die gr√∂√üte Auswirkung wird jedoch auf die Aufgaben der Themenbestimmung erwartet. <br><a name="habracut"></a><br>  Die Hauptidee ist, nur die Texte aus Wikipedia zu extrahieren, die uns helfen, unser Klassifizierungsproblem zu l√∂sen, und andere zu ignorieren.  Wenn wir Texte √ºber Katzen klassifizieren, ist es unwahrscheinlich, dass wir Texte zur Quantenphysik ben√∂tigen, obwohl Texte zu anderen Tierarten n√ºtzlich sein k√∂nnen.  Die automatische Trennung solcher Texte voneinander ist das Wesentliche des beschriebenen Ansatzes. <br><br>  Wie Sie wissen, ist Wikipedia eine Sammlung von Artikeln zu vielen Wissens- und Interessenbereichen.  Gleichzeitig enth√§lt ein wesentlicher Teil der Artikel Links zu Artikeln eines √§hnlichen Themas, jedoch in anderen Sprachen.  Dies sind keine √úbersetzungen, n√§mlich Artikel eines allgemeinen Themas.  Au√üerdem fallen die meisten Artikel in eine oder mehrere Kategorien.  Kategorien wiederum sind gr√∂√ütenteils in Form eines hierarchischen Baums organisiert.  Das hei√üt, die Aufgabe, Wikipedia-Artikel zu f√ºr uns interessanten Themen zu gruppieren, kann gel√∂st werden. <br><br>  Wir verwenden die Ressource DBPedia - eine vorverdrahtete und strukturierte Version von Wikipedia.  DBPedia gibt uns alle notwendigen Informationen - die Namen der Artikel, ihre Anmerkungen, die Kategorien der Artikel und die √ºbergeordneten Kategorien f√ºr die Kategorien.  Wir beginnen mit der auf Wikipedia am h√§ufigsten vertretenen Sprache - Englisch.  Wenn Ihre Aufgabe keine oder nur wenige englischsprachige Texte enth√§lt, verwenden Sie die Sprache, f√ºr die es viele Dokumente gibt. <br><br><h3>  Schritt 1. Wikipedia gruppieren </h3><br>  Konzentrieren Sie sich auf Artikelkategorien.  Ignorieren Sie vorerst ihren Inhalt.  Kategorien bilden ein Diagramm, meistens baumartig, aber es gibt auch Zyklen.  Artikel sind die Endpunkte des Diagramms (Bl√§tter), die mit einem oder mehreren Knoten des Diagramms verbunden sind.  Wir verwenden das Node2Vec-Tool, um eine Vektordarstellung jeder Kategorie und jedes Artikels zu erhalten.  Artikel √§hnlicher Themen werden im Vektorraum zusammengefasst. <br><br>  Wir gruppieren nach einer geeigneten Methode des Artikels in eine ziemlich gro√üe (Hunderte) Anzahl von Clustern. <br><br><h3>  Schritt 2. Klassifizierertraining auf Wikipedia </h3><br>  Wir ersetzen die Namen der Artikel in den resultierenden Clustern durch ihre Anmerkungen (Long Abstract und Short Abstract - ungef√§hr ein Textabschnitt pro Artikel).  Jetzt haben wir Hunderte von Clustern, die als Texts√§tze definiert sind.  Wir verwenden ein praktisches Modell und erstellen einen Klassifikator, der das Problem der Klassifizierung mehrerer Klassen l√∂st: einen Cluster - eine Klasse.  Wir haben FastText verwendet. <br>  Bei der Ausgabe erhalten wir ein Modell, das Text als Eingabe verwendet, und bei der Ausgabe erhalten Sie einen Vektor mit Sch√§tzungen des Grades, zu dem der Text zu unseren Hunderten von Klassenclustern geh√∂rt. <br><br>  Wenn der erste Schritt darin besteht, Wikipedia-Artikel nicht nach Kategorien, sondern nach Inhalt zu gruppieren, verlieren wir erstens Informationen nach Kategorien, aber es ist wichtig, und zweitens erhalten wir ein entartetes System, das nach Texten gruppiert und aufgebaut wird Klassifikatormodell.  Die endg√ºltige Qualit√§t wird wahrscheinlich schlechter sein als bei einem separaten Ansatz.  Obwohl ich nicht √ºberpr√ºft habe. <br><br><h3>  Schritt 3. Erstellen Sie ein Modell aus Ihren eigenen Kampfdaten </h3><br>  Wir verwenden eine Auswahl unserer Kampfdaten und senden jedes Dokument ab Schritt 2 an die Eingabe des Modells. Das Modell gibt einen Vektor von Sch√§tzungen zur√ºck.  Wir verwenden diesen Vektor als Merkmalsvektor f√ºr das betreffende Dokument.  Nachdem wir alle unsere Trainingsmuster von Kampfdokumenten verarbeitet haben, erhalten wir eine Tabelle in der Standardform f√ºr maschinelles Lernen - ein Klassenetikett, eine Reihe von numerischen Zeichen.  Wir nennen diesen Tisch ein Trainingsset. <br><br>  Wir bauen auf dem Trainingsbeispiel einen Klassifikator auf, der den Informationsgehalt einzelner Attribute bewerten kann.  Entscheidungsb√§ume und zuf√§llige Waldvariationen davon sind gut geeignet.  Die informativsten Anzeichen sind jene Cluster von Wikipedia-Artikeln, die nicht nur √§hnliche Themen wie die Themen unserer Kampfdokumente haben, sondern vor allem die Themen dieser Artikel erm√∂glichen es uns, unsere Kampfklassen gut zu trennen.  Bei den ersten Iterationen ist das Histogramm der Informativit√§t von Zeichen normalerweise ziemlich flach - mehrere informative Cluster und ein langer Schwanz sind in Bezug auf die Informativit√§t fast gleich den verbleibenden Hunderten von Zeichen. <br><br>  Nach dem Studium des Histogramms des Informationsgehalts der Zeichen wurde jedes Mal ein Wendepunkt empirisch bestimmt, und ungef√§hr 10 bis 30% der Cluster gingen zur n√§chsten Iteration.  Das Wesentliche der Iteration ist, dass Artikel aus den ausgew√§hlten informativen Clustern kombiniert, den Schritten 1 bis 3 unterzogen wurden, wo sie erneut geclustert wurden, zwei Klassifizierer erneut erstellt wurden und alles mit einer Analyse des Histogramms des Informationsinhalts endete.  Es dauert 3-4 Iterationen. <br><br>  Es stellte sich heraus, dass auf unseren Daten die digitalen Zeichen, insbesondere die Zahlen der Jahre, ein sehr starkes Gewicht haben und die Informativit√§t des gesamten Clusters auf sich ziehen.  Als logisches Ergebnis wurden die Cluster, die den j√§hrlichen Sportveranstaltungen gewidmet waren, am informativsten - eine Masse von Zahlen und Daten, ein enger Wortschatz.  Ich musste alle Zahlen in den Texten der Artikelanmerkungen entfernen (im zweiten Schritt).  Es wurde merklich besser, Artikelcluster, die wirklich ein bestimmtes Thema hatten, stachen heraus (wie wir es uns vorgestellt hatten).  Gleichzeitig tauchten unerwartete Cluster auf, die logischerweise auf unsere Kampfmission fielen, das richtige Vokabular hatten, aber es war sehr schwierig, die N√ºtzlichkeit solcher Cluster von vornherein zu erraten. <br><br><h3>  Schritt 4. Finalisieren Sie das Modell </h3><br>  Nach mehreren Iterationen der Schritte 1 bis 3 haben wir eine angemessene Anzahl von Artikeln aus Wikipedia ausgew√§hlt, deren Themen dazu beitragen, unsere Kampfdokumente zu teilen.  Wir erweitern die Auswahl um √§hnliche Artikel in anderen Sprachen, die f√ºr uns von Interesse sind, und bilden endg√ºltige Cluster, diesmal zehn.  Diese Cluster k√∂nnen auf zwei Arten verwendet werden: Erstellen Sie entweder einen Klassifikator √§hnlich Schritt 2 und erweitern Sie damit den digitalen Merkmalsvektor in Ihrer Kampfmission, oder verwenden Sie diese Texts√§tze als Quelle f√ºr zus√§tzliches Vokabular und integrieren Sie sie in Ihren Kampfklassifikator.  Wir haben den zweiten Weg benutzt. <br><br>  Unser Kampfklassifikator besteht aus zwei Modellen - abgeschnittenen naiven Bayes und xgboost.  Naive Bayes arbeitet mit langen Gramm, dies sind Gramm mit einer L√§nge von 1 bis 16 Elementen, und jedes gefundene Gramm neigt die Summe zu einer der Klassen, aber Bayes trifft keine endg√ºltige Entscheidung - es gibt nur die Summe der Grammgewichte an, die sich auf jedes beziehen aus Klassen.  Xgboost akzeptiert die Ausgabe von Bayes, anderen Klassifizierern und einigen digitalen Attributen, die unabh√§ngig vom Text erstellt werden, und xgboost liefert bereits das endg√ºltige Modell und die endg√ºltige Bewertung.  Dieser Ansatz macht es einfach, beliebige Texts√§tze mit dem Gramm Bayes-Modell zu verbinden, einschlie√ülich der resultierenden S√§tze von Wikipedia-Artikeln, und xgboost sucht bereits nach Mustern in Form typischer Reaktionen von Wikipedia-Clustern auf Kampftexte. <br><br><h3>  Ergebnisse und Schlussfolgerungen </h3><br>  Das erste Ergebnis ergab eine Erh√∂hung von der bedingten Genauigkeit von 60% auf 62%.  Beim Ersetzen der Anmerkungen von Wikipedia-Artikeln in Schritt 4 durch die entleerten Artikel selbst wurde die Genauigkeit auf 66% erh√∂ht.  Das Ergebnis ist nat√ºrlich, da die Gr√∂√üe der Anmerkung zwei oder drei S√§tze betr√§gt und die Gr√∂√üe des Artikels um Gr√∂√üenordnungen gr√∂√üer ist.  Mehr Sprachmaterial - h√∂here Wirkung. <br><br>  Wir sollten erwarten, dass nach Abschluss des gesamten Verfahrens f√ºr die Texte von Artikeln anstelle von Anmerkungen die Qualit√§tssteigerung noch gr√∂√üer ist, aber es gibt bereits ein Problem mit der technischen Nummer - das Herunterladen und Verarbeiten der gesamten Wikipedia oder ihres erkennbaren Teils (wenn Sie nicht mit der ersten Iteration beginnen) ist schwierig.  Wenn Sie anf√§nglich nicht nur Englisch, sondern alle Sprachen von Interesse verwenden, k√∂nnen Sie trotzdem etwas anderes gewinnen.  In diesem Fall ist das Wachstum der verarbeiteten Mengen vielf√§ltig und nicht wie im ersten Fall um Gr√∂√üenordnungen. <br><br><h4>  Semantischer Dokumentvektor </h4><br>  F√ºr jedes Dokument wird ein Vektor aus der Beziehung des Dokuments zu den angegebenen Themen basierend auf Wikipedia-Kategorien erstellt.  Der Vektor wird entweder nach der in Schritt 3 beschriebenen Methode oder nach unseren Gramm Bayes berechnet.  Dementsprechend k√∂nnen Kampfdokumente gem√§√ü diesen Vektoren gruppiert werden und eine Gruppierung von Kampfdokumenten nach Subjekt erhalten.  Es m√ºssen nur noch die Hashtags abgelegt werden und jedes neue Dokument kann bereits mit Tags in die Datenbank fallen.  Welche dann Benutzer suchen k√∂nnen.  Dies ist der Fall, wenn Sie dem Benutzer Tags explizit und sichtbar anbringen.  Es sieht modisch aus, obwohl ich kein Unterst√ºtzer bin. <br><br><h4>  Adaptive Suche </h4><br>  Eine interessantere Methode zur Verwendung semantischer Dokumentvektoren ist die adaptive Suche.  Wenn Sie die Aktivit√§t des Benutzers beobachten, auf welchen Dokumenten er verweilt und welche er nicht einmal liest, k√∂nnen Sie den Interessenbereich des Benutzers im langfristigen Sinne (schlie√ülich haben Benutzer auch eine Aufteilung der Verantwortlichkeiten und jeder sucht haupts√§chlich nach seinen eigenen) und im Rahmen der aktuellen Suchsitzung skizzieren. <br><br>  Dokumente mit √§hnlichen Themen haben √§hnliche semantische Vektoren mit einem hohen Kosinusma√ü. Auf diese Weise k√∂nnen Sie Dokumente in den Suchergebnissen im laufenden Betrieb entsprechend dem Grad ihrer erwarteten √úbereinstimmung mit den Interessen des Benutzers bewerten, wodurch Sie die erforderlichen Dokumente in den Suchergebnissen erh√∂hen k√∂nnen. <br><br>  Selbst bei identischen Suchanfragen f√ºr jeden Benutzer k√∂nnen die Suchergebnisse f√ºr ihn personalisiert werden. Je nachdem, an welchen Dokumenten im vorherigen Schritt der Benutzer interessiert war, wird der n√§chste Suchschritt an die Bed√ºrfnisse des Benutzers angepasst, auch wenn sich die Suchanfrage selbst nicht ge√§ndert hat. <br><br>  Wir arbeiten jetzt am Problem der adaptiven Suche. <br><br><h4>  Testen von Gesch√§ftshypothesen </h4><br>  Das Gesch√§ft kommt regelm√§√üig mit guten Ideen, die sehr schwer umzusetzen sind.  Wir m√ºssen lernen, Dokumente anhand ihrer Beschreibung zu finden, ohne entweder ein markiertes Muster f√ºr die Schulung zu haben oder die M√∂glichkeit zu haben, den Pr√ºfern eine Reihe von Dokumenten zur Bewertung vorzulegen.  Dies ist normalerweise der Fall, wenn Zieldokumente in Bezug auf den allgemeinen Dokumentenfluss selten gefunden werden. Wenn Sie daher einen Pool von 10.000 Dokumenten ohne vorl√§ufige Filterung an die Pr√ºfer senden, erhalten Sie 1-2 notwendige oder sogar weniger Ausgaben. <br><br>  Unser Ansatz ist es, einen iterativen Lernprozess basierend auf semantischen Vektoren zu erstellen.  Im ersten Schritt finden wir mehrere Texte, die unser Zielthema festlegen - dies k√∂nnen Wikipedia-Artikel oder Texte aus anderen Quellen sein.  F√ºr jeden Text wird sein semantischer Vektor erzeugt.  Wenn das Zielthema komplex ist, funktioniert die Algebra der Mengen - Vereinheitlichung, Schnittmenge, Ausschluss einiger Themen von anderen.  Zum Beispiel - es gibt Wikipedia-Artikel √ºber "Forschung und Entwicklung" und √ºber "Kosmetik", die Schnittmenge von Sets wird "F &amp; E √ºber Kosmetik" geben. <br><br>  Alle Dokumente in der Datenbank k√∂nnen nach dem Grad ihrer √úbereinstimmung mit den angegebenen Themen sortiert werden. Die Algebra der Mengen wirkt sich dann wie folgt auf die Dokumente selbst aus. Ein Dokument wird als themenrelevant angesehen, wenn sein semantischer Vektor n√§her am Vektor von Wikipedia-Artikeln eines bestimmten Themas liegt als der Durchschnitt der Datenbank.  Schnittpunkt - wenn gleichzeitig der semantische Vektor des Dokuments n√§her an beiden Themen liegt als der Durchschnitt f√ºr die Datenbank.  Andere Operationen sind √§hnlich. <br><br>  Wir finden eine Reihe von Hunderten oder zwei Dokumenten, die allen positiven Themen am n√§chsten kommen und gleichzeitig allen negativen Themen am n√§chsten kommen (wenn wir uns nicht f√ºr finanzielle Fragen in der von uns gesuchten Forschung interessieren, werden wir den Artikel aus der Kategorie ‚ÄûFinanzen‚Äú als negatives Beispiel setzen). )  Wir werden diese Dokumente den Pr√ºfern geben, sie werden mehrere positive Beispiele darin finden, basierend auf diesen Beispielen werden wir nach anderen Dokumenten mit engen semantischen Vektoren suchen, sie markieren und am Ausgang genug Dokumente f√ºr die positive Klasse erhalten, um einen geeigneten Klassifikator zu erstellen.  Es kann mehrere Iterationen dauern. <br><br><h4>  Zusammenfassung </h4><br>  Der beschriebene Ansatz erm√∂glicht automatisch ohne manuelle Analyse die Auswahl aus Wikipedia oder anderen Quellens√§tzen von Texten, die zur L√∂sung des Klassifizierungsproblems beitragen.  Durch einfaches Verbinden der Cluster von Wikipedia mit einem funktionierenden Klassifikator kann eine signifikante Qualit√§tssteigerung erwartet werden, ohne dass eine Anpassung des Klassifikators selbst erforderlich ist. <br><br>  Nun, adaptive Suche ist interessant. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de446228/">https://habr.com/ru/post/de446228/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de446210/index.html">ADAM-3600 - eine multifunktionale industrielle Steuerung</a></li>
<li><a href="../de446212/index.html">SIEM-Tiefen: Out-of-Box-Korrelationen. Teil 5. Methodik zur Entwicklung von Korrelationsregeln</a></li>
<li><a href="../de446214/index.html">OS1: Ein primitiver Kernel auf Rust f√ºr x86. Teil 3. Speicherkarte, Seitenfehlerausnahme, Heap und Zuordnungen</a></li>
<li><a href="../de446218/index.html">Game Designer unterscheidet sich nicht viel von einem Psycho. Wie wir das CMAN-Spiel gemacht haben</a></li>
<li><a href="../de446222/index.html">Nutzung thermischer Potentiale f√ºr die Gebietsanalyse</a></li>
<li><a href="../de446230/index.html">Die Fern√ºberwachung und -verwaltung von Linux / OpenWrt / Lede-basierten Ger√§ten √ºber Port 80 wurde fortgesetzt</a></li>
<li><a href="../de446234/index.html">Wie Freiwillige aus aller Welt Live-√úbertragungen von ICPC-2019 erstellen</a></li>
<li><a href="../de446236/index.html">Yandex wird die Spracherkennungsalgorithmen verbessern</a></li>
<li><a href="../de446238/index.html">Ausnutzen signierter Bootloader, um UEFI Secure Boot zu umgehen</a></li>
<li><a href="../de446242/index.html">Aufschub als Werkzeug f√ºr Zeitreisen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>