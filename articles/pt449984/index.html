<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèæ ‚ôÄÔ∏è üì∏ Google News e Leo Tolstoy: visualizando incorpora√ß√µes de palavras do Word2Vec usando t-SNE ü§æüèæ üßîüèæ üë©‚Äçüíª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Todo mundo percebe de maneira √∫nica os textos, independentemente de essa pessoa ler not√≠cias na Internet ou romances cl√°ssicos mundialmente conhecidos...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Google News e Leo Tolstoy: visualizando incorpora√ß√µes de palavras do Word2Vec usando t-SNE</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/449984/"><img src="https://habrastorage.org/webt/6c/ux/7m/6cux7mvmp3phb8d8efjwmqrb_yc.gif"><br>  Todo mundo percebe de maneira √∫nica os textos, independentemente de essa pessoa ler not√≠cias na Internet ou romances cl√°ssicos mundialmente conhecidos.  Isso tamb√©m se aplica a uma variedade de algoritmos e t√©cnicas de aprendizado de m√°quina, que compreendem textos de uma maneira mais matem√°tica, ou seja, usando espa√ßo vetorial de alta dimens√£o. <br><br>  Este artigo √© dedicado √† visualiza√ß√£o de incorpora√ß√µes de palavras do Word2Vec de alta dimens√£o usando t-SNE.  A visualiza√ß√£o pode ser √∫til para entender como o Word2Vec funciona e como interpretar rela√ß√µes entre vetores capturados em seus textos antes de us√°-los em redes neurais ou em outros algoritmos de aprendizado de m√°quina.  Como dados de treinamento, usaremos artigos do Google Not√≠cias e obras liter√°rias cl√°ssicas de Leo Tolstoy, o escritor russo considerado um dos maiores autores de todos os tempos. <br><br>  Analisamos a breve vis√£o geral do algoritmo t-SNE, depois passamos para o c√°lculo de incorpora√ß√£o de palavras usando o Word2Vec e, finalmente, prosseguimos para a visualiza√ß√£o de vetores de palavras com t-SNE no espa√ßo 2D e 3D.  Escreveremos nossos scripts em Python usando o Jupyter Notebook. <br><br><a name="habracut"></a><br><h1>  Incorpora√ß√£o estoc√°stica de vizinhos distribu√≠dos em T </h1><br>  O T-SNE √© um algoritmo de aprendizado de m√°quina para visualiza√ß√£o de dados, baseado em uma t√©cnica de redu√ß√£o de linearidade n√£o linear.  A id√©ia b√°sica do t-SNE √© reduzir o espa√ßo dimensional, mantendo a dist√¢ncia pareada entre os pontos.  Em outras palavras, o algoritmo mapeia dados multidimensionais para duas ou mais dimens√µes, onde pontos que inicialmente estavam distantes um do outro tamb√©m est√£o localizados longe, e pontos pr√≥ximos tamb√©m s√£o convertidos em pontos pr√≥ximos.  Pode-se dizer que o t-SNE procura uma nova representa√ß√£o de dados onde as rela√ß√µes de vizinhan√ßa s√£o preservadas.  A descri√ß√£o detalhada de toda a l√≥gica do t-SNE pode ser encontrada no artigo original [1]. <br><br><h1>  O modelo Word2Vec </h1><br>  Para come√ßar, devemos obter representa√ß√µes vetoriais de palavras.  Para esse fim, selecionei o Word2vec [2], ou seja, um modelo preditivo computacionalmente eficiente para aprender a incorpora√ß√£o de palavras multidimensionais a partir de dados textuais brutos.  O conceito principal do Word2Vec √© localizar palavras, que compartilham contextos comuns no corpus de treinamento, em estreita proximidade no espa√ßo vetorial em compara√ß√£o com outros. <br><br>  Como dados de entrada para visualiza√ß√£o, usaremos artigos do Google Not√≠cias e alguns romances de Leo Tolstoy.  Vetores pr√©-treinados, treinados em parte do conjunto de dados do Google Not√≠cias (cerca de 100 bilh√µes de palavras), foram publicados pelo Google na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">p√°gina oficial</a> , portanto, vamos us√°-lo. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gensim model = gensim.models.KeyedVectors.load_word2vec_format(<span class="hljs-string"><span class="hljs-string">'GoogleNews-vectors-negative300.bin'</span></span>, binary=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><br>  Al√©m do modelo pr√©-treinado, treinaremos outro modelo nos romances de Tolstoi usando a biblioteca Gensim [3].  O Word2Vec usa senten√ßas como dados de entrada e produz vetores de palavras como sa√≠da.  Primeiro, √© necess√°rio fazer o download do Punkt Senten√ßa Tokenizer pr√©-treinado, que divide um texto em uma lista de frases, considerando palavras de abrevia√ß√£o, coloca√ß√µes e palavras, o que provavelmente indica o in√≠cio ou o fim das frases.  Por padr√£o, o pacote de dados NLTK n√£o inclui um tokenizador Punkt pr√©-treinado para russo; portanto, usaremos modelos de terceiros em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">github.com/mhq/train_punkt</a> . <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> codecs <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocess_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(text)</span></span></span><span class="hljs-function">:</span></span> text = re.sub(<span class="hljs-string"><span class="hljs-string">'[^a-zA-Z--1-9]+'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, text) text = re.sub(<span class="hljs-string"><span class="hljs-string">' +'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, text) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text.strip() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">prepare_for_w2v</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename_from, filename_to, lang)</span></span></span><span class="hljs-function">:</span></span> raw_text = codecs.open(filename_from, <span class="hljs-string"><span class="hljs-string">"r"</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'windows-1251'</span></span>).read() <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(filename_to, <span class="hljs-string"><span class="hljs-string">'w'</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> sentence <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> nltk.sent_tokenize(raw_text, lang): print(preprocess_text(sentence.lower()), file=f)</code> </pre><br><br>  No est√°gio de treinamento do Word2Vec, foram utilizados os seguintes hiperpar√¢metros: <br><br><ul><li>  A dimensionalidade do vetor de recurso √© 200. </li><li>  A dist√¢ncia m√°xima entre as palavras analisadas em uma frase √© 5. </li><li>  Ignora todas as palavras com a frequ√™ncia total menor que 5 por corpus. </li></ul><br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> multiprocessing <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> gensim.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Word2Vec <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train_word2vec</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> data = gensim.models.word2vec.LineSentence(filename) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Word2Vec(data, size=<span class="hljs-number"><span class="hljs-number">200</span></span>, window=<span class="hljs-number"><span class="hljs-number">5</span></span>, min_count=<span class="hljs-number"><span class="hljs-number">5</span></span>, workers=multiprocessing.cpu_count())</code> </pre><br><br><h1>  Visualizando incorpora√ß√µes de palavras usando t-SNE </h1><br>  O T-SNE √© bastante √∫til caso seja necess√°rio visualizar a semelhan√ßa entre objetos que est√£o localizados no espa√ßo multidimensional.  Com um grande conjunto de dados, est√° se tornando cada vez mais desafiador fazer um gr√°fico t-SNE f√°cil de ler, por isso √© pr√°tica comum visualizar grupos das palavras mais semelhantes. <br>  Vamos selecionar algumas palavras do vocabul√°rio do modelo pr√©-treinado do Google Not√≠cias e preparar vetores de palavras para visualiza√ß√£o. <br><br><pre> <code class="python hljs">keys = [<span class="hljs-string"><span class="hljs-string">'Paris'</span></span>, <span class="hljs-string"><span class="hljs-string">'Python'</span></span>, <span class="hljs-string"><span class="hljs-string">'Sunday'</span></span>, <span class="hljs-string"><span class="hljs-string">'Tolstoy'</span></span>, <span class="hljs-string"><span class="hljs-string">'Twitter'</span></span>, <span class="hljs-string"><span class="hljs-string">'bachelor'</span></span>, <span class="hljs-string"><span class="hljs-string">'delivery'</span></span>, <span class="hljs-string"><span class="hljs-string">'election'</span></span>, <span class="hljs-string"><span class="hljs-string">'expensive'</span></span>, <span class="hljs-string"><span class="hljs-string">'experience'</span></span>, <span class="hljs-string"><span class="hljs-string">'financial'</span></span>, <span class="hljs-string"><span class="hljs-string">'food'</span></span>, <span class="hljs-string"><span class="hljs-string">'iOS'</span></span>, <span class="hljs-string"><span class="hljs-string">'peace'</span></span>, <span class="hljs-string"><span class="hljs-string">'release'</span></span>, <span class="hljs-string"><span class="hljs-string">'war'</span></span>] embedding_clusters = [] word_clusters = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> keys: embeddings = [] words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> similar_word, _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> model.most_similar(word, topn=<span class="hljs-number"><span class="hljs-number">30</span></span>): words.append(similar_word) embeddings.append(model[similar_word]) embedding_clusters.append(embeddings) word_clusters.append(words)</code> </pre> <br><img src="https://habrastorage.org/webt/uc/1k/o6/uc1ko6efgx_d-wnbwolgdabifl8.gif"><br>  <i>Fig.</i>  <i>1. O efeito de v√°rios valores de perplexidade na forma dos conjuntos de palavras.</i> <br><br>  Em seguida, prosseguimos para a parte fascinante deste artigo, a configura√ß√£o do t-SNE.  Nesta se√ß√£o, devemos prestar aten√ß√£o aos seguintes hiperpar√¢metros. <br><br><ul><li>  <i>O n√∫mero de componentes</i> , ou seja, a dimens√£o do espa√ßo de sa√≠da. </li><li>  <i>O valor da perplexidade</i> , que no contexto do t-SNE, pode ser visto como uma medida suave do n√∫mero efetivo de vizinhos.  Est√° relacionado ao n√∫mero de vizinhos mais pr√≥ximos empregados em muitos outros alunos m√∫ltiplos (veja a figura acima).  De acordo com [1], √© recomend√°vel selecionar um valor entre 5 e 50. </li><li>  <i>O tipo de inicializa√ß√£o inicial</i> para incorpora√ß√£o. </li></ul><br><br><pre> <code class="python hljs">tsne_model_en_2d = TSNE(perplexity=<span class="hljs-number"><span class="hljs-number">15</span></span>, n_components=<span class="hljs-number"><span class="hljs-number">2</span></span>, init=<span class="hljs-string"><span class="hljs-string">'pca'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">3500</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">32</span></span>) embedding_clusters = np.array(embedding_clusters) n, m, k = embedding_clusters.shape embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, <span class="hljs-number"><span class="hljs-number">2</span></span>)</code> </pre> <br><br>  Deve-se mencionar que o t-SNE tem uma fun√ß√£o objetiva n√£o convexa, que √© minimizada usando uma otimiza√ß√£o de descida gradiente com inicia√ß√£o aleat√≥ria, para que execu√ß√µes diferentes produzam resultados ligeiramente diferentes. <br><br>  Abaixo, voc√™ encontra um script para criar um gr√°fico de dispers√£o 2D usando o Matplotlib, uma das bibliotecas mais populares para visualiza√ß√£o de dados no Python. <br><br><img src="https://habrastorage.org/webt/34/9y/7h/349y7hxuanvvttqfxkpb48j4n-q.png"><br>  <i>Fig.</i>  <i>2. Clusters de palavras semelhantes do Google Not√≠cias (preplexidade = 15).</i> <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.manifold <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> TSNE <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.cm <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> cm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np % matplotlib inline <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tsne_plot_similar_words</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(labels, embedding_clusters, word_clusters, a=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.7</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">16</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) colors = cm.rainbow(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, len(labels))) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> label, embeddings, words, color <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(labels, embedding_clusters, word_clusters, colors): x = embeddings[:,<span class="hljs-number"><span class="hljs-number">0</span></span>] y = embeddings[:,<span class="hljs-number"><span class="hljs-number">1</span></span>] plt.scatter(x, y, c=color, alpha=a, label=label) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(words): plt.annotate(word, alpha=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, xy=(x[i], y[i]), xytext=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), textcoords=<span class="hljs-string"><span class="hljs-string">'offset points'</span></span>, ha=<span class="hljs-string"><span class="hljs-string">'right'</span></span>, va=<span class="hljs-string"><span class="hljs-string">'bottom'</span></span>, size=<span class="hljs-number"><span class="hljs-number">8</span></span>) plt.legend(loc=<span class="hljs-number"><span class="hljs-number">4</span></span>) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) plt.savefig(<span class="hljs-string"><span class="hljs-string">"f/.png"</span></span>, format=<span class="hljs-string"><span class="hljs-string">'png'</span></span>, dpi=<span class="hljs-number"><span class="hljs-number">150</span></span>, bbox_inches=<span class="hljs-string"><span class="hljs-string">'tight'</span></span>) plt.show() tsne_plot_similar_words(keys, embeddings_en_2d, word_clusters)</code> </pre> <br><br>  Em alguns casos, pode ser √∫til plotar todos os vetores de palavras de uma s√≥ vez para ver a figura inteira.  Vamos agora analisar Anna Karenina, um romance √©pico de paix√£o, intriga, trag√©dia e reden√ß√£o. <br><br><pre> <code class="python hljs">prepare_for_w2v(<span class="hljs-string"><span class="hljs-string">'data/Anna Karenina by Leo Tolstoy (ru).txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'train_anna_karenina_ru.txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'russian'</span></span>) model_ak = train_word2vec(<span class="hljs-string"><span class="hljs-string">'train_anna_karenina_ru.txt'</span></span>) words = [] embeddings = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> list(model_ak.wv.vocab): embeddings.append(model_ak.wv[word]) words.append(word) tsne_ak_2d = TSNE(n_components=<span class="hljs-number"><span class="hljs-number">2</span></span>, init=<span class="hljs-string"><span class="hljs-string">'pca'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">3500</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">32</span></span>) embeddings_ak_2d = tsne_ak_2d.fit_transform(embeddings)</code> </pre><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tsne_plot_2d</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(label, embeddings, words=[], a=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">16</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) colors = cm.rainbow(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) x = embeddings[:,<span class="hljs-number"><span class="hljs-number">0</span></span>] y = embeddings[:,<span class="hljs-number"><span class="hljs-number">1</span></span>] plt.scatter(x, y, c=colors, alpha=a, label=label) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(words): plt.annotate(word, alpha=<span class="hljs-number"><span class="hljs-number">0.3</span></span>, xy=(x[i], y[i]), xytext=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), textcoords=<span class="hljs-string"><span class="hljs-string">'offset points'</span></span>, ha=<span class="hljs-string"><span class="hljs-string">'right'</span></span>, va=<span class="hljs-string"><span class="hljs-string">'bottom'</span></span>, size=<span class="hljs-number"><span class="hljs-number">10</span></span>) plt.legend(loc=<span class="hljs-number"><span class="hljs-number">4</span></span>) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) plt.savefig(<span class="hljs-string"><span class="hljs-string">"hhh.png"</span></span>, format=<span class="hljs-string"><span class="hljs-string">'png'</span></span>, dpi=<span class="hljs-number"><span class="hljs-number">150</span></span>, bbox_inches=<span class="hljs-string"><span class="hljs-string">'tight'</span></span>) plt.show() tsne_plot_2d(<span class="hljs-string"><span class="hljs-string">'Anna Karenina by Leo Tolstoy'</span></span>, embeddings_ak_2d, a=<span class="hljs-number"><span class="hljs-number">0.1</span></span>)</code> </pre> <br><img src="https://habrastorage.org/webt/j5/hn/82/j5hn8285nih2kwlop_badd2lzlk.png"><br><br><img src="https://habrastorage.org/webt/x6/jc/i7/x6jci7vka7-efczouqxpiqueisq.png"><br>  <i>Fig.</i>  <i>3. Visualiza√ß√£o do modelo Word2Vec treinado em Anna Karenina.</i> <br><br>  A imagem inteira pode ser ainda mais informativa se mapearmos as incorpora√ß√µes iniciais no espa√ßo 3D.  Neste momento, vamos dar uma olhada em Guerra e paz, uma das novelas vitais da literatura mundial e uma das maiores realiza√ß√µes liter√°rias de Tolstoi. <br><br><pre> <code class="python hljs">prepare_for_w2v(<span class="hljs-string"><span class="hljs-string">'data/War and Peace by Leo Tolstoy (ru).txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'train_war_and_peace_ru.txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'russian'</span></span>) model_wp = train_word2vec(<span class="hljs-string"><span class="hljs-string">'train_war_and_peace_ru.txt'</span></span>) words_wp = [] embeddings_wp = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> list(model_wp.wv.vocab): embeddings_wp.append(model_wp.wv[word]) words_wp.append(word) tsne_wp_3d = TSNE(perplexity=<span class="hljs-number"><span class="hljs-number">30</span></span>, n_components=<span class="hljs-number"><span class="hljs-number">3</span></span>, init=<span class="hljs-string"><span class="hljs-string">'pca'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">3500</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">12</span></span>) embeddings_wp_3d = tsne_wp_3d.fit_transform(embeddings_wp)</code> </pre><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mpl_toolkits.mplot3d <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Axes3D <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tsne_plot_3d</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(title, label, embeddings, a=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> fig = plt.figure() ax = Axes3D(fig) colors = cm.rainbow(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) plt.scatter(embeddings[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], embeddings[:, <span class="hljs-number"><span class="hljs-number">1</span></span>], embeddings[:, <span class="hljs-number"><span class="hljs-number">2</span></span>], c=colors, alpha=a, label=label) plt.legend(loc=<span class="hljs-number"><span class="hljs-number">4</span></span>) plt.title(title) plt.show() tsne_plot_3d(<span class="hljs-string"><span class="hljs-string">'Visualizing Embeddings using t-SNE'</span></span>, <span class="hljs-string"><span class="hljs-string">'War and Peace'</span></span>, embeddings_wp_3d, a=<span class="hljs-number"><span class="hljs-number">0.1</span></span>)</code> </pre> <br><img src="https://habrastorage.org/webt/ch/jm/os/chjmos082qn6ktw9afdeavbz6_a.png"><br>  <i>Fig.</i>  <i>4. Visualiza√ß√£o do modelo Word2Vec treinado em Guerra e Paz.</i> <br><br><h1>  Os resultados </h1><br>  √â assim que os textos se parecem com as perspectivas para Word2Vec e t-SNE.  Tra√ßamos um gr√°fico bastante informativo para palavras semelhantes do Google Not√≠cias e dois diagramas para os romances de Tolstoi.  Al√©m disso, mais uma coisa, GIFs!  Os GIFs s√£o impressionantes, mas plotar GIFs √© quase o mesmo que plotar gr√°ficos regulares.  Decidi n√£o mencion√°-las no artigo, mas voc√™ pode encontrar o c√≥digo para a gera√ß√£o de anima√ß√µes nas fontes. <br><br>  O c√≥digo fonte est√° dispon√≠vel no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Github</a> . <br><br>  O artigo foi publicado originalmente na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Towards Data Science</a> . <br><br><h1>  Refer√™ncias </h1><br><ol><li>  L. Maate e G. Hinton, "Visualizando dados usando t-SNE", Journal of Machine Learning Research, vol.  9, pp.  2579-2605, 2008. </li><li>  T. Mikolov, I. Sutskever, K. Chen, G. Corrado e J. Dean, "Representa√ß√µes distribu√≠das de palavras e frases e sua composicionalidade", Avan√ßos em sistemas de processamento de informa√ß√µes neurais, pp.  3111-3119, 2013. </li><li>  R. Rehurek e P. Sojka, ‚ÄúEstrutura de Software para Modelagem de T√≥picos com Grandes Corpora‚Äù, Anais do Workshop LREC 2010 sobre Novos Desafios para Estruturas de PNL, 2010. </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt449984/">https://habr.com/ru/post/pt449984/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt449970/index.html">Poste com a bandeira preta ou Como eu n√£o coloquei seu curso em v√≠deo no rastreador</a></li>
<li><a href="../pt449972/index.html">Como injetar rapidamente pools no upstream?</a></li>
<li><a href="../pt449974/index.html">Netramesh - solu√ß√£o leve de malha de servi√ßo</a></li>
<li><a href="../pt449976/index.html">Cont√™ineres associativos multithread em C ++. Relat√≥rio Yandex</a></li>
<li><a href="../pt449978/index.html">Igor Antarov do Moscow Tesla Club luta contra 20 mitos sobre Tesla e carros el√©tricos</a></li>
<li><a href="../pt449986/index.html">Blockchain: o que devemos construir um caso?</a></li>
<li><a href="../pt449990/index.html">Como fazer amigos de l√°tex, f√≥rmulas e Habr?</a></li>
<li><a href="../pt449992/index.html">Vitrine do modelo de driver simples (SDM) do NodeMCU: interface de usu√°rio din√¢mica</a></li>
<li><a href="../pt449994/index.html">As oito regras de ouro de Schneiderman o ajudar√£o a criar uma interface melhor</a></li>
<li><a href="../pt449996/index.html">Compreendendo o algoritmo da FFT</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>