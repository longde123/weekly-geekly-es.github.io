<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘¨ğŸ¾â€âš•ï¸ ğŸ‘§ğŸ½ ğŸ¤ GUNAKAN, MERAH, PgBouncer, pengaturan dan pemantauannya ğŸ¦ ğŸ™ğŸ¿ ğŸ‘†ğŸ¿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Kami mulai memperbarui pemantauan untuk PgBouncer di layanan kami dan memutuskan untuk menyisir semuanya sedikit. Agar semuanya cocok, kami menggunaka...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>GUNAKAN, MERAH, PgBouncer, pengaturan dan pemantauannya</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/okmeter/blog/420429/"><img align="left" width="359" src="https://habrastorage.org/webt/l4/xy/iz/l4xyize9ztzrmf303fot3iylnc8.png" alt="Pgbouncer GUNAKAN MERAH"><br><p>  Kami mulai memperbarui pemantauan untuk PgBouncer di layanan kami dan memutuskan untuk menyisir semuanya sedikit.  Agar semuanya cocok, kami menggunakan metodologi pemantauan kinerja paling terkenal: USE (Utilization, Saturation, Errors) oleh Brendan Gregg dan RED (Permintaan, Kesalahan, Durasi) dari Tom Wilkie. </p><br><p>  Di bawah cutscene adalah cerita dengan grafik tentang bagaimana pgbouncer bekerja, konfigurasi apa yang dipegangnya dan bagaimana, menggunakan USE / RED, untuk memilih metrik yang tepat untuk memonitornya. </p><a name="habracut"></a><br><h2 id="snachala-pro-sami-metody">  Pertama tentang metode itu sendiri </h2><br><p>  Meskipun metode ini cukup terkenal (tentang <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">mereka itu sudah ada di HabrÃ©, meskipun tidak dengan sangat rinci</a> ), bukan bahwa mereka tersebar luas dalam praktik. </p><br><h3 id="use">  GUNAKAN </h3><br><blockquote>  Untuk setiap sumber daya, catat pembuangan, saturasi, dan kesalahan. <br>  Brendan gregg </blockquote><p>  Di sini, <strong>sumber daya</strong> adalah komponen fisik yang terpisah - CPU, disk, bus, dll.  Tetapi tidak hanya - kinerja beberapa sumber daya perangkat lunak juga dapat dipertimbangkan dengan metode ini, khususnya sumber daya virtual, seperti wadah / cgroup dengan batas, juga nyaman untuk mempertimbangkan ini. </p><br><p> <strong>U - Disposal</strong> : baik persentase waktu (dari interval pengamatan) ketika sumber daya sibuk dengan pekerjaan yang bermanfaat.  Seperti, misalnya, memuat penggunaan CPU atau disk 90% berarti bahwa 90% dari waktu diambil oleh sesuatu yang bermanfaat) atau, untuk sumber daya seperti memori, ini adalah persentase memori yang digunakan. </p><br><p>  Bagaimanapun, 100% daur ulang berarti bahwa sumber daya tidak dapat digunakan lebih dari sekarang.  Dan salah satu pekerjaan akan macet menunggu rilis / pergi ke antrian, atau akan ada kesalahan.  Dua skenario ini dicakup oleh dua metrik USE yang sesuai: </p><br><p>  <strong>S - Saturasi</strong> , itu juga saturasi: ukuran jumlah "ditangguhkan" / bekerja antri. </p><br><p>  <strong>E - Kesalahan</strong> : kami hanya menghitung jumlah kegagalan.  Kesalahan / kegagalan memengaruhi kinerja, tetapi mungkin tidak segera terlihat karena mengambil operasi yang dibalik atau mekanisme toleransi kesalahan dengan perangkat cadangan, dll. </p><br><h3 id="red">  Merah </h3><br><p>  Tom Wilkie (sekarang bekerja di Grafana Labs) merasa frustrasi dengan metodologi USE, atau lebih tepatnya, penerapannya yang buruk dalam beberapa kasus dan tidak konsisten dengan praktik.  Bagaimana, misalnya, untuk mengukur saturasi memori?  Atau bagaimana mengukur kesalahan bus sistem dalam praktiknya? </p><br><blockquote>  Linux, ternyata, benar-benar melaporkan jumlah bug. <br>  T. Wilkie </blockquote><p>  Singkatnya, untuk memantau kinerja dan perilaku layanan mikro, ia mengusulkan metode lain yang cocok: untuk mengukur, sekali lagi, tiga indikator: </p><br><p>  <strong>R - Rate</strong> : Jumlah permintaan per detik. <br>  <strong>E - Kesalahan</strong> : berapa banyak permintaan mengembalikan kesalahan. <br>  <strong>D - Durasi</strong> : waktu yang dibutuhkan untuk memproses permintaan.  Ini adalah latensi, "latensi" (Â© Sveta Smirnova :), waktu respons, dll. </p><br><p>  Secara umum, USE lebih cocok untuk memantau sumber daya, dan RED untuk layanan dan beban kerja / payload mereka. </p><br><h2 id="pgbouncer">  Pgbouncer </h2><br><p>  Menjadi layanan, pada saat yang sama ia memiliki segala macam batasan dan sumber daya internal.  Hal yang sama dapat dikatakan tentang Postgres, yang diakses oleh klien melalui PgBouncer ini.  Oleh karena itu, untuk pemantauan penuh dalam situasi ini, kedua metode diperlukan. </p><br><p>  Untuk memahami cara menerapkan metode ini ke penjaga, Anda harus memahami detail perangkatnya.  Tidak cukup untuk memantaunya sebagai kotak hitam - "apakah proses pgbouncer hidup" atau "adalah port terbuka", karena  jika ada masalah, ini tidak akan memberikan pemahaman tentang apa sebenarnya dan bagaimana itu pecah dan apa yang harus dilakukan. </p><br><p>  Apa yang secara umum terlihat seperti apa PgBouncer dari sudut pandang klien: </p><br><ol><li>  klien terhubung </li><li>  [klien mengajukan permintaan - menerima tanggapan] x berapa kali ia membutuhkan </li></ol><br><p>  Di sini saya telah menggambar diagram status klien yang sesuai dari sudut pandang PgBoucer: <br><img src="https://habrastorage.org/webt/zb/mh/ot/zbmhotvidvuxnsbzp04-bqtgqhk.jpeg"></p><br><p> Dalam proses login, otorisasi dapat terjadi baik secara lokal (file, sertifikat, dan bahkan PAM dan hba dari versi baru), dan dari jarak jauh - yaitu:  di dalam basis data itu sendiri di mana koneksi sedang dicoba.  Dengan demikian, status masuk memiliki substate tambahan.  Mari kita sebut <code>Executing</code> untuk menunjukkan bahwa <code>auth_query</code> sedang <code>auth_query</code> di database saat ini: <br><img src="https://habrastorage.org/webt/e4/ib/pb/e4ibpbf6ef5q9xcdy2fantydkc4.png"></p><br><p>  Tetapi koneksi klien ini benar-benar cocok dengan koneksi backend / hulu yang dibuka PgBouncer di dalam kumpulan dan memegang jumlah terbatas.  Dan mereka memberikan koneksi seperti itu kepada klien hanya untuk waktu - selama sesi, transaksi atau permintaan, tergantung pada jenis pooling (ditentukan oleh pengaturan <code>pool_mode</code> ).  Paling sering, pooling transaksi digunakan (kita akan membahasnya nanti) - ketika koneksi dikeluarkan ke klien untuk satu transaksi, dan sisa waktu klien sebenarnya tidak terhubung ke server.  Dengan demikian, keadaan "aktif" klien memberi tahu kami sedikit, dan kami akan membaginya menjadi substrat: <br><img src="https://habrastorage.org/webt/uv/q4/tj/uvq4tjzbbauunpwzhboxc8s3pgu.png"></p><br><p>  Setiap klien tersebut termasuk dalam kelompok koneksi sendiri, yang akan dikeluarkan untuk digunakan oleh koneksi nyata ke Postgres.  Ini adalah tugas utama PgBouncer - untuk membatasi jumlah koneksi ke Postgres. </p><br><p>  Karena koneksi server terbatas, suatu situasi dapat muncul ketika klien perlu memenuhi permintaan secara langsung, tetapi tidak ada koneksi gratis sekarang.  Kemudian klien diantrekan dan koneksinya masuk ke status <code>CL_WAITING</code> .  Dengan demikian, diagram keadaan harus dilengkapi: <br><img src="https://habrastorage.org/webt/2v/ny/yu/2vnyyuhlqher6cc5q5izwjtu7te.png"><br>  Karena ini dapat terjadi dalam kasus ketika klien hanya masuk dan dia perlu menjalankan permintaan otorisasi, negara <code>CL_WAITING_LOGIN</code> juga <code>CL_WAITING_LOGIN</code> . </p><br><p>  Jika sekarang kita melihat dari sisi belakang - dari sisi koneksi server, maka, mereka berada dalam kondisi seperti itu: ketika otorisasi terjadi segera setelah koneksi - <code>SV_LOGIN</code> , dikeluarkan dan (mungkin) digunakan oleh klien - <code>SV_ACTIVE</code> , atau bebas - <code>SV_IDLE</code> . </p><br><h2 id="use-dlya-pgbouncer">  GUNAKAN untuk PgBouncer </h2><br><p>  Jadi kita sampai pada (versi naif) Pemanfaatan kumpulan spesifik: </p><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">Pool</span></span> utiliz =    /  </code> </pre> <br><p>  PgBouncer memiliki database utilitas pgbouncer khusus di mana ada <code>SHOW POOLS</code> yang menampilkan status saat ini dari koneksi setiap kumpulan: <br><img src="https://habrastorage.org/webt/xz/_o/eb/xz_oebvf-0yahuvdfzrxx3iass0.png"><br>  Ada 4 koneksi klien terbuka dan semuanya <code>cl_active</code> .  Dari 5 koneksi server - 4 <code>sv_active</code> dan satu di status baru <code>sv_used</code> . </p><br><div class="spoiler">  <b class="spoiler_title">Apa yang benar-benar digunakan tentang pengaturan berbeda pgbouncer yang tidak terkait dengan pemantauan</b> <div class="spoiler_text"><p>  Jadi <code>sv_used</code> tidak berarti "koneksi sedang digunakan", seperti yang mungkin Anda pikirkan, tetapi "koneksi pernah digunakan dan tidak pernah digunakan untuk waktu yang lama".  Faktanya adalah bahwa PgBouncer menggunakan koneksi server dalam mode LIFO secara default - mis.  Pertama, koneksi yang baru dirilis digunakan, kemudian yang baru digunakan, dll.  secara bertahap pindah ke senyawa yang sudah lama digunakan.  Dengan demikian, koneksi server dari dasar tumpukan seperti itu dapat "memburuk".  Dan mereka harus diperiksa untuk keaktifan sebelum digunakan, yang dilakukan menggunakan <code>server_check_query</code> , ketika mereka sedang diperiksa, keadaan akan <code>sv_tested</code> . </p><br><p>  Dokumentasi mengatakan bahwa LIFO diaktifkan secara default, sebagai  kemudian "sejumlah kecil koneksi mendapat beban kerja paling banyak. Dan ini memberikan kinerja terbaik ketika ada satu server yang melayani database di belakang pgbouncer", yaitu.  seolah-olah dalam kasus yang paling khas.  Saya percaya bahwa peningkatan kinerja potensial adalah karena penghematan dalam beralih kinerja antara beberapa proses backend.  Tapi itu tidak berhasil, karena  Detail implementasi ini telah ada selama&gt; 12 tahun dan melampaui sejarah komit di github dan kedalaman minat saya =) </p><br><p>  Jadi, sepertinya aneh dan <code>server_check_delay</code> dengan kenyataan saat ini bahwa nilai default dari pengaturan <code>server_check_delay</code> , yang menentukan bahwa server tidak digunakan terlalu lama dan harus diperiksa sebelum memberikannya kepada klien, adalah 30 detik.  Ini terlepas dari kenyataan bahwa secara default tcp_keepalive diaktifkan secara bersamaan dengan pengaturan default - mulailah memeriksa koneksi tetap hidup dengan sampel 2 jam setelah idle'ing. <br>  Ternyata dalam situasi ledakan / lonjakan koneksi klien yang ingin melakukan sesuatu di server, penundaan tambahan diperkenalkan pada <code>server_check_query</code> , yang, meskipun " <code>SELECT 1;</code> mungkin masih membutuhkan ~ 100 mikrodetik, dan jika <code>server_check_query = ';'</code>  maka Anda dapat menyimpan ~ 30 microseconds =) </p><br><p>  Tetapi asumsi bahwa melakukan pekerjaan hanya dalam beberapa koneksi = pada beberapa proses postgres back-end "utama" akan lebih efisien, tampaknya meragukan bagi saya.  Informasi proses post cache pekerja cache (meta) tentang setiap tabel yang diakses dalam koneksi ini.  Jika Anda memiliki sejumlah besar tabel, maka relcache ini dapat tumbuh sangat banyak dan membutuhkan banyak memori, hingga pertukaran halaman dari proses 0_o.  Untuk mengatasinya, gunakan pengaturan <code>server_lifetime</code> (default adalah 1 jam), di mana koneksi server akan ditutup untuk rotasi.  Tetapi di sisi lain, ada pengaturan <code>server_round_robin</code> yang akan mengalihkan mode menggunakan koneksi dari LIFO ke FIFO, menyebarkan permintaan klien pada koneksi server lebih merata. </p></div></div><br><p>  <code>SHOW POOLS</code> mengambil metrik dari <code>SHOW POOLS</code> (oleh beberapa eksportir prometheus) kita dapat merencanakan negara-negara ini: </p><br><p><img src="https://habrastorage.org/webt/i8/8f/-x/i88f-xpwo_2hj7z6iq6qbnatsju.png"></p><br><p>  Tetapi untuk sampai pada pembuangan Anda perlu menjawab beberapa pertanyaan: </p><br><ul><li>  Berapa ukuran kolam? </li><li>  Bagaimana cara menghitung berapa banyak senyawa yang digunakan?  Dalam lelucon atau dalam waktu, rata-rata atau di puncak? </li></ul><br><h3 id="razmer-pula">  Ukuran kolam </h3><br><p>  Semuanya rumit di sini, seperti dalam kehidupan.  Secara total, sudah ada lima batas pengaturan di pbbouncer! </p><br><ul><li>  <code>pool_size</code> dapat diatur untuk setiap basis data.  Kumpulan terpisah dibuat untuk setiap pasangan pengguna / DB, yaitu  dari pengguna <em>tambahan</em> apa pun, Anda dapat membuat <code>pool_size</code> lain / pekerja Postgres.  Karena  jika <code>pool_size</code> tidak disetel, jatuh pada <code>default_pool_size</code> , yang defaultnya adalah 20, maka ternyata setiap pengguna yang memiliki hak untuk terhubung ke database (dan bekerja melalui pgbouncer) berpotensi dapat membuat 20 proses Postgres, yang tampaknya tidak banyak.  Tetapi jika Anda memiliki banyak pengguna berbeda dari basis data atau basis data itu sendiri, dan kumpulan tidak terdaftar dengan pengguna tetap, mis.  akan dibuat dengan cepat (dan kemudian dihapus oleh <code>autodb_idle_timeout</code> ), maka ini bisa berbahaya =) <br><blockquote>  Mungkin layak membiarkan <code>default_pool_size</code> kecil, hanya untuk setiap pemadam kebakaran. <br></blockquote></li><li>  <code>max_db_connections</code> - hanya perlu membatasi jumlah koneksi ke satu basis data, karena  jika tidak, perilaku klien yang buruk dapat membuat banyak proses backend / postgres.  Dan secara default di sini - tidak terbatas Â¯_ (ãƒ„) _ / Â¯ <br><blockquote>  Mungkin Anda harus mengubah <code>max_db_connections</code> default, misalnya, Anda dapat fokus pada <code>max_connections</code> Postgres Anda (secara default 100).  Tetapi jika Anda memiliki banyak PgBouncer ... <br></blockquote></li><li>  <code>reserve_pool_size</code> - sebenarnya, jika <code>pool_size</code> digunakan, maka PgBouncer dapat membuka beberapa koneksi lagi ke basis.  Seperti yang saya pahami, ini dilakukan untuk mengatasi lonjakan beban.  Kami akan kembali ke sini. </li><li>  <code>max_user_connections</code> - Ini, sebaliknya, adalah batas koneksi dari satu pengguna ke semua database, mis.  relevan jika Anda memiliki beberapa database dan mereka menggunakan pengguna yang sama. </li><li>  <code>max_client_conn</code> - berapa banyak koneksi klien yang akan diterima PgBouncer secara total.  Default, seperti biasa, memiliki arti yang sangat aneh - 100. Artinya,  diasumsikan bahwa jika lebih dari 100 klien tiba-tiba macet, maka mereka hanya perlu <code>reset</code> diam-diam di tingkat TCP dan <code>reset</code> (baik, dalam log, saya harus mengakui, ini akan menjadi "tidak ada lagi koneksi yang diizinkan (max_client_conn)"). <br><blockquote>  Mungkin bernilai membuat <code>max_client_conn &gt;&gt; SUM ( pool_size' )</code> , misalnya, 10 kali lebih banyak. <br></blockquote></li></ul><br><p>  Selain <code>SHOW POOLS</code> pgbouncer pseudo-base layanan juga menyediakan perintah <code>SHOW DATABASES</code> , yang menunjukkan batas yang sebenarnya diterapkan pada kumpulan tertentu: <br><img src="https://habrastorage.org/webt/1v/lv/4h/1vlv4hviyhxz1pbh9puc6xxim9o.jpeg"></p><br><h3 id="servernye-soedineniya">  Koneksi server </h3><br><p>  Sekali lagi - bagaimana mengukur berapa banyak senyawa yang digunakan? <br>  Dalam lelucon rata-rata / dalam puncak / waktu? </p><br><p>  Dalam praktiknya, cukup bermasalah untuk memantau penggunaan kolam oleh penjaga dengan alat yang tersebar luas  pgbouncer sendiri hanya memberikan gambaran sesaat, dan karena sering tidak melakukan survei, masih ada kemungkinan gambar yang salah karena pengambilan sampel.  Berikut adalah contoh nyata kapan, tergantung pada saat eksportir bekerja - pada awal menit atau pada akhir - gambar senyawa terbuka dan bekas berubah secara mendasar: </p><br><p><img src="https://habrastorage.org/webt/i3/ch/qb/i3chqbtvyp3p6nm0bayw62pf3hq.png"></p><br><p>  Di sini semua perubahan beban / penggunaan koneksi hanyalah sebuah fiksi, sebuah artefak dari restart kolektor statistik.  Di sini Anda dapat melihat grafik koneksi di Postgres selama waktu ini dan deskriptor file bouncer dan PG - tidak ada perubahan: </p><br><p><img src="https://habrastorage.org/webt/n7/bh/4r/n7bh4r_2h21ypaxhtr7njv2u8xa.png"></p><br><p>  Kembali ke masalah pembuangan.  Kami memutuskan untuk menggunakan pendekatan gabungan dalam layanan kami - kami mencicipi <code>SHOW POOLS</code> sekali dalam satu detik, dan sekali dalam satu menit kami membuat rata-rata dan jumlah koneksi maksimum di setiap kelas: </p><br><p><img src="https://habrastorage.org/webt/ea/v5/ei/eav5eimcl7fofctvc24oaymzsu4.png"></p><br><p>  Dan jika kita membagi jumlah koneksi status aktif ini dengan ukuran pool, kita mendapatkan rata-rata dan puncak utilisasi pool ini dan dapat mengingatkan jika mendekati 100%. </p><br><p>  Selain itu, PgBouncer memiliki perintah <code>SHOW STATS</code> yang akan menampilkan statistik penggunaan untuk setiap basis data yang diproksi: <br><img src="https://habrastorage.org/webt/mo/wz/_q/mowz_qavy_zspkljbvnehbj1bpc.png"><br>  Kami paling tertarik pada kolom <code>total_query_time</code> - waktu yang dihabiskan oleh semua koneksi dalam proses mengeksekusi query di postgres.  Dan dari versi 1.8 ada juga <code>total_xact_time</code> metrik - waktu yang dihabiskan dalam transaksi.  Berdasarkan metrik ini, kita dapat membangun pemanfaatan waktu koneksi server, indikator ini tidak tunduk, berbeda dengan yang dihitung dari status koneksi, untuk masalah pengambilan sampel, karena  penghitung <code>total_..._time</code> ini bersifat kumulatif dan tidak lulus apa pun: </p><br><p><img src="https://habrastorage.org/webt/p3/1l/iv/p31liv1gccpj3rxnxav-nbelck0.png"><br>  Bandingkan <br><img src="https://habrastorage.org/webt/yk/rt/gd/ykrtgdu5s8_pcltl3w3mmcuj4oo.png"><br>  Dapat dilihat bahwa sampling tidak menunjukkan semua momen pemanfaatan tinggi ~ 100%, dan query_time menunjukkan. </p><br><h3 id="saturation-i-pgbouncer">  Kejenuhan dan PgBuncer </h3><br><p>  Mengapa Anda perlu memantau Saturasi, karena pemanfaatan yang tinggi sudah jelas bahwa semuanya buruk? </p><br><p>  Masalahnya adalah tidak peduli bagaimana Anda mengukur pemanfaatan, bahkan penghitung yang terakumulasi tidak dapat menunjukkan pemanfaatan sumber daya 100% lokal jika itu terjadi hanya pada interval yang sangat singkat.  Misalnya, Anda memiliki mahkota atau proses sinkron lainnya yang secara bersamaan dapat mulai membuat kueri ke database pada perintah.  Jika permintaan ini pendek, maka pemanfaatannya, diukur pada skala menit dan bahkan detik, mungkin rendah, tetapi pada saat yang sama, pada beberapa titik, permintaan ini terpaksa menunggu dalam antrean untuk dieksekusi.  Ini mirip dengan situasi penggunaan CPU yang tidak 100% dan waktu prosesor yang mirip rata-rata yang tinggi masih ada, tetapi banyak proses yang menunggu untuk dieksekusi. </p><br><p>  Bagaimana situasi ini dapat dipantau - yah, sekali lagi, kita bisa menghitung jumlah klien dalam status <code>cl_waiting</code> sesuai dengan <code>SHOW POOLS</code> .  Dalam situasi normal, ada nol, dan lebih dari nol berarti melimpah kumpulan ini: </p><br><p><img src="https://habrastorage.org/webt/q1/tg/tj/q1tgtjcqgrqpavfpkf0kyg31hjq.png"></p><br><p>  Masih ada masalah bahwa <code>SHOW POOLS</code> hanya dapat disampel, dan dalam situasi dengan mahkota sinkron atau sesuatu seperti itu, kita dapat dengan mudah melewati dan tidak melihat klien yang menunggu. </p><br><p>  Anda dapat menggunakan trik ini, pgbouncer sendiri dapat mendeteksi 100% penggunaan pool dan membuka pool cadangan.  Dua pengaturan bertanggung jawab untuk ini: <code>reserve_pool_size</code> - untuk ukurannya, seperti yang saya katakan, dan <code>reserve_pool_timeout</code> - berapa detik beberapa klien harus <code>waiting</code> sebelum menggunakan kumpulan cadangan.  Jadi, jika kita melihat pada grafik koneksi server bahwa jumlah koneksi yang terbuka ke Postgres lebih besar dari pool_size, maka ada saturasi dari pool, seperti ini: <br><img src="https://habrastorage.org/webt/ne/ec/hn/neechnkp9sffd8g3rjov_3jjijm.png"><br>  Jelas, sesuatu seperti mahkota sekali dalam satu jam membuat banyak permintaan dan benar-benar menempati kolam.  Dan meskipun kita tidak melihat saat ketika koneksi <code>active</code> melebihi batas <code>pool_size</code> , masih pgbouncer terpaksa membuka koneksi tambahan. </p><br><p>  Juga pada grafik ini, pengaturan <code>server_idle_timeout</code> bekerja terlihat jelas - setelah berapa banyak berhenti memegang dan menutup koneksi yang tidak digunakan.  Secara default, ini adalah 10 menit, yang kita lihat di grafik - setelah puncak <code>active</code> tepat jam 5:00, jam 6:00, dll.  (sesuai dengan cron <code>0 * * * *</code> ), koneksi hang + <code>used</code> 10 menit dan ditutup. </p><br><p>  Jika Anda hidup di garis depan kemajuan dan telah memperbarui PgBouncer selama 9 bulan terakhir, Anda dapat menemukan di kolom <code>SHOW STATS</code> <code>total_wait_time</code> , yang menunjukkan kejenuhan terbaik, karena  secara kumulatif mempertimbangkan waktu yang dihabiskan oleh pelanggan dalam keadaan <code>waiting</code> .  Misalnya, di sini - <code>waiting</code> muncul pukul 16:30: <br><img src="https://habrastorage.org/webt/ck/-q/qt/ck-qqth3dhvsqsw1zzeuvkjg4wa.png"><br>  Dan <code>wait_time</code> , yang sebanding dan jelas mempengaruhi <code>average query time</code> , dapat dilihat dari 15:15 hingga hampir 19: <br><img src="https://habrastorage.org/webt/gn/av/bb/gnavbbmcfchzhkt0d0egcybthzc.png"></p><br><p>  Meskipun demikian, pemantauan status koneksi klien masih sangat bermanfaat, karena  ini memungkinkan Anda untuk mengetahui tidak hanya fakta bahwa semua koneksi ke database seperti itu telah dihabiskan dan klien harus menunggu, tetapi juga karena <code>SHOW POOLS</code> dibagi menjadi kumpulan yang terpisah oleh pengguna, dan <code>SHOW STATS</code> tidak, itu memungkinkan Anda untuk mengetahui klien mana yang menggunakan semua koneksi. ke basis yang ditentukan - sesuai dengan kolom <code>sv_active</code> dari kumpulan yang sesuai.  Atau dengan metrik </p><br><pre> <code class="hljs pgsql">sum_by(<span class="hljs-keyword"><span class="hljs-keyword">user</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">database</span></span>, metric(<span class="hljs-type"><span class="hljs-type">name</span></span>="pgbouncer.clients.count", state="active-link")):</code> </pre> <br><p><img src="https://habrastorage.org/webt/k_/0s/lf/k_0slfupzfrdz4npoiz8kbxgpko.png"></p><br><p>  Kami di okmeter melangkah lebih jauh dan menambahkan rincian koneksi yang digunakan oleh alamat IP klien yang membuka dan menggunakannya.  Ini memungkinkan Anda untuk memahami secara tepat instance aplikasi mana yang berperilaku berbeda: <br><img src="https://habrastorage.org/webt/dk/fr/5j/dkfr5j_yvxgmm2f0b5dvru4b9nk.png"><br>  Di sini kita melihat IP kubernet perapian tertentu yang perlu kita tangani. </p><br><h3 id="errors">  Kesalahan </h3><br><p>  Tidak ada yang rumit di sini: pgbouncer menulis log yang melaporkan kesalahan jika batas koneksi klien tercapai, batas waktu untuk terhubung ke server, dll.  Kami belum mencapai log pgbouncer sendiri :( </p><br><h2 id="red-dlya-pgbouncer">  MERAH untuk PgBouncer </h2><br><p>  Sementara USE lebih fokus pada kinerja, dalam arti kemacetan, RED, menurut pendapat saya, lebih tentang karakteristik lalu lintas masuk dan keluar secara umum, dan bukan tentang kemacetan.  Yaitu, RED menjawab pertanyaan - apakah semuanya berfungsi dengan baik, dan jika tidak, maka USE akan membantu untuk memahami apa masalahnya. </p><br><h2 id="requests">  Persyaratan </h2><br><p>  Tampaknya semuanya cukup sederhana untuk database SQL dan untuk penarik proksi / koneksi dalam database seperti itu - klien menjalankan pernyataan SQL, yang merupakan Permintaan.  Dari <code>SHOW STATS</code> kami mengambil <code>total_requests</code> dan merencanakan turunan waktunya </p><br><pre> <code class="hljs lisp">rate(<span class="hljs-name"><span class="hljs-name">metric</span></span>(<span class="hljs-name"><span class="hljs-name">name=</span></span><span class="hljs-string"><span class="hljs-string">"pgbouncer.total_requests"</span></span>, database: <span class="hljs-string"><span class="hljs-string">"*"</span></span>))</code> </pre> <br><p><img src="https://habrastorage.org/webt/fa/7u/o2/fa7uo2r8b6_4y6z9e2bounudzmw.png"></p><br><p>  Tetapi pada kenyataannya ada berbagai cara menarik, dan yang paling umum adalah transaksi.  Unit kerja untuk mode ini adalah transaksi, bukan permintaan.  Dengan demikian, mulai dari versi 1.8, Pgbouner sudah menyediakan dua statistik lain - <code>total_query_count</code> , bukan <code>total_requests</code> , dan <code>total_xact_count</code> - jumlah transaksi yang diselesaikan. </p><br><p>  Sekarang beban kerja dapat dikarakterisasi tidak hanya dalam hal jumlah permintaan / transaksi yang diselesaikan, tetapi, misalnya, Anda dapat melihat jumlah rata-rata permintaan per transaksi dalam basis data yang berbeda, membagi satu menjadi yang lain </p><br><pre> <code class="hljs lisp">rate(<span class="hljs-name"><span class="hljs-name">metric</span></span>(<span class="hljs-name"><span class="hljs-name">name=</span></span><span class="hljs-string"><span class="hljs-string">"total_requests"</span></span>, database=<span class="hljs-string"><span class="hljs-string">"*"</span></span>)) / rate(<span class="hljs-name"><span class="hljs-name">metric</span></span>(<span class="hljs-name"><span class="hljs-name">name=</span></span><span class="hljs-string"><span class="hljs-string">"total_xact"</span></span>, database=<span class="hljs-string"><span class="hljs-string">"*"</span></span>))</code> </pre> <br><p><img src="https://habrastorage.org/webt/xd/uh/3w/xduh3wsb-bww1tysfwtdhcw-seg.png"></p><br><p>  Di sini kita melihat perubahan nyata pada profil pemuatan, yang mungkin menjadi alasan untuk perubahan kinerja.  Dan jika Anda hanya melihat pada tingkat transaksi atau permintaan, Anda mungkin tidak melihat ini. </p><br><h2 id="red-errors">  Kesalahan MERAH </h2><br><p>  Jelas bahwa RED dan USE berpotongan pada pemantauan kesalahan, tetapi bagi saya tampaknya kesalahan dalam USE terutama tentang kesalahan pemrosesan permintaan karena pemanfaatan 100%, yaitu.  ketika layanan menolak untuk menerima lebih banyak pekerjaan.  Dan kesalahan untuk RED akan lebih baik untuk mengukur kesalahan secara tepat dari sudut pandang klien, permintaan klien.  Yaitu, tidak hanya dalam situasi di mana kumpulan di PgBouncer penuh atau batas lain telah berfungsi, tetapi juga ketika meminta batas waktu permintaan seperti "membatalkan pernyataan karena batas waktu pernyataan", membatalkan dan mengembalikan transaksi oleh klien sendiri telah bekerja, dll. e.  tingkat yang lebih tinggi, lebih dekat ke jenis kesalahan logika bisnis. </p><br><h2 id="durations">  Durasi </h2><br><p>  Di sini lagi <code>SHOW STATS</code> dengan penghitung kumulatif <code>total_xact_time</code> , <code>total_query_time</code> dan <code>total_wait_time</code> akan membantu kami, membaginya dengan jumlah permintaan dan transaksi, masing-masing, kami mendapatkan waktu permintaan rata-rata, waktu transaksi rata-rata, waktu tunggu rata-rata per transaksi.  Saya sudah menunjukkan grafik tentang yang pertama dan ketiga: <br><img src="https://habrastorage.org/webt/gn/av/bb/gnavbbmcfchzhkt0d0egcybthzc.png"></p><br><p>  Apa lagi yang bisa Anda lakukan agar keren?  Antipattern terkenal dalam bekerja dengan database dan Postgres, khususnya, ketika aplikasi membuka transaksi, membuat permintaan, kemudian mulai (untuk waktu yang lama) untuk memproses hasilnya, atau bahkan lebih buruk - pergi ke beberapa layanan / database lain dan membuat permintaan di sana.  Selama ini, transaksi "hang" di postgres terbuka, layanan kemudian kembali dan membuat beberapa permintaan lagi, pembaruan dalam database, dan baru kemudian menutup transaksi.  Untuk postgres, ini sangat tidak menyenangkan, karena  pekerja pg itu mahal.  Jadi kita dapat memonitor ketika aplikasi seperti itu <code>idle in transaction</code> di postgres itu sendiri - sesuai dengan kolom <code>state</code> di <code>pg_stat_activity</code> , tetapi masih ada masalah yang dijelaskan dengan sampling, karena  <code>pg_stat_activity</code> hanya memberikan gambar saat ini.  Di PgBouncer, kita dapat mengurangi waktu yang dihabiskan oleh klien dalam <code>total_query_time</code> permintaan dari waktu yang dihabiskan dalam transaksi <code>total_xact_time</code> - ini akan menjadi waktu pemalasan seperti itu.  Jika hasilnya masih dibagi dengan <code>total_xact_time</code> , maka itu akan dinormalisasi: nilai 1 sesuai dengan situasi di mana klien <code>idle in transaction</code> 100% dari waktu.  Dan dengan normalisasi seperti itu, membuatnya mudah untuk memahami seberapa buruk semuanya: </p><br><p><img src="https://habrastorage.org/webt/t9/kn/io/t9knioh2ckzd_photgqvcq543x4.png"></p><br><p>  Selain itu, kembali ke Durasi, metrik <code>total_xact_time - total_query_time</code> dapat dibagi dengan jumlah transaksi untuk melihat berapa rata-rata aplikasi idle per transaksi. </p><br><hr><br><p>  Menurut pendapat saya, metode USE / RED paling berguna untuk menyusun metrik yang Anda potret dan alasannya.  Karena kami terlibat dalam pemantauan penuh waktu dan kami harus melakukan pemantauan untuk berbagai komponen infrastruktur, metode ini membantu kami untuk mengambil metrik yang benar, membuat jadwal dan pemicu yang tepat untuk klien kami. </p><br><p>  <em>Pemantauan yang baik tidak dapat dilakukan segera, ini merupakan proses berulang.</em>  <em>Di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">okmeter.io,</a> kami baru saja memonitor secara terus-menerus (ada banyak hal, tapi besok akan lebih baik dan lebih rinci :)</em> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id420429/">https://habr.com/ru/post/id420429/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id420413/index.html">SQLite dan NW.js - petunjuk langkah demi langkah untuk menciptakan persahabatan yang kuat</a></li>
<li><a href="../id420415/index.html">Segala sesuatu yang ingin Anda ketahui tentang pengujian adaptor Wi-Fi, tetapi takut untuk bertanya</a></li>
<li><a href="../id420419/index.html">Pelari untuk mereka yang suka penghinaan atau bagaimana kami mengubah dan memodifikasi PixJam</a></li>
<li><a href="../id420423/index.html">Masalah antarmuka crossing darat</a></li>
<li><a href="../id420425/index.html">Teori dan praktik menggunakan HBase</a></li>
<li><a href="../id420431/index.html">Mars Panduan Praktis untuk Terraforming untuk Ibu Rumah Tangga</a></li>
<li><a href="../id420433/index.html">"Format Jumat": jalan musikal - apa itu dan mengapa mereka tidak ada di Rusia</a></li>
<li><a href="../id420435/index.html">Mulai cepat dengan ARM Mbed: pengembangan mikrokontroler modern untuk pemula</a></li>
<li><a href="../id420437/index.html">Pengantar praktis untuk manajer paket untuk Kubernetes - Helm</a></li>
<li><a href="../id420439/index.html">Intisari Fintech: investasi dalam fintech mencapai $ 57 miliar, kecepatan transaksi meningkat, dan biaya turun</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>