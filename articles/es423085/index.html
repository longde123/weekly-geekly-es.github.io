<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®‚Äçüëß‚Äçüëß üêª üëÜüèæ Ajuste fino del equilibrio de carga üë®üèª‚Äçüéì üì£ üÜò</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Este art√≠culo se centrar√° en el equilibrio de carga en proyectos web. Muchos creen que la soluci√≥n a este problema es la distribuci√≥n de la carga entr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ajuste fino del equilibrio de carga</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/423085/">  Este art√≠culo se centrar√° en el equilibrio de carga en proyectos web.  Muchos creen que la soluci√≥n a este problema es la distribuci√≥n de la carga entre servidores: cuanto m√°s precisa, mejor.  Pero sabemos que esto no es del todo cierto.  <strong>La estabilidad del sistema es mucho m√°s importante desde el punto de vista comercial</strong> . <br><br><img src="https://habrastorage.org/webt/6i/vb/-w/6ivb-w0bzdgl_oa-hkep6luitfi.png"><br><br>  El peque√±o pico de minutos a 84 RPS de "quinientos" es de cinco mil errores que recibieron los usuarios reales.  Esto es mucho y es muy importante.  Es necesario buscar razones, trabajar en los errores e intentar continuar para evitar tales situaciones. <br><br>  <strong>Nikolay Sivko</strong> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" class="user_link">NikolaySivko</a> ) en su informe sobre RootConf 2018 habl√≥ sobre los aspectos sutiles y a√∫n no muy populares del equilibrio de carga: <br><br><ul><li>  cu√°ndo repetir la solicitud (reintentos); </li><li>  c√≥mo seleccionar valores para tiempos de espera; </li><li>  c√≥mo no matar los servidores subyacentes en el momento del accidente / congesti√≥n; </li><li>  si se necesitan controles de salud; </li><li>  C√≥mo manejar los problemas de parpadeo. </li></ul><br>  Bajo decodificaci√≥n de gato de este informe. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/2-j2ADWFkkE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><a name="habracut"></a><br>  <strong>Sobre el orador:</strong> Nikolay Sivko cofundador de okmeter.io.  Trabaj√≥ como administrador del sistema y l√≠der de un grupo de administradores.  Operaci√≥n supervisada en hh.ru.  Fund√≥ el servicio de monitoreo okmeter.io.  Como parte de este informe, el monitoreo de la experiencia de desarrollo es la principal fuente de casos. <br><br><h2>  ¬øDe qu√© vamos a hablar? <br></h2><br>  Este art√≠culo hablar√° sobre proyectos web.  A continuaci√≥n se muestra un ejemplo de producci√≥n en vivo: el gr√°fico muestra las solicitudes por segundo para un determinado servicio web. <br><br><img src="https://habrastorage.org/webt/oy/5c/qt/oy5cqtlz-halhw7y5ayuz6xl9lm.png"><br><br>  Cuando hablo sobre el equilibrio, muchos lo perciben como "necesitamos distribuir la carga entre los servidores; cuanto m√°s preciso, mejor". <br><br><img src="https://habrastorage.org/webt/pm/g2/sp/pmg2spartsnxrxcyhzi_4-ui64g.png"><br><br>  De hecho, esto no es del todo cierto.  Este problema es relevante para un n√∫mero muy peque√±o de empresas.  Con mayor frecuencia, las empresas est√°n preocupadas por los errores y la estabilidad del sistema. <br><br><img src="https://habrastorage.org/webt/6i/vb/-w/6ivb-w0bzdgl_oa-hkep6luitfi.png"><br><br>  El peque√±o pico en el gr√°fico es "quinientos", que el servidor devolvi√≥ en un minuto y luego se detuvo.  Desde el punto de vista de una empresa, como una tienda en l√≠nea, este peque√±o pico a 84 RPS de "quinientos" es 5040 errores para usuarios reales.  Algunos no encontraron algo en su cat√°logo, otros no pudieron poner los productos en la cesta.  Y esto es muy importante.  Aunque este pico no se ve muy grande en el gr√°fico, <strong>es mucho en usuarios reales</strong> . <br><br>  Como regla general, todos tienen esos picos, y los administradores no siempre responden a ellos.  Muy a menudo, cuando una empresa pregunta qu√© fue, le responden: <br><br><ul><li>  "Esta es una breve explosi√≥n!" </li><li>  "Es solo un lanzamiento rodante". </li><li>  "El servidor est√° muerto, pero todo ya est√° en orden". </li><li>  "Vasya cambi√≥ la red de uno de los servidores". </li></ul><br>  A menudo, las personas <strong>ni siquiera tratan de entender las razones</strong> por <strong>las</strong> que esto sucedi√≥, y no hacen ning√∫n trabajo posterior para que no vuelva a suceder. <br><br><h2>  Buena sinton√≠a <br></h2><br>  Llam√© al informe "Ajuste fino" (Ing. Ajuste fino), porque pens√© que no todos llegan a esta tarea, pero valdr√≠a la pena.  ¬øPor qu√© no llegan all√≠? <br><br><ul><li>  <strong>No todos llegan a esta tarea,</strong> porque cuando todo funciona, no es visible.  Esto es muy importante para los problemas.  Fakapa no ocurre todos los d√≠as, y un problema tan peque√±o requiere esfuerzos muy serios para resolverlo. </li><li>  <strong>Necesitas pensar mucho.</strong>  Muy a menudo, el administrador, la persona que ajusta el equilibrio, no puede resolver este problema de forma independiente.  A continuaci√≥n veremos por qu√©. </li><li>  <strong>Atrapa los niveles subyacentes.</strong>  Esta tarea est√° muy relacionada con el desarrollo, con la adopci√≥n de decisiones que afectan a su producto y a sus usuarios. </li></ul><br>  <strong>Afirmo que es hora de hacer esta tarea por varias razones:</strong> <br><br><ul><li>  El mundo est√° cambiando, volvi√©ndose m√°s din√°mico, hay muchos lanzamientos.  Dicen que ahora es correcto lanzar 100 veces al d√≠a, y el lanzamiento es el futuro fakap con una probabilidad de 50 a 50 (al igual que la probabilidad de encontrarse con un dinosaurio) </li><li>  Desde el punto de vista de la tecnolog√≠a, todo tambi√©n es muy din√°mico.  Kubernetes y otros orquestadores aparecieron.  No hay una buena implementaci√≥n anterior, cuando se apaga un backend en alguna IP, se actualiza una actualizaci√≥n y el servicio se activa.  Ahora, en el proceso de implementaci√≥n en k8s, la lista de IP ascendente est√° cambiando por completo. </li><li>  Microservicios: ahora todos se comunican a trav√©s de la red, lo que significa que debe hacerlo de manera confiable.  El equilibrio juega un papel importante. </li></ul><br><h2>  Banco de pruebas <br></h2><br>  Comencemos con casos simples y obvios.  Para mayor claridad, usar√© un banco de pruebas.  Esta es una aplicaci√≥n de Golang que proporciona http-200, o puede cambiarla al modo "give http-503". <br><br>  Comenzamos 3 instancias: <br><br><ul><li>  127.0.0.1:20001 </li><li>  127.0.0.1:20002 </li><li>  127.0.0.1:20003 </li></ul><br>  Servimos 100 rps a trav√©s de yandex.tank a trav√©s de nginx. <br><br>  Nginx fuera de la caja: <br><br><pre><code class="plaintext hljs">upstream backends { server 127.0.0.1:20001; server 127.0.0.1:20002; server 127.0.0.1:20003; } server { listen 127.0.0.1:30000; location / { proxy_pass http://backends; } }</code> </pre> <br><h3>  Escenario primitivo </h3><br>  En alg√∫n momento, encienda uno de los backends en el modo give 503, y obtenemos exactamente un tercio de los errores. <br><br><img src="https://habrastorage.org/webt/qp/m1/ro/qpm1rolcydmcpwpvule4pw97b-o.png"><br><br>  Est√° claro que nada funciona fuera de la caja: nginx no vuelve a intentarlo si recibi√≥ <strong>alguna respuesta</strong> del servidor. <br><br><pre> <code class="plaintext hljs">Nginx default: proxy_next_upstream error timeout;</code> </pre><br>  De hecho, esto es bastante l√≥gico desde el punto de vista de los desarrolladores de nginx: nginx no tiene derecho a decidir por usted qu√© desea retransmitir y qu√© no. <br><br>  Por consiguiente, necesitamos reintentos, reintentos, y comenzamos a hablar sobre ellos. <br><br><h2>  Reintentos <br></h2><br>  Es necesario encontrar un compromiso entre: <br><br><ul><li>  La solicitud del usuario es sagrada, hiere, pero responde.  Queremos responder al usuario a toda costa, el usuario es lo m√°s importante. </li><li>  Es mejor responder con un error que sobrecargar los servidores. </li><li>  Integridad de los datos (para solicitudes no idempotentes), es decir, es imposible repetir ciertos tipos de solicitudes. </li></ul><br>  <strong>La verdad, como de costumbre, est√° en alg√∫n punto intermedio:</strong> nos vemos obligados a equilibrar estos tres puntos.  Tratemos de entender qu√© y c√≥mo. <br><br>  Divid√≠ los intentos fallidos en 3 categor√≠as: <br><br>  1. <strong>Error de transporte</strong> <br>  Para el transporte HTTP es TCP y, por regla general, aqu√≠ hablamos de los errores de configuraci√≥n de conexi√≥n y los tiempos de espera de configuraci√≥n de conexi√≥n.  En mi informe, mencionar√© 3 balanceadores comunes (hablaremos un poco m√°s sobre Envoy): <br><br><ul><li>  <strong>nginx</strong> : errores + tiempo de espera (proxy_connect_timeout); </li><li>  <strong>HAProxy</strong> : tiempo de espera de conexi√≥n; </li><li>  <strong>Enviado</strong> : error de conexi√≥n + flujo rechazado. </li></ul><br>  Nginx tiene la oportunidad de decir que un intento fallido es un error de conexi√≥n y un tiempo de espera de conexi√≥n;  HAProxy tiene un tiempo de espera de conexi√≥n, Envoy tambi√©n tiene todo est√°ndar y normal. <br><br>  2. <strong>Solicitar tiempo de espera:</strong> <br>  Supongamos que enviamos una solicitud al servidor, que nos conectamos con √©xito, pero la respuesta no llega a nosotros, la esperamos y entendemos que no tiene sentido esperar m√°s.  Esto se llama tiempo de espera de solicitud: <br><br><ul><li>  <strong>Nginx</strong> tiene: tiempo de espera (prox_send_timeout * + proxy_read_timeout *); </li><li>  <strong>HAProxy tiene</strong> <strong>OOPS :(</strong> - no existe en principio. Muchas personas no saben que HAProxy, si ha establecido una conexi√≥n con √©xito, nunca intentar√° reenviar la solicitud. </li><li>  <strong>El enviado</strong> puede hacer todo: tiempo de espera ||  per_try_timeout. </li></ul><br>  3. <strong>estado HTTP</strong> <br>  Todos los equilibradores, a excepci√≥n de HAProxy, pueden procesar, sin embargo, el backend le respondi√≥, pero con alg√∫n tipo de c√≥digo err√≥neo. <br><br><ul><li>  <strong>nginx</strong> : http_ * </li><li>  <strong>HAProxy</strong> : <strong>OOPS :(</strong> </li><li>  <strong>Enviado</strong> : 5xx, error de puerta de enlace (502, 503, 504), recuperable-4xx (409) </li></ul><br><h3>  Tiempos de espera <br></h3><br>  Ahora hablemos en detalle sobre los tiempos de espera, me parece que vale la pena prestar atenci√≥n a esto.  No habr√° m√°s ciencia espacial: se trata simplemente de informaci√≥n estructurada sobre lo que generalmente sucede y c√≥mo se relaciona con √©l. <br><br><h4>  Tiempo de espera de conexi√≥n <br></h4><br>  El tiempo de espera de conexi√≥n es el momento de establecer una conexi√≥n.  Esta es una caracter√≠stica de su red y su servidor espec√≠fico, y no depende de la solicitud.  Por lo general, el valor predeterminado para el tiempo de espera de conexi√≥n se establece en peque√±o.  En todos los proxies, el valor predeterminado es lo suficientemente grande, y esto es incorrecto: deber√≠an ser <strong>unidades, a veces decenas de milisegundos</strong> (si estamos hablando de una red dentro de un DC). <br><br>  Si desea identificar servidores problem√°ticos un poco m√°s r√°pido que estas unidades (decenas de milisegundos), puede ajustar la carga en el backend configurando una peque√±a acumulaci√≥n para recibir conexiones TCP.  En este caso, puede, cuando el trabajo atrasado de la aplicaci√≥n est√© lleno, indicarle a Linux que lo reinicie para desbordar el trabajo atrasado.  Entonces podr√° disparar al backend sobrecargado "malo" un poco antes del tiempo de espera de conexi√≥n: <br><br><pre> <code class="plaintext hljs">fail fast: listen backlog + net.ipv4.tcp_abort_on_overflow</code> </pre> <br><h4>  Solicitar tiempo de espera <br></h4><br>  El tiempo de espera de la solicitud no es una caracter√≠stica de la red, sino una <strong>caracter√≠stica de un grupo de solicitudes</strong> (manejador).  Hay diferentes solicitudes: su gravedad es diferente, tienen una l√≥gica completamente diferente en su interior, necesitan acceder a repositorios completamente diferentes. <br><br>  Nginx en s√≠ <strong>no tiene un tiempo de espera para toda la solicitud.</strong>  El tiene: <br><br><ul><li>  proxy_send_timeout: tiempo entre dos operaciones de escritura exitosas write (); </li><li>  proxy_read_timeout: tiempo entre dos lecturas de lectura exitosas (). </li></ul><br>  Es decir, si tienes un backend lento, un byte de veces, da algo en un tiempo de espera, entonces todo est√° bien.  Como tal, nginx no tiene request_timeout.  Pero estamos hablando de aguas arriba.  En nuestro centro de datos son controlados por nosotros, por lo tanto, suponiendo que la red no tenga loris lentos, entonces, en principio, read_timeout puede usarse como request_timeout. <br><br>  El enviado lo tiene todo: tiempo de espera ||  per_try_timeout. <br><br><h4>  Seleccionar solicitud de tiempo de espera <br></h4><br>  Ahora, lo m√°s importante, en mi opini√≥n, es qu√© request_timeout poner.  Procedemos de cu√°nto est√° permitido que el usuario espere; este es un cierto m√°ximo.  Est√° claro que el usuario no esperar√° m√°s de 10 s, por lo que debe responderle m√°s r√°pido. <br><br><ul><li>  Si queremos manejar la falla de un solo servidor, entonces el tiempo de espera debe ser menor que el tiempo de espera m√°ximo permitido: <strong>request_timeout &lt;max.</strong> </li><li>  Si desea tener <strong>2 intentos garantizados para</strong> enviar una solicitud a dos backends diferentes, el tiempo de espera para un intento es igual a la mitad de este intervalo permitido: <strong>per_try_timeout = 0.5 * max.</strong> </li><li>  Tambi√©n hay una opci√≥n intermedia: <strong>2 intentos optimistas</strong> en caso de que el primer backend se haya "embotado", pero el segundo responder√° r√°pidamente: <strong>per_try_timeout = k * max (donde k&gt; 0.5).</strong> </li></ul><br>  Existen diferentes enfoques, pero en general, <strong>elegir un tiempo de espera es dif√≠cil</strong> .  Siempre habr√° casos l√≠mite, por ejemplo, el mismo controlador en el 99% de los casos se procesa en 10 ms, pero hay un 1% de los casos cuando esperamos 500 ms, y esto es normal.  Esto tendr√° que ser resuelto. <br><br>  Con este 1%, es necesario hacer algo, porque todo el grupo de solicitudes debe, por ejemplo, cumplir con el SLA y ajustarse en 100 ms.  Muy a menudo en estos momentos se procesa la solicitud: <br><br><ul><li>  La paginaci√≥n aparece en aquellos lugares donde es imposible devolver todos los datos en un tiempo de espera. </li><li>  El administrador / informes se separan en un grupo separado de URL para aumentar el tiempo de espera para ellos, y s√≠ para reducir las solicitudes de los usuarios. </li><li>  Reparamos / optimizamos aquellas solicitudes que no se ajustan a nuestro tiempo de espera. </li></ul><br>  Inmediatamente, debemos tomar una decisi√≥n que no sea muy simple desde el punto de vista psicol√≥gico de que si no tenemos tiempo para responder al usuario en el tiempo asignado, damos un error (es como en un antiguo dicho chino: "Si la yegua est√° muerta, ¬°b√°jese!") <strong>.</strong> <br><br>  Despu√©s de eso, el proceso de monitoreo de su servicio desde el punto de vista del usuario se simplifica: <br><br><ul><li>  Si hay errores, todo est√° mal, necesita ser reparado. </li><li>  Si no hay errores, encajamos en el tiempo de respuesta correcto, entonces todo est√° bien. </li></ul><br><h3>  Reintentos especulativos # nifig <br></h3><br>  Nos aseguramos de que elegir un valor de tiempo de espera sea bastante dif√≠cil.  Como sabes, para simplificar algo, necesitas complicar algo :) <br><br>  <strong>Retraso especulativo</strong> : una solicitud repetida a otro servidor, que se inicia por alguna condici√≥n, pero la primera solicitud no se interrumpe.  Tomamos la respuesta del servidor que respondi√≥ m√°s r√°pido. <br><br>  No vi esta caracter√≠stica en equilibradores que conozco, pero hay un excelente ejemplo con Cassandra (protecci√≥n de lectura r√°pida): <br><br>  speculative_retry = N ms |  <strong>Percentil M</strong> <br><br>  De esta manera <strong>no</strong> tienes <strong>que esperar</strong> .  Puede dejarlo en un nivel aceptable y, en cualquier caso, tener un segundo intento de obtener una respuesta a la solicitud. <br><br>  Cassandra tiene una oportunidad interesante para establecer un intento especulativo est√°tico o din√°mico, luego el segundo intento se realizar√° a trav√©s del percentil del tiempo de respuesta.  Cassandra acumula estad√≠sticas sobre los tiempos de respuesta de solicitudes anteriores y adapta un valor de tiempo de espera espec√≠fico.  Esto funciona bastante bien. <br><br>  En este enfoque, todo se basa en el equilibrio entre la confiabilidad y la carga espuria. No los servidores. Usted brinda confiabilidad, pero a veces recibe solicitudes adicionales al servidor.  Si ten√≠a prisa en alg√∫n lugar y envi√≥ una segunda solicitud, pero la primera a√∫n respondi√≥, el servidor recibi√≥ un poco m√°s de carga.  En un solo caso, este es un peque√±o problema. <br><br><img src="https://habrastorage.org/webt/uv/7c/bs/uv7cbswancegyh5vc8t7mwvr8uy.png"><br><br>  La coherencia del tiempo de espera es otro aspecto importante.  Hablaremos m√°s sobre la cancelaci√≥n de la solicitud, pero en general, si el tiempo de espera para toda la solicitud del usuario es de 100 ms, entonces no tiene sentido establecer el tiempo de espera para la solicitud en la base de datos durante 1 s.  Hay sistemas que le permiten hacer esto din√°micamente: el servicio a servicio transfiere el resto del tiempo que esperar√° una respuesta a esta solicitud.  Es complicado, pero si de repente lo necesita, puede descubrir f√°cilmente c√≥mo hacerlo en el mismo Enviado. <br><br>  ¬øQu√© m√°s necesitas saber sobre el reintento? <br><br><h3>  Punto de no retorno (V1) <br></h3><br>  Aqu√≠ V1 no es la versi√≥n 1. En la aviaci√≥n existe ese concepto: velocidad V1.  Esta es la velocidad despu√©s de la cual es imposible reducir la velocidad de aceleraci√≥n en la pista.  Es necesario despegar y luego tomar una decisi√≥n sobre qu√© hacer a continuaci√≥n. <br><br>  El mismo punto de no retorno se encuentra en los equilibradores de carga: <strong>cuando pasa 1 byte de la respuesta a su cliente, no se pueden corregir los errores</strong> .  Si el backend muere en este punto, ning√∫n reintento ayudar√°.  Solo puede reducir la probabilidad de que se desencadene un escenario de este tipo, hacer un cierre elegante, es decir, decirle a su aplicaci√≥n: "¬°No acepta nuevas solicitudes ahora, pero modifica las antiguas!", Y solo luego extingue. <br><br>  Si controla al cliente, esta es una aplicaci√≥n m√≥vil o Ajax complicada, puede intentar repetir la solicitud y luego puede salir de esta situaci√≥n. <br><br><h3>  Punto de No Retorno [Enviado] <br></h3><br>  El enviado ten√≠a un truco tan extra√±o.  Hay per_try_timeout: limita cu√°nto puede tomar cada intento de obtener una respuesta a una solicitud.  Si este tiempo de espera funcion√≥, pero el backend ya comenz√≥ a responder al cliente, entonces todo se interrumpi√≥, el cliente recibi√≥ un error. <br><br>  Mi colega Pavel Trukhanov ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" class="user_link">tru_pablo</a> ) hizo un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">parche</a> , que ya est√° en Master Envoy y estar√° en 1.7.  Ahora funciona como deber√≠a: si la respuesta ha comenzado a transmitirse, solo funcionar√° el tiempo de espera global. <br><br><h3>  Reintentos: es necesario limitar <br></h3><br>  Los reintentos son buenos, pero existen las llamadas solicitudes asesinas: las consultas pesadas que realizan una l√≥gica muy compleja acceden mucho a la base de datos y a menudo no se ajustan a per_try_timeout.  Si enviamos reintentos una y otra vez, entonces matamos nuestra base.  Porque <strong>en la mayor√≠a de los servicios de bases de datos (99.9%) no hay cancelaci√≥n de solicitud</strong> . <br><br>  La cancelaci√≥n de la solicitud significa que el cliente se ha desenganchado, debe detener todo el trabajo en este momento.  Golang est√° promoviendo activamente este enfoque, pero desafortunadamente termina con un back-end, y muchos repositorios de bases de datos no lo admiten. <br><br>  En consecuencia, los reintentos deben ser limitados, lo que permite casi todos los equilibradores (de ahora en adelante, dejamos de considerar HAProxy). <br><br>  <strong>Nginx:</strong> <br><br><ul><li>  proxy_next_upstream_timeout (global) </li><li>  proxt_read_timeout ** como per_try_timeout </li><li>  proxy_next_upstream_tries </li></ul><br>  <strong>Enviado</strong> <br><br><ul><li>  tiempo de espera (global) </li><li>  per_try_timeout </li><li>  num_retries </li></ul><br>  En Nginx, podemos decir que estamos tratando de hacer reintentos en toda la ventana X, es decir, en un intervalo de tiempo dado, por ejemplo, 500 ms, hacemos tantos reintentos como sea conveniente.  O hay una configuraci√≥n que limita el n√∫mero de muestras repetidas.  En <strong>Envoy</strong> , lo mismo es cantidad o tiempo de espera (global). <br><br><h4>  Reintentos: aplicar [nginx] <br></h4><br>  Considere un ejemplo: establecemos intentos de reintento en nginx 2; en consecuencia, despu√©s de recibir HTTP 503, intentamos enviar una solicitud al servidor nuevamente.  Luego apague los <strong>dos</strong> backends. <br><br><pre> <code class="plaintext hljs">upstream backends { server 127.0.0.1:20001; server 127.0.0.1:20002; server 127.0.0.1:20003; } server { listen 127.0.0.1:30000; proxy_next_upstream error timeout http_503; proxy_next_upstream_tries 2; location / { proxy_pass http://backends; } }</code> </pre><br>  A continuaci√≥n se muestran los gr√°ficos de nuestro banco de pruebas.  No hay errores en el gr√°fico superior, porque hay muy pocos de ellos.  Si deja solo errores, est√° claro que lo son. <br><br><img src="https://habrastorage.org/webt/3h/sx/aq/3hsxaq8qyifcoyq3mvcyzmxm_cc.png"><br><br><img src="https://habrastorage.org/webt/sc/f_/2w/scf_2wz9tctmouvrs9hpqtpmau0.png"><br><br>  <strong>Que paso</strong> <br><br><ul><li>  proxy_next_upstream_tries = <strong>2.</strong> </li><li>  En el caso de que haga el primer intento al servidor "muerto", y el segundo al otro "muerto", obtendr√° HTTP-503 en caso de que <strong>ambos</strong> intentos lleguen al servidor "malo". </li><li>  Hay pocos errores, ya que nginx "proh√≠be" un servidor defectuoso.  Es decir, si en nginx han regresado algunos errores del backend, deja de hacer los siguientes intentos de enviarle una solicitud.  Esto se rige por la variable <strong>fail_timeout.</strong> </li></ul><br>  Pero hay errores, y esto no nos conviene. <br><br>  <strong>¬øQu√© hacer al respecto?</strong> <br><br>  Podemos aumentar el n√∫mero de reintentos (pero luego volver al problema de las "solicitudes asesinas"), o podemos reducir la probabilidad de que una solicitud llegue a backends "muertos".  Esto se puede hacer con <strong>controles de salud.</strong> <br><br><h2>  Controles de salud <br></h2><br>  Sugiero considerar las comprobaciones de estado como una optimizaci√≥n del proceso de elecci√≥n de un servidor "en vivo".  <strong>Esto de ninguna manera da ninguna garant√≠a.</strong>  En consecuencia, durante la ejecuci√≥n de una solicitud de usuario, es m√°s probable que accedamos solo a servidores "en vivo".  El equilibrador accede regularmente a una URL espec√≠fica, el servidor le responde: "Estoy vivo y listo". <br><br><h4>  Verificaciones de salud: en t√©rminos de backend <br></h4><br>  Desde el punto de vista del backend, puedes hacer cosas interesantes: <br><br><ul><li>  Compruebe la disponibilidad para el funcionamiento de todos los subsistemas subyacentes de los que depende la operaci√≥n de back-end: se establece el n√∫mero necesario de conexiones a la base de datos, el grupo tiene conexiones libres, etc., etc. </li><li>  Puede colgar su propia l√≥gica en la URL de comprobaciones de estado si el equilibrador utilizado no es muy inteligente (por ejemplo, toma el equilibrador de carga del host).  El servidor puede recordar que "en el √∫ltimo minuto comet√≠ tantos errores, probablemente soy una especie de servidor" incorrecto "y durante los pr√≥ximos 2 minutos responder√© con" quinientos "a las comprobaciones de estado.  ¬°As√≠ me prohibir√©! "  Esto a veces ayuda mucho cuando tienes un Load Balancer no controlado. </li><li>  Por lo general, el intervalo de verificaci√≥n es de aproximadamente un segundo, y necesita el controlador de verificaci√≥n de estado para no matar su servidor.  Deber√≠a ser ligero. </li></ul><br><h4>  Comprobaciones de estado: implementaciones <br></h4><br>  Como regla general, todo aqu√≠ es igual para todos: <br><br><ul><li>  Solicitud; </li><li>  Tiempo de espera en √©l; </li><li>  Intervalo a trav√©s del cual hacemos verificaciones.  Los proxies enga√±ados tienen <strong>jitter</strong> , es decir, algo de aleatorizaci√≥n para que todos los controles de salud no lleguen al backend a la vez y no lo maten. </li><li>  <strong>Umbral no saludable</strong> : el umbral de cu√°ntas comprobaciones de salud fallidas deben pasar para que el servicio lo marque como no saludable. </li><li>  <strong>Umbral saludable</strong> : por el contrario, cu√°ntos intentos exitosos deben pasar para que el servidor vuelva a funcionar. </li><li>  L√≥gica adicional  Puede analizar Verificar estado + cuerpo, etc. </li></ul><br>  Nginx implementa funciones de comprobaci√≥n de estado solo en la versi√≥n paga de nginx +. <br><br>  Observo una caracter√≠stica de <strong>Envoy</strong> , tiene un <strong>modo de p√°nico de</strong> comprobaci√≥n de estado <strong>.</strong>  Cuando prohibimos, como "insalubres", m√°s del N% de los hosts (por ejemplo, el 70%), √©l cree que todos nuestros controles de salud est√°n mintiendo y que todos los hosts est√°n realmente vivos.  En un caso muy malo, esto lo ayudar√° a no encontrarse con una situaci√≥n en la que usted mismo le dispar√≥ a su pierna y prohibi√≥ todos los servidores.  Esta es una manera de estar seguro nuevamente. <br><br><h2>  Poniendo todo junto <br></h2><br>  Por lo general, para los controles de salud establecidos: <br><br><ul><li>  O nginx +; </li><li>  O nginx + algo m√°s :) </li></ul><br>  En nuestro pa√≠s, existe una tendencia a establecer nginx + HAProxy, porque la versi√≥n gratuita de nginx no tiene controles de salud, y hasta 1.11.5 no hab√≠a l√≠mite en el n√∫mero de conexiones al backend.  Pero esta opci√≥n es mala porque HAProxy no sabe c√≥mo retirarse despu√©s de establecer una conexi√≥n.  Mucha gente piensa que si HAProxy devuelve un error en nginx y nginx reintentos, entonces todo estar√° bien.  En realidad no  Puede acceder a otro HAProxy y al mismo backend, porque los grupos de backend son los mismos.  Entonces, introduce un nivel m√°s de abstracci√≥n para usted, lo que reduce la precisi√≥n de su equilibrio y, en consecuencia, la disponibilidad del servicio. <br><br>  Tenemos nginx + Envoy, pero si te confundes, puedes limitarte solo a Envoy. <br><br><h2>  ¬øQu√© tipo de enviado? <br></h2><br>  Envoy es un equilibrador de carga juvenil moderno, desarrollado originalmente en Lyft, escrito en C ++.  <strong>Fuera de la caja, puede hacer un mont√≥n de bollos sobre nuestro tema hoy.</strong>  Probablemente lo viste como una malla de servicio para Kubernetes.  Como regla, Envoy act√∫a como un plano de datos, es decir, equilibra directamente el tr√°fico, y tambi√©n hay un plano de control que proporciona informaci√≥n sobre lo que necesita para distribuir la carga (descubrimiento de servicio, etc.). <br><br>  Te dir√© algunas palabras sobre sus bollos. <br><br>  Para aumentar la probabilidad de una respuesta de reintento exitosa la pr√≥xima vez que lo intente, puede dormir un poco y esperar a que los backends recuperen sus sentidos.  De esta manera manejaremos problemas cortos de la base de datos.  Enviado tiene un <strong>retraso para los reintentos</strong> : pausas entre reintentos.  Adem√°s, el intervalo de retraso entre intentos aumenta exponencialmente.  El primer reintento ocurre despu√©s de 0-24 ms, el segundo despu√©s de 0-74 ms, y luego, para cada intento posterior, el intervalo aumenta, y el retraso espec√≠fico se selecciona aleatoriamente de este intervalo. <br><br>  El segundo enfoque no es espec√≠fico del Enviado, sino un patr√≥n llamado <strong>Interrupci√≥n de circuito</strong> (encendido, interruptor de circuito o fusible).  Cuando nuestro backend se embota, de hecho tratamos de terminarlo cada vez.  Esto se debe a que los usuarios en cualquier situaci√≥n incomprensible hacen clic en la p√°gina de actualizaci√≥n y le env√≠an m√°s y m√°s solicitudes nuevas.  Sus equilibradores se ponen nerviosos, env√≠an reintentos, aumenta el n√∫mero de solicitudes: la carga est√° creciendo y, en esta situaci√≥n, ser√≠a bueno no enviar solicitudes. <br><br>  El interruptor de circuito simplemente le permite determinar que estamos en este estado, disparar r√°pidamente el error y darles a los backends "recuperar el aliento". <br><br><img src="https://habrastorage.org/webt/xb/mm/i8/xbmmi88cqacoqvkzdmujynq6da0.gif"><br>  <em>Disyuntor (hystrix como libs),</em> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><em>original</em></a> <em>en el blog de eBay.</em> <br><br>  Arriba est√° el circuito del disyuntor Hystrix.  Hystrix es la biblioteca Java de Netflix que est√° dise√±ada para implementar patrones de tolerancia a fallas. <br><br><ul><li>  El "fusible" puede estar en el estado "cerrado" cuando todas las solicitudes se env√≠an al backend y no hay errores. </li><li>  Cuando se activa un cierto umbral de falla, es decir, se han producido algunos errores, el disyuntor entra en el estado "Abierto".  Devuelve r√°pidamente un error al cliente y las solicitudes no llegan al backend. </li><li>  Una vez en un cierto per√≠odo de tiempo, todav√≠a se env√≠a una peque√±a parte de las solicitudes al backend.  Si se dispara un error, el estado permanece "Abierto".  Si todo comienza a funcionar bien y responde, el "fusible" se cierra y el trabajo contin√∫a. </li></ul><br>  En Enviado, como tal, esto no es todo.  Existen l√≠mites de nivel superior en el hecho de que no puede haber m√°s de N solicitudes para un grupo ascendente espec√≠fico.  Si hay m√°s, algo est√° mal aqu√≠: devolvemos un error.  No puede haber m√°s N reintentos activos (es decir, reintentos que est√°n ocurriendo en este momento). <br><br>  No tuvo reintentos, algo explot√≥: env√≠e reintentos.  Envoy entiende que m√°s de N es anormal, y todas las solicitudes deben ser disparadas con un error. <br><br>  <strong>Interrupci√≥n de circuito [Enviado]</strong> <br><br><ul><li>  Conexiones m√°ximas de cl√∫ster (grupo ascendente) </li><li>  Cluster max solicitudes pendientes </li><li>  Solicitudes m√°ximas de cl√∫ster </li><li>  Cluster max reintentos activos </li></ul><br>  Esta cosa simple funciona bien, es configurable, no tiene que presentar par√°metros especiales y la configuraci√≥n predeterminada es bastante buena. <br><br><h4>  Disyuntor: nuestra experiencia <br></h4><br>  Sol√≠amos tener un recopilador de m√©tricas HTTP, es decir, los agentes instalados en los servidores de nuestros clientes enviaban m√©tricas a nuestra nube a trav√©s de HTTP.  Si tenemos alg√∫n problema en la infraestructura, el agente escribe las m√©tricas en su disco y luego trata de envi√°rnoslas. <br><br>  Y los agentes constantemente intentan enviarnos datos, no est√°n molestos porque de alguna manera respondemos incorrectamente y no nos vamos. <br><br>         (       ,      )  ,     ,           . <br><br>            nginx limit req.    ,    , , 200 RPS.       ,   ,          ,   limit req. <br><br>           TCP     HTTP (  nginx limit req).            .      limit req . <br><br>     ,      ,  .   <strong> </strong>  Circuit breaker,  ,     N  ,   ,   - ,   ,  .   ,    ,      spool  . <br><br>  <strong></strong>   Circuit breaker       + request cancellation ( ).  ,    N   Cassandra, N   Elastic,  ,    ‚Äî   ,         .      ‚Äî ,   . <br><br><img src="https://habrastorage.org/webt/jo/l2/jk/jol2jk44vlcmz3twgvr0jgtpbii.png"><br><br><img src="https://habrastorage.org/webt/1a/7b/x4/1a7bx4yq20uehqoagd4tutqxgmw.png"><br><br>    ,         (:  ‚Äî  ¬´¬ª,  ‚Äî ¬´¬ª). ,      800 RPS    20-30.     ¬´¬ª, ,    . <br><br><h2>    <br></h2><br>    ‚Äî  ,  . <br><br>         ,   ,        ‚Äî      .    . <br><br>  ,       , ,      ,   Health checks ‚Äî HTTP 200. <br><br>    . <br><br><img src="https://habrastorage.org/webt/yu/cy/9s/yucy9sofdr-z7brvjedmnxt4_gc.png"><br><br>     Load Balancer, 3 ,         Cassandra.      Cassandra,   Cassandra   ,    Cassandra     data noda. <br><br>   ‚Äî    : <strong>kernel: NETDEV WATCHDOG: eth0 (ixgbe): transmit queue 3 timed out.</strong> <br><br>   :     (    ),    64     . , 1/64   .     reboot,    . <br><br> ,  ,    ,      .  , ,        ,            .   ,    ,   .     ,   . <br><br> <strong>Cassandra: coordinator -&gt; nodes</strong> <br><br>  Cassandra,      (speculative retries),      .    latency  99 ,         . <br><br> <strong>App -&gt; cassandra coordinator</strong> <br><br>     .     Cassandra      ¬´¬ª ,    ,  ,  latency  .. <br><br>      gocql ‚Äî   cassandra client.       .   HostSelectionPolicy,       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">bitly/go-hostpool</a> .    Epsilon greedy  ,       . <br><br>     ,    <strong>Epsilon-greedy</strong> . <br><br>      (multi-armed bandit):       ,     ,     N     . <br><br>    : <br><br><ol><li>  ¬´ <strong>explore¬ª</strong> ‚Äî   : 10    ,  ,   . <br></li><li>  ¬´ <strong>exploit¬ª</strong> ‚Äî      . <br></li></ol><br> ,    (10 ‚Äî 30%)   <strong>round</strong> - <strong>robin</strong>    ,  ,  ,  .  70 ‚Äî 90%        . <br><br> Host-pool          .         .        (    ‚Äî ,    ,  ).      .      ,     , ,       . <br><br><h2>   </h2><br>   ¬´¬ª ()   ‚ÄîCassandra  Cassandra coordinator-data.     (nginx, Envoy ‚Äî  )    ¬´¬ª Application,     Cassandra  ,       ,      . <br><br>  Envoy    <strong>Outlier detection</strong> : <br><br><ul><li> Consecutive http-5xx. </li><li> Consecutive gateway errors (502,503,504). </li><li> Success rate. </li></ul><br>   ¬´¬ª  ,    -  ,   .   ,    .        ‚Äî    ,   ,     .  ,    ,          . <br><br>   ,       ¬´¬ª,   max_ejection_percent.    ,      outlier,     .  ,    70%  ‚Äî  ,   ‚Äî , ! <br><br>      ,       ‚Äî ! <br><br><h2>  <br></h2><br> ,     ,      .  ,       latency    , : <br><br><ul><li>  ,        .. </li><li>    ,      -,       . </li></ul><br> ,  <strong>    </strong> ,   .  ,      ,     ,         ‚Äî  ,    . <br><br> <strong>      </strong> .  99%     nginx/ <s>HAProxy</s> /Envoy.   proxy ,           ¬´¬ª. <br><br> <strong>    proxy</strong> (   HAProxy:)), <strong>  ,    .</strong> <br><br><blockquote>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DevOpsConf Russia</a>      Kubernetes         .          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> . <br><br>    ,       ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a>      DevOps. <br><br>    ,   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">YouTube-</a> ‚Äî              . <br></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es423085/">https://habr.com/ru/post/es423085/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es423073/index.html">Sistema de archivos interplanetarios: hash trivial (identidad), bloque DAG y buffers de protocolo</a></li>
<li><a href="../es423075/index.html">¬øPor qu√© los directores financieros est√°n tan ansiosos por traducir el gasto de capital en TI en funcionamiento?</a></li>
<li><a href="../es423077/index.html">Gu√≠a de ensamblador X86 para principiantes</a></li>
<li><a href="../es423079/index.html">Puntos clave de una entrevista con Elon Musk en Joe Rogan</a></li>
<li><a href="../es423083/index.html">C√≥mo me convert√≠ en desarrollador en ABBYY</a></li>
<li><a href="../es423087/index.html">No me empujes en el ojo</a></li>
<li><a href="../es423089/index.html">Programadores en MBLT DEV 2018</a></li>
<li><a href="../es423091/index.html">Flutter para desarrolladores de Android. C√≥mo crear una interfaz de usuario para una actividad usando Flutter</a></li>
<li><a href="../es423093/index.html">Aumentamos la aleatoriedad del hecho de que [probablemente] [casi] por accidente</a></li>
<li><a href="../es423095/index.html">Novedades en la presentaci√≥n de Apple</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>