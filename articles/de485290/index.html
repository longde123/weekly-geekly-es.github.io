<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🔃 👰🏿 ♟️ Einfache BERT-Destillationsanleitung 🤶🏽 👩🏽‍⚕️ 🧕🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wenn Sie sich für maschinelles Lernen interessieren, haben Sie wahrscheinlich von BERT und Transformatoren gehört. 


 BERT ist ein Sprachmodell von G...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Einfache BERT-Destillationsanleitung</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/avito/blog/485290/"><p>  Wenn Sie sich für maschinelles Lernen interessieren, haben Sie wahrscheinlich von BERT und Transformatoren gehört. </p><br><p>  BERT ist ein Sprachmodell von Google, das bei einer Reihe von Aufgaben mit großem Abstand die neuesten Ergebnisse zeigt.  BERT und allgemein Transformatoren sind zu einem völlig neuen Schritt in der Entwicklung von Algorithmen zur Verarbeitung natürlicher Sprache (NLP) geworden.  Der Artikel darüber und die „Rangliste“ für verschiedene Benchmarks finden Sie <a href="https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional">auf der Website von Papers With Code</a> . </p><br><p>  Bei BERT gibt es ein Problem: Der Einsatz in industriellen Systemen ist problematisch.  BERT-Base enthält 110M Parameter, BERT-Large - 340M.  Aufgrund einer so großen Anzahl von Parametern ist es schwierig, dieses Modell auf Geräte mit begrenzten Ressourcen, wie z. B. Mobiltelefone, herunterzuladen.  Darüber hinaus macht die lange Inferenzzeit dieses Modell ungeeignet, wenn die Reaktionsgeschwindigkeit kritisch ist.  Daher ist die Suche nach Wegen zur Beschleunigung des BERT ein sehr aktuelles Thema. </p><br><p>  In Avito müssen wir häufig Probleme mit der Textklassifizierung lösen.  Dies ist eine typische angewandte maschinelle Lernaufgabe, die gut untersucht wurde.  Aber es gibt immer die Versuchung, etwas Neues auszuprobieren.  Dieser Artikel entstand aus dem Versuch heraus, BERT in alltäglichen maschinellen Lernaufgaben anzuwenden.  Darin werde ich zeigen, wie Sie die Qualität eines vorhandenen Modells mithilfe von BERT erheblich verbessern können, ohne neue Daten hinzuzufügen oder das Modell zu komplizieren. </p><br><p><img src="https://habrastorage.org/webt/c_/po/z2/c_poz2e3dkggx7ekt3gn_9wva3g.png"></p><a name="habracut"></a><br><h2 id="knowledge-distillation-kak-metod-uskoreniya-neyronnyh-setey">  Wissensdestillation als Methode zur Beschleunigung neuronaler Netze </h2><br><p>  Es gibt verschiedene Möglichkeiten, neuronale Netze zu beschleunigen / zu vereinfachen.  Die detaillierteste Rezension, die ich getroffen habe, ist <a href="https://blog.inten.to/speeding-up-bert-5528e18bb4ea">im Intento-Blog auf dem Medium veröffentlicht</a> . </p><br><p>  Die Methoden lassen sich grob in drei Gruppen einteilen: </p><br><ol><li>  Änderung der Netzwerkarchitektur. </li><li>  Modellkomprimierung (Quantisierung, Bereinigung). </li><li>  Wissensdestillation. </li></ol><br><p>  Wenn die ersten beiden Methoden relativ gut bekannt und verständlich sind, ist die dritte weniger verbreitet.  Zum ersten Mal schlug Rich Caruana die Idee der Destillation <a href="https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf">in dem Artikel „Model Compression“ vor</a> .  Das Wesentliche ist einfach: Sie können ein leichtgewichtiges Modell trainieren, das das Verhalten eines Lehrermodells oder sogar eines Ensembles von Modellen nachahmt.  In unserem Fall wird der Lehrer BERT sein und der Schüler wird ein beliebiges leichtes Modell sein. </p><br><h2 id="zadacha">  Herausforderung </h2><br><p>  Analysieren wir die Destillation am Beispiel der binären Klassifikation.  Nehmen Sie den offenen SST-2-Datensatz aus dem Standardsatz von Tasks, mit denen Modelle für NLP getestet werden. </p><br><p>  Dieser Datensatz ist eine Sammlung von Rezensionen zu Filmen mit IMDb, aufgeschlüsselt nach emotionalen Farben - positiv oder negativ.  Die Metrik in diesem Datensatz ist die Genauigkeit. </p><br><h2 id="obuchenie-bert-based-modeli-ili-uchitelya">  Ausbildung von BERT-basierten Modellen oder „Lehrern“ </h2><br><p>  Zunächst müssen Sie das „große“ BERT-basierte Modell trainieren, das Lehrer wird.  Der einfachste Weg, dies zu tun, besteht darin, die Einbettungen von BERT zu nehmen und den Klassifikator darüber zu trainieren und dem Netzwerk eine Ebene hinzuzufügen. </p><br><p>  Dank <a href="https://github.com/huggingface/transformers">der Transformers-Bibliothek ist</a> dies ziemlich einfach, da es eine fertige Klasse für das BertForSequenceClassification-Modell gibt.  Meiner Meinung nach wurde das ausführlichste und verständlichste Tutorial zum Unterrichten dieses Modells <a href="https-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca">von Towards Data Science veröffentlicht</a> . </p><br><p>  Stellen wir uns vor, wir hätten ein geschultes BertForSequenceClassification-Modell.  In unserem Fall ist num_labels = 2, da wir eine binäre Klassifikation haben.  Wir werden dieses Modell als "Lehrer" verwenden. </p><br><h2 id="obuchenie-uchenika">  "Schüler" lernen </h2><br><p>  Als Student können Sie jede Architektur nehmen: ein neuronales Netzwerk, ein lineares Modell, einen Entscheidungsbaum.  Versuchen wir, BiLSTM für eine bessere Visualisierung beizubringen.  Zunächst unterrichten wir BiLSTM ohne BERT. </p><br><p>  Um Text an die Eingabe eines neuronalen Netzwerks zu senden, müssen Sie ihn als Vektor darstellen.  Eine der einfachsten Möglichkeiten besteht darin, jedes Wort seinem Index im Wörterbuch zuzuordnen.  Das Wörterbuch besteht aus den n beliebtesten Wörtern in unserem Datensatz sowie zwei Servicewörtern: "pad" - "dummy word", damit alle Sequenzen gleich lang sind, und "unk" - für Wörter außerhalb des Wörterbuchs.  Wir werden das Wörterbuch mit den Standardwerkzeugen von torchtext erstellen.  Der Einfachheit halber habe ich keine vorgefertigten Worteinbettungen verwendet. <br></p><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torchtext <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> data <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_vocab</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X)</span></span></span><span class="hljs-function">:</span></span> X_split = [t.split() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> X] text_field = data.Field() text_field.build_vocab(X_split, max_size=<span class="hljs-number"><span class="hljs-number">10000</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text_field <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">pad</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(seq, max_len)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(seq) &lt; max_len: seq = seq + [<span class="hljs-string"><span class="hljs-string">'&lt;pad&gt;'</span></span>] * (max_len - len(seq)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> seq[<span class="hljs-number"><span class="hljs-number">0</span></span>:max_len] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">to_indexes</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(vocab, words)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [vocab.stoi[w] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> w <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">to_dataset</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x, y, y_real)</span></span></span><span class="hljs-function">:</span></span> torch_x = torch.tensor(x, dtype=torch.long) torch_y = torch.tensor(y, dtype=torch.float) torch_real_y = torch.tensor(y_real, dtype=torch.long) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> TensorDataset(torch_x, torch_y, torch_real_y)</code> </pre> <br><h3 id="model-bilstm">  Modell BiLSTM </h3><br><p>  Der Code für das Modell sieht folgendermaßen aus: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> nn <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.autograd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Variable <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SimpleLSTM</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, batch_size, device=None)</span></span></span><span class="hljs-function">:</span></span> super(SimpleLSTM, self).__init__() self.batch_size = batch_size self.hidden_dim = hidden_dim self.n_layers = n_layers self.embedding = nn.Embedding(input_dim, embedding_dim) self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout) self.fc = nn.Linear(hidden_dim * <span class="hljs-number"><span class="hljs-number">2</span></span>, output_dim) self.dropout = nn.Dropout(dropout) self.device = self.init_device(device) self.hidden = self.init_hidden() @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">init_device</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(device)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> device <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> torch.device(<span class="hljs-string"><span class="hljs-string">'cuda'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> device <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">init_hidden</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (Variable(torch.zeros(<span class="hljs-number"><span class="hljs-number">2</span></span> * self.n_layers, self.batch_size, self.hidden_dim).to(self.device)), Variable(torch.zeros(<span class="hljs-number"><span class="hljs-number">2</span></span> * self.n_layers, self.batch_size, self.hidden_dim).to(self.device))) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, text, text_lengths=None)</span></span></span><span class="hljs-function">:</span></span> self.hidden = self.init_hidden() x = self.embedding(text) x, self.hidden = self.rnn(x, self.hidden) hidden, cell = self.hidden hidden = self.dropout(torch.cat((hidden[<span class="hljs-number"><span class="hljs-number">-2</span></span>, :, :], hidden[<span class="hljs-number"><span class="hljs-number">-1</span></span>, :, :]), dim=<span class="hljs-number"><span class="hljs-number">1</span></span>)) x = self.fc(hidden) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> x</code> </pre> <br><h3 id="obuchenie">  Schulung </h3><br><p>  Für dieses Modell ist die Dimension des Ausgabevektors (batch_size, output_dim).  Im Training verwenden wir den üblichen logloss.  PyTorch verfügt über eine BCEWithLogitsLoss-Klasse, die Sigmoid- und Kreuzentropie kombiniert.  Was du brauchst. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, output, bert_prob, real_label)</span></span></span><span class="hljs-function">:</span></span> criterion = torch.nn.BCEWithLogitsLoss() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> criterion(output, real_label.float())</code> </pre> <br><p>  Code für eine Ära des Lernens: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_optimizer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model)</span></span></span><span class="hljs-function">:</span></span> optimizer = torch.optim.Adam(model.parameters()) scheduler = torch.optim.lr_scheduler.StepLR(optimizer, <span class="hljs-number"><span class="hljs-number">2</span></span>, gamma=<span class="hljs-number"><span class="hljs-number">0.9</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> optimizer, scheduler <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">epoch_train_func</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model, dataset, loss_func, batch_size)</span></span></span><span class="hljs-function">:</span></span> train_loss = <span class="hljs-number"><span class="hljs-number">0</span></span> train_sampler = RandomSampler(dataset) data_loader = DataLoader(dataset, sampler=train_sampler, batch_size=batch_size, drop_last=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) model.train() optimizer, scheduler = get_optimizer(model) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, (text, bert_prob, real_label) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(tqdm(data_loader, desc=<span class="hljs-string"><span class="hljs-string">'Train'</span></span>)): text, bert_prob, real_label = to_device(text, bert_prob, real_label) model.zero_grad() output = model(text.t(), <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>).squeeze(<span class="hljs-number"><span class="hljs-number">1</span></span>) loss = loss_func(output, bert_prob, real_label) loss.backward() optimizer.step() train_loss += loss.item() scheduler.step() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> train_loss / len(data_loader)</code> </pre> <br><p>  Code zur Überprüfung nach der Ära: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">epoch_evaluate_func</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model, eval_dataset, loss_func, batch_size)</span></span></span><span class="hljs-function">:</span></span> eval_sampler = SequentialSampler(eval_dataset) data_loader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=batch_size, drop_last=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) eval_loss = <span class="hljs-number"><span class="hljs-number">0.0</span></span> model.eval() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, (text, bert_prob, real_label) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(tqdm(data_loader, desc=<span class="hljs-string"><span class="hljs-string">'Val'</span></span>)): text, bert_prob, real_label = to_device(text, bert_prob, real_label) output = model(text.t(), <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>).squeeze(<span class="hljs-number"><span class="hljs-number">1</span></span>) loss = loss_func(output, bert_prob, real_label) eval_loss += loss.item() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> eval_loss / len(data_loader)</code> </pre> <br><p>  Wenn all dies zusammengestellt ist, erhalten wir den folgenden Code zum Trainieren des Modells: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.utils.data <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> (TensorDataset, random_split, RandomSampler, DataLoader, SequentialSampler) <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torchtext <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> data <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tqdm <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">device</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> torch.device(<span class="hljs-string"><span class="hljs-string">"cuda"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> torch.cuda.is_available() <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">"cpu"</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">to_device</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(text, bert_prob, real_label)</span></span></span><span class="hljs-function">:</span></span> text = text.to(device()) bert_prob = bert_prob.to(device()) real_label = real_label.to(device()) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text, bert_prob, real_label <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">LSTMBaseline</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> vocab_name = <span class="hljs-string"><span class="hljs-string">'text_vocab.pt'</span></span> weights_name = <span class="hljs-string"><span class="hljs-string">'simple_lstm.pt'</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, settings)</span></span></span><span class="hljs-function">:</span></span> self.settings = settings self.criterion = torch.nn.BCEWithLogitsLoss().to(device()) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, output, bert_prob, real_label)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.criterion(output, real_label.float()) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, text_field)</span></span></span><span class="hljs-function">:</span></span> model = SimpleLSTM( input_dim=len(text_field.vocab), embedding_dim=<span class="hljs-number"><span class="hljs-number">64</span></span>, hidden_dim=<span class="hljs-number"><span class="hljs-number">128</span></span>, output_dim=<span class="hljs-number"><span class="hljs-number">1</span></span>, n_layers=<span class="hljs-number"><span class="hljs-number">1</span></span>, bidirectional=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, dropout=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, batch_size=self.settings[<span class="hljs-string"><span class="hljs-string">'train_batch_size'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, X, y, y_real, output_dir)</span></span></span><span class="hljs-function">:</span></span> max_len = self.settings[<span class="hljs-string"><span class="hljs-string">'max_seq_length'</span></span>] text_field = get_vocab(X) X_split = [t.split() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> X] X_pad = [pad(s, max_len) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tqdm(X_split, desc=<span class="hljs-string"><span class="hljs-string">'pad'</span></span>)] X_index = [to_indexes(text_field.vocab, s) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tqdm(X_pad, desc=<span class="hljs-string"><span class="hljs-string">'to index'</span></span>)] dataset = to_dataset(X_index, y, y_real) val_len = int(len(dataset) * <span class="hljs-number"><span class="hljs-number">0.1</span></span>) train_dataset, val_dataset = random_split(dataset, (len(dataset) - val_len, val_len)) model = self.model(text_field) model.to(device()) self.full_train(model, train_dataset, val_dataset, output_dir) torch.save(text_field, os.path.join(output_dir, self.vocab_name)) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">full_train</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, model, train_dataset, val_dataset, output_dir)</span></span></span><span class="hljs-function">:</span></span> train_settings = self.settings num_train_epochs = train_settings[<span class="hljs-string"><span class="hljs-string">'num_train_epochs'</span></span>] best_eval_loss = <span class="hljs-number"><span class="hljs-number">100000</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> epoch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(num_train_epochs): train_loss = epoch_train_func(model, train_dataset, self.loss, self.settings[<span class="hljs-string"><span class="hljs-string">'train_batch_size'</span></span>]) eval_loss = epoch_evaluate_func(model, val_dataset, self.loss, self.settings[<span class="hljs-string"><span class="hljs-string">'eval_batch_size'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> eval_loss &lt; best_eval_loss: best_eval_loss = eval_loss torch.save(model.state_dict(), os.path.join(output_dir, self.weights_name))</code> </pre> <br><h3 id="distillyaciya">  Destillation </h3><br><p>  Die Idee dieser Destillationsmethode stammt <a href="https://arxiv.org/abs/1903.12136">aus einem Artikel von Forschern der University of Waterloo</a> .  Wie ich bereits sagte, muss der „Schüler“ lernen, das Verhalten des „Lehrers“ nachzuahmen.  Was genau ist das Verhalten?  In unserem Fall sind dies die Vorhersagen des Lehrermodells auf dem Trainingssatz.  Die wichtigste Idee ist, die Netzwerkausgabe zu verwenden, bevor die Aktivierungsfunktion angewendet wird.  Es wird angenommen, dass das Modell auf diese Weise die interne Repräsentation besser lernen kann als im Fall der endgültigen Wahrscheinlichkeiten. </p><br><p>  Der ursprüngliche Artikel schlägt vor, der Verlustfunktion einen Begriff hinzuzufügen, der für den "Imitations" -Fehler (MSE) zwischen Modellprotokollen verantwortlich ist. </p><br><p><img src="https://habrastorage.org/webt/hx/_d/w9/hx_dw9ypkcwc_fhgui_jcappoo4.png"></p><br><p>  Zu diesem Zweck nehmen wir zwei kleine Änderungen vor: Ändern Sie die Anzahl der Netzwerkausgänge von 1 auf 2 und korrigieren Sie die Verlustfunktion. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, output, bert_prob, real_label)</span></span></span><span class="hljs-function">:</span></span> a = <span class="hljs-number"><span class="hljs-number">0.5</span></span> criterion_mse = torch.nn.MSELoss() criterion_ce = torch.nn.CrossEntropyLoss() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> a*criterion_ce(output, real_label) + (<span class="hljs-number"><span class="hljs-number">1</span></span>-a)*criterion_mse(output, bert_prob)</code> </pre> <br><p>  Sie können den gesamten Code, den wir geschrieben haben, wiederverwenden, indem Sie nur das Modell und den Verlust neu definieren: </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">LSTMDistilled</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(LSTMBaseline)</span></span></span><span class="hljs-class">:</span></span> vocab_name = <span class="hljs-string"><span class="hljs-string">'distil_text_vocab.pt'</span></span> weights_name = <span class="hljs-string"><span class="hljs-string">'distil_lstm.pt'</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, settings)</span></span></span><span class="hljs-function">:</span></span> super(LSTMDistilled, self).__init__(settings) self.criterion_mse = torch.nn.MSELoss() self.criterion_ce = torch.nn.CrossEntropyLoss() self.a = <span class="hljs-number"><span class="hljs-number">0.5</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, output, bert_prob, real_label)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.a * self.criterion_ce(output, real_label) + (<span class="hljs-number"><span class="hljs-number">1</span></span> - self.a) * self.criterion_mse(output, bert_prob) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, text_field)</span></span></span><span class="hljs-function">:</span></span> model = SimpleLSTM( input_dim=len(text_field.vocab), embedding_dim=<span class="hljs-number"><span class="hljs-number">64</span></span>, hidden_dim=<span class="hljs-number"><span class="hljs-number">128</span></span>, output_dim=<span class="hljs-number"><span class="hljs-number">2</span></span>, n_layers=<span class="hljs-number"><span class="hljs-number">1</span></span>, bidirectional=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, dropout=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, batch_size=self.settings[<span class="hljs-string"><span class="hljs-string">'train_batch_size'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model</code> </pre> <br><p>  Das ist alles, jetzt lernt unser Modell "nachzuahmen". </p><br><h3 id="sravnenie-modeley">  Modellvergleich </h3><br><p>  Im Originalartikel werden die besten Klassifizierungsergebnisse für SST-2 bei a = 0 erhalten, wenn das Modell nur lernt, zu imitieren, ohne reale Bezeichnungen zu berücksichtigen.  Die Genauigkeit ist immer noch geringer als bei BERT, aber deutlich besser als bei regulärem BiLSTM. </p><br><p><img src="https://habrastorage.org/webt/0m/y6/g7/0my6g7eahypj6eq3o52arxxldxq.png"></p><br><p>  Ich habe versucht, die Ergebnisse des Artikels zu wiederholen, aber in meinen Experimenten wurde das beste Ergebnis bei a = 0,5 erzielt. </p><br><p>  So sehen Verlust- und Genauigkeitsdiagramme aus, wenn Sie LSTM wie gewohnt lernen.  Gemessen am Verlustverhalten lernte das Modell schnell und irgendwann nach der sechsten Ära begann die Umschulung. </p><br><p><img src="https://habrastorage.org/webt/lk/bg/rn/lkbgrnank6obtu7pewsoalba9yk.png"></p><br><p>  Destillationsdiagramme: </p><br><p><img src="https://habrastorage.org/webt/gl/u5/7g/glu57giappdlhkc31wlpdulsdkc.png"></p><br><p>  Destilliertes BiLSTM ist durchweg besser als normal.  Es ist wichtig, dass sie in der Architektur absolut identisch sind, der einzige Unterschied liegt in der Art des Unterrichts.  <a href="https://github.com/pvgladkov/knowledge-distillation/tree/master/experiments/sst2">Ich habe den</a> vollständigen Trainingscode <a href="https://github.com/pvgladkov/knowledge-distillation/tree/master/experiments/sst2">auf GitHub gepostet</a> . </p><br><h2 id="zaklyuchenie">  Fazit </h2><br><p>  In diesem Leitfaden habe ich versucht, die Grundidee eines Destillationsansatzes zu erläutern.  Die spezifische Architektur des Schülers hängt von der jeweiligen Aufgabe ab.  Im Allgemeinen ist dieser Ansatz jedoch bei jeder praktischen Aufgabe anwendbar.  Aufgrund der Komplexität in der Phase des Modelltrainings können Sie die Qualität erheblich steigern und gleichzeitig die ursprüngliche Einfachheit der Architektur beibehalten. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de485290/">https://habr.com/ru/post/de485290/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de485278/index.html">Die Hand Gottes. Gutschein-Hilfe</a></li>
<li><a href="../de485280/index.html">Geh Fakedb Datenbankemulation in Tests</a></li>
<li><a href="../de485284/index.html">Funktionen von SPIKE ™ Prime LEGO® Education</a></li>
<li><a href="../de485286/index.html">Wie wir Waren oder eine kleine Automatisierungs-Ode gewogen haben</a></li>
<li><a href="../de485288/index.html">Liebe es, Indie Gamedev'a zu hassen</a></li>
<li><a href="../de485294/index.html">Moderner Kurs auf Node.js im Jahr 2020</a></li>
<li><a href="../de485298/index.html">Das mysteriöse LyX-Programm. Teil 4</a></li>
<li><a href="../de485300/index.html">Neuer Ausbruch von H2Miner-Würmern, die Redis RCE ausnutzen, entdeckt</a></li>
<li><a href="../de485304/index.html">Ein paar Tricks mit Iframe-Elementen</a></li>
<li><a href="../de485312/index.html">DevOps für Mobile Apps</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>