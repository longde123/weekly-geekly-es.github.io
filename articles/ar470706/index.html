<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>๐ฉ๐ผโ๐คโ๐จ๐ฝ ๐คด๐ผ ๐ Python + Keras + LSTM: ุงุตูุน ูุชุฑุฌู ูุต ูู ูุตู ุณุงุนุฉ ๐ ๐๐ฟ ๐ง๐ฟโ๐คโ๐ง๐ผ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="ูุฑุญุจุง ูุง ูุจุฑ. 

 ูู ุงูุฌุฒุก ุงูุณุงุจู ุ ูุธุฑุช ุฅูู ุฅูุดุงุก ุชูููุฒ ุจุณูุท ูููุต ุนูู ุฃุณุงุณ ุดุจูุฉ ุนุตุจูุฉ. ุงูููู ุณูู ูุณุชุฎุฏู ููุฌูุง ููุงุซููุง ูููุชุจ ูุชุฑุฌููุง ุชููุงุฆููุง ูููุตูุต ูู...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Python + Keras + LSTM: ุงุตูุน ูุชุฑุฌู ูุต ูู ูุตู ุณุงุนุฉ</h1><div class="post__body post__body_full" style=";text-align:right;direction:rtl"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/470706/" style=";text-align:right;direction:rtl">  ูุฑุญุจุง ูุง ูุจุฑ. <br><br>  ูู <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">ุงูุฌุฒุก ุงูุณุงุจู ุ</a> ูุธุฑุช ุฅูู ุฅูุดุงุก ุชูููุฒ ุจุณูุท ูููุต ุนูู ุฃุณุงุณ ุดุจูุฉ ุนุตุจูุฉ.  ุงูููู ุณูู ูุณุชุฎุฏู ููุฌูุง ููุงุซููุง ูููุชุจ ูุชุฑุฌููุง ุชููุงุฆููุง ูููุตูุต ูู ุงูุฅูุฌููุฒูุฉ ุฅูู ุงูุฃููุงููุฉ. <br><br><img src="https://habrastorage.org/webt/gf/ft/jx/gfftjxwflb7yxrwtffish1hkqsc.jpeg"><br><br>  ุจุงููุณุจุฉ ูุฃููุฆู ุงูุฐูู ููุชููู ุจููููุฉ ุนูู ุฐูู ุ ูุฅู ุงูุชูุงุตูู ููุฏ ุงูุชูููุฐ. <br><a name="habracut"></a><br>  <i>ููุงุญุธุฉ</i> : ูุฐุง ุงููุดุฑูุน ุงูุฎุงุต ุจุงุณุชุฎุฏุงู ุดุจูุฉ ุนุตุจูุฉ ููุชุฑุฌูุฉ ูู ุจุฑูุงูุฌ ุชุนูููู ุญุตุฑู ุ ูุจุงูุชุงูู ูุฅู ุงูุณุคุงู "ููุงุฐุง" ูู ูุชู ุงููุธุฑ ููู.  ูููุชุนุฉ ููุท.  ูุง ุฃูุตุฏ ุฅุซุจุงุช ุฃู ูุฐู ุงูุทุฑููุฉ ุฃู ุชูู ุฃูุถู ุฃู ุฃุณูุฃ ุ ููุฏ ูุงู ูู ุงููุซูุฑ ููุงูุชูุงู ุงูุชุญูู ููุง ูุญุฏุซ.  ุฅู ุงูุทุฑููุฉ ุงููุณุชุฎุฏูุฉ ุฃุฏูุงู ูุจุณุทุฉ ุ ุจุงูุทุจุน ุ ููููู ุขูู ุฃูุง ูุฃูู ุฃุญุฏ ูู ุฃู ููุชุจ ุฑุณุงูุฉ Lingvo ุซุงููุฉ ุฎูุงู ูุตู ุณุงุนุฉ. <br><br><h2 style=";text-align:right;direction:rtl">  ุฌูุน ุงูุจูุงูุงุช </h2><br>  ุชู ุงุณุชุฎุฏุงู ููู ููุฌูุฏ ุนูู ุงูุดุจูุฉ ูุญุชูู ุนูู ุนุจุงุฑุงุช ุฅูุฌููุฒูุฉ ูุฃููุงููุฉ ููุตููุฉ ุจุนูุงูุงุช ุฌุฏููุฉ ููุฌููุนุฉ ุจูุงูุงุช ุงููุตุฏุฑ.  ูุฌููุนุฉ ูู ุงูุนุจุงุฑุงุช ุชุจุฏู ูุซู ูุฐุง: <br><br><pre style=";text-align:right;direction:rtl"><code class="python hljs">Hi. Hallo! Hi. Grรผร Gott! Run! Lauf! Wow! Potzdonner! Wow! Donnerwetter! Fire! Feuer! Help! Hilfe! Help! Zu Hรผlf! Stop! Stopp! Wait! Warte! Go on. Mach weiter. Hello! Hallo! I ran. Ich rannte. I see. Ich verstehe. ...</code> </pre> <br>  ูุญุชูู ุงูููู ุนูู 192 ุฃูู ุณุทุฑ ููู ุญุฌู 13 ููุบุงุจุงูุช.  ูููู ุจุชุญููู ุงููุต ูู ุงูุฐุงูุฑุฉ ูุชูุณูู ุงูุจูุงูุงุช ุฅูู ูุชูุชูู ุ ููููุงุช ุฅูุฌููุฒูุฉ ูุฃููุงููุฉ. <br><br><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">read_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(filename, mode=<span class="hljs-string"><span class="hljs-string">'rt'</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> file: text = file.read() sents = text.strip().split(<span class="hljs-string"><span class="hljs-string">'\n'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [i.split(<span class="hljs-string"><span class="hljs-string">'\t'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> sents] data = read_text(<span class="hljs-string"><span class="hljs-string">"deutch.txt"</span></span>) deu_eng = np.array(data) deu_eng = deu_eng[:<span class="hljs-number"><span class="hljs-number">30000</span></span>,:] print(<span class="hljs-string"><span class="hljs-string">"Dictionary size:"</span></span>, deu_eng.shape) <span class="hljs-comment"><span class="hljs-comment"># Remove punctuation deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]] deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]] # convert text to lowercase for i in range(len(deu_eng)): deu_eng[i,0] = deu_eng[i,0].lower() deu_eng[i,1] = deu_eng[i,1].lower()</span></span></code> </pre><br>  ูููุง ุฃูุถูุง ุจุชุญููู ุฌููุน ุงููููุงุช ุฅูู ุฃุญุฑู ุตุบูุฑุฉ ูุฅุฒุงูุฉ ุนูุงูุงุช ุงูุชุฑููู. <br><br>  ูุงูุฎุทูุฉ ุงูุชุงููุฉ ูู ุฅุนุฏุงุฏ ุงูุจูุงูุงุช ููุดุจูุฉ ุงูุนุตุจูุฉ.  ุงูุดุจูุฉ ูุง ุชุนุฑู ูุง ูู ุงููููุงุช ุ ูุชุนูู ุญุตุฑุง ูุน ุงูุฃุฑูุงู.  ูุญุณู ุงูุญุธ ุจุงููุณุจุฉ ููุง ุ ุชุญุชูู keras ุจุงููุนู ุนูู ูุฆุฉ Tokenizer ุ ูุงูุชู ุชุญู ูุญู ุงููููุงุช ูู ุงูุฌูู ุจุงูุฑููุฒ ุงูุฑูููุฉ. <br><br>  ูุชุถุญ ุงุณุชุฎุฏุงูู ุจุจุณุงุทุฉ ูุน ูุซุงู: <br><br><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.text <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Tokenizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.sequence <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pad_sequences s = <span class="hljs-string"><span class="hljs-string">"To be or not to be"</span></span> eng_tokenizer = Tokenizer() eng_tokenizer.fit_on_texts([s]) seq = eng_tokenizer.texts_to_sequences([s]) seq = pad_sequences(seq, maxlen=<span class="hljs-number"><span class="hljs-number">8</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'post'</span></span>) print(seq)</code> </pre><br>  ุณูุชู ุงุณุชุจุฏุงู ุงูุนุจุงุฑุฉ "ูุชููู ุฃู ูุง ุชููู" ุจุงูุตููู [1 2 3 4 1 2 0 0] ุ ุญูุซ ูุตุนุจ ุชุฎูููู ุ 1 = ุฅูู ุ 2 = ูููู ุ 3 = ุฃู ุ 4 = ูุง.  ูููููุง ุจุงููุนู ุชูุฏูู ูุฐู ุงูุจูุงูุงุช ุฅูู ุงูุดุจูุฉ ุงูุนุตุจูุฉ. <br><br><h2 style=";text-align:right;direction:rtl">  ุชุฏุฑูุจ ุงูุดุจูุฉ ุงูุนุตุจูุฉ </h2><br>  ุจูุงูุงุชูุง ุฌุงูุฒุฉ ุฑูููุง.  ููุณู ุงูุตููู ุฅูู ูุชูุชูู ูุฅุฏุฎุงู (ุงูุฎุทูุท ุงูุฅูุฌููุฒูุฉ) ูุฅุฎุฑุงุฌ (ุงูุฎุทูุท ุงูุฃููุงููุฉ) ุงูุจูุงูุงุช.  ุณูููู ุฃูุถูุง ุจุฅุนุฏุงุฏ ูุญุฏุฉ ูููุตูุฉ ููุชุญูู ูู ุตุญุฉ ุนูููุฉ ุงูุชุนูู. <br><br><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># split data into train and test set train, test = train_test_split(deu_eng, test_size=0.2, random_state=12) # prepare training data trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 0]) trainY = encode_sequences(deu_tokenizer, deu_length, train[:, 1]) # prepare validation data testX = encode_sequences(eng_tokenizer, eng_length, test[:, 0]) testY = encode_sequences(deu_tokenizer, deu_length, test[:, 1])</span></span></code> </pre><br>  ุงูุขู ูููููุง ุฅูุดุงุก ูููุฐุฌ ููุดุจูุฉ ุงูุนุตุจูุฉ ูุจุฏุก ุงูุชุฏุฑูุจ.  ููุง ุชุฑูู ุ ุชุญุชูู ุงูุดุจูุฉ ุงูุนุตุจูุฉ ุนูู ุทุจูุงุช LSTM ููุง ุฎูุงูุง ุฐุงูุฑุฉ.  ุนูู ุงูุฑุบู ูู ุฃูู ูู ุงููุญุชูู ุฃู ูุนูู ุนูู ุดุจูุฉ "ููุชุธูุฉ" ุ ูููู ูุฃููุฆู ุงูุฐูู ูุฑุบุจูู ูู ุงูุชุญูู ุจููุณูู. <br><br><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">make_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(in_vocab, out_vocab, in_timesteps, out_timesteps, n)</span></span></span><span class="hljs-function">:</span></span> model = Sequential() model.add(Embedding(in_vocab, n, input_length=in_timesteps, mask_zero=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) model.add(LSTM(n)) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.3</span></span>)) model.add(RepeatVector(out_timesteps)) model.add(LSTM(n, return_sequences=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.3</span></span>)) model.add(Dense(out_vocab, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)) model.compile(optimizer=optimizers.RMSprop(lr=<span class="hljs-number"><span class="hljs-number">0.001</span></span>), loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model eng_vocab_size = len(eng_tokenizer.word_index) + <span class="hljs-number"><span class="hljs-number">1</span></span> deu_vocab_size = len(deu_tokenizer.word_index) + <span class="hljs-number"><span class="hljs-number">1</span></span> eng_length, deu_length = <span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span> model = make_model(eng_vocab_size, deu_vocab_size, eng_length, deu_length, <span class="hljs-number"><span class="hljs-number">512</span></span>) num_epochs = <span class="hljs-number"><span class="hljs-number">40</span></span> model.fit(trainX, trainY.reshape(trainY.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], trainY.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>), epochs=num_epochs, batch_size=<span class="hljs-number"><span class="hljs-number">512</span></span>, validation_split=<span class="hljs-number"><span class="hljs-number">0.2</span></span>, callbacks=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">1</span></span>) model.save(<span class="hljs-string"><span class="hljs-string">'en-de-model.h5'</span></span>)</code> </pre><br>  ุงูุชุฏุฑูุจ ูู ุญุฏ ุฐุงุชู ูุจุฏู ูุซู ูุฐุง: <br><br><img src="https://habrastorage.org/webt/fj/xl/b4/fjxlb4yorszz5ojzpcih4ixlria.png"><br><br>  ุงูุนูููุฉ ุ ููุง ุชุฑูู ุ ููุณุช ุณุฑูุนุฉ ุ ูุชุณุชุบุฑู ุญูุงูู ูุตู ุณุงุนุฉ ุนูู Core i7 + GeForce 1060 ููุฌููุนุฉ ูู 30 ุฃูู ุณุทุฑ.  ูู ููุงูุฉ ุงูุชุฏุฑูุจ (ูุฌุจ ุฃู ูุชู ุฐูู ูุฑุฉ ูุงุญุฏุฉ ููุท) ุ ูุชู ุญูุธ ุงููููุฐุฌ ูู ููู ุ ุซู ูููู ุฅุนุงุฏุฉ ุงุณุชุฎุฏุงูู. <br><br>  ููุญุตูู ุนูู ุงูุชุฑุฌูุฉ ุ ูุณุชุฎุฏู ุฏุงูุฉ Forecast_classes ุ ุงูุชู ููุฏู ูุฏุฎูุงุช ูููุง ุจุถุน ุนุจุงุฑุงุช ุจุณูุทุฉ.  ูุชู ุงุณุชุฎุฏุงู ุงูุฏุงูุฉ get_word ูุนูุณ ุงููููุงุช ุฅูู ุฃุฑูุงู. <br><br><pre style=";text-align:right;direction:rtl"> <code class="python hljs">model = load_model(<span class="hljs-string"><span class="hljs-string">'en-de-model.h5'</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_word</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n, tokenizer)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n == <span class="hljs-number"><span class="hljs-number">0</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-string"><span class="hljs-string">""</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word, index <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tokenizer.word_index.items(): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> index == n: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-string"><span class="hljs-string">""</span></span> phrs_enc = encode_sequences(eng_tokenizer, eng_length, [<span class="hljs-string"><span class="hljs-string">"the weather is nice today"</span></span>, <span class="hljs-string"><span class="hljs-string">"my name is tom"</span></span>, <span class="hljs-string"><span class="hljs-string">"how old are you"</span></span>, <span class="hljs-string"><span class="hljs-string">"where is the nearest shop"</span></span>]) preds = model.predict_classes(phrs_enc) print(<span class="hljs-string"><span class="hljs-string">"Preds:"</span></span>, preds.shape) print(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer)) print(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer)) print(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer)) print(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer))</code> </pre><br><h2 style=";text-align:right;direction:rtl">  ุงููุชุงุฆุฌ </h2><br>  ุงูุขู ุ ูู ุงููุงูุน ุ ุงูุดูุก ุงูุฃูุซุฑ ูุถููุงู ูู ุงููุชุงุฆุฌ.  ูู ุงููุซูุฑ ููุงูุชูุงู ูุนุฑูุฉ ููููุฉ ุชุนูู ุงูุดุจูุฉ ุงูุนุตุจูุฉ ู "ุชุฐูุฑ" ุงููุฑุงุณูุงุช ุจูู ุงูุฌูู ุงูุฅูุฌููุฒูุฉ ูุงูุฃููุงููุฉ.  ุฃุฎุฐุช ุนูู ูุฌู ุงูุชุญุฏูุฏ ุนุจุงุฑุงุช 2 ุฃุณูู ูุฃุตุนุจ 2 ููุนุฑูุฉ ุงููุฑู. <br><br>  <b>5 ุฏูุงุฆู ูู ุงูุชุฏุฑูุจ</b> <br><br>  "ุงูุฌู ุฌููู ุงูููู" - "das ist ist tom" <br>  "ุงุณูู ุชูู" - "wie fรผr tom tom" <br>  "ูู ุนูุฑู" - "wie geht ist es" <br>  "ุฃูู ููุฌุฏ ุฃูุฑุจ ูุชุฌุฑ" - "wo ist der" <br><br>  ููุง ุชุฑูู ุ ุญุชู ุงูุขู ููุงู ุนุฏุฏ ูููู ูู "ุงูุฒูุงุฑุงุช".  ููุงู ุฌุฒุก ูู ุนุจุงุฑุฉ "ูู ุนูุฑู" ูุฎูุท ุจูู ุงูุดุจูุฉ ุงูุนุตุจูุฉ ูุนุจุงุฑุฉ "ููู ุญุงูู" ูุฃูุชุฌ ุชุฑุฌูุฉ "wie geht ist es" (ููู ุญุงููุ).  ูู ุนุจุงุฑุฉ "ุฃูู ..." ุ ุญุฏุฏุช ุงูุดุจูุฉ ุงูุนุตุจูุฉ ุงููุนู ููุท ูุฃูุชุฌุช ุงูุชุฑุฌูุฉ "wo ist der" (ุฃูู ููุ) ุ ูุงูุชู ุ ูู ุญูุซ ุงููุจุฏุฃ ุ ูุง ุชุฎูู ูู ูุนูู.  ุจุดูู ุนุงู ุ ุชูุฑูุจูุง ููุณ ุงูุดูุก ุงูุฐู ูุชุฑุฌู ุฅูู ุงูุฃููุงููุฉ ูุงุฏููุง ุฌุฏูุฏูุง ุฅูู ุงููุฌููุนุฉ A1 ุ) <br><br>  <b>10 ุฏูุงุฆู ูู ุงูุชุฏุฑูุจ</b> <br><br>  "ุงูุฌู ุฌููู ุงูููู" - "das haus ist bereit" <br>  "ุงุณูู ุชูู" - "mein heiรe heiรe tom" <br>  "ูู ุนูุฑู" - "wie alt sind sie" <br>  "ุฃูู ููุฌุฏ ุฃูุฑุจ ูุชุฌุฑ" - "wo ist paris" <br><br>  ุจุนุถ ุงูุชูุฏู ูุฑุฆู.  ุงูุนุจุงุฑุฉ ุงูุฃููู ูู ุฎุงุฑุฌ ุงูููุงู ุชูุงูุง.  ูู ุงูุนุจุงุฑุฉ ุงูุซุงููุฉ ุ "ุชุนููุช" ุงูุดุจูุฉ ุงูุนุตุจูุฉ ุงููุนู heiรen (ูุณูู) ุ ูููู "mein heiรe heiรe tom" ูุง ุชุฒุงู ุบูุฑ ุตุญูุญุฉ ุ ุนูู ุงูุฑุบู ูู ุฃูู ููููู ุจุงููุนู ุชุฎููู ุงููุนูู.  ุงูุนุจุงุฑุฉ ุงูุซุงูุซุฉ ุตุญูุญุฉ ุจุงููุนู.  ูู ุงูุฌุฒุก ุงูุฑุงุจุน ุ ุงูุฌุฒุก ุงูุฃูู ุงูุตุญูุญ ูู "wo ist" ุ ูููู ุชู ุงุณุชุจุฏุงู ุฃูุฑุจ ูุชุฌุฑ ูุณุจุจ ูุง ุจุจุงุฑูุณ. <br><br>  <b>30 ุฏูููุฉ ูู ุงูุชุฏุฑูุจ</b> <br><br>  "ุงูุฌู ุฌููู ุงูููู" - "das ist ist aus" <br>  "ุงุณูู ุชูู" - "ุชูู" ist mein name " <br>  "ูู ุนูุฑู" - "wie alt sind sie" <br>  "ุฃูู ููุฌุฏ ุฃูุฑุจ ูุชุฌุฑ" - "wo ist der" <br><br>  ููุง ุชุฑู ุ ุงูุนุจุงุฑุฉ ุงูุซุงููุฉ ุฃุตุจุญุช ุตุญูุญุฉ ุ ุนูู ุงูุฑุบู ูู ุฃู ุงูุชุตููู ูุจุฏู ุบูุฑ ุนุงุฏู ุฅูู ุญุฏ ูุง.  ุงูุนุจุงุฑุฉ ุงูุซุงูุซุฉ ุตุญูุญุฉ ุ ููู ุงูุฌููุชูู ุงูุฃููู ูุงูุฑุงุจุนุฉ ูู ูุชู "ุชุนููููุง" ุจุนุฏ.  ูุน ูุฐุง ูู <s>ุฃุฌู ุชูููุฑ ุงูููุฑุจุงุก ุ</s> ุงูุชููุช ูู ุงูุนูููุฉ. <br><br><h2 style=";text-align:right;direction:rtl">  ุงุณุชูุชุงุฌ </h2><br>  ููุง ุชุฑูู ุ ูู ุญูุซ ุงููุจุฏุฃ ุ ูุฐุง ูุนูู.  ุฃูุฏ ุฃู ุฃุญูุธ ูุบุฉ ุฌุฏูุฏุฉ ุจูุฐู ุงูุณุฑุนุฉ :) ุจุงูุทุจุน ุ ุงููุชูุฌุฉ ููุณุช ูุซุงููุฉ ุญุชู ุงูุขู ุ ูููู ุงูุชุฏุฑูุจ ุนูู ูุฌููุนุฉ ูุงููุฉ ูู 190 ุฃูู ุฎุท ุณูุณุชุบุฑู ุฃูุซุฑ ูู ุณุงุนุฉ ูุงุญุฏุฉ. <br><br>  ุจุงููุณุจุฉ ูุฃููุฆู ุงูุฐูู ูุฑุบุจูู ูู ุชุฌุฑุจุฉ ูู ุชููุงุก ุฃููุณูู ุ ุดูุฑุฉ ุงููุตุฏุฑ ูู ุชุญุช ุงูููุณุฏ.  ูููู ููุจุฑูุงูุฌ ูู ุงููุงุญูุฉ ุงููุธุฑูุฉ ุงุณุชุฎุฏุงู ุฃู ุฒูุฌ ูู ุงููุบุงุช ุ ูููุณ ุงูุฅูุฌููุฒูุฉ ูุงูุฃููุงููุฉ ููุท (ูุฌุจ ุฃู ูููู ุงูููู ุจุชุฑููุฒ UTF-8).  ุชุธู ูุณุฃูุฉ ุฌูุฏุฉ ุงูุชุฑุฌูุฉ ููุชูุญุฉ ุฃูุถูุง ุ ูููุงู ุดูุก ูุฌุจ ุงุฎุชุจุงุฑู. <br><br><div class="spoiler" style=";text-align:right;direction:rtl">  <b class="spoiler_title">keras_translate.py</b> <div class="spoiler_text" style=";text-align:right;direction:rtl"><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-comment"><span class="hljs-comment"># os.environ["CUDA_VISIBLE_DEVICES"] = "-1" # Force CPU os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # 0 = all messages are logged, 3 - INFO, WARNING, and ERROR messages are not printed import string import re import numpy as np import pandas as pd from keras.models import Sequential from keras.layers import Dense, LSTM, Embedding, RepeatVector from keras.preprocessing.text import Tokenizer from keras.callbacks import ModelCheckpoint from keras.preprocessing.sequence import pad_sequences from keras.models import load_model from keras import optimizers from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt pd.set_option('display.max_colwidth', 200) # Read raw text file def read_text(filename): with open(filename, mode='rt', encoding='utf-8') as file: text = file.read() sents = text.strip().split('\n') return [i.split('\t') for i in sents] data = read_text("deutch.txt") deu_eng = np.array(data) deu_eng = deu_eng[:30000,:] print("Dictionary size:", deu_eng.shape) # Remove punctuation deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]] deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]] # Convert text to lowercase for i in range(len(deu_eng)): deu_eng[i,0] = deu_eng[i,0].lower() deu_eng[i,1] = deu_eng[i,1].lower() # Prepare English tokenizer eng_tokenizer = Tokenizer() eng_tokenizer.fit_on_texts(deu_eng[:, 0]) eng_vocab_size = len(eng_tokenizer.word_index) + 1 eng_length = 8 # Prepare Deutch tokenizer deu_tokenizer = Tokenizer() deu_tokenizer.fit_on_texts(deu_eng[:, 1]) deu_vocab_size = len(deu_tokenizer.word_index) + 1 deu_length = 8 # Encode and pad sequences def encode_sequences(tokenizer, length, lines): # integer encode sequences seq = tokenizer.texts_to_sequences(lines) # pad sequences with 0 values seq = pad_sequences(seq, maxlen=length, padding='post') return seq # Split data into train and test set train, test = train_test_split(deu_eng, test_size=0.2, random_state=12) # Prepare training data trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 0]) trainY = encode_sequences(deu_tokenizer, deu_length, train[:, 1]) # Prepare validation data testX = encode_sequences(eng_tokenizer, eng_length, test[:, 0]) testY = encode_sequences(deu_tokenizer, deu_length, test[:, 1]) # Build NMT model def make_model(in_vocab, out_vocab, in_timesteps, out_timesteps, n): model = Sequential() model.add(Embedding(in_vocab, n, input_length=in_timesteps, mask_zero=True)) model.add(LSTM(n)) model.add(Dropout(0.3)) model.add(RepeatVector(out_timesteps)) model.add(LSTM(n, return_sequences=True)) model.add(Dropout(0.3)) model.add(Dense(out_vocab, activation='softmax')) model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='sparse_categorical_crossentropy') return model print("deu_vocab_size:", deu_vocab_size, deu_length) print("eng_vocab_size:", eng_vocab_size, eng_length) # Model compilation (with 512 hidden units) model = make_model(eng_vocab_size, deu_vocab_size, eng_length, deu_length, 512) # Train model num_epochs = 250 history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), epochs=num_epochs, batch_size=512, validation_split=0.2, callbacks=None, verbose=1) # plt.plot(history.history['loss']) # plt.plot(history.history['val_loss']) # plt.legend(['train','validation']) # plt.show() model.save('en-de-model.h5') # Load model model = load_model('en-de-model.h5') def get_word(n, tokenizer): if n == 0: return "" for word, index in tokenizer.word_index.items(): if index == n: return word return "" phrs_enc = encode_sequences(eng_tokenizer, eng_length, ["the weather is nice today", "my name is tom", "how old are you", "where is the nearest shop"]) print("phrs_enc:", phrs_enc.shape) preds = model.predict_classes(phrs_enc) print("Preds:", preds.shape) print(preds[0]) print(get_word(preds[0][0], deu_tokenizer), get_word(preds[0][1], deu_tokenizer), get_word(preds[0][2], deu_tokenizer), get_word(preds[0][3], deu_tokenizer)) print(preds[1]) print(get_word(preds[1][0], deu_tokenizer), get_word(preds[1][1], deu_tokenizer), get_word(preds[1][2], deu_tokenizer), get_word(preds[1][3], deu_tokenizer)) print(preds[2]) print(get_word(preds[2][0], deu_tokenizer), get_word(preds[2][1], deu_tokenizer), get_word(preds[2][2], deu_tokenizer), get_word(preds[2][3], deu_tokenizer)) print(preds[3]) print(get_word(preds[3][0], deu_tokenizer), get_word(preds[3][1], deu_tokenizer), get_word(preds[3][2], deu_tokenizer), get_word(preds[3][3], deu_tokenizer)) print()</span></span></code> </pre><br></div></div><br>  ุงููุงููุณ ููุณู ูุจูุฑ ุฌุฏูุง ุจุญูุซ ูุง ูููู ุฅุฑูุงูู ุจุงูููุงู ุ ูุงูุฑุงุจุท ููุฌูุฏ ูู ุงูุชุนูููุงุช. <br><br>  ูุงูุนุงุฏุฉ ุ ูู ุงูุชุฌุงุฑุจ ุงููุงุฌุญุฉ. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/ar470706/">https://habr.com/ru/post/ar470706/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ar470688/index.html">ูุง ูู ูุนุฑูู ุนู VMworld 2019</a></li>
<li><a href="../ar470692/index.html">ููู ุตูุนูุง ูููุน Rosbank ุฌุฏูุฏ ุ ููุงุฐุง ุฌุงุก ููู</a></li>
<li><a href="../ar470694/index.html">ุงุฎุชูุงุฑ ููุตุฉ ุงูุชุณููู ุนุจุฑ ุงูุจุฑูุฏ ุงูุฅููุชุฑููู: ูุง ูุฌุจ ุงูุงูุชุจุงู ุฅูู ุงูุดุฑูุงุช ุงูุฑูุณูุฉ</a></li>
<li><a href="../ar470696/index.html">ููุงุฐุง ูุนุชุจุฑ ูุงูุฏู ุฌูุฏูุง ูู ุงูุชุนุฑู ุนูู ุงูููุงูุ (ุชู ุงูุชุญุฏูุซ ูู 12.25.2019)</a></li>
<li><a href="../ar470700/index.html">ุงูุฌุฏูู. ุงููุนุฏู. ุงูุตูุช. ููุ</a></li>
<li><a href="../ar470710/index.html">ุชุนูู ุงูุขูุฉ ูุตูุฏู ุงููุณุทุญ. ุงูุฌุฒุก 2</a></li>
<li><a href="../ar470718/index.html">"ุงูุขุซุงุฑ ุงูุฌุจุฑูุฉ" ูู ุงููุบุฉ ุงูุจุดุฑูุฉ</a></li>
<li><a href="../ar470720/index.html">ููููุฉ ูุชุงุจุฉ ุนูุฏ ุฐูู ูุน ุจูุซูู ุนูู ุงูุฃูุทูููุฌูุงุ ุงูุฌุฒุก 2: ุงูุชุฎุฒูู API</a></li>
<li><a href="../ar470722/index.html">ููููุฉ ูุชุงุจุฉ ุนูุฏ ุฐูู ูุน ุจูุซูู ุนูู ุงูุฃูุทูููุฌูุงุ ุงูุฌุฒุก 3: API ููุช ุงูุชุดุบูู</a></li>
<li><a href="../ar470726/index.html">ููู ูุง ูุบุฑู ูู ุงูุฑูุชูู ุ ุฃู ุชุฌุฑุจุชูุง ููุงุฑูุฉ AWR ููุงูุจ ุฃุซูุงุก ุงุฎุชุจุงุฑ ุงูุฅุฌูุงุฏ</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>