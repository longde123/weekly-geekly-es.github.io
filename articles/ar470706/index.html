<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘©ğŸ¼â€ğŸ¤â€ğŸ‘¨ğŸ½ ğŸ¤´ğŸ¼ ğŸ“ Python + Keras + LSTM: Ø§ØµÙ†Ø¹ Ù…ØªØ±Ø¬Ù… Ù†Øµ ÙÙŠ Ù†ØµÙ Ø³Ø§Ø¹Ø© ğŸ’™ ğŸ‘ğŸ¿ ğŸ§‘ğŸ¿â€ğŸ¤â€ğŸ§‘ğŸ¼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ù…Ø±Ø­Ø¨Ø§ ÙŠØ§ Ù‡Ø¨Ø±. 

 ÙÙŠ Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø³Ø§Ø¨Ù‚ ØŒ Ù†Ø¸Ø±Øª Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ ØªÙ…ÙŠÙŠØ² Ø¨Ø³ÙŠØ· Ù„Ù„Ù†Øµ Ø¹Ù„Ù‰ Ø£Ø³Ø§Ø³ Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ©. Ø§Ù„ÙŠÙˆÙ… Ø³ÙˆÙ Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù‡Ø¬Ù‹Ø§ Ù…Ù…Ø§Ø«Ù„Ù‹Ø§ ÙˆÙ†ÙƒØªØ¨ Ù…ØªØ±Ø¬Ù…Ù‹Ø§ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ù„Ù„Ù†ØµÙˆØµ Ù…Ù†...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Python + Keras + LSTM: Ø§ØµÙ†Ø¹ Ù…ØªØ±Ø¬Ù… Ù†Øµ ÙÙŠ Ù†ØµÙ Ø³Ø§Ø¹Ø©</h1><div class="post__body post__body_full" style=";text-align:right;direction:rtl"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/470706/" style=";text-align:right;direction:rtl">  Ù…Ø±Ø­Ø¨Ø§ ÙŠØ§ Ù‡Ø¨Ø±. <br><br>  ÙÙŠ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø³Ø§Ø¨Ù‚ ØŒ</a> Ù†Ø¸Ø±Øª Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ ØªÙ…ÙŠÙŠØ² Ø¨Ø³ÙŠØ· Ù„Ù„Ù†Øµ Ø¹Ù„Ù‰ Ø£Ø³Ø§Ø³ Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ©.  Ø§Ù„ÙŠÙˆÙ… Ø³ÙˆÙ Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù‡Ø¬Ù‹Ø§ Ù…Ù…Ø§Ø«Ù„Ù‹Ø§ ÙˆÙ†ÙƒØªØ¨ Ù…ØªØ±Ø¬Ù…Ù‹Ø§ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ù„Ù„Ù†ØµÙˆØµ Ù…Ù† Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù„Ù…Ø§Ù†ÙŠØ©. <br><br><img src="https://habrastorage.org/webt/gf/ft/jx/gfftjxwflb7yxrwtffish1hkqsc.jpeg"><br><br>  Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ø£ÙˆÙ„Ø¦Ùƒ Ø§Ù„Ø°ÙŠÙ† ÙŠÙ‡ØªÙ…ÙˆÙ† Ø¨ÙƒÙŠÙÙŠØ© Ø¹Ù…Ù„ Ø°Ù„Ùƒ ØŒ ÙØ¥Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ù‚ÙŠØ¯ Ø§Ù„ØªÙ†ÙÙŠØ°. <br><a name="habracut"></a><br>  <i>Ù…Ù„Ø§Ø­Ø¸Ø©</i> : Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø®Ø§Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ© Ù„Ù„ØªØ±Ø¬Ù…Ø© Ù‡Ùˆ Ø¨Ø±Ù†Ø§Ù…Ø¬ ØªØ¹Ù„ÙŠÙ…ÙŠ Ø­ØµØ±ÙŠ ØŒ ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠ ÙØ¥Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ "Ù„Ù…Ø§Ø°Ø§" Ù„Ù… ÙŠØªÙ… Ø§Ù„Ù†Ø¸Ø± ÙÙŠÙ‡.  Ù„Ù„Ù…ØªØ¹Ø© ÙÙ‚Ø·.  Ù„Ø§ Ø£Ù‚ØµØ¯ Ø¥Ø«Ø¨Ø§Øª Ø£Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø£Ùˆ ØªÙ„Ùƒ Ø£ÙØ¶Ù„ Ø£Ùˆ Ø£Ø³ÙˆØ£ ØŒ Ù„Ù‚Ø¯ ÙƒØ§Ù† Ù…Ù† Ø§Ù„Ù…Ø«ÙŠØ± Ù„Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ ÙŠØ­Ø¯Ø«.  Ø¥Ù† Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ø£Ø¯Ù†Ø§Ù‡ Ù…Ø¨Ø³Ø·Ø© ØŒ Ø¨Ø§Ù„Ø·Ø¨Ø¹ ØŒ Ù„ÙƒÙ†Ù†ÙŠ Ø¢Ù…Ù„ Ø£Ù„Ø§ ÙŠØ£Ù…Ù„ Ø£Ø­Ø¯ ÙÙŠ Ø£Ù† Ù†ÙƒØªØ¨ Ø±Ø³Ø§Ù„Ø© Lingvo Ø«Ø§Ù†ÙŠØ© Ø®Ù„Ø§Ù„ Ù†ØµÙ Ø³Ø§Ø¹Ø©. <br><br><h2 style=";text-align:right;direction:rtl">  Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª </h2><br>  ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù„Ù Ù…ÙˆØ¬ÙˆØ¯ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø¨ÙƒØ© ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ø¨Ø§Ø±Ø§Øª Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙˆØ£Ù„Ù…Ø§Ù†ÙŠØ© Ù…ÙØµÙˆÙ„Ø© Ø¨Ø¹Ù„Ø§Ù…Ø§Øª Ø¬Ø¯ÙˆÙ„Ø© ÙƒÙ…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØµØ¯Ø±.  Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª ØªØ¨Ø¯Ùˆ Ù…Ø«Ù„ Ù‡Ø°Ø§: <br><br><pre style=";text-align:right;direction:rtl"><code class="python hljs">Hi. Hallo! Hi. GrÃ¼ÃŸ Gott! Run! Lauf! Wow! Potzdonner! Wow! Donnerwetter! Fire! Feuer! Help! Hilfe! Help! Zu HÃ¼lf! Stop! Stopp! Wait! Warte! Go on. Mach weiter. Hello! Hallo! I ran. Ich rannte. I see. Ich verstehe. ...</code> </pre> <br>  ÙŠØ­ØªÙˆÙŠ Ø§Ù„Ù…Ù„Ù Ø¹Ù„Ù‰ 192 Ø£Ù„Ù Ø³Ø·Ø± ÙˆÙ„Ù‡ Ø­Ø¬Ù… 13 Ù…ÙŠØºØ§Ø¨Ø§ÙŠØª.  Ù†Ù‚ÙˆÙ… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Øµ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ÙƒØªÙ„ØªÙŠÙ† ØŒ Ù„ÙƒÙ„Ù…Ø§Øª Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙˆØ£Ù„Ù…Ø§Ù†ÙŠØ©. <br><br><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">read_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(filename, mode=<span class="hljs-string"><span class="hljs-string">'rt'</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> file: text = file.read() sents = text.strip().split(<span class="hljs-string"><span class="hljs-string">'\n'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [i.split(<span class="hljs-string"><span class="hljs-string">'\t'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> sents] data = read_text(<span class="hljs-string"><span class="hljs-string">"deutch.txt"</span></span>) deu_eng = np.array(data) deu_eng = deu_eng[:<span class="hljs-number"><span class="hljs-number">30000</span></span>,:] print(<span class="hljs-string"><span class="hljs-string">"Dictionary size:"</span></span>, deu_eng.shape) <span class="hljs-comment"><span class="hljs-comment"># Remove punctuation deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]] deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]] # convert text to lowercase for i in range(len(deu_eng)): deu_eng[i,0] = deu_eng[i,0].lower() deu_eng[i,1] = deu_eng[i,1].lower()</span></span></code> </pre><br>  Ù‚Ù…Ù†Ø§ Ø£ÙŠØ¶Ù‹Ø§ Ø¨ØªØ­ÙˆÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¥Ù„Ù‰ Ø£Ø­Ø±Ù ØµØºÙŠØ±Ø© ÙˆØ¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ…. <br><br>  ÙˆØ§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù‡ÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©.  Ø§Ù„Ø´Ø¨ÙƒØ© Ù„Ø§ ØªØ¹Ø±Ù Ù…Ø§ Ù‡ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØŒ ÙˆØªØ¹Ù…Ù„ Ø­ØµØ±Ø§ Ù…Ø¹ Ø§Ù„Ø£Ø±Ù‚Ø§Ù….  Ù„Ø­Ø³Ù† Ø§Ù„Ø­Ø¸ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù†Ø§ ØŒ ØªØ­ØªÙˆÙŠ keras Ø¨Ø§Ù„ÙØ¹Ù„ Ø¹Ù„Ù‰ ÙØ¦Ø© Tokenizer ØŒ ÙˆØ§Ù„ØªÙŠ ØªØ­Ù„ Ù…Ø­Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙÙŠ Ø§Ù„Ø¬Ù…Ù„ Ø¨Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø±Ù‚Ù…ÙŠØ©. <br><br>  ÙŠØªØ¶Ø­ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ø¨Ø¨Ø³Ø§Ø·Ø© Ù…Ø¹ Ù…Ø«Ø§Ù„: <br><br><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.text <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Tokenizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.sequence <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pad_sequences s = <span class="hljs-string"><span class="hljs-string">"To be or not to be"</span></span> eng_tokenizer = Tokenizer() eng_tokenizer.fit_on_texts([s]) seq = eng_tokenizer.texts_to_sequences([s]) seq = pad_sequences(seq, maxlen=<span class="hljs-number"><span class="hljs-number">8</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'post'</span></span>) print(seq)</code> </pre><br>  Ø³ÙŠØªÙ… Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© "Ù„ØªÙƒÙˆÙ† Ø£Ùˆ Ù„Ø§ ØªÙƒÙˆÙ†" Ø¨Ø§Ù„ØµÙÙŠÙ [1 2 3 4 1 2 0 0] ØŒ Ø­ÙŠØ« ÙŠØµØ¹Ø¨ ØªØ®Ù…ÙŠÙ†Ù‡ ØŒ 1 = Ø¥Ù„Ù‰ ØŒ 2 = ÙŠÙƒÙˆÙ† ØŒ 3 = Ø£Ùˆ ØŒ 4 = Ù„Ø§.  ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø§Ù„ÙØ¹Ù„ ØªÙ‚Ø¯ÙŠÙ… Ù‡Ø°Ù‡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©. <br><br><h2 style=";text-align:right;direction:rtl">  ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© </h2><br>  Ø¨ÙŠØ§Ù†Ø§ØªÙ†Ø§ Ø¬Ø§Ù‡Ø²Ø© Ø±Ù‚Ù…ÙŠØ§.  Ù†Ù‚Ø³Ù… Ø§Ù„ØµÙÙŠÙ Ø¥Ù„Ù‰ ÙƒØªÙ„ØªÙŠÙ† Ù„Ø¥Ø¯Ø®Ø§Ù„ (Ø§Ù„Ø®Ø·ÙˆØ· Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©) ÙˆØ¥Ø®Ø±Ø§Ø¬ (Ø§Ù„Ø®Ø·ÙˆØ· Ø§Ù„Ø£Ù„Ù…Ø§Ù†ÙŠØ©) Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.  Ø³Ù†Ù‚ÙˆÙ… Ø£ÙŠØ¶Ù‹Ø§ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØ­Ø¯Ø© Ù…Ù†ÙØµÙ„Ø© Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù…. <br><br><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># split data into train and test set train, test = train_test_split(deu_eng, test_size=0.2, random_state=12) # prepare training data trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 0]) trainY = encode_sequences(deu_tokenizer, deu_length, train[:, 1]) # prepare validation data testX = encode_sequences(eng_tokenizer, eng_length, test[:, 0]) testY = encode_sequences(deu_tokenizer, deu_length, test[:, 1])</span></span></code> </pre><br>  Ø§Ù„Ø¢Ù† ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© ÙˆØ¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨.  ÙƒÙ…Ø§ ØªØ±ÙˆÙ† ØŒ ØªØ­ØªÙˆÙŠ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø¹Ù„Ù‰ Ø·Ø¨Ù‚Ø§Øª LSTM Ù„Ù‡Ø§ Ø®Ù„Ø§ÙŠØ§ Ø°Ø§ÙƒØ±Ø©.  Ø¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù†Ù‡ Ù…Ù† Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ø£Ù† ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ Ø´Ø¨ÙƒØ© "Ù…Ù†ØªØ¸Ù…Ø©" ØŒ ÙŠÙ…ÙƒÙ† Ù„Ø£ÙˆÙ„Ø¦Ùƒ Ø§Ù„Ø°ÙŠÙ† ÙŠØ±ØºØ¨ÙˆÙ† ÙÙŠ Ø§Ù„ØªØ­Ù‚Ù‚ Ø¨Ù†ÙØ³Ù‡Ù…. <br><br><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">make_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(in_vocab, out_vocab, in_timesteps, out_timesteps, n)</span></span></span><span class="hljs-function">:</span></span> model = Sequential() model.add(Embedding(in_vocab, n, input_length=in_timesteps, mask_zero=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) model.add(LSTM(n)) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.3</span></span>)) model.add(RepeatVector(out_timesteps)) model.add(LSTM(n, return_sequences=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.3</span></span>)) model.add(Dense(out_vocab, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)) model.compile(optimizer=optimizers.RMSprop(lr=<span class="hljs-number"><span class="hljs-number">0.001</span></span>), loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model eng_vocab_size = len(eng_tokenizer.word_index) + <span class="hljs-number"><span class="hljs-number">1</span></span> deu_vocab_size = len(deu_tokenizer.word_index) + <span class="hljs-number"><span class="hljs-number">1</span></span> eng_length, deu_length = <span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span> model = make_model(eng_vocab_size, deu_vocab_size, eng_length, deu_length, <span class="hljs-number"><span class="hljs-number">512</span></span>) num_epochs = <span class="hljs-number"><span class="hljs-number">40</span></span> model.fit(trainX, trainY.reshape(trainY.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], trainY.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>), epochs=num_epochs, batch_size=<span class="hljs-number"><span class="hljs-number">512</span></span>, validation_split=<span class="hljs-number"><span class="hljs-number">0.2</span></span>, callbacks=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">1</span></span>) model.save(<span class="hljs-string"><span class="hljs-string">'en-de-model.h5'</span></span>)</code> </pre><br>  Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙÙŠ Ø­Ø¯ Ø°Ø§ØªÙ‡ ÙŠØ¨Ø¯Ùˆ Ù…Ø«Ù„ Ù‡Ø°Ø§: <br><br><img src="https://habrastorage.org/webt/fj/xl/b4/fjxlb4yorszz5ojzpcih4ixlria.png"><br><br>  Ø§Ù„Ø¹Ù…Ù„ÙŠØ© ØŒ ÙƒÙ…Ø§ ØªØ±ÙˆÙ† ØŒ Ù„ÙŠØ³Øª Ø³Ø±ÙŠØ¹Ø© ØŒ ÙˆØªØ³ØªØºØ±Ù‚ Ø­ÙˆØ§Ù„ÙŠ Ù†ØµÙ Ø³Ø§Ø¹Ø© Ø¹Ù„Ù‰ Core i7 + GeForce 1060 Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† 30 Ø£Ù„Ù Ø³Ø·Ø±.  ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (ÙŠØ¬Ø¨ Ø£Ù† ÙŠØªÙ… Ø°Ù„Ùƒ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·) ØŒ ÙŠØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ù…Ù„Ù ØŒ Ø«Ù… ÙŠÙ…ÙƒÙ† Ø¥Ø¹Ø§Ø¯Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡. <br><br>  Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±Ø¬Ù…Ø© ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© Forecast_classes ØŒ Ø§Ù„ØªÙŠ Ù†Ù‚Ø¯Ù… Ù…Ø¯Ø®Ù„Ø§Øª Ù…Ù†Ù‡Ø§ Ø¨Ø¶Ø¹ Ø¹Ø¨Ø§Ø±Ø§Øª Ø¨Ø³ÙŠØ·Ø©.  ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø© get_word Ù„Ø¹ÙƒØ³ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù…. <br><br><pre style=";text-align:right;direction:rtl"> <code class="python hljs">model = load_model(<span class="hljs-string"><span class="hljs-string">'en-de-model.h5'</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_word</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n, tokenizer)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n == <span class="hljs-number"><span class="hljs-number">0</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-string"><span class="hljs-string">""</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word, index <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tokenizer.word_index.items(): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> index == n: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-string"><span class="hljs-string">""</span></span> phrs_enc = encode_sequences(eng_tokenizer, eng_length, [<span class="hljs-string"><span class="hljs-string">"the weather is nice today"</span></span>, <span class="hljs-string"><span class="hljs-string">"my name is tom"</span></span>, <span class="hljs-string"><span class="hljs-string">"how old are you"</span></span>, <span class="hljs-string"><span class="hljs-string">"where is the nearest shop"</span></span>]) preds = model.predict_classes(phrs_enc) print(<span class="hljs-string"><span class="hljs-string">"Preds:"</span></span>, preds.shape) print(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer)) print(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer)) print(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer)) print(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer))</code> </pre><br><h2 style=";text-align:right;direction:rtl">  Ø§Ù„Ù†ØªØ§Ø¦Ø¬ </h2><br>  Ø§Ù„Ø¢Ù† ØŒ ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ ØŒ Ø§Ù„Ø´ÙŠØ¡ Ø§Ù„Ø£ÙƒØ«Ø± ÙØ¶ÙˆÙ„Ø§Ù‹ Ù‡Ùˆ Ø§Ù„Ù†ØªØ§Ø¦Ø¬.  Ù…Ù† Ø§Ù„Ù…Ø«ÙŠØ± Ù„Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ù…Ø¹Ø±ÙØ© ÙƒÙŠÙÙŠØ© ØªØ¹Ù„Ù… Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ùˆ "ØªØ°ÙƒØ±" Ø§Ù„Ù…Ø±Ø§Ø³Ù„Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙˆØ§Ù„Ø£Ù„Ù…Ø§Ù†ÙŠØ©.  Ø£Ø®Ø°Øª Ø¹Ù„Ù‰ ÙˆØ¬Ù‡ Ø§Ù„ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¨Ø§Ø±Ø§Øª 2 Ø£Ø³Ù‡Ù„ ÙˆØ£ØµØ¹Ø¨ 2 Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„ÙØ±Ù‚. <br><br>  <b>5 Ø¯Ù‚Ø§Ø¦Ù‚ Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨</b> <br><br>  "Ø§Ù„Ø¬Ùˆ Ø¬Ù…ÙŠÙ„ Ø§Ù„ÙŠÙˆÙ…" - "das ist ist tom" <br>  "Ø§Ø³Ù…ÙŠ ØªÙˆÙ…" - "wie fÃ¼r tom tom" <br>  "ÙƒÙ… Ø¹Ù…Ø±Ùƒ" - "wie geht ist es" <br>  "Ø£ÙŠÙ† ÙŠÙˆØ¬Ø¯ Ø£Ù‚Ø±Ø¨ Ù…ØªØ¬Ø±" - "wo ist der" <br><br>  ÙƒÙ…Ø§ ØªØ±ÙˆÙ† ØŒ Ø­ØªÙ‰ Ø§Ù„Ø¢Ù† Ù‡Ù†Ø§Ùƒ Ø¹Ø¯Ø¯ Ù‚Ù„ÙŠÙ„ Ù…Ù† "Ø§Ù„Ø²ÙŠØ§Ø±Ø§Øª".  Ù‡Ù†Ø§Ùƒ Ø¬Ø²Ø¡ Ù…Ù† Ø¹Ø¨Ø§Ø±Ø© "ÙƒÙ… Ø¹Ù…Ø±Ùƒ" ÙŠØ®Ù„Ø· Ø¨ÙŠÙ† Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© ÙˆØ¹Ø¨Ø§Ø±Ø© "ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ" ÙˆØ£Ù†ØªØ¬ ØªØ±Ø¬Ù…Ø© "wie geht ist es" (ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ).  ÙÙŠ Ø¹Ø¨Ø§Ø±Ø© "Ø£ÙŠÙ† ..." ØŒ Ø­Ø¯Ø¯Øª Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„ÙØ¹Ù„ ÙÙ‚Ø· ÙˆØ£Ù†ØªØ¬Øª Ø§Ù„ØªØ±Ø¬Ù…Ø© "wo ist der" (Ø£ÙŠÙ† Ù‡ÙˆØŸ) ØŒ ÙˆØ§Ù„ØªÙŠ ØŒ Ù…Ù† Ø­ÙŠØ« Ø§Ù„Ù…Ø¨Ø¯Ø£ ØŒ Ù„Ø§ ØªØ®Ù„Ùˆ Ù…Ù† Ù…Ø¹Ù†Ù‰.  Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù… ØŒ ØªÙ‚Ø±ÙŠØ¨Ù‹Ø§ Ù†ÙØ³ Ø§Ù„Ø´ÙŠØ¡ Ø§Ù„Ø°ÙŠ ÙŠØªØ±Ø¬Ù… Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù„Ù…Ø§Ù†ÙŠØ© Ù‚Ø§Ø¯Ù…Ù‹Ø§ Ø¬Ø¯ÙŠØ¯Ù‹Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© A1 Ø›) <br><br>  <b>10 Ø¯Ù‚Ø§Ø¦Ù‚ Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨</b> <br><br>  "Ø§Ù„Ø¬Ùˆ Ø¬Ù…ÙŠÙ„ Ø§Ù„ÙŠÙˆÙ…" - "das haus ist bereit" <br>  "Ø§Ø³Ù…ÙŠ ØªÙˆÙ…" - "mein heiÃŸe heiÃŸe tom" <br>  "ÙƒÙ… Ø¹Ù…Ø±Ùƒ" - "wie alt sind sie" <br>  "Ø£ÙŠÙ† ÙŠÙˆØ¬Ø¯ Ø£Ù‚Ø±Ø¨ Ù…ØªØ¬Ø±" - "wo ist paris" <br><br>  Ø¨Ø¹Ø¶ Ø§Ù„ØªÙ‚Ø¯Ù… Ù…Ø±Ø¦ÙŠ.  Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù‡ÙŠ Ø®Ø§Ø±Ø¬ Ø§Ù„Ù…ÙƒØ§Ù† ØªÙ…Ø§Ù…Ø§.  ÙÙŠ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ© ØŒ "ØªØ¹Ù„Ù…Øª" Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„ÙØ¹Ù„ heiÃŸen (ÙŠØ³Ù…Ù‰) ØŒ ÙˆÙ„ÙƒÙ† "mein heiÃŸe heiÃŸe tom" Ù„Ø§ ØªØ²Ø§Ù„ ØºÙŠØ± ØµØ­ÙŠØ­Ø© ØŒ Ø¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù†Ù‡ ÙŠÙ…ÙƒÙ†Ùƒ Ø¨Ø§Ù„ÙØ¹Ù„ ØªØ®Ù…ÙŠÙ† Ø§Ù„Ù…Ø¹Ù†Ù‰.  Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø© ØµØ­ÙŠØ­Ø© Ø¨Ø§Ù„ÙØ¹Ù„.  ÙÙŠ Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø±Ø§Ø¨Ø¹ ØŒ Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„ Ø§Ù„ØµØ­ÙŠØ­ Ù‡Ùˆ "wo ist" ØŒ ÙˆÙ„ÙƒÙ† ØªÙ… Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø£Ù‚Ø±Ø¨ Ù…ØªØ¬Ø± Ù„Ø³Ø¨Ø¨ Ù…Ø§ Ø¨Ø¨Ø§Ø±ÙŠØ³. <br><br>  <b>30 Ø¯Ù‚ÙŠÙ‚Ø© Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨</b> <br><br>  "Ø§Ù„Ø¬Ùˆ Ø¬Ù…ÙŠÙ„ Ø§Ù„ÙŠÙˆÙ…" - "das ist ist aus" <br>  "Ø§Ø³Ù…ÙŠ ØªÙˆÙ…" - "ØªÙˆÙ…" ist mein name " <br>  "ÙƒÙ… Ø¹Ù…Ø±Ùƒ" - "wie alt sind sie" <br>  "Ø£ÙŠÙ† ÙŠÙˆØ¬Ø¯ Ø£Ù‚Ø±Ø¨ Ù…ØªØ¬Ø±" - "wo ist der" <br><br>  ÙƒÙ…Ø§ ØªØ±Ù‰ ØŒ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ© Ø£ØµØ¨Ø­Øª ØµØ­ÙŠØ­Ø© ØŒ Ø¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù† Ø§Ù„ØªØµÙ…ÙŠÙ… ÙŠØ¨Ø¯Ùˆ ØºÙŠØ± Ø¹Ø§Ø¯ÙŠ Ø¥Ù„Ù‰ Ø­Ø¯ Ù…Ø§.  Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø© ØµØ­ÙŠØ­Ø© ØŒ Ù„ÙƒÙ† Ø§Ù„Ø¬Ù…Ù„ØªÙŠÙ† Ø§Ù„Ø£ÙˆÙ„Ù‰ ÙˆØ§Ù„Ø±Ø§Ø¨Ø¹Ø© Ù„Ù… ÙŠØªÙ… "ØªØ¹Ù„Ù…Ù‡Ù…Ø§" Ø¨Ø¹Ø¯.  Ù…Ø¹ Ù‡Ø°Ø§ Ù…Ù† <s>Ø£Ø¬Ù„ ØªÙˆÙÙŠØ± Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ ØŒ</s> Ø§Ù†ØªÙ‡ÙŠØª Ù…Ù† Ø§Ù„Ø¹Ù…Ù„ÙŠØ©. <br><br><h2 style=";text-align:right;direction:rtl">  Ø§Ø³ØªÙ†ØªØ§Ø¬ </h2><br>  ÙƒÙ…Ø§ ØªØ±ÙˆÙ† ØŒ Ù…Ù† Ø­ÙŠØ« Ø§Ù„Ù…Ø¨Ø¯Ø£ ØŒ Ù‡Ø°Ø§ ÙŠØ¹Ù…Ù„.  Ø£ÙˆØ¯ Ø£Ù† Ø£Ø­ÙØ¸ Ù„ØºØ© Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ø³Ø±Ø¹Ø© :) Ø¨Ø§Ù„Ø·Ø¨Ø¹ ØŒ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù„ÙŠØ³Øª Ù…Ø«Ø§Ù„ÙŠØ© Ø­ØªÙ‰ Ø§Ù„Ø¢Ù† ØŒ ÙˆÙ„ÙƒÙ† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© ÙƒØ§Ù…Ù„Ø© Ù…Ù† 190 Ø£Ù„Ù Ø®Ø· Ø³ÙŠØ³ØªØºØ±Ù‚ Ø£ÙƒØ«Ø± Ù…Ù† Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©. <br><br>  Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ø£ÙˆÙ„Ø¦Ùƒ Ø§Ù„Ø°ÙŠÙ† ÙŠØ±ØºØ¨ÙˆÙ† ÙÙŠ ØªØ¬Ø±Ø¨Ø© Ù…Ù† ØªÙ„Ù‚Ø§Ø¡ Ø£Ù†ÙØ³Ù‡Ù… ØŒ Ø´ÙØ±Ø© Ø§Ù„Ù…ØµØ¯Ø± Ù‡ÙŠ ØªØ­Øª Ø§Ù„Ù…ÙØ³Ø¯.  ÙŠÙ…ÙƒÙ† Ù„Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù…Ù† Ø§Ù„Ù†Ø§Ø­ÙŠØ© Ø§Ù„Ù†Ø¸Ø±ÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙŠ Ø²ÙˆØ¬ Ù…Ù† Ø§Ù„Ù„ØºØ§Øª ØŒ ÙˆÙ„ÙŠØ³ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙˆØ§Ù„Ø£Ù„Ù…Ø§Ù†ÙŠØ© ÙÙ‚Ø· (ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù…Ù„Ù Ø¨ØªØ±Ù…ÙŠØ² UTF-8).  ØªØ¸Ù„ Ù…Ø³Ø£Ù„Ø© Ø¬ÙˆØ¯Ø© Ø§Ù„ØªØ±Ø¬Ù…Ø© Ù…ÙØªÙˆØ­Ø© Ø£ÙŠØ¶Ù‹Ø§ ØŒ ÙÙ‡Ù†Ø§Ùƒ Ø´ÙŠØ¡ ÙŠØ¬Ø¨ Ø§Ø®ØªØ¨Ø§Ø±Ù‡. <br><br><div class="spoiler" style=";text-align:right;direction:rtl">  <b class="spoiler_title">keras_translate.py</b> <div class="spoiler_text" style=";text-align:right;direction:rtl"><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-comment"><span class="hljs-comment"># os.environ["CUDA_VISIBLE_DEVICES"] = "-1" # Force CPU os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # 0 = all messages are logged, 3 - INFO, WARNING, and ERROR messages are not printed import string import re import numpy as np import pandas as pd from keras.models import Sequential from keras.layers import Dense, LSTM, Embedding, RepeatVector from keras.preprocessing.text import Tokenizer from keras.callbacks import ModelCheckpoint from keras.preprocessing.sequence import pad_sequences from keras.models import load_model from keras import optimizers from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt pd.set_option('display.max_colwidth', 200) # Read raw text file def read_text(filename): with open(filename, mode='rt', encoding='utf-8') as file: text = file.read() sents = text.strip().split('\n') return [i.split('\t') for i in sents] data = read_text("deutch.txt") deu_eng = np.array(data) deu_eng = deu_eng[:30000,:] print("Dictionary size:", deu_eng.shape) # Remove punctuation deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]] deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]] # Convert text to lowercase for i in range(len(deu_eng)): deu_eng[i,0] = deu_eng[i,0].lower() deu_eng[i,1] = deu_eng[i,1].lower() # Prepare English tokenizer eng_tokenizer = Tokenizer() eng_tokenizer.fit_on_texts(deu_eng[:, 0]) eng_vocab_size = len(eng_tokenizer.word_index) + 1 eng_length = 8 # Prepare Deutch tokenizer deu_tokenizer = Tokenizer() deu_tokenizer.fit_on_texts(deu_eng[:, 1]) deu_vocab_size = len(deu_tokenizer.word_index) + 1 deu_length = 8 # Encode and pad sequences def encode_sequences(tokenizer, length, lines): # integer encode sequences seq = tokenizer.texts_to_sequences(lines) # pad sequences with 0 values seq = pad_sequences(seq, maxlen=length, padding='post') return seq # Split data into train and test set train, test = train_test_split(deu_eng, test_size=0.2, random_state=12) # Prepare training data trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 0]) trainY = encode_sequences(deu_tokenizer, deu_length, train[:, 1]) # Prepare validation data testX = encode_sequences(eng_tokenizer, eng_length, test[:, 0]) testY = encode_sequences(deu_tokenizer, deu_length, test[:, 1]) # Build NMT model def make_model(in_vocab, out_vocab, in_timesteps, out_timesteps, n): model = Sequential() model.add(Embedding(in_vocab, n, input_length=in_timesteps, mask_zero=True)) model.add(LSTM(n)) model.add(Dropout(0.3)) model.add(RepeatVector(out_timesteps)) model.add(LSTM(n, return_sequences=True)) model.add(Dropout(0.3)) model.add(Dense(out_vocab, activation='softmax')) model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='sparse_categorical_crossentropy') return model print("deu_vocab_size:", deu_vocab_size, deu_length) print("eng_vocab_size:", eng_vocab_size, eng_length) # Model compilation (with 512 hidden units) model = make_model(eng_vocab_size, deu_vocab_size, eng_length, deu_length, 512) # Train model num_epochs = 250 history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), epochs=num_epochs, batch_size=512, validation_split=0.2, callbacks=None, verbose=1) # plt.plot(history.history['loss']) # plt.plot(history.history['val_loss']) # plt.legend(['train','validation']) # plt.show() model.save('en-de-model.h5') # Load model model = load_model('en-de-model.h5') def get_word(n, tokenizer): if n == 0: return "" for word, index in tokenizer.word_index.items(): if index == n: return word return "" phrs_enc = encode_sequences(eng_tokenizer, eng_length, ["the weather is nice today", "my name is tom", "how old are you", "where is the nearest shop"]) print("phrs_enc:", phrs_enc.shape) preds = model.predict_classes(phrs_enc) print("Preds:", preds.shape) print(preds[0]) print(get_word(preds[0][0], deu_tokenizer), get_word(preds[0][1], deu_tokenizer), get_word(preds[0][2], deu_tokenizer), get_word(preds[0][3], deu_tokenizer)) print(preds[1]) print(get_word(preds[1][0], deu_tokenizer), get_word(preds[1][1], deu_tokenizer), get_word(preds[1][2], deu_tokenizer), get_word(preds[1][3], deu_tokenizer)) print(preds[2]) print(get_word(preds[2][0], deu_tokenizer), get_word(preds[2][1], deu_tokenizer), get_word(preds[2][2], deu_tokenizer), get_word(preds[2][3], deu_tokenizer)) print(preds[3]) print(get_word(preds[3][0], deu_tokenizer), get_word(preds[3][1], deu_tokenizer), get_word(preds[3][2], deu_tokenizer), get_word(preds[3][3], deu_tokenizer)) print()</span></span></code> </pre><br></div></div><br>  Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ù†ÙØ³Ù‡ ÙƒØ¨ÙŠØ± Ø¬Ø¯Ù‹Ø§ Ø¨Ø­ÙŠØ« Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ø±ÙØ§Ù‚Ù‡ Ø¨Ø§Ù„Ù…Ù‚Ø§Ù„ ØŒ ÙØ§Ù„Ø±Ø§Ø¨Ø· Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª. <br><br>  ÙƒØ§Ù„Ø¹Ø§Ø¯Ø© ØŒ ÙƒÙ„ Ø§Ù„ØªØ¬Ø§Ø±Ø¨ Ø§Ù„Ù†Ø§Ø¬Ø­Ø©. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/ar470706/">https://habr.com/ru/post/ar470706/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ar470688/index.html">Ù…Ø§ Ù‡Ùˆ Ù…Ø¹Ø±ÙˆÙ Ø¹Ù† VMworld 2019</a></li>
<li><a href="../ar470692/index.html">ÙƒÙŠÙ ØµÙ†Ø¹Ù†Ø§ Ù…ÙˆÙ‚Ø¹ Rosbank Ø¬Ø¯ÙŠØ¯ ØŒ ÙˆÙ…Ø§Ø°Ø§ Ø¬Ø§Ø¡ Ù…Ù†Ù‡</a></li>
<li><a href="../ar470694/index.html">Ø§Ø®ØªÙŠØ§Ø± Ù…Ù†ØµØ© Ø§Ù„ØªØ³ÙˆÙŠÙ‚ Ø¹Ø¨Ø± Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ: Ù…Ø§ ÙŠØ¬Ø¨ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ø´Ø±ÙƒØ§Øª Ø§Ù„Ø±ÙˆØ³ÙŠØ©</a></li>
<li><a href="../ar470696/index.html">Ù„Ù…Ø§Ø°Ø§ ÙŠØ¹ØªØ¨Ø± ÙƒØ§Ù„Ø¯ÙŠ Ø¬ÙŠØ¯Ù‹Ø§ ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…ØŸ (ØªÙ… Ø§Ù„ØªØ­Ø¯ÙŠØ« ÙÙŠ 12.25.2019)</a></li>
<li><a href="../ar470700/index.html">Ø§Ù„Ø¬Ø¯ÙˆÙ„. Ø§Ù„Ù…Ø¹Ø¯Ù†. Ø§Ù„ØµÙ…Øª. Ù„ÙƒØŸ</a></li>
<li><a href="../ar470710/index.html">ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© Ù„ØµÙŠØ¯Ùƒ Ø§Ù„Ù…Ø³Ø·Ø­. Ø§Ù„Ø¬Ø²Ø¡ 2</a></li>
<li><a href="../ar470718/index.html">"Ø§Ù„Ø¢Ø«Ø§Ø± Ø§Ù„Ø¬Ø¨Ø±ÙŠØ©" ÙÙŠ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¨Ø´Ø±ÙŠØ©</a></li>
<li><a href="../ar470720/index.html">ÙƒÙŠÙÙŠØ© ÙƒØªØ§Ø¨Ø© Ø¹Ù‚Ø¯ Ø°ÙƒÙŠ Ù…Ø¹ Ø¨ÙŠØ«ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ø·ÙˆÙ„ÙˆØ¬ÙŠØ§ØŸ Ø§Ù„Ø¬Ø²Ø¡ 2: Ø§Ù„ØªØ®Ø²ÙŠÙ† API</a></li>
<li><a href="../ar470722/index.html">ÙƒÙŠÙÙŠØ© ÙƒØªØ§Ø¨Ø© Ø¹Ù‚Ø¯ Ø°ÙƒÙŠ Ù…Ø¹ Ø¨ÙŠØ«ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ø·ÙˆÙ„ÙˆØ¬ÙŠØ§ØŸ Ø§Ù„Ø¬Ø²Ø¡ 3: API ÙˆÙ‚Øª Ø§Ù„ØªØ´ØºÙŠÙ„</a></li>
<li><a href="../ar470726/index.html">ÙƒÙŠÙ Ù„Ø§ Ù†ØºØ±Ù‚ ÙÙŠ Ø§Ù„Ø±ÙˆØªÙŠÙ† ØŒ Ø£Ùˆ ØªØ¬Ø±Ø¨ØªÙ†Ø§ Ù…Ù‚Ø§Ø±Ù†Ø© AWR Ù…Ù‚Ø§Ù„Ø¨ Ø£Ø«Ù†Ø§Ø¡ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¥Ø¬Ù‡Ø§Ø¯</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>