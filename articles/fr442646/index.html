<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üì∫ üêâ üöø R√©seaux Kubernetes: Ingress ü§∏üèº üö∫ üçπ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Aujourd'hui, nous publions une traduction de la troisi√®me partie du Guide de mise en r√©seau Kubernetes. La premi√®re partie portait sur les pods, la se...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>R√©seaux Kubernetes: Ingress</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/442646/">  Aujourd'hui, nous publions une traduction de la troisi√®me partie du Guide de mise en r√©seau Kubernetes.  La <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">premi√®re</a> partie portait sur les pods, la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">seconde</a> sur les services, et aujourd'hui nous parlerons de l'√©quilibrage de charge et des ressources Kubernetes de type Ingress. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/te/wp/ce/tewpcee5cggzqu97irog_mj_qgo.png"></div><a name="habracut"></a><h2>  <font color="#3AC1EF">Le routage n'est pas l'√©quilibrage de charge</font> </h2><br>  Dans l'article <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pr√©c√©dent</a> de cette s√©rie, nous avons consid√©r√© une configuration compos√©e d'une paire de foyers et d'un service auquel une adresse IP a √©t√© affect√©e appel√©e ¬´IP de cluster¬ª.  Des requ√™tes destin√©es aux foyers ont √©t√© envoy√©es √† cette adresse.  Ici, nous continuerons √† travailler sur notre syst√®me de formation, en commen√ßant l√† o√π nous avons obtenu notre dipl√¥me la derni√®re fois.  Rappelons que l'adresse IP de cluster du service, <code>10.3.241.152</code> , appartient √† une plage d'adresses IP diff√©rente de celle utilis√©e dans le r√©seau de foyer et de celle utilis√©e dans le r√©seau dans lequel les n≈ìuds sont situ√©s.  J'ai appel√© le r√©seau d√©fini par cet espace d'adressage ¬´r√©seau de service¬ª, bien qu'il ne m√©rite gu√®re un nom sp√©cial, car aucun p√©riph√©rique n'est connect√© √† ce r√©seau, et son espace d'adressage, en fait, est enti√®rement compos√© de r√®gles de routage.  Il a √©t√© pr√©c√©demment d√©montr√© comment ce r√©seau est impl√©ment√© sur la base du composant Kubernetes appel√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">kube-proxy</a> et interagit avec le module <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">netfilter du</a> noyau Linux pour intercepter et rediriger le trafic envoy√© vers le cluster IP pour travailler sous. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/615/35f/1ec/61535f1ec0169dbd13732aba4c9a5621.png"></div><br>  <i><font color="#999999">Sch√©ma de r√©seau</font></i> <br><br>  Jusqu'√† pr√©sent, nous avons parl√© de ¬´connexions¬ª et de ¬´demandes¬ª et nous avons m√™me utilis√© le concept difficile √† interpr√©ter de ¬´trafic¬ª, mais pour comprendre les caract√©ristiques du m√©canisme Kubernetes Ingress, nous devons utiliser des termes plus pr√©cis.  Ainsi, les connexions et les requ√™tes fonctionnent au 4√®me niveau du <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">mod√®le OSI</a> (tcp) ou au 7√®me niveau (http, rpc, etc.).  Les r√®gles Netfilter sont des r√®gles de routage, elles fonctionnent avec des paquets IP au troisi√®me niveau.  Tous les routeurs, y compris netfilter, prennent des d√©cisions plus ou moins bas√©es uniquement sur les informations contenues dans le paquet.  En g√©n√©ral, ils s'int√©ressent √† la provenance et √† la destination du paquet.  Par cons√©quent, afin de d√©crire ce comportement en termes de troisi√®me niveau du mod√®le OSI, il faut dire que chaque paquet destin√© au service situ√© √† <code>10.3.241.152:80</code> , qui arrive √† l'interface du n≈ìud <code>eth0</code> , est trait√© par netfilter, et, conform√©ment √† les r√®gles d√©finies pour notre service sont redirig√©es vers l'adresse IP d'un foyer fonctionnel. <br><br>  Il semble assez √©vident que tout m√©canisme que nous utilisons pour permettre aux clients externes d'acc√©der aux pods doit utiliser la m√™me infrastructure de routage.  En cons√©quence, ces clients externes acc√©deront √† l'adresse IP et au port du cluster, car ils sont le ¬´point d'acc√®s¬ª √† tous les m√©canismes dont nous avons parl√© jusqu'√† pr√©sent.  Ils nous permettent de ne pas nous soucier de l'endroit exact o√π il est ex√©cut√© √† un certain moment.  Cependant, il n'est pas du tout √©vident de savoir comment le faire fonctionner. <br><br>  Le service IP de cluster est accessible uniquement avec l'interface Ethernet du n≈ìud.  Rien en dehors du cluster ne sait quoi faire des adresses de la plage √† laquelle appartient cette adresse.  Comment puis-je rediriger le trafic d'une adresse IP publique vers une adresse accessible uniquement si le paquet est d√©j√† arriv√© sur l'h√¥te? <br><br>  Si nous essayons de trouver une solution √† ce probl√®me, alors l'une des choses qui peuvent √™tre faites dans le processus de recherche d'une solution sera l'√©tude des r√®gles de netfilter en utilisant l'utilitaire <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">iptables</a> .  Si vous faites cela, vous pouvez d√©couvrir quelque chose qui, √† premi√®re vue, peut sembler inhabituel: les r√®gles du service ne sont pas limit√©es √† un r√©seau source sp√©cifique.  Cela signifie que tous les paquets g√©n√©r√©s n'importe o√π qui arrivent sur l'interface Ethernet du n≈ìud et ont une adresse de destination de <code>10.3.241.152:80</code> seront reconnus comme conformes √† la r√®gle et seront redirig√©s vers le sous-marin.  Pouvons-nous simplement donner aux clients un cluster IP, peut-√™tre en le liant √† un nom de domaine appropri√©, puis d√©finir un itin√©raire qui nous permet d'organiser la livraison de ces paquets √† l'un des n≈ìuds? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5f3/6ba/ea7/5f36baea7589e3559632de385f3f2bf6.png"></div><br>  <i><font color="#999999">Client et cluster externes</font></i> <br><br>  Si tout est configur√© de cette fa√ßon, une telle conception s'av√©rera efficace.  Les clients acc√®dent √† l'IP du cluster, les paquets suivent la route menant √† l'h√¥te, puis ils sont redirig√©s vers le bas.  En ce moment, il peut vous sembler qu'une telle solution peut √™tre limit√©e, mais elle souffre de graves probl√®mes.  Le premier est que les n≈ìuds, en fait, le concept d'√©ph√©m√®re, ils ne sont pas particuli√®rement diff√©rents √† cet √©gard des foyers.  Ils sont, bien s√ªr, un peu plus proches du monde mat√©riel que les pods, mais ils peuvent migrer vers de nouvelles machines virtuelles, les clusters peuvent √©voluer vers le haut ou vers le bas, etc.  Les routeurs fonctionnent au troisi√®me niveau du mod√®le OSI et les paquets ne peuvent pas faire la distinction entre les services qui fonctionnent normalement et ceux qui ne fonctionnent pas correctement.  Ils s'attendent √† ce que la prochaine transition sur l'itin√©raire soit accessible et stable.  Si le n≈ìud est inaccessible, l'itin√©raire deviendra inop√©rant et le restera, dans la plupart des cas, beaucoup de temps.  M√™me si la route r√©siste aux pannes, un tel sch√©ma entra√Ænera le fait que tout le trafic externe passera par un seul n≈ìud, ce qui n'est probablement pas optimal. <br><br>  Quelle que soit la fa√ßon dont nous apportons le trafic client au syst√®me, nous devons le faire pour qu'il ne d√©pende pas de l'√©tat d'un n≈ìud de cluster unique.  Et, en fait, il n'y a aucun moyen fiable de le faire en utilisant uniquement le routage, sans certains moyens de g√©rer activement le routeur.  En fait, c'est pr√©cis√©ment ce r√¥le, le r√¥le du syst√®me de contr√¥le, que kube-proxy joue par rapport √† netfilter.  √âtendre la responsabilit√© de Kubernetes √† la gestion d'un routeur externe n'avait probablement pas beaucoup de sens pour les architectes syst√®me, d'autant plus que nous avons d√©j√† des outils √©prouv√©s pour distribuer le trafic client sur plusieurs serveurs.  Ils sont appel√©s √©quilibreurs de charge, et il n'est pas surprenant qu'ils soient la solution vraiment fiable pour Kubernetes Ingress.  Afin de comprendre exactement comment cela se produit, nous devons sortir du troisi√®me niveau de l'OSI et parler √† nouveau des connexions. <br><br>  Afin d'utiliser l'√©quilibreur de charge pour r√©partir le trafic client entre les n≈ìuds de cluster, nous avons besoin d'une adresse IP publique √† laquelle les clients peuvent se connecter, et nous avons √©galement besoin des adresses des n≈ìuds eux-m√™mes vers lesquels l'√©quilibreur de charge peut rediriger les demandes.  Pour les raisons ci-dessus, nous ne pouvons pas simplement cr√©er une route statique stable entre le routeur de passerelle et les n≈ìuds √† l'aide d'un r√©seau bas√© sur les services (cluster IP). <br><br>  Parmi les autres adresses avec lesquelles vous pouvez travailler, seules les adresses du r√©seau auquel sont connect√©es les interfaces Ethernet des n≈ìuds, c'est-√†-dire, dans cet exemple, <code>10.100.0.0/24</code> , peuvent √™tre not√©es.  Le routeur sait d√©j√† comment transf√©rer les paquets vers ces interfaces, et les connexions envoy√©es de l'√©quilibreur de charge au routeur iront l√† o√π elles devraient aller.  Mais si le client souhaite se connecter √† notre service sur le port 80, nous ne pouvons pas simplement envoyer des paquets √† ce port sur les interfaces r√©seau des n≈ìuds. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4cd/c77/40b/4cdc7740be5ecfc3069a8b3fb157c605.png"></div><br>  <i><font color="#999999">√âquilibreur de charge, tentative infructueuse d'acc√©der au port 80 de l'interface r√©seau h√¥te</font></i> <br><br>  La raison pour laquelle cela ne peut pas √™tre fait est tout √† fait √©vidente.  √Ä savoir, nous parlons du fait qu'il n'y a pas de processus en attente de connexions √† <code>10.100.0.3:80</code> (et s'il y en a, ce n'est certainement pas le m√™me processus), et les r√®gles de netfilter, qui, comme nous l'esp√©rions, intercepteraient la demande et ils le lui enverront, ils ne travailleront pas √† cette adresse de destination.  Ils ne r√©pondent qu'√† un r√©seau IP de cluster bas√© sur des services, c'est-√†-dire √† l'adresse <code>10.3.241.152:80</code> .  En cons√©quence, ces paquets, √† leur arriv√©e, ne peuvent pas √™tre livr√©s √† l'adresse de destination, et le noyau √©mettra une r√©ponse <code>ECONNREFUSED</code> .  Cela nous place dans une position d√©routante: il n'est pas facile de travailler avec le r√©seau pour la redirection de paquets vers laquelle netfilter est configur√© lors de la redirection des donn√©es de la passerelle vers les n≈ìuds, et un r√©seau pour lequel le routage est facile √† configurer n'est pas le r√©seau vers lequel netfilter redirige les paquets.  Afin de r√©soudre ce probl√®me, vous pouvez cr√©er un pont entre ces r√©seaux.  C'est exactement ce que fait Kubernetes en utilisant un service comme NodePort. <br><br><h2>  <font color="#3AC1EF">Des services comme NodePort</font> </h2><br>  Le service que nous avons, par exemple, cr√©√© dans l'article pr√©c√©dent, n'a pas de type, il a donc adopt√© le type par d√©faut - <code>ClusterIP</code> .  Il existe deux autres types de services qui diff√®rent par des fonctionnalit√©s suppl√©mentaires, et celui qui nous int√©resse actuellement est <code>NodePort</code> .  Voici un exemple de description d'un service de ce type: <br><br><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: service-test spec: type: NodePort selector:   app: service_test_pod ports: - port: 80   targetPort: http</code> </pre> <br>  Les services de type <code>NodePort</code> sont des services de type <code>ClusterIP</code> qui ont une fonctionnalit√© suppl√©mentaire: leur acc√®s peut √™tre obtenu √† la fois par l'adresse IP attribu√©e √† l'h√¥te et par l'adresse affect√©e au cluster dans le r√©seau de services.  Ceci est r√©alis√© de mani√®re assez simple: lorsque Kubernetes cr√©e un service NodePort, kube-proxy alloue un port dans la plage 30000-32767 et ouvre ce port sur l'interface <code>eth0</code> de chaque n≈ìud (d'o√π le nom du type de service - <code>NodePort</code> ).  Les connexions √©tablies √† ce port (nous appellerons ces ports <code>NodePort</code> ) sont redirig√©es vers l'IP du cluster du service.  Si nous cr√©ons le service d√©crit ci-dessus et <code>kubectl get svc service-test</code> la commande <code>kubectl get svc service-test</code> , nous pouvons voir le port qui lui est attribu√©. <br><br><pre> <code class="plaintext hljs">$ kubectl get svc service-test NAME           CLUSTER-IP EXTERNAL-IP   PORT(S) AGE service-test   10.3.241.152 &lt;none&gt;        80:32213/TCP 1m</code> </pre> <br>  Dans ce cas, le service re√ßoit le NodePort <code>32213</code> .  Cela signifie que nous pouvons maintenant nous connecter au service via n'importe quel n≈ìud de notre cluster exp√©rimental √† <code>10.100.0.2:32213</code> ou √† <code>10.100.0.3:32213</code> .  Dans ce cas, le trafic sera redirig√© vers le service. <br><br>  Une fois que cette partie du syst√®me a pris sa place, nous avons tous les fragments du pipeline pour √©quilibrer la charge cr√©√©e par les demandes des clients vers tous les n≈ìuds du cluster. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/da5/78e/735/da578e735e468dce8077f8a1d27d8490.png"></div><br>  <i><font color="#999999">Service NodePort</font></i> <br><br>  Dans la figure pr√©c√©dente, le client se connecte √† l'√©quilibreur de charge via une adresse IP publique, l'√©quilibreur de charge s√©lectionne le n≈ìud et s'y connecte √† <code>10.100.0.3:32213</code> , kube-proxy accepte cette connexion et la redirige vers le service accessible via le cluster IP <code>10.3.241.152:80</code> .  Ici, la demande est trait√©e avec succ√®s selon les r√®gles d√©finies par netfilter et est redirig√©e vers le module serveur √† l'adresse <code>10.0.2.2:8080</code> .  Peut-√™tre que tout cela peut sembler un peu compliqu√©, et dans une certaine mesure, mais il n'est pas facile de trouver une solution plus simple qui prend en charge toutes les merveilleuses fonctionnalit√©s qui nous donnent des pods et des r√©seaux bas√©s sur des services. <br><br>  Ce m√©canisme n'est cependant pas sans poser ses propres probl√®mes.  L'utilisation de services tels que <code>NodePort</code> permet aux clients d'acc√©der aux services √† l'aide d'un port non standard.  Souvent, ce n'est pas un probl√®me, car l'√©quilibreur de charge peut leur fournir un port normal et masquer <code>NodePort</code> aux utilisateurs finaux.  Mais dans certains sc√©narios, par exemple, lors de l'utilisation d'un √©quilibreur de charge de plateforme Google Cloud externe, il peut √™tre n√©cessaire de d√©ployer <code>NodePort</code> clients.  Il convient de noter que ces ports, en outre, repr√©sentent des ressources limit√©es, bien que 2768 ports soient probablement suffisants m√™me pour les plus grands clusters.  Dans la plupart des cas, vous pouvez laisser Kubernetes s√©lectionner des num√©ros de port au hasard, mais vous pouvez les d√©finir vous-m√™me si n√©cessaire.  Un autre probl√®me concerne certaines limitations concernant le stockage des adresses IP source dans les requ√™tes.  Pour savoir comment r√©soudre ces probl√®mes, vous pouvez vous r√©f√©rer √† <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ce</a> mat√©riel dans la documentation de Kubernetes. <br><br>  Ports <code>NodePorts</code> est le m√©canisme fondamental par lequel tout le trafic externe entre dans le cluster Kubernetes.  Cependant, eux-m√™mes ne nous pr√©sentent pas de solution toute faite.  Pour les raisons ci-dessus, avant le cluster, que les clients soient des entit√©s internes ou externes situ√©es dans un r√©seau public, il est toujours n√©cessaire d'avoir une sorte d'√©quilibreur de charge. <br><br>  Les architectes de la plate-forme, r√©alisant cela, ont fourni deux fa√ßons de configurer l'√©quilibreur de charge √† partir de la plate-forme Kubernetes elle-m√™me.  Discutons-en. <br><br><h2>  <font color="#3AC1EF">Des services tels que LoadBalancer et des ressources de type Ingress</font> </h2><br>  Des services comme <code>LoadBalancer</code> et des ressources de type <code>Ingress</code> sont parmi les m√©canismes Kubernetes les plus complexes.  Cependant, nous n'y consacrerons pas trop de temps, car leur utilisation n'entra√Æne pas de changements fondamentaux dans tout ce dont nous avons parl√© jusqu'√† pr√©sent.  Tout le trafic externe, comme pr√©c√©demment, entre dans le cluster via <code>NodePort</code> . <br><br>  Les architectes pourraient s'arr√™ter l√†, permettant √† ceux qui cr√©ent des clusters de se soucier uniquement des adresses IP publiques et des √©quilibreurs de charge.  En fait, dans certaines situations, comme le d√©marrage d'un cluster sur des serveurs r√©guliers ou √† domicile, c'est exactement ce qu'ils font.  Mais dans les environnements qui prennent en charge les configurations de ressources r√©seau contr√¥l√©es par l'API, Kubernetes vous permet de configurer tout ce dont vous avez besoin en un seul endroit. <br><br>  La premi√®re approche pour r√©soudre ce probl√®me, la plus simple, consiste √† utiliser les services Kubernetes tels que <code>LoadBalancer</code> .  Ces services ont toutes les capacit√©s de services tels que <code>NodePort</code> et, en outre, ont la possibilit√© de cr√©er des chemins d'acc√®s complets pour le trafic entrant, en supposant que le cluster s'ex√©cute dans des environnements tels que GCP ou AWS, qui prennent en charge la configuration des ressources r√©seau via l'API. <br><br><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: service-test spec: type: LoadBalancer selector:   app: service_test_pod ports: - port: 80   targetPort: http</code> </pre> <br>  Si nous supprimons et recr√©ons le service de notre exemple dans le moteur Google Kubernetes, peu de temps apr√®s, √† l'aide de la commande <code>kubectl get svc service-test</code> , nous pouvons v√©rifier que l'IP externe est attribu√©e. <br><br><pre> <code class="plaintext hljs">$ kubectl get svc service-test NAME      CLUSTER-IP      EXTERNAL-IP PORT(S)          AGE openvpn   10.3.241.52     35.184.97.156 80:32213/TCP     5m</code> </pre> <br>  Il est dit ci-dessus que nous pourrons v√©rifier le fait d'attribuer une adresse IP externe "bient√¥t", malgr√© le fait que l'attribution d'une adresse IP externe puisse prendre plusieurs minutes, ce qui n'est pas surprenant √©tant donn√© la quantit√© de ressources qui doivent √™tre port√©es √† un √©tat sain.  Sur la plate-forme GCP, par exemple, cela n√©cessite que le syst√®me cr√©e une adresse IP externe, des r√®gles de redirection du trafic, un serveur proxy cible, un service principal et, √©ventuellement, une instance de groupe.  Apr√®s avoir attribu√© une adresse IP externe, vous pouvez vous connecter au service via cette adresse, lui attribuer un nom de domaine et informer les clients.  Jusqu'√† ce que le service soit d√©truit et recr√©√© (pour ce faire, rarement lorsqu'il y a une bonne raison), l'adresse IP ne changera pas. <br><br>  Des services comme <code>LoadBalancer</code> ont certaines limitations.  Un tel service ne peut pas √™tre configur√© pour d√©chiffrer le trafic HTTPS.  Vous ne pouvez pas cr√©er d'h√¥tes virtuels ou configurer le routage en fonction de chemins, vous ne pouvez donc pas, √† l'aide de configurations pratiques, utiliser un seul √©quilibreur de charge avec de nombreux services.  Ces limitations ont conduit √† l'introduction de Kubernetes 1.1.  Une ressource sp√©ciale pour configurer les √©quilibreurs de charge.  Il s'agit d'une ressource de type <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Ingress</a> .  Des services tels que <code>LoadBalancer</code> visent √† √©tendre les capacit√©s d'un service unique pour prendre en charge des clients externes.  En revanche, les ressources <code>Ingress</code> sont des ressources sp√©ciales qui vous permettent de configurer de mani√®re flexible les √©quilibreurs de charge.  L'API Ingress prend en charge le d√©chiffrement du trafic TLS, des h√¥tes virtuels et du routage bas√© sur le chemin.  √Ä l'aide de cette API, l'√©quilibreur de charge peut facilement √™tre configur√© pour fonctionner avec plusieurs services principaux. <br><br>  L'API de ressource de type <code>Ingress</code> est trop volumineuse pour discuter de ses fonctionnalit√©s ici; en outre, elle n'affecte pas particuli√®rement le fonctionnement des ressources Ingress au niveau du r√©seau.  L'impl√©mentation de cette ressource suit le mod√®le Kubernetes habituel: il existe un type de ressource et un contr√¥leur pour contr√¥ler ce type.  Dans ce cas, la ressource est la ressource <code>Ingress</code> , qui d√©crit les demandes de ressources r√©seau.  Voici √† quoi pourrait ressembler la description d'une ressource <code>Ingress</code> . <br><br><pre> <code class="plaintext hljs">apiVersion: extensions/v1beta1 kind: Ingress metadata: name: test-ingress annotations:   kubernetes.io/ingress.class: "gce" spec: tls:   - secretName: my-ssl-secret rules: - host: testhost.com   http:     paths:     - path: /*       backend:         serviceName: service-test         servicePort: 80</code> </pre> <br>  Le contr√¥leur Ingress est responsable de l'ex√©cution de ces demandes en amenant les autres ressources √† l'√©tat souhait√©.  Lorsque vous utilisez Ingress, des services tels que <code>NodePort</code> sont cr√©√©s, apr√®s quoi le contr√¥leur Ingress est autoris√© √† prendre des d√©cisions sur la fa√ßon de diriger le trafic vers les n≈ìuds.  Il existe une impl√©mentation du contr√¥leur Ingress pour les √©quilibreurs de charge GCE, pour les √©quilibreurs AWS, pour les serveurs proxy populaires tels que nginx et haproxy.  Notez que le m√©lange de ressources et de services Ingress comme <code>LoadBalancer</code> peut provoquer des probl√®mes mineurs dans certains environnements.  Ils sont faciles √† manipuler, mais, en g√©n√©ral, il est pr√©f√©rable d‚Äôutiliser Ingress m√™me pour des services simples. <br><br><h2>  <font color="#3AC1EF">HostPort et HostNetwork</font> </h2><br>  Ce dont nous allons parler maintenant, √† savoir <code>HostPort</code> et <code>HostNetwork</code> , peut √™tre attribu√© plut√¥t √† la cat√©gorie des raret√©s int√©ressantes et non √† des outils utiles.  En fait, je m'engage √† affirmer que dans 99,99% des cas leur utilisation peut √™tre consid√©r√©e comme anti-pattern, et tout syst√®me dans lequel ils sont utilis√©s doit subir une v√©rification obligatoire de son architecture. <br><br>  Je pensais que cela ne valait pas la peine d‚Äôen parler, mais c‚Äôest quelque chose comme les outils utilis√©s par les ressources Ingress pour traiter le trafic entrant, alors j‚Äôai d√©cid√© qu‚Äôil valait la peine de les mentionner, du moins bri√®vement. <br><br>  Parlons d' <code>HostPort</code> de <code>HostPort</code> .  Il s'agit d'une propri√©t√© de conteneur (d√©clar√©e dans la structure <code>ContainerPort</code> ).  Lorsqu'un certain num√©ro de port y est √©crit, cela conduit √† l'ouverture de ce port sur le n≈ìud et √† sa redirection directement vers le conteneur.  Il n'y a aucun m√©canisme de proxy et le port s'ouvre uniquement sur les n≈ìuds sur lesquels le conteneur s'ex√©cute.  Au d√©but de la plate-forme, avant que les m√©canismes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">DaemonSet</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">StatefulSet</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">n'y</a> apparaissent, <code>HostPort</code> √©tait une astuce qui permettait de lancer un seul conteneur d'un certain type sur n'importe quel n≈ìud.  Par exemple, j'ai d√©j√† utilis√© cela pour cr√©er un cluster Elasticsearch en d√©finissant <code>HostPort</code> sur <code>9200</code> et en sp√©cifiant autant de r√©pliques qu'il y avait de n≈ìuds.       ,          Kubernetes,    -     <code>HostPort</code> . <br><br>   <code>NostNetwork</code> , ,   Kubernetes    ,  <code>HostPort</code> .       <code>true</code> ,       - <code>network=host</code>  <code>docker run</code> .    ,           .            <code>eth0</code>    .  ,             .      ,  ,  ,    Kubernetes,     - . <br><br><h2>  <font color="#3AC1EF">R√©sum√©</font> </h2><br>        Kubernetes,   ,          Ingress. ,  ,    ,       Kubernetes. <br><br>  <b>Chers lecteurs!</b>     Ingress? <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr442646/">https://habr.com/ru/post/fr442646/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr442636/index.html">Apportez-vous de mauvaises nouvelles √† la direction?</a></li>
<li><a href="../fr442638/index.html">Mise √† l'√©chelle des applications Kubernetes bas√©e sur les m√©triques de Prometheus</a></li>
<li><a href="../fr442640/index.html">Bug parfait: utilisation de la confusion de types dans Flash. Partie 1</a></li>
<li><a href="../fr442642/index.html">Que lire en mars: 22 nouveaux livres pour les sp√©cialistes du marketing, les gestionnaires, les d√©veloppeurs et les concepteurs</a></li>
<li><a href="../fr442644/index.html">La plupart des comp√©tences hors programmation augmentent la valeur pour les d√©veloppeurs</a></li>
<li><a href="../fr442648/index.html">M√©canismes d'allocation de Go</a></li>
<li><a href="../fr442650/index.html">Analyse et optimisation des applications React</a></li>
<li><a href="../fr442652/index.html">Utilisation de Fastify et de Preact pour prototyper rapidement des applications Web</a></li>
<li><a href="../fr442654/index.html">Passer √† Next.js et acc√©l√©rer 7,5 fois le chargement de la page d'accueil de manifold.co</a></li>
<li><a href="../fr442658/index.html">8 astuces pour travailler avec CSS: parallaxe, pied de page collant et autres</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>