<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèø üôéüèΩ üôçüèø Schreiben eines einfachen neuronalen Netzwerks mit Mathematik und Numpy üïã üòë üö†</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Warum der n√§chste Artikel dar√ºber, wie man neuronale Netze von Grund auf neu schreibt? Leider konnte ich keine Artikel finden, in denen Theorie und Co...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Schreiben eines einfachen neuronalen Netzwerks mit Mathematik und Numpy</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/460589/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/5_/h5/0t/5_h50teeaqngyx0ccp8cqjqfumm.jpeg" width="350"></div><br>  Warum der n√§chste Artikel dar√ºber, wie man neuronale Netze von Grund auf neu schreibt?  Leider konnte ich keine Artikel finden, in denen Theorie und Code von Grund auf zu einem voll funktionsf√§higen Modell beschrieben wurden.  Ich warne sofort, dass es viel Mathematik geben wird.  Ich gehe davon aus, dass der Leser mit den Grundlagen der linearen Algebra, partiellen Ableitungen und zumindest teilweise mit der Wahrscheinlichkeitstheorie sowie Python und Numpy vertraut ist.  Wir werden uns mit einem vollst√§ndig verbundenen neuronalen Netzwerk und MNIST befassen. <br><a name="habracut"></a><br><h2>  Mathe  Teil 1 (einfach) </h2><br>  Was ist eine vollst√§ndig verbundene Schicht (FC-Schicht)?  Normalerweise sagen sie etwas wie "Eine vollst√§ndig verbundene Schicht ist eine Schicht, von der jedes Neuron mit allen Neuronen der vorherigen Schicht verbunden ist".  Es ist einfach nicht klar, was Neuronen sind, wie sie verbunden sind, insbesondere im Code.  Jetzt werde ich versuchen, dies anhand eines Beispiels zu analysieren.  Es gebe eine Schicht von 100 Neuronen.  Ich wei√ü, dass ich noch nicht erkl√§rt habe, was es ist, aber stellen wir uns vor, dass es 100 Neuronen gibt, die eine Eingabe haben, von der die Daten gesendet werden, und eine Ausgabe, von der sie die Daten geben.  Der Eingabe wird ein Schwarzwei√übild mit 28 x 28 Pixel zugef√ºhrt - nur 784 Werte, wenn Sie es in einen Vektor strecken.  Ein Bild kann als Eingabeebene bezeichnet werden.  Damit sich jedes der 100 Neuronen mit jedem "Neuron" oder, wenn Sie m√∂chten, dem Wert der vorherigen Schicht (dh dem Bild) verbindet, muss jedes der 100 Neuronen 784 Werte des Originalbilds akzeptieren.  Zum Beispiel reicht es f√ºr jedes der 100 Neuronen aus, 784 Werte des Bildes mit 784 Zahlen zu multiplizieren und diese zu addieren, wodurch eine Zahl herauskommt.  Das hei√üt, dies ist ein Neuron: <br><p><math> </math> $$ display $$ \ text {Neuronenausgabe} = \ text {eine Zahl} _ {1} \ cdot \ text {Bildwert} _1 ~ + \\ + ~ ... ~ + ~ \ text {some- diese Zahl} _ {784} \ cdot \ text {Bildwert} _ {784} $$ display $$ </p><br>  Dann stellt sich heraus, dass jedes Neuron 784 Zahlen hat und alle diese Zahlen: (Anzahl der Neuronen auf dieser Schicht) x (Anzahl der Neuronen auf der vorherigen Schicht) = <math> </math> $ inline $ 100 \ times784 $ inline $   = 78.400 Stellen.  Diese Zahlen werden √ºblicherweise als Schichtgewichte bezeichnet.  Jedes Neuron gibt seine Nummer aus und als Ergebnis erhalten wir einen 100-dimensionalen Vektor. Tats√§chlich k√∂nnen wir schreiben, dass dieser 100-dimensionale Vektor erhalten wird, indem der 784-dimensionale Vektor (unser Originalbild) mit einer Gewichtsmatrix der Gr√∂√üe multipliziert wird <math> </math> $ inline $ 100 \ times784 $ inline $   :: <br><p><math> </math> $$ display $$ \ boldsymbol {x} ^ {100} = W_ {100 \ times784} \ cdot \ boldsymbol {x} ^ {784} $$ display $$ </p><br><br>  Ferner werden die resultierenden 100 Zahlen an die Aktivierungsfunktion weitergegeben - einige nichtlineare Funktionen - die jede Zahl separat beeinflusst.  Zum Beispiel Sigmoid, hyperbolische Tangente, ReLU und andere.  Die Aktivierungsfunktion ist notwendigerweise nichtlinear, andernfalls lernt das neuronale Netzwerk nur einfache Transformationen. <br><br><img src="https://habrastorage.org/webt/0j/rl/ba/0jrlbaqv0486mryhqj32u8et0cw.png"><br><br>  Dann werden die resultierenden Daten erneut einer vollst√§ndig verbundenen Schicht, jedoch mit einer anderen Anzahl von Neuronen, und erneut der Aktivierungsfunktion zugef√ºhrt.  Dies passiert mehrmals.  Die letzte Schicht des Netzwerks ist die Schicht, die die Antwort erzeugt.  In diesem Fall ist die Antwort eine Information √ºber die Nummer auf dem Bild. <br><br><img src="https://habrastorage.org/webt/9f/73/q-/9f73q-feve3kb5k9u5fxvmz4kxk.png"><br><br>  W√§hrend des Trainings des Netzwerks ist es notwendig, dass wir wissen, welche Abbildung auf dem Bild gezeigt wird.  Das hei√üt, dass der Datensatz markiert ist.  Dann k√∂nnen Sie ein anderes Element verwenden - die Fehlerfunktion.  Sie betrachtet die Antwort des neuronalen Netzwerks und vergleicht sie mit der tats√§chlichen Antwort.  Dank dessen lernt das neuronale Netzwerk. <br><br><h2>  Allgemeine Erkl√§rung des Problems </h2><br>  Der gesamte Datensatz ist ein gro√üer Tensor (wir werden ein mehrdimensionales Datenarray als Tensor bezeichnen). <math> </math> $ inline $ \ boldsymbol {X} = \ left [\ boldsymbol {x} _1, \ boldsymbol {x} _2, \ ldots, \ boldsymbol {x} _n \ right] $ inline $   wo <math> </math> $ inline $ \ boldsymbol {x} _i $ inline $   - i-te Objekt, zum Beispiel ein Bild, das auch ein Tensor ist.  F√ºr jedes Objekt gibt es <math> </math> $ inline $ y_i $ inline $   - die richtige Antwort auf das i-te Objekt.  In diesem Fall kann ein neuronales Netzwerk als eine Funktion dargestellt werden, die ein Objekt als Eingabe verwendet und eine Antwort darauf gibt: <br><p><math> </math> $$ display $$ F (\ boldsymbol {x} _i) = \ hat {y} _i $$ display $$ </p><br>  Schauen wir uns nun die Funktion genauer an <math> </math> $ inline $ F (\ boldsymbol {x} _i) $ inline $   .  Da das neuronale Netzwerk aus Schichten besteht, ist jede einzelne Schicht eine Funktion.  Und das hei√üt <br><p><math> </math> $$ display $$ F (\ boldsymbol {x} _i) = f_k (f_ {k-1} (\ ldots (f_1 (\ boldsymbol {x} _i))) = \ hat {y} _i $$ display $ $ </p><br>  Das hei√üt, in der allerersten Funktion - der ersten Schicht - wird ein Bild in Form eines Tensors dargestellt.  Funktion <math> </math> $ inline $ f_1 $ inline $   gibt eine Antwort - auch ein Tensor, aber von einer anderen Dimension.  Dieser Tensor wird als interne Darstellung bezeichnet.  Diese interne Darstellung wird nun dem Eingang der Funktion zugef√ºhrt <math> </math> $ inline $ f_2 $ inline $   , die seine interne Darstellung gibt.  Und so weiter bis zur Funktion <math> </math> $ inline $ f_k $ inline $   - letzte Schicht - gibt keine Antwort <math> </math> $ inline $ \ hat {y} _i $ inline $   . <br><br>  Jetzt besteht die Aufgabe darin, das Netzwerk zu trainieren, damit die Netzwerkantwort mit der richtigen Antwort √ºbereinstimmt.  Zuerst m√ºssen Sie messen, wie falsch das neuronale Netzwerk ist.  Dies zu messen ist eine Fehlerfunktion. <math> </math> $ inline $ L (\ hat {y} _i, y_i) $ inline $   .  Und wir legen Beschr√§nkungen fest: <br><br>  1. <math> </math> $ inline $ \ hat {y} _i \ xrightarrow {} y_i \ Rightarrow L (\ hat {y} _i, y_i) \ xrightarrow {} 0 $ inline $ <br>  2. <math> </math> $ inline $ \ existiert ~ dL (\ hat {y} _i, y_i) $ inline $ <br>  3. <math> </math> $ inline $ L (\ hat {y} _i, y_i) \ geq 0 $ inline $ <br><br>  Die Einschr√§nkung 2 gilt f√ºr alle Funktionen der Schichten <math> </math> $ inline $ f_j $ inline $   - Lassen Sie sie alle differenzierbar sein. <br><br>  Dar√ºber hinaus (einige davon habe ich nicht erw√§hnt) h√§ngen einige dieser Funktionen von den Parametern ab - den Gewichten des neuronalen Netzwerks - <math> </math> $ inline $ f_j (\ boldsymbol {x} _i | \ boldsymbol {\ omega} _j) $ inline $   .  Und die ganze Idee ist, solche Gewichte so aufzunehmen, dass <math> </math> $ inline $ \ hat {y} _i $ inline $   fiel mit zusammen <math> </math> $ inline $ y_i $ inline $   auf allen Objekten eines Datensatzes.  Ich stelle fest, dass nicht alle Funktionen Gewichte haben. <br><br>  Wo haben wir aufgeh√∂rt?  Alle Funktionen des neuronalen Netzes sind differenzierbar, die Fehlerfunktion ist ebenfalls differenzierbar.  Erinnern Sie sich an eine der Eigenschaften des Gradienten - zeigen Sie die Wachstumsrichtung der Funktion.  Wir nutzen dies, Einschr√§nkungen 1 und 3, die Tatsache, dass <br><p><math> </math> $$ display $$ L (F (\ boldsymbol {x} _i)) = L (f_k (f_ {k-1} (\ ldots (f_1 (\ boldsymbol {x} _i)))) = L (\ hat {y} _i) $$ display $$ </p><br>  und die Tatsache, dass ich partielle Ableitungen und Ableitungen einer komplexen Funktion betrachten kann.  Jetzt gibt es alles, was Sie zum Berechnen ben√∂tigen <br><p><math> </math> $$ display $$ \ frac {\ partielles L (F (\ boldsymbol {x} _i))} {\ partielles \ boldsymbol {\ omega_j}} $$ display $$ </p><br>  f√ºr jedes i und j.  Diese partielle Ableitung zeigt die Richtung, in die ge√§ndert werden soll <math> </math> $ inline $ \ boldsymbol {\ omega_j} $ inline $   zu vergr√∂√üern <math> </math> $ inline $ L $ inline $   .  Um zu reduzieren, m√ºssen Sie einen Schritt zur Seite machen <math> </math> $ inline $ - \ frac {\ partielles L (F (\ boldsymbol {x} _i))} {\ partielles \ boldsymbol {\ omega_j}} $ inline $   nichts kompliziertes. <br><br>  Der Prozess des Trainings des Netzwerks ist also wie folgt aufgebaut: Mehrmals in einem Zyklus durchlaufen wir f√ºr jedes betrachtete Datensatzobjekt den gesamten Datensatz (dies wird als √Ñra bezeichnet) <math> </math> $ inline $ L (\ hat {y} _i, y_i) $ inline $   (dies wird als Vorw√§rtsdurchlauf bezeichnet) und ber√ºcksichtigen Sie die partielle Ableitung <math> </math> $ inline $ \ partielle L $ inline $   f√ºr alle Gewichte <math> </math> $ inline $ \ boldsymbol {\ omega_j} $ inline $   Aktualisieren Sie dann die Gewichte (dies wird als R√ºckw√§rtsdurchlauf bezeichnet). <br><br>  Ich stelle fest, dass ich noch keine spezifischen Funktionen und Ebenen eingef√ºhrt habe.  Wenn zu diesem Zeitpunkt nicht klar ist, was mit all dem zu tun ist, schlage ich vor, weiterzulesen - es wird mehr Mathematik geben, aber jetzt wird es mit Beispielen gehen. <br><br><h2>  Mathe  Teil 2 (schwierig) </h2><br><h3>  Fehlerfunktion </h3><br>  Ich werde am Ende beginnen und die Fehlerfunktion f√ºr das Klassifizierungsproblem ableiten.  F√ºr das Regressionsproblem ist die Ableitung der Fehlerfunktion im Buch ‚ÄûDeep Learning.  Eintauchen in die Welt der neuronalen Netze. " <br><br>  Der Einfachheit halber gibt es ein neuronales Netzwerk (NN), das Katzenfotos von Hundefotos trennt, und es gibt eine Reihe von Fotos von Katzen und Hunden, auf die es eine richtige Antwort gibt <math> </math> $ inline $ y_ {true} $ inline $   . <br><p><math> </math> $$ display $$ NN (Bild | \ Omega) = y_ {pred} $$ display $$ </p><br>  Alles, was ich als n√§chstes tun werde, ist der Maximum-Likelihood-Methode sehr √§hnlich.  Daher besteht die Hauptaufgabe darin, die Wahrscheinlichkeitsfunktion zu finden.  Wenn wir die Details weglassen, ergibt eine solche Funktion, die die Vorhersage des neuronalen Netzwerks und die richtige Antwort vergleicht und wenn sie zusammenfallen, einen gro√üen Wert, wenn nicht, umgekehrt.  Die Wahrscheinlichkeit einer korrekten Antwort ergibt sich aus den angegebenen Parametern: <br><p><math> </math> $$ display $$ p (y_ {pred} = y_ {true} | \ Omega) $$ display $$ </p><br>  Und jetzt machen wir eine Finte, die anscheinend nicht von irgendwoher folgt.  Lassen Sie das neuronale Netzwerk eine Antwort in Form eines zweidimensionalen Vektors geben, dessen Summe 1 ist. Das erste Element dieses Vektors kann als Konfidenzma√ü bezeichnet werden, dass sich die Katze auf dem Foto befindet, und das zweite Element als Konfidenzma√ü, dass sich der Hund auf dem Foto befindet.  Ja, das ist fast wahrscheinlich! <br><p><math> </math> $$ display $$ NN (Bild | \ Omega) = \ left [\ begin {matrix} p_0 \\ p_1 \\\ end {matrix} \ right] $$ display $$ </p><br>  Jetzt kann die Wahrscheinlichkeitsfunktion wie folgt umgeschrieben werden: <br><p><math> </math> $$ display $$ p (y_ {pred} = y_ {true} | \ Omega) = p_ \ Omega (y_ {pred}) ^ t_ {0} * (1 - p_ \ Omega (y_ {pred})) ^ t_ {1} = \\ p_0 ^ {t_0} * p_1 ^ {t_1} $$ display $$ </p><br>  Wo <math> </math> $ inline $ t_0, t_1 $ inline $   Beschriftungen der richtigen Klasse, zum Beispiel, wenn <math> </math> $ inline $ y_ {true} = cat $ inline $   dann <math> </math> $ inline $ t_0 == 1, t_1 == 0 $ inline $   wenn <math> </math> $ inline $ y_ {true} = Hund $ inline $   dann <math> </math> $ inline $ t_0 == 0, t_1 == 1 $ inline $   .  Daher wird immer die Wahrscheinlichkeit einer Klasse ber√ºcksichtigt, die von einem neuronalen Netzwerk vorhergesagt werden sollte (aber nicht unbedingt von diesem vorhergesagt wird).  Dies kann nun auf eine beliebige Anzahl von Klassen verallgemeinert werden (z. B. m Klassen): <br><p><math> </math> $$ display $$ p (y_ {pred} = y_ {true} | \ Omega) = \ prod_0 ^ m p_i ^ {t_i} $$ display $$ </p><br>  In jedem Datensatz gibt es jedoch viele Objekte (z. B. N Objekte).  Ich m√∂chte, dass das neuronale Netzwerk auf jedes oder die meisten Objekte die richtige Antwort gibt.  Dazu m√ºssen Sie die Ergebnisse der obigen Formel f√ºr jedes Objekt aus dem Datensatz multiplizieren. <br><p><math> </math> $$ display $$ MaximumLikelyhood = \ prod_ {j = 0} ^ N \ prod_ {i = 0} ^ m p_ {i, j} ^ {t_ {i, j}} $$ display $$ </p><br>  Um gute Ergebnisse zu erzielen, muss diese Funktion maximiert werden.  Aber erstens ist es steiler zu minimieren, weil wir einen stochastischen Gradientenabstieg und alle Br√∂tchen daf√ºr haben - weisen Sie einfach ein Minus zu, und zweitens ist es schwierig, mit einer gro√üen Arbeit zu arbeiten - es ist Logarithmus. <br><p><math> </math> $$ display $$ CrossEntropyLoss = - \ sum \ limit_ {j = 0} ^ {N} \ sum \ limit_ {i = 0} ^ {m} t_ {i, j} \ cdot \ log (p_ {i, j }) $$ display $$ </p><br>  Gro√üartig!  Das Ergebnis war Kreuzentropie oder im bin√§ren Fall Logloss.  Diese Funktion ist einfach zu z√§hlen und noch einfacher zu unterscheiden: <br><p><math> </math> $$ display $$ \ frac {\ partielle CrossEntropyLoss} {\ partielle p_j} = - \ frac {\ boldsymbol {t_j}} {\ boldsymbol {p_ {j}}} $$ display $$ </p><br>  Sie m√ºssen f√ºr den Backpropagation-Algorithmus differenzieren.  Ich stelle fest, dass die Fehlerfunktion die Dimension des Vektors nicht √§ndert.  Wenn die Ausgabe wie im Fall von MNIST ein 10-dimensionaler Vektor von Antworten ist, erhalten wir bei der Berechnung der Ableitung einen 10-dimensionalen Vektor von Ableitungen.  Eine andere interessante Sache ist, dass nur ein Element der Ableitung nicht Null sein wird, bei dem <math> </math> $ inline $ t_ {i, j} \ neq 0 $ inline $   das hei√üt, mit der richtigen Antwort.  Und je geringer die Wahrscheinlichkeit einer korrekten Antwort ist, die von einem neuronalen Netzwerk an einem bestimmten Objekt vorhergesagt wird, desto gr√∂√üer ist die Fehlerfunktion. <br><br><h3>  Aktivierungsfunktionen </h3><br>  Am Ausgang jeder vollst√§ndig verbundenen Schicht eines neuronalen Netzwerks muss eine nichtlineare Aktivierungsfunktion vorhanden sein.  Ohne sie ist es unm√∂glich, ein sinnvolles neuronales Netzwerk zu trainieren.  Mit Blick auf die Zukunft ist eine vollst√§ndig verbundene Schicht eines neuronalen Netzwerks einfach eine Multiplikation der Eingabedaten mit einer Gewichtsmatrix.  In der linearen Algebra wird dies als lineare Karte bezeichnet - eine lineare Funktion.  Die Kombination von linearen Funktionen ist auch eine lineare Funktion.  Dies bedeutet jedoch, dass eine solche Funktion nur lineare Funktionen approximieren kann.  Leider werden aus diesem Grund keine neuronalen Netze ben√∂tigt. <br><br><h4>  Softmax </h4><br>  Normalerweise wird diese Funktion auf der letzten Schicht des Netzwerks verwendet, da sie den Vektor aus der letzten Schicht in einen Vektor von ‚ÄûWahrscheinlichkeiten‚Äú verwandelt: Jedes Element des Vektors liegt zwischen 0 und 1 und ihre Summe ist 1. Sie √§ndert die Dimension des Vektors nicht. <br><p><math> </math> $$ display $$ Softmax_i = \ frac {e ^ {x_i}} {\ sum \ limit_ {j} e ^ {x_j}} $$ display $$ </p><br>  Fahren wir nun mit der abgeleiteten Suche fort.  Als <math> </math> $ inline $ \ boldsymbol {x} $ inline $   Ist ein Vektor und alle seine Elemente sind immer im Nenner vorhanden, dann erhalten wir bei der Ableitung den Jacobi: <br><p><math> </math> $$ Anzeige $$ J_ {Softmax} = \ begin {F√§lle} x_i - x_i \ cdot x_j, i = j \\ - x_i \ cdot x_j, i \ neq j \ Ende {F√§lle} $$ Anzeige $$ </p><br>  Nun zur Backpropagation.  Der Vektor der Ableitungen stammt aus der vorherigen Schicht (normalerweise ist dies eine Fehlerfunktion). <math> </math> $ inline $ \ boldsymbol {dz} $ inline $   .  F√ºr den Fall <math> </math> $ inline $ \ boldsymbol {dz} $ inline $   kam von einer Fehlerfunktion auf mnist, <math> </math> $ inline $ \ boldsymbol {dz} $ inline $   - 10-dimensionaler Vektor.  Dann hat der Jacobianer eine Dimension von 10x10.  Um zu bekommen <math> </math> $ inline $ \ boldsymbol {dz_ {new}} $ inline $   , die weiter zur vorherigen Schicht geht (vergessen Sie nicht, dass wir vom Ende zum Anfang des Netzwerks gehen, wenn sich der Fehler zur√ºck ausbreitet), m√ºssen wir multiplizieren <math> </math> $ inline $ \ boldsymbol {dz} $ inline $   auf <math> </math> $ inline $ J_ {Softmax} $ inline $   (Zeile pro Spalte): <br><p><math> </math> $$ display $$ dz_ {new} = \ boldsymbol {dz} \ times J_ {Softmax} $$ display $$ </p><br>  Am Ausgang erhalten wir einen 10-dimensionalen Vektor von Ableitungen <math> </math> $ inline $ \ boldsymbol {dz_ {new}} $ inline $   . <br><br><h4>  Relu </h4><br><p><math> </math> $$ Anzeige $$ ReLU (x) = \ begin {F√§lle} x, x&gt; 0 \\ 0, x &lt;0 \ Ende {F√§lle} $$ Anzeige $$ </p><br>  ReLU wurde nach 2011 massiv eingesetzt, als der Artikel ‚ÄûDeep Sparse Rectifier Neural Networks‚Äú ver√∂ffentlicht wurde.  Eine solche Funktion war jedoch zuvor bekannt.  Das Konzept der ‚ÄûAktivierungskraft‚Äú gilt f√ºr ReLU (weitere Einzelheiten finden Sie im Buch ‚ÄûDeep Learning. Eintauchen in die Welt der neuronalen Netze‚Äú).  Das Hauptmerkmal, das ReLU attraktiver macht als andere Aktivierungsfunktionen, ist die einfache Ableitungsberechnung: <br><p><math> </math> $$ Anzeige $$ d (ReLU (x)) = \ begin {F√§lle} 1, x&gt; 0 \\ 0, x &lt;0 \ Ende {F√§lle} $$ Anzeige $$ </p><br>  Somit ist ReLU rechnerisch effizienter als andere Aktivierungsfunktionen (Sigmoid, hyperbolische Tangente usw.). <br><br><h3>  Vollst√§ndig verbundene Schicht </h3><br>  Jetzt ist es an der Zeit, eine vollst√§ndig verbundene Schicht zu diskutieren.  Das wichtigste von allen anderen, denn in dieser Schicht befinden sich alle Gewichte, die angepasst werden m√ºssen, damit das neuronale Netzwerk gut funktioniert.  Eine vollst√§ndig verbundene Schicht ist einfach eine Gewichtsmatrix: <br><p><math> </math> $$ display $$ W = | w_ {i, j} | $$ display $$ </p><br>  Eine neue interne Darstellung wird erhalten, wenn die Gewichtsmatrix mit der Eingabespalte multipliziert wird: <br><p><math> </math> $$ display $$ \ boldsymbol {x} _ {new} = W \ cdot \ boldsymbol {x} $$ display $$ </p><br>  Wo <math> </math> $ inline $ \ boldsymbol {x} $ inline $   hat Gr√∂√üe <math> </math> $ inline $ input \ _shape $ inline $   und <math> </math> $ inline $ x_ {new} $ inline $   - - <math> </math> $ inline $ output \ _shape $ inline $   .  Zum Beispiel <math> </math> $ inline $ \ boldsymbol {x} $ inline $   - 784-dimensionaler Vektor und <math> </math> $ inline $ \ boldsymbol {x} _ {new} $ inline $   Ist ein 100-dimensionaler Vektor, dann hat die Matrix W eine Gr√∂√üe von 100x784.  Es stellt sich heraus, dass auf dieser Ebene 100x784 = 78.400 Gewichte sind. <br><br>  Mit der R√ºckausbreitung des Fehlers muss man die Ableitung in Bezug auf jedes Gewicht dieser Matrix nehmen.  Vereinfachen Sie das Problem und nehmen Sie nur die Ableitung in Bezug auf <math> </math> $ inline $ w_ {1,1} $ inline $   .  Beim Multiplizieren der Matrix und des Vektors das erste Element des neuen Vektors <math> </math> $ inline $ \ boldsymbol {x} _ {new} $ inline $   ist gleich <math> </math> $ inline $ x_ {new ~ 1} = w_ {1,1} \ cdot x_1 + ... + w_ {1,784} \ cdot x_ {784} $ inline $   und die Ableitung <math> </math> $ inline $ x_ {new ~ 1} $ inline $   von <math> </math> $ inline $ w_ {1,1} $ inline $   wird einfach sein <math> </math> $ inline $ x_1 $ inline $   , m√ºssen Sie nur die Ableitung des oben genannten Betrags nehmen.  √Ñhnliches gilt f√ºr alle anderen Gewichte.  Dies ist jedoch kein Algorithmus zur Fehlerr√ºck√ºbertragung, solange es sich nur um eine Matrix von Ableitungen handelt.  Sie m√ºssen sich daran erinnern, dass von der n√§chsten Ebene zu dieser (der Fehler geht von Ende zu Anfang) ein 100-dimensionaler Gradientenvektor kommt <math> </math> $ inline $ d \ boldsymbol {z} $ inline $   .  Erstes Element dieses Vektors <math> </math> $ inline $ dz_1 $ inline $   wird mit allen Elementen der Matrix von Derivaten multipliziert, die an der Erstellung "teilgenommen" haben <math> </math> $ inline $ x_ {new ~ 1} $ inline $   d.h. <math> </math> $ inline $ x_1, x_2, ..., x_ {784} $ inline $   .  Ebenso der Rest der Elemente.  Wenn Sie dies in die Sprache der linearen Algebra √ºbersetzen, dann ist es so geschrieben: <br><p><math> </math> $$ display $$ \ frac {\ partielles L} {\ partielles W} = (d \ boldsymbol {z}, ~ dW) = \ left (\ begin {matrix} dz_ {1} \ cdot \ boldsymbol {x} \ \ ... \\ dz_ {100} \ cdot \ boldsymbol {x} \ end {matrix} \ right) _ {100} $$ display $$ </p><br>  Die Ausgabe ist eine 100x784-Matrix. <br><img src="https://habrastorage.org/webt/1m/8_/hl/1m8_hljpr28gm3dikkgpsk4zss8.png"><br><br>  Jetzt m√ºssen Sie verstehen, was auf die vorherige Ebene √ºbertragen werden soll.  Aus diesem Grund und um besser zu verstehen, was jetzt passiert ist, m√∂chte ich aufschreiben, was passiert ist, wenn Ableitungen auf dieser Ebene in einer etwas anderen Sprache verwendet werden, um von den Besonderheiten von ‚Äûwas multipliziert wird‚Äú zu Funktionen (wieder) zu gelangen. <br><br>  Als ich die Gewichte anpassen wollte, wollte ich die Ableitung der Fehlerfunktion f√ºr diese Gewichte nehmen: <math> </math> $ inline $ \ frac {\ partielles L} {\ partielles W} $ inline $   .  Es wurde oben gezeigt, wie Ableitungen von Fehlerfunktionen und Aktivierungsfunktionen verwendet werden.  Daher k√∂nnen wir einen solchen Fall betrachten (in <math> </math> $ inline $ d \ boldsymbol {z} $ inline $   Alle Ableitungen der Fehlerfunktion und der Aktivierungsfunktionen sitzen bereits): <br><p><math> </math> $$ display $$ \ frac {\ partielles L} {\ partielles W} = d \ boldsymbol {z} \ cdot \ frac {\ partielles \ boldsymbol {x} _ {new} (W)} {\ partielles W} $ $ display $$ </p><br>  Dies kann getan werden, weil Sie √ºberlegen k√∂nnen <math> </math> $ inline $ \ boldsymbol {x} _ {new} $ inline $   als Funktion von W: <math> </math> $ inline $ \ boldsymbol {x} _ {new} = W \ cdot \ boldsymbol {x} $ inline $   . <br>  Sie k√∂nnen dies in die obige Formel einsetzen: <br><br><p><math> </math> $$ Anzeige $$ \ frac {\ partielles L} {\ partielles W} = d \ Boldsymbol {z} \ cdot \ frac {\ partielles W \ cdot \ Boldsymbol {x}} {\ partielles W} = d \ Boldsymbol { z} \ cdot E \ cdot \ boldsymbol {x} $$ display $$ </p><br>  Wobei E eine aus Einheiten bestehende Matrix ist (KEINE Einheitsmatrix). <br><br>  Wenn Sie nun die Ableitung der vorherigen Ebene verwenden m√ºssen (auch wenn es sich zur Vereinfachung der Berechnungen um eine vollst√§ndig verbundene Ebene handelt, die jedoch im Allgemeinen nichts √§ndert), m√ºssen Sie dies ber√ºcksichtigen <math> </math> $ inline $ \ boldsymbol {x} $ inline $   als Funktion der vorherigen Schicht <math> </math> $ inline $ \ boldsymbol {x} (W_ {old}) $ inline $   :: <br><p><math> </math> $$ Anzeige $$ \ begin {gesammelt} \ frac {\ partiell L} {\ partiell W_ {alt}} = d \ boldsymbol {z} \ cdot \ frac {\ partiell \ boldsymbol {x} _ {neu} (W. )} {\ partielles W_ {alt}} = d \ boldsymbol {z} \ cdot \ frac {\ partielles W \ cdot \ boldsymbol {x} (W_ {alt})} {\ partielles W_ {alt}} = \\ = d \ boldsymbol {z} \ cdot \ frac {\ partielles W \ cdot W_ {alt} \ cdot \ boldsymbol {x} _ {alt}} {\ partielles W_ {alt}} = d \ boldsymbol {z} \ cdot W \ cdot E \ cdot \ boldsymbol {x} _ {alt} = \\ = d \ boldsymbol {z} _ {neu} \ cdot E \ cdot \ boldsymbol {x} _ {alt} \ end {gesammelt} $$ $$ anzeigen </p><br>  Genau <math> </math> $ inline $ d \ boldsymbol {z} _ {new} = d \ boldsymbol {z} \ cdot W $ inline $   und Sie m√ºssen zur vorherigen Ebene senden. <br><br><h2>  Code </h2><br><blockquote>  Dieser Artikel zielt haupts√§chlich darauf ab, die Mathematik neuronaler Netze zu erkl√§ren.  Ich werde dem Code sehr wenig Zeit widmen. </blockquote><br>  Dies ist eine Beispielimplementierung der Fehlerfunktion: <br><br><pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CrossEntropy</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, y_true, y_hat)</span></span></span><span class="hljs-function">:</span></span> self.y_hat = y_hat self.y_true = y_true self.loss = -np.sum(self.y_true * np.log(y_hat)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.loss <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">backward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> dz = -self.y_true / self.y_hat <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> dz</code> </pre> <br>  Die Klasse verf√ºgt √ºber Methoden f√ºr den direkten und den umgekehrten Durchlauf.  Zum Zeitpunkt des direkten Durchlaufs speichert die Klasseninstanz die Daten in der Ebene und verwendet sie zum Zeitpunkt des R√ºcklaufs zur Berechnung des Gradienten.  Die restlichen Schichten sind auf die gleiche Weise aufgebaut.  Dank dessen wird es m√∂glich, ein vollst√§ndig verbundenes Neuronales in diesem Stil zu schreiben: <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">MnistNet</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.d1_layer = Dense(<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) self.a1_layer = ReLu() self.drop1_layer = Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>) self.d2_layer = Dense(<span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>) self.a2_layer = ReLu() self.drop2_layer = Dropout(<span class="hljs-number"><span class="hljs-number">0.25</span></span>) self.d3_layer = Dense(<span class="hljs-number"><span class="hljs-number">50</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>) self.a3_layer = Softmax() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, train=True)</span></span></span><span class="hljs-function">:</span></span> ... <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">backward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, dz, learning_rate=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.01</span></span></span></span><span class="hljs-function"><span class="hljs-params">, mini_batch=True, update=False, len_mini_batch=None)</span></span></span><span class="hljs-function">:</span></span> ...</code> </pre><br>  Den vollst√§ndigen Code finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> . <br>  Ich rate auch, diesen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel √ºber Habr√©</a> zu studieren. <br><br><h2>  Fazit </h2><br>  Ich hoffe, dass ich erkl√§ren und zeigen konnte, dass hinter neuronalen Netzen eine recht einfache Mathematik steckt und dass dies √ºberhaupt nicht be√§ngstigend ist.  F√ºr ein tieferes Verst√§ndnis lohnt es sich jedoch, ein eigenes ‚ÄûFahrrad‚Äú zu schreiben.  Korrekturen und Vorschl√§ge lesen Sie gerne in den Kommentaren. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de460589/">https://habr.com/ru/post/de460589/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de460567/index.html">Native reagieren: Erstellen Sie eine ziehbare und wischbare Liste</a></li>
<li><a href="../de460569/index.html">Schreiben von Software mit der Funktionalit√§t von Client-Server-Dienstprogrammen Windows, Teil 01</a></li>
<li><a href="../de460573/index.html">Google gibt an, dass "reCAPTCHA" keine Benutzerdaten missbraucht. Lohnt es sich zu glauben?</a></li>
<li><a href="../de460577/index.html">Es lebe der K√∂nig: grausame Welt der Hierarchie in einem Rudel streunender Hunde</a></li>
<li><a href="../de460587/index.html">Funkmodul f√ºr kapazitiven Bodenfeuchtesensor am nRF52832</a></li>
<li><a href="../de460591/index.html">Root auf einem Tenda Nova MW6-Router erhalten</a></li>
<li><a href="../de460593/index.html">"Universal" im Entwicklungsteam: Nutzen oder Schaden?</a></li>
<li><a href="../de460597/index.html">So diagnostizieren Sie SDK-Integrationsprobleme. Die Erfahrung des SDK-Entwicklungsteams von Yandex Mobile Ads</a></li>
<li><a href="../de460599/index.html">Nachrichten aus der Welt von OpenStreetMap Nr. 468 (07/02/2019 - 08/07/2019)</a></li>
<li><a href="../de460603/index.html">V2G. Elektroautos werden dazu beitragen, die Produktion und den Verbrauch von Strom auszugleichen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>