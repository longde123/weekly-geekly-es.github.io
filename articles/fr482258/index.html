<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üì≥ üôÜüèæ üññüèª Comment fonctionnent les r√©seaux de neurones et pourquoi ils ont commenc√© √† rapporter beaucoup d'argent ü§ôüèº üë©üèø‚Äçüè≠ ‚ùå</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Les r√©seaux de neurones sont pass√©s d'un √©tat de curiosit√© acad√©mique √† une industrie massive 


 Au cours de la derni√®re d√©cennie, les ordinateurs on...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment fonctionnent les r√©seaux de neurones et pourquoi ils ont commenc√© √† rapporter beaucoup d'argent</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/482258/"><h3>  Les r√©seaux de neurones sont pass√©s d'un √©tat de curiosit√© acad√©mique √† une industrie massive </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/36a/21a/fd3/36a21afd35a805d95a2a67b2ec52080a.jpg"><br><br>  Au cours de la derni√®re d√©cennie, les ordinateurs ont consid√©rablement am√©lior√© leur capacit√© √† comprendre le monde qui les entoure.  Le logiciel pour l'√©quipement photo reconna√Æt automatiquement les visages des personnes.  Les smartphones convertissent la parole en texte.  Les robots motoris√©s reconnaissent les objets sur la route et √©vitent les collisions avec eux. <br><br>  Au c≈ìur de toutes ces perc√©es se trouve la technologie de l'intelligence artificielle (IA) appel√©e deep learning (GO).  GO est bas√© sur des r√©seaux de neurones (NS), des structures de donn√©es inspir√©es de r√©seaux compos√©s de neurones biologiques.  Les NS sont organis√©s en couches et les entr√©es d'une couche sont connect√©es aux sorties de la voisine. <br><br>  Les informaticiens exp√©rimentent la NS depuis les ann√©es 50.  Cependant, le fondement de la vaste industrie GO actuelle a √©t√© pos√© par deux perc√©es majeures - l'une survenue en 1986, la seconde en 2012. La perc√©e de 2012 - la r√©volution de GO - a √©t√© associ√©e √† la d√©couverte que l'utilisation de NS avec un grand nombre de couches nous permettra d'am√©liorer consid√©rablement leur efficacit√©.  La d√©couverte a √©t√© facilit√©e par les volumes croissants de donn√©es et de puissance de calcul. <br><a name="habracut"></a><br>  Dans cet article, nous vous pr√©senterons le monde de l'Assembl√©e nationale.  Nous expliquerons ce qu'est la NS, comment elle fonctionne et d'o√π elle vient.  Et nous √©tudierons pourquoi - malgr√© de nombreuses d√©cennies de recherches ant√©rieures - les SN ne sont devenues vraiment utiles qu'en 2012. <br><br><h2>  Les r√©seaux de neurones sont apparus dans les ann√©es 1950 </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/770/c2e/327/770c2e3276b0e875a99025f4887dda36.jpg"><br>  <i>Frank Rosenblatt travaille sur son perceptron - un des premiers mod√®les NS</i> <br><br>  L'id√©e de l'Assembl√©e nationale est assez ancienne - du moins selon les normes de l'informatique.  En 1957, <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25BE%25D0%25B7%25D0%25B5%25D0%25BD%25D0%25B1%25D0%25BB%25D0%25B0%25D1%2582%25D1%2582,_%25D0%25A4%25D1%2580%25D1%258D%25D0%25BD%25D0%25BA">Frank Rosenblatt</a> de l'Universit√© Cornell a publi√© un <a href="https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf">rapport</a> d√©crivant un des premiers concepts NS appel√© le perceptron.  En 1958, avec le soutien de l'US Navy, il cr√©e un syst√®me primitif capable d'analyser 20x20 pixels et de reconna√Ætre des formes g√©om√©triques simples. <br><br>  L'objectif principal de Rosenblatt n'√©tait pas de cr√©er un syst√®me de classification d'images pratique.  Il a essay√© de comprendre le fonctionnement du cerveau humain, cr√©ant des syst√®mes informatiques organis√©s √† son image.  Cependant, ce concept a suscit√© un enthousiasme excessif de la part de tiers. <br><br>  "Aujourd'hui, l'US Navy a r√©v√©l√© au monde l'embryon d'un ordinateur √©lectronique qui devrait pouvoir marcher, parler, voir, √©crire, se reproduire et √™tre conscient de son existence", √©crit le New York Times. <br><br>  En fait, chaque neurone de la NS n'est qu'une fonction math√©matique.  Chaque neurone calcule la somme pond√©r√©e des donn√©es d'entr√©e - plus le poids d'entr√©e est √©lev√©, plus ces donn√©es d'entr√©e affectent fortement la sortie du neurone.  Ensuite, la somme pond√©r√©e est appliqu√©e √† la fonction ¬´d'activation¬ª non lin√©aire - √† cette √©tape, les NS peuvent simuler des ph√©nom√®nes non lin√©aires complexes. <br><br>  Les capacit√©s des premiers perceptrons que Rosenblatt a exp√©riment√©s - et NS en g√©n√©ral - d√©coulent de leur capacit√© √† "apprendre" avec des exemples.  Les NS sont form√©s en ajustant les poids d'entr√©e des neurones sur la base des r√©sultats du r√©seau avec les donn√©es d'entr√©e s√©lectionn√©es par exemple.  Si le r√©seau classe correctement l'image, les pond√©rations contribuant √† la r√©ponse correcte augmentent, tandis que d'autres diminuent.  Si le r√©seau est incorrect, les poids s'ajustent dans l'autre sens. <br><br>  Une telle proc√©dure a permis aux premiers SN ¬´d'apprendre¬ª d'une mani√®re qui rappelle le comportement du syst√®me nerveux humain.  Le battage m√©diatique autour de cette approche ne s'est pas arr√™t√© dans les ann√©es 1960.  Cependant, le <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25B5%25D1%2580%25D1%2586%25D0%25B5%25D0%25BF%25D1%2582%25D1%2580%25D0%25BE%25D0%25BD%25D1%258B_(%25D0%25BA%25D0%25BD%25D0%25B8%25D0%25B3%25D0%25B0)">livre influent de</a> 1969 des auteurs des informaticiens Marvin Minsky et Seymour Papert a montr√© que ces premi√®res NS avaient des limites importantes. <br><br>  Les premiers NS de Rosenblatt n'avaient qu'une ou deux couches entra√Æn√©es.  Minsky et Papert ont montr√© que de telles NS sont math√©matiquement incapables de mod√©liser des ph√©nom√®nes complexes du monde r√©el. <br><br>  En principe, les SN plus profonds √©taient plus capables.  Cependant, une telle NS surchargerait ces mis√©rables ressources informatiques que les ordinateurs poss√©daient √† l'√©poque.  Les algorithmes de <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25BE%25D0%25B8%25D1%2581%25D0%25BA_%25D0%25B2%25D0%25BE%25D1%2581%25D1%2585%25D0%25BE%25D0%25B6%25D0%25B4%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5%25D0%25BC_%25D0%25BA_%25D0%25B2%25D0%25B5%25D1%2580%25D1%2588%25D0%25B8%25D0%25BD%25D0%25B5">recherche ascendants les</a> plus simples utilis√©s dans les premiers NS n'ont pas √©volu√© pour les NS plus profonds. <br><br>  En cons√©quence, l'Assembl√©e nationale a perdu tout soutien dans les ann√©es 1970 et au d√©but des ann√©es 1980 - elle faisait partie de l'√®re de ¬´l'hiver de l'IA¬ª. <br><br><h2>  Algorithme r√©volutionnaire </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/970/c5b/807/970c5b8074a5b4516be251bd4b9a31b0.jpg"><br>  <i>Mon propre r√©seau de neurones bas√© sur des ¬´√©quipements souples¬ª estime que la probabilit√© d'avoir un hot dog sur cette photo est de 1. Nous deviendrons riches!</i> <br><br>  La chance s'est de nouveau tourn√©e vers la NS gr√¢ce au c√©l√®bre <a href="https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf">travail de</a> 1986, qui a introduit le concept de la propagation arri√®re - une m√©thode pratique d'enseignement de la NS. <br><br>  Supposons que vous travailliez en tant que programmeur dans une soci√©t√© de logiciels imaginaires et que l'on vous ait demand√© de cr√©er une application qui d√©termine s'il y a un hot dog dans l'image.  Vous commencez √† travailler avec un NS initialis√© au hasard, qui prend une image d'entr√©e et sort une valeur de 0 √† 1 - o√π 1 signifie ¬´hot dog¬ª et 0 signifie ¬´pas hot dog¬ª. <br><br>  Pour former le r√©seau, vous collectez des milliers d'images, sous chacune desquelles se trouve une √©tiquette indiquant s'il y a un hot dog sur cette image.  Vous lui donnez la premi√®re image - et il y a un hot-dog dessus - dans le r√©seau neuronal.  Il donne une valeur de sortie de 0,07, ce qui signifie ¬´pas de hot-dog¬ª.  Ce n'est pas la bonne r√©ponse;  le r√©seau aurait d√ª renvoyer une r√©ponse proche de 1. <br><br>  Le but de l'algorithme de r√©tropropagation est d'ajuster les poids d'entr√©e afin que le r√©seau produise une valeur plus √©lev√©e s'il re√ßoit √† nouveau cette image - et, de pr√©f√©rence, d'autres images o√π il y a des hot dogs.  Pour cela, l'algorithme de r√©tropropagation commence par examiner les neurones d'entr√©e de la couche de sortie.  Chaque valeur a une variable de poids.  L'algorithme de r√©tropropagation ajuste chaque poids dans une direction telle que le NS donne une valeur plus √©lev√©e.  Plus la valeur d'entr√©e est √©lev√©e, plus son poids augmente. <br><br>  Jusqu'√† pr√©sent, je d√©cris la mont√©e la plus simple au sommet famili√®re aux chercheurs dans les ann√©es 1960.  La perc√©e de r√©tropropagation a √©t√© l'√©tape suivante: l'algorithme utilise des d√©riv√©es partielles pour r√©partir le ¬´d√©faut¬ª pour la sortie incorrecte entre les entr√©es des neurones.  L'algorithme calcule comment un petit changement dans chaque valeur d'entr√©e affectera la sortie finale d'un neurone, et si ce changement rapprochera le r√©sultat de la bonne r√©ponse, ou vice versa. <br><br>  Le r√©sultat est un ensemble de valeurs d'erreur pour chaque neurone de la couche pr√©c√©dente - en fait, un signal qui √©value si la valeur de chaque neurone est trop grande ou trop petite.  Ensuite, l'algorithme r√©p√®te le processus de r√©glage pour les nouveaux neurones de la deuxi√®me couche [de la fin].  Il modifie l√©g√®rement les poids d'entr√©e de chaque neurone pour rapprocher le r√©seau de la bonne r√©ponse. <br><br>  Ensuite, l'algorithme utilise √† nouveau des d√©riv√©es partielles pour calculer comment la valeur de chaque entr√©e de la couche pr√©c√©dente a affect√© les erreurs de sortie de cette couche - et propage ces erreurs √† la couche pr√©c√©dente, o√π le processus se r√©p√®te. <br><br>  Il s'agit simplement d'un mod√®le de r√©tropropagation simplifi√©.  Si vous avez besoin de d√©tails math√©matiques d√©taill√©s, je recommande le livre de Michael Nielsen sur ce sujet [ <a href="https://habr.com/ru/post/456738/">et nous avons sa traduction</a> / env.  trad.].  Pour nos besoins, il suffit que la distribution inverse change radicalement la gamme des NS form√©s.  Les gens n'√©taient plus limit√©s √† de simples r√©seaux √† une ou deux couches.  Ils pourraient cr√©er des r√©seaux √† cinq, dix ou cinquante couches, et ces r√©seaux pourraient avoir une structure interne arbitrairement complexe. <br><br>  L'invention de la r√©tropropagation a lanc√© le deuxi√®me boom de l'Assembl√©e nationale, qui a commenc√© √† produire des r√©sultats pratiques.  En 1998, un groupe de chercheurs d'AT &amp; T a montr√© comment les r√©seaux de neurones peuvent √™tre utilis√©s pour reconna√Ætre des nombres manuscrits, ce qui a permis d'automatiser le traitement des ch√®ques. <br><br>  "Le principal message de ce travail est que nous pouvons cr√©er des syst√®mes am√©lior√©s pour reconna√Ætre les mod√®les, en s'appuyant davantage sur l'apprentissage automatique et moins sur l'heuristique d√©velopp√©e manuellement", ont √©crit les auteurs. <br><br>  Et pourtant, dans cette phase, les NS n'√©taient qu'une des nombreuses technologies √† la disposition des chercheurs en apprentissage automatique.  Lorsque j'ai √©tudi√© dans un cours d'IA √† l'institut en 2008, les r√©seaux de neurones n'√©taient qu'un des neuf algorithmes MO, parmi lesquels nous pouvions choisir l'option appropri√©e pour la t√¢che.  Cependant, GO s'appr√™tait d√©j√† √† occulter le reste de la technologie. <br><br>  Le Big Data d√©montre la puissance du deep learning <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0ff/b2d/663/0ffb2d6630240722208088bd6be34644.jpg"><br>  <i>D√©tente d√©tect√©e.</i>  <i>Chance de la plage 1.0.</i>  <i>Nous commen√ßons la proc√©dure d'utilisation de Mai Tai.</i> <br><br>  La r√©tropropagation a facilit√© le processus de calcul de la NS, mais les r√©seaux plus profonds avaient encore besoin de plus de ressources informatiques que les petits.  Les r√©sultats d'√©tudes men√©es dans les ann√©es 1990 et 2000 ont souvent montr√© qu'il √©tait possible de b√©n√©ficier de moins en moins de complications suppl√©mentaires de la NS. <br><br>  Puis la pens√©e des gens a √©t√© chang√©e par le c√©l√®bre travail de 2012, qui d√©crivait la NS sous le nom d'AlexNet, du nom du chercheur principal Alex Krizhevsky.  Tout comme les r√©seaux plus profonds pourraient fournir une efficacit√© r√©volutionnaire, mais uniquement en combinaison avec une abondance de puissance informatique et une √©norme quantit√© de donn√©es. <br><br>  AlexNet a d√©velopp√© un trio d'informaticiens de l'Universit√© de Toronto pour participer au concours scientifique ImageNet.  Les organisateurs du concours ont collect√© un million d'images sur Internet, chacune √©tant √©tiquet√©e et affect√©e √† l'une des milliers de cat√©gories d'objets, par exemple ¬´cerise¬ª, ¬´porte-conteneurs¬ª ou ¬´l√©opard¬ª.  Les chercheurs en IA ont √©t√© invit√©s √† former leurs programmes MO sur des parties de ces images, puis √† essayer de mettre les √©tiquettes correctes pour d'autres images que le logiciel n'avait pas rencontr√©es auparavant.  Le logiciel devait s√©lectionner cinq √©tiquettes possibles pour chaque image, et la tentative √©tait consid√©r√©e comme r√©ussie si l'une d'elles co√Øncidait avec la vraie. <br><br>  C'√©tait une t√¢che difficile et jusqu'en 2012, les r√©sultats n'√©taient pas tr√®s bons.  Pour le vainqueur 2011, le taux d'erreur √©tait de 25%. <br><br>  En 2012, l'√©quipe AlexNet a surpass√© tous les concurrents en donnant des r√©ponses avec 15% d'erreurs.  Pour le concurrent le plus proche, ce chiffre √©tait de 26%. <br><br>  Des chercheurs de Toronto ont combin√© plusieurs techniques pour obtenir des r√©sultats r√©volutionnaires.  L'un d'eux √©tait l'utilisation de <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B2%25D1%2591%25D1%2580%25D1%2582%25D0%25BE%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C">n√©vroses convolutives</a> (SNS).  En fait, le SNA, pour ainsi dire, entra√Æne de petits r√©seaux de neurones - dont les donn√©es d'entr√©e sont des carr√©s avec un c√¥t√© de 7 √† 11 pixels - puis les ¬´superpose¬ª sur une image plus grande. <br><br>  "C'est comme si vous preniez un petit gabarit ou un pochoir et tentiez de le comparer avec chaque point de l'image", nous a dit l'an dernier le chercheur en IA Jie Tan.  - Vous avez un pochoir de chien, et vous le fixez √† l'image, et voyez s'il y a un chien l√†-bas?  Sinon, d√©placez le pochoir.  Et donc pour l'image enti√®re.  Et peu importe o√π le chien appara√Æt sur la photo.  Le pochoir co√Øncidera avec lui.  Chaque sous-section du r√©seau ne doit pas devenir un classificateur de chien distinct. ¬ª <br><br>  Un autre facteur cl√© de succ√®s pour AlexNet a √©t√© l'utilisation de cartes graphiques pour acc√©l√©rer le processus d'apprentissage.  Les cartes graphiques ont une puissance de traitement parall√®le, bien adapt√©e aux calculs r√©p√©titifs n√©cessaires pour former un r√©seau neuronal.  En transf√©rant la charge de calcul √† une paire de GPU - le Nvidia GTX 580, avec 3 Go de m√©moire chacun -, les chercheurs ont pu d√©velopper et former un r√©seau extr√™mement vaste et complexe.  AlexNet avait huit couches entra√Ænables, 650 000 neurones et 60 millions de param√®tres. <br><br>  Enfin, le succ√®s d'AlexNet a √©galement √©t√© assur√© par la grande taille de la base de donn√©es d'images de formation d'ImageNet: un million de pi√®ces.  De nombreuses images sont n√©cessaires pour affiner 60 millions de param√®tres.  Pour remporter une victoire d√©cisive, AlexNet a √©t√© aid√© par la combinaison d'un r√©seau complexe et d'un grand ensemble de donn√©es. <br><br>  Je me demande pourquoi une telle perc√©e ne s'est pas produite plus t√¥t: <br><br><ul><li>  La paire de GPU grand public utilis√©e par les chercheurs d'AlexNet √©tait loin d'√™tre le dispositif informatique le plus puissant de 2012.  Cinq et m√™me dix ans auparavant, il y avait des ordinateurs plus puissants.  De plus, la technologie d'acc√©l√©ration de l'apprentissage des NS √† l'aide de cartes graphiques est connue depuis au moins 2004. </li><li>  La base d'un million d'images √©tait inhabituellement importante pour l'enseignement des algorithmes MO en 2012, cependant, la collecte de telles donn√©es n'√©tait pas une nouvelle technologie pour cette ann√©e.  Une √©quipe de recherche bien financ√©e pourrait facilement constituer une base de donn√©es de cette taille cinq ou dix ans plus t√¥t. </li><li>  Les principaux algorithmes utilis√©s dans AlexNet n'√©taient pas nouveaux.  L'algorithme de r√©tropropagation en 2012 existait d√©j√† depuis environ un quart de si√®cle.  Des id√©es cl√©s li√©es aux r√©seaux de neurones convolutifs ont √©t√© d√©velopp√©es dans les ann√©es 1980 et 1990. </li></ul><br>  Ainsi, chacun des √©l√©ments de r√©ussite d'AlexNet existait s√©par√©ment bien avant la perc√©e.  De toute √©vidence, personne n'a pens√© √† les combiner - en grande partie parce que personne ne savait √† quel point cette combinaison serait puissante. <br><br>  L'augmentation de la profondeur des SN n'a pratiquement pas am√©lior√© l'efficacit√© de leur travail s'ils n'utilisaient pas des ensembles de donn√©es de formation suffisamment volumineux.  Et l'extension de l'ensemble de donn√©es n'a pas am√©lior√© les performances des petits r√©seaux.  Pour voir l'augmentation de l'efficacit√©, nous avions besoin √† la fois de r√©seaux plus profonds et de plus grands ensembles de donn√©es - ainsi que d'une puissance de calcul importante qui nous a permis de mener le processus de formation dans un d√©lai raisonnable.  L'√©quipe AlexNet a √©t√© la premi√®re √† r√©unir les trois √©l√©ments en un seul programme. <br><br><h2>  Le boom du deep learning </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/ef0/fb4/922/ef0fb4922c3164251c722134b84460e3.jpg"><br><br>  La d√©monstration de toute la puissance de la NS profonde, fournie par une quantit√© suffisante de donn√©es de formation, a √©t√© remarqu√©e par de nombreuses personnes - √† la fois parmi les scientifiques, les chercheurs et les repr√©sentants de l'industrie. <br><br>  Le premier concours ImageNet √† changer.  Jusqu'en 2012, la plupart des candidats utilisaient des technologies autres que le deep learning.  Lors du concours de 2013, comme l'ont √©crit les sponsors, ¬´la majorit√©¬ª des candidats ont utilis√© GO. <br><br>  Le pourcentage d'erreurs parmi les gagnants a progressivement diminu√© - passant d'un impressionnant 16% chez AlexNet en 2012 √† 2,3% en 2017: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9ba/95e/b4b/9ba95eb4baee6580ea97ac76347a02e4.png"><br><br>  La r√©volution GO s'est rapidement propag√©e dans toute l'industrie.  En 2013, Google a acquis une startup form√©e des auteurs d'AlexNet et a utilis√© sa technologie comme base pour la fonction de recherche d'images dans Google Photos.  En 2014, Facebook vantait son propre logiciel qui reconna√Æt les images √† l'aide de GO.  Apple utilise GO pour la reconnaissance faciale dans iOS depuis au moins 2016. <br><br>  GO sous-tend √©galement l'am√©lioration r√©cente de la technologie de reconnaissance vocale.  Siri d'Apple, Alexa d'Amazon, Cortana de Microsoft et l'assistant de Google utilisent GO - soit pour comprendre les mots d'une personne, soit pour g√©n√©rer une voix plus naturelle, ou les deux. <br><br>  Ces derni√®res ann√©es, une tendance autosuffisante est apparue dans l'industrie, dans laquelle l'augmentation de la puissance de calcul, du volume de donn√©es et de la profondeur du r√©seau se soutiennent mutuellement.  L'√©quipe AlexNet a utilis√© le GPU car elle offrait l'informatique parall√®le √† un prix raisonnable.  Mais au cours des derni√®res ann√©es, de plus en plus d'entreprises ont commenc√© √† d√©velopper leurs propres puces, sp√©cialement con√ßues pour √™tre utilis√©es dans le domaine des MO. <br><br>  Google a annonc√© la sortie de la puce Tensor Processing Unit sp√©cialement con√ßue pour la NS en 2016. La m√™me ann√©e, Nvidia a annonc√© la sortie d'un nouveau GPU appel√© Tesla P100, optimis√© pour la NS.  Intel a r√©pondu √† l'appel avec sa puce AI en 2017. En 2018, Amazon a annonc√© la sortie de sa propre puce AI, qui peut √™tre utilis√©e dans le cadre des services cloud de l'entreprise.  M√™me Microsoft travaillerait sur sa puce AI. <br><br>  Les fabricants de smartphones travaillent √©galement sur des puces qui permettront aux appareils mobiles de faire plus d'informatique en utilisant NS localement, sans avoir √† t√©l√©charger de donn√©es sur les serveurs.  Une telle informatique sur les appareils r√©duit la latence et am√©liore la confidentialit√©. <br><br>  M√™me Tesla est entr√© dans ce jeu avec des jetons sp√©ciaux.  Cette ann√©e, Tesla a pr√©sent√© un nouvel ordinateur puissant, optimis√© pour le calcul de NS.  Tesla l'a nomm√© Full Self-Driving Computer et l'a pr√©sent√© comme un moment cl√© dans la strat√©gie de l'entreprise pour transformer la flotte Tesla en v√©hicules robotis√©s. <br><br>  La disponibilit√© de capacit√©s informatiques optimis√©es pour l'IA a g√©n√©r√© une demande de donn√©es n√©cessaires pour former des SN de plus en plus complexes.  Cette dynamique est plus √©vidente dans le secteur de la robotique, o√π les entreprises collectent des donn√©es sur des millions de kilom√®tres de routes r√©elles.  Tesla peut collecter ces donn√©es automatiquement √† partir des voitures des utilisateurs, et ses concurrents Waymo et Cruise ont pay√© les conducteurs qui conduisaient leur voiture sur la voie publique. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2eb/80a/3be/2eb80a3be8fd038777829faff752e8bf.jpg"><br><br>  La demande de donn√©es donne un avantage aux grandes entreprises en ligne qui ont d√©j√† acc√®s √† de grands volumes de donn√©es utilisateur. <br><br>  Le deep learning a conquis tant de domaines diff√©rents en raison de son extr√™me flexibilit√©.  Des d√©cennies d'essais et d'erreurs ont permis aux chercheurs de d√©velopper les blocs de construction de base pour les t√¢ches les plus courantes dans le domaine de la MO - comme les r√©seaux de convolution pour une reconnaissance d'image efficace.  Cependant, si vous avez un r√©seau de haut niveau adapt√© au programme et suffisamment de donn√©es, le processus de formation sera simple.  Les NS profonds sont capables de reconna√Ætre une gamme exceptionnellement large de mod√®les complexes sans conseils particuliers de d√©veloppeurs humains. <br><br>  Il y a bien s√ªr des limites.  Par exemple, certaines personnes se sont lanc√©es dans l'id√©e de former des robots √† l'aide de GO seul - c'est-√†-dire nourrir des images re√ßues d'une cam√©ra, d'un r√©seau de neurones et recevoir des instructions de celui-ci pour tourner le volant et appuyer sur les p√©dales.  Je suis sceptique quant √† cette approche.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">L'Assembl√©e nationale n'a pas encore d√©montr√© sa capacit√© √† mener un raisonnement logique complexe, qui est n√©cessaire pour comprendre certaines conditions qui se pr√©sentent sur la route. </font><font style="vertical-align: inherit;">De plus, les NS sont des ¬´bo√Ætes noires¬ª dont le flux de travail est pratiquement invisible. </font><font style="vertical-align: inherit;">Il serait difficile d'√©valuer et de confirmer la s√©curit√© d'un tel syst√®me. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cependant, GO a permis de faire de tr√®s grands sauts dans une gamme d'applications √©tonnamment large. </font><font style="vertical-align: inherit;">Dans les ann√©es √† venir, on peut s'attendre aux prochains progr√®s dans ce domaine.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr482258/">https://habr.com/ru/post/fr482258/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr482246/index.html">R√©ponses correctes et annonce du gagnant</a></li>
<li><a href="../fr482250/index.html">Une machine d'√©tat simple pour VueJS</a></li>
<li><a href="../fr482252/index.html">Toilette automatique pour chats - suite</a></li>
<li><a href="../fr482254/index.html">Exp√©rience VonmoTrade. Partie 3: Livre des mandats. Traitement et stockage des informations commerciales</a></li>
<li><a href="../fr482256/index.html">L'IA et l'avenir du travail: perspectives d'emploi dans un avenir proche</a></li>
<li><a href="../fr482260/index.html">TelegramBot. La fonctionnalit√© de base. Autocollants et √©motic√¥nes. (Partie 3)</a></li>
<li><a href="../fr482262/index.html">Comment se connecter √† Talend Open Studio</a></li>
<li><a href="../fr482264/index.html">Br√©sil, magie noire, Mortal Kombat, Mars et 15 000 personnes. R√©sultats de l'ann√©e Ontiko</a></li>
<li><a href="../fr482268/index.html">M√©gastructures du futur: la sph√®re Dyson, le moteur stellaire et la ¬´bombe du trou noir¬ª</a></li>
<li><a href="../fr482272/index.html">Choisir un entrep√¥t de donn√©es pour Prometheus: Thanos vs VictoriaMetrics</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>