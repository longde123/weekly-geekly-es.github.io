<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§òüèª üîò üëÇüèº CephFS vs GlusterFS ‚õèÔ∏è üëãüèº üíΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Como ingeniero de infraestructura en el equipo de desarrollo de la plataforma en la nube , tuve la oportunidad de trabajar con muchos sistemas de alma...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>CephFS vs GlusterFS</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/croccloudteam/blog/430474/"><p>  Como ingeniero de infraestructura en el equipo de desarrollo de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">plataforma</a> en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">nube</a> , tuve la oportunidad de trabajar con muchos sistemas de almacenamiento distribuido, incluidos los que se indican en el encabezado.  Parece que hay una comprensi√≥n de sus fortalezas y debilidades, y tratar√© de compartir mis pensamientos con usted sobre este tema.  Por as√≠ decirlo, veamos qui√©n tiene m√°s tiempo la funci√≥n hash. </p><br><p><img src="https://habrastorage.org/webt/fb/qn/vj/fbqnvjedf1ujf2hknyuxqbv82qy.png"></p><a name="habracut"></a><br><p>  Descargo de responsabilidad: anteriormente en este blog pod√≠as ver art√≠culos sobre GlusterFS.  No tengo nada que ver con estos art√≠culos.  Este es el blog del autor del equipo de proyecto de nuestra nube y cada uno de sus miembros puede contar su historia.  El autor de esos art√≠culos es ingeniero de nuestro grupo operativo y tiene sus propias tareas y su experiencia, que comparti√≥.  Tenga esto en cuenta si de repente ve una diferencia de opini√≥n.  ¬°Aprovecho esta oportunidad para transmitir mis saludos al autor de esos art√≠culos! </p><br><h2 id="o-chem-poydet-rech">  Lo que se discutir√° </h2><br><p> Hablemos de los sistemas de archivos que se pueden construir sobre la base de GlusterFS y CephFS.  Discutiremos la arquitectura de estos dos sistemas, los veremos desde diferentes √°ngulos y al final incluso me arriesgar√© a sacar conclusiones.  Otras caracter√≠sticas de Ceph, como RBD y RGW, no se ver√°n afectadas. </p><br><h2 id="terminologiya">  Terminolog√≠a </h2><br><p>  Para que el art√≠culo sea completo y comprensible para todos, veamos la terminolog√≠a b√°sica de ambos sistemas: </p><br><p>  Terminolog√≠a cef√°lica: </p><br><p>  <strong>RADOS</strong> (Reliable Autonomic Distributed Object Store) es un almacenamiento de objetos aut√≥nomo, que es la base del proyecto Ceph. <br>  <strong>CephFS</strong> , <strong>RBD</strong> (RADOS Block Device), <strong>RGW</strong> (RADOS Gateway): gadgets de alto nivel para RADOS, que proporcionan a los usuarios finales varias interfaces para RADOS. <br>  Espec√≠ficamente, CephFS proporciona una interfaz de sistema de archivos compatible con POSIX.  De hecho, los datos de CephFS se almacenan en RADOS. <br>  <strong>OSD</strong> (Object Storage Daemon) es un proceso que sirve un almacenamiento separado de disco / objeto en un cl√∫ster RADOS. <br>  <strong>RADOS Pool</strong> (pool): varios <strong>OSD</strong> unidos por un conjunto com√∫n de reglas, como, por ejemplo, una pol√≠tica de replicaci√≥n.  Desde el punto de vista de la jerarqu√≠a de datos, un grupo es un directorio o un espacio de nombres separado (plano, sin subdirectorios) para objetos. <br>  <strong>PG</strong> (Placement Group): presentar√© el concepto de PG un poco m√°s tarde, en el contexto, para una mejor comprensi√≥n. </p><br><p>  Dado que RADOS es la base sobre la cual se construye CephFS, a menudo hablar√© sobre esto y esto se aplicar√° autom√°ticamente a CephFS. </p><br><p>  Terminolog√≠a de GlusterFS (en adelante gl): </p><br><p>  <strong>Brick</strong> es un proceso que sirve un solo disco, un an√°logo de OSD en la terminolog√≠a RADOS. <br>  <strong>Volumen</strong> : volumen en el que se unen los ladrillos.  Tom es un an√°logo de grupo en RADOS, tambi√©n tiene una topolog√≠a de replicaci√≥n espec√≠fica entre ladrillos. </p><br><h2 id="raspredelenie-dannyh">  Distribuci√≥n de datos </h2><br><p>  Para hacerlo m√°s claro, considere un ejemplo simple que puede ser implementado por ambos sistemas. </p><br><p>  La configuraci√≥n que se utilizar√° como ejemplo: </p><br><ul><li>  2 servidores (S1, S2) con 3 discos de igual volumen (sda, sdb, sdc) en cada uno; </li><li>  volumen / grupo con replicaci√≥n 2. </li></ul><br><p>  Ambos sistemas necesitan al menos 3 servidores para su funcionamiento normal.  Pero hacemos la vista gorda a esto, ya que este es solo un ejemplo para un art√≠culo. </p><br><p>  En el caso de gl, este ser√° un volumen <strong>distribuido-replicado que</strong> consta de 3 grupos de replicaci√≥n: </p><br><p><img src="https://habrastorage.org/webt/ai/k_/pg/aik_pgd6mwqy1wlfyhjx-mdsf6u.png"></p><br><p>  Cada grupo de replicaci√≥n es dos ladrillos en diferentes servidores. <br>  De hecho, resulta el volumen que combina los tres RAID-1. <br>  Cuando lo monte, obtenga el sistema de archivos deseado y comience a escribir archivos en √©l, encontrar√° que cada archivo que escribe pertenece a uno de estos grupos de replicaci√≥n como un todo. <br>  La distribuci√≥n de archivos entre estos grupos distribuidos es manejada por <strong>DHT</strong> (Distributed Hash Tables), que es esencialmente una funci√≥n hash (volveremos a ello m√°s adelante). </p><br><p>  En el "diagrama" se ver√° as√≠: </p><br><p><img src="https://habrastorage.org/webt/d1/-8/uk/d1-8ukcptl3owiyuqw11v0plxqw.png"></p><br><p>  Como si las primeras caracter√≠sticas arquitect√≥nicas ya se hubieran manifestado: </p><br><ul><li>  el lugar en grupos se desecha de manera desigual, depende del tama√±o de los archivos; </li><li>  al escribir un archivo, IO va solo a un grupo, el resto est√° inactivo; </li><li>  No puede obtener el IO de todo el volumen al escribir un solo archivo; </li><li>  Si no hay suficiente espacio en el grupo para escribir el archivo, recibir√° un error, el archivo no se escribir√° y no se redistribuir√° a otro grupo. </li></ul><br><p>  Si usa otros tipos de vol√∫menes, por ejemplo, Distributed-Striped-Replicated o incluso Dispersed (Erasure Coding), solo la mec√°nica de la distribuci√≥n de datos dentro de un grupo cambiar√° fundamentalmente.  DHT tambi√©n descompondr√° los archivos completamente en estos grupos, y al final tendremos los mismos problemas.  S√≠, si el volumen consistir√° en un solo grupo, o si tiene todos los archivos de aproximadamente el mismo tama√±o, entonces no habr√° ning√∫n problema.  Pero estamos hablando de sistemas normales, bajo cientos de terabytes de datos, incluidos archivos de diferentes tama√±os, por lo que creemos que hay un problema. </p><br><p>  Ahora echemos un vistazo a CephFS.  Los RADOS mencionados anteriormente entran en escena.  En RADOS, cada disco es servido por un proceso separado: OSD.  Seg√∫n nuestra configuraci√≥n, solo obtenemos 6 de ellos, 3 en cada servidor.  A continuaci√≥n, necesitamos crear un grupo para los datos y establecer el n√∫mero de PG y el factor de replicaci√≥n de datos en este grupo, en nuestro caso 2. <br>  Digamos que creamos un grupo con 8 PG.  Estas PG se distribuir√°n de manera m√°s o menos uniforme en todo el OSD: </p><br><p><img src="https://habrastorage.org/webt/cn/ea/hs/cneahsczaws7dzuqtu1syubb4cy.png"></p><br><p>  Es hora de aclarar que PG es un grupo l√≥gico que combina varios objetos.  Dado que establecemos el hecho de replicaci√≥n 2, cada PG tiene una r√©plica en alg√∫n otro OSD en otro servidor (por defecto).  Por ejemplo, PG1, que est√° en OSD-1 en el servidor S1, tiene un gemelo en S2 en OSD-6.  En cada par de PG (o triple, si la replicaci√≥n 3) es PRIMARY PG, que se est√° registrando.  Por ejemplo, PRIMARY para PG4 est√° en S1, pero PRIMARY para PG3 est√° en S2. </p><br><p>  Ahora que sabe c√≥mo funciona RADOS, podemos pasar a escribir archivos en nuestro nuevo grupo.  Aunque RADOS es un almacenamiento completo, no es posible montarlo como un sistema de archivos o usarlo como un dispositivo de bloque.  Para escribir datos directamente en √©l, debe usar una utilidad o biblioteca especial. </p><br><p>  Escribimos los mismos tres archivos que en el ejemplo anterior: </p><br><p><img src="https://habrastorage.org/webt/ut/0z/zd/ut0zzd20fmocwke70q9rj-0snog.png"></p><br><p>  En el caso de RADOS, todo se ha vuelto algo m√°s complicado, de acuerdo. </p><br><p>  Entonces CRUSH (Replicaci√≥n controlada bajo hash escalable) apareci√≥ en la cadena.  CRUSH es el algoritmo en el que se apoya RADOS (volveremos a ello m√°s adelante).  En este caso particular, usando este algoritmo, se determina d√≥nde se debe escribir el archivo en qu√© PG.  Aqu√≠ CRUSH realiza la misma funci√≥n que DHT en gl.  Como resultado de esta distribuci√≥n pseudoaleatoria de archivos en PG, obtuvimos los mismos problemas que gl, solo en un esquema m√°s complejo. </p><br><p>  Pero deliberadamente guard√© silencio sobre un punto importante.  Casi nadie usa RADOS en su forma pura.  Para un trabajo conveniente con RADOS, se desarrollaron las siguientes capas: RBD, CephFS, RGW, que ya mencion√©. </p><br><p>  Todos estos traductores (clientes RADOS) proporcionan una interfaz de cliente diferente, pero son similares en su trabajo con RADOS.  La similitud m√°s importante es que todos los datos que pasan a trav√©s de ellos se cortan en pedazos y se colocan en RADOS como objetos RADOS separados.  Por defecto, los clientes oficiales cortan el flujo de entrada en 4 MB.  Para RBD, el tama√±o de la franja se puede establecer al crear el volumen.  En el caso de CephFS, este es el atributo (xattr) del archivo y se puede administrar a nivel de archivos individuales o para todos los archivos de cat√°logo.  Bueno, RGW tambi√©n tiene un par√°metro correspondiente. </p><br><p>  Ahora supongamos que apilamos CephFS sobre el grupo RADOS que se present√≥ en el ejemplo anterior.  Ahora los sistemas en cuesti√≥n est√°n completamente en igualdad de condiciones y proporcionan una interfaz de acceso a archivos id√©ntica. </p><br><p>  Si volvemos a escribir nuestros archivos de prueba en el nuevo CephFS, encontraremos una distribuci√≥n de datos completamente diferente y casi uniforme en el OSD.  Por ejemplo, el archivo 2 de 2 GB de tama√±o se dividir√° en 512 piezas, que se distribuir√°n en diferentes PG y, como resultado, en diferentes OSD de manera casi uniforme, y esto pr√°cticamente resuelve los problemas con la distribuci√≥n de datos descritos anteriormente. </p><br><p>  En nuestro ejemplo, solo se utilizan 8 PG, aunque se recomienda tener ~ 100 PG en un OSD.  Y necesita 2 grupos para que CephFS funcione. Tambi√©n necesita algunos demonios de servicio para que RADOS funcione en principio.  No pienses que todo es tan simple, espec√≠ficamente omito mucho, para no apartarme de la esencia. </p><br><p>  Entonces ahora CephFS parece m√°s interesante, ¬øverdad?  Pero no mencion√© otro punto importante, esta vez sobre gl.  Gl tambi√©n tiene un mecanismo para cortar archivos en trozos y ejecutar esos trozos a trav√©s de DHT.  El llamado sharding ( <strong>Sharding</strong> ). </p><br><p>  Historia de cinco minutos </p><br><blockquote>  El 21 de abril de 2016, el equipo de desarrollo de Ceph lanz√≥ "Jewel", la primera versi√≥n de Ceph en la que CephFS se considera estable. </blockquote><p>  ¬°Esto es todo un grito a la izquierda y a la derecha sobre CephFS!  Y hace 3-4 a√±os usarlo ser√≠a al menos una decisi√≥n dudosa.  Buscamos otras soluciones, y gl con la arquitectura descrita anteriormente no era bueno.  Pero cre√≠amos en √©l m√°s que en CephFS, y esperamos la fragmentaci√≥n, que se estaba preparando para el lanzamiento. </p><br><p>  Y aqu√≠ est√° el d√≠a X: </p><br><blockquote>  4 de junio de 2015: la Comunidad Gluster anunci√≥ hoy la disponibilidad general del software de almacenamiento definido por software abierto GlusterFS 3.7. </blockquote><p>  3.7 - la primera versi√≥n de gl, en la cual el sharding se anunci√≥ como una oportunidad experimental.  Ten√≠an casi un a√±o antes del lanzamiento estable de CephFS para poder establecerse en el podio ... </p><br><p>  Entonces fragmentar significa.  Como todo en gl, esto se implementa en un traductor separado, que estaba por encima del DHT (tambi√©n traductor) en la pila.  Como es m√°s alto que DHT, DHT recibe fragmentos listos para usar en la entrada y los distribuye entre los grupos de replicaci√≥n como archivos normales.  Sharding est√° habilitado en el nivel de volumen individual.  El tama√±o del fragmento se puede configurar, de forma predeterminada, 4 MB, como lociones Ceph. </p><br><p>  Cuando realic√© las primeras pruebas, ¬°estaba encantado!  ¬°Les dije a todos que gl ahora es lo m√°s importante y ahora viviremos!  Con el fragmentado habilitado, la grabaci√≥n de un archivo va en paralelo a diferentes grupos de replicaci√≥n.  La descompresi√≥n despu√©s de la compresi√≥n "al escribir" puede ser incremental al nivel de fragmento.  En presencia de disparos en cach√© aqu√≠, tambi√©n, todo se vuelve bueno y fragmentos separados se mueven al cach√©, y no a los archivos completos.  En general, me alegr√©, porque  Parec√≠a que hab√≠a conseguido un instrumento genial en sus manos. </p><br><p>  Quedaba por esperar las primeras correcciones de errores y el estado de "listo para la producci√≥n".  Pero todo result√≥ no tan color de rosa ... Para no estirar el art√≠culo con una lista de errores cr√≠ticos relacionados con el fragmentaci√≥n, que de vez en cuando surgen en las pr√≥ximas versiones, solo puedo decir que el √∫ltimo "problema importante" con la siguiente descripci√≥n: </p><br><blockquote>  La expansi√≥n de un volumen de reflejo fragmentado puede provocar da√±os en el archivo.  Los vol√∫menes fragmentados generalmente se usan para im√°genes de VM, si tales vol√∫menes se expanden o posiblemente se contraen (es decir, agregar / eliminar ladrillos y reequilibrar) hay informes de im√°genes de VM que se corrompen. </blockquote><p>  se cerr√≥ en la versi√≥n 3.13.2, 20 de enero de 2018 ... ¬øtal vez esta no sea la √∫ltima? </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Comentario</a> sobre uno de nuestros art√≠culos sobre esto, por as√≠ decirlo, de primera mano. </p><br><p>  RedHat en su documentaci√≥n para el actual RedHat Gluster Storage 3.4 se√±ala que el √∫nico caso de fragmentaci√≥n que admiten es el almacenamiento para discos VM. </p><br><blockquote>  Sharding tiene un caso de uso compatible: en el contexto de proporcionar Red Hat Gluster Storage como un dominio de almacenamiento para Red Hat Enterprise Virtualization, para proporcionar almacenamiento para im√°genes de m√°quinas virtuales en vivo.  Tenga en cuenta que el particionamiento tambi√©n es un requisito para este caso de uso, ya que proporciona mejoras significativas de rendimiento con respecto a implementaciones anteriores. </blockquote><p>  No s√© por qu√© tal restricci√≥n, pero debes admitir que es alarmante. </p><br><h2 id="seychas-ya-tebe-tut-vse-zaheshiruyu">  Ahora tengo todo aqu√≠ para ti </h2><br><p>  Ambos sistemas utilizan una funci√≥n hash para distribuir datos de forma pseudoaleatoria entre los discos. </p><br><p>  Para RADOS, se ve m√°s o menos as√≠: </p><br><pre><code class="plaintext hljs">PG = pool_id + "." + jenkins_hash(object_name) % pg_coun # eg pool id=5 =&gt; pg = 5.1f OSD = crush_hash_based_on_jenkins(PG) # eg pg=5.1f =&gt; OSD = 12</code> </pre> <br><p>  Gl usa el llamado <strong>hashing consistente</strong> .  Cada ladrillo obtiene un "rango dentro de un espacio hash de 32 bits".  Es decir, todos los ladrillos comparten el espacio hash de direcci√≥n lineal completo sin rangos ni agujeros de intersecci√≥n.  El cliente ejecuta el nombre del archivo a trav√©s de la funci√≥n hash, y luego determina en qu√© rango de hash cae el hash recibido.  As√≠ se selecciona ladrillo.  Si hay varios ladrillos en el grupo de replicaci√≥n, entonces todos tienen el mismo rango de hash.  Algo como esto: </p><br><p><img src="https://habrastorage.org/webt/o_/y5/ye/o_y5yeby9vn5enx7r-5xa3zfwuq.png"></p><br><p>  Si llevamos el trabajo de dos sistemas a una cierta forma l√≥gica, resultar√° algo como esto: </p><br><pre> <code class="plaintext hljs">file -&gt; HASH -&gt; placement_unit</code> </pre> <br><p>  donde ubicaci√≥n_unidad en el caso de RADOS es PG, y en el caso de gl es un grupo de replicaci√≥n de varios ladrillos. </p><br><p>  Entonces, una funci√≥n hash, luego esta distribuye, distribuye archivos, y de repente resulta que una ubicaci√≥n_unidad se utiliza m√°s que la otra.  Tal es la caracter√≠stica fundamental de los sistemas de distribuci√≥n hash.  Y nos enfrentamos a una tarea muy com√∫n: desequilibrar los datos. </p><br><p>  Gl es capaz de reconstruir, pero debido a la arquitectura con los rangos de hash descritos anteriormente, puede ejecutar la reconstrucci√≥n tanto como desee, pero ning√∫n rango de hash (y, como resultado, los datos) no se mover√°n.  El √∫nico criterio para redistribuir los rangos de hash es un cambio en la capacidad de volumen.  Y le queda una opci√≥n: agregar ladrillos.  Y si estamos hablando de un volumen con replicaci√≥n, entonces debemos agregar un grupo de replicaci√≥n completo, es decir, dos nuevos ladrillos en nuestra configuraci√≥n.  Despu√©s de expandir el volumen, puede comenzar a reconstruir: los rangos de hash se redistribuir√°n teniendo en cuenta el nuevo grupo y se distribuir√°n los datos.  Cuando se elimina un grupo de replicaci√≥n, los rangos de hash se asignan autom√°ticamente. </p><br><p>  RADOS tiene todo un coche de posibilidades.  En un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo de</a> Ceph, me quej√© mucho sobre el concepto de PG, pero aqu√≠, en comparaci√≥n con gl, por supuesto, RADOS a caballo.  Cada OSD tiene su propio peso, generalmente se establece en funci√≥n del tama√±o del disco.  A su vez, los OSD distribuyen los PG seg√∫n el peso de este √∫ltimo.  Todo, luego solo cambiamos el peso del OSD hacia arriba o hacia abajo y el PG (junto con los datos) comienza a moverse a otros OSD.  Adem√°s, cada OSD tiene un peso de ajuste adicional, que le permite equilibrar los datos entre los discos de un servidor.  Todo esto es inherente a CRUSH.  El beneficio principal es que no es necesario expandir la capacidad del grupo para desequilibrar mejor los datos.  Y no es necesario agregar discos en grupos, solo puede agregar un OSD y una parte de PG se transferir√° a √©l. </p><br><p>  S√≠, es posible que al crear un grupo no crearan suficiente PG y result√≥ que cada uno de los PG es bastante grande en volumen, y donde quiera que se muevan, el desequilibrio permanecer√°.  En este caso, puede aumentar el n√∫mero de PG, y se dividen en m√°s peque√±os.  S√≠, si el cl√∫ster est√° lleno de datos, entonces duele, pero lo principal en nuestra comparaci√≥n es que existe esa oportunidad.  Ahora solo se permite un aumento en el n√∫mero de PG y con esto debe ser m√°s cuidadoso, pero en la pr√≥xima versi√≥n de Ceph - Nautilus habr√° soporte para reducir el n√∫mero de PG (fusi√≥n de p√°ginas). </p><br><h2 id="replikaciya-dannyh">  Replicaci√≥n de datos </h2><br><p>  Nuestros grupos y vol√∫menes de prueba tienen un factor de replicaci√≥n de 2. Curiosamente, los sistemas en cuesti√≥n utilizan diferentes enfoques para lograr este n√∫mero de r√©plicas. </p><br><p>  En el caso de RADOS, el esquema de grabaci√≥n se parece a esto: </p><br><p><img src="https://habrastorage.org/webt/lx/vb/q-/lxvbq-niuingzw76ad7aqanp2pg.png"></p><br><p>  El cliente conoce la topolog√≠a de todo el cl√∫ster, usa CRUSH (paso 0) para seleccionar un PG espec√≠fico para escritura, escribe en PRIMARY PG en OSD-0 (paso 1), luego OSD-0 replica sincr√≥nicamente los datos en SECUNDARY PG (paso 2), y solo despu√©s paso 2 exitoso / no exitoso, OSD confirma / no confirma la operaci√≥n al cliente (paso 3).  La replicaci√≥n de datos entre dos OSD es transparente para el cliente.  Los OSD generalmente pueden usar un "cl√∫ster" separado, una red m√°s r√°pida para la replicaci√≥n de datos. </p><br><p>  Si se configura la replicaci√≥n triple, entonces tambi√©n se ejecuta sincr√≥nicamente con OSD PRIMARIO en dos SECUNDARIOS, transparente para el cliente ... bueno, solo que la letan√≠a es mayor. </p><br><p>  Gl funciona de manera diferente: </p><br><p><img src="https://habrastorage.org/webt/ll/zz/q-/llzzq-m2jhfaf83-dtk_fokavw0.png"></p><br><p>  El cliente conoce la topolog√≠a del volumen, usa DHT (paso 0) para determinar el ladrillo deseado, luego escribe en √©l (paso 1).  Todo es simple y claro.  Pero aqu√≠ recordamos que todos los ladrillos en el grupo de replicaci√≥n tienen el mismo rango de hash.  Y esta caracter√≠stica menor hace que todas las vacaciones.  El cliente escribe en paralelo a todos los ladrillos que tienen un rango hash adecuado. </p><br><p>  En nuestro caso, con doble replicaci√≥n, el cliente realiza una grabaci√≥n dual en paralelo en dos ladrillos diferentes.  Durante la replicaci√≥n triple, se realizar√° una grabaci√≥n triple, respectivamente, y 1 MB de datos se convertir√° aproximadamente en 3 MB de tr√°fico de red desde el cliente al lado de los servidores gl.  De acuerdo, los conceptos de sistemas son perpendiculares. </p><br><p>  En tal esquema, se asigna m√°s trabajo al cliente gl y, como resultado, necesita m√°s CPU, bueno, ya dije sobre la red. </p><br><p>  La replicaci√≥n la realiza el traductor AFP (Replicaci√≥n autom√°tica de archivos): un xlator del lado del cliente que realiza la replicaci√≥n sincr√≥nica.  Las r√©plicas escriben en todos los ladrillos de la r√©plica ‚Üí Utiliza un modelo de transacci√≥n. </p><br><p>  Si es necesario, sincronice las r√©plicas en el grupo (curaci√≥n), por ejemplo, despu√©s de una indisponibilidad temporal de un ladrillo, los demonios gl lo hacen solos utilizando el AFP incorporado, transparente para los clientes y sin su participaci√≥n. </p><br><p>  Es interesante que si no trabaja a trav√©s del cliente gl nativo, sino que escribe a trav√©s del servidor NFS incorporado en gl, obtendremos el mismo comportamiento que RADOS.  En este caso, AFP se utilizar√° en demonios gl para replicar datos sin intervenci√≥n del cliente.  Pero el NFS incorporado est√° asegurado en gl v4, y si desea este comportamiento, se recomienda usar NFS-Ganesha. </p><br><p>  Por cierto, debido a un comportamiento tan diferente cuando se usa NFS y el cliente nativo, puede ver indicadores de rendimiento completamente diferentes. </p><br><h2 id="a-u-vas-est-takoy-zhe-klaster-tolko-na-kolenke">  ¬øTienes el mismo grupo, solo "en la rodilla"? </h2><br><p>  A menudo veo en Internet discusiones sobre todo tipo de configuraciones de r√≥tula, donde se construye un cl√∫ster de datos a partir de lo que est√° a la mano.  En este caso, una soluci√≥n basada en RADOS puede darle m√°s libertad al elegir sus unidades.  En RADOS, puede agregar unidades de casi cualquier tama√±o.  Cada disco tendr√° un peso correspondiente a su tama√±o (generalmente), y los datos se distribuir√°n entre los discos casi proporcionalmente a su peso.  En el caso de gl, no existe el concepto de "discos separados" en vol√∫menes con replicaci√≥n.  Los discos se agregan en pares en doble replicaci√≥n o se triplican en triple.  Si hay discos de diferentes tama√±os en un grupo de replicaci√≥n, se encontrar√° con un lugar en el disco m√°s peque√±o del grupo y desplegar√° la capacidad de los discos grandes.  En dicho esquema, gl supondr√° que la capacidad de un grupo de replicaci√≥n es igual a la capacidad del disco m√°s peque√±o del grupo, lo cual es l√≥gico.  Al mismo tiempo, se permite tener grupos de replicaci√≥n que consisten en discos de diferentes tama√±os, grupos de diferentes tama√±os.  Los grupos m√°s grandes pueden recibir un rango de hash m√°s grande en relaci√≥n con otros grupos y, como resultado, recibir m√°s datos. </p><br><p>  Hemos estado viviendo con Ceph por quinto a√±o.  Comenzamos con discos del mismo volumen, ahora presentamos otros m√°s espaciosos.  Con Ceph, puede quitar el disco y reemplazarlo por otro m√°s grande o un poco m√°s peque√±o sin ninguna dificultad arquitect√≥nica.  Con gl, todo es m√°s complicado: sac√≥ un disco de 2 TB, coloque el mismo, por favor.  Bueno, o retirar todo el grupo en su conjunto, lo que no es muy bueno, de acuerdo. </p><br><h2 id="obrabotka-otkazov">  Conmutaci√≥n por error </h2><br><p>  Ya nos familiarizamos un poco con la arquitectura de las dos soluciones y ahora podemos hablar sobre c√≥mo vivir con ella y cu√°les son las caracter√≠sticas al realizar el mantenimiento. </p><br><p>  Supongamos que rechazamos sda en s1, algo com√∫n. </p><br><p>  En el caso de gl: </p><br><ul><li>  una copia de los datos en el disco en vivo que queda en el grupo no se redistribuye autom√°ticamente a otros grupos; </li><li>  hasta que se reemplace el disco, solo queda una copia de los datos; </li><li>  Cuando se reemplaza un disco fallido por uno nuevo, la replicaci√≥n se realiza de un disco en funcionamiento a uno nuevo (1 en 1). </li></ul><br><p>  Esto es como servir un estante con m√∫ltiples RAID-1.  S√≠, con la triple replicaci√≥n, si falla una unidad, no queda una copia, sino dos, pero este enfoque a√∫n tiene serios inconvenientes, y los mostrar√© con un buen ejemplo con RADOS. </p><br><p>  Supongamos que fallamos sda en S1 (OSD-0), algo com√∫n: </p><br><ul><li>  Los PG que estaban en OSD-0 se reasignar√°n autom√°ticamente a otros OSD despu√©s de 10 minutos (predeterminado).  En nuestro ejemplo, en OSD 1 y 2. Si hab√≠a m√°s servidores, entonces en un mayor n√∫mero de OSD. </li><li>  Los PG que almacenan la segunda copia sobreviviente de los datos los replicar√°n autom√°ticamente en aquellos OSD donde se transfieren los PG restaurados.  Resulta una replicaci√≥n de muchos a muchos, no una replicaci√≥n uno a uno como gl. </li><li>  Cuando se introduce un nuevo disco, en lugar de uno roto, se acumular√°n algunas PG de acuerdo con su peso en el nuevo OSD y se redistribuir√°n los datos de otros OSD. </li></ul><br><p>  Creo que no tiene sentido explicar las ventajas arquitect√≥nicas de RADOS.  No puede contraerse cuando recibe una carta que dice que la unidad fall√≥.  Y cuando venga a trabajar por la ma√±ana, descubra que todas las copias que faltan ya se han restaurado en docenas de otros OSD o en el proceso.  En cl√∫steres grandes, donde cientos de PG se distribuyen en un mont√≥n de discos, la recuperaci√≥n de datos de un OSD puede tener lugar a velocidades mucho m√°s altas que la velocidad de un disco debido al hecho de que est√°n involucradas docenas de OSD (lectura y escritura).  Bueno, tampoco debes olvidarte del equilibrio de carga. </p><br><h2 id="masshtabirovanie">  Escalamiento </h2><br><p>  En este contexto, probablemente le dar√© al pedestal gl.  En un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo</a> sobre Ceph, ya escrib√≠ sobre algunas de las complejidades de la escala RADOS asociadas con el concepto PG.  Si el aumento de PG con el crecimiento del grupo a√∫n se puede experimentar, entonces, ¬øqu√© pasa con Ceph MDS no est√° claro?  CephFS se ejecuta sobre RADOS y utiliza un grupo separado para metadatos y un proceso especial, el servidor de metadatos seph (MDS), para atender los metadatos del sistema de archivos y coordinar todas las operaciones con el FS.  No digo que tener MDS ponga fin a la escalabilidad de CephFS, no, especialmente porque puede ejecutar m√∫ltiples MDS en modo activo-activo.  Solo quiero se√±alar que gl est√° arquitect√≥nicamente desprovisto de todo esto.  No tiene contraparte PG, nada como MDS.  Gl realmente escala perfectamente simplemente agregando grupos de replicaci√≥n, casi linealmente. </p><br><p>  En los d√≠as anteriores a CephFS, dise√±amos la soluci√≥n para los petabytes de datos y analizamos gl.  Luego tuvimos dudas sobre la escalabilidad de gl y lo descubrimos a trav√©s de la lista de correo.  Aqu√≠ est√° una de las respuestas (P: mi pregunta): </p><br><blockquote>  Estoy usando 60 servidores, cada uno tiene discos de 26x8TB, un total de 1560 discos, 16 + 4 de volumen EC con 9PB de espacio utilizable. <br><br>  P: ¬øUtiliza libgfapi o FUSE o NFS en el lado del cliente? <br><br>  Uso FUSE y tengo casi 1000 clientes. <br><br>  P: ¬øCu√°ntos archivos tienes en tu volumen? <br>  P: ¬øLos archivos son m√°s grandes o m√°s peque√±os? <br><br>  Tengo m√°s de 1 mill√≥n de archivos y se utiliza% 13 del cl√∫ster, lo que hace que el tama√±o promedio de los archivos sea de 1 GB. <br>  El tama√±o m√≠nimo / m√°ximo del archivo es de 100 MB / 2 GB.  Todos los d√≠as entran en el volumen 10-20 TB de datos nuevos. <br><br>  P: ¬øQu√© tan r√°pido funciona "ls"? <br><br>  Las operaciones de metadatos son lentas como esperas.  Intento no poner m√°s de 2-3K archivos en un directorio.  Mi caso de uso es para copia de seguridad / archivo, por lo que rara vez hago operaciones de metadatos. </blockquote><br><h2 id="pereimenovanie-faylov">  Renombrar archivos </h2><br><p>  Volver a las funciones hash nuevamente.  Descubrimos c√≥mo se enrutan archivos espec√≠ficos a discos espec√≠ficos, y ahora la pregunta se vuelve relevante, pero ¬øqu√© suceder√° al cambiar el nombre de los archivos? </p><br><p>  Despu√©s de todo, si cambiamos el nombre del archivo, entonces el hash en su nombre tambi√©n cambiar√°, lo que significa el lugar de este archivo en otro disco (en un rango de hash diferente) o en otro PG / OSD en el caso de RADOS.  S√≠, pensamos correctamente, y aqu√≠ en dos sistemas todo vuelve a ser perpendicular. </p><br><p>  En el caso de gl, al cambiar el nombre de un archivo, el nuevo nombre se ejecuta a trav√©s de una funci√≥n hash, se define un nuevo bloque y se crea un enlace especial al antiguo bloque, donde el archivo permanece como antes.  Topovka, ¬øverdad?  Para que los datos realmente se muevan a un nuevo lugar, y el cliente no hizo clic en el enlace innecesariamente, debe hacer un nuevo equilibrio. </p><br><p>  Pero RADOS generalmente no tiene un m√©todo para renombrar objetos solo por la necesidad de su movimiento posterior.  Se propone utilizar una copia justa para renombrar, lo que conduce a un movimiento sincr√≥nico del objeto.  Y CephFS, que se ejecuta sobre RADOS, tiene una carta de triunfo bajo la manga en forma de un grupo con metadatos y MDS.  Cambiar el nombre del archivo no afecta el contenido del archivo en el grupo de datos. </p><br><h2 id="replikaciya-25">  Replicaci√≥n 2.5 </h2><br><p>  Gl tiene una caracter√≠stica muy interesante que me gustar√≠a mencionar por separado.  Todos entienden que la replicaci√≥n 2 no es una configuraci√≥n confiable, pero sin embargo se realiza peri√≥dicamente para estar bastante justificada.  Para protegerse contra el cerebro dividido en tales esquemas y para garantizar la coherencia de los datos, gl le permite crear vol√∫menes con la r√©plica 2 y un √°rbitro adicional.  El √°rbitro es aplicable para la replicaci√≥n de 3 o m√°s.  Este es el mismo bloque en el grupo que los otros dos, solo que en realidad solo crea una estructura de archivos a partir de archivos y directorios.  Los archivos en un ladrillo de este tipo son de tama√±o cero, pero sus atributos extendidos del sistema de archivos (atributos extendidos) se mantienen sincronizados con archivos de tama√±o completo en la misma r√©plica.  Creo que la idea es clara.  Creo que esta es una buena oportunidad. </p><br><p>  El √∫nico momento ... el tama√±o del lugar en el grupo de replicaci√≥n est√° determinado por el tama√±o del ladrillo m√°s peque√±o, y esto significa que el √°rbitro debe deslizar un disco al menos del mismo tama√±o que el resto del grupo.  Para hacer esto, se recomienda crear ficticios LV delgados (delgados), tama√±os grandes, para no usar un disco real. </p><br><h2 id="a-che-po-klientam">  ¬øY qu√© hay de los clientes? </h2><br><p>  La API nativa de los dos sistemas se implementa en forma de bibliotecas libgfapi (gl) y libcephfs (CephFS).  Los enlaces para idiomas populares tambi√©n est√°n disponibles.  En general, con las bibliotecas, todo es igual de bueno.  El ubicuo NFS-Ganesha es compatible con ambas bibliotecas como FSAL, que tambi√©n es la norma.  Qemu tambi√©n admite la API gl nativa a trav√©s de libgfapi. </p><br><p>  Pero fio (Flexible I / O Tester) ha soportado libgfapi por mucho tiempo y con √©xito, pero no es compatible con libcephfs.  Este es un plus gl, porque  usar fio es realmente bueno para probar gl directamente.  Solo trabajando desde el espacio de usuario a trav√©s de libgfapi obtendr√° todo lo que gl puede de gl. </p><br><p>  Pero si estamos hablando del sistema de archivos POSIX y de c√≥mo montarlo, gl solo puede ofrecer el cliente FUSE y la implementaci√≥n de CephFS en el n√∫cleo ascendente.  Est√° claro que en el m√≥dulo del n√∫cleo puede hacer un truco que FUSE mostrar√° un mejor rendimiento.  Pero en la pr√°ctica, FUSE siempre es una sobrecarga en el cambio de contexto.  Personalmente, he visto m√°s de una vez c√≥mo FUSE dobl√≥ un servidor de doble socket solo con CS. <br>  De alguna manera, Linus dijo: </p><br><blockquote>  Sistema de archivos del espacio de usuario?  El problema est√° justo ah√≠.  Siempre ha sido.  Las personas que piensan que los sistemas de archivos del espacio de usuario son realistas para cualquier cosa que no sean juguetes simplemente est√°n equivocados. </blockquote><p>  Los desarrolladores de Gl, por el contrario, piensan que FUSE es genial.  Se dice que esto brinda m√°s flexibilidad y se separa de las versiones del kernel.  En cuanto a m√≠, usan FUSE porque gl no se trata de velocidad.  De alguna manera est√° escrito, bueno, es normal, y molestarse con la implementaci√≥n en el n√∫cleo es realmente extra√±o. </p><br><h2 id="proizvoditelnost">  Rendimiento </h2><br><p>  No habr√° comparaciones). </p><br><p>  Esto es muy complicado.  Incluso en una configuraci√≥n id√©ntica, es demasiado dif√≠cil realizar pruebas objetivas.  De todos modos, habr√° alguien en los comentarios que dar√° 100500 par√°metros que "aceleran" uno de los sistemas y dir√°n que las pruebas son una mierda.  Por lo tanto, si est√° interesado, pru√©belo, por favor. </p><br><h2 id="zaklyuchenie">  Conclusi√≥n </h2><br><p>  RADOS y CephFS, en particular, son una soluci√≥n m√°s compleja tanto en comprensi√≥n, configuraci√≥n y mantenimiento. </p><br><p>  Pero personalmente, me gusta la arquitectura de RADOS y correr sobre CephFS m√°s que GlusterFS.  M√°s manejadores (PG, peso OSD, jerarqu√≠a CRUSH, etc.), los metadatos CephFS aumentan la complejidad, pero dan m√°s flexibilidad y hacen que esta soluci√≥n sea m√°s efectiva, en mi opini√≥n. </p><br><p>  Ceph se adapta mucho mejor a los criterios SDS actuales y me parece m√°s prometedor.  Pero esta es mi opini√≥n, ¬øqu√© te parece? </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es430474/">https://habr.com/ru/post/es430474/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es430462/index.html">En Rusia, apareci√≥ un proyecto de ley sobre el suministro de datos de usuarios de redes sociales a un c√≠rculo ilimitado de personas. Redes sociales contra</a></li>
<li><a href="../es430466/index.html">Mini AI Cup # 3: Escribiendo un Top Bot</a></li>
<li><a href="../es430468/index.html">Sensibilizar a los ciudadanos.</a></li>
<li><a href="../es430470/index.html">¬øPor qu√© mantener el contexto en la cuenta del cliente? Honestamente y de manera rentable</a></li>
<li><a href="../es430472/index.html">Red DECT sin costura de bricolaje</a></li>
<li><a href="../es430476/index.html">NCBI Genome Workbench: Investigaci√≥n en peligro de extinci√≥n</a></li>
<li><a href="../es430478/index.html">Comercio de bots para el mercado de criptomonedas. Por donde empezar</a></li>
<li><a href="../es430480/index.html">Mientras escrib√≠amos la aplicaci√≥n en el hackathon del Desaf√≠o de Aplicaciones Espaciales de la NASA</a></li>
<li><a href="../es430482/index.html">El tema de las placas de armadura en la cultura de Oriente y Occidente</a></li>
<li><a href="../es430484/index.html">Escenarios t√≠picos de implementaci√≥n de NGFW</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>