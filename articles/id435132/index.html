<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>😡 ⛈️ 🦀 Pengembara: masalah dan solusi 🔬 🔽 🙏🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Layanan pertama di Nomad I diluncurkan pada September 2016. Saat ini saya menggunakannya sebagai programmer dan dukungan sebagai administrator dari du...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Pengembara: masalah dan solusi</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/435132/"><p>  Layanan pertama di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Nomad</a> I diluncurkan pada September 2016.  Saat ini saya menggunakannya sebagai programmer dan dukungan sebagai administrator dari dua cluster Nomad - satu "rumah" untuk proyek pribadi saya (6 mesin mikro-virtual di Hetzner Cloud dan ArubaCloud di 5 pusat data berbeda di Eropa) dan yang kedua bekerja (sekitar 40 server virtual dan fisik pribadi di dua pusat data). </p><br><p>  Selama masa lalu, cukup banyak pengalaman yang telah terakumulasi dengan lingkungan Nomad, dalam artikel saya akan menjelaskan masalah yang dihadapi oleh Nomad dan bagaimana cara mengatasinya. </p><br><p><img src="https://habrastorage.org/webt/k5/9m/pp/k59mpp5iyvtxtj2q9nrvthzpelo.jpeg"><br>  <em>Yamal nomad menjadikan instance Continous Delivery perangkat lunak Anda © National Geographic Russia</em> </p><a name="habracut"></a><br><h2 id="1-kolichestvo-servernyh-nod-na-odin-datacentr">  1. Jumlah node server per pusat data </h2><br><p>  <strong>Solusi: satu node server sudah cukup untuk satu pusat data.</strong> </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Dokumentasi</a> tidak secara eksplisit menunjukkan berapa banyak node server yang diperlukan dalam satu pusat data.  Hanya diindikasikan bahwa 3-5 node diperlukan per wilayah, yang logis untuk konsensus protokol rakit. </p><br><p><img src="https://habrastorage.org/webt/go/yt/gu/goytgumjr0zxxqodicboxfgipze.png"></p><br><p>  Pada awalnya, saya merencanakan 2-3 server node di setiap pusat data untuk menyediakan redundansi. </p><br><p>  Setelah digunakan ternyata: </p><br><ol><li>  Ini sama sekali tidak diperlukan, karena jika terjadi kegagalan simpul di pusat data, peran simpul server untuk agen di pusat data ini akan dimainkan oleh node server lain di wilayah tersebut. </li><li>  Ternyata lebih buruk lagi jika masalah 8 tidak diselesaikan.  Ketika wizard terpilih kembali, ketidakkonsistenan dapat terjadi dan Nomad akan memulai kembali beberapa bagian dari layanan. </li></ol><br><h2 id="2-resursy-servera-dlya-servernoy-nody">  2. Sumber daya server untuk node server </h2><br><p>  <strong>Solusi: mesin virtual kecil sudah cukup untuk node server.</strong>  <strong>Di server yang sama, diizinkan untuk menjalankan layanan non-sumber daya intensif lainnya.</strong> </p><br><p>  Konsumsi memori daemon Nomad tergantung pada jumlah tugas yang berjalan.  Konsumsi CPU - berdasarkan pada jumlah tugas dan jumlah server / agen di wilayah tersebut (tidak linier). </p><br><p>  Dalam kasus kami: untuk 300 tugas yang berjalan, konsumsi memori adalah sekitar 500 MB untuk node master saat ini. </p><br><p>  Dalam cluster yang berfungsi, mesin virtual untuk node server: 4 CPU, 6 GB RAM. <br>  Diluncurkan tambahan: Konsul, Etcd, Vault. </p><br><h2 id="3-konsensus-pri-nehvatke-datacentrov">  3. Konsensus tentang kurangnya pusat data </h2><br><p>  <strong>Solusi: kami membuat tiga pusat data virtual dan tiga node server untuk dua pusat data fisik.</strong> </p><br><p>  Pekerjaan Nomad di wilayah ini didasarkan pada protokol rakit.  Untuk operasi yang benar, Anda memerlukan setidaknya 3 node server yang terletak di pusat data yang berbeda.  Ini akan memungkinkan operasi yang benar dengan hilangnya konektivitas jaringan dengan salah satu pusat data. </p><br><p>  Tetapi kami hanya memiliki dua pusat data.  Kami membuat kompromi: kami memilih pusat data, yang lebih kami percayai, dan membuat simpul server tambahan di dalamnya.  Kami melakukan ini dengan memperkenalkan pusat data virtual tambahan, yang secara fisik akan berlokasi di pusat data yang sama (lihat sub-paragraf 2 masalah 1). </p><br><p>  <strong>Solusi alternatif: kami memecah pusat data menjadi wilayah yang terpisah.</strong> </p><br><p>  Akibatnya, pusat data berfungsi secara independen dan konsensus hanya diperlukan dalam satu pusat data.  Di dalam pusat data, dalam hal ini lebih baik untuk membuat 3 node server dengan menerapkan tiga pusat data virtual dalam satu fisik. </p><br><p>  Opsi ini kurang nyaman untuk distribusi tugas, tetapi memberikan jaminan 100% atas independensi layanan jika terjadi masalah jaringan antara pusat data. </p><br><h2 id="4-server-i-agent-na-odnom-servere">  4. "Server" dan "agen" pada server yang sama </h2><br><p>  <strong>Solusi: valid jika Anda memiliki sejumlah server.</strong> </p><br><p>  Dokumentasi pengembara mengatakan bahwa melakukan ini tidak diinginkan.  Tetapi jika Anda tidak memiliki kesempatan untuk mengalokasikan mesin virtual terpisah untuk node server, Anda dapat menempatkan node server dan agen pada server yang sama. </p><br><p>  Menjalankan secara bersamaan berarti memulai Nomad daemon dalam mode klien dan mode server. </p><br><p>  Apa yang mengancam ini?  Dengan beban yang besar pada CPU server ini, simpul server Nomad akan bekerja secara tidak stabil, mungkin ada kehilangan konsensus dan detak jantung, dan layanan yang dimuat ulang. <br>  Untuk menghindari ini, kami meningkatkan batasan dari uraian masalah No. 8. </p><br><h2 id="5-realizaciya-prostranstv-imyon-namespaces">  5. Implementasi ruang nama </h2><br><p>  <strong>Solusi: mungkin melalui organisasi pusat data virtual.</strong> </p><br><p>  Terkadang Anda perlu menjalankan bagian dari layanan di server yang terpisah. </p><br><p>  Solusinya adalah yang pertama, sederhana, tetapi lebih menuntut sumber daya.  Kami membagi semua layanan ke dalam kelompok sesuai dengan tujuannya: frontend, backend, ... Tambahkan atribut meta ke server, meresepkan atribut untuk menjalankan semua layanan. </p><br><p>  Solusi kedua sederhana.  Kami menambahkan server baru, meresepkan atribut meta untuknya, meresepkan atribut peluncuran ini ke layanan yang diperlukan, semua layanan lain meresepkan larangan peluncuran di server dengan atribut ini. </p><br><p>  Solusi ketiga rumit.  Kami membuat pusat data virtual: luncurkan Konsul untuk pusat data baru, luncurkan simpul server Nomad untuk pusat data ini, jangan lupa jumlah node server untuk wilayah ini.  Sekarang Anda dapat menjalankan layanan individual di pusat data virtual khusus ini. </p><br><h2 id="6-integraciya-s-vault">  6. Integrasi dengan Vault </h2><br><p>  <strong>Solusi: Hindari Nomad &lt;-&gt; Ketergantungan melingkar Vault.</strong> </p><br><p>  Vault yang diluncurkan seharusnya tidak memiliki dependensi pada Nomad.  Alamat Vault yang terdaftar di Nomad sebaiknya mengarah langsung ke Vault, tanpa lapisan penyeimbang (tetapi valid).  Reservasi vault dalam hal ini dapat dilakukan melalui DNS - Konsul DNS atau eksternal. </p><br><p>  Jika data Vault ditulis dalam file konfigurasi Nomad, maka Nomad mencoba mengakses Vault saat startup.  Jika akses tidak berhasil, maka Nomad menolak untuk memulai. </p><br><p>  Saya membuat kesalahan dengan ketergantungan siklik sejak lama, ini sempat hampir sepenuhnya menghancurkan cluster Nomad.  Vault diluncurkan dengan benar, terlepas dari Nomad, tetapi Nomad melihat alamat Vault melalui penyeimbang yang berjalan di Nomad itu sendiri.  Konfigurasi ulang dan me-reboot node server Nomad menyebabkan restart layanan penyeimbang, yang menyebabkan kegagalan untuk memulai node server sendiri. </p><br><h2 id="7-zapusk-vazhnyh-statefull-servisov">  7. Meluncurkan layanan statefull penting </h2><br><p>  <strong>Solusi: valid, tetapi saya tidak.</strong> </p><br><p>  Apakah mungkin menjalankan PostgreSQL, ClickHouse, Redis Cluster, RabbitMQ, MongoDB via Nomad? </p><br><p>  Bayangkan Anda memiliki serangkaian layanan penting, yang pekerjaannya terkait dengan sebagian besar layanan lainnya.  Misalnya, database di PostgreSQL / ClickHouse.  Atau penyimpanan jangka pendek umum di Redis Cluster / MongoDB.  Atau bus data di Redis Cluster / RabbitMQ. </p><br><p>  Semua layanan ini dalam beberapa bentuk mengimplementasikan skema toleransi kesalahan: Stolon / Patroni untuk PostgreSQL, implementasi rakitnya sendiri di Redis Cluster, implementasi clusternya sendiri di RabbitMQ, MongoDB, ClickHouse. </p><br><p>  Ya, semua layanan ini dapat diluncurkan melalui Nomad dengan merujuk ke server tertentu, tetapi mengapa? </p><br><p>  Plus - kemudahan peluncuran, format skrip tunggal, seperti layanan lainnya.  Tidak perlu khawatir dengan skrip yang memungkinkan / apa pun. </p><br><p>  Minus adalah titik kegagalan tambahan, yang tidak memberikan keuntungan apa pun.  Secara pribadi, saya benar-benar menjatuhkan cluster Nomad dua kali karena berbagai alasan: sekali "rumah", sekali bekerja.  Ini pada tahap awal memperkenalkan Nomad dan karena kecerobohan. <br>  Juga, Nomad mulai berperilaku buruk dan memulai kembali layanan karena masalah nomor 8.  Tetapi bahkan jika masalah itu diselesaikan, bahaya tetap ada. </p><br><h2 id="8-stabilizaciya-raboty-i-restartov-servisov-v-nestabilnoy-seti">  8. Stabilisasi pekerjaan dan layanan restart dalam jaringan yang tidak stabil </h2><br><p>  <strong>Solusi: gunakan opsi penyetelan detak jantung.</strong> </p><br><p>  Secara default, Nomad dikonfigurasi sehingga masalah jaringan jangka pendek atau beban CPU menyebabkan hilangnya konsensus dan pemilihan kembali wizard atau menandai simpul agen tidak dapat diakses.  Dan ini mengarah pada reboot layanan secara spontan dan transfernya ke node lain. </p><br><p>  Statistik cluster "rumah" sebelum memperbaiki masalah: masa hidup maksimum wadah sebelum memulai kembali adalah sekitar 10 hari.  Di sini, masih dibebani dengan menjalankan agen dan server pada server yang sama dan menempatkannya di 5 pusat data yang berbeda di Eropa, yang menyiratkan beban besar pada CPU dan jaringan yang kurang stabil. </p><br><p>  Statistik gugus kerja sebelum memperbaiki masalah: masa pakai kontainer maksimum sebelum memulai kembali adalah lebih dari 2 bulan.  Semuanya relatif baik di sini karena server terpisah untuk node server Nomad dan jaringan yang sangat baik antara pusat data. </p><br><p>  Nilai default </p><br><pre><code class="plaintext hljs">heartbeat_grace = "10s" min_heartbeat_ttl = "10s" max_heartbeats_per_second = 50.0</code> </pre> <br><p>  Dilihat oleh kode: dalam konfigurasi ini, detak jantung dilakukan setiap 10 detik.  Dengan hilangnya dua detak jantung, pemilihan ulang master atau transfer layanan dari agen node dimulai.  Pengaturan kontroversial, menurut saya.  Kami mengeditnya tergantung pada aplikasi. </p><br><p>  Jika Anda memiliki semua layanan yang berjalan dalam beberapa kasus dan didistribusikan oleh pusat data, maka kemungkinan besar, itu tidak masalah bagi Anda untuk menentukan periode tidak dapat diaksesnya server (sekitar 5 menit, dalam contoh di bawah) - kami membuat interval detak jantung lebih jarang dan periode yang lebih lama menentukan tidak dapat diaksesnya.  Ini adalah contoh pengaturan cluster rumah saya: </p><br><pre> <code class="plaintext hljs"> heartbeat_grace = "300s" min_heartbeat_ttl = "30s" max_heartbeats_per_second = 10.0</code> </pre> <br><p>  Jika Anda memiliki konektivitas jaringan yang baik, memisahkan server untuk node server, dan periode menentukan tidak dapat diaksesnya server adalah penting (ada beberapa layanan yang berjalan dalam satu contoh dan penting untuk dengan cepat mentransfernya), kemudian tambahkan periode penentuan tidak dapat diaksesnya (heartbeat_grace).  Secara opsional, Anda dapat melakukan lebih banyak detak jantung (dengan mengurangi min_heartbeat_ttl) - ini akan sedikit meningkatkan beban pada CPU.  Contoh konfigurasi kluster kerja: </p><br><pre> <code class="plaintext hljs"> heartbeat_grace = "60s" min_heartbeat_ttl = "10s" max_heartbeats_per_second = 50.0</code> </pre> <br><p>  Pengaturan ini sepenuhnya memperbaiki masalah. </p><br><h2 id="9-zapusk-periodicheskih-zadach">  9. Memulai tugas berkala </h2><br><p>  <strong>Solusi: Layanan berkala Nomad dapat digunakan, tetapi cron lebih nyaman untuk dukungan.</strong> </p><br><p>  Nomad memiliki kemampuan untuk meluncurkan layanan secara berkala. </p><br><p>  Satu-satunya plus adalah kesederhanaan konfigurasi ini. </p><br><p>  Kekurangan pertama adalah bahwa jika layanan mulai sering, itu akan mengotori daftar tugas.  Misalnya, saat startup setiap 5 menit, 12 tugas tambahan akan ditambahkan ke daftar setiap jam, sampai Nomad GC terpicu, yang akan menghapus tugas-tugas lama. </p><br><p>  Kekurangan kedua - tidak jelas cara mengkonfigurasi pemantauan layanan semacam itu dengan benar.  Bagaimana memahami bahwa suatu layanan dimulai, memenuhi dan melakukan tugasnya sampai akhir? </p><br><p>  Akibatnya, untuk diri saya sendiri, saya sampai pada implementasi tugas berkala "cron": </p><br><ol><li>  Ini bisa menjadi cron reguler dalam wadah yang terus berjalan.  Cron secara berkala menjalankan skrip tertentu.  Pemeriksaan kesehatan skrip mudah ditambahkan ke wadah seperti itu, yang memeriksa setiap bendera yang membuat skrip yang sedang berjalan. </li><li>  Ini bisa menjadi wadah yang terus berjalan, dengan layanan yang terus berjalan.  Peluncuran berkala telah diterapkan di dalam layanan.  Baik skrip-pemeriksaan kesehatan atau http-pemeriksaan kesehatan dapat dengan mudah ditambahkan ke layanan seperti itu, yang segera memeriksa status dengan "bagian dalamnya". </li></ol><br><p>  Saat ini, saya menulis sebagian besar waktu di Go, masing-masing, saya lebih suka opsi kedua dengan http healthcheck - on Go dan peluncuran berkala, dan http healthcheck'i ditambahkan dengan beberapa baris kode. </p><br><h2 id="10-obespechenie-rezervirovaniya-servisov">  10. Menyediakan layanan yang berlebihan </h2><br><p>  <strong>Solusi: Tidak ada solusi sederhana.</strong>  <strong>Ada dua opsi yang lebih sulit.</strong> </p><br><p>  Skema penyediaan yang disediakan oleh pengembang Nomad adalah untuk mendukung jumlah layanan yang berjalan.  Anda mengatakan pengembara "meluncurkan saya 5 contoh layanan" dan dia mulai di suatu tempat di sana.  Tidak ada kontrol atas distribusi.  Mesin virtual dapat berjalan di server yang sama. </p><br><p>  Jika server crash, instans ditransfer ke server lain.  Ketika instance sedang ditransfer, layanan tidak berfungsi.  Ini adalah opsi ketentuan cadangan yang buruk. </p><br><p>  Kami melakukannya dengan benar: </p><br><ol><li>  Kami mendistribusikan instance di server melalui <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">differ_hosts</a> . </li><li>  Kami mendistribusikan contoh di seluruh pusat data.  Sayangnya, hanya dengan membuat salinan skrip dari formulir service1, service2 dengan konten yang sama, nama yang berbeda dan indikasi peluncuran di pusat data yang berbeda. </li></ol><br><p>  Dalam Nomad 0.9, sebuah fungsionalitas akan muncul yang akan memperbaiki masalah ini: akan mungkin untuk mendistribusikan layanan dalam rasio persentase antara server dan pusat data. </p><br><h2 id="11-web-ui-nomad">  11. Web UI Nomad </h2><br><p>  <strong>Solusi: UI bawaan sangat buruk, hashi-ui indah.</strong> </p><br><p>  Klien konsol melakukan sebagian besar fungsi yang diperlukan, tetapi kadang-kadang Anda ingin melihat grafik, tekan tombol ... </p><br><p>  Nomad memiliki UI bawaan.  Ini sangat tidak nyaman (bahkan lebih buruk daripada konsol). </p><br><p><img src="https://habrastorage.org/webt/0s/fv/6y/0sfv6yrspbj5easwnweyx8yzsme.png"></p><br><p>  Satu-satunya alternatif yang saya tahu adalah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">hashi-ui</a> . </p><br><p><img src="https://habrastorage.org/webt/vd/4x/rv/vd4xrvrrnewmotnnio-yxbvriis.png"></p><br><p>  Bahkan, sekarang saya pribadi membutuhkan klien konsol hanya untuk "menjalankan nomad".  Dan bahkan ini berencana untuk mentransfer ke CI. </p><br><h2 id="12-podderzhka-oversubscription-po-pamyati">  12. Dukungan untuk kelebihan langganan dari memori </h2><br><p>  <strong>Solusi: no.</strong> </p><br><p>  Dalam versi Nomad saat ini, Anda harus menentukan batas memori ketat untuk layanan ini.  Jika batas terlampaui, layanan akan dibunuh oleh Pembunuh OOM. </p><br><p>  Berlangganan berlebih adalah saat batas untuk layanan dapat ditentukan "dari dan ke."  Beberapa layanan memerlukan lebih banyak memori saat startup daripada selama operasi normal.  Beberapa layanan mungkin menggunakan lebih banyak memori daripada biasanya untuk waktu yang singkat. </p><br><p>  Pilihan pembatasan ketat atau lunak adalah topik untuk diskusi, tetapi, misalnya, Kubernetes memungkinkan programmer untuk membuat pilihan.  Sayangnya, dalam versi Nomad saat ini tidak ada kemungkinan seperti itu.  Saya akui itu akan muncul di versi mendatang. </p><br><h2 id="13-ochistka-servera-ot-servisov-nomad">  13. Membersihkan server dari layanan Nomad </h2><br><p>  <strong>Solusi:</strong> </p><br><pre> <code class="plaintext hljs">sudo systemctl stop nomad mount | fgrep alloc | awk '{print $3}' | xargs -I QQ sudo umount QQ sudo rm -rf /var/lib/nomad sudo docker ps | grep -v '(-1|-2|...)' | fgrep -v IMAGE | awk '{print $1}' | xargs -I QQ sudo docker stop QQ sudo systemctl start nomad</code> </pre> <br><p>  Terkadang "ada yang tidak beres."  Di server, ia membunuh node agen dan menolak untuk memulai.  Atau simpul agen berhenti merespons.  Atau simpul agen "kehilangan" layanan di server ini. <br>  Ini kadang-kadang terjadi dengan versi Nomad yang lebih lama, sekarang ini tidak terjadi, atau sangat jarang. </p><br><p>  Apa dalam hal ini yang paling mudah dilakukan, mengingat server drain tidak akan menghasilkan hasil yang diinginkan?  Kami membersihkan server secara manual: </p><br><ol><li>  Hentikan agen nomad. </li><li>  Buat umount pada mount yang dibuatnya. </li><li>  Hapus semua data agen. </li><li>  Kami menghapus semua wadah dengan memfilter wadah layanan (jika ada). </li><li>  Kami memulai agen. </li></ol><br><h2 id="14-kak-luchshe-razvorachivat-nomad">  14. Apa cara terbaik untuk menggunakan Nomad? </h2><br><p>  <strong>Solusi: tentu saja, melalui Konsul.</strong> </p><br><p>  Konsul dalam hal ini sama sekali bukan lapisan tambahan, tetapi layanan yang secara organik cocok dengan infrastruktur, yang memberikan lebih banyak plus daripada minus: DNS, penyimpanan KV, mencari layanan, memantau ketersediaan layanan, kemampuan untuk bertukar informasi dengan aman. </p><br><p>  Selain itu, itu terungkap semudah Nomad sendiri. </p><br><h2 id="15-chto-luchshe---nomad-ili-kubernetes">  15. Mana yang lebih baik - Pengembara atau Kubernet? </h2><br><p>  <strong>Solusi: tergantung pada ...</strong> </p><br><p>  Sebelumnya, kadang-kadang saya berpikir untuk memulai migrasi ke Kubernetes - saya sangat terganggu dengan reboot layanan berkala yang berkala (lihat masalah nomor 8).  Tetapi setelah solusi lengkap untuk masalah ini, saya dapat mengatakan: Nomad cocok untuk saya saat ini. </p><br><p>  Di sisi lain: Kubernetes juga memiliki layanan reload semi-spontan - ketika scheduler Kubernet mendistribusikan kembali instance tergantung pada beban.  Ini tidak terlalu keren, tetapi ada kemungkinan besar dikonfigurasi. </p><br><p>  Keuntungan Nomad: infrastrukturnya sangat mudah digunakan, skrip sederhana, dokumentasi yang baik, dukungan bawaan untuk Konsul / Vault, yang pada gilirannya memberikan: solusi sederhana untuk masalah penyimpanan kata sandi, DNS bawaan, helcheck yang mudah dikonfigurasikan. </p><br><p>  Pro dari Kubernetes: Sekarang ini adalah "standar de facto."  Dokumentasi yang baik, banyak solusi siap pakai, dengan deskripsi dan standarisasi peluncuran yang baik. </p><br><p>  Sayangnya, saya tidak memiliki keahlian hebat yang sama di Kubernetes untuk dengan tegas menjawab pertanyaan - apa yang harus digunakan untuk cluster baru.  Tergantung kebutuhan yang direncanakan. <br>  Jika Anda memiliki banyak ruang nama yang direncanakan (masalah nomor 5) atau layanan spesifik Anda menggunakan banyak memori di awal, kemudian membebaskannya (masalah nomor 12) - pasti Kubernetes, karena  dua masalah ini dalam Pengembara tidak sepenuhnya diselesaikan atau tidak nyaman. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id435132/">https://habr.com/ru/post/id435132/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id435120/index.html">Fungsi Lambda dalam SQL ... mari kita pikirkan</a></li>
<li><a href="../id435122/index.html">Bagaimana nyala api diimplementasikan dalam Doom di Playstation</a></li>
<li><a href="../id435124/index.html">Karya konstruksi kolom dunia: transformator monitor studio dengan jumlah band yang bervariasi</a></li>
<li><a href="../id435126/index.html">Pengalaman dalam mengatur dan melakukan konferensi perusahaan untuk analis</a></li>
<li><a href="../id435128/index.html">Pi-Sonos: hobi yang tidak terkendali</a></li>
<li><a href="../id435134/index.html">Sederhanakan bekerja dengan database di Qt dengan QSqlRelationalTableModel</a></li>
<li><a href="../id435136/index.html">Sergey dan metode ilmiahnya</a></li>
<li><a href="../id435138/index.html">Bagaimana mengendalikan infrastruktur jaringan Anda. Bab Tiga Keamanan jaringan. Bagian satu</a></li>
<li><a href="../id435142/index.html">Jejak Belajar Menggunakan eBPF: Sebuah Panduan dan Contoh</a></li>
<li><a href="../id435144/index.html">Pengantar Spring Boot: Membuat API REST Sederhana di Jawa</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>