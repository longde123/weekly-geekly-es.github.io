<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§æüèº üë©üèæ‚Äç‚öñÔ∏è üßôüèæ Memperkenalkan ODE Saraf üòë üêõ üëßüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Persamaan Diferensial Biasa Neural 
 Proporsi proses yang signifikan dijelaskan oleh persamaan diferensial, ini mungkin merupakan evolusi sistem fisik...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Memperkenalkan ODE Saraf</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/442002/"><h1>  Persamaan Diferensial Biasa Neural </h1><br>  Proporsi proses yang signifikan dijelaskan oleh persamaan diferensial, ini mungkin merupakan evolusi sistem fisik dari waktu ke waktu, kondisi medis pasien, karakteristik dasar pasar saham, dll.  Data tentang proses-proses semacam itu sifatnya konsisten dan berkelanjutan, dalam arti bahwa pengamatan hanyalah manifestasi dari semacam keadaan yang terus berubah. <br><br>  Ada juga tipe lain dari data serial, itu adalah data diskrit, misalnya, data tugas NLP.  Keadaan dalam data tersebut bervariasi secara terpisah: dari satu karakter atau kata ke yang lain. <br><br>  Sekarang kedua jenis data serial tersebut biasanya diproses oleh jaringan rekursif, meskipun sifatnya berbeda dan tampaknya memerlukan pendekatan yang berbeda. <br><br>  Satu artikel yang sangat menarik disajikan pada <em>konferensi NIPS</em> terakhir, yang dapat membantu menyelesaikan masalah ini.  Para penulis mengusulkan pendekatan yang mereka sebut <strong>ODE Neural</strong> . <br><br>  Di sini saya mencoba mereproduksi dan merangkum hasil artikel ini untuk membuat perkenalan dengan idenya sedikit lebih mudah.  Tampak bagi saya bahwa arsitektur baru ini mungkin menemukan tempat dalam alat standar ilmuwan data bersama dengan jaringan convolutional dan berulang. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7e/65/hf/7e65hfxs1amdqyy_uy6emdwulkg.png"></div><br><a name="habracut"></a><br><i></i><p>  <em>Gambar</em> 1: <em>Backpropagation</em> gradien kontinu membutuhkan pemecahan persamaan diferensial augmented kembali dalam waktu. <br><br>  Panah mewakili penyesuaian gradien yang diperbanyak mundur dengan gradien dari pengamatan. <br><br>  Ilustrasi dari artikel asli. </p><br><h2>  Pernyataan masalah </h2><br>  Biarkan ada proses yang mematuhi beberapa ODE yang tidak diketahui dan biarkan ada beberapa (berisik) pengamatan di sepanjang lintasan proses <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/438/dde/8ca/438dde8cabcfd6a826ed0257752e6499.svg" alt="\ frac {dz} {dt} = f (z (t), t) \; (1)"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/190/072/e64/190072e645d65dc29d6160aaf0dde065.svg" alt="\ {(z_0, t_0), (z_1, t_1), ..., (z_M, t_M) \} - \ text {pengamatan}"></div><br>  Cara menemukan perkiraan <img src="https://habrastorage.org/getpro/habr/post_images/287/bb8/c63/287bb8c637a8a70238be4ea690a0a8e1.svg" alt="\ widehat {f} (z, t, \ theta)">  fungsi speaker <img src="https://habrastorage.org/getpro/habr/post_images/6ad/6ab/1fa/6ad6ab1fa56e2c8b6d987dc5c1f68004.svg" alt="f (z, t)">  ? <br><br>  Pertama, pertimbangkan tugas yang lebih sederhana: hanya ada 2 pengamatan, di awal dan di akhir lintasan, <img src="https://habrastorage.org/getpro/habr/post_images/547/808/5c2/5478085c21b85393c8e9e0f78c2821b3.svg" alt="(z_0, t_0), (z_1, t_1)">  . <br><br>  Evolusi sistem dimulai dari keadaan <img src="https://habrastorage.org/getpro/habr/post_images/7e2/d13/d9a/7e2d13d9a9cc05a84422c3b63260dd83.svg" alt="z_0, t_0">  tepat waktu <img src="https://habrastorage.org/getpro/habr/post_images/4fc/683/cd0/4fc683cd033494d093a0e2c6f021805b.svg" alt="t_1 - t_0">  dengan beberapa fungsi dinamika parameter menggunakan metode evolusi sistem ODE.  Setelah sistem dalam keadaan baru <img src="https://habrastorage.org/getpro/habr/post_images/56d/8d1/11e/56d8d111e514029cf892aff24a82c768.svg" alt="\ hat {z_1}, t_1">  , itu dibandingkan dengan negara <img src="https://habrastorage.org/getpro/habr/post_images/f80/709/9a5/f807099a57f179abaa3ab5e70cee4c9c.svg" alt="z_1">  dan perbedaan di antara mereka diminimalkan dengan memvariasikan parameter <img src="https://habrastorage.org/getpro/habr/post_images/2cb/bcb/347/2cbbcb347a44c276c1095ac5bb3f8242.svg" alt="\ theta">  fungsi dinamika. <br><br>  Atau, lebih formal, pertimbangkan meminimalkan fungsi kerugian <img src="https://habrastorage.org/getpro/habr/post_images/b03/557/43f/b0355743fe8383284ef53436870494da.svg" alt="L (\ hat {z_1})">  : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/251/944/489/251944489756d64d40b61f0dc0d7cb0b.svg" alt="L (z (t_1)) = L \ Besar (\ int_ {t_0} ^ {t_1} f (z (t), t, \ theta) dt \ Besar) = L \ besar (\ teks {ODESolve} (z ( t_0), f, t_0, t_1, \ theta) \ big) \; (2)"></div><br>  Untuk meminimalkan <img src="https://habrastorage.org/getpro/habr/post_images/899/a67/551/899a675510d28419769a9b42281f0c65.svg" alt="L.">  , Anda perlu menghitung gradien untuk semua parameternya: <img src="https://habrastorage.org/getpro/habr/post_images/194/e4f/6e5/194e4f6e59bbfefc52efebce3bb857a3.svg" alt="z (t_0), t_0, t_1, \ theta">  .  Untuk melakukan ini, Anda harus terlebih dahulu menentukan caranya <img src="https://habrastorage.org/getpro/habr/post_images/899/a67/551/899a675510d28419769a9b42281f0c65.svg" alt="L.">  tergantung pada keadaan setiap saat <img src="https://habrastorage.org/getpro/habr/post_images/823/f33/fc8/823f33fc81685e76b1d88f48c68eea32.svg" alt="(z (t))">  : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e79/64f/b7f/e7964fb7f074d4f9c16893b53a1562c6.svg" alt="a (t) = - \ frac {\ partial L} {\ partial z (t)} \; (3)"></div><br><img src="https://habrastorage.org/getpro/habr/post_images/2e3/af6/935/2e3af69359cb79517739f0ccf9a8bc3e.svg" alt="a (t)">  disebut keadaan <em>adjoint</em> , dinamikanya diberikan oleh persamaan diferensial lain, yang dapat dianggap sebagai analog kontinu dari diferensiasi fungsi kompleks ( <em>aturan rantai</em> ): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/006/b66/5d5/006b665d51719470f25859e6271463dc.svg" alt="\ frac {d a (t)} {d t} = -a (t) \ frac {\ partial f (z (t), t, \ theta)} {\ partial z} \; (4)"></div><br>  Output dari formula ini dapat ditemukan di lampiran artikel asli. <br><br>  <i>Vektor dalam artikel ini harus dianggap sebagai vektor huruf kecil, meskipun artikel asli menggunakan representasi baris dan kolom.</i> <br><br>  Memecahkan perbedaan (4) kembali dalam waktu, kami memperoleh ketergantungan pada keadaan awal <img src="https://habrastorage.org/getpro/habr/post_images/9ec/7ef/827/9ec7ef8276505d510f609b983c58fdc3.svg" alt="z (t_0)">  : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/556/75a/7f0/55675a7f0c820f2a1da88e0802c97dc7.svg" alt="\ frac {\ partial L} {\ partial z (t_0)} = \ int_ {t_1} ^ {t_0} a (t) \ frac {\ partial f (z (t), t, \ theta)} {\ partial z} dt \; (5)"></div><br>  Untuk menghitung gradien sehubungan dengan <img src="https://habrastorage.org/getpro/habr/post_images/2a3/102/4ea/2a31024ea1803c34a47496e24a53a1ef.svg" alt="t">  dan <img src="https://habrastorage.org/getpro/habr/post_images/2cb/bcb/347/2cbbcb347a44c276c1095ac5bb3f8242.svg" alt="\ theta">  , Anda bisa menganggap mereka bagian dari negara.  Kondisi ini disebut <em>augmented</em> .  Dinamika keadaan ini secara sepele diperoleh dari dinamika aslinya: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/161/f11/79c/161f1179cbb6dfe3c18057d8abd0f45b.svg" alt="\ frac {d} {dt} \ begin {bmatrix} z ‚Äã‚Äã\\ \ theta \\ t \ end {bmatrix} (t) = f _ {\ text {aug}} ([z, \ theta, t]): = \ begin {bmatrix} f ([z, \ theta, t]) \\ 0 \\ 1 \ end {bmatrix} \; (6)"></div><br>  Kemudian kondisi konjugasi ke kondisi augmented ini: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7dc/7d0/553/7dc7d055304addbb695d1f23a9e640e6.svg" alt="a _ {\ text {aug}}: = \ begin {bmatrix} a \\ a _ {\ theta} \\ a_t \ end {bmatrix}, a _ {\ theta} (t): = \ frac {\ partial L} { \ partial \ theta (t)}, a_t (t): = \ frac {\ partial L} {\ partial t (t)} \; (7)"></div><br>  Gradient Augmented Dynamics: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/985/437/7a4/9854377a42cda72fa53bedb928a9d023.svg" alt="\ frac {\ partial f _ {\ text {aug}}} {\ partial [z, \ theta, t]} = \ begin {bmatrix} \ frac {\ partial f} {\ partial z} &amp;; \ frac {\ partial f} {\ partial \ theta} &amp;; \ frac {\ partial f} {\ partial t} \\ 0 &amp;; 0 &amp;; 0 \\ 0 &amp;; 0 &amp;; 0 \ end {bmatrix} \; (8)"></div><br>  Persamaan diferensial dari kondisi augmented terkonjugasi dari rumus (4) kemudian: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bd3/b9f/7f3/bd3b9f7f3e6ec08881594346a8a3e4f3.svg" alt="\ frac {d a _ {\ text {aug}}} {dt} = - \ begin {bmatrix} a \ frac {\ partial f} {\ partial z} &amp;; a \ frac {\ partial f} {\ partial \ theta} &amp;; a \ frac {\ partial f} {\ partial t} \ end {bmatrix} \; (9)"></div><br>  Memecahkan ODE ini kembali dalam hasil waktu: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5a0/01e/3d9/5a001e3d928ce08d05ff084353134d78.svg" alt="\ frac {\ partial L} {\ partial z (t_0)} = \ int_ {t_1} ^ {t_0} a (t) \ frac {\ partial f (z (t), t, \ theta)} {\ partial z} dt \; (10)"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e09/ba3/cc6/e09ba3cc6674830a8847894c39020ec0.svg" alt="\ frac {\ partial L} {\ partial \ theta} = \ int_ {t_1} ^ {t_0} a (t) \ frac {\ partial f (z (t), t, \ theta)} {\ partial \ theta } dt \; (11)"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/022/f84/715/022f84715f62b101017a6dcb591d5de3.svg" alt="\ frac {\ partial L} {\ partial t_0} = \ int_ {t_1} ^ {t_0} a (t) \ frac {\ partial f (z (t), t, \ theta)} {\ partial t} dt \; (12)"></div><br>  Ada apa dengan <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0f9/b43/8a8/0f9b438a88408c879af759bcaf4d1d5e.svg" alt="\ frac {\ partial L} {\ partial t_1} = - a (t) \ frac {\ partial f (z (t), t, \ theta)} {\ partial t} \; (13)"></div><br>  memberikan gradien di semua parameter input ke <em>ODES solve</em> ODE <em>solver</em> . <br><br>  Semua gradien (10), (11), (12), (13) dapat dihitung bersama dalam satu panggilan <em>ODESolve</em> dengan dinamika keadaan augmented terkonjugasi (9). <br><br><img src="https://habrastorage.org/webt/8k/pz/uk/8kpzukmizpmezmywov4b3zm29lc.png"><br>  <i>Ilustrasi dari artikel asli.</i> <br><br>  Algoritma di atas menjelaskan propagasi balik gradien dari solusi ODE untuk pengamatan berturut-turut. <br><br>  Dalam kasus beberapa pengamatan pada satu lintasan, semuanya dihitung dengan cara yang sama, tetapi pada saat pengamatan, kebalikan dari gradien terdistribusi harus disesuaikan dengan gradien dari pengamatan saat ini, seperti yang ditunjukkan pada <em>Gambar 1</em> . <br><br><h1>  Implementasi </h1><br>  Kode di bawah ini adalah implementasi <strong>ODE Neural saya</strong> .  Saya melakukannya murni untuk pemahaman yang lebih baik tentang apa yang terjadi.  Namun, sangat dekat dengan apa yang diterapkan dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">repositori</a> penulis artikel.  Ini berisi semua kode yang perlu Anda pahami di satu tempat, juga sedikit lebih banyak dikomentari.  Untuk aplikasi dan eksperimen nyata, masih lebih baik untuk menggunakan implementasi penulis dari artikel asli. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> math <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> IPython.display <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> clear_output <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tqdm_notebook <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> mpl <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt %matplotlib inline <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns sns.color_palette(<span class="hljs-string"><span class="hljs-string">"bright"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> mpl <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.cm <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> cm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Tensor <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> nn <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> functional <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> F <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.autograd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Variable use_cuda = torch.cuda.is_available()</code> </pre> <br>  Pertama, Anda perlu menerapkan metode apa pun untuk evolusi sistem ODE.  Demi kesederhanaan, metode Euler diimplementasikan di sini, meskipun metode eksplisit atau implisit cocok. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">ode_solve</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(z0, t0, t1, f)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""     -   """</span></span> h_max = <span class="hljs-number"><span class="hljs-number">0.05</span></span> n_steps = math.ceil((abs(t1 - t0)/h_max).max().item()) h = (t1 - t0)/n_steps t = t0 z = z0 <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i_step <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_steps): z = z + h * f(z, t) t = t + h <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> z</code> </pre><br>  Ini juga menjelaskan superclass dari fungsi dinamika parameter dengan beberapa metode yang berguna. <br><br>  Pertama: Anda harus mengembalikan semua parameter yang fungsinya tergantung dalam bentuk vektor. <br><br>  Kedua: perlu untuk menghitung dinamika augmented.  Dinamika ini tergantung pada gradien fungsi parameter dalam hal parameter dan input data.  Agar tidak harus mendaftarkan gradien dengan masing-masing tangan untuk setiap arsitektur baru, kita akan menggunakan metode <strong>torch.autograd.grad</strong> . <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ODEF</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward_with_grad</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, z, t, grad_outputs)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""Compute f and a df/dz, a df/dp, a df/dt"""</span></span> batch_size = z.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] out = self.forward(z, t) a = grad_outputs adfdz, adfdt, *adfdp = torch.autograd.grad( (out,), (z, t) + tuple(self.parameters()), grad_outputs=(a), allow_unused=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, retain_graph=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span> ) <span class="hljs-comment"><span class="hljs-comment">#  grad       , #  expand   if adfdp is not None: adfdp = torch.cat([p_grad.flatten() for p_grad in adfdp]).unsqueeze(0) adfdp = adfdp.expand(batch_size, -1) / batch_size if adfdt is not None: adfdt = adfdt.expand(batch_size, 1) / batch_size return out, adfdz, adfdt, adfdp def flatten_parameters(self): p_shapes = [] flat_parameters = [] for p in self.parameters(): p_shapes.append(p.size()) flat_parameters.append(p.flatten()) return torch.cat(flat_parameters)</span></span></code> </pre><br>  Kode di bawah ini menjelaskan propagasi maju dan mundur untuk <em>ODE Neural</em> .  Hal ini diperlukan untuk memisahkan kode ini dari <strong><em>torch.nn.Module</em></strong> utama dalam bentuk fungsi <strong><em>torch.autograd.Fungsi</em></strong> karena dalam yang terakhir Anda dapat menerapkan metode <strong><em>backpropagation</em></strong> sewenang-wenang, tidak seperti modul.  Jadi ini hanya penopang. <br><br>  Fitur ini mendasari seluruh pendekatan <em>ODE Neural</em> . <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ODEAdjoint</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(torch.autograd.Function)</span></span></span><span class="hljs-class">:</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(ctx, z0, t, flat_parameters, func)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">assert</span></span> isinstance(func, ODEF) bs, *z_shape = z0.size() time_len = t.size(<span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> torch.no_grad(): z = torch.zeros(time_len, bs, *z_shape).to(z0) z[<span class="hljs-number"><span class="hljs-number">0</span></span>] = z0 <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i_t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(time_len - <span class="hljs-number"><span class="hljs-number">1</span></span>): z0 = ode_solve(z0, t[i_t], t[i_t+<span class="hljs-number"><span class="hljs-number">1</span></span>], func) z[i_t+<span class="hljs-number"><span class="hljs-number">1</span></span>] = z0 ctx.func = func ctx.save_for_backward(t, z.clone(), flat_parameters) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> z @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">backward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(ctx, dLdz)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" dLdz shape: time_len, batch_size, *z_shape """</span></span> func = ctx.func t, z, flat_parameters = ctx.saved_tensors time_len, bs, *z_shape = z.size() n_dim = np.prod(z_shape) n_params = flat_parameters.size(<span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-comment"><span class="hljs-comment">#   , #       def augmented_dynamics(aug_z_i, t_i): """   -     t_i -   : bs, 1 aug_z_i -   : bs, n_dim*2 + n_params + 1 """ #     z_i, a = aug_z_i[:, :n_dim], aug_z_i[:, n_dim:2*n_dim] # Unflatten z and a z_i = z_i.view(bs, *z_shape) a = a.view(bs, *z_shape) with torch.set_grad_enabled(True): t_i = t_i.detach().requires_grad_(True) z_i = z_i.detach().requires_grad_(True) faug = func.forward_with_grad(z_i, t_i, grad_outputs=a) func_eval, adfdz, adfdt, adfdp = faug adfdz = adfdz if adfdz is not None else torch.zeros(bs, *z_shape) adfdp = adfdp if adfdp is not None else torch.zeros(bs, n_params) adfdt = adfdt if adfdt is not None else torch.zeros(bs, 1) adfdz = adfdz.to(z_i) adfdp = adfdp.to(z_i) adfdt = adfdt.to(z_i) # Flatten f and adfdz func_eval = func_eval.view(bs, n_dim) adfdz = adfdz.view(bs, n_dim) return torch.cat((func_eval, -adfdz, -adfdp, -adfdt), dim=1) dLdz = dLdz.view(time_len, bs, n_dim) # flatten dLdz   with torch.no_grad(): ##      #    , #       adj_z = torch.zeros(bs, n_dim).to(dLdz) adj_p = torch.zeros(bs, n_params).to(dLdz) #    z  p,        adj_t = torch.zeros(time_len, bs, 1).to(dLdz) for i_t in range(time_len-1, 0, -1): z_i = z[i_t] t_i = t[i_t] f_i = func(z_i, t_i).view(bs, n_dim) #      dLdz_i = dLdz[i_t] dLdt_i = torch.bmm(torch.transpose(dLdz_i.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0] #     adj_z += dLdz_i adj_t[i_t] = adj_t[i_t] - dLdt_i #      aug_z = torch.cat(( z_i.view(bs, n_dim), adj_z, torch.zeros(bs, n_params).to(z) adj_t[i_t]), dim=-1 ) #  ()      aug_ans = ode_solve(aug_z, t_i, t[i_t-1], augmented_dynamics) #       adj_z[:] = aug_ans[:, n_dim:2*n_dim] adj_p[:] += aug_ans[:, 2*n_dim:2*n_dim + n_params] adj_t[i_t-1] = aug_ans[:, 2*n_dim + n_params:] del aug_z, aug_ans ##         #    dLdz_0 = dLdz[0] dLdt_0 = torch.bmm(torch.transpose(dLdz_0.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0] #  adj_z += dLdz_0 adj_t[0] = adj_t[0] - dLdt_0 return adj_z.view(bs, *z_shape), adj_t, adj_p, None</span></span></code> </pre><br>  Sekarang untuk kenyamanan, bungkus fungsi ini dalam <strong>nn.Module</strong> . <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">NeuralODE</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, func)</span></span></span><span class="hljs-function">:</span></span> super(NeuralODE, self).__init__() <span class="hljs-keyword"><span class="hljs-keyword">assert</span></span> isinstance(func, ODEF) self.func = func <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, z0, t=Tensor</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">([</span></span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-number">0.</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">, </span></span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-number">1.</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">])</span></span></span></span><span class="hljs-function"><span class="hljs-params">, return_whole_sequence=False)</span></span></span><span class="hljs-function">:</span></span> t = t.to(z0) z = ODEAdjoint.apply(z0, t, self.func.flatten_parameters(), self.func) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> return_whole_sequence: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> z <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> z[<span class="hljs-number"><span class="hljs-number">-1</span></span>]</code> </pre><br><br><h1>  Aplikasi </h1><br><h2>  Pemulihan fungsi dinamika nyata (verifikasi pendekatan) </h2><br>  Sebagai tes dasar, sekarang mari kita periksa apakah <strong>Neural ODE</strong> dapat mengembalikan fungsi sebenarnya dari dinamika menggunakan data pengamatan. <br><br>  Untuk melakukan ini, pertama-tama kita menentukan fungsi dinamika ODE, mengembangkan lintasan berdasarkan itu, dan kemudian mencoba mengembalikannya dari fungsi dinamika parameter acak. <br><br>  Pertama, mari kita periksa kasus paling sederhana dari ODE linier.  Fungsi dinamika hanyalah aksi dari sebuah matriks. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/eb4/fdc/86a/eb4fdc86a1c5f56ab1b6e37e575e7c72.svg" alt="\ frac {dz} {dt} = \ begin {bmatrix} -0.1 &amp;; -1.0 \\ 1.0 &amp;; -0.1 \ end {bmatrix} z"></div><br>  Fungsi yang dilatih ditentukan oleh matriks acak. <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nj/q6/es/njq6eswuevh4rhx-8nhzyz-cq_k.gif"></div><br>  Selanjutnya, dinamika sedikit lebih canggih (tanpa gif, karena proses belajarnya tidak begitu indah :)) <br>  Fungsi pembelajaran di sini adalah jaringan yang sepenuhnya terhubung dengan satu lapisan tersembunyi. <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tr/4n/eb/tr4nebjdrs4pt4v5vlzleai8exe.png"></div><br><div class="spoiler">  <b class="spoiler_title">Kode</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">LinearODEF</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(ODEF)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, W)</span></span></span><span class="hljs-function">:</span></span> super(LinearODEF, self).__init__() self.lin = nn.Linear(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) self.lin.weight = nn.Parameter(W) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, t)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.lin(x)</code> </pre><br>  Fungsi dinamika hanyalah sebuah matriks <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SpiralFunctionExample</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(LinearODEF)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> matrix = Tensor([[<span class="hljs-number"><span class="hljs-number">-0.1</span></span>, <span class="hljs-number"><span class="hljs-number">-1.</span></span>], [<span class="hljs-number"><span class="hljs-number">1.</span></span>, <span class="hljs-number"><span class="hljs-number">-0.1</span></span>]]) super(SpiralFunctionExample, self).__init__(matrix)</code> </pre><br>  Matriks yang diparameterisasi secara acak <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">RandomLinearODEF</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(LinearODEF)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> super(RandomLinearODEF, self).__init__(torch.randn(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>)/<span class="hljs-number"><span class="hljs-number">2.</span></span>)</code> </pre><br>  Dinamika untuk lintasan yang lebih canggih <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">TestODEF</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(ODEF)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, A, B, x0)</span></span></span><span class="hljs-function">:</span></span> super(TestODEF, self).__init__() self.A = nn.Linear(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) self.A.weight = nn.Parameter(A) self.B = nn.Linear(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) self.B.weight = nn.Parameter(B) self.x0 = nn.Parameter(x0) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, t)</span></span></span><span class="hljs-function">:</span></span> xTx0 = torch.sum(x*self.x0, dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) dxdt = torch.sigmoid(xTx0) * self.A(x - self.x0) + torch.sigmoid(-xTx0) * self.B(x + self.x0) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> dxdt</code> </pre><br>  Belajar dinamika dalam bentuk jaringan yang terhubung penuh <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">NNODEF</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(ODEF)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, in_dim, hid_dim, time_invariant=False)</span></span></span><span class="hljs-function">:</span></span> super(NNODEF, self).__init__() self.time_invariant = time_invariant <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> time_invariant: self.lin1 = nn.Linear(in_dim, hid_dim) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: self.lin1 = nn.Linear(in_dim+<span class="hljs-number"><span class="hljs-number">1</span></span>, hid_dim) self.lin2 = nn.Linear(hid_dim, hid_dim) self.lin3 = nn.Linear(hid_dim, in_dim) self.elu = nn.ELU(inplace=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, t)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> self.time_invariant: x = torch.cat((x, t), dim=<span class="hljs-number"><span class="hljs-number">-1</span></span>) h = self.elu(self.lin1(x)) h = self.elu(self.lin2(h)) out = self.lin3(h) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> out <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">to_np</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> x.detach().cpu().numpy() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">plot_trajectories</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(obs=None, times=None, trajs=None, save=None, figsize=</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-number">16</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">, </span></span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-number">8</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> plt.figure(figsize=figsize) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> obs <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> times <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>: times = [<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>] * len(obs) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> o, t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(obs, times): o, t = to_np(o), to_np(t) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> b_i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(o.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>]): plt.scatter(o[:, b_i, <span class="hljs-number"><span class="hljs-number">0</span></span>], o[:, b_i, <span class="hljs-number"><span class="hljs-number">1</span></span>], c=t[:, b_i, <span class="hljs-number"><span class="hljs-number">0</span></span>], cmap=cm.plasma) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> trajs <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> z <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> trajs: z = to_np(z) plt.plot(z[:, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>], z[:, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], lw=<span class="hljs-number"><span class="hljs-number">1.5</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> save <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>: plt.savefig(save) plt.show() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">conduct_experiment</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(ode_true, ode_trained, n_steps, name, plot_freq=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">10</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># Create data z0 = Variable(torch.Tensor([[0.6, 0.3]])) t_max = 6.29*5 n_points = 200 index_np = np.arange(0, n_points, 1, dtype=np.int) index_np = np.hstack([index_np[:, None]]) times_np = np.linspace(0, t_max, num=n_points) times_np = np.hstack([times_np[:, None]]) times = torch.from_numpy(times_np[:, :, None]).to(z0) obs = ode_true(z0, times, return_whole_sequence=True).detach() obs = obs + torch.randn_like(obs) * 0.01 # Get trajectory of random timespan min_delta_time = 1.0 max_delta_time = 5.0 max_points_num = 32 def create_batch(): t0 = np.random.uniform(0, t_max - max_delta_time) t1 = t0 + np.random.uniform(min_delta_time, max_delta_time) idx = sorted(np.random.permutation( index_np[(times_np &gt; t0) &amp; (times_np &lt; t1)] )[:max_points_num]) obs_ = obs[idx] ts_ = times[idx] return obs_, ts_ # Train Neural ODE optimizer = torch.optim.Adam(ode_trained.parameters(), lr=0.01) for i in range(n_steps): obs_, ts_ = create_batch() z_ = ode_trained(obs_[0], ts_, return_whole_sequence=True) loss = F.mse_loss(z_, obs_.detach()) optimizer.zero_grad() loss.backward(retain_graph=True) optimizer.step() if i % plot_freq == 0: z_p = ode_trained(z0, times, return_whole_sequence=True) plot_trajectories(obs=[obs], times=[times], trajs=[z_p], save=f"assets/imgs/{name}/{i}.png") clear_output(wait=True) ode_true = NeuralODE(SpiralFunctionExample()) ode_trained = NeuralODE(RandomLinearODEF()) conduct_experiment(ode_true, ode_trained, 500, "linear") func = TestODEF(Tensor([[-0.1, -0.5], [0.5, -0.1]]), Tensor([[0.2, 1.], [-1, 0.2]]), Tensor([[-1., 0.]])) ode_true = NeuralODE(func) func = NNODEF(2, 16, time_invariant=True) ode_trained = NeuralODE(func) conduct_experiment(ode_true, ode_trained, 3000, "comp", plot_freq=30)</span></span></code> </pre><br></div></div><br>  Seperti yang Anda lihat, <em>Neural ODE</em> cukup bagus dalam memulihkan dinamika.  Artinya, konsep secara keseluruhan berfungsi. <br>  Sekarang periksa masalah yang sedikit lebih rumit (MNIST, haha). <br><br><h2>  ODE Neural terinspirasi oleh ResNets </h2><br>  Di ResNet'ax, status laten berubah sesuai dengan rumus <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/30a/6b2/d3c/30a6b2d3c27bb27afaeb4736072e4446.svg" alt="h_ {t + 1} = h_ {t} + f (h_ {t}, \ theta_ {t})"></div><br>  dimana <img src="https://habrastorage.org/getpro/habr/post_images/7ca/860/97d/7ca86097dc5dc0d9cbb9b454168de928.svg" alt="t \ in \ {0 ... T \}">  Apakah nomor blok dan <img src="https://habrastorage.org/getpro/habr/post_images/eb2/5f7/ddf/eb25f7ddf77e46681e256b28fecaef35.svg" alt="f">  ini adalah fungsi yang dipelajari oleh lapisan di dalam blok. <br><br>  Dalam batasnya, jika kita mengambil jumlah blok yang tak terbatas dengan langkah-langkah yang lebih kecil, kita mendapatkan dinamika kontinu dari lapisan tersembunyi dalam bentuk ODE, seperti apa yang ada di atas. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/629/67a/d41/62967ad41e5f50cc65e1c1e55a2860d3.svg" alt="\ frac {dh (t)} {dt} = f (h (t), t, \ theta)"></div><br>  Mulai dari lapisan input <img src="https://habrastorage.org/getpro/habr/post_images/c33/9b9/bc0/c339b9bc0244968c806390c2e66fdb62.svg" alt="h (0)">  kita dapat mendefinisikan layer output <img src="https://habrastorage.org/getpro/habr/post_images/b7e/dec/ef3/b7edecef308ce0df4f3d1c8aaa753617.svg" alt="h (T)">  sebagai solusi untuk ODE ini pada waktu T. <br><br>  Sekarang kita bisa menghitung <img src="https://habrastorage.org/getpro/habr/post_images/2cb/bcb/347/2cbbcb347a44c276c1095ac5bb3f8242.svg" alt="\ theta">  sebagai parameter terdistribusi ( <em>dibagi</em> ) antara semua blok sangat kecil. <br><br><h3>  Memvalidasi Arsitektur ODE Neural pada MNIST </h3><br>  Pada bagian ini, kami akan menguji kemampuan <em>Neural ODE untuk</em> digunakan sebagai komponen dalam arsitektur yang lebih akrab. <br><br>  Secara khusus, kami akan mengganti blok residual dengan <em>Neural ODE</em> di pengklasifikasi MNIST. <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7c/gw/ia/7cgwiawqx6-_kxk82qpenqplowi.png" width="400"></div><br><br><div class="spoiler">  <b class="spoiler_title">Kode</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">norm</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(dim)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> nn.BatchNorm2d(dim) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">conv3x3</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(in_feats, out_feats, stride=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> nn.Conv2d(in_feats, out_feats, kernel_size=<span class="hljs-number"><span class="hljs-number">3</span></span>, stride=stride, padding=<span class="hljs-number"><span class="hljs-number">1</span></span>, bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">add_time</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(in_tensor, t)</span></span></span><span class="hljs-function">:</span></span> bs, c, w, h = in_tensor.shape <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> torch.cat((in_tensor, t.expand(bs, <span class="hljs-number"><span class="hljs-number">1</span></span>, w, h)), dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ConvODEF</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(ODEF)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, dim)</span></span></span><span class="hljs-function">:</span></span> super(ConvODEF, self).__init__() self.conv1 = conv3x3(dim + <span class="hljs-number"><span class="hljs-number">1</span></span>, dim) self.norm1 = norm(dim) self.conv2 = conv3x3(dim + <span class="hljs-number"><span class="hljs-number">1</span></span>, dim) self.norm2 = norm(dim) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, t)</span></span></span><span class="hljs-function">:</span></span> xt = add_time(x, t) h = self.norm1(torch.relu(self.conv1(xt))) ht = add_time(h, t) dxdt = self.norm2(torch.relu(self.conv2(ht))) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> dxdt <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ContinuousNeuralMNISTClassifier</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, ode)</span></span></span><span class="hljs-function">:</span></span> super(ContinuousNeuralMNISTClassifier, self).__init__() self.downsampling = nn.Sequential( nn.Conv2d(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), norm(<span class="hljs-number"><span class="hljs-number">64</span></span>), nn.ReLU(inplace=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>), nn.Conv2d(<span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), norm(<span class="hljs-number"><span class="hljs-number">64</span></span>), nn.ReLU(inplace=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>), nn.Conv2d(<span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), ) self.feature = ode self.norm = norm(<span class="hljs-number"><span class="hljs-number">64</span></span>) self.avg_pool = nn.AdaptiveAvgPool2d((<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) self.fc = nn.Linear(<span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x)</span></span></span><span class="hljs-function">:</span></span> x = self.downsampling(x) x = self.feature(x) x = self.norm(x) x = self.avg_pool(x) shape = torch.prod(torch.tensor(x.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>:])).item() x = x.view(<span class="hljs-number"><span class="hljs-number">-1</span></span>, shape) out = self.fc(x) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> out func = ConvODEF(<span class="hljs-number"><span class="hljs-number">64</span></span>) ode = NeuralODE(func) model = ContinuousNeuralMNISTClassifier(ode) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: model = model.cuda() <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torchvision img_std = <span class="hljs-number"><span class="hljs-number">0.3081</span></span> img_mean = <span class="hljs-number"><span class="hljs-number">0.1307</span></span> batch_size = <span class="hljs-number"><span class="hljs-number">32</span></span> train_loader = torch.utils.data.DataLoader( torchvision.datasets.MNIST(<span class="hljs-string"><span class="hljs-string">"data/mnist"</span></span>, train=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, download=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, transform=torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((img_mean,), (img_std,)) ]) ), batch_size=batch_size, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span> ) test_loader = torch.utils.data.DataLoader( torchvision.datasets.MNIST(<span class="hljs-string"><span class="hljs-string">"data/mnist"</span></span>, train=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, download=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, transform=torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((img_mean,), (img_std,)) ]) ), batch_size=<span class="hljs-number"><span class="hljs-number">128</span></span>, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span> ) optimizer = torch.optim.Adam(model.parameters()) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(epoch)</span></span></span><span class="hljs-function">:</span></span> num_items = <span class="hljs-number"><span class="hljs-number">0</span></span> train_losses = [] model.train() criterion = nn.CrossEntropyLoss() print(<span class="hljs-string"><span class="hljs-string">f"Training Epoch </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{epoch}</span></span></span><span class="hljs-string">..."</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> batch_idx, (data, target) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tqdm(enumerate(train_loader), total=len(train_loader)): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: data = data.cuda() target = target.cuda() optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() train_losses += [loss.item()] num_items += data.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] print(<span class="hljs-string"><span class="hljs-string">'Train loss: {:.5f}'</span></span>.format(np.mean(train_losses))) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> train_losses <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">test</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> accuracy = <span class="hljs-number"><span class="hljs-number">0.0</span></span> num_items = <span class="hljs-number"><span class="hljs-number">0</span></span> model.eval() criterion = nn.CrossEntropyLoss() print(<span class="hljs-string"><span class="hljs-string">f"Testing..."</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> torch.no_grad(): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> batch_idx, (data, target) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tqdm(enumerate(test_loader), total=len(test_loader)): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: data = data.cuda() target = target.cuda() output = model(data) accuracy += torch.sum(torch.argmax(output, dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) == target).item() num_items += data.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] accuracy = accuracy * <span class="hljs-number"><span class="hljs-number">100</span></span> / num_items print(<span class="hljs-string"><span class="hljs-string">"Test Accuracy: {:.3f}%"</span></span>.format(accuracy)) n_epochs = <span class="hljs-number"><span class="hljs-number">5</span></span> test() train_losses = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> epoch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, n_epochs + <span class="hljs-number"><span class="hljs-number">1</span></span>): train_losses += train(epoch) test() <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">9</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>)) history = pd.DataFrame({<span class="hljs-string"><span class="hljs-string">"loss"</span></span>: train_losses}) history[<span class="hljs-string"><span class="hljs-string">"cum_data"</span></span>] = history.index * batch_size history[<span class="hljs-string"><span class="hljs-string">"smooth_loss"</span></span>] = history.loss.ewm(halflife=<span class="hljs-number"><span class="hljs-number">10</span></span>).mean() history.plot(x=<span class="hljs-string"><span class="hljs-string">"cum_data"</span></span>, y=<span class="hljs-string"><span class="hljs-string">"smooth_loss"</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), title=<span class="hljs-string"><span class="hljs-string">"train error"</span></span>)</code> </pre><br></div></div><br><pre> <code class="plaintext hljs">Testing... 100% 79/79 [00:01&lt;00:00, 45.69it/s] Test Accuracy: 9.740% Training Epoch 1... 100% 1875/1875 [01:15&lt;00:00, 24.69it/s] Train loss: 0.20137 Testing... 100% 79/79 [00:01&lt;00:00, 46.64it/s] Test Accuracy: 98.680% Training Epoch 2... 100% 1875/1875 [01:17&lt;00:00, 24.32it/s] Train loss: 0.05059 Testing... 100% 79/79 [00:01&lt;00:00, 46.11it/s] Test Accuracy: 97.760% Training Epoch 3... 100% 1875/1875 [01:16&lt;00:00, 24.63it/s] Train loss: 0.03808 Testing... 100% 79/79 [00:01&lt;00:00, 45.65it/s] Test Accuracy: 99.000% Training Epoch 4... 100% 1875/1875 [01:17&lt;00:00, 24.28it/s] Train loss: 0.02894 Testing... 100% 79/79 [00:01&lt;00:00, 45.42it/s] Test Accuracy: 99.130% Training Epoch 5... 100% 1875/1875 [01:16&lt;00:00, 24.67it/s] Train loss: 0.02424 Testing... 100% 79/79 [00:01&lt;00:00, 45.89it/s] Test Accuracy: 99.170%</code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/zx/zv/5x/zxzv5x4ktqyy9lt19of-d2i4few.png"></div><br>  Setelah pelatihan yang sangat kasar selama hanya 5 era dan 6 menit pelatihan, model tersebut telah mencapai kesalahan pengujian kurang dari 1%.  Kita dapat mengatakan bahwa <em>ODE Neural</em> terintegrasi dengan baik <em>sebagai</em> komponen ke dalam jaringan yang lebih tradisional. <br><br>  Dalam artikel mereka, penulis juga membandingkan pengelompokan ini (ODE-Net) dengan jaringan yang terhubung penuh reguler, dengan ResNet dengan arsitektur yang sama, dan dengan arsitektur yang sama persis, di mana gradien merambat langsung melalui operasi dalam <em>ODESolve</em> (tanpa metode gradien konjugat) ( RK-Net). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/vn/41/--/vn41--frzbdkftca4ehe7hh-kea.png"></div><br>  <i>Ilustrasi dari artikel asli.</i> <br><br>  Menurut mereka, jaringan 1-layer yang terhubung sepenuhnya dengan jumlah parameter yang sama dengan <em>Neural ODE</em> memiliki kesalahan yang jauh lebih tinggi dalam pengujian, ResNet dengan banyak kesalahan yang sama memiliki lebih banyak parameter, dan RK-Net tanpa metode gradien konjugasi memiliki kesalahan yang sedikit lebih tinggi dan dengan konsumsi memori yang meningkat secara linear (semakin kecil kesalahan yang diizinkan, semakin banyak langkah yang harus diambil <em>ODESolve</em> , yang meningkatkan konsumsi memori secara linear dengan jumlah langkah). <br><br>  Para penulis menggunakan metode Runge-Kutta implisit dengan ukuran langkah adaptif dalam implementasinya, tidak seperti metode Euler yang lebih sederhana di sini.  Mereka juga mempelajari beberapa properti dari arsitektur baru. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/u7/li/fq/u7lifqdsf72otgcyfigm6-fiyww.png"></div><br>  <i>Fitur ODE-Net (NFE Forward - jumlah kalkulasi fungsi secara langsung)</i> <i><br></i>  <i>Ilustrasi dari artikel asli.</i> <br><br><ul><li>  (a) Mengubah tingkat kesalahan numerik yang dapat diterima mengubah jumlah langkah dalam distribusi langsung. <br></li><li>  (B) Waktu yang dihabiskan untuk distribusi langsung sebanding dengan jumlah perhitungan fungsi. <br></li><li>  (C) Jumlah perhitungan fungsi untuk propagasi kembali adalah sekitar setengah dari propagasi langsung, yang menunjukkan bahwa metode gradien konjugat mungkin lebih efisien secara komputasi daripada menyebarkan gradien langsung melalui <em>ODESolve</em> . <br></li><li>  (d) Ketika ODE-Net menjadi semakin terlatih, ini membutuhkan lebih banyak dan lebih banyak komputasi dari suatu fungsi (langkah yang semakin kecil), mungkin beradaptasi dengan meningkatnya kompleksitas model. <br></li></ul><br><br><h2>  Fungsi Generatif Tersembunyi untuk Pemodelan Rangkaian Waktu </h2><br>  Neural ODE cocok untuk memproses data serial terus menerus bahkan ketika jalurnya terletak di ruang tersembunyi yang tidak diketahui. <br><br>  Di bagian ini, kami akan bereksperimen <em>dan</em> mengubah generasi urutan berkelanjutan menggunakan <em>Neural ODE</em> , dan melihat ruang tersembunyi yang dipelajari. <br><br>  Para penulis juga membandingkan ini dengan urutan serupa yang dihasilkan oleh jaringan berulang. <br><br>  Percobaan di sini sedikit berbeda dari contoh terkait dalam repositori penulis, di sini ada serangkaian lintasan yang lebih beragam. <br><br><h3>  Data </h3><br>  Data pelatihan terdiri dari spiral acak, setengahnya searah jarum jam, dan yang kedua berlawanan arah jarum jam.  Lebih lanjut, sampel acak diambil dari spiral ini, diproses oleh model pengulangan pengkodean dalam arah yang berlawanan, sehingga menimbulkan keadaan awal yang tersembunyi, yang kemudian berevolusi, menciptakan lintasan di ruang tersembunyi.  Jalur laten ini kemudian dipetakan ke ruang data dan dibandingkan dengan sampel berikutnya.  Dengan demikian, model belajar untuk menghasilkan lintasan yang mirip dengan dataset. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/or/4z/lg/or4zlgwkqjm-kzhvlmqtzmeqnl4.png"></div><br>  <i>Contoh dataset spiral</i> <br><br><h3>  VAE sebagai model generatif </h3><br>  Model generatif melalui prosedur pengambilan sampel: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fde/a91/b7b/fdea91b7b77af8c87a50e6c9e361f13b.svg" alt="z_ {t_0} \ sim \ mathcal {N} (0, I)"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fb0/9b2/ad7/fb09b2ad71d820266be632be7c5b5f97.svg" alt="z_ {t_1}, z_ {t_2}, ..., z_ {t_M} = \ text {ODESolve} (z_ {t_0}, f, \ theta_f, t_0, ..., t_M)"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/477/5a1/439/4775a14397c7842e67c0756d585c3d5b.svg" alt="x_ {t_i} \ sim p (x \ mid z_ {t_i}; \ theta_x)"></div><br>  Yang dapat dilatih menggunakan pendekatan auto-encoder variasional. <br><ol><li>  Pergi melalui encoder berulang melalui urutan waktu kembali pada waktunya untuk mendapatkan parameter <img src="https://habrastorage.org/getpro/habr/post_images/26b/84a/1dd/26b84a1dd2d618b4dd85d805e95b5db3.svg" alt="\ mu_ {z_ {t_0}}">  , <img src="https://habrastorage.org/getpro/habr/post_images/a1f/9e2/8ac/a1f9e28acee2636547023da51c1af36e.svg" alt="\ sigma_ {z_ {t_0}}">  distribusi posterior variasional, dan kemudian sampel darinya: <br></li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/816/997/3be/8169973bece43135c717f8abd5088e4c.svg" alt="z_ {t_0} \ sim q \ kiri (z_ {t_0} \ pertengahan x_ {t_0}, ..., x_ {t_M}; t_0, ..., t_M; \ theta_q \ kanan) = \ mathcal {N} \ kiri (z_ {t_0} \ mid \ mu_ {z_ {t_0}} \ sigma_ {z_ {t_0}} \ kanan)"></div><br><ol><li>  Dapatkan lintasan tersembunyi: <br></li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/416/810/833/416810833bb304242b3ec7bccfeb91ad.svg" alt="z_ {t_1}, z_ {t_2}, ..., z_ {t_N} = \ text {ODESolve} (z_ {t_0}, f, \ theta_f, t_0, ..., t_N), \ text {where} \ frac {dz} {dt} = f (z, t; \ theta_f)"></div><br><ol><li>  Memetakan jalur tersembunyi ke jalur dalam data menggunakan jaringan saraf lain: <img src="https://habrastorage.org/getpro/habr/post_images/fd1/2b0/73d/fd12b073dd1cede0a98b312c0e3240d1.svg" alt="\ hat {x_ {t_i}} (z_ {t_i}, t_i; \ theta_x)"><br></li><li>  Maksimalkan penilaian batas bawah validitas (ELBO) untuk jalur sampel: <br></li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9cf/838/1e3/9cf8381e35978cf70219a1f3bea211b6.svg" alt="\ text {ELBO} \ kira-kira N \ Besar (\ sum_ {i = 0} ^ {M} \ log p (x_ {t_i} \ mid z_ {t_i} (z_ {t_0}; \ theta_f); \ theta_x) + KL \ kiri (q (z_ {t_0} \ mid x_ {t_0}, ..., x_ {t_M}; t_0, ..., t_M; \ theta_q) \ parallel \ mathcal {N} (0, I) \ benar) \ Besar)"></div><br>  Dan dalam kasus distribusi posterior Gaussian <img src="https://habrastorage.org/getpro/habr/post_images/9a9/404/aa0/9a9404aa0267a6c257815abc794e95c3.svg" alt="p (x \ mid z_ {t_i}; \ theta_x)">  dan tingkat kebisingan yang diketahui <img src="https://habrastorage.org/getpro/habr/post_images/87a/7c9/aba/87a7c9abaa12e58e75ef64aa8312050e.svg" alt="\ sigma_x">  : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7b0/a0d/c43/7b0a0dc43667f287d584b6865afa1cfb.svg" alt="\ text {ELBO} \ approx -N \ Besar (\ sum_ {i = 1} ^ {M} \ frac {(x_i - \ hat {x_i}) ^ 2} {\ sigma_x ^ 2} - \ log \ sigma_ { z_ {t_0}} ^ 2 + \ mu_ {z_ {t_0}} ^ 2 + \ sigma_ {z_ {t_0}} ^ 2 \ Besar) + C"></div><br>  Grafik perhitungan model ODE tersembunyi dapat direpresentasikan sebagai berikut <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/zl/57/ui/zl57uisgnvjy7-y94fkker9m1eq.png"></div><br>  <i>Ilustrasi dari artikel asli.</i> <br><br>  Model ini kemudian dapat diuji untuk mengetahui bagaimana interpolasi jalur hanya menggunakan pengamatan awal. <br><br><div class="spoiler">  <b class="spoiler_title">Kode</b> <div class="spoiler_text"><h3>  Tentukan model </h3><br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">RNNEncoder</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, input_dim, hidden_dim, latent_dim)</span></span></span><span class="hljs-function">:</span></span> super(RNNEncoder, self).__init__() self.input_dim = input_dim self.hidden_dim = hidden_dim self.latent_dim = latent_dim self.rnn = nn.GRU(input_dim+<span class="hljs-number"><span class="hljs-number">1</span></span>, hidden_dim) self.hid2lat = nn.Linear(hidden_dim, <span class="hljs-number"><span class="hljs-number">2</span></span>*latent_dim) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, t)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># Concatenate time to input t = t.clone() t[1:] = t[:-1] - t[1:] t[0] = 0. xt = torch.cat((x, t), dim=-1) _, h0 = self.rnn(xt.flip((0,))) # Reversed # Compute latent dimension z0 = self.hid2lat(h0[0]) z0_mean = z0[:, :self.latent_dim] z0_log_var = z0[:, self.latent_dim:] return z0_mean, z0_log_var class NeuralODEDecoder(nn.Module): def __init__(self, output_dim, hidden_dim, latent_dim): super(NeuralODEDecoder, self).__init__() self.output_dim = output_dim self.hidden_dim = hidden_dim self.latent_dim = latent_dim func = NNODEF(latent_dim, hidden_dim, time_invariant=True) self.ode = NeuralODE(func) self.l2h = nn.Linear(latent_dim, hidden_dim) self.h2o = nn.Linear(hidden_dim, output_dim) def forward(self, z0, t): zs = self.ode(z0, t, return_whole_sequence=True) hs = self.l2h(zs) xs = self.h2o(hs) return xs class ODEVAE(nn.Module): def __init__(self, output_dim, hidden_dim, latent_dim): super(ODEVAE, self).__init__() self.output_dim = output_dim self.hidden_dim = hidden_dim self.latent_dim = latent_dim self.encoder = RNNEncoder(output_dim, hidden_dim, latent_dim) self.decoder = NeuralODEDecoder(output_dim, hidden_dim, latent_dim) def forward(self, x, t, MAP=False): z_mean, z_log_var = self.encoder(x, t) if MAP: z = z_mean else: z = z_mean + torch.randn_like(z_mean) * torch.exp(0.5 * z_log_var) x_p = self.decoder(z, t) return x_p, z, z_mean, z_log_var def generate_with_seed(self, seed_x, t): seed_t_len = seed_x.shape[0] z_mean, z_log_var = self.encoder(seed_x, t[:seed_t_len]) x_p = self.decoder(z_mean, t) return x_p</span></span></code> </pre><br><br><h3>  Generasi dataset </h3><br><br><pre> <code class="python hljs">t_max = <span class="hljs-number"><span class="hljs-number">6.29</span></span>*<span class="hljs-number"><span class="hljs-number">5</span></span> n_points = <span class="hljs-number"><span class="hljs-number">200</span></span> noise_std = <span class="hljs-number"><span class="hljs-number">0.02</span></span> num_spirals = <span class="hljs-number"><span class="hljs-number">1000</span></span> index_np = np.arange(<span class="hljs-number"><span class="hljs-number">0</span></span>, n_points, <span class="hljs-number"><span class="hljs-number">1</span></span>, dtype=np.int) index_np = np.hstack([index_np[:, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>]]) times_np = np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, t_max, num=n_points) times_np = np.hstack([times_np[:, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>]] * num_spirals) times = torch.from_numpy(times_np[:, :, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>]).to(torch.float32) <span class="hljs-comment"><span class="hljs-comment"># Generate random spirals parameters normal01 = torch.distributions.Normal(0, 1.0) x0 = Variable(normal01.sample((num_spirals, 2))) * 2.0 W11 = -0.1 * normal01.sample((num_spirals,)).abs() - 0.05 W22 = -0.1 * normal01.sample((num_spirals,)).abs() - 0.05 W21 = -1.0 * normal01.sample((num_spirals,)).abs() W12 = 1.0 * normal01.sample((num_spirals,)).abs() xs_list = [] for i in range(num_spirals): if i % 2 == 1: # Make it counter-clockwise W21, W12 = W12, W21 func = LinearODEF(Tensor([[W11[i], W12[i]], [W21[i], W22[i]]])) ode = NeuralODE(func) xs = ode(x0[i:i+1], times[:, i:i+1], return_whole_sequence=True) xs_list.append(xs) orig_trajs = torch.cat(xs_list, dim=1).detach() samp_trajs = orig_trajs + torch.randn_like(orig_trajs) * noise_std samp_ts = times fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 9)) axes = axes.flatten() for i, ax in enumerate(axes): ax.scatter(samp_trajs[:, i, 0], samp_trajs[:, i, 1], c=samp_ts[:, i, 0], cmap=cm.plasma) plt.show() import numpy.random as npr def gen_batch(batch_size, n_sample=100): n_batches = samp_trajs.shape[1] // batch_size time_len = samp_trajs.shape[0] n_sample = min(n_sample, time_len) for i in range(n_batches): if n_sample &gt; 0: probs = [1. / (time_len - n_sample)] * (time_len - n_sample) t0_idx = npr.multinomial(1, probs) t0_idx = np.argmax(t0_idx) tM_idx = t0_idx + n_sample else: t0_idx = 0 tM_idx = time_len frm, to = batch_size*i, batch_size*(i+1) yield samp_trajs[t0_idx:tM_idx, frm:to], samp_ts[t0_idx:tM_idx, frm:to]</span></span></code> </pre><br><br><h3>  Pelatihan </h3><br><br><pre> <code class="python hljs">vae = ODEVAE(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>) vae = vae.cuda() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: vae = vae.cuda() optim = torch.optim.Adam(vae.parameters(), betas=(<span class="hljs-number"><span class="hljs-number">0.9</span></span>, <span class="hljs-number"><span class="hljs-number">0.999</span></span>), lr=<span class="hljs-number"><span class="hljs-number">0.001</span></span>) preload = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span> n_epochs = <span class="hljs-number"><span class="hljs-number">20000</span></span> batch_size = <span class="hljs-number"><span class="hljs-number">100</span></span> plot_traj_idx = <span class="hljs-number"><span class="hljs-number">1</span></span> plot_traj = orig_trajs[:, plot_traj_idx:plot_traj_idx+<span class="hljs-number"><span class="hljs-number">1</span></span>] plot_obs = samp_trajs[:, plot_traj_idx:plot_traj_idx+<span class="hljs-number"><span class="hljs-number">1</span></span>] plot_ts = samp_ts[:, plot_traj_idx:plot_traj_idx+<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: plot_traj = plot_traj.cuda() plot_obs = plot_obs.cuda() plot_ts = plot_ts.cuda() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> preload: vae.load_state_dict(torch.load(<span class="hljs-string"><span class="hljs-string">"models/vae_spirals.sd"</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> epoch_idx <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_epochs): losses = [] train_iter = gen_batch(batch_size) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x, t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> train_iter: optim.zero_grad() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: x, t = x.cuda(), t.cuda() max_len = np.random.choice([<span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>]) permutation = np.random.permutation(t.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]) np.random.shuffle(permutation) permutation = np.sort(permutation[:max_len]) x, t = x[permutation], t[permutation] x_p, z, z_mean, z_log_var = vae(x, t) z_var = torch.exp(z_log_var) kl_loss = <span class="hljs-number"><span class="hljs-number">-0.5</span></span> * torch.sum(<span class="hljs-number"><span class="hljs-number">1</span></span> + z_log_var - z_mean**<span class="hljs-number"><span class="hljs-number">2</span></span> - z_var, <span class="hljs-number"><span class="hljs-number">-1</span></span>) loss = <span class="hljs-number"><span class="hljs-number">0.5</span></span> * ((x-x_p)**<span class="hljs-number"><span class="hljs-number">2</span></span>).sum(<span class="hljs-number"><span class="hljs-number">-1</span></span>).sum(<span class="hljs-number"><span class="hljs-number">0</span></span>) / noise_std**<span class="hljs-number"><span class="hljs-number">2</span></span> + kl_loss loss = torch.mean(loss) loss /= max_len loss.backward() optim.step() losses.append(loss.item()) print(<span class="hljs-string"><span class="hljs-string">f"Epoch </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{epoch_idx}</span></span></span><span class="hljs-string">"</span></span>) frm, to, to_seed = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">200</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span> seed_trajs = samp_trajs[frm:to_seed] ts = samp_ts[frm:to] <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: seed_trajs = seed_trajs.cuda() ts = ts.cuda() samp_trajs_p = to_np(vae.generate_with_seed(seed_trajs, ts)) fig, axes = plt.subplots(nrows=<span class="hljs-number"><span class="hljs-number">3</span></span>, ncols=<span class="hljs-number"><span class="hljs-number">3</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">15</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) axes = axes.flatten() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, ax <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(axes): ax.scatter(to_np(seed_trajs[:, i, <span class="hljs-number"><span class="hljs-number">0</span></span>]), to_np(seed_trajs[:, i, <span class="hljs-number"><span class="hljs-number">1</span></span>]), c=to_np(ts[frm:to_seed, i, <span class="hljs-number"><span class="hljs-number">0</span></span>]), cmap=cm.plasma) ax.plot(to_np(orig_trajs[frm:to, i, <span class="hljs-number"><span class="hljs-number">0</span></span>]), to_np(orig_trajs[frm:to, i, <span class="hljs-number"><span class="hljs-number">1</span></span>])) ax.plot(samp_trajs_p[:, i, <span class="hljs-number"><span class="hljs-number">0</span></span>], samp_trajs_p[:, i, <span class="hljs-number"><span class="hljs-number">1</span></span>]) plt.show() print(np.mean(losses), np.median(losses)) clear_output(wait=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) spiral_0_idx = <span class="hljs-number"><span class="hljs-number">3</span></span> spiral_1_idx = <span class="hljs-number"><span class="hljs-number">6</span></span> homotopy_p = Tensor(np.linspace(<span class="hljs-number"><span class="hljs-number">0.</span></span>, <span class="hljs-number"><span class="hljs-number">1.</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>)[:, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>]) vae = vae <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: homotopy_p = homotopy_p.cuda() vae = vae.cuda() spiral_0 = orig_trajs[:, spiral_0_idx:spiral_0_idx+<span class="hljs-number"><span class="hljs-number">1</span></span>, :] spiral_1 = orig_trajs[:, spiral_1_idx:spiral_1_idx+<span class="hljs-number"><span class="hljs-number">1</span></span>, :] ts_0 = samp_ts[:, spiral_0_idx:spiral_0_idx+<span class="hljs-number"><span class="hljs-number">1</span></span>, :] ts_1 = samp_ts[:, spiral_1_idx:spiral_1_idx+<span class="hljs-number"><span class="hljs-number">1</span></span>, :] <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: spiral_0, ts_0 = spiral_0.cuda(), ts_0.cuda() spiral_1, ts_1 = spiral_1.cuda(), ts_1.cuda() z_cw, _ = vae.encoder(spiral_0, ts_0) z_cc, _ = vae.encoder(spiral_1, ts_1) homotopy_z = z_cw * (<span class="hljs-number"><span class="hljs-number">1</span></span> - homotopy_p) + z_cc * homotopy_p t = torch.from_numpy(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>*np.pi, <span class="hljs-number"><span class="hljs-number">200</span></span>)) t = t[:, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>].expand(<span class="hljs-number"><span class="hljs-number">200</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>)[:, :, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>].cuda() t = t.cuda() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> t hom_gen_trajs = vae.decoder(homotopy_z, t) fig, axes = plt.subplots(nrows=<span class="hljs-number"><span class="hljs-number">2</span></span>, ncols=<span class="hljs-number"><span class="hljs-number">5</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">15</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>)) axes = axes.flatten() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, ax <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(axes): ax.plot(to_np(hom_gen_trajs[:, i, <span class="hljs-number"><span class="hljs-number">0</span></span>]), to_np(hom_gen_trajs[:, i, <span class="hljs-number"><span class="hljs-number">1</span></span>])) plt.show() torch.save(vae.state_dict(), <span class="hljs-string"><span class="hljs-string">"models/vae_spirals.sd"</span></span>)</code> </pre><br></div></div><br>       <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fv/pn/pi/fvpnpimixf3sq0cezhepgzh2txy.png"></div><br> <i> ‚Äî      (), <br>  ‚Äî     ,    . <br><br>    .</i> <br><br>       .        .       . <br><br>    ,     -   - . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/at/g_/7e/atg_7ecofins-uohnv99qccfxfq.png"></div><br><br>         <em>Neural ODE</em>    . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/kj/ak/-e/kjak-evgr-7mrrtpgimf0gfmfxq.png"></div><br> <i>   </i> <br><br><h2>    </h2><br>         .   ,       ,         (, ),            . <br>  ,           ,   . <br><br> <em> </em>       <em> </em> , <em>  </em>     . <br><br>  , ,    <em></em> ,  ,  ,     . <br><br>  : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nd/e_/wn/nde_wn590scz9dff_0hzxo1-tgg.png"></div><br> <i>    ( )   ( )   ; <br><br> -X        ¬´¬ª ( )  ¬´¬ª ( ). <br><br>    </i> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=" class="user_link">bekemax</a>            . <br><br>      <strong>Neural ODEs</strong> .   ! <br><br><h1>   </h1><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">   + . </a> <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> </a> <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> </a> <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> </a> <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">   VAE ()</a> <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> VAE</a> <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">   </a> <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Variational Inference with Normalizing Flows Paper</a> <br></li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id442002/">https://habr.com/ru/post/id442002/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id441990/index.html">Cara paling efektif untuk memerangi pembajakan - layanan hukum yang mudah dan murah</a></li>
<li><a href="../id441992/index.html">Memilih Hadiah untuk Gadis Geek</a></li>
<li><a href="../id441994/index.html">NASA: jumlah planet yang bisa dihuni di galaksi kita jauh lebih sedikit daripada yang diyakini secara umum</a></li>
<li><a href="../id441996/index.html">Teknologi dari tahun 80-an: yang menghidupkan kembali prosesor waferscale</a></li>
<li><a href="../id441998/index.html">"Kontainer memenangkan pertempuran, tetapi kalah dalam perang pada arsitektur tanpa server," - Simon Wardley</a></li>
<li><a href="../id442004/index.html">Efek penyaringan SVG. Bagian 7. Maju</a></li>
<li><a href="../id442006/index.html">Manajemen file salah dilakukan - Bagian 2: Masterpiece of Shit</a></li>
<li><a href="../id442008/index.html">k3 adalah Kubernet yang kecil namun bersertifikat oleh Rancher Labs</a></li>
<li><a href="../id442010/index.html">Python dan FPGA. Pengujian</a></li>
<li><a href="../id442012/index.html">Eksperimen: kami mengumpulkan direktori unit yang mengeluarkan paspor</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>