<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôãüèª üóìÔ∏è üëâ Hoch geladener GPU-Computerdienst üë©üèæ‚Äçü§ù‚Äçüë©üèº üè° üì†</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo habr Ich leite die Entwicklung der Vision- Plattform - dies ist unsere √∂ffentliche Plattform, die Zugriff auf Computer-Vision-Modelle bietet und...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Hoch geladener GPU-Computerdienst</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/472928/"><img src="https://habrastorage.org/webt/_v/yk/sa/_vyksasjmhcbsn1feox_egbqs_4.jpeg"><br><br>  Hallo habr  Ich leite die Entwicklung der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vision-</a> Plattform - dies ist unsere √∂ffentliche Plattform, die Zugriff auf Computer-Vision-Modelle bietet und es Ihnen erm√∂glicht, Aufgaben wie das Erkennen von Gesichtern, Zahlen, Objekten und ganzen Szenen zu l√∂sen.  Und heute m√∂chte ich am Beispiel von Vision erl√§utern, wie ein schneller, hoch geladener Dienst mithilfe von Grafikkarten implementiert und bereitgestellt und betrieben werden kann. <br><a name="habracut"></a><br><h1>  Was ist Vision? </h1><br>  Dies ist im Wesentlichen eine REST-API.  Der Benutzer generiert eine HTTP-Anfrage mit einem Foto und sendet es an den Server. <br><br>  Angenommen, Sie m√ºssen ein Gesicht in einem Bild erkennen.  Das System findet es, schneidet es, extrahiert einige Eigenschaften aus dem Gesicht, speichert es in der Datenbank und weist eine bedingte Nummer zu.  Zum Beispiel person42.  Der Benutzer l√§dt dann das n√§chste Foto hoch, das dieselbe Person hat.  Das System extrahiert Eigenschaften aus seinem Gesicht, durchsucht die Datenbank und gibt die bedingte Nummer zur√ºck, die der Person urspr√ºnglich zugewiesen wurde, d. H.  person42. <br><br>  Heute sind die Hauptnutzer von Vision verschiedene Projekte der Mail.ru Group.  Die meisten Anfragen kommen von Mail und Cloud. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c77/448/9a8/c774489a8bcd0a3badf0a8413d95e97c.png" width="400"></div><br>  In der Cloud haben Benutzer Ordner, in die Fotos hochgeladen werden.  Die Cloud f√ºhrt Dateien √ºber Vision aus und gruppiert sie in Kategorien.  Danach kann der Benutzer bequem durch seine Fotos bl√§ttern.  Wenn Sie beispielsweise Freunden oder Familienmitgliedern Fotos zeigen m√∂chten, k√∂nnen Sie schnell die Fotos finden, die Sie ben√∂tigen. <br><br>  Sowohl Mail als auch Cloud sind sehr gro√üe Dienste mit Millionen von Menschen, sodass Vision Hunderttausende von Anfragen pro Minute verarbeitet.  Das hei√üt, es ist ein klassischer hoch geladener Dienst, aber mit einer Wendung: Er verf√ºgt √ºber Nginx, einen Webserver, eine Datenbank und Warteschlangen, aber auf der untersten Ebene dieses Dienstes befindet sich die Inferenz - das Ausf√ºhren von Bildern √ºber neuronale Netze.  Es ist der Lauf neuronaler Netze, der die meiste Zeit in Anspruch nimmt und Ressourcen ben√∂tigt.  Computernetzwerke bestehen aus einer Folge von Matrixoperationen, die auf der CPU normalerweise lange dauern, auf der GPU jedoch perfekt parallelisiert sind.  Um Netzwerke effektiv zu betreiben, verwenden wir einen Cluster von Servern mit Grafikkarten. <br><br>  In diesem Artikel m√∂chte ich eine Reihe von Tipps ver√∂ffentlichen, die beim Erstellen eines solchen Dienstes hilfreich sein k√∂nnen. <br><br><h1>  Service-Entwicklung </h1><br><h3>  Bearbeitungszeit f√ºr eine Anfrage </h3><br>  F√ºr ein System mit hoher Last sind die Verarbeitungszeit einer Anforderung und der Durchsatz des Systems wichtig.  Eine hohe Geschwindigkeit der Abfrageverarbeitung wird vor allem durch die richtige Auswahl der neuronalen Netzwerkarchitektur erreicht.  In ML k√∂nnen wie in jeder anderen Programmieraufgabe dieselben Aufgaben auf unterschiedliche Weise gel√∂st werden.  Nehmen wir die Gesichtserkennung: Um dieses Problem zu l√∂sen, haben wir zuerst neuronale Netze mit R-FCN-Architektur verwendet.  Sie zeigen eine ziemlich hohe Qualit√§t, dauerten jedoch etwa 40 ms bei einem Bild, was nicht zu uns passte. Dann wandten wir uns der MTCNN-Architektur zu und erzielten eine zweifache Geschwindigkeitssteigerung bei leichtem Qualit√§tsverlust. <br><br>  Manchmal kann es zur Optimierung der Rechenzeit neuronaler Netze vorteilhaft sein, Inferenz in einem anderen Rahmen zu implementieren, nicht in dem, der gelehrt wurde.  Manchmal ist es beispielsweise sinnvoll, Ihr Modell auf NVIDIA TensorRT umzustellen.  Es wendet eine Reihe von Optimierungen an und eignet sich besonders f√ºr recht komplexe Modelle.  Zum Beispiel kann es einige Ebenen irgendwie neu anordnen, zusammenf√ºhren und sogar wegwerfen.  Das Ergebnis √§ndert sich nicht und die Geschwindigkeit der Inferenzberechnung erh√∂ht sich.  Mit TensorRT k√∂nnen Sie den Speicher auch besser verwalten und nach einigen Tricks auf die Berechnung von Zahlen mit geringerer Genauigkeit reduzieren, wodurch sich auch die Geschwindigkeit der Inferenzberechnung erh√∂ht. <br><br><h3>  Grafikkarte herunterladen </h3><br>  Die Netzwerkinferenz wird auf der GPU ausgef√ºhrt. Die Grafikkarte ist der teuerste Teil des Servers. Daher ist es wichtig, sie so effizient wie m√∂glich zu nutzen.  Wie k√∂nnen wir verstehen, haben wir die GPU vollst√§ndig geladen oder k√∂nnen wir die Last erh√∂hen?  Diese Frage kann beispielsweise mit dem Parameter GPU Utilization im Dienstprogramm nvidia-smi aus dem Standardvideotreiberpaket beantwortet werden.  Diese Abbildung zeigt nat√ºrlich nicht, wie viele CUDA-Kerne direkt auf die Grafikkarte geladen sind, sondern wie viele sich im Leerlauf befinden, aber Sie k√∂nnen das Laden der GPU irgendwie bewerten.  Aus Erfahrung k√∂nnen wir sagen, dass eine Beladung von 80-90% gut ist.  Wenn es zu 10-20% geladen ist, ist dies schlecht und es besteht immer noch Potenzial. <br><br>  Eine wichtige Konsequenz dieser Beobachtung: Sie m√ºssen versuchen, das System so zu organisieren, dass das Laden von Grafikkarten maximiert wird.  Wenn Sie 10 Grafikkarten haben, von denen jede zu 10 bis 20% geladen ist, k√∂nnen h√∂chstwahrscheinlich zwei Grafikkarten mit hoher Last das gleiche Problem l√∂sen. <br><br><h3>  Systemdurchsatz </h3><br>  Wenn Sie ein Bild an den Eingang eines neuronalen Netzwerks senden, wird die Bildverarbeitung auf eine Vielzahl von Matrixoperationen reduziert.  Die Grafikkarte ist ein Mehrkernsystem, und die Eingabebilder, die wir normalerweise einreichen, sind klein.  Angenommen, unsere Grafikkarte enth√§lt 1.000 Kerne, und das Bild enth√§lt 250 x 250 Pixel.  Alleine k√∂nnen sie aufgrund ihrer bescheidenen Gr√∂√üe nicht alle Kerne laden.  Und wenn wir solche Bilder einzeln an das Modell senden, wird das Laden der Grafikkarte 25% nicht √ºberschreiten. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4b9/b25/191/4b9b2519170a86b151420cc0a746a2fe.png"></div><br>  Daher m√ºssen Sie mehrere Bilder hochladen, um daraus zu schlie√üen, und daraus einen Stapel bilden. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/04c/367/51b/04c36751b27fa428e9828a6fa007e25d.png"></div><br>  In diesem Fall steigt die Grafikkartenlast auf 95%, und die Berechnung der Inferenz dauert wie bei einem einzelnen Bild. <br><br>  Aber was ist, wenn sich keine 10 Bilder in der Warteschlange befinden, damit wir sie zu einem Stapel kombinieren k√∂nnen?  Sie k√∂nnen beispielsweise 50-100 ms warten, in der Hoffnung, dass Anfragen kommen.  Diese Strategie wird als Fix-Latenz-Strategie bezeichnet.  Sie k√∂nnen Anforderungen von Clients in einem internen Puffer kombinieren.  Infolgedessen erh√∂hen wir unsere Verz√∂gerung um einen festen Betrag, erh√∂hen jedoch den Systemdurchsatz erheblich. <br><br><h3>  Inferenz starten </h3><br>  Wir trainieren Modelle mit Bildern eines festen Formats und einer festen Gr√∂√üe (z. B. 200 x 200 Pixel), aber der Dienst muss die M√∂glichkeit unterst√ºtzen, verschiedene Bilder hochzuladen.  Daher m√ºssen Sie alle Bilder vor dem Senden an die Inferenz ordnungsgem√§√ü vorbereiten (Gr√∂√üe √§ndern, zentrieren, normalisieren, in Float √ºbersetzen usw.).  Wenn alle diese Operationen in einem Prozess ausgef√ºhrt werden, der die Inferenz startet, sieht der Arbeitszyklus ungef√§hr so ‚Äã‚Äãaus: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f75/5be/112/f755be112bc557d3ab602aee47e40d2e.png"><br><br>  Er verbringt einige Zeit im Prozessor, bereitet die Eingabedaten vor und wartet einige Zeit auf eine Antwort von der GPU.  Es ist besser, die Intervalle zwischen den Schlussfolgerungen zu minimieren, damit die GPU weniger inaktiv ist. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fdb/783/cd7/fdb783cd70c90fcc06ba2cfca5f9c7f2.png"><br><br>  Zu diesem Zweck k√∂nnen Sie einen anderen Stream starten oder die Vorbereitung von Bildern auf andere Server √ºbertragen, ohne Grafikkarten, aber mit leistungsstarken Prozessoren. <br><br>  Wenn m√∂glich, sollte sich der f√ºr die Inferenz verantwortliche Prozess nur damit befassen: Zugriff auf den gemeinsam genutzten Speicher, Sammeln der Eingabedaten, sofortiges Kopieren in den Speicher der Grafikkarte und Ausf√ºhren der Inferenz. <br><br><h3>  Turbo-Boost </h3><br>  Das Starten neuronaler Netze ist eine Operation, die nicht nur Ressourcen der GPU, sondern auch des Prozessors verbraucht.  Selbst wenn alles in Bezug auf die Bandbreite korrekt organisiert ist und der Thread, der die Inferenz durchf√ºhrt, bereits auf neue Daten wartet, haben Sie auf einem schwachen Prozessor einfach keine Zeit, diesen Stream mit neuen Daten zu s√§ttigen. <br><br>  Viele Prozessoren unterst√ºtzen die Turbo Boost-Technologie.  Sie k√∂nnen die Frequenz des Prozessors erh√∂hen, dies ist jedoch nicht immer standardm√§√üig aktiviert.  Es lohnt sich, es sich anzusehen.  Zu diesem <code>$ cpupower frequency-info -m</code> verf√ºgt Linux √ºber das Dienstprogramm CPU Power: <code>$ cpupower frequency-info -m</code> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fd4/78f/ca4/fd478fca423620ee3bd684d2adfe9d73.png"></div><br>  Die Prozessoren verf√ºgen auch √ºber einen Stromverbrauchsmodus, der von einem solchen CPU Power: <code>performance</code> Befehl erkannt werden kann. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/12a/ed4/a1b/12aed4a1b0dec7240bd744232b5d4770.png"></div><br>  Im Powersave-Modus kann der Prozessor seine Frequenz drosseln und langsamer laufen.  Sie sollten in das BIOS gehen und den Leistungsmodus ausw√§hlen.  Dann arbeitet der Prozessor immer mit maximaler Frequenz. <br><br><h1>  Anwendungsbereitstellung </h1><br>  Docker eignet sich hervorragend f√ºr die Bereitstellung der Anwendung. Sie k√∂nnen Anwendungen auf der GPU im Container ausf√ºhren.  Um auf die Grafikkarten zuzugreifen, m√ºssen Sie zuerst die Treiber f√ºr die Grafikkarte auf dem Hostsystem installieren - einem physischen Server.  Um den Container zu starten, m√ºssen Sie viel Handarbeit leisten: Werfen Sie die Grafikkarten mit den richtigen Parametern korrekt in den Container.  Nach dem Starten des Containers m√ºssen weiterhin Videotreiber installiert werden.  Und erst danach k√∂nnen Sie Ihre Anwendung verwenden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/20a/ae0/cb2/20aae0cb286e3cef415f2a32f4f12bfa.png"><br><br>  Dieser Ansatz hat eine Einschr√§nkung.  Server k√∂nnen aus dem Cluster verschwinden und hinzugef√ºgt werden.  Es ist m√∂glich, dass verschiedene Server unterschiedliche Treiberversionen haben und sich von der im Container installierten Version unterscheiden.  In diesem Fall wird ein einfacher Docker unterbrochen: Die Anwendung erh√§lt beim Versuch, auf die Grafikkarte zuzugreifen, einen Fehler bei der Nicht√ºbereinstimmung der Treiberversion. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bb4/1af/0b2/bb41af0b281cf8a660d2cc35db133101.png"><br><br>  Wie gehe ich damit um?  Es gibt eine Version von Docker von NVIDIA, mit der die Verwendung des Containers einfacher und angenehmer wird.  Laut NVIDIA selbst und nach praktischen Beobachtungen betr√§gt der Aufwand f√ºr die Verwendung von nvidia-docker etwa 1%. <br><br>  In diesem Fall m√ºssen die Treiber nur auf dem Hostcomputer installiert werden.  Wenn Sie den Container starten, m√ºssen Sie nichts hineinwerfen, und die Anwendung hat sofort Zugriff auf die Grafikkarten. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c95/967/a3a/c95967a3a8ac668acaed1fc14550dcb8.png"><br><br>  Die "Unabh√§ngigkeit" von nvidia-docker von Treibern erm√∂glicht es Ihnen, einen Container mit demselben Image auf verschiedenen Computern auszuf√ºhren, auf denen verschiedene Versionen von Treibern installiert sind.  Wie wird das umgesetzt?  Docker hat ein Konzept namens Docker-Runtime: Es handelt sich um eine Reihe von Standards, die beschreiben, wie ein Container mit dem Host-Kernel kommunizieren soll, wie er gestartet und gestoppt werden soll, wie mit dem Kernel und dem Treiber interagiert werden soll.  Ab einer bestimmten Version von Docker kann diese Laufzeit ersetzt werden.  Dies hat NVIDIA getan: Sie ersetzen die Laufzeit, fangen die Anrufe an den darin enthaltenen Videotreiber ab und konvertieren die richtige Version in die Anrufe an den Videotreiber. <br><br><h1>  Orchestrierung </h1><br>  Wir haben Kubernetes als Orchester gew√§hlt.  Es unterst√ºtzt viele sehr sch√∂ne Funktionen, die f√ºr jedes stark ausgelastete System n√ºtzlich sind.  Durch die automatische Erkennung k√∂nnen Dienste beispielsweise innerhalb eines Clusters ohne komplexe Routing-Regeln aufeinander zugreifen.  Oder Fehlertoleranz - Wenn Kubernetes immer mehrere Container bereit h√§lt und Ihnen etwas passiert ist, startet Kubernetes sofort einen neuen Container. <br><br>  Wenn Sie bereits einen Kubernetes-Cluster konfiguriert haben, ben√∂tigen Sie nicht so viel, um Grafikkarten im Cluster zu verwenden: <br><br><ul><li>  relativ frische Fahrer <br></li><li>  installierte nvidia-docker version 2 <br></li><li>  Docker-Laufzeit standardm√§√üig auf "nvidia" in /etc/docker/daemon.json festgelegt: <br> <code>"default-runtime": "nvidia"</code> <br> </li><li>  Installiertes Plugin <code>kubectl create -f https://githubusercontent.com/k8s-device-plugin/v1.12/plugin.yml</code> </li></ul><br>  Nachdem Sie Ihren Cluster konfiguriert und das Ger√§te-Plugin installiert haben, k√∂nnen Sie eine Grafikkarte als Ressource angeben. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/414/c90/8f6/414c908f6d3ba6d4a9bded55921790c6.png"><br><br>  Was betrifft das?  Nehmen wir an, wir haben zwei Knoten, physische Maschinen.  Auf der einen befindet sich eine Grafikkarte, auf der anderen nicht.  Kubernetes erkennt eine Maschine mit einer Grafikkarte und nimmt unseren Pod darauf auf. <br><br>  Es ist wichtig zu beachten, dass Kubernetes nicht wei√ü, wie man eine Grafikkarte kompetent zwischen Pods fummelt.  Wenn Sie 4 Grafikkarten haben und 1 GPU zum Starten des Containers ben√∂tigen, k√∂nnen Sie nicht mehr als 4 Pods in Ihrem Cluster erstellen. <br><br>  Wir nehmen in der Regel 1 Pod = 1 Modell = 1 GPU. <br><br>  Es gibt eine Option, um mehr Instanzen auf 4 Grafikkarten auszuf√ºhren, aber wir werden dies in diesem Artikel nicht ber√ºcksichtigen, da diese Option nicht sofort verf√ºgbar ist. <br><br>  Wenn sich mehrere Modelle gleichzeitig drehen sollen, ist es praktisch, f√ºr jedes Modell eine Bereitstellung in Kubernetes zu erstellen.  In der Konfigurationsdatei k√∂nnen Sie die Anzahl der Herde f√ºr jedes Modell angeben, wobei die Beliebtheit des Modells ber√ºcksichtigt wird.  Wenn viele Anfragen an das Modell kommen, m√ºssen Sie viele Pods daf√ºr angeben. Wenn es nur wenige Anfragen gibt, gibt es nur wenige Pods.  Insgesamt sollte die Anzahl der Herde der Anzahl der Grafikkarten im Cluster entsprechen. <br><br>  Betrachten Sie einen interessanten Punkt.  Nehmen wir an, wir haben 4 Grafikkarten und 3 Modelle. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/04c/767/23a/04c76723a17fef7311eb41462db1d5a7.png"></div><br>  Lassen Sie auf den ersten beiden Grafikkarten die Schlussfolgerung des Gesichtserkennungsmodells steigen, bei einer weiteren Erkennung von Objekten und bei einer weiteren Erkennung von Fahrzeugnummern. <br><br>  Sie arbeiten, Kunden kommen und gehen, und einmal, zum Beispiel nachts, tritt eine Situation auf, in der eine Grafikkarte mit Inferenzobjekten einfach nicht geladen wird, eine kleine Anzahl von Anforderungen eingeht und Grafikkarten mit Gesichtserkennung √ºberlastet werden.  Ich m√∂chte in diesem Moment ein Modell mit Objekten l√∂schen und an seiner Stelle Gesichter starten, um die Linien zu entladen. <br><br>  F√ºr die automatische Skalierung von Modellen auf Grafikkarten gibt es in Kubernetes Tools - die automatische Skalierung des horizontalen Herds (HPA, horizontaler Pod-Autoscaler). <br>  Kubernetes unterst√ºtzt standardm√§√üig die automatische Skalierung der CPU-Auslastung.  Bei einer Aufgabe mit Grafikkarten ist es jedoch viel sinnvoller, Informationen √ºber die Anzahl der Aufgaben f√ºr jedes Modell zur Skalierung zu verwenden. <br><br>  Wir tun dies: Stellen Sie Anforderungen f√ºr jedes Modell in eine Warteschlange.  Wenn die Anforderungen abgeschlossen sind, entfernen wir sie aus dieser Warteschlange.  Wenn es uns gelingt, Anfragen nach g√§ngigen Modellen schnell zu verarbeiten, w√§chst die Warteschlange nicht.  Wenn die Anzahl der Anforderungen f√ºr ein bestimmtes Modell pl√∂tzlich zunimmt, w√§chst die Warteschlange.  Es wird klar, dass Sie Grafikkarten hinzuf√ºgen m√ºssen, die beim Harken der Linie helfen. <br><br>  Informationen zu den Warteschlangen, die wir √ºber Prometheus √ºber die HPA vertreten: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/aa3/da3/6ba/aa3da36baec71d52dcd0bee93c8e52cf.png"><br><br>  Und dann skalieren wir die Modelle auf den Grafikkarten im Cluster automatisch, abh√§ngig von der Anzahl der Anfragen an sie. <br><br><h3>  CI / CD </h3><br>  Nachdem Sie die Anwendung beigef√ºgt und in Kubernetes verpackt haben, haben Sie buchst√§blich noch einen Schritt bis zum Anfang des Projekts.  Sie k√∂nnen CI / CD hinzuf√ºgen. Hier ist ein Beispiel aus unserer Pipeline: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3d3/982/b23/3d3982b23d9ff4f1d413cee86c601d8d.png"><br><br>  Hier startete der Programmierer den neuen Code in den Hauptzweig, wonach das Docker-Image mit unseren Backend-Daemons automatisch erfasst und die Tests ausgef√ºhrt werden.  Wenn alle H√§kchen gr√ºn sind, wird die Anwendung in die Testumgebung gegossen.  Wenn es keine Probleme gibt, k√∂nnen Sie das Bild problemlos in Betrieb nehmen. <br><br><h1>  Fazit </h1><br>  In meinem Artikel habe ich einige Aspekte der Arbeit eines hoch ausgelasteten Dienstes mit einer GPU angesprochen.  Wir haben √ºber M√∂glichkeiten gesprochen, die Reaktionszeit eines Dienstes zu verk√ºrzen, wie zum Beispiel: <br><br><ul><li>  Auswahl der optimalen neuronalen Netzwerkarchitektur zur Reduzierung der Latenz; </li><li>  Anwendungen zur Optimierung von Frameworks wie TensorRT. </li></ul><br>  Wir haben die Probleme der Steigerung des Durchsatzes angesprochen: <br><br><ul><li>  die Verwendung von Bildstapeln; </li><li>  Anwenden einer Strategie mit fester Latenz, so dass die Anzahl der Inferenzl√§ufe verringert wird, aber jede Inferenz eine gr√∂√üere Anzahl von Bildern verarbeiten w√ºrde; </li><li>  Optimierung der Dateneingabepipeline zur Minimierung von GPU-Ausfallzeiten; </li><li>  "Kampf" mit Prozessor-Trab, Entfernung von CPU-gebundenen Operationen auf andere Server. </li></ul><br>  Wir haben uns den Prozess der Bereitstellung einer Anwendung mit einer GPU angesehen: <br><br><ul><li>  Verwenden von nvidia-docker in Kubernetes </li><li>  Skalierung basierend auf der Anzahl der Anforderungen und HPA (horizontaler Pod-Autoscaler). </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de472928/">https://habr.com/ru/post/de472928/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de472914/index.html">Wolfram Function Repository: Open Access-Plattform f√ºr Wolfram-Spracherweiterungen</a></li>
<li><a href="../de472916/index.html">Backend, maschinelles Lernen und Serverless sind die interessantesten auf der Juli-Habr-Konferenz</a></li>
<li><a href="../de472918/index.html">ZX Spectrum in Russland und der GUS: Wie sich das Streben nach Online offline ver√§ndert hat</a></li>
<li><a href="../de472922/index.html">Defender-Programmierer st√§rker als Entropie</a></li>
<li><a href="../de472926/index.html">Das Gesetz der Beschleunigung der Rendite (Teil 1)</a></li>
<li><a href="../de472930/index.html">Silicon Valley Astrophysiker Quantifizierung der Mode</a></li>
<li><a href="../de472932/index.html">IntelliJ IDEA Static Analysis vs Human Mind</a></li>
<li><a href="../de472934/index.html">Was ist ein Zero Trust? Sicherheitsmodell</a></li>
<li><a href="../de472936/index.html">Operation TA505: Gruppieren der Netzwerkinfrastruktur. Teil 3</a></li>
<li><a href="../de472940/index.html">√úber Betr√ºger und Menschen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>