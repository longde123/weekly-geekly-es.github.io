<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üöµ üò∞ üòú Le fer n'√©chouera pas. Comment je pr√©pare des dizaines de serveurs par jour pour la bataille üè§ üç° üßó</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La v√©rification d'un serveur n'est pas un probl√®me. Vous prenez la liste de contr√¥le et v√©rifiez dans l'ordre: processeur, m√©moire, disques. Mais avec...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Le fer n'√©chouera pas. Comment je pr√©pare des dizaines de serveurs par jour pour la bataille</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/426733/"> La v√©rification d'un serveur n'est pas un probl√®me.  Vous prenez la liste de contr√¥le et v√©rifiez dans l'ordre: processeur, m√©moire, disques.  Mais avec une centaine de serveurs, il est peu probable que cette m√©thode fonctionne bien.  Pour exclure le facteur humain, pour rendre les contr√¥les plus fiables et plus rapides, il est n√©cessaire d'automatiser le processus.  Qui a besoin de savoir comment faire mieux qu'un fournisseur d'h√©bergement?  Artyom Artemyev sur HighLoad ++ Siberia a expliqu√© quelles m√©thodes peuvent √™tre utilis√©es, ce qui est pr√©f√©rable d'ex√©cuter avec vos mains et ce qui fonctionne bien pour automatiser.  En outre, une version texte du rapport avec des conseils que toute personne qui travaille avec du fer et doit v√©rifier r√©guli√®rement ses performances peut r√©p√©ter. <br><br><img src="https://habrastorage.org/webt/eg/iq/xc/egiqxczvizqa8elpiks7967fg3y.png"><br><br>  <strong>A propos du conf√©rencier:</strong> Artyom Artemyev ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">artemirk</a> ) directeur technique d'un grand h√©bergeur FirstVDS, il travaille avec le fer. <br><a name="habracut"></a><br>  FirstVDS dispose de deux centres de donn√©es.  Le premier est le leur, ils ont construit leur propre b√¢timent, apport√© et install√© leurs racks, ils entretiennent eux-m√™mes, s'inqui√®tent du courant et du refroidissement du datacenter.  Le deuxi√®me centre de donn√©es est une grande salle dans un grand centre de donn√©es lou√©, tout est plus facile avec lui, mais il existe aussi.  Au total, c'est 60 racks et environ 3000 serveurs de fer.  Il y avait quelque chose pour se former et tester diff√©rentes approches, ce qui signifie que nous attendons des recommandations pratiquement confirm√©es.  Commen√ßons par consulter ou lire le rapport. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/eQjNQ2RnjUY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Il y a environ 6 √† 7 ans, nous avons r√©alis√© que le simple fait de mettre le syst√®me d'exploitation sur le serveur ne suffisait pas.  L'OS est allum√©, le serveur est r√©veill√© et pr√™t pour la bataille.  Nous le lan√ßons en production - des red√©marrages et des gels incompr√©hensibles commencent.  Que faire, ce n'est pas clair - le processus est en cours, le transfert de l'ensemble du projet de travail vers une nouvelle pi√®ce de m√©tal est difficile, co√ªteux et douloureux.  O√π courir? <br><br>  Les m√©thodes de d√©ploiement modernes nous permettent d'√©viter cela et de transporter le serveur en 5 secondes, mais nos clients (surtout il y a 6 ans) ne volaient tout simplement pas dans les nuages, marchaient au sol et utilisaient des morceaux de fer ordinaires. <br><br><img src="https://habrastorage.org/webt/li/kc/fa/likcfazjalyfvnjz_vq17bqqjpm.png"><br><br>  Dans cet article, je vais vous dire quelles m√©thodes nous avons essay√©es, lesquelles nous avons pris racine, lesquelles n'ont pas pris racine, lesquelles sont bonnes √† ex√©cuter avec vos mains et comment automatiser tout cela.  Je vais vous donner des conseils, et vous pouvez le r√©p√©ter dans votre entreprise si vous travaillez avec du fer et que vous avez un tel besoin. <br><br><h2>  Quel est le probl√®me? <br></h2><br>  En th√©orie, la v√©rification du serveur n'est pas un probl√®me.  Au d√©part, nous avions un processus, comme dans l'image ci-dessous.  Un homme s'assoit, prend une liste de contr√¥le, v√©rifie: processeur, m√©moire, disques, se froisse le front, prend une d√©cision. <br><br><img src="https://habrastorage.org/webt/bq/75/ac/bq75acnj5b9-wk-mhonnk06vtbc.png"><br><br>  Ensuite, 3 serveurs ont √©t√© install√©s par mois.  Mais, quand il y a de plus en plus de serveurs, cette personne commence √† pleurer et √† se plaindre de sa mort au travail.  Une personne se trompe de plus en plus, car la v√©rification est devenue une routine. <br><br>  <strong>Nous avons pris une d√©cision: nous automatisons!</strong>  Une personne fera des choses plus utiles. <br><br><h3>  Petite excursion <br></h3><br><img src="https://habrastorage.org/webt/yp/xx/ar/ypxxarfkx9huiayenmyjsiauh2g.png"><br><br>  Je vais clarifier ce que je veux dire quand je parle du serveur aujourd'hui.  Nous, comme tout le monde, √©conomisons de l'espace en rack et utilisons des serveurs haute densit√©.  Aujourd'hui, il s'agit de 2 unit√©s, qui peuvent accueillir soit 12 n≈ìuds de serveurs √† processeur unique, soit 4 n≈ìuds de serveurs √† double processeur.  Autrement dit, chaque serveur re√ßoit 4 disques - tout honn√™tement.  De plus, il y a deux alimentations dans le rack, c'est-√†-dire que tout est redondant et que tout le monde l'aime. <br><br><h2>  D'o√π vient le fer? <br></h2><br>  Le fer est apport√© √† notre centre de donn√©es par nos fournisseurs - g√©n√©ralement Supermicro et Intel.  Dans le centre de donn√©es, nos gars-op√©rateurs installent les serveurs dans un espace vide dans le rack et connectent deux c√¢bles, un r√©seau et l'alimentation.  Il est √©galement de la responsabilit√© des op√©rateurs de configurer le BIOS sur le serveur.  Autrement dit, connectez le clavier, le moniteur et configurez deux param√®tres: <code>Restore on AC/Power Loss ‚Äî [Power On]</code> , de sorte que le serveur s'allume toujours d√®s que l'alimentation appara√Æt.  Cela devrait fonctionner sans arr√™t.  Le deuxi√®me <code>First boot device ‚Äî [PXE]</code> , c'est-√†-dire que nous avons mis le premier p√©riph√©rique de d√©marrage sur le r√©seau, sinon nous ne pourrons pas atteindre le serveur, car ce n'est pas un fait qu'il dispose de disques imm√©diatement, etc. <br><br><img src="https://habrastorage.org/webt/ab/hf/i2/abhfi2f6a412ead4dsnxyjjmhe8.png"><br><br>  Apr√®s cela, l'op√©rateur ouvre le panneau de comptabilit√© des serveurs de fer, dans lequel vous devez enregistrer le fait d'installer le serveur, pour lequel il est indiqu√©: <br><br><ul><li>  rack; </li><li>  autocollant </li><li>  ports r√©seau </li><li>  ports d'alimentation </li><li>  num√©ro d'unit√©. </li></ul><br>  Apr√®s cela, le port r√©seau sur lequel l'op√©rateur a install√© le nouveau serveur, pour des raisons de s√©curit√©, va vers un VLAN de quarantaine sp√©cial, qui contient √©galement DHCP, Pxe, TFtp.  Ensuite, le serveur charge notre Linux pr√©f√©r√©, qui poss√®de tous les utilitaires n√©cessaires, et le processus de diagnostic d√©marre. <br><br>  √âtant donn√© que le serveur poss√®de toujours le premier p√©riph√©rique de d√©marrage sur le r√©seau, pour les serveurs qui entrent en production, le port bascule vers un autre VLAN.  Il n'y a pas de DHCP dans un autre VLAN, et nous n'avons pas peur de r√©installer accidentellement notre serveur de production.  Pour cela, nous avons un VLAN s√©par√©. <br><br>  Il arrive que le serveur ait √©t√© install√©, tout va bien, mais il n'a pas d√©marr√© dans le syst√®me de diagnostic.  Cela se produit, en r√®gle g√©n√©rale, du fait qu'avec un retard dans la commutation des VLAN, tous les commutateurs r√©seau ne commutent pas rapidement les VLAN, etc. <br><br><img src="https://habrastorage.org/webt/q0/qa/wl/q0qawlis6oq5tykqjri3hif8bwg.png"><br><br>  Ensuite, l'op√©rateur re√ßoit la t√¢che de red√©marrer le serveur avec ses mains.  Auparavant, il n'y avait pas d'IPMI, nous avons configur√© des sockets distants et fix√© sur quel port les sockets du serveur, tir√© la socket sur le r√©seau et le serveur a red√©marr√©. <br><br>  Mais les points de vente g√©r√©s ne fonctionnent pas toujours bien, nous g√©rons donc d√©sormais l'alimentation du serveur via IPMI.  Mais lorsque le serveur est nouveau, IPMI n'est pas configur√©, il ne peut √™tre red√©marr√© qu'en montant et en appuyant sur le bouton.  Par cons√©quent, un homme s'assoit, attend - la lumi√®re s'allume - court et appuie sur le bouton.  Tel est son travail. <br><br>  Si apr√®s cela, le serveur n'a pas d√©marr√©, il est inscrit dans une liste sp√©ciale pour r√©paration.  Cette liste comprend les serveurs sur lesquels les diagnostics n'ont pas d√©marr√© ou ses r√©sultats n'√©taient pas satisfaisants.  Une personne individuelle - qui aime le fer - s'assoit et se d√©sassemble tous les jours - recueille, regarde, pourquoi ne fonctionne pas. <br><br><h2>  CPU <br></h2><br>  Tout va bien, le serveur a d√©marr√©, nous commen√ßons √† tester.  Nous testons d'abord le processeur comme l'un des √©l√©ments les plus importants. <br><br><img src="https://habrastorage.org/webt/8k/6m/n0/8k6mn0cwmprfgrqoanohbc1lyyo.png"><br><br>  La premi√®re impulsion a √©t√© d'utiliser l'application du fournisseur.  Nous avons presque tous les processeurs Intel - ils sont all√©s sur le site, ont t√©l√©charg√© l'outil de diagnostic du processeur Intel - tout va bien, il montre beaucoup d'informations int√©ressantes, y compris les heures de fonctionnement du serveur en heures et le graphique de la consommation d'√©nergie. <br><br>  Mais le probl√®me est qu'Intel PTD fonctionne sous Windows, ce que nous n'aimions plus.  Pour commencer un test, il vous suffit de d√©placer la souris, d'appuyer sur le bouton "START" et le test commencera.  Le r√©sultat est affich√© √† l'√©cran, mais il n'y a aucun moyen de l'exporter n'importe o√π.  Cela ne nous convient pas, car le processus n'est pas automatis√©. <br><br><img src="https://habrastorage.org/webt/ax/zo/t1/axzot1gbcsfefqbkta32bixrxk4.png"><br><br>  Nous sommes all√©s lire les forums et avons trouv√© les deux moyens les plus simples. <br><br><ol><li>  <strong>La boucle √©ternelle cat / dev / zero&gt; / dev / null</strong> .  Vous pouvez vous enregistrer en haut - 100% d'un c≈ìur est consomm√©.  Nous comptons le nombre de c≈ìurs, ex√©cutons le nombre requis de cat / dev / z√©ro, multipli√© par le nombre de c≈ìurs souhait√©.  Tout fonctionne tr√®s bien! <br></li><li>  <strong>Utilitaire / bac / stress</strong> .  Elle construit des matrices en m√©moire et commence √† les retourner constamment.  Tout va bien aussi - le processeur chauffe, il y a une charge. <br></li></ol><br>  Nous donnons les serveurs en production, les utilisateurs reviennent et disent que le processeur est instable.  V√©rifi√© - le processeur est instable.  Ils ont commenc√© √† enqu√™ter, ont pris un serveur qui r√©ussit les v√©rifications, mais il se bloque lors d'une bataille, a activ√© le noyau de d√©bogage sous Linux et collect√© le vidage de m√©moire.  Le serveur avant le red√©marrage vide dans le fichier tout ce qui √©tait en m√©moire avant le crash. <br><br><img src="https://habrastorage.org/webt/i1/iu/z0/i1iuz0zoafq9lm8aaimgyqzyiic.png"><br><br>  Diverses optimisations sont int√©gr√©es dans les processeurs pour les op√©rations fr√©quentes.  Nous pouvons voir des indicateurs refl√©tant les optimisations prises en charge par le processeur, par exemple, les optimisations pour travailler avec des nombres √† virgule flottante, les optimisations multim√©dias, etc.  Mais notre / bin / stress et le cycle √©ternel ne font que graver le processeur en une seule op√©ration et n'utilisent pas de fonctionnalit√©s suppl√©mentaires.  L'enqu√™te a montr√© que le processeur se bloque lors de l'utilisation de la fonctionnalit√© de l'un des drapeaux int√©gr√©s. <br><br>  La premi√®re impulsion a √©t√© de laisser / bin / stress - laisser le processeur se r√©chauffer.  Ensuite, dans un cycle, nous parcourons tous les drapeaux, les tirons.  En r√©fl√©chissant √† la fa√ßon de mettre en ≈ìuvre ceci, qui commande d'appeler afin d'appeler les fonctions de chaque indicateur, nous lisons les forums. <br><br>  Au forum des overclockers, nous sommes tomb√©s sur un projet int√©ressant de recherche de nombres premiers <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Great Internet Mersenne Prime Search</a> .  Les scientifiques ont cr√©√© un r√©seau distribu√© auquel tout le monde peut se connecter et aider √† trouver un nombre premier.  Les scientifiques ne croient personne, donc le programme fonctionne tr√®s intelligemment: d'abord vous l'ex√©cutez, il calcule les nombres premiers qu'il conna√Æt d√©j√† et compare le r√©sultat avec ce qu'il sait.  Si le r√©sultat ne correspond pas, le processeur ne fonctionne pas correctement.  Nous avons vraiment aim√© cette propri√©t√©: avec n'importe quel non-sens, elle est sujette √† tomber. <br><br>  De plus, le but du projet est de trouver autant de nombres premiers que possible, donc le programme est constamment optimis√© pour les propri√©t√©s des nouveaux processeurs, en cons√©quence il tire beaucoup de drapeaux. <br><br>  Mprime n'a pas de limite de temps, s'il n'est pas arr√™t√©, il fonctionne pour toujours.  Nous l'ex√©cutons pendant 30 minutes. <br><br><pre> <code class="hljs powershell">/usr/bin/timeout <span class="hljs-number"><span class="hljs-number">30</span></span>m /opt/mprime <span class="hljs-literal"><span class="hljs-literal">-t</span></span> /bin/grep <span class="hljs-literal"><span class="hljs-literal">-i</span></span> error /root/result.txt</code> </pre><br>  Apr√®s avoir termin√© le travail, nous v√©rifions qu'il n'y a pas d'erreurs dans result.txt, et regardons les journaux du noyau, en particulier, dans le fichier / proc / kmsg, nous recherchons des erreurs. <br><br><h3>  Une autre excursion <br></h3><br><img src="https://habrastorage.org/webt/es/ic/kj/esickjc00tooyxznjt_6lyumimg.png"><br><br>  Le 3 janvier 2018, ils ont trouv√© le 50e nombre premier de Mersenne (2 <sup>p</sup> -1).  De ce nombre, seulement 23 millions de chiffres.  Vous pouvez le t√©l√©charger pour le voir - <a href="">il</a> s'agit d' <a href="">une archive zip de 12 Mo.</a> <br><br>  Pourquoi avons-nous besoin de nombres premiers?  Tout d'abord, tout chiffrement RSA utilise des nombres premiers.  Plus nous en savons, plus votre cl√© SSH est fiable.  Deuxi√®mement, les scientifiques testent leurs hypoth√®ses et leurs th√©or√®mes math√©matiques, et cela ne nous d√©range pas d'aider les scientifiques - cela ne nous co√ªte rien.  Il se r√©v√®le une histoire gagnant-gagnant. <br><br><img src="https://habrastorage.org/webt/v0/nd/cl/v0ndcljwlkwops-dtp9nwk9bsru.png"><br><br>  Donc, le processeur fonctionne, tout va bien.  Reste √† savoir de quel type de processeur il s'agit.  Nous utilisons le processeur dmidecode -t et voyons tous les emplacements qui se trouvent sur la carte m√®re, et quels processeurs se trouvent dans ces emplacements.  Ces informations entrent dans notre syst√®me comptable, nous les interpr√©terons plus tard. <br><br><h3>  Attraper <br></h3><br>  Ainsi, √©tonnamment, des jambes cass√©es peuvent √™tre trouv√©es.  / bin / stress et le cycle perp√©tuel ont fonctionn√©, et Mprime est tomb√©.  Ils ont roul√© longtemps, cherch√©, d√©couvert - le r√©sultat dans l'image ci-dessous - tout est clair ici. <br><br><img src="https://habrastorage.org/webt/13/ae/gc/13aegcfaw9ozlrzzzgog3povm5w.jpeg"><br><br>  Un tel processeur n'a tout simplement pas d√©marr√©.  L'op√©rateur √©tait tr√®s fort, a pris le mauvais processeur - mais a pu livrer. <br><br><img src="https://habrastorage.org/webt/gj/xr/_w/gjxr_wu8lxf_cgsf4ehxs84-yb8.jpeg"><br><br>  Encore une belle affaire.  La rang√©e noire sur la photo ci-dessous repr√©sente les fans, la fl√®che montre comment l'air souffle.  Nous voyons: le radiateur se trouve √† travers le ruisseau.  Bien s√ªr, tout a surchauff√© et s'est √©teint. <br><br><img src="https://habrastorage.org/webt/oh/-c/hp/oh-chp3arnnkslirn8elylfcfww.jpeg"><br><br><h2>  La m√©moire <br></h2><br>  Avec la m√©moire, tout est assez simple.  Ce sont des cellules dans lesquelles nous √©crivons des informations, et apr√®s un certain temps nous les relisons.  S'il reste le m√™me que nous avons not√©, alors cette cellule fonctionne. <br><br><img src="https://habrastorage.org/webt/91/gl/_s/91gl_slwgtws8webfnnjxfajui8.png"><br><br>  Tout le monde conna√Æt le bon programme, directement classique, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Memtest86 +</a> , qui s'ex√©cute √† partir de n'importe quel support, sur le r√©seau, ou m√™me √† partir d'une disquette.  Elle est r√©alis√©e afin de v√©rifier autant de cellules m√©moire que possible.  Les cellules occup√©es ne peuvent plus √™tre v√©rifi√©es.  Memtest86 + a donc une taille minimale pour ne pas occuper de m√©moire.  Malheureusement, <strong>memtest86 + n'affiche que ses statistiques √† l'√©cran</strong> .  Nous avons essay√© de l'√©tendre d'une mani√®re ou d'une autre, mais tout cela est d√ª au fait qu'√† l'int√©rieur du programme, il n'y avait m√™me pas de pile r√©seau.  Pour l'√©tendre, il faudrait apporter le noyau Linux et tout le reste. <br><br>  Il existe une version payante de ce programme qui sait d√©j√† comment d√©poser des informations sur le disque.  Mais nos serveurs n'ont pas toujours de disque, et il n'y a pas toujours de syst√®me de fichiers sur ces disques.  Mais le lecteur r√©seau, comme nous l'avons d√©j√† d√©couvert, ne peut pas √™tre connect√©. <br><br>  Nous avons commenc√© √† creuser plus loin et avons trouv√© un programme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Memtester</a> similaire.  Ce programme fonctionne √† partir du niveau OS de Linux.  Son plus gros inconv√©nient est que le syst√®me d'exploitation lui-m√™me et Memtester occupent certaines cellules de m√©moire, et ces cellules ne seront pas v√©rifi√©es. <br><br>  Memtester est d√©marr√© avec la commande: memtester `cat / proc / meminfo | grep MemFree |  awk '{print $ 2-1024}' 'k 5 <br><br>  Ici, nous transf√©rons la quantit√© de m√©moire libre moins 1 Mo.  Cela est fait, car sinon le Memtester prend toute la m√©moire et le tueur de duvet la tue.  Nous effectuons ce test pendant 5 cycles, √† la sortie, nous avons une plaque avec OK ou √©chec. <br><br><table width="300"><tbody><tr><td>  Adresse bloqu√©e </td><td>  ok </td></tr><tr><td>  Valeur al√©atoire </td><td>  ok </td></tr><tr><td>  Comparez XOR </td><td>  ok </td></tr><tr><td>  Comparez SUB </td><td>  ok </td></tr><tr><td>  Comparez MUL </td><td>  ok </td></tr><tr><td>  Comparez DIV </td><td>  ok </td></tr><tr><td>  Comparer OU </td><td>  ok </td></tr><tr><td>  Comparer ET </td><td>  ok </td></tr></tbody></table><br><br>  Nous enregistrons le r√©sultat final et analysons davantage les √©checs. <br><br><img src="https://habrastorage.org/webt/ah/55/kt/ah55kt9fxuqwkmqsz8-dozgunic.png"><br><br>  Pour comprendre l'√©tendue du probl√®me - notre plus petit serveur a 32 Go de m√©moire, notre image Linux avec Memtester prend 60 Mo, <strong>nous ne v√©rifions pas 2% de la m√©moire</strong> .  Mais selon les statistiques des 6 derni√®res ann√©es, il n'y a rien eu de tel que la m√©moire franchement battue soit entr√©e en production.  C'est le compromis que nous acceptons, et qu'il nous co√ªte cher de fixer, et nous vivons avec. <br><br>  En cours de route, nous collectons √©galement la m√©moire dmidecode -t, qui donne toutes les banques de m√©moire que nous avons sur la carte m√®re (g√©n√©ralement jusqu'√† 24 pi√®ces), et quels d√©s se trouvent dans chaque banque.  Ces informations sont utiles si nous voulons mettre √† niveau le serveur - nous saurons o√π ajouter quoi, combien de bandes √† prendre et √† quel serveur aller. <br><br><h2>  P√©riph√©riques de stockage <br></h2><br>  Il y a 6 ans, tous les disques √©taient avec des cr√™pes qui tournaient.  Une autre histoire consistait √† dresser une liste de tous les disques.  Il y avait plusieurs approches diff√©rentes, car on ne pensait pas que vous pouviez simplement regarder ls / dev / sd.  Mais √† la fin, nous avons cess√© de regarder ls / dev / sd * et ls / dev / cciss / c0d *.  Dans le premier cas, il s'agit d'un p√©riph√©rique SATA, dans le second - SCSI et SAS. <br><br><img src="https://habrastorage.org/webt/e7/gm/br/e7gmbraujo_ko4qmfanfwnru71y.png"><br><br>  Litt√©ralement cette ann√©e, ils ont commenc√© √† vendre des disques nvme et ont ajout√© la liste nvme ici. <br><br>  Une fois la liste des disques compil√©e, nous essayons de lire 0 octet pour comprendre qu'il s'agit d'un p√©riph√©rique bloc et que tout va bien.  Si vous ne pouviez pas le lire, alors nous pensons que c'est une sorte de fant√¥me, et nous n'avons pas et n'avons jamais eu un tel disque. <br><br>  La premi√®re approche de la v√©rification des disques √©tait √©vidente: ¬´ <code>dd -o nocache -o direct if=/dev/urandom of=${disk}</code> des donn√©es al√©atoires sur le disque et voyons la vitesse¬ª - <code>dd -o nocache -o direct if=/dev/urandom of=${disk}</code> .  En r√®gle g√©n√©rale, les disques √† cr√™pes fournissent 130-150 Mb / s.  Nous avons pliss√© les yeux et d√©cid√© pour nous-m√™mes que 90 Mo / s est le nombre apr√®s lequel il y a des disques r√©parables, tout ce qui est plus petit est d√©fectueux. <br><br>  Mais encore une fois, les utilisateurs ont commenc√© √† revenir et √† dire que les lecteurs √©taient mauvais.  Il s'est av√©r√© que la physique insidieuse plaisantait √† nouveau avec nous. <br><br><img src="https://habrastorage.org/webt/m_/uh/tf/m_uhtfarwgyskpslnxxkufxynty.png"><br><br>  Il y a une vitesse angulaire et, en r√®gle g√©n√©rale, lorsque vous ex√©cutez -dd, il √©crit pr√®s de la broche.  Si, pour une raison quelconque, la vitesse de rotation de la broche s'est d√©grad√©e, cela est moins visible que si vous √©crivez √† partir du bord du disque. <br><br>  J'ai d√ª changer le principe de la v√©rification.  Maintenant, nous v√©rifions √† trois endroits: pr√®s de la broche, au milieu et √† l'ext√©rieur.  Probablement, il ne peut √™tre v√©rifi√© que de l'ext√©rieur, mais c'est ainsi que cela s'est produit historiquement.  Et ce qui fonctionne, ne touchez pas. <br><br>  Vous pouvez utiliser <strong>smartctl</strong> pour demander au disque comment il fonctionne.  Nous pensons qu'une bonne conduite: <br><br><ul><li>  Il n'y a pas de secteurs r√©allou√©s (nombre de secteurs <strong>r√©allou√©s = 0)</strong> , c'est-√†-dire tous les secteurs qui ont quitt√© l'usine. </li><li>  Nous <strong>n'utilisons pas de disques de plus de 4 ans</strong> , bien qu'ils fonctionnent assez bien.  Avant d'introduire cette pratique, nous avions des disques depuis 7 ans.  Nous pensons maintenant qu'apr√®s 4 ans, le disque a pay√©, et nous ne sommes pas pr√™ts √† accepter le risque d'usure. </li><li>  Aucun secteur ne va √™tre r√©affect√© ( <strong>Current_Pending_Sector = 0</strong> ). </li><li>  <strong>Nombre d'erreurs UltraDMA CRC = 0</strong> - ce sont des erreurs sur le c√¢ble SATA.  S'il y a une erreur, il vous suffit de changer le fil, vous n'avez pas besoin de changer le disque. </li></ul><br><img src="https://habrastorage.org/webt/01/jy/48/01jy48l5jyw-x1yx7wse-yawhdq.png"><br><br>  Les SSD distribu√©s sont g√©n√©ralement d'excellents disques, ils fonctionnent rapidement, ne font pas de bruit, ne chauffent pas.  Nous pensons qu'un bon SSD a une vitesse d'√©criture de plus de 200 Mo / s.  Nos clients adorent les prix bas, et les mod√®les de serveurs qui √©mettent 320-350 Mo / s ne nous parviennent pas toujours. <br><br>  Pour les SSD, nous recherchons √©galement smartctl.  Les m√™mes r√©affect√©s, Power_On_Hours, Current_Pending_Sector.  Tous les SSD sont capables d'afficher le degr√© d'usure, il affiche le param√®tre Media_Wearout_Indicator.  Nous essuyons les disques jusqu'√† 5% de leur dur√©e de vie, puis nous les retirons.  Ces disques trouvent parfois une seconde vie dans les besoins personnels des employ√©s.  Par exemple, j'ai r√©cemment d√©couvert qu'en 2 ans, un tel disque s'√©tait √©puis√© de 1% suppl√©mentaire dans l'ordinateur portable d'un employ√©, bien que dans notre pays, le cache SSD √©puise 95% en environ 10 mois. <br><br>  Mais le probl√®me est que tous les fabricants de disques n'√©taient pas d'accord sur les noms de param√®tres, et ce Media_Wearout_Indicator, par exemple, s'appelle Percent_Lifetime_Used pour Toshiba, autre Wear Leveling Count, Percent Lifetime Remaining pour d'autres fabricants, ou tout simplement. * Wear. *. <br><br>  Crucial n'a pas du tout cette option.  Ensuite, nous consid√©rons simplement la quantit√© de r√©√©criture du disque - ¬´octet √©crit¬ª - combien d'octets nous avons d√©j√† √©crit sur ce disque.  De plus, selon les sp√©cifications, nous essayons de savoir combien de r√©√©critures ce disque est calcul√© par le fabricant.  Par les math√©matiques √©l√©mentaires, nous d√©terminons combien il vivra de plus.  S'il est temps de changer - changez. <br><br><h2>  RAID <br></h2><br>  Je ne sais pas pourquoi dans le monde moderne nos clients veulent toujours des RAID.  Les gens ach√®tent du RAID, y mettent 4 SSD, qui sont beaucoup plus rapides que ce RAID (6 Go).  Ils ont une sorte d'instruction, et ils la collectent.  Je pense que c'est une chose presque inutile. <br><br>  Il y avait auparavant 3 fabricants: Adaptec;  3ware;  Intel  Nous avions 3 utilitaires, nous d√©rangeons, mais nous avons ex√©cut√© des diagnostics pour tout le monde.  Maintenant, LSI a achet√© tout le monde - il n'y a plus qu'un seul utilitaire. <br><br>  Lorsque notre syst√®me de diagnostic d√©tecte le RAID, il analyse le volume logique sur des disques s√©par√©s afin que vous puissiez mesurer la vitesse de chaque disque et lire son Smart.  Apr√®s cela, il reste au RAID de v√©rifier la batterie.  Qui ne sait pas - il y a suffisamment de batteries sur le RAID pour faire tourner tous les disques pendant encore 2 heures.  Autrement dit, vous √©teignez le serveur, le retirez et il fait tourner le disque pendant 2 heures suppl√©mentaires pour terminer tous les enregistrements. <br><br><h2>  R√©seau <br></h2><br>  Avec le r√©seau, tout est assez simple - il devrait y avoir moins de 300 Mbit √† l'int√©rieur du centre de donn√©es.  Si moins, vous devez le r√©parer.  Nous examinons √©galement les erreurs sur l'interface.  <strong>Les erreurs sur l'interface r√©seau ne devraient pas √™tre du tout</strong> , et si elles le sont, alors tout est mauvais. <br><br><img src="https://habrastorage.org/webt/ad/m2/yw/adm2yw4vmjpztcicdovupo84l20.png"><br><br>  Nous essayons de mettre √† jour le BIOS et le firmware IPMI en cours de route.  Il s'est av√©r√© que nous n'aimons pas tous les BIOS.  Nous avons encore des BIOS qui ne savent pas comment utiliser UEFI et d'autres fonctionnalit√©s que nous utilisons.  Nous essayons de le mettre √† jour automatiquement, mais cela ne fonctionne pas toujours, tout n'est pas tr√®s simple l√†-bas.  Si cela ne fonctionne pas, la personne se met √† jour avec ses mains. <br><br>  Nous ne donnons pas IPMI Supermicro au monde, nous l'avons sur des adresses grises via OpenVPN.  N√©anmoins, nous craignons qu'un jour une autre vuln√©rabilit√© ne se manifeste et que nous en souffrions.  Par cons√©quent, nous essayons de garder le firmware IPMI toujours le dernier.  Si ce n'est pas le cas, mettez √† jour. <br><br>  D'une chose √©trange, il est r√©cemment ressorti qu'Intel sur les cartes r√©seau 10 et 40 gigabits n'inclut pas le d√©marrage PXE.  Il s'av√®re que si le serveur est dans un rack dans lequel il n'y a qu'une carte de 40 gigabits, il est impossible de d√©marrer sur le r√©seau, car vous devez d√©marrer sur une carte gigabit.  Nous flashons s√©par√©ment les cartes r√©seau sur 40G afin qu'elles aient PXE et puissent continuer √† vivre. <br><br>  <strong>Une fois tout v√©rifi√©, le serveur est imm√©diatement mis en vente</strong> .  Son prix est calcul√©, auquel il est mis sur le site et vendu. <br><br><img src="https://habrastorage.org/webt/by/ca/ah/bycaah2szao5f0-lcrkk5zcwzyo.png"><br><br>  Au total, nous effectuons environ 350 v√©rifications par mois, 69% des serveurs sont r√©parables, 31% ne le sont pas.  Cela est d√ª au fait que nous avons une histoire riche, certains serveurs existent depuis 10 ans.  La plupart des serveurs qui n'ont pas r√©ussi le test, nous venons de jeter. <br><br><blockquote>  Pour les curieux: nous avons 3 clients qui vivent toujours sur le Pentium IV et qui ne veulent partir nulle part.  Ils ont 512 Mo de RAM. <br></blockquote><br>  L'avenir est venu!  Si je devais cl√¥turer ce syst√®me aujourd'hui ... <br><br><img src="https://habrastorage.org/webt/4v/bb/e_/4vbbe_vodbx8hzm_zk-nf2oejf4.png"><br><br>  Un merveilleux utilitaire, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Hardware Lister</a> (lshw), a √©t√© publi√©, qui peut communiquer avec le noyau, afficher magnifiquement quel type de mat√©riel se trouve dans le noyau, ce que le noyau pourrait d√©tecter.  Toutes ces danses ne sont pas n√©cessaires.  Si vous r√©p√©tez - je vous conseille fortement de regarder cet utilitaire et de l'utiliser.  Tout deviendra beaucoup plus simple. <br><br><h1>  R√©sum√©: <br></h1><br><ul><li>  Le compromis n'est pas mauvais, c'est juste une question de prix.  Si la solution est tr√®s co√ªteuse, vous devez rechercher un niveau o√π la fiabilit√© et le prix sont acceptables. </li><li>  Les programmes non essentiels sont parfois int√©ressants √† tester.  Il ne reste plus qu'√† les trouver. </li><li>  Testez tout ce que vous atteignez! </li></ul><br><blockquote>  Le prochain grand <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">HighLoad ++ aura</a> d√©j√† lieu les 8 et 9 novembre √† Moscou.  Le programme comprend des sp√©cialistes c√©l√®bres et de nouveaux noms, des t√¢ches traditionnelles et nouvelles.  Dans la section DevOps, par exemple, les √©l√©ments suivants sont d√©j√† accept√©s: <br><br><ul><li>  David O'Brien (Xirus), qui discutera de l'√©ternel - ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Metrics!</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Mesures!</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Mesures!</a>  " <br></li><li>  Vladimir Kolobaev (Avito) avec un rapport d'appel ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Monitoring - aux d√©veloppeurs!</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">La technologie √† la communaut√©!</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">B√©n√©fice - pour tout le monde!</a>  " <br></li><li>  Elena Grahovac (N26) pr√©sentera les ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Meilleures pratiques pour les services cloud natifs</a> ¬ª. <br></li></ul><br>  √âtudiez la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">liste des</a> rapports et d√©p√™chez-vous de vous joindre.  Ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">abonnez-vous</a> √† notre newsletter et vous recevrez des critiques r√©guli√®res des rapports, des rapports sur les nouveaux articles et vid√©os. <br></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr426733/">https://habr.com/ru/post/fr426733/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr426723/index.html">Microsoft et ses partenaires esp√®rent cr√©er une capsule temporelle sur la lune</a></li>
<li><a href="../fr426725/index.html">Comment faire des choses quand vous n‚Äôavez pas envie de les faire</a></li>
<li><a href="../fr426727/index.html">EME? Cdm? DRM? CENC? IDK! Ce dont vous avez besoin pour cr√©er votre propre lecteur vid√©o dans un navigateur</a></li>
<li><a href="../fr426729/index.html">√âcole de magie TypeScript: g√©n√©riques et extension de type</a></li>
<li><a href="../fr426731/index.html">CSS: caract√©ristiques int√©ressantes de border-radius</a></li>
<li><a href="../fr426735/index.html">Bienvenue au JETHACK Hackathon</a></li>
<li><a href="../fr426737/index.html">En bref sur l'architecture des processeurs neuromorphiques: un regard int√©rieur</a></li>
<li><a href="../fr426739/index.html">Fichiers proxy d'AWS S3 utilisant nginx</a></li>
<li><a href="../fr426741/index.html">Un m√©mo pour ceux qui envisagent de recruter des stagiaires pour la premi√®re fois</a></li>
<li><a href="../fr426743/index.html">Les fa√ßons d'utiliser la blockchain ont tourn√© ailleurs. Sony annonce un syst√®me DRM bas√© sur la blockchain</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>