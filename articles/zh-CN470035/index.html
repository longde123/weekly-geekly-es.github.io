<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘‡ğŸ¿ ğŸ“ˆ ğŸ˜‹ å¦„æƒ³ç”Ÿæˆå™¨ï¼šä½¿ç”¨ç¥ç»ç½‘ç»œä»¥ä»»ä½•è¯­è¨€åˆ›å»ºæ–‡æœ¬ ğŸ¤°ğŸ¼ ğŸ•‰ï¸ ğŸ™ğŸ½</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="å“ˆHa 

 æœ¬æ–‡å°†é‡‡ç”¨â€œæ˜ŸæœŸäº”â€æ ¼å¼ï¼Œä»Šå¤©æˆ‘ä»¬å°†ä»‹ç»NLPã€‚ NLPä¸æ˜¯å…³äºåœ°ä¸‹é€šé“å‡ºå”®å“ªäº›ä¹¦ç±çš„ä¹¦ï¼Œè€Œæ˜¯æœ‰å…³è‡ªç„¶è¯­è¨€å¤„ç†æ­£åœ¨å¤„ç†è‡ªç„¶è¯­è¨€çš„ä¹¦ã€‚ ä½œä¸ºè¿™ç§å¤„ç†çš„ç¤ºä¾‹ï¼Œå°†ä½¿ç”¨ä½¿ç”¨ç¥ç»ç½‘ç»œçš„æ–‡æœ¬ç”Ÿæˆã€‚ æˆ‘ä»¬å¯ä»¥åˆ›å»ºä»»ä½•è¯­è¨€çš„æ–‡æœ¬ï¼Œä»ä¿„è¯­æˆ–è‹±è¯­åˆ°C ++ã€‚ ç»“æœéå¸¸æœ‰è¶£ï¼Œæ‚¨å¯èƒ½å¯ä»¥ä»å›¾ç‰‡ä¸­çŒœæµ‹å‡ºæ¥ã€‚ 
...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>å¦„æƒ³ç”Ÿæˆå™¨ï¼šä½¿ç”¨ç¥ç»ç½‘ç»œä»¥ä»»ä½•è¯­è¨€åˆ›å»ºæ–‡æœ¬</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/470035/"> å“ˆHa <br><br> æœ¬æ–‡å°†é‡‡ç”¨â€œæ˜ŸæœŸäº”â€æ ¼å¼ï¼Œä»Šå¤©æˆ‘ä»¬å°†ä»‹ç»NLPã€‚  NLPä¸æ˜¯å…³äºåœ°ä¸‹é€šé“å‡ºå”®å“ªäº›ä¹¦ç±çš„ä¹¦ï¼Œè€Œæ˜¯æœ‰å…³<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">è‡ªç„¶è¯­è¨€å¤„ç†</a>æ­£åœ¨å¤„ç†è‡ªç„¶è¯­è¨€çš„ä¹¦ã€‚ ä½œä¸ºè¿™ç§å¤„ç†çš„ç¤ºä¾‹ï¼Œå°†ä½¿ç”¨ä½¿ç”¨ç¥ç»ç½‘ç»œçš„æ–‡æœ¬ç”Ÿæˆã€‚ æˆ‘ä»¬å¯ä»¥åˆ›å»ºä»»ä½•è¯­è¨€çš„æ–‡æœ¬ï¼Œä»ä¿„è¯­æˆ–è‹±è¯­åˆ°C ++ã€‚ ç»“æœéå¸¸æœ‰è¶£ï¼Œæ‚¨å¯èƒ½å¯ä»¥ä»å›¾ç‰‡ä¸­çŒœæµ‹å‡ºæ¥ã€‚ <br><br><img src="https://habrastorage.org/webt/vc/cy/we/vccywe4c6r0vbryvvx3qiale_j8.jpeg"><br><br> å¯¹äºé‚£äº›å¯¹å‘ç”Ÿçš„äº‹æƒ…æ„Ÿå…´è¶£çš„äººï¼Œå…¶ç»“æœå’Œæºä»£ç å·²è¢«å‰Šå‡ã€‚ <br><a name="habracut"></a><br><h2> èµ„æ–™å‡†å¤‡ </h2><br> ä¸ºäº†è¿›è¡Œå¤„ç†ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ç±»ç‰¹æ®Šçš„ç¥ç»ç½‘ç»œ-æ‰€è°“çš„é€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ã€‚ è¯¥ç½‘ç»œä¸å¸¸è§„ç½‘ç»œçš„ä¸åŒä¹‹å¤„åœ¨äºï¼Œé™¤äº†å¸¸è§„å•å…ƒä¹‹å¤–ï¼Œå®ƒè¿˜å…·æœ‰å­˜å‚¨å•å…ƒã€‚ è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ†ææ›´å¤æ‚çš„ç»“æ„çš„æ•°æ®ï¼Œå®é™…ä¸Šæ˜¯æ›´æ¥è¿‘äººç±»è®°å¿†çš„æ•°æ®ï¼Œå› ä¸ºæˆ‘ä»¬ä¹Ÿä¸æ˜¯â€œä»å¤´å¼€å§‹â€ã€‚ ç¼–å†™ä»£ç æ—¶ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">LSTM</a> ï¼ˆé•¿æœŸçŸ­æœŸè®°å¿†ï¼‰ç½‘ç»œï¼Œå› ä¸ºKeraså·²æ”¯æŒå®ƒä»¬ã€‚ <br><br><img src="https://habrastorage.org/webt/ay/ox/ou/ayoxourylcbidznetfkphgdv5ni.jpeg"><br><br> å®é™…ä¸Šï¼Œä¸‹ä¸€ä¸ªéœ€è¦è§£å†³çš„é—®é¢˜æ˜¯å¤„ç†æ–‡æœ¬ã€‚ è¿™é‡Œæœ‰ä¸¤ç§æ–¹æ³•-å°†ç¬¦å·æˆ–æ•´ä¸ªå•è¯æäº¤ç»™è¾“å…¥ã€‚ ç¬¬ä¸€ç§æ–¹æ³•çš„åŸç†å¾ˆç®€å•ï¼šå°†æ–‡æœ¬åˆ†ä¸ºå¤šä¸ªçŸ­å—ï¼Œå…¶ä¸­â€œè¾“å…¥â€æ˜¯ä¸€æ®µæ–‡æœ¬ï¼Œè€Œâ€œè¾“å‡ºâ€æ˜¯ä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚ ä¾‹å¦‚ï¼Œå¯¹äºæœ€åä¸€ä¸ªçŸ­è¯­ï¼Œâ€œè¾“å…¥æ˜¯ä¸€æ®µæ–‡æœ¬â€ï¼š <br><br> <code>input:    output: "" <br> input:    : output: "" <br> input:    : output:"" <br> input:    : output: "" <br> input:    : output: "". <br></code> <br> ä¾æ­¤ç±»æ¨ã€‚ å› æ­¤ï¼Œç¥ç»ç½‘ç»œåœ¨è¾“å…¥å¤„æ¥æ”¶æ–‡æœ¬ç‰‡æ®µï¼Œå¹¶åœ¨è¾“å‡ºå¤„æ¥æ”¶åº”å½¢æˆçš„å­—ç¬¦ã€‚ <br><br> ç¬¬äºŒç§æ–¹æ³•åŸºæœ¬ç›¸åŒï¼Œåªä½¿ç”¨æ•´ä¸ªå•è¯è€Œä¸æ˜¯å•è¯ã€‚ é¦–å…ˆï¼Œç¼–è¾‘å•è¯è¯å…¸ï¼Œç„¶ååœ¨ç½‘ç»œè¾“å…¥ä¸­è¾“å…¥æ•°å­—è€Œä¸æ˜¯å•è¯ã€‚ <br><br> å½“ç„¶ï¼Œè¿™æ˜¯ä¸€ä¸ªç›¸å½“ç®€åŒ–çš„æè¿°ã€‚  Keras <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">å·²ç»æœ‰</a>æ–‡æœ¬ç”Ÿæˆçš„ç¤ºä¾‹ï¼Œä½†é¦–å…ˆï¼Œæ²¡æœ‰å¯¹å®ƒä»¬è¿›è¡Œè¯¦ç»†æè¿°ï¼Œå…¶æ¬¡ï¼Œæ‰€æœ‰è‹±è¯­æ•™ç¨‹éƒ½ä½¿ç”¨ç›¸å½“æŠ½è±¡çš„æ–‡æœ¬ï¼Œä¾‹å¦‚èå£«æ¯”äºšï¼Œè¿™å¯¹äºæœ¬åœ°äººæ¥è¯´å¾ˆéš¾ç†è§£ã€‚ å¥½å§ï¼Œæˆ‘ä»¬æ­£åœ¨æˆ‘ä»¬å¼ºå¤§è€Œå¼ºå¤§çš„ç¥ç»ç½‘ç»œä¸Šæµ‹è¯•ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå®ƒå½“ç„¶ä¼šæ›´åŠ æ¸…æ™°å’Œæ˜“äºç†è§£ã€‚ <br><br><h2> ç½‘ç»œåŸ¹è®­ </h2><br> ä½œä¸ºè¾“å…¥æ–‡æœ¬ï¼Œæˆ‘ä½¿ç”¨äº†... ... Habrçš„æ³¨é‡Šï¼Œæºæ–‡ä»¶çš„å¤§å°ä¸º1 MBï¼ˆå½“ç„¶ï¼Œæ³¨é‡Šå®é™…ä¸Šç¡®å®æ›´å¤šï¼Œä½†æ˜¯æˆ‘åªéœ€è¦ä½¿ç”¨ä¸€éƒ¨åˆ†ï¼Œå¦åˆ™è¯¥ç½‘ç»œå°†è¢«è®­ç»ƒä¸€ä¸ªæ˜ŸæœŸï¼Œè€Œè¯»è€…åœ¨æ˜ŸæœŸäº”ä¹‹å‰å°†ä¸ä¼šçœ‹åˆ°æ­¤æ–‡æœ¬ï¼‰ã€‚ è®©æˆ‘æé†’æ‚¨ï¼Œåªæœ‰å­—æ¯è¢«è¾“å…¥åˆ°ç¥ç»ç½‘ç»œçš„è¾“å…¥ä¸­ï¼Œè¯¥ç½‘ç»œå¯¹è¯­è¨€æˆ–å…¶ç»“æ„ä¸€æ— æ‰€çŸ¥ã€‚ æˆ‘ä»¬å¼€å§‹ç½‘ç»œåŸ¹è®­ã€‚ <br><br>  <b>5åˆ†é’Ÿçš„è®­ç»ƒï¼š</b> <br><br> åˆ°ç›®å‰ä¸ºæ­¢ï¼Œè¿˜ä¸æ¸…æ¥šï¼Œä½†æ˜¯æ‚¨å·²ç»å¯ä»¥çœ‹åˆ°ä¸€äº›å¯è¯†åˆ«çš„å­—æ¯ç»„åˆï¼š <br><br> <code>                          .                   .                                                     .          Â«                    <br></code> <br>  <b>15åˆ†é’Ÿçš„è®­ç»ƒï¼š</b> <br><br> ç»“æœå·²ç»æ˜æ˜¾å¥½äº†ï¼š <br><br> <code>                                                                                                                                 <br></code> <br>  <b>1å°æ—¶çš„åŸ¹è®­ï¼š</b> <br><br> <code>                                                                  Â« Â» â€”                                                            Â«     Â» Â» â€”             </code> <br> <br> ç”±äºæŸç§åŸå› ï¼Œæ‰€æœ‰æ–‡æœ¬éƒ½è¢«è¯æ˜æ²¡æœ‰ç‚¹å’Œå¤§å†™å­—æ¯ï¼Œä¹Ÿè®¸utf-8å¤„ç†æœªæ­£ç¡®å®Œæˆã€‚ ä½†æ€»çš„æ¥è¯´ï¼Œè¿™ä»¤äººå°è±¡æ·±åˆ»ã€‚ é€šè¿‡ä»…åˆ†æå’Œè®°ä½ç¬¦å·ä»£ç ï¼Œè¯¥ç¨‹åºå®é™…ä¸Šâ€œç‹¬ç«‹åœ°â€å­¦ä¹ äº†ä¿„è¯­å•è¯ï¼Œå¹¶ä¸”å¯ä»¥ç”Ÿæˆçœ‹èµ·æ¥å¯ä¿¡çš„æ–‡æœ¬ã€‚ <br><br> åŒæ ·æœ‰è¶£çš„æ˜¯ï¼Œè¯¥ç¨‹åºå¯ä»¥å¾ˆå¥½åœ°è®°ä½æ–‡æœ¬æ ·å¼ã€‚ åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œä¸€äº›æ³•å¾‹çš„æ–‡å­—è¢«ç”¨ä½œæ•™å…·ã€‚ ç½‘ç»œåŸ¹è®­æ—¶é—´ä¸º5åˆ†é’Ÿã€‚ <br><br> <code>  ""  ,  ,  ,  ,  ,  , ,  ,                 <br></code> <br> åœ¨è¿™é‡Œï¼Œå°†è¯ç‰©çš„åŒ»å­¦æ³¨é‡Šç”¨ä½œè¾“å…¥é›†ã€‚ ç½‘ç»œåŸ¹è®­æ—¶é—´ä¸º5åˆ†é’Ÿã€‚ <br><br> <code>  <br>      <br>                                          ,    ,             <br></code> <br> åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°å‡ ä¹æ•´ä¸ªçŸ­è¯­ã€‚ è¿™æ˜¯ç”±äºä»¥ä¸‹äº‹å®ï¼šåŸå§‹æ–‡æœ¬å¾ˆçŸ­ï¼Œè€Œç¥ç»ç½‘ç»œå®é™…ä¸Šâ€œæ•´ä½“â€å­˜å‚¨äº†ä¸€äº›çŸ­è¯­ã€‚ æ­¤æ•ˆæœç§°ä¸ºâ€œå†åŸ¹è®­â€ï¼Œåº”é¿å…ã€‚ ç†æƒ³æƒ…å†µä¸‹ï¼Œæ‚¨éœ€è¦åœ¨å¤§å‹æ•°æ®é›†ä¸Šæµ‹è¯•ç¥ç»ç½‘ç»œï¼Œä½†æ˜¯åœ¨è¿™ç§æƒ…å†µä¸‹è¿›è¡Œçš„è®­ç»ƒå¯èƒ½ä¼šèŠ±è´¹å¾ˆå¤šå°æ—¶ï¼Œè€Œä¸”ä¸å¹¸çš„æ˜¯ï¼Œæˆ‘æ²¡æœ‰é¢å¤–çš„è¶…çº§è®¡ç®—æœºã€‚ <br><br> ä½¿ç”¨è¿™ç§ç½‘ç»œçš„ä¸€ä¸ªæœ‰è¶£ç¤ºä¾‹æ˜¯åç§°ç”Ÿæˆã€‚ é€šè¿‡å°†ç”·æ€§å’Œå¥³æ€§åå­—åˆ—è¡¨ä¸Šä¼ åˆ°æ–‡ä»¶ä¸­ï¼Œæˆ‘å¾—åˆ°äº†éå¸¸æœ‰è¶£çš„æ–°é€‰é¡¹ï¼Œè¿™äº›é€‰é¡¹éå¸¸é€‚åˆç§‘å¹»å°è¯´ï¼šRlarï¼ŒLaaaï¼ŒAriaï¼ŒAreraï¼ŒAeliaï¼ŒNinranï¼ŒAirã€‚ ä»–ä»¬èº«ä¸Šæœ‰äº›æ„Ÿè§‰åˆ°åŸƒå¤«é›·è«å¤«å’Œä»™å¥³åº§æ˜Ÿäº‘çš„é£æ ¼... <br><br><h2>  C ++ </h2><br> æœ‰è¶£çš„æ˜¯ï¼Œæ€»çš„æ¥è¯´ï¼Œç¥ç»ç½‘ç»œå°±åƒè®°å¿†ã€‚ ä¸‹ä¸€æ­¥æ˜¯æ£€æŸ¥ç¨‹åºå¦‚ä½•å¤„ç†æºä»£ç ã€‚ ä½œä¸ºæµ‹è¯•ï¼Œæˆ‘é‡‡ç”¨äº†ä¸åŒçš„C ++æºå¹¶å°†å®ƒä»¬ç»„åˆåˆ°ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ä¸­ã€‚ <br><br> è€å®è¯´ï¼Œç»“æœæ¯”ä¿„è¯­æ›´ä»¤äººæƒŠè®¶ã€‚ <br><br>  <b>5åˆ†é’Ÿçš„è®­ç»ƒ</b> <br><br> è¯¥æ­»çš„ï¼Œè¿™å‡ ä¹æ˜¯çœŸæ­£çš„C ++ã€‚ <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> ( snd_pcm_state_channels = <span class="hljs-number"><span class="hljs-number">0</span></span> ) { errortext_ = <span class="hljs-string"><span class="hljs-string">"rtapialsa::probedeviceopen: esror stream_.buffer stream!"</span></span>; errortext_ = errorstream_.str(); <span class="hljs-keyword"><span class="hljs-keyword">goto</span></span> unlock; } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> ( stream_.mode == input &amp;&amp; stream_.mode == output || <span class="hljs-literal"><span class="hljs-literal">false</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> ( stream_.state == stream_stopped ) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> ( <span class="hljs-keyword"><span class="hljs-keyword">unsigned</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i=<span class="hljs-number"><span class="hljs-number">0</span></span>; i&lt;stream_.nuserbuffer[i] ) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (j=<span class="hljs-number"><span class="hljs-number">0</span></span>; j&lt;info.channels; } } }</code> </pre><br>  <b>30åˆ†é’Ÿçš„è®­ç»ƒ</b> <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">void</span></span> maxirecorder::stopstream() { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; ainchannels; i++ ) { <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> input=(stream, null; conternallock( pthread_cond_wate); <span class="hljs-comment"><span class="hljs-comment">// this should not ported to the device is a structure are is and the stream the buffer of the interleaved as and the probe the arauliored format for the the stream os and the software and the supported of the is so aa is set the directsound of allocates the reset and real the read the setup the buffer // this is the setup to the the devices. // asio will open the supported by the supported buffer are asioned structure. // this should not have to setup the counting and the proper in the position we mast it alw the aftware configuration. // this is the the stream and probe the supported be audio for sched/ond changed to this the sample rate it in the counting the fres // mecorend. windevoppart( null, 0, &amp;phase ); possint=&gt;buffersize( 1, bufferlist); } } } ///********************************************* void maxirecorder::abortdevice( device ) { if (cout &lt;&lt; aindeff( thread_cont_start); for (int i = 0; i &lt; sample_rates[0]) { if (count == int) { std::cout &lt;&lt; "checksing of convernion allocation handler (" &lt;&lt; device &lt;&lt; ")."; errortext_ = errorstream_.str(); return failure; } } } if ( stream_.userbuffer[i] ) { errortext_ = "rtapicore::probedeviceopen: system error (" &lt;&lt; geterrorcode( result ) &lt;&lt; ") getting stream configuration info state."; error( rterror::warning ); return; } if ( stream_.mode == output || stream_.mode == duplex ) { callbackinfo *info = (const char *) callback( stream_.userbuffer[1], stream_.convertinfo[0] ); return info; } // setup buffer callback for conversion on do any the supported be audio bet that al alinums be of support of the supported buffer // the buffer size a stream and buffers. info.name = dsdevices[ device ]; return 0; }</span></span></code> </pre> <br> å¦‚æ‚¨æ‰€è§ï¼Œè¯¥ç¨‹åºå·²ç»â€œå­¦ä¹ â€äº†ç¼–å†™å®Œæ•´çš„å‡½æ•°ã€‚ åŒæ—¶ï¼Œå®ƒå®Œå…¨â€œäººä¸ºâ€åœ°å°†åŠŸèƒ½ä¸å¸¦æ˜Ÿå·çš„æ³¨é‡Šåˆ†å¼€ï¼Œåœ¨ä»£ç ä¸­æ·»åŠ æ³¨é‡Šï¼Œä»¥åŠæ‰€æœ‰è¿™äº›ã€‚ æˆ‘æƒ³ä»¥è¿™æ ·çš„é€Ÿåº¦å­¦ä¹ ä¸€ç§æ–°çš„ç¼–ç¨‹è¯­è¨€...å½“ç„¶ï¼Œä»£ç ä¸­æœ‰é”™è¯¯ï¼Œå½“ç„¶ä¹Ÿä¸ä¼šç¼–è¯‘ã€‚ é¡ºä¾¿è¯´ä¸€å¥ï¼Œæˆ‘æ²¡æœ‰æ ¼å¼åŒ–ä»£ç ï¼Œè¯¥ç¨‹åºè¿˜å­¦ä¼šäº†åœ¨æ–¹æ‹¬å·å’Œç¼©è¿›â€œæˆ‘è‡ªå·±â€ä¸­æ·»åŠ æ–‡å­—ã€‚ <br><br> å½“ç„¶ï¼Œè¿™äº›ç¨‹åºæ²¡æœ‰ä¸»è¦<i>å«ä¹‰</i> - <i>å«ä¹‰</i> ï¼Œå› æ­¤çœ‹èµ·æ¥åƒæ˜¯åœ¨æ¢¦ä¸­å†™çš„ï¼Œæˆ–è€…ä¸æ˜¯ç”±ä¸€ä¸ªå®Œå…¨å¥åº·çš„äººç¼–å†™çš„ï¼Œå› æ­¤å…·æœ‰è¶…ç°å®æ„Ÿã€‚ ç„¶è€Œï¼Œç»“æœä»¤äººå°è±¡æ·±åˆ»ã€‚ ä¹Ÿè®¸æ›´æ·±å…¥åœ°ç ”ç©¶ä¸åŒæ–‡æœ¬çš„äº§ç”Ÿå°†æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£çœŸå®æ‚£è€…çš„æŸäº›ç²¾ç¥ç–¾ç—…ã€‚ é¡ºä¾¿è¯´ä¸€å¥ï¼Œæ­£å¦‚è¯„è®ºä¸­æ‰€å»ºè®®çš„é‚£æ ·ï¼Œç¡®å®å­˜åœ¨è¿™æ ·ä¸€ç§ç²¾ç¥ç–¾ç—…ï¼Œå³ä¸€ä¸ªäººè¯´å‡ºä¸è¯­æ³•ç›¸å…³ä½†å®Œå…¨æ²¡æœ‰æ„ä¹‰çš„æ–‡æœ¬ï¼ˆ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">ç²¾ç¥åˆ†è£‚ç—‡</a> ï¼‰ã€‚ <br><br><h2> ç»“è®º </h2><br> å¨±ä¹ç¥ç»ç½‘ç»œè¢«è®¤ä¸ºéå¸¸æœ‰å‰é€”ï¼Œä¸æ²¡æœ‰å†…å­˜çš„MLPç­‰â€œæ™®é€šâ€ç½‘ç»œç›¸æ¯”ï¼Œè¿™ç¡®å®æ˜¯å‘å‰è¿ˆå‡ºäº†ä¸€å¤§æ­¥ã€‚ ç¡®å®ï¼Œç¥ç»ç½‘ç»œå­˜å‚¨å’Œå¤„ç†ç›¸å½“å¤æ‚çš„ç»“æ„çš„èƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ã€‚ æ­£æ˜¯åœ¨è¿™äº›æµ‹è¯•ä¹‹åï¼Œå½“æˆ‘å†™é“æœªæ¥çš„AIå¯èƒ½æ˜¯â€œå¯¹äººç±»æœ€å¤§çš„é£é™©â€æ—¶ï¼Œæˆ‘æ‰ä»¥ä¸ºIlon Maskå¯èƒ½æ˜¯æ­£ç¡®çš„-å³ä½¿ç®€å•çš„ç¥ç»ç½‘ç»œå¯ä»¥è½»æ¾è®°ä½å¹¶å¤åˆ¶ç›¸å½“å¤æ‚çš„æ¨¡å¼ï¼Œæ•°åäº¿ä¸ªç»„ä»¶çš„ç½‘ç»œèƒ½åšä»€ä¹ˆï¼Ÿ ä½†æ˜¯å¦ä¸€æ–¹é¢ï¼Œè¯·ä¸è¦å¿˜è®°æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œæ— æ³•<i>æ€è€ƒ</i> ï¼Œå®ƒæœ¬è´¨ä¸Šåªæ˜¯æœºæ¢°åœ°è®°ä½å­—ç¬¦åºåˆ—ï¼Œè€Œä¸æ˜¯ç†è§£å®ƒä»¬çš„å«ä¹‰ã€‚ è¿™ä¸€ç‚¹å¾ˆé‡è¦-å³ä½¿æ‚¨åœ¨è¶…çº§è®¡ç®—æœºå’Œåºå¤§çš„æ•°æ®é›†ä¸Šè®­ç»ƒç¥ç»ç½‘ç»œï¼Œå……å…¶é‡ä¹Ÿåªèƒ½å­¦ä¼šåœ¨è¯­æ³•ä¸Šç”Ÿæˆ100ï¼…æ­£ç¡®ä½†å®Œå…¨æ²¡æœ‰æ„ä¹‰çš„å¥å­ã€‚ <br><br> ä½†æ˜¯å®ƒä¸ä¼šåœ¨å“²å­¦ä¸Šè¢«åˆ é™¤ï¼Œå¯¹äºå®è·µè€…è€Œè¨€ï¼Œè¯¥æ–‡ç« ä»ç„¶æ›´å¤šã€‚ å¯¹äºé‚£äº›æƒ³è¦è‡ªå·±è¿›è¡Œå®éªŒçš„äººï¼ŒPython 3.7ä¸­çš„<b>æºä»£ç </b>ä½äºç ´åè€…ä¹‹ä¸‹ã€‚ è¿™æ®µä»£ç æ˜¯æ¥è‡ªå„ä¸ªgithubé¡¹ç›®çš„æ±‡ç¼–ï¼Œä¸æ˜¯æœ€ä½³ä»£ç çš„ç¤ºä¾‹ï¼Œä½†ä¼¼ä¹å¯ä»¥æ‰§è¡Œå…¶ä»»åŠ¡ã€‚ <br><br> ä½¿ç”¨è¯¥ç¨‹åºä¸éœ€è¦ç¼–ç¨‹æŠ€èƒ½ï¼Œè¶³ä»¥çŸ¥é“å¦‚ä½•å®‰è£…Pythonã€‚ ä»å‘½ä»¤è¡Œå¼€å§‹çš„ç¤ºä¾‹ï¼š <br>  -åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹ä»¥åŠç”Ÿæˆæ–‡æœ¬ï¼š <br>  python \ keras_textgen.py --text = text_habr.txt --epochs = 10 --out_len = 4000 <br>  -ä»…æ–‡æœ¬ç”Ÿæˆè€Œæ— éœ€æ¨¡å‹è®­ç»ƒï¼š <br>  python \ keras_textgen.py --text = text_habr.txt --epochs = 10 --out_len = 4000-ç”Ÿæˆ <br><br><div class="spoiler">  <b class="spoiler_title">keras_textgen.py</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-comment"><span class="hljs-comment"># Force CPU os.environ["CUDA_VISIBLE_DEVICES"] = "-1" os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # 0 = all messages are logged, 3 - INFO, WARNING, and ERROR messages are not printed from keras.callbacks import LambdaCallback from keras.models import Sequential from keras.layers import Dense, Dropout, Embedding, LSTM, TimeDistributed from keras.optimizers import RMSprop from keras.utils.data_utils import get_file import keras from collections import Counter import pickle import numpy as np import random import sys import time import io import re import argparse # Transforms text to vectors of integer numbers representing in text tokens and back. Handles word and character level tokenization. class Vectorizer: def __init__(self, text, word_tokens, pristine_input, pristine_output): self.word_tokens = word_tokens self._pristine_input = pristine_input self._pristine_output = pristine_output tokens = self._tokenize(text) # print('corpus length:', len(tokens)) token_counts = Counter(tokens) # Sort so most common tokens come first in our vocabulary tokens = [x[0] for x in token_counts.most_common()] self._token_indices = {x: i for i, x in enumerate(tokens)} self._indices_token = {i: x for i, x in enumerate(tokens)} self.vocab_size = len(tokens) print('Vocab size:', self.vocab_size) def _tokenize(self, text): if not self._pristine_input: text = text.lower() if self.word_tokens: if self._pristine_input: return text.split() return Vectorizer.word_tokenize(text) return text def _detokenize(self, tokens): if self.word_tokens: if self._pristine_output: return ' '.join(tokens) return Vectorizer.word_detokenize(tokens) return ''.join(tokens) def vectorize(self, text): """Transforms text to a vector of integers""" tokens = self._tokenize(text) indices = [] for token in tokens: if token in self._token_indices: indices.append(self._token_indices[token]) else: print('Ignoring unrecognized token:', token) return np.array(indices, dtype=np.int32) def unvectorize(self, vector): """Transforms a vector of integers back to text""" tokens = [self._indices_token[index] for index in vector] return self._detokenize(tokens) @staticmethod def word_detokenize(tokens): # A heuristic attempt to undo the Penn Treebank tokenization above. Pass the # --pristine-output flag if no attempt at detokenizing is desired. regexes = [ # Newlines (re.compile(r'[ ]?\\n[ ]?'), r'\n'), # Contractions (re.compile(r"\b(can)\s(not)\b"), r'\1\2'), (re.compile(r"\b(d)\s('ye)\b"), r'\1\2'), (re.compile(r"\b(gim)\s(me)\b"), r'\1\2'), (re.compile(r"\b(gon)\s(na)\b"), r'\1\2'), (re.compile(r"\b(got)\s(ta)\b"), r'\1\2'), (re.compile(r"\b(lem)\s(me)\b"), r'\1\2'), (re.compile(r"\b(mor)\s('n)\b"), r'\1\2'), (re.compile(r"\b(wan)\s(na)\b"), r'\1\2'), # Ending quotes (re.compile(r"([^' ]) ('ll|'re|'ve|n't)\b"), r"\1\2"), (re.compile(r"([^' ]) ('s|'m|'d)\b"), r"\1\2"), (re.compile(r'[ ]?â€'), r'"'), # Double dashes (re.compile(r'[ ]?--[ ]?'), r'--'), # Parens and brackets (re.compile(r'([\[\(\{\&lt;]) '), r'\1'), (re.compile(r' ([\]\)\}\&gt;])'), r'\1'), (re.compile(r'([\]\)\}\&gt;]) ([:;,.])'), r'\1\2'), # Punctuation (re.compile(r"([^']) ' "), r"\1' "), (re.compile(r' ([?!\.])'), r'\1'), (re.compile(r'([^\.])\s(\.)([\]\)}&gt;"\']*)\s*$'), r'\1\2\3'), (re.compile(r'([#$]) '), r'\1'), (re.compile(r' ([;%:,])'), r'\1'), # Starting quotes (re.compile(r'(â€œ)[ ]?'), r'"') ] text = ' '.join(tokens) for regexp, substitution in regexes: text = regexp.sub(substitution, text) return text.strip() @staticmethod def word_tokenize(text): # Basic word tokenizer based on the Penn Treebank tokenization script, but # setup to handle multiple sentences. Newline aware, ie newlines are # replaced with a specific token. You may want to consider using a more robust # tokenizer as a preprocessing step, and using the --pristine-input flag. regexes = [ # Starting quotes (re.compile(r'(\s)"'), r'\1 â€œ '), (re.compile(r'([ (\[{&lt;])"'), r'\1 â€œ '), # Punctuation (re.compile(r'([:,])([^\d])'), r' \1 \2'), (re.compile(r'([:,])$'), r' \1 '), (re.compile(r'\.\.\.'), r' ... '), (re.compile(r'([;@#$%&amp;])'), r' \1 '), (re.compile(r'([?!\.])'), r' \1 '), (re.compile(r"([^'])' "), r"\1 ' "), # Parens and brackets (re.compile(r'([\]\[\(\)\{\}\&lt;\&gt;])'), r' \1 '), # Double dashes (re.compile(r'--'), r' -- '), # Ending quotes (re.compile(r'"'), r' â€ '), (re.compile(r"([^' ])('s|'m|'d) "), r"\1 \2 "), (re.compile(r"([^' ])('ll|'re|'ve|n't) "), r"\1 \2 "), # Contractions (re.compile(r"\b(can)(not)\b"), r' \1 \2 '), (re.compile(r"\b(d)('ye)\b"), r' \1 \2 '), (re.compile(r"\b(gim)(me)\b"), r' \1 \2 '), (re.compile(r"\b(gon)(na)\b"), r' \1 \2 '), (re.compile(r"\b(got)(ta)\b"), r' \1 \2 '), (re.compile(r"\b(lem)(me)\b"), r' \1 \2 '), (re.compile(r"\b(mor)('n)\b"), r' \1 \2 '), (re.compile(r"\b(wan)(na)\b"), r' \1 \2 '), # Newlines (re.compile(r'\n'), r' \\n ') ] text = " " + text + " " for regexp, substitution in regexes: text = regexp.sub(substitution, text) return text.split() def _create_sequences(vector, seq_length, seq_step): # Take strips of our vector at seq_step intervals up to our seq_length # and cut those strips into seq_length sequences passes = [] for offset in range(0, seq_length, seq_step): pass_samples = vector[offset:] num_pass_samples = pass_samples.size // seq_length pass_samples = np.resize(pass_samples, (num_pass_samples, seq_length)) passes.append(pass_samples) # Stack our sequences together. This will technically leave a few "breaks" # in our sequence chain where we've looped over are entire dataset and # return to the start, but with large datasets this should be neglegable return np.concatenate(passes) def shape_for_stateful_rnn(data, batch_size, seq_length, seq_step): """ Reformat our data vector into input and target sequences to feed into our RNN. Tricky with stateful RNNs. """ # Our target sequences are simply one timestep ahead of our input sequences. # eg with an input vector "wherefore"... # targets: herefore # predicts ^ ^ ^ ^ ^ ^ ^ ^ # inputs: wherefor inputs = data[:-1] targets = data[1:] # We split our long vectors into semi-redundant seq_length sequences inputs = _create_sequences(inputs, seq_length, seq_step) targets = _create_sequences(targets, seq_length, seq_step) # Make sure our sequences line up across batches for stateful RNNs inputs = _batch_sort_for_stateful_rnn(inputs, batch_size) targets = _batch_sort_for_stateful_rnn(targets, batch_size) # Our target data needs an extra axis to work with the sparse categorical # crossentropy loss function targets = targets[:, :, np.newaxis] return inputs, targets def _batch_sort_for_stateful_rnn(sequences, batch_size): # Now the tricky part, we need to reformat our data so the first # sequence in the nth batch picks up exactly where the first sequence # in the (n - 1)th batch left off, as the RNN cell state will not be # reset between batches in the stateful model. num_batches = sequences.shape[0] // batch_size num_samples = num_batches * batch_size reshuffled = np.zeros((num_samples, sequences.shape[1]), dtype=np.int32) for batch_index in range(batch_size): # Take a slice of num_batches consecutive samples slice_start = batch_index * num_batches slice_end = slice_start + num_batches index_slice = sequences[slice_start:slice_end, :] # Spread it across each of our batches in the same index position reshuffled[batch_index::batch_size, :] = index_slice return reshuffled def load_data(data_file, word_tokens, pristine_input, pristine_output, batch_size, seq_length=50, seq_step=25): global vectorizer try: with open(data_file, encoding='utf-8') as input_file: text = input_file.read() except FileNotFoundError: print("No input.txt in data_dir") sys.exit(1) skip_validate = True # try: # with open(os.path.join(data_dir, 'validate.txt'), encoding='utf-8') as validate_file: # text_val = validate_file.read() # skip_validate = False # except FileNotFoundError: # pass # Validation text optional # Find some good default seed string in our source text. # self.seeds = find_random_seeds(text) # Include our validation texts with our vectorizer all_text = text if skip_validate else '\n'.join([text, text_val]) vectorizer = Vectorizer(all_text, word_tokens, pristine_input, pristine_output) data = vectorizer.vectorize(text) x, y = shape_for_stateful_rnn(data, batch_size, seq_length, seq_step) print("Word_tokens:", word_tokens) print('x.shape:', x.shape) print('y.shape:', y.shape) if skip_validate: return x, y, None, None, vectorizer data_val = vectorizer.vectorize(text_val) x_val, y_val = shape_for_stateful_rnn(data_val, batch_size, seq_length, seq_step) print('x_val.shape:', x_val.shape) print('y_val.shape:', y_val.shape) return x, y, x_val, y_val, vectorizer def make_model(batch_size, vocab_size, embedding_size=64, rnn_size=128, num_layers=2): # Conversely if your data is large (more than about 2MB), feel confident to increase rnn_size and train a bigger model (see details of training below). # It will work significantly better. For example with 6MB you can easily go up to rnn_size 300 or even more. model = Sequential() model.add(Embedding(vocab_size, embedding_size, batch_input_shape=(batch_size, None))) for layer in range(num_layers): model.add(LSTM(rnn_size, stateful=True, return_sequences=True)) model.add(Dropout(0.2)) model.add(TimeDistributed(Dense(vocab_size, activation='softmax'))) model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) return model def train(model, x, y, x_val, y_val, batch_size, num_epochs): print('Training...') # print("Shape:", x.shape, y.shape) # print(num_epochs, batch_size, x[0], y[0]) train_start = time.time() validation_data = (x_val, y_val) if (x_val is not None) else None callbacks = None model.fit(x, y, validation_data=validation_data, batch_size=batch_size, shuffle=False, epochs=num_epochs, verbose=1, callbacks=callbacks) # self.update_sample_model_weights() train_end = time.time() print('Training time', train_end - train_start) def sample_preds(preds, temperature=1.0): """ Samples an unnormalized array of probabilities. Use temperature to flatten/amplify the probabilities. """ preds = np.asarray(preds).astype(np.float64) # Add a tiny positive number to avoid invalid log(0) preds += np.finfo(np.float64).tiny preds = np.log(preds) / temperature exp_preds = np.exp(preds) preds = exp_preds / np.sum(exp_preds) probas = np.random.multinomial(1, preds, 1) return np.argmax(probas) def generate(model, vectorizer, seed, length=100, diversity=0.5): seed_vector = vectorizer.vectorize(seed) # Feed in seed string print("Seed:", seed, end=' ' if vectorizer.word_tokens else '') model.reset_states() preds = None for char_index in np.nditer(seed_vector): preds = model.predict(np.array([[char_index]]), verbose=0) sampled_indices = [] # np.array([], dtype=np.int32) # Sample the model one token at a time for i in range(length): char_index = 0 if preds is not None: char_index = sample_preds(preds[0][0], diversity) sampled_indices.append(char_index) # = np.append(sampled_indices, char_index) preds = model.predict(np.array([[char_index]]), verbose=0) sample = vectorizer.unvectorize(sampled_indices) return sample if __name__ == "__main__": batch_size = 32 # Batch size for each train num_epochs = 10 # Number of epochs of training out_len = 200 # Length of the output phrase seq_length = 50 # 50 # Determines, how long phrases will be used for training use_words = False # Use words instead of characters (slower speed, bigger vocabulary) data_file = "text_habr.txt" # Source text file seed = "A" # Initial symbol of the text parser = argparse.ArgumentParser() parser.add_argument("-t", "--text", action="store", required=False, dest="text", help="Input text file") parser.add_argument("-e", "--epochs", action="store", required=False, dest="epochs", help="Number of training epochs") parser.add_argument("-p", "--phrase_len", action="store", required=False, dest="phrase_len", help="Phrase analyse length") parser.add_argument("-o", "--out_len", action="store", required=False, dest="out_len", help="Output text length") parser.add_argument("-g", "--generate", action="store_true", required=False, dest='generate', help="Generate output only without training") args = parser.parse_args() if args.text is not None: data_file = args.text if args.epochs is not None: num_epochs = int(args.epochs) if args.phrase_len is not None: seq_length = int(args.phrase_len) if args.out_len is not None: out_len = int(args.out_len) # Load text data pristine_input, pristine_output = False, False x, y, x_val, y_val, vectorizer = load_data(data_file, use_words, pristine_input, pristine_output, batch_size, seq_length) model_file = data_file.lower().replace('.txt', '.h5') if args.generate is False: # Make model model = make_model(batch_size, vectorizer.vocab_size) # Train model train(model, x, y, x_val, y_val, batch_size, num_epochs) # Save model to file model.save(filepath=model_file) model = keras.models.load_model(model_file) predict_model = make_model(1, vectorizer.vocab_size) predict_model.set_weights(model.get_weights()) # Generate phrases res = generate(predict_model, vectorizer, seed=seed, length=out_len) print(res)</span></span></code> </pre><br></div></div><br> æˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªéå¸¸<s>æ—¶é«¦çš„</s>å·¥ä½œæ–‡æœ¬ç”Ÿæˆå™¨ï¼Œ <s>å¯¹äºåœ¨Habrä¸Šå†™æ–‡ç« å¾ˆæœ‰ç”¨</s> ã€‚ åœ¨å¤§æ–‡æœ¬å’Œå¤§é‡è¿­ä»£åŸ¹è®­ä¸Šè¿›è¡Œæµ‹è¯•ç‰¹åˆ«æœ‰è¶£ï¼Œå¦‚æœä»»ä½•äººéƒ½å¯ä»¥ä½¿ç”¨å¿«é€Ÿè®¡ç®—æœºï¼Œåˆ™æŸ¥çœ‹ç»“æœå°†å¾ˆæœ‰è¶£ã€‚ <br><br> å¦‚æœæœ‰äººæƒ³æ›´è¯¦ç»†åœ°ç ”ç©¶è¯¥ä¸»é¢˜ï¼Œå¯ä»¥åœ¨<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">http://karpathy.github.io/2015/05/21/rnn-efficiency/</a>ä¸Šæ‰¾åˆ°æœ‰å…³ä½¿ç”¨RNNçš„è¯¦ç»†è¯´æ˜å’Œè¯¦ç»†ç¤ºä¾‹ã€‚ <br><br>  PSï¼šæœ€åï¼Œè¿˜æœ‰å‡ èŠ‚ç»æ–‡ï¼›ï¼‰æœ‰è¶£çš„æ˜¯ï¼Œä¸æ˜¯æˆ‘æœ¬äººæ¥æ ¼å¼åŒ–æ–‡æœ¬ç”šè‡³æ·»åŠ æ˜Ÿå·ï¼Œè€Œæ˜¯â€œæ˜¯æˆ‘è‡ªå·±â€ã€‚ ä¸‹ä¸€æ­¥å¾ˆæœ‰è¶£ï¼Œå¯ä»¥æ£€æŸ¥ç»˜åˆ¶å›¾ç‰‡å’Œåˆ›ä½œéŸ³ä¹çš„å¯èƒ½æ€§ã€‚ æˆ‘è®¤ä¸ºç¥ç»ç½‘ç»œåœ¨è¿™é‡Œå¾ˆæœ‰å‰é€”ã€‚ <br><br>  <i>xxx</i> <i><br><br></i>  <i>å¯¹äºæŸäº›äººæ¥è¯´ï¼Œå®ƒä»¬è¢«å¤¹åœ¨é¥¼å¹²ä¸­-ç¥ä¸€åˆ‡é¡ºåˆ©ã€‚</i> <i><br></i>  <i>æ™šä¸Šä»ç‰ç½®</i> <i><br></i>  <i>åœ¨èœ¡çƒ›ä¸‹çˆ¬å±±ã€‚</i> <i><br><br></i>  <i>xxx</i> <i><br><br></i>  <i>å¾ˆå¿«ï¼Œå„¿å­åœ¨è’™å¤§æ‹¿å·çš„ç”µè½¦</i> <i><br></i>  <i>çœ‹ä¸è§çš„æ¬¢ä¹çš„æ°”å‘³</i> <i><br></i>  <i>è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘åœ¨ä¸€èµ·æˆé•¿</i> <i><br></i>  <i>æ‚¨ä¸ä¼šä¸ºæœªçŸ¥è€Œç”Ÿç—…ã€‚</i> <i><br><br></i>  <i>äº¤é”™çš„å°ä»“é¼ çš„å¿ƒ</i> <i><br></i>  <i>éº¦ç‰‡ç²¥è¿˜æ²¡è€ï¼Œ</i> <i><br></i>  <i>æˆ‘å®ˆå«é€šå¾€æŠ¢çƒçš„æ¡¥æ¢ã€‚</i> <i><br><br></i>  <i>ç”¨ä¸å¤šå·´ï¼ˆDobaï¼‰çš„è¾¾é‡Œçº³ï¼ˆDarinaï¼‰ç›¸åŒçš„æ–¹å¼ï¼Œ</i> <i><br></i>  <i>æˆ‘å¬è§å¿ƒä¸­æœ‰é›ªã€‚</i> <i><br></i>  <i>æˆ‘ä»¬å”±ç™½å¤šå°‘æŸ”å¼±çš„æœç¾</i> <i><br></i>  <i>æˆ‘æŠŠçŸ¿çŸ³é‡å…½æ‹’ä¹‹é—¨å¤–ã€‚</i> <i><br><br></i>  <i>xxx</i> <i><br><br></i>  <i>å…½åŒ»ç”¨å’’è¯­æŠŠæ²¹æ¡é’‰æ­»</i> <i><br></i>  <i>å¹¶æ´’åœ¨è¢«é—å¿˜çš„ä¸‹æ–¹ã€‚</i> <i><br></i>  <i>å’Œä½ ä¸€æ ·ï¼Œå¤å·´çš„åˆ†æ”¯</i> <i><br></i>  <i>åœ¨å…¶ä¸­é—ªè€€ã€‚</i> <i><br></i>  <i>åœ¨zakotoå¼€å¿ƒ</i> <i><br></i>  <i>éšç€ç‰›å¥¶çš„é£è¡Œã€‚</i> <i><br><br></i>  <i>å“¦ï¼Œä½ æ˜¯ä¸€æœµç«ç‘°ï¼Œè½»ç›ˆ</i> <i><br></i>  <i>æ‰‹ä¸Šæœ‰äº‘å…‰ï¼š</i> <i><br></i>  <i>åœ¨é»æ˜æ—¶æ»šåŠ¨</i> <i><br></i>  <i>ä½ å¥½ï¼Œæˆ‘çš„éª‘å£«ï¼</i> <i><br><br></i>  <i>ä»–åœ¨æ™šä¸ŠæœåŠ¡ï¼Œè€Œä¸æ˜¯éª¨å­é‡Œ</i> <i><br></i>  <i>æ™šä¸Šåœ¨Tanyaçš„è“å…‰</i> <i><br></i>  <i>åƒä¸€ç§æ‚²ä¼¤</i> <br><br> æœ€åå‡ èŠ‚ç»æ–‡å­¦ä¹ æ¨¡å¼ã€‚ åœ¨è¿™é‡Œï¼ŒéŸµå¾‹æ¶ˆå¤±äº†ï¼Œä½†æ˜¯å‡ºç°äº†ä¸€äº›å«ä¹‰ï¼ˆï¼Ÿï¼‰ã€‚ <br><br>  <i>å’Œä½ ï¼Œä»ç«ç„°ä¸­</i> <i><br></i>  <i>æ˜Ÿæ˜Ÿã€‚</i> <i><br></i>  <i>è·Ÿè¿œæ–¹çš„äººè¯´è¯ã€‚</i> <i><br><br></i>  <i>æ‹…å¿ƒæ‚¨çš„ç½—æ–¯ï¼Œæ˜å¤©ã€‚</i> <i><br></i>  <i>â€œé¸½å­é›¨ï¼Œ</i> <i><br></i>  <i>å’Œå‡¶æ‰‹çš„å®¶ï¼Œ</i> <i><br></i>  <i>ç»™å…¬ä¸»çš„å¥³å­©</i> <i><br></i>  <i>ä»–çš„è„¸ã€‚</i> <i><br><br></i>  <i>xxx</i> <i><br><br></i>  <i>å™¢ï¼Œç‰§ç¾Šäººï¼Œæ‘‡è¡æˆ¿é—´</i> <i><br></i>  <i>åœ¨æ˜¥å¤©çš„æ ‘æ—é‡Œã€‚</i> <i><br><br></i>  <i>æˆ‘è¦ç©¿è¿‡æˆ¿å­çš„ä¸­å¿ƒåˆ°æ± å¡˜</i> <i><br></i>  <i>å’Œè€é¼ æ´»æ³¼</i> <i><br></i>  <i>ä¸‹è¯ºå¤«å“¥ç½—å¾·çš„é’Ÿå£°ã€‚</i> <i><br><br></i>  <i>ä½†ä¸è¦æ‹…å¿ƒï¼Œæ™¨é£ï¼Œ</i> <i><br></i>  <i>ä¸€æ¡å°è·¯ï¼Œä¸€ä¸ªé“æ£ï¼Œ</i> <i><br></i>  <i>å’Œç‰¡è›ä¸€èµ·æ€è€ƒ</i> <i><br></i>  <i>è—åœ¨æ± å¡˜ä¸Š</i> <i><br></i>  <i>åœ¨è´«å›°çš„æ‹‰åŸºç‰¹ã€‚</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN470035/">https://habr.com/ru/post/zh-CN470035/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN470021/index.html">GRASPæ¨¡æ¿ä¸­çš„è´«è¡€å’Œä¸°å¯Œæ¨¡å‹</a></li>
<li><a href="../zh-CN470023/index.html">æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨Telebotåº“çš„ç¬¬3éƒ¨åˆ†åœ¨pythonä¸­ä¸ºTelegramæœºå™¨äººç¼–å†™ä»˜æ¬¾</a></li>
<li><a href="../zh-CN470027/index.html">VK Hackathon 2019ï¼ˆåŸæ ·ï¼‰</a></li>
<li><a href="../zh-CN470029/index.html">æç«¯æ•™å­¦æ³•ï¼šâ€œæˆ‘ä»¬çŸ¥é“â€ä¸å„¿ç«¥è¿›è¡Œé•¿æœŸæ²»ç–—çš„å…³ç³»</a></li>
<li><a href="../zh-CN470033/index.html">Fï¼ƒ2ï¼šFSIç¯å¢ƒ</a></li>
<li><a href="../zh-CN470037/index.html">Fï¼ƒ3ï¼šæ–‡æœ¬æ ¼å¼</a></li>
<li><a href="../zh-CN470043/index.html">æˆ‘ä»¬çš„å¤§è„‘å¦‚ä½•è¿ä½œä»¥åŠæŠ€æœ¯å’Œç¯å¢ƒå¦‚ä½•æä¾›å¸®åŠ©çš„èƒŒåçš„ç§‘å­¦</a></li>
<li><a href="../zh-CN470045/index.html">Visual Studio for Macï¼šæ–°ç¼–è¾‘å™¨çš„ä¸»è¦åŠŸèƒ½</a></li>
<li><a href="../zh-CN470049/index.html">åœ¨Macçš„Visual Studioä¸­å¼•å…¥è§£å†³æ–¹æ¡ˆçº§åˆ«çš„NuGetç¨‹åºåŒ…ç®¡ç†</a></li>
<li><a href="../zh-CN470051/index.html">ç¥ç»ç§‘å­¦ï¼šä½•æ—¶æˆ‘ä»¬çš„å¤§è„‘è¿è½¬æœ€ä½³ï¼Œä»¥åŠæŠ€æœ¯å¦‚ä½•å¸®åŠ©å®ƒ</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>