<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßëüèø‚Äçü§ù‚Äçüßëüèº üëÅÔ∏è üöî M√©thodes quasi-newtoniennes ou quand il y a trop de d√©riv√©es secondes pour Athos üëºüèª üçò ‚úäüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="√Ä la premi√®re connaissance des m√©thodes quasi-newtoniennes, on peut √™tre surpris deux fois. Premi√®rement, apr√®s un rapide coup d'≈ìil aux formules, des...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>M√©thodes quasi-newtoniennes ou quand il y a trop de d√©riv√©es secondes pour Athos</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/470638/">  √Ä la premi√®re connaissance des m√©thodes quasi-newtoniennes, on peut √™tre surpris deux fois.  Premi√®rement, apr√®s un rapide coup d'≈ìil aux formules, des doutes surgissent que cela puisse fonctionner.  Cependant, ils fonctionnent.  De plus, il semble douteux qu'ils fonctionnent bien.  Et il est d‚Äôautant plus surprenant de voir √† quel point elles sont plus rapides que les diff√©rentes variations de descente en pente, non pas sur des t√¢ches sp√©cialement construites, mais sur des t√¢ches r√©elles tir√©es de la pratique.  Et si apr√®s cela, il y a encore des doutes m√©lang√©s avec de l'int√©r√™t, alors vous devez comprendre pourquoi ce quelque chose fonctionne. <br><a name="habracut"></a><br>  L'origine et les id√©es de base qui animent les m√©thodes de gradient, y compris la m√©thode de Newton, ont <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d√©j√† √©t√© examin√©es</a> .  √Ä savoir, nous nous sommes appuy√©s sur les informations sur le comportement de la fonction au voisinage de la position actuelle, ce qui nous donne une analyse math√©matique simple.  √Ä tout le moins, nous avons suppos√© que nous disposions d'informations sur les premiers d√©riv√©s.  Et si c'est tout ce dont nous disposons?  La descente en gradient est-elle notre phrase?  Bien s√ªr, oui, sauf si vous vous souvenez soudainement que nous avons affaire √† un <i>processus</i> dans lequel la fonction objectif est correctement trait√©e.  Et si oui, pourquoi ne pas utiliser les informations accumul√©es sur le comportement de la fonction pour rendre notre marche √† sa surface un peu moins aveugle? <br><br>  L'id√©e d'utiliser des informations sur le chemin parcouru est au c≈ìur de la plupart des moyens d'acc√©l√©rer les m√©thodes de descente.  Cet article pr√©sente l'une des m√©thodes les plus efficaces, mais pas les moins ch√®res, pour rendre compte de ce type d'informations, ce qui a conduit √† l'id√©e de m√©thodes quasi-newtoniennes. <br><br>  Afin de comprendre o√π croissent les jambes des m√©thodes quasi-newtoniennes et d'o√π vient le nom, nous devons √† nouveau revenir √† la m√©thode de minimisation bas√©e sur la solution directe de l'√©quation ponctuelle stationnaire <img src="https://habrastorage.org/getpro/habr/post_images/c43/946/5f9/c439465f905d9f366a2f4b3296306290.gif" title="&quot;\ bigtriangledown f = 0&quot;">  .  Tout comme la prise en compte de la m√©thode de Newton appliqu√©e √† la solution de cette √©quation nous a conduit √† la m√©thode d'optimisation du m√™me nom (qui, contrairement √† son anc√™tre, a une r√©gion de convergence globale), on peut s'attendre √† ce que la consid√©ration d'autres m√©thodes de r√©solution de syst√®mes d'√©quations non lin√©aires soit fructueuse. planifier des id√©es pour construire d'autres m√©thodes d'optimisation. <br><br><h2>  M√©thodes s√©cantes </h2><br>  Permettez-moi de vous rappeler que la m√©thode de Newton pour r√©soudre le syst√®me d'√©quations <img src="https://habrastorage.org/getpro/habr/post_images/0a0/ec7/804/0a0ec780406efe57ca6444290ccfde09.gif" title="&quot;F (x) = 0&quot;">  , est bas√© sur le remplacement au voisinage d'un point proche de la solution <img src="https://habrastorage.org/getpro/habr/post_images/779/0dd/0ef/7790dd0efb4a03a4c876741804d9b559.gif" title="x">  les fonctions <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  son approximation lin√©aire <img src="https://habrastorage.org/getpro/habr/post_images/d15/479/f23/d15479f235f0d60ce8837c9043a0d2cc.gif" title="&quot;L (p) = F (x) + J (x) p&quot;">  o√π <img src="https://habrastorage.org/getpro/habr/post_images/206/f34/999/206f349991c0724c2fdce788124abe1c.gif" title="&quot;J&quot;">  Est un op√©rateur lin√©aire qui, lorsqu'il <img src="https://habrastorage.org/getpro/habr/post_images/779/0dd/0ef/7790dd0efb4a03a4c876741804d9b559.gif" title="x">  est un vecteur et <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  a des d√©riv√©es partielles par rapport √† chaque variable, co√Øncide avec la matrice de Jacobi <img src="https://habrastorage.org/getpro/habr/post_images/4d2/826/ff6/4d2826ff6ba22f9f67cab70bfbe17a16.gif" title="&quot;J_ {ij} = \ dfrac {\ partial F_ {i}} {\ partial x_ {j}}&quot;">  .  Ensuite, l'√©quation est r√©solue <img src="https://habrastorage.org/getpro/habr/post_images/c9d/8be/3f2/c9d8be3f2d70054db890ea34e3409544.gif" title="&quot;L (p) = 0&quot;">  et pointer <img src="https://habrastorage.org/getpro/habr/post_images/2c4/a7b/e55/2c4a7be5582848bfbcdd9ee141e7d764.gif" title="&amp; quot; x '= x + p &amp; quot;">  comme une nouvelle approximation de la solution souhait√©e.  C'est simple et √ßa marche. <br><br>  Mais que faire si, pour une raison quelconque, nous ne pouvons pas calculer la matrice de Jacobi?  La premi√®re chose qui vient √† l'esprit dans ce cas est que si nous ne pouvons pas calculer analytiquement les d√©riv√©es partielles, alors nous pouvons bien obtenir une approximation num√©rique pour elles.  L'option la plus simple (bien que nullement la seule) pour une telle approximation peut √™tre la formule des diff√©rences finies droites: <img src="https://habrastorage.org/getpro/habr/post_images/149/708/f5b/149708f5b8bab4374023295557622e82.gif" title="&quot;\ dfrac {\ partial F_ {i}} {\ partial x_ {j}} \ approx \ dfrac {F_ {i} (x + h_ {j} e_ {j}) - F_ {i} (x)} { h_ {j}} &quot;">  o√π <img src="https://habrastorage.org/getpro/habr/post_images/459/e61/aa0/459e61aa08f7fe807167a596e7ebd8a9.gif" title="&quot;e_ {j}&quot;">  Est le j√®me vecteur de base.  La matrice compos√©e de telles approximations sera d√©sign√©e par <img src="https://habrastorage.org/getpro/habr/post_images/a51/533/990/a5153399048e881eb8661304792b8c81.gif" title="&quot;\ bar {J}&quot;">  .  Une analyse de la quantit√© de remplacement <img src="https://habrastorage.org/getpro/habr/post_images/206/f34/999/206f349991c0724c2fdce788124abe1c.gif" title="&quot;J&quot;">  sur <img src="https://habrastorage.org/getpro/habr/post_images/a51/533/990/a5153399048e881eb8661304792b8c81.gif" title="&quot;\ bar {J}&quot;">  dans la m√©thode de Newton, sa convergence affecte, un assez grand nombre d'ouvrages sont consacr√©s, mais dans ce cas nous nous int√©ressons √† un autre aspect.  A savoir, une telle approximation n√©cessite le calcul de la fonction en N points suppl√©mentaires, et, en outre, la fonction <img src="https://habrastorage.org/getpro/habr/post_images/6c4/afd/100/6c4afd1002ddcfa43d07afbc9f103a9d.gif" title="&quot;\ bar {L} (p) = F (x) + \ bar {J} p&quot;">  √† ces points <i>interpole la</i> fonction <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  , c'est-√†-dire <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c56/5c9/4b4/c565c94b4a37b9cd5f42fc1be92b2e15.gif" title="&quot;\ bar {L (} h_ {j} e_ {j}) = F (x) + h_ {j} \ dfrac {F (x + h_ {j} e_ {j}) - F (x)} {h_ {j}} = F (x) + F (x + h_ {j} e_ {j}) - F (x) = F (x + h_ {j} e_ {j}). &quot;"><br><br>  Toutes les approximations de la matrice de Jacobi n'ont pas cette propri√©t√©, mais chaque matrice d'une fonction affine qui a cette propri√©t√© est une approximation de la matrice de Jacobi.  En effet, si <img src="https://habrastorage.org/getpro/habr/post_images/88f/5c8/dd7/88f5c8dd7e9876a2d0e0980882f261da.gif" title="&quot;F (x + p_ {j}) = F (x) + J (x) p_ {j} + o \ left (\ left \ Vert p_ {j} \ right \ Vert ^ {2} \ right)&quot;">  et <img src="https://habrastorage.org/getpro/habr/post_images/5ad/e1e/ad2/5ade1ead2804a3bfaa8ffdf9122a179a.gif" title="&quot;\ bar {J} p_ {j} = F (x + p_ {j}) - F (x)&quot;">  puis √† <img src="https://habrastorage.org/getpro/habr/post_images/803/ca4/351/803ca4351b87edf1a13a2a2947772fa7.gif" title="&quot;\ left \ Vert p_ {j} \ right \ Vert \ rightarrow0 \ quad \ bar {J} (x) p_ {j} \ rightarrow J (x) p_ {j}&quot;">  .  Cette propri√©t√©, √† savoir la propri√©t√© d'interpolation, nous donne un moyen constructif de g√©n√©raliser la m√©thode de Newton. <br><br>  Soit <img src="https://habrastorage.org/getpro/habr/post_images/194/ad1/d42/194ad1d42aa4320679b9498748ceb78d.gif" title="&quot;\ bar {L} (p) = a + Ap&quot;">  - fonction satisfaisant √† l'exigence <img src="https://habrastorage.org/getpro/habr/post_images/06e/34e/3a7/06e34e3a7a0d0058ef351da74258a637.gif" title="&quot;\ bar {L} (p_ {i}) = F (x + p_ {i})&quot;">  pour un syst√®me de vecteurs lin√©airement ind√©pendants <img src="https://habrastorage.org/getpro/habr/post_images/cf2/deb/64e/cf2deb64e8b0e4d34902a32a5fd93b7b.gif" title="&quot;p_ {i}&quot;">  .  Ensuite, une telle fonction est appel√©e fonction <i>s√©cante</i> <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  et l'√©quation qui le d√©finit est <i>l'√©quation s√©cante</i> .  Si le syst√®me de vecteurs <img src="https://habrastorage.org/getpro/habr/post_images/cf2/deb/64e/cf2deb64e8b0e4d34902a32a5fd93b7b.gif" title="&quot;p_ {i}&quot;">  est complet (c'est-√†-dire qu'il y en a exactement N et qu'ils sont toujours lin√©airement ind√©pendants), et, en plus, le syst√®me de vecteurs <img src="https://habrastorage.org/getpro/habr/post_images/94f/cf5/579/94fcf55798902795ffb670e35359d2af.gif" title="&quot;\ left \ {F (x + p_ {i}), i = 1 \ dots N \ right \}&quot;">  lin√©airement ind√©pendant alors <img src="https://habrastorage.org/getpro/habr/post_images/77f/eec/dd2/77feecdd2ae9a4795d2f81f3eec18b1b.gif" title="&quot;\ bar {L}&quot;">  d√©fini de fa√ßon unique. <br><br>  Toute m√©thode bas√©e sur un changement local d'√©quation <img src="https://habrastorage.org/getpro/habr/post_images/0a0/ec7/804/0a0ec780406efe57ca6444290ccfde09.gif" title="&quot;F (x) = 0&quot;">  √©quation de la forme <img src="https://habrastorage.org/getpro/habr/post_images/ccb/557/80f/ccb55780f0c8e65187b0f4c9126be81c.gif" title="&quot;\ bar {L} (p) = 0&quot;">  o√π <img src="https://habrastorage.org/getpro/habr/post_images/77f/eec/dd2/77feecdd2ae9a4795d2f81f3eec18b1b.gif" title="&quot;\ bar {L}&quot;">  satisfait <i>l'√©quation s√©cante</i> , appel√©e <i>la m√©thode s√©cante</i> . <br><br>  Une bonne question se pose de savoir comment construire la s√©cante d'une fonction de la mani√®re la plus rationnelle. <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  .  Le raisonnement suivant semble √©vident: laissez un mod√®le affine √™tre construit au point x qui interpole la fonction donn√©e aux points <img src="https://habrastorage.org/getpro/habr/post_images/462/bcf/f32/462bcff32469c0ec5f8ccfc80534c05c.gif" title="&quot;x-x_ {1}, x-x_ {2}, \ points, x-x_ {N}&quot;">  .  Solution d'√©quation <img src="https://habrastorage.org/getpro/habr/post_images/ccb/557/80f/ccb55780f0c8e65187b0f4c9126be81c.gif" title="&quot;\ bar {L} (p) = 0&quot;">  nous donne un nouveau point <img src="https://habrastorage.org/getpro/habr/post_images/2c4/a7b/e55/2c4a7be5582848bfbcdd9ee141e7d764.gif" title="&amp; quot; x '= x + p &amp; quot;">  .  Ensuite, pour construire un mod√®le affine √† un point <img src="https://habrastorage.org/getpro/habr/post_images/787/cf7/c3a/787cf7c3a3d374114b3a07305b7fa446.gif" title="&amp; quot; x '&amp; quot;">  il est plus raisonnable de choisir des points d'interpolation pour que la valeur <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  d√©j√† connu - c'est-√†-dire, prenez-les de l'ensemble <img src="https://habrastorage.org/getpro/habr/post_images/333/297/225/33329722533f0b608b0994d2a5ba83fa.gif" title="&amp; quot; \ left \ {x'-x, x'-x_ {1}, x'-x_ {2}, \ dots, x'-x_ {N} \ right \} &amp; quot;">  .  Il existe diff√©rentes options pour choisir les points parmi les nombreux pr√©c√©demment utilis√©s.  Par exemple, vous pouvez prendre comme points d'interpolation ceux dans lesquels <img src="https://habrastorage.org/getpro/habr/post_images/bb3/e3a/cd5/bb3e3acd5043b859fe89006d4cabe5a0.gif" title="&quot;\ gauche \ Vert F \ droite \ Vert&quot;">  importe le moins ou juste le premier <img src="https://habrastorage.org/getpro/habr/post_images/055/8e9/3d9/0558e93d918ff32e873b6a71703e9969.gif" title="&quot;N&quot;">  des points.  En tout cas, il semble √©vident que <img src="https://habrastorage.org/getpro/habr/post_images/95f/756/92b/95f75692ba0aeefcef24ae42714dbc1b.gif" title="&amp; quot; p = x'-x &amp; quot;">  devrait √™tre inclus dans de nombreux points d'interpolation pour le nouveau mod√®le affine.  Au-del√† <img src="https://habrastorage.org/getpro/habr/post_images/f24/8e8/91e/f248e891effc6650d9d31fbefc54cbe4.gif" title="&quot;n&quot;">  les √©tapes du processus it√©ratif dans notre ensemble peuvent aller jusqu'√† <img src="https://habrastorage.org/getpro/habr/post_images/f24/8e8/91e/f248e891effc6650d9d31fbefc54cbe4.gif" title="&quot;n&quot;">  d√©placements construits sur des points pr√©c√©demment pass√©s.  Si le processus est construit de telle mani√®re que le nouveau mod√®le affine n'utilise plus <img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;">  des valeurs pr√©c√©dentes, alors un tel processus est appel√© la m√©thode s√©cante de p-point. <br><br>  √Ä premi√®re vue, il peut sembler que la m√©thode s√©cante √† N points est la meilleure candidate pour remplacer la m√©thode de Newton, car elle utilise au maximum les informations que nous obtenons dans le processus de r√©solution, tout en minimisant le nombre de calculs suppl√©mentaires - nous utilisons la valeur de la fonction dans cette derni√®re N points pass√©s.  Ce n'est malheureusement pas le cas.  Le fait est que le syst√®me vectoriel <img src="https://habrastorage.org/getpro/habr/post_images/ed0/117/8ca/ed01178ca46506fa4588780d16d705a1.gif" title="&quot;F (x_ {0}), F (x_ {1}), \ points F (x_ {N})&quot;">  refuse obstin√©ment d'√™tre ind√©pendant lin√©airement avec un N. suffisamment grand. De plus, m√™me si cette condition s'av√®re √™tre remplie et que le mod√®le affine correspondant existe toujours, alors il y a une chance que les directions <img src="https://habrastorage.org/getpro/habr/post_images/602/ff2/50c/602ff250c473d5b28e08a1453d4175b3.gif" title="&quot;p_ {j} = x_ {j} -x_ {0}&quot;">  s'av√®rent √©galement lin√©airement ind√©pendants, il s'av√®re encore moins.  Et cela implique le fait que le mod√®le affine, bien qu'il existe, est d√©g√©n√©r√© et pratiquement inadapt√©. <br><br>  En g√©n√©ral, la plus stable est la m√©thode s√©cante √† 2 points.  Autrement dit, une m√©thode dans laquelle, √† chaque it√©ration, nous devons calculer des valeurs N-1 suppl√©mentaires de la fonction.  Cela ne convient manifestement pas √† nos fins pratiques. <br><br>  Alors la question est - qu'est-ce que tout cela? <br><br><h2>  M√©thodes quasi-newtoniennes pour r√©soudre des √©quations </h2><br><br>  La sortie est simple, mais pas √©vidente.  Si nous n'avons pas la capacit√© technique, sur la base des valeurs d√©j√† calcul√©es, de d√©terminer de mani√®re unique le mod√®le affine qui satisfait l'√©quation s√©cante, alors ce n'est pas n√©cessaire.  Nous prenons l'√©quation des s√©cantes comme base, mais nous exigerons qu'elle ne soit satisfaite que pour un syst√®me incomplet de vecteurs <img src="https://habrastorage.org/getpro/habr/post_images/e5e/f2b/432/e5ef2b43292735aa2a68afffb80bf520.gif" title="&quot;\ left \ {p_ {1}, p_ {2}, \ dots, p_ {m} \ right \}, m &amp; Lt; N&quot;">  .  En d'autres termes, nous exigerons que la condition d'interpolation ne soit satisfaite que pour un nombre suffisamment petit de valeurs connues.  Bien s√ªr, dans ce cas, nous ne pouvons plus garantir que la matrice utilis√©e dans un tel mod√®le tendra vers la matrice de Jacobi, mais nous n'en aurons pas besoin.  De plus, le mod√®le affine doit interpoler la fonction au point courant, c'est-√†-dire <img src="https://habrastorage.org/getpro/habr/post_images/3b9/9d1/7fa/3b99d17fa378aeaf36097faef3830bd5.gif" title="&quot;\ bar {L} (0) = F (x)&quot;">  , nous obtenons la formulation suivante de la m√©thode s√©cante: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/78c/f30/421/78cf304219a4bc90d4f900062bf2d027.gif" title="&quot;\\ \ bar {L} (p_ {i}) = F (x) + Ap_ {i} = F (x + p_ {i}), \ quad i = 1 \ points m \\ \ bar {L} (p) = 0 \ quad \ Rightarrow p = A ^ {- 1} F (x) &quot;"><br><br>  Bruiden a √©t√© le premier √† envisager des m√©thodes de ce type pour m = 1, les qualifiant de quasi-newtoniennes.  Il est clair que la condition s√©cante dans ce cas nous permet d'identifier de mani√®re unique la matrice <img src="https://habrastorage.org/getpro/habr/post_images/c9d/999/d9a/c9d999d9a4e8bd3d6f8e50519d1dfaa8.gif" title="&quot;A&quot;">  seulement si des conditions suppl√©mentaires lui sont impos√©es, et chacune de ces conditions suppl√©mentaires donne lieu √† une m√©thode distincte.  Bruyden lui-m√™me a raisonn√© comme suit: <br><br>  <i>comme le mouvement dans le sens</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;"></i>  <i>du point</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/460/82f/7d6/46082f7d6471c3fabb832d8f94075758.gif" title="&quot;x_ {0}&quot;"></i>  <i>au point</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/1d0/56f/301/1d056f3016bc715aacc23418d8629173.gif" title="&quot;x_ {1}&quot;"></i>  <i>ne nous donne aucune information suppl√©mentaire sur la fa√ßon dont la fonction change</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;"></i>  <i>directions, puis l'effet de la nouvelle fonction affine sur le vecteur</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/9fc/c76/a21/9fcc76a21130891ea5d5b10efa979bff.gif" title="q"></i>  <i>devrait diff√©rer de l'effet de l'ancienne fonction sur le m√™me vecteur moins le plus diff√©rent</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/9fc/c76/a21/9fcc76a21130891ea5d5b10efa979bff.gif" title="q"></i>  <i>de</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;"></i>  <i>.</i>  <i>En dernier recours, lorsque</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/9fc/c76/a21/9fcc76a21130891ea5d5b10efa979bff.gif" title="q"></i>  <i>orthogonale</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;"></i>  <i>, le comportement de la nouvelle fonction ne doit pas √™tre diff√©rent de celui de l'ancienne.</i> <i><br></i> <br>  L'id√©e de Breiden est brillante dans sa simplicit√©.  En effet, si nous ne disposons pas de nouvelles informations sur le comportement de la fonction, le mieux que nous puissions faire est d'essayer de ne pas encrasser l'ancienne.  Ensuite, la condition suppl√©mentaire <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0b3/d43/d20/0b3d43d207b144b926274d0c81abccbf.gif" title="&quot;\ bar {L} _ {1} q = \ bar {L} _ {0} q&quot;">  pour tous <img src="https://habrastorage.org/getpro/habr/post_images/9fc/c76/a21/9fcc76a21130891ea5d5b10efa979bff.gif" title="q">  tel que <img src="https://habrastorage.org/getpro/habr/post_images/c16/9f6/315/c169f6315171249a34b50b26a2975c6e.gif" title="&quot;q ^ {T} p = 0&quot;"><br><br>  vous permet de d√©terminer de mani√®re unique la matrice de la nouvelle transformation - elle est obtenue en ajoutant une correction de rang 1 √† l'ancienne matrice. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/522/f36/1f9/522f361f94a7b5a2e9da68094983b21d.gif" title="&quot;\\ A_ {1} = A_ {0} + \ dfrac {(y-A_ {0} p) p ^ {T}} {p ^ {T} p} \\ y = F (x_ {0}) -F (x_ {1}) &quot;"><br><br>  Cependant, malgr√© la simplicit√© et la coh√©rence des conclusions de Bruiden, elles ne fournissent pas le point d'appui qui pourrait servir de base √† la construction d'autres m√©thodes similaires.  Heureusement, il existe une expression plus formelle de son id√©e.  √Ä savoir, la matrice construite de cette fa√ßon <img src="https://habrastorage.org/getpro/habr/post_images/147/7e7/ca0/1477e7ca06155c3e43fd4a640e0f7f98.gif" title="&quot;A_ {1}&quot;">  Il s'av√®re √™tre la solution au probl√®me suivant: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/028/96f/3fa/02896f3facb898d70f26abad02fe90a9.gif" title="&quot;\\ \ left \ Vert A_ {1} -A_ {0} \ right \ Vert _ {F} \ rightarrow \ min \\ F (x_ {1}) - Ap = F (x_ {0})&quot;"><br><br>  La contrainte de t√¢che n'est rien d'autre que l'√©quation s√©cante, et la condition de minimisation refl√®te notre d√©sir de sauvegarder autant d'informations que possible dans la matrice <img src="https://habrastorage.org/getpro/habr/post_images/107/a45/803/107a45803b226180325815eaa7be8706.gif" title="&quot;A_ {0}&quot;">  .  La mesure de l'√©cart entre les matrices dans ce cas est la norme Frobenius, dans laquelle le probl√®me pos√© a une solution non ambigu√´.  Cette formulation pourrait bien servir de point de d√©part √† la construction d'autres m√©thodes.  √Ä savoir, nous pouvons changer √† la fois la <i>mesure</i> par laquelle nous √©valuons les changements introduits et resserrer les <i>conditions</i> impos√©es √† la matrice.  En g√©n√©ral, on peut d√©j√† travailler avec une telle formulation de la m√©thode. <br><br><h2>  M√©thodes d'optimisation Quasi-Newton </h2><br><br>  Ayant compris l'id√©e principale, nous pouvons enfin revenir √† des probl√®mes d'optimisation et remarquer que l'application de la formule de Bruyden pour recalculer le mod√®le affine ne correspond pas tr√®s bien √† notre t√¢che.  En fait, la premi√®re d√©riv√©e de la fonction de gradient <img src="https://habrastorage.org/getpro/habr/post_images/6b8/82e/be7/6b882ebe727121dcb5fc21b091044b5a.gif" title="&quot;\ bigtriangledown f&quot;">  il n'y a rien d'autre que la matrice de Hesse, qui par construction est sym√©trique.  Dans le m√™me temps, la mise √† jour selon la r√®gle de Bruyden conduit √† une matrice asym√©trique <img src="https://habrastorage.org/getpro/habr/post_images/147/7e7/ca0/1477e7ca06155c3e43fd4a640e0f7f98.gif" title="&quot;A_ {1}&quot;">  m√™me si <img src="https://habrastorage.org/getpro/habr/post_images/107/a45/803/107a45803b226180325815eaa7be8706.gif" title="&quot;A_ {0}&quot;">  √©tait sym√©trique.  Cela ne signifie pas que la m√©thode de Bruden ne peut pas √™tre appliqu√©e pour r√©soudre l'√©quation ponctuelle stationnaire, mais sur la base d'une telle r√®gle de mise √† jour, il est peu probable que nous puissions construire de bonnes m√©thodes d'optimisation.  En g√©n√©ral, il est assez √©vident que la m√©thode quasi-Newton devrait fonctionner mieux, plus le syst√®me de conditions du probl√®me d√©crit avec pr√©cision les sp√©cificit√©s d'une matrice Jacobi sp√©cifique. <br><br>  Pour corriger cet inconv√©nient, nous ajoutons une contrainte suppl√©mentaire au probl√®me de minimisation de Bruden, exigeant explicitement que la nouvelle matrice soit sym√©trique avec l'ancienne: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/03e/167/aa2/03e167aa25e0f4aa6b8df8546552e79a.gif" title="&quot;\\ \ left \ Vert A_ {1} -A_ {0} \ right \ Vert _ {F} \ rightarrow \ min \\ \ bigtriangledown f (x_ {1}) - Ap = \ bigtriangledown f (x_ {0} ) \\ A_ {1} ^ {T} = A_ {1} &quot;"><br><br>  La solution √† ce probl√®me est <br><br><img src="https://habrastorage.org/getpro/habr/post_images/df8/356/74b/df835674b94bab190bca3c18efed98ce.gif" title="&quot;A_ {1} = A_ {0} + \ dfrac {(y-A_ {0} p) p ^ {T} + p (y-A_ {0} p) ^ {T}} {p ^ {T} p} - \ dfrac {(y-A_ {0} p) ^ {T} p} {\ gauche (p ^ {T} p \ droite) ^ {2}} pp ^ {T} &quot;"><br><br>  Ici <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/89c/f84/f29/89cf84f292ed72d1b20755677688a054.gif" title="y = \ bigtriangledown f (x_ {1}) - \ bigtriangledown f (x_ {0})"></a>  et la formule de recalcul de la matrice porte le nom de ses cr√©ateurs - Powell, Shanno et Bruyden (PSB).  La matrice r√©sultante est sym√©trique, mais clairement pas d√©finie positive, ne serait-ce que soudainement <img src="https://habrastorage.org/getpro/habr/post_images/6c7/040/47d/6c704047d3148fd7a8b563aaf79dd7f4.gif" title="&quot;y&quot;">  ne sera pas colin√©aire <img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;">  .  Et nous avons <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">vu</a> qu'une certitude positive est hautement souhaitable dans les m√©thodes d'optimisation. <br><br>  Encore une fois, nous corrigerons l'√©tat du probl√®me, en utilisant cette fois la norme de Frobenius mise √† l'√©chelle comme mesure de la divergence matricielle. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f37/eb1/0f4/f37eb10f4eb10d0c54acc9adab962f10.gif" title="&quot;\\ \ left \ Vert T ^ {- T} \ left (A_ {1} -A_ {0} \ right) T ^ {- 1} \ right \ Vert _ {F} \ rightarrow \ min \\ \ bigtriangledown f (x_ {1}) - Ap = \ bigtriangledown f (x_ {0}) \\ A_ {1} ^ {T} = A_ {1} &quot;"><br><br>  L'origine d'un tel √©nonc√© de la question est un grand sujet distinct, mais il est int√©ressant de noter que si la matrice T est telle que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/673/872/131/673872131fa6cb0f44e6839be0e448e7.gif" title="T ^ {T} T = G, Gp = y"></a>  (c'est-√†-dire que G est √©galement une matrice de transformation affine satisfaisant l'√©quation s√©cante pour la direction p), alors la solution √† ce probl√®me se r√©v√®le √™tre ind√©pendante du choix de T et conduit √† la formule de mise √† jour <br><br><img src="https://habrastorage.org/getpro/habr/post_images/135/ea6/c14/135ea6c14ea8f63f961e83576f1be5d5.gif" title="&quot;A_ {1} = A_ {0} + \ dfrac {(y-A_ {0} p) y ^ {T} + y (y-A_ {0} p) ^ {T}} {y ^ {T} p} - \ dfrac {\ gauche (y-A_ {0} p \ droite) ^ {T} p} {\ gauche (y ^ {T} p \ droite) ^ {2}} yy ^ {T} &quot;"><br><br>  connue sous le nom de formule Davidon-Fletcher-Powell.  Cette m√©thode de mise √† jour a fait ses preuves dans la pratique, car elle a la propri√©t√© suivante: <br><br>  <i>si</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/e3e/c44/1c1/e3ec441c17e1b43df108a7d8e15d3dd6.gif" title="&quot;y ^ {T} p &amp; gt; 0&quot;"></i>  <i>et</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/107/a45/803/107a45803b226180325815eaa7be8706.gif" title="&quot;A_ {0}&quot;"></i>  <i>positif d√©fini alors</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/147/7e7/ca0/1477e7ca06155c3e43fd4a640e0f7f98.gif" title="&quot;A_ {1}&quot;"></i>  <i>√©galement identifi√©es positivement.</i> <br><br>  Je note ensuite que si la premi√®re condition n'est pas remplie, alors il n'existe pas de fonction affine avec une matrice d√©finie positive qui satisfasse l'√©quation s√©cante. <br><br>  Si dans le probl√®me conduisant √† la m√©thode DFP, on prend, comme mesure de la divergence des mod√®les affines, la distance non pas entre les matrices elles-m√™mes, mais entre les matrices inverses, on obtient un probl√®me de forme <br><br><img src="https://habrastorage.org/getpro/habr/post_images/337/0b0/af2/3370b0af216ab9695789eeb586cf3604.gif" title="&quot;\\ \ left \ Vert T ^ {- T} \ left (A_ {1} ^ {- 1} -A_ {0} ^ {- 1} \ right) T ^ {- 1} \ right \ Vert _ { F} \ rightarrow \ min \\ \ bigtriangledown f (x_ {1}) - Ap = \ bigtriangledown f (x_ {0}) \\ A_ {1} ^ {T} = A_ {1} &quot;"><br><br>  Sa solution est une formule bien connue, d√©couverte presque simultan√©ment par Breiden, Fletcher, Goldfarb et Shanno (BFGS). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3fa/840/c7b/3fa840c7b6ec3de81eb02bb0e9240722.gif" title="&quot;A_ {1} = A_ {0} + \ dfrac {yy ^ {T}} {y ^ {T} p} - \ dfrac {A_ {0} pp ^ {T} A_ {0}} {p ^ { T} A_ {0} p} &quot;"><br><br>  √Ä ce jour, on pense que le recalcul selon cette formule est le plus efficace d'un point de vue informatique et en m√™me temps moins sujet √† la d√©g√©n√©rescence de la matrice avec un grand nombre d'it√©rations.  Dans les m√™mes conditions que DFP, cette formule conserve la propri√©t√© du caract√®re d√©finitif positif. <br><br>  Toutes les m√©thodes d√©crites pour la mise √† jour de la matrice n√©cessitent une correction de rang 2. Cela permet d'inverser facilement et facilement la matrice <img src="https://habrastorage.org/getpro/habr/post_images/147/7e7/ca0/1477e7ca06155c3e43fd4a640e0f7f98.gif" title="&quot;A_ {1}&quot;">  en utilisant la formule Sherman-Morrison et la valeur <img src="https://habrastorage.org/getpro/habr/post_images/5f6/3ac/2d9/5f63ac2d91f47a730fee01b5db38f3bd.gif" title="&quot;A_ {0} ^ {- 1}&quot;">  . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ed0/9f8/002/ed09f80027e56f58a3502cc943758509.gif" title="&quot;B_ {1} = B_ {0} + uv ^ {T} \ Rightarrow B_ {1} ^ {- 1} = B_ {0} ^ {- 1} + \ dfrac {B_ {0} ^ {- 1} uv ^ {T} B_ {0} ^ {- 1}} {1 + v ^ {T} B_ {0} ^ {- 1} u} &quot;"><br><br>  √† condition que le d√©nominateur de la formule soit non nul.  Je ne donnerai pas de formules sp√©cifiques pour mettre √† jour les matrices inverses des m√©thodes list√©es, car elles sont faciles √† trouver ou √† d√©river ind√©pendamment.  La seule chose √† noter dans ce cas est que les variantes des m√©thodes de mise √† jour de la matrice inverse sont g√©n√©ralement beaucoup moins stables (c'est-√†-dire qu'elles souffrent davantage d'erreurs d'arrondi) que celles qui sugg√®rent de mettre √† jour la matrice d'origine.  Il est plus efficace de mettre √† jour non pas la matrice elle-m√™me, mais sa d√©composition Cholesky (√† moins, bien s√ªr, qu'une telle d√©composition ait lieu), car une telle option de mise en ≈ìuvre est plus stable num√©riquement et, en outre, minimise le co√ªt de r√©solution d'une √©quation qui d√©termine la direction du mouvement. <br><br>  Il reste √† examiner la question de savoir √† quoi devrait ressembler la toute premi√®re matrice dans le processus quasi-newtonien.  Tout est √©vident ici - plus elle est proche de la matrice de Hesse ou de sa version corrig√©e, si la Hesse ne se r√©v√®le pas soudainement d√©finie positive, mieux ce sera du point de vue de la convergence.  Cependant, en principe, toute matrice d√©finie positive peut nous convenir.  La version la plus simple d'une telle matrice est une seule, puis la premi√®re it√©ration co√Øncide avec l'it√©ration de la descente de gradient.  Fletcher et Powell ont montr√© (naturellement, pour la m√©thode DFP) que si la fonction quadratique est minimis√©e, quelle que soit la matrice (d√©finie positive) utilis√©e comme it√©ration DFP initiale, elles conduiront √† une solution dans exactement N it√©rations, o√π N est dimension du probl√®me, et la matrice quasi-newtonienne co√Øncide avec la matrice de Hesse au point minimum.  Dans le cas g√©n√©ral non lin√©aire d'un tel bonheur, nous n'attendrons bien s√ªr pas, mais cela donne au moins des raisons de ne pas trop se soucier du mauvais choix de la matrice initiale. <br><br><h2>  Conclusion </h2><br><br>  L'approche d√©crite de la construction de m√©thodes quasi-newtoniennes n'est pas la seule possible.  Au minimum, les d√©couvreurs des m√©thodes quasi-newtoniennes d√©crites et de nombreux chercheurs ult√©rieurs sont arriv√©s aux m√™mes formules bas√©es sur des consid√©rations compl√®tement diff√©rentes.  Cependant, il est int√©ressant de noter que d√®s qu'une certaine m√©thode quasi-newtonienne est apparue, quelle que soit la m√©thode d'obtention, apr√®s un temps assez court, il est devenu clair qu'il s'agissait d'une solution √† un probl√®me d'optimisation tr√®s facilement interpr√©table.  √Ä mon avis, il est remarquable qu'il soit possible d'apporter un d√©nominateur commun pour des m√©thodes aussi diverses, car cela fournit la base pour construire d'autres m√©thodes qui prennent mieux en compte les sp√©cificit√©s d'une t√¢che particuli√®re.  En particulier, il existe des m√©thodes quasi-Newton con√ßues pour mettre √† jour des matrices clairsem√©es, des m√©thodes dans lesquelles le plus petit nombre possible d'√©l√©ments subissent des modifications, et bien d'autres seraient un fantasme. <br><br>  Il convient √©galement de noter que les m√©thodes de m√©triques variables, malgr√© leur nom, ne conduisent pas toujours √† la construction de matrices, qui sont en fait des m√©triques, bien qu'elles le fassent chaque fois que cela est possible.      ,              ,          ‚Äî ,      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> -</a> . ,           ,        .         ,       .      ,         ‚Äî . <br><br>  ,                  .      ,         ( ,   ,       N ,    ,     ).           (  ,     ,     ),      .   ,   ,             ‚Äî .      ‚Äî    . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr470638/">https://habr.com/ru/post/fr470638/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr470620/index.html">Infrastructure en tant que code: comment surmonter les probl√®mes avec XP</a></li>
<li><a href="../fr470622/index.html">Pr√©sentation des m√©thodes de s√©lection des fonctionnalit√©s</a></li>
<li><a href="../fr470628/index.html">Construction navale de simulateur de vaisseau spatial</a></li>
<li><a href="../fr470632/index.html">Arend - Langage de type d√©pendant bas√© sur HoTT (partie 2)</a></li>
<li><a href="../fr470634/index.html">Identifiez la communaut√© intercommunautaire sur Instagram pour identifier les int√©r√™ts des utilisateurs</a></li>
<li><a href="../fr470640/index.html">Dimensionnement d'Elasticsearch</a></li>
<li><a href="../fr470642/index.html">D√©couvrez Yandex.Station Mini. Grande histoire d'un petit appareil</a></li>
<li><a href="../fr470644/index.html">Pourquoi les blogs d'entreprise sont parfois aigris: quelques observations et conseils</a></li>
<li><a href="../fr470646/index.html">Math√©matiques pour la science des donn√©es. Nouveau cours d'OTUS</a></li>
<li><a href="../fr470648/index.html">IBM LTO-8 - Un moyen facile de stocker des donn√©es froides</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>