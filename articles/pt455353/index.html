<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üåõ ‚è∞ üõ©Ô∏è Acelera√ß√£o de hardware de redes neurais profundas: GPU, FPGA, ASIC, TPU, VPU, IPU, DPU, NPU, RPU, NNP e outras letras üçú üï£ üèë</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Em 14 de maio, quando Trump estava se preparando para lan√ßar todos os c√£es na Huawei, sentei-me pacificamente em Shenzhen no Huawei STW 2019 - uma gra...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Acelera√ß√£o de hardware de redes neurais profundas: GPU, FPGA, ASIC, TPU, VPU, IPU, DPU, NPU, RPU, NNP e outras letras</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/455353/"><img src="https://habrastorage.org/getpro/habr/post_images/1d8/7a7/de6/1d87a7de6c72f1f712049ce550978d7c.png"><br><br>  Em 14 de maio, quando Trump estava se preparando para lan√ßar todos os c√£es na Huawei, sentei-me pacificamente em Shenzhen no Huawei STW 2019 - uma grande confer√™ncia para 1000 participantes - que inclu√≠a relat√≥rios de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Philip Wong</a> , vice-presidente de pesquisa da TSMC sobre as perspectivas da computa√ß√£o n√£o-von Neumann arquiteturas, e Heng Liao, Huawei Fellow, Cientista Chefe Huawei 2012 Lab, no desenvolvimento de uma nova arquitetura de processadores tensoriais e neuroprocessadores.  O TSMC, se voc√™ sabe, produz aceleradores neurais para Apple e Huawei usando a tecnologia de 7 nm (que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">poucas pessoas possuem</a> ), e a Huawei est√° pronta para competir com o Google e a NVIDIA por neuroprocessadores. <br><br>  O Google na China est√° proibido, eu n√£o me preocupei em colocar uma VPN no tablet, ent√£o usei o Yandex de forma <s>patri√≥tica</s> para ver qual √© a situa√ß√£o com outros fabricantes de ferro semelhante e o que geralmente acontece.  Em geral, observei a situa√ß√£o, mas somente depois desses relat√≥rios percebi o qu√£o em grande escala a revolu√ß√£o estava sendo preparada nas entranhas das empresas e no sil√™ncio das salas cient√≠ficas. <br><br>  Somente no ano passado, foram investidos mais de US $ 3 bilh√µes no t√≥pico.  O Google h√° muito tempo declara as redes neurais uma √°rea estrat√©gica, est√° construindo ativamente seu suporte de hardware e software.  A NVIDIA, sentindo que o trono √© impressionante, est√° fazendo esfor√ßos fant√°sticos nas bibliotecas de acelera√ß√£o de redes neurais e no novo hardware.  Em 2016, a Intel gastou 0,8 bilh√µes para comprar duas empresas envolvidas na acelera√ß√£o de hardware de redes neurais.  E isso apesar do fato de que as principais compras ainda n√£o come√ßaram e o n√∫mero de jogadores excedeu cinquenta e est√° crescendo rapidamente. <br><br><div style="text-align:center;"><img width="66%" src="https://habrastorage.org/getpro/habr/post_images/74c/308/a37/74c308a372700574cc0e29f13347ede5.png"></div><br>  TPU, VPU, IPU, DPU, NPU, RPU, NNP - o que tudo isso significa e quem vencer√°?  Vamos tentar descobrir.  Quem se importa - Bem-vindo ao gato! <br><a name="habracut"></a><br><hr>  <b><font color="#ff0000">Isen√ß√£o de responsabilidade: o</font></b> autor teve que reescrever completamente os algoritmos de processamento de v√≠deo para uma implementa√ß√£o efetiva no ASIC, e os clientes fizeram prot√≥tipos no FPGA, para que haja uma id√©ia da profundidade da diferen√ßa nas arquiteturas.  No entanto, o autor n√£o trabalhou diretamente com ferro recentemente.  Mas ele antecipa que ter√° que se aprofundar nisso. <br><br><h2>  Antecedentes dos problemas </h2><br>  O n√∫mero de c√°lculos necess√°rios est√° crescendo rapidamente, as pessoas gostariam de ter mais camadas, mais op√ß√µes de arquitetura, jogar mais ativamente com hiperpar√¢metros, mas ... depende do desempenho.  Ao mesmo tempo, por exemplo, com o crescimento da produtividade dos bons e antigos processadores - grandes problemas.  Todas as coisas boas chegam ao fim: a lei de Moore, como voc√™ sabe, est√° se esgotando e a taxa de crescimento do desempenho do processador cai: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/704/8e8/bab/7048e8bab326e1f4f62058e5f3a925f4.png"></div><br>  <i>C√°lculos do desempenho real de opera√ß√µes inteiras no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SPECint</a> em compara√ß√£o com o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">VAX11-780</a> , daqui em diante frequentemente uma escala logar√≠tmica</i> <br><br>  Se de meados da d√©cada de 80 a meados da d√©cada de 2000 - nos anos aben√ßoados do auge dos computadores - o crescimento foi a uma taxa m√©dia de 52% ao ano, nos √∫ltimos anos diminuiu para 3% ao ano.  E isso √© um problema (uma tradu√ß√£o de um artigo recente do patriarca John Hennessey sobre os problemas e as perspectivas da arquitetura moderna <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">estava em Habr√©</a> ). <br><br>  H√° muitas raz√µes, por exemplo, a frequ√™ncia dos processadores parou de crescer: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/a49/5d6/be5/a495d6be5c00ce436bb4c2f1f7f356fc.png"></div><br>  Tornou-se mais dif√≠cil reduzir o tamanho dos transistores.  O √∫ltimo infort√∫nio que reduz drasticamente a produtividade (incluindo o desempenho das CPUs j√° lan√ßadas) √© (rolo de tambor) ... certo, seguran√ßa.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Fus√£o</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Spectre</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">outras</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">vulnerabilidades</a> causam enormes danos √† taxa de crescimento da pot√™ncia de processamento da CPU ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">um exemplo de desativa√ß√£o do hyperthreading</a> (!)).  O t√≥pico se tornou popular e novas vulnerabilidades desse tipo s√£o encontradas <i>quase que mensalmente</i> .  E isso √© algum tipo de pesadelo, porque d√≥i em termos de desempenho. <br><br>  Ao mesmo tempo, o desenvolvimento de muitos algoritmos est√° firmemente ligado ao crescimento familiar da pot√™ncia do processador.  Por exemplo, hoje muitos pesquisadores n√£o est√£o preocupados com a velocidade dos algoritmos - eles ter√£o alguma coisa.  E seria bom aprender - as redes se tornam grandes e "dif√≠ceis" de usar.  Isso √© especialmente evidente no v√≠deo, para o qual a maioria das abordagens, em princ√≠pio, n√£o √© aplic√°vel em alta velocidade.  E eles costumam fazer sentido apenas em tempo real.  Isso tamb√©m √© um problema. <br><br>  Da mesma forma, novos padr√µes de compress√£o est√£o sendo desenvolvidos, o que implica um aumento na pot√™ncia do decodificador.  E se a energia do processador n√£o aumentar?  A gera√ß√£o mais velha lembra como, nos anos 2000, houve problemas na reprodu√ß√£o de v√≠deo em alta defini√ß√£o no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">H.264</a> ent√£o fresco em computadores mais antigos.  Sim, a qualidade foi melhor com um tamanho menor, mas em cenas r√°pidas a imagem foi interrompida ou o som foi rasgado.  Eu tenho que me comunicar com os desenvolvedores do novo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">VVC / H.266</a> (um lan√ßamento est√° planejado para o pr√≥ximo ano).  Voc√™ n√£o os invejar√°. <br><br>  Ent√£o, o que o pr√≥ximo s√©culo nos prepara √† luz da diminui√ß√£o da taxa de crescimento do desempenho do processador aplicado √†s redes neurais? <br><br><h2>  CPU </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/f1e/b44/44b/f1eb4444b3e5e320447c6f65fa4ef14c.png"><br><br>  Uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CPU</a> comum √© um grande n√∫mero de britadores aperfei√ßoados h√° d√©cadas.  Infelizmente, para outras tarefas. <br><br>  Quando trabalhamos com redes neurais, especialmente as profundas, nossa pr√≥pria rede pode ocupar centenas de megabytes.  Por exemplo, os requisitos de mem√≥ria das redes de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">detec√ß√£o</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">objetos s√£o os</a> seguintes: <br><div class="scrollable-table"><table><tbody><tr><td>  modelo <br></td><td>  tamanho de entrada <br></td><td>  mem√≥ria param <br></td><td>  mem√≥ria caracter√≠stica <br></td></tr><tr><td>  <a href="">rfcn-res50-pascal</a> <br></td><td>  600 x 850 <br></td><td>  122 MB <br></td><td>  1 GB <br></td></tr><tr><td>  <a href="">rfcn-res101-pascal</a> <br></td><td>  600 x 850 <br></td><td>  194 MB <br></td><td>  2 GB <br></td></tr><tr><td>  <a href="">ssd-pascal-vggvd-300</a> <br></td><td>  300 x 300 <br></td><td>  100 MB <br></td><td>  116 MB <br></td></tr><tr><td>  <a href="">ssd-pascal-vggvd-512</a> <br></td><td>  512 x 512 <br></td><td>  104 MB <br></td><td>  337 MB <br></td></tr><tr><td>  <a href="">ssd-pascal-mobilenet-ft</a> <br></td><td>  300 x 300 <br></td><td>  22 MB <br></td><td>  37 MB <br></td></tr><tr><td>  <a href="">mais r√°pido-rcnn-vggvd-pascal</a> <br></td><td>  600 x 850 <br></td><td>  523 MB <br></td><td>  600 MB <br></td></tr></tbody></table></div><br><br>  Em nossa experi√™ncia, os coeficientes de uma rede neural profunda para processar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">bordas transl√∫cidas</a> podem ocupar 150-200 MB.  Os colegas da rede neural determinam a idade e o sexo do tamanho dos coeficientes da ordem de 50 MB.  E durante a otimiza√ß√£o para a vers√£o m√≥vel de precis√£o reduzida - cerca de 25 MB (float32‚áífloat16). <br><br>  Ao mesmo tempo, o gr√°fico de atraso ao acessar a mem√≥ria, dependendo do tamanho dos dados, √© distribu√≠do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aproximadamente assim</a> (a escala horizontal √© logar√≠tmica): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/1f4/2aa/134/1f42aa13427972cd1762383b33cfe974.png"></div><br><br>  I.e.  com um aumento no volume de dados de mais de 16 MB, o atraso aumenta 50 vezes ou mais, o que afeta fatalmente o desempenho.  De fato, na maioria das vezes a CPU, ao trabalhar com redes neurais profundas, espera <s>estupidamente</s> por dados.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Os dados da Intel</a> sobre acelera√ß√£o de v√°rias redes s√£o interessantes, onde, de fato, a acelera√ß√£o ocorre apenas quando a rede se torna pequena (por exemplo, como resultado da quantiza√ß√£o de pesos), para come√ßar a entrar pelo menos parcialmente no cache juntamente com os dados processados.  Observe que o cache de uma CPU moderna consome at√© metade da energia do processador.  No caso de redes neurais pesadas, √© ineficaz e funciona aquecedor excessivamente caro. <br><br><div class="spoiler">  <b class="spoiler_title">Para adeptos de redes neurais na CPU</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">De acordo</a> com nossos testes internos, at√© o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Intel OpenVINO</a> perde a implementa√ß√£o da estrutura de multiplica√ß√£o de matrizes + NNPACK em muitas arquiteturas de rede (especialmente em arquiteturas simples onde a largura de banda √© importante para o processamento de dados em tempo real no modo de thread √∫nico).  Esse cen√°rio √© relevante para v√°rios classificadores de objetos na imagem (onde a rede neural precisa ser executada um grande n√∫mero de vezes - 50‚Äì100 em termos do n√∫mero de objetos na imagem) e a sobrecarga de iniciar o OpenVINO se torna excessivamente alta. <br></div></div><br>  <b>Pr√≥s:</b> <br><br><ul><li>  "Todo mundo tem", e geralmente fica ocioso, ou seja,  pre√ßo de <i>entrada</i> relativamente baixo para cobran√ßa e implementa√ß√£o. <br></li><li>  Existem redes n√£o CV separadas que se encaixam bem na CPU, os colegas chamam, por exemplo, Wide &amp; Deep e GNMT. <br></li></ul><br>  <b>Menos:</b> <br><ul><li>  A CPU √© ineficiente ao trabalhar com redes neurais profundas (quando o n√∫mero de camadas de rede e o tamanho dos dados de entrada s√£o grandes), tudo funciona dolorosamente devagar. <br></li></ul><br><h2>  GPU </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/aee/06a/d5b/aee06ad5beab0896005e75769b3ba910.png"><br><br>  Como o t√≥pico √© bem conhecido, descrevemos brevemente o principal.  No caso de redes neurais, a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">GPU</a> possui uma vantagem significativa de desempenho em tarefas massivamente paralelas: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/717/03d/d9b/71703dd9bce82918d1e0c7138a09adf0.png"></div><br>  Preste aten√ß√£o em como o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Xeon Phi 7290 de</a> 72 n√∫cleos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">√©</a> recozido, enquanto o "azul" tamb√©m √© o servidor Xeon, ou seja,  A Intel n√£o desiste t√£o facilmente, o que ser√° discutido abaixo.  Mais importante, por√©m, a mem√≥ria das placas de v√≠deo foi originalmente projetada para um desempenho cerca de 5 vezes maior.  Nas redes neurais, a computa√ß√£o com dados √© extremamente simples.  Algumas a√ß√µes elementares e precisamos de novos dados.  Como resultado, a velocidade de acesso aos dados √© cr√≠tica para a opera√ß√£o eficiente de uma rede neural.  Uma mem√≥ria de alta velocidade "integrada" na GPU e um sistema de gerenciamento de cache mais flex√≠vel do que na CPU podem resolver este problema: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/30a/d1b/07e/30ad1b07e7b22d97dc3372e4351c2e3c.png"></div><br><br>  Por v√°rios anos, Tim Detmers apoia a interessante revis√£o de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">"Quais GPU (s) obter (s) para o Deep Learning: minha experi√™ncia e conselhos para usar GPUs no Deep Learning"</a> ("Qual GPU √© melhor para o aprendizado profundo ...").  √â claro que Tesla e Titans governam o treinamento, embora a diferen√ßa nas arquiteturas possa causar explos√µes interessantes, por exemplo, no caso de redes neurais recorrentes (e o l√≠der em geral √© o TPU, observe para o futuro): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6e6/bb0/43a/6e6bb043aab85679d1ddf01028e19728.png"></div><br>  No entanto, existe um gr√°fico de desempenho extremamente √∫til para o d√≥lar, no cavalo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">RTX</a> (provavelmente devido aos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">seus</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">n√∫cleos tensores</a> ), se voc√™ tiver mem√≥ria suficiente para isso, √© claro: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f13/338/032/f13338032ca812397cbbe0228bda9ed5.png"></div><br>  Obviamente, o custo da computa√ß√£o √© importante.  O segundo lugar da primeira classifica√ß√£o e o √∫ltimo da segunda - o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tesla V100</a> √© vendido por 700 mil rublos, como 10 computadores "comuns" (+ o caro switch Infiniband, se voc√™ deseja treinar em v√°rios n√≥s).  V100 verdadeiro e trabalha para dez.  As pessoas est√£o dispostas a pagar em excesso pela acelera√ß√£o tang√≠vel do aprendizado. <br><br>  Total, resumir! <br><br>  <b>Pr√≥s:</b> <br><ul><li>  Cardinal - 10-100 vezes - acelera√ß√£o em compara√ß√£o com a CPU. <br></li><li>  Extremamente eficaz para treinamento (e um pouco menos eficaz para uso). <br></li></ul><br>  <b>Menos:</b> <br><ul><li>  O custo das placas de v√≠deo topo de linha (que t√™m mem√≥ria suficiente para treinar redes grandes) excede o custo do restante do computador ... <br></li></ul><br><h2>  FPGA </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/0d5/554/90b/0d555490b015730051341cc857d14606.png"><br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">FPGA</a> j√° √© mais interessante.  Esta √© uma rede de v√°rios milh√µes de blocos program√°veis, os quais tamb√©m podemos interconectar programaticamente.  A rede e os blocos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">s√£o mais ou menos</a> assim (o gargalo √© o gargalo, preste aten√ß√£o, novamente na frente da mem√≥ria do chip, mas √© mais f√°cil, que ser√° descrito abaixo): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0d8/8f9/789/0d88f9789f60720da892b7acae7ab8ec.png"><br>  Naturalmente, faz sentido usar o FPGA j√° no est√°gio de uso de uma rede neural (na maioria dos casos, n√£o h√° mem√≥ria suficiente para o treinamento).  Al√©m disso, o t√≥pico de execu√ß√£o no FPGA come√ßou a se desenvolver ativamente.  Por exemplo, aqui est√° a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">estrutura fpgaConvNet</a> , que pode acelerar significativamente o uso da CNN em FPGAs e reduzir o consumo de energia. <br><br>  A principal vantagem do FPGA √© que podemos armazenar a rede diretamente nas c√©lulas, ou seja,  um ponto fino na forma de centenas de megabytes dos mesmos dados sendo transferidos 25 vezes por segundo (para v√≠deo) na mesma dire√ß√£o desaparece magicamente.  Isso permite uma velocidade de clock mais baixa e a aus√™ncia de caches, em vez de um desempenho menor, para obter um aumento percept√≠vel.  Sim, e reduza drasticamente <s>o</s> consumo de energia do <s>aquecimento global</s> por unidade de c√°lculo. <br><br>  A Intel ingressou ativamente no processo, lan√ßando o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">OpenVINO Toolkit</a> em c√≥digo aberto no ano passado, que inclui o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Deep Learning Deployment Toolkit</a> (parte do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">OpenCV</a> ).  Al√©m disso, o desempenho em FPGAs em diferentes redes parece bastante interessante, e a vantagem dos FPGAs em compara√ß√£o com as GPUs (embora as GPUs integradas da Intel) sejam bastante significativas: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/8ba/a66/37e/8baa6637e904732e4e883a196b8d803d.png"></div><br>  O que especialmente aquece a alma do autor - FPS s√£o comparados, ou seja,  quadros por segundo √© a m√©trica mais pr√°tica para v√≠deo.  Dado que a Intel comprou a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Altera</a> , o segundo maior player do mercado de FPGA, em 2015, o gr√°fico fornece bons pensamentos. <br><br>  E, obviamente, a barreira de entrada para essas arquiteturas √© mais alta; portanto, √© necess√°rio algum tempo para que apare√ßam ferramentas convenientes que efetivamente levem em conta a arquitetura FPGA fundamentalmente diferente.  Mas subestimar o potencial da tecnologia n√£o vale a pena.  Dolorosamente muitos lugares magros que ela rodeia. <br><br>  Finalmente, enfatizamos que a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">programa√ß√£o de FPGAs</a> √© uma arte separada.  Como tal, o programa n√£o √© executado l√° e todos os c√°lculos s√£o feitos em termos de fluxos de dados, atrasos no fluxo (que afetam o desempenho) e portas usadas (que sempre faltam).  Portanto, para iniciar a programa√ß√£o eficaz, voc√™ precisa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">alterar</a> completamente <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">seu pr√≥prio firmware</a> (na rede neural que fica entre seus ouvidos).  Com boa efici√™ncia, isso n√£o √© obtido.  No entanto, os novos quadros em breve esconder√£o a diferen√ßa externa dos pesquisadores. <br><br>  <b>Pr√≥s:</b> <br><br><ul><li>  Execu√ß√£o de rede potencialmente mais r√°pida. <br></li><li>  Consumo de energia significativamente mais baixo comparado ao CPU e GPU (isso √© especialmente importante para solu√ß√µes m√≥veis). <br></li></ul><br>  <b>Contras:</b> <br><br><ul><li>  Principalmente, eles ajudam a acelerar a execu√ß√£o; o treinamento neles, ao contr√°rio da GPU, √© visivelmente menos conveniente. <br></li><li>  Programa√ß√£o mais complexa em compara√ß√£o com as op√ß√µes anteriores. <br></li><li>  Notavelmente menos especialistas. <br></li></ul><br><h2>  ASIC </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/dee/5e4/059/dee5e40597454e07651718c325f2ac3f.png"><br><br>  Em seguida, √© o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ASIC</a> , que √© a abrevia√ß√£o de Circuito Integrado Espec√≠fico √† Aplica√ß√£o,  circuito integrado para a nossa tarefa.  Por exemplo, realizando uma rede neural colocada em ferro.  No entanto, a maioria dos n√≥s de computa√ß√£o pode trabalhar em paralelo.  De fato, apenas depend√™ncias de dados e computa√ß√£o desigual em diferentes n√≠veis da rede podem impedir que usemos constantemente todas as ALUs em funcionamento. <br><br>  Talvez a minera√ß√£o de criptomoedas tenha feito o maior an√∫ncio do ASIC entre o p√∫blico em geral nos √∫ltimos anos.  No come√ßo, a minera√ß√£o na CPU era bastante lucrativa, depois tive que comprar uma GPU, depois FPGA e depois ASICs especializadas, pois as pessoas (leia - o mercado) amadureceram para pedidos nos quais sua produ√ß√£o se tornou lucrativa. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/de1/fda/062/de1fda062c797262a51047f57eac2f11.png"><br>  Em nossa √°rea, os <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">servi√ßos</a> tamb√©m j√° apareceram (naturalmente!), <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Que</a> ajudam a colocar uma rede neural no ferro com as caracter√≠sticas necess√°rias para o consumo de energia, FPS e pre√ßo.  Magicamente, concordo! <br><br>  MAS!  Estamos perdendo a personaliza√ß√£o da rede.  E, claro, as pessoas tamb√©m pensam nisso.  Por exemplo, aqui est√° um artigo com o ditado, ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Uma arquitetura reconfigur√°vel pode vencer o ASIC como um acelerador da CNN?</a> ‚Äù (‚ÄúUma arquitetura configur√°vel pode vencer o ASIC como um acelerador da CNN?‚Äù).  H√° trabalho suficiente sobre esse t√≥pico, porque a pergunta n√£o est√° ociosa.  A principal desvantagem do ASIC √© que, depois de direcionar a rede para o hardware, fica dif√≠cil alter√°-la.  Eles s√£o mais ben√©ficos para os casos em que j√° precisamos de uma rede que funcione bem, com milh√µes de chips com baixo consumo de energia e alto desempenho.  E essa situa√ß√£o est√° se desenvolvendo gradualmente no mercado de carros de piloto autom√°tico, por exemplo.  Ou em c√¢meras de vigil√¢ncia.  Ou nas c√¢maras dos aspiradores de p√≥ rob√≥ticos.  Ou nas c√¢maras de uma geladeira dom√©stica.  Ou em uma c√¢mara de cafeteira.  <s>Ou na c√¢mara de ferro.</s>  Bem, voc√™ entende a ideia, em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">suma</a> ! <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/51a/b5e/949/51ab5e9497fee71cf9f23606d3de4fb6.png"></div><br><br>  √â importante que, na produ√ß√£o em massa, o chip seja barato, funcione rapidamente e consuma um m√≠nimo de energia. <br><br>  <b>Pr√≥s:</b> <br><br><ul><li>  O menor custo de chip comparado a todas as solu√ß√µes anteriores. <br></li><li>  Menor consumo de energia por unidade de opera√ß√£o. <br></li><li>  Alta velocidade (incluindo, se desejado, um registro). <br></li></ul><br>  <b>Contras:</b> <br><br><ul><li>  Capacidade muito limitada de atualizar a rede e a l√≥gica. <br></li><li>  Maior custo de desenvolvimento em compara√ß√£o com todas as solu√ß√µes anteriores. <br></li><li>  O uso do ASIC √© econ√¥mico, principalmente para grandes tiragens. <br></li></ul><br><h2>  TPU </h2><br>  Lembre-se de que, ao trabalhar com redes, h√° duas tarefas - treinamento e execu√ß√£o (infer√™ncia).  Se os FPGA / ASICs estiverem focados principalmente em acelerar a execu√ß√£o (incluindo algum tipo de rede fixa), o TPU (Tensor Processing Unit ou processadores de tensores) √© uma acelera√ß√£o de aprendizado baseada em hardware ou uma acelera√ß√£o relativamente universal de uma rede arbitr√°ria.  O nome √© bonito, concordo, embora, de fato, ainda estejam sendo usados <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tensores de</a> classifica√ß√£o 2 com uma Unidade Multiplicada Mista (MXU) conectada √† Mem√≥ria de Alta Largura de Banda (HBM).  Abaixo est√° o diagrama da arquitetura da vers√£o 2 e 3 do TPU Google: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/f5e/29c/696/f5e29c696e265435fadbc2e56f67e12e.png"></div><br><h2>  TPU Google </h2><br>  Em geral, o Google fez um an√∫ncio para o nome TPU, revelando desenvolvimentos internos em 2017: <br><br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/4af/ea1/4ca/4afea14cad5dc0a7d941ae6b7963c171.png"></div><br>  Eles come√ßaram o trabalho preliminar em processadores especializados para redes neurais com suas palavras em 2006, em 2013 eles criaram um projeto com um bom financiamento e em 2015 come√ßaram a trabalhar com os primeiros chips que ajudavam muito em redes neurais para o servi√ßo em nuvem do Google Translate e muito mais.  E isso foi, enfatizamos, a acelera√ß√£o da rede.  Uma vantagem importante para os datacenters √© a efici√™ncia energ√©tica de TPU de duas ordens de magnitude mais alta em compara√ß√£o com as CPUs (gr√°fico para TPU v1): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/877/a91/f61/877a91f61ce45f2d04c7c3fe9df55348.png"></div><br>  Al√©m disso, como regra geral, em compara√ß√£o com a GPU, o <i>desempenho da</i> rede √© 10 a 30 vezes melhor para melhor: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/10a/302/83b/10a30283b9c0fe8f0c7c1993f0bc68aa.png"></div><br>  A diferen√ßa √© 10 vezes significativa.  √â claro que a diferen√ßa com a GPU em 20 a 30 vezes determina o desenvolvimento dessa dire√ß√£o. <br><br>  E, felizmente, o Google n√£o est√° sozinho. <br><br><h2>  TPU Huawei </h2><br>  Hoje, a sofredora Huawei tamb√©m come√ßou a desenvolver TPU h√° v√°rios anos sob o nome Huawei Ascend, e em duas vers√µes ao mesmo tempo - para data centers (como o Google) e para dispositivos m√≥veis (que o Google tamb√©m come√ßou a fazer recentemente).  Se voc√™ acredita nos materiais da Huawei, eles substitu√≠ram o novo Google TPU v3 por FP16 2,5 vezes e NVIDIA V100 2 vezes: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bb8/692/66a/bb869266a2cfa5e28c2f086026309b01.png"><br><br>  Como sempre, uma boa pergunta: como esse chip se comportar√° em tarefas reais.  Pois no gr√°fico, como voc√™ pode ver, o desempenho m√°ximo.  Al√©m disso, o Google TPU v3 √© bom de v√°rias maneiras, pois pode funcionar efetivamente em clusters de 1024 processadores.  A Huawei tamb√©m anunciou clusters de servidores para o Ascend 910, mas n√£o h√° detalhes.  Em geral, os engenheiros da Huawei mostraram-se extremamente competentes nos √∫ltimos 10 anos, e h√° todas as chances de que o desempenho de pico 2,8 vezes maior comparado ao Google TPU v3, juntamente com a mais recente tecnologia de processo de 7 nm, seja usado no caso. <br><br>  A mem√≥ria e o barramento de dados s√£o cr√≠ticos para o desempenho, e o slide mostra que uma aten√ß√£o consider√°vel foi dada a esses componentes (incluindo a velocidade de comunica√ß√£o com a mem√≥ria muito mais rapidamente que a da GPU): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/98e/7bd/ab1/98e7bdab1c9e1a81e891bb72db3976c2.png"><br><br>  O chip tamb√©m usa uma abordagem ligeiramente diferente - n√£o as escalas bidimensionais do MXU 128x128, mas os c√°lculos em um cubo tridimensional de tamanho menor 16x16xN, onde N = {16.8,4,2,1}.  Portanto, a quest√£o principal √© qu√£o bem ela estar√° na acelera√ß√£o real de redes espec√≠ficas (por exemplo, c√°lculos em um cubo s√£o convenientes para imagens).  Al√©m disso, um estudo cuidadoso do slide mostra que, diferentemente do Google, o chip incorpora imediatamente o trabalho com v√≠deo FullHD compactado.  Para o autor, isso parece <b>muito</b> encorajador! <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como mencionado acima, na mesma linha, os processadores s√£o desenvolvidos para dispositivos m√≥veis para os quais a efici√™ncia energ√©tica √© essencial e nos quais a rede ser√° executada principalmente (ou seja, separadamente - processadores para aprendizado na nuvem e separadamente - para execu√ß√£o): </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/ea0/a3c/7cb/ea0a3c7cb72def7195afa2011ba02907.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">E com esse par√¢metro, tudo Parece bom em compara√ß√£o com a NVIDIA, pelo menos (observe que eles n√£o trouxeram uma compara√ß√£o com o Google, no entanto, o Google n√£o oferece TPUs na nuvem em m√£os). E seus chips m√≥veis competir√£o com processadores da Apple, Google e outras empresas, mas √© muito cedo para fazer um balan√ßo aqui. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√â claramente visto que os novos chips Nano, Tiny e Lite devem ser ainda melhores. Torna-se claro </font></font><s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">por que Trump estava com medo</font></font></s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> por que muitos fabricantes est√£o examinando cuidadosamente os sucessos da Huawei (que superaram todas as empresas de ferro dos EUA em receita, incluindo a Intel em 2018). </font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Redes anal√≥gicas profundas </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como voc√™ sabe, a tecnologia geralmente se desenvolve em espiral, quando abordagens antigas e esquecidas se tornam relevantes em uma nova rodada. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Algo semelhante poderia muito bem acontecer com redes neurais. Voc√™ deve ter ouvido falar que, uma vez que as opera√ß√µes de multiplica√ß√£o e adi√ß√£o foram realizadas por tubos e transistores de el√©trons (por exemplo, a convers√£o de espa√ßos de cores - uma multiplica√ß√£o t√≠pica de matrizes - ocorreu em todas as TVs coloridas at√© meados dos anos 90)? Uma boa pergunta surgiu: se nossa rede neural √© relativamente resistente a c√°lculos imprecisos, e se convertermos esses c√°lculos em forma anal√≥gica? Temos imediatamente uma acelera√ß√£o not√°vel dos c√°lculos e uma redu√ß√£o potencialmente dram√°tica no consumo de energia para uma opera√ß√£o:</font></font><br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/6b2/e08/aff/6b2e08afff1639668e02a548ed663fba.png"></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Com essa abordagem, o DNN (Deep Neural Network) √© calculado rapidamente e com efici√™ncia de energia. Mas h√° um problema - esses s√£o conversores DAC / ADCs (DAC / ADC) - de digital para anal√≥gico e vice-versa, que reduzem a efici√™ncia energ√©tica e a precis√£o do processo. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No entanto, em 2017, a IBM Research </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">prop√¥s CMOS anal√≥gico</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para RPUs ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Unidades de Processamento Resistivo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), que permitem armazenar dados processados ‚Äã‚Äãtamb√©m na forma anal√≥gica e aumentar significativamente a efici√™ncia geral da abordagem:</font></font><br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/0d3/720/3a8/0d37203a85a89791e8701098583e01ea.png"></div><br> ,          ‚Äî     RPU,  ,       .   IBM   ,           2-          (   ),     100  (!)       GPU: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/1c4/1eb/c09/1c41ebc09f5e53d206534eefbb105eac.png"></div><br>       ,         : <br><br><div style="text-align:center;"><img width="35%" src="https://habrastorage.org/getpro/habr/post_images/498/f6c/bdb/498f6cbdb53f20806cf41a78b7110e8b.png"></div><br>  No entanto, a dire√ß√£o potencial da computa√ß√£o anal√≥gica parece <b>extremamente</b> interessante. <br><br>  A √∫nica coisa que confunde √© que √© a IBM, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">que j√° registrou dezenas de patentes sobre o assunto</a> .  Segundo a experi√™ncia, devido √†s peculiaridades da cultura corporativa, elas cooperam relativamente fracamente com outras empresas e, possuindo alguma tecnologia, t√™m mais probabilidade de desacelerar seu desenvolvimento entre outras do que compartilh√°-lo efetivamente.  Por exemplo, a IBM se recusou a licenciar a compress√£o aritm√©tica para JPEG ao comit√™ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ISO</a> , apesar do fato de o rascunho do padr√£o ser uma op√ß√£o com compress√£o aritm√©tica.  Como resultado, o JPEG ganhou vida com a compress√£o de Huffman e ficou 10 a 15% pior do que poderia.  A mesma situa√ß√£o ocorreu com os padr√µes de compacta√ß√£o de v√≠deo.  E a ind√∫stria mudou massivamente para a compress√£o aritm√©tica nos codecs somente quando cinco patentes da IBM expiraram 12 anos depois ... Esperamos que a IBM esteja mais inclinada a cooperar dessa vez e, consequentemente, <b>desejamos o m√°ximo sucesso no campo para todos que n√£o estiverem associados √† IBM</b> , o benef√≠cio de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tais pessoas e empresas √© muito</a> . <br><br>  Se funcionar, <b>ser√° uma revolu√ß√£o no uso de redes neurais e uma revolu√ß√£o em muitas √°reas da ci√™ncia da computa√ß√£o.</b> <br><br><h2>  Outras letras diversas </h2><br>  Em geral, o t√≥pico de acelerar redes neurais se tornou moda, todas as principais empresas e dezenas de startups est√£o envolvidas nele, e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pelo menos cinco delas atra√≠ram mais de US $ 100 milh√µes em</a> investimentos at√© o in√≠cio de 2018.  No total, em 2017, US $ 1,5 bilh√£o foram investidos em startups relacionadas ao desenvolvimento de chips.  Apesar do fato de os investidores n√£o perceberem os fabricantes de chips h√° 15 anos (porque n√£o havia nada para pegar l√° no contexto de gigantes).  Em geral - agora h√° uma chance real de uma pequena revolu√ß√£o do ferro.  Al√©m disso, √© extremamente dif√≠cil prever qual arquitetura vencer√°, a necessidade de revolu√ß√£o amadureceu e as possibilidades de aumentar a produtividade s√£o grandes.  A situa√ß√£o revolucion√°ria cl√°ssica amadureceu: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Moore</a> n√£o pode mais e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Dean</a> ainda n√£o est√° pronto. <br><br>  Bem, como a lei de mercado mais importante - seja diferente, h√° muitas novas cartas, por exemplo: <br><br><ul><li>  <b>Unidade de processamento neural ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">NPU</a> )</b> - Um neuroprocessador, √†s vezes lindamente - um chip neurom√≥rfico - em geral, o nome geral de um acelerador de redes neurais, chamadas chips <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Samsung</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Huawei</a> e mais adiante na lista ... <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/d28/900/72e/d2890072e95d7e63f70543b09ba759bb.png"></div>  <i>A seguir, nesta se√ß√£o, ser√£o apresentados principalmente slides de apresenta√ß√µes corporativas como exemplos de <b>nomes pr√≥prios</b> de tecnologia</i> <br><br>  √â claro que uma compara√ß√£o direta √© problem√°tica, mas aqui est√£o alguns dados interessantes que comparam chips com neuroprocessadores da Apple e Huawei, produzidos pela TSMC mencionados no in√≠cio.  Pode-se observar que a concorr√™ncia √© dura, a nova gera√ß√£o mostra um aumento de produtividade de 2 a 8 vezes e a complexidade dos processos tecnol√≥gicos: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/webt/ee/ga/iu/eegaiu5u_rw5trv0-exwjngc7sw.png"></div><br></li><li>  <b>Processador de rede neural (NNP)</b> - processador de rede neural. <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/917/1bb/ad2/9171bbad26941f97399ce80a56373e51.png"></div><br>  Esse √© o nome de sua fam√≠lia de chips, por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Intel</a> (originalmente era a empresa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Nervana Systems</a> , que a Intel comprou em 2016 por US $ 400 milh√µes).  No entanto, em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigos</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">livros, o</a> nome NNP tamb√©m √© bastante comum. <br></li><li>  <b>Unidade de Processamento de Intelig√™ncia (IPU)</b> - um processador inteligente - o nome dos chips promovidos pela <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Graphcore</a> (a prop√≥sito, que j√° recebeu um investimento de US $ 310 milh√µes). <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/e93/18f/8e7/e9318f8e7711ea5b3a669946484c72bf.png"></div><br>  Produz placas especiais para computadores, mas voltadas para o treinamento de redes neurais, com um desempenho de treinamento da RNN 180 a 240 vezes maior do que o da NVIDIA P100. <br></li><li>  <b>Unidade de Processamento de Fluxo de Dados (DPU)</b> - processador de processamento de dados - o nome √© promovido pela <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">WAVE Computing</a> , que j√° recebeu um investimento de US $ 203 milh√µes.  Produz os mesmos aceleradores do Graphcore: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/f53/e5e/25a/f53e5e25abb2fc2fee636d1949242be8.png"></div><br>  Como eles receberam 100 milh√µes a menos, eles declaram o treinamento apenas 25 vezes mais r√°pido que na GPU (embora prometam que ser√£o 1000 vezes em breve).  Vamos ver ... <br></li><li>  <b>Unidade de Processamento de Vis√£o ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">VPU</a> )</b> - Computer Vision Processor: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/80e/756/110/80e7561100a794bb005635899b06ec40.png"></div><br>  O termo √© usado em produtos de v√°rias empresas, por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Myriad X VPU</a> da Movidius (tamb√©m <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">comprado pela Intel</a> em 2016). <br></li><li>  Um dos concorrentes da IBM (que, lembramos, usa o termo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">RPU</a> ) - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Mythic</a> - est√° movendo o <b>DNN anal√≥gico</b> , que tamb√©m armazena a rede no chip e uma execu√ß√£o relativamente r√°pida.  At√© agora, eles t√™m apenas promessas, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">embora s√©rias</a> : <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/01e/ae6/253/01eae6253a1d7ef3e2dec9401857a33a.png"></div><br></li></ul><br>  E isso lista apenas as maiores √°reas no desenvolvimento das quais centenas de milh√µes foram investidas (isso √© importante no desenvolvimento do ferro). <br><br>  Em geral, como vemos, todas as flores florescem rapidamente.  Gradualmente, as empresas digerem bilh√µes de d√≥lares em investimentos (geralmente leva de 1,5 a 3 anos para produzir chips), a poeira se acalma, o l√≠der fica claro, os vencedores, como sempre, escrevem uma hist√≥ria e o nome da tecnologia de maior sucesso no mercado ser√° geralmente aceito.  Isso j√° aconteceu mais de uma vez ("IBM PC", "Smartphone", "Xerox" etc.). <br><br><h2>  Algumas palavras sobre a compara√ß√£o correta </h2><br>  Como j√° mencionado acima, comparar corretamente o desempenho de redes neurais n√£o √© f√°cil.  √â exatamente por isso que o Google publica um gr√°fico no qual o TPU v1 produz a NVIDIA V100.  A NVIDIA, vendo essa desgra√ßa, publica uma programa√ß√£o em que o Google TPU v1 perde o V100.  (Ent√£o!) O Google publica o gr√°fico a seguir, onde o V100 perde no Google TPU v2 e v3.  E, finalmente, a Huawei √© o cronograma em que todos perdem no Huawei Ascend, mas o V100 √© melhor que o TPU v3.  Circo, em suma.  O que √© caracter√≠stico - <i>cada</i> gr√°fico <i>tem sua pr√≥pria</i> verdade! <br><br>  As causas da situa√ß√£o s√£o claras: <br><br><ul><li>  Voc√™ pode medir a velocidade de aprendizado ou a velocidade de execu√ß√£o (o que for mais conveniente). <br></li><li>  √â poss√≠vel medir diferentes redes neurais, porque a velocidade de execu√ß√£o / treinamento de diferentes redes neurais em arquiteturas espec√≠ficas pode diferir significativamente devido √† arquitetura da rede e √† quantidade de dados necess√°rios. <br></li><li>  E voc√™ pode medir o desempenho m√°ximo do acelerador (talvez o mais abstrato de todos os itens acima). <br></li></ul><br>  Como uma tentativa de colocar as coisas em ordem neste zool√≥gico, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">apareceu o</a> teste <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MLPerf</a> , que agora tem a vers√£o 0.5 dispon√≠vel, ou seja,  ele est√° no processo de desenvolver uma metodologia de compara√ß√£o, que est√° prevista para ser lan√ßada no primeiro lan√ßamento no <a href="">terceiro trimestre deste ano</a> : <br><br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/d14/e35/575/d14e3557543c05b83b99f12dc9e572ce.png"></div><br>  Como os autores s√£o um dos principais contribuidores do TensorFlow, h√° todas as chances de descobrir qual √© a melhor maneira de treinar e possivelmente us√°-lo (porque a vers√£o m√≥vel do TF provavelmente tamb√©m ser√° inclu√≠da neste teste ao longo do tempo). <br><br>  Recentemente, a organiza√ß√£o internacional <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">IEEE</a> , que publica a terceira parte da literatura t√©cnica mundial em r√°dio eletr√¥nica, computadores e engenharia el√©trica, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">baniu a Huawei do rosto de uma</a> crian√ßa e logo, no entanto, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cancelou a</a> proibi√ß√£o.  A Huawei ainda n√£o est√° no ranking <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">atual da</a> MLPerf, enquanto a Huawei TPU √© uma concorrente s√©ria dos cart√µes Google TPUs e NVIDIA (ou seja, al√©m dos pol√≠ticos, h√° raz√µes econ√¥micas para ignorar a Huawei, francamente).  Com interesse indiscut√≠vel, acompanharemos o desenvolvimento de eventos! <br><br><h2>  Tudo para o c√©u!  Mais perto das nuvens! </h2><br>  E, como se tratava de treinamento, vale a pena dizer algumas palavras sobre seus detalhes: <br><br><ul><li>  Com a partida generalizada da pesquisa em redes neurais profundas (com dezenas e centenas de camadas que realmente impressionam a todos), foi necess√°rio moer centenas de megabytes de coeficientes, o que imediatamente tornou ineficazes todos os caches de processadores das gera√ß√µes anteriores.  Ao mesmo tempo, o ImageNet cl√°ssico <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">discute</a> uma correla√ß√£o estrita entre o tamanho da rede e sua precis√£o (quanto maior, melhor, direita, maior a rede, o eixo horizontal √© logar√≠tmico): <br><br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/5fa/788/84f/5fa78884f561b6f93dfa126be53376f4.png"></div><br></li><li>  O processo de c√°lculo dentro da rede neural segue um esquema fixo, ou seja,  onde todas as ‚Äúramifica√ß√µes‚Äù e ‚Äútransi√ß√µes‚Äù (em termos do s√©culo passado) ocorrer√£o na grande maioria dos casos, √© precisamente conhecido com anteced√™ncia, o que deixa a execu√ß√£o especulativa das instru√ß√µes sem trabalho, o que anteriormente aumenta significativamente a produtividade: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/webt/ss/-6/9n/ss-69n-vr5c3rszuvmhkaomtct0.png"></div><br>  Isso torna ineficazes os mecanismos de previs√£o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">superescalares</a> para ramifica√ß√µes e pr√©-c√°lculos das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">d√©cadas</a> anteriores <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">de</a> aprimoramento <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">do</a> processador (essa parte do chip, infelizmente, tamb√©m contribui para o aquecimento global, como o DNN no cache do DNN). <br></li><li>  Al√©m disso, o treinamento da rede neural √© relativamente pouco <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">dimensionado horizontalmente</a> .  I.e.  n√£o podemos pegar 1000 computadores poderosos e aprender a acelerar a acelera√ß√£o 1000 vezes.  E mesmo com 100, n√£o podemos (pelo menos at√© que o problema te√≥rico da deteriora√ß√£o da qualidade do treinamento em um grande tamanho do lote seja resolvido).  Em geral, √© bastante dif√≠cil para n√≥s distribuir algo em v√°rios computadores, porque assim que a velocidade de acesso √† mem√≥ria unificada na qual a rede se encontra diminui, a velocidade de seu aprendizado cai catastroficamente.  Portanto, se um pesquisador tiver acesso a 1000 computadores poderosos <s>de gra√ßa</s> , ele certamente os pegar√° em breve, mas provavelmente (se n√£o houver infinita de banda + RDMA), haver√° muitas redes neurais com diferentes hiperpar√¢metros.  I.e.  o tempo total de treinamento ser√° apenas v√°rias vezes menor do que com 1 computador.  L√° √© poss√≠vel brincar com os tamanhos do lote, a educa√ß√£o continuada e outras novas tecnologias da moda, mas a principal conclus√£o √© sim, com um aumento no n√∫mero de computadores, a efici√™ncia do trabalho e a probabilidade de alcan√ßar um resultado aumentar√£o, mas n√£o linearmente.  Hoje, o tempo de um pesquisador de ci√™ncia de dados √© caro e, muitas vezes, se voc√™ pode gastar muitos carros (embora n√£o razo√°vel), mas obt√©m acelera√ß√£o - isso √© feito (veja o exemplo com 1, 2 e 4 V100s caros nas nuvens logo abaixo). <br></li></ul><br>  Exatamente esses pontos explicam por que tantas pessoas correram para o desenvolvimento de ferro especializado para redes neurais profundas.  E por que eles conseguiram bilh√µes?  Realmente h√° luz vis√≠vel no fim do t√∫nel e n√£o apenas o Graphcore (que, lembre-se, 240 vezes o treinamento da RNN acelerou). <br><br>  Por exemplo, os senhores da IBM Research <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">est√£o otimistas</a> quanto ao desenvolvimento de chips especiais que aumentar√£o a efici√™ncia dos c√°lculos em uma ordem de grandeza ap√≥s 5 anos (e ap√≥s 10 anos em 2 ordens de grandeza, atingindo um aumento de 1000 vezes em compara√ß√£o com o n√≠vel de 2016 neste gr√°fico, embora , em efici√™ncia por watt, mas a pot√™ncia principal tamb√©m aumentar√°): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/51c/81f/226/51c81f2267b99ba00231532e606cf7fd.png"></div><br>  Tudo isso significa a apar√™ncia de peda√ßos de ferro, cujo treinamento ser√° relativamente r√°pido, mas caro, o que naturalmente leva √† id√©ia de compartilhar o tempo de uso desse caro peda√ßo de ferro entre os pesquisadores.  E essa id√©ia hoje n√£o menos naturalmente nos leva √† computa√ß√£o em nuvem.  E a transi√ß√£o do aprendizado para as nuvens tem sido ativamente ativa. <br><br>  Observe que agora o treinamento dos mesmos modelos pode diferir no tempo por uma ordem de magnitude de diferentes servi√ßos em nuvem.  A Amazon lidera e o Colab gratuito do Google vem por √∫ltimo.  Observe como o resultado do n√∫mero de V100 muda entre os l√≠deres - um aumento no n√∫mero de cart√µes em 4 vezes (!) Aumenta a produtividade em menos de um ter√ßo (!!!) de azul para roxo, e o Google tem ainda menos: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/78d/892/d83/78d892d833c0858572548aee7fca2697.png"></div><br>  Parece que nos pr√≥ximos anos a diferen√ßa crescer√° para duas ordens de magnitude.  Senhor!  Cozinhando dinheiro!  Devolveremos amigavelmente investimentos de bilh√µes de d√≥lares aos investidores mais bem-sucedidos ... <br><br><h2>  Em suma </h2><br>  Vamos tentar resumir os pontos principais do tablet: <br><div class="scrollable-table"><table><tbody><tr><td>  Tipo <br></td><td>  O que acelera <br></td><td>  Coment√°rio <br></td></tr><tr><td>  CPU <br></td><td>  Basicamente fazendo <br></td><td>  Geralmente o pior em velocidade e efici√™ncia energ√©tica, mas bastante adequado para a realiza√ß√£o de redes neurais de pequeno porte <br></td></tr><tr><td>  GPU <br></td><td>  Execu√ß√£o + <br>  treinamento <br></td><td>  A solu√ß√£o mais universal, mas bastante cara, tanto em termos de custo de c√°lculos quanto em efici√™ncia energ√©tica <br></td></tr><tr><td>  FPGA <br></td><td>  Cumprimento <br></td><td>  Uma solu√ß√£o relativamente universal para a execu√ß√£o de redes, em alguns casos pode acelerar drasticamente a implementa√ß√£o <br></td></tr><tr><td>  ASIC <br></td><td>  Cumprimento <br></td><td>  A vers√£o mais barata, r√°pida e com maior efici√™ncia energ√©tica da rede, mas s√£o necess√°rias grandes tiragens <br></td></tr><tr><td>  TPU <br></td><td>  Execu√ß√£o + <br>  treinamento <br></td><td>  As primeiras vers√µes foram usadas para acelerar a execu√ß√£o, agora s√£o usadas para acelerar muito rapidamente a execu√ß√£o e o treinamento <br></td></tr><tr><td>  IPU, DPU ... PNN <br></td><td>  Principalmente treinando <br></td><td>  Muitas cartas de marketing que ser√£o esquecidas com seguran√ßa nos pr√≥ximos anos.  A principal vantagem deste zool√≥gico √© a verifica√ß√£o de diferentes dire√ß√µes da acelera√ß√£o DNN <br></td></tr><tr><td>  DNN / RPU anal√≥gico <br></td><td>  Execu√ß√£o + <br>  treinamento <br></td><td>  Aceleradores potencialmente anal√≥gicos podem revolucionar a velocidade e a efici√™ncia energ√©tica de executar e treinar redes neurais <br></td></tr></tbody></table></div><br><h2>  Algumas palavras sobre acelera√ß√£o de software </h2><br>  Para ser justo, mencionamos que hoje o grande t√≥pico √© a acelera√ß√£o por software da execu√ß√£o e treinamento de redes neurais profundas.  A execu√ß√£o pode ser significativamente acelerada principalmente devido √† chamada quantiza√ß√£o da rede.  Talvez isso seja, primeiro, uma vez que o intervalo de pesos usado n√£o √© t√£o grande e geralmente √© poss√≠vel aumentar pesos de um valor de ponto flutuante de 4 bytes para um n√∫mero inteiro de 1 byte (e, lembrando os sucessos da IBM, ainda mais fortes).  Em segundo lugar, a rede treinada como um todo √© bastante resistente ao ru√≠do computacional e a precis√£o da transi√ß√£o para o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">int8</a> cai um pouco.  Ao mesmo tempo, apesar do n√∫mero de opera√ß√µes poder aumentar (devido ao dimensionamento no c√°lculo), o fato de a rede ter um tamanho reduzido em 4 vezes e poder ser considerado opera√ß√µes vetoriais r√°pidas aumenta significativamente a velocidade geral de execu√ß√£o.  Isso √© especialmente importante para aplicativos m√≥veis, mas tamb√©m funciona nas nuvens (um exemplo de execu√ß√£o acelerada nas nuvens da Amazon): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/e52/bbd/2cc/e52bbd2cc36e24eff9a788c2d8371c83.png"></div><br>  Existem outras maneiras de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">acelerar</a> algoritmicamente <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">a execu√ß√£o da rede</a> e ainda mais maneiras de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">acelerar o aprendizado</a> .  No entanto, esses s√£o grandes t√≥picos separados sobre os quais n√£o desta vez. <br><br><h2>  Em vez de uma conclus√£o </h2><br>  Em suas palestras, o investidor e autor <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tony Ceba</a> d√° um exemplo magn√≠fico: em 2000, o supercomputador n¬∫ 1, com capacidade de 1 teraflops, ocupou 150 metros quadrados, custou US $ 46 milh√µes e consumiu 850 kW: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/webt/2v/6y/xe/2v6yxeo59aewcqngaybonbftsae.png"></div><br>  15 anos depois, a GPU NVIDIA com um desempenho de 2,3 teraflops (2 vezes mais) cabia em uma m√£o, custava US $ 59 (uma melhoria de cerca de um milh√£o de vezes) e consumia 15 watts (uma melhoria de 56 mil vezes): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/webt/gx/wz/qq/gxwzqqj1si3e3ho33nnypq_au3w.png"></div><br>  Em mar√ßo deste ano, o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Google lan√ßou os TPU Pods</a> , que na verdade s√£o supercomputadores de refrigera√ß√£o l√≠quida baseados no TPU v3, cuja principal caracter√≠stica √© que eles podem trabalhar juntos em sistemas de 1024 TPU.  Eles parecem bastante impressionantes: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/37c/849/2ec/37c8492ec7e380c5a26f8a420fc591d9.png"></div><br>  Os dados exatos n√£o s√£o fornecidos, mas diz-se que o sistema √© compar√°vel aos 5 maiores supercomputadores do mundo.  O TPU Pod pode aumentar drasticamente a velocidade de aprendizado de redes neurais.  Para aumentar a velocidade de intera√ß√£o, as TPUs s√£o conectadas por linhas de alta velocidade a uma estrutura toroidal: <br><br><img width="25%" src="https://habrastorage.org/getpro/habr/post_images/0f0/e26/532/0f0e2653272b633b8af1b6103540e95c.gif"><br>  Parece que, ap√≥s 15 anos, esse neuroprocessador duas vezes mais poderoso tamb√©m poder√° caber na sua m√£o, como o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">processador Skynet</a> (voc√™ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">deve</a> admitir, √© algo semelhante): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/cf2/5da/db6/cf25dadb60a350332cfef6bd147ec91b.png"></div>  <i>Filmado a partir da vers√£o diretorial do filme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">"Terminator 2"</a></i> <br><br>  Dada a atual taxa de aprimoramento dos aceleradores de hardware de redes neurais profundas e o exemplo acima, isso √© completamente real.  Em alguns anos, h√° uma chance de adquirir um chip com um desempenho como o TPU Pod de hoje. <br><br>  A prop√≥sito, √© engra√ßado que, no filme, os fabricantes de chips (aparentemente imaginando para onde a rede de auto-treinamento levaria) desativassem a reciclagem por padr√£o.  Caracteristicamente, o pr√≥prio <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">T-800</a> n√£o p√¥de ativar o modo de treinamento e trabalhou no modo de infer√™ncia (consulte a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">vers√£o</a> mais longa do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">diret√≥rio</a> ).  Al√©m disso, seu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">processador de rede neural</a> era avan√ßado e, ao ativar a reciclagem, poderia usar os dados acumulados anteriormente para atualizar o modelo.  Nada mal para 1991. <br><br>  Este texto foi iniciado no quente 13 milh√µes de Shenzhen.  Sentei-me em um dos 27.000 t√°xis el√©tricos da cidade e olhei para as quatro telas de cristal l√≠quido do carro com grande interesse.  Um pequeno - entre os dispositivos na frente do motorista, dois - no centro do painel e o √∫ltimo - transl√∫cido - no espelho retrovisor, combinado com um DVR, uma c√¢mera de vigil√¢ncia por v√≠deo e um androide a bordo (a julgar pela linha superior com o n√≠vel de carga e comunica√ß√£o com a rede).  Ele exibia dados do motorista (a quem reclamar, se isso), uma nova previs√£o do tempo e parecia haver uma conex√£o com a frota de t√°xis.  O motorista n√£o sabia ingl√™s e n√£o conseguiu perguntar sobre suas impress√µes sobre a m√°quina el√©trica.  Portanto, ele pregui√ßosamente pressionou o pedal, movendo levemente o carro em um engarrafamento.  E eu observei a janela com um olhar futurista com interesse - os chineses em suas jaquetas estavam dirigindo do trabalho em scooters el√©tricos e monowheels ... e me perguntei como ficaria em 15 anos ... <br><br>  Atualmente, atualmente, o espelho retrovisor, usando os dados da c√¢mera do DVR e a <i>acelera√ß√£o de hardware das redes neurais</i> , √© capaz de controlar o carro no tr√¢nsito e tra√ßar o caminho.  √Ä tarde, pelo menos).  Ap√≥s 15 anos, o sistema claramente n√£o apenas poder√° dirigir um carro, mas tamb√©m ter√° o prazer de me fornecer as caracter√≠sticas dos ve√≠culos el√©tricos chineses frescos.  Em russo, naturalmente (como uma op√ß√£o: ingl√™s, chin√™s ... alban√™s, finalmente).  O motorista aqui √© sup√©rfluo, mal treinado, um link. <br><br>  Senhor!  <b>EXTREMAMENTE INTERESSANTE</b> 15 anos est√£o esperando por n√≥s! <br><br>  Fique atento! <br><br>  Eu voltarei!  ))) <br><br><img width="35%" src="https://habrastorage.org/getpro/habr/post_images/3e8/caf/2cd/3e8caf2cde12f7b9bce6cd64de106357.png"><br><br>  <b>UPD:</b> Os coment√°rios mais interessantes: <br>  Sobre quantiza√ß√£o e acelera√ß√£o de c√°lculos no FPGA <br><div class="spoiler">  <b class="spoiler_title">Coment√°rios @Mirn</b> <div class="spoiler_text"><br>  No FPGA, n√£o apenas a aritm√©tica de precis√£o arbitr√°ria est√° dispon√≠vel, mas tamb√©m uma importante capacidade de salvar e processar dados de bits arbitr√°rios.  Por exemplo, h√° muitos coeficientes nos irritantes MobileNetV2 W e B e voc√™ pode quantific√°-los sem muita perda de precis√£o para apenas 16 bits ou ter√° que treinar novamente.  Mas se voc√™ olhar para dentro e coletar estat√≠sticas sobre canais e camadas, poder√° ver que todos os 16 bits s√£o usados ‚Äã‚Äãapenas na entrada dos primeiros coeficientes de 1000 W, o restante possui 8-11 bits, dos quais apenas 2-3 e sinais mais significativos s√£o realmente importantes, e estat√≠sticas sobre o uso de canais, de modo que existam muitos canais em que geralmente zeros ou valores pequenos ou canais em que quase todos os valores s√£o de 8 a 11 bits, ou seja,  √© poss√≠vel pregar o expositor nas unhas em tempo de compila√ß√£o e n√£o armazenar, ou seja,  na verdade, √© poss√≠vel armazenar na mem√≥ria ROM valores n√£o de 16 bits, mas de 4 bits, e voc√™ pode at√© armazenar toda a rede neural em FPGAs baratos sem muita perda de precis√£o (menos de 1%) e tamb√©m processar em velocidades de at√© dezenas de milhares de FPS com lat√™ncia, para obter uma resposta de rede neural imediatamente Como termina a recep√ß√£o do quadro. <br><br>  Sobre quantiza√ß√£o: minha ideia √© que, em v√°rios est√°gios da computa√ß√£o W, os coeficientes do canal n ¬∞ 0 mudam apenas de +50 para -50, faz sentido comprimir a bit para 7, e se de -123 para +124, por exemplo, para 8 (incluindo o sinal )    FPGA      ,      7, 8         ROM        .                 ,      . <br><br>           (,  , ),  RTL            ,          ,      .        GCC  AVX256    bitperfect (    FPGA  )           FPS      (       W  B,         ). <br><br>            W  fc   , ..      -100  +100   +10000      255      9        ( ). <br><br>                  !  porque dephwise    . <br><br>        u-law       (  !                ). <br><br>  ,          ,   6,      ,       . <br><br>                          (         ).  ‚Äî   ,    FixedPoint  dot product  ‚Äî     Fractional part,         ‚Äî       ,    ,      fc             . </div></div><br>       GPU, FPGA, ASIC    <br><div class="spoiler"> <b class="spoiler_title"> @BigPack</b> <div class="spoiler_text"><br> -        TVM   ( tvm.ai/about),      (   Keras)    .   ,      ‚Äî  ¬´¬ª-  (bare metal,    ISA, FPGA  .)       edge computing.       TVM   HLS  TVM    FPGA.  HLS     FPGA ¬´¬ª    ,  ( )      FPGA    ,    GPU/TPU . <br><br> PS  FPGA    transparent hardware (  ‚Äî open-source hardware),        ,          (    ¬´¬ª )    .     -.  , FPGA        ‚Äî         </div></div><br>      FPGA,  FPGA Microsoft     <br><div class="spoiler"> <b class="spoiler_title">  @Brak0del</b> <div class="spoiler_text"><br>    FPGA,   2019       ,      .       ‚Äî   .   /   dsp-  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Xilinx</a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Achronix</a> ,       DDR. <br><br>   , ,    , FPGA    ASIC-.   FPGA     :        ,       ASIC     ,  FPGA     -    .  I.e.     -    .            , ASIC-, ,     .  ,        FPGA  ,   ASIC. <br><br> ,    CPU, FPGA           ,      ,        . <br> ,    GPU      ,   FPGA    ,     :  , - ,    GPU      ,       ,     , -   (     ,   ,  ,   ,  FPGA   ,   GPU     , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">  </a> ).  , FPGA       ,   ,   ,    ASIC-. <br><br>       Microsoft ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> Catapult v.2</a> ),       FPGA-.  ,        FPGA.      ()    . <br><br>       FPGA         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Ristretto</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Deephi</a> ,      ,  Deephi       FPGA.   ,     ,         ,  . <br>   FPGA              . </div></div><br>    FPGA    ASIC <br><div class="spoiler"> <b class="spoiler_title">  @Mirn</b> <div class="spoiler_text"> <br>  Eu acho que o FPGA √© uma medida necess√°ria: <br>  eles combinam com sucesso o baixo custo relativo, a velocidade computacional e a velocidade de prototipagem e modifica√ß√£o em compara√ß√£o com os ASICs. <br><br>  Por exemplo: <br>  <b>FPGA</b> <br>  √â necess√°rio organizar a acelera√ß√£o do c√°lculo (mas n√£o o treinamento), v√°rios trabalhos s√£o necess√°rios (um conjunto de uma licen√ßa de quartus, licen√ßa de modelo, licen√ßas principais de IP e cascas de terceiros custam de 30 a 50 mil d√≥lares h√° cerca de 5 anos quando eu as comprei). <br>  voc√™ precisa comprar cart√µes, pelo menos intermedi√°rios como Arriya10 em termos de n√∫mero de empregos mais um (se queimarem e n√£o esperarem), isso √© 5kbaks * * (N + 1) <br>  Bem, sal√°rios, escrit√≥rio, contabilidade e outras despesas - aproximadamente 10 mil d√≥lares por m√™s para um desenvolvedor, eles obviamente n√£o funcionar√£o por alguns meses, mas alguns anos, isso √© 120 mil * N <br>  Durante esse per√≠odo, voc√™ pode fazer uma d√∫zia de vers√µes (fazer o primeiro sucesso em um ano e depois fazer corre√ß√µes todo m√™s e meio) <br>  Total para o ano: (120 + 50 + 5) * N, para 5 pessoas, s√£o 880 t de d√≥lares <br>  muito, mas voc√™ pode encontrar quem quiser pagar tente sem garantias <br><br>  <b>ASIC</b> <br>  Aqui eu n√£o sou especialista, mas parece-me que o software custar√° muito mais (otimista assumirei que 2 vezes) <br>  O pedido de chips custar√° na faixa de um milh√£o de d√≥lares por itera√ß√£o <br>  uma itera√ß√£o do ver√≠logo para o ferro levar√° v√°rias vezes mais (3-4 meses) <br>  no ASIC, voc√™ n√£o pode lan√ßar uma rede altamente especializada com bits e arquitetura "acertados", n√£o faz sentido faz√™-lo - significa que a estrutura se tornar√° mais complicada: deve ser moderadamente universal, √© muitas vezes mais complicada <br>  precisamos de negociadores, grande capital financeiro e um nome (antes de tudo, a f√°brica entrar√° na carteira e, se voc√™ ignorar tudo), gerentes e outros engenheiros tamb√©m s√£o necess√°rios - um chip √© necess√°rio para o chip e, de prefer√™ncia, um caso de refrigera√ß√£o, esse √© um n√≠vel muito trivial de circuitos e tecnologia. <br>  observe aqui: que na mesma minera√ß√£o, as placas s√£o especialmente primitivas - elas tentaram empurrar todos dentro do chip, incluindo comutadores de reguladores de tens√£o, drivers de corrente para interfaces, etc., etc. <br><br>  E, novamente, n√£o s√£o necess√°rios desenvolvedores simples de ver√≠logos, mas com conhecimento do anal√≥gico e conhecimento da f√≠sica de semicondutores e tecnologias (eu o estudei superficialmente no MiT - √© MUITO COMPLEXO, √© apenas o espa√ßo e a vanguarda de uma liga de ci√™ncia e tecnologia, t√£o dura sem smoothies, unic√≥rnios e pregadores marketing de inicializa√ß√£o, engenharia limpa e ci√™ncia) <br><br>  √â dif√≠cil calcular o total para o ASIC, mas s√£o claramente dezenas de milh√µes de d√≥lares, a equipe √© 10 vezes mais para as pessoas e o momento da primeira vers√£o de um buggy, mas de alguma forma trabalhando por 3-5 anos, com um risco muito alto de falha (e social - fa√ßa a equipe trabalhar, manter e trazer para a linha de chegada, e t√©cnico n√£o √© um fato que a arquitetura planejada ir√° disparar e os neg√≥cios n√£o s√£o um fato que todos os contratados n√£o ir√£o falhar) e as chances de fazer v√°rias tentativas diferentes com arquitetura diferente s√£o pequenas, mais precisamente uma tentativa: ningu√©m vai refazer do zero. <br>  Este √© um caso de mega corpora√ß√£o!  j√°!  desenvolvimentos e pessoas dispon√≠veis.  por exemplo, NEC e SONY (trabalhei com eles e as Olimp√≠adas em bi√¥nica, conhe√ßo os termos reais de 10 a 15 anos para uma primeira itera√ß√£o do zero, n√£o √© segredo) <br><br>  Resumidamente: FPGA √© uma maneira de acelerar e reduzir o custo de desenvolvimento em dezenas e centenas de vezes em rela√ß√£o ao ASIC. </div></div><br><div class="spoiler">  <b class="spoiler_title">Agradecimentos</b> <div class="spoiler_text">  Gostaria de agradecer cordialmente: <br><br><ul><li>  Laborat√≥rio de Computa√ß√£o Gr√°fica VMK Moscow State University  MV Lomonosov por sua contribui√ß√£o ao desenvolvimento de computa√ß√£o gr√°fica na R√∫ssia e n√£o apenas <br></li><li>  nossos colegas Mikhail Erofeev e Nikita Bagrov, cujos exemplos s√£o usados ‚Äã‚Äãacima, <br></li><li>  pessoalmente Konstantin Kozhemyakov, que fez muito para tornar este artigo melhor e mais visual, <br></li><li>  e, finalmente, muito obrigado a Alexander Bokov, Mikhail Erofeev, Vitaly Ludvichenko, Roman Kazantsev, Nikita Bagrov, Ivan Molodetsky, Yegor Sklyarov, Alexei Solovyov, Evgeny Lyapustin, Sergey Lavrushkin e Nikolai Oplachko por muitos coment√°rios e corre√ß√µes √∫teis que fizeram este texto melhor! <br></li></ul><br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt455353/">https://habr.com/ru/post/pt455353/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt455341/index.html">O que se sabe sobre a certifica√ß√£o ITIL 4</a></li>
<li><a href="../pt455343/index.html">Treinamento Cisco 200-125 CCNA v3.0. Dia 9. O mundo f√≠sico dos switches. Parte 2</a></li>
<li><a href="../pt455345/index.html">Cuidado M√©dico</a></li>
<li><a href="../pt455347/index.html">Interfaces funcionais ... em VBA</a></li>
<li><a href="../pt455351/index.html">VMware EMPOWER 2019 - os principais an√∫ncios e conclus√µes da confer√™ncia</a></li>
<li><a href="../pt455355/index.html">Redes de TV a cabo para os menores. Parte 8: Rede de backbone √≥ptico</a></li>
<li><a href="../pt455359/index.html">Swift funcional √© f√°cil</a></li>
<li><a href="../pt455361/index.html">Criamos uma extens√£o do navegador que verifica os resultados do exame</a></li>
<li><a href="../pt455367/index.html">VueJs + MVC c√≥digo m√≠nimo funcionalidade m√°xima</a></li>
<li><a href="../pt455369/index.html">Certifica√ß√£o de administradores de banco de dados e muito mais no anivers√°rio do DevConfX (21 a 22 de junho em Moscou)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>