<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🔅 👙 🔚 序列到序列第1部分模型 🌇 🕘 💔</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="祝大家有美好的一天！ 

 我们再次为修订后的“ 数据科学家”课程开辟了新渠道 ：另一位优秀的老师 ，基于更新的稍有改进的程序。 好吧，像往常一样，有趣的公开课和有趣的资料集。 今天，我们将开始分析Tensor Flow中的seq2seq模型。 

 走吧 

 正如RNN教程中已经讨论过的（建议您...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>序列到序列第1部分模型</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/430780/"> 祝大家有美好的一天！ <br><br> 我们再次为修订后的“ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">数据科学家”</a>课程开辟了新<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">渠道</a> ：另一位<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">优秀的老师</a> ，基于更新的稍有改进的程序。 好吧，像往常一样，有趣的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">公开课</a>和有趣的资料集。 今天，我们将开始分析Tensor Flow中的seq2seq模型。 <br><br> 走吧 <br><br> 正如<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">RNN教程中</a>已经讨论过的（建议您在阅读本文之前先熟悉它），可以教递归神经网络对语言进行建模。 随之而来的是一个有趣的问题：是否有可能在某些数据上训练网络以产生有意义的答案？ 例如，我们可以教一个神经网络将英语翻译成法语吗？ 事实证明，我们可以做到。 <br><br> 本指南将向您展示如何创建和培训这种端到端系统。  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">从GitHub</a>复制<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Tensor Flow核心存储库</a>和<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">TensorFlow模型存储库</a> 。 然后，您可以通过启动翻译程序开始： <br><br><pre><code class="python hljs">cd models/tutorials/rnn/translate python translate.py --data_dir [your_data_directory]</code> </pre> <br><img src="https://habrastorage.org/webt/ra/j0/rr/raj0rraitsp6itojzydkhrk2yoi.png"><a name="habracut"></a><br><br> 她将从<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">WMT'15网站上</a>下载数据，将其从英语翻译为法语， <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">以</a>备培训和培训之用。 这将需要大约20GB的硬盘驱动器，并且需要大量的时间进行下载和准备，因此您可以立即开始该过程并继续阅读本教程。 <br><br> 该手册将访问以下文件： <br><br><table><tbody><tr><th> 档案文件 </th><th> 里面有什么？ </th></tr><tr><td>  tensorflow /张量流/ python / ops / seq2seq.py </td><td> 用于创建序列到序列模型的库 </td></tr><tr><td> 模型/教程/ rnn /翻译/ seq2seq_model.py </td><td> 序列到序列的神经翻译模型 </td></tr><tr><td> 模型/教程/ rnn /翻译/ data_utils.py </td><td> 用于准备翻译数据的辅助功能 </td></tr><tr><td> 模型/教程/rnn/translate/translate.py </td><td> 训练和运行翻译模型的二进制文件 </td></tr></tbody></table><br>  <b>序列到序列的基础知识</b> <br><br>  <a href="">Cho et al。，2014</a> （ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">pdf</a> ）提出的基本序列到序列模型由两个递归神经网络（RNN）组成：处理输入数据的编码器（encoder）和生成数据的解码器（decoder）输出。 基本架构如下所示： <br><br><img src="https://habrastorage.org/webt/e-/df/cu/e-dfcuvlsbykvyxvzac9rc0nrow.png"><br><br> 上图中的每个矩形代表RNN中的一个单元，通常是一个GRU单元-一个受控的递归块或一个LSTM单元-长期短期记忆（请阅读<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">RNN教程</a>以了解更多信息）。 编码器和解码器可以具有相同的权重，或更常见的是，使用不同的参数集。 多层细胞已成功用于序列到序列模型中，例如，翻译<a href="">Sutskever等人，2014</a> （ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">pdf</a> ）。 <br><br> 在上述基本模型中，必须将每个输入编码为固定大小的状态向量，因为这是唯一传输到解码器的内容。 为了使解码器更直接地访问输入数据， <a href="">Bahdanau等人（2014年</a> ）引入了一种注意力机制（ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">pdf</a> ）。 我们将不涉及注意力机制的细节（为此，您可以在这里熟悉自己的工作）； 可以说，它允许解码器在每个解码步骤中查看输入数据。 具有LSTM单元和解码器中的注意机制的多层序列到序列网络如下： <br><br><img src="https://habrastorage.org/webt/c4/ro/0z/c4ro0zvzu8m-y4qjlnfbhv9x4qa.png"><br><br>  <b>TensorFlow库seq2seq</b> <br><br> 从上面可以看到，有不同的序列到序列模型。 它们都可以使用不同的RNN单元，但是它们都接受编码器输入数据和解码器输入数据。 这是TensorFlow seq2seq库接口（tensorflow / tensorflow / python / ops / seq2seq.py）的基础。 这种基本的RNN，编解码器，序列到序列模型的工作原理如下。 <br><br><pre> <code class="python hljs">outputs, states = basic_rnn_seq2seq(encoder_inputs, decoder_inputs, cell)</code> </pre> <br> 在上面的调用中， <code>encoder_inputs</code>是代表编码器输入数据的张量列表，对应于上图中的字母A，B，C。 类似地， <code>decoder_inputs</code>是表示解码器输入数据的张量。 从第一张图片开始GO，W，X，Y，Z。 <br><br>  <code>cell</code>参数是<code>tf.contrib.rnn.RNNCell</code>类的实例，该类确定将在模型中使用哪个单元格。 您可以使用现有的单元格，例如<code>GRUCell</code>或<code>LSTMCell</code> ，也可以编写自己的单元格。 另外， <code>tf.contrib.rnn</code>提供了用于创建多层单元的外壳，为单元输入和输出或其他转换添加了例外。 查看<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">RNN教程</a>中的示例。 <br><br>  <code>basic_rnn_seq2seq</code>调用返回两个参数： <code>outputs</code>和<code>states</code> 。 它们都表示与<code>decoder_inputs</code>输入相同长度的张量列表。  <code>outputs</code>对应于每个时间步长的解码器输出数据，在第一张图片中是W，X，Y，Z，EOS。 返回的<code>states</code>表示每个时间步解码器的内部状态。 <br><br> 在使用序列到序列模型的许多应用中，时间t处的解码器输出在时间t +1处被传输回输入到解码器。 在测试过程中，在序列解码过程中，这就是构造新序列的方式。 另一方面，在训练期间，习惯上在每个时间步长将正确的输入数据发送到解码器，即使先前解码器被弄错了。  <code>seq2seq.py</code>函数通过<code>feed_previous</code>参数支持两种模式。 例如，考虑以下对嵌套RNN模型的使用。 <br><br><pre> <code class="python hljs">outputs, states = embedding_rnn_seq2seq( encoder_inputs, decoder_inputs, cell, num_encoder_symbols, num_decoder_symbols, embedding_size, output_projection=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, feed_previous=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)</code> </pre> <br> 在<code>embedding_rnn_seq2seq</code>模型中，所有输入数据（ <code>decoder_inputs</code>输入和<code>decoder_inputs</code>输入）都是反映离散值的整数张量。 它们将嵌套在一个紧密的表示形式中（有关附件的详细信息，请参见“ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">向量视图指南”</a> ），但是要创建这些附件，您需要指定最大离散字符数：在编码器端为<code>num_decoder_symbols</code> ，在解码器端为<code>num_decoder_symbols</code> 。 <br><br> 在上面的调用中，我们将<code>feed_previous</code>设置为False。 这意味着解码器将以提供它们的形式使用解码器<code>decoder_inputs</code>张量。 如果将<code>feed_previous</code>设置为True，则解码器将仅使用第一个<code>decoder_inputs</code>元素。 列表中的所有其他张量将被忽略，而将使用解码器输出的先前值代替。 这用于解码翻译模型中的翻译，但也可以在训练过程中使用，以提高模型对其错误的稳定性。 大致与<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Bengio et al。，2015</a> （ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">pdf</a> ）相同。 <br><br> 上面使用的另一个重要参数是<code>output_projection</code> 。 无需澄清，嵌入式模型的结论将为张量，其形式为每<code>num_decoder_symbols</code>训练样本的数量，因为它们表示每个生成符号的对数。 当训练具有大输出字典的模型（例如，具有大<code>num_decoder_symbols</code> ，存储这些大张量变得不切实际。 相反，最好返回较小的张量，随后将使用<code>output_projection</code>将其投影到较大的张量上。 这使我们可以使用seq2seq模型和采样的softmax损失，如<a href="">Jean等人所述。</a>  <a href="">等人，2014年</a> （ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">pdf</a> ）。 <br><br> 除了<code>basic_rnn_seq2seq</code>和<code>embedding_rnn_seq2seq</code>之外， <code>basic_rnn_seq2seq</code>还有更多的序列到序列模型。 注意他们。 它们都具有相似的界面，因此我们将不深入研究它们的细节。 对于下面的翻译模型，请使用<code>embedding_attention_seq2seq</code> 。 <br><br> 待续。 </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN430780/">https://habr.com/ru/post/zh-CN430780/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN430768/index.html">ADOM创作者Thomas Biscap访谈</a></li>
<li><a href="../zh-CN430770/index.html">Backup for Linux，或如何创建快照</a></li>
<li><a href="../zh-CN430774/index.html">您准备好在广告牌上使用AI了吗？</a></li>
<li><a href="../zh-CN430776/index.html">制作IP是唯一的方法</a></li>
<li><a href="../zh-CN430778/index.html">3DEXPERIENCE端到端电气系统设计流程</a></li>
<li><a href="../zh-CN430782/index.html">您需要多少程序员来支持以前编写的代码？</a></li>
<li><a href="../zh-CN430784/index.html">从初级到导演：一个卫兵的故事</a></li>
<li><a href="../zh-CN430788/index.html">我在伦敦的IB IT（Java开发人员，投资银行）的采访历史，以及典型任务的示例</a></li>
<li><a href="../zh-CN430790/index.html">Ledger Nano S：通往710个代币和加密货币所在空间的钥匙</a></li>
<li><a href="../zh-CN430792/index.html">在Unity中为LWRP创建大纲</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>