<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëÇüèº üï• üí∏ Wie wir bei TsIAN Terabytes an Protokollen gez√§hmt haben üë®üèø‚Äçüîß üôÄ üë©‚Äç‚öñÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo allerseits, mein Name ist Alexander, ich arbeite als Ingenieur bei CIAN und bin in der Systemadministration und Automatisierung von Infrastruktu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie wir bei TsIAN Terabytes an Protokollen gez√§hmt haben</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/cian/blog/478564/"><img src="https://habrastorage.org/getpro/habr/post_images/f5a/f37/994/f5af37994ecad978b8cd3edd3dc7ae0a.png"><br><br>  Hallo allerseits, mein Name ist Alexander, ich arbeite als Ingenieur bei CIAN und bin in der Systemadministration und Automatisierung von Infrastrukturprozessen t√§tig.  In den Kommentaren zu einem der vorherigen Artikel wurden wir gebeten, anzugeben, wo wir 4 TB an Protokollen pro Tag erhalten und was wir damit machen.  Ja, wir haben viele Protokolle und es wurde ein separater Infrastruktur-Cluster erstellt, um diese zu verarbeiten, sodass wir Probleme schnell l√∂sen k√∂nnen.  In diesem Artikel werde ich dar√ºber sprechen, wie wir es im Laufe des Jahres angepasst haben, um mit einem stetig wachsenden Datenfluss zu arbeiten. <br><a name="habracut"></a><br><h3>  Wo haben wir angefangen? </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/6d2/1eb/3da/6d21eb3da4aa0189ae44b9ca951b15a8.jpg"><br><br>  In den letzten Jahren ist die Auslastung von cian.ru sehr schnell gewachsen, und im dritten Quartal 2018 erreichte der Ressourcenverkehr 11,2 Millionen Unique User pro Monat.  Zu diesem Zeitpunkt, in kritischen Momenten, haben wir bis zu 40% der Protokolle verloren, weshalb wir Vorf√§lle nicht schnell bearbeiten konnten und viel Zeit und M√ºhe auf ihre Behebung verwendet haben.  Wir konnten die Ursache des Problems oft nicht finden und es trat nach einiger Zeit wieder auf.  Es war die H√∂lle, mit der du etwas anfangen musstest. <br><br>  Zu diesem Zeitpunkt verwendeten wir einen Cluster von 10 Datenknoten mit ElasticSearch Version 5.5.2 mit typischen Indexeinstellungen zum Speichern von Protokollen.  Es wurde vor mehr als einem Jahr als beliebte und erschwingliche L√∂sung eingef√ºhrt: Damals war der Protokolldatenstrom nicht so gro√ü, und es ergab keinen Sinn, nicht standardm√§√üige Konfigurationen zu entwickeln. <br><br>  Logstash auf verschiedenen Ports erm√∂glichte die Verarbeitung eingehender Protokolle auf f√ºnf ElasticSearch-Koordinatoren.  Ein Index, unabh√§ngig von der Gr√∂√üe, bestand aus f√ºnf Scherben.  Es wurde eine st√ºndliche und t√§gliche Rotation organisiert, sodass st√ºndlich etwa 100 neue Scherben im Cluster erschienen.  Obwohl es nicht sehr viele Protokolle gab, verwaltete der Cluster und niemand machte auf seine Einstellungen aufmerksam. <br><br><h3>  Wachstumsprobleme </h3><br>  Das Volumen der generierten Protokolle wuchs sehr schnell, da sich zwei Prozesse √ºberlappten.  Einerseits gab es immer mehr Nutzer des Dienstes.  Andererseits begannen wir, aktiv auf Microservice-Architektur umzusteigen und unsere alten Monolithen in C # und Python zu zers√§gen.  Mehrere Dutzend neue Mikrodienste, die Teile des Monolithen ersetzten, erzeugten erheblich mehr Protokolle f√ºr den Infrastrukturcluster. <br><br>  Es war die Skalierung, die dazu f√ºhrte, dass der Cluster praktisch unkontrollierbar wurde.  Als die Protokolle mit einer Geschwindigkeit von 20.000 Nachrichten pro Sekunde ankamen, erh√∂hte die h√§ufige nutzlose Rotation die Anzahl der Shards auf 6.000, und auf einen Knoten entfielen mehr als 600 Shards. <br><br>  Dies f√ºhrte zu Problemen bei der Zuweisung des Arbeitsspeichers. Als ein Knoten ausfiel, begannen alle Shards gleichzeitig zu verschieben, den Datenverkehr zu vervielfachen und die verbleibenden Knoten zu laden. Dies machte das Schreiben von Daten in den Cluster nahezu unm√∂glich.  Und w√§hrend dieser Zeit blieben wir ohne Protokolle.  Und bei einem Serverproblem haben wir im Prinzip 1/10 des Clusters verloren.  Eine gro√üe Anzahl kleiner Indizes erh√∂hte die Komplexit√§t. <br><br>  Ohne Protokolle haben wir die Ursachen des Vorfalls nicht verstanden und konnten fr√ºher oder sp√§ter wieder auf den gleichen Rechen treten. In der Ideologie unseres Teams war dies jedoch inakzeptabel, da alle Arbeitsmechanismen, an denen wir gearbeitet hatten, genau umgekehrt waren - wiederholen Sie niemals dieselben Probleme.  Zu diesem Zweck ben√∂tigten wir eine vollst√§ndige Menge von Protokollen und deren Lieferung in nahezu Echtzeit, da ein Team von Einsatzingenieuren Warnungen nicht nur anhand von Messwerten, sondern auch anhand von Protokollen √ºberwachte.  Um das Ausma√ü des Problems zu verstehen - zu diesem Zeitpunkt betrug das Gesamtvolumen der Protokolle etwa 2 TB pro Tag. <br><br>  Wir haben uns zum Ziel gesetzt, den Verlust von Protokollen vollst√§ndig zu vermeiden und die Zeit bis zu deren Zustellung an den ELK-Cluster w√§hrend h√∂herer Gewalt auf maximal 15 Minuten zu verk√ºrzen (wir haben uns in Zukunft auf diese Zahl als internen KPI gest√ºtzt). <br><br><h3>  Neuer Rotationsmechanismus und hei√ü-warme Knoten </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/363/96f/05e/36396f05e01c97e805388ff27d134e2a.jpg"><br><br>  Wir haben die Cluster-Transformation gestartet, indem wir die Version von ElasticSearch von 5.5.2 auf 6.4.3 aktualisiert haben.  Wieder einmal ist ein Cluster der Version 5 auf uns heruntergekommen, und wir haben beschlossen, es zur√ºckzuzahlen und vollst√§ndig zu aktualisieren - es sind noch keine Protokolle vorhanden.  So haben wir diesen √úbergang in nur wenigen Stunden geschafft. <br><br>  Die ehrgeizigste Transformation in dieser Phase war die Einf√ºhrung von drei Knoten mit dem Koordinator als Zwischenpuffer Apache Kafka.  Der Nachrichtenbroker hat uns vor dem Verlust von Protokollen bei Problemen mit ElasticSearch bewahrt.  Gleichzeitig haben wir dem Cluster zwei Knoten hinzugef√ºgt und auf eine Hot-Warm-Architektur mit drei ‚ÄûHot‚Äú -Knoten umgestellt, die in verschiedenen Racks im Rechenzentrum angeordnet sind.  Wir haben Protokolle an diese weitergeleitet, die auf keinen Fall verloren gehen sollten - nginx sowie Anwendungsfehlerprotokolle.  Kleinere Protokolle - Debugging, Warnung usw. - wurden an andere Knoten gesendet und nach 24 Stunden wurden wichtige Protokolle von aktiven Knoten verschoben. <br><br>  Um die Anzahl der kleinen Indizes nicht zu erh√∂hen, haben wir von der Zeitrotation auf den Rollover-Mechanismus umgestellt.  In den Foren gab es viele Informationen dar√ºber, dass die Drehung um die Indexgr√∂√üe sehr unzuverl√§ssig ist. Daher haben wir uns f√ºr die Drehung um die Anzahl der Dokumente im Index entschieden.  Wir haben jeden Index analysiert und die Anzahl der Dokumente aufgezeichnet, nach denen die Rotation funktionieren soll.  Damit haben wir die optimale Gr√∂√üe des Shards erreicht - nicht mehr als 50 GB. <br><br><h3>  Cluster-Optimierung </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/cf3/c46/45e/cf3c4645e6b74cf5491f9878dacfa185.jpg"><br><br>  Die Probleme haben wir jedoch nicht ganz beseitigt.  Leider erschienen kleine Indizes trotzdem: Sie erreichten nicht das festgelegte Volumen, drehten sich nicht und wurden durch die globale Bereinigung von Indizes, die √§lter als drei Tage waren, gel√∂scht, da wir die Drehung nach Datum entfernt haben.  Dies f√ºhrte zu Datenverlusten, da der Index aus dem Cluster vollst√§ndig verschwand und der Versuch, in einen nicht vorhandenen Index zu schreiben, die f√ºr die Steuerung verwendete Kuratorenlogik unterbrach.  Der Alias ‚Äã‚Äãf√ºr die Aufzeichnung wurde in einen Index umgewandelt und brach die Rollover-Logik, was zu einem unkontrollierten Wachstum einiger Indizes auf 600 GB f√ºhrte. <br><br>  So konfigurieren Sie beispielsweise die Drehung: <br><br><pre><code class="plaintext hljs">urator-elk-rollover.yaml --- actions:   1:     action: rollover     options:       name: "nginx_write"       conditions:         max_docs: 100000000   2:     action: rollover     options:       name: "python_error_write"       conditions:         max_docs: 10000000</code> </pre> <br><br>  In Abwesenheit eines Rollover-Alias ‚Äã‚Äãist ein Fehler aufgetreten: <br><br><pre> <code class="plaintext hljs">ERROR   alias "nginx_write" not found. ERROR   Failed to complete action: rollover. &lt;type 'exceptions.ValueError'&gt;: Unable to perform index rollover with alias "nginx_write".</code> </pre><br><br>  Wir haben die L√∂sung f√ºr dieses Problem f√ºr die n√§chste Iteration hinterlassen und eine andere Frage aufgeworfen: Wir haben auf die Logik von Logstash umgestellt, die eingehende Protokolle verarbeitet (unn√∂tige Informationen entfernen und anreichern).  Wir haben es in docker platziert, das wir √ºber docker-compose ausf√ºhren, und logstash-exporter an derselben Stelle platziert, wodurch Prometheus die Metriken f√ºr die Betriebs√ºberwachung des Protokolldatenstroms erh√§lt.  Deshalb haben wir uns die M√∂glichkeit gegeben, die Anzahl der f√ºr die Verarbeitung der einzelnen Protokolltypen verantwortlichen Protokollstash-Instanzen reibungslos zu √§ndern. <br><br>  W√§hrend wir den Cluster verbesserten, stieg der Traffic von cian.ru auf 12,8 Millionen Unique User pro Monat.  Infolgedessen stellte sich heraus, dass unsere Conversions nicht mit den √Ñnderungen in der Produktion Schritt hielten und wir mit der Tatsache konfrontiert waren, dass die "warmen" Knoten die Last nicht bew√§ltigen konnten und die gesamte Zustellung von Protokollen verlangsamten.  Wir haben die "hei√üen" Daten ohne Fehler erhalten, mussten jedoch in die Zustellung der restlichen Daten eingreifen und einen manuellen Rollover durchf√ºhren, um die Indizes gleichm√§√üig zu verteilen. <br><br>  Gleichzeitig wurde das Skalieren und √Ñndern der Einstellungen von Logstash-Instanzen im Cluster durch die Tatsache erschwert, dass es sich um einen lokalen Docker-Compose handelte und alle Aktionen von Hand ausgef√ºhrt wurden (um neue Aufgaben hinzuzuf√ºgen, mussten Sie alle Server mit Ihren H√§nden durchgehen und Docker-Compose-D √ºberall ausf√ºhren). <br><br><h3>  Log redistribution </h3><br>  Im September dieses Jahres sahen wir immer noch den Monolithen, die Auslastung des Clusters nahm zu und der Protokolldatenstrom n√§herte sich 30.000 Nachrichten pro Sekunde. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/28c/ff7/c76/28cff7c7661c901102a69fe49beeccd0.png"><br><br>  Wir begannen die n√§chste Iteration mit der Aktualisierung des Eisens.  Wir wechselten von f√ºnf auf drei Koordinatoren, ersetzten Datenknoten und gewannen in Bezug auf Geld und Speichervolumen.  F√ºr Knoten verwenden wir zwei Konfigurationen: <br><br><ul><li>  F√ºr Hot Nodes: E3-1270 v6 / 960 Gb SSD / 32 Gb x 3 x 2 (3 f√ºr Hot1 und 3 f√ºr Hot2). <br></li><li>  F√ºr warme Knoten: E3-1230 v6 / 4 TB SSD / 32 GB x 4. <br></li></ul><br>  Bei dieser Iteration haben wir den Index mit den Zugriffsprotokollen f√ºr Mikroservices, der genauso viel Speicherplatz beansprucht wie die Front-End-Nginx-Protokolle, in die zweite Gruppe von drei Hot Nodes aufgenommen.  Wir speichern jetzt Daten auf Hot Nodes f√ºr 20 Stunden und √ºbertragen sie dann in andere Protokolle. <br><br>  Wir haben das Problem des Verschwindens kleiner Indizes gel√∂st, indem wir ihre Rotation neu konfiguriert haben.  Indizes werden jetzt ohnehin alle 23 Stunden gedreht, auch wenn nur wenige Daten vorliegen.  Dies erh√∂hte leicht die Anzahl der Shards (sie wurden ungef√§hr 800), aber aus Sicht der Cluster-Leistung ist dies tolerierbar. <br><br>  Infolgedessen wurden sechs "hei√üe" und nur vier "warme" Knoten im Cluster gefunden.  Dies f√ºhrt zu einer leichten Verz√∂gerung der Anforderungen √ºber lange Zeitintervalle, aber eine Erh√∂hung der Anzahl der Knoten in der Zukunft wird dieses Problem l√∂sen. <br><br>  In dieser Iteration wurde auch das Problem des Fehlens einer halbautomatischen Skalierung behoben.  Zu diesem Zweck haben wir einen Infrastruktur-Nomad-Cluster bereitgestellt - √§hnlich dem, den wir bereits f√ºr die Produktion bereitgestellt haben.  Die Anzahl der Logstashs √§ndert sich zwar nicht automatisch je nach Auslastung, aber wir werden darauf zur√ºckkommen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5c5/331/493/5c533149373a09c73e329e3bb43e0d81.png"><br><br><h3>  Zukunftspl√§ne </h3><br>  Die implementierte Konfiguration l√§sst sich gut skalieren, und jetzt speichern wir 13,3 TB an Daten - alle Protokolle in 4 Tagen, was f√ºr die Notfallanalyse von Warnungen erforderlich ist.  Wir konvertieren einen Teil der Protokolle in Metriken, die wir zu Graphite hinzuf√ºgen.  Um den Ingenieuren die Arbeit zu erleichtern, verf√ºgen wir √ºber Metriken f√ºr den Infrastrukturcluster und Skripte zur halbautomatischen Behebung typischer Probleme.  Nachdem wir die Anzahl der Datenknoten erh√∂ht haben, die f√ºr das n√§chste Jahr geplant ist, werden wir von 4 auf 7 Tage umstellen.  Dies wird f√ºr die operative Arbeit ausreichen, da wir immer versuchen, Vorf√§lle so schnell wie m√∂glich zu untersuchen und Telemetriedaten f√ºr Langzeituntersuchungen zur Verf√ºgung stehen. <br><br>  Im Oktober 2019 stieg der Traffic von cian.ru auf 15,3 Millionen Unique User pro Monat.  Dies war ein schwerwiegender Test der Architekturl√∂sung f√ºr die Lieferung von Protokollen. <br><br>  Jetzt bereiten wir ein Upgrade von ElasticSearch auf Version 7 vor. Hierzu m√ºssen wir jedoch die Zuordnung vieler Indizes in ElasticSearch aktualisieren, da sie von Version 5.5 verschoben wurden und in Version 6 als veraltet deklariert wurden (sie sind in Version 7 einfach nicht vorhanden).  Und das bedeutet, dass es w√§hrend des Aktualisierungsprozesses mit Sicherheit zu h√∂herer Gewalt kommen wird, die uns vorerst ohne Protokolle l√§sst.  Von den 7 Versionen freuen wir uns am meisten auf Kibana mit einer verbesserten Benutzeroberfl√§che und neuen Filtern. <br><br>  Wir haben das Hauptziel erreicht: Wir haben den Verlust von Protokollen gestoppt und die Ausfallzeit des Infrastrukturclusters von 2-3 Ausf√§llen pro Woche auf ein paar Stunden Service pro Monat reduziert.  All diese Arbeiten an der Produktion sind fast unsichtbar.  Jetzt k√∂nnen wir jedoch genau bestimmen, was mit unserem Service passiert. Wir k√∂nnen dies schnell und leise tun und m√ºssen uns keine Sorgen mehr machen, dass die Protokolle verloren gehen.  Im Allgemeinen sind wir zufrieden, gl√ºcklich und bereiten uns auf neue Exploits vor, √ºber die wir sp√§ter sprechen werden. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de478564/">https://habr.com/ru/post/de478564/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de478546/index.html">Websockets Erfahrung in Entwicklung und Betrieb. Wir modifizieren den Kunden</a></li>
<li><a href="../de478550/index.html">Wie verwalte ich eine Uhr? Analyse der Front-End-Strecke der zweiten Programmiermeisterschaft</a></li>
<li><a href="../de478552/index.html">Zweites Applet, Schlie√üen und transparente Schaltfl√§chen in Verarbeitung 3</a></li>
<li><a href="../de478554/index.html">Webinar "SRE - Hype oder die Zukunft?" 12. Dezember um 11:00 Uhr</a></li>
<li><a href="../de478560/index.html">Sind kostenlose Instant Messenger anonym?</a></li>
<li><a href="../de478566/index.html">iOS Netzwerk, wenn die Anwendung nicht ausgef√ºhrt wird</a></li>
<li><a href="../de478572/index.html">Bot in neuronalen Netzen: Wie ein virtueller Assistent funktioniert und lernt</a></li>
<li><a href="../de478574/index.html">Die Wahrheit √ºber Eisenbahnbremsen: Teil 4 - Fahrgastbremsen</a></li>
<li><a href="../de478582/index.html">Globaler VPN-Bericht √ºber mobile Ger√§te im Jahr 2019</a></li>
<li><a href="../de478584/index.html">JVM-Interna, Teil 2 - Dateistruktur der Klasse</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>