<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üêî ‚ù£Ô∏è ü§Ωüèæ C√≥mo aprendimos a explotar Java en Docker üë©üèæ‚Äçüîß ü•° ü§¥üèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Debajo del cap√≥, hh.ru contiene una gran cantidad de servicios Java que se ejecutan en contenedores acoplables. Durante su operaci√≥n, encontramos much...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>C√≥mo aprendimos a explotar Java en Docker</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/hh/blog/450954/">  Debajo del cap√≥, hh.ru contiene una gran cantidad de servicios Java que se ejecutan en contenedores acoplables.  Durante su operaci√≥n, encontramos muchos problemas no triviales.  En muchos casos, para llegar al fondo de la soluci√≥n, tuve que buscar en Google durante mucho tiempo, leer las fuentes de OpenJDK e incluso perfilar los servicios en producci√≥n.  En este art√≠culo intentar√© transmitir la quintaesencia del conocimiento adquirido en el proceso. <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">L√≠mites de CPU</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Docker y m√°quina de clase de servidor</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">L√≠mites de CPU (s√≠, nuevamente) y fragmentaci√≥n de memoria</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Procesamos Java-OOM</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Optimizando el consumo de memoria</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Limitar el consumo de memoria: memoria directa heap, no heap</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Limitaci√≥n del consumo de memoria: seguimiento de memoria nativa</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Java y unidades</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">¬øC√≥mo hacer un seguimiento de todo?</a> </li></ul><br><a name="habracut"></a><a name="cpu"></a><h4>  L√≠mites de CPU </h4><br>  Sol√≠amos vivir en m√°quinas virtuales kvm con limitaciones de CPU y memoria y, al pasar a Docker, establecimos restricciones similares en cgroups.  Y el primer problema que encontramos fue precisamente los l√≠mites de la CPU.  Debo decir de inmediato que este problema ya no es relevante para las versiones recientes de Java 8 y Java ‚â• 10. Si se mantiene al d√≠a, puede saltarse esta secci√≥n de manera segura. <br><br>  Entonces, comenzamos un peque√±o servicio en el contenedor y vemos que produce una gran cantidad de hilos.  O la CPU consume mucho m√°s de lo esperado, tiempo de espera cu√°nto en vano.  O aqu√≠ hay otra situaci√≥n real: en una m√°quina, el servicio comienza normalmente, y en otra, con la misma configuraci√≥n, se bloquea, clavado por un asesino OOM. <br><br>  La soluci√≥n resulta ser muy simple: solo Java no ve las limitaciones de <code>--cpus</code> establecidas en la <code>--cpus</code> acoplable y cree que todos los n√∫cleos de la m√°quina host son accesibles.  Y puede haber muchos de ellos (en nuestra configuraci√≥n est√°ndar - 80). <br>  Las bibliotecas ajustan el tama√±o de los grupos de subprocesos a la cantidad de procesadores disponibles, de ah√≠ la gran cantidad de subprocesos. <br>  El propio Java escala el n√∫mero de subprocesos de GC de la misma manera, de ah√≠ el consumo de CPU y los tiempos de espera: el servicio comienza a gastar una gran cantidad de recursos en la recolecci√≥n de basura, utilizando la mayor parte de la cuota asignada. <br>  Adem√°s, las bibliotecas (en particular Netty) pueden, en ciertos casos, ajustar el tama√±o de la memoria fuera de la cadera al n√∫mero de CPU, lo que conduce a una alta probabilidad de exceder los l√≠mites establecidos para el contenedor cuando se ejecuta en un hardware m√°s potente. <br><br>  Al principio, cuando este problema se manifest√≥, intentamos usar las siguientes rondas de trabajo: <br>  - intent√≥ utilizar un par de servicios <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">libnumcpus</a> , una biblioteca que le permite "enga√±ar" a Java configurando un n√∫mero diferente de procesadores disponibles; <br>  - indic√≥ expl√≠citamente el n√∫mero de hilos GC, <br>  - establecer expl√≠citamente l√≠mites en el uso de buffers de bytes directos. <br><br>  Pero, por supuesto, moverse con esas muletas no es muy conveniente, y el cambio a Java 10 (y luego a Java 11), en el que todos estos problemas est√°n <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ausentes</a> , fue una soluci√≥n real.  Para ser justos, vale la pena decir que tambi√©n en los ocho, todo estuvo bien con la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">actualizaci√≥n 191</a> , lanzada en octubre de 2018.  Para entonces ya no era relevante para nosotros, lo que tambi√©n deseo para ti. <br><br>  Este es un ejemplo en el que la actualizaci√≥n de la versi√≥n de Java brinda no solo satisfacci√≥n moral, sino tambi√©n un beneficio real tangible en forma de operaci√≥n simplificada y mayor rendimiento del servicio. <br><br><a name="server-class"></a><h4>  Docker y m√°quina de clase de servidor </h4><br>  Entonces, en Java 10, <code>-XX:ActiveProcessorCount</code> las <code>-XX:ActiveProcessorCount</code> y <code>-XX:+UseContainerSupport</code> (y fueron retroportadas a Java 8), teniendo en cuenta los l√≠mites predeterminados de cgroups.  Ahora todo fue maravilloso.  O no? <br><br>  Alg√∫n tiempo despu√©s de mudarnos a Java 10/11, comenzamos a notar algunas rarezas.  Por alguna raz√≥n, en algunos servicios, los gr√°ficos de GC parec√≠an no usar G1: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gc/69/bo/gc69boghbrkf2wmxiuioszq5qgg.png"></div><br><br>  Esto fue, por decirlo suavemente, un poco inesperado, ya que sab√≠amos con certeza que G1 es el recopilador predeterminado, comenzando con Java 9. Al mismo tiempo, no hay tal problema en algunos servicios: G1 est√° activado, como se esperaba. <br><br>  Comenzamos a entender y tropezar con <a href="">algo interesante</a> .  Resulta que si Java se ejecuta en menos de 3 procesadores y con un l√≠mite de memoria de menos de 2 GB, entonces se considera cliente y no permite usar nada m√°s que SerialGC. <br><br>  Por cierto, esto afecta solo la <a href="">elecci√≥n de GC</a> y no tiene nada que ver con las opciones de compilaci√≥n -client / -server y JIT. <br><br>  Obviamente, cuando usamos Java 8, no tuvo en cuenta los l√≠mites de la ventana acoplable y pens√≥ que ten√≠a muchos procesadores y memoria.  Despu√©s de actualizar a Java 10, muchos servicios con l√≠mites m√°s bajos comenzaron de repente a usar SerialGC.  Afortunadamente, esto se trata de manera muy simple, configurando expl√≠citamente la <code>-XX:+AlwaysActAsServerClassMachine</code> . <br><br><a name="malloc"></a><h4>  L√≠mites de CPU (s√≠, nuevamente) y fragmentaci√≥n de memoria </h4><br>  Al observar los gr√°ficos en el monitoreo, de alguna manera notamos que el tama√±o del conjunto residente del contenedor es demasiado grande, hasta tres veces m√°s que el tama√±o m√°ximo de la cadera.  ¬øPodr√≠a ser este el caso en alg√∫n pr√≥ximo mecanismo complicado que se escala de acuerdo con el n√∫mero de procesadores en el sistema y no conoce las limitaciones del acoplador? <br><br>  Resulta que el mecanismo no es del todo complicado: es el conocido malloc de glibc.  En resumen, glibc usa las llamadas arenas para asignar memoria.  Al crear, a cada hilo se le asigna una de las arenas.  Cuando un hilo que usa glibc quiere asignar una cierta cantidad de memoria en el mont√≥n nativo a sus necesidades y llama a malloc, entonces la memoria se asigna en la arena asignada a √©l.  Si la arena sirve varios hilos, entonces estos hilos competir√°n por √©l.  Cuantas m√°s arenas, menos competencia, pero m√°s fragmentaci√≥n, ya que cada arena tiene su propia lista de √°reas libres. <br><br>  En sistemas de 64 bits, el n√∫mero predeterminado de arenas se establece en 8 * el n√∫mero de CPU.  Obviamente, esta es una gran sobrecarga para nosotros, porque no todas las CPU est√°n disponibles para el contenedor.  Adem√°s, para las aplicaciones basadas en Java, la competencia por las arenas no es tan relevante, ya que la mayor√≠a de las asignaciones se realizan en el mont√≥n de Java, cuya memoria se puede asignar por completo al inicio. <br><br>  Esta caracter√≠stica de malloc se conoce desde hace <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">mucho tiempo</a> , as√≠ como su soluci√≥n: utilizar la variable de entorno <code>MALLOC_ARENA_MAX</code> para indicar expl√≠citamente el n√∫mero de arenas.  Es muy f√°cil de hacer para cualquier contenedor.  Aqu√≠ est√° el efecto de especificar <code>MALLOC_ARENA_MAX = 4</code> para nuestro backend principal: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jk/zq/lo/jkzqlo_pqiu4xppzbe-itkjvviy.png"></div><br><br>  Hay dos instancias en el gr√°fico RSS: en una (azul) <code>MALLOC_ARENA_MAX</code> , en la otra (rojo) simplemente reiniciamos.  La diferencia es obvia. <br><br>  Pero despu√©s de eso, existe un deseo razonable de descubrir en qu√© Java generalmente gasta memoria.  ¬øEs posible ejecutar un microservicio en Java con un l√≠mite de memoria de 300-400 megabytes y no tener miedo de que se caiga de Java-OOM o no sea asesinado por un asesino de OOM del sistema? <br><br><a name="oom"></a><h4>  Procesamos Java-OOM </h4><br>  En primer lugar, debe prepararse para el hecho de que los OOM son inevitables, y debe manejarlos correctamente, al menos para guardar volcados de cadera.  Por extra√±o que parezca, incluso esta simple empresa tiene sus propios matices.  Por ejemplo, los volcados de cadera no se sobrescriben: si un volcado de cadera con el mismo nombre ya est√° guardado, simplemente no se crear√° uno nuevo. <br><br>  Java puede <a href="">agregar autom√°ticamente el</a> n√∫mero de serie <a href="">del</a> volcado y la identificaci√≥n del proceso al nombre del archivo, pero esto no nos ayudar√°.  El n√∫mero de serie no es √∫til, porque esto es OOM, y no el volcado de cadera solicitado regularmente: la aplicaci√≥n se reinicia despu√©s de esto, restableciendo el contador.  Y la identificaci√≥n del proceso no es adecuada, ya que en Docker siempre es la misma (la mayor√≠a de las veces 1). <br><br>  Por lo tanto, llegamos a esta opci√≥n: <br><br> <code>-XX:+HeapDumpOnOutOfMemoryError <br> -XX:+ExitOnOutOfMemoryError <br> -XX:HeapDumpPath=/var/crash/java.hprof <br> -XX:OnOutOfMemoryError="mv /var/crash/java.hprof /var/crash/heapdump.hprof"</code> <br> <br>  Es bastante simple y con algunas mejoras, incluso puede ense√±ar a almacenarlo no solo el √∫ltimo volcado de cadera, sino que para nuestras necesidades es m√°s que suficiente. <br><br>  Java OOM no es lo √∫nico que tenemos que enfrentar.  Cada contenedor tiene un l√≠mite en la memoria que ocupa, y se puede superar.  Si esto sucede, el asesino de OOM del sistema mata el contenedor y se reinicia (usamos <code>restart_policy: always</code> ).  Naturalmente, esto no es deseable, y queremos aprender a establecer correctamente los l√≠mites de los recursos utilizados por la JVM. <br><br><a name="opt-mem"></a><h4>  Optimizando el consumo de memoria </h4><br>  Pero antes de establecer l√≠mites, debe asegurarse de que la JVM no est√© desperdiciando recursos.  Ya hemos logrado reducir el consumo de memoria mediante el uso de un l√≠mite en el n√∫mero de CPU y la variable <code>MALLOC_ARENA_MAX</code> .  ¬øHay alguna otra forma "casi gratuita" de hacer esto? <br><br>  Resulta que hay un par de trucos m√°s que ahorrar√°n un poco de memoria. <br><br>  El primero es el uso de la <code>-Xss</code> (o <code>-XX:ThreadStackSize</code> ), que controla el tama√±o de la pila de subprocesos.  El valor predeterminado para una JVM de 64 bits es 1 MB.  Descubrimos que 512 KB es suficiente para nosotros.  Debido a esto, nunca se ha detectado una StackOverflowException, pero admito que esto no es adecuado para todos.  Y el beneficio de esto es muy peque√±o. <br><br>  El segundo es el <code>-XX:+UseStringDeduplication</code> (con G1 GC habilitado).  Le permite ahorrar en memoria al colapsar filas duplicadas debido a la carga adicional del procesador.  La compensaci√≥n entre la memoria y la CPU depende solo de la aplicaci√≥n espec√≠fica y la configuraci√≥n del mecanismo de deduplicaci√≥n en s√≠.  Lea el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">dock</a> y pruebe en sus servicios, tenemos esta opci√≥n que a√∫n no ha encontrado su aplicaci√≥n. <br><br>  Y finalmente, un m√©todo que no es adecuado para todos (pero nos conviene) es usar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">jemalloc en</a> lugar del malloc nativo.  Esta implementaci√≥n est√° orientada a reducir la fragmentaci√≥n de la memoria y un mejor soporte de subprocesos m√∫ltiples en comparaci√≥n con malloc de glibc.  Para nuestros servicios, jemalloc proporcion√≥ un poco m√°s de ganancia de memoria que malloc con <code>MALLOC_ARENA_MAX=4</code> , sin afectar significativamente el rendimiento. <br><br>  Otras opciones, incluidas las descritas por Alexei Shipilev en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">JVM Anatomy Quark # 12: Native Memory Tracking</a> , parec√≠an bastante peligrosas o provocaron una degradaci√≥n notable en el rendimiento.  Sin embargo, con fines educativos, recomiendo leer este art√≠culo. <br><br>  Mientras tanto, pasemos al siguiente tema y, finalmente, intentemos aprender a limitar el consumo de memoria y seleccionar los l√≠mites correctos. <br><br><a name="lim-mem-1"></a><h4>  Limitar el consumo de memoria: memoria directa heap, no heap </h4><br>  Para hacer todo bien, debe recordar en qu√© consiste la memoria en general en Java.  Primero, veamos los grupos cuyo estado se puede monitorear a trav√©s de JMX. <br><br>  El primero, por supuesto, es <b>moderno</b> .  Es simple: <code>-Xmx</code> , pero ¬øc√≥mo hacerlo bien?  Desafortunadamente, no existe una receta universal aqu√≠, todo depende de la aplicaci√≥n y el perfil de carga.  Para los nuevos servicios, comenzamos con un tama√±o de almacenamiento din√°mico relativamente razonable (128 MB) y, si es necesario, aumentamos o disminuimos.  Para admitir los existentes, hay monitoreo con gr√°ficos de consumo de memoria y m√©tricas de GC. <br><br>  Al mismo tiempo que <code>-Xmx</code> establecemos <code>-Xms == -Xmx</code> .  No tenemos una sobreventa de memoria, por lo que nos interesa que el servicio utilice al m√°ximo los recursos que le dimos.  Adem√°s, en los servicios ordinarios incluimos <code>-XX:+AlwaysPreTouch</code> y el mecanismo Transparent Huge Pages: <code>-XX:+UseTransparentHugePages -XX:+UseLargePagesInMetaspace</code> .  Sin embargo, antes de habilitar THP, lea detenidamente la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentaci√≥n</a> y pruebe c√≥mo se comportan los servicios con esta opci√≥n durante mucho tiempo.  No se descartan sorpresas en m√°quinas con RAM insuficiente (por ejemplo, tuvimos que apagar el THP en los bancos de prueba). <br><br>  Lo siguiente es <b>no mont√≥n</b> .  La memoria sin almacenamiento din√°mico incluye: <br>  - Metaspace y Compressed Class Space, <br>  - C√≥digo de cach√©. <br><br>  Considere estas piscinas en orden. <br><br>  Por supuesto, todo el mundo ha escuchado sobre <b>Metaspace</b> , no hablar√© en detalle.  Almacena metadatos de clase, c√≥digo de bytes del m√©todo, etc.  De hecho, el uso de Metaspace depende directamente del n√∫mero y el tama√±o de las clases cargadas, y puede determinarlo, como hip, solo iniciando la aplicaci√≥n y eliminando las m√©tricas a trav√©s de JMX.  Por defecto, Metaspace no est√° limitado por nada, pero es bastante f√°cil hacerlo con la <code>-XX:MaxMetaspaceSize</code> . <br><br>  <b>Compressed Class Space</b> es parte de Metaspace y aparece cuando la <code>-XX:+UseCompressedClassPointers</code> est√° habilitada (habilitada de manera predeterminada para montones de menos de 32 GB, es decir, cuando puede proporcionar una ganancia de memoria real).  El tama√±o de este grupo puede estar limitado por la opci√≥n <code>-XX:CompressedClassSpaceSize</code> , pero no tiene mucho sentido, ya que Compressed Class Space est√° incluido en Metaspace y la cantidad total de memoria bloqueada para Metaspace y Compressed Class Space est√° en √∫ltima instancia limitada a una <code>-XX:MaxMetaspaceSize</code> . <br><br>  Por cierto, si observa las lecturas de JMX, la cantidad de memoria no heap siempre se calcula como la <a href="">suma de</a> Metaspace, Compressed Class Space y Code Cache.  De hecho, solo necesita resumir Metaspace y CodeCache. <br><br>  Por lo tanto, en el no almacenamiento din√°mico solo qued√≥ <b>Code Cache</b> , el repositorio de c√≥digo compilado por el compilador JIT.  De forma predeterminada, su tama√±o m√°ximo est√° establecido en 240 MB, y para servicios peque√±os es varias veces mayor de lo necesario.  El tama√±o de la cach√© de c√≥digo se puede establecer con la opci√≥n <code>-XX:ReservedCodeCacheSize</code> .  El tama√±o correcto solo se puede determinar ejecutando la aplicaci√≥n y sigui√©ndola bajo un perfil de carga t√≠pico. <br><br>  Es importante no cometer un error aqu√≠, ya que la Cach√© de c√≥digo insuficiente elimina el c√≥digo fr√≠o y antiguo del cach√© (la <code>-XX:+UseCodeCacheFlushing</code> habilitada de forma predeterminada), y esto, a su vez, puede conducir a un mayor consumo de CPU y una degradaci√≥n del rendimiento .  Ser√≠a genial si pudieras lanzar OOM cuando se desborde Code Cache, para esto incluso hay el <code>-XX:+ExitOnFullCodeCache</code> , pero, desafortunadamente, solo est√° disponible en la <a href="">versi√≥n de desarrollo de la</a> JVM. <br><br>  El √∫ltimo grupo sobre el que hay informaci√≥n en JMX es <b>la memoria directa</b> .  De forma predeterminada, su tama√±o no est√° limitado, por lo que es importante establecer alg√∫n tipo de l√≠mite para ello, al menos las bibliotecas como Netty, que utilizan activamente buffers de bytes directos, se guiar√°n por √©l.  No es dif√≠cil establecer un l√≠mite con el <code>-XX:MaxDirectMemorySize</code> y, nuevamente, solo el monitoreo nos ayudar√° a determinar el valor correcto. <br><br>  Entonces, ¬øqu√© llegamos hasta ahora? <br><br><pre>  Memoria de proceso Java = 
     Heap + Metaspace + Cach√© de c√≥digo + Memoria directa =
         -Xmx +
         -XX: MaxMetaspaceSize +
         -XX: ReservedCodeCacheSize +
         -XX: MaxDirectMemorySize </pre><br><br>  Intentemos dibujar todo en el gr√°fico y compararlo con el contenedor de acopladores RSS. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pg/ue/fx/pguefx_0kisoyxg8mna7dxlmimo.png"></div><br><br>  La l√≠nea de arriba es el RSS del contenedor y es una vez y media m√°s que el consumo de memoria de la JVM, que podemos monitorear a trav√©s de JMX. <br><br>  Cavando m√°s! <br><br><a name="lim-mem-2"></a><h4>  Limitaci√≥n del consumo de memoria: seguimiento de memoria nativa </h4><br>  Por supuesto, adem√°s de la memoria heap, no heap y directa, la JVM usa un mont√≥n de otras agrupaciones de memoria.  La bandera <code>-XX:NativeMemoryTracking=summary</code> nos ayudar√° a <code>-XX:NativeMemoryTracking=summary</code> con ellos <code>-XX:NativeMemoryTracking=summary</code> .  Al habilitar esta opci√≥n, podremos obtener informaci√≥n sobre grupos conocidos por JVM, pero no disponibles en JMX.  Puede leer m√°s sobre el uso de esta opci√≥n en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentaci√≥n</a> . <br><br>  Comencemos con lo m√°s obvio: la memoria ocupada por las <b>pilas de hilos</b> .  NMT produce algo como lo siguiente para nuestro servicio: <br><br><pre>  Hilo (reservado = 32166 KB, comprometido = 5358 KB)
     (hilo # 52)
     (pila: reservado = 31920 KB, comprometido = 5112 KB)
     (malloc = 185KB # 270) 
     (arena = 61KB # 102) </pre><br>  Por cierto, su tama√±o tambi√©n se puede encontrar sin Native Memory Tracking, usando jstack y cavando un poco en <code>/proc/&lt;pid&gt;/smaps</code> .  Andrey Pangin present√≥ una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">utilidad especial</a> para esto. <br><br>  El tama√±o del <b>espacio de clase compartido</b> es a√∫n m√°s f√°cil de evaluar: <br><br><pre>  Espacio de clase compartida (reservado = 17084 KB, comprometido = 17084 KB)
     (mmap: reservado = 17084 KB, comprometido = 17084 KB) </pre><br>  Este es el mecanismo de intercambio de datos de clase, <code>-Xshare</code> <code>-XX:+UseAppCDS</code> <code>-Xshare</code> y <code>-XX:+UseAppCDS</code> .  En Java 11, la opci√≥n <code>-Xshare</code> est√° configurada en auto de manera predeterminada, lo que significa que si tiene el <code>$JAVA_HOME/lib/server/classes.jsa</code> (est√° en la imagen oficial del acoplador OpenJDK), cargar√° el mapa de memoria- Ohm al inicio de la JVM, acelerando el tiempo de inicio.  En consecuencia, el tama√±o de Shared Class Space es f√°cil de determinar si conoce el tama√±o de los archivos jsa. <br><br>  Las siguientes son las estructuras nativas del <b>recolector de basura</b> : <br><br><pre>  GC (reservado = 42137 KB, comprometido = 41801 KB)
     (malloc = 5705KB # 9460) 
     (mmap: reservado = 36432 KB, comprometido = 36096 KB) </pre><br>  Alexey Shipilev en el manual ya mencionado sobre el seguimiento de memoria nativa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">dice</a> que ocupan alrededor del 4-5% del tama√±o del mont√≥n, pero en nuestra configuraci√≥n para el mont√≥n peque√±o (hasta varios cientos de megabytes) la sobrecarga alcanz√≥ el 50% del tama√±o del mont√≥n. <br><br>  Las <b>tablas de s√≠mbolos</b> pueden ocupar mucho espacio: <br><br><pre>  S√≠mbolo (reservado = 16421 KB, comprometido = 16421 KB)
     (malloc = 15261KB # 203089) 
     (arena = 1159KB # 1) </pre><br>  Almacenan los nombres de m√©todos, firmas, as√≠ como enlaces a cadenas internados.  Desafortunadamente, parece posible estimar el tama√±o de la tabla de s√≠mbolos solo despu√©s de factum usando Native Memory Tracking. <br><br>  Lo que queda  Seg√∫n Native Memory Tracking, muchas cosas: <br><br><pre>  Compilador (reservado = 509 KB, comprometido = 509 KB)
 Interno (reservado = 1647 KB, comprometido = 1647 KB)
 Otro (reservado = 2110 KB, comprometido = 2110 KB)
 Arena Chunk (reservado = 1712 KB, comprometido = 1712 KB)
 Registro (reservado = 6 KB, comprometido = 6 KB)
 Argumentos (reservado = 19 KB, comprometido = 19 KB)
 M√≥dulo (reservado = 227 KB, comprometido = 227 KB)
 Desconocido (reservado = 32 KB, comprometido = 32 KB) </pre><br>  Pero todo esto ocupa bastante espacio. <br><br>  Desafortunadamente, muchas de las √°reas de memoria mencionadas no pueden ser limitadas ni controladas, y si pudiera ser, la configuraci√≥n se convertir√≠a en un infierno.  Incluso monitorear su estado es una tarea no trivial, ya que la inclusi√≥n de Native Memory Tracking agota ligeramente el rendimiento de la aplicaci√≥n y no es una buena idea habilitarla para la producci√≥n en un servicio cr√≠tico. <br><br>  Sin embargo, por inter√©s, intentemos reflejar en el gr√°fico todo lo que informa Native Memory Tracking: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/t_/5d/kn/t_5dkncjh0wrn9qmftrp3wgwg2w.png"></div><br><br>  No esta mal!  La diferencia restante es una sobrecarga para la fragmentaci√≥n / asignaci√≥n de memoria (es muy peque√±a, ya que usamos jemalloc) o la memoria que asignaron las bibliotecas nativas.  Solo usamos uno de estos para el almacenamiento eficiente del √°rbol de prefijos. <br><br>  Entonces, para nuestras necesidades, es suficiente limitar lo que podemos: Heap, Metaspace, Code Cache, Direct Memory.  Para todo lo dem√°s, dejamos algunas bases razonables, determinadas por los resultados de mediciones pr√°cticas. <br><br>  Despu√©s de ocuparse de la CPU y la memoria, pasamos al siguiente recurso por el cual las aplicaciones pueden competir: los discos. <br><br><a name="disks"></a><h4>  Java y unidades </h4><br>  Y con ellos, todo es muy malo: son lentos y pueden provocar una opacidad tangible de la aplicaci√≥n.  Por lo tanto, desenlazamos Java de los discos tanto como sea posible: <br><br><ul><li>  Escribimos todos los registros de aplicaciones en el syslog local a trav√©s de UDP.  Esto deja alguna posibilidad de que los registros necesarios se pierdan en alg√∫n lugar del camino, pero, como lo ha demostrado la pr√°ctica, estos casos son muy raros. </li><li>  Escribiremos registros JVM en tmpfs, para esto solo necesitamos montar la ventana acoplable en la ubicaci√≥n deseada con el <code>/dev/shm</code> . </li></ul><br><br>  Si escribimos registros en syslog o en tmpfs, y la aplicaci√≥n en s√≠ misma no escribe nada en el disco, excepto los volcados de cadera, ¬øentonces resulta que la historia con discos puede considerarse cerrada al respecto? <br><br>  Por supuesto que no. <br><br>  Prestamos atenci√≥n al gr√°fico de la duraci√≥n de las pausas de detener el mundo y vemos una imagen triste: las pausas de Stop-The-World en los hosts son cientos de milisegundos, y en un host pueden alcanzar hasta un segundo: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tf/nd/pb/tfndpbg7mtpaylny7-cflvrukzg.png"></div><br><br>  ¬øNo hace falta decir que esto afecta negativamente a la aplicaci√≥n?  Aqu√≠, por ejemplo, hay un gr√°fico que refleja el tiempo de respuesta del servicio seg√∫n los clientes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nc/d0/_n/ncd0_ndoybiyy42wzrh68ppj-_0.png"></div><br><br>  Este es un servicio muy simple, en su mayor parte que da respuestas en cach√©, entonces, ¬øde d√≥nde provienen esos tiempos prohibitivos, comenzando con el percentil 95?  Otros servicios tienen una imagen similar, adem√°s, los tiempos de espera est√°n lloviendo con una constancia envidiable al tomar conexiones del grupo de conexiones a la base de datos, al ejecutar solicitudes, etc. <br><br>  ¬øQu√© tiene que ver el disco con √©l?  - usted pregunta  Resulta mucho que ver con eso. <br>  Un an√°lisis detallado del problema mostr√≥ que surgen largas pausas STW debido al hecho de que los hilos van al punto seguro durante mucho tiempo.  Despu√©s de leer el c√≥digo JVM, nos dimos cuenta de que durante la sincronizaci√≥n de subprocesos en el punto seguro, la JVM puede escribir el archivo <code>/tmp/hsperfdata*</code> trav√©s del mapa de memoria, al que exporta algunas estad√≠sticas.  Las utilidades como <code>jstat</code> y <code>jps</code> usan <code>jstat</code> <code>jps</code> . <br><br>  Deshabil√≠telo en la misma m√°quina con la opci√≥n <code>-XX:+PerfDisableSharedMem</code> y ... <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1r/aw/q7/1rawq7kjvmrjznko2781or7kzdm.png"></div><br><br>  Las m√©tricas del muelle de embarcadero se estabilizan: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qz/bs/pj/qzbspjvpdjfhjjtbwrn6et55wns.png"></div><br><br>         (,         ): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ni/ig/ji/niigjizzguoke8dcfdz2ssqnqa8.png"></div><br><br>  ,         ,  ,        . <br><br><a name="monitor"></a><h4>    ? </h4><br>     Java-  , ,  ,    . <br><br>         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Nuts and Bolts</a> ,          .              ,     .     ,      ,  JMX. <br><br>      ,          .          . <br><br>     statsd    JVM,    (heap,   non-heap   ): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/n9/iw/vs/n9iwvsjan7hxeo-xsksaggthrqy.png"></div><br><br>  ,    ,       . <br><br>    ‚Äî       ,    ,  ,    ,    ?        .     ()  -,     ,   RPS   . <br><br>     :   ,              .         .        ammo-  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">.</a> .    . : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qr/ry/wh/qrrywh-id3u5lbk7ldms8n-n_ck.png"></div><br><br>        . <br><br>               ,     .  ,      ,     - ,   ,   . <br><br><h4>  En conclusi√≥n </h4><br>   ,  Java  Docker ‚Äî    ,      .     . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/450954/">https://habr.com/ru/post/450954/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../450942/index.html">Sidecar para una divisi√≥n de c√≥digo</a></li>
<li><a href="../450946/index.html">Disco celular en LPC810</a></li>
<li><a href="../450948/index.html">MU-MIMO: uno de los algoritmos de implementaci√≥n</a></li>
<li><a href="../450950/index.html">Conceptos b√°sicos de Dart Streams</a></li>
<li><a href="../450952/index.html">√çndice medio y Antibank</a></li>
<li><a href="../450958/index.html">AnyStub, biblioteca de stub de conexi√≥n Java</a></li>
<li><a href="../450962/index.html">Bombas de insulina, microchips a prueba de manipulaciones y radio definida por software.</a></li>
<li><a href="../450964/index.html">Nueva biblioteca intr√≠nseca x86 SIMD - depuraci√≥n de immintrin</a></li>
<li><a href="../450966/index.html">Grabar video de una computadora vieja - m√©todos de LGR</a></li>
<li><a href="../450970/index.html">C√≥mo comparar realmente los precios de Apple en los Estados Unidos y Rusia. Experiencia personal</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>