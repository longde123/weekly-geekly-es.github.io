<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëåüèº üë∞üèª üòÇ Reconnaissance faciale √† l'aide de r√©seaux siamois üê≥ üôãüèª „Ä∞Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Le r√©seau neuronal siamois est l'un des algorithmes d'apprentissage simples les plus simples et les plus populaires. M√©thodes dans lesquelles pour cha...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Reconnaissance faciale √† l'aide de r√©seaux siamois</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/jetinfosystems/blog/465279/"><img src="https://habrastorage.org/webt/mg/dh/mb/mgdhmb0xf-4onn6dny1renuuhla.jpeg"><br><br>  Le r√©seau neuronal siamois est l'un des algorithmes d'apprentissage simples les plus simples et les plus populaires.  M√©thodes dans lesquelles pour chaque classe est prise une seule √©tude de cas.  Ainsi, le r√©seau siamois est g√©n√©ralement utilis√© dans des applications o√π il n'y a pas beaucoup d'unit√©s de donn√©es dans chaque classe. <br><br>  Supposons que nous devions cr√©er un mod√®le de reconnaissance faciale pour une organisation qui emploie environ 500 personnes.  Si vous cr√©ez un tel mod√®le √† partir de z√©ro √† partir du r√©seau neuronal convolutif (CNN), puis pour former le mod√®le et obtenir une bonne pr√©cision de reconnaissance, nous aurons besoin de nombreuses images de chacune de ces 500 personnes.  Mais il est √©vident que nous ne pouvons pas collecter un tel ensemble de donn√©es, vous ne devez donc pas cr√©er un mod√®le bas√© sur CNN ou tout autre algorithme d' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">apprentissage en profondeur</a> si nous ne disposons pas de suffisamment de donn√©es.  Dans de tels cas, vous pouvez utiliser l'algorithme d'apprentissage unique complexe, comme le r√©seau siamois, qui peut √™tre form√© sur moins de donn√©es. <br><a name="habracut"></a><br>  En fait, les r√©seaux siamois se composent de deux r√©seaux de neurones sym√©triques, avec les m√™mes poids et la m√™me architecture, qui √† la fin combinent et utilisent la fonction d'√©nergie - E. <br>  Regardons le r√©seau siamois, cr√©ant un mod√®le de reconnaissance faciale bas√© sur lui.  Nous lui apprendrons √† d√©terminer quand deux visages sont identiques ou non.  Et pour commencer, nous utiliserons l'ensemble de donn√©es AT&amp;T Database of Faces, qui peut √™tre t√©l√©charg√© sur le site Web du <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">laboratoire informatique de l'Universit√© de Cambridge</a> . <br><br>  T√©l√©chargez, d√©compressez et voyez les dossiers de s1 √† s40: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/152/0fc/158/1520fc1584e7ba6335ba2ea99460d8f6.png"><br><br>  Chaque dossier contient 10 photographies diff√©rentes d'une m√™me personne prises sous diff√©rents angles.  Voici le contenu du dossier s1: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5d4/02b/86f/5d402b86fc12e1b23512b1a54c7fea57.png"><br><br>  Et voici ce qui se trouve dans le dossier s13: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b7c/819/716/b7c8197164861f8568e86d32a2294bc2.png"><br><br>  Les r√©seaux siamois doivent entrer des valeurs appari√©es avec des marquages, alors cr√©ons de tels ensembles.  Prenez deux photos au hasard dans le m√™me dossier et marquez-les comme une paire ¬´authentique¬ª.  Ensuite, nous prenons deux photos de diff√©rents dossiers et les marquons comme une ¬´fausse¬ª paire (imposite): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5b3/1f3/8bf/5b31f38bf3363aaa53110772b7643e3b.png"><br><br>  Apr√®s avoir distribu√© toutes les photos en paires marqu√©es, nous √©tudierons le r√©seau.  De chaque paire, nous transf√©rerons une photo vers le r√©seau A et la seconde vers le r√©seau B. Les deux r√©seaux extraient uniquement des vecteurs de propri√©t√©.  Pour ce faire, nous utilisons deux couches convolutives avec activation de l'unit√© lin√©aire rectifi√©e (ReLU).  Apr√®s avoir √©tudi√© les propri√©t√©s, nous transf√©rons les vecteurs g√©n√©r√©s par les deux r√©seaux dans une fonction √©nerg√©tique qui estime la similitude.  Nous utilisons la distance euclidienne en fonction. <br><br>
<h2>  Examinez maintenant toutes ces √©tapes plus en d√©tail. </h2><br>  Tout d'abord, importez les biblioth√®ques n√©cessaires: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> PIL <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Image <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> backend <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> K <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Activation <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Input, Lambda, Dense, Dropout, Convolution2D, MaxPooling2D, Flatten <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential, Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.optimizers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RMSprop</code> </pre> <br>  Nous d√©finissons maintenant une fonction pour lire les images d'entr√©e.  La fonction <code>read_image</code> prend une photo et retourne un tableau NumPy: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">read_image</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename, byteorder=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'&gt;'</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#first we read the image, as a raw file to the buffer with open(filename, 'rb') as f: buffer = f.read() #using regex, we extract the header, width, height and maxval of the image header, width, height, maxval = re.search( b"(^P5\s(?:\s*#.*[\r\n])*" b"(\d+)\s(?:\s*#.*[\r\n])*" b"(\d+)\s(?:\s*#.*[\r\n])*" b"(\d+)\s(?:\s*#.*[\r\n]\s)*)", buffer).groups() #then we convert the image to numpy array using np.frombuffer which interprets buffer as one dimensional array return np.frombuffer(buffer, dtype='u1' if int(maxval)</span></span></code> </pre> <br>  Par exemple, ouvrez cette photo: <br><br><pre> <code class="python hljs">Image.open(<span class="hljs-string"><span class="hljs-string">"data/orl_faces/s1/1.pgm"</span></span>)</code> </pre> <br><img src="https://habrastorage.org/getpro/habr/post_images/ee2/954/697/ee2954697908ce55ffaf4fc8080914c7.png"><br><br>  Nous le passons √† la fonction <code>read_image</code> et obtenons un tableau NumPy: <br><br><pre> <code class="python hljs">img = read_image(<span class="hljs-string"><span class="hljs-string">'data/orl_faces/s1/1.pgm'</span></span>) img.shape (<span class="hljs-number"><span class="hljs-number">112</span></span>, <span class="hljs-number"><span class="hljs-number">92</span></span>)</code> </pre> <br>  Nous d√©finissons <code>get_data</code> fonction <code>get_data</code> qui g√©n√©rera les donn√©es.  Permettez-moi de vous rappeler que les r√©seaux siamois doivent soumettre des paires de donn√©es (authentiques et imposantes) avec marquage binaire. <br><br>  Tout d'abord, lisez les images ( <code>img1</code> , <code>img2</code> ) √† partir d'un r√©pertoire, enregistrez-les dans le tableau <code>x_genuine_pair,</code> d√©finissez <code>y_genuine</code> sur <code>1</code> .  Ensuite, nous lisons les images ( <code>img1</code> , <code>img2</code> ) √† partir de diff√©rents r√©pertoires, les enregistrons dans la paire <code>x_imposite,</code> et d√©finissons <code>y_imposite</code> √† <code>0</code> . <br><br>  Concat√©ner <code>x_genuine_pair</code> et <code>x_imposite</code> dans <code>X</code> , et <code>y_genuine</code> et <code>y_imposite</code> dans <code>Y</code> : <br><br><pre> <code class="python hljs">size = <span class="hljs-number"><span class="hljs-number">2</span></span> total_sample_size = <span class="hljs-number"><span class="hljs-number">10000</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(size, total_sample_size)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#read the image image = read_image('data/orl_faces/s' + str(1) + '/' + str(1) + '.pgm', 'rw+') #reduce the size image = image[::size, ::size] #get the new size dim1 = image.shape[0] dim2 = image.shape[1] count = 0 #initialize the numpy array with the shape of [total_sample, no_of_pairs, dim1, dim2] x_geuine_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2]) # 2 is for pairs y_genuine = np.zeros([total_sample_size, 1]) for i in range(40): for j in range(int(total_sample_size/40)): ind1 = 0 ind2 = 0 #read images from same directory (genuine pair) while ind1 == ind2: ind1 = np.random.randint(10) ind2 = np.random.randint(10) # read the two images img1 = read_image('data/orl_faces/s' + str(i+1) + '/' + str(ind1 + 1) + '.pgm', 'rw+') img2 = read_image('data/orl_faces/s' + str(i+1) + '/' + str(ind2 + 1) + '.pgm', 'rw+') #reduce the size img1 = img1[::size, ::size] img2 = img2[::size, ::size] #store the images to the initialized numpy array x_geuine_pair[count, 0, 0, :, :] = img1 x_geuine_pair[count, 1, 0, :, :] = img2 #as we are drawing images from the same directory we assign label as 1. (genuine pair) y_genuine[count] = 1 count += 1 count = 0 x_imposite_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2]) y_imposite = np.zeros([total_sample_size, 1]) for i in range(int(total_sample_size/10)): for j in range(10): #read images from different directory (imposite pair) while True: ind1 = np.random.randint(40) ind2 = np.random.randint(40) if ind1 != ind2: break img1 = read_image('data/orl_faces/s' + str(ind1+1) + '/' + str(j + 1) + '.pgm', 'rw+') img2 = read_image('data/orl_faces/s' + str(ind2+1) + '/' + str(j + 1) + '.pgm', 'rw+') img1 = img1[::size, ::size] img2 = img2[::size, ::size] x_imposite_pair[count, 0, 0, :, :] = img1 x_imposite_pair[count, 1, 0, :, :] = img2 #as we are drawing images from the different directory we assign label as 0. (imposite pair) y_imposite[count] = 0 count += 1 #now, concatenate, genuine pairs and imposite pair to get the whole data X = np.concatenate([x_geuine_pair, x_imposite_pair], axis=0)/255 Y = np.concatenate([y_genuine, y_imposite], axis=0) return X, Y</span></span></code> </pre> <br>  Nous allons maintenant g√©n√©rer les donn√©es et v√©rifier leur taille.  Nous avons 20 000 photos, dont 10 000 authentiques et 10 000 fausses paires ont √©t√© collect√©es: <br><br><pre> <code class="python hljs">X, Y = get_data(size, total_sample_size) X.shape (<span class="hljs-number"><span class="hljs-number">20000</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">56</span></span>, <span class="hljs-number"><span class="hljs-number">46</span></span>) Y.shape (<span class="hljs-number"><span class="hljs-number">20000</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br>  Nous partagerons l'ensemble des informations: 75% des paires iront √† la formation et 25% aux tests: <br> <code>x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.25)</code> <br> <br>  Cr√©ez maintenant un r√©seau siamois.  Nous d√©finissons d'abord le r√©seau central - ce sera un r√©seau neuronal convolutif pour extraire les propri√©t√©s.  Cr√©ez deux couches convolutives √† l'aide des activations ReLU et une couche avec un regroupement maximal apr√®s une couche plate: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">build_base_network</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(input_shape)</span></span></span><span class="hljs-function">:</span></span> seq = Sequential() nb_filter = [<span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>] kernel_size = <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-comment"><span class="hljs-comment">#convolutional layer 1 seq.add(Convolution2D(nb_filter[0], kernel_size, kernel_size, input_shape=input_shape, border_mode='valid', dim_ordering='th')) seq.add(Activation('relu')) seq.add(MaxPooling2D(pool_size=(2, 2))) seq.add(Dropout(.25)) #convolutional layer 2 seq.add(Convolution2D(nb_filter[1], kernel_size, kernel_size, border_mode='valid', dim_ordering='th')) seq.add(Activation('relu')) seq.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='th')) seq.add(Dropout(.25)) #flatten seq.add(Flatten()) seq.add(Dense(128, activation='relu')) seq.add(Dropout(0.1)) seq.add(Dense(50, activation='relu')) return seq</span></span></code> </pre> <br><br>  Ensuite, nous transf√©rerons une paire d'images du r√©seau central, qui renverra des repr√©sentations vectorielles, c'est-√†-dire des vecteurs de propri√©t√©: <br><br><pre> <code class="python hljs">input_dim = x_train.shape[<span class="hljs-number"><span class="hljs-number">2</span></span>:] img_a = Input(shape=input_dim) img_b = Input(shape=input_dim) base_network = build_base_network(input_dim) feat_vecs_a = base_network(img_a) feat_vecs_b = base_network(img_b)</code> </pre><br>  <code>feat_vecs_a</code> et <code>feat_vecs_b</code> sont des vecteurs de propri√©t√© d'une paire d'images.  Passons leurs fonctions √©nerg√©tiques pour calculer la distance entre elles.  Et en fonction de l'√©nergie, nous utilisons la distance euclidienne: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">euclidean_distance</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(vects)</span></span></span><span class="hljs-function">:</span></span> x, y = vects <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> K.sqrt(K.sum(K.square(x - y), axis=<span class="hljs-number"><span class="hljs-number">1</span></span>, keepdims=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">eucl_dist_output_shape</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(shapes)</span></span></span><span class="hljs-function">:</span></span> shape1, shape2 = shapes <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (shape1[<span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>) distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([feat_vecs_a, feat_vecs_b])</code> </pre> <br>  Nous d√©finissons le nombre d'√©poques √† 13, appliquons la propri√©t√© RMS pour l'optimisation et d√©clarons le mod√®le: <br><br><pre> <code class="python hljs">epochs = <span class="hljs-number"><span class="hljs-number">13</span></span> rms = RMSprop() model = Model(input=[input_a, input_b], output=distance)</code> </pre> <br>  Maintenant, nous d√©finissons la fonction de perte fonction <code>contrastive_loss</code> et compilons le mod√®le: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">contrastive_loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(y_true, y_pred)</span></span></span><span class="hljs-function">:</span></span> margin = <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> K.mean(y_true * K.square(y_pred) + (<span class="hljs-number"><span class="hljs-number">1</span></span> - y_true) * K.square(K.maximum(margin - y_pred, <span class="hljs-number"><span class="hljs-number">0</span></span>))) model.compile(loss=contrastive_loss, optimizer=rms)</code> </pre> <br>  √âtudions le mod√®le: <br><br><pre> <code class="python hljs">img_1 = x_train[:, <span class="hljs-number"><span class="hljs-number">0</span></span>] img_2 = x_train[:, <span class="hljs-number"><span class="hljs-number">1</span></span>] model.fit([img_1, img_2], y_train, validation_split=<span class="hljs-number"><span class="hljs-number">.25</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">128</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">2</span></span>, nb_epoch=epochs)</code> </pre> <br>  Vous voyez comment les pertes diminuent au fil des √©poques: <br><br><pre> <code class="python hljs">Train on <span class="hljs-number"><span class="hljs-number">11250</span></span> samples, validate on <span class="hljs-number"><span class="hljs-number">3750</span></span> samples Epoch <span class="hljs-number"><span class="hljs-number">1</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">60</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.2179</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.2156</span></span> Epoch <span class="hljs-number"><span class="hljs-number">2</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">53</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.1520</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.2102</span></span> Epoch <span class="hljs-number"><span class="hljs-number">3</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">53</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.1190</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.1545</span></span> Epoch <span class="hljs-number"><span class="hljs-number">4</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">55</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0959</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.1705</span></span> Epoch <span class="hljs-number"><span class="hljs-number">5</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0801</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.1181</span></span> Epoch <span class="hljs-number"><span class="hljs-number">6</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0684</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0821</span></span> Epoch <span class="hljs-number"><span class="hljs-number">7</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0591</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0762</span></span> Epoch <span class="hljs-number"><span class="hljs-number">8</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0526</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0655</span></span> Epoch <span class="hljs-number"><span class="hljs-number">9</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0475</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0662</span></span> Epoch <span class="hljs-number"><span class="hljs-number">10</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0444</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0469</span></span> Epoch <span class="hljs-number"><span class="hljs-number">11</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0408</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0478</span></span> Epoch <span class="hljs-number"><span class="hljs-number">12</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0381</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0498</span></span> Epoch <span class="hljs-number"><span class="hljs-number">13</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">54</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0356</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0363</span></span></code> </pre> <br>  Et maintenant, testons le mod√®le sur des donn√©es de test: <br><br><pre> <code class="python hljs">pred = model.predict([x_test[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], x_test[:, <span class="hljs-number"><span class="hljs-number">1</span></span>]])</code> </pre> <br>  D√©finissez une fonction pour calculer la pr√©cision: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">compute_accuracy</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(predictions, labels)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> labels[predictions.ravel()</code> </pre> <br>  Nous calculons la pr√©cision: <br><br><pre> <code class="plaintext hljs">compute_accuracy(pred, y_test) 0.9779092702169625</code> </pre><br><h2>  Conclusions </h2><br>  Dans ce guide, nous avons appris √† cr√©er des mod√®les de reconnaissance faciale bas√©s sur des r√©seaux siamois.  L'architecture de ces r√©seaux se compose de deux r√©seaux de neurones identiques ayant le m√™me poids et la m√™me structure, et les r√©sultats de leur travail sont transf√©r√©s vers une seule fonction √©nerg√©tique - cela d√©termine l'identit√© des donn√©es d'entr√©e.  Pour plus d'informations sur le m√©ta-apprentissage avec <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Python,</a> voir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">M√©ta-apprentissage pratique avec Python.</a> <br><br><h2>  Mon commentaire </h2><br>  La connaissance des r√©seaux siamois est actuellement requise pour travailler avec des images.  Il existe de nombreuses approches pour la formation des r√©seaux dans de petits √©chantillons, la g√©n√©ration de nouvelles donn√©es, les m√©thodes d'augmentation.  Cette m√©thode permet d'obtenir des r√©sultats relativement ¬´bon march√©¬ª, voici un exemple plus classique du r√©seau siamois sur ¬´Hello world¬ª pour les r√©seaux de neurones - jeu de donn√©es MNIST <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">keras.io/examples/mnist_siamese</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr465279/">https://habr.com/ru/post/fr465279/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr465269/index.html">Formation Cisco 200-125 CCNA v3.0. Jour 26. DNS et DHCP</a></li>
<li><a href="../fr465271/index.html">Les pirates informatiques volent et blanchissent de l'argent gr√¢ce aux services de livraison de nourriture et de r√©servation d'h√¥tel.</a></li>
<li><a href="../fr465273/index.html">Comment les d√©veloppeurs de logiciels Microgaming prot√®gent les utilisateurs contre les hacks</a></li>
<li><a href="../fr465275/index.html">Alice obtient des comp√©tences</a></li>
<li><a href="../fr465277/index.html">Analyser et analyser la s√©mantique pour le r√©f√©rencement: 5 mod√®les gratuits de Google Sheets</a></li>
<li><a href="../fr465281/index.html">Surveillance continue du glucose (NMH) avec la pompe Medtronic 640g</a></li>
<li><a href="../fr465283/index.html">¬´Il y a tout ce dont on a besoin, et rien ne rend furieux¬ª - la v√©rit√© parle √† travers les l√®vres du client</a></li>
<li><a href="../fr465285/index.html">Comme nous l'avons √©crit, l'interface de notre propre panneau de contr√¥le d'h√©bergement: framework et backdoors</a></li>
<li><a href="../fr465289/index.html">Le condens√© des √©v√©nements pour les professionnels des RH dans le domaine des TI pour septembre 2019</a></li>
<li><a href="../fr465291/index.html">Plus pr√®s du sol: comment j'ai chang√© le coworking en maison de village</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>