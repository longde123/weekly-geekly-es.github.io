<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòÅ üë©üèª üß• Automatisez le remplacement de disque avec Ansible üëàüèº üì≤ üë©üèæ‚Äçü§ù‚Äçüë®üèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour √† tous. Je travaille en tant qu'administrateur syst√®me de premier plan dans OK et je suis responsable du fonctionnement stable du portail. Je ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Automatisez le remplacement de disque avec Ansible</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/odnoklassniki/blog/452110/"><img src="https://habrastorage.org/webt/s9/ga/q9/s9gaq9eke8gooeu1-nnsr5ocv7o.jpeg"><br><br>  Bonjour √† tous.  Je travaille en tant qu'administrateur syst√®me de premier plan dans OK et je suis responsable du fonctionnement stable du portail.  Je veux parler de la fa√ßon dont nous avons construit le processus de remplacement automatique des disques, puis, en tant qu'administrateur, il a √©t√© exclu de ce processus et l'a remplac√© par un bot. <br><br>  Cet article est une sorte de translitt√©ration des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">performances</a> de HighLoad + 2018 <br><a name="habracut"></a><br><h2>  Construire un processus de remplacement de disque </h2><br><h3>  Quelques chiffres d'abord </h3><br>  OK est un gigantesque service utilis√© par des millions de personnes.  Il est desservi par environ 7 000 serveurs, qui sont situ√©s dans 4 centres de donn√©es diff√©rents.  Les serveurs co√ªtent plus de 70 000 disques.  Si vous les empilez les uns sur les autres, vous obtenez une tour d'une hauteur de plus de 1 km. <br><br>  Les disques durs sont un composant du serveur qui plante le plus souvent.  Avec de tels volumes, nous devons changer environ 30 disques par semaine, et cette proc√©dure est devenue une routine peu agr√©able. <br><br><img src="https://habrastorage.org/webt/i8/92/cg/i892cglklhooat_z_qrsexsmzfu.png"><br><br><h3>  Incidents </h3><br>  Nous avons introduit une gestion des incidents √† part enti√®re dans notre entreprise.  Chaque incident que nous enregistrons √† Jira, puis nous le r√©solvons et le d√©montons.  Si l'incident a eu un effet pour les utilisateurs, nous allons certainement r√©fl√©chir √† la mani√®re de r√©agir plus rapidement dans de tels cas, √† r√©duire l'effet et bien s√ªr √† √©viter une r√©cidive. <br><br>  Les lecteurs ne font pas exception.  Leur statut est surveill√© par Zabbix.  Nous surveillons les messages dans Syslog pour les erreurs d'√©criture / lecture, analysons l'√©tat des raids HW / SW, surveillons SMART et calculons l'usure des SSD. <br><br><h3>  Comment les disques ont chang√© avant </h3><br>  Lorsqu'un d√©clencheur s'allume dans Zabbix, un incident est cr√©√© dans Jira et est automatiquement transmis aux ing√©nieurs appropri√©s dans les centres de donn√©es.  Nous le faisons avec tous les incidents mat√©riels, c'est-√†-dire ceux qui n√©cessitent une sorte de travail physique avec l'√©quipement du centre de donn√©es. <br>  Un ing√©nieur d'un centre de donn√©es est une personne qui r√©sout les probl√®mes li√©s au mat√©riel, est responsable de l'installation, de la maintenance, du d√©montage des serveurs.  Ayant re√ßu un ticket, l'ing√©nieur commence √† travailler.  Dans les √©tag√®res √† disques, il change les disques par lui-m√™me.  Mais s'il n'a pas acc√®s √† l'appareil souhait√©, l'ing√©nieur se tourne vers les administrateurs syst√®me de garde pour obtenir de l'aide.  Tout d'abord, vous devez retirer le disque de la rotation.  Pour ce faire, vous devez apporter les modifications n√©cessaires sur le serveur, arr√™ter l'application, d√©monter le disque. <br><br>  L'administrateur syst√®me en service pendant le quart de travail est responsable du fonctionnement de l'ensemble du portail.  Il enqu√™te sur les incidents, les r√©parations, aide les d√©veloppeurs √† effectuer de petites t√¢ches.  Il ne s'occupe pas seulement des disques durs. <br><br>  Auparavant, les ing√©nieurs du centre de donn√©es discutaient avec l'administrateur syst√®me.  Les ing√©nieurs ont envoy√© des liens vers des tickets Jira, l'administrateur les a parcourus, a gard√© un journal de travail dans un bloc-notes.  Mais les chats ne sont pas pratiques pour de telles t√¢ches: les informations ne sont pas structur√©es et sont rapidement perdues.  Et l'administrateur pouvait simplement s'√©loigner de l'ordinateur et pendant un certain temps ne pas r√©pondre aux demandes, et l'ing√©nieur se tenait devant le serveur avec un tas de disques et attendait. <br><br>  Mais le pire, c'est que les administrateurs n'ont pas vu la situation dans son ensemble: quels incidents de disque existent, o√π le probl√®me pourrait potentiellement survenir.  Cela est d√ª au fait que nous confions tous les incidents mat√©riels aux ing√©nieurs.  Oui, il √©tait possible d'afficher tous les incidents sur le tableau de bord d'administration.  Mais il y en a beaucoup, et l'administrateur n'√©tait impliqu√© que dans certains d'entre eux. <br><br>  De plus, l'ing√©nieur n'a pas pu √©tablir correctement les priorit√©s, car il ne sait rien de l'objectif de serveurs sp√©cifiques, de la distribution des informations sur les disques. <br><br><h3>  Nouvelle proc√©dure de remplacement </h3><br>  La premi√®re chose que nous avons faite a √©t√© de prendre tous les incidents de disque dans un type distinct de ¬´disque HW¬ª et d'y ajouter les champs ¬´nom de p√©riph√©rique de bloc¬ª, ¬´taille¬ª et ¬´type de disque¬ª afin que ces informations soient enregistr√©es dans le ticket et n'aient pas √† bavarder constamment. <br><br><div style="text-align:center;"><img width="300" height="400" src="https://habrastorage.org/webt/0y/uz/jm/0yuzjmgoz4hgulcpf5o5vhwde18.png"></div><br>  Nous avons √©galement convenu que dans le cadre d'un incident, nous ne changerons qu'un seul disque.  Cela a grandement simplifi√© le processus d'automatisation, la collecte de statistiques et le travail. <br><br>  De plus, le champ ¬´administrateur responsable¬ª a √©t√© ajout√©.  L'administrateur syst√®me y est automatiquement remplac√©.  C'est tr√®s pratique, car maintenant l'ing√©nieur voit toujours qui est responsable.  Pas besoin d'aller au calendrier et de chercher.  C'est ce champ qui a permis de mettre des tickets sur le tableau de bord de l'administrateur, dans lequel, peut-√™tre, son aide serait n√©cessaire. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9x/b1/ue/9xb1ueokh0450wgkhq0ije1lfqa.png"></div><br>  Pour s'assurer que tous les participants b√©n√©ficient au maximum des innovations, nous avons cr√©√© des filtres et des tableaux de bord, en ont parl√© aux gars.  Lorsque les gens comprennent les changements, ils ne se distancient pas d'eux comme de quelque chose d'inutile.  Il est important pour un ing√©nieur de conna√Ætre le num√©ro de rack o√π se trouve le serveur, la taille et le type de disque.  L'administrateur doit tout d'abord comprendre de quel type de groupe de serveurs il s'agit, de quel type d'effet il peut s'agir lors du remplacement d'un disque. <br><br>  La pr√©sence de champs et leur affichage est pratique, mais cela ne nous a pas √©pargn√© la n√©cessit√© d'utiliser des chats.  Pour ce faire, j'ai d√ª modifier le workflow. <br><br>  C'√©tait comme √ßa: <br><br><div style="text-align:center;"><img width="300" height="400" src="https://habrastorage.org/webt/we/wc/du/wewcdu4q8fqy-gd9wkbutorbabi.png"></div><br>  Aujourd'hui, les ing√©nieurs continuent de travailler comme √ßa quand ils n'ont pas besoin d'aide de l'administrateur. <br><br>  La premi√®re chose que nous avons faite a √©t√© d'introduire un nouveau statut d' <b>enqu√™te</b> .  Le ticket est dans ce statut lorsque l'ing√©nieur n'a pas encore d√©cid√© s'il aura besoin d'un administrateur ou non.  Gr√¢ce √† ce statut, l'ing√©nieur peut transmettre le ticket √† l'administrateur.  De plus, nous marquons les tickets avec ce statut lorsqu'un remplacement de disque est requis, mais il n'y a pas de disque lui-m√™me sur le site.  Cela se produit avec les CDN et les sites distants. <br><br>  Nous avons √©galement ajout√© le statut <b>Pr√™t</b> .  Le ticket lui est transf√©r√© apr√®s avoir remplac√© le disque.  Autrement dit, tout a d√©j√† √©t√© fait, mais HW / SW RAID est synchronis√© sur le serveur.  Cela peut prendre beaucoup de temps. <br><br>  Si un administrateur est impliqu√©, le sch√©ma est un peu plus compliqu√©. <br><br><div style="text-align:center;"><img width="400" src="https://habrastorage.org/webt/ev/5a/58/ev5a58jyosbyuc8cfrhr7sru5zo.png"></div><br>  √Ä partir du statut <b>Ouvert</b> , un ticket peut √™tre transf√©r√© √† la fois par un administrateur syst√®me et un ing√©nieur.  Dans l'√©tat <b>En cours</b> , l'administrateur supprime le disque de la rotation afin que l'ing√©nieur puisse simplement le supprimer: il allume le r√©tro√©clairage, d√©monte le disque et arr√™te les applications, selon le groupe de serveurs sp√©cifique. <br><br>  Ensuite, le ticket est converti en <b>pr√™t √† changer</b> : c'est un signal √† l'ing√©nieur que le disque peut √™tre retir√©.  Tous les champs de Jira sont d√©j√† remplis, l'ing√©nieur sait quel type et quelle taille du disque.  Ces donn√©es sont appos√©es automatiquement sur l'√©tat pr√©c√©dent ou par l'administrateur. <br><br>  Apr√®s avoir remplac√© le disque, le ticket est transf√©r√© √† l'√©tat <b>Chang√©</b> .  Il est v√©rifi√© que le bon disque a √©t√© ins√©r√©, le balisage est effectu√©, l'application est lanc√©e et certaines t√¢ches de r√©cup√©ration de donn√©es sont effectu√©es.  En outre, le ticket peut √™tre transf√©r√© √† l'√©tat <b>Pr√™t</b> , auquel cas l'administrateur restera responsable, car il a d√©marr√© le disque en rotation.  Le contour complet ressemble √† ceci. <br><br><div style="text-align:center;"><img width="500" src="https://habrastorage.org/webt/xg/x7/_z/xgx7_zk2mlyubhzepgfw0oogbt4.png"></div><br>  L'ajout de nouveaux champs a rendu notre vie beaucoup plus facile.  Les gars ont commenc√© √† travailler avec des informations structur√©es, il est devenu clair quoi et √† quelle √©tape faire.  Les priorit√©s sont devenues beaucoup plus pertinentes car elles sont d√©sormais d√©finies par l'administrateur. <br><br>  Le besoin de chats a disparu.  Bien s√ªr, l'administrateur peut √©crire √† l'ing√©nieur "vous devez remplacer plus vite ici" ou "d√©j√† le soir, aurez-vous le temps de remplacer?".  Mais nous ne discutons plus quotidiennement sur ces questions. <br><br>  Les disques ont commenc√© √† changer en packs.  Si l'administrateur est venu travailler un peu plus t√¥t, il a du temps libre et rien ne s'est pass√©, il peut pr√©parer un certain nombre de serveurs pour le remplacement: d√©poser des champs, retirer les disques de la rotation et transf√©rer la t√¢che √† l'ing√©nieur.  Un ing√©nieur arrive plus tard au centre de donn√©es, voit la t√¢che, prend les disques n√©cessaires de l'entrep√¥t et les change imm√©diatement.  En cons√©quence, la vitesse de remplacement a augment√©. <br><br><h3>  Le√ßons apprises dans la cr√©ation d'un flux de travail </h3><br><ul><li>  <b>Lors de la cr√©ation d'une proc√©dure, vous devez collecter des informations aupr√®s de diff√©rentes sources.</b> <br>  Certains de nos administrateurs ne savaient pas que l'ing√©nieur avait chang√© les disques par eux-m√™mes.  Certains pensaient que les ing√©nieurs surveillaient la synchronisation MD RAID, bien que certains d'entre eux n'y aient m√™me pas acc√®s.  Certains ing√©nieurs de premier plan l'ont fait, mais pas toujours, car le processus n'√©tait d√©crit nulle part. </li><li>  <b>La proc√©dure doit √™tre simple et directe.</b> <br>  Il est difficile pour une personne de garder plusieurs √©tapes dans sa t√™te.  Les statuts voisins les plus importants de Jira doivent √™tre affich√©s sur l'√©cran principal.  Vous pouvez les renommer, par exemple, En cours, nous appelons Pr√™t √† changer.  Et les statuts restants peuvent √™tre cach√©s dans le menu d√©roulant afin qu'ils ne callent pas les yeux.  Mais il vaut mieux ne pas limiter les gens, donner la possibilit√© de faire la transition. <br>  Expliquez la valeur de l'innovation.  Lorsque les gens comprennent, ils feraient mieux d'accepter la nouvelle proc√©dure.  Il √©tait tr√®s important pour nous que les gens n'appellent pas tout le processus, mais le suivent.  Ensuite, nous avons construit sur cette automatisation. </li><li>  <b>Attendez, analysez, comprenez.</b> <br>  Il nous a fallu environ un mois pour √©laborer la proc√©dure, la mise en ≈ìuvre technique, les r√©unions et les discussions.  Et pour la mise en ≈ìuvre - plus de trois mois.  J'ai vu comment les gens commencent lentement √† utiliser l'innovation.  Au d√©but, il y avait beaucoup de n√©gativit√©.  Mais il √©tait compl√®tement ind√©pendant de la proc√©dure elle-m√™me, de sa mise en ≈ìuvre technique.  Par exemple, un administrateur n'a pas utilis√© Jira, mais le plugin Jira dans Confluence, et certaines choses n'√©taient pas disponibles pour lui.  Lui a montr√© Jira, l'administrateur a augment√© la productivit√© et les t√¢ches globales, ainsi que le remplacement des disques. </li></ul><br><h2>  Automatisation du remplacement des disques </h2><br>  Nous sommes pass√©s √† l'automatisation du remplacement des disques √† plusieurs reprises.  Nous avions d√©j√† du temps de fonctionnement, des scripts, mais tous fonctionnaient de mani√®re interactive ou en mode manuel, ils n√©cessitaient un lancement.  Et seulement apr√®s l'introduction de la nouvelle proc√©dure, nous avons r√©alis√© que c'√©tait juste que nous manquions. <br><br>  Depuis maintenant, le processus de remplacement est divis√© en √©tapes, chacune ayant un ex√©cuteur et une liste d'actions, nous pouvons activer l'automatisation par √©tapes, et pas toutes en m√™me temps.  Par exemple, l'√©tape la plus simple - Pr√™t (v√©rification de la synchronisation RAID / donn√©es) peut √™tre facilement d√©l√©gu√©e au bot.  Lorsque le bot apprend un peu, vous pouvez lui confier une t√¢che plus responsable - mettre le disque en rotation, etc. <br><br><h3>  Configurations du zoo </h3><br>  Avant de parler du bot, nous ferons une courte excursion dans notre zoo d'installation.  Tout d'abord, cela est d√ª √† la taille gigantesque de notre infrastructure.  Deuxi√®mement, pour chaque service, nous essayons de choisir la configuration optimale du fer.  Nous avons environ 20 mod√®les RAID mat√©riels, principalement LSI et Adaptec, mais il existe √† la fois HP et DELL de versions diff√©rentes.  Chaque contr√¥leur RAID poss√®de son propre utilitaire de gestion.  L'ensemble des commandes et leur √©mission peuvent diff√©rer d'une version √† l'autre de chaque contr√¥leur RAID.  Si HW-RAID n'est pas utilis√©, il peut avoir peur. <br><br>  Presque toutes les nouvelles installations que nous faisons sans sauvegarde sur disque.  Nous essayons de ne plus utiliser de RAID mat√©riel et logiciel, car nous r√©servons nos syst√®mes au niveau des centres de donn√©es, pas des serveurs.  Mais bien s√ªr, de nombreux serveurs h√©rit√©s doivent √™tre pris en charge. <br><br>  Quelque part, les disques des contr√¥leurs RAID lancent des p√©riph√©riques bruts; quelque part, ils utilisent JBOD.  Il existe des configurations avec un disque syst√®me sur le serveur, et si vous devez le remplacer, vous devez reformater le serveur avec l'installation du syst√®me d'exploitation et des applications, avec les m√™mes versions, puis ajouter des fichiers de configuration, lancer des applications.  Il existe √©galement de nombreux groupes de serveurs o√π la redondance n'est pas effectu√©e au niveau du sous-syst√®me de disque, mais directement dans les applications elles-m√™mes. <br><br>  Au total, nous avons plus de 400 groupes de serveurs uniques qui ex√©cutent environ 100 applications diff√©rentes.  Pour couvrir un si grand nombre d'options, nous avions besoin d'un outil d'automatisation multifonctionnel.  Il est conseill√© avec une simple DSL, afin que non seulement la personne qui l'a √©crit puisse la supporter. <br><br>  Nous avons choisi Ansible car il est sans agent: pas besoin de pr√©parer l'infrastructure, d√©marrage rapide.  De plus, il est √©crit en Python, qui est accept√© comme standard dans l'√©quipe. <br><br><h3>  Sch√©ma g√©n√©ral </h3><br>  Examinons un sch√©ma d'automatisation g√©n√©ral utilisant un incident comme exemple.  Zabbix d√©tecte que le lecteur sdb est hors service, le d√©clencheur s'allume, un ticket est cr√©√© dans Jira.  L'administrateur l'a regard√©, s'est rendu compte qu'il ne s'agissait pas d'un doublon et non d'un faux positif, c'est-√†-dire que vous devez changer le disque et traduire le ticket en cours. <br><br><img src="https://habrastorage.org/webt/9s/fy/q5/9sfyq5cucem0dbbhahfd3_w7tac.png"><br>  L'application DiskoBot √©crite en Python interroge p√©riodiquement Jira pour de nouveaux tickets.  Il remarque qu'un nouveau ticket En cours est apparu, le thread correspondant est d√©clench√©, ce qui lance le playbook dans Ansible (cela se fait pour chaque statut dans Jira).  Dans ce cas, Prepare2change d√©marre. <br><br>  Ansible se rend sur l'h√¥te, supprime le disque de la rotation et signale l'√©tat √† l'application via des rappels. <br><br><img src="https://habrastorage.org/webt/i2/bk/nq/i2bknqits8exw9dtr7sj4zydl58.png"><br>  Selon les r√©sultats, le bot transf√®re automatiquement le ticket √† Ready to change.  L'ing√©nieur re√ßoit une notification et va changer le disque, apr√®s quoi il transf√®re le ticket √† Changed. <br><br><img src="https://habrastorage.org/webt/h9/m8/zw/h9m8zwfrzltmu0q1tz8opctrccu.png"><br>  Selon le sch√©ma ci-dessus, le ticket revient au bot, il lance un autre playbook, va √† l'h√¥te et entre le disque en rotation.  Le bot ferme le ticket.  Hourra! <br><br><img src="https://habrastorage.org/webt/cv/js/w1/cvjsw1y9qrpkra2twsx-sy4jokc.png"><br>  Parlons maintenant de certains des composants du syst√®me. <br><br><h3>  Diskobot </h3><br>  Cette application est √©crite en Python.  Il s√©lectionne les billets de Jira selon <abbr title="JIRA Query Language">JQL</abbr> .  Selon le statut du ticket, ce dernier parvient au gestionnaire correspondant, qui √† son tour lance le statut du playbook Ansible correspondant. <br><br>  JQL et les intervalles d'interrogation sont d√©finis dans le fichier de configuration de l'application. <br><br><pre><code class="plaintext hljs">jira_states: investigate: jql: '‚Ä¶ status = Open and "Disk Size" is EMPTY' interval: 180 inprogress: jql: '‚Ä¶ and "Disk Size" is not EMPTY and "Device Name" is not EMPTY' ready: jql: '‚Ä¶ and (labels not in ("dbot_ignore") or labels is EMPTY)' interval: 7200</code> </pre> <br>  Par exemple, parmi les tickets dont l'√©tat est En cours, seuls ceux avec les champs Taille du disque et Nom du p√©riph√©rique sont remplis.  Le nom du p√©riph√©rique est le nom du p√©riph√©rique de bloc n√©cessaire pour ex√©cuter le playbook.  La taille du disque est n√©cessaire pour que l'ing√©nieur sache quelle taille de disque est n√©cessaire. <br><br>  Et parmi les tickets avec le statut Pr√™t, les tickets avec le label dbot_ignore sont filtr√©s.  Soit dit en passant, nous utilisons les √©tiquettes Jira √† la fois pour ce filtrage, pour marquer les tickets en double et collecter des statistiques. <br><br>  Si le playbook plante, Jira attribue le label dbot_failed afin que vous puissiez le d√©couvrir plus tard. <br><br><h3>  Interaction avec Ansible </h3><br>  L'application interagit avec Ansible via l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">API Ansible Python</a> .  Dans playbook_executor, nous transmettons le nom de fichier et l'ensemble de variables.  Cela vous permet de conserver le projet Ansible sous la forme de fichiers yml standard, plut√¥t que de le d√©crire en code Python. <br><br>  √âgalement dans Ansible via * extra_vars *, le nom du p√©riph√©rique de blocage, l'√©tat du ticket, ainsi que callback_url, dans lequel la cl√© de probl√®me est cousue, est utilis√© - il est utilis√© pour le rappel en HTTP. <br><br>  Pour chaque lancement, un inventaire temporaire est g√©n√©r√©, compos√© d'un h√¥te et du groupe auquel appartient cet h√¥te afin que group_vars soit appliqu√©. <br><br>  Voici un exemple de t√¢che dans laquelle le rappel HTTP est impl√©ment√©. <br><br>  Le r√©sultat des playbooks que nous obtenons en utilisant callaback (s).  Ils sont de deux types: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Plugin de rappel ansible</a> , il fournit des donn√©es sur les r√©sultats d'un playbook.  Il d√©crit les t√¢ches qui ont √©t√© lanc√©es, ex√©cut√©es avec succ√®s ou sans succ√®s.  Ce rappel est appel√© √† la fin du playbook. </li><li>  Rappel HTTP pour obtenir des informations lors de la lecture d'un playbook.  Dans Ansible, nous effectuons une requ√™te POST / GET √† c√¥t√© de notre application. </li></ul><br>  Via le (s) callback (s) HTTP, les variables qui ont √©t√© d√©finies lors de l'ex√©cution du playbook et que nous voulons sauvegarder et utiliser dans les runs suivants sont transmises.  Nous √©crivons ces donn√©es dans sqlite. <br><br>  De plus, via le rappel HTTP, nous laissons des commentaires et modifions le statut du ticket. <br><br><div class="spoiler">  <b class="spoiler_title">Rappel HTTP</b> <div class="spoiler_text"><pre> <code class="plaintext hljs"># Make callback to Diskobot App # Variables: # callback_post_body: # A dict with follow keys. All keys are optional # msg: If exist it would be posted to Jira as comment # data: If exist it would be saved in Incident.variables # desire_state: Set desire_state for incident # status: If exist Proceed issue to that status - name: Callback to Diskobot app (jira comment/status) uri: url: "{{ callback_url }}/{{ devname }}" user: "{{ diskobot_user }}" password: "{{ diskobot_pass }}" force_basic_auth: True method: POST body: "{{ callback_post_body | to_json }}" body_format: json delegate_to: 127.0.0.1</code> </pre><br></div></div><br>  Comme beaucoup de t√¢ches du m√™me type, nous les mettons dans un fichier commun s√©par√© et les incluons si n√©cessaire, afin de ne pas r√©p√©ter constamment dans les playbooks.  L'URL de rappel_appara√Æt ici, dans laquelle la cl√© du probl√®me et le nom d'h√¥te sont prot√©g√©s.  Quand Ansible ex√©cute cette requ√™te POST, le bot se rend compte qu'elle est venue dans le cadre d'un tel incident. <br><br>  Et voici un exemple d'un playbook, dans lequel nous avons affich√© un disque √† partir d'un p√©riph√©rique MD: <br><br><pre> <code class="plaintext hljs"> # Save mdadm configuration - include: common/callback.yml vars: callback_post_body: status: 'Ready to change' msg: "Removed disk from mdraid {{ mdadm_remove_disk.msg | comment_jira }}" data: mdadm_data: "{{ mdadm_remove_disk.removed }}" parted_info: "{{ parted_info | default() }}" when: - mdadm_remove_disk | changed - mdadm_remove_disk.removed</code> </pre><br>  Cette t√¢che place le ticket Jira dans l'√©tat ¬´Pr√™t √† changer¬ª et ajoute un commentaire.  De plus, la variable mdam_data stocke la liste des p√©riph√©riques md dont le disque a √©t√© supprim√© et le vidage parted_ de la partition partitionn√©e dans parted_info. <br><br>  Lorsque l'ing√©nieur ins√®re un nouveau disque, nous pourrons utiliser ces variables pour restaurer le vidage de partition, ainsi que pour ins√©rer le disque dans les p√©riph√©riques md dont il a √©t√© supprim√©. <br><br><h3>  Mode de v√©rification impossible </h3><br>  Activer l'automatisation √©tait effrayant.  Par cons√©quent, nous avons d√©cid√© d'ex√©cuter tous les playbooks en mode <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ex√©cution √† sec</a> , dans laquelle Ansible n'ex√©cute aucune action sur les serveurs, mais les √©mule uniquement. <br><br>  Un tel lancement est ex√©cut√© via un module de rappel s√©par√© et le r√©sultat du playbook est enregistr√© dans Jira en tant que commentaire. <br><br><img src="https://habrastorage.org/webt/aa/rz/0s/aarz0srsrqqxbru88guob9spjqm.png"><br><br>  Tout d'abord, il a permis de valider le travail du bot et des playbooks.  Deuxi√®mement, cela a accru la confiance des administrateurs dans le bot. <br><br>  Lorsque nous avons effectu√© la validation et r√©alis√© que vous pouvez ex√©cuter Ansible non seulement en mode de fonctionnement √† sec, nous avons cr√©√© le bouton Ex√©cuter Diskobot dans Jira pour d√©marrer le m√™me playbook avec les m√™mes variables sur le m√™me h√¥te, mais en mode normal. <br><br>  De plus, le bouton permet de red√©marrer le playbook en cas de panne. <br><br><h3>  Structure des Playbooks </h3><br>  J'ai d√©j√† mentionn√© qu'en fonction du statut du ticket Jira, le bot lance diff√©rents playbooks. <br><br>  Premi√®rement, il est tellement plus facile d'organiser l'entr√©e. <br>  Deuxi√®mement, dans certains cas, c'est tout simplement n√©cessaire. <br><br>  Par exemple, lorsque vous remplacez un disque syst√®me, vous devez d'abord vous rendre sur le syst√®me de d√©ploiement, cr√©er une t√¢che, et apr√®s le d√©ploiement correct, le serveur sera accessible via ssh et vous pourrez y faire rouler l'application.  Si nous faisions tout cela dans un playbook, Ansible ne serait pas en mesure de l'ex√©cuter en raison de l'inaccessibilit√© de l'h√¥te. <br><br>  Nous utilisons des r√¥les Ansible pour chaque groupe de serveurs.  Ici, vous pouvez voir comment les playbooks sont organis√©s dans l'un d'eux. <br><br><img src="https://habrastorage.org/webt/ef/0o/hu/ef0ohuo0obwa1htij7o4at6dvus.png"><br><br>  C'est pratique, car il est imm√©diatement clair o√π se trouvent les t√¢ches.  Dans main.yml, qui est l'entr√©e pour le r√¥le Ansible, nous pouvons simplement inclure par statut de ticket ou t√¢ches g√©n√©rales n√©cessaires pour tout le monde, par exemple, passer une identification ou recevoir un jeton. <br><br><h4>  Investigation.yml </h4><br>  Fonctionne pour les tickets ayant le statut Enqu√™te et Ouvert.  La chose la plus importante pour ce playbook est le nom du p√©riph√©rique de bloc.  Ces informations ne sont pas toujours disponibles. <br><br>  Pour l'obtenir, nous analysons le r√©sum√© Jira, la derni√®re valeur du d√©clencheur Zabbix.  Il peut contenir le nom du p√©riph√©rique de bloc - chanceux.  Ou il peut contenir un point de montage, - alors vous devez vous rendre sur le serveur, analyser et calculer le lecteur souhait√©.  De plus, un d√©clencheur peut transmettre une adresse scsi ou d'autres informations.  Mais il arrive aussi qu'il n'y ait aucun indice et que vous deviez analyser. <br><br>  Apr√®s avoir d√©couvert le nom du p√©riph√©rique de bloc, nous collectons des informations sur le type et la taille du disque √† remplir dans les champs de Jira.  Nous supprimons √©galement les informations sur le fournisseur, le mod√®le, le micrologiciel, l'ID, SMART et ins√©rons tout cela dans un commentaire dans le ticket Jira.  L'administrateur et l'ing√©nieur n'ont plus besoin de rechercher ces donn√©es.  :) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/m6/64/kr/m664krcgyi4vkc-rq0almmqv6uy.png"></div><br><br><h4>  prepare2change.yml </h4><br>  La sortie du disque de rotation, pr√©paration pour le remplacement.  L'√©tape la plus difficile et cruciale.  C'est l√† que vous pouvez arr√™ter l'application lorsqu'elle ne peut pas √™tre arr√™t√©e.  Ou retirez un disque qui ne disposait pas de suffisamment de r√©pliques, et donc avoir un effet sur les utilisateurs, perdez certaines donn√©es.  Ici, nous avons le plus de contr√¥les et de notifications dans le chat. <br><br>  Dans le cas le plus simple, nous parlons de la suppression d'un disque dur HW / MD RAID. <br><br>  Dans des situations plus complexes (dans nos syst√®mes de stockage), lorsque la sauvegarde est effectu√©e au niveau de l'application, vous devez acc√©der √† l'application √† l'aide de l'API, signaler la sortie du disque, la d√©sactiver et d√©marrer la r√©cup√©ration. <br><br>  Nous migrons maintenant massivement vers le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cloud</a> , et si le serveur est cloud, alors Diskobot acc√®de √† l'API cloud, dit qu'il va fonctionner avec ce s√©ide - le serveur sur lequel les conteneurs s'ex√©cutent - et demande ¬´migrer tous les conteneurs de ce s√©ide¬ª.  Et en m√™me temps, il allume le r√©tro-√©clairage pour que l'ing√©nieur voit imm√©diatement lequel retirer. <br><br><h4>  chang√©.yml </h4><br>  Apr√®s avoir remplac√© un disque, nous v√©rifions d'abord sa disponibilit√©. <br><br>  Les ing√©nieurs ne mettent pas toujours de nouveaux disques, nous avons donc ajout√© une v√©rification des valeurs SMART qui nous satisfont. <br><br><div class="spoiler">  <b class="spoiler_title">Quels attributs examinons-nous</b> <div class="spoiler_text">  Nombre de secteurs r√©affect√©s (5) &lt;100 <br>  Nombre de secteurs en attente (107) == 0 <br></div></div><br>  Si le lecteur √©choue au test, l'ing√©nieur est inform√© d'un remplacement.  Si tout est en ordre, le r√©tro-√©clairage s'√©teint, le balisage est appliqu√© et le disque est ins√©r√© en rotation. <br><br><h4>  ready.yml </h4><br>  Le cas le plus simple: v√©rifier la synchronisation du raid HW / SW ou terminer la synchronisation des donn√©es dans l'application. <br><br><h3>  API d'application </h3><br>  J'ai mentionn√© √† plusieurs reprises que le bot acc√®de souvent aux API d'application.  Bien s√ªr, toutes les applications n'avaient pas les m√©thodes n√©cessaires, j'ai donc d√ª les affiner.  Voici les m√©thodes les plus importantes que nous utilisons: <br><ul><li>  Statut  Le statut d'un cluster ou d'un disque pour comprendre s'il est possible de travailler avec lui; <br></li><li>  D√©marrer / arr√™ter.  Activation-d√©sactivation du disque; <br></li><li>  Migrer / restaurer.  Migration et r√©cup√©ration de donn√©es pendant et apr√®s le remplacement. <br></li></ul><br><h3>  Le√ßons apprises par Ansible </h3><br>  J'adore vraiment Ansible.  Mais souvent, quand je regarde diff√©rents projets open source et que je vois comment les gens √©crivent des playbooks, j'ai un peu peur.  Tissage logique complexe √† partir de quand / boucle, manque de flexibilit√© et d'idempotence en raison de l'utilisation fr√©quente de shell / commande. <br><br>  Nous avons d√©cid√© de tout simplifier au maximum, en profitant de la modularit√© d'Ansible.  Au plus haut niveau se trouvent les playbooks, ils peuvent √™tre √©crits par n'importe quel administrateur, un d√©veloppeur tiers qui conna√Æt un peu Ansible. <br><br><pre> <code class="plaintext hljs">- name: Blink disk become: True register: locate_action disk_locate: locate: '{{ locate }}' devname: '{{ devname }}' ids: '{{ locate_ids | default(pd_id) | default(omit) }}'</code> </pre><br><br>  Si une logique est difficile √† impl√©menter dans les playbooks, nous la pla√ßons dans un module ou un filtre Ansible.  Les scripts peuvent √™tre √©crits √† la fois en Python et dans n'importe quelle autre langue. <br><br>  Ils sont faciles et rapides √† √©crire.  Par exemple, le module de mise en √©vidence du disque, dont un exemple d'utilisation est donn√© ci-dessus, se compose de 265 lignes. <br><br><img width="400" height="400" src="https://habrastorage.org/webt/c4/ma/bp/c4mabpsyezbjbfp2likixvta24k.png"><br><br>  Au niveau le plus bas se trouve la biblioth√®que.  Pour ce projet, nous avons √©crit une application distincte, une sorte d'abstraction sur les RAID mat√©riels et logiciels qui ex√©cutent les requ√™tes correspondantes. <br><br><img width="400" height="400" src="https://habrastorage.org/webt/qf/vt/pr/qfvtprdi9jw2vxserynmxbrmdg8.png"><br><br>  Les plus grandes forces d'Ansible sont sa simplicit√© et ses playbooks compr√©hensibles.  Je crois que vous devez utiliser cela et ne pas g√©n√©rer de fichiers Yaml effrayants et un grand nombre de conditions, de code shell et de boucles. <br><br>  Si vous souhaitez r√©p√©ter notre exp√©rience avec l'API Ansible, gardez √† l'esprit deux choses: <br><br><ul><li>  Playbook_executor et g√©n√©ralement le playbook ne peuvent pas √™tre d√©pass√©s.  Il y a un timeout sur la session ssh, mais il n'y a pas de timeout sur le playbook.  Si nous essayons de d√©monter un lecteur qui n'existe pas d√©j√† dans le syst√®me, le playbook fonctionnera ind√©finiment, nous avons donc d√ª le mettre dans un emballage s√©par√© et le tuer par timeout. <br></li><li>  Ansible est fork√©, donc son API n'est pas thread-safe.  Nous lan√ßons tous nos playbooks et monofils. <br></li></ul><br>  En cons√©quence, nous avons pu automatiser le remplacement d'environ 80% des disques.  En g√©n√©ral, le taux de remplacement a doubl√©.  Aujourd'hui, l'administrateur ne regarde que l'incident et d√©cide de changer le disque ou non, puis fait un clic. <br><br>  Mais maintenant, nous commen√ßons √† faire face √† un autre probl√®me: certains nouveaux administrateurs ne savent pas comment changer de lecteur.  :) </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr452110/">https://habr.com/ru/post/fr452110/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr452094/index.html">.NET: outils pour travailler avec le multithreading et l'asynchronie. Partie 1</a></li>
<li><a href="../fr452098/index.html">Journaux du d√©veloppeur frontal Habr: refactor et reflex</a></li>
<li><a href="../fr452102/index.html">Jeu photo pour ceux qui aiment les drones: bref sur AirSelfie 2</a></li>
<li><a href="../fr452106/index.html">Nous invitons les conf√©renciers √† la r√©union de bricolage d'√©t√© du 16 juin 2019</a></li>
<li><a href="../fr452108/index.html">Docker: des conseils inoffensifs</a></li>
<li><a href="../fr452112/index.html">CRM ++</a></li>
<li><a href="../fr452114/index.html">HolyJS 2019: d√©briefing de SEMrush (Partie 1)</a></li>
<li><a href="../fr452116/index.html">Index dans PostgreSQL - 8 (RUM)</a></li>
<li><a href="../fr452118/index.html">Un scientifique rompt le code du myst√©rieux manuscrit de Voynich</a></li>
<li><a href="../fr452122/index.html">"Pilule du d√©mon" en mouvement</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>