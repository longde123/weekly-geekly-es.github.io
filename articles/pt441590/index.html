<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§üüèº üëáüèº üëßüèæ A evolu√ß√£o da intera√ß√£o de cluster. Como implementamos o ActiveMQ e o Hazelcast üë©üèΩ‚Äçüåæ üöõ üñêüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nos √∫ltimos 7 anos, juntamente com a equipe, tenho apoiado e desenvolvido o n√∫cleo do produto Miro (ex-RealtimeBoard): intera√ß√£o cliente-servidor e cl...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>A evolu√ß√£o da intera√ß√£o de cluster. Como implementamos o ActiveMQ e o Hazelcast</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/miro/blog/441590/">  Nos √∫ltimos 7 anos, juntamente com a equipe, tenho apoiado e desenvolvido o n√∫cleo do produto Miro (ex-RealtimeBoard): intera√ß√£o cliente-servidor e cluster, trabalhando com o banco de dados. <br><br>  Temos Java com diferentes bibliotecas a bordo.  Tudo √© lan√ßado fora do cont√™iner, atrav√©s do plugin Maven.  Baseia-se na plataforma de nossos parceiros, que nos permite trabalhar com o banco de dados e fluxos, gerenciar a intera√ß√£o cliente-servidor, etc.  DB - Redis e PostgreSQL (meu colega <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">escreveu sobre como passamos de um banco de dados para outro</a> ). <br><br>  Em termos de l√≥gica de neg√≥cios, o aplicativo cont√©m: <br><br><ul><li>  trabalhar com quadros personalizados e seu conte√∫do; </li><li>  funcionalidade para registro de usu√°rios, cria√ß√£o e gerenciamento de placas; </li><li> gerador de recursos personalizados.  Por exemplo, ele otimiza imagens grandes carregadas no aplicativo para que elas n√£o diminuam a velocidade em nossos clientes; </li><li>  muitas integra√ß√µes com servi√ßos de terceiros. </li></ul><br>  Em 2011, quando est√°vamos come√ßando, todo o Miro estava no mesmo servidor.  Tudo estava nele: Nginx, no qual o php de um site estava ativado, um aplicativo Java e bancos de dados. <br><br>  O produto desenvolvido, o n√∫mero de usu√°rios e o conte√∫do que eles adicionaram √†s placas aumentaram; portanto, a carga no servidor tamb√©m aumentou.  Devido ao grande n√∫mero de aplicativos em nosso servidor, naquele momento n√£o conseguimos entender o que exatamente carrega a carga e, portanto, n√£o conseguimos otimiz√°-la.Para corrigir isso, dividimos tudo em diferentes servidores e obtivemos um servidor web, um servidor com nosso aplicativo e servidor de banco de dados. <br><br>  Infelizmente, ap√≥s algum tempo, os problemas voltaram a surgir, √† medida que a carga no aplicativo continuava aumentando.  Depois, pensamos em como dimensionar a infraestrutura. <br><br><img src="https://habrastorage.org/webt/_5/zq/_3/_5zq_3c16pydjklapiqamfzyxcg.png"><br><br>  A seguir, falarei sobre as dificuldades que encontramos no desenvolvimento de clusters e no dimensionamento de aplicativos e infraestrutura Java. <a name="habracut"></a><br><br><h2>  Dimensionar a infraestrutura horizontalmente </h2><br>  Come√ßamos coletando m√©tricas: o uso de mem√≥ria e CPU, o tempo necess√°rio para executar consultas do usu√°rio, o uso de recursos do sistema e o trabalho com o banco de dados.  A partir das m√©tricas, ficou claro que a gera√ß√£o de recursos do usu√°rio era um processo imprevis√≠vel.  Podemos carregar o processador 100% e esperar dezenas de segundos at√© que tudo esteja pronto.  √Äs vezes, solicita√ß√µes de usu√°rios por placas tamb√©m forneciam uma carga inesperada.  Por exemplo, quando um usu√°rio seleciona mil widgets e come√ßa a mov√™-los espontaneamente. <br><br>  Come√ßamos a pensar em como dimensionar essas partes do sistema e chegamos a solu√ß√µes √≥bvias. <br><br>  <b>Dimensione o trabalho com quadros e conte√∫do</b> .  O usu√°rio abre o quadro da seguinte maneira: o usu√°rio abre o cliente ‚Üí indica qual painel ele deseja abrir ‚Üí se conecta ao servidor ‚Üí um fluxo √© criado no servidor ‚Üí todos os usu√°rios deste quadro se conectam a um fluxo ‚Üí qualquer altera√ß√£o ou cria√ß√£o do widget ocorre nesse fluxo.  Acontece que todo o trabalho com a placa √© estritamente limitado pelo fluxo, o que significa que podemos distribuir esses fluxos entre os servidores. <br><br>  <b>Escale a gera√ß√£o de recursos do usu√°rio</b> .  Podemos retirar o servidor para gerar recursos separadamente, e ele receber√° mensagens para gera√ß√£o e, em seguida, responde que tudo √© gerado. <br><br>  Tudo parece ser simples.  Mas assim que come√ßamos a estudar esse t√≥pico mais profundamente, descobrimos que precis√°vamos resolver adicionalmente alguns problemas indiretos.  Por exemplo, se os usu√°rios expirarem uma assinatura paga, devemos notific√°-los sobre isso, independentemente do quadro em que estejam.  Ou, se o usu√°rio atualizou a vers√£o do recurso, √© necess√°rio garantir que o cache esteja corretamente liberado em todos os servidores e que forne√ßamos a vers√£o correta. <br><br>  Identificamos os requisitos do sistema.  O pr√≥ximo passo √© entender como colocar isso em pr√°tica.  De fato, precis√°vamos de um sistema que permitisse que os servidores do cluster se comuniquem entre si e com base no qual realizaremos todas as nossas id√©ias. <br><br><h2>  O primeiro cluster pronto para uso </h2><br>  N√£o selecionamos a primeira vers√£o do sistema, porque ela j√° estava parcialmente implementada na plataforma de parceiros que usamos.  Nele, todos os servidores estavam conectados um ao outro via TCP e, usando essa conex√£o, poder√≠amos enviar mensagens RPC para um ou todos os servidores ao mesmo tempo. <br><br>  Por exemplo, temos tr√™s servidores, eles est√£o conectados um ao outro via TCP, e no Redis temos uma lista desses servidores.  Iniciamos um novo servidor no cluster ‚Üí ele se adiciona √† lista em Redis ‚Üí l√™ a lista para descobrir todos os servidores no cluster ‚Üí se conecta a todos. <br><br><img src="https://habrastorage.org/webt/yj/9c/hv/yj9chvfavcbrixqnn_12jho2k7k.png"><br><br>  Com base no RPC, o suporte para liberar o cache e redirecionar os usu√°rios para o servidor desejado j√° foi implementado.  Tivemos que gerar uma gera√ß√£o de recursos do usu√°rio e notificar os usu√°rios de que algo havia acontecido (por exemplo, uma conta expirou).  Para gerar recursos, escolhemos um servidor arbitr√°rio e enviamos a ele um pedido de gera√ß√£o e, para notifica√ß√µes sobre a expira√ß√£o de uma assinatura, enviamos um comando a todos os servidores na esperan√ßa de que a mensagem atinja a meta. <br><br><h3>  O pr√≥prio servidor determina para quem enviar a mensagem. </h3><br>  Parece um recurso, n√£o um problema.  Mas o servidor se concentra apenas na conex√£o com outro servidor.  Se houver conex√µes, haver√° um candidato para enviar uma mensagem. <br><br>  O problema √© que o servidor n√∫mero 1 n√£o sabe que o servidor n√∫mero 4 est√° sob carga alta no momento e n√£o pode respond√™-lo com rapidez suficiente.  Como resultado, as solicita√ß√µes do servidor n¬∫ 1 s√£o processadas mais lentamente do que podiam. <br><br><img src="https://habrastorage.org/webt/g7/mw/ez/g7mwezzba78vsgcvx8mpa_fzdou.png"><br><br><h3>  O servidor n√£o sabe que o segundo servidor est√° congelado </h3><br>  Mas e se o servidor n√£o estiver apenas muito carregado, mas geralmente congela?  Al√©m disso, ele trava para que n√£o volte mais √† vida.  Por exemplo, eu acabei com toda a mem√≥ria dispon√≠vel. <br><br>  Nesse caso, o servidor n¬∫ 1 n√£o sabe qual √© o problema e continua aguardando uma resposta.  Os servidores restantes no cluster tamb√©m n√£o sabem sobre a situa√ß√£o com o servidor n√∫mero 4, portanto, eles enviar√£o muitas mensagens para o servidor n√∫mero 4 e aguardar√£o uma resposta.  Portanto, ser√° at√© que o servidor n√∫mero 4 morra. <br><br><img src="https://habrastorage.org/webt/5y/et/pg/5yetpgodx1zi38he2nchwnnniiq.png"><br><br>  O que fazer  Podemos adicionar independentemente uma verifica√ß√£o de status do servidor ao sistema.  Ou podemos redirecionar mensagens de servidores "doentes" para servidores "saud√°veis".  Tudo isso levar√° muito tempo para os desenvolvedores.  Em 2012, como t√≠nhamos pouca experi√™ncia nessa √°rea, come√ßamos a procurar solu√ß√µes prontas para todos os nossos problemas de uma s√≥ vez. <br><br><h2>  Agente de mensagens.  Activemq </h2><br>  Decidimos ir na dire√ß√£o do Message broker para configurar corretamente a comunica√ß√£o entre os servidores.  Eles escolheram o ActiveMQ devido √† capacidade de configurar o recebimento de mensagens no consumidor em um determinado momento.  √â verdade que nunca aproveitamos essa oportunidade para poder escolher o RabbitMQ, por exemplo. <br><br>  Como resultado, transferimos todo o sistema de cluster para o ActiveMQ.  O que deu: <br><br><ol><li>  O servidor n√£o determina mais para quem a mensagem √© enviada, porque todas as mensagens passam pela fila. </li><li>  Toler√¢ncia de falha configurada.  Para ler a fila, voc√™ pode executar n√£o um, mas v√°rios servidores.  Mesmo se um deles cair, o sistema continuar√° funcionando. </li><li>  Os servidores apareceram fun√ß√µes, o que permitiu dividir o servidor por tipo de carga.  Por exemplo, um gerador de recursos pode se conectar apenas a uma fila para ler mensagens para gerar recursos, e um servidor com placas pode se conectar a uma fila para abrir placas. </li><li>  A comunica√ß√£o RPC, ou seja,  cada servidor tem sua pr√≥pria fila privada, onde outros servidores enviam eventos para ele. </li><li>  Voc√™ pode enviar mensagens para todos os servidores atrav√©s do T√≥pico, que usamos para redefinir as assinaturas. </li></ol><br><br>  O esquema parece simples: todos os servidores est√£o conectados ao broker e gerencia a comunica√ß√£o entre eles.  Tudo funciona, as mensagens s√£o enviadas e recebidas, os recursos s√£o criados.  Mas h√° novos problemas. <br><br><h3>  O que fazer quando todos os servidores necess√°rios estiverem mentindo? </h3><br>  Digamos que o servidor n¬∫ 3 queira enviar uma mensagem para gerar recursos em uma fila.  Ele espera que sua mensagem seja processada.  Mas ele n√£o sabe que, por algum motivo, n√£o h√° um √∫nico destinat√°rio da mensagem.  Por exemplo, os destinat√°rios falharam devido a um erro. <br><br>  Durante todo o tempo de espera, o servidor envia muitas mensagens com uma solicita√ß√£o, e √© por isso que uma fila de mensagens aparece.  Portanto, quando os servidores em funcionamento aparecem, eles s√£o for√ßados a processar primeiro a fila acumulada, o que leva tempo.  No lado do usu√°rio, isso leva ao fato de que a imagem carregada por ele n√£o aparece imediatamente.  Ele n√£o est√° pronto para esperar, ent√£o deixa o conselho. <br><br>  Como resultado, gastamos a capacidade do servidor na gera√ß√£o de recursos e ningu√©m precisa do resultado. <br><br><img src="https://habrastorage.org/webt/oq/p8/fd/oqp8fd0mdctqarlauh84jdjn8rc.png"><br><br>  Como posso resolver o problema?  Podemos configurar o monitoramento, que notificar√° voc√™ sobre o que est√° acontecendo.  Mas a partir do momento em que o monitoramento relata algo, at√© o momento em que entendemos que nossos servidores est√£o ruins, o tempo passa.  Isso n√£o nos conv√©m. <br><br>  Outra op√ß√£o √© executar o Service Discovery, ou um registro de servi√ßos que saber√° quais servidores com quais fun√ß√µes est√£o em execu√ß√£o.  Nesse caso, receberemos imediatamente uma mensagem de erro se n√£o houver servidores livres. <br><br><h3>  Alguns servi√ßos n√£o podem ser dimensionados horizontalmente </h3><br>  Este √© um problema do nosso c√≥digo inicial, n√£o do ActiveMQ.  Deixe-me mostrar um exemplo: <br><br><pre><code class="plaintext hljs">Permission ownerPermission = service.getOwnerPermission(board); Permission permission = service.getPermission(board,user); ownerPermission.setRole(EDITOR); permission.setRole(OWNER);</code> </pre> <br>  Temos um servi√ßo para trabalhar com direitos de usu√°rio no quadro: o usu√°rio pode ser o propriet√°rio do quadro ou seu editor.  S√≥ pode haver um propriet√°rio no quadro.  Suponha que tenhamos um cen√°rio em que queremos transferir a propriedade de uma placa de um usu√°rio para outro.  Na primeira linha, obtemos o atual propriet√°rio do quadro, na segunda - pegamos o usu√°rio que era o editor e agora se torna o propriet√°rio.  Al√©m disso, o propriet√°rio atual colocamos o papel de EDITOR e o ex-editor - o papel de OWNER. <br><br>  Vamos ver como isso funcionar√° em um ambiente multithread.  Quando o primeiro thread estabelece a fun√ß√£o EDITOR e o segundo thread tenta assumir o PROPRIET√ÅRIO atual, pode acontecer que OWNER n√£o exista, mas existem dois EDITORES. <br><br>  O motivo √© a falta de sincroniza√ß√£o.  Podemos resolver o problema adicionando um bloco de sincroniza√ß√£o no quadro. <br><br><pre> <code class="plaintext hljs">synchronized (board) { Permission ownerPermission = service.getOwnerPermission(board); Permission permission = service.getPermission(board,user); ownerPermission.setRole(EDITOR); permission.setRole(OWNER); }</code> </pre><br>  Esta solu√ß√£o n√£o funcionar√° no cluster.  O banco de dados SQL pode nos ajudar com isso, com a ajuda de transa√ß√µes.  Mas n√≥s temos Redis. <br><br>  Outra solu√ß√£o √© adicionar bloqueios distribu√≠dos ao cluster para que a sincroniza√ß√£o fique dentro de todo o cluster, e n√£o apenas em um servidor. <br><br><h2>  Um √∫nico ponto de falha ao entrar no quadro </h2><br>  O modelo de intera√ß√£o entre o cliente e o servidor √© est√°vel.  Portanto, devemos armazenar o estado do quadro no servidor.  Portanto, criamos uma fun√ß√£o separada para servidores - BoardServer, que lida com solicita√ß√µes de usu√°rios relacionadas a placas. <br><br>  Imagine que temos tr√™s BoardServer, um dos quais √© o principal.  O usu√°rio envia a ele uma solicita√ß√£o "Abra-me o quadro com id = 123" ‚Üí o servidor verifica em seu banco de dados se o quadro est√° aberto e em qual servidor est√°.  Neste exemplo, o quadro est√° aberto. <br><br><img src="https://habrastorage.org/webt/ej/kf/sd/ejkfsdptym30e-gdvkycc225zpw.png"><br><br>  O servidor principal responde que voc√™ precisa se conectar ao servidor n¬∫ 1 ‚Üí o usu√°rio est√° se conectando.  Obviamente, se o servidor principal morrer, o usu√°rio n√£o poder√° mais acessar novas placas. <br><br>  Ent√£o, por que precisamos de um servidor que saiba onde as placas est√£o abertas?  Para que tenhamos um √∫nico ponto de decis√£o.  Se algo acontecer com os servidores, precisamos entender se a placa est√° realmente dispon√≠vel para remover a placa do registro ou reabrir em outro lugar.  Seria poss√≠vel organizar isso com a ajuda de um quorum, quando v√°rios servidores resolverem um problema semelhante, mas naquela √©poca n√£o t√≠nhamos o conhecimento necess√°rio para implementar o quorum independentemente. <br><br><h2>  Mudar para Hazelcast </h2><br>  De um jeito ou de outro, lidamos com os problemas que surgiram, mas pode n√£o ser o caminho mais bonito.  Agora, precis√°vamos entender como resolv√™-los corretamente, e formulamos uma lista de requisitos para uma nova solu√ß√£o de cluster: <br><br><ol><li>  Precisamos de algo que monitore o status de todos os servidores e suas fun√ß√µes.  Chame isso de descoberta de servi√ßo. </li><li>  Precisamos de bloqueios de cluster que ajudem a garantir consist√™ncia ao executar consultas perigosas. </li><li>  Precisamos de uma estrutura de dados distribu√≠dos que garanta que as placas estejam em determinados servidores e informe se algo deu errado. </li></ol><br>  Foi o ano de 2015.  Optamos pelo Hazelcast - In-Memory Data Grid, um sistema de cluster para armazenar informa√ß√µes na RAM.  Ent√£o pensamos que t√≠nhamos encontrado uma solu√ß√£o milagrosa, o santo graal do mundo da intera√ß√£o de cluster, uma estrutura milagrosa que pode fazer tudo e combina estruturas de dados distribu√≠dos, bloqueios, mensagens e filas RPC. <br><br><img src="https://habrastorage.org/webt/ce/ws/c9/cewsc9gdgzsmtebczs9jxbs4j2e.png"><br><br>  Como no ActiveMQ, transferimos quase tudo para o Hazelcast: <br><br><ul><li>  gera√ß√£o de recursos do usu√°rio atrav√©s do ExecutorService; </li><li>  bloqueio distribu√≠do quando os direitos s√£o alterados; </li><li>  fun√ß√µes e atributos de servidores (Service Discovery); </li><li>  um √∫nico registro de placas abertas, etc. </li></ul><br><h3>  Topologias Hazelcast </h3><br>  Hazelcast pode ser configurado em duas topologias.  A primeira op√ß√£o √© Cliente-Servidor, quando os membros est√£o localizados separadamente do aplicativo principal, eles pr√≥prios formam um cluster e todos os aplicativos se conectam a eles como um banco de dados. <br><br><img src="https://habrastorage.org/webt/r4/lg/vm/r4lgvmm7ni0dmyb6yp60cueklwm.png"><br><br>  A segunda topologia √© Incorporada, quando os membros Hazelcast s√£o incorporados no pr√≥prio aplicativo.  Nesse caso, podemos usar menos inst√¢ncias, o acesso aos dados √© mais r√°pido, porque os dados e a pr√≥pria l√≥gica de neg√≥cios est√£o no mesmo local. <br><br><img src="https://habrastorage.org/webt/gq/rz/fa/gqrzfappt3yspdlfpfe5sm3mhyg.png"><br><br>  Escolhemos a segunda solu√ß√£o porque consideramos mais eficaz e econ√¥mica de implementar.  Eficaz, porque a velocidade de acesso aos dados Hazelcast ser√° menor, porque  talvez esses dados estejam no servidor atual.  Econ√¥mico, porque n√£o precisamos gastar dinheiro em inst√¢ncias adicionais. <br><br><h3>  O cluster trava quando o membro trava </h3><br>  Algumas semanas depois de ativar o Hazelcast, surgiram problemas no produto. <br><br>  Inicialmente, nosso monitoramento mostrou que um dos servidores come√ßou a sobrecarregar gradualmente a mem√≥ria.  Enquanto assistia esse servidor, o restante dos servidores tamb√©m come√ßou a carregar: a CPU cresceu, depois a RAM e, ap√≥s cinco minutos, todos os servidores usaram toda a mem√≥ria dispon√≠vel. <br><br>  Neste ponto nos consoles, vimos estas mensagens: <br><br><pre> <code class="java hljs"><span class="hljs-number"><span class="hljs-number">2015</span></span>-<span class="hljs-number"><span class="hljs-number">07</span></span>-<span class="hljs-number"><span class="hljs-number">15</span></span> <span class="hljs-number"><span class="hljs-number">15</span></span>:<span class="hljs-number"><span class="hljs-number">35</span></span>:<span class="hljs-number"><span class="hljs-number">51</span></span>,<span class="hljs-number"><span class="hljs-number">466</span></span> [WARN] (cached18) com.hazelcast.spi.impl.operationservice.impl.Invocation: [my.host.address.com]:<span class="hljs-number"><span class="hljs-number">5701</span></span> [dev] [<span class="hljs-number"><span class="hljs-number">3.5</span></span>] Asking ifoperation execution has been started: com.hazelcast.spi.impl.operationservice.impl.IsStillRunningService$InvokeIsStillRunningOperationRunnable@<span class="hljs-number"><span class="hljs-number">6</span></span>d4274d7 <span class="hljs-number"><span class="hljs-number">2015</span></span>-<span class="hljs-number"><span class="hljs-number">07</span></span>-<span class="hljs-number"><span class="hljs-number">15</span></span> <span class="hljs-number"><span class="hljs-number">15</span></span>:<span class="hljs-number"><span class="hljs-number">35</span></span>:<span class="hljs-number"><span class="hljs-number">51</span></span>,<span class="hljs-number"><span class="hljs-number">467</span></span> [WARN] (hz._hzInstance_1_dev.async.thread-<span class="hljs-number"><span class="hljs-number">3</span></span>) com.hazelcast.spi.impl.operationservice.impl.Invocation:[my.host.address.com]:<span class="hljs-number"><span class="hljs-number">5701</span></span> [dev] [<span class="hljs-number"><span class="hljs-number">3.5</span></span>] <span class="hljs-string"><span class="hljs-string">'is-executing'</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">true</span></span> -&gt; Invocation{ serviceName=<span class="hljs-string"><span class="hljs-string">'hz:impl:executorService'</span></span>, op=com.hazelcast.executor.impl.operations.MemberCallableTaskOperation{serviceName=<span class="hljs-string"><span class="hljs-string">'null'</span></span>, partitionId=-<span class="hljs-number"><span class="hljs-number">1</span></span>, callId=<span class="hljs-number"><span class="hljs-number">18062</span></span>, invocationTime=<span class="hljs-number"><span class="hljs-number">1436974430783</span></span>, waitTimeout=-<span class="hljs-number"><span class="hljs-number">1</span></span>,callTimeout=<span class="hljs-number"><span class="hljs-number">60000</span></span>}, partitionId=-<span class="hljs-number"><span class="hljs-number">1</span></span>, replicaIndex=<span class="hljs-number"><span class="hljs-number">0</span></span>, tryCount=<span class="hljs-number"><span class="hljs-number">250</span></span>, tryPauseMillis=<span class="hljs-number"><span class="hljs-number">500</span></span>, invokeCount=<span class="hljs-number"><span class="hljs-number">1</span></span>, callTimeout=<span class="hljs-number"><span class="hljs-number">60000</span></span>,target=Address[my.host2.address.com]:<span class="hljs-number"><span class="hljs-number">5701</span></span>, backupsExpected=<span class="hljs-number"><span class="hljs-number">0</span></span>, backupsCompleted=<span class="hljs-number"><span class="hljs-number">0</span></span>}</code> </pre><br>  Aqui, o Hazelcast verifica se a opera√ß√£o que foi enviada para o primeiro servidor "moribundo" est√° em andamento.  O Hazelcast tentou se manter a par e verificou o status da opera√ß√£o v√°rias vezes por segundo.  Como resultado, ele enviou spam a todos os outros servidores com esta opera√ß√£o e, ap√≥s alguns minutos, eles ficaram sem mem√≥ria e coletamos v√°rios GB de logs de cada um deles. <br><br>  A situa√ß√£o foi repetida v√°rias vezes.  Aconteceu que este √© um erro na vers√£o 3.5 do Hazelcast, na qual o mecanismo de pulsa√ß√£o foi implementado, que verifica o status das solicita√ß√µes.  N√£o verificou alguns dos casos de fronteira que encontramos.  Eu tive que otimizar o aplicativo para n√£o cair nesses casos e, depois de algumas semanas, o Hazelcast corrigiu o erro em casa. <br><br><h3>  Adicionando e removendo membros frequentemente do Hazelcast </h3><br>  A pr√≥xima edi√ß√£o que descobrimos √© adicionar e remover membros do Hazelcast. <br><br>  Primeiro, descreverei brevemente como o Hazelcast funciona com parti√ß√µes.  Por exemplo, existem quatro servidores, e cada um armazena parte dos dados (na figura, eles s√£o de cores diferentes).  A unidade √© a parti√ß√£o prim√°ria, o empate √© a parti√ß√£o secund√°ria, ou seja,  backup da parti√ß√£o principal. <br><br><img src="https://habrastorage.org/webt/ex/qz/vj/exqzvjxs9rxlmfgssghnrnqxnn8.png"><br><br>  Quando um servidor est√° desligado, as parti√ß√µes s√£o enviadas para outros servidores.  Caso o servidor morra, as parti√ß√µes s√£o transferidas n√£o dele, mas daqueles servidores que ainda est√£o ativos e mantendo um backup dessas parti√ß√µes. <br><br><img src="https://habrastorage.org/webt/eu/ds/-0/euds-0xurnqjlbhisjoj8k9ucis.png"><br><br>  Este √© um mecanismo confi√°vel.  O problema √© que geralmente ligamos e desligamos os servidores para equilibrar a carga, e o reequil√≠brio das parti√ß√µes tamb√©m leva tempo.  E quanto mais servidores estiverem em execu√ß√£o e mais dados armazenarmos no Hazelcast, mais tempo ser√° necess√°rio para reequilibrar as parti√ß√µes. <br><br>  Obviamente, podemos reduzir o n√∫mero de backups, ou seja,  parti√ß√µes secund√°rias.  Mas isso n√£o √© seguro, pois algo definitivamente vai dar errado. <br><br>  Outra solu√ß√£o √© alternar para a topologia Cliente-Servidor para que ligar e desligar os servidores n√£o afete o cluster principal do Hazelcast.  Tentamos fazer isso, e as solicita√ß√µes de RPC n√£o podem ser executadas nos clientes.  Vamos ver o porqu√™. <br><br>  Para fazer isso, considere o exemplo do envio de uma solicita√ß√£o RPC para outro servidor.  Tomamos o ExecutorService, que permite enviar mensagens RPC, e enviamos com uma nova tarefa. <br><br><pre> <code class="plaintext hljs">hazelcastInstance .getExecutorService(...) .submit(new Task(), ...);</code> </pre><br>  A tarefa em si parece uma classe Java regular que implementa o Callable. <br><pre> <code class="plaintext hljs">public class Task implements Callable&lt;Long&gt; { @Override public Long call() { return 42; } }</code> </pre><br>  O problema √© que os clientes Hazelcast podem ser n√£o apenas aplicativos Java, mas tamb√©m aplicativos C ++, .NET e outros.  Naturalmente, n√£o podemos gerar e converter nossa classe Java para outra plataforma. <br><br>  Uma op√ß√£o √© mudar para o uso de solicita√ß√µes http, caso desejemos enviar algo de um servidor para outro e obter uma resposta.  Mas ent√£o teremos que abandonar parcialmente o Hazelcast. <br><br>  Portanto, como solu√ß√£o, optamos por usar filas em vez de ExecutorService.  Para fazer isso, implementamos independentemente um mecanismo para aguardar a execu√ß√£o de um elemento na fila, que processa casos de limite e retorna o resultado ao servidor solicitante. <br><br><h2>  O que aprendemos </h2><br>  <b>Coloque flexibilidade no sistema.</b>  O futuro est√° mudando constantemente, ent√£o n√£o h√° solu√ß√µes perfeitas.  Fazer o certo "certo" n√£o funciona, mas voc√™ pode tentar ser flex√≠vel e coloc√°-lo no sistema.  Isso nos permitiu adiar decis√µes arquiteturais importantes at√© o momento em que n√£o √© mais imposs√≠vel aceit√°-las. <br><br>  Robert Martin, em Arquitetura Limpa, escreve sobre esse princ√≠pio: <br><blockquote>  ‚ÄúO objetivo do arquiteto √© criar um formul√°rio para o sistema que tornar√° a pol√≠tica o elemento mais importante e os detalhes n√£o relacionados √† pol√≠tica.  Isso atrasar√° e atrasar√° as decis√µes sobre detalhes. ‚Äù </blockquote><br><br>  <b>Ferramentas e solu√ß√µes universais n√£o existem.</b>  Se lhe parece que alguma estrutura resolve todos os seus problemas, provavelmente isso n√£o √© verdade.  Portanto, ao implementar qualquer estrutura, √© importante entender n√£o apenas quais problemas ela resolver√°, mas quais ela trar√°. <br><br>  <b>N√£o reescreva tudo imediatamente.</b>  Se voc√™ se deparar com um problema na arquitetura e parece que a √∫nica solu√ß√£o certa √© escrever tudo do zero, aguarde.  Se o problema for realmente s√©rio, encontre uma solu√ß√£o r√°pida e observe como o sistema funcionar√° no futuro.  Provavelmente, esse n√£o ser√° o √∫nico problema na arquitetura, com o tempo voc√™ encontrar√° mais.  E somente quando voc√™ seleciona um n√∫mero suficiente de √°reas problem√°ticas voc√™ pode come√ßar a refatorar.  Somente neste caso, haver√° mais vantagens do que seu valor. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt441590/">https://habr.com/ru/post/pt441590/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt441578/index.html">Tutorial Reagir Parte 19: M√©todos do Ciclo de Vida dos Componentes</a></li>
<li><a href="../pt441580/index.html">Tutorial Reagir Parte 20: Primeira li√ß√£o de renderiza√ß√£o condicional</a></li>
<li><a href="../pt441582/index.html">Otimiza√ß√£o do sistema de controle LQR</a></li>
<li><a href="../pt441584/index.html">PHP Digest No. 150 (11 a 25 de fevereiro de 2019)</a></li>
<li><a href="../pt441586/index.html">Como recomendar m√∫sicas que quase ningu√©m ouviu. Relat√≥rio Yandex</a></li>
<li><a href="../pt441594/index.html">Napalm corporativo</a></li>
<li><a href="../pt441596/index.html">O primeiro espa√ßoporto privado ser√° constru√≠do na R√∫ssia</a></li>
<li><a href="../pt441598/index.html">Miss√£o lunar "Bereshit" - um portal online com simulador de trajet√≥ria e monitoramento dos par√¢metros atuais de voo</a></li>
<li><a href="../pt441600/index.html">Interface do usu√°rio fraca, programador fraco</a></li>
<li><a href="../pt441602/index.html">Por que um carro autom√°tico cl√°ssico √© imposs√≠vel e n√£o tem perspectivas comerciais</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>