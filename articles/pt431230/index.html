<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëÅ‚Äçüó® üßùüèΩ üëåüèº Armazenamento para infraestrutura HPC, ou como coletamos 65 PB de armazenamento no RIKEN Japan Research Center üë©‚Äç‚öñÔ∏è ‚òÅÔ∏è üë©‚Äç‚ù§Ô∏è‚Äçüë®</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="datacenterknowledge.com 

 No ano passado, a maior instala√ß√£o de armazenamento baseada em RAIDIX no momento foi implementada. Um sistema de 11 cluster...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Armazenamento para infraestrutura HPC, ou como coletamos 65 PB de armazenamento no RIKEN Japan Research Center</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/raidix/blog/431230/"><img src="https://habrastorage.org/webt/gt/hj/ib/gthjibaqdw82ss1jxo2tmhpa97o.jpeg"><br>  <i><font color="#99999">datacenterknowledge.com</font></i> <br><br>  No ano passado, a maior instala√ß√£o de armazenamento baseada em RAIDIX no momento foi implementada.  Um sistema de 11 clusters de failover foi implantado no Instituto de Ci√™ncias da Computa√ß√£o RIKEN (Jap√£o).  O principal objetivo do sistema √© o armazenamento da infraestrutura HPC (HPCI), que √© implementada como parte do interc√¢mbio acad√™mico em larga escala de informa√ß√µes acad√™micas Academic Cloud (com base na rede SINET). <br><br>  Uma caracter√≠stica significativa deste projeto √© o seu volume total de 65 PB, dos quais o volume utiliz√°vel do sistema √© 51,4 PB.  Para entender melhor esse valor, adicionamos que s√£o 6512 discos de 10 TB cada (o mais moderno no momento da instala√ß√£o).  Isso √© muito. <br><a name="habracut"></a><br>  O trabalho no projeto continuou ao longo do ano, ap√≥s o qual o monitoramento da estabilidade do sistema continuou por cerca de um ano.  Os indicadores obtidos atendem aos requisitos estabelecidos e agora podemos falar sobre o sucesso desse registro e um projeto significativo para n√≥s. <br><br><h2>  Supercomputador no RIKEN Institute Computing Center </h2><br>  Para a ind√∫stria de TIC, o Instituto RIKEN √© conhecido principalmente por seu lend√°rio ‚ÄúK-computer‚Äù (do japon√™s ‚Äúkei‚Äù, que significa 10 quatrilh√µes), que na √©poca do lan√ßamento (junho de 2011) era considerado o supercomputador mais poderoso do mundo. <br><br><div class="spoiler">  <b class="spoiler_title">Leia sobre o K-computer</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">www.nytimes.com/2011/06/20/technology/20computer.html</a> <br></div></div><br>  O supercomputador ajuda o Centro de Ci√™ncias Computacionais na implementa√ß√£o de pesquisas complexas em larga escala: permite modelar clima, condi√ß√µes clim√°ticas e comportamento molecular, calcular e analisar rea√ß√µes na f√≠sica nuclear, previs√£o de terremotos e muito mais.  As capacidades de supercomputadores tamb√©m s√£o usadas para pesquisas mais "cotidianas" e aplicadas - para procurar campos de petr√≥leo e prever tend√™ncias nos mercados de a√ß√µes. <br><br>  Tais c√°lculos e experimentos geram uma enorme quantidade de dados, cujo valor e signific√¢ncia n√£o podem ser superestimados.  Para tirar o m√°ximo proveito disso, os cientistas japoneses desenvolveram um conceito para um √∫nico espa√ßo de informa√ß√µes no qual profissionais de HPC de diferentes centros de pesquisa ter√£o acesso aos recursos recebidos de HPC. <br><br><h2>  Infraestrutura de computa√ß√£o de alto desempenho (HPCI) </h2><br>  O HPCI opera com base no SINET (The Science Information Network), uma rede principal para o interc√¢mbio de dados cient√≠ficos entre universidades japonesas e centros de pesquisa.  Atualmente, o SINET re√∫ne cerca de 850 institutos e universidades, criando enormes oportunidades para o interc√¢mbio de informa√ß√µes em pesquisas que afetam a f√≠sica nuclear, astronomia, geod√©sia, sismologia e ci√™ncia da computa√ß√£o. <br><br>  O HPCI √© um projeto de infraestrutura exclusivo que forma um sistema unificado de troca de informa√ß√µes no campo da computa√ß√£o de alto desempenho entre universidades e centros de pesquisa no Jap√£o. <br><br>  Ao combinar os recursos do supercomputador ‚ÄúK‚Äù e de outros centros de pesquisa de forma acess√≠vel, a comunidade cient√≠fica recebe benef√≠cios √≥bvios por trabalhar com dados valiosos criados pela computa√ß√£o de supercomputadores. <br><br>  Para fornecer acesso conjunto eficaz ao usu√°rio ao ambiente HPCI, foram impostos altos requisitos de armazenamento √† velocidade de acesso.  E, gra√ßas √† "hiperprodutividade" do computador K, o cluster de armazenamento no Centro de Ci√™ncias da Computa√ß√£o do Instituto RIKEN foi calculado para ser criado com um volume de trabalho de pelo menos 50 PB. <br><br>  A infraestrutura do projeto HPCI foi constru√≠da sobre o sistema de arquivos Gfarm, que permitiu um alto n√≠vel de desempenho e combinou clusters de armazenamento diferentes em um √∫nico espa√ßo compartilhado. <br><br><h2>  Sistema de arquivos Gfarm </h2><br>  Gfarm √© um sistema de arquivos distribu√≠dos de c√≥digo aberto desenvolvido por engenheiros japoneses.  Gfarm √© o fruto do desenvolvimento do Instituto de Ci√™ncia e Tecnologia Industrial Avan√ßada (AIST), e o nome do sistema refere-se √† arquitetura usada pelo Grid Data Farm. <br><br>  Este sistema de arquivos combina v√°rias propriedades aparentemente incompat√≠veis: <br><br><ul><li>  Alta escalabilidade em volume e desempenho </li><li>  Distribui√ß√£o de rede de longa dist√¢ncia com suporte para um √∫nico espa√ßo de nome para diversos centros de pesquisa </li><li>  Suporte √† API POSIX </li><li>  Alto desempenho necess√°rio para computa√ß√£o paralela </li><li>  Seguran√ßa de armazenamento de dados </li></ul><br>  O Gfarm cria um sistema de arquivos virtual usando recursos de armazenamento de v√°rios servidores.  Os dados s√£o distribu√≠dos pelo servidor de metadados e o pr√≥prio esquema de distribui√ß√£o √© oculto aos usu√°rios.  Devo dizer que o Gfarm consiste n√£o apenas em um cluster de armazenamento, mas tamb√©m em uma grade computacional que utiliza os recursos dos mesmos servidores.  O princ√≠pio de opera√ß√£o do sistema se assemelha ao Hadoop: o trabalho enviado √© "reduzido" para o n√≥ em que os dados est√£o. <br><br>  A arquitetura do sistema de arquivos √© assim√©trica.  As fun√ß√µes est√£o claramente alocadas: Servidor de Armazenamento, Servidor de Metadados, Cliente.  Mas, ao mesmo tempo, todas as tr√™s fun√ß√µes podem ser executadas pela mesma m√°quina.  Os servidores de armazenamento armazenam muitas c√≥pias de arquivos e os servidores de metadados operam no modo mestre-escravo. <br><br><h2>  Trabalho de projeto </h2><br>  A Core Micro Systems, um parceiro estrat√©gico e fornecedor exclusivo da RAIDIX no Jap√£o, implementou a implementa√ß√£o no Instituto de Ci√™ncias da Computa√ß√£o RIKEN.  Para implementar o projeto, foram necess√°rios 12 meses de trabalho minucioso, nos quais participaram n√£o apenas os funcion√°rios da Core Micro Systems, mas tamb√©m os especialistas t√©cnicos da equipe da Reydix. <br><br>  Ao mesmo tempo, a transi√ß√£o para outro sistema de armazenamento parecia improv√°vel: o sistema existente tinha muitas liga√ß√µes t√©cnicas, o que complicou a transi√ß√£o para qualquer nova marca. <br><br>  Durante longos testes, verifica√ß√µes e melhorias, o RAIDIX demonstrou consistentemente alto desempenho e efici√™ncia ao trabalhar com uma quantidade impressionante de dados. <br><br>  Sobre as melhorias, vale a pena contar um pouco mais.  Era necess√°rio n√£o apenas criar a integra√ß√£o dos sistemas de armazenamento com o sistema de arquivos Gfarm, mas expandir algumas caracter√≠sticas funcionais do software.  Por exemplo, para atender aos requisitos estabelecidos das especifica√ß√µes t√©cnicas, era necess√°rio desenvolver e implementar a tecnologia de Grava√ß√£o Autom√°tica o mais r√°pido poss√≠vel. <br><br>  A implanta√ß√£o do pr√≥prio sistema foi sistem√°tica.  Os engenheiros da Core Micro Systems conduziram com cuidado e precis√£o cada etapa do teste, aumentando gradualmente a escala do sistema. <br><br>  Em agosto de 2017, a primeira fase de implanta√ß√£o foi conclu√≠da quando o volume do sistema atingiu 18 PB.  Em outubro do mesmo ano, foi implementada a segunda fase, na qual o volume atingiu o recorde de 51 PB. <br><br><h2>  Arquitetura da solu√ß√£o </h2><br>  A solu√ß√£o foi criada atrav√©s da integra√ß√£o dos sistemas de armazenamento RAIDIX e o sistema de arquivos distribu√≠dos Gfarm.  Em conjunto com o Gfarm, a capacidade de criar armazenamento escal√°vel usando 11 sistemas RAIDIX de controlador duplo. <br><br>  A conex√£o com os servidores Gfarm √© feita atrav√©s de 8 x SAS 12G. <br><br><img src="https://habrastorage.org/webt/ii/3x/wk/ii3xwkawuyvwht0pczwonxbumnu.png"><br><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">1. Imagem de um cluster com um servidor de dados separado para cada n√≥</font></i> <br><br>  (1) conex√µes SAN Mesh de 48 Gbps √ó 8;  largura de banda: 384Gbps <br>  (2) conex√µes de malha de 48 Gbps √ó 40;  largura de banda: 1920Gbps <br><br><h3>  Configura√ß√£o da plataforma de controlador duplo </h3><br><table><tbody><tr><td>  CPU </td><td>  Intel Xeon E5-2637 - 4pcs </td></tr><tr><td>  Placa-m√£e </td><td>  Compat√≠vel com o modelo de processador compat√≠vel com PCI Express 3.0 x8 / x16 </td></tr><tr><td>  Cache interno </td><td>  256 GB para cada n√≥ </td></tr><tr><td>  Chassis </td><td>  2U </td></tr><tr><td>  Controladores SAS para conectar prateleiras de disco, servidores e sincroniza√ß√£o de cache de grava√ß√£o </td><td>  Broadcom 9305 16e, 9300 8e </td></tr><tr><td>  HDD </td><td>  HDD HGST H√©lio 10TB SAS </td></tr><tr><td>  Sincroniza√ß√£o de pulsa√ß√£o </td><td>  Ethernet 1 GbE </td></tr><tr><td>  Sincroniza√ß√£o CacheSync </td><td>  6 x SAS 12G </td></tr></tbody></table><br>  Os dois n√≥s do cluster de failover s√£o conectados a 10 JBODs (60 discos de 10 TB cada) por meio de 20 portas SAS 12G para cada n√≥.  Nestas prateleiras de disco, foram criadas 58 matrizes RAID6 de 10 TB (8 discos de dados (D) + 2 discos de paridade (P)) e 12 discos foram alocados para ‚Äúhot swap‚Äù. <br><br>  10 JBOD =&gt; 58 √ó RAID6 (8 discos de dados (D) + 2 discos de paridade (P)), LUN de 580 HDD + 12 HDD para ‚Äúhot swap‚Äù (2,06% do volume total) <br><br>  592 HDD (10TB SAS / 7.2k HDD) por cluster * HDD: HGST (MTBF: 2 500 000 horas) <br><br><img src="https://habrastorage.org/webt/qv/vc/0e/qvvc0egbrc1txvsztmx3o6yl918.png"><br><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">2. Cluster de failover com diagrama de conex√£o 10 JBOD</font></i> <br><br><h3>  Sistema geral e diagrama de conex√£o </h3><br><img src="https://habrastorage.org/webt/6m/ju/xv/6mjuxvlhx5zlcnqb1eopx9nknfu.png"><br><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">3. Imagem de um √∫nico cluster no sistema HPCI</font></i> <br><br><h2>  Principais indicadores do projeto </h2><br><blockquote>  Capacidade √∫til por cluster: <b>4,64 PB</b> ((RAID6 / 8D + 2P) LUN √ó 58) <br><br>  A capacidade √∫til total de todo o sistema: <b>51,04 PB</b> (4,64 PB √ó 11 clusters). <br><br>  Capacidade total do sistema: <b>65 PB</b> . <br><br>  O desempenho do sistema foi: <b>17 GB / s</b> para grava√ß√£o, <b>22 GB / s</b> para leitura. <br><br>  O desempenho total do subsistema de disco do cluster em 11 sistemas de armazenamento RAIDIX: <b>250 GB / s</b> . </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt431230/">https://habr.com/ru/post/pt431230/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt431218/index.html">Como eu fiz um jogo de quadrinhos Lovecraft</a></li>
<li><a href="../pt431220/index.html">O olhar de um bi√≥logo sobre as ra√≠zes do nosso envelhecimento</a></li>
<li><a href="../pt431222/index.html">Arquivamento de sites</a></li>
<li><a href="../pt431226/index.html">O jogo Snake para FPGA Cyclone IV (com joystick VGA e SPI)</a></li>
<li><a href="../pt431228/index.html">Obst√°culo para luz: cristais l√≠quidos ajudam</a></li>
<li><a href="../pt431232/index.html">Geramos marcadores SVG bonitos no Node.js</a></li>
<li><a href="../pt431234/index.html">11 de dezembro de Moscou - Alfa JS MeetUp</a></li>
<li><a href="../pt431236/index.html">Como escrever no Objective-C em 2018. Parte 1</a></li>
<li><a href="../pt431238/index.html">O resumo de eventos para profissionais de RH na √°rea de TI em dezembro de 2018</a></li>
<li><a href="../pt431242/index.html">TLS e certificados da Web</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>