<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëâüèΩ ‚òùüèª ‚òòÔ∏è Melhorando a qualidade da classifica√ß√£o de texto conectando a Wikipedia üë©üèæ‚Äçü§ù‚Äçüë®üèª üëéüèº ‚úäüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Utilizamos uma grande fonte estruturada de textos multil√≠ngues - a Wikipedia para melhorar a classifica√ß√£o dos textos. A abordagem √© boa com um alto g...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Melhorando a qualidade da classifica√ß√£o de texto conectando a Wikipedia</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/446228/"> Utilizamos uma grande fonte estruturada de textos multil√≠ngues - a Wikipedia para melhorar a classifica√ß√£o dos textos.  A abordagem √© boa com um alto grau de automatismo e independ√™ncia a partir do qual um problema espec√≠fico de classifica√ß√£o est√° sendo resolvido.  O maior efeito, no entanto, √© esperado nas tarefas de determina√ß√£o do t√≥pico. <br><a name="habracut"></a><br>  A id√©ia principal √© extrair da Wikipedia apenas os textos que nos ajudam a resolver nosso problema de classifica√ß√£o, ignorando outros.  Se classificarmos textos sobre gatos, √© improv√°vel que precisemos de textos sobre f√≠sica qu√¢ntica, embora textos sobre outros tipos de animais possam ser √∫teis.  A separa√ß√£o autom√°tica de tais textos um do outro √© a ess√™ncia da abordagem descrita. <br><br>  A Wikipedia, como voc√™ sabe, √© uma cole√ß√£o de artigos sobre muitas √°reas de conhecimento e interesses.  Ao mesmo tempo, uma parte significativa dos artigos tem links para artigos de um assunto semelhante, mas em outros idiomas.  Estas n√£o s√£o tradu√ß√µes, ou seja, artigos de um assunto geral.  Al√©m disso, a maioria dos artigos se enquadra em uma ou mais categorias.  As categorias, por sua vez, s√£o geralmente organizadas na forma de uma √°rvore hier√°rquica.  Ou seja, a tarefa de agrupar artigos da Wikipedia sobre t√≥picos de nosso interesse pode ser resolvida. <br><br>  Utilizamos o recurso DBPedia - uma vers√£o pr√©-cabeada e estruturada da Wikipedia.  O DBPedia nos fornece todas as informa√ß√µes necess√°rias - os nomes dos artigos, suas anota√ß√µes, as categorias dos artigos e as categorias superiores para as categorias.  Come√ßamos com o idioma mais representado na Wikipedia - ingl√™s.  Se sua tarefa n√£o possui, ou poucos, textos em ingl√™s, use o idioma para o qual existem muitos documentos. <br><br><h3>  Etapa 1. Clustering Wikipedia </h3><br>  Concentre-se nas categorias de artigos.  Por enquanto, ignore o conte√∫do deles.  As categorias formam um gr√°fico, principalmente do tipo √°rvore, mas tamb√©m existem ciclos.  Os artigos s√£o os pontos finais do gr√°fico (folhas) conectados a um ou mais n√≥s do gr√°fico.  Usamos a ferramenta Node2Vec para obter uma representa√ß√£o vetorial de cada categoria e cada artigo.  Artigos de assuntos semelhantes s√£o agrupados no espa√ßo vetorial. <br><br>  Por qualquer m√©todo conveniente do artigo, agrupamos em um n√∫mero bastante grande (centenas) de clusters. <br><br><h3>  Etapa 2. Treinamento de classificador na Wikipedia </h3><br>  Substitu√≠mos os nomes dos artigos nos clusters resultantes pelas anota√ß√µes (Resumo longo e Resumo resumido - aproximadamente um par√°grafo de texto por artigo).  Agora, temos centenas de clusters definidos como conjuntos de textos.  Utilizamos um modelo conveniente e constru√≠mos um classificador que resolve o problema da classifica√ß√£o em v√°rias classes: um cluster - uma classe.  Usamos o FastText. <br>  Na sa√≠da, obtemos um modelo que recebe o texto como entrada e, na sa√≠da, fornece um vetor de estimativas do grau em que o texto pertence √†s nossas centenas de clusters de classes. <br><br>  Se o primeiro passo √© agrupar artigos da Wikipedia n√£o por suas categorias, mas por seu conte√∫do, primeiro perderemos informa√ß√µes por categorias, mas √© importante e, em segundo lugar, obteremos um sistema degenerado - que, por textos, √© agrupado e constr√≥i modelo classificador.  A qualidade final provavelmente ser√° pior do que com uma abordagem separada.  Embora eu n√£o verifiquei. <br><br><h3>  Etapa 3. Criando um modelo por conta pr√≥pria, combate, dados </h3><br>  Utilizamos uma sele√ß√£o de nossos dados de combate e submetemos cada documento √† entrada do modelo na etapa 2. O modelo retorna um vetor de estimativas.  Usamos esse vetor como um vetor de recurso para o documento em quest√£o.  Como resultado, tendo processado toda a nossa amostra de treinamento de documentos de combate, obtemos uma tabela no formul√°rio padr√£o para aprendizado de m√°quina - um r√≥tulo de classe, um conjunto de sinais num√©ricos.  Chamamos essa tabela de um conjunto de treinamento. <br><br>  Constru√≠mos na amostra de treinamento um classificador que pode avaliar o conte√∫do de informa√ß√µes de atributos individuais.  As √°rvores de decis√£o e quaisquer varia√ß√µes aleat√≥rias da floresta s√£o adequadas.  Os sinais mais informativos s√£o aqueles grupos de artigos da Wikipedia que n√£o apenas t√™m temas semelhantes aos de nossos documentos de combate, mas, o mais importante, os t√≥picos desses artigos nos permitem separar bem nossas classes de luta.  Nas primeiras itera√ß√µes, o histograma da informatividade dos sinais geralmente √© bastante plano - v√°rios agrupamentos informativos e uma cauda longa s√£o quase iguais em termos de informatividade √†s centenas restantes de sinais. <br><br>  Depois de estudar o histograma do conte√∫do informativo dos caracteres, um ponto de inflex√£o foi determinado empiricamente a cada vez e aproximadamente 10 a 30% dos clusters foram para a pr√≥xima itera√ß√£o.  A ess√™ncia da itera√ß√£o √© que os artigos dos clusters informativos selecionados foram combinados, submetidos √†s etapas 1 a 3, onde foram agrupados novamente, dois classificadores foram constru√≠dos novamente e tudo terminou com uma an√°lise do histograma do conte√∫do das informa√ß√µes.  S√£o necess√°rias 3-4 itera√ß√µes. <br><br>  Descobriu-se que, em nossos dados, os sinais digitais, especialmente os n√∫meros dos anos, t√™m um peso muito forte e arrastam para si a informa√ß√£o de todo o cluster.  Como resultado l√≥gico, os grupos dedicados a eventos esportivos anuais se tornaram os mais informativos - uma massa de n√∫meros e datas, vocabul√°rio restrito.  Eu tive que remover todos os n√∫meros nos textos das anota√ß√µes do artigo (na segunda etapa).  Tornou-se visivelmente melhor, grupos de artigos realmente com um assunto espec√≠fico come√ßaram a se destacar (como imagin√°vamos).  Ao mesmo tempo, apareceram grupos inesperados que logicamente ca√≠ram em nossa miss√£o de combate, tinham o vocabul√°rio certo, mas era muito dif√≠cil adivinhar, a priori, a utilidade de tais grupos. <br><br><h3>  Etapa 4. Finalize o modelo </h3><br>  Ap√≥s v√°rias itera√ß√µes das etapas 1 a 3, temos um n√∫mero razo√°vel de artigos selecionados da Wikipedia, cujos t√≥picos ajudam a compartilhar nossos documentos de combate.  Estamos expandindo a sele√ß√£o com artigos semelhantes em outros idiomas de nosso interesse e criando agrupamentos finais, desta vez dezenas.  Esses clusters podem ser usados ‚Äã‚Äãde duas maneiras - crie um classificador semelhante √† etapa 2 e use-o para expandir o vetor de recursos digitais em sua miss√£o de combate ou use esses conjuntos de textos como fonte de vocabul√°rio adicional e os integre ao seu classificador de combate.  Usamos o segundo caminho. <br><br>  Nosso classificador de combate √© um conjunto de dois modelos - bayes ing√™nuos truncados e xgboost.  Naive Bayes trabalha em gramas longos, gramas com comprimentos de 1 a 16 elementos, e cada grama encontrada inclina o total para uma das classes, mas Bayes n√£o toma uma decis√£o final - apenas fornece a soma dos pesos gramados de cada uma das aulas.  O Xgboost aceita a sa√≠da de bayes, outros classificadores e alguns atributos digitais que s√£o constru√≠dos independentemente do texto, e o xgboost j√° fornece o modelo final e a avalia√ß√£o final.  Essa abordagem facilita a conex√£o de qualquer conjunto de textos ao modelo de gram bayes, incluindo os conjuntos resultantes de artigos da Wikipedia, e o xgboost j√° est√° procurando padr√µes na forma de rea√ß√µes t√≠picas de clusters da wikipedia para combater textos. <br><br><h3>  Resultados e Conclus√µes </h3><br>  O primeiro resultado deu um aumento da precis√£o condicional de 60% para 62%.  Ao substituir as anota√ß√µes dos artigos da Wikipedia na etapa 4 pelos artigos desinflados, a precis√£o aumentou para 66%.  O resultado √© natural, porque o tamanho da anota√ß√£o √© de duas ou tr√™s frases e o tamanho do artigo √© de magnitude maior.  Mais material lingu√≠stico - efeito mais alto. <br><br>  Devemos esperar que, ap√≥s a conclus√£o de todo o procedimento nos textos dos artigos, em vez das anota√ß√µes, o aumento da qualidade seja ainda maior, mas j√° existe um problema t√©cnico de n√∫mero - √© dif√≠cil desenvolver e processar toda a Wikipedia ou sua parte vis√≠vel (se voc√™ n√£o come√ßar na primeira itera√ß√£o).  Al√©m disso, se voc√™ usar inicialmente n√£o apenas o ingl√™s, mas todos os idiomas de interesse, ainda poder√° ganhar outra coisa.  Nesse caso, o crescimento nos volumes processados ‚Äã‚Äã√© m√∫ltiplo, e n√£o em ordens de magnitude, como no primeiro caso. <br><br><h4>  Vetor de documento sem√¢ntico </h4><br>  Para cada documento, √© constru√≠do um vetor da rela√ß√£o do documento com os t√≥picos fornecidos, com base nas categorias da Wikipedia.  O vetor √© calculado pelo m√©todo descrito na etapa 3 ou por nossos gram bayes.  Dessa forma, os documentos de combate podem ser agrupados de acordo com esses vetores e obter um agrupamento de documentos de combate por assunto.  Resta apenas colocar as hashtags e cada novo documento j√° pode cair no banco de dados com tags.  Que ent√£o os usu√°rios podem pesquisar.  Este √© o caso se voc√™ afixar tags de maneira expl√≠cita e vis√≠vel ao usu√°rio.  Parece elegante, embora eu n√£o seja um defensor. <br><br><h4>  Pesquisa adaptativa </h4><br>  Um m√©todo mais interessante de usar vetores de documentos sem√¢nticos √© a pesquisa adaptativa.  Observando a atividade do usu√°rio, em quais documentos ele permanece e quais ele nem l√™, √© poss√≠vel delinear a √°rea de interesse do usu√°rio no sentido de longo prazo (afinal, os usu√°rios tamb√©m t√™m uma divis√£o de responsabilidades e todos est√£o principalmente procurando por si) e dentro da estrutura da sess√£o de pesquisa atual. <br><br>  Documentos com t√≥picos semelhantes t√™m vetores sem√¢nticos semelhantes com uma medida de cosseno alto, e isso permite avaliar documentos nos resultados da pesquisa em tempo real, de acordo com o grau de conformidade esperada com os interesses do usu√°rio, resultando no aumento dos documentos necess√°rios nos resultados da pesquisa. <br><br>  Como resultado, mesmo com consultas de pesquisa id√™nticas para cada usu√°rio, os resultados da pesquisa podem ser personalizados para ele e, dependendo de quais documentos da etapa anterior o usu√°rio estava interessado, a pr√≥xima etapa da pesquisa ser√° ajustada √†s necessidades do usu√°rio, mesmo que a consulta de pesquisa em si n√£o tenha sido alterada. <br><br>  Agora estamos trabalhando no problema da pesquisa adaptativa. <br><br><h4>  Teste de Hip√≥teses de Neg√≥cios </h4><br>  Os neg√≥cios v√™m periodicamente com id√©ias brilhantes que s√£o muito dif√≠ceis de implementar.  Devemos aprender a encontrar documentos por sua descri√ß√£o, sem ter uma amostra marcada para treinamento ou a capacidade de enviar aos avaliadores algum conjunto de documentos para marca√ß√£o.  Isso geralmente acontece quando os documentos de destino raramente s√£o encontrados com rela√ß√£o ao fluxo geral de documentos e, como resultado, enviando um conjunto de 10 mil documentos aos avaliadores sem filtragem preliminar, voc√™ pode obter 1-2 ou menos resultados necess√°rios. <br><br>  Nossa abordagem √© criar um processo de aprendizado iterativo baseado em vetores sem√¢nticos.  Na primeira etapa, encontramos v√°rios textos que definem nosso t√≥pico de destino - podem ser artigos da Wikipedia ou textos de outras fontes.  Para cada texto, seu vetor sem√¢ntico √© produzido.  Se o t√≥pico de destino √© complexo, a √°lgebra de conjuntos funciona - unifica√ß√£o, interse√ß√£o, exclus√£o de alguns t√≥picos de outros.  Por exemplo - existem artigos da Wikipedia sobre "Pesquisa e Desenvolvimento" e "Cosm√©ticos", a interse√ß√£o de conjuntos dar√° "P&amp;D sobre cosm√©ticos". <br><br>  Todos os documentos no banco de dados podem ser classificados pelo grau de conformidade com os t√≥picos fornecidos, ent√£o a √°lgebra de conjuntos funciona nos pr√≥prios documentos da seguinte maneira - o documento √© considerado relevante para o t√≥pico se seu vetor sem√¢ntico estiver mais pr√≥ximo do vetor de artigos da Wikipedia de um determinado t√≥pico do que a m√©dia do banco de dados.  Interse√ß√£o - se ao mesmo tempo o vetor sem√¢ntico do documento estiver mais pr√≥ximo dos dois t√≥picos do que a m√©dia do banco de dados.  Outras opera√ß√µes s√£o semelhantes. <br><br>  Encontramos um conjunto de centenas ou dois documentos que t√™m a maior proximidade de todos os t√≥picos positivos e, ao mesmo tempo, a maior proximidade de todos os t√≥picos negativos (se n√£o estivermos interessados ‚Äã‚Äãem quest√µes financeiras na pesquisa que estamos procurando, definiremos o artigo da categoria "Finan√ßas" como um exemplo negativo) )  Daremos esses documentos aos avaliadores, eles encontrar√£o v√°rios exemplos positivos neles, com base nesses exemplos, procuraremos outros documentos com vetores sem√¢nticos pr√≥ximos, marc√°-los e, na sa√≠da, obteremos documentos suficientes para que a classe positiva construa qualquer classificador conveniente.  Pode levar v√°rias itera√ß√µes. <br><br><h4>  Sum√°rio </h4><br>  A abordagem descrita permite selecionar automaticamente, sem an√°lise manual, da Wikipedia ou de outro conjunto de fontes de textos que ajudem a resolver o problema de classifica√ß√£o.  Simplesmente conectando os clusters da Wikipedia a um classificador funcional, pode-se esperar um aumento significativo na qualidade, sem a necessidade de adapta√ß√£o do pr√≥prio classificador. <br><br>  Bem, a pesquisa adaptativa √© interessante. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt446228/">https://habr.com/ru/post/pt446228/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt446210/index.html">ADAM-3600 - um controlador industrial multifuncional</a></li>
<li><a href="../pt446212/index.html">Profundidades do SIEM: correla√ß√µes prontas para uso. Parte 5. Metodologia para o desenvolvimento de regras de correla√ß√£o</a></li>
<li><a href="../pt446214/index.html">OS1: um kernel primitivo no Rust para x86. Parte 3. Cart√£o de mem√≥ria, exce√ß√£o de falha de p√°gina, pilha e aloca√ß√µes</a></li>
<li><a href="../pt446218/index.html">O designer de jogos n√£o √© muito diferente de um psicopata. Como fizemos o jogo CMAN</a></li>
<li><a href="../pt446222/index.html">Uso de potenciais t√©rmicos para an√°lise de territ√≥rio</a></li>
<li><a href="../pt446230/index.html">Monitoramento e gerenciamento remotos de dispositivos baseados em Linux / OpenWrt / Lede atrav√©s da porta 80, continua√ß√£o</a></li>
<li><a href="../pt446234/index.html">Como volunt√°rios de todo o mundo criam transmiss√µes ao vivo do ICPC-2019</a></li>
<li><a href="../pt446236/index.html">Yandex melhorar√° algoritmos de reconhecimento de voz</a></li>
<li><a href="../pt446238/index.html">Explorando gerenciadores de inicializa√ß√£o assinados para contornar a Inicializa√ß√£o segura UEFI</a></li>
<li><a href="../pt446242/index.html">Procrastina√ß√£o como ferramenta para viagens no tempo</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>