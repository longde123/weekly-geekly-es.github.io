<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üî¢ ‚è∫Ô∏è üë©üèº‚Äçüåæ Seguimiento de tono, o determinaci√≥n de la frecuencia de tono en el habla, utilizando Praat, YAAPT e YIN como ejemplos üï∫üèæ üë®üèº‚Äç‚öïÔ∏è üéâ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En el campo del reconocimiento de emociones, la voz es la segunda fuente m√°s importante de datos emocionales despu√©s de la cara. La voz puede caracter...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Seguimiento de tono, o determinaci√≥n de la frecuencia de tono en el habla, utilizando Praat, YAAPT e YIN como ejemplos</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/neurodatalab/blog/416441/"><img src="https://habrastorage.org/getpro/habr/post_images/37d/3f1/975/37d3f19758eb7d646ccff079d37772f8.png" alt="imagen"><br><br>  En el campo del reconocimiento de emociones, la voz es la segunda fuente m√°s importante de datos emocionales despu√©s de la cara.  La voz puede caracterizarse por varios par√°metros.  El tono de voz es una de las principales caracter√≠sticas, sin embargo, en el campo de la tecnolog√≠a ac√∫stica es m√°s correcto llamar a este par√°metro la frecuencia fundamental. <br><br>  La frecuencia del tono fundamental est√° directamente relacionada con lo que llamamos entonaci√≥n.  Y la entonaci√≥n, por ejemplo, est√° asociada con las caracter√≠sticas emocionalmente expresivas de la voz. <br><br>  Sin embargo, determinar la frecuencia del tono fundamental no es una tarea completamente trivial con matices interesantes.  En este art√≠culo, discutiremos las caracter√≠sticas de los algoritmos para su determinaci√≥n y compararemos las soluciones existentes con ejemplos de grabaciones de audio espec√≠ficas. <br><a name="habracut"></a><br>  <b>Introduccion</b> <br><br>  Para empezar, recordemos cu√°l es, en esencia, la frecuencia del tono fundamental y en qu√© tareas puede ser necesario.  <i>La frecuencia fundamental</i> , que tambi√©n se conoce como CHOT, Frecuencia fundamental o F0, es la frecuencia de las cuerdas vocales cuando pronuncian sonidos sonoros.  Al pronunciar sonidos que no son de tono (sordos), por ejemplo, hablar en voz baja o emitir silbidos y silbidos, los ligamentos no vacilan, lo que significa que esta caracter√≠stica no es relevante para ellos. <br><br>  * Tenga en cuenta que la divisi√≥n en sonidos tonales y no tonales no es equivalente a la divisi√≥n en vocales y consonantes. <br><br>  La variabilidad de la frecuencia del tono fundamental es bastante grande, y puede variar mucho no solo entre las personas (para las voces masculinas promedio m√°s bajas, la frecuencia es de 70-200 Hz, y para las voces femeninas puede alcanzar los 400 Hz), sino tambi√©n para una persona, especialmente en el lenguaje emocional. . <br><br>  La determinaci√≥n de la frecuencia del tono fundamental se utiliza para resolver una amplia gama de problemas: <br><br><ul><li>  Reconocimiento de emociones, como dijimos anteriormente; </li><li>  Determinaci√≥n sexual; </li><li>  Al resolver el problema de segmentar audio con m√∫ltiples voces o dividir el discurso en frases; </li><li>  En medicina, para determinar las caracter√≠sticas patol√≥gicas de la voz (por ejemplo, utilizando los par√°metros ac√∫sticos Jitter y Shimmer).  Por ejemplo, la identificaci√≥n de signos de la enfermedad de Parkinson [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">1</a> ].  Jitter y Shimmer tambi√©n pueden usarse para reconocer emociones [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">2</a> ]. </li></ul><br>  Sin embargo, hay una serie de dificultades para determinar F0.  Por ejemplo, a menudo es posible confundir F0 con arm√≥nicos, lo que puede conducir a los llamados efectos de duplicaci√≥n de tono / reducci√≥n a la mitad de tono [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">3</a> ].  Y en grabaciones de audio de baja calidad, F0 es bastante dif√≠cil de calcular, ya que el pico deseado a bajas frecuencias casi desaparece. <br><br>  Por cierto, ¬ørecuerdas la historia de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Laurel y Yanny</a> ?  Las diferencias en las palabras que las personas escuchan cuando escuchan la misma grabaci√≥n de audio, surgieron precisamente debido a la diferencia en la percepci√≥n F0, que est√° influenciada por muchos factores: la edad del oyente, el grado de fatiga y el dispositivo de reproducci√≥n.  Entonces, cuando escuche grabaciones en parlantes con reproducci√≥n de alta calidad de bajas frecuencias, escuchar√° a Laurel, y en sistemas de audio donde las bajas frecuencias se reproducen mal, Yanny.  El efecto de transici√≥n se puede ver en un dispositivo, por ejemplo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> .  Y en este <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo</a> , la red neuronal act√∫a como oyente.  En otro <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo,</a> puede leer c√≥mo se explica el fen√≥meno Yanny / Laurel en t√©rminos de formaci√≥n del habla. <br><br>  Dado que un an√°lisis detallado de todos los m√©todos para determinar F0 ser√≠a demasiado voluminoso, el art√≠culo es de naturaleza general y puede ayudar a navegar el tema. <br><br>  <b>M√©todos para determinar F0</b> <br><br>  Los m√©todos para determinar F0 se pueden dividir en tres categor√≠as: en funci√≥n de la din√°mica de tiempo de la se√±al, o dominio de tiempo;  basado en la estructura de frecuencia, o dominio de frecuencia, as√≠ como m√©todos combinados.  Sugerimos que se familiarice con el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo de</a> revisi√≥n sobre el tema, donde se analizan en detalle los m√©todos indicados para extraer F0. <br><br>  Tenga en cuenta que cualquiera de los algoritmos discutidos consta de 3 pasos principales: <br><br>  Preprocesamiento (filtrado de la se√±al, divisi√≥n en cuadros) <br>  Buscar posibles valores de F0 (candidatos) <br>  El seguimiento es la elecci√≥n de la trayectoria m√°s probable F0 (dado que para cada momento en el tiempo tenemos varios candidatos en competencia, necesitamos encontrar la pista m√°s probable entre ellos) <br><br>  <b>Dominio del tiempo</b> <br><br>  Esbozamos algunos puntos generales.  Antes de aplicar los m√©todos en el dominio del tiempo, la se√±al se filtra previamente, dejando solo bajas frecuencias.  Se establecen umbrales: las frecuencias m√≠nimas y m√°ximas, por ejemplo, de 75 a 500 Hz.  La determinaci√≥n de F0 se realiza solo para √°reas con voz arm√≥nica, ya que para pausas o sonidos de ruido esto no solo no tiene sentido, sino que tambi√©n puede introducir errores en cuadros adyacentes cuando se aplica la interpolaci√≥n y / o suavizado.  La longitud del cuadro se selecciona de modo que contenga al menos tres per√≠odos. <br><br>  El m√©todo principal, en base al cual apareci√≥ posteriormente toda una familia de algoritmos, es la autocorrelaci√≥n.  El enfoque es bastante simple: es necesario calcular la funci√≥n de autocorrelaci√≥n y tomar su primer m√°ximo.  Mostrar√° el componente de frecuencia m√°s pronunciado en la se√±al.  ¬øCu√°l podr√≠a ser la dificultad en el caso de utilizar la autocorrelaci√≥n y por qu√© est√° lejos de ser siempre que el primer m√°ximo corresponder√° a la frecuencia deseada?  Incluso en condiciones cercanas a las ideales en grabaciones de alta calidad, el m√©todo puede confundirse debido a la compleja estructura de la se√±al.  En condiciones cercanas a la real, donde, entre otras cosas, podemos encontrar la desaparici√≥n del pico deseado en grabaciones ruidosas o grabaciones de baja calidad inicial, el n√∫mero de errores aumenta considerablemente. <br><br>  A pesar de los errores, el m√©todo de autocorrelaci√≥n es bastante conveniente y atractivo debido a su simplicidad y l√≥gica b√°sicas, raz√≥n por la cual se toma como base en muchos algoritmos, incluido YIN.  Incluso el nombre del algoritmo nos remite al equilibrio entre la conveniencia y la inexactitud del m√©todo de autocorrelaci√≥n: "El nombre YIN de" yin "y" yang "de la filosof√≠a oriental alude a la interacci√≥n entre la autocorrelaci√≥n y la cancelaci√≥n que implica".  [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">4</a> ] <br><br>  Los creadores de YIN intentaron corregir las debilidades del enfoque de autocorrelaci√≥n.  El primer cambio es el uso de la funci√≥n de diferencia normalizada media acumulativa, que deber√≠a reducir la sensibilidad a las modulaciones de amplitud, hacer que los picos sean m√°s pronunciados: <br><br>  \ begin {ecuaci√≥n} <br>  d'_t (\ tau) = <br>  \ begin {cases} <br>  1, &amp; \ tau = 0 \\ <br>  d_t (\ tau) \ bigg / \ bigg [\ frac {1} {\ tau} \ sum \ limits_ {j = 1} ^ {\ tau} d_t (j) \ bigg], y \ text {de lo contrario} <br>  \ end {casos} <br>  \ end {ecuaci√≥n} <br>  YIN tambi√©n trata de evitar errores que ocurren en casos donde la longitud de la funci√≥n de ventana no est√° completamente dividida por el per√≠odo de oscilaci√≥n.  Para esto, se utiliza la interpolaci√≥n m√≠nima parab√≥lica.  En el √∫ltimo paso del procesamiento de la se√±al de audio, se ejecuta la funci√≥n Mejor estimaci√≥n local para evitar saltos bruscos en los valores (ya sea bueno o malo, este es un punto discutible). <br><br>  <b>Dominio de frecuencia</b> <br><br>  Si hablamos del dominio de la frecuencia, entonces la estructura arm√≥nica de la se√±al se destaca, es decir, la presencia de picos espectrales en frecuencias que son m√∫ltiplos de F0.  Puede "colapsar" este patr√≥n peri√≥dico en un pico claro utilizando el an√°lisis cepstral.  Cepstrum - Transformada de Fourier del logaritmo del espectro de potencia;  el pico cepstral corresponde al componente m√°s peri√≥dico del espectro (uno puede leer sobre esto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> ). <br><br>  <b>M√©todos h√≠bridos para determinar F0</b> <br><br>  El siguiente algoritmo, que vale la pena explorar con m√°s detalle, tiene el nombre parlante YAAPT, otro algoritmo de seguimiento de tono, y de hecho es h√≠brido, ya que utiliza informaci√≥n de frecuencia y tiempo.  Una descripci√≥n completa est√° en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo</a> , aqu√≠ describimos solo las etapas principales. <br><br><img src="https://habrastorage.org/webt/r2/mu/uj/r2muujzlcxgdgp5a0bqem3t_iuu.png"><br>  <i>Figura 1. Diagrama del algoritmo YAAPTalgo ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace</a> )</i> . <br><br>  YAAPT consta de varios pasos principales, el primero de los cuales es el preprocesamiento.  En esta etapa, los valores de la se√±al original son al cuadrado y se obtiene una segunda versi√≥n de la se√±al.  Este paso persigue el mismo objetivo que la funci√≥n de diferencia normalizada media acumulada en YIN: amplificaci√≥n y restauraci√≥n de picos de autocorrelaci√≥n "atascados".  Ambas versiones de la se√±al se filtran, por lo general toman el rango de 50-1500 Hz, a veces 50-900 Hz. <br><br>  Luego, la trayectoria base F0 se calcula a partir del espectro de la se√±al convertida.  Los candidatos para F0 se determinan utilizando la funci√≥n de correlaci√≥n de arm√≥nicos espectrales (SHC). <br><br>  \ begin {ecuaci√≥n} <br>  SHC (t, f) = \ sum \ limits_ {f '= - WL / 2} ^ {WL / 2} \ prod \ limits_ {r = 1} ^ {NH + 1} S (t, rf + f') <br>  \ end {ecuaci√≥n} <br>  donde S (t, f) es el espectro de magnitud para el cuadro t y la frecuencia f, WL es la longitud de la ventana en Hz, NH es el n√∫mero de arm√≥nicos (los autores recomiendan usar los primeros tres arm√≥nicos).  La potencia espectral tambi√©n se usa para determinar los cuadros sonoros y no sonoros, despu√©s de lo cual se busca la trayectoria m√°s √≥ptima, y ‚Äã‚Äãse tiene en cuenta la posibilidad de duplicaci√≥n de tono / reducci√≥n a la mitad de tono [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">3</a> , Secci√≥n II, C]. <br><br>  Adem√°s, los candidatos para F0 se determinan tanto para la se√±al inicial como para la convertida, y en lugar de la funci√≥n de autocorrelaci√≥n, aqu√≠ se utiliza la Correlaci√≥n cruzada normalizada (NCCF). <br><br>  \ begin {ecuaci√≥n} <br>  NCCF (m) = \ frac {\ sum \ limits_ {n = 0} ^ {Nm-1} x (n) * x (n + m)} {\ sqrt {\ sum \ limits_ {n = 0} ^ { Nm-1} x ^ 2 (n) * \ sum \ limits_ {n = 0} ^ {Nm-1} x ^ 2 (n + m)}} \ text {,} \ hspace {0.3cm} 0 &lt;m &lt;M_ {0} <br>  \ end {ecuaci√≥n} <br>  El siguiente paso es evaluar a todos los posibles candidatos y calcular su importancia o peso (m√©rito).  El peso de los candidatos obtenidos de la se√±al de audio depende no solo de la amplitud del pico NCCF, sino tambi√©n de su proximidad a la trayectoria F0 determinada a partir del espectro.  Es decir, el dominio de frecuencia se considera grueso en t√©rminos de precisi√≥n, pero estable [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">3</a> , Secci√≥n II, D]. <br><br>  Luego, para todos los pares de los candidatos restantes, se calcula la matriz de Costo de transici√≥n: el precio de transici√≥n, en el que finalmente encuentran la trayectoria √≥ptima [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">3</a> , Secci√≥n II, E]. <br><br>  <b>Ejemplos</b> <br><br>  Ahora aplicamos todos los algoritmos anteriores a grabaciones de audio espec√≠ficas.  Como punto de partida, utilizaremos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Praat</a> , una herramienta que es fundamental para muchos estudiosos del habla.  Y luego, en Python, analizaremos la implementaci√≥n de YIN y YAAPT y compararemos los resultados recibidos. <br><br>  Como material de audio, puede usar cualquier audio disponible.  Tomamos varios extractos de nuestra <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">base de</a> datos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RAMAS</a> , un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">conjunto de datos</a> multimodal creado con la participaci√≥n de actores de VGIK.  Tambi√©n puede usar material de otras bases de datos abiertas, como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">LibriSpeech</a> o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RAVDESS</a> . <br><br>  Para un ejemplo ilustrativo, tomamos extractos de varias grabaciones con voces masculinas y femeninas, tanto neutrales como emocionalmente coloreadas, y para mayor claridad, los combinamos en una sola <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">grabaci√≥n</a> .  Veamos nuestra se√±al, su espectrograma, intensidad (color naranja) y F0 (color azul).  En Praat, esto se puede hacer usando Ctrl + O (Abrir - Leer desde archivo) y luego el bot√≥n Ver y editar. <br><br><img src="https://habrastorage.org/webt/ir/ay/kh/iraykhpzwfetpahhiymdic6pdwi.png"><br>  <i>Figura 2. Espectrograma, intensidad (color naranja), F0 (color azul) en Praat.</i> <br><br>  El audio muestra claramente que en el discurso emocional, el tono aumenta tanto en hombres como en mujeres.  Al mismo tiempo, F0 para el discurso emocional masculino puede compararse con F0 de una voz femenina. <br><br>  <b>Seguimiento</b> <br><br>  Seleccione la pesta√±a Analizar periodicidad - a Pitch (ac) en el men√∫ Praat, es decir, la definici√≥n de F0 usando autocorrelaci√≥n.  Aparecer√° una ventana para configurar par√°metros en la que es posible establecer 3 par√°metros para determinar candidatos para F0 y 6 par√°metros m√°s para el algoritmo de buscador de ruta, que construye la ruta F0 m√°s probable entre todos los candidatos. <br><br><div class="spoiler">  <b class="spoiler_title">Muchos par√°metros (en Praat, su descripci√≥n tambi√©n est√° en el bot√≥n Ayuda)</b> <div class="spoiler_text"><ul><li>  Umbral de silencio: el umbral de la amplitud relativa de la se√±al para determinar el silencio, el valor est√°ndar es 0.03. </li><li>  Umbral de sonorizaci√≥n: el peso del candidato sordo, el valor m√°ximo es 1. Cuanto mayor sea este par√°metro, m√°s cuadros se definir√°n como sordos, es decir, que no contienen sonidos de tono.  En estos cuadros, F0 no se determinar√°.  El valor de este par√°metro es el umbral para los picos de la funci√≥n de autocorrelaci√≥n.  El valor predeterminado es 0.45. </li><li>  Costo de octava: determina cu√°nto m√°s peso tienen los candidatos de alta frecuencia en relaci√≥n con los de baja frecuencia.  Cuanto mayor sea el valor, m√°s preferencia se le da al candidato de alta frecuencia.  El valor predeterminado es 0.01 por octava. </li><li>  Costo de salto de octava: con un aumento en este coeficiente, el n√∫mero de transiciones bruscas como saltos entre valores sucesivos de F0 disminuye.  El valor predeterminado es 0.35. </li><li>  Costo sonoro / sordo: aumentar este coeficiente disminuye el n√∫mero de transiciones sonoras / sonoras.  El valor predeterminado es 0.14. </li><li>  Techo de paso (Hz): no se consideran candidatos por encima de esta frecuencia.  El valor predeterminado es 600 Hz. </li></ul><br></div></div><br>  Se puede encontrar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">una</a> descripci√≥n detallada del algoritmo en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">un art√≠culo de</a> 1993. <br><br>  Puede verse el resultado del rastreador (buscador de ruta) haciendo clic en Aceptar y luego viendo (Ver y editar) el archivo Pitch resultante.  Se puede ver que, adem√°s de la trayectoria seleccionada, todav√≠a hab√≠a candidatos bastante significativos con una frecuencia m√°s baja. <br><br><img src="https://habrastorage.org/webt/wq/rq/vf/wqrqvf_cbnbn8orcajij6sfrasu.png"><br>  <i>Figura 3. PitchPath durante los primeros 1.3 segundos de grabaci√≥n de audio.</i> <br><br>  <b>¬øPero qu√© hay de Python?</b> <br><br>  Tomemos dos bibliotecas que ofrecen seguimiento de tono: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aubio</a> , en el que el algoritmo predeterminado es YIN, y la biblioteca <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AMFM_decompsition</a> , que tiene una implementaci√≥n del algoritmo YAAPT.  En el archivo separado (archivo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">PraatPitch.txt</a> ), <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">inserte los</a> valores F0 de Praat (esto se puede hacer manualmente: seleccione el archivo de sonido, haga clic en Ver y editar, seleccione el archivo completo y seleccione la lista Pitch-Pitch en el men√∫ superior). <br><br>  Ahora compare los resultados para los tres algoritmos (YIN, YAAPT, Praat). <br><br><div class="spoiler">  <b class="spoiler_title">Mucho c√≥digo</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> amfm_decompy.basic_tools <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> basic <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> amfm_decompy.pYAAPT <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pYAAPT <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> aubio <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> source, pitch <span class="hljs-comment"><span class="hljs-comment"># load audio signal = basic.SignalObj('/home/eva/Documents/papers/habr/media/audio.wav') filename = '/home/eva/Documents/papers/habr/media/audio.wav' # YAAPT pitches pitchY = pYAAPT.yaapt(signal, frame_length=40, tda_frame_length=40, f0_min=75, f0_max=600) # YIN pitches downsample = 1 samplerate = 0 win_s = 1764 // downsample # fft size hop_s = 441 // downsample # hop size s = source(filename, samplerate, hop_s) samplerate = s.samplerate tolerance = 0.8 pitch_o = pitch("yin", win_s, hop_s, samplerate) pitch_o.set_unit("midi") pitch_o.set_tolerance(tolerance) pitchesYIN = [] confidences = [] total_frames = 0 while True: samples, read = s() pitch = pitch_o(samples)[0] pitch = int(round(pitch)) confidence = pitch_o.get_confidence() pitchesYIN += [pitch] confidences += [confidence] total_frames += read if read &lt; hop_s: break # load PRAAT pitches praat = np.genfromtxt('/home/eva/Documents/papers/habr/PraatPitch.txt', filling_values=0) praat = praat[:,1] # plot fig, (ax1,ax2,ax3) = plt.subplots(3, 1, sharex=True, sharey=True, figsize=(12, 8)) ax1.plot(np.asarray(pitchesYIN), label='YIN', color='green') ax1.legend(loc="upper right") ax2.plot(pitchY.samp_values, label='YAAPT', color='blue') ax2.legend(loc="upper right") ax3.plot(praat, label='Praat', color='red') ax3.legend(loc="upper right") plt.show()</span></span></code> </pre> <br></div></div><br><br><img src="https://habrastorage.org/webt/tv/k7/uq/tvk7uqi50ctcnd3j3rjq0sjzl8s.png"><br>  <i>Figura 4. Comparaci√≥n del funcionamiento de los algoritmos YIN, YAAPT y Praat.</i> <br><br>  Vemos que con los par√°metros predeterminados, YIN est√° bastante noqueado, obteniendo una trayectoria muy plana con valores m√°s bajos que Praat y perdiendo por completo las transiciones entre las voces masculinas y femeninas, as√≠ como entre el habla emocional y no emocional. <br><br>  YAAPT recort√≥ un tono muy alto en el discurso emocional femenino, pero en general se las arregl√≥ claramente mejor.  Debido a sus caracter√≠sticas espec√≠ficas, YAAPT funciona mejor: es imposible responder de inmediato, por supuesto, pero se puede suponer que el papel se juega al obtener candidatos de tres fuentes y un c√°lculo m√°s meticuloso de su peso que en YIN. <br><br>  <b>Conclusi√≥n</b> <br><br>  Dado que la cuesti√≥n de determinar la frecuencia del tono fundamental (F0) de una forma u otra surge antes que casi todos los que trabajan con sonido, hay muchas maneras de resolverlo.  La cuesti√≥n de la precisi√≥n y las caracter√≠sticas necesarias del material de audio en cada caso determinan cu√°n cuidadosamente es necesario seleccionar los par√°metros, o en otro caso, puede restringirse a una soluci√≥n b√°sica como YAAPT.  Tomando Praat como el est√°ndar del algoritmo para el procesamiento del habla (sin embargo, un gran n√∫mero de investigadores lo usa), podemos concluir que YAAPT es, en primera aproximaci√≥n, m√°s confiable y preciso que YIN, aunque nuestro ejemplo result√≥ ser complicado para √©l. <br><br>  Publicado por <b>Eva Kazimirova</b> , investigadora del laboratorio de Neurodata, especialista en procesamiento del habla. <br><br>  <font color="green"><b>Offtop</b></font> : ¬øTe gusta el art√≠culo?  De hecho, tenemos muchas tareas interesantes en ML, matem√°ticas y programaci√≥n, y necesitamos cerebros.  ¬øEst√°s interesado en esto?  Ven a nosotros!  Correo electr√≥nico: hr@neurodatalab.com <br><br><div class="spoiler">  <b class="spoiler_title">Referencias</b> <div class="spoiler_text"><ol><li>  Rusz, J., Cmejla, R., Ruzickova, H., Ruzicka, E. Mediciones ac√∫sticas cuantitativas para la caracterizaci√≥n de trastornos del habla y la voz en la enfermedad de Parkinson temprana no tratada.  The Journal of the Acoustical Society of America, vol.  129, n√∫mero 1 (2011), pp.  350-367.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Acceso</a> </li><li>  Farr√∫s, M., Hernando, J., Ejarque, P. Jitter y Shimmer Measurements for Speaker Recognition.  Actas de la Conferencia Anual de la International Speech Communication Association, INTERSPEECH, vol.  2 (2007), pp.  1153-1156.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Acceso</a> </li><li>  Zahorian, S., Hu, HA.  M√©todo espectral / temporal para un seguimiento de frecuencia fundamental robusto.  The Journal of the Acoustical Society of America, vol.  123, n√∫mero 6 (2008), pp.  4559-4571.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Acceso</a> </li><li>  De Cheveign√©, A., Kawahara, H. YIN, un estimador de frecuencia fundamental para el habla y la m√∫sica.  The Journal of the Acoustical Society of America, vol.  111, n√∫mero 4 (2002), pp.  1917-1930.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Acceso</a> </li></ol></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es416441/">https://habr.com/ru/post/es416441/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es416431/index.html">Los mejores proyectos blockchain. ICO julio de 2018 (votaci√≥n)</a></li>
<li><a href="../es416433/index.html">Preg√∫ntele a Ethan: ¬øPueden las p√©rdidas de radiaci√≥n estelar explicar la energ√≠a oscura?</a></li>
<li><a href="../es416435/index.html">¬øPor qu√© es tan efectivo el cerebro humano?</a></li>
<li><a href="../es416437/index.html">¬øHay suficientes productos qu√≠micos en los mundos helados para mantener la vida all√≠?</a></li>
<li><a href="../es416439/index.html">iOS 12: agrupaci√≥n de notificaciones</a></li>
<li><a href="../es416443/index.html">9 secretos de ASP.NET Core</a></li>
<li><a href="../es416445/index.html">Seminarios web de Skillbox: los m√°s interesantes, gratis</a></li>
<li><a href="../es416449/index.html">.NET Core + Docker en Raspberry Pi. ¬øEs esto legal?</a></li>
<li><a href="../es416451/index.html">Las bases de datos de Microsoft Research ahora est√°n disponibles para todos</a></li>
<li><a href="../es416453/index.html">Esquemas de robo en sistemas RBS y cinco niveles de contrarrestarlos</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>