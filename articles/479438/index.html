<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143967986-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-143967986-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üêä üñãÔ∏è ‚óæÔ∏è Postgres-Tuesday # 5: ‚ÄúPostgreSQL and Kubernetes. CI / CD. Test Automation ¬ª ‚è™ üòê üêè</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="At the end of last year, another live broadcast of the Russian PostgreSQL community #RuPostgres took place , during which its co-founder Nikolai Samok...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Postgres-Tuesday # 5: ‚ÄúPostgreSQL and Kubernetes. CI / CD. Test Automation ¬ª</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/479438/"><img src="https://habrastorage.org/webt/qm/rm/ln/qmrmlnm8gjj_8gih4dzhy2ybrvy.jpeg"><br><br>  At the end of last year, another live broadcast of the Russian PostgreSQL community <a href="https://www.meetup.com/postgresqlrussia/">#RuPostgres took place</a> , during which its co-founder Nikolai Samokhvalov talked with Flanta technical director Dmitry Stolyarov about this DBMS in the context of Kubernetes. <br><br>  We're posting a transcript of the bulk of this discussion, and a full video has been posted <a href="https://www.youtube.com/channel/UC0SBGSNmBLrTZIkbN-lJHnw">on the community‚Äôs YouTube channel</a> : <a name="habracut"></a><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/qXc9VTr4TFc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h2>  Databases and Kubernetes </h2><br>  <i><b>NS</b> : We will not talk about VACUUM and CHECKPOINTs today.</i>  <i>We want to talk about Kubernetes.</i>  <i>I know that you have many years of experience.</i>  <i>I watched your videos and even reviewed some of them ... Let's go straight to the quarry: why is Postgres or MySQL in K8s at all?</i> <br><br>  <b>DS</b> : There is no single answer to this question and it cannot be.  But in general, it is simplicity and convenience ... potential.  After all, everyone wants managed services. <br><br>  <i><b>NS</b> : To like <a href="https://aws.amazon.com/rds/">RDS</a> , only at home?</i> <br><br>  <b>DS</b> : Yes: to like RDS, only anywhere. <br><br>  <i><b>NS</b> : ‚ÄúAnywhere‚Äù is a good point.</i>  <i>In large companies, everything is located in different places.</i>  <i>And why then, if this is a big company, do not take a ready-made solution?</i>  <i>For example, Nutanix has its own developments, while other companies (VMware ...) have the same ‚ÄúRDS, only at home‚Äù.</i> <br><br>  <b>DS</b> : But we are talking about a single implementation that will work only under certain conditions.  And if we are talking about Kubernetes, then there is a huge variety of infrastructure (which can be in K8s).  This is essentially the standard for the API to the cloud ... <br><br>  <i><b>NS</b> : It‚Äôs free too!</i> <br><br>  <b>DS</b> : This is not so important.  Free is not important for a very large segment of the market.  Another thing is important ... You probably recall the report " <a href="https://habr.com/ru/company/flant/blog/431500/">Databases and Kubernetes</a> "? <br><br>  <i><b>NS</b> : Yes.</i> <br><br>  <b>DS</b> : I realized that he was perceived very ambiguously.  Some people thought that I was saying: ‚ÄúGuys, we went all the databases to Kubernetes!‚Äù, While others decided that they were all terrible bicycles.  And I wanted to say something else altogether: ‚ÄúLook at what is happening, what are the problems and how they can be solved.  Now go bases in Kubernetes?  Production?  Well, only if you love ... doing certain things.  But for dev, I can say that I recommend it.  For dev, dynamically creating / deleting environments is very important. ‚Äù <br><br>  <i>NS: By dev, do you mean all environments that are not prod?</i>  <i>Staging, QA ...</i> <br><br>  <b>DS</b> : If we are talking about perf stands, then probably not already, because the requirements are specific there.  If we are talking about special cases where a very large database is needed on staging, then probably not too ... If this is a static environment, long-lived, then what is the benefit of having the base located in K8s? <br><br>  <i><b>NS</b> : None.</i>  <i>But where do we see static environments?</i>  <i>The static environment is outdated tomorrow.</i> <br><br>  <b>DS</b> : Staging can be static.  We have clients ... <br><br>  <i><b>NS</b> : Yes, I also have.</i>  <i>The big problem is if you have a base of 10 TB and staging - 200 GB ...</i> <br><br>  <b>DS</b> : I have a very cool case!  On staging there is a prod'ovy base in which changes are made.  And a button is provided: "roll out to production".  These changes - deltas - are added (it seems, they are just synchronized by API's) in production.  This is a very exotic option. <br><br>  <i><b>NS</b> : I saw startups in the Valley who are sitting in RDS, or even in Heroku yet - these are stories of 2-3 years ago - and they download the dump to their laptop.</i>  <i>Because the base is only 80 GB so far, and there is a place on the laptop.</i>  <i>Then they buy disks for everyone, so that they have 3 bases, so that they can carry out different developments.</i>  <i>This happens too.</i>  <i>I also saw that they are not afraid to copy prod into staging - it depends very much on the company.</i>  <i>But he saw that they were very afraid, and that often they did not have enough time and hands.</i>  <i>But before we move on to this topic, I want to hear about Kubernetes.</i>  <i>I understand correctly that in prod'e so far no one?</i> <br><br>  <b>DS</b> : We have small bases in prod.  We are talking about volumes of tens of gigabytes and non-critical services, for which it was too lazy to make replicas (and there is no such need).  And provided that under Kubernetes there is a normal storage.  This database worked in a virtual machine - conditionally in VMware, on top of storage.  We placed it in <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PV</a> and now we can transfer it from car to car. <br><br>  <i><b>NS</b> : Bases of this size, up to 100 GB, on good disks and with a good network can be rolled out in a few minutes, right?</i>  <i>A speed of 1 GB per second is no longer exotic.</i> <br><br>  <b>DS</b> : Yes, for a linear operation this is not a problem. <br><br>  <i><b>NS</b> : Okay, we should only think about prod.</i>  <i>And if we consider Kubernetes for non-prod environments - how to do it?</i>  <i>I see that in Zalando <a href="https://github.com/zalando/postgres-operator">they are making an operator</a> , in Crunchy they are <a href="https://github.com/CrunchyData/postgres-operator">sawing</a> , there are some other options.</i>  <i>And there is <a href="https://ongres.com/">OnGres</a> - this is our good friend Alvaro from Spain: in fact, they do not just an <a href="https://habr.com/ru/company/flant/blog/326414/">operator</a> , but a whole distribution ( <a href="https://gitlab.com/ongresinc/stackgres">StackGres</a> ), in which, in addition to Postgres itself, they also decided to stuff the backup, Envoy proxy ...</i> <br><br>  <b>DS</b> : Envoy for what?  Postgres traffic balancing exactly? <br><br>  <i><b>NS</b> : Yes.</i>  <i>That is, they see it as: if you take the Linux distribution and the kernel, then the usual PostgreSQL is the kernel, and they want to make a distribution that is cloud-friendly and runs on Kubernetes.</i>  <i>They dock components (backups, etc.) and debug so that they work well.</i> <br><br>  <b>DS</b> : Very cool!  In essence, it is software to make your managed Postgres. <br><br>  <i><b>NS</b> : Linux distributions have eternal problems: how to make drivers so that all hardware is supported.</i>  <i>And they have the idea that they will work at Kubernetes.</i>  <i>I know that in the Zalando operator we recently saw the eyeballs on AWS and this is not very good.</i>  <i>There should not be ties to a specific infrastructure - what is the point then?</i> <br><br>  <b>DS</b> : I don‚Äôt know in what specific situation Zalando got involved, but in Kubernetes storage is now made in such a way that it is impossible to remove a disk backup in a generic way.  Recently, the standard - in the latest version of <a href="https://habr.com/ru/company/flant/blog/465417/">the CSI specification</a> - made the possibility of snapshots, but where is it implemented?  Honestly, it‚Äôs still so raw ... We are trying CSI on top of AWS, GCE, Azure, vSphere, but we are starting to use it a bit, as you can see that it is not ready yet. <br><br>  <i><b>NS</b> : Therefore, sometimes you have to tie up with infrastructure.</i>  <i>I think this is still an early stage - growth problems.</i>  <i>Question: what would you recommend to beginners who want to try PgSQL in K8s?</i>  <i>Which operator maybe?</i> <br><br>  <b>DS</b> : The problem is that for us Postgres is 3%.  We still have a very large list of different software in Kubernetes, I won‚Äôt even list everything.  For example, Elasticsearch.  There are a lot of operators: some are developing actively, others are not.  For ourselves, we made requirements that should be in the operator, so that we take him seriously.  The operator is specifically for Kubernetes - not the ‚Äúoperator to do something under Amazon‚Äôs conditions ..." In fact, we use a single operator quite massly (= for almost all clients) - <a href="https://github.com/spotahome/redis-operator">for Redis</a> <i>(we will publish an article about it soon)</i> . <br><br>  <i><b>NS</b> : But for MySQL, too?</i>  <i>I know that Percona ... since they are now involved in MySQL, MongoDB, and Postgres, they will have to fix some kind of universal one: for all databases, for all cloud providers.</i> <br><br>  <b>DS</b> : We did not have time to look at the statements for MySQL.  For us, this is not the main focus now.  MySQL works fine in standalone.  Why an operator, if you can just start the database ... You can start the Docker container with Postrges, or you can start it in a simple way. <br><br>  <i><b>NS</b> : This was also a question.</i>  <i>No operator at all?</i> <br><br>  <b>DS</b> : Yes, 100% of us have PostgreSQL running without an operator.  So far so.  We actively use the operator for Prometheus, for Redis.  We have plans to find an operator for Elasticsearch - it burns the most because we want to install it in 100% of cases in Kubernetes.  Just as we want to ensure that MongoDB is always installed in Kubernetes too.  Certain Wishlist appear here - there is a feeling that in these cases something can be done.  And about Postgres we did not even look.  Of course, we know about the existence of different options, but in fact we have standalone. <br><br><h2>  Testing Database in Kubernetes </h2><br>  <i><b>NS</b> : Let's move on to the topic of testing.</i>  <i>How to roll out changes in the database - from the point of view of the DevOps perspective.</i>  <i>There are microservices, many databases, all the time somewhere something is changing.</i>  <i>How to ensure normal CI / CD so that everything is in order from the DBMS position.</i>  <i>What is your approach?</i> <br><br>  <b>DS</b> : There can be no one answer.  There are several options.  The first is the size of the base we want to roll out.  You yourself mentioned that companies have a different attitude to having a copy of the prod base on dev and stage. <br><br>  <i><b>NS</b> : And in terms of GDPR, I think they are more and more neat ... I can say that in Europe they have already started to fine.</i> <br><br>  <b>DS</b> : But you can often write software that dumps production and obfuscates it.  It turns out prod'ovye data (snapshot, dump, binary copy ...), but they are anonymous.  Instead, there may be generation scripts: it can be fixtures or just a script that generates a large database.  The problem is what: how long does the base image take to be created?  And how much time to deploy it on the right environment? <br><br>  We came to the scheme: if the client has a fixture dataset (minimum version of the database), then by default we use them.  If we are talking about review environments, when we created branch, we have deployed an application instance - we are rolling out a small database there.  But the <a href="https://habr.com/ru/company/flant/blog/417509/">option</a> turned out well, when we remove the dump from production once a day (at night) and collect on its basis a Docker container with PostgreSQL and MySQL with these loaded data.  If you need to deploy the base 50 times from this image, this is done quite simply and quickly. <br><br>  <i><b>NS</b> : Simple copying?</i> <br><br>  <b>DS</b> : Data is stored directly in the Docker image.  Those.  we have a ready-made image, albeit 100 GB.  Thanks to the layers in Docker, we can quickly deploy this image as many times as needed.  The method is dumb, but it works pretty well. <br><br>  <i><b>NS</b> : Further, when testing, it changes right inside the Docker, right?</i>  <i>Copy-on-write inside Docker - throw it away and go again, everything is fine.</i>  <i>Class!</i>  <i>And you already use it with might and main?</i> <br><br>  <b>DS</b> : For a long time. <br><br>  <i><b>NS</b> : We do very similar things.</i>  <i>Only we do not use Docker's copy-on-write, but some more.</i> <br><br>  <b>JS</b> : He's not generic.  And Docker'ny works everywhere. <br><br>  <i><b>NS</b> : In theory, yes.</i>  <i>But we also have modules there, you can make different modules and work with different file systems.</i>  <i>What a moment.</i>  <i>From Postgres, we look at all this differently.</i>  <i>Now I looked from the side of Docker and saw that everything works for you.</i>  <i>But if the database is huge, for example, 1 TB, then this is all long: both operations at night and stuff everything in Docker ... And if 5 TB is stuffed in Docker ... Or is everything normal?</i> <br><br>  <b>DS</b> : What difference does it make: it's blobs, just bits and bytes. <br><br>  <i><b>NS</b> : The difference is this: do you do this through dump and restore?</i> <br><br>  <b>DS</b> : Not at all necessary.  The methods for generating this image can be different. <br><br>  <i><b>NS</b> : For some clients, we have made it so that instead of regularly generating a basic image, we constantly keep it up to date.</i>  <i>It is essentially a replica, but the data is not received directly from the master, but through the archive.</i>  <i>The binary archive where the WALs are rolled every day, the backups are also removed there ... These WALs then fly - with a slight delay (literally 1-2 seconds) - to the base image.</i>  <i>We clone it in any way - now we have ZFS by default.</i> <br><br>  <b>DS</b> : But with ZFS you are limited to one node. <br><br>  <i><b>NS</b> : Yes.</i>  <i>But ZFS also has a magical <a href="https://docs.oracle.com/cd/E18752_01/html/819-5461/gbchx.html">send</a> : you can send a snapshot with it and even (I haven‚Äôt really tested it yet, but ...) you can send a delta between two <code>PGDATA</code> .</i>  <i>In fact, we have another tool that we did not particularly consider for such tasks.</i>  <i>PostgreSQL has <a href="https://www.postgresql.org/docs/12/app-pgrewind.html">pg_rewind</a> , which works as a ‚Äúsmart‚Äù rsync, skipping a lot of things that you don‚Äôt have to watch, because nothing has changed there for sure.</i>  <i>We can do a quick synchronization between the two servers and rewind in exactly the same way.</i> <br><br>  <i>So, we are trying on this, more DBA'noy, side to make a tool that allows you to do the same thing that you said: we have one base, but we want to test something 50 times, almost at the same time.</i> <br><br>  <b>DS</b> : 50 times means you need to order 50 Spot instances. <br><br>  <i><b>NS</b> : No, we do everything on one machine.</i> <br><br>  <b>DS</b> : But how do you deploy 50 times if this one base is, say, a terabyte.  Most likely she needs conditionally 256 GB of RAM? <br><br>  <i><b>NS</b> : Yes, sometimes a lot of memory is needed - this is normal.</i>  <i>But such an example from life.</i>  <i>The production machine has 96 cores and 600 GB.</i>  <i>At the same time, 32 cores are used for the database (even 16 cores are sometimes now used) and 100-120 GB of memory.</i> <br><br>  <b>DS</b> : And 50 copies get in there? <br><br>  <i><b>NS</b> : So there is only one copy, then copy-on-write (ZFS'ny) works ... I'll tell you more.</i> <br><br>  <i>For example, we have a base of 10 TB.</i>  <i>They made a disk for it, ZFS still squeezed its percent size by 30-40.</i>  <i>Since we do not do load testing, the exact response time is not important to us: let it be up to 2 times slower - that's okay.</i> <br><br>  <i>We enable programmers, QA, DBA, etc.</i>  <i>Perform testing in 1-2 threads.</i>  <i>For example, they can start some kind of migration.</i>  <i>It does not require 10 cores at once - it needs 1 Postgres backend, 1 core.</i>  <i>Migration will start - maybe <a href="https://www.postgresql.org/docs/12/routine-vacuuming.html">autovacuum will</a> still start, then the second core is activated.</i>  <i>We have allocated 16-32 cores, so 10 people can work simultaneously, there are no problems.</i> <br><br>  <i>Since <code>PGDATA</code> physically the same, it turns out that we are actually fooling Postgres.</i>  <i>The trick is this: it starts, for example, 10 Postgres at the same time.</i>  <i>What problem is usually what?</i>  <i>They <a href="https://www.postgresql.org/docs/current/runtime-config-resource.html">put shared_buffers</a> , say, at 25%.</i>  <i>Accordingly, this is 200 GB.</i>  <i>You won‚Äôt start more than three of them, because the memory will end.</i> <br><br>  <i>But at some point we realized that this was not necessary: ‚Äã‚Äãwe set shared_buffers to 2 GB.</i>  <i>PostgreSQL has <a href="https://www.postgresql.org/docs/current/runtime-config-query.html">effective_cache_size</a> , and in reality only it affects <a href="https://en.wikipedia.org/wiki/Query_plan">plans</a> .</i>  <i>We put it at 0.5 Tb.</i>  <i>And it doesn‚Äôt even matter that they aren‚Äôt really there: he makes plans as if they are.</i> <br><br>  <i>Accordingly, when we test some kind of migration, we can collect all the plans - we will see how it will happen in production.</i>  <i>The seconds there will be different (slower), but the data that we actually read, and the plans themselves (what kind of JOINs, etc.) are obtained exactly the same as on production.</i>  <i>And in parallel, you can run many of these checks on one machine.</i> <br><br>  <b>DS</b> : Do you think that there are several problems?  The first is a solution that works only on PostgreSQL.  This approach is very private, it is not generic.  The second - Kubernetes (and that's where the cloud is going now) involves a lot of nodes, and these nodes are ephemeral.  And in your case it is a stateful, persistent node.  These things contradict me. <br><br>  <i><b>NS</b> : First - I agree, this is a purely Postgres story.</i>  <i>I think if we have any direct IO and a buffer pool for almost all memory, this approach will not work - there will be different plans.</i>  <i>But we are only working with Postgres for now, we don‚Äôt think about others.</i> <br><br>  <i>About Kubernetes.</i>  <i>You yourself always say that we have a persistent base.</i>  <i>If the instance crashes, the main thing is to save the disk.</i>  <i>Here we also have the entire platform in Kubernetes, and the component with Postgres is separate (although it will be there someday).</i>  <i>Therefore, everything is so: the instance fell, but we saved it PV and just connected to another (new) instance, as if nothing had happened.</i> <br><br>  <b>DS</b> : From my point of view, we create pods in Kubernetes.  K8s - elastic: components are ordered on their own as needed.  The task is to simply create a pod and say that it needs X resources, and then K8s will figure it out.  But the storage support in Kubernetes is still unstable: in <a href="https://habr.com/ru/company/flant/blog/467477/">1.16</a> , in <a href="https://habr.com/ru/company/flant/blog/476998/">1.17</a> (this release was released <i>weeks</i> ago), these features become only beta. <br><br>  Six months or a year will pass - it will become more or less stable, or at least will be declared as such.  Then the possibility of snapshots and resize'a already solves your problem completely.  Because you have a base.  Yes, it may not be very fast, but the speed depends on what is ‚Äúunder the hood,‚Äù because some implementations can copy and copy-on-write at the level of the disk subsystem. <br><br>  <i><b>NS</b> : It‚Äôs also necessary for all the engines (Amazon, Google ...) to start supporting this version - it also takes some time.</i> <br><br>  <b>DS</b> : While we do not use them.  We use ours. <br><br><h2>  Local development under Kubernetes </h2><br>  <i><b>NS</b> : Have you encountered such a Wishlist when you need to raise all the pods on one machine and do such a little testing.</i>  <i>In order to quickly get a proof of concept, see that the application works in Kubernetes, without allocating a bunch of machines for it.</i>  <i>Is there a Minikube, right?</i> <br><br>  <b>DS</b> : It seems to me that this case - deploy on one node - is exclusively about local development.  Or some manifestations of such a pattern.  There is <a href="https://habr.com/ru/company/flant/blog/333470/">Minikube</a> , there are <a href="https://k3s.io/">k3s</a> , <a href="https://github.com/kubernetes-sigs/kind">KIND</a> .  We are going to use Kubernetes IN Docker.  Now they started working with him for tests. <br><br>  <i><b>NS</b> : I used to think that this is an attempt to wrap all pods in one Docker image.</i>  <i>But it turned out that this is about something else.</i>  <i>Anyway, there are separate containers, separate pods - just in the Docker.</i> <br><br>  <b>DS</b> : Yes.  And there a rather funny imitation is done, but the point is ... We have a deployment tool - <a href="https://werf.io/">werf</a> .  We want to make a mode in it - conditionally <code>werf up</code> : ‚ÄúRaise me a local Kubernetes‚Äù.  And then run the conditional <code>werf follow</code> .  Then the developer will be able to edit in the IDE, and a process is launched in the system that sees the changes and reassembles the images, remodels them into the local K8s.  So we want to try to solve the problem of local development. <br><br><h2>  Snapshots and database cloning in the realities of K8s </h2><br>  <i><b>NS</b> : If you go back to copy-on-write.</i>  <i>I noticed that the clouds also have snapshots.</i>  <i>They work differently.</i>  <i>For example, in GCP: you have a multi-terabyte instance on the east coast of the USA.</i>  <i>You do periodically snapshots.</i>  <i>You pick up a disk copy on the west coast from a snapshot - in a few minutes everything is ready, it works very quickly, only the cache needs to be filled in memory.</i>  <i>But these clones (snapshots) - in order to 'provision'it a new volume.</i>  <i>This is great when you need to create many instances.</i> <br><br>  <i>But for the tests, it seems to me, snapshots that you talk about in Docker or I talk about in ZFS, btrfs and even LVM ... - they allow you to not make really new data on the same machine.</i>  <i>In the cloud, you still have to pay for them each time and wait not minutes, but minutes (and in the case of a <a href="https://aws.amazon.com/about-aws/whats-new/2019/11/amazon-ebs-fast-snapshot-restore-eliminates-need-for-prewarming-data-into-volumes-created-snapshots/">lazy load</a> , it‚Äôs probably hours).</i> <br><br>  <i>Instead, you can get this data in a second or two, drive the test and throw it away.</i>  <i>These snapshots solve different problems.</i>  <i>In the first case - to scale and get new replicas, and in the second - for tests.</i> <br><br>  <b>DS</b> : I do not agree.  Cloning volumes normally is the task of the cloud.  I did not watch their implementation, but I know how we do it on hardware.  We have Ceph, in it you can tell any physical volume ( <a href="https://docs.ceph.com/docs/master/rbd/">RBD</a> ) to <i>clone</i> and get a second volume with the same characteristics, <a href="https://en.wikipedia.org/wiki/IOPS">IOPSs</a> , etc., in tens of milliseconds.  You have to understand that there is a tricky copy-on-write inside.  Why doesn't the cloud do the same?  I am sure that they are somehow trying to do this. <br><br>  <i><b>NS</b> : But they will still take seconds, tens of seconds to raise the instance, bring Docker there, etc.</i> <br><br>  <b>DS</b> : Why is it necessary to raise an entire instance?  But we have an instance for 32 cores, for 16 ... and it somehow fits into it - for example, four.  When we order the fifth, the instance will rise, and then it will be deleted. <br><br>  <i><b>NS</b> : Yes, interestingly, Kubernetes has a different story.</i>  <i>Our database is not in K8s, and one instance.</i>  <i>But cloning a multi-terabyte database takes no more than two seconds.</i> <br><br>  <b>DS</b> : That's cool.  But my initial message is that this is not a generic solution.  Yes, it‚Äôs cool, but only Postgres is suitable and only on one node. <br><br>  <i><b>NS</b> : It is suitable not only for Postgres: these plans, as I described, will work only in that way.</i>  <i>But if you don‚Äôt bother with the plans, but we just need all the data for functional testing, then this is suitable for any DBMS.</i> <br><br>  <b>DS</b> : Many years ago we did this on LVM snapshots.  This is a classic.  This approach has been very actively used.  Just stateful nodes are a pain.  Because they need not be dropped, always remember about them ... <br><br>  <i><b>NS</b> : Do you see any hybrid possibility here?</i>  <i>Let's say stateful is some kind of pod, it works for several people (many testers).</i>  <i>We have one volume, but thanks to the file system, the clones are local.</i>  <i>If the pod falls, the disk remains ‚Äî the pod rises, it considers the information about all the clones, takes everything back and says: ‚ÄúHere are your clones on these ports, start working with them further.‚Äù</i> <br><br>  <b>DS</b> : Technically, this means that within Kubernetes, this is one pod, inside which we run many Postgres. <br><br>  <i><b>NS</b> : Yes.</i>  <i>He has a limit: suppose, at the same time, no more than 10 people work with him.</i>  <i>If you need 20 - run the second such pod.</i>  <i>Fully realistically clone it, having received the second full volume, it will have the same 10 ‚Äúthin‚Äù clones.</i>  <i>Do not see such an opportunity?</i> <br><br>  <b>DS</b> : We have to add security issues here.  Such an organization option implies that this pod has high capabilities because it can perform non-standard operations on the file system ... But I repeat: I believe that in the medium term, storage will be fixed in Kubernetes, the whole story with volumes will be fixed in the clouds - everything will be "just working."  It will resize, cloning ... There is a volume - we say: ‚ÄúCreate a new one on the basis of that‚Äù - and after a second and a half we get what we need. <br><br>  <i><b>NS</b> : I do not believe in one and a half seconds for many terabytes.</i>  <i>At Ceph, you do it yourself, and you talk about clouds.</i>  <i>Go to the cloud, on EC2, make a clone of the EBS volume of many terabytes and see what performance will be.</i>  <i>It does not take a few seconds.</i>  <i>I am very interested when they reach such an indicator.</i>  <i>I understand what you're talking about, but let me disagree.</i> <br><br>  <b>DS</b> : Ok, but I said that in the medium term, not short term.  For several years. <br><br><h2>  Pro operator for PostgreSQL from Zalando </h2><br>  In the middle of this meeting, Alexey Klyukin, a former developer from Zalando, who spoke about the history of the PostgreSQL operator, also joined her: <br><br><blockquote>  It‚Äôs great that in general this topic was touched upon: both Postgres and Kubernetes.  When we started doing it in Zalando in 2017, it was such a topic that everyone wanted to do, but no one did.  Everyone already had Kubernetes, but when asked what to do with the databases, even people like <a href="https://github.com/kelseyhightower">Kelsey Hightower</a> who preached K8s said something like this: <br><br>  <i>‚ÄúGo to managed services and use them; do not start the database in Kubernetes.</i>  <i>Otherwise, your K8s will decide, for example, to upgrade, put out all the nodes, and your data will fly far, far away. "</i> <br><br>  We decided to make an operator that, contrary to this advice, will launch the Postgres database in Kubernetes.  And we had a good foundation - <a href="https://github.com/zalando/patroni">Patroni</a> .  This is an automatic failover for PostgreSQL, done correctly, i.e.  using etcd, consul or ZooKeeper as a repository for cluster information.  Such a repository that will be given to everyone who asks, for example, what kind of leader is now, the same information - despite the fact that we have everything distributed - so that there is no split brain.  Plus, we had a <a href="https://github.com/zalando/patroni/tree/master/docker">Docker image</a> for him. <br><br>  In general, the need for auto failover in the company appeared after the migration from the internal iron data center to the cloud.  The cloud was based on PaaS (Platform-as-a-Service) proprietary solution.  It is Open Source, but to raise it, you had to work hard.  It was called <a href="https://stups.io/">STUPS</a> . <br><br>  Initially, there was no Kubernetes.  More precisely, when its own solution was deployed, K8s was already, but so crude that it was not suitable for production.  It was, in my opinion, 2015 or 2016.  By 2017, Kubernetes became more or less mature - there was a need to migrate there. <br><br>  And we already had a docker container.  There was PaaS that used Docker.  Why not try K8s?  Why not write your own statement?  Murat Kabilov, who came to us from Avito, started this as a project on his own initiative - ‚Äúplay‚Äù - and the project ‚Äútook off‚Äù. <br><br>  But in general, I wanted to talk about AWS.  Why was there historically AWS related code ... <br><br>  When you run something in Kubernetes, you need to understand that K8s is such a work in progress.  It is constantly developing, improving and periodically even breaking.  You need to carefully monitor all the changes in Kubernetes, you need to be prepared to immerse yourself in it, and find out how it works in detail - perhaps more than you would like.  This is, in principle, any platform on which you run your databases ... <br><br>  So, when we did the statement, we had Postgres, which worked with an external volume (in this case, EBS, since we worked in AWS).  The database was growing, at some point it was necessary to resize: for example, the original size of EBS is 100 TB, the database has grown to it, now we want to make EBS in 200 TB.  How?  Suppose you can dump / restore to a new instance, but this is long and with downtime. <br><br>  Therefore, I wanted a resize that would expand the EBS partition and then tell the file system to use the new space.  And we did it, but at that time Kubernetes had no API for the resize operation.  Since we worked on AWS, we wrote code for its API. <br><br>  No one bothers to do the same for other platforms.  There is no complication in the statement that it can only be run on AWS, and it won‚Äôt work on everything else.  In general, this is an Open Source project: if anyone wants to accelerate the emergence of the use of the new API, we are welcome.  There is <a href="https://github.com/zalando/postgres-operator">GitHub</a> , pull-requests - the Zalando team is trying to quickly respond to them and promote the operator.  As far as I know, the project <a href="https://summerofcode.withgoogle.com/archive/2019/organizations/6187982082539520/">participated</a> in Google Summer of Code and some other similar initiatives.  Zalando is very active on it. <br></blockquote><br><h2>  PS Bonus! </h2><br>  If you are interested in the topic of PostgreSQL and Kubernetes, then we also draw attention to the fact that last week the next Postgres took place, where <b>Alexander Kukushkin from Zalando</b> talked with Nikolai.  Video from it is available <a href="https://www.youtube.com/watch%3Fv%3DFE0xi7SBqsg">here</a> . <br><br><h2>  PPS </h2><br>  Read also in our blog: <br><br><ul><li>  ‚Äú <a href="https://habr.com/ru/company/flant/blog/431500/">Databases and Kubernetes (review and video report)</a> ‚Äù; </li><li>  ‚Äú <a href="https://habr.com/ru/company/flant/blog/475036/">Cassandra Migration to Kubernetes: Features and Solutions</a> ‚Äù; </li><li>  ‚Äú <a href="https://habr.com/ru/company/flant/blog/461149/">Unobstructed migration of MongoDB to Kubernetes</a> ‚Äù; </li><li>  " <a href="https://habr.com/ru/company/flant/blog/450662/">Unobstructed RabbitMQ migration to Kubernetes</a> ." </li></ul></div></div><p>Source: <a href="https://habr.com/ru/post/479438/">https://habr.com/ru/post/479438/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../479422/index.html">[Video animation] Wired world: how in 35 years a network of submarine cables enveloped the globe</a></li>
<li><a href="../479426/index.html">Security Week 50: Man-in-the-middle attacks in Confluence and Linux</a></li>
<li><a href="../479428/index.html">Digital events in Moscow from December 9 to 15</a></li>
<li><a href="../479430/index.html">Digital events in St. Petersburg from December 9 to 15</a></li>
<li><a href="../479432/index.html">Yandex.Maps: I went to the card controller - I immediately got the user's position (okay, now seriously)</a></li>
<li><a href="../479442/index.html">Alexey Savvateev: Game-theoretic model of social schism (+ nginx survey)</a></li>
<li><a href="../479446/index.html">Cars are already ahead of people in reading tests; but do they understand what they read?</a></li>
<li><a href="../479450/index.html">AppCode 2019.3: works faster, understands Swift better, knows about Mac Catalyst, conveniently displays assembly messages</a></li>
<li><a href="../479452/index.html">How the Domain Name System Developed: The ARPANET Era</a></li>
<li><a href="../479458/index.html">Beauty or practicality in the server room</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter54458986 = new Ya.Metrika({
                  id:54458986,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/54458986" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-143967986-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=IU0EG0jaqnehka2lu5TyzAcchrZXI4Yb1QXKQvJxpqE&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>