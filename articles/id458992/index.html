<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏼‍⚕️ 👕 📒 Perhatian untuk boneka dan implementasi di Keras 👎🏻 📚 👆🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Tentang artikel tentang kecerdasan buatan dalam bahasa Rusia 
 Terlepas dari kenyataan bahwa mekanisme Perhatian dijelaskan dalam literatur bahasa Ing...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Perhatian untuk boneka dan implementasi di Keras</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/458992/"><h2>  Tentang artikel tentang kecerdasan buatan dalam bahasa Rusia </h2><br>  Terlepas dari kenyataan bahwa mekanisme Perhatian dijelaskan dalam literatur bahasa Inggris, saya masih belum melihat deskripsi yang layak tentang teknologi ini di sektor berbahasa Rusia.  Ada banyak artikel tentang Kecerdasan Buatan (AI) dalam bahasa kita.  Namun, artikel-artikel yang ditemukan hanya mengungkapkan model AI paling sederhana, misalnya, jaringan konvolusi, jaringan generatif.  Namun, menurut perkembangan terkini yang mutakhir di bidang AI, ada sangat sedikit artikel di sektor berbahasa Rusia. <br><br><a name="habracut"></a>  Kurangnya artikel dalam bahasa Rusia tentang perkembangan terakhir menjadi masalah bagi saya ketika saya memasuki topik tersebut, mempelajari keadaan terkini di bidang AI.  Saya tahu bahasa Inggris dengan baik, saya membaca artikel dalam bahasa Inggris tentang topik AI.  Namun, ketika konsep baru atau prinsip baru AI keluar, pemahamannya dalam bahasa asing menyakitkan dan panjang.  Mengetahui Bahasa Inggris, untuk menembus ke dalam non-pribumi dalam objek yang kompleks masih bernilai lebih banyak waktu dan usaha.  Setelah membaca deskripsi, Anda bertanya pada diri sendiri pertanyaan: berapa persen yang Anda mengerti?  Jika ada artikel dalam bahasa Rusia, saya akan mengerti 100% setelah bacaan pertama.  Ini terjadi dengan jaringan generatif, yang mana ada serangkaian artikel yang sangat baik: setelah membaca semuanya menjadi jelas.  Namun dalam dunia jaringan, ada banyak pendekatan yang hanya dijelaskan dalam bahasa Inggris dan yang harus ditangani selama berhari-hari. <br><br>  Saya akan secara berkala menulis artikel dalam bahasa ibu saya, membawa pengetahuan ke bidang bahasa kami.  Seperti yang Anda ketahui, cara terbaik untuk memahami suatu topik adalah menjelaskannya kepada seseorang.  Jadi siapa lagi selain saya yang harus memulai serangkaian artikel tentang AI arsitektur yang paling modern, kompleks, canggih.  Pada akhir artikel, saya sendiri akan memahami pendekatan 100%, dan itu akan berguna bagi seseorang yang membaca dan meningkatkan pemahaman mereka (omong-omong, saya suka Gesser, tetapi lebih baik ** Blanche de bruxelles **). <br><br>  Saat Anda memahami subjek, ada 4 level pemahaman: <br><br><ol><li>  Anda memahami prinsip dan input serta output dari Algoritma / Level </li><li>  Anda memahami pertemuan yang keluar dan secara umum cara kerjanya </li><li>  Anda memahami semua hal di atas, serta perangkat dari setiap tingkat jaringan (misalnya, dalam model VAE Anda memahami prinsipnya, dan Anda juga memahami inti dari trik reparameterisasi) </li><li>  Saya mengerti segalanya, termasuk setiap level, saya juga mengerti mengapa semuanya belajar, dan pada saat yang sama saya dapat memilih parameter hiper untuk tugas saya, daripada salin-rekatkan solusi yang sudah jadi. </li></ol><br>  Untuk arsitektur baru, transisi dari level 1 ke level 4 sering sulit: penulis menekankan bahwa mereka lebih dekat menggambarkan berbagai detail penting secara dangkal (apakah mereka memahaminya sendiri?).  Atau otak Anda tidak mengandung konstruksi, jadi bahkan setelah membaca deskripsi itu tidak menguraikan dan tidak berubah menjadi keterampilan.  Ini terjadi jika selama tahun-tahun siswa Anda tidur di pelajaran matan yang sama, setelah pesta malam hari  di mana Anda memberikan tikar yang tepat.  aparatur.  Dan di sini kita membutuhkan artikel dalam bahasa asli kita yang mengungkapkan nuansa dan kehalusan dari setiap operasi. <br><br><h2>  Konsep dan aplikasi perhatian </h2><br>  Di atas adalah skenario tingkat pemahaman.  Untuk memilah Perhatian, mari kita mulai dari tingkat satu.  Sebelum menjelaskan input dan output, kami akan menganalisis esensi: di mana konsep dasar, dapat dipahami bahkan untuk seorang anak, konsep ini didasarkan.  Dalam artikel ini kita akan menggunakan istilah bahasa Inggris Perhatian, karena dalam formulir ini juga merupakan panggilan ke fungsi perpustakaan Keras (tidak secara langsung diterapkan di dalamnya, modul tambahan diperlukan, tetapi lebih pada yang di bawah).  Untuk membaca lebih lanjut, Anda harus memiliki pemahaman tentang pustaka Keras dan python, karena kode sumber akan disediakan. <br><br><img src="https://habrastorage.org/webt/lf/nv/-a/lfnv-ayy8tlpfgkiwcrgiinkme4.png" align="right">  Perhatian diterjemahkan dari bahasa Inggris sebagai "perhatian".  Istilah ini dengan tepat menggambarkan esensi dari pendekatan: jika Anda seorang pengendara mobil dan polisi lalu lintas umum ditampilkan di foto, Anda secara intuisi mementingkan itu, terlepas dari konteks foto.  Anda cenderung melihat lebih dekat pada sang jenderal.  Anda saring mata Anda, lihat tali pengikat hati-hati: berapa banyak bintang yang ada di sana secara khusus.  Jika jendral tidak terlalu tinggi, abaikan dia.  Kalau tidak, anggap itu sebagai faktor kunci dalam pengambilan keputusan.  Beginilah cara otak kita bekerja.  Dalam budaya Rusia, kita telah dilatih dari generasi ke generasi untuk memperhatikan peringkat tinggi, otak kita secara otomatis menempatkan prioritas tinggi pada objek-objek tersebut. <br><br>  Perhatian adalah cara untuk memberi tahu jaringan apa yang harus Anda perhatikan, yaitu, untuk melaporkan probabilitas hasil tertentu tergantung pada keadaan neuron dan data input.  Lapisan Attention yang diimplementasikan dalam Keras sendiri mengidentifikasi faktor-faktor berdasarkan pada set pelatihan, perhatian yang mengurangi kesalahan jaringan.  Identifikasi faktor-faktor penting dilakukan melalui metode propagasi kesalahan kembali, mirip dengan bagaimana hal ini dilakukan untuk jaringan konvolusi. <br><br>  Dalam pelatihan, Perhatian menunjukkan sifat probabilistiknya.  Mekanisme itu sendiri membentuk matriks skala kepentingan.  Jika kita tidak melatih Perhatian, kita dapat menetapkan pentingnya, misalnya, secara empiris (umum lebih penting daripada panji).  Tetapi ketika kita melatih jaringan pada data, kepentingan menjadi fungsi dari probabilitas hasil tertentu, tergantung pada data yang diterima pada input jaringan.  Sebagai contoh, jika kita bertemu dengan seorang jenderal yang tinggal di Rusia Tsar, maka kemungkinan mendapatkan tantangan akan tinggi.  Setelah memastikan ini, akan dimungkinkan melalui beberapa pertemuan pribadi, mengumpulkan statistik.  Setelah itu, otak kita akan memberi bobot yang sesuai pada fakta pertemuan subjek ini dan memberi tanda pada tali dan garis bahu.  Perlu dicatat bahwa penanda yang ditetapkan tidak mungkin: sekarang pertemuan jenderal akan membawa konsekuensi yang sama sekali berbeda untuk Anda daripada itu, di samping itu, bobotnya mungkin lebih dari satu.  Tapi, berat badan bisa dikurangi menjadi probabilitas dengan menormalkannya. <br><br>  Sifat probabilistik mekanisme Attention dalam pembelajaran dimanifestasikan dalam tugas penerjemahan mesin.  Sebagai contoh, mari kita beri tahu jaringan bahwa ketika menerjemahkan dari bahasa Rusia ke bahasa Inggris, kata Cinta diterjemahkan dalam 90% kasus sebagai Cinta, dalam 9% kasus sebagai Seks, dalam 1% kasus seperti sebaliknya.  Jaringan segera menandai banyak opsi, menunjukkan kualitas pelatihan terbaik.  Saat menerjemahkan, kami memberi tahu jaringan: "ketika menerjemahkan kata cinta, beri perhatian khusus pada kata bahasa Inggris Cinta, lihat juga apakah itu masih Seks." <br><br>  Pendekatan Attention diterapkan untuk bekerja dengan teks, serta seri suara dan waktu.  Untuk pemrosesan teks, jaringan saraf berulang (RNN, LSTM, GRU) banyak digunakan.  Perhatian dapat melengkapi atau menggantinya, memindahkan jaringan ke arsitektur yang lebih sederhana dan lebih cepat. <br><br>  Salah satu kegunaan Perhatian yang paling terkenal adalah menggunakannya untuk meninggalkan jaringan perulangan dan beralih ke model yang sepenuhnya terhubung.  Jaringan berulang memiliki serangkaian kelemahan: ketidakmampuan untuk memberikan pelatihan tentang GPU, pelatihan ulang cepat.  Dengan menggunakan mekanisme Attention, kita dapat membangun jaringan yang mampu mempelajari urutan berdasarkan jaringan yang sepenuhnya terhubung, melatihnya pada GPU, menggunakan droput. <br><br>  Perhatian banyak digunakan untuk meningkatkan kinerja jaringan perulangan, misalnya, dalam bidang terjemahan dari bahasa ke bahasa.  Saat menggunakan pendekatan encoding / decoding, yang sering digunakan dalam AI modern (misalnya, auto-encoder variasional).  Ketika lapisan Attention ditambahkan antara encoder dan decoder, hasil operasi jaringan terasa membaik. <br><br>  Dalam artikel ini, saya tidak mengutip arsitektur jaringan spesifik menggunakan Perhatian, ini akan menjadi subjek pekerjaan yang terpisah.  Daftar semua kemungkinan penggunaan perhatian layak untuk artikel terpisah. <br><br><h2>  Menerapkan Perhatian dalam Keras Out of the Box </h2><br>  Ketika Anda memahami pendekatan seperti apa, sangat berguna untuk mempelajari prinsip dasar.  Namun seringkali pemahaman penuh datang hanya dengan melihat implementasi teknis.  Anda melihat aliran data yang membentuk fungsi operasi, menjadi jelas apa yang sebenarnya dihitung.  Tetapi pertama-tama Anda harus menjalankannya dan menulis "Attention hello word". <br><br>  Perhatian saat ini tidak diterapkan di Keras itu sendiri.  Tetapi sudah ada implementasi pihak ketiga, seperti <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">attention-keras,</a> yang dapat diinstal dengan github.  Maka kode Anda akan menjadi sangat sederhana: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> attention_keras.layers.attention <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> AttentionLayer attn_layer = AttentionLayer(name=<span class="hljs-string"><span class="hljs-string">'attention_layer'</span></span>) attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])</code> </pre> <br>  Implementasi ini mendukung fungsi visualisasi skala Perhatian.  Setelah melatih Perhatian, Anda bisa mendapatkan pensinyalan matriks, yang, menurut jaringan, sangat penting tentang jenis ini (gambar dari github dari halaman perpustakaan attention-keras). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ju/hr/yj/juhryjvb8eedahp2h77q07p_pwm.png"></div><br>  Pada dasarnya, Anda tidak memerlukan yang lain: sertakan kode ini di jaringan Anda sebagai salah satu level dan nikmati belajar jaringan Anda.  Jaringan apa pun, algoritma apa pun dirancang pada tahap pertama pada tingkat konseptual (seperti database, omong-omong), setelah itu implementasi ditentukan dalam representasi logis dan fisik sebelum implementasi.  Metode desain ini belum dikembangkan untuk jaringan saraf (oh ya, ini akan menjadi topik artikel saya berikutnya).  Anda tidak mengerti bagaimana lapisan konvolusi bekerja di dalam?  Prinsipnya dijelaskan, Anda menggunakannya. <br><br><h2>  Implementasi keras Attention rendah </h2><br>  Untuk akhirnya memahami topik tersebut, di bawah ini kami akan menganalisis secara rinci penerapan Perhatian di bawah tenda.  Konsepnya bagus, tetapi bagaimana tepatnya kerjanya dan mengapa hasilnya diperoleh persis seperti yang dinyatakan? <br><br>  Implementasi paling sederhana dari mekanisme Attention dalam Keras hanya membutuhkan 3 baris: <br><br><pre> <code class="python hljs">inputs = Input(shape=(input_dims,)) attention_probs = Dense(input_dims, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>, name=<span class="hljs-string"><span class="hljs-string">'attention_probs'</span></span>)(inputs) attention_mul = merge([inputs, attention_probs], output_shape=<span class="hljs-number"><span class="hljs-number">32</span></span>, name=<span class="hljs-string"><span class="hljs-string">'attention_mul'</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'mul'</span></span></code> </pre><br>  Dalam hal ini, lapisan Input dideklarasikan di baris pertama, kemudian muncul lapisan yang sepenuhnya terhubung dengan fungsi aktivasi softmax dengan jumlah neuron yang sama dengan jumlah elemen di lapisan pertama.  Lapisan ketiga mengalikan hasil dari lapisan yang sepenuhnya terhubung dengan elemen data input oleh elemen. <br><br>  Di bawah ini adalah seluruh kelas Attention, yang mengimplementasikan mekanisme perhatian diri yang sedikit lebih kompleks, yang dapat digunakan sebagai level penuh dalam model, kelas mewarisi kelas lapisan Keras. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Attention class Attention(Layer): def __init__(self, step_dim, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None, bias=True, **kwargs): self.supports_masking = True self.init = initializers.get('glorot_uniform') self.W_regularizer = regularizers.get(W_regularizer) self.b_regularizer = regularizers.get(b_regularizer) self.W_constraint = constraints.get(W_constraint) self.b_constraint = constraints.get(b_constraint) self.bias = bias self.step_dim = step_dim self.features_dim = 0 super(Attention, self).__init__(**kwargs) def build(self, input_shape): assert len(input_shape) == 3 self.W = self.add_weight((input_shape[-1],), initializer=self.init, name='{}_W'.format(self.name), regularizer=self.W_regularizer, constraint=self.W_constraint) self.features_dim = input_shape[-1] if self.bias: self.b = self.add_weight((input_shape[1],), initializer='zero', name='{}_b'.format(self.name), regularizer=self.b_regularizer, constraint=self.b_constraint) else: self.b = None self.built = True def compute_mask(self, input, input_mask=None): return None def call(self, x, mask=None): features_dim = self.features_dim step_dim = self.step_dim eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim)) if self.bias: eij += self.b eij = K.tanh(eij) a = K.exp(eij) if mask is not None: a *= K.cast(mask, K.floatx()) a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx()) a = K.expand_dims(a) weighted_input = x * a return K.sum(weighted_input, axis=1) def compute_output_shape(self, input_shape): return input_shape[0], self.features_dim</span></span></code> </pre><br>  Di sini kita melihat kira-kira hal yang sama yang diterapkan di atas melalui lapisan Keras yang sepenuhnya terhubung, hanya dilakukan melalui logika yang lebih dalam di tingkat yang lebih rendah.  Level parametrik (self.W) dibuat dalam fungsi, yang kemudian dikalikan secara skal (K.dot) dengan vektor input.  Logika kabel dalam varian ini sedikit lebih rumit: pergeseran (jika parameter bias diungkapkan), garis singgung hiperbolik, eksposur, mask (jika ditentukan), normalisasi diterapkan pada vektor input kali sendiri.W, maka vektor input lagi ditimbang oleh hasil yang didapat.  Saya tidak memiliki penjelasan tentang logika yang ditetapkan dalam contoh ini, saya mereproduksi operasi membaca kode.  Ngomong-ngomong, silakan tulis di komentar jika Anda mengenali beberapa fungsi matematika tingkat tinggi dalam logika ini. <br><br>  Kelas memiliki parameter "bias" yaitu  bias.  Jika parameter diaktifkan, maka setelah menerapkan layer Dense, vektor akhir akan ditambahkan ke vektor parameter layer "self.b", yang akan memungkinkan tidak hanya untuk menentukan "bobot" untuk fungsi perhatian kita, tetapi juga untuk menggeser tingkat perhatian dengan angka.  Contoh kehidupan: kita takut pada hantu, tetapi belum pernah bertemu mereka.  Jadi, kami melakukan koreksi untuk ketakutan -100 poin.  Yaitu, hanya jika rasa takut keluar dari skala untuk 100 poin, kami akan membuat keputusan untuk melindungi terhadap hantu, memanggil agen penghantu hantu, membeli perangkat menakut-nakuti, dll. <br><br><h2>  Kesimpulan </h2><br>  Mekanisme perhatian memiliki variasi.  Opsi Attention paling sederhana yang diterapkan pada kelas di atas disebut Self-Attention.  Self-attention adalah mekanisme yang dirancang untuk memproses data sekuensial, dengan mempertimbangkan konteks setiap cap waktu.  Ini paling sering digunakan untuk bekerja dengan informasi tekstual.  Implementasi self-attention dapat dilakukan dengan mengimpor perpustakaan keras-self-attention.  Ada variasi lain dari Perhatian.  Mempelajari materi berbahasa Inggris, dimungkinkan untuk menghitung lebih dari 5 variasi. <br><br>  Ketika menulis bahkan artikel yang relatif singkat ini, saya mempelajari lebih dari 10 artikel berbahasa Inggris.  Tentu saja, saya tidak dapat mengunduh semua data dari semua artikel ini menjadi 5 halaman, saya hanya membuat perasan untuk membuat "panduan untuk boneka".  Untuk memahami semua nuansa mekanisme Perhatian, Anda memerlukan buku halaman 150-200.  Saya benar-benar berharap bahwa saya dapat mengungkapkan esensi dasar dari mekanisme ini sehingga mereka yang baru mulai memahami pembelajaran mesin memahami bagaimana semua ini bekerja. <br><br><h2>  Sumber </h2><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Mekanisme perhatian dalam Jaringan Saraf Tiruan dengan Keras</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Perhatian di Deep Networks dengan Keras</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Sequence-to-Sequence berbasis Attention di Keras</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Klasifikasi Teks menggunakan Mekanisme Perhatian dalam Keras</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pervasive Attention: Jaringan Syaraf Konvolusional 2D untuk Prediksi Sequence-to-Sequence</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bagaimana cara mengimplementasikan Attention Layer di Keras?</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Perhatian?</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Perhatian!</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Terjemahan Mesin Saraf dengan Perhatian</a> </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id458992/">https://habr.com/ru/post/id458992/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id458982/index.html">Resep Nginx: Konversi dari HTML dan URL ke PDF dan PS</a></li>
<li><a href="../id458984/index.html">Cara membuat aplikasi pertama untuk berdagang di bursa: 3 langkah awal</a></li>
<li><a href="../id458986/index.html">Resep PostgreSQL: Konversi dari HTML dan URL ke PDF dan PS</a></li>
<li><a href="../id458988/index.html">Tekstur, atau apa yang perlu Anda ketahui untuk menjadi Artis Permukaan. Bagian 4. Model, normals, dan sapuan</a></li>
<li><a href="../id458990/index.html">Berhentilah bersemangat dengan komentar dalam kode</a></li>
<li><a href="../id458994/index.html">Raspberry Pi + CentOS = Wi-Fi Hotspot (atau Router Raspberry dengan Topi Merah)</a></li>
<li><a href="../id458996/index.html">User Inyerface - bagaimana tidak menyiksa pengguna</a></li>
<li><a href="../id459000/index.html">Bagaimana saya mencoba meningkatkan Halo 2, tetapi hampir merusaknya</a></li>
<li><a href="../id459002/index.html">Cara mengkonfigurasi HTTPS - Generator Konfigurasi SSL akan membantu</a></li>
<li><a href="../id459004/index.html">Algoritma kriptografi Grasshopper: kompleks</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>