<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>💭 🕊️ 🧔🏼 HighLoad ++, Mikhail Makurov (Intersvyaz): Erfahrung in der Erstellung eines Backup- und Cluster-Zabbix-Dienstes 🙎 🤾🏼 🌡️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Zabbix ist ein beliebtes offenes Überwachungssystem, das von einer Vielzahl von Unternehmen eingesetzt wird. Ich werde über die Erfahrungen beim Erste...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>HighLoad ++, Mikhail Makurov (Intersvyaz): Erfahrung in der Erstellung eines Backup- und Cluster-Zabbix-Dienstes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ua-hosting/blog/485534/">  Zabbix ist ein beliebtes offenes Überwachungssystem, das von einer Vielzahl von Unternehmen eingesetzt wird.  Ich werde über die Erfahrungen beim Erstellen eines Überwachungsclusters sprechen. <br><br>  In dem Bericht werde ich kurz auf die zuvor vorgenommenen Änderungen (Patches) eingehen, die die Fähigkeiten des Systems erheblich erweitern und die Basis für den Cluster vorbereiten (Upload des Verlaufs in „Clickhouse“, asynchrones Polling).  Und ich werde im Detail auf die Probleme eingehen, die während des System-Clusters aufgetreten sind - das Lösen von Identitätskonflikten in der Datenbank, ein wenig über das CAP-Theorem und das Überwachen mit verteilten Datenbanken, über die Nuancen der Arbeit von Zabbix im Cluster-Modus: Sicherung und Koordination von Servern und Proxys, über das Überwachen von Domänen und ein neues Aussehen auf Systemarchitektur. <br><br>  Ich werde kurz darüber sprechen, wie ein Cluster zu Hause gestartet wird, woher die Quellen stammen und welche zusätzlichen.  Für den Cluster sind Einstellungen erforderlich. <br><br><img src="https://habrastorage.org/webt/uv/cy/1r/uvcy1rw-6ttiumnzmjuizgl4ml4.jpeg"><br><br>  HighLoad ++ Sibirien 2019. Tomsk Hall.  24. Juni, 17 Uhr  Abstracts und <a href="https://www.highload.ru/siberia/2019/abstracts/5210">Präsentation</a> .  Die nächste HighLoad ++ Konferenz findet am 6. und 7. April 2020 in St. Petersburg statt.  Details und Tickets <a href="http://bit.ly/2sSxgBx">hier</a> . <a name="habracut"></a><br><br>  <b>Mikhaili Makurov (im Folgenden - MM):</b> - Ich arbeite für eine Providerfirma.  Der Anbieter heißt Intersvyaz, er arbeitet in der Stadt Tscheljabinsk.  Wir haben ungefähr 1,5 Millionen Menschen.  Und damit der Anbieter funktioniert, gibt es eine riesige Infrastruktur.  Wir haben ungefähr 70.000 Geräte: Switches, IoT-Geräte ... - eine Menge von allem, was überwacht werden muss.  Dieser Bericht befasst sich insbesondere mit der Verwendung von Zabbix und dem Aufbau eines auf Zabbix basierenden Clusters für die Überwachung der Infrastruktur. <br><br>  Ich bin 12 Jahre im Anbieter.  Jetzt mache ich überhaupt keine technischen Dinge, es geht mehr darum, Leute zu managen.  Und das (technische Sachen) ist eigentlich mein Hobby.  Ich werde dieses Thema etwas weiterentwickeln. <br><br><h3>  Probleme beim Überwachen </h3><br>  Ich glaube, ich habe Glück.  Vor ungefähr anderthalb Jahren endete ich in einem Projekt, das so klang: "Wir müssen einige Probleme mit unserer Überwachung lösen."  Ich habe eine Verantwortungszone (Überwachung) geerbt, die aus einer Reihe von Servern bestand, insbesondere aus 21 Servern: <br><br><img src="https://habrastorage.org/webt/eu/om/mx/euommxg6onyd7rdbncwqbnrhvtc.jpeg"><br><br>  Es gab 4 leistungsstarke Server und 15 Proxys - es war alles Hardware.  Es gab einige Beschwerden über diese Überwachung.  Das erste ist, dass es viel war.  Wir haben nicht einen einzigen Server mit dem Provider so viel Platz belegt.  Das ist Geld, Elektrizität ... Tatsächlich ist das kein großes Problem. <br><br><img src="https://habrastorage.org/webt/ui/jf/u3/uijfu32ebg7upocsepwtvptenk4.jpeg"><br><br>  Das große Problem war, dass die Überwachung nicht mit dem Schritt hielt, was wir von ihm wollten.  Für diejenigen, die Zabbix noch nicht aktiv genutzt haben, ist dies ein Dashboard, das die Verspätung bei Überprüfungen anzeigt: <br><br><img src="https://habrastorage.org/webt/ig/jd/ti/igjdtilbejkchus711uvbzzeuyo.jpeg"><br><br>  Die meisten unserer Schecks befanden sich in der roten Zone.  Sie liefen mehr als 10 Minuten langsamer als wir wollten, das heißt, sie waren 10 Minuten zu spät.  Es war nicht sehr angenehm, aber es war immer noch möglich, mehr oder weniger zu leben.  Das größte Problem war das folgende: <br><br><img src="https://habrastorage.org/webt/vf/ai/nw/vfainwua91q6pgxnj2msbx_tzle.jpeg"><br><br>  Es war ein Überwachungssystem eines funktionierenden Netzwerks.  Bei der Ausführung der geplanten Arbeiten fiel ein Tausendersegment an fünf Schaltern ab.  Zusammen mit diesen Schaltern gerieten Schalter und Überwachung in Vergessenheit.  Als alles wiederhergestellt war, wurde zwei Stunden später die Überwachung wiederhergestellt.  Es war schmerzlich unangenehm, und dieser Satz sollte in jedem Bericht stehen: <br><br><img src="https://habrastorage.org/webt/aw/o-/t3/awo-t3j-xbu181dyneis-q4ozjo.jpeg"><br><br><h3>  "Wir müssen etwas mit diesem Projekt machen!" </h3><br>  Und hier werde ich zwei Geschichten erzählen.  Dann haben wir versucht, gleichzeitig auf zwei Arten zu gehen.  Wir haben eine Integrationsgruppe - sie hat den Weg zum Bau eines modularen Systems gewählt (es gab einen sehr coolen Bericht von Avito zu Highload im November letzten Jahres in Moskau - sie haben darüber gesprochen): <br><br><img src="https://habrastorage.org/webt/qg/f5/nv/qgf5nvonycfltfd8vge1tbeloj8.jpeg"><br><br><h3>  Zabbix = Menschen + API + Effizienz </h3><br>  Die Jungs aus kleinen Stücken begannen ein System aufzubauen.  Und mit einigen Enthusiasten arbeitete ich weiter an Zabbix.  Dafür gab es Gründe.  Was sind die Gründe? <br><br><ul><li>  Erstens gibt es eine coole API.  Und wenn Sie 60-70.000 Überwachungselemente haben, funktioniert dies natürlich nur automatisch. Sie können nicht so viele Hände ohne Fehler hinzufügen. </li><li>  Personal.  Es gibt Dienstüberwachungsschichten, die rund um die Uhr arbeiten.  Das sind keine IT-Spezialisten, das sind Leute im Dienst.  Wir haben dem "Grafan" einige andere Systeme gezeigt - es ist schwer für sie.  Es gibt Administratoren, die an Vielfalt und den Komfort der Überwachung im Zabbix selbst gewöhnt sind: Vorlagen, automatische Erkennung - und das ist alles cool! </li><li>  Zabbix kann effektiv sein. </li></ul><br><h3>  Wird die SQL-Datenbank langsamer?  Eine Antwort - Clickhouse </h3><br>  Der erste Grund war offensichtlich.  Wir haben dann an MySQL gearbeitet, und es kam zu 6-7.000 Metriken pro Sekunde, und wir sahen konstante Verzögerungen auf den Festplatten. <br><br><img src="https://habrastorage.org/webt/kb/ke/wf/kbkewfpy7l6b2qdexbuljdwr6ia.jpeg"><br><br>  Heute hat es schon 100 Mal geklungen: Die einzige Antwort ist Clickhouse: <br><br><img src="https://habrastorage.org/webt/w-/lm/iw/w-lmiwpxynx-rmj4spjyv_gc1cc.jpeg"><br><br>  In der Struktur von Abfragen handelt es sich bei dem Großteil der Abfragen (unser Profiling in wenigen Stunden) um Metrikdatensätze.  Das Schreiben von Metriken in eine SQL-Datenbank ist extrem teuer.  Hier erschien TimeScaleDB ... Dann hatten wir ein „Clickhouse“ für ungefähr ein Jahr für andere Aufgaben in Betrieb (wir machen Big Data, wir haben eine große Anwendung - im Allgemeinen ist ein Anbieter jetzt ein ganzes IT-Geschäft). <br><br>  Nachdem wir uns schöne Grafiken aus dem Internet angesehen hatten (das „Clickhouse“ ist hunderte Male schneller, es benötigt sehr wenig Speicherplatz) und über aktuelle Erfahrungen verfügten, haben wir unser HistoryStorage-Modul für „Zabbix“ geschrieben, damit die „Clickhouse“ -Daten direkt gespeichert werden können (d. H. nicht vom Dateiexport, sondern direkt im laufenden Betrieb). <br><br><img src="https://habrastorage.org/webt/tp/f4/p2/tpf4p2z5jftziqwqhtdtxsb99-g.jpeg"><br><br>  Außerdem haben wir ein Modul für die „Front“ geschrieben.  Alle diese schönen Grafiken im Zabbix-Admin-Panel können von Clickhouse aus erstellt werden.  Es ist klar, dass die API auch funktioniert. <br><br>  Der Effekt ist ungefähr derselbe - der SQL Server als dedizierte Entität ist nicht vollständig geworden, dh die Last ist auf Null gefallen.  Was am bemerkenswertesten ist, wir hatten bereits ein dediziertes „Clickhouse“ -Cluster: Als wir dort unsere gesamte Ladung abgaben, stieg es von 6 auf 10.000 Metriken.  Die Administratoren sagten: "Aber wir sehen nichts, was gekommen ist.  Nein! " <br><br><h3>  Wie wir Clickhouse erweitert haben </h3><br>  Ich sage noch weiter: Für Tests haben wir versucht, bis zu 140-150.000 Metriken pro Sekunde zu laden (wir konnten Zabbix nicht mehr auspressen, später werde ich sagen, warum), und das Clickhouse sieht diese Last auch nicht.  Das heißt, es ist sehr bequem, kühle Ladung.  In der Regel gibt es ein solches Modul. <br><br>  Außerdem haben wir es etwas erweitert: <br><br><img src="https://habrastorage.org/webt/_u/ha/aj/_uhaajlqcp9mcjuh-ih8sc-dn3k.jpeg"><br><br>  In unserer Version können Sie die Nanosekunden ausschalten.  Sie wissen wahrscheinlich: Zabbix schreibt Sekunden und Nanosekunden in zwei Felder.  Nehmen Sie in den „Clickhouse“ -Feldern, in denen die Variabilität sehr groß ist, viel Platz ein. <br><br>  Übrigens über den Ort.  Eine Metrik in Clickhouse (wir haben jetzt ungefähr 700 Milliarden Metriken aufgezeichnet) benötigt 2,9 Bytes.  Gemäß der Zabbix-Dokumentation benötigt eine Metrik in SQL-Datenbanken 40 bis 100 Byte.  Das Ausschalten der Nanosekunden spart weitere 40%, d. H. Etwa 1,5 Bytes pro Metrik.  Das heißt, "Clickhouse" ist sehr effektiv in Bezug auf den Standort. <br><br>  Auf Wunsch unserer Mitarbeiter, die sich mit maschinellem Lernen beschäftigen, haben wir eine Option getroffen, damit wir den Host und den Namen der Metrik schreiben können.  Da die Variabilität der Daten groß ist, nimmt dies nicht viel zusätzlichen Platz ein, obwohl die Textdaten signifikant sein können (dies wurde mit langen Tests noch nicht verifiziert). <br><br>  Außerdem haben wir zwei Ergänzungen vorgenommen, da wir Zabbix entwickelt haben und es oft ziehen mussten.  Eine sehr coole Ergänzung: Zu Beginn können wir den Verlaufscache füllen, da Sie mit „Clickhouse“ Millionen von Datensätzen lesen können.  Zu Beginn haben wir eine Verzögerung von 30 bis 40 Sekunden, erhalten jedoch einen sofort gestarteten Dienst mit einem erwärmten Cache. <br><br>  In Fällen, in denen das Sammeln über die Infrastruktur einfacher ist, gibt es immer noch eine solche Option: das Lesen aus dem Cache für einige Zeit zu unterbinden.  Es ist besser, 5 Minuten schnell zu arbeiten, ohne die Auslöser zu zählen, und dann füllt sich der Cache - wenn Sie dies nicht tun, beginnt die Stagnation der Historiensenker. <br><br>  Im Allgemeinen gibt es ein „Clickhouse“ -Modul.  Es kann verwendet werden. <br><br><h3>  Polling-Effizienz </h3><br>  Trotz der Tatsache, dass wir dann die Probleme mit der Basis gelöst haben, blieben die Bremsen und das Problem mit fünfzehn Proxies bestehen.  Sie waren damit verbunden: <br><br><img src="https://habrastorage.org/webt/un/ec/u8/unecu8ujsgk7scstaonbt3dueim.jpeg"><br><br>  Dies ist die Hauptdatenverarbeitungs-Pipeline bei Zabbix.  Es gibt eine Phase der Datenerfassung, eine Vorverarbeitung und Verlaufssynchronisierungen, die die gesamte Arbeit erledigen (Berechnung von Triggern, Warnungen, Speichern des Verlaufs).  Der Cache-Engpass stellte sich heraus als: <br><br><img src="https://habrastorage.org/webt/er/jg/ei/erjgein-xwdpitoa8eql3b5dw1g.jpeg"><br><br>  Warum ist die Abfrage langsam?  Denn die Threads, die die Anforderungen stellen, werden in der Cache-Konfiguration für Einheitenmetriken in die Warteschlange gestellt und blockiert.  Es gibt andere Orte, aber sie sind nicht so eng.  Beispielsweise gibt es eine Vorverarbeitung selbst und einen Verlaufscache.  In unserem SQL haben wir folgende Einschränkungen: <br><br><img src="https://habrastorage.org/webt/-w/ys/-b/-wys-btlu1uxulrqcinsvbmac5a.jpeg"><br><br>  Möglicherweise liegt dies an der Tatsache, dass in unserem Fall die Basis ungefähr 5 Millionen Metriken ist, die wir entfernen.  Mit all den Optimierungen, die wir durchgeführt haben, konnten wir 70.000 Metriken im Engpass (im Konfigurations-Cache) erhalten, aber nur in dem Fall, in dem wir sie in großen Mengen verarbeiteten. <br><br>  Was ist Massenverarbeitung?  Poller wechselt zum Konfigurations-Cache und übernimmt die Aufgabe nicht für eine Metrik, sondern für 4 oder 8 Tausend.  Gleichzeitig bietet sich ihm eine weitere wunderbare Gelegenheit: Er kann jetzt asynchron abrufen, weil er 4.000 Messdaten hat ... Warum tun sie eine nach der anderen?  Sie können sofort alles fragen! <br><br><h3>  Asynchrones Polling ist effizienter als Proxy! </h3><br>  Für die Haupttypen, die vom Anbieter verwendet werden - SNMP und AGENT - haben wir die Abfrage in den asynchronen Modus umgeschrieben, und dies führte insgesamt zu einer Geschwindigkeitssteigerung von 100 auf 200.  Wir hatten 15 Proxies, wir haben sie in 150 geteilt - sie waren komplett weg.  Das Ergebnis waren zwei Banken, die nur für die Reserve benötigt werden: <br><br><img src="https://habrastorage.org/webt/ax/7z/lk/ax7zlkk-yg9oafyuruz9blq7l-q.jpeg"><br><br>  Uniprozessor-Bank (eine Xeon 1280 kostet).  Dies ist meine letzte Zeit: <br><br><img src="https://habrastorage.org/webt/zq/qf/nc/zqqfncyu4fxo38fiegvr0kunhhw.jpeg"><br><br>  Ungefähr 60% sind kostenlos, aber bei diesem Klingeln von 60% bis 40% werden regelmäßige Skripten auf dem Computer selbst ausgeführt (externe Skripten).  Sie können optimiert werden, bis Probleme entstehen. <br><br>  Die Skalierung sieht ungefähr so ​​aus: <br><br><img src="https://habrastorage.org/webt/km/pa/nj/kmpanjjtpdm3660spp8haz7ypo0.jpeg"><br><br>  Dies sind 62.000 Hosts, ungefähr 5 Millionen Metriken.  Unser aktueller Bedarf beträgt ungefähr 20.000 Metriken pro Sekunde. <br><br>  Na wie alles?  Wir haben die Performance-Probleme gelöst, die Historie erweitert und das Polling ist fantastisch.  Das Problem ist gelöst?  Nicht wirklich ... Alles wäre zu einfach. <br><br>  Ich habe auf der vorherigen Karte einen Streich gespielt (nicht alle gezeigt): <br><br><img src="https://habrastorage.org/webt/ds/ag/k5/dsagk5zrh8bog_ogrq35w1uvawy.jpeg"><br><br>  Es gibt zwei Probleme.  Ich möchte sagen: "Narren, Straßen."  Es gibt einen menschlichen Faktor, es gibt Ausrüstung. <br><br>  Ein Server reicht immer noch nicht aus.  In etwa einem Jahr gab es zwei Fälle mit Hardwareproblemen - ein SSD-Laufwerk und etwas anderes.  Die meisten Probleme sind der menschliche Faktor, wenn Menschen Tests durchführen.  In unserem Unternehmen wird Zabbix als Dienstleistung genutzt: Alle Abteilungen können dort etwas Eigenes schreiben. <br><br>  Ich würde gerne erweitern.  Ich möchte mich nicht auf eine Dose verlassen können.  Ich wollte, dass wir noch stärker bleiben können.  Und ich möchte nach dem Scale-out-Prinzip skalieren.  Hier gibt es gar nichts zu diskutieren: Wachsen, die Kapazität einer Dose erhöhen, ist schon seit 20 Jahren irrelevant. <br><br><img src="https://habrastorage.org/webt/yl/hy/vn/ylhyvn7za42rcr2allqhjzlmw2i.jpeg"><br><br><h3>  Der Cluster angefordert ... </h3><br>  Irgendwann im Dezember erschien die erste Version.  Eine atomare Cluster-Einheit wird auf einem separaten Host verarbeitet.  Der Host wurde ausgewählt. <br><br><img src="https://habrastorage.org/webt/9j/yo/sy/9jyosycj6yj7ecx7k5mn0esrfnu.jpeg"><br><br>  Tatsache ist, dass in Zabbix ziemlich starke Verbindungen zwischen Elementen bestehen, die sich auf demselben Host befinden können, dh Trigger können verbunden werden, sie können in der Vorverarbeitung zusammen verarbeitet werden.  Zwischen den Hosts ist die Konnektivität jedoch nicht so hoch. Daher ist es normal, diesen Cluster zwischen den Knoten des Clusters zu verwenden - dort wird nicht viel Datenverkehr stattfinden.  Die Hauptaufgabe von Clustern besteht darin, untereinander zu vereinbaren, wer an welchen Hosts beteiligt ist. <br><br>  Ich würde gerne unser maximales Limit von 60-70.000 Metriken überschreiten, weil der Appetit mit dem Essen einhergeht.  Wir haben Leute, die sich mit QoE beschäftigen ... Quality of Experience - eine Analyse der Funktionsweise des Internets für Abonnenten auf der Grundlage von Transitmetriken, dh Sie liefern alle TCP-Metriken an 1,5 Millionen Menschen und geben sie in die Überwachung ein - es gibt eine Menge Daten. <br><br>  Und ich wollte Zuverlässigkeit.  Ich wollte es, wenn etwas passiert ... Der Schichtdienstbeamte rief an und sagte: "Wir haben Probleme mit dem Server", schaltete ihn aus, wir werden es morgen herausfinden. <br><br><h3>  Erster Cluster </h3><br>  Die erste Version wurde basierend auf etcd implementiert: <br><br><img src="https://habrastorage.org/webt/oh/kr/qu/ohkrqu9wh-gldd6q6yheyoztro4.jpeg"><br><br>  Etcd ist ein verteilter Schlüsselwertspeicher, der in vielen fortschrittlichen Projekten verwendet wird (soweit ich das verstehe, in Kubernetes).  Alles war großartig.  Etcd bietet sehr interessante Tools - zum Beispiel löst es das Problem der Auswahl des Hauptservers.  Aber so ein Problem ... <br><br>  Wir hatten ein klassisches "Zabbix" mit drei Links: "web" - die Basis - der Server selbst.  Und wir haben dort "Clickhouse" hinzugefügt, und jetzt haben wir auch etcd hinzugefügt.  Die Admins begannen sich hinter den Köpfen zu kratzen: Hier gibt es zu viele Abhängigkeiten - es wird wahrscheinlich nicht zuverlässig sein.  Im Verlauf der Entwicklung wurde eines noch klarer: In Zabbix selbst ist bereits eine Art der serverübergreifenden Kommunikation integriert, die nur zwischen dem Server und dem Proxy verwendet wird, der sogenannte Proxy-Poller-Prozess: <br><br><img src="https://habrastorage.org/webt/4x/cf/jh/4xcfjhgqaf-2h33nnzrjduw8rn8.jpeg"><br><br>  Es ist ziemlich cool für die Kommunikation zwischen Servern mit minimalen Änderungen.  Dies erlaubte etcd (zumindest vorübergehend) nicht zu verwenden, den Code stark zu vereinfachen und vor allem an verifiziertem Code zu arbeiten (es scheint, dass dieser Code 5 oder 7 Jahre alt ist). <br><br><h3>  Wie werden Server in einem Cluster koordiniert? </h3><br>  Die Koordination erfolgt wie beim IGP-Protokoll nach Typ.  Damit die Server Priorität haben (ich sage jetzt, warum dies erforderlich ist) und um Konflikte in der SQL-Datenbank beim Schreiben von Protokollen zu vermeiden, wird jedem Server eine ID zugewiesen (bisher manuell) - dies ist eine Zahl von 0 bis 63 (63 - es ist nur eine Konstante, vielleicht mehr): <br><br><img src="https://habrastorage.org/webt/kb/ei/xf/kbeixf3xjk6fo4oucwf9yk9yi9c.jpeg"><br><br>  Der Server mit der maximalen Kennung wird zum "Master".  Als wir unsere ersten Testcluster starteten, sagten unsere Administratoren als Erstes: „Wow!  Und lasst sie uns auf verschiedene Seiten stellen.  Gut, großartig! “(Wir werden darauf zurückkommen).  Und wenn jemand Cluster verteilt hat, ist es möglich zu steuern, wie die Topologie neu verteilt wird: Wohin geht die Rolle des "Masters" im Falle eines Ausfalls des "Zabbix" Hauptservers: <br><br><img src="https://habrastorage.org/webt/fa/n8/nx/fan8nxkovjbepqscnyljtfbc00o.jpeg"><br><br>  In diesem Fall wie folgt: <br><br><img src="https://habrastorage.org/webt/5l/qu/ou/5lquouk5h0tkzky-e6_m8dfdkn4.jpeg"><br><br><h3>  Schritt </h3><br>  Im ursprünglichen Zabbix geschieht dies folgendermaßen: Der Server selbst ist für die Generierung von Auto-Increment-Indizes verantwortlich.  Um zu verhindern, dass viele Instanzen einander auf den Fersen sind (um keine Protokolle mit denselben Indizes zu erstellen), wird Stepping verwendet: „Zabbix“ mit dem Bezeichner „1“ generiert ein Vielfaches von eins - 1, 11, 21;  mit der Kennung "7" - 7, 17, 27 (mit Nuancen). <br>  Wir sind mit Modifikatoren gefahren. <br><br><img src="https://habrastorage.org/webt/t3/fo/2-/t3fo2-qoqlx0zwmxds7vc_7y2pw.jpeg"><br><br><h3>  Wie interagieren die Server miteinander? </h3><br>  Dies ist das Erbe von IGPs Hello Packets alle 5 Sekunden.  Die Server wissen also, dass sie Nachbarn haben.  Der „Master“ weiß also, dass Nachbarn in der Nähe sind, und auf dieser Grundlage entscheidet der „Master“, welche Hosts auf welche Server verteilt werden können. <br><br><img src="https://habrastorage.org/webt/um/vm/pn/umvmpn5x2hljbhbuzfp6b3umqpo.jpeg"><br><br>  Dementsprechend gibt es eine Konfiguration.  Nach alter Erinnerung nenne ich es Topologie.  Eine Topologie ist im Wesentlichen eine Liste von Servern und Hosts, die zu ihnen gehören. <br><br>  Das Protokoll ist einfach - das ist JSON: <br><br><img src="https://habrastorage.org/webt/qd/xx/rz/qdxxrzw2rj3akue1h8nwcqum1cc.jpeg"><br><br>  Dies ist auch das Erbe der Zabbix-Proxy- und Zabbix-Serverkommunikation.  Im Allgemeinen macht es keinen Sinn, etwas anderes zu verwenden.  Das Einzige ist, dass es bei Zabbix 4 Bytes (ZBXD) gibt, aber das ist nicht der Punkt. <br><br>  Im Hallo-Paket wird die Server-ID übertragen: Wenn der Server das Paket sendet, wird seine ID und seine Version der Topologie angegeben. Auf diese Weise stellen die Server schnell fest, dass es eine neue Version der Topologie gibt, und werden sehr schnell aktualisiert. <br><br>  Tatsächlich ist die Topologie selbst nur ein Baum, eine Liste von Servern.  Für jeden Server eine Liste der unterstützten Hosts: <br><br><img src="https://habrastorage.org/webt/xp/dv/ec/xpdvecth8pp7eaukwxmykgysqiu.jpeg"><br><br>  Und dann entsteht ein interessantes Problem. <br><br><h3>  Es gibt eine solche magische Phrase - Überwachungsdomänen </h3><br>  Worum geht es?  Im klassischen Zabbix war alles einfach - eine eindeutige Einstellung: Dieser Host wird von diesem Proxy überwacht, dieser Proxy gibt Daten an den Server weiter.  Wenn der Proxy nicht installiert wurde (oder nicht benötigt wird), überwacht dieser Server alle Hosts: <br><br><img src="https://habrastorage.org/webt/x2/ly/fh/x2lyfh4prsystuyggxb0elwclx4.jpeg"><br><br>  Was tun, wenn wir viele Server haben?  Darüber hinaus könnte die Tatsache problematisch sein, dass wir über geografisch verteilte Server verfügen, und der Server in einem langsam arbeitenden Büro in Kemerowo wird versuchen, die gesamte Infrastruktur von Nowosibirsk zu überwachen. <br><br><img src="https://habrastorage.org/webt/ep/06/cy/ep06cyiqx3dx4kkyfer1wcgn8um.jpeg"><br><br>  Das wollen wir nicht.  Wir möchten einen Mechanismus haben, mit dem nicht alle Server, sondern die von uns ausgewählten (möglicherweise auf der Grundlage der Geografie) einen bestimmten Host überwachen können.  Gleichzeitig wollen wir das schaffen, und wir wollen, dass es einfach ist.  Hierfür wurde die Idee der Domänenüberwachung erfunden.  Tatsächlich handelt es sich hierbei um einfache Gruppen. Es sind einfach bereits Gruppen im Datensatz enthalten. <br>  Und als ich das tat, sprachen die Leute von der Operation mit mir - sie sagten: „Die Gruppen verwirren uns sehr.  Wir fangen immer an, über normale Gruppen nachzudenken. “  Daher dieser Name: Überwachungsdomänen. <br><br>  Hosts beziehen sich eindeutig auf: einen Host - eine Domain: <br><br><img src="https://habrastorage.org/webt/6x/hi/z5/6xhiz5ocnajnjzoxin17log0liq.jpeg"><br><br>  Die Hostdomäne kann eine beliebige Anzahl von Servern enthalten.  Server können sich in einer beliebigen Anzahl von Domänen befinden.  Dies ist eine sehr flexible Sache.  Um die Flexibilität zu erweitern und das Gehirn vollständig zu zerstören, gibt es auch eine Standarddomäne: <br><br><img src="https://habrastorage.org/webt/ad/zl/2x/adzl2xgso7st_v9c7vg9-fyryvg.jpeg"><br><br>  Server, die Mitglieder der Standarddomäne sind, werden von allen Hosts überwacht, die keine Live-Server oder keine Überwachungsdomäne haben. <br><br>  Auf diese Weise können wir die Hosts nur topologisch an einige Server binden und die Verteilung der Hosts für den Fall steuern, dass ein Server ausfällt: <br><br><img src="https://habrastorage.org/webt/fm/og/nr/fmognrgop74jgnsqkwtxdzddqgw.jpeg"><br><br>  Das nächste Problem, auf das wir gestoßen sind ... <br><br><h3>  Cluster: Anders denken </h3><br>  Wenn wir über viele Server verfügen, gibt es neue Möglichkeiten zum Erstellen eines Clusters und einer Topologie.  Dies ist ein Klassiker, wenn es eine Art zentralen Standort gibt und es abgelegene Standorte gibt.  oder sagen wir, ein Proxy, an den die Last delegiert wird: <br><br><img src="https://habrastorage.org/webt/rn/ar/qd/rnarqdcbs1knledntrvqoibsozs.jpeg"><br><br>  Im Falle des Clusters Zabbix kann es auf zwei Arten implementiert werden.  Sie können den klassischen Weg gehen: Verdoppeln Sie einfach die Infrastruktur.  In der Mitte haben wir zwei Server, die einen Cluster bilden, die Hosts neu anordnen oder die Last selbst tragen können, wenn der Nachbar abstürzt.  Dementsprechend können Sie zusätzliche Proxys auf denselben Servern einrichten - wir erhalten eine doppelte Reserve: <br><br><img src="https://habrastorage.org/webt/bx/es/g4/bxesg4ijxn4b9pirro22u3-omoa.jpeg"><br><br>  Sie können die neuen "Funktionen" verwenden und dies tun: <br><br><img src="https://habrastorage.org/webt/v0/-o/ki/v0-okiiibpq1yyurxipbugjqfwq.jpeg"><br><br>  Die Hauptsache ist, nicht in eine Situation zu gehen, in der ein geografisch entfernter Server eine große Infrastruktur an einem anderen Ort überwacht.  Dies ist eher ein Verwaltungsproblem (ich nenne es Geschäft), da es sich um ein Konfigurationsproblem handelt. <br><br><h3>  Cluster: Split Gehirn und Sicht </h3><br>       ,    : <br><br><ul><li> split brain; </li><li> point of view ( ). </li></ul><br>   . Split brain –       ,         .     ,  -  –     ? ,      ,            ( ). <br><br>  point of view  : ,      ,          ,    .     . ,   RTT ,    . <br>       : <br><br><img src="https://habrastorage.org/webt/ab/qa/gc/abqagc2w3ky3kufvm6omjfsjheo.jpeg"><br><br>      ,   .   ,   ,       .    ,   –   .    ,       ,  ,  . <br><br><h3>  SQL- </h3><br> ,     ,    ,       .       .  , ,       …      .    . <br><br> -,  , ,  -      –    . , Galera  MySQL. <br><br>        PostgreSQL.    «»   :    ,   ,           –   . «», ,    . <br><br><h3>    ? </h3><br>        ,    : <br><br><img src="https://habrastorage.org/webt/q4/u1/4s/q4u14s11rryz7afktbdfkn0s9sy.jpeg"><br><br><img src="https://habrastorage.org/webt/pj/0p/uq/pj0puqdn9zkhsm2uwolcaw40a1w.jpeg"><br><br>           –  .      : <br><br><ul><li>  -  (Logs),    .   problems, events  events recovery.  ,   –   ,   . </li><li>  15   (State).  –     (    –   – «»   ).        .   ,  ;   –        … </li><li>  -       (Configuration update). </li></ul><br>   «.    «»,    SQL-: <br><br><img src="https://habrastorage.org/webt/ne/if/cc/neifccswcjikrxfetvhuip-agva.jpeg"><br><br> -,       : <br><br><img src="https://habrastorage.org/webt/4f/o4/ti/4fo4ticljuykrkh6zjmop4qc5sg.jpeg"><br><br>  Das ist richtig.          -,   ,   …    –   ,   2  !    : «  ,    ».  -  ,       ,    . <br><br> .         ,           : <br><br><img src="https://habrastorage.org/webt/lj/wh/ug/ljwhugg1tdfbehcxjcrloiolwks.jpeg"><br><br>   ,  .  SQL-     .     ,   SQL-.       (   -  ),        «» (        ). … <br><br><h3> .  Installation </h3><br> , , «»  . .       ,   ? <br><br><img src="https://habrastorage.org/webt/ci/yo/ne/ciyonezibe3bfla-7xodyyx_zwg.jpeg"><br><br>    «»- (. .  «» daemon).       (   ):   (  1  63,      «»)    (   ,       ). <br><br>      ServerIP  IP-.    ,      ,     IP-   . -    ,         proxy poller,    trapper  hello-,  proxy poller  . <br><br>   . ,       ,    « »: <br><br><img src="https://habrastorage.org/webt/dr/1z/pi/dr1zpi2hbe9penikzazqcycw-oa.jpeg"><br><br>      : <br><br><img src="https://habrastorage.org/webt/kf/pg/jb/kfpgjbghzsonjmsizlbcatyo3ja.jpeg"><br><br>  ,       default.       .  –   ,     IP-,    ,      (  ).      «» –   default. <br><br>  -, . <br><br><ul><li>   . </li><li> ,      : «    ,    ».   . </li><li>  - ,   . </li><li>     ,    hello-time,  : «   »;    . </li><li>  . </li></ul><br> ,   , ,   .               30-40 .   ,      ,    ,   . <br><br><h3>    </h3><br>            ,      .      -   : «    ,  !»  -! <br><br><img src="https://habrastorage.org/webt/nq/ta/ah/nqtaahv3gipyew36m8nhrmwukew.jpeg"><br><br>   –  :  -  ,  - , ,  GitLab, CI/CD,   . , ,  –    . <br><br>  ,      ,       –  4.0.9  (4.2   ).   Roadmap –        -. -,    «»;   ,   RPM'. <br><br><img src="https://habrastorage.org/webt/er/kn/2r/erkn2r6h9law8l3xynd6i8_jwne.jpeg"><br><br>      (   )  «»       «»-.   .         ,    .   –   :   , -   …  ? !..   «»,  . <br><br>        SQL-   ,    ,   . History Storage. <br><br><h3>  Referenzen </h3><br>     5 .      . <br> -, ,   ,      , . .      -. <br><br><img src="https://habrastorage.org/webt/zp/3a/gr/zp3agr9epncvnjndf-hgpwt_zke.jpeg"><br><br><h3>     </h3><br> ,     ?     !  , , ,  .   -   - ,         . ,         ,        .       ,    : <br><br><img src="https://habrastorage.org/webt/ac/fo/pf/acfopfqzuwvtfvtni2n9rjx-ywy.jpeg"><br><br>  . «»- ,      . <br><br><ul><li>    ,         ,     , . </li><li>   «»  :  ,       Configuration Cache,   . </li><li>   ,           ,     .        ,    ,   . </li><li>   -    ,   .      ,    ,    .    200     ,     –   . </li></ul><br><h3>   </h3><br> <b>:     !</b> <br><br>   .    ,        ,         . <br><br>      .     Server ( ).      Servers: <br><br><img src="https://habrastorage.org/webt/jm/ym/6t/jmym6tfhwlg_ix6o5xnnkbwphri.jpeg"><br><br>      ?   KPI-   ,    ;  ,        .    . ,    ,   «»-,  «»-  –    ,     ;      . <br><br><h3>  Fragen </h3><br>  Frage des Publikums (im Folgenden - A): - Ich möchte klären, wie es zwischen den Servern läuft.  Mit welchem ​​Protokoll kommunizieren sie?  Gibt es irgendeine Art von Sicherheit?  Weil es nicht sehr "sicher" ist, die Kommunikation zwischen Servern ins Internet zu bringen ... Wie läuft das? <br><br>  <b>MM:</b> - Ich denke, das ist ein Anwärter auf die beste Frage - auf den Punkt!  Als wir zur Standardkommunikation übergingen, übernahmen die Server für ihre Kommunikation zwischen den Servern alle Kommunikationsprotokollfunktionen, die zwischen dem Server und dem Proxy bestehen.  Ich werde klarstellen: Es gibt Verschlüsselung, Datenkomprimierung.  Bitte - auf die gleiche Weise wird alles über das Web konfiguriert, wie es standardmäßig für den Server und den Proxy konfiguriert ist;  alles wird funktionieren. <br><br>  <b>A:</b> - Wie arbeitet Hauskiper für Sie bei Clickhouse? <br><br>  <b>MM:</b> - Im Standard "Zabbix" gibt es keine Schnittstelle von der "Haushälterin" zur Verlaufsschnittstelle, dh die Verlaufsschnittstelle unterstützt keine Datenrotation (ElasticSearch unterstützt z. B. nicht).  Vielleicht ist es in 4.2 (habe ich nicht geschaut), aber bisher auf 4.0.9. <br><br>  Mach es dir einfach!  Das neue "Clickhouse" hat eine Partition.  Ich würde gerne veraltete Partitionen deaktivieren.  Es ist klar, dass es keine Rotation auf der Ebene einzelner Elemente gibt, aber es gibt einen Trick in Zabbix: Sie können globale Werte angeben (z. B. den gesamten Verlauf nicht länger als 90 Tage speichern) - Sie können alle Elemente aus diesen globalen Werten löschen. .  Und es wird geschafft!  Es gibt mehr zu diesem Thema bei Gitlab. <br><br>  Wir wollen das architektonische richtig machen: ob man das History Interface so erweitert, dass es im Grunde genommen wäre ... Im Allgemeinen möchte ich keine technischen Schulden hinterlassen, aber es wird gemacht.  Weil es notwendig ist, begann das mehr "Clickhouse" zu unterstützen. <br><br>  <b>A:</b> - Wie fühlst du dich dabei?  Wie sich herausstellt, erledigen Sie ziemlich viele Arbeiten, die nicht von einem Anbieter ausgeführt werden. <br><br>  <b>MM:</b> - Ich habe es wahrscheinlich nicht richtig ausgedrückt.  Das ist mein Hobby!  Ich bin nicht wirklich ein technischer Spezialist - ich bin ein Manager.  In meiner Freizeit übe ich. <br><br>  <b>A:</b> - Ich dachte, Sie machen das als Teil Ihres Kerngeschäfts ... <br><br>  <b>MM:</b> - Geschäft gibt mir einen coolen Ort zum Testen.  In der Tat empfehle ich sehr - es entlastet das Gehirn.  Irgendwo auf der Management-"Sache" würde ich dies sagen - wenn Sie von menschlichen Problemen zu diesen wechseln können.  Sie sind so cool gelöst!  Dies sind technische Probleme.  Sie haben programmiert und es funktioniert so, wie Sie es programmiert haben!  Schade, dass die Leute das nicht tun sollten. <br><br>  <b>A:</b> - Schreiben Sie über einen Proxy oder direkt an „Clickhouse“? <br><br>  <b>MM:</b> - Direkt.  Tatsächlich wird auch die geänderte Verlaufsschnittstelle, die für das "Elastix" verwendet wird, vererbt.  Die URL wird verwendet, dh über die http-Schnittstelle sendet "Zabbiks" "Clickhouse".  Was cool ist, Zabbix aggregiert, wenn es einen großen Datenstrom gibt, Tausende von Metriken in einer Packung, und dies fällt sehr cool auf das Clickhouse. <br><br>  <b>A:</b> - Tatsächlich schreibt er bachi für ihn? <br><br>  <b>MM:</b> - Ja.  Eine SQL-Abfrage, die von der URL ausgeführt wird, enthält normalerweise tausend Metriken.  Admins "Clickhouse" freuen sich einfach. <br><br>  Moderator: - Dies ist das Ende des Programms in diesem Raum.  Es gibt ein Abendprogramm, das organisiert wird, und es gibt etwas, das nur Sie tun können.  Und ich schlage vor, während Sie miteinander kommunizieren, darüber nachzudenken, welche interessanten Dinge Sie tun können ... Wenn Sie sich gegenseitig von Ihren Fällen erzählen, können Sie höchstwahrscheinlich darüber Bericht erstatten.  Wenn Sie miteinander diskutieren, finden Sie nur einen Überblick - das Programmkomitee nimmt Ihre Bewerbung an, überlegt und hilft, daraus eine gute, packende Geschichte zu machen.  Vielleicht haben Sie eine Geschichte über die Arbeit mit dem Programmkomitee? <br><br>  <b>MM:</b> - Eigentlich wird viel Feedback gegeben.  Ich hatte so viel Glück: Eine Person aus dem Programmkomitee lebt in meinem Tscheljabinsk, und Highload ist die einzige Konferenz, die so eng mit Sprechern zusammenarbeitet.  Ich habe so etwas nirgendwo anders gesehen.  Es ist sehr vorteilhaft!  Verschiedene Phasen: Die Jungs schauen sich das Video an, kommentieren die Folien - es passiert wirklich im Thema (Rechtschreibung, Tippfehler).  Sehr cool  Ich empfehle!  Versuchen Sie es selbst! <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/wbIpn44Z2_8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h3>  Ein bisschen Werbung :) </h3><br>  Vielen Dank für Ihren Aufenthalt bei uns.  Mögen Sie unsere Artikel?  Möchten Sie weitere interessante Materialien sehen?  Unterstützen Sie uns, indem Sie eine Bestellung aufgeben oder Ihren Freunden <a href="https://ua-hosting.company/cloudvps/nl">Cloud-basiertes VPS für Entwickler ab 4,99 US-Dollar</a> empfehlen, ein <b>einzigartiges Analogon zu Einstiegsservern, das wir für Sie erfunden haben:</b> <a href="https://habr.com/company/ua-hosting/blog/347386/">Die ganze Wahrheit über VPS (KVM) E5-2697 v3 (6 Kerne) 10 GB DDR4 480 GB SSD 1 Gbit / s ab 19 Dollar oder wie teilt man den Server?</a>  (Optionen sind mit RAID1 und RAID10, bis zu 24 Kernen und bis zu 40 GB DDR4 verfügbar). <br><br>  <b>Dell R730xd 2-mal billiger im Equinix Tier IV-Rechenzentrum in Amsterdam?</b>  Nur wir haben <b><a href="https://ua-hosting.company/serversnl">2 x Intel TetraDeca-Core Xeon 2 x E5-2697v3 2,6 GHz 14C 64 GB DDR4 4 x 960 GB SSD 1 Gbit / s 100 TV ab 199 US-Dollar</a> in den Niederlanden!</b>  <b><b>Dell R420 - 2x E5-2430 2,2 GHz 6C 128 GB DDR3 2x960 GB SSD 1 Gbit / s 100 TB - ab 99 US-Dollar!</b></b>  Lesen Sie mehr über <a href="https://habr.com/company/ua-hosting/blog/329618/">das Erstellen von Infrastruktur-Bldg.</a>  <a href="https://habr.com/company/ua-hosting/blog/329618/">Klasse mit Dell R730xd E5-2650 v4 Servern für 9.000 Euro für einen Cent?</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de485534/">https://habr.com/ru/post/de485534/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de485524/index.html">Die Zusammenstellung interessanter Materialien für den mobilen Entwickler # 330 (20. - 26. Januar)</a></li>
<li><a href="../de485526/index.html">Wer und warum will das Internet "geteilt" machen</a></li>
<li><a href="../de485528/index.html">So schließen Sie ein Softwareentwicklungsprojekt richtig ab</a></li>
<li><a href="../de485530/index.html">Geführtes Lernen</a></li>
<li><a href="../de485532/index.html">Interview Guide für diejenigen Programmierer, die sie nicht verstehen</a></li>
<li><a href="../de485536/index.html">Ist es möglich ein Flugzeug zu hacken - 2</a></li>
<li><a href="../de485538/index.html">Zabbix: alles hintereinander überwachen (am Beispiel von Redis)</a></li>
<li><a href="../de485542/index.html">Hinzufügen von Grafiken zu Notion</a></li>
<li><a href="../de485544/index.html">Schach als dynamisches System</a></li>
<li><a href="../de485546/index.html">Die Apokalypse kommt</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>