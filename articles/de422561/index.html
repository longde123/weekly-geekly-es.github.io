<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🦃 👊🏾 👂🏾 Wie Yandex Computer Vision einsetzte, um die Qualität von Videosendungen zu verbessern. DeepHD-Technologie 👎 ↖️ 😜</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wenn Menschen im Internet nach einem Bild oder Video suchen, fügen sie häufig den Ausdruck "in guter Qualität" hinzu. Qualität bezieht sich normalerwe...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie Yandex Computer Vision einsetzte, um die Qualität von Videosendungen zu verbessern. DeepHD-Technologie</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/422561/">  Wenn Menschen im Internet nach einem Bild oder Video suchen, fügen sie häufig den Ausdruck "in guter Qualität" hinzu.  Qualität bezieht sich normalerweise auf die Auflösung - Benutzer möchten, dass das Bild groß ist und gleichzeitig auf dem Bildschirm eines modernen Computers, Smartphones oder Fernsehgeräts gut aussieht.  Aber was ist, wenn die Quelle in guter Qualität einfach nicht existiert? <br><br>  Heute werden wir den Lesern von Habr erzählen, wie wir die Auflösung von Videos mithilfe neuronaler Netze in Echtzeit erhöhen können.  Sie erfahren auch, wie sich der theoretische Ansatz zur Lösung dieses Problems vom praktischen unterscheidet.  Wenn Sie sich nicht für technische Details interessieren, können Sie sicher durch den Beitrag scrollen - am Ende finden Sie Beispiele unserer Arbeit. <br><br><img width="800" src="https://habrastorage.org/webt/hx/lu/ak/hxluakxdy2mxmmskebqieei5zq4.png"><br><br>  Es gibt viele Videoinhalte im Internet in geringer Qualität und Auflösung.  Es können Filme sein, die vor Jahrzehnten gedreht wurden, oder Fernsehkanäle, die aus verschiedenen Gründen nicht in bester Qualität sind.  Wenn Benutzer ein solches Video auf den Vollbildmodus ausdehnen, wird das Bild trüb und unscharf.  Eine ideale Lösung für alte Filme wäre, den Originalfilm zu finden, ihn mit modernen Geräten zu scannen und manuell wiederherzustellen. Dies ist jedoch nicht immer möglich.  Sendungen sind noch komplizierter - sie müssen live verarbeitet werden.  In dieser Hinsicht besteht die akzeptabelste Option für uns darin, die Auflösung zu erhöhen und Artefakte mithilfe der Computer-Vision-Technologie zu bereinigen. <br><br><a name="habracut"></a>  In der Industrie wird die Aufgabe, Bilder und Videos zu vergrößern, ohne an Qualität zu verlieren, als Superauflösung bezeichnet.  Es wurden bereits viele Artikel zu diesem Thema verfasst, aber die Realität der „Kampf“ -Anwendung erwies sich als viel komplizierter und interessanter.  Kurz zu den Hauptproblemen, die wir in unserer eigenen DeepHD-Technologie lösen mussten: <br><br><ul><li>  Sie müssen in der Lage sein, Details wiederherzustellen, die aufgrund ihrer geringen Auflösung und Qualität nicht im Originalvideo enthalten waren, um sie zu „beenden“. </li><li>  Lösungen aus dem Bereich der Superauflösung stellen Details wieder her, machen jedoch nicht nur die Objekte im Video klar und detailliert, sondern auch Komprimierungsartefakte, die das Publikum nicht mögen. </li><li> Es gibt ein Problem beim Sammeln des Trainingsmusters - es ist eine große Anzahl von Paaren erforderlich, bei denen dasselbe Video sowohl in niedriger Auflösung als auch in hoher Qualität und in hoher Qualität vorhanden ist.  In der Realität gibt es normalerweise kein Qualitätspaar für schlechte Inhalte. </li><li>  Die Lösung sollte in Echtzeit funktionieren. </li></ul><br><h3>  Technologieauswahl </h3><br>  In den letzten Jahren hat die Verwendung neuronaler Netze zu erheblichen Erfolgen bei der Lösung fast aller Aufgaben der Bildverarbeitung geführt, und die Aufgabe der Superauflösung ist keine Ausnahme.  Wir haben die vielversprechendsten Lösungen basierend auf GAN gefunden (Generative Adversarial Networks, generative konkurrierende Netzwerke).  Mit ihnen können Sie hochauflösende fotorealistische Bilder erhalten, die durch fehlende Details ergänzt werden, z. B. durch Zeichnen von Haaren und Wimpern auf den Bildern von Personen. <br><br><img src="https://habrastorage.org/webt/gq/hl/kz/gqhlkzdwwmq3ad9p78j7wapfhzs.png"><br><br>  Im einfachsten Fall besteht ein neuronales Netzwerk aus zwei Teilen.  Der erste Teil - der Generator - nimmt ein Eingabebild auf und gibt eine doppelte Vergrößerung zurück.  Der zweite Teil - der Diskriminator - empfängt das erzeugte und "echte" Bild als Eingabe und versucht, es voneinander zu unterscheiden. <br><br><img width="700" src="https://habrastorage.org/webt/kn/3s/sc/kn3sscgtqtwqzcnga59cwaor-8y.png"><br><br><h3>  Vorbereitung des Trainingssets </h3><br>  Für das Training haben wir Dutzende von Clips in UltraHD-Qualität gesammelt.  Zuerst haben wir sie auf eine Auflösung von 1080p reduziert, um Referenzbeispiele zu erhalten.  Dann haben wir diese Videos halbiert und sie auf dem Weg mit einer anderen Bitrate komprimiert, um etwas Ähnliches wie ein echtes Video in geringer Qualität zu erhalten.  Wir haben die resultierenden Videos in Frames aufgeteilt und sie so verwendet, um das neuronale Netzwerk zu trainieren. <br><br><h3>  Deblocking </h3><br>  Natürlich wollten wir eine End-to-End-Lösung: Das neuronale Netzwerk so trainieren, dass hochauflösendes Video und Qualität direkt aus dem Original generiert werden.  Die GANs erwiesen sich jedoch als sehr launisch und versuchten ständig, die Komprimierungsartefakte zu verfeinern, anstatt sie zu beseitigen.  Daher musste ich den Prozess in mehrere Phasen unterteilen.  Das erste ist die Unterdrückung von Videokomprimierungsartefakten, die auch als Deblocking bezeichnet werden. <br><br>  Ein Beispiel für eine der Freigabemethoden: <br><br><img src="https://habrastorage.org/webt/0c/sg/zx/0csgzx4zwbtceyujcgcay4mclac.jpeg"><br><br>  Zu diesem Zeitpunkt haben wir die Standardabweichung zwischen dem generierten und dem ursprünglichen Frame minimiert.  Obwohl wir die Auflösung des Bildes erhöht haben, haben wir aufgrund der Regression auf den Durchschnitt keine echte Erhöhung der Auflösung erzielt: Das neuronale Netzwerk, das nicht wusste, in welchen bestimmten Pixeln ein bestimmter Rand im Bild verläuft, musste mehrere Optionen mitteln, um ein verschwommenes Ergebnis zu erhalten.  Die Hauptsache, die wir in dieser Phase erreicht haben, ist die Beseitigung von Videokomprimierungsartefakten, sodass das generative Netzwerk in der nächsten Phase nur benötigt wird, um die Klarheit zu erhöhen und die fehlenden kleinen Details und Texturen hinzuzufügen.  Nach Hunderten von Experimenten haben wir die optimale Architektur in Bezug auf Leistung und Qualität ausgewählt, die vage an die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DRCN-</a> Architektur erinnert: <br><br><img width="800" src="https://habrastorage.org/webt/oq/au/pc/oqaupcp8k9m4rdspvx8rrrbhpy0.png"><br><br>  Die Hauptidee einer solchen Architektur ist der Wunsch, die tiefste Architektur zu erhalten, ohne Probleme mit der Konvergenz in ihrer Ausbildung zu haben.  Einerseits extrahiert jede nachfolgende Faltungsschicht immer komplexere Merkmale des Eingabebildes, wodurch Sie bestimmen können, welche Art von Objekt sich an einem bestimmten Punkt im Bild befindet, und komplexe und stark beschädigte Teile wiederherstellen können.  Andererseits bleibt der Abstand in der Grafik eines neuronalen Netzwerks von einer seiner Schichten zum Ausgang klein, was die Konvergenz des neuronalen Netzwerks verbessert und die Verwendung einer großen Anzahl von Schichten ermöglicht. <br><br><h3>  Generatives Netzwerktraining </h3><br>  Wir haben die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SRGAN-</a> Architektur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">als</a> Grundlage für ein neuronales Netzwerk zur Erhöhung der Auflösung verwendet.  Bevor Sie ein wettbewerbsfähiges Netzwerk trainieren, müssen Sie den Generator vorab trainieren - trainieren Sie ihn auf die gleiche Weise wie in der Deblockierungsphase.  Andernfalls gibt der Generator zu Beginn des Trainings nur Rauschen zurück, der Diskriminator beginnt sofort zu „gewinnen“ - er lernt leicht, Rauschen von realen Frames zu unterscheiden, und es funktioniert kein Training. <br><br><img width="800" src="https://habrastorage.org/webt/tx/pb/r-/txpbr-pwdisdcwj62mrd6h4wuxm.png"><br><br>  Dann trainieren wir GAN, aber es gibt einige Nuancen.  Für uns ist es wichtig, dass der Generator nicht nur fotorealistische Bilder erstellt, sondern auch die darauf verfügbaren Informationen speichert.  Zu diesem Zweck fügen wir der klassischen GAN-Architektur die Funktion zum Verlust von Inhalten hinzu.  Es repräsentiert mehrere Schichten des neuronalen VGG19-Netzwerks, die auf dem Standard-ImageNet-Datensatz trainiert wurden.  Diese Ebenen verwandeln das Bild in eine Feature-Map, die Informationen zum Inhalt des Bildes enthält.  Die Verlustfunktion minimiert den Abstand zwischen solchen Karten, die aus den erzeugten und ursprünglichen Rahmen erhalten werden.  Das Vorhandensein einer solchen Verlustfunktion ermöglicht es auch, den Generator in den ersten Trainingsschritten nicht zu verderben, wenn der Diskriminator noch nicht trainiert ist und nutzlose Informationen liefert. <br><br><img width="800" src="https://habrastorage.org/webt/d7/5p/uu/d75puuaa6jqsy6wmvknjqo-hh84.png"><br><br><h3>  Beschleunigung des neuronalen Netzes </h3><br>  Alles lief gut und nach einer Reihe von Experimenten erhielten wir ein gutes Modell, das bereits auf alte Filme angewendet werden konnte.  Es war jedoch immer noch zu langsam, um Streaming-Videos zu verarbeiten.  Es stellte sich heraus, dass es unmöglich ist, den Generator einfach ohne einen signifikanten Qualitätsverlust des endgültigen Modells zu reduzieren.  Dann kam uns der Wissensdestillationsansatz zu Hilfe.  Bei dieser Methode wird ein leichteres Modell so trainiert, dass die Ergebnisse eines schwereren Modells wiederholt werden.  Wir haben viele echte Videos in geringer Qualität aufgenommen, sie mit dem im vorherigen Schritt erhaltenen generativen neuronalen Netzwerk verarbeitet und das leichtere Netzwerk trainiert, um das gleiche Ergebnis aus denselben Frames zu erzielen.  Aufgrund dieser Technik haben wir ein Netzwerk erhalten, dessen Qualität dem Original nicht sehr unterlegen ist, das jedoch zehnmal schneller ist: Um einen Fernsehkanal mit einer Auflösung von 576p zu verarbeiten, ist eine NVIDIA Tesla V100-Karte erforderlich. <br><br><img width="800" src="https://habrastorage.org/webt/15/b3/eg/15b3eguc_ikkl-fdaclwdsga2ka.png"><br><br><h3>  Bewertung der Qualität von Lösungen </h3><br>  Der vielleicht schwierigste Moment bei der Arbeit mit generativen Netzwerken ist die Bewertung der Qualität der resultierenden Modelle.  Es gibt keine eindeutige Fehlerfunktion, wie zum Beispiel bei der Lösung des Klassifizierungsproblems.  Stattdessen kennen wir nur die Genauigkeit des Diskriminators, die nicht die Qualität des Generators widerspiegelt, der uns interessiert (ein Leser, der mit diesem Bereich gut vertraut war, könnte vorschlagen, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die Wasserstein-Metrik zu verwenden</a> , aber leider ergab sich ein merklich schlechteres Ergebnis). <br><br>  Die Leute haben uns geholfen, dieses Problem zu lösen.  Wir haben Benutzern des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Yandex.Tolok-</a> Dienstes Bildpaare gezeigt, von denen eines die Quelle war und das andere von einem neuronalen Netzwerk verarbeitet wurde oder beide von verschiedenen Versionen unserer Lösungen verarbeitet wurden.  Gegen eine Gebühr wählten die Benutzer ein besseres Video aus einem Paar aus, sodass wir einen statistisch signifikanten Vergleich der Versionen erhielten, selbst bei Änderungen, die mit dem Auge schwer zu erkennen sind.  Unsere endgültigen Modelle gewinnen in mehr als 70% der Fälle, was ziemlich viel ist, da die Benutzer nur wenige Sekunden damit verbringen, ein paar Videos zu bewerten. <br><br>  Ein interessantes Ergebnis war auch die Tatsache, dass Videos mit einer Auflösung von 576p, die durch die DeepHD-Technologie auf 720p erhöht wurden, in 60% der Fälle dasselbe Originalvideo mit einer Auflösung von 720p übertreffen - d. H.  Die Verarbeitung erhöht nicht nur die Auflösung des Videos, sondern verbessert auch dessen visuelle Wahrnehmung. <br><br><h3>  Beispiele </h3><br>  Im Frühjahr haben wir die DeepHD-Technologie an mehreren alten Filmen getestet, die im KinoPoisk zu sehen sind: „ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Rainbow</a> “ von Mark Donskoy (1943), „ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Cranes are Flying</a> “ von Mikhail Kalatozov (1957), „ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">My Dear Man</a> “ von Joseph Kheifits (1958), „ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">The Fate of a Man</a> “. Sergei Bondarchuk (1959), " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ivan Childhood</a> " von Andrei Tarkovsky (1962), " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vater eines Soldaten</a> " Rezo Chkheidze (1964) und " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tango of Our Childhood</a> " von Albert Mkrtchyan (1985). <br><br><img width="800" src="https://habrastorage.org/webt/zh/un/-d/zhun-dugkeykn9bmodmrgrjfxma.png"><br><br>  Der Unterschied zwischen den Versionen vor und nach der Verarbeitung macht sich besonders bemerkbar, wenn Sie sich die Details ansehen: Studieren Sie die Mimik der Helden in Nahaufnahmen, berücksichtigen Sie die Textur der Kleidung oder ein Stoffmuster.  Es war möglich, einige der Mängel der Digitalisierung zu kompensieren: zum Beispiel die Überbelichtung der Gesichter zu beseitigen oder sichtbarere Objekte im Schatten zu platzieren. <br><br>  Später wurde die DeepHD-Technologie eingesetzt, um die Qualität der Sendungen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einiger</a> Kanäle im Yandex.Air-Dienst zu verbessern.  Das Erkennen solcher Inhalte ist durch das <b>dHD-</b> Tag einfach. <br><br>  Jetzt können Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">auf Yandex</a> in verbesserter Qualität „Die Schneekönigin“, „Bremer Stadtmusiker“, „Goldene Antilope“ und andere beliebte Cartoons des Filmstudios Sojusmultfilm sehen.  Einige Beispiele für Dynamik sind im Video zu sehen: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/ainlhiNn0Yk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Für anspruchsvolle Betrachter wird der Unterschied besonders deutlich: Das Bild ist schärfer geworden, Baumblätter, Schneeflocken, Sterne am Nachthimmel über dem Dschungel und andere kleine Details sind besser sichtbar. <br><br>  Mehr ist mehr. <br><br><h3>  Nützliche Links </h3><br>  Jiwon Kim, Jung Kwon Lee und Kyoung Mu Lee Tief rekursives Faltungsnetzwerk für Bild-Superauflösung [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arXiv: 1511.04491</a> ]. <br><br>  Christian Ledig et al.  Fotorealistische Einzelbild-Superauflösung unter Verwendung eines generativen gegnerischen Netzwerks [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arXiv: 1609.04802</a> ]. <br><br>  Mehdi SM Sajjadi, Bernhard Schölkopf, Michael Hirsch EnhanceNet: Einzelbild-Superauflösung durch automatisierte Textur-Synthese [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arXiv: 1612.07919</a> ]. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de422561/">https://habr.com/ru/post/de422561/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de422547/index.html">Um Spectre und Meltdown zu eliminieren, müssen Sie möglicherweise einen völlig neuen Prozessortyp erstellen</a></li>
<li><a href="../de422549/index.html">Corda: Kotlin</a></li>
<li><a href="../de422551/index.html">So stehlen Sie Geld von einer kontaktlosen Karte und Apple Pay</a></li>
<li><a href="../de422553/index.html">Die offizielle Mega-Browser-Erweiterung stiehlt Dateifreigabedaten und Kryptowährung</a></li>
<li><a href="../de422555/index.html">Android-Multimodul-Architektur. Von A bis Z.</a></li>
<li><a href="../de422565/index.html">Skillbox Friday Webinars: Alles für Programmierer und Designer</a></li>
<li><a href="../de422569/index.html">Stündliche Zeiterfassungs-App</a></li>
<li><a href="../de422571/index.html">Parallelisieren von Aufgaben mit Abhängigkeiten - .NET-Beispiel</a></li>
<li><a href="../de422573/index.html">Das Reverse Engineering des Renderings von The Witcher 3</a></li>
<li><a href="../de422575/index.html">Seltenes Einstiegstelefon</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>