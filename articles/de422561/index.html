<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü¶É üëäüèæ üëÇüèæ Wie Yandex Computer Vision einsetzte, um die Qualit√§t von Videosendungen zu verbessern. DeepHD-Technologie üëé ‚ÜñÔ∏è üòú</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wenn Menschen im Internet nach einem Bild oder Video suchen, f√ºgen sie h√§ufig den Ausdruck "in guter Qualit√§t" hinzu. Qualit√§t bezieht sich normalerwe...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie Yandex Computer Vision einsetzte, um die Qualit√§t von Videosendungen zu verbessern. DeepHD-Technologie</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/422561/">  Wenn Menschen im Internet nach einem Bild oder Video suchen, f√ºgen sie h√§ufig den Ausdruck "in guter Qualit√§t" hinzu.  Qualit√§t bezieht sich normalerweise auf die Aufl√∂sung - Benutzer m√∂chten, dass das Bild gro√ü ist und gleichzeitig auf dem Bildschirm eines modernen Computers, Smartphones oder Fernsehger√§ts gut aussieht.  Aber was ist, wenn die Quelle in guter Qualit√§t einfach nicht existiert? <br><br>  Heute werden wir den Lesern von Habr erz√§hlen, wie wir die Aufl√∂sung von Videos mithilfe neuronaler Netze in Echtzeit erh√∂hen k√∂nnen.  Sie erfahren auch, wie sich der theoretische Ansatz zur L√∂sung dieses Problems vom praktischen unterscheidet.  Wenn Sie sich nicht f√ºr technische Details interessieren, k√∂nnen Sie sicher durch den Beitrag scrollen - am Ende finden Sie Beispiele unserer Arbeit. <br><br><img width="800" src="https://habrastorage.org/webt/hx/lu/ak/hxluakxdy2mxmmskebqieei5zq4.png"><br><br>  Es gibt viele Videoinhalte im Internet in geringer Qualit√§t und Aufl√∂sung.  Es k√∂nnen Filme sein, die vor Jahrzehnten gedreht wurden, oder Fernsehkan√§le, die aus verschiedenen Gr√ºnden nicht in bester Qualit√§t sind.  Wenn Benutzer ein solches Video auf den Vollbildmodus ausdehnen, wird das Bild tr√ºb und unscharf.  Eine ideale L√∂sung f√ºr alte Filme w√§re, den Originalfilm zu finden, ihn mit modernen Ger√§ten zu scannen und manuell wiederherzustellen. Dies ist jedoch nicht immer m√∂glich.  Sendungen sind noch komplizierter - sie m√ºssen live verarbeitet werden.  In dieser Hinsicht besteht die akzeptabelste Option f√ºr uns darin, die Aufl√∂sung zu erh√∂hen und Artefakte mithilfe der Computer-Vision-Technologie zu bereinigen. <br><br><a name="habracut"></a>  In der Industrie wird die Aufgabe, Bilder und Videos zu vergr√∂√üern, ohne an Qualit√§t zu verlieren, als Superaufl√∂sung bezeichnet.  Es wurden bereits viele Artikel zu diesem Thema verfasst, aber die Realit√§t der ‚ÄûKampf‚Äú -Anwendung erwies sich als viel komplizierter und interessanter.  Kurz zu den Hauptproblemen, die wir in unserer eigenen DeepHD-Technologie l√∂sen mussten: <br><br><ul><li>  Sie m√ºssen in der Lage sein, Details wiederherzustellen, die aufgrund ihrer geringen Aufl√∂sung und Qualit√§t nicht im Originalvideo enthalten waren, um sie zu ‚Äûbeenden‚Äú. </li><li>  L√∂sungen aus dem Bereich der Superaufl√∂sung stellen Details wieder her, machen jedoch nicht nur die Objekte im Video klar und detailliert, sondern auch Komprimierungsartefakte, die das Publikum nicht m√∂gen. </li><li> Es gibt ein Problem beim Sammeln des Trainingsmusters - es ist eine gro√üe Anzahl von Paaren erforderlich, bei denen dasselbe Video sowohl in niedriger Aufl√∂sung als auch in hoher Qualit√§t und in hoher Qualit√§t vorhanden ist.  In der Realit√§t gibt es normalerweise kein Qualit√§tspaar f√ºr schlechte Inhalte. </li><li>  Die L√∂sung sollte in Echtzeit funktionieren. </li></ul><br><h3>  Technologieauswahl </h3><br>  In den letzten Jahren hat die Verwendung neuronaler Netze zu erheblichen Erfolgen bei der L√∂sung fast aller Aufgaben der Bildverarbeitung gef√ºhrt, und die Aufgabe der Superaufl√∂sung ist keine Ausnahme.  Wir haben die vielversprechendsten L√∂sungen basierend auf GAN gefunden (Generative Adversarial Networks, generative konkurrierende Netzwerke).  Mit ihnen k√∂nnen Sie hochaufl√∂sende fotorealistische Bilder erhalten, die durch fehlende Details erg√§nzt werden, z. B. durch Zeichnen von Haaren und Wimpern auf den Bildern von Personen. <br><br><img src="https://habrastorage.org/webt/gq/hl/kz/gqhlkzdwwmq3ad9p78j7wapfhzs.png"><br><br>  Im einfachsten Fall besteht ein neuronales Netzwerk aus zwei Teilen.  Der erste Teil - der Generator - nimmt ein Eingabebild auf und gibt eine doppelte Vergr√∂√üerung zur√ºck.  Der zweite Teil - der Diskriminator - empf√§ngt das erzeugte und "echte" Bild als Eingabe und versucht, es voneinander zu unterscheiden. <br><br><img width="700" src="https://habrastorage.org/webt/kn/3s/sc/kn3sscgtqtwqzcnga59cwaor-8y.png"><br><br><h3>  Vorbereitung des Trainingssets </h3><br>  F√ºr das Training haben wir Dutzende von Clips in UltraHD-Qualit√§t gesammelt.  Zuerst haben wir sie auf eine Aufl√∂sung von 1080p reduziert, um Referenzbeispiele zu erhalten.  Dann haben wir diese Videos halbiert und sie auf dem Weg mit einer anderen Bitrate komprimiert, um etwas √Ñhnliches wie ein echtes Video in geringer Qualit√§t zu erhalten.  Wir haben die resultierenden Videos in Frames aufgeteilt und sie so verwendet, um das neuronale Netzwerk zu trainieren. <br><br><h3>  Deblocking </h3><br>  Nat√ºrlich wollten wir eine End-to-End-L√∂sung: Das neuronale Netzwerk so trainieren, dass hochaufl√∂sendes Video und Qualit√§t direkt aus dem Original generiert werden.  Die GANs erwiesen sich jedoch als sehr launisch und versuchten st√§ndig, die Komprimierungsartefakte zu verfeinern, anstatt sie zu beseitigen.  Daher musste ich den Prozess in mehrere Phasen unterteilen.  Das erste ist die Unterdr√ºckung von Videokomprimierungsartefakten, die auch als Deblocking bezeichnet werden. <br><br>  Ein Beispiel f√ºr eine der Freigabemethoden: <br><br><img src="https://habrastorage.org/webt/0c/sg/zx/0csgzx4zwbtceyujcgcay4mclac.jpeg"><br><br>  Zu diesem Zeitpunkt haben wir die Standardabweichung zwischen dem generierten und dem urspr√ºnglichen Frame minimiert.  Obwohl wir die Aufl√∂sung des Bildes erh√∂ht haben, haben wir aufgrund der Regression auf den Durchschnitt keine echte Erh√∂hung der Aufl√∂sung erzielt: Das neuronale Netzwerk, das nicht wusste, in welchen bestimmten Pixeln ein bestimmter Rand im Bild verl√§uft, musste mehrere Optionen mitteln, um ein verschwommenes Ergebnis zu erhalten.  Die Hauptsache, die wir in dieser Phase erreicht haben, ist die Beseitigung von Videokomprimierungsartefakten, sodass das generative Netzwerk in der n√§chsten Phase nur ben√∂tigt wird, um die Klarheit zu erh√∂hen und die fehlenden kleinen Details und Texturen hinzuzuf√ºgen.  Nach Hunderten von Experimenten haben wir die optimale Architektur in Bezug auf Leistung und Qualit√§t ausgew√§hlt, die vage an die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DRCN-</a> Architektur erinnert: <br><br><img width="800" src="https://habrastorage.org/webt/oq/au/pc/oqaupcp8k9m4rdspvx8rrrbhpy0.png"><br><br>  Die Hauptidee einer solchen Architektur ist der Wunsch, die tiefste Architektur zu erhalten, ohne Probleme mit der Konvergenz in ihrer Ausbildung zu haben.  Einerseits extrahiert jede nachfolgende Faltungsschicht immer komplexere Merkmale des Eingabebildes, wodurch Sie bestimmen k√∂nnen, welche Art von Objekt sich an einem bestimmten Punkt im Bild befindet, und komplexe und stark besch√§digte Teile wiederherstellen k√∂nnen.  Andererseits bleibt der Abstand in der Grafik eines neuronalen Netzwerks von einer seiner Schichten zum Ausgang klein, was die Konvergenz des neuronalen Netzwerks verbessert und die Verwendung einer gro√üen Anzahl von Schichten erm√∂glicht. <br><br><h3>  Generatives Netzwerktraining </h3><br>  Wir haben die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SRGAN-</a> Architektur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">als</a> Grundlage f√ºr ein neuronales Netzwerk zur Erh√∂hung der Aufl√∂sung verwendet.  Bevor Sie ein wettbewerbsf√§higes Netzwerk trainieren, m√ºssen Sie den Generator vorab trainieren - trainieren Sie ihn auf die gleiche Weise wie in der Deblockierungsphase.  Andernfalls gibt der Generator zu Beginn des Trainings nur Rauschen zur√ºck, der Diskriminator beginnt sofort zu ‚Äûgewinnen‚Äú - er lernt leicht, Rauschen von realen Frames zu unterscheiden, und es funktioniert kein Training. <br><br><img width="800" src="https://habrastorage.org/webt/tx/pb/r-/txpbr-pwdisdcwj62mrd6h4wuxm.png"><br><br>  Dann trainieren wir GAN, aber es gibt einige Nuancen.  F√ºr uns ist es wichtig, dass der Generator nicht nur fotorealistische Bilder erstellt, sondern auch die darauf verf√ºgbaren Informationen speichert.  Zu diesem Zweck f√ºgen wir der klassischen GAN-Architektur die Funktion zum Verlust von Inhalten hinzu.  Es repr√§sentiert mehrere Schichten des neuronalen VGG19-Netzwerks, die auf dem Standard-ImageNet-Datensatz trainiert wurden.  Diese Ebenen verwandeln das Bild in eine Feature-Map, die Informationen zum Inhalt des Bildes enth√§lt.  Die Verlustfunktion minimiert den Abstand zwischen solchen Karten, die aus den erzeugten und urspr√ºnglichen Rahmen erhalten werden.  Das Vorhandensein einer solchen Verlustfunktion erm√∂glicht es auch, den Generator in den ersten Trainingsschritten nicht zu verderben, wenn der Diskriminator noch nicht trainiert ist und nutzlose Informationen liefert. <br><br><img width="800" src="https://habrastorage.org/webt/d7/5p/uu/d75puuaa6jqsy6wmvknjqo-hh84.png"><br><br><h3>  Beschleunigung des neuronalen Netzes </h3><br>  Alles lief gut und nach einer Reihe von Experimenten erhielten wir ein gutes Modell, das bereits auf alte Filme angewendet werden konnte.  Es war jedoch immer noch zu langsam, um Streaming-Videos zu verarbeiten.  Es stellte sich heraus, dass es unm√∂glich ist, den Generator einfach ohne einen signifikanten Qualit√§tsverlust des endg√ºltigen Modells zu reduzieren.  Dann kam uns der Wissensdestillationsansatz zu Hilfe.  Bei dieser Methode wird ein leichteres Modell so trainiert, dass die Ergebnisse eines schwereren Modells wiederholt werden.  Wir haben viele echte Videos in geringer Qualit√§t aufgenommen, sie mit dem im vorherigen Schritt erhaltenen generativen neuronalen Netzwerk verarbeitet und das leichtere Netzwerk trainiert, um das gleiche Ergebnis aus denselben Frames zu erzielen.  Aufgrund dieser Technik haben wir ein Netzwerk erhalten, dessen Qualit√§t dem Original nicht sehr unterlegen ist, das jedoch zehnmal schneller ist: Um einen Fernsehkanal mit einer Aufl√∂sung von 576p zu verarbeiten, ist eine NVIDIA Tesla V100-Karte erforderlich. <br><br><img width="800" src="https://habrastorage.org/webt/15/b3/eg/15b3eguc_ikkl-fdaclwdsga2ka.png"><br><br><h3>  Bewertung der Qualit√§t von L√∂sungen </h3><br>  Der vielleicht schwierigste Moment bei der Arbeit mit generativen Netzwerken ist die Bewertung der Qualit√§t der resultierenden Modelle.  Es gibt keine eindeutige Fehlerfunktion, wie zum Beispiel bei der L√∂sung des Klassifizierungsproblems.  Stattdessen kennen wir nur die Genauigkeit des Diskriminators, die nicht die Qualit√§t des Generators widerspiegelt, der uns interessiert (ein Leser, der mit diesem Bereich gut vertraut war, k√∂nnte vorschlagen, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die Wasserstein-Metrik zu verwenden</a> , aber leider ergab sich ein merklich schlechteres Ergebnis). <br><br>  Die Leute haben uns geholfen, dieses Problem zu l√∂sen.  Wir haben Benutzern des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Yandex.Tolok-</a> Dienstes Bildpaare gezeigt, von denen eines die Quelle war und das andere von einem neuronalen Netzwerk verarbeitet wurde oder beide von verschiedenen Versionen unserer L√∂sungen verarbeitet wurden.  Gegen eine Geb√ºhr w√§hlten die Benutzer ein besseres Video aus einem Paar aus, sodass wir einen statistisch signifikanten Vergleich der Versionen erhielten, selbst bei √Ñnderungen, die mit dem Auge schwer zu erkennen sind.  Unsere endg√ºltigen Modelle gewinnen in mehr als 70% der F√§lle, was ziemlich viel ist, da die Benutzer nur wenige Sekunden damit verbringen, ein paar Videos zu bewerten. <br><br>  Ein interessantes Ergebnis war auch die Tatsache, dass Videos mit einer Aufl√∂sung von 576p, die durch die DeepHD-Technologie auf 720p erh√∂ht wurden, in 60% der F√§lle dasselbe Originalvideo mit einer Aufl√∂sung von 720p √ºbertreffen - d. H.  Die Verarbeitung erh√∂ht nicht nur die Aufl√∂sung des Videos, sondern verbessert auch dessen visuelle Wahrnehmung. <br><br><h3>  Beispiele </h3><br>  Im Fr√ºhjahr haben wir die DeepHD-Technologie an mehreren alten Filmen getestet, die im KinoPoisk zu sehen sind: ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Rainbow</a> ‚Äú von Mark Donskoy (1943), ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Cranes are Flying</a> ‚Äú von Mikhail Kalatozov (1957), ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">My Dear Man</a> ‚Äú von Joseph Kheifits (1958), ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">The Fate of a Man</a> ‚Äú. Sergei Bondarchuk (1959), " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ivan Childhood</a> " von Andrei Tarkovsky (1962), " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vater eines Soldaten</a> " Rezo Chkheidze (1964) und " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tango of Our Childhood</a> " von Albert Mkrtchyan (1985). <br><br><img width="800" src="https://habrastorage.org/webt/zh/un/-d/zhun-dugkeykn9bmodmrgrjfxma.png"><br><br>  Der Unterschied zwischen den Versionen vor und nach der Verarbeitung macht sich besonders bemerkbar, wenn Sie sich die Details ansehen: Studieren Sie die Mimik der Helden in Nahaufnahmen, ber√ºcksichtigen Sie die Textur der Kleidung oder ein Stoffmuster.  Es war m√∂glich, einige der M√§ngel der Digitalisierung zu kompensieren: zum Beispiel die √úberbelichtung der Gesichter zu beseitigen oder sichtbarere Objekte im Schatten zu platzieren. <br><br>  Sp√§ter wurde die DeepHD-Technologie eingesetzt, um die Qualit√§t der Sendungen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einiger</a> Kan√§le im Yandex.Air-Dienst zu verbessern.  Das Erkennen solcher Inhalte ist durch das <b>dHD-</b> Tag einfach. <br><br>  Jetzt k√∂nnen Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">auf Yandex</a> in verbesserter Qualit√§t ‚ÄûDie Schneek√∂nigin‚Äú, ‚ÄûBremer Stadtmusiker‚Äú, ‚ÄûGoldene Antilope‚Äú und andere beliebte Cartoons des Filmstudios Sojusmultfilm sehen.  Einige Beispiele f√ºr Dynamik sind im Video zu sehen: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/ainlhiNn0Yk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  F√ºr anspruchsvolle Betrachter wird der Unterschied besonders deutlich: Das Bild ist sch√§rfer geworden, Baumbl√§tter, Schneeflocken, Sterne am Nachthimmel √ºber dem Dschungel und andere kleine Details sind besser sichtbar. <br><br>  Mehr ist mehr. <br><br><h3>  N√ºtzliche Links </h3><br>  Jiwon Kim, Jung Kwon Lee und Kyoung Mu Lee Tief rekursives Faltungsnetzwerk f√ºr Bild-Superaufl√∂sung [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arXiv: 1511.04491</a> ]. <br><br>  Christian Ledig et al.  Fotorealistische Einzelbild-Superaufl√∂sung unter Verwendung eines generativen gegnerischen Netzwerks [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arXiv: 1609.04802</a> ]. <br><br>  Mehdi SM Sajjadi, Bernhard Sch√∂lkopf, Michael Hirsch EnhanceNet: Einzelbild-Superaufl√∂sung durch automatisierte Textur-Synthese [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arXiv: 1612.07919</a> ]. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de422561/">https://habr.com/ru/post/de422561/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de422547/index.html">Um Spectre und Meltdown zu eliminieren, m√ºssen Sie m√∂glicherweise einen v√∂llig neuen Prozessortyp erstellen</a></li>
<li><a href="../de422549/index.html">Corda: Kotlin</a></li>
<li><a href="../de422551/index.html">So stehlen Sie Geld von einer kontaktlosen Karte und Apple Pay</a></li>
<li><a href="../de422553/index.html">Die offizielle Mega-Browser-Erweiterung stiehlt Dateifreigabedaten und Kryptow√§hrung</a></li>
<li><a href="../de422555/index.html">Android-Multimodul-Architektur. Von A bis Z.</a></li>
<li><a href="../de422565/index.html">Skillbox Friday Webinars: Alles f√ºr Programmierer und Designer</a></li>
<li><a href="../de422569/index.html">St√ºndliche Zeiterfassungs-App</a></li>
<li><a href="../de422571/index.html">Parallelisieren von Aufgaben mit Abh√§ngigkeiten - .NET-Beispiel</a></li>
<li><a href="../de422573/index.html">Das Reverse Engineering des Renderings von The Witcher 3</a></li>
<li><a href="../de422575/index.html">Seltenes Einstiegstelefon</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>