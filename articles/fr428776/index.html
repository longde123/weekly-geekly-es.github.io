<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚¨áÔ∏è üçÖ üç® Une nouvelle prise de conscience de la curiosit√© en IA. Entra√Ænement avec une r√©compense qui d√©pend de la difficult√© de pr√©dire le r√©sultat üêõ üöà üíí</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Les progr√®s dans le jeu "Montezuma's Revenge" ont √©t√© consid√©r√©s par beaucoup comme un synonyme de r√©alisations dans l'√©tude des environnements inconn...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Une nouvelle prise de conscience de la curiosit√© en IA. Entra√Ænement avec une r√©compense qui d√©pend de la difficult√© de pr√©dire le r√©sultat</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/428776/"><img src="https://habrastorage.org/getpro/habr/post_images/49b/e3e/fbf/49be3efbf10821888431e9529873176a.svg" width="780"><br>  <i><font color="gray">Les progr√®s dans le jeu "Montezuma's Revenge" ont √©t√© consid√©r√©s par beaucoup comme un synonyme de r√©alisations dans l'√©tude des environnements inconnus</font></i> <br><br>  Nous avons d√©velopp√© une m√©thode de distillation en r√©seau al√©atoire (RND) bas√©e sur la pr√©diction qui encourage les agents d'apprentissage renforc√©s √† explorer l'environnement par curiosit√©.  Cette m√©thode a pour la premi√®re fois d√©pass√© les r√©sultats humains moyens dans le jeu informatique <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">"Montezuma Revenge"</a> (sauf pour l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">application</a> anonyme de l'ICLR, o√π le r√©sultat est pire que le n√¥tre).  <b>RND fait preuve d'une efficacit√© ultramoderne, trouve p√©riodiquement les 24 chambres et passe le premier niveau sans d√©monstration pr√©alable et sans acc√®s √† l'√©tat de base du jeu.</b> <br><a name="habracut"></a><br>  La m√©thode RND stimule la transition d'un agent vers des √©tats inconnus en mesurant la complexit√© de pr√©dire le r√©sultat de la superposition d'un r√©seau neuronal al√©atoire al√©atoire sur des donn√©es d'√©tat.  Si la condition n'est pas famili√®re, le r√©sultat final est difficile √† pr√©voir, ce qui signifie que la r√©compense est √©lev√©e.  La m√©thode peut √™tre appliqu√©e √† n'importe quel algorithme d'apprentissage par renforcement; elle est simple √† mettre en ≈ìuvre et efficace pour la mise √† l'√©chelle.  Vous trouverez ci-dessous un lien vers la mise en ≈ìuvre de RND, qui reproduit les r√©sultats de notre article. <br><br><blockquote>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Texte d'un article scientifique</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">code</a> </blockquote><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/40VZeFppDEM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h1>  R√©sultats dans Montezuma‚Äôs Revenge </h1><br>  Pour atteindre l'objectif souhait√©, l'agent doit d'abord √©tudier quelles actions sont possibles dans l'environnement et ce qui constitue un progr√®s vers l'objectif.  De nombreux signaux de r√©compense dans les jeux fournissent un programme, donc m√™me des strat√©gies de recherche simples suffisent pour atteindre l'objectif.  Dans le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">travail initial avec la pr√©sentation de DQN</a> , Montezuma‚Äôs Revenge √©tait le <b>seul jeu o√π DQN montrait le r√©sultat de 0% du score humain moyen (4700)</b> .  Il est peu probable que les strat√©gies d'intelligence simples collectent des r√©compenses et ne trouvent pas plus de quelques pi√®ces au niveau.  Depuis lors, les progr√®s dans le jeu Montezuma's Revenge ont √©t√© consid√©r√©s par beaucoup comme synonymes d'avanc√©es dans l'√©tude d'un environnement inconnu. <br><br>  Des progr√®s significatifs ont √©t√© r√©alis√©s en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">2016</a> en combinant DQN avec un bonus sur le comptoir, ce qui a permis √† l'agent de trouver 15 chambres et d'obtenir le score le plus √©lev√© de 6600 avec une moyenne d'environ 3700. Depuis lors, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">des</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">am√©liorations</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">significatives</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">du</a> r√©sultat n'ont √©t√© obtenues que gr√¢ce √† des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d√©monstrations</a> d' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">experts</a> ou en acc√©dant aux √©tats de base de l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">√©mulateur</a> . <br><br>  Nous avons men√© une exp√©rience RND √† grande √©chelle avec 1024 travailleurs, obtenant un <b>r√©sultat moyen de 10 000 sur 9 d√©marrages</b> et un <b>meilleur r√©sultat moyen de 14 500</b> .  Dans chaque cas, l'agent a trouv√© 20 √† 22 chambres.  De plus, dans un lancement plus petit mais plus long (sur 10), le <b>r√©sultat maximum est de 17 500, ce qui correspond √† passer le premier niveau et √† trouver les 24 chambres</b> .  Le graphique ci-dessous compare ces deux exp√©riences, montrant la valeur moyenne en fonction des param√®tres de mise √† jour. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cde/262/bde/cde262bde2a497752d59599ba524d41b.svg" width="780"><br><br>  La visualisation ci-dessous montre la progression de l'exp√©rience √† plus petite √©chelle.  L'agent, sous l'influence de la curiosit√©, ouvre de nouvelles salles et trouve des moyens de marquer des points.Au cours de l'entra√Ænement, cette r√©compense externe l'oblige √† y retourner plus tard. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Votre navigateur ne prend pas en charge la vid√©o HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/animated-pyramid_10-29e.mp4" type="video/mp4"></video></div></div></div><br>  <i><font color="gray">Les pi√®ces d√©couvertes par l'agent et le r√©sultat moyen lors de la formation.</font></i>  <i><font color="gray">Le degr√© de transparence de la pi√®ce correspond au nombre de fois sur 10 passages de l'agent d√©tect√©.</font></i>  <i><font color="gray"><a href="">Vid√©o</a></font></i> <br><br><h1>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">√âtude √† grande √©chelle de l'apprentissage bas√©e sur la curiosit√©</a> </h1><br>  Avant de d√©velopper RND, nous, avec le personnel de l'Universit√© de Californie √† Berkeley, avons explor√© l'apprentissage sans aucune r√©compense environnementale.  La curiosit√© offre un moyen plus simple d'apprendre aux agents √† interagir avec <i>n'importe quel</i> environnement, plut√¥t que d'utiliser une fonction de r√©compense sp√©cialement con√ßue pour une t√¢che sp√©cifique, ce qui n'est pas encore un fait qui correspond √† la solution du probl√®me.  Dans des projets comme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ALE</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Universe</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Malmo</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Gym</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Gym Retro</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Unity</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">DeepMind Lab</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">CommAI</a> , un grand nombre d'environnements simul√©s sont ouverts pour l'agent via une interface standardis√©e.  Un agent utilisant une fonction de r√©compense g√©n√©ralis√©e qui n'est pas sp√©cifique √† un environnement particulier peut acqu√©rir un niveau de comp√©tence de base dans un large √©ventail d'environnements.  Cela lui permet de d√©terminer un comportement utile m√™me en l'absence de r√©compenses √©labor√©es. <br><br><blockquote>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Texte d'un article scientifique</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">code</a> </blockquote><br>  Dans les param√®tres de formation standard avec renforcement √† chaque pas de temps discret, l'agent envoie l'action √† l'environnement, et il r√©agit, donnant √† l'agent une nouvelle observation, une r√©compense pour la transition et un indicateur de la fin de l'√©pisode.  Dans notre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article pr√©c√©dent,</a> nous avons configur√© l'environnement <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pour produire</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">uniquement l'</a> observation suivante.  L√†, l'agent √©tudie le mod√®le de pr√©diction de l'√©tat suivant sur la base de son exp√©rience et utilise l'erreur de pr√©diction comme r√©compense interne.  En cons√©quence, il est attir√© par l'impr√©visibilit√©.  Par exemple, un changement de compte de jeu n'est r√©compens√© que si le compte est affich√© √† l'√©cran et que le changement est difficile √† pr√©voir.  Un agent, en r√®gle g√©n√©rale, trouve des interactions utiles avec de nouveaux objets, car les r√©sultats de ces interactions sont g√©n√©ralement plus difficiles √† pr√©voir que d'autres aspects de l'environnement. <br><br>  Comme d' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">autres</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">chercheurs</a> , nous avons essay√© d'√©viter de mod√©liser tous les aspects de l'environnement, qu'ils soient pertinents ou non, en choisissant les caract√©ristiques d'observation pour la mod√©lisation.  √âtonnamment, nous avons constat√© que m√™me les fonctions al√©atoires fonctionnent bien. <br><br><h1>  Que font les agents curieux? </h1><br>  Nous avons test√© notre agent dans plus de 50 environnements diff√©rents et observ√© une gamme de comp√©tences allant d'actions apparemment al√©atoires √† une interaction consciente avec l'environnement.  √Ä notre grande surprise, dans certains cas, l'agent a r√©ussi √† passer le match, bien qu'il n'ait pas √©t√© inform√© de l'objectif gr√¢ce √† une r√©compense externe. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Votre navigateur ne prend pas en charge la vid√©o HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/breakout_int_rew_440.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">R√©mun√©ration interne en d√©but de formation</font></i> <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Votre navigateur ne prend pas en charge la vid√©o HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/breakout_int_rew_440.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Le saut en r√©compense interne au premier passage du niveau</font></i> <br><br>  <b>Breakout</b> - saute dans la r√©compense interne lorsque l'agent voit une nouvelle configuration de blocs √† un stade pr√©coce de la formation et lorsque le niveau passe pour la premi√®re fois apr√®s une formation de plusieurs heures. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Votre navigateur ne prend pas en charge la vid√©o HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/BowlingSmaller.mp4" type="video/mp4"></video></div></div></div><br>  <b>Pong</b> - nous avons form√© l'agent √† contr√¥ler les deux plates-formes simultan√©ment, et il a appris √† garder le ballon dans le jeu, ce qui a entra√Æn√© des combats prolong√©s.  M√™me lors de l'entra√Ænement contre l'IA en jeu, l'agent a essay√© de maximiser le jeu et de ne pas gagner. <br><br>  <b><a href="">Bowling</a></b> - l'agent a appris √† mieux jouer le jeu que les autres agents qui ont √©t√© form√©s directement pour maximiser la r√©compense externe.  Nous pensons que cela se produit parce que l'agent est attir√© par le clignotement √† peine pr√©visible du tableau de bord apr√®s les lancers. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Votre navigateur ne prend pas en charge la vid√©o HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/Mario.mp4" type="video/mp4"></video></div></div></div><br>  <b>Mario</b> - La r√©compense interne est particuli√®rement bien align√©e avec l'objectif du jeu: progression de niveau.  L'agent est r√©compens√© pour la recherche de nouvelles zones, car les d√©tails de la zone nouvellement trouv√©e ne peuvent pas √™tre pr√©dits.  En cons√©quence, l'agent a d√©couvert 11 niveaux, trouv√© des salles secr√®tes et m√™me vaincu des boss. <br><br><h1>  Probl√®me de t√©l√©vision bruyante </h1><br>  En tant que joueur sur une machine √† sous, attir√© par des r√©sultats al√©atoires, l'agent tombe parfois dans le pi√®ge de sa curiosit√© √† cause du ¬´probl√®me de t√©l√©vision bruyante¬ª.  L'agent trouve une source de hasard dans l'environnement et continue de l'observer, subissant toujours une r√©compense interne √©lev√©e pour de telles transitions.  Un exemple d'un tel pi√®ge consiste √† regarder une t√©l√©vision qui produit du bruit statique.  Nous le d√©montrons litt√©ralement en pla√ßant l'agent dans le labyrinthe Unity avec un t√©l√©viseur qui diffuse des cha√Ænes au hasard. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Votre navigateur ne prend pas en charge la vid√©o HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/Navigation_withTV.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Agent dans un labyrinthe avec une t√©l√©vision bruyante</font></i> <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Votre navigateur ne prend pas en charge la vid√©o HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/Navigation_withoutTV.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Agent dans un labyrinthe sans t√©l√©vision bruyante</font></i> <br><br>  Th√©oriquement, le probl√®me d'une t√©l√©vision bruyante est vraiment grave, mais nous nous attendions toujours √† ce que dans des environnements d√©terministes comme la vengeance de Montezuma, la curiosit√© am√®ne l'agent √† trouver des pi√®ces et √† interagir avec des objets.  Nous avons essay√© plusieurs options pour pr√©dire le prochain √©tat en fonction de la curiosit√©, en combinant un bonus de recherche avec un compte de jeu. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Votre navigateur ne prend pas en charge la vid√©o HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/montezuma.mp4" type="video/mp4"></video></div></div></div><br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Votre navigateur ne prend pas en charge la vid√©o HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/pitfall.mp4" type="video/mp4"></video></div></div></div><br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Votre navigateur ne prend pas en charge la vid√©o HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/privateeye.mp4" type="video/mp4"></video></div></div></div><br>  Dans ces exp√©riences, l'agent contr√¥le l'environnement via un contr√¥leur de bruit qui, avec une certaine probabilit√©, r√©p√®te la derni√®re action au lieu de l'actuelle.  Ce param√®tre avec des actions r√©p√©titives ¬´collantes¬ª a √©t√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">propos√©</a> comme meilleure pratique pour former des agents √† des jeux enti√®rement d√©terministes, comme Atari, pour √©viter la m√©morisation.  Les actions ¬´collantes¬ª rendent la transition d'une pi√®ce √† l'autre impr√©visible. <br><br><h1>  Distillation de r√©seau al√©atoire </h1><br>  √âtant donn√© que la pr√©diction de l'√©tat suivant est intrins√®quement sensible au probl√®me d'un t√©l√©viseur bruyant, nous avons identifi√© les sources pertinentes d'erreurs de pr√©diction suivantes: <br><br><ul><li>  <b>Facteur 1</b> .  L'erreur de pr√©vision est √©lev√©e si le pr√©dicteur ne parvient pas √† g√©n√©raliser √† partir des exemples pr√©c√©demment consid√©r√©s.  Une nouvelle exp√©rience correspond √† une erreur de pr√©diction √©lev√©e. </li><li>  <b>Facteur 2</b> .  L'erreur de pr√©vision est √©lev√©e en raison de l'objectif de pr√©vision stochastique. </li><li>  <b>Facteur 3</b> .  L'erreur de pr√©vision est √©lev√©e en raison du manque d'informations n√©cessaires √† la pr√©vision ou parce que la classe du mod√®le de pr√©diction est trop limit√©e pour s'adapter √† la complexit√© de la fonction objectif. </li></ul><br>  Nous avons d√©termin√© que le facteur 1 est une source d'erreurs utile car il quantifie la nouveaut√© de l'exp√©rience, tandis que les facteurs 2 et 3 entra√Ænent le probl√®me d'une t√©l√©vision bruyante.  Pour √©viter les facteurs 2 et 3, nous avons d√©velopp√© RND - un nouveau bonus de recherche bas√© sur la <b>pr√©diction de l'√©mission d'un r√©seau neuronal constant et initialis√© au hasard dans l'√©tat suivant, en tenant compte de l'√©tat suivant lui-m√™me</b> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/db6/ac9/7fc/db6ac97fc37b0914e1a62145f855820c.svg" width="780"><br><br>  L'intuition sugg√®re que les mod√®les pr√©dictifs ont une faible erreur dans la pr√©vision des conditions dans lesquelles elle a √©t√© form√©e.  En particulier, les pr√©dictions de l'agent concernant l'√©mission d'un r√©seau de neurones initialis√© au hasard seront moins pr√©cises dans les nouveaux √©tats que dans les √©tats que l'agent rencontrait souvent auparavant.  L'avantage de l'utilisation du probl√®me de pr√©vision synth√©tique est qu'il peut √™tre d√©terministe (contourner le facteur 2), et au sein de la classe de fonctions, le pr√©dicteur peut choisir un pr√©dicteur de la m√™me architecture que le r√©seau cible (contourner le facteur 3).  Cela √©limine le probl√®me RND d'un t√©l√©viseur bruyant. <br><br>  Nous avons combin√© le bonus de recherche avec des r√©compenses externes gr√¢ce √† une forme d'optimisation de la politique la plus proche - l'optimisation de la politique <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">proximale</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">PPO</a> ), qui utilise <b>deux valeurs pour deux flux de r√©compense</b> .  Cela vous permet d'utiliser diff√©rentes remises pour diff√©rentes r√©compenses et de combiner des r√©compenses √©pisodiques et non √©pisodiques.  <b>En raison de cette flexibilit√© suppl√©mentaire, notre meilleur agent trouve souvent 22 des 24 chambres au premier niveau dans Montezuma‚Äôs Revenge, et passe parfois le premier niveau apr√®s avoir trouv√© les deux chambres restantes.</b>  La m√™me m√©thode d√©montre des performances record dans les jeux Venture et Gravitar. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/043/51a/ee8/04351aee8d6be917caf1994b968e04b9.svg" width="780"><br>  La visualisation ci-dessous montre un graphique de la r√©compense interne dans l'√©pisode de la vengeance de Montezuma, o√π l'agent trouve d'abord la torche. <br><br><img src="https://habrastorage.org/webt/hu/vg/px/huvgpxpbzzc3-reechovxyhjcjs.gif"><br><br><h1>  Une mise en ≈ìuvre comp√©tente est importante </h1><br>  Pour s√©lectionner un bon algorithme, il est important de prendre en consid√©ration des consid√©rations g√©n√©rales, telles que la sensibilit√© au probl√®me d'un t√©l√©viseur bruyant.  Cependant, nous avons constat√© que des changements apparemment tr√®s faibles dans notre algorithme simple affectent consid√©rablement son efficacit√©: d'un agent qui ne peut pas quitter la premi√®re pi√®ce √† un agent qui passe par le premier niveau.  Pour ajouter de la stabilit√© √† l'entra√Ænement, nous avons √©vit√© la saturation des traits et apport√© des r√©compenses internes √† une plage pr√©visible.  Nous avons √©galement remarqu√© <b>des am√©liorations significatives de l'efficacit√© de RND chaque fois que nous trouvions et corrigions un bug</b> (notre pr√©f√©r√© incluait la mise √† z√©ro al√©atoire du tableau, ce qui conduisait au fait que les r√©compenses externes √©taient consid√©r√©es comme non √©pisodiques; nous l'avons r√©alis√© uniquement apr√®s avoir pens√© √† la fonction de valeur externe , qui semblait √©trangement p√©riodique).  La correction de ces d√©tails est devenue une partie importante de la r√©alisation de hautes performances m√™me lors de l'utilisation d'algorithmes conceptuellement similaires aux travaux pr√©c√©dents.  C'est l'une des raisons pour lesquelles il est pr√©f√©rable de choisir des algorithmes simples dans la mesure du possible. <br><br><h1>  Travaux futurs </h1><br>  Nous proposons les domaines de recherche suivants: <br><br><ul><li>  Analyse des avantages des diff√©rentes m√©thodes de recherche et recherche de nouvelles fa√ßons de les combiner. </li><li>  Former un agent curieux dans de nombreux environnements diff√©rents sans r√©compenses et apprendre √† transf√©rer dans un environnement cible avec des r√©compenses. </li><li>  Intelligence globale, y compris des solutions coordonn√©es √† long terme. </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr428776/">https://habr.com/ru/post/fr428776/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr428766/index.html">Le condens√© de mati√®res fra√Æches du monde du front-end de la derni√®re semaine n ¬∞ 337 (29 octobre - 4 novembre 2018)</a></li>
<li><a href="../fr428768/index.html">En trois articles sur les moindres carr√©s: programme √©ducatif sur la th√©orie des probabilit√©s</a></li>
<li><a href="../fr428770/index.html">Macros de clavier pour les t√¢ches quotidiennes</a></li>
<li><a href="../fr428772/index.html">D√©mocratisation des donn√©es Uber</a></li>
<li><a href="../fr428774/index.html">Pare-feu GPS pour centres de donn√©es - pourquoi est-il n√©cessaire et comment fonctionne-t-il</a></li>
<li><a href="../fr428778/index.html">Voyez l'invisible. Infrarouge proche (0,9-1,7 Œºm)</a></li>
<li><a href="../fr428786/index.html">Processeur quantique bas√© sur la r√©sonance de spin et les manipulations avec un syst√®me singlet-triplet</a></li>
<li><a href="../fr428788/index.html">Sous le capot de Bitfury Clarke - comment fonctionne notre nouvelle puce mini√®re</a></li>
<li><a href="../fr428790/index.html">Nous √©crivons un chat bot pour VKontakte en python en utilisant longpoll. Deuxi√®me partie Boucles doubles, exceptions et autres h√©r√©sies</a></li>
<li><a href="../fr428792/index.html">La nouvelle puce Apple T2 rend difficile l'√©coute via le microphone int√©gr√© de l'ordinateur portable</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>