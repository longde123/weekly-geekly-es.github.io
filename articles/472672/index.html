<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ùì ü•ñ ü§∞üèª El t√≠tulo "Leer art√≠culos para usted". Julio - septiembre 2019 ‚úåüèª üé∫ ü§≤</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola Habr! Continuamos publicando rese√±as de art√≠culos cient√≠ficos de miembros de la comunidad Open Data Science del canal #article_essense. Si quiere...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>El t√≠tulo "Leer art√≠culos para usted". Julio - septiembre 2019</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/472672/"><img src="https://habrastorage.org/webt/gx/-y/xl/gx-yxlo7xiz-5y8krpyoj3rgswq.png"><br><p><br>  Hola Habr!  Continuamos publicando rese√±as de art√≠culos cient√≠ficos de miembros de la comunidad Open Data Science del canal #article_essense.  Si quieres recibirlos antes que los dem√°s, ¬°√∫nete a la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">comunidad</a> ! </p><br><p>  Art√≠culos para hoy: </p><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Rotaci√≥n de capas: ¬øun indicador sorprendentemente poderoso de generalizaci√≥n en redes profundas?</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">(Universit√© catholique de Louvain, B√©lgica, 2018)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Aprendizaje de transferencia eficiente de par√°metros para PNL (Google Research, Jagiellonian University, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RoBERTa: un enfoque de capacitaci√≥n previa BERT robustamente optimizado (Universidad de Washington, Facebook AI, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">EfficientNet: repensar la escala del modelo para redes neuronales convolucionales (Google Research, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">C√≥mo pasa el cerebro de la percepci√≥n consciente a la subliminal (EE. UU., Argentina, Espa√±a, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Grandes capas de memoria con claves de producto (Facebook AI Research, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">¬øRealmente estamos progresando mucho?</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Un an√°lisis preocupante de los enfoques recientes de recomendaciones neuronales (Politecnico di Milano, Universidad de Klagenfurt, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Omni-Scale Feature Learning para reidentificaci√≥n de personas (University of Surrey, Queen Mary University, Samsung AI, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">La reparameterizaci√≥n neuronal mejora la optimizaci√≥n estructural (Google Research, 2019)</a> </li></ol><a name="habracut"></a><br><div class="spoiler">  <b class="spoiler_title">Enlaces a colecciones pasadas de la serie:</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Enero - junio 2019</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Febrero - marzo 2018</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Diciembre de 2017 - enero de 2018</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Octubre - noviembre 2017</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Septiembre de 2017</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Agosto de 2017</a> </li></ul></div></div><br><h3 id="1-layer-rotation-a-surprisingly-powerful-indicator-of-generalization-in-deep-networks">  1. Rotaci√≥n de capas: ¬øun indicador sorprendentemente poderoso de generalizaci√≥n en redes profundas? </h3><br><p>  Autores: Simon Carbonnelle, Christophe De Vleeschouwer (Universit√© catholique de Louvain, B√©lgica, 2018) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">‚Üí Art√≠culo original</a> <br>  Autor de la revisi√≥n: Svyatoslav Skoblov (en slack error_derivative) </p><br><img src="https://habrastorage.org/webt/tt/n5/g8/ttn5g8j27-ihyqnwk0rowhg8oie.png" width="500" height="250"><br><p><br>  En este art√≠culo, los autores llamaron la atenci√≥n sobre una observaci√≥n bastante simple: la distancia del coseno entre los pesos de las capas durante la inicializaci√≥n y despu√©s del entrenamiento (el proceso de aumentar la distancia durante el entrenamiento se llama rotaci√≥n de capas).  Los caballeros dicen que en la mayor√≠a de los experimentos, las redes que han alcanzado una distancia de 1 en todas las capas son consistentemente superiores en precisi√≥n a otras configuraciones.  El documento tambi√©n presenta el algoritmo <strong>Layca</strong> (Cantidad controlada de rotaci√≥n de peso a nivel de capa), que permite utilizar esta tasa de aprendizaje en capas para controlar esta misma rotaci√≥n de capa.  De hecho, difiere del algoritmo SGD habitual por la presencia de proyecci√≥n ortogonal y normalizaci√≥n.  En el art√≠culo se puede encontrar una lista detallada del algoritmo junto con el esquema de capacitaci√≥n. </p><br><p>  La idea principal que deducen los autores es: cuanto <strong>mayor sea la rotaci√≥n de las capas, mejor</strong> ser√° <strong>el rendimiento de la generalizaci√≥n</strong> .  La mayor parte del art√≠culo es un registro de experimentos en los que se estudiaron varios escenarios de capacitaci√≥n: MNIST, CIFAR-10 / CIFAR-100, se utiliz√≥ una peque√±a ImageNet con diferentes arquitecturas, desde una red de una sola capa hasta la familia ResNet. </p><br><p>  Una serie de experimentos se dividi√≥ en varias etapas: </p><br><ol><li>  <strong>Vanilla SGD Result√≥</strong> que, en general, el comportamiento de las escalas coincide con la hip√≥tesis (grandes cambios en la distancia correspondieron a los mejores valores m√©tricos), sin embargo, tambi√©n se notaron problemas: la rotaci√≥n de la capa se detuvo mucho antes de los valores deseados;  Tambi√©n se not√≥ inestabilidad en el cambio de distancia. </li><li>  <strong>Descenso de peso SGD + La</strong> disminuci√≥n de la norma de <strong>peso</strong> mejor√≥ en gran medida la imagen de entrenamiento: la mayor√≠a de las capas alcanzaron la distancia m√°xima, y ‚Äã‚Äãel rendimiento de la prueba es similar al Layca propuesto.  La ventaja indudable del m√©todo del autor es la falta de un hiperpar√°metro adicional. </li><li>  <strong>Calentamientos LR</strong> Result√≥ que el calentamiento ayuda a SGD a superar el problema de la rotaci√≥n inestable de la capa, sin embargo, no tiene ning√∫n efecto en Layca. </li><li>  <strong>M√©todos de gradiente adaptativos</strong> Adem√°s de la verdad bien conocida (que al usar estos m√©todos es m√°s dif√≠cil lograr el nivel de generalizaci√≥n que SGD + descomposici√≥n de peso puede dar), result√≥ que los efectos de la rotaci√≥n de capas son muy diferentes: el primer aumento de rotaci√≥n en las √∫ltimas capas, mientras que SGD en las capas iniciales .  Los autores insin√∫an que esta puede ser la mezquindad de los m√©todos adaptativos.  Y sugieren usar Layca junto con ellos (mejorando la capacidad de generalizar en m√©todos adaptativos y acelerando el aprendizaje en SGD). </li></ol><br><p>  El art√≠culo concluye con un intento de interpretar el fen√≥meno.  Para hacer esto, los autores entrenaron una red con 1 capa oculta en una versi√≥n reducida de MNIST, despu√©s de lo cual visualizaron neuronas aleatorias, llegando a una conclusi√≥n bastante l√≥gica: un mayor grado de rotaci√≥n de la capa corresponde a un menor efecto de inicializaci√≥n y un mejor estudio de las caracter√≠sticas, lo que contribuye a una mejor generalizaci√≥n. </p><br><p>  Se cargan el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">c√≥digo del algoritmo implementado (tf / keras)</a> y el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">c√≥digo para reproducir experimentos.</a> </p><br><h3 id="2-parameter-efficient-transfer-learning-for-nlp">  2. Aprendizaje de transferencia eficiente de par√°metros para PNL </h3><br><p>  Autores del art√≠culo: Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly (Google Research, Universidad Jagiellonian, 2019) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">‚Üí Art√≠culo original</a> <br>  Autor de la revisi√≥n: Alexey Karnachev (en slack zhirzemli) </p><br><img src="https://habrastorage.org/webt/ka/lg/gp/kalggpbjkmd8zc7ep451lysxpc8.png"><br><p><br>  Aqu√≠ los caballeros ofrecen una t√©cnica de ajuste simple pero efectiva para los modelos de PNL (en este caso, BERT).  La idea es integrar capas de aprendizaje (adaptadores) directamente en la red.  Cada una de estas capas es una red con un cuello de botella, que adapta los estados latentes del modelo original a una tarea espec√≠fica aguas abajo.  Los pesos del modelo original, a su vez, permanecen congelados. </p><br><p>  <strong>Motivaci√≥n</strong> <br>  En las condiciones de la capacitaci√≥n de transmisi√≥n (o capacitaci√≥n casi en l√≠nea), donde hay muchas tareas posteriores, realmente no quiero presentar el modelo completo.  En primer lugar, durante mucho tiempo, y en segundo lugar, es dif√≠cil, y en tercer lugar, incluso si est√° ajustado, el modelo debe almacenarse de alguna manera: para volcar o guardar en la memoria.  Y no podremos reutilizar este modelo para la siguiente tarea: cada vez que tengamos que sintonizar de una manera nueva.  Como resultado, podemos intentar adaptar los estados de red ocultos al problema actual.  Adem√°s, el modelo original permanece intacto, y los adaptadores en s√≠ mismos son mucho m√°s capaces que el modelo principal (~ 4% del n√∫mero total de par√°metros) </p><br><p>  <strong>Implementaci√≥n</strong> <br>  El problema se resuelve de una manera incre√≠blemente simple: agregamos 2 adaptadores a cada capa del modelo.  Antes de la normalizaci√≥n de capa en los modelos basados ‚Äã‚Äãen transformadores, se produce una conexi√≥n de omisi√≥n: la entrada transformada (estado oculto actual) se agrega a la entrada original. </p><br><p>  Hay 2 secciones de este tipo en cada capa de transformador: una despu√©s de la atenci√≥n de m√∫ltiples cabezales y la segunda despu√©s de la alimentaci√≥n hacia adelante.  Por lo tanto, los estados ocultos de estas secciones se pasan adicionalmente a trav√©s del adaptador: una red poco profunda con una capa oculta de 1 cuello de botella y con salida de la misma dimensi√≥n que la entrada.  La no linealidad se aplica al estado de cuello de botella y la entrada (conexi√≥n de omisi√≥n) se agrega a la salida.  Resulta que el n√∫mero total de par√°metros entrenados es: 2md + m + d, donde d es la dimensi√≥n del estado oculto del modelo original, m es el tama√±o de la capa del adaptador de cuello de botella.  Resulta que para el modelo base BERT (12 capas, par√°metros de 110M) y para el tama√±o del adaptador bottlneck'a 128, obtenemos el 4,3% del n√∫mero total de par√°metros </p><br><p>  <strong>Resultados</strong> <br>  La comparaci√≥n se realiz√≥ con el ajuste completo del modelo.  Para todas las tareas, este enfoque mostr√≥ una p√©rdida menor en las m√©tricas (en promedio menos de 1 punto), con el n√∫mero de pesas entrenadas: 3% del total.  No enumerar√© las tareas en s√≠, hay muchas, hay una tableta en el art√≠culo. </p><br><p>  <strong>Ajuste fino</strong> <br>  En este modelo, solo se ajusta la parte del adaptador (+ el clasificador de salida en s√≠).  Para escalas de adaptadores, proponen hacer una inicializaci√≥n de identidad cercana.  Por lo tanto, un modelo no entrenado no cambiar√° los estados ocultos de la red de ninguna manera, y esto ya har√° posible que en el proceso de capacitaci√≥n del modelo decida qu√© estados adaptar para la tarea y cu√°les dejar sin cambios. </p><br><p>  La tasa de aprendizaje recomienda tomar m√°s que con el ajuste fino BERT est√°ndar.  Personalmente, en mi tarea, 1e-04 lr funcion√≥ bien.  Adem√°s, (ya personalmente mi observaci√≥n) durante el proceso de ajuste, el modelo casi siempre explota los gradientes, por lo que debe recordar hacer el recorte.  Optimizador - Adam con calentamiento 10% </p><br><p>  <strong>C√≥digo</strong> <br>  Se adjunta el c√≥digo en su art√≠culo.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Implementaci√≥n en Tensorflow</a> . <br>  Para Torch, el autor de la revisi√≥n bifurc√≥ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">los transformadores de pytorch y agreg√≥ una capa de Adaptador</a> (al comienzo del archivo README.md hay un peque√±o manual de inicio) </p><br><h3 id="3-roberta-a-robustly-optimized-bert-pretraining-approach">  3. RoBERTa: un enfoque de preentrenamiento BERT robustamente optimizado </h3><br><p>  Autores del art√≠culo: Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov (Universidad de Washington, Facebook AI, 2019) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">‚Üí Art√≠culo original</a> <br>  Autor de la revisi√≥n: Artem Rodichev (en slack fuckai) </p><br><p>  Elev√≥ dr√°sticamente la calidad de los modelos BERT, primer lugar en la tabla de clasificaci√≥n de GLUE y SOTA en muchas tareas de PNL.  Sugirieron varias formas de entrenar el modelo BERT lo mejor posible sin ning√∫n cambio en la arquitectura del modelo en s√≠. </p><br><p>  Diferencias clave con el BERT original: </p><br><ol><li>  Se increment√≥ la construcci√≥n de trenes 10 veces, de 16 GB de texto sin formato a 160 GB </li><li>  Enmascaramiento din√°mico para cada muestra </li><li>  Se elimin√≥ el uso de la siguiente oraci√≥n de predicci√≥n de p√©rdida </li><li>  Se increment√≥ el tama√±o del mini lote de 256 muestras a 8k </li><li>  Se mejor√≥ la codificaci√≥n BPE al traducir la base de datos de Unicode a bytes. </li></ol><br><p>  El mejor modelo final fue entrenado en 1024 tarjetas Nvidia V100 (128 servidores DGX-1) durante 5 d√≠as. </p><br><p>  <strong>La esencia del enfoque:</strong> </p><br><p>  <em>Datos.</em>  Adem√°s de los shells Wiki y BookCorpus (16GB en total), que ense√±aron el BERT original, agregaron 3 shells m√°s grandes, todos en ingl√©s: </p><br><ol><li>  SS-News 63 millones de noticias en 2.5 a√±os con 76GB </li><li>  OpenWebText es el marco en el que se ense√±√≥ a OpenAI el modelo GPT2.  Estos son art√≠culos rastreados a los que se dieron enlaces en publicaciones en un reddit con al menos tres actualizaciones.  38 GB de datos </li><li>  Historias - 31GB CommonCrawl Story Case </li></ol><br><p>  <em>Enmascaramiento din√°mico.</em>  En el BERT original, el 15% de los tokens est√°n enmascarados en cada muestra y estos tokens se predicen usando la parte sin enmascarar de la secuencia.  Se genera una m√°scara para cada muestra una vez durante el preprocesamiento y no cambia.  Al mismo tiempo, la misma muestra en el tren puede ocurrir varias veces, dependiendo del n√∫mero de eras en el cuerpo.  La idea del enmascaramiento din√°mico es crear una nueva m√°scara para la secuencia cada vez, en lugar de usar una m√°scara fija en el preprocesamiento. </p><br><p>  <em>Objetivo de predicci√≥n de la siguiente oraci√≥n.</em>  ¬øVamos a cortar este objektiv y ver si empeor√≥?  Ha mejorado o tambi√©n se ha mantenido, en las tareas SQuAD, MNLI, SST y RACE. </p><br><p>  <em>Aumentar el tama√±o del mini lote.</em>  En muchos lugares, en particular en la traducci√≥n autom√°tica, se demostr√≥ que cuanto m√°s grande es el mini lote, mejores son los resultados finales del tren.  Mostraron que si aumenta el minibatch de 256 muestras, como en el BERT original, a 2k, y luego a 8k, entonces la perplejidad en la validaci√≥n cae, y las m√©tricas en MNLI y SST-2 crecen. </p><br><p>  <em>BPE</em>  El BPE de la implementaci√≥n BERT original utiliza caracteres Unicode como base para las unidades de subpalabras.  Esto lleva al hecho de que, en casos grandes y diversos, una parte significativa del diccionario estar√° ocupada por caracteres Unicode individuales.  OpenAI en GPT2 sugiri√≥ usar no caracteres Unicode, sino bytes como base para las subpalabras.  Si usamos un diccionario BPE de 50k, no tendremos tokens desconocidos.  En comparaci√≥n con el BERT original, el tama√±o del modelo ha crecido en 15 millones de par√°metros para el modelo base y en 20 millones para los grandes, es decir, 5-10% m√°s. </p><br><p>  <strong>Resultados:</strong> <br>  BERT-large y XLNet-large se utilizan como modelos de comparaci√≥n.  RoBERTa en s√≠ es el mismo en par√°metros que BERT-large. Como resultado, ganaron el primer lugar en el punto de referencia GLUE.  Utilizamos el ajuste de archivos de una sola tarea, a diferencia de muchos otros enfoques desde el punto de referencia GLUE que hacen el ajuste de archivos de tareas m√∫ltiples.  En las chicas de GLUE, se comparan los resultados del modelo √∫nico, obtuvieron SOTA en las 9 tareas.  En el conjunto de prueba, se compara el conjunto de modelos, SOTA para 4 de 9 tareas y la velocidad final de la cola.  En dos versiones de SQuAD en la red de desarrollo SOTA, en el conjunto de prueba en el nivel XLNet.  Adem√°s, a diferencia de XLNet, no quedan atrapados en paquetes de control de calidad adicionales antes de resolver SQuAD. </p><br><img src="https://habrastorage.org/webt/5x/ue/u9/5xueu9hpmqwowfuf0yn1_zopqxy.png" width="500" height="250"><br><p><br>  SOTA en la tarea RACE en la que se proporciona un texto, una pregunta sobre este texto y 4 opciones de respuesta donde debe elegir la correcta.  Para resolver esta tarea, concatenan el texto, la pregunta y la respuesta, ejecutan BERT, obtienen una representaci√≥n del token CLF, se aplican a una capa totalmente conectada y predicen si la respuesta es correcta.  Esto se hace 4 veces, para cada una de las opciones de respuesta. </p><br><p>  Publicamos el c√≥digo y el pre-entrenamiento del modelo <a href="">RoBERTa</a> en <a href="">nabo fairseq</a> .  Puedes usarlo, todo se ve ordenado y simple. </p><br><h3 id="4-efficientnet-rethinking-model-scaling-for-convolutional-neural-networks">  4. EfficientNet: repensando la escala del modelo para redes neuronales convolucionales </h3><br><p>  Autores: Mingxing Tan, Quoc V. Le (Google Research, 2019) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">‚Üí Art√≠culo original</a> <br>  Autor de la revisi√≥n: Alexander Denisenko (en flojo Alexander Denisenko) </p><br><img src="https://habrastorage.org/webt/ey/se/0k/eyse0kouanmvflpgz9ev--x1oqm.png" width="500" height="250"><br><p><br>  Estudian la escala (escala) de los modelos y el equilibrio entre ellos la profundidad y el ancho (n√∫mero de canales) de la red, as√≠ como la resoluci√≥n de las im√°genes en la cuadr√≠cula.  Ofrecen un nuevo m√©todo de escala que escala uniformemente la profundidad / ancho / resoluci√≥n.  Muestre su efectividad en MobileNet y ResNet. </p><br><p>  Tambi√©n usan Neural Architecture Search para crear una nueva malla y escalarla, obteniendo as√≠ una clase de nuevos modelos: EfficientNets.  Son mejores y mucho m√°s econ√≥micos que las cuadr√≠culas anteriores.  En ImageNet, EfficientNet-B7 logra una precisi√≥n de 84.4% top-1 y 97.1% top-5 de vanguardia, siendo 8.4 veces menos y 6.1 veces m√°s r√°pido en inferencia que el mejor ConvNet actual en su clase.  Se transfiere bien a otros conjuntos de datos: obtuvieron SOTA en 5 de los 8 conjuntos de datos m√°s populares. </p><br><p>  <strong>Escala de modelo compuesto</strong> <br>  El escalado es cuando las operaciones realizadas dentro de la cuadr√≠cula son fijas y solo cambian la profundidad (n√∫mero de repeticiones de los mismos m√≥dulos) d, el ancho (n√∫mero de canales en convoluci√≥n) w y la resoluci√≥n r.  En el localizador, la escala se formula como un problema de optimizaci√≥n: queremos la m√°xima precisi√≥n (Neto (d, w, r)) a pesar de que no nos arrastramos fuera de los l√≠mites en la memoria y en FLOPS. </p><br><p>  Realizamos experimentos y nos aseguramos de que realmente ayude tambi√©n a escalar en profundidad y resoluci√≥n al escalar en ancho.  Con los mismos FLOPS, logramos un resultado significativamente mejor en ImageNet (vea la imagen de arriba).  En general, esto es razonable, porque parece que con un aumento en la resoluci√≥n de la imagen de la red, se necesitan m√°s capas en profundidad para aumentar el campo receptivo y m√°s canales para capturar todos los patrones de la imagen con una resoluci√≥n m√°s alta. </p><br><p>  La esencia de la escala compuesta: tomamos el coeficiente compuesto phi, que escala uniformemente d, w y r con este coeficiente: <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><msup><mi>a</mi><mtext>&amp;#xA0;</mtext></msup><mi>p</mi><mi>h</mi><mi>i</mi><mo>,</mo><mi>w</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>e</mi><mi>t</mi><msup><mi>a</mi><mtext>&amp;#xA0;</mtext></msup><mi>p</mi><mi>h</mi><mi>i</mi><mo>,</mo><mi>r</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><msup><mi>a</mi><mtext>&amp;#xA0;</mtext></msup><mi>p</mi><mi>h</mi><mi>i</mi><mo>,</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="47.52ex" height="2.419ex" viewBox="0 -780.1 20460 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-64" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMAIN-3D" x="801" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-61" x="2107" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-6C" x="2637" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-70" x="2935" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-68" x="3439" y="0"></use><g transform="translate(4015,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-61" x="0" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-70" x="4895" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-68" x="5398" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-69" x="5975" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMAIN-2C" x="6320" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-77" x="6765" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMAIN-3D" x="7760" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-62" x="9066" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-65" x="9495" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-74" x="9962" y="0"></use><g transform="translate(10323,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-61" x="0" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-70" x="11203" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-68" x="11706" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-69" x="12283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMAIN-2C" x="12628" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-72" x="13073" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMAIN-3D" x="13803" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-67" x="15109" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-61" x="15589" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-6D" x="16119" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-6D" x="16998" y="0"></use><g transform="translate(17876,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-61" x="0" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-70" x="18756" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-68" x="19259" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-69" x="19836" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMAIN-2C" x="20181" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">d</font></font></mi><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">=</font></font></mo><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">p</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">h</font></font></mi><msup><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un</font></font></mi><mtext>&nbsp;</mtext></msup><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">p</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">h</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">yo</font></font></mi><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">,</font></font></mo><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w</font></font></mi><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">=</font></font></mo><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">b</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">e</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t</font></font></mi><msup><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un</font></font></mi><mtext>&nbsp;</mtext></msup><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">p</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">h</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">yo</font></font></mi><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">,</font></font></mo><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">r</font></font></mi><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">=</font></font></mo><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">g</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></mi><msup><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un</font></font></mi><mtext>&nbsp;</mtext></msup><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">p</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">h</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">yo</font></font></mi><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">,</font></font></mo></math></span></span><script type="math/tex" id="MathJax-Element-1"> d = \ alpha ^ \ phi, w = \ beta ^ \ phi, r = \ gamma ^ \ phi, </script>  donde <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi><mo>,</mo><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo>,</mo><mtext>&amp;#xA0;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="21.278ex" height="2.419ex" viewBox="0 -780.1 9161.3 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-61" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-6C" x="779" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-70" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-68" x="1581" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-61" x="2158" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMAIN-2C" x="2687" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-62" x="3382" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-65" x="3812" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-74" x="4278" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-61" x="4640" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMAIN-2C" x="5169" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-67" x="5864" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-61" x="6345" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-6D" x="6874" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-6D" x="7753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-61" x="8631" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">p</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">h</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un</font></font></mi><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">,</font></font></mo><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">b</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">e</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un</font></font></mi><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">,</font></font></mo><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">g</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un</font></font></mi></math></span></span><script type="math/tex" id="MathJax-Element-2"> \ alpha, \ beta, \ gamma </script>  - constantes obtenidas de una vista de cuadr√≠cula peque√±a en la cuadr√≠cula de origen. <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>h</mi><mi>i</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.891ex" height="2.419ex" viewBox="0 -780.1 1675.5 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-68" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-69" x="1330" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">p</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">h</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">yo</font></font></mi></math></span></span><script type="math/tex" id="MathJax-Element-3"> \ phi </script>  - coeficiente que caracteriza la cantidad de recursos inform√°ticos disponibles. </p><br><p>  <strong>Red eficiente</strong> <br>  Para crear la cuadr√≠cula, utilizamos la b√∫squeda de arquitectura neuronal de objetivos m√∫ltiples, precisi√≥n optimizada y FLOPS con el par√°metro responsable de la compensaci√≥n entre ellos.  Tal b√∫squeda dio EfficientNet-B0.  En resumen: Conv seguido de varios MBConv, al final de Conv1x1, Pool, FC. </p><br><p>  Luego realice el escalado en dos pasos: </p><br><ol><li>  Para comenzar, arreglamos <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>h</mi><mi>i</mi><mo>=</mo><mn>1</mn></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="8.152ex" height="2.419ex" viewBox="0 -780.1 3510.1 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-68" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-69" x="1330" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMAIN-3D" x="1953" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMAIN-31" x="3009" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">p</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">h</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">yo</font></font></mi><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">=</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></mn></math></span></span><script type="math/tex" id="MathJax-Element-4"> \ phi = 1 </script>  , hacer b√∫squeda de cuadr√≠cula para buscar <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi><mo>,</mo><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo>,</mo><mtext>&amp;#xA0;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="21.278ex" height="2.419ex" viewBox="0 -780.1 9161.3 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-61" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-6C" x="779" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-70" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-68" x="1581" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-61" x="2158" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMAIN-2C" x="2687" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-62" x="3382" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-65" x="3812" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-74" x="4278" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-61" x="4640" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMAIN-2C" x="5169" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-67" x="5864" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-61" x="6345" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-6D" x="6874" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-6D" x="7753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-61" x="8631" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">p</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">h</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un</font></font></mi><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">,</font></font></mo><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">b</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">e</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un</font></font></mi><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">,</font></font></mo><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">g</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un</font></font></mi></math></span></span><script type="math/tex" id="MathJax-Element-5"> \ alpha, \ beta, \ gamma </script>  . </li><li>  Escale la cuadr√≠cula usando las f√≥rmulas para d, w y r.  Tengo EffiientNet-B1.  Del mismo modo, aumentando <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>h</mi><mi>i</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.891ex" height="2.419ex" viewBox="0 -780.1 1675.5 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-68" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhjgWW_H-TrqlhEgJNvKNEjtd3f9WQ#MJMATHI-69" x="1330" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">p</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">h</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">yo</font></font></mi></math></span></span><script type="math/tex" id="MathJax-Element-6"> \ phi </script>  , obtenga EfficientNet-B2, ... B7. </li></ol><br><p>  Escalado para diferentes ResNet y MobileNet, en todas partes recibi√≥ mejoras significativas en ImageNet, el escalado compuesto dio un aumento significativo en comparaci√≥n con el escalado en una sola dimensi√≥n.  Tambi√©n realizamos experimentos con EfficientNet en ocho conjuntos de datos m√°s populares, en todas partes obtuvimos SOTA o un resultado cercano con un n√∫mero significativamente menor de par√°metros. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">C√≥digo</a> </p><br><h3 id="5-how-the-brain-transitions-from-conscious-to-subliminal-perception">  5. C√≥mo pasa el cerebro de la percepci√≥n consciente a la subliminal </h3><br><p>  Autores del art√≠culo: Francesca Arese Lucini, Gino Del Ferraro, Mariano Sigman, Hernan A. Makse (EE. UU., Argentina, Espa√±a, 2019) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">‚Üí Art√≠culo original</a> <br>  Autor de la revisi√≥n: Svyatoslav Skoblov (en slack error_derivative) </p><br><p>  Este art√≠culo es una continuaci√≥n y un replanteamiento del trabajo de <em>Dehaene, S, Naccache, L, Cohen, L, Le Bihan, D, Mangin, JF, Poline, JB y Rivie`re, D. Mecanismos cerebrales de enmascaramiento de palabras y cebado de repetici√≥n inconsciente</em> , en que los autores trataron de considerar los modos de funci√≥n cerebral consciente e inconsciente. </p><br><img src="https://habrastorage.org/webt/fi/o4/te/fio4terok3udte6bcmpajzfs_um.png" width="500" height="250"><br><p><br>  <strong>Experimento:</strong> <br>  A los voluntarios se les muestran im√°genes (palabras de 4 letras, o una pantalla en blanco, o garabatos).  Cada uno de ellos se muestra durante 30 ms, en general, toda la acci√≥n dura 5 minutos. </p><br><ol><li>  En el modo "consciente" del experimento, una pantalla en blanco se alterna con palabras, lo que permite a una persona percibir conscientemente el texto. </li><li>  En el modo "inconsciente", las palabras se alternan con garabatos, lo que interfiere bastante efectivamente con la percepci√≥n del texto en un nivel consciente. </li></ol><br><p>  <strong>Datos:</strong> <br>  Durante esta presentaci√≥n, los cerebros de nuestros primates fueron escaneados usando fMRI.  En total, los investigadores tuvieron 15 voluntarios, cada uno repiti√≥ el experimento 5 veces, un total de 75 flujos de fMRI.  Vale la pena se√±alar que el escaneo de voxel result√≥ ser bastante grande (muy simplificado: voxel es un cubo 3D que contiene una cantidad bastante grande de celdas) - 4x4x4mm. </p><br><p>  <strong>Magia:</strong> <br>  Llamemos al nodo voxel activo de nuestra secuencia.  Dado que el cerebro es un pa√±o modular, introducimos dos tipos de conexiones en √©l: externo e interno (correspondiente a la disposici√≥n espacial de los nodos).  Las conexiones se ensamblan de una manera interesante: construimos una matriz de correlaci√≥n cruzada entre nodos y conectamos los nodos con una conexi√≥n si la correlaci√≥n es mayor que alg√∫n par√°metro adaptativo lambda.  Este par√°metro afecta la descarga de nuestra red. </p><br><p>  El ajuste de par√°metros se lleva a cabo utilizando el procedimiento de "filtrado".  Si balanceamos nuestro lambda un poco, las transiciones bruscas entre las dimensiones finales de la red se vuelven notorias (es decir, un cambio de par√°metro suficientemente peque√±o corresponde a un gran incremento de tama√±o). </p><br><p>  Entonces: las conexiones internas se activan con el valor lambda-1, que corresponde al valor lambda justo antes de una transici√≥n brusca.  Externo: valor lambda-2 correspondiente al valor lambda inmediatamente despu√©s de una transici√≥n brusca. </p><br><p>  <strong>Magia 2:</strong> <br>  filtrado de k-core.  El concepto k-core describe la conectividad de red y est√° formulado de manera bastante simple: la subred m√°xima, cuyos nodos tienen al menos k vecinos.  Dicha subred se puede obtener mediante la eliminaci√≥n iterativa de nodos con menos de k vecinos.  Como los nodos restantes perder√°n vecinos, el proceso contin√∫a hasta que no haya nada que eliminar.  Lo que queda es la red k-core. </p><br><p>  <strong>Resultados:</strong> <br>  Aplicando esta artiller√≠a a nuestros cerebros, puede ver una serie de caracter√≠sticas muy interesantes. </p><br><ol><li>  El n√∫mero de nodos en k-core con k peque√±o / muy grande es extremadamente grande.  Pero para el medio k, por el contrario, no es suficiente.  En la imagen, parece una forma de U, es decir, una configuraci√≥n de red de este tipo proporciona la mayor estabilidad del sistema (resistencia a los errores locales y globales). </li><li>  <strong>y los</strong> nodos <strong>m√°s importantes que</strong> pertenecen a k-core con k peque√±a se pueden ver en casi cualquier estado de la red.  Pero un k-core con k muy grande es caracter√≠stico solo para aquellas partes del cerebro que est√°n activas en el estado inconsciente de <em>giro fusiforme y giro precentral izquierdo</em> .  Las mismas partes de la corteza son m√°s activas y en un estado consciente. </li></ol><br><p>  Para verificar el resultado, los autores crearon un mill√≥n de redes aleatorias basadas en redes reales, haciendo cableado aleatorio, manteniendo el grado original de los nodos (el mismo que el grado del v√©rtice en el gr√°fico).  Las redes reales difer√≠an de las aleatorias por valores mucho mayores de k m√°ximo.  Al mismo tiempo, la forma de U del n√∫mero de nodos en los grupos permaneci√≥ notable en redes aleatorias, lo que llev√≥ a los autores a la idea de que es el grado de los nodos el responsable de este fen√≥meno. </p><br><p>  <strong>Conclusiones:</strong> <br> ,  ,   ,              .     ,     ,      ,    -     (     , , ,     ). </p><br><p>  ,   ,         ,       ,        ,         , ,    - . , ,            qualia. </p><br><h3 id="6-large-memory-layers-with-product-keys"> 6. Large Memory Layers with Product Keys </h3><br><p>  : Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic Denoyer, Herv√© J√©gou (Facebook AI Research, 2019) <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">‚Üí  </a> <br>  :   (  belerafon) </p><br><img src="https://habrastorage.org/webt/4b/_q/nm/4b_qnmw2tilj5zyscirkrogerrg.png"><br><p><br> ,       key-value        ,   . </p><br><p>     -    attention.    q,     k   v.  q,    k,    ,      value  .  ,       .    ,         .      ,         ,   .     -    q       (, -10).        .      . </p><br><p>    ‚Äî     q   k   .   ,    "Product Keys".      ,         q   ,     .        -10   , ,     O(N)    ""  ,   (sqrt(N)). </p><br><p>         key-value .      ,    (  ,     ). ,   BERT      28  . ,          ,     .  : 12-       2  ,  24-  ,    perplexity     . </p><br><p>            (      self-attention). ,     -         .  ,         multy-head attention.  Es decir    query  ,      value,     .         -. </p><br><p>        , ,        ,    ,    BERT  .      . </p><br><h3 id="7-are-we-really-making-much-progress-a-worrying-analysis-of-recent-neural-recommendation-approaches"> 7. Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches </h3><br><p>  : Maurizio Ferrari Dacrema, Paolo Cremonesi, Dietmar Jannach (Politecnico di Milano, University of Klagenfurt, 2019) <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">‚Üí  </a> <br>  :   (  netcitizen) </p><br><img src="https://habrastorage.org/webt/zk/89/wu/zk89wuyudxpggdl6-0tyxe2wwrw.png"><br><p><br>       DL    , ,             . </p><br><p> <strong></strong> <br>             DL       top-n.    DL      KDD, SIGIR, TheWebConf (WWW)  RecSys     : </p><br><ol><li>     </li><li>    -       </li><li>       </li></ol><br><p> <strong></strong> </p><br><ol><li>    7/18 (39%) </li><li>        ‚Äú‚Äù    train/test,     .,   , ,   . </li><li>     (Variational Autoencoders for Collaborative Filtering (Mult-VAE)  ¬±   )     KNN, SVD, PR. </li></ol><br><p> <strong></strong> <br>  DL,       CV, NLP      ,       . </p><br><h3 id="8-omni-scale-feature-learning-for-person-re-identification"> 8. Omni-Scale Feature Learning for Person Re-Identification </h3><br><p>  : Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, Tao Xiang (University of Surrey, Queen Mary University, Samsung AI, 2019) <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">‚Üí  </a> <br>  : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> </a> (  graviton) </p><br><p>  Person Re-Identification,    Face Recognition,    ,          .  (Kaiyang Zhou)        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">deep-person-reid</a>     ,      (OSNet),          Person Re-Identification.     . </p><br><p> <strong>  </strong> Person Re-Identification: </p><br><img src="https://habrastorage.org/webt/6d/rq/jv/6drqjvwsyg33s_e7zv2t4nov0ts.png" width="500" height="250"><br><p><br> <strong> :</strong> </p><br><ol><li>   conv1x1  deepwise conv3x3   conv3x3   (figure 3). </li><li>  ,      .    ResNeXt         ,     Inception      (figure 4). </li><li>      ‚Äúaggregation gate‚Äù       .  ,    Inception     . </li></ol><br><img src="https://habrastorage.org/webt/ic/tc/9z/ictc9z6mcdkv3o0i23swu_f5zty.png"><br><p><br>  OSNet       , ..      ,    :      (  ,  )   . </p><br><p> <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Los resultados de la prueba ReID</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para OSNet (aproximadamente 2 millones de par√°metros) indican la ventaja de esta arquitectura sobre otros modelos ligeros (Mercado: R1 93.6%, mAP 81.0% para OSNet y R1 87.0%, mAP 69.5% para MobileNetV2) y la ausencia de una diferencia significativa en la precisi√≥n con modelos pesados ‚Äã‚Äãde ResNet y DenseNet (Mercado: R1 94.8%, mAP 84.9% para OSNet y R1 94.8%, mAP 86.0% para ResNet).</font></font></p><br><p> Otro desaf√≠o es la <strong>adaptaci√≥n del dominio</strong> : los modelos entrenados en un conjunto de datos tienen mala calidad en otro.  OSNet tambi√©n muestra buenos resultados en este segmento sin el uso de "adaptaci√≥n de dominio no supervisada" (utilizando datos de prueba en una forma no asignada para igualar la distribuci√≥n de datos). </p><br><p>  La arquitectura tambi√©n se ha probado en ImageNet, donde logr√≥ una precisi√≥n similar con MobileNetV2 con menos par√°metros, pero m√°s operaciones. </p><br><h3 id="9-neural-reparameterization-improves-structural-optimization">  9. La reparameterizaci√≥n neural mejora la optimizaci√≥n estructural. </h3><br><p>  Autores: Stephan Hoyer, Jascha Sohl-Dickstein, Sam Greydanus (Google Research, 2019) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">‚Üí Art√≠culo original</a> <br>  Autor de la revisi√≥n: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Alexey</a> (en Arech slack) </p><br><img src="https://habrastorage.org/webt/ga/sd/hd/gasdhdqgy5febp9elrdgdhy9nqg.png"><br><p><br>  En la construcci√≥n y otras tecnolog√≠as, existen tareas de optimizaci√≥n de la estructura / topolog√≠a de alguna soluci√≥n.  Hablando en t√©rminos generales, esta es una respuesta de computadora a una pregunta como, por ejemplo, c√≥mo dise√±ar un puente / edificio / ala de avi√≥n / pala de turbina / blablabla para que se cumplan ciertas restricciones y la estructura sea lo suficientemente fuerte.  Hay un conjunto de m√©todos de soluci√≥n "est√°ndar": funciona, pero no todo es f√°cil all√≠. </p><br><p>  ¬øQu√© inventaron estos tipos de Google?  Dijeron: vamos a generar una soluci√≥n a trav√©s de una red neuronal (la parte de muestreo superior de UNet), y luego utilizando un modelo f√≠sico diferenciable, que calcular√° el comportamiento de una soluci√≥n bajo la influencia de todas las fuerzas y la gravedad, calcularemos la funci√≥n objetivo - fuerza (m√°s precisamente, lo inverso de ella - cumplimiento) ) dise√±os.  Luego, dado que todo es autom√°ticamente diferenciable, obtenemos el gradiente de la funci√≥n objetivo, que se empuja a trav√©s de toda la estructura hacia los pesos y la entrada de la red neuronal.  Cambiamos los pesos y la entrada y continuamos el ciclo hasta la convergencia hacia una soluci√≥n estable. </p><br><p>  Los resultados resultaron en problemas peque√±os (en t√©rminos del tama√±o del espacio de posibles soluciones) comparables a los m√©todos tradicionales para optimizar las topolog√≠as, y para problemas grandes, son notablemente mejores que los tradicionales (sobrepeso en 99 frente a 66 de 116 problemas).  Adem√°s, las soluciones resultantes son a menudo significativamente m√°s tecnol√≥gicas y √≥ptimas que las decisiones de las l√≠neas de base. </p><br><p>  Es decir  de hecho, utilizaron el NS como una forma complicada de parametrizar el modelo f√≠sico de la estructura, que impl√≠citamente (debido a la arquitectura del NS) puede imponer algunas restricciones √∫tiles sobre los valores de los par√°metros (controlados eliminando el NS del m√©todo y la optimizaci√≥n directa de los valores de p√≠xeles). </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">C√≥digo fuente</a> </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Una descripci√≥n m√°s detallada de este art√≠culo sobre habr.</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/472672/">https://habr.com/ru/post/472672/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../472658/index.html">Casi todo sobre el futuro HolyJS 2019 Mosc√∫</a></li>
<li><a href="../472660/index.html">C√≥mo crear prototipos de dispositivos r√°pidamente y por qu√© es importante. Informe Yandex.Taxi</a></li>
<li><a href="../472662/index.html">Con quien ir para exportar</a></li>
<li><a href="../472668/index.html">Pensamiento de producto. ¬øQu√© es y c√≥mo desarrollarlo?</a></li>
<li><a href="../472670/index.html">Limely oto√±o, Limely invierno ...</a></li>
<li><a href="../472674/index.html">Variables de entorno para proyectos de Python</a></li>
<li><a href="../472676/index.html">Creamos el departamento de jones para ayudar a los equipos principales, utilizando solo Slack, Jira y la cinta aislante azul.</a></li>
<li><a href="../472682/index.html">Retrasando el envejecimiento con sinergias de drogas en C. elegans</a></li>
<li><a href="../472684/index.html">Sorpresa fsync () PostgreSQL</a></li>
<li><a href="../472686/index.html">Video Studio basado en i486</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>