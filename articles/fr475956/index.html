<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üöÖ üë®‚Äçüë©‚Äçüëß‚Äçüëß ü¶ì Comment nous avons cr√©√© la technologie de reconnaissance optique de texte. OCR sur Yandex üí£ üëäüèæ üôÜüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Salut Aujourd'hui, je vais expliquer aux lecteurs de Habr comment nous avons cr√©√© une technologie de reconnaissance de texte qui fonctionne dans 45 la...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment nous avons cr√©√© la technologie de reconnaissance optique de texte. OCR sur Yandex</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/475956/">  Salut  Aujourd'hui, je vais expliquer aux lecteurs de Habr comment nous avons cr√©√© une technologie de reconnaissance de texte qui fonctionne dans 45 langues et est accessible aux utilisateurs de Yandex.Cloud, quelles t√¢ches nous avons d√©finies et comment nous les avons r√©solues.  Il sera utile si vous travaillez sur des projets similaires ou si vous voulez savoir comment cela s'est pass√©. Aujourd'hui, il vous suffit de photographier une enseigne d'un magasin turc pour qu'Alice la traduise en russe. <br><br><img src="https://habrastorage.org/webt/fm/c4/rx/fmc4rxrj9iczvwvfjku7cm-nwim.png"><br><a name="habracut"></a><br>  La technologie de reconnaissance optique de caract√®res (OCR) se d√©veloppe dans le monde depuis des d√©cennies.  Chez Yandex, nous avons commenc√© √† d√©velopper notre propre technologie OCR pour am√©liorer nos services et offrir aux utilisateurs plus d'options.  Les images repr√©sentent une grande partie d'Internet et sans la capacit√© de les comprendre, la recherche sur Internet sera incompl√®te. <br><br>  Les solutions d'analyse d'images sont de plus en plus populaires.  Cela est d√ª √† la prolif√©ration de r√©seaux de neurones artificiels et d'appareils dot√©s de capteurs de haute qualit√©.  Il est clair que nous parlons tout d'abord de smartphones, mais pas seulement d'eux. <br><br>  La complexit√© des t√¢ches dans le domaine de la reconnaissance de texte est en constante augmentation - tout a commenc√© avec la reconnaissance des documents num√©ris√©s.  Ensuite, la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">reconnaissance des</a> images Born-Digital avec du texte provenant d'Internet a √©t√© ajout√©e.  Ensuite, avec la popularit√© croissante des cam√©ras mobiles, la reconnaissance des bons plans de cam√©ra ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">texte de sc√®ne focalis√©</a> ).  Et plus les param√®tres sont compliqu√©s: le texte peut √™tre flou ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">texte de sc√®ne incidente</a> ), <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">√©crit</a> avec n'importe quel coude ou en spirale, de diff√©rentes cat√©gories - des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">photographies de</a> re√ßus aux <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">√©tag√®res</a> et enseignes. <br><br><h3>  Dans quelle direction sommes-nous all√©s </h3><br>  La reconnaissance de texte est une classe distincte de t√¢ches de vision par ordinateur.  Comme de nombreux algorithmes de vision par ordinateur, avant la popularit√© des r√©seaux de neurones, il √©tait largement bas√© sur des fonctionnalit√©s manuelles et des heuristiques.  Cependant, r√©cemment, avec la transition vers des approches de r√©seau de neurones, la qualit√© de la technologie a consid√©rablement augment√©.  Regardez l'exemple sur la photo.  Comment cela s'est produit, je le dirai plus loin. <br><br>  Comparez les r√©sultats de reconnaissance d'aujourd'hui avec les r√©sultats de d√©but 2018: <br><div class="scrollable-table"><table><tbody><tr><td><img src="https://habrastorage.org/webt/uf/ux/-g/ufux-gtblu3a_fc96rdowuul7co.png"></td><td><img src="https://habrastorage.org/webt/so/pa/yg/sopaygej-n6c9gurt75zfwa-f3y.png"></td></tr><tr><td> <b><i>2018</i></b> <b><i><br></i></b>  <b><i>Hydratant</i></b> <b><i><br></i></b>  <b><i>n HO - micellaire</i></b> <b><i><br></i></b>  <b><i>eau de douceur luxueuse.</i></b> <b><i><br></i></b>  <b><i>multifonctionnel doux</i></b> <b><i><br></i></b>  <b><i>la formule</i></b> <b><i><br></i></b>  <b><i>utiliser comme moyen</i></b> <b><i><br></i></b>  <b><i>Sl FOR</i></b> <b><i><br></i></b>  <b><i>au lieu d'une lotion nettoyante ou</i></b> <b><i><br></i></b>  <b><i>tonique</i></b> <b><i><br></i></b>  <b><i>Sans alcool, colorants, parab√®nes</i></b> <b><i><br></i></b>  <b><i>...</i></b> </td><td>  <b><i>2019</i></b> <b><i><br></i></b>  <b><i>HYDRATANT</i></b> <b><i><br></i></b>  <b><i>EAU THERMO-MICELLAIRE</i></b> <b><i><br></i></b>  <b><i>DOUCEUR DE LUXE</i></b> <b><i><br></i></b>  <b><i>AUBY Doux et doux</i></b> <b><i><br></i></b>  <b><i>formule multifonctionnelle</i></b> <b><i><br></i></b>  <b><i>pour une utilisation quotidienne dans</i></b> <b><i><br></i></b>  <b><i>comme un moyen de</i></b> <b><i><br></i></b>  <b><i>d√©maquillant au lieu de nettoyer</i></b> <b><i><br></i></b>  <b><i>lotion ou tonique.</i></b> <b><i><br></i></b>  <b><i>Sans alcool, colorants, parab√®nes</i></b> <b><i><br></i></b>  <b><i>...</i></b> </td></tr></tbody></table></div><h3>  Quelles difficult√©s avons-nous rencontr√©es au d√©but? </h3><br>  Au d√©but de notre voyage, nous avons cr√©√© une technologie de reconnaissance pour le russe et l'anglais, et les principaux cas d'utilisation √©taient des pages de texte et des images photographi√©es sur Internet.  Mais au cours du travail, nous avons r√©alis√© que cela ne suffisait pas: le texte sur les images se trouvait dans n'importe quelle langue, sur n'importe quelle surface, et les images s'av√©raient parfois de qualit√© tr√®s diff√©rente.  Cela signifie que la reconnaissance devrait fonctionner dans toutes les situations et sur tous les types de donn√©es entrantes. <br><br>  Et ici, nous sommes confront√©s √† un certain nombre de difficult√©s.  Voici quelques exemples: <br><br><ul><li>  <b>D√©tails</b>  Pour une personne habitu√©e √† obtenir des informations √† partir d'un texte, le texte de l'image est constitu√© de paragraphes, de lignes, de mots et de lettres, mais pour un r√©seau de neurones, tout semble diff√©rent.  En raison de la nature complexe du texte, le r√©seau est oblig√© de voir √† la fois l'image dans son ensemble (par exemple, si les gens se sont joints la main et ont construit une inscription), et les plus petits d√©tails (dans la langue vietnamienne, les symboles similaires ·ª≠ et ·ª´ changent la signification des mots).  Des d√©fis distincts consistent √† reconna√Ætre le texte arbitraire et les polices non standard. </li><li>  <b>Multilinguisme</b> .  Plus nous ajoutions de langues, plus nous √©tions confront√©s √† leurs sp√©cificit√©s: en cyrillique et en latin, les mots sont compos√©s de lettres s√©par√©es, en arabe ils sont √©crits ensemble, en japonais aucun mot s√©par√© n'est distingu√©.  Certaines langues utilisent l'orthographe de gauche √† droite, d'autres de droite √† gauche.  Certains mots sont √©crits horizontalement, d'autres verticalement.  Un outil universel devrait prendre en compte toutes ces fonctionnalit√©s. </li><li>  <b>La structure du texte</b> .  Pour reconna√Ætre des images sp√©cifiques, telles que des ch√®ques ou des documents complexes, une structure qui prend en compte la disposition des paragraphes, des tableaux et d'autres √©l√©ments est cruciale. </li><li>  <b>Performance</b> .  La technologie est utilis√©e sur une grande vari√©t√© d'appareils, y compris hors ligne, nous avons donc d√ª tenir compte des exigences de performances strictes. </li></ul><br><h3>  S√©lection du mod√®le de d√©tection </h3><br>  La premi√®re √©tape pour reconna√Ætre un texte consiste √† d√©terminer sa position (d√©tection). <br>  La d√©tection de texte peut √™tre consid√©r√©e comme une t√¢che de reconnaissance d'objet, o√π des <b>caract√®res</b> , <b>mots</b> ou <b>lignes</b> individuels peuvent agir comme un objet. <br><br>  Il √©tait important pour nous que le mod√®le soit ensuite mis √† l'√©chelle vers d'autres langues (nous prenons d√©sormais en charge 45 langues). <br><br>  De nombreux articles de recherche sur la d√©tection de texte utilisent des mod√®les qui pr√©disent la position des <b>mots</b> individuels.  Mais dans le cas d'un <b>mod√®le universel,</b> cette approche a plusieurs limites - par exemple, le concept m√™me d'un mot pour la langue chinoise est fondamentalement diff√©rent du concept d'un mot, par exemple, en anglais.  Les mots individuels en chinois ne sont pas s√©par√©s par un espace.  En tha√Ø, seules les phrases simples sont supprim√©es avec un espace. <br><br>  Voici des exemples du m√™me texte en russe, chinois et tha√Ø: <br><br> <code>  .    . <br>‰ªäÂ§©Â§©Ê∞îÂæàÂ•Ω ËøôÊòØ‰∏Ä‰∏™Áæé‰∏ΩÁöÑ‰∏ÄÂ§©Êï£Ê≠•„ÄÇ <br> ‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ ‡∏°‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏î‡∏¥‡∏ô‡πÄ‡∏•‡πà‡∏ô‡∏Å‡∏±‡∏ô‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß</code> <br> <br>  <b>Les lignes</b> , √† leur tour, sont tr√®s variables en termes de rapport d'aspect.  De ce fait, les possibilit√©s de ces mod√®les de d√©tection communs (par exemple, bas√©s sur SSD ou RCNN) pour la pr√©diction de ligne sont limit√©es, car ces mod√®les sont bas√©s sur des r√©gions / bo√Ætes d'ancrage candidates avec de nombreux rapports d'aspect pr√©d√©finis.  De plus, les lignes peuvent avoir une forme arbitraire, par exemple courbe, donc pour une description qualitative des lignes il ne suffit pas de d√©crire exclusivement un quad, m√™me avec un angle de rotation. <br><br>  Malgr√© le fait que les positions des <b>caract√®res</b> individuels <b>sont</b> locales et d√©crites, leur inconv√©nient est qu'une √©tape de post-traitement distinct est requise - vous devez s√©lectionner des heuristiques pour coller des caract√®res en mots et en lignes. <br><br>  Par cons√©quent, nous avons pris <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le mod√®le SegLink</a></b> comme base de d√©tection, dont l'id√©e principale est de d√©composer les lignes / mots en deux entit√©s plus locales: les segments et les relations entre eux. <br><br><h3>  Architecture du d√©tecteur </h3><br>  L'architecture du mod√®le est bas√©e sur SSD, qui pr√©dit la position des objets √† plusieurs √©chelles de fonctionnalit√©s.  Ce n'est qu'en plus de pr√©dire les coordonn√©es des ¬´segments¬ª individuels que sont √©galement pr√©dites les ¬´connexions¬ª entre les segments adjacents, c'est-√†-dire si deux segments appartiennent √† la m√™me ligne.  Des ¬´connexions¬ª sont pr√©vues √† la fois pour les segments voisins sur la m√™me √©chelle d'entit√©s et pour les segments situ√©s dans des zones adjacentes √† des √©chelles adjacentes (les segments d'√©chelles d'entit√©s diff√©rentes peuvent varier l√©g√®rement en taille et appartiennent √† la m√™me ligne). <br><br>  Pour chaque √©chelle, chaque cellule d'entit√© est associ√©e √† un ¬´segment¬ª correspondant.  Pour chaque segment s <sup>(x, y, l)</sup> au point (x, y) sur une √©chelle l, on suit: <br>  - p <sub>s</sub> si le segment donn√© est du texte; <br>  - x <sub>s</sub> , y <sub>s</sub> , w <sub>s</sub> , h <sub>s</sub> , Œ∏ <sub>s</sub> - le d√©calage des coordonn√©es de base et l'angle d'inclinaison du segment; <br>  - 8 scores pour la pr√©sence de ¬´connexions¬ª avec des segments adjacents √† la l-i√®me √©chelle (L <sup>w</sup> <sub>s, s '</sub> , s' from {s <sup>(x ', y', l)</sup> } / s <sup>(x, y, l)</sup> , o√π x ‚Äì1 ‚â§ x '‚â§ x + 1, y - 1 ‚â§ y' ‚â§ y + 1); <br>  - 4 scores pour la pr√©sence de ¬´connexions¬ª avec des segments adjacents √† l'√©chelle l-1 (L <sup>c</sup> <sub>s, s '</sub> , s' de {s <sup>(x ', y', l-1)</sup> }, o√π 2x ‚â§ x '‚â§ 2x + 1 , 2y ‚â§ y '‚â§ 2y + 1) (ce qui est vrai du fait que la dimension des entit√©s aux √©chelles voisines diff√®re exactement de 2 fois). <br><br><img src="https://habrastorage.org/webt/fn/ox/en/fnoxen3f1izpei9ecdbdl_eq2xk.png"><br><h5>  <sup><sub>Illustration op√©rationnelle du d√©tecteur SegLink √† partir de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d√©tection de texte orient√© dans des images naturelles en liant des segments</a></sub></sup> </h5><br>  Selon de telles pr√©dictions, si nous prenons comme sommets tous les segments pour lesquels la probabilit√© qu'ils soient du texte est sup√©rieure au seuil Œ±, et comme les ar√™tes sont toutes des liaisons dont la probabilit√© est sup√©rieure au seuil Œ≤, alors les segments forment des composants connect√©s, chacun d√©crivant une ligne de texte . <br><br>  Le mod√®le r√©sultant a une <b>grande capacit√© de g√©n√©ralisation</b> : m√™me form√© aux premi√®res approches sur les donn√©es russes et anglaises, il a trouv√© qualitativement du texte chinois et arabe. <br><br><h3>  Dix scripts </h3><br>  Si pour la d√©tection, nous avons pu cr√©er un mod√®le qui fonctionne imm√©diatement pour toutes les langues, alors pour la reconnaissance des lignes trouv√©es, un tel mod√®le est beaucoup plus difficile √† obtenir.  Par cons√©quent, nous avons d√©cid√© d'utiliser un <b>mod√®le distinct pour chaque script</b> (cyrillique, latin, arabe, h√©breu, grec, arm√©nien, g√©orgien, cor√©en, tha√Ø).  Un mod√®le g√©n√©ral distinct est utilis√© pour le chinois et le japonais en raison de la grande intersection des hi√©roglyphes. <br><br>  Le mod√®le commun √† l'ensemble du script diff√®re du mod√®le distinct pour chaque langue par moins de 1 p.p.  la qualit√©.  Dans le m√™me temps, la cr√©ation et la mise en ≈ìuvre d'un mod√®le est plus simple que, par exemple, 25 mod√®les (le nombre de langues latines prises en charge par notre mod√®le).  Mais en raison de la pr√©sence fr√©quente de l'anglais dans toutes les langues, tous nos mod√®les sont capables de pr√©dire, en plus du script principal, des caract√®res latins. <br><br>  Pour comprendre quel mod√®le doit √™tre utilis√© pour la reconnaissance, nous d√©terminons d'abord si les lignes re√ßues appartiennent √† l'un des 10 scripts disponibles pour la reconnaissance. <br><br>  Il convient de noter s√©par√©ment qu'il n'est pas toujours possible de d√©terminer de mani√®re unique son script le long de la ligne.  Par exemple, de nombreux scripts ou caract√®res latins uniques sont contenus dans de nombreux scripts, de sorte qu'une des classes de sortie du mod√®le est un script "non d√©fini". <br><br><h3>  D√©finition de script </h3><br>  Pour d√©finir le script, nous avons cr√©√© un classificateur distinct.  La t√¢che de d√©finir un script est beaucoup plus simple que la t√¢che de reconnaissance, et le r√©seau neuronal est facilement recycl√© sur des donn√©es synth√©tiques.  Par cons√©quent, dans nos exp√©riences, une <b>pr√©-formation sur le probl√®me de reconnaissance des cordes</b> a permis une am√©lioration significative de la qualit√© du mod√®le.  Pour ce faire, nous avons d'abord form√© le r√©seau au probl√®me de reconnaissance pour toutes les langues disponibles.  Apr√®s cela, l'√©pine dorsale r√©sultante a √©t√© utilis√©e pour initialiser le mod√®le √† la t√¢che de classification de script. <br><br>  Alors qu'un script sur une ligne individuelle est souvent assez bruyant, l'image dans son ensemble contient le plus souvent du texte dans une langue, soit en plus du principal entrecoup√© d'anglais (ou dans le cas de nos utilisateurs russes).  Par cons√©quent, pour <b>augmenter la</b> stabilit√©, nous agr√©gons les pr√©dictions des lignes de l'image afin d'obtenir une pr√©diction plus stable du script d'image.  Les lignes avec une classe pr√©dite ¬´ind√©finie¬ª ne sont pas prises en compte dans l'agr√©gation. <br><br><h3>  Reconnaissance de ligne </h3><br>  L'√©tape suivante, lorsque nous avons d√©j√† d√©termin√© la position de chaque ligne et son script, nous devons <b>reconna√Ætre la s√©quence de caract√®res du script donn√©</b> qui y est affich√©, c'est-√†-dire de la s√©quence de pixels pour pr√©dire la s√©quence de caract√®res.  Apr√®s de nombreuses exp√©riences, nous sommes arriv√©s au mod√®le suivant bas√© sur l'attention de sequence2sequence: <br><br><img src="https://habrastorage.org/webt/_0/6k/sf/_06ksfdetbjobwudopmi4xq0j4c.png"><br><br>  L'utilisation de CNN + BiLSTM dans l'encodeur vous permet d'obtenir des signes qui capturent les contextes local et mondial.  Pour le texte, cela est important - il est souvent √©crit dans une seule police (il est beaucoup plus facile de distinguer des lettres similaires avec des informations sur la police).  Et pour distinguer deux lettres √©crites avec un espace de celles cons√©cutives, des statistiques globales sont √©galement n√©cessaires pour la ligne. <br><br>  <b>Une observation int√©ressante</b> : dans le mod√®le r√©sultant, les sorties du masque d'attention pour un symbole particulier peuvent √™tre utilis√©es pour pr√©dire sa position dans l'image. <br><br>  Cela nous a inspir√© pour essayer de <b>¬´concentrer¬ª clairement l‚Äôattention du mod√®le</b> .  De telles id√©es ont √©galement √©t√© trouv√©es dans des articles - par exemple, dans l'article <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Focusing Attention: Towards Accurate Text Recognition in Natural Images</a> . <br><br>  √âtant donn√© que le m√©canisme d'attention donne une distribution de probabilit√© sur l'espace des caract√©ristiques, si nous prenons comme perte suppl√©mentaire la somme des sorties d'attention √† l'int√©rieur du masque correspondant √† la lettre pr√©dite √† cette √©tape, nous obtiendrons la partie de ¬´l'attention¬ª qui se concentre directement sur lui. <br><br>  En introduisant la perte -log (‚àë <sub>i, j‚ààM <sub>t</sub></sub> Œ± <sub>i, j</sub> ), o√π M <sub>t</sub> est le masque de la ti√®me lettre, Œ± est la sortie de l'attention, nous encouragerons ¬´l'attention¬ª √† se concentrer sur le symbole donn√© et ainsi aider les r√©seaux de neurones apprennent mieux. <br><br>  Pour les exemples d'entra√Ænement pour lesquels l'emplacement des caract√®res individuels est inconnu ou inexact (toutes les donn√©es d'entra√Ænement n'ont pas de marquages ‚Äã‚Äãau niveau des caract√®res individuels, pas des mots), ce terme n'a pas √©t√© pris en compte dans la perte finale. <br><br>  Autre fonctionnalit√© int√©ressante: cette architecture vous permet de pr√©dire la <b>reconnaissance des lignes de droite √† gauche</b> sans modifications suppl√©mentaires (ce qui est important, par exemple, pour des langues comme l'arabe, l'h√©breu).  Le mod√®le lui-m√™me commence √† √©mettre une reconnaissance de droite √† gauche. <br><br><h3>  Mod√®les rapides et lents </h3><br>  Dans le processus, nous avons rencontr√© un probl√®me: <b>pour les polices ¬´hautes¬ª</b> , c'est-√†-dire les polices allong√©es verticalement, le mod√®le fonctionnait mal.  Cela est d√ª au fait que la dimension des signes au niveau de l'attention est 8 fois plus petite que la dimension de l'image d'origine en raison de foul√©es et de tiraillements dans l'architecture de la partie convolutionnelle du r√©seau.  Et les emplacements de plusieurs caract√®res voisins dans l'image source peuvent correspondre √† l'emplacement du m√™me vecteur caract√©ristique, ce qui peut entra√Æner des erreurs dans de tels exemples.  L'utilisation d'une architecture avec un r√©tr√©cissement moindre de la dimension des fonctionnalit√©s a conduit √† une augmentation de la qualit√©, mais aussi √† une augmentation du temps de traitement. <br><br>  Pour r√©soudre ce probl√®me et <b>√©viter d'augmenter le temps de traitement</b> , nous avons apport√© les am√©liorations suivantes au mod√®le: <br><br><img src="https://habrastorage.org/webt/po/pp/ob/poppobis-rbsdqtyrbgyaq8jzik.png"><br><br>  Nous avons form√© √† la fois un mod√®le rapide avec beaucoup de foul√©es et un mod√®le lent avec moins.  Sur la couche o√π les param√®tres du mod√®le ont commenc√© √† diff√©rer, nous avons ajout√© une sortie r√©seau distincte qui pr√©dit quel mod√®le aurait moins d'erreur de reconnaissance.  La perte totale du mod√®le √©tait compos√©e de la <sub>qualit√©</sub> L <sub>small</sub> + L <sub>big</sub> + L.  Ainsi, sur la couche interm√©diaire, le mod√®le a appris √† d√©terminer la ¬´complexit√©¬ª de cet exemple.  De plus, au stade de l'application, la partie g√©n√©rale et la pr√©diction de la ¬´complexit√©¬ª de l'exemple ont √©t√© prises en compte pour toutes les lignes, et selon sa sortie, le mod√®le rapide ou le mod√®le lent a √©t√© utilis√© √† l'avenir en fonction de la valeur seuil.  Cela nous a permis d'obtenir une qualit√© qui n'est presque pas diff√©rente de la qualit√© d'un mod√®le long, tandis que la vitesse n'a augment√© que de 5% au lieu des 30% estim√©s. <br><br><h3>  Donn√©es d'entra√Ænement </h3><br>  Une √©tape importante dans la cr√©ation d'un mod√®le de haute qualit√© est la pr√©paration d'un √©chantillon de formation large et vari√©.  La nature "synth√©tique" du texte permet de g√©n√©rer de grandes quantit√©s d'exemples et d'obtenir des r√©sultats d√©cents sur des donn√©es r√©elles. <br><br>  Apr√®s la premi√®re approche de la g√©n√©ration de donn√©es synth√©tiques, nous avons soigneusement examin√© les r√©sultats du mod√®le obtenu et avons constat√© que le mod√®le ne reconna√Æt pas bien les lettres simples `` I '' en raison du biais dans les textes utilis√©s pour cr√©er l'ensemble d'apprentissage.  Par cons√©quent, nous avons clairement g√©n√©r√© un <b>ensemble d'exemples ¬´probl√©matiques¬ª</b> , et lorsque nous l'avons ajout√© aux donn√©es initiales du mod√®le, la qualit√© a consid√©rablement augment√©.  Nous avons r√©p√©t√© ce processus √† plusieurs reprises, en ajoutant des tranches de plus en plus complexes, sur lesquelles nous voulions am√©liorer la qualit√© de la reconnaissance. <br><br>  Le point important est que les <b>donn√©es</b> g√©n√©r√©es <b>doivent √™tre diverses et similaires aux vraies</b> .  Et si vous souhaitez que le mod√®le fonctionne sur des photographies de texte sur des feuilles de papier et que l'ensemble de donn√©es synth√©tiques contient du texte √©crit au-dessus de paysages, cela peut ne pas fonctionner. <br><br>  Une autre √©tape importante consiste √† utiliser pour la formation des exemples sur lesquels la reconnaissance actuelle est erron√©e.  S'il y a un grand nombre d'images pour lesquelles il n'y a pas de majoration, vous pouvez prendre les sorties du syst√®me de reconnaissance actuel dans lesquelles elle n'est pas s√ªre, et les marquer uniquement, r√©duisant ainsi le co√ªt du balisage. <br><br>  Pour des exemples complexes, nous avons demand√© aux utilisateurs du service Yandex.Tolok des frais pour photographier et nous envoyer des <b>images d'un certain groupe ¬´complexe¬ª</b> - par exemple, des photos de colis de marchandises: <br><br><img src="https://habrastorage.org/webt/tm/zx/0k/tmzx0kmyswtdxz6u_ri_yfxdrzy.png" width="50%"><img src="https://habrastorage.org/webt/n9/pb/ru/n9pbrufm0gcwp8lggc9bk9kxaxe.png" width="50%"><br><br><h3>  Qualit√© du travail sur les donn√©es "complexes" </h3><br>  Nous voulons donner √† nos utilisateurs la possibilit√© de travailler avec des photographies de toute complexit√©, car il peut √™tre n√©cessaire de reconna√Ætre ou de traduire le texte non seulement sur la page du livre ou le document num√©ris√©, mais √©galement sur une plaque de rue, une annonce ou un emballage de produit.  Par cons√©quent, tout en maintenant la haute qualit√© du travail sur le flux des livres et des documents (nous consacrerons une histoire distincte √† ce sujet), nous accordons une attention particuli√®re aux ¬´ensembles d'images complexes¬ª. <br><br>  De la mani√®re d√©crite ci-dessus, nous avons compil√© un ensemble d'images contenant du texte √† l'√©tat sauvage qui peuvent √™tre utiles √† nos utilisateurs: photographies d'enseignes, annonces, tablettes, couvertures de livres, textes sur les appareils √©lectrom√©nagers, les v√™tements et les objets.  Sur cet ensemble de donn√©es (dont le lien est ci-dessous), nous avons √©valu√© la qualit√© de notre algorithme. <br><br>  En tant que mesure de comparaison, nous avons utilis√© la mesure standard d'exactitude et d'exhaustivit√© de la reconnaissance des mots dans l'ensemble de donn√©es, ainsi que la mesure F.  Un mot reconnu est consid√©r√© comme correctement trouv√© si ses coordonn√©es correspondent aux coordonn√©es du mot balis√© (IoU&gt; 0,3) et la reconnaissance co√Øncide avec le balis√© exactement au cas.  Chiffres sur l'ensemble de donn√©es r√©sultant: <br><div class="scrollable-table"><table><tbody><tr><td>  Syst√®me de reconnaissance </td><td>  Compl√©tude </td><td>  Pr√©cision </td><td>  Mesure F </td></tr><tr><td>  Yandex Vision </td><td>  73,99 </td><td>  86,57 </td><td>  79,79 </td></tr></tbody></table></div><br>  L'ensemble de donn√©es, les m√©triques et les scripts pour reproduire les r√©sultats sont disponibles <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . <br><br>  Upd.  Mes amis, la comparaison de notre technologie avec une solution similaire d'Abbyy a suscit√© beaucoup de controverses.  Nous respectons les opinions de la communaut√© et des pairs de l'industrie.  Mais en m√™me temps, nous sommes confiants dans nos r√©sultats, nous avons donc d√©cid√© de cette fa√ßon: nous retirerons les r√©sultats d'autres produits de la comparaison, discuterons de nouveau de la m√©thodologie de test avec eux et reviendrons sur les r√©sultats dans lesquels nous arriverons √† un accord g√©n√©ral. <br><br><h3>  Prochaines √©tapes </h3><br>  √Ä la jonction d'√©tapes individuelles, telles que la d√©tection et la reconnaissance, des probl√®mes surviennent toujours: les moindres changements dans le mod√®le de d√©tection entra√Ænent la n√©cessit√© de changer le mod√®le de reconnaissance, nous exp√©rimentons donc activement la cr√©ation d'une solution de bout en bout. <br><br>  En plus des moyens d√©j√† d√©crits d'am√©liorer la technologie, nous d√©velopperons une direction d'analyse de la structure du document, qui est fondamentalement importante lors de l'extraction d'informations et qui est demand√©e par les utilisateurs. <br><br><h3>  Conclusion </h3><br>  Les utilisateurs sont d√©j√† habitu√©s aux technologies pratiques et, sans h√©sitation, allumez l‚Äôappareil photo, pointez le signe du magasin, le menu du restaurant ou la page du livre dans une langue √©trang√®re et recevez rapidement une traduction.  Nous reconnaissons le texte dans 45 langues avec une pr√©cision √©prouv√©e, et les opportunit√©s ne feront que s'√©largir.  Un ensemble d'outils √† l'int√©rieur de Yandex.Cloud permet √† quiconque souhaite utiliser les meilleures pratiques que Yandex fait depuis longtemps. <br><br>  Aujourd'hui, vous pouvez simplement prendre la technologie finie, l'int√©grer dans votre propre application et l'utiliser pour cr√©er de nouveaux produits et automatiser vos propres processus.  La documentation de notre OCR est disponible <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . <br><br>  Que lire: <br><br><ol><li><a name="karatzas1"></a>  D. Karatzas, SR Mestre, J. Mas, F. Nourbakhsh et PP Roy, ¬´ICDAR 2011 robust reading competition-challenge 1: reading text in born-digital images (web and email)¬ª, in Document Analysis and Recognition (ICDAR ), Conf√©rence internationale de 2011 sur.  IEEE, 2011, pp.  1485-1490. </li><li><a name="karatzas2"></a>  Karatzas D. et al.  Concours ICDAR 2015 sur la lecture robuste // 2015 13e Conf√©rence internationale sur l'analyse et la reconnaissance des documents (ICDAR).  - IEEE, 2015 .-- S. 1156-1160. </li><li><a name="chng"></a>  Chee-Kheng Chng et.  al.  ICDAR2019 D√©fi de lecture robuste sur le texte de forme arbitraire (RRC-ArT) [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">arxiv: 1909.07145v1</a> ] </li><li><a name="icdar2019"></a>  D√©fi de lecture robuste ICDAR 2019 sur les OCR des re√ßus num√©ris√©s et l'extraction d'informations <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">rrc.cvc.uab.es/?ch=13</a> </li><li><a name="shopsign"></a>  ShopSign: ensemble de donn√©es de texte de sc√®nes diverses d'enseignes chinoises dans les vues de rue [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">arxiv: 1903.10412</a> ] </li><li><a name="seglink"></a>  Baoguang Shi, Xiang Bai, Serge Belongie D√©tecter du texte orient√© dans des images naturelles en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">liant des</a> segments [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">arxiv: 1703.06520</a> ]. </li><li><a name="focusing"></a>  Zhanzhan Cheng, Fan Bai, Yunlu Xu, Gang Zheng, Shiliang Pu, Shuigeng Zhou Attirer l'attention: Vers une reconnaissance pr√©cise du texte dans les images naturelles [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">arxiv: 1709.02054</a> ]. </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr475956/">https://habr.com/ru/post/fr475956/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr475940/index.html">Vue Storefront: deuxi√®me approche Shell</a></li>
<li><a href="../fr475942/index.html">Guide illustr√© OAuth et OpenID Connect</a></li>
<li><a href="../fr475944/index.html">La course √† pied est un sport id√©al pour un travailleur √† distance. Partie 2: physique et mat√©riel</a></li>
<li><a href="../fr475948/index.html">JH Rainwater "Comment faire pa√Ætre les chats" (deuxi√®me partie): tout ce qui reste √† ma√Ætriser technique</a></li>
<li><a href="../fr475950/index.html">Pourquoi un robot devrait-il se limiter √† ramasser des balles de golf? Il y a aussi le tennis</a></li>
<li><a href="../fr475958/index.html">L'histoire de la fa√ßon dont la fille s'est r√©unie en informatique</a></li>
<li><a href="../fr475960/index.html">AHURATUS Smart Home Voice Assistant</a></li>
<li><a href="../fr475968/index.html">Nouvelles int√©ressantes Vue 3</a></li>
<li><a href="../fr475974/index.html">Comment nous avons fait un hackathon dans le train et ce qui en est sorti</a></li>
<li><a href="../fr475978/index.html">√Ä quoi sert le si√®ge du train?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>