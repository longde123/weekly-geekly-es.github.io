<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚òùÔ∏è üßõüèº ü§ü D√©veloppement de base de donn√©es dans Dropbox. Le chemin d'une base de donn√©es MySQL globale √† des milliers de serveurs üßõüèΩ üó°Ô∏è üöª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Lorsque Dropbox vient de d√©marrer, un utilisateur de Hacker News a d√©clar√© qu'il pouvait √™tre impl√©ment√© avec plusieurs scripts bash en utilisant FTP ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>D√©veloppement de base de donn√©es dans Dropbox. Le chemin d'une base de donn√©es MySQL globale √† des milliers de serveurs</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/417315/">  Lorsque Dropbox vient de d√©marrer, un utilisateur de Hacker News a d√©clar√© qu'il pouvait √™tre impl√©ment√© avec plusieurs scripts bash en utilisant FTP et Git.  Maintenant, cela ne peut en aucun cas √™tre dit, il s'agit d'un grand stockage de fichiers dans le cloud avec des milliards de nouveaux fichiers chaque jour, qui ne sont pas simplement stock√©s d'une mani√®re ou d'une autre dans la base de donn√©es, mais de telle mani√®re que n'importe quelle base de donn√©es puisse √™tre restaur√©e √† tout moment au cours des six derniers jours. <br><br>  Sous la coupe, la transcription du rapport de <strong>Glory Bakhmutov</strong> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">m0sth8</a> ) √† Highload ++ 2017, sur le d√©veloppement des bases de donn√©es dans Dropbox et leur organisation actuelle. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/hUFFsLoCRNU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <strong>√Ä propos du conf√©rencier:</strong> Gloire √† Bakhmutov - ing√©nieur en fiabilit√© de site dans l'√©quipe Dropbox, aime beaucoup Go et appara√Æt parfois dans le podcast golangshow.com. <br><br><h2>  Table des mati√®res <br></h2><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">En bref sur l'architecture Dropbox</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">L'histoire du d√©veloppement</a> des bases de donn√©es et le fonctionnement de l'architecture Dropbox actuelle </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Les op√©rations les plus simples sur les bases de donn√©es</a> (feylovers, sauvegardes, clones, promotions) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Automatisation</a> - qui g√®re toutes les bases de donn√©es et ex√©cute les op√©rations </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Suivi</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Test, staging et DRT</a> </li></ul><br><img src="https://habrastorage.org/webt/dh/gt/p7/dhgtp7pgu4eyl6rc3ncfnaf9s3i.jpeg"><br><a name="habracut"></a><br><a name="dropbox_architecture"></a><h2>  Architecture Dropbox en langage clair </h2><br>  Dropbox est apparu en 2008.  Il s'agit essentiellement d'un stockage de fichiers cloud.  Lorsque Dropbox vient de d√©marrer, un utilisateur de Hacker News a d√©clar√© qu'il pouvait √™tre impl√©ment√© avec plusieurs scripts bash en utilisant FTP et Git.  Mais, n√©anmoins, Dropbox se d√©veloppe et c'est maintenant un service assez important avec plus de 1,5 milliard d'utilisateurs, 200 000 entreprises et un grand nombre (plusieurs milliards!) De nouveaux fichiers chaque jour. <br><br>  <strong>√Ä quoi ressemble Dropbox?</strong> <br><img src="https://habrastorage.org/webt/ed/em/km/edemkm1wqvlbv6jb0qobp5c2dgc.jpeg"><br><br>  Nous avons plusieurs clients (interface web, API pour les applications qui utilisent Dropbox, applications de bureau).  Tous ces clients utilisent l'API et communiquent avec deux grands services qui peuvent logiquement √™tre divis√©s en: <br><br><ol><li>  <strong>Metaserver</strong> </li><li>  <strong>Blockserver</strong> </li></ol><br>  Metaserver stocke des m√©ta-informations sur le fichier: taille, commentaires sur celui-ci, liens vers ce fichier dans Dropbox, etc.  Blockserver ne stocke que des informations sur les fichiers: dossiers, chemins, etc. <br><br>  <strong>Comment √ßa marche?</strong> <br><br>  Par exemple, vous avez un fichier video.avi avec une sorte de vid√©o. <br><img src="https://habrastorage.org/webt/kc/i9/op/kci9op0l7f_tecaxetg7uzpzemw.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lien depuis la diapositive</a></em> <br><br><ul><li>  Le client divise ce fichier en plusieurs morceaux (dans ce cas, 4 Mo chacun), calcule la somme de contr√¥le et envoie une demande au Metaserver: "J'ai un fichier * .avi, je veux le t√©l√©charger, les quantit√©s de hachage sont telles et telles." </li><li>  Metaserver renvoie la r√©ponse: "Je n'ai pas ces blocs, t√©l√©chargeons!"  Ou il peut r√©pondre qu'il a tout ou partie des blocs, et que seuls les autres doivent √™tre charg√©s. </li></ul><br><img src="https://habrastorage.org/webt/qa/zr/79/qazr79svhai2ouk6v8lta1zcp-u.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lien depuis la diapositive</a></em> <br><br><ul><li>  Apr√®s cela, le client va √† Blockserver, envoie le montant du hachage et le bloc de donn√©es lui-m√™me, qui est stock√© sur le Blockserver. </li><li>  Blockserver confirme l'op√©ration. </li></ul><br><img src="https://habrastorage.org/webt/kl/dx/xw/kldxxw1ncgj4ytt1pqq5bh95iue.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lien depuis la diapositive</a></em> <br><br>  Bien s√ªr, c'est un sch√©ma tr√®s simplifi√©, le protocole est beaucoup plus compliqu√©: il y a une synchronisation entre les clients au sein du m√™me r√©seau, il y a des pilotes de noyau, la possibilit√© de r√©soudre les collisions, etc.  C'est un protocole assez complexe, mais il fonctionne de mani√®re sch√©matique. <br><img src="https://habrastorage.org/webt/ev/-x/_h/ev-x_hwfhwpfixld61yqiraelc4.jpeg"><br><br>  Lorsqu'un client enregistre quelque chose sur Metaserver, toutes les informations vont √† MySQL.  Blockserver stocke √©galement des informations sur les fichiers, leur structure, leurs blocs, dans MySQL.  Blockserver stocke √©galement les blocs eux-m√™mes dans Block Storage, qui, √† son tour, stocke des informations sur l'endroit o√π se trouve le bloc, sur quel serveur et comment il est trait√©, √©galement dans MYSQL. <br><br><blockquote>  Pour stocker des exaoctets de fichiers utilisateur, nous stockons simultan√©ment des informations suppl√©mentaires dans une base de donn√©es de plusieurs dizaines de p√©taoctets r√©partis sur 6 000 serveurs. </blockquote><br><a name="history_development"></a><h2>  Historique de d√©veloppement de la base de donn√©es </h2><br>  Comment les bases de donn√©es ont-elles √©volu√© dans Dropbox? <br><img src="https://habrastorage.org/webt/oe/w9/ok/oew9okiqdeivyhvhycznly7kvbe.jpeg"><br><br>  En 2008, tout a commenc√© avec un Metaserver et une base de donn√©es globale.  Toutes les informations dont Dropbox avait besoin d'√™tre stock√©es quelque part, il les a enregistr√©es dans le seul MySQL global.  Cela n'a pas dur√© longtemps, car le nombre d'utilisateurs a augment√© et les bases de donn√©es et tablettes individuelles √† l'int√©rieur des bases de donn√©es ont gonfl√© plus rapidement que les autres. <br><img src="https://habrastorage.org/webt/ry/lt/h9/rylth906zfbbcou6nz7ve_yqib8.jpeg"><br><br>  Par cons√©quent, en 2011, plusieurs tableaux ont √©t√© soumis √† des serveurs distincts: <br><br><ul><li>  <strong>Utilisateur</strong> , avec des informations sur les utilisateurs, par exemple, les connexions et les jetons oAuth; </li><li>  <strong>H√¥te</strong> , avec des informations sur les fichiers de Blockserver; </li><li>  <strong>Divers</strong> , qui n'√©tait pas impliqu√© dans le traitement des demandes de production, mais √©tait utilis√© pour des fonctions utilitaires, comme les travaux par lots. </li></ul><br><img src="https://habrastorage.org/webt/ja/ec/ja/jaecja2eklv8znsqhf5lt-dyez8.jpeg"><br><br>  Mais apr√®s 2012, Dropbox a commenc√© √† se d√©velopper consid√©rablement, depuis lors, <strong>nous</strong> avons augment√© d' <strong>environ 100 millions d'utilisateurs par an</strong> . <br><img src="https://habrastorage.org/webt/ie/cr/-s/iecr-syyi6qprj2zj45qx4l42k4.jpeg"><br><br>  Il √©tait n√©cessaire de prendre en compte une croissance aussi √©norme, et donc √† la fin de 2011, nous avions des tessons - une base compos√©e de 1600 tessons.  Initialement, seulement 8 serveurs avec 200 fragments chacun.  Il s'agit maintenant de 400 serveurs ma√Ætres avec chacun 4 fragments. <br><img src="https://habrastorage.org/webt/b3/v9/vy/b3v9vyjqzwau2kgmadhgb2vg0vo.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lien depuis la diapositive</a></em> <br><br>  En 2012, nous avons r√©alis√© que la cr√©ation de tables et leur mise √† jour dans la base de donn√©es pour chaque logique m√©tier ajout√©e est tr√®s difficile, morne et probl√©matique.  Par cons√©quent, en 2012, nous avons invent√© notre propre stockage graphique, que nous avons appel√© <strong>Edgestore</strong> , et depuis lors, toute la logique m√©tier et les m√©ta-informations que l'application g√©n√®re sont stock√©es dans Edgestore. <br><br>  Edgestore extrait essentiellement MySQL des clients.  Les clients ont certaines entit√©s qui sont interconnect√©es par des liens de l'API gRPC √† Edgestore Core, qui convertit ces donn√©es en MySQL et les stocke en quelque sorte l√†-bas (en gros, il donne tout cela √† partir du cache). <br><img src="https://habrastorage.org/webt/bj/s7/dz/bjs7dz7-cdsvjblcgftjdeybrgk.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lien depuis la diapositive</a></em> <br><br>  <strong>En 2015, nous avons quitt√© Amazon S3</strong> , d√©velopp√© notre propre stockage cloud appel√© Magic Pocket.  Il contient des informations sur l'emplacement d'un fichier de bloc, sur quel serveur, sur les mouvements de ces blocs entre les serveurs, stock√©s dans MySQL. <br><img src="https://habrastorage.org/webt/f_/bz/lm/f_bzlm3tk9e3lwqo64x4kfuhhok.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lien depuis la diapositive</a></em> <br><br>  Mais MySQL est utilis√© de mani√®re tr√®s d√©licate - essentiellement, comme une grande table de hachage distribu√©e.  Il s'agit d'une charge tr√®s diff√©rente, principalement lors de la lecture d'enregistrements al√©atoires.  90% de l'utilisation est des E / S. <br><br><h2>  Architecture de base de donn√©es </h2><br>  Tout d'abord, nous avons imm√©diatement identifi√© certains principes par lesquels nous construisons l'architecture de notre base de donn√©es: <br><br><ol><li>  <strong>Fiabilit√© et durabilit√©</strong> .  C'est le principe le plus important et ce que les clients attendent de nous - les donn√©es ne doivent pas √™tre perdues. </li><li>  <strong>L'optimalit√© de la solution</strong> est un principe tout aussi important.  Par exemple, les sauvegardes doivent √™tre effectu√©es rapidement et restaur√©es rapidement √©galement. </li><li>  <strong>Simplicit√© de la solution</strong> - √† la fois sur le plan architectural et en termes de service et de support de d√©veloppement ult√©rieur. </li><li>  <strong>Co√ªt de possession</strong> .  Si quelque chose optimise la solution, mais co√ªte tr√®s cher, cela ne nous convient pas.  Par exemple, un esclave qui se trouve un jour derri√®re le ma√Ætre est tr√®s pratique pour les sauvegardes, mais vous devez ensuite ajouter 1 000 de plus √† 6 000 serveurs - le co√ªt de possession d'un tel esclave est tr√®s √©lev√©. </li></ol><br>  Tous les principes doivent √™tre <strong>v√©rifiables et mesurables</strong> , c'est-√†-dire qu'ils doivent avoir des param√®tres.  Si nous parlons du co√ªt de possession, nous devons calculer le nombre de serveurs que nous avons, par exemple, va aux bases de donn√©es, combien de serveurs vont aux sauvegardes et combien cela co√ªte pour Dropbox √† la fin.  Lorsque nous choisissons une nouvelle solution, nous comptons toutes les m√©triques et nous concentrons sur elles.  Lors du choix d'une solution, nous sommes pleinement guid√©s par ces principes. <br><br><h2>  Topologie de base </h2><br>  La base de donn√©es est structur√©e comme suit: <br><br><ul><li>  Dans le centre de donn√©es principal, nous avons un ma√Ætre, dans lequel tous les enregistrements se produisent. </li><li>  Le serveur ma√Ætre poss√®de deux serveurs esclaves vers lesquels se produit la r√©plication semi-synchronis√©e.  Les serveurs meurent souvent (environ 10 par semaine), nous avons donc besoin de deux serveurs esclaves. </li><li>  Les serveurs esclaves sont dans des clusters s√©par√©s.  Les clusters sont des pi√®ces compl√®tement s√©par√©es dans le centre de donn√©es qui ne sont pas connect√©es les unes aux autres.  Si une pi√®ce br√ªle, la seconde reste compl√®tement fonctionnelle. </li><li>  Dans un autre centre de donn√©es, nous avons √©galement le pseudo-ma√Ætre (ma√Ætre interm√©diaire), qui n'est en fait qu'un esclave, qui a un autre esclave. </li></ul><br><img src="https://habrastorage.org/webt/k6/6s/x6/k66sx6siyp6efjxxmfrot21ueha.jpeg"><br><br>  Une telle topologie a √©t√© choisie parce que si le premier centre de donn√©es meurt soudainement en nous, alors dans le deuxi√®me centre de donn√©es, nous avons une <strong>topologie presque compl√®te</strong> .  Nous changeons simplement toutes les adresses dans Discovery, et les clients peuvent travailler. <br><br><h3>  Topologies sp√©cialis√©es </h3><br>  Nous avons √©galement des topologies sp√©cialis√©es. <br><br>  La topologie <strong>Magic Pocket se</strong> compose d'un serveur ma√Ætre et de deux serveurs esclaves.  Cela est d√ª au fait que Magic Pocket duplique lui-m√™me les donn√©es entre les zones.  S'il perd un cluster, il peut restaurer toutes les donn√©es d'autres zones via le code d'effacement. <br><img src="https://habrastorage.org/webt/gk/o7/bi/gko7bifb-4ted4cvwmzgyn5lasw.jpeg"><br><br>  La topologie <strong>active-active</strong> est la topologie personnalis√©e utilis√©e par Edgestore.  Il a un ma√Ætre et deux esclaves dans chacun des deux centres de donn√©es, et ils sont esclaves l'un pour l'autre.  C'est un <strong>sch√©ma</strong> tr√®s <strong>dangereux</strong> , mais Edgestore √† son niveau sait exactement quelles donn√©es sur quel ma√Ætre sur quelle plage il peut √©crire.  Par cons√©quent, cette topologie ne se casse pas. <br><img src="https://habrastorage.org/webt/xe/nv/wx/xenvwx3ls9ct8fssverq3htck10.jpeg"><br><br><h3>  Instance </h3><br>  Nous avons install√© des serveurs assez simples avec une configuration de 4 √† 5 ans: <br><br><ul><li>  <strong>2x noyaux Xeon 10;</strong> </li><li>  <strong>5 To (8 SSD Raid 0 *);</strong> </li><li>  <strong>384 Go de m√©moire.</strong> </li></ul><br>  * Raid 0 - car il est plus facile et beaucoup plus rapide de remplacer un serveur entier que des lecteurs. <br><br><h4>  Instance unique </h4><br>  Sur ce serveur, nous avons une grande instance MySQL sur laquelle se trouvent plusieurs fragments.  Cette instance MySQL alloue imm√©diatement presque toute la m√©moire.  D'autres processus sont √©galement en cours d'ex√©cution sur le serveur: proxy, collecte de statistiques, journaux, etc. <br><br><img src="https://habrastorage.org/webt/z8/yd/vw/z8ydvwabte3v8pytwbrl1vi8yc0.jpeg"><br><br>  Cette solution est bonne en ce que: <br><br>  + C'est <strong>facile √† g√©rer</strong> .  Si vous devez remplacer l'instance MySQL, remplacez simplement le serveur. <br><br>  + <strong>Faylovers</strong> . <br><br>  D'un autre c√¥t√©: <br><br>  - Il est probl√©matique que toutes les op√©rations se produisent sur l'ensemble de l'instance de MySQL et imm√©diatement sur tous les fragments.  Par exemple, si vous avez besoin de sauvegarder, nous sauvegardons tous les fragments en m√™me temps.  Si vous devez faire un faylover, nous faisons un faylover des quatre fragments √† la fois.  En cons√©quence, l'accessibilit√© en souffre 4 fois plus. <br><br>  - Les probl√®mes de r√©plication d'un fragment affectent d'autres fragments.  La r√©plication MySQL n'est pas parall√®le et tous les fragments fonctionnent sur un seul thread.  Si quelque chose arrive √† un √©clat, les autres deviennent √©galement des victimes. <br><br>  Alors maintenant, nous passons √† une topologie diff√©rente. <br><br><h4>  Multi-instance </h4><br><img src="https://habrastorage.org/webt/lg/7x/ks/lg7xks5vbogjaf6slr7tidlc6ty.jpeg"><br><br>  Dans la nouvelle version, plusieurs instances MySQL sont lanc√©es simultan√©ment sur le serveur, chacune avec un fragment.  Quoi de mieux? <br><br>  + Nous ne pouvons <strong>effectuer des op√©rations que sur un fragment sp√©cifique</strong> .  Autrement dit, si vous avez besoin d'un faylover, changez un seul fragment, si vous avez besoin d'une sauvegarde, nous ne sauvegardons qu'un seul fragment.  Cela signifie que les op√©rations sont consid√©rablement acc√©l√©r√©es - 4 fois pour un serveur √† quatre fragments. <br><br>  + Les <strong>√©clats ne s'influencent gu√®re</strong> . <br><br>  + <strong>Am√©lioration de la r√©plication.</strong>  Nous pouvons m√©langer diff√©rentes cat√©gories et classes de bases de donn√©es.  Edgestore prend beaucoup d'espace, par exemple, les 4 To, et Magic Pocket ne prend que 1 To, mais il a une utilisation de 90%.  Autrement dit, nous pouvons combiner diff√©rentes cat√©gories qui utilisent les E / S et les ressources machine de diff√©rentes mani√®res, et d√©marrer 4 flux de r√©plication. <br><br>  Bien s√ªr, cette solution a ses inconv√©nients: <br><br>  - Le plus gros inconv√©nient est qu'il est <strong>beaucoup plus difficile de g√©rer tout cela</strong> .  Nous avons besoin d'un planificateur intelligent qui saura o√π il peut prendre cette instance, o√π il y aura une charge optimale. <br><br>  - <strong>Plus dur que les basculements</strong> . <br><br>  Par cons√©quent, nous en venons seulement √† cette d√©cision. <br><br><h3>  D√©couverte </h3><br>  Les clients doivent savoir comment se connecter √† la base de donn√©es souhait√©e, nous avons donc Discovery, qui devrait: <br><br><ol><li>  Informez le client tr√®s rapidement des changements de topologie.  Si nous changeons de ma√Ætre et d'esclave, les clients devraient en √™tre inform√©s presque instantan√©ment. <br></li><li>  La topologie ne doit pas d√©pendre de la topologie de r√©plication MySQL, car avec certaines op√©rations, nous changeons la topologie MySQL.  Par exemple, lorsque nous divisons, √† l'√©tape pr√©paratoire sur le ma√Ætre cible, o√π nous transf√©rerons une partie des fragments, certains des serveurs esclaves sont reconfigur√©s sur ce ma√Ætre cible.  Les clients n'ont pas besoin de le savoir. <br></li><li>  Il est important qu'il y ait atomicit√© des op√©rations et v√©rification de l'√©tat.  Il est impossible que deux serveurs diff√©rents de la m√™me base de donn√©es deviennent ma√Ætres au m√™me moment. <br></li></ol><br><h4>  Comment la d√©couverte s'est d√©velopp√©e </h4><br>  Au d√©but, tout √©tait simple: l'adresse de la base de donn√©es dans le code source de la configuration.  Lorsque nous avions besoin de mettre √† jour l'adresse, tout s'est d√©ploy√© tr√®s rapidement. <br><img src="https://habrastorage.org/webt/7d/yb/oo/7dyboo0h7eo4o9_9xp-n-6lzosy.jpeg"><br><br>  Malheureusement, cela ne fonctionne pas s'il y a beaucoup de serveurs. <br><img src="https://habrastorage.org/webt/3u/26/pf/3u26pfl_s796zdaoix-zdplb9du.jpeg"><br><br>  Ci-dessus est la toute premi√®re d√©couverte que nous avons.  Il y avait des scripts de base de donn√©es qui changeaient la plaque signal√©tique dans ConfigDB - c'√©tait une plaque signal√©tique MySQL distincte, et les clients √©coutaient d√©j√† cette base de donn√©es et en prenaient p√©riodiquement des donn√©es. <br><img src="https://habrastorage.org/webt/ml/qn/mh/mlqnmhmmteylazgl4itjokes_mu.jpeg"><br><br>  Le tableau est tr√®s simple, il y a une cat√©gorie de base de donn√©es, une cl√© de partition, un ma√Ætre / esclave de classe de base de donn√©es, un proxy et une adresse de base de donn√©es.  En fait, le client a demand√© une cat√©gorie, une classe DB, une cl√© de partition et l'adresse MySQL a √©t√© retourn√©e √† laquelle il pouvait d√©j√† √©tablir une connexion. <br><img src="https://habrastorage.org/webt/vb/ht/en/vbhteniyw4x7s56a1xwckuz268e.jpeg"><br><br>  D√®s qu'il y avait beaucoup de serveurs, Memcache a √©t√© ajout√© et les clients ont d√©j√† commenc√© √† communiquer avec lui. <br><br>  Mais ensuite nous l'avons retravaill√©.  Les scripts MySQL ont commenc√© √† communiquer via gRPC, via un client l√©ger avec un service appel√© RegisterService.  Lorsque certains changements se sont produits, RegisterService avait une file d'attente et il a compris comment appliquer ces changements.  RegisterService a enregistr√© les donn√©es dans AFS.  AFS est notre syst√®me interne bas√© sur ZooKeeper. <br><img src="https://habrastorage.org/webt/mi/lm/yz/milmyzvyvayuv2av8pah4neh9zq.jpeg"><br><br>  La deuxi√®me solution, qui n'est pas pr√©sent√©e ici, utilisait directement ZooKeeper, ce qui cr√©ait des probl√®mes car chaque fragment √©tait un n≈ìud dans ZooKeeper.  Par exemple, 100 000 clients se connectent √† ZooKeeper, s'ils sont morts subitement √† cause d'une sorte de bogue tous ensemble, alors 100 000 demandes √† ZooKeeper viendront imm√©diatement, ce qui le laissera simplement tomber et il ne pourra pas augmenter. <br><br>  Par cons√©quent, <strong>le syst√®me AFS a</strong> √©t√© d√©velopp√© <strong>, qui est utilis√© par l'ensemble de Dropbox</strong> .  En fait, il r√©sume le travail avec ZooKeeper pour tous les clients.  Le d√©mon AFS s'ex√©cute localement sur chaque serveur et fournit une API de fichier tr√®s simple du formulaire: cr√©er un fichier, supprimer un fichier, demander un fichier, recevoir une notification de modification de fichier et comparer et √©changer des op√©rations.  Autrement dit, vous pouvez essayer de remplacer le fichier par une version et si cette version a chang√© pendant la modification, l'op√©ration est annul√©e. <br><br>  Essentiellement, une telle abstraction sur ZooKeeper, dans laquelle il existe un algorithme d'interruption et de gigue local.  ZooKeeper ne plante plus sous charge.  Avec AFS, nous prenons des sauvegardes dans S3 et dans GIT, puis l'AFS local lui-m√™me informe les clients que les donn√©es ont chang√©. <br><img src="https://habrastorage.org/webt/ry/_0/wv/ry_0wvkyux23gicrlwmfceq0eoe.jpeg"><br><br>  Dans AFS, les donn√©es sont stock√©es sous forme de fichiers, c'est-√†-dire qu'il s'agit d'une API de syst√®me de fichiers.  Par exemple, ce qui pr√©c√®de est le fichier shard.slave_proxy - le plus grand, il prend environ 28 Ko, et lorsque nous changeons la cat√©gorie de la classe shard et slave_proxy, tous les clients qui s'abonnent √† ce fichier re√ßoivent une notification.  Ils ont relu ce fichier, qui contient toutes les informations n√©cessaires.  √Ä l'aide de la cl√© de partition, ils obtiennent une cat√©gorie et reconfigurent le pool de connexions √† la base de donn√©es. <br><br><a name="perations_databases"></a><h2>  Les op√©rations </h2><br>  Nous utilisons des op√©rations tr√®s simples: promotion, clone, sauvegardes / restauration. <br><img src="https://habrastorage.org/webt/q3/5c/df/q35cdfiso51bhhiitqja71qbysc.jpeg"><br><br>  <strong>Une op√©ration est une simple machine √† √©tats</strong> .  Lorsque nous entrons dans l'op√©ration, nous effectuons quelques v√©rifications, par exemple, spin-check, qui plusieurs fois par timeout v√©rifie si nous pouvons effectuer cette op√©ration.  Apr√®s cela, nous faisons une action pr√©paratoire qui n'affecte pas les syst√®mes externes.  Ensuite, l'op√©ration elle-m√™me. <br><br>  Toutes les √©tapes d'une op√©ration ont une <strong>√©tape de restauration</strong> (annuler).  En cas de probl√®me avec l'op√©ration, l'op√©ration tente de restaurer le syst√®me √† sa position d'origine.  Si tout va bien, le nettoyage a lieu et l'op√©ration est termin√©e. <br><br>  Nous avons une machine d'√©tat si simple pour toute op√©ration. <br><br><h4>  <strong>Promotion (changement de master)</strong> </h4><br>  Il s'agit d'une op√©ration tr√®s courante dans la base de donn√©es.  Il y avait des questions sur la fa√ßon de modifier sur un serveur ma√Ætre chaud qui fonctionne - il obtiendra un enjeu.  C'est juste que toutes ces op√©rations sont effectu√©es sur des serveurs esclaves, puis les changements d'esclaves avec les emplacements ma√Ætres.  Par cons√©quent, l' <strong>op√©ration de promotion est tr√®s fr√©quente</strong> . <br><img src="https://habrastorage.org/webt/xx/79/jv/xx79jvszxb9wjffwqf4ld_euofo.jpeg"><br><br>  Nous devons mettre √† jour le noyau - nous √©changeons, nous devons mettre √† jour la version de MySQL - nous mettons √† jour sur l'esclave, passons au ma√Ætre, y mettons √† jour. <br><img src="https://habrastorage.org/webt/wc/op/o9/wcopo9jlmz9aeoynpv-hbbseois.jpeg"><br><br>  Nous avons r√©alis√© une promotion tr√®s rapide.  Par exemple, <strong>pour quatre fragments, nous avons maintenant une promotion pour environ 10-15 s.</strong>  Le graphique ci-dessus montre qu'avec la disponibilit√© des promotions, il a souffert de 0,0003%. <br><br>  Mais la promotion normale n'est pas si int√©ressante, car ce sont des op√©rations ordinaires qui sont effectu√©es tous les jours.  Les basculements sont int√©ressants. <br><br><h4>  <strong>Basculement (remplacement d'un ma√Ætre cass√©)</strong> <br></h4><br>  Un basculement signifie que la base de donn√©es est morte. <br><br><ul><li>  Si le serveur est vraiment mort, c'est juste un cas id√©al. </li><li>  En fait, il arrive que les serveurs soient partiellement vivants. </li><li>  Parfois, le serveur meurt tr√®s lentement.  Les contr√¥leurs RAID, le syst√®me de disques √©chouent, certaines demandes renvoient des r√©ponses, mais certains flux sont bloqu√©s et ne renvoient pas de r√©ponses. </li><li>  Il arrive que le ma√Ætre soit simplement surcharg√© et ne r√©ponde pas √† notre bilan de sant√©.  Mais si nous faisons de la promotion, le nouveau ma√Ætre sera √©galement surcharg√©, et cela ne fera qu'empirer. </li></ul><br>  Le remplacement des serveurs ma√Ætres d√©c√©d√©s a lieu environ <strong>2-3 fois par jour</strong> , il s'agit d'un processus enti√®rement automatis√©, aucune intervention humaine n'est n√©cessaire.  La section critique prend environ 30 secondes, et elle contient un tas de v√©rifications suppl√©mentaires pour voir si le serveur est r√©ellement vivant, ou peut-√™tre qu'il est d√©j√† mort. <br><br>  Voici un exemple de sch√©ma de fonctionnement du faylover. <br><img src="https://habrastorage.org/webt/ks/5d/6o/ks5d6oovtnchnmlr2zlgpwvmt5g.jpeg"><br><br>  Dans la section s√©lectionn√©e, nous <strong>red√©marrons le serveur ma√Ætre</strong> .  Ceci est n√©cessaire car nous avons MySQL 5.6, et la r√©plication semi-synchrone n'est pas sans perte.  Par cons√©quent, les lectures fant√¥mes sont possibles, et nous avons besoin de ce ma√Ætre, m√™me s'il n'est pas mort, tuez le plus rapidement possible afin que les clients s'en d√©connectent.  Par cons√©quent, nous effectuons une r√©initialisation mat√©rielle via Ipmi - c'est la premi√®re op√©ration la plus importante que nous devons faire.  Dans la version MySQL 5.7, ce n'est pas si critique. <br><br>  <strong>Synchronisation de cluster.</strong>  Pourquoi avons-nous besoin d'une synchronisation de cluster? <br><img src="https://habrastorage.org/webt/gh/pa/go/ghpago_p12c1jittnig4rwhc1sg.jpeg"><br><br>  Si nous rappelons l'image pr√©c√©dente avec notre topologie, un serveur ma√Ætre a trois serveurs esclaves: deux dans un centre de donn√©es, un dans l'autre.  Avec la promotion, nous avons besoin que master soit dans le m√™me centre de donn√©es principal.  Mais parfois, lorsque des esclaves sont charg√©s, avec semisync, il arrive qu'un esclave semi-sync devienne un esclave dans un autre centre de donn√©es, car il n'est pas charg√©.  Par cons√©quent, nous devons d'abord synchroniser l'ensemble du cluster, puis faire d√©j√† la promotion sur l'esclave dans le centre de donn√©es dont nous avons besoin.  Cela se fait tr√®s simplement: <br><br><ul><li>  Nous arr√™tons tous les threads d'E / S sur tous les serveurs esclaves. </li><li>  Apr√®s cela, nous savons d√©j√† avec certitude que master est en "lecture seule", car semisync s'est d√©connect√© et personne d'autre ne peut rien y √©crire. </li><li>  Ensuite, nous s√©lectionnons l'esclave avec le plus grand ensemble GTID r√©cup√©r√© / ex√©cut√©, c'est-√†-dire avec la plus grande transaction qu'il a t√©l√©charg√©e ou d√©j√† appliqu√©e. </li><li>  Nous reconfigurons tous les serveurs esclaves sur cet esclave s√©lectionn√©, d√©marrons le thread d'E / S et ils sont synchronis√©s. </li><li>  Nous attendons qu'ils soient synchronis√©s, apr√®s quoi l'ensemble du cluster est synchronis√©.   ,     executed GTID set       . </li></ul><br>    ‚Äî <strong> </strong> .   <strong>promotion</strong> ,    : <br><img src="https://habrastorage.org/webt/bd/s-/b8/bds-b8fbieqxhtc4dy9ookntiri.jpeg"><br><br><ul><li>    slave    -,  ,   master,     promotion. </li><li>    slave-   master,   ,  ACLs,  ,  - proxy, , - . </li><li>      read_only = 0,   ,    master  ,   .        master     . </li><li>       - .     -    ,  ,   ,    , ,  proxy  . </li><li>     . </li></ul><br>   ,       rollback   ,   .       rollback  reboot.   ,    , ,  ‚Äî change master ‚Äî    master   . <br><br><h4> <strong></strong> </h4><br>  ‚Äî      .   ,    ,   ,    ,    . <br><br> <strong> </strong> <br><br> ‚óè   slave <br><br>   ,       slave-,      .   . <br><br> ‚óè       <br><br>     ,     ,      .             . <br><br> ‚óè       <br><br>  ,     ,      .          .      3  . <br><br><blockquote>    ,   ,   ,     : <br><br><ol><li>      .       1  40 . <br></li><li>            . <br></li></ol></blockquote><br>    ,     .   1   40 ,      ,      ,     . <br><br><h4> <strong></strong> </h4><br>    ,  .           .     4  . <br><img src="https://habrastorage.org/webt/bv/-k/_z/bv-k_znrl7zi2obmotthihjogyo.jpeg"><br><br><ul><li>    <strong> 24 </strong> .         HDFS,      . </li><li> <strong> 6 </strong>     unsharded databases,        Global DB.      , ,  ,     . </li><li> <strong> 3 </strong>          S3. </li><li> <strong> 3 </strong>     S3     . </li></ul><br><img src="https://habrastorage.org/webt/e1/yx/3s/e1yx3s-1ikyhxnuzeympjsvem14.jpeg"><br><br>       . ,    3 ,   HDFS     3 ,   6   S3.     . <br><br>  ,   . <br><img src="https://habrastorage.org/webt/sn/hz/1l/snhz1lmio2naq40wziys-jggaaw.jpeg"><br><br>         ,      ,   .       ,   ,    recovery  -   .  ,      ,  -       .      100  ,   . <br><br>     ,    ,    ,    ,   ,     ,  ,     .        . <br><br><h5>   </h5><br><img src="https://habrastorage.org/webt/4m/4b/kb/4m4bkboro5zunrljkwxbybj7jsi.jpeg"><br><br>     hot-,      Percona xtrabackup.     ‚Äîstream=xbstream,        ,   .     script-splitter,        ,      . <br><br> MySQL              2x.     3 , ,   ,    1 500 .     ,      ,    HDFS   S3. <br><br>        . <br><img src="https://habrastorage.org/webt/j3/il/jm/j3iljma8c0rekqweak5ngqvaxkk.jpeg"><br><br>  ,    ,    HDFS   S3,    , splitter       xtrabackup,      .   crash-recovery. <br><br>      hot   ,  crash-recovery    .         ,    .     binlog,      master. <br><br> <strong>   binlogs?</strong> <br><br>     binlog'.    master ,    4 ,   100 ,    HDFS. <br><br>      :   Binlog Backuper,         . ,  ,   binlog       HDFS. <br><img src="https://habrastorage.org/webt/on/o3/ce/ono3cesuissuuwuzcglcfautfjo.jpeg"><br><br> ,       4   ,    5 ,    ,    ,    .    HDFS   S3    . <br><br><h5>   </h5><br>      . <br><br>   : <br><br><ol><li>        ‚Äî  10 ,  45  ‚Äî   . <br></li><li>      ,       scheduler  multi instance      slave  master    . <br></li><li>    ‚Äî      ,   .  ,     ,    ,    ,     ,  ,    .  pt-table-checksum   ,      . <br></li></ol><br> <strong></strong> ,        : <br><br><ol><li>       1  10 ,      .    crash-recovery,     . <br></li><li>            . <br></li></ol><br><img src="https://habrastorage.org/webt/2m/bd/ar/2mbdarekbku4hsxygdufshzyhnm.jpeg"><br><br>     slave   -,     .    ,      .  Tout est tr√®s simple. <br><br><h4>  ++ </h4><br>     .       Hardware ,          (HDD)  10 ,       + crash recovery xtrabackup,      . ,         ,    . , ,     ,   ,   HDD  ,    HDFS  . <br><br><h4>  </h4><br>    ,  ‚Äî   : <br><br><ol><li>         ; <br></li><li>       . <br></li></ol><br>  ,     HDFS,       ,   ,       . <br><br><a name="automation"></a><h2>  Automatisation </h2><br>  ,  6 000      .         ,   ,     ‚Äî : <br><br><ul><li> Auto-replace; </li><li> DBManager; </li><li> Naoru, Wheelhouse </li></ul><br><h3> Auto-replace </h3><br>   ,   ,   ,    ,     ‚Äî ,     -.   ,   . <br><br> <strong>Availability ()</strong> ‚Äî         ,         .      ‚Äî   recovery  ,         . <br><img src="https://habrastorage.org/webt/-k/em/tr/-kemtrpvhgocinlvlmnsuj1eq-s.jpeg"><br><br>    MySQL  ,   heartbeat. Heartbeat ‚Äî   timestamp. <br><img src="https://habrastorage.org/webt/cn/r8/fn/cnr8fn-fpy_ddnthddjem3afr4o.jpeg"><br><br>    ,     , ,  master   read-write.          heartbeat. <br><br>    auto-replace ,    . <br><img src="https://habrastorage.org/webt/3u/r9/sx/3ur9sxfxf8dsxtgsgvjz3ublvye.jpeg"> <em>           <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> ,   91 .</em> <br><br> <strong>  ?</strong> <br><br><ul><li>   ,     heartbeat    . ,     .  heartbeat', ,    heartbeat'  30 . </li><li> Ensuite, voyez si leur nombre satisfait la valeur de seuil.  Sinon, alors quelque chose ne va pas avec le serveur - car il n'a pas envoy√© de battement de c≈ìur. </li><li>  Apr√®s cela, nous effectuons une v√©rification inverse au cas o√π - soudainement, ces deux services sont morts, quelque chose est avec le r√©seau, ou la base de donn√©es mondiale ne peut pas √©crire le rythme cardiaque pour une raison quelconque.  En contre-v√©rification, nous nous connectons √† une base de donn√©es cass√©e et v√©rifions son √©tat. </li><li>  Si tout le reste √©choue, nous regardons si la position principale progresse ou non, s'il y a des enregistrements dessus.  Si rien ne se passe, alors ce serveur ne fonctionne d√©finitivement pas. </li><li>  La derni√®re √©tape est en fait le remplacement automatique. </li></ul><br>  Le remplacement automatique est tr√®s conservateur, il ne veut jamais faire beaucoup d'op√©rations automatiques. <br><br><ol><li>  Tout d'abord, nous v√©rifions s'il y a eu des op√©rations de topologie r√©cemment?  Peut-√™tre que ce serveur vient d'√™tre ajout√© et que quelque chose ne fonctionne pas encore. </li><li>  Nous v√©rifions s'il y a √† tout moment des remplacements dans le m√™me cluster. </li><li>  V√©rifiez quelle limite de panne nous avons.  Si nous avons de nombreux probl√®mes en m√™me temps - 10, 20 - alors nous ne les r√©soudrons pas tous automatiquement, car nous pouvons perturber par inadvertance le fonctionnement de toutes les bases de donn√©es. </li></ol><br>  Par cons√©quent, nous <strong>ne r√©solvons qu'un seul probl√®me √† la fois</strong> . <br><br>  En cons√©quence, pour le serveur esclave, nous commen√ßons le clonage et le supprimons simplement de la topologie, et s'il est ma√Ætre, nous lan√ßons alors le feylover, la soi-disant promotion d'urgence. <br><br><h3>  DBManager </h3><br>  DBManager est un service de gestion de nos bases de donn√©es.  Il a: <br><br><ul><li>  planificateur de t√¢ches intelligent qui sait exactement quand commencer le travail; </li><li>  journaux et toutes les informations: qui, quand et quoi a √©t√© lanc√© - c'est la source de la v√©rit√©; </li><li>  point de synchronisation. </li></ul><br><img src="https://habrastorage.org/webt/xx/g6/pi/xxg6pitu-pau9ifyqivpa9wul3e.jpeg"><br><br>  DBManager est assez simple sur le plan architectural. <br><br><ul><li>  Il y a des clients, soit des DBA qui font quelque chose via l'interface Web, soit des scripts / services qui ont √©crit des DBA qui acc√®dent via gRPC. </li><li>  Il existe des syst√®mes externes comme Wheelhouse et Naoru, qui vont √† DBManager via gRPC. </li><li>  Il y a un planificateur qui comprend quelle op√©ration, quand et o√π il peut commencer. </li><li>  Il y a un travailleur tr√®s stupide qui, quand une op√©ration vient √† lui, la d√©marre, v√©rifie par PID.  Le travailleur peut red√©marrer, les processus ne sont pas interrompus.  Tous les employ√©s sont situ√©s aussi pr√®s que possible des serveurs sur lesquels les op√©rations ont lieu, de sorte que, par exemple, lors de la mise √† jour d'ACLS, nous n'avons pas besoin d'effectuer de nombreux allers-retours. </li><li>  Sur chaque h√¥te SQL, nous avons un DBAgent - c'est un serveur RPC.  Lorsque vous devez effectuer une op√©ration sur le serveur, nous envoyons une demande RPC. </li></ul><br>  Nous avons une interface Web pour DBManager, o√π vous pouvez voir les t√¢ches en cours d'ex√©cution, les journaux de ces t√¢ches, qui l'a d√©marr√©e et quand, quelles op√©rations ont √©t√© effectu√©es pour le serveur d'une base de donn√©es sp√©cifique, etc. <br><img src="https://habrastorage.org/webt/yj/tq/3m/yjtq3mrfimptba1kizre2bwie48.jpeg"><br><br>  Il existe une interface CLI assez simple o√π vous pouvez ex√©cuter des t√¢ches et √©galement les afficher dans des vues pratiques. <br><img src="https://habrastorage.org/webt/qi/rc/vp/qircvpuoutswvmcpnuca4wem9cu.jpeg"><br><br><h3>  Rem√©diations </h3><br>  Nous avons √©galement un syst√®me pour r√©pondre aux probl√®mes.  Quand quelque chose est cass√©, par exemple, le disque tombe en panne, ou un service ne fonctionne pas, <strong>Naoru</strong> fonctionne <strong>.</strong>  C'est le syst√®me qui fonctionne dans Dropbox, tout le monde l'utilise et il est sp√©cialement con√ßu pour ces petites t√¢ches.  J'ai parl√© de Naoru dans mon <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">rapport</a> en 2016. <br><br>  <strong>La timonerie est</strong> bas√©e sur une machine d' <strong>√©tat</strong> et est con√ßue pour de longs processus.  Par exemple, nous devons mettre √† jour le noyau sur tout MySQL sur l'ensemble de notre cluster de 6 000 machines.  Wheelhouse le fait clairement - met √† jour le serveur esclave, lance la promotion, l'esclave devient ma√Ætre, met √† jour le serveur ma√Ætre.  Cette op√©ration peut prendre un mois voire deux. <br><br><a name="monitoring"></a><h2>  Suivi </h2><br><img src="https://habrastorage.org/webt/vo/n_/ys/von_ysxtakb7m5ctiadwzewi3dw.jpeg"><br><br>  C'est tr√®s important. <br><br><blockquote>  Si vous ne surveillez pas le syst√®me, il est tr√®s probable qu'il ne fonctionne pas. </blockquote><br>  Nous surveillons tout dans MySQL - toutes les informations que nous pouvons obtenir de MySQL sont stock√©es quelque part, nous pouvons y acc√©der √† temps.  Nous stockons des informations sur InnoDb, des statistiques sur les demandes, sur les transactions, sur la dur√©e des transactions, le centile sur les longueurs des transactions, sur la r√©plication, sur le r√©seau - tout-tout-tout - un grand nombre de mesures. <br><br><h3>  Alerte </h3><br>  Nous avons 992 alertes configur√©es.  En fait, personne ne regarde les m√©triques, il me semble qu'il n'y a personne qui vient travailler et commence √† regarder le tableau des m√©triques, il y a des t√¢ches plus int√©ressantes. <br><img src="https://habrastorage.org/webt/q3/nj/sl/q3njslzahbxmh585uzeatrn4plm.jpeg"><br><br>  Par cons√©quent, il existe des alertes qui fonctionnent lorsque certaines valeurs de seuil sont atteintes.  <strong>Nous avons 992 alertes, quoi qu'il arrive, nous le saurons</strong> . <br><br><h3>  Incidents </h3><br><img src="https://habrastorage.org/webt/bi/ce/tp/bicetpzgstf2lggazas6t27ru10.jpeg"><br><br>  Nous avons PagerDuty - un service par lequel des alertes sont envoy√©es aux personnes responsables qui commencent √† agir. <br><img src="https://habrastorage.org/webt/6n/hl/sr/6nhlsrj28tloxq4xg5ld3a-mpvy.jpeg"><br><br>  Dans ce cas, une erreur s'est produite lors de la promotion d'urgence et, imm√©diatement apr√®s, une alerte a √©t√© enregistr√©e indiquant que le capitaine est tomb√©.  Apr√®s cela, l'officier de permanence a v√©rifi√© ce qui emp√™chait la promotion d'urgence et a effectu√© les op√©rations manuelles n√©cessaires. <br><br>  Nous analyserons certainement chaque incident qui s'est produit, pour chaque incident, nous avons une t√¢che dans le traqueur de t√¢ches.  M√™me si cet incident est un probl√®me dans nos alertes, nous cr√©ons √©galement une t√¢che, car si le probl√®me est dans la logique et les seuils d'alerte, alors ils doivent √™tre modifi√©s.  Les alertes ne devraient pas simplement g√¢cher la vie des gens.  Une alerte est toujours douloureuse, surtout √† 4 heures du matin. <br><br><a name="testing"></a><h2>  Test </h2><br>  Comme pour la surveillance, je suis s√ªr que tout le monde teste.  En plus des tests unitaires avec lesquels nous couvrons notre code, nous avons des tests d'int√©gration dans lesquels nous testons: <br><br><ul><li>  toutes les topologies que nous avons; </li><li>  toutes les op√©rations sur ces topologies. </li></ul><br>  Si nous avons des op√©rations de promotion, nous testons les op√©rations de promotion dans le test d'int√©gration.  Si nous avons du clonage, nous faisons du clonage pour toutes les topologies que nous avons. <br><br>  <strong>Exemple de topologie</strong> <br><img src="https://habrastorage.org/webt/dv/hh/va/dvhhvacdschtqr7s_ynecm0lyk0.jpeg"><br><br>  Nous avons des topologies pour toutes les occasions: 2 centres de donn√©es avec plusieurs instances, avec des fragments, pas de fragments, avec des clusters, un centre de donn√©es - g√©n√©ralement presque n'importe quelle topologie - m√™me ceux que nous n'utilisons pas, juste pour voir. <br><img src="https://habrastorage.org/webt/f-/oy/pl/f-oypluoyzosnphjl_aukp4vsks.jpeg"><br><br>  Dans ce fichier, nous avons juste les param√®tres, quels serveurs et avec ce que nous devons augmenter.  Par exemple, nous devons √©lever le ma√Ætre, et nous disons que nous devons le faire avec telle ou telle donn√©e d'instance, avec telle ou telle base de donn√©es sur tel ou tel port.  Presque tout va de pair avec Bazel, qui cr√©e une topologie √† partir de ces fichiers, d√©marre le serveur MySQL, puis le test d√©marre. <br><img src="https://habrastorage.org/webt/bg/zd/b_/bgzdb_dwnggh3kf8uv9f7mn9jpe.jpeg"><br><br>  Le test semble tr√®s simple: nous indiquons quelle topologie est utilis√©e.  Dans ce test, nous testons auto_replace. <br><br><ul><li>  Nous cr√©ons le service auto_replace, nous le d√©marrons. </li><li>  Nous tuons le ma√Ætre dans notre topologie, attendons un peu et constatons que l'esclave cible est devenu ma√Ætre.  Sinon, le test a √©chou√©. </li></ul><br><h3>  Les √©tapes </h3><br>  Les environnements de sc√®ne sont les m√™mes bases de donn√©es qu'en production, mais il n'y a pas de trafic utilisateur sur eux, mais il y a un trafic synth√©tique similaire √† la production via Percona Playback, sysbench et des syst√®mes similaires. <br><br>  Dans Percona Playback, nous enregistrons le trafic, puis nous le perdons dans l'environnement sc√©nique avec diff√©rentes intensit√©s, nous pouvons perdre 2-3 fois plus vite.  Autrement dit, il est artificiel, mais tr√®s proche de la charge r√©elle. <br><br>  Cela est n√©cessaire car dans les tests d'int√©gration, nous ne pouvons pas tester notre production.  Nous ne pouvons pas tester l'alerte ou le fait que les mesures fonctionnent.  Au stade des tests, nous testons les alertes, les m√©triques, les op√©rations, tuons p√©riodiquement les serveurs et voyons qu'ils sont collect√©s normalement. <br><br>  De plus, nous testons toute l'automatisation ensemble, car dans les tests d'int√©gration, tr√®s probablement, une partie du syst√®me est test√©e, et dans la mise en sc√®ne, tous les syst√®mes automatis√©s fonctionnent simultan√©ment.  Parfois, vous pensez que le syst√®me se comportera de cette fa√ßon et non autrement, mais il peut se comporter de mani√®re compl√®tement diff√©rente. <br><br><h3>  DRT (Disaster Recovery Testing) </h3><br>  Nous effectuons √©galement des tests en production - sur des bases r√©elles.  C'est ce qu'on appelle les tests de r√©cup√©ration apr√®s sinistre.  Pourquoi en avons-nous besoin? <br><br>  ‚óè Nous voulons tester nos garanties. <br><br>  Cela est fait par de nombreuses grandes entreprises.  Par exemple, Google a un service qui a fonctionn√© de mani√®re si stable - 100% du temps - que tous les services qui l'ont utilis√© ont d√©cid√© que ce service √©tait vraiment 100% stable et ne se bloquait jamais.  Par cons√©quent, Google a d√ª abandonner ce service expr√®s, afin que les utilisateurs prennent en compte cette possibilit√©. <br><br>  Nous sommes donc - nous avons une garantie que MySQL fonctionne - et parfois cela ne fonctionne pas!  Et nous avons la garantie que cela peut ne pas fonctionner pendant une certaine p√©riode de temps, les clients doivent en tenir compte.  De temps en temps, nous tuons le ma√Ætre de production, ou si nous voulons faire un faylover, nous tuons tous les esclaves pour voir comment se comporte la r√©plication semi-sync. <br><br>  ‚óè Les clients sont pr√©par√©s √† ces erreurs (remplacement et d√©c√®s du ma√Ætre) <br><br>  Pourquoi est-ce bien?  Nous avons eu un cas o√π lors de la promotion 4 fragments sur 1600, la disponibilit√© est tomb√©e √† 20%.  Il semble que quelque chose ne va pas, pour 4 √©clats de 1600, il devrait y avoir d'autres nombres.  Les basculements pour ce syst√®me √©taient rares, environ une fois par mois, et tout le monde a d√©cid√©: "Eh bien, c'est un basculement, √ßa arrive." <br><br>  √Ä un moment donn√©, lorsque nous sommes pass√©s √† un nouveau syst√®me, une personne a d√©cid√© d'optimiser ces deux services d'enregistrement du rythme cardiaque et de les combiner en un seul.  Ce service a fait autre chose et, √† la fin, il est mort et les battements de c≈ìur ont cess√© d'enregistrer.  Il se trouve que pour ce client, nous avions 8 faylovers par jour.  Tout reposait - 20% de disponibilit√©. <br><br>  Il s'est av√©r√© que chez ce client, la dur√©e de vie est de 6 heures.  En cons√©quence, d√®s que le ma√Ætre est d√©c√©d√©, toutes les connexions ont √©t√© maintenues pendant 6 heures suppl√©mentaires.  Le pool n'a pas pu continuer √† fonctionner - ses connexions sont conserv√©es, il est limit√© et ne fonctionne pas.  Il a √©t√© r√©par√©. <br><br>  Nous faisons √† nouveau le feylover - non plus 20%, mais encore beaucoup.  Quelque chose ne va toujours pas.  Il s'est av√©r√© qu'un bug dans l'impl√©mentation du pool.  √Ä la demande, le pool s'est tourn√© vers de nombreux fragments, puis a connect√© tout cela.  Si certains √©clats √©taient f√©briles, une condition de race s'est produite dans le code Go, et l'ensemble du pool √©tait obstru√©.  Tous ces fragments ne pouvaient plus fonctionner. <br><br>  Les tests de r√©cup√©ration apr√®s sinistre sont tr√®s utiles, car les clients doivent √™tre pr√©par√©s √† ces erreurs, ils doivent v√©rifier leur code. <br><br>  ‚óè De plus, les tests de reprise apr√®s sinistre sont bons car ils ont lieu pendant les heures de bureau et tout est en place, moins de stress, les gens savent ce qui va se passer maintenant.  Cela ne se produit pas la nuit et c'est super. <br><br><h2>  Conclusion </h2><br>  1. Tout doit √™tre automatis√©, ne mettez jamais la main dessus. <br>  Chaque fois que quelqu'un monte dans le syst√®me avec nos mains, tout meurt et se brise dans notre syst√®me - √† chaque fois!  - m√™me sur des op√©rations simples.  Par exemple, un esclave est d√©c√©d√©, une personne a d√ª en ajouter une seconde, mais a d√©cid√© de retirer l'esclave mort avec ses mains de la topologie.  Cependant, au lieu du d√©funt, il a copi√© la commande en direct - le ma√Ætre a √©t√© laiss√© sans esclave du tout.  Ces op√©rations ne doivent pas √™tre effectu√©es manuellement. <br><br>  2. Les tests doivent √™tre continus et automatis√©s (et en production). <br>  Votre syst√®me √©volue, votre infrastructure √©volue.  Si vous avez v√©rifi√© une fois et que cela a sembl√© fonctionner, cela ne signifie pas que cela fonctionnera demain.  Par cons√©quent, vous devez constamment effectuer des tests automatis√©s chaque jour, y compris en production. <br><br>  3. Assurez-vous de poss√©der des clients (biblioth√®ques). <br>  Les utilisateurs peuvent ne pas savoir comment fonctionnent les bases de donn√©es.  Ils peuvent ne pas comprendre pourquoi des d√©lais d'attente sont n√©cessaires, garder en vie.  Par cons√©quent, il vaut mieux poss√©der ces clients - vous serez plus calme. <br><br>  4. Il est n√©cessaire de d√©terminer vos principes de construction du syst√®me et vos garanties, et de toujours vous y conformer. <br><br>  Ainsi, vous pouvez prendre en charge 6 000 serveurs de bases de donn√©es. <br><br><div class="spoiler">  <b class="spoiler_title">Dans les questions qui suivent le rapport, et en particulier les r√©ponses, il y a aussi beaucoup d'informations utiles.</b> <div class="spoiler_text"><h2>  Q &amp; A <br></h2><br><blockquote>  - Que se passera-t-il s'il y a un d√©s√©quilibre dans la charge des fragments - certaines m√©ta-informations sur certains fichiers se sont av√©r√©es plus populaires?  Est-il possible de diffuser ce fragment ou la charge sur les fragments ne diff√®re nulle part par ordre de grandeur? </blockquote><br>  Elle ne diff√®re pas par ordre de grandeur.  Il est distribu√© presque normalement.  Nous avons la limitation, c'est-√†-dire que nous ne pouvons pas surcharger le fragment en fait, nous limitons au niveau du client.  En g√©n√©ral, il arrive que certaines √©toiles t√©l√©chargent une photo et l'√©clat explose pratiquement.  Ensuite, nous interdisons ce lien <br><br><blockquote>  - Vous avez dit que vous aviez 992 alertes.  Pourriez-vous pr√©ciser ce que c'est - est-il sorti de la bo√Æte ou est-il cr√©√©?  S'il est cr√©√©, s'agit-il d'un travail manuel ou de quelque chose comme l'apprentissage automatique? </blockquote><br>  Tout est cr√©√© manuellement.  Nous avons notre propre syst√®me interne appel√© Vortex, o√π les mesures sont stock√©es, les alertes y sont prises en charge.  Il existe un fichier yaml qui indique qu'il existe une condition, par exemple, que les sauvegardes doivent √™tre ex√©cut√©es tous les jours, et si cette condition est remplie, l'alerte ne fonctionne pas.  S'il n'est pas ex√©cut√©, une alerte arrive. <br><br>  Il s'agit de notre d√©veloppement interne, car peu de personnes peuvent stocker autant de m√©triques que nous en avons besoin. <br><br><blockquote>  - Quelle doit √™tre la force des nerfs pour faire du DRT?  Vous avez chut√©, CODERED, ne monte pas, avec chaque minute de panique de plus. </blockquote><br>  En g√©n√©ral, travailler dans des bases de donn√©es est vraiment p√©nible.  Si la base de donn√©es se bloque, le service ne fonctionne pas, l'ensemble de Dropbox ne fonctionne pas.  C'est vraiment une douleur.  DRT est utile car c'est une montre d'affaires.  Autrement dit, je suis pr√™t, je suis assis √† mon bureau, j'ai pris un caf√©, je suis frais, je suis pr√™t √† tout. <br><br>  Pire encore quand cela se produit √† 4 heures du matin, et ce n'est pas DRT.  Par exemple, le dernier √©chec majeur que nous avons eu r√©cemment.  Lors de l'injection d'un nouveau syst√®me, nous avons oubli√© de d√©finir le score OOM pour notre MySQL.  Il y avait un autre service qui lisait binlog.  √Ä un moment donn√©, notre op√©rateur est manuel - encore une fois manuellement!  - ex√©cute la commande pour supprimer certaines informations dans la table de total de contr√¥le Percona.  Juste une simple suppression, une op√©ration simple, mais cette op√©ration a g√©n√©r√© un √©norme binlog.  Le service a lu ce binlog dans la m√©moire, OOM Killer est venu et pense qui tuer?  Et nous avons oubli√© de d√©finir le score OOM, et cela tue MySQL! <br><br>  Nous avons 40 ma√Ætres qui meurent √† 4 heures du matin.  Quand 40 ma√Ætres meurent, c'est vraiment tr√®s effrayant et dangereux.  DRT n'est ni effrayant ni dangereux.  Nous restons allong√©s pendant environ une heure. <br><br>  Soit dit en passant, DRT est un bon moyen de r√©p√©ter de tels moments afin que nous sachions exactement quelle s√©quence d'actions est n√©cessaire si quelque chose se brise en masse. <br><br><blockquote>  - Je voudrais en savoir plus sur la commutation ma√Ætre-ma√Ætre.  Tout d'abord, pourquoi un cluster n'est-il pas utilis√©, par exemple?  Un cluster de base de donn√©es, c'est-√†-dire non pas un ma√Ætre-esclave avec commutation, mais une application ma√Ætre-ma√Ætre, de sorte que si l'on tombe, alors ce n'est pas effrayant. </blockquote><br>  Voulez-vous dire quelque chose comme la r√©plication de groupe, l'amas de gal√®res, etc.?  Il me semble que la candidature de groupe n'est pas encore pr√™te pour la vie.  Malheureusement, nous n'avons pas encore essay√© Galera.  C'est formidable lorsqu'un faylover est √† l'int√©rieur de votre protocole, mais, malheureusement, ils ont tellement d'autres probl√®mes, et ce n'est pas si facile de passer √† cette solution. <br><br><blockquote>  - Il semble que dans MySQL 8, il y ait quelque chose comme un cluster InnoDb.  N'a pas essay√©? </blockquote><br>  Nous en avons encore 5,6.  Je ne sais pas quand nous passerons √† 8. Peut-√™tre que nous essaierons. <br><br><blockquote>  - Dans ce cas, si vous avez un gros ma√Ætre, lors du passage de l'un √† l'autre, il s'av√®re que la file d'attente s'accumule sur les serveurs esclaves avec une charge √©lev√©e.  Si le ma√Ætre est √©teint, est-il n√©cessaire que la file d'attente atteigne, de sorte que l'esclave passe en mode ma√Ætre - ou est-ce fait d'une mani√®re ou d'une autre? </blockquote><br>  La charge sur le ma√Ætre est r√©gul√©e par semi-synchronisation.  Semisync limite l'enregistrement ma√Ætre aux performances du serveur esclave.  Bien s√ªr, il se peut que la transaction soit arriv√©e, la semi-synchronisation a fonctionn√©, mais les esclaves ont perdu cette transaction pendant tr√®s longtemps.  Vous devez ensuite attendre jusqu'√† ce que l'esclave perde cette transaction jusqu'√† la fin. <br><br><blockquote>  - Mais alors de nouvelles donn√©es viendront √† ma√Ætriser, et ce sera n√©cessaire ... </blockquote><br>  Lorsque nous d√©marrons le processus de promotion, nous d√©sactivons les E / S.  Apr√®s cela, master ne peut rien √©crire car la semi-synchronisation est r√©pliqu√©e.  La lecture fant√¥me peut arriver, malheureusement, mais c'est d√©j√† un autre probl√®me. <br><br><blockquote>  - Ce sont toutes de belles machines √† √©tats - sur quoi sont √©crits les scripts et √† quel point est-il difficile d'ajouter une nouvelle √©tape?  Que faut-il faire √† la personne qui √©crit ce syst√®me? </blockquote><br>  Tous les scripts sont √©crits en Python, tous les services sont √©crits en Go.  Telle est notre politique.  Changer la logique est facile - juste dans le code Python qui g√©n√®re le diagramme d'√©tat. <br><br><blockquote>  - Et vous pouvez en savoir plus sur les tests.  Comment sont √©crits les tests, comment d√©ploient-ils les n≈ìuds dans une machine virtuelle - ces conteneurs sont-ils? </blockquote><br>  Oui  Nous allons tester avec l'aide de Bazel.  Il existe des fichiers de configuration (json) et Bazel r√©cup√®re un script qui cr√©e la topologie pour notre test √† l'aide de ce fichier de configuration.  Diff√©rentes topologies y sont d√©crites. <br><br>  Tout cela fonctionne pour nous dans les conteneurs Docker: soit cela fonctionne en CI ou sur Devbox.  Nous avons un syst√®me Devbox.  Nous d√©veloppons tous sur un serveur distant, et cela peut fonctionner, par exemple.  L√†, il s'ex√©cute √©galement √† l'int√©rieur de Bazel, √† l'int√©rieur d'un conteneur docker ou dans le bac √† sable Bazel.  Bazel est tr√®s compliqu√© mais amusant. <br><br><blockquote>  - Lorsque vous avez cr√©√© 4 instances sur un serveur, avez-vous perdu en efficacit√© m√©moire? </blockquote><br>  Chaque instance est devenue plus petite.  Par cons√©quent, moins MySQL fonctionne avec de la m√©moire, plus il est facile √† vivre.  Tout syst√®me est plus facile √† utiliser avec une petite quantit√© de m√©moire.  En ce lieu, nous n'avons rien perdu.  Nous avons les groupes C les plus simples qui limitent ces instances de la m√©moire. <br><br><blockquote>  - Si vous avez 6 000 serveurs qui stockent des bases de donn√©es, pouvez-vous nommer combien de milliards de p√©taoctets sont stock√©s dans vos fichiers? </blockquote><br>  Ce sont des dizaines d'exaoctets, nous avons vers√© des donn√©es d'Amazon pendant un an. <br><br><blockquote>  - Il s'av√®re qu'au d√©but, vous aviez 8 serveurs, 200 fragments sur eux, puis 400 serveurs avec 4 fragments chacun.  Vous avez 1600 fragments - est-ce une sorte de valeur cod√©e en dur?  Pouvez-vous plus jamais le refaire?  Cela fera-t-il mal si vous avez besoin, par exemple, de 3200 √©clats? </blockquote><br>  Oui, c'√©tait √† l'origine 1600. Cela a √©t√© fait il y a un peu moins de 10 ans, et nous vivons toujours.  Mais nous avons encore 4 √©clats - 4 fois nous pouvons encore augmenter l'espace. <br><br><blockquote>  - Comment les serveurs meurent, principalement pour quelles raisons?  Que se passe-t-il plus souvent, moins souvent, et c'est particuli√®rement int√©ressant, des carapteurs de bloc spontan√©s se produisent-ils? </blockquote><br>  La chose la plus importante est que les disques s'envolent.  Nous avons RAID 0 - le disque est tomb√© en panne, le ma√Ætre est mort.  C'est le principal probl√®me, mais il nous est plus facile de remplacer ce serveur.  Google est plus facile de remplacer le centre de donn√©es, nous avons toujours un serveur.  Nous n'avons presque jamais eu de somme de contr√¥le de la corruption.  Pour √™tre honn√™te, je ne me souviens pas quand c'√©tait la derni√®re fois.  Nous mettons souvent √† jour l'assistant.  Notre dur√©e de vie pour un ma√Ætre est limit√©e √† 60 jours.  Il ne peut pas vivre plus longtemps, apr√®s quoi nous le rempla√ßons par un nouveau serveur, car pour une raison quelconque, quelque chose s'accumule constamment dans MySQL, et apr√®s 60 jours, nous voyons que des probl√®mes commencent √† se produire.  Peut-√™tre pas dans MySQL, peut-√™tre sous Linux. <br><br>  Nous ne savons pas quel est ce probl√®me et nous ne voulons pas y faire face.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nous venons de limiter le d√©lai √† 60 jours et de mettre √† jour l'int√©gralit√© de la pile. </font><font style="vertical-align: inherit;">Pas besoin de s'en tenir √† un seul ma√Ætre.</font></font><br><br><blockquote> ‚Äî  ,    6        . ,   JPEG   ,     JPEG,  ,      ?  , ,      -   ?    ‚Äî      ,       ? </blockquote><br>     ,  .   ‚Äî  Dropbox    . <br><br><blockquote> ‚Äî      ?         ?     , ,  - ,    , ? ,   10   . ,  7     ,    6    ,    .    ? </blockquote><br>   Dropbox  - ,       .   .  ,    ,       ,   -  . <br><br>  ,    .  ,  ,      ,      .        - ,     6 ,   ,     ,    ,    . <br></div></div><br><blockquote>       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">facebook</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">youtube-</a> ‚Äî          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Highload++ 2018</a> .      , <strong> 1 </strong>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">  </a> . <br></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr417315/">https://habr.com/ru/post/fr417315/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr417303/index.html">Hack pour prendre en charge les boutons du casque Windows Android</a></li>
<li><a href="../fr417305/index.html">Ultima Online: un look backstage</a></li>
<li><a href="../fr417307/index.html">Glaucome - vous n'avez pas entendu parler d'elle? Rencontrez le tueur en s√©rie de la vision silencieuse</a></li>
<li><a href="../fr417309/index.html">Heureusement, le gestionnaire ITSM: comment la profession du futur aide √† √©largir les fronti√®res du Service Desk</a></li>
<li><a href="../fr417311/index.html">Cr√©ation d'un bot pour participer √† l'IA mini cup 2018 bas√© sur un r√©seau de neurones r√©current</a></li>
<li><a href="../fr417317/index.html">√Ä Highload ++ 2018 √† toute vitesse</a></li>
<li><a href="../fr417319/index.html">Syst√®mes dans le bo√Ætier ou ce qui se trouve r√©ellement sous le capot du microprocesseur</a></li>
<li><a href="../fr417321/index.html">Comment recherchons-nous des enseignants de cours en ligne parmi les d√©veloppeurs?</a></li>
<li><a href="../fr417323/index.html">Probl√®mes pour garantir l'accessibilit√© √† 100% du projet</a></li>
<li><a href="../fr417325/index.html">Journ√©e portes ouvertes Netrology, th√®me Science des donn√©es</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>