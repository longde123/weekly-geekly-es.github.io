<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòî üõåüèª ü•à Gu√≠a simple de destilaci√≥n BERT ü§¨ üöù ü¶è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Si est√° interesado en el aprendizaje autom√°tico, probablemente escuch√≥ sobre BERT y los transformadores. 


 BERT es un modelo de lenguaje de Google, ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Gu√≠a simple de destilaci√≥n BERT</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/avito/blog/485290/"><p>  Si est√° interesado en el aprendizaje autom√°tico, probablemente escuch√≥ sobre BERT y los transformadores. </p><br><p>  BERT es un modelo de lenguaje de Google, que muestra resultados de vanguardia por un amplio margen en una serie de tareas.  BERT, y en general los transformadores, se han convertido en un paso completamente nuevo en el desarrollo de algoritmos de procesamiento de lenguaje natural (PNL).  El art√≠culo sobre ellos y las "posiciones" para varios puntos de referencia se pueden encontrar <a href="https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional">en el sitio web de Papers With Code</a> . </p><br><p>  Hay un problema con BERT: es problem√°tico de usar en sistemas industriales.  BERT-base contiene 110M par√°metros, BERT-large - 340M.  Debido a la gran cantidad de par√°metros, este modelo es dif√≠cil de descargar en dispositivos con recursos limitados, como tel√©fonos m√≥viles.  Adem√°s, el largo tiempo de inferencia hace que este modelo sea inadecuado cuando la velocidad de respuesta es cr√≠tica.  Por lo tanto, encontrar formas de acelerar el BERT es un tema muy candente. </p><br><p>  En Avito, a menudo tenemos que resolver problemas de clasificaci√≥n de texto.  Esta es una tarea de aprendizaje autom√°tico aplicada t√≠pica que ha sido bien estudiada.  Pero siempre existe la tentaci√≥n de probar algo nuevo.  Este art√≠culo naci√≥ de un intento de aplicar BERT en las tareas cotidianas de aprendizaje autom√°tico.  En √©l, mostrar√© c√≥mo puede mejorar significativamente la calidad de un modelo existente usando BERT sin agregar nuevos datos o complicar el modelo. </p><br><p><img src="https://habrastorage.org/webt/c_/po/z2/c_poz2e3dkggx7ekt3gn_9wva3g.png"></p><a name="habracut"></a><br><h2 id="knowledge-distillation-kak-metod-uskoreniya-neyronnyh-setey">  La destilaci√≥n del conocimiento como m√©todo para acelerar las redes neuronales </h2><br><p>  Hay varias formas de acelerar / aligerar las redes neuronales.  La revisi√≥n m√°s detallada que he conocido se publica <a href="https://blog.inten.to/speeding-up-bert-5528e18bb4ea">en el blog de Intento en el Medio</a> . </p><br><p>  Los m√©todos se pueden dividir aproximadamente en tres grupos: </p><br><ol><li>  Cambio de arquitectura de red. </li><li>  Modelo de compresi√≥n (cuantizaci√≥n, poda). </li><li>  Destilaci√≥n del conocimiento. </li></ol><br><p>  Si los dos primeros m√©todos son relativamente conocidos y comprensibles, entonces el tercero es menos com√∫n.  Por primera vez, la idea de la destilaci√≥n fue propuesta por Rich Caruana <a href="https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf">en el art√≠culo "Modelo de compresi√≥n"</a> .  Su esencia es simple: puede entrenar un modelo liviano que imitar√° el comportamiento de un modelo de maestro o incluso un conjunto de modelos.  En nuestro caso, el maestro ser√° BERT y el alumno ser√° cualquier modelo ligero. </p><br><h2 id="zadacha">  Desaf√≠o </h2><br><p>  Analicemos la destilaci√≥n utilizando la clasificaci√≥n binaria como ejemplo.  Tome el conjunto de datos SST-2 abierto del conjunto est√°ndar de tareas que prueban los modelos para PNL. </p><br><p>  Este conjunto de datos es una colecci√≥n de rese√±as de pel√≠culas con IMDb desglosadas por color emocional, positivo o negativo.  La m√©trica en este conjunto de datos es la precisi√≥n. </p><br><h2 id="obuchenie-bert-based-modeli-ili-uchitelya">  Capacitaci√≥n de modelos basados ‚Äã‚Äãen BERT o "maestros" </h2><br><p>  En primer lugar, debe entrenar el modelo "grande" basado en BERT, que se convertir√° en un maestro.  La forma m√°s f√°cil de hacer esto es tomar las incrustaciones de BERT y entrenar el clasificador encima de ellas, agregando una capa a la red. </p><br><p> Gracias a <a href="https://github.com/huggingface/transformers">la biblioteca de transformadores,</a> esto es bastante f√°cil de hacer, porque hay una clase de modelo BertForSequenceClassification ya hecha.  En mi opini√≥n, <a href="https-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca">Towards Data Science public√≥ el</a> tutorial m√°s detallado y comprensible para ense√±ar este modelo. </p><br><p>  Imaginemos que tenemos un modelo entrenado BertForSequenceClassification.  En nuestro caso, num_labels = 2, ya que tenemos una clasificaci√≥n binaria.  Utilizaremos este modelo como un "maestro". </p><br><h2 id="obuchenie-uchenika">  Aprendizaje "estudiante" </h2><br><p>  Puede tomar cualquier arquitectura como estudiante: una red neuronal, un modelo lineal, un √°rbol de decisi√≥n.  Intentemos ense√±ar BiLSTM para una mejor visualizaci√≥n.  Para comenzar, ense√±aremos BiLSTM sin BERT. </p><br><p>  Para enviar texto a la entrada de una red neuronal, debe presentarlo como un vector.  Una de las formas m√°s f√°ciles es asignar cada palabra a su √≠ndice en el diccionario.  El diccionario constar√° de las palabras m√°s populares de n en nuestro conjunto de datos m√°s dos palabras de servicio: "pad" - "palabra ficticia" para que todas las secuencias tengan la misma longitud, y "unk" - para palabras fuera del diccionario.  Construiremos el diccionario usando el conjunto est√°ndar de herramientas de torchtext.  Para simplificar, no utilic√© incrustaciones de palabras pre-entrenadas. <br></p><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torchtext <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> data <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_vocab</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X)</span></span></span><span class="hljs-function">:</span></span> X_split = [t.split() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> X] text_field = data.Field() text_field.build_vocab(X_split, max_size=<span class="hljs-number"><span class="hljs-number">10000</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text_field <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">pad</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(seq, max_len)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(seq) &lt; max_len: seq = seq + [<span class="hljs-string"><span class="hljs-string">'&lt;pad&gt;'</span></span>] * (max_len - len(seq)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> seq[<span class="hljs-number"><span class="hljs-number">0</span></span>:max_len] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">to_indexes</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(vocab, words)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [vocab.stoi[w] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> w <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">to_dataset</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x, y, y_real)</span></span></span><span class="hljs-function">:</span></span> torch_x = torch.tensor(x, dtype=torch.long) torch_y = torch.tensor(y, dtype=torch.float) torch_real_y = torch.tensor(y_real, dtype=torch.long) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> TensorDataset(torch_x, torch_y, torch_real_y)</code> </pre> <br><h3 id="model-bilstm">  Modelo BiLSTM </h3><br><p>  El c√≥digo para el modelo se ver√° as√≠: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> nn <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.autograd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Variable <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SimpleLSTM</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, batch_size, device=None)</span></span></span><span class="hljs-function">:</span></span> super(SimpleLSTM, self).__init__() self.batch_size = batch_size self.hidden_dim = hidden_dim self.n_layers = n_layers self.embedding = nn.Embedding(input_dim, embedding_dim) self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout) self.fc = nn.Linear(hidden_dim * <span class="hljs-number"><span class="hljs-number">2</span></span>, output_dim) self.dropout = nn.Dropout(dropout) self.device = self.init_device(device) self.hidden = self.init_hidden() @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">init_device</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(device)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> device <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> torch.device(<span class="hljs-string"><span class="hljs-string">'cuda'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> device <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">init_hidden</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (Variable(torch.zeros(<span class="hljs-number"><span class="hljs-number">2</span></span> * self.n_layers, self.batch_size, self.hidden_dim).to(self.device)), Variable(torch.zeros(<span class="hljs-number"><span class="hljs-number">2</span></span> * self.n_layers, self.batch_size, self.hidden_dim).to(self.device))) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, text, text_lengths=None)</span></span></span><span class="hljs-function">:</span></span> self.hidden = self.init_hidden() x = self.embedding(text) x, self.hidden = self.rnn(x, self.hidden) hidden, cell = self.hidden hidden = self.dropout(torch.cat((hidden[<span class="hljs-number"><span class="hljs-number">-2</span></span>, :, :], hidden[<span class="hljs-number"><span class="hljs-number">-1</span></span>, :, :]), dim=<span class="hljs-number"><span class="hljs-number">1</span></span>)) x = self.fc(hidden) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> x</code> </pre> <br><h3 id="obuchenie">  Entrenamiento </h3><br><p>  Para este modelo, la dimensi√≥n del vector de salida ser√° (batch_size, output_dim).  En el entrenamiento, usaremos el logloss habitual.  PyTorch tiene una clase BCEWithLogitsLoss que combina entrop√≠a sigmoidea y cruzada.  Lo que necesitas </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, output, bert_prob, real_label)</span></span></span><span class="hljs-function">:</span></span> criterion = torch.nn.BCEWithLogitsLoss() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> criterion(output, real_label.float())</code> </pre> <br><p>  C√≥digo para una era de aprendizaje: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_optimizer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model)</span></span></span><span class="hljs-function">:</span></span> optimizer = torch.optim.Adam(model.parameters()) scheduler = torch.optim.lr_scheduler.StepLR(optimizer, <span class="hljs-number"><span class="hljs-number">2</span></span>, gamma=<span class="hljs-number"><span class="hljs-number">0.9</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> optimizer, scheduler <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">epoch_train_func</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model, dataset, loss_func, batch_size)</span></span></span><span class="hljs-function">:</span></span> train_loss = <span class="hljs-number"><span class="hljs-number">0</span></span> train_sampler = RandomSampler(dataset) data_loader = DataLoader(dataset, sampler=train_sampler, batch_size=batch_size, drop_last=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) model.train() optimizer, scheduler = get_optimizer(model) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, (text, bert_prob, real_label) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(tqdm(data_loader, desc=<span class="hljs-string"><span class="hljs-string">'Train'</span></span>)): text, bert_prob, real_label = to_device(text, bert_prob, real_label) model.zero_grad() output = model(text.t(), <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>).squeeze(<span class="hljs-number"><span class="hljs-number">1</span></span>) loss = loss_func(output, bert_prob, real_label) loss.backward() optimizer.step() train_loss += loss.item() scheduler.step() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> train_loss / len(data_loader)</code> </pre> <br><p>  C√≥digo de verificaci√≥n despu√©s de la era: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">epoch_evaluate_func</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model, eval_dataset, loss_func, batch_size)</span></span></span><span class="hljs-function">:</span></span> eval_sampler = SequentialSampler(eval_dataset) data_loader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=batch_size, drop_last=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) eval_loss = <span class="hljs-number"><span class="hljs-number">0.0</span></span> model.eval() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, (text, bert_prob, real_label) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(tqdm(data_loader, desc=<span class="hljs-string"><span class="hljs-string">'Val'</span></span>)): text, bert_prob, real_label = to_device(text, bert_prob, real_label) output = model(text.t(), <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>).squeeze(<span class="hljs-number"><span class="hljs-number">1</span></span>) loss = loss_func(output, bert_prob, real_label) eval_loss += loss.item() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> eval_loss / len(data_loader)</code> </pre> <br><p>  Si todo esto se combina, obtenemos el siguiente c√≥digo para entrenar el modelo: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.utils.data <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> (TensorDataset, random_split, RandomSampler, DataLoader, SequentialSampler) <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torchtext <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> data <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tqdm <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">device</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> torch.device(<span class="hljs-string"><span class="hljs-string">"cuda"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> torch.cuda.is_available() <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">"cpu"</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">to_device</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(text, bert_prob, real_label)</span></span></span><span class="hljs-function">:</span></span> text = text.to(device()) bert_prob = bert_prob.to(device()) real_label = real_label.to(device()) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text, bert_prob, real_label <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">LSTMBaseline</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> vocab_name = <span class="hljs-string"><span class="hljs-string">'text_vocab.pt'</span></span> weights_name = <span class="hljs-string"><span class="hljs-string">'simple_lstm.pt'</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, settings)</span></span></span><span class="hljs-function">:</span></span> self.settings = settings self.criterion = torch.nn.BCEWithLogitsLoss().to(device()) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, output, bert_prob, real_label)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.criterion(output, real_label.float()) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, text_field)</span></span></span><span class="hljs-function">:</span></span> model = SimpleLSTM( input_dim=len(text_field.vocab), embedding_dim=<span class="hljs-number"><span class="hljs-number">64</span></span>, hidden_dim=<span class="hljs-number"><span class="hljs-number">128</span></span>, output_dim=<span class="hljs-number"><span class="hljs-number">1</span></span>, n_layers=<span class="hljs-number"><span class="hljs-number">1</span></span>, bidirectional=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, dropout=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, batch_size=self.settings[<span class="hljs-string"><span class="hljs-string">'train_batch_size'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, X, y, y_real, output_dir)</span></span></span><span class="hljs-function">:</span></span> max_len = self.settings[<span class="hljs-string"><span class="hljs-string">'max_seq_length'</span></span>] text_field = get_vocab(X) X_split = [t.split() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> X] X_pad = [pad(s, max_len) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tqdm(X_split, desc=<span class="hljs-string"><span class="hljs-string">'pad'</span></span>)] X_index = [to_indexes(text_field.vocab, s) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tqdm(X_pad, desc=<span class="hljs-string"><span class="hljs-string">'to index'</span></span>)] dataset = to_dataset(X_index, y, y_real) val_len = int(len(dataset) * <span class="hljs-number"><span class="hljs-number">0.1</span></span>) train_dataset, val_dataset = random_split(dataset, (len(dataset) - val_len, val_len)) model = self.model(text_field) model.to(device()) self.full_train(model, train_dataset, val_dataset, output_dir) torch.save(text_field, os.path.join(output_dir, self.vocab_name)) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">full_train</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, model, train_dataset, val_dataset, output_dir)</span></span></span><span class="hljs-function">:</span></span> train_settings = self.settings num_train_epochs = train_settings[<span class="hljs-string"><span class="hljs-string">'num_train_epochs'</span></span>] best_eval_loss = <span class="hljs-number"><span class="hljs-number">100000</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> epoch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(num_train_epochs): train_loss = epoch_train_func(model, train_dataset, self.loss, self.settings[<span class="hljs-string"><span class="hljs-string">'train_batch_size'</span></span>]) eval_loss = epoch_evaluate_func(model, val_dataset, self.loss, self.settings[<span class="hljs-string"><span class="hljs-string">'eval_batch_size'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> eval_loss &lt; best_eval_loss: best_eval_loss = eval_loss torch.save(model.state_dict(), os.path.join(output_dir, self.weights_name))</code> </pre> <br><h3 id="distillyaciya">  Destilaci√≥n </h3><br><p>  La idea de este m√©todo de destilaci√≥n est√° tomada <a href="https://arxiv.org/abs/1903.12136">de un art√≠culo de investigadores de la Universidad de Waterloo</a> .  Como dije anteriormente, el "estudiante" debe aprender a imitar el comportamiento del "maestro".  ¬øCu√°l es exactamente el comportamiento?  En nuestro caso, estas son las predicciones del modelo del maestro en el conjunto de entrenamiento.  Y la idea clave es usar la salida de red antes de aplicar la funci√≥n de activaci√≥n.  Se supone que de esta manera el modelo podr√° aprender mejor la representaci√≥n interna que en el caso de las probabilidades finales. </p><br><p>  El art√≠culo original propone agregar un t√©rmino a la funci√≥n de p√©rdida, que ser√° responsable del error de "imitaci√≥n" - MSE entre los registros del modelo. </p><br><p><img src="https://habrastorage.org/webt/hx/_d/w9/hx_dw9ypkcwc_fhgui_jcappoo4.png"></p><br><p>  Para estos fines, hacemos dos peque√±os cambios: cambiar el n√∫mero de salidas de red de 1 a 2 y corregir la funci√≥n de p√©rdida. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, output, bert_prob, real_label)</span></span></span><span class="hljs-function">:</span></span> a = <span class="hljs-number"><span class="hljs-number">0.5</span></span> criterion_mse = torch.nn.MSELoss() criterion_ce = torch.nn.CrossEntropyLoss() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> a*criterion_ce(output, real_label) + (<span class="hljs-number"><span class="hljs-number">1</span></span>-a)*criterion_mse(output, bert_prob)</code> </pre> <br><p>  Puede reutilizar todo el c√≥digo que escribimos redefiniendo solo el modelo y la p√©rdida: </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">LSTMDistilled</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(LSTMBaseline)</span></span></span><span class="hljs-class">:</span></span> vocab_name = <span class="hljs-string"><span class="hljs-string">'distil_text_vocab.pt'</span></span> weights_name = <span class="hljs-string"><span class="hljs-string">'distil_lstm.pt'</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, settings)</span></span></span><span class="hljs-function">:</span></span> super(LSTMDistilled, self).__init__(settings) self.criterion_mse = torch.nn.MSELoss() self.criterion_ce = torch.nn.CrossEntropyLoss() self.a = <span class="hljs-number"><span class="hljs-number">0.5</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, output, bert_prob, real_label)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.a * self.criterion_ce(output, real_label) + (<span class="hljs-number"><span class="hljs-number">1</span></span> - self.a) * self.criterion_mse(output, bert_prob) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, text_field)</span></span></span><span class="hljs-function">:</span></span> model = SimpleLSTM( input_dim=len(text_field.vocab), embedding_dim=<span class="hljs-number"><span class="hljs-number">64</span></span>, hidden_dim=<span class="hljs-number"><span class="hljs-number">128</span></span>, output_dim=<span class="hljs-number"><span class="hljs-number">2</span></span>, n_layers=<span class="hljs-number"><span class="hljs-number">1</span></span>, bidirectional=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, dropout=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, batch_size=self.settings[<span class="hljs-string"><span class="hljs-string">'train_batch_size'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model</code> </pre> <br><p>  Eso es todo, ahora nuestro modelo est√° aprendiendo a "imitar". </p><br><h3 id="sravnenie-modeley">  Comparaci√≥n de modelo </h3><br><p>  En el art√≠culo original, los mejores resultados de clasificaci√≥n para SST-2 se obtienen en a = 0, cuando el modelo aprende solo a imitar, sin tener en cuenta las etiquetas reales.  La precisi√≥n es a√∫n menor que BERT, pero significativamente mejor que la BiLSTM normal. </p><br><p><img src="https://habrastorage.org/webt/0m/y6/g7/0my6g7eahypj6eq3o52arxxldxq.png"></p><br><p>  Trat√© de repetir los resultados del art√≠culo, pero en mis experimentos el mejor resultado se obtuvo a = 0.5. </p><br><p>  As√≠ es como se ven los gr√°ficos de p√©rdida y precisi√≥n cuando se aprende LSTM de la manera habitual.  A juzgar por el comportamiento de la p√©rdida, el modelo aprendi√≥ r√°pidamente, y en alg√∫n lugar despu√©s de la sexta era, comenz√≥ el reciclaje. </p><br><p><img src="https://habrastorage.org/webt/lk/bg/rn/lkbgrnank6obtu7pewsoalba9yk.png"></p><br><p>  Gr√°ficos de destilaci√≥n: </p><br><p><img src="https://habrastorage.org/webt/gl/u5/7g/glu57giappdlhkc31wlpdulsdkc.png"></p><br><p>  El BiLSTM destilado es consistentemente mejor de lo normal.  Es importante que sean absolutamente id√©nticos en arquitectura, la √∫nica diferencia est√° en la forma de ense√±ar.  <a href="https://github.com/pvgladkov/knowledge-distillation/tree/master/experiments/sst2">Publiqu√© el</a> c√≥digo de entrenamiento completo <a href="https://github.com/pvgladkov/knowledge-distillation/tree/master/experiments/sst2">en GitHub</a> . </p><br><h2 id="zaklyuchenie">  Conclusi√≥n </h2><br><p>  En esta gu√≠a, trat√© de explicar la idea b√°sica de un enfoque de destilaci√≥n.  La arquitectura espec√≠fica del alumno depender√° de la tarea en cuesti√≥n.  Pero en general, este enfoque es aplicable en cualquier tarea pr√°ctica.  Debido a la complejidad en la etapa de entrenamiento del modelo, puede obtener un aumento significativo en su calidad, manteniendo la simplicidad original de la arquitectura. </p></div></div><p>Source: <a href="https://habr.com/ru/post/485290/">https://habr.com/ru/post/485290/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../485278/index.html">La mano de Dios Ayuda de cupones</a></li>
<li><a href="../485280/index.html">Ir Fakedb Emulaci√≥n de base de datos en pruebas</a></li>
<li><a href="../485284/index.html">Caracter√≠sticas de SPIKE ‚Ñ¢ Prime LEGO¬Æ Education</a></li>
<li><a href="../485286/index.html">C√≥mo pesamos los productos o una peque√±a oda de automatizaci√≥n</a></li>
<li><a href="../485288/index.html">Me encanta odiar el juego indie</a></li>
<li><a href="../485294/index.html">Curso moderno sobre Node.js en 2020</a></li>
<li><a href="../485298/index.html">El misterioso programa LyX. Parte 4</a></li>
<li><a href="../485300/index.html">Nuevo brote de gusanos H2Miner que explotan Redis RCE descubierto</a></li>
<li><a href="../485304/index.html">Un par de trucos de elementos de iframe</a></li>
<li><a href="../485312/index.html">DevOps para aplicaciones m√≥viles</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>