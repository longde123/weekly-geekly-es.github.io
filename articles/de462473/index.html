<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧑🏾‍🤝‍🧑🏾 👩🏼‍🌾 ✒️ Wir entwickeln eine Umgebung für die Arbeit mit Microservices. Teil 1 Installation von Kubernetes HA auf Bare Metal (Debian) 👇 🚈 👎🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo liebe Leser von Habr! 


 Mit dieser Veröffentlichung möchte ich eine Reihe von Artikeln über die Bereitstellung einer vollwertigen Orchestrieru...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wir entwickeln eine Umgebung für die Arbeit mit Microservices. Teil 1 Installation von Kubernetes HA auf Bare Metal (Debian)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/462473/"><p><img src="https://habrastorage.org/webt/aq/dx/l3/aqdxl3a5akxfl3rh9b7bzc_kqji.jpeg"></p><br><h4>  Hallo liebe Leser von Habr! </h4><br><p>  Mit dieser Veröffentlichung möchte ich eine Reihe von Artikeln über die Bereitstellung einer vollwertigen Orchestrierungsumgebung mit Kubernetes-Containern beginnen, die betriebsbereit sind und Anwendungen starten. <br>  Ich möchte nicht nur erläutern, wie ein Kubernetes-Cluster bereitgestellt wird, sondern auch, wie ein Cluster nach der Installation konfiguriert wird und wie praktische Tools und Add-Ons hinzugefügt werden, um die Microservice-Architektur zu verwenden. </p><br><h2>  Dieser Zyklus besteht aus mindestens vier Artikeln: </h2><br><ol><li>  Im ersten Teil werde ich Ihnen erklären, wie Sie einen ausfallsicheren Kubernetes-Cluster auf Bare Iron installieren, ein Standard-Dashboard installieren und den Zugriff darauf konfigurieren sowie einen Ingress-Controller installieren. </li><li>  In einem zweiten Artikel werde ich Ihnen zeigen, wie Sie einen Ceph-Failovercluster bereitstellen und wie Sie RBD-Volumes in unserem Kubernetes-Cluster verwenden.  Ich werde auch ein wenig auf andere Arten von Speichern eingehen (Speicher) und die lokale Speicherung genauer betrachten.  Außerdem werde ich Ihnen erklären, wie Sie einen fehlertoleranten S3-Speicher basierend auf dem erstellten CEPH-Cluster organisieren </li><li>  Im dritten Artikel werde ich beschreiben, wie ein Failover-Cluster MySql in unserem Kubernetes-Cluster bereitgestellt wird, nämlich Percona XtraDB-Cluster auf Kubernetes.  Außerdem werde ich alle Probleme beschreiben, auf die wir gestoßen sind, als wir beschlossen haben, die Datenbank auf Kubernetes zu übertragen. </li><li>  Im vierten Artikel werde ich versuchen, alles zusammenzustellen und Ihnen zu erklären, wie Sie eine Anwendung bereitstellen und ausführen, die die Datenbank- und Ceph-Volumes verwendet.  Ich werde Ihnen erklären, wie Sie den Ingress-Controller für den Zugriff auf unsere Anwendung von außen und den automatischen Zertifikatbestelldienst von Let's Encrypt konfigurieren.  Zum anderen können diese Zertifikate automatisch auf dem neuesten Stand gehalten werden.  Wir werden RBAC auch im Zusammenhang mit dem Zugriff auf das Control Panel ansprechen.  Ich erzähle Ihnen kurz und bündig von Helm und seiner Installation. <br>  Wenn Sie an den Informationen in diesen Veröffentlichungen interessiert sind, dann willkommen! <a name="habracut"></a></li></ol><br><h2>  Eintrag: </h2><br><p>  Für wen sind diese Artikel?  Zuallererst für diejenigen, die gerade ihre Reise in das Studium von Kubernetes beginnen.  Dieser Zyklus ist auch nützlich für Ingenieure, die daran denken, von einem Monolithen zu Microservices zu wechseln.  Alles, was beschrieben wird, ist meine Erfahrung, einschließlich derjenigen, die ich bei der Übersetzung mehrerer Projekte von einem Monolithen nach Kubernetes erhalten habe.  Es ist möglich, dass einige Teile der Veröffentlichungen für erfahrene Ingenieure von Interesse sind. </p><br><h4>  Was ich in dieser Reihe von Veröffentlichungen nicht im Detail betrachten werde: </h4><br><ul><li>  Erklären Sie ausführlich, was Kubernetes-Grundelemente sind, z. B.: Pod, Bereitstellung, Service, Ingress usw. </li><li>  Ich werde CNI (Container Networking Interface) sehr oberflächlich betrachten, wir verwenden callico daher andere Lösungen, die ich nur auflisten werde. </li><li>  Docker-Image-Erstellungsprozess. </li><li>  CI \ CD-Prozesse.  (Vielleicht eine separate Veröffentlichung, aber nach dem gesamten Zyklus) </li><li>  Helm;  Es wurde ziemlich viel über ihn geschrieben. Ich werde nur auf den Prozess der Installation im Cluster und der Einrichtung des Clients eingehen. </li></ul><br><h4>  Was ich im Detail betrachten möchte: </h4><br><ul><li>  Schrittweise schrittweise Bereitstellung des Kubernetes-Clusters.  Ich werde Kubeadm verwenden.  Gleichzeitig werde ich Schritt für Schritt detailliert auf den Prozess der Installation eines Clusters auf Bare Metal, verschiedene Arten der ETCD-Installation und die Konfiguration von Dateien für kube admina eingehen.  Ich werde versuchen, alle Ausgleichsoptionen für den Ingress-Controller und den Unterschied in den verschiedenen Zugriffsschemata von Arbeitsknoten auf die Server-API zu klären. <br>  Ich weiß, dass es heute viele großartige Tools für die Bereitstellung von Kubernetes gibt, zum Beispiel Kubespray oder denselben Rancher.  Vielleicht ist es für jemanden bequemer, sie zu benutzen.  Aber ich denke, es gibt viele Ingenieure, die das Problem genauer betrachten möchten. </li><li>  CEPH-Terminologie und schrittweise Installation des CEPH-Clusters sowie schrittweise Anweisungen zum Verbinden des Ceph-Speichers mit dem von Kubernetes erstellten Cluster. </li><li>  lokale Speicher, Verbindung zum Kubernetes-Cluster sowie Unterschiede zu Verbindungen wie Hostpfad usw. </li><li>  kubernetes-Betreiber und die Bereitstellung von Percona XtraDB Cluster mit Hilfe des Betreibers sowie der Versuch, nach sechs Monaten Erfahrung in der Produktion über die Vor- und Nachteile einer solchen Lösung zu sprechen.  Und ich werde auch ein paar Pläne für die Finalisierung des Betreibers von percona teilen. </li></ul><br><h2>  Inhaltsverzeichnis: </h2><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Liste der Hosts, Hostressourcen, Betriebssystem- und Softwareversionen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes Cluster HA-Diagramm</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bevor Sie beginnen oder bevor Sie beginnen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Füllen Sie die Datei create-config.sh aus</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Betriebssystem-Kernel-Update</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vorbereiten von Knoten Installieren von Kubelet, Kubectl, Kubeadm und Docker</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ETCD installieren (verschiedene Optionen)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Starten des ersten Kubernetes-Assistenten</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CNI Callico Installation</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Start des zweiten und dritten Kubernetes-Assistenten</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fügen Sie dem Cluster einen Arbeitsknoten hinzu</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Installieren Sie Haproxy auf Worker-Knoten für HA</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ingress Controller installieren</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Installieren Sie die Web-Benutzeroberfläche (Dashboard)</a> </li></ol><br><a name="vm"></a><br><h2>  Liste und Ziel der Hosts </h2><br><p>  Alle Knoten meines Clusters befinden sich auf virtuellen Maschinen mit einem vorinstallierten Debian 9-Stretch-System mit dem Kernel 4.19.0-0.bpo.5-amd64.  Für die Virtualisierung verwende ich Proxmox VE. </p><br><h4>  Tabellen-VM und ihre Leistungsmerkmale: </h4><br><div class="scrollable-table"><table><tbody><tr><th>  <b>Name</b> </th><th>  <b>IP-Adresse</b> </th><th>  <b>Kommentar</b> </th><th>  <b>CPU</b> </th><th>  <b>Mem</b> </th><th>  <b>DISK1</b> </th><th>  <b>DISK2</b> </th></tr><tr><td>  master01 </td><td>  10.73.71.25 </td><td>  Hauptknoten </td><td>  4vcpu </td><td>  4 GB </td><td>  Festplatte </td><td>  --- ---. </td></tr><tr><td>  master02 </td><td>  10.73.71.26 </td><td>  Hauptknoten </td><td>  4vcpu </td><td>  4 GB </td><td>  Festplatte </td><td>  --- ---. </td></tr><tr><td>  master03 </td><td>  10.73.71.27 </td><td>  Hauptknoten </td><td>  4vcpu </td><td>  4 GB </td><td>  Festplatte </td><td>  --- ---. </td></tr><tr><td>  worknode01 </td><td>  10.73.75.241 </td><td>  Arbeitsknoten </td><td>  4vcpu </td><td>  4 GB </td><td>  Festplatte </td><td>  SSD </td></tr><tr><td>  worknode02 </td><td>  10.73.75.242 </td><td>  Arbeitsknoten </td><td>  4vcpu </td><td>  4 GB </td><td>  Festplatte </td><td>  SSD </td></tr><tr><td>  worknode03 </td><td>  10.73.75.243 </td><td>  Arbeitsknoten </td><td>  4vcpu </td><td>  4 GB </td><td>  Festplatte </td><td>  SSD </td></tr></tbody></table></div><br><p>  Es ist nicht erforderlich, dass Sie nur eine solche Konfiguration von Maschinen haben müssen, aber ich rate Ihnen dennoch, die Empfehlungen der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen Dokumentation</a> zu befolgen und für Master die RAM-Größe auf mindestens 4 GB zu erhöhen.  Mit Blick auf die Zukunft werde ich sagen, dass ich mit einer kleineren Anzahl Störungen in der Arbeit von CNI Callico festgestellt habe <br>  Ceph ist auch in Bezug auf Speicher- und Festplattenleistung ziemlich unersättlich. <br>  Unsere Produktionsinstallationen funktionieren ohne Bare-Metal-Virtualisierung, aber ich kenne viele Beispiele, bei denen virtuelle Maschinen mit relativ bescheidenen Ressourcen ausreichten.  Es hängt alles von Ihren Bedürfnissen und Arbeitslasten ab. </p><br><h2>  Listen- und Softwareversionen </h2><br><div class="scrollable-table"><table><tbody><tr><th>  <b>Name</b> </th><th>  <b>Version</b> </th></tr><tr><td>  Kubernetes </td><td>  1.15.1 </td></tr><tr><td>  Docker </td><td>  19.3.1 </td></tr></tbody></table></div><br><p>  Ab Version 1.14 hat Kubeadm die Unterstützung der v1alpha3-API-Version eingestellt und vollständig auf die v1beta1-API-Version umgestellt, die in naher Zukunft unterstützt wird. In diesem Artikel werde ich daher nur auf v1beta1 eingehen. <br>  Wir glauben, dass Sie die Maschinen für den Kubernetes-Cluster vorbereitet haben.  Sie sind alle über das Netzwerk für einander zugänglich, haben eine „Internetverbindung“ zum Internet und ein „sauberes“ Betriebssystem ist auf ihnen installiert. <br>  Für jeden Installationsschritt werde ich klären, auf welchen Maschinen der Befehl oder Befehlsblock ausgeführt wird.  Führen Sie alle Befehle als root aus, sofern nicht anders angegeben. <br>  Alle Konfigurationsdateien sowie ein Skript zur Vorbereitung stehen in meinem <a href="">Github</a> zum Download zur Verfügung <br>  Also fangen wir an. </p><br><a name="ha-image"></a><br><h2>  Kubernetes Cluster HA-Diagramm </h2><br><p><img src="https://habrastorage.org/webt/ch/mx/pg/chmxpgzdyk0bvxwx7-6lawqfmvo.jpeg"><br>  Ein ungefähres Diagramm des HA-Clusters.  Der Künstler von mir ist, um ehrlich zu sein, so lala, aber ich werde versuchen, es kurz und bündig zu erklären, ohne mich besonders mit der Theorie zu befassen. <br>  Unser Cluster wird also aus drei Masterknoten und drei Arbeiterknoten bestehen.  Auf jedem Kubernetes-Masterknoten funktionieren etcd (grüne Pfeile im Diagramm) und kubernetes-Serviceteile für uns.  Nennen wir sie generisch - Kubeapi. <br>  Über den Mastercluster etcd tauschen Knoten den Status des Kubernetes-Clusters aus.  Ich werde die gleichen Adressen wie die Einstiegspunkte des Eingangscontrollers für den externen Verkehr angeben (rote Pfeile im Diagramm). <br>  Auf Worker-Knoten arbeitet kubelet für uns, das über haproxy, das lokal auf jedem Worker-Knoten installiert ist, mit dem kubernetes-API-Server kommuniziert.  Als Server-API-Adresse für kubelet verwende ich den localhost 127.0.0.1:6443, und haproxy on roundrobin verteilt Anforderungen auf drei Masterknoten und überprüft auch die Verfügbarkeit der Masterknoten.  Mit diesem Schema können wir HA erstellen. Im Falle eines Ausfalls eines der Masterknoten senden die Arbeiterknoten leise Anforderungen an die beiden verbleibenden Masterknoten. </p><br><a name="begin"></a><br><h2>  Bevor Sie beginnen </h2><br><p>  Bevor wir mit der Arbeit an jedem Knoten des Clusters beginnen, stellen wir die Pakete bereit, die wir für die Arbeit benötigen: </p><br><pre><code class="plaintext hljs">apt-get update &amp;&amp; apt-get install -y curl apt-transport-https git</code> </pre> <br><p>  Kopieren Sie auf den Masterknoten das Repository mit Konfigurationsvorlagen </p><br><pre> <code class="plaintext hljs">sudo -i git clone https://github.com/rjeka/kubernetes-ceph-percona.git</code> </pre> <br><p>  Überprüfen Sie, ob die IP-Adresse der Hosts auf den Assistenten mit der übereinstimmt, die der kubernetes-Server abhört </p><br><pre> <code class="plaintext hljs">hostname &amp;&amp; hostname -i master01 10.73.71.25</code> </pre> <br><p>  und so für alle Masterknoten. </p><br><p>  Deaktivieren Sie unbedingt SWAP, da sonst kubeadm einen Fehler auslöst </p><br><pre> <code class="plaintext hljs">[ERROR Swap]: running with swap on is not supported. Please disable swap</code> </pre> <br><p>  Sie können den Befehl deaktivieren </p><br><pre> <code class="plaintext hljs">swapoff -a</code> </pre> <br><p>  Denken Sie daran, in / etc / fstab zu kommentieren </p><br><a name="create-config"></a><br><h2>  Füllen Sie die Datei create-config.sh aus </h2><br><p>  Um die für die Installation des kubernetes-Clusters erforderlichen Konfigurationen automatisch auszufüllen, habe ich ein kleines Skript create-config.sh hochgeladen.  Sie müssen es buchstäblich 8 Zeilen ausfüllen.  Geben Sie die IP-Adressen und den Hostnamen Ihrer Master an.  Und geben Sie auch etcd tocken an, Sie können es nicht ändern.  Ich werde unten den Teil des Skripts angeben, in dem Sie Änderungen vornehmen müssen. </p><br><pre> <code class="plaintext hljs">#!/bin/bash ####################################### # all masters settings below must be same ####################################### # master01 ip address export K8SHA_IP1=10.73.71.25 # master02 ip address export K8SHA_IP2=10.73.71.26 # master03 ip address export K8SHA_IP3=10.73.71.27 # master01 hostname export K8SHA_HOSTNAME1=master01 # master02 hostname export K8SHA_HOSTNAME2=master02 # master03 hostname export K8SHA_HOSTNAME3=master03 #etcd tocken: export ETCD_TOKEN=9489bf67bdfe1b3ae077d6fd9e7efefd #etcd version export ETCD_VERSION="v3.3.10"</code> </pre><br><a name="kernel"></a><br><h2>  Betriebssystem-Kernel-Update </h2><br><p>  Dieser Schritt ist optional, da der Kernel über die Backports aktualisiert werden muss und Sie dies auf eigene Gefahr und Gefahr tun.  Möglicherweise tritt dieses Problem nie auf. In diesem Fall können Sie den Kernel auch nach der Bereitstellung von Kubernetes aktualisieren.  Im Allgemeinen entscheiden Sie. <br>  Ein Kernel-Update ist erforderlich, um den alten Docker-Fehler zu beheben, der nur in der Linux-Kernel-Version 4.18 behoben wurde.  Sie können hier mehr über diesen Fehler lesen.  Ein Fehler wurde im periodischen Hängen der Netzwerkschnittstelle auf den Kubernetes-Knoten mit dem Fehler ausgedrückt: </p><br><pre> <code class="plaintext hljs">waiting for eth0 to become free. Usage count = 1</code> </pre> <br><p>  Nach der Installation des Betriebssystems hatte ich die Kernel-Version 4.9 </p><br><pre> <code class="bash hljs">uname -a Linux master01 4.9.0-7-amd64 <span class="hljs-comment"><span class="hljs-comment">#1 SMP Debian 4.9.110-3+deb9u2 (2018-08-13) x86_64 GNU/Linux</span></span></code> </pre> <br><p>  Auf jeder Maschine für Kubernetes führen wir aus <br>  Schritt 1 <br>  Fügen Sie der Quellliste Back-Ports hinzu </p><br><pre> <code class="plaintext hljs">echo deb http://ftp.debian.org/debian stretch-backports main &gt; /etc/apt/sources.list apt-get update apt-cache policy linux-compiler-gcc-6-x86</code> </pre> <br><p>  Schritt Nummer 2 <br>  Paketinstallation </p><br><pre> <code class="plaintext hljs">apt install -y -t stretch-backports linux-image-amd64 linux-headers-amd64</code> </pre> <br><p>  Schritt Nummer 3 <br>  Starten Sie neu </p><br><pre> <code class="plaintext hljs">reboot</code> </pre> <br><p>  Überprüfen Sie, ob alles in Ordnung ist </p><br><pre> <code class="plaintext hljs">uname -a Linux master01 4.19.0-0.bpo.5-amd64 #1 SMP Debian 4.19.37-4~bpo9+1 (2019-06-19) x86_64 GNU/Linux</code> </pre><br><a name="kubelet"></a><br><h2>  Vorbereiten von Knoten Installieren von Kubelet, Kubectl, Kubeadm und Docker </h2><br><h4>  Installieren Sie Kubelet, Kubectl, Kubeadm </h4><br><p>  Wir setzen alle Knoten des Clusters gemäß der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation von Kubernetes ein</a> </p><br><pre> <code class="plaintext hljs">apt-get update &amp;&amp; apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl apt-mark hold kubelet kubeadm kubectl</code> </pre> <br><h4>  Installieren Sie Docker </h4><br><p>  Installieren Sie Docker gemäß den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Anweisungen in der Dokumentation</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><br></a> </p><br><pre> <code class="plaintext hljs">apt-get remove docker docker-engine docker.io containerd runc apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common</code> </pre> <br><pre> <code class="plaintext hljs">curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add - apt-key fingerprint 0EBFCD88</code> </pre> <br><pre> <code class="plaintext hljs">add-apt-repository \ "deb [arch=amd64] https://download.docker.com/linux/debian \ $(lsb_release -cs) \ stable"</code> </pre> <br><pre> <code class="plaintext hljs">apt-get update apt-get install docker-ce docker-ce-cli containerd.io</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Installation Installation von Kubelet, Kubectl, Kubeadm und Docker mit ansible</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">git clone https://github.com/rjeka/kubernetes-ceph-percona.git cd kubernetes-ceph-percona/playbooks vim masters.ini</code> </pre> <br><p>  Registrieren Sie in der Master-Gruppe die IP-Master. <br>  Schreiben Sie in der Worker-Gruppe die IP-Adresse der Arbeitsknoten. </p><br><pre> <code class="plaintext hljs"># sudo c  ansible-playbook -i hosts.ini kubelet.yaml -K ansible-playbook -i hosts.ini docker.yaml -K # sudo  ansible-playbook -i hosts.ini kubelet.yaml ansible-playbook -i hosts.ini docker.yaml</code> </pre> </div></div><br><p>  Wenn Sie aus irgendeinem Grund kein Docker verwenden möchten, können Sie ein beliebiges CRI verwenden.  Sie können zum Beispiel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> darüber lesen, aber dieses Thema geht über den Rahmen dieses Artikels hinaus. </p><br><a name="etcd"></a><br><h2>  ETCD-Installation </h2><br><p>  Ich werde nicht kurz auf die Theorie eingehen: etcd ist ein Open-Source-verteilter Schlüsselwertspeicher.  etcd ist in GO geschrieben und wird in Kubernetes tatsächlich als Datenbank zum Speichern des Clusterstatus verwendet.  Eine ausführlichere Übersicht finden Sie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation zu kubernetes</a> . <br>  etcd kann auf viele Arten installiert werden.  Sie können es lokal installieren und als Daemon ausführen, Sie können es in Docker-Containern ausführen, Sie können es sogar als Kubernetes-Bottom installieren.  Sie können es von Hand oder mit kubeadm installieren (ich habe diese Methode nicht ausprobiert).  Kann auf Cluster-Computern oder einzelnen Servern installiert werden. <br>  Ich werde etcd lokal auf den Masterknoten installieren und als Daemon über systemd ausführen sowie die Installation in Docker in Betracht ziehen.  Ich verwende etcd ohne TLS. Wenn Sie TLS benötigen, lesen Sie die Dokumentation zu etcd oder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kubernetes</a> <br>  Auch in meinem Github wird Ansible-Playbook für die Installation von etcd mit Start über systemd sein. </p><br><h4>  Option Nummer 1 <br>  Lokal installieren, über systemd ausführen </h4><br><p>  Auf allen Mastern: (Auf den Arbeitsknoten des Clusters ist dieser Schritt nicht erforderlich.) <br>  Schritt 1 <br>  Laden Sie das Archiv herunter und entpacken Sie es mit etcd: </p><br><pre> <code class="plaintext hljs">mkdir archives cd archives export etcdVersion=v3.3.10 wget https://github.com/coreos/etcd/releases/download/$etcdVersion/etcd-$etcdVersion-linux-amd64.tar.gz tar -xvf etcd-$etcdVersion-linux-amd64.tar.gz -C /usr/local/bin/ --strip-components=1</code> </pre> <br><p>  Schritt Nummer 2 <br>  Erstellen Sie eine Konfigurationsdatei für ETCD </p><br><pre> <code class="plaintext hljs">cd .. ./create-config.sh etcd</code> </pre> <br><p>  Das Skript verwendet den Wert etcd als Eingabe und generiert eine Konfigurationsdatei im Verzeichnis etcd.  Nachdem das Skript ausgeführt wurde, befindet sich die fertige Konfigurationsdatei im Verzeichnis etcd. <br>  Bei allen anderen Konfigurationen funktioniert das Skript nach dem gleichen Prinzip.  Es nimmt einige Eingaben entgegen und erstellt eine Konfiguration in einem bestimmten Verzeichnis. </p><br><p>  Schritt Nummer 3 <br>  Wir starten den etcd-Cluster und überprüfen seine Leistung </p><br><pre> <code class="plaintext hljs">systemctl start etcd</code> </pre> <br><p>  Überprüfen der Leistung des Dämons </p><br><pre> <code class="plaintext hljs">systemctl status etcd ● etcd.service - etcd Loaded: loaded (/etc/systemd/system/etcd.service; disabled; vendor preset: enabled) Active: active (running) since Sun 2019-07-07 02:34:28 MSK; 4min 46s ago Docs: https://github.com/coreos/etcd Main PID: 7471 (etcd) Tasks: 14 (limit: 4915) CGroup: /system.slice/etcd.service └─7471 /usr/local/bin/etcd --name master01 --data-dir /var/lib/etcd --listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 --advertise-client-urls http://10.73.71.25:2379,http://10.73.71. Jul 07 02:34:28 master01 etcd[7471]: b11e73358a31b109 [logterm: 1, index: 3, vote: 0] cast MsgVote for f67dd9aaa8a44ab9 [logterm: 2, index: 5] at term 554 Jul 07 02:34:28 master01 etcd[7471]: raft.node: b11e73358a31b109 elected leader f67dd9aaa8a44ab9 at term 554 Jul 07 02:34:28 master01 etcd[7471]: published {Name:master01 ClientURLs:[http://10.73.71.25:2379 http://10.73.71.25:4001]} to cluster d0979b2e7159c1e6 Jul 07 02:34:28 master01 etcd[7471]: ready to serve client requests Jul 07 02:34:28 master01 etcd[7471]: serving insecure client requests on [::]:4001, this is strongly discouraged! Jul 07 02:34:28 master01 systemd[1]: Started etcd. Jul 07 02:34:28 master01 etcd[7471]: ready to serve client requests Jul 07 02:34:28 master01 etcd[7471]: serving insecure client requests on [::]:2379, this is strongly discouraged! Jul 07 02:34:28 master01 etcd[7471]: set the initial cluster version to 3.3 Jul 07 02:34:28 master01 etcd[7471]: enabled capabilities for version 3.3 lines 1-19</code> </pre> <br><p>  Und die Gesundheit des Clusters selbst: </p><br><pre> <code class="plaintext hljs">etcdctl cluster-health member 61db137992290fc is healthy: got healthy result from http://10.73.71.27:2379 member b11e73358a31b109 is healthy: got healthy result from http://10.73.71.25:2379 member f67dd9aaa8a44ab9 is healthy: got healthy result from http://10.73.71.26:2379 cluster is healthy etcdctl member list 61db137992290fc: name=master03 peerURLs=http://10.73.71.27:2380 clientURLs=http://10.73.71.27:2379,http://10.73.71.27:4001 isLeader=false b11e73358a31b109: name=master01 peerURLs=http://10.73.71.25:2380 clientURLs=http://10.73.71.25:2379,http://10.73.71.25:4001 isLeader=false f67dd9aaa8a44ab9: name=master02 peerURLs=http://10.73.71.26:2380 clientURLs=http://10.73.71.26:2379,http://10.73.71.26:4001 isLeader=true</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Installieren Sie etcd lokal mit ansible und führen Sie es über systemd aus</b> <div class="spoiler_text"><p>  Mit github klonen wir das Repository mit dem Code auf den Computer, von dem aus Sie das Playbook ausführen.  Diese Maschine sollte SSH-Zugriff auf einen Schlüssel zum Master unseres zukünftigen Clusters haben. </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/rjeka/kubernetes-ceph-percona.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> kubernetes-ceph-percona/playbooks vim masters.ini</code> </pre> <br><p>  Registrieren Sie in der Master-Gruppe die IP-Master. <br>  etcd_version ist die Version von etcd.  Sie können es auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">etcd-Seite in github sehen</a> .  Zum Zeitpunkt des Schreibens gab es Version v3.3.13, die ich v3.3.10 verwende. <br>  etcdToken - Sie können dasselbe verlassen oder Ihr eigenes generieren. <br>  Führen Sie das Playbook-Team </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># sudo c  ansible-playbook -i hosts.ini -l masters etcd.yaml -K BECOME password: &lt;sudo &gt; # sudo  ansible-playbook -i hosts.ini -l masters etcd.yaml</span></span></code> </pre> </div></div><br><p>  Wenn Sie etcd im Docker ausführen möchten, befindet sich unter dem Spoiler eine Anweisung. </p><br><div class="spoiler">  <b class="spoiler_title">Installieren Sie etcd mit Docker-Compose und starten Sie es in Docker</b> <div class="spoiler_text"><p>  Diese Befehle müssen auf allen Cluster-Masterknoten ausgeführt werden. <br>  Mit github klonen wir das Repository mit Code </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/rjeka/kubernetes-ceph-percona.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> kubernetes-ceph-percona</code> </pre> <br><p>  etcd_version ist die Version von etcd.  Sie können es auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">etcd-Seite in github sehen</a> .  Zum Zeitpunkt des Schreibens gab es Version v3.3.13, die ich v3.3.10 verwende. <br>  etcdToken - Sie können dasselbe verlassen oder Ihr eigenes generieren. </p><br><p>  Wir setzen Docker-Compose </p><br><pre> <code class="plaintext hljs">apt-get install -y docker-compose</code> </pre> <br><p>  Wir generieren eine Konfiguration </p><br><pre> <code class="plaintext hljs">./create-config.sh docker</code> </pre> <br><p>  Führen Sie die Installation des etcd-Clusters im Docker aus </p><br><pre> <code class="plaintext hljs">docker-compose --file etcd-docker/docker-compose.yaml up -d</code> </pre> <br><p>  Überprüfen Sie, ob die Behälter leer sind </p><br><pre> <code class="plaintext hljs">docker ps</code> </pre> <br><p>  Und Clusterstatus etcd </p><br><pre> <code class="plaintext hljs">root@master01:~/kubernetes-ceph-percona# docker exec -ti etcd etcdctl cluster-health member 61db137992290fc is healthy: got healthy result from http://10.73.71.27:2379 member b11e73358a31b109 is healthy: got healthy result from http://10.73.71.25:2379 member f67dd9aaa8a44ab9 is healthy: got healthy result from http://10.73.71.26:2379 cluster is healthy root@master01:~/kubernetes-ceph-percona# docker exec -ti etcd etcdctl member list 61db137992290fc: name=etcd3 peerURLs=http://10.73.71.27:2380 clientURLs=http://10.73.71.27:2379,http://10.73.71.27:4001 isLeader=false b11e73358a31b109: name=etcd1 peerURLs=http://10.73.71.25:2380 clientURLs=http://10.73.71.25:2379,http://10.73.71.25:4001 isLeader=true f67dd9aaa8a44ab9: name=etcd2 peerURLs=http://10.73.71.26:2380 clientURLs=http://10.73.71.26:2379,http://10.73.71.26:4001 isLeader=false</code> </pre> <br><p>  Wenn etwas schief gelaufen ist </p><br><pre> <code class="plaintext hljs">docker logs etcd</code> </pre> </div></div><br><a name="master-one"></a><br><h2>  Starten des ersten Kubernetes-Assistenten </h2><br><p>  Zunächst müssen wir eine Konfiguration für kubeadmin generieren </p><br><pre> <code class="plaintext hljs">./create-config.sh kubeadm</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Wir zerlegen eine Konfiguration für kubeadm</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: kubeadm.k8s.io/v1beta1 kind: InitConfiguration localAPIEndpoint: advertiseAddress: 10.73.71.25 #    API- --- apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration kubernetesVersion: stable #      apiServer: #    kubeadm   certSANs: - 127.0.0.1 - 10.73.71.25 - 10.73.71.26 - 10.73.71.27 controlPlaneEndpoint: 10.73.71.25 #     etcd: #  etc external: endpoints: - http://10.73.71.25:2379 - http://10.73.71.26:2379 - http://10.73.71.27:2379 networking: podSubnet: 192.168.0.0/16 #   ,   CNI  .</code> </pre> <br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Informationen zu</a> CNI-Subnetzen finden Sie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation zu kubernetes.</a> <br>  Dies ist eine minimal funktionierende Konfiguration.  Für einen Cluster mit drei Assistenten können Sie ihn in die Konfiguration Ihres Clusters ändern.  Wenn Sie beispielsweise zwei Assistenten verwenden möchten, geben Sie einfach zwei Adressen in certSANs an. <br>  Alle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Konfigurationsparameter</a> finden Sie in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Beschreibung der kubeadm-API</a> . </p></div></div><br><p>  Wir initiieren den ersten Meister </p><br><pre> <code class="plaintext hljs">kubeadm init --config=kubeadmin/kubeadm-init.yaml</code> </pre> <br><p>  Wenn kubeadm fehlerfrei funktioniert, erhalten wir am Ausgang ungefähr den folgenden Ausgang: </p><br><pre> <code class="plaintext hljs">You can now join any number of control-plane nodes by copying certificate authorities and service account keys on each node and then running the following as root: kubeadm join 10.73.71.25:6443 --token ivwoap.259retezqf34amx8 \ --discovery-token-ca-cert-hash sha256:b5c93e32457c8e6478782ff62e8ef77acf72738dda59cd603cdf4821abe12ca3 \ --control-plane Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10.73.71.25:6443 --token ivwoap.259retezqf34amx8 \ --discovery-token-ca-cert-hash sha256:b5c93e32457c8e6478782ff62e8ef77acf72738dda59cd603cdf4821abe12ca3</code> </pre> <br><a name="callica"></a><br><h2>  CNI Calico Installation </h2><br><p>  Es ist an der Zeit, ein Netzwerk aufzubauen, in dem unsere Pods arbeiten werden.  Ich benutze Kaliko, und wir werden es sagen. <br>  Konfigurieren Sie zunächst den Zugriff für Kubelet.  Wir führen alle Befehle auf master01 aus <br>  Wenn Sie als root ausgeführt werden </p><br><pre> <code class="plaintext hljs">export KUBECONFIG=/etc/kubernetes/admin.conf</code> </pre> <br><p>  Wenn unter dem einfachen Benutzer </p><br><pre> <code class="plaintext hljs">mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config</code> </pre> <br><p>  Sie können den Cluster auch von Ihrem Laptop oder einem beliebigen lokalen Computer aus verwalten.  Kopieren Sie dazu die Datei /etc/kubernetes/admin.conf auf Ihren Laptop oder einen anderen Computer in $ HOME / .kube / config </p><br><p>  Wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">setzen</a> CNI <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gemäß der Kubernetes-Dokumentation ein</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><br></a> </p><br><pre> <code class="plaintext hljs">kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml</code> </pre> <br><p>  Wir warten, bis alle Schoten aufgehen </p><br><pre> <code class="plaintext hljs">watch -n1 kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-59f54d6bbc-psr2z 1/1 Running 0 96s kube-system calico-node-hm49z 1/1 Running 0 96s kube-system coredns-5c98db65d4-svcx9 1/1 Running 0 77m kube-system coredns-5c98db65d4-zdlb8 1/1 Running 0 77m kube-system kube-apiserver-master01 1/1 Running 0 76m kube-system kube-controller-manager-master01 1/1 Running 0 77m kube-system kube-proxy-nkdqn 1/1 Running 0 77m kube-system kube-scheduler-master01 1/1 Running 0 77m</code> </pre> <br><a name="mastes-other"></a><br><h2>  Start des zweiten und dritten Kubernetes-Assistenten </h2><br><p>  Bevor Sie master02 und master03 starten, müssen Sie die Zertifikate mit master01 kopieren, die kubeadm beim Erstellen des Clusters generiert hat.  Ich werde über scp kopieren <br>  Auf master01 </p><br><pre> <code class="plaintext hljs">export master02=10.73.71.26 export master03=10.73.71.27 scp -r /etc/kubernetes/pki $master02:/etc/kubernetes/ scp -r /etc/kubernetes/pki $master03:/etc/kubernetes/</code> </pre> <br><p>  Auf master02 und master03 <br>  Erstellen Sie eine Konfiguration für kubeadm </p><br><pre> <code class="plaintext hljs">./create-config.sh kubeadm</code> </pre> <br><p>  Fügen Sie dem Cluster master02 und master03 hinzu </p><br><pre> <code class="plaintext hljs">kubeadm init --config=kubeadmin/kubeadm-init.yaml</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Störungen an mehreren Netzwerkschnittstellen !!!!</b> <div class="spoiler_text"><p>  In der Produktion verwende ich kubernetes v1.13.5 und calico v3.3.  Und ich hatte keine solchen Pannen. <br>  Bei der Vorbereitung des Artikels und der Verwendung der stabilen Version (zum Zeitpunkt des Schreibens waren dies v1.15.1 kubernetes und Version 3.8 callico) stieß ich auf ein Problem, das in CNI-Startfehlern ausgedrückt wurde </p><br><pre> <code class="plaintext hljs">root@master01:~/kubernetes-ceph-percona# kubectl get pods -A -w NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-658558ddf8-t6gfs 0/1 ContainerCreating 0 11s kube-system calico-node-7td8g 1/1 Running 0 11s kube-system calico-node-dthg5 0/1 CrashLoopBackOff 1 11s kube-system calico-node-tvhkq 0/1 CrashLoopBackOff 1 11s</code> </pre> <br><p>  Dies ist ein Fehler im Calico-Daemon-Set, wenn der Server über mehrere Netzwerkschnittstellen verfügt <br>  Auf Githab gibt es ein Problem mit dieser Panne <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://github.com/projectcalico/calico/issues/2720</a> <br>  Es wird gelöst, indem der Daemon-Set calico-node bearbeitet und der Parameter IP_AUTODETECTION_METHOD zu env hinzugefügt wird </p><br><pre> <code class="plaintext hljs">kubectl edit -n kube-system ds calico-node</code> </pre> <br><p>  Fügen Sie den Parameter IP_AUTODETECTION_METHOD mit dem Namen Ihrer Schnittstelle hinzu, an der der Assistent arbeitet.  in meinem Fall ist es ens19 </p><br><pre> <code class="plaintext hljs">- name: IP_AUTODETECTION_METHOD value: ens19</code> </pre> <br><p><img src="https://habrastorage.org/webt/xe/pq/7h/xepq7hbpjwhgowfk0hkc6-ryw8a.png"><br>  Überprüfen Sie, ob alle Knoten im Cluster aktiv sind </p><br><pre> <code class="plaintext hljs"># kubectl get nodes NAME STATUS ROLES AGE VERSION master01 Ready master 28m v1.15.1 master02 Ready master 26m v1.15.1 master03 Ready master 18m v1.15.1</code> </pre> <br><p>  Und was ist Calica lebendig </p><br><pre> <code class="plaintext hljs"># kubectl get pods -A -o wide | grep calico kube-system calico-kube-controllers-59f54d6bbc-5lxgn 1/1 Running 0 27m kube-system calico-node-fngpz 1/1 Running 1 24m kube-system calico-node-gk7rh 1/1 Running 0 8m55s kube-system calico-node-w4xtt 1/1 Running 0 25m</code> </pre> </div></div><br><a name="worknodes"></a><br><h2>  Fügen Sie dem Cluster Arbeitsknoten hinzu </h2><br><p>  Im Moment haben wir einen Cluster, in dem drei Masterknoten ausgeführt werden.  Masterknoten sind jedoch Computer, auf denen API, Scheduler und andere Dienste des Kubernetes-Clusters ausgeführt werden.  Damit wir unsere Pods ausführen können, benötigen wir die sogenannten Worker-Knoten. <br>  Wenn Sie nur über begrenzte Ressourcen verfügen, können Sie Pods auf Masterknoten ausführen. Ich persönlich rate jedoch nicht dazu. </p><br><div class="spoiler">  <b class="spoiler_title">Ausführen der Herd-Masterknoten</b> <div class="spoiler_text"><p>  Führen Sie den folgenden Befehl auf einem der Assistenten aus, um das Starten von Herden auf Masterknoten zu ermöglichen </p><br><pre> <code class="plaintext hljs">kubectl taint nodes --all node-role.kubernetes.io/master-</code> </pre> </div></div><br><p>  Installieren Sie die Knoten kubelet, kubeadm, kubectl und docker auf dem Worker wie auf den Masterknoten </p><br><div class="spoiler">  <b class="spoiler_title">Installieren Sie kubelet, kubeadm, kubectl und docker</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apt-get update &amp;&amp; apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl apt-mark hold kubelet kubeadm kubectl</code> </pre> <br><h4>  Installieren Sie Docker </h4><br><p>  Installieren Sie Docker gemäß den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Anweisungen in der Dokumentation</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><br></a> </p><br><pre> <code class="plaintext hljs">apt-get remove docker docker-engine docker.io containerd runc apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common</code> </pre> <br><pre> <code class="plaintext hljs">curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add - apt-key fingerprint 0EBFCD88</code> </pre> <br><pre> <code class="plaintext hljs">add-apt-repository \ "deb [arch=amd64] https://download.docker.com/linux/debian \ $(lsb_release -cs) \ stable"</code> </pre> <br><pre> <code class="plaintext hljs">apt-get update apt-get install docker-ce docker-ce-cli containerd.io</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Installation Installation von Kubelet, Kubectl, Kubeadm und Docker mit ansible</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">git clone https://github.com/rjeka/kubernetes-ceph-percona.git cd kubernetes-ceph-percona/playbooks vim masters.ini</code> </pre> <br><p>  Registrieren Sie in der Master-Gruppe die IP-Master. <br>  Schreiben Sie in der Worker-Gruppe die IP-Adresse der Arbeitsknoten. </p><br><pre> <code class="plaintext hljs"># sudo c  ansible-playbook -i hosts.ini kubelet.yaml -K ansible-playbook -i hosts.ini docker.yaml -K # sudo  ansible-playbook -i hosts.ini kubelet.yaml ansible-playbook -i hosts.ini docker.yaml</code> </pre> </div></div></div></div><br><p>  Jetzt ist es Zeit, zu der Zeile zurückzukehren, die kubeadm bei der Installation des Masterknotens generiert hat. <br>  Sie sieht für mich so aus. </p><br><pre> <code class="plaintext hljs">kubeadm join 10.73.71.25:6443 --token ivwoap.259retezqf34amx8 \ --discovery-token-ca-cert-hash sha256:b5c93e32457c8e6478782ff62e8ef77acf72738dda59cd603cdf4821abe12ca3</code> </pre> <br><p>  Dieser Befehl muss auf jedem Arbeitsknoten ausgeführt werden. <br>  Wenn Sie kein Token geschrieben haben, können Sie ein neues generieren </p><br><pre> <code class="plaintext hljs">kubeadm token create --print-join-command --ttl=0</code> </pre> <br><p>  Nachdem kubeadm funktioniert hat, wird Ihr neuer Knoten in den Cluster eingegeben und ist arbeitsbereit </p><br><pre> <code class="plaintext hljs">This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster.</code> </pre> <br><p>  Schauen wir uns nun das Ergebnis an </p><br><pre> <code class="plaintext hljs">root@master01:~# kubectl get nodes NAME STATUS ROLES AGE VERSION master01 Ready master 10d v1.15.1 master02 Ready master 10d v1.15.1 master03 Ready master 10d v1.15.1 worknode01 Ready &lt;none&gt; 5m44s v1.15.1 worknode02 Ready &lt;none&gt; 59s v1.15.1 worknode03 Ready &lt;none&gt; 51s v1.15.1</code> </pre> <br><a name="haproxy"></a><br><h2>  Installieren Sie haproxy auf Arbeitsknoten </h2><br><p>  Jetzt haben wir einen Arbeitscluster mit drei Hauptknoten und drei Arbeitsknoten. <br>  Das Problem ist, dass unsere Worker-Knoten jetzt keinen HA-Modus haben. <br>  Wenn Sie sich die Kubelet-Konfigurationsdatei ansehen, werden Sie feststellen, dass unsere Worker-Knoten nur auf einen der drei Master-Knoten zugreifen. </p><br><pre> <code class="plaintext hljs">root@worknode01:~# cat /etc/kubernetes/kubelet.conf | grep server: server: https://10.73.71.27:6443</code> </pre> <br><p>  In meinem Fall ist dies master03.  Bei dieser Konfiguration verliert der Worker-Knoten bei einem Absturz von master03 die Kommunikation mit dem Cluster-API-Server.  Um unseren Cluster vollständig HA zu machen, installieren wir auf jedem der Worker einen Load Balancer (Haproxy), der laut Round Robin Anforderungen für drei Masterknoten verteilt, und in der Kubelet-Konfiguration auf Worker-Knoten ändern wir die Serveradresse in 127.0.0.1:6443 <br>  Installieren Sie zunächst HAProxy auf jedem Arbeitsknoten. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Es gibt einen guten Spickzettel für die Installation</a> </p><br><pre> <code class="plaintext hljs">curl https://haproxy.debian.net/bernat.debian.org.gpg | \ apt-key add - echo deb http://haproxy.debian.net stretch-backports-2.0 main | \ tee /etc/apt/sources.list.d/haproxy.list apt-get update apt-get install haproxy=2.0.\*</code> </pre> <br><p>  Nach der Installation von HAproxy müssen wir eine Konfiguration dafür erstellen. <br>  Wenn auf den Worker-Knoten kein Verzeichnis mit Konfigurationsdateien vorhanden ist, klonen wir es </p><br><pre> <code class="plaintext hljs">git clone https://github.com/rjeka/kubernetes-ceph-percona.git cd kubernetes-ceph-percona/</code> </pre> <br><p>  Führen Sie das Konfigurationsskript mit dem Haproxy-Flag aus </p><br><pre> <code class="plaintext hljs">./create-config.sh haproxy</code> </pre> <br><p>  Das Skript konfiguriert haproxy und startet es neu. <br>  Überprüfen Sie, ob Haproxy Port 6443 abhört. </p><br><pre> <code class="plaintext hljs">root@worknode01:~/kubernetes-ceph-percona# netstat -alpn | grep 6443 tcp 0 0 127.0.0.1:6443 0.0.0.0:* LISTEN 30675/haproxy tcp 0 0 10.73.75.241:6443 0.0.0.0:* LISTEN 30675/haproxy</code> </pre> <br><p>  Jetzt müssen wir kubelet anweisen, auf localhost anstatt auf den Masterknoten zuzugreifen.  Bearbeiten Sie dazu den Serverwert in den Dateien /etc/kubernetes/kubelet.conf und /etc/kubernetes/bootstrap-kubelet.conf auf allen Arbeitsknoten. </p><br><pre> <code class="plaintext hljs">vim /etc/kubernetes/kubelet.conf vim nano /etc/kubernetes/bootstrap-kubelet.conf</code> </pre> <br><p>  Der Serverwert sollte folgende Form annehmen: </p><br><pre> <code class="plaintext hljs">server: https://127.0.0.1:6443</code> </pre> <br><p>  Starten Sie nach dem Vornehmen der Änderungen die Kubelet- und Docker-Dienste neu </p><br><pre> <code class="plaintext hljs">systemctl restart kubelet &amp;&amp; systemctl restart docker</code> </pre> <br><p>  Überprüfen Sie, ob alle Knoten ordnungsgemäß funktionieren. </p><br><pre> <code class="plaintext hljs">kubectl get nodes NAME STATUS ROLES AGE VERSION master01 Ready master 29m v1.15.1 master02 Ready master 27m v1.15.1 master03 Ready master 26m v1.15.1 worknode01 Ready &lt;none&gt; 25m v1.15.1 worknode02 Ready &lt;none&gt; 3m15s v1.15.1 worknode03 Ready &lt;none&gt; 3m16s v1.15.1</code> </pre> <br><p>  Bisher haben wir keine Anwendungen im Cluster, um HA zu testen.  Wir können jedoch den Betrieb von kubelet auf dem ersten Masterknoten stoppen und sicherstellen, dass unser Cluster betriebsbereit bleibt. </p><br><pre> <code class="plaintext hljs">systemctl stop kubelet &amp;&amp; systemctl stop docker</code> </pre> <br><p>  Überprüfen Sie vom zweiten Masterknoten </p><br><pre> <code class="plaintext hljs">root@master02:~# kubectl get nodes NAME STATUS ROLES AGE VERSION master01 NotReady master 15h v1.15.1 master02 Ready master 15h v1.15.1 master03 Ready master 15h v1.15.1 worknode01 Ready &lt;none&gt; 15h v1.15.1 worknode02 Ready &lt;none&gt; 15h v1.15.1 worknode03 Ready &lt;none&gt; 15h v1.15.1</code> </pre> <br><p>  Alle Knoten funktionieren normal, außer dem, auf dem wir die Dienste gestoppt haben. <br>  Vergessen Sie nicht, die kubernetes-Dienste auf dem ersten Masterknoten zurückzusetzen </p><br><pre> <code class="plaintext hljs">systemctl start kubelet &amp;&amp; systemctl start docker</code> </pre> <br><a name="ingress"></a><br><h2>  Ingress Controller installieren </h2><br><p>  Ingress Controller ist das Kubernetes-Add-On, mit dem wir von außen auf unsere Anwendungen zugreifen können.  Eine ausführliche Beschreibung finden Sie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kuberbnetes-Dokumentation</a> .  Es gibt ziemlich viele Controller, ich verwende einen Controller von Nginx.  Ich werde über die Installation sprechen.  Die Dokumentation zu Betrieb, Konfiguration und Installation des Ingress-Controllers von Nginx finden Sie auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen Website</a> </p><br><p>  Beginnen wir mit der Installation, alle Befehle können mit master01 ausgeführt werden. <br>  Installieren Sie den Controller selbst </p><br><pre> <code class="plaintext hljs">kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml</code> </pre> <br><p>  Und jetzt - ein Service, über den Ingress verfügbar sein wird <br>  Bereiten Sie dazu die Konfiguration vor </p><br><pre> <code class="plaintext hljs">./create-config.sh ingress</code> </pre> <br><p>  Und senden Sie es an unseren Cluster </p><br><pre> <code class="plaintext hljs">kubectl apply -f ingress/service-nodeport.yaml</code> </pre> <br><p>  Überprüfen Sie, ob unser Ingress an den richtigen Adressen arbeitet und die richtigen Ports überwacht. </p><br><pre> <code class="plaintext hljs"># kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 10.99.35.95 10.73.71.25,10.73.71.26,10.73.71.27 80:31669/TCP,443:31604/TCP 10m</code> </pre> <br><pre> <code class="plaintext hljs"> kubectl describe svc -n ingress-nginx ingress-nginx Name: ingress-nginx Namespace: ingress-nginx Labels: app.kubernetes.io/name=ingress-nginx app.kubernetes.io/part-of=ingress-nginx Annotations: kubectl.kubernetes.io/last-applied-configuration: {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app.kubernetes.io/name":"ingress-nginx","app.kubernetes.io/par... Selector: app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/part-of=ingress-nginx Type: NodePort IP: 10.99.35.95 External IPs: 10.73.71.25,10.73.71.26,10.73.71.27 Port: http 80/TCP TargetPort: 80/TCP NodePort: http 31669/TCP Endpoints: 192.168.142.129:80 Port: https 443/TCP TargetPort: 443/TCP NodePort: https 31604/TCP Endpoints: 192.168.142.129:443 Session Affinity: None External Traffic Policy: Cluster Events: &lt;none&gt;</code> </pre> <br><a name="dashboard"></a><br><h2>  Installieren Sie die Web-Benutzeroberfläche (Dashboard) </h2><br><p>  Kubernetes verfügt über eine Standard-Web-Benutzeroberfläche, über die es manchmal bequem ist, den Status eines Clusters oder seiner einzelnen Teile schnell anzuzeigen.  In meiner Arbeit verwende ich häufig das Dashboard für die Erstdiagnose von Bereitstellungen oder den Status von Teilen eines Clusters. <br>  Der Link zur Dokumentation befindet sich auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Website kubernetes</a> <br>  Installation  Ich benutze die stabile Version, ich habe 2.0 noch nicht ausprobiert. </p><br><pre> <code class="plaintext hljs">#  kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml # 2.0 kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta1/aio/deploy/recommended.yaml</code> </pre> <br><p>  Nachdem wir das Panel in unserem Cluster installiert hatten, wurde das Panel unter verfügbar </p><br><pre> <code class="plaintext hljs">http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/.</code> </pre> <br><p>  Um dorthin zu gelangen, müssen wir die Ports vom lokalen Computer mithilfe des kubectl-Proxys weiterleiten.  Für mich ist dieses Schema nicht sehr praktisch.  Daher werde ich den Dienst des Control Panels so ändern, dass das Dashboard unter der Adresse eines beliebigen Clusterknotens an Port 30443 verfügbar wird. Es gibt noch andere Möglichkeiten, auf das Dashboard zuzugreifen, z. B. über Ingress.  Vielleicht werde ich diese Methode in den folgenden Veröffentlichungen betrachten. <br>  Führen Sie zum Ändern des Dienstes die Bereitstellung des geänderten Dienstes aus </p><br><pre> <code class="plaintext hljs">kubectl apply -f dashboard/service-nodeport.yaml</code> </pre> <br><p>  Der Administrator und das Token müssen noch erstellt werden, um über das Dashboard auf den Cluster zuzugreifen </p><br><pre> <code class="plaintext hljs">kubectl apply -f dashboard/rbac.yaml kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')</code> </pre> <br><p>  Danach können Sie sich unter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://10.73.71.25:30443</a> bei der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Systemsteuerung</a> anmelden <br><img src="https://habrastorage.org/webt/p7/zu/8q/p7zu8qv47mwdsmdydtvyo7gs_y4.png"><br>  Dashboard-Startbildschirm <br><img src="https://habrastorage.org/webt/h2/ks/jq/h2ksjq_7egqatf4ulnl0zkwqvqk.png"></p><br><p>  Glückwunsch!  Wenn Sie diesen Schritt erreicht haben, verfügen Sie über einen funktionierenden HA-Cluster von Kubernetes, der für die Bereitstellung Ihrer Anwendungen bereit ist. <br> Kubernetes    ,      .          . <br>       ,  GitHub,    ,    . <br> C ,  . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de462473/">https://habr.com/ru/post/de462473/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de462461/index.html">Ergebnisse der GOES-17-Crash-Untersuchung</a></li>
<li><a href="../de462465/index.html">Verwenden der nativen Orte von Apple</a></li>
<li><a href="../de462467/index.html">Frontend Weekly Digest (29. Juli - 4. August 2019)</a></li>
<li><a href="../de462469/index.html">Einige Überlegungen zum gleichzeitigen Rechnen in R für "Unternehmens" -Aufgaben</a></li>
<li><a href="../de462471/index.html">Lösen eines Jobs mit pwnable.kr 16 - uaf. Verwendung nach freier Sicherheitslücke</a></li>
<li><a href="../de462475/index.html">Alexey Savvateev: Wie man Korruption mit Hilfe der Mathematik bekämpft (Nobelpreis für Wirtschaftswissenschaften für 2016)</a></li>
<li><a href="../de462477/index.html">Wissenschaftler behaupten, AI sei der Autor eines neuen Patents und versuchen, das Patentrecht zu ändern</a></li>
<li><a href="../de462479/index.html">Eskalation der lokalen Berechtigungen des Steam Windows-Clients 0 Tag</a></li>
<li><a href="../de462481/index.html">Geben Sie System-FAQs ein</a></li>
<li><a href="../de462483/index.html">Funktionale Programmierung: Ein verrücktes Spielzeug, das die Arbeitsproduktivität beeinträchtigt. Teil 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>