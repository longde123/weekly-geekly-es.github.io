<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßëüèæ‚Äçü§ù‚Äçüßëüèæ üë©üèº‚Äçüåæ ‚úíÔ∏è Wir entwickeln eine Umgebung f√ºr die Arbeit mit Microservices. Teil 1 Installation von Kubernetes HA auf Bare Metal (Debian) üëá üöà üëéüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo liebe Leser von Habr! 


 Mit dieser Ver√∂ffentlichung m√∂chte ich eine Reihe von Artikeln √ºber die Bereitstellung einer vollwertigen Orchestrieru...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wir entwickeln eine Umgebung f√ºr die Arbeit mit Microservices. Teil 1 Installation von Kubernetes HA auf Bare Metal (Debian)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/462473/"><p><img src="https://habrastorage.org/webt/aq/dx/l3/aqdxl3a5akxfl3rh9b7bzc_kqji.jpeg"></p><br><h4>  Hallo liebe Leser von Habr! </h4><br><p>  Mit dieser Ver√∂ffentlichung m√∂chte ich eine Reihe von Artikeln √ºber die Bereitstellung einer vollwertigen Orchestrierungsumgebung mit Kubernetes-Containern beginnen, die betriebsbereit sind und Anwendungen starten. <br>  Ich m√∂chte nicht nur erl√§utern, wie ein Kubernetes-Cluster bereitgestellt wird, sondern auch, wie ein Cluster nach der Installation konfiguriert wird und wie praktische Tools und Add-Ons hinzugef√ºgt werden, um die Microservice-Architektur zu verwenden. </p><br><h2>  Dieser Zyklus besteht aus mindestens vier Artikeln: </h2><br><ol><li>  Im ersten Teil werde ich Ihnen erkl√§ren, wie Sie einen ausfallsicheren Kubernetes-Cluster auf Bare Iron installieren, ein Standard-Dashboard installieren und den Zugriff darauf konfigurieren sowie einen Ingress-Controller installieren. </li><li>  In einem zweiten Artikel werde ich Ihnen zeigen, wie Sie einen Ceph-Failovercluster bereitstellen und wie Sie RBD-Volumes in unserem Kubernetes-Cluster verwenden.  Ich werde auch ein wenig auf andere Arten von Speichern eingehen (Speicher) und die lokale Speicherung genauer betrachten.  Au√üerdem werde ich Ihnen erkl√§ren, wie Sie einen fehlertoleranten S3-Speicher basierend auf dem erstellten CEPH-Cluster organisieren </li><li>  Im dritten Artikel werde ich beschreiben, wie ein Failover-Cluster MySql in unserem Kubernetes-Cluster bereitgestellt wird, n√§mlich Percona XtraDB-Cluster auf Kubernetes.  Au√üerdem werde ich alle Probleme beschreiben, auf die wir gesto√üen sind, als wir beschlossen haben, die Datenbank auf Kubernetes zu √ºbertragen. </li><li>  Im vierten Artikel werde ich versuchen, alles zusammenzustellen und Ihnen zu erkl√§ren, wie Sie eine Anwendung bereitstellen und ausf√ºhren, die die Datenbank- und Ceph-Volumes verwendet.  Ich werde Ihnen erkl√§ren, wie Sie den Ingress-Controller f√ºr den Zugriff auf unsere Anwendung von au√üen und den automatischen Zertifikatbestelldienst von Let's Encrypt konfigurieren.  Zum anderen k√∂nnen diese Zertifikate automatisch auf dem neuesten Stand gehalten werden.  Wir werden RBAC auch im Zusammenhang mit dem Zugriff auf das Control Panel ansprechen.  Ich erz√§hle Ihnen kurz und b√ºndig von Helm und seiner Installation. <br>  Wenn Sie an den Informationen in diesen Ver√∂ffentlichungen interessiert sind, dann willkommen! <a name="habracut"></a></li></ol><br><h2>  Eintrag: </h2><br><p>  F√ºr wen sind diese Artikel?  Zuallererst f√ºr diejenigen, die gerade ihre Reise in das Studium von Kubernetes beginnen.  Dieser Zyklus ist auch n√ºtzlich f√ºr Ingenieure, die daran denken, von einem Monolithen zu Microservices zu wechseln.  Alles, was beschrieben wird, ist meine Erfahrung, einschlie√ülich derjenigen, die ich bei der √úbersetzung mehrerer Projekte von einem Monolithen nach Kubernetes erhalten habe.  Es ist m√∂glich, dass einige Teile der Ver√∂ffentlichungen f√ºr erfahrene Ingenieure von Interesse sind. </p><br><h4>  Was ich in dieser Reihe von Ver√∂ffentlichungen nicht im Detail betrachten werde: </h4><br><ul><li>  Erkl√§ren Sie ausf√ºhrlich, was Kubernetes-Grundelemente sind, z. B.: Pod, Bereitstellung, Service, Ingress usw. </li><li>  Ich werde CNI (Container Networking Interface) sehr oberfl√§chlich betrachten, wir verwenden callico daher andere L√∂sungen, die ich nur auflisten werde. </li><li>  Docker-Image-Erstellungsprozess. </li><li>  CI \ CD-Prozesse.  (Vielleicht eine separate Ver√∂ffentlichung, aber nach dem gesamten Zyklus) </li><li>  Helm;  Es wurde ziemlich viel √ºber ihn geschrieben. Ich werde nur auf den Prozess der Installation im Cluster und der Einrichtung des Clients eingehen. </li></ul><br><h4>  Was ich im Detail betrachten m√∂chte: </h4><br><ul><li>  Schrittweise schrittweise Bereitstellung des Kubernetes-Clusters.  Ich werde Kubeadm verwenden.  Gleichzeitig werde ich Schritt f√ºr Schritt detailliert auf den Prozess der Installation eines Clusters auf Bare Metal, verschiedene Arten der ETCD-Installation und die Konfiguration von Dateien f√ºr kube admina eingehen.  Ich werde versuchen, alle Ausgleichsoptionen f√ºr den Ingress-Controller und den Unterschied in den verschiedenen Zugriffsschemata von Arbeitsknoten auf die Server-API zu kl√§ren. <br>  Ich wei√ü, dass es heute viele gro√üartige Tools f√ºr die Bereitstellung von Kubernetes gibt, zum Beispiel Kubespray oder denselben Rancher.  Vielleicht ist es f√ºr jemanden bequemer, sie zu benutzen.  Aber ich denke, es gibt viele Ingenieure, die das Problem genauer betrachten m√∂chten. </li><li>  CEPH-Terminologie und schrittweise Installation des CEPH-Clusters sowie schrittweise Anweisungen zum Verbinden des Ceph-Speichers mit dem von Kubernetes erstellten Cluster. </li><li>  lokale Speicher, Verbindung zum Kubernetes-Cluster sowie Unterschiede zu Verbindungen wie Hostpfad usw. </li><li>  kubernetes-Betreiber und die Bereitstellung von Percona XtraDB Cluster mit Hilfe des Betreibers sowie der Versuch, nach sechs Monaten Erfahrung in der Produktion √ºber die Vor- und Nachteile einer solchen L√∂sung zu sprechen.  Und ich werde auch ein paar Pl√§ne f√ºr die Finalisierung des Betreibers von percona teilen. </li></ul><br><h2>  Inhaltsverzeichnis: </h2><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Liste der Hosts, Hostressourcen, Betriebssystem- und Softwareversionen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes Cluster HA-Diagramm</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bevor Sie beginnen oder bevor Sie beginnen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">F√ºllen Sie die Datei create-config.sh aus</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Betriebssystem-Kernel-Update</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vorbereiten von Knoten Installieren von Kubelet, Kubectl, Kubeadm und Docker</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ETCD installieren (verschiedene Optionen)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Starten des ersten Kubernetes-Assistenten</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CNI Callico Installation</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Start des zweiten und dritten Kubernetes-Assistenten</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">F√ºgen Sie dem Cluster einen Arbeitsknoten hinzu</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Installieren Sie Haproxy auf Worker-Knoten f√ºr HA</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ingress Controller installieren</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Installieren Sie die Web-Benutzeroberfl√§che (Dashboard)</a> </li></ol><br><a name="vm"></a><br><h2>  Liste und Ziel der Hosts </h2><br><p>  Alle Knoten meines Clusters befinden sich auf virtuellen Maschinen mit einem vorinstallierten Debian 9-Stretch-System mit dem Kernel 4.19.0-0.bpo.5-amd64.  F√ºr die Virtualisierung verwende ich Proxmox VE. </p><br><h4>  Tabellen-VM und ihre Leistungsmerkmale: </h4><br><div class="scrollable-table"><table><tbody><tr><th>  <b>Name</b> </th><th>  <b>IP-Adresse</b> </th><th>  <b>Kommentar</b> </th><th>  <b>CPU</b> </th><th>  <b>Mem</b> </th><th>  <b>DISK1</b> </th><th>  <b>DISK2</b> </th></tr><tr><td>  master01 </td><td>  10.73.71.25 </td><td>  Hauptknoten </td><td>  4vcpu </td><td>  4 GB </td><td>  Festplatte </td><td>  --- ---. </td></tr><tr><td>  master02 </td><td>  10.73.71.26 </td><td>  Hauptknoten </td><td>  4vcpu </td><td>  4 GB </td><td>  Festplatte </td><td>  --- ---. </td></tr><tr><td>  master03 </td><td>  10.73.71.27 </td><td>  Hauptknoten </td><td>  4vcpu </td><td>  4 GB </td><td>  Festplatte </td><td>  --- ---. </td></tr><tr><td>  worknode01 </td><td>  10.73.75.241 </td><td>  Arbeitsknoten </td><td>  4vcpu </td><td>  4 GB </td><td>  Festplatte </td><td>  SSD </td></tr><tr><td>  worknode02 </td><td>  10.73.75.242 </td><td>  Arbeitsknoten </td><td>  4vcpu </td><td>  4 GB </td><td>  Festplatte </td><td>  SSD </td></tr><tr><td>  worknode03 </td><td>  10.73.75.243 </td><td>  Arbeitsknoten </td><td>  4vcpu </td><td>  4 GB </td><td>  Festplatte </td><td>  SSD </td></tr></tbody></table></div><br><p>  Es ist nicht erforderlich, dass Sie nur eine solche Konfiguration von Maschinen haben m√ºssen, aber ich rate Ihnen dennoch, die Empfehlungen der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen Dokumentation</a> zu befolgen und f√ºr Master die RAM-Gr√∂√üe auf mindestens 4 GB zu erh√∂hen.  Mit Blick auf die Zukunft werde ich sagen, dass ich mit einer kleineren Anzahl St√∂rungen in der Arbeit von CNI Callico festgestellt habe <br>  Ceph ist auch in Bezug auf Speicher- und Festplattenleistung ziemlich uners√§ttlich. <br>  Unsere Produktionsinstallationen funktionieren ohne Bare-Metal-Virtualisierung, aber ich kenne viele Beispiele, bei denen virtuelle Maschinen mit relativ bescheidenen Ressourcen ausreichten.  Es h√§ngt alles von Ihren Bed√ºrfnissen und Arbeitslasten ab. </p><br><h2>  Listen- und Softwareversionen </h2><br><div class="scrollable-table"><table><tbody><tr><th>  <b>Name</b> </th><th>  <b>Version</b> </th></tr><tr><td>  Kubernetes </td><td>  1.15.1 </td></tr><tr><td>  Docker </td><td>  19.3.1 </td></tr></tbody></table></div><br><p>  Ab Version 1.14 hat Kubeadm die Unterst√ºtzung der v1alpha3-API-Version eingestellt und vollst√§ndig auf die v1beta1-API-Version umgestellt, die in naher Zukunft unterst√ºtzt wird. In diesem Artikel werde ich daher nur auf v1beta1 eingehen. <br>  Wir glauben, dass Sie die Maschinen f√ºr den Kubernetes-Cluster vorbereitet haben.  Sie sind alle √ºber das Netzwerk f√ºr einander zug√§nglich, haben eine ‚ÄûInternetverbindung‚Äú zum Internet und ein ‚Äûsauberes‚Äú Betriebssystem ist auf ihnen installiert. <br>  F√ºr jeden Installationsschritt werde ich kl√§ren, auf welchen Maschinen der Befehl oder Befehlsblock ausgef√ºhrt wird.  F√ºhren Sie alle Befehle als root aus, sofern nicht anders angegeben. <br>  Alle Konfigurationsdateien sowie ein Skript zur Vorbereitung stehen in meinem <a href="">Github</a> zum Download zur Verf√ºgung <br>  Also fangen wir an. </p><br><a name="ha-image"></a><br><h2>  Kubernetes Cluster HA-Diagramm </h2><br><p><img src="https://habrastorage.org/webt/ch/mx/pg/chmxpgzdyk0bvxwx7-6lawqfmvo.jpeg"><br>  Ein ungef√§hres Diagramm des HA-Clusters.  Der K√ºnstler von mir ist, um ehrlich zu sein, so lala, aber ich werde versuchen, es kurz und b√ºndig zu erkl√§ren, ohne mich besonders mit der Theorie zu befassen. <br>  Unser Cluster wird also aus drei Masterknoten und drei Arbeiterknoten bestehen.  Auf jedem Kubernetes-Masterknoten funktionieren etcd (gr√ºne Pfeile im Diagramm) und kubernetes-Serviceteile f√ºr uns.  Nennen wir sie generisch - Kubeapi. <br>  √úber den Mastercluster etcd tauschen Knoten den Status des Kubernetes-Clusters aus.  Ich werde die gleichen Adressen wie die Einstiegspunkte des Eingangscontrollers f√ºr den externen Verkehr angeben (rote Pfeile im Diagramm). <br>  Auf Worker-Knoten arbeitet kubelet f√ºr uns, das √ºber haproxy, das lokal auf jedem Worker-Knoten installiert ist, mit dem kubernetes-API-Server kommuniziert.  Als Server-API-Adresse f√ºr kubelet verwende ich den localhost 127.0.0.1:6443, und haproxy on roundrobin verteilt Anforderungen auf drei Masterknoten und √ºberpr√ºft auch die Verf√ºgbarkeit der Masterknoten.  Mit diesem Schema k√∂nnen wir HA erstellen. Im Falle eines Ausfalls eines der Masterknoten senden die Arbeiterknoten leise Anforderungen an die beiden verbleibenden Masterknoten. </p><br><a name="begin"></a><br><h2>  Bevor Sie beginnen </h2><br><p>  Bevor wir mit der Arbeit an jedem Knoten des Clusters beginnen, stellen wir die Pakete bereit, die wir f√ºr die Arbeit ben√∂tigen: </p><br><pre><code class="plaintext hljs">apt-get update &amp;&amp; apt-get install -y curl apt-transport-https git</code> </pre> <br><p>  Kopieren Sie auf den Masterknoten das Repository mit Konfigurationsvorlagen </p><br><pre> <code class="plaintext hljs">sudo -i git clone https://github.com/rjeka/kubernetes-ceph-percona.git</code> </pre> <br><p>  √úberpr√ºfen Sie, ob die IP-Adresse der Hosts auf den Assistenten mit der √ºbereinstimmt, die der kubernetes-Server abh√∂rt </p><br><pre> <code class="plaintext hljs">hostname &amp;&amp; hostname -i master01 10.73.71.25</code> </pre> <br><p>  und so f√ºr alle Masterknoten. </p><br><p>  Deaktivieren Sie unbedingt SWAP, da sonst kubeadm einen Fehler ausl√∂st </p><br><pre> <code class="plaintext hljs">[ERROR Swap]: running with swap on is not supported. Please disable swap</code> </pre> <br><p>  Sie k√∂nnen den Befehl deaktivieren </p><br><pre> <code class="plaintext hljs">swapoff -a</code> </pre> <br><p>  Denken Sie daran, in / etc / fstab zu kommentieren </p><br><a name="create-config"></a><br><h2>  F√ºllen Sie die Datei create-config.sh aus </h2><br><p>  Um die f√ºr die Installation des kubernetes-Clusters erforderlichen Konfigurationen automatisch auszuf√ºllen, habe ich ein kleines Skript create-config.sh hochgeladen.  Sie m√ºssen es buchst√§blich 8 Zeilen ausf√ºllen.  Geben Sie die IP-Adressen und den Hostnamen Ihrer Master an.  Und geben Sie auch etcd tocken an, Sie k√∂nnen es nicht √§ndern.  Ich werde unten den Teil des Skripts angeben, in dem Sie √Ñnderungen vornehmen m√ºssen. </p><br><pre> <code class="plaintext hljs">#!/bin/bash ####################################### # all masters settings below must be same ####################################### # master01 ip address export K8SHA_IP1=10.73.71.25 # master02 ip address export K8SHA_IP2=10.73.71.26 # master03 ip address export K8SHA_IP3=10.73.71.27 # master01 hostname export K8SHA_HOSTNAME1=master01 # master02 hostname export K8SHA_HOSTNAME2=master02 # master03 hostname export K8SHA_HOSTNAME3=master03 #etcd tocken: export ETCD_TOKEN=9489bf67bdfe1b3ae077d6fd9e7efefd #etcd version export ETCD_VERSION="v3.3.10"</code> </pre><br><a name="kernel"></a><br><h2>  Betriebssystem-Kernel-Update </h2><br><p>  Dieser Schritt ist optional, da der Kernel √ºber die Backports aktualisiert werden muss und Sie dies auf eigene Gefahr und Gefahr tun.  M√∂glicherweise tritt dieses Problem nie auf. In diesem Fall k√∂nnen Sie den Kernel auch nach der Bereitstellung von Kubernetes aktualisieren.  Im Allgemeinen entscheiden Sie. <br>  Ein Kernel-Update ist erforderlich, um den alten Docker-Fehler zu beheben, der nur in der Linux-Kernel-Version 4.18 behoben wurde.  Sie k√∂nnen hier mehr √ºber diesen Fehler lesen.  Ein Fehler wurde im periodischen H√§ngen der Netzwerkschnittstelle auf den Kubernetes-Knoten mit dem Fehler ausgedr√ºckt: </p><br><pre> <code class="plaintext hljs">waiting for eth0 to become free. Usage count = 1</code> </pre> <br><p>  Nach der Installation des Betriebssystems hatte ich die Kernel-Version 4.9 </p><br><pre> <code class="bash hljs">uname -a Linux master01 4.9.0-7-amd64 <span class="hljs-comment"><span class="hljs-comment">#1 SMP Debian 4.9.110-3+deb9u2 (2018-08-13) x86_64 GNU/Linux</span></span></code> </pre> <br><p>  Auf jeder Maschine f√ºr Kubernetes f√ºhren wir aus <br>  Schritt 1 <br>  F√ºgen Sie der Quellliste Back-Ports hinzu </p><br><pre> <code class="plaintext hljs">echo deb http://ftp.debian.org/debian stretch-backports main &gt; /etc/apt/sources.list apt-get update apt-cache policy linux-compiler-gcc-6-x86</code> </pre> <br><p>  Schritt Nummer 2 <br>  Paketinstallation </p><br><pre> <code class="plaintext hljs">apt install -y -t stretch-backports linux-image-amd64 linux-headers-amd64</code> </pre> <br><p>  Schritt Nummer 3 <br>  Starten Sie neu </p><br><pre> <code class="plaintext hljs">reboot</code> </pre> <br><p>  √úberpr√ºfen Sie, ob alles in Ordnung ist </p><br><pre> <code class="plaintext hljs">uname -a Linux master01 4.19.0-0.bpo.5-amd64 #1 SMP Debian 4.19.37-4~bpo9+1 (2019-06-19) x86_64 GNU/Linux</code> </pre><br><a name="kubelet"></a><br><h2>  Vorbereiten von Knoten Installieren von Kubelet, Kubectl, Kubeadm und Docker </h2><br><h4>  Installieren Sie Kubelet, Kubectl, Kubeadm </h4><br><p>  Wir setzen alle Knoten des Clusters gem√§√ü der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation von Kubernetes ein</a> </p><br><pre> <code class="plaintext hljs">apt-get update &amp;&amp; apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl apt-mark hold kubelet kubeadm kubectl</code> </pre> <br><h4>  Installieren Sie Docker </h4><br><p>  Installieren Sie Docker gem√§√ü den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Anweisungen in der Dokumentation</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><br></a> </p><br><pre> <code class="plaintext hljs">apt-get remove docker docker-engine docker.io containerd runc apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common</code> </pre> <br><pre> <code class="plaintext hljs">curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add - apt-key fingerprint 0EBFCD88</code> </pre> <br><pre> <code class="plaintext hljs">add-apt-repository \ "deb [arch=amd64] https://download.docker.com/linux/debian \ $(lsb_release -cs) \ stable"</code> </pre> <br><pre> <code class="plaintext hljs">apt-get update apt-get install docker-ce docker-ce-cli containerd.io</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Installation Installation von Kubelet, Kubectl, Kubeadm und Docker mit ansible</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">git clone https://github.com/rjeka/kubernetes-ceph-percona.git cd kubernetes-ceph-percona/playbooks vim masters.ini</code> </pre> <br><p>  Registrieren Sie in der Master-Gruppe die IP-Master. <br>  Schreiben Sie in der Worker-Gruppe die IP-Adresse der Arbeitsknoten. </p><br><pre> <code class="plaintext hljs"># sudo c  ansible-playbook -i hosts.ini kubelet.yaml -K ansible-playbook -i hosts.ini docker.yaml -K # sudo  ansible-playbook -i hosts.ini kubelet.yaml ansible-playbook -i hosts.ini docker.yaml</code> </pre> </div></div><br><p>  Wenn Sie aus irgendeinem Grund kein Docker verwenden m√∂chten, k√∂nnen Sie ein beliebiges CRI verwenden.  Sie k√∂nnen zum Beispiel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> dar√ºber lesen, aber dieses Thema geht √ºber den Rahmen dieses Artikels hinaus. </p><br><a name="etcd"></a><br><h2>  ETCD-Installation </h2><br><p>  Ich werde nicht kurz auf die Theorie eingehen: etcd ist ein Open-Source-verteilter Schl√ºsselwertspeicher.  etcd ist in GO geschrieben und wird in Kubernetes tats√§chlich als Datenbank zum Speichern des Clusterstatus verwendet.  Eine ausf√ºhrlichere √úbersicht finden Sie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation zu kubernetes</a> . <br>  etcd kann auf viele Arten installiert werden.  Sie k√∂nnen es lokal installieren und als Daemon ausf√ºhren, Sie k√∂nnen es in Docker-Containern ausf√ºhren, Sie k√∂nnen es sogar als Kubernetes-Bottom installieren.  Sie k√∂nnen es von Hand oder mit kubeadm installieren (ich habe diese Methode nicht ausprobiert).  Kann auf Cluster-Computern oder einzelnen Servern installiert werden. <br>  Ich werde etcd lokal auf den Masterknoten installieren und als Daemon √ºber systemd ausf√ºhren sowie die Installation in Docker in Betracht ziehen.  Ich verwende etcd ohne TLS. Wenn Sie TLS ben√∂tigen, lesen Sie die Dokumentation zu etcd oder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kubernetes</a> <br>  Auch in meinem Github wird Ansible-Playbook f√ºr die Installation von etcd mit Start √ºber systemd sein. </p><br><h4>  Option Nummer 1 <br>  Lokal installieren, √ºber systemd ausf√ºhren </h4><br><p>  Auf allen Mastern: (Auf den Arbeitsknoten des Clusters ist dieser Schritt nicht erforderlich.) <br>  Schritt 1 <br>  Laden Sie das Archiv herunter und entpacken Sie es mit etcd: </p><br><pre> <code class="plaintext hljs">mkdir archives cd archives export etcdVersion=v3.3.10 wget https://github.com/coreos/etcd/releases/download/$etcdVersion/etcd-$etcdVersion-linux-amd64.tar.gz tar -xvf etcd-$etcdVersion-linux-amd64.tar.gz -C /usr/local/bin/ --strip-components=1</code> </pre> <br><p>  Schritt Nummer 2 <br>  Erstellen Sie eine Konfigurationsdatei f√ºr ETCD </p><br><pre> <code class="plaintext hljs">cd .. ./create-config.sh etcd</code> </pre> <br><p>  Das Skript verwendet den Wert etcd als Eingabe und generiert eine Konfigurationsdatei im Verzeichnis etcd.  Nachdem das Skript ausgef√ºhrt wurde, befindet sich die fertige Konfigurationsdatei im Verzeichnis etcd. <br>  Bei allen anderen Konfigurationen funktioniert das Skript nach dem gleichen Prinzip.  Es nimmt einige Eingaben entgegen und erstellt eine Konfiguration in einem bestimmten Verzeichnis. </p><br><p>  Schritt Nummer 3 <br>  Wir starten den etcd-Cluster und √ºberpr√ºfen seine Leistung </p><br><pre> <code class="plaintext hljs">systemctl start etcd</code> </pre> <br><p>  √úberpr√ºfen der Leistung des D√§mons </p><br><pre> <code class="plaintext hljs">systemctl status etcd ‚óè etcd.service - etcd Loaded: loaded (/etc/systemd/system/etcd.service; disabled; vendor preset: enabled) Active: active (running) since Sun 2019-07-07 02:34:28 MSK; 4min 46s ago Docs: https://github.com/coreos/etcd Main PID: 7471 (etcd) Tasks: 14 (limit: 4915) CGroup: /system.slice/etcd.service ‚îî‚îÄ7471 /usr/local/bin/etcd --name master01 --data-dir /var/lib/etcd --listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 --advertise-client-urls http://10.73.71.25:2379,http://10.73.71. Jul 07 02:34:28 master01 etcd[7471]: b11e73358a31b109 [logterm: 1, index: 3, vote: 0] cast MsgVote for f67dd9aaa8a44ab9 [logterm: 2, index: 5] at term 554 Jul 07 02:34:28 master01 etcd[7471]: raft.node: b11e73358a31b109 elected leader f67dd9aaa8a44ab9 at term 554 Jul 07 02:34:28 master01 etcd[7471]: published {Name:master01 ClientURLs:[http://10.73.71.25:2379 http://10.73.71.25:4001]} to cluster d0979b2e7159c1e6 Jul 07 02:34:28 master01 etcd[7471]: ready to serve client requests Jul 07 02:34:28 master01 etcd[7471]: serving insecure client requests on [::]:4001, this is strongly discouraged! Jul 07 02:34:28 master01 systemd[1]: Started etcd. Jul 07 02:34:28 master01 etcd[7471]: ready to serve client requests Jul 07 02:34:28 master01 etcd[7471]: serving insecure client requests on [::]:2379, this is strongly discouraged! Jul 07 02:34:28 master01 etcd[7471]: set the initial cluster version to 3.3 Jul 07 02:34:28 master01 etcd[7471]: enabled capabilities for version 3.3 lines 1-19</code> </pre> <br><p>  Und die Gesundheit des Clusters selbst: </p><br><pre> <code class="plaintext hljs">etcdctl cluster-health member 61db137992290fc is healthy: got healthy result from http://10.73.71.27:2379 member b11e73358a31b109 is healthy: got healthy result from http://10.73.71.25:2379 member f67dd9aaa8a44ab9 is healthy: got healthy result from http://10.73.71.26:2379 cluster is healthy etcdctl member list 61db137992290fc: name=master03 peerURLs=http://10.73.71.27:2380 clientURLs=http://10.73.71.27:2379,http://10.73.71.27:4001 isLeader=false b11e73358a31b109: name=master01 peerURLs=http://10.73.71.25:2380 clientURLs=http://10.73.71.25:2379,http://10.73.71.25:4001 isLeader=false f67dd9aaa8a44ab9: name=master02 peerURLs=http://10.73.71.26:2380 clientURLs=http://10.73.71.26:2379,http://10.73.71.26:4001 isLeader=true</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Installieren Sie etcd lokal mit ansible und f√ºhren Sie es √ºber systemd aus</b> <div class="spoiler_text"><p>  Mit github klonen wir das Repository mit dem Code auf den Computer, von dem aus Sie das Playbook ausf√ºhren.  Diese Maschine sollte SSH-Zugriff auf einen Schl√ºssel zum Master unseres zuk√ºnftigen Clusters haben. </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/rjeka/kubernetes-ceph-percona.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> kubernetes-ceph-percona/playbooks vim masters.ini</code> </pre> <br><p>  Registrieren Sie in der Master-Gruppe die IP-Master. <br>  etcd_version ist die Version von etcd.  Sie k√∂nnen es auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">etcd-Seite in github sehen</a> .  Zum Zeitpunkt des Schreibens gab es Version v3.3.13, die ich v3.3.10 verwende. <br>  etcdToken - Sie k√∂nnen dasselbe verlassen oder Ihr eigenes generieren. <br>  F√ºhren Sie das Playbook-Team </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># sudo c  ansible-playbook -i hosts.ini -l masters etcd.yaml -K BECOME password: &lt;sudo &gt; # sudo  ansible-playbook -i hosts.ini -l masters etcd.yaml</span></span></code> </pre> </div></div><br><p>  Wenn Sie etcd im Docker ausf√ºhren m√∂chten, befindet sich unter dem Spoiler eine Anweisung. </p><br><div class="spoiler">  <b class="spoiler_title">Installieren Sie etcd mit Docker-Compose und starten Sie es in Docker</b> <div class="spoiler_text"><p>  Diese Befehle m√ºssen auf allen Cluster-Masterknoten ausgef√ºhrt werden. <br>  Mit github klonen wir das Repository mit Code </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/rjeka/kubernetes-ceph-percona.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> kubernetes-ceph-percona</code> </pre> <br><p>  etcd_version ist die Version von etcd.  Sie k√∂nnen es auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">etcd-Seite in github sehen</a> .  Zum Zeitpunkt des Schreibens gab es Version v3.3.13, die ich v3.3.10 verwende. <br>  etcdToken - Sie k√∂nnen dasselbe verlassen oder Ihr eigenes generieren. </p><br><p>  Wir setzen Docker-Compose </p><br><pre> <code class="plaintext hljs">apt-get install -y docker-compose</code> </pre> <br><p>  Wir generieren eine Konfiguration </p><br><pre> <code class="plaintext hljs">./create-config.sh docker</code> </pre> <br><p>  F√ºhren Sie die Installation des etcd-Clusters im Docker aus </p><br><pre> <code class="plaintext hljs">docker-compose --file etcd-docker/docker-compose.yaml up -d</code> </pre> <br><p>  √úberpr√ºfen Sie, ob die Beh√§lter leer sind </p><br><pre> <code class="plaintext hljs">docker ps</code> </pre> <br><p>  Und Clusterstatus etcd </p><br><pre> <code class="plaintext hljs">root@master01:~/kubernetes-ceph-percona# docker exec -ti etcd etcdctl cluster-health member 61db137992290fc is healthy: got healthy result from http://10.73.71.27:2379 member b11e73358a31b109 is healthy: got healthy result from http://10.73.71.25:2379 member f67dd9aaa8a44ab9 is healthy: got healthy result from http://10.73.71.26:2379 cluster is healthy root@master01:~/kubernetes-ceph-percona# docker exec -ti etcd etcdctl member list 61db137992290fc: name=etcd3 peerURLs=http://10.73.71.27:2380 clientURLs=http://10.73.71.27:2379,http://10.73.71.27:4001 isLeader=false b11e73358a31b109: name=etcd1 peerURLs=http://10.73.71.25:2380 clientURLs=http://10.73.71.25:2379,http://10.73.71.25:4001 isLeader=true f67dd9aaa8a44ab9: name=etcd2 peerURLs=http://10.73.71.26:2380 clientURLs=http://10.73.71.26:2379,http://10.73.71.26:4001 isLeader=false</code> </pre> <br><p>  Wenn etwas schief gelaufen ist </p><br><pre> <code class="plaintext hljs">docker logs etcd</code> </pre> </div></div><br><a name="master-one"></a><br><h2>  Starten des ersten Kubernetes-Assistenten </h2><br><p>  Zun√§chst m√ºssen wir eine Konfiguration f√ºr kubeadmin generieren </p><br><pre> <code class="plaintext hljs">./create-config.sh kubeadm</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Wir zerlegen eine Konfiguration f√ºr kubeadm</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: kubeadm.k8s.io/v1beta1 kind: InitConfiguration localAPIEndpoint: advertiseAddress: 10.73.71.25 #    API- --- apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration kubernetesVersion: stable #      apiServer: #    kubeadm   certSANs: - 127.0.0.1 - 10.73.71.25 - 10.73.71.26 - 10.73.71.27 controlPlaneEndpoint: 10.73.71.25 #     etcd: #  etc external: endpoints: - http://10.73.71.25:2379 - http://10.73.71.26:2379 - http://10.73.71.27:2379 networking: podSubnet: 192.168.0.0/16 #   ,   CNI  .</code> </pre> <br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Informationen zu</a> CNI-Subnetzen finden Sie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation zu kubernetes.</a> <br>  Dies ist eine minimal funktionierende Konfiguration.  F√ºr einen Cluster mit drei Assistenten k√∂nnen Sie ihn in die Konfiguration Ihres Clusters √§ndern.  Wenn Sie beispielsweise zwei Assistenten verwenden m√∂chten, geben Sie einfach zwei Adressen in certSANs an. <br>  Alle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Konfigurationsparameter</a> finden Sie in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Beschreibung der kubeadm-API</a> . </p></div></div><br><p>  Wir initiieren den ersten Meister </p><br><pre> <code class="plaintext hljs">kubeadm init --config=kubeadmin/kubeadm-init.yaml</code> </pre> <br><p>  Wenn kubeadm fehlerfrei funktioniert, erhalten wir am Ausgang ungef√§hr den folgenden Ausgang: </p><br><pre> <code class="plaintext hljs">You can now join any number of control-plane nodes by copying certificate authorities and service account keys on each node and then running the following as root: kubeadm join 10.73.71.25:6443 --token ivwoap.259retezqf34amx8 \ --discovery-token-ca-cert-hash sha256:b5c93e32457c8e6478782ff62e8ef77acf72738dda59cd603cdf4821abe12ca3 \ --control-plane Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10.73.71.25:6443 --token ivwoap.259retezqf34amx8 \ --discovery-token-ca-cert-hash sha256:b5c93e32457c8e6478782ff62e8ef77acf72738dda59cd603cdf4821abe12ca3</code> </pre> <br><a name="callica"></a><br><h2>  CNI Calico Installation </h2><br><p>  Es ist an der Zeit, ein Netzwerk aufzubauen, in dem unsere Pods arbeiten werden.  Ich benutze Kaliko, und wir werden es sagen. <br>  Konfigurieren Sie zun√§chst den Zugriff f√ºr Kubelet.  Wir f√ºhren alle Befehle auf master01 aus <br>  Wenn Sie als root ausgef√ºhrt werden </p><br><pre> <code class="plaintext hljs">export KUBECONFIG=/etc/kubernetes/admin.conf</code> </pre> <br><p>  Wenn unter dem einfachen Benutzer </p><br><pre> <code class="plaintext hljs">mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config</code> </pre> <br><p>  Sie k√∂nnen den Cluster auch von Ihrem Laptop oder einem beliebigen lokalen Computer aus verwalten.  Kopieren Sie dazu die Datei /etc/kubernetes/admin.conf auf Ihren Laptop oder einen anderen Computer in $ HOME / .kube / config </p><br><p>  Wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">setzen</a> CNI <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gem√§√ü der Kubernetes-Dokumentation ein</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><br></a> </p><br><pre> <code class="plaintext hljs">kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml</code> </pre> <br><p>  Wir warten, bis alle Schoten aufgehen </p><br><pre> <code class="plaintext hljs">watch -n1 kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-59f54d6bbc-psr2z 1/1 Running 0 96s kube-system calico-node-hm49z 1/1 Running 0 96s kube-system coredns-5c98db65d4-svcx9 1/1 Running 0 77m kube-system coredns-5c98db65d4-zdlb8 1/1 Running 0 77m kube-system kube-apiserver-master01 1/1 Running 0 76m kube-system kube-controller-manager-master01 1/1 Running 0 77m kube-system kube-proxy-nkdqn 1/1 Running 0 77m kube-system kube-scheduler-master01 1/1 Running 0 77m</code> </pre> <br><a name="mastes-other"></a><br><h2>  Start des zweiten und dritten Kubernetes-Assistenten </h2><br><p>  Bevor Sie master02 und master03 starten, m√ºssen Sie die Zertifikate mit master01 kopieren, die kubeadm beim Erstellen des Clusters generiert hat.  Ich werde √ºber scp kopieren <br>  Auf master01 </p><br><pre> <code class="plaintext hljs">export master02=10.73.71.26 export master03=10.73.71.27 scp -r /etc/kubernetes/pki $master02:/etc/kubernetes/ scp -r /etc/kubernetes/pki $master03:/etc/kubernetes/</code> </pre> <br><p>  Auf master02 und master03 <br>  Erstellen Sie eine Konfiguration f√ºr kubeadm </p><br><pre> <code class="plaintext hljs">./create-config.sh kubeadm</code> </pre> <br><p>  F√ºgen Sie dem Cluster master02 und master03 hinzu </p><br><pre> <code class="plaintext hljs">kubeadm init --config=kubeadmin/kubeadm-init.yaml</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">St√∂rungen an mehreren Netzwerkschnittstellen !!!!</b> <div class="spoiler_text"><p>  In der Produktion verwende ich kubernetes v1.13.5 und calico v3.3.  Und ich hatte keine solchen Pannen. <br>  Bei der Vorbereitung des Artikels und der Verwendung der stabilen Version (zum Zeitpunkt des Schreibens waren dies v1.15.1 kubernetes und Version 3.8 callico) stie√ü ich auf ein Problem, das in CNI-Startfehlern ausgedr√ºckt wurde </p><br><pre> <code class="plaintext hljs">root@master01:~/kubernetes-ceph-percona# kubectl get pods -A -w NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-658558ddf8-t6gfs 0/1 ContainerCreating 0 11s kube-system calico-node-7td8g 1/1 Running 0 11s kube-system calico-node-dthg5 0/1 CrashLoopBackOff 1 11s kube-system calico-node-tvhkq 0/1 CrashLoopBackOff 1 11s</code> </pre> <br><p>  Dies ist ein Fehler im Calico-Daemon-Set, wenn der Server √ºber mehrere Netzwerkschnittstellen verf√ºgt <br>  Auf Githab gibt es ein Problem mit dieser Panne <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://github.com/projectcalico/calico/issues/2720</a> <br>  Es wird gel√∂st, indem der Daemon-Set calico-node bearbeitet und der Parameter IP_AUTODETECTION_METHOD zu env hinzugef√ºgt wird </p><br><pre> <code class="plaintext hljs">kubectl edit -n kube-system ds calico-node</code> </pre> <br><p>  F√ºgen Sie den Parameter IP_AUTODETECTION_METHOD mit dem Namen Ihrer Schnittstelle hinzu, an der der Assistent arbeitet.  in meinem Fall ist es ens19 </p><br><pre> <code class="plaintext hljs">- name: IP_AUTODETECTION_METHOD value: ens19</code> </pre> <br><p><img src="https://habrastorage.org/webt/xe/pq/7h/xepq7hbpjwhgowfk0hkc6-ryw8a.png"><br>  √úberpr√ºfen Sie, ob alle Knoten im Cluster aktiv sind </p><br><pre> <code class="plaintext hljs"># kubectl get nodes NAME STATUS ROLES AGE VERSION master01 Ready master 28m v1.15.1 master02 Ready master 26m v1.15.1 master03 Ready master 18m v1.15.1</code> </pre> <br><p>  Und was ist Calica lebendig </p><br><pre> <code class="plaintext hljs"># kubectl get pods -A -o wide | grep calico kube-system calico-kube-controllers-59f54d6bbc-5lxgn 1/1 Running 0 27m kube-system calico-node-fngpz 1/1 Running 1 24m kube-system calico-node-gk7rh 1/1 Running 0 8m55s kube-system calico-node-w4xtt 1/1 Running 0 25m</code> </pre> </div></div><br><a name="worknodes"></a><br><h2>  F√ºgen Sie dem Cluster Arbeitsknoten hinzu </h2><br><p>  Im Moment haben wir einen Cluster, in dem drei Masterknoten ausgef√ºhrt werden.  Masterknoten sind jedoch Computer, auf denen API, Scheduler und andere Dienste des Kubernetes-Clusters ausgef√ºhrt werden.  Damit wir unsere Pods ausf√ºhren k√∂nnen, ben√∂tigen wir die sogenannten Worker-Knoten. <br>  Wenn Sie nur √ºber begrenzte Ressourcen verf√ºgen, k√∂nnen Sie Pods auf Masterknoten ausf√ºhren. Ich pers√∂nlich rate jedoch nicht dazu. </p><br><div class="spoiler">  <b class="spoiler_title">Ausf√ºhren der Herd-Masterknoten</b> <div class="spoiler_text"><p>  F√ºhren Sie den folgenden Befehl auf einem der Assistenten aus, um das Starten von Herden auf Masterknoten zu erm√∂glichen </p><br><pre> <code class="plaintext hljs">kubectl taint nodes --all node-role.kubernetes.io/master-</code> </pre> </div></div><br><p>  Installieren Sie die Knoten kubelet, kubeadm, kubectl und docker auf dem Worker wie auf den Masterknoten </p><br><div class="spoiler">  <b class="spoiler_title">Installieren Sie kubelet, kubeadm, kubectl und docker</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apt-get update &amp;&amp; apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl apt-mark hold kubelet kubeadm kubectl</code> </pre> <br><h4>  Installieren Sie Docker </h4><br><p>  Installieren Sie Docker gem√§√ü den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Anweisungen in der Dokumentation</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><br></a> </p><br><pre> <code class="plaintext hljs">apt-get remove docker docker-engine docker.io containerd runc apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common</code> </pre> <br><pre> <code class="plaintext hljs">curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add - apt-key fingerprint 0EBFCD88</code> </pre> <br><pre> <code class="plaintext hljs">add-apt-repository \ "deb [arch=amd64] https://download.docker.com/linux/debian \ $(lsb_release -cs) \ stable"</code> </pre> <br><pre> <code class="plaintext hljs">apt-get update apt-get install docker-ce docker-ce-cli containerd.io</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Installation Installation von Kubelet, Kubectl, Kubeadm und Docker mit ansible</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">git clone https://github.com/rjeka/kubernetes-ceph-percona.git cd kubernetes-ceph-percona/playbooks vim masters.ini</code> </pre> <br><p>  Registrieren Sie in der Master-Gruppe die IP-Master. <br>  Schreiben Sie in der Worker-Gruppe die IP-Adresse der Arbeitsknoten. </p><br><pre> <code class="plaintext hljs"># sudo c  ansible-playbook -i hosts.ini kubelet.yaml -K ansible-playbook -i hosts.ini docker.yaml -K # sudo  ansible-playbook -i hosts.ini kubelet.yaml ansible-playbook -i hosts.ini docker.yaml</code> </pre> </div></div></div></div><br><p>  Jetzt ist es Zeit, zu der Zeile zur√ºckzukehren, die kubeadm bei der Installation des Masterknotens generiert hat. <br>  Sie sieht f√ºr mich so aus. </p><br><pre> <code class="plaintext hljs">kubeadm join 10.73.71.25:6443 --token ivwoap.259retezqf34amx8 \ --discovery-token-ca-cert-hash sha256:b5c93e32457c8e6478782ff62e8ef77acf72738dda59cd603cdf4821abe12ca3</code> </pre> <br><p>  Dieser Befehl muss auf jedem Arbeitsknoten ausgef√ºhrt werden. <br>  Wenn Sie kein Token geschrieben haben, k√∂nnen Sie ein neues generieren </p><br><pre> <code class="plaintext hljs">kubeadm token create --print-join-command --ttl=0</code> </pre> <br><p>  Nachdem kubeadm funktioniert hat, wird Ihr neuer Knoten in den Cluster eingegeben und ist arbeitsbereit </p><br><pre> <code class="plaintext hljs">This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster.</code> </pre> <br><p>  Schauen wir uns nun das Ergebnis an </p><br><pre> <code class="plaintext hljs">root@master01:~# kubectl get nodes NAME STATUS ROLES AGE VERSION master01 Ready master 10d v1.15.1 master02 Ready master 10d v1.15.1 master03 Ready master 10d v1.15.1 worknode01 Ready &lt;none&gt; 5m44s v1.15.1 worknode02 Ready &lt;none&gt; 59s v1.15.1 worknode03 Ready &lt;none&gt; 51s v1.15.1</code> </pre> <br><a name="haproxy"></a><br><h2>  Installieren Sie haproxy auf Arbeitsknoten </h2><br><p>  Jetzt haben wir einen Arbeitscluster mit drei Hauptknoten und drei Arbeitsknoten. <br>  Das Problem ist, dass unsere Worker-Knoten jetzt keinen HA-Modus haben. <br>  Wenn Sie sich die Kubelet-Konfigurationsdatei ansehen, werden Sie feststellen, dass unsere Worker-Knoten nur auf einen der drei Master-Knoten zugreifen. </p><br><pre> <code class="plaintext hljs">root@worknode01:~# cat /etc/kubernetes/kubelet.conf | grep server: server: https://10.73.71.27:6443</code> </pre> <br><p>  In meinem Fall ist dies master03.  Bei dieser Konfiguration verliert der Worker-Knoten bei einem Absturz von master03 die Kommunikation mit dem Cluster-API-Server.  Um unseren Cluster vollst√§ndig HA zu machen, installieren wir auf jedem der Worker einen Load Balancer (Haproxy), der laut Round Robin Anforderungen f√ºr drei Masterknoten verteilt, und in der Kubelet-Konfiguration auf Worker-Knoten √§ndern wir die Serveradresse in 127.0.0.1:6443 <br>  Installieren Sie zun√§chst HAProxy auf jedem Arbeitsknoten. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Es gibt einen guten Spickzettel f√ºr die Installation</a> </p><br><pre> <code class="plaintext hljs">curl https://haproxy.debian.net/bernat.debian.org.gpg | \ apt-key add - echo deb http://haproxy.debian.net stretch-backports-2.0 main | \ tee /etc/apt/sources.list.d/haproxy.list apt-get update apt-get install haproxy=2.0.\*</code> </pre> <br><p>  Nach der Installation von HAproxy m√ºssen wir eine Konfiguration daf√ºr erstellen. <br>  Wenn auf den Worker-Knoten kein Verzeichnis mit Konfigurationsdateien vorhanden ist, klonen wir es </p><br><pre> <code class="plaintext hljs">git clone https://github.com/rjeka/kubernetes-ceph-percona.git cd kubernetes-ceph-percona/</code> </pre> <br><p>  F√ºhren Sie das Konfigurationsskript mit dem Haproxy-Flag aus </p><br><pre> <code class="plaintext hljs">./create-config.sh haproxy</code> </pre> <br><p>  Das Skript konfiguriert haproxy und startet es neu. <br>  √úberpr√ºfen Sie, ob Haproxy Port 6443 abh√∂rt. </p><br><pre> <code class="plaintext hljs">root@worknode01:~/kubernetes-ceph-percona# netstat -alpn | grep 6443 tcp 0 0 127.0.0.1:6443 0.0.0.0:* LISTEN 30675/haproxy tcp 0 0 10.73.75.241:6443 0.0.0.0:* LISTEN 30675/haproxy</code> </pre> <br><p>  Jetzt m√ºssen wir kubelet anweisen, auf localhost anstatt auf den Masterknoten zuzugreifen.  Bearbeiten Sie dazu den Serverwert in den Dateien /etc/kubernetes/kubelet.conf und /etc/kubernetes/bootstrap-kubelet.conf auf allen Arbeitsknoten. </p><br><pre> <code class="plaintext hljs">vim /etc/kubernetes/kubelet.conf vim nano /etc/kubernetes/bootstrap-kubelet.conf</code> </pre> <br><p>  Der Serverwert sollte folgende Form annehmen: </p><br><pre> <code class="plaintext hljs">server: https://127.0.0.1:6443</code> </pre> <br><p>  Starten Sie nach dem Vornehmen der √Ñnderungen die Kubelet- und Docker-Dienste neu </p><br><pre> <code class="plaintext hljs">systemctl restart kubelet &amp;&amp; systemctl restart docker</code> </pre> <br><p>  √úberpr√ºfen Sie, ob alle Knoten ordnungsgem√§√ü funktionieren. </p><br><pre> <code class="plaintext hljs">kubectl get nodes NAME STATUS ROLES AGE VERSION master01 Ready master 29m v1.15.1 master02 Ready master 27m v1.15.1 master03 Ready master 26m v1.15.1 worknode01 Ready &lt;none&gt; 25m v1.15.1 worknode02 Ready &lt;none&gt; 3m15s v1.15.1 worknode03 Ready &lt;none&gt; 3m16s v1.15.1</code> </pre> <br><p>  Bisher haben wir keine Anwendungen im Cluster, um HA zu testen.  Wir k√∂nnen jedoch den Betrieb von kubelet auf dem ersten Masterknoten stoppen und sicherstellen, dass unser Cluster betriebsbereit bleibt. </p><br><pre> <code class="plaintext hljs">systemctl stop kubelet &amp;&amp; systemctl stop docker</code> </pre> <br><p>  √úberpr√ºfen Sie vom zweiten Masterknoten </p><br><pre> <code class="plaintext hljs">root@master02:~# kubectl get nodes NAME STATUS ROLES AGE VERSION master01 NotReady master 15h v1.15.1 master02 Ready master 15h v1.15.1 master03 Ready master 15h v1.15.1 worknode01 Ready &lt;none&gt; 15h v1.15.1 worknode02 Ready &lt;none&gt; 15h v1.15.1 worknode03 Ready &lt;none&gt; 15h v1.15.1</code> </pre> <br><p>  Alle Knoten funktionieren normal, au√üer dem, auf dem wir die Dienste gestoppt haben. <br>  Vergessen Sie nicht, die kubernetes-Dienste auf dem ersten Masterknoten zur√ºckzusetzen </p><br><pre> <code class="plaintext hljs">systemctl start kubelet &amp;&amp; systemctl start docker</code> </pre> <br><a name="ingress"></a><br><h2>  Ingress Controller installieren </h2><br><p>  Ingress Controller ist das Kubernetes-Add-On, mit dem wir von au√üen auf unsere Anwendungen zugreifen k√∂nnen.  Eine ausf√ºhrliche Beschreibung finden Sie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kuberbnetes-Dokumentation</a> .  Es gibt ziemlich viele Controller, ich verwende einen Controller von Nginx.  Ich werde √ºber die Installation sprechen.  Die Dokumentation zu Betrieb, Konfiguration und Installation des Ingress-Controllers von Nginx finden Sie auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen Website</a> </p><br><p>  Beginnen wir mit der Installation, alle Befehle k√∂nnen mit master01 ausgef√ºhrt werden. <br>  Installieren Sie den Controller selbst </p><br><pre> <code class="plaintext hljs">kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml</code> </pre> <br><p>  Und jetzt - ein Service, √ºber den Ingress verf√ºgbar sein wird <br>  Bereiten Sie dazu die Konfiguration vor </p><br><pre> <code class="plaintext hljs">./create-config.sh ingress</code> </pre> <br><p>  Und senden Sie es an unseren Cluster </p><br><pre> <code class="plaintext hljs">kubectl apply -f ingress/service-nodeport.yaml</code> </pre> <br><p>  √úberpr√ºfen Sie, ob unser Ingress an den richtigen Adressen arbeitet und die richtigen Ports √ºberwacht. </p><br><pre> <code class="plaintext hljs"># kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 10.99.35.95 10.73.71.25,10.73.71.26,10.73.71.27 80:31669/TCP,443:31604/TCP 10m</code> </pre> <br><pre> <code class="plaintext hljs"> kubectl describe svc -n ingress-nginx ingress-nginx Name: ingress-nginx Namespace: ingress-nginx Labels: app.kubernetes.io/name=ingress-nginx app.kubernetes.io/part-of=ingress-nginx Annotations: kubectl.kubernetes.io/last-applied-configuration: {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app.kubernetes.io/name":"ingress-nginx","app.kubernetes.io/par... Selector: app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/part-of=ingress-nginx Type: NodePort IP: 10.99.35.95 External IPs: 10.73.71.25,10.73.71.26,10.73.71.27 Port: http 80/TCP TargetPort: 80/TCP NodePort: http 31669/TCP Endpoints: 192.168.142.129:80 Port: https 443/TCP TargetPort: 443/TCP NodePort: https 31604/TCP Endpoints: 192.168.142.129:443 Session Affinity: None External Traffic Policy: Cluster Events: &lt;none&gt;</code> </pre> <br><a name="dashboard"></a><br><h2>  Installieren Sie die Web-Benutzeroberfl√§che (Dashboard) </h2><br><p>  Kubernetes verf√ºgt √ºber eine Standard-Web-Benutzeroberfl√§che, √ºber die es manchmal bequem ist, den Status eines Clusters oder seiner einzelnen Teile schnell anzuzeigen.  In meiner Arbeit verwende ich h√§ufig das Dashboard f√ºr die Erstdiagnose von Bereitstellungen oder den Status von Teilen eines Clusters. <br>  Der Link zur Dokumentation befindet sich auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Website kubernetes</a> <br>  Installation  Ich benutze die stabile Version, ich habe 2.0 noch nicht ausprobiert. </p><br><pre> <code class="plaintext hljs">#  kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml # 2.0 kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta1/aio/deploy/recommended.yaml</code> </pre> <br><p>  Nachdem wir das Panel in unserem Cluster installiert hatten, wurde das Panel unter verf√ºgbar </p><br><pre> <code class="plaintext hljs">http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/.</code> </pre> <br><p>  Um dorthin zu gelangen, m√ºssen wir die Ports vom lokalen Computer mithilfe des kubectl-Proxys weiterleiten.  F√ºr mich ist dieses Schema nicht sehr praktisch.  Daher werde ich den Dienst des Control Panels so √§ndern, dass das Dashboard unter der Adresse eines beliebigen Clusterknotens an Port 30443 verf√ºgbar wird. Es gibt noch andere M√∂glichkeiten, auf das Dashboard zuzugreifen, z. B. √ºber Ingress.  Vielleicht werde ich diese Methode in den folgenden Ver√∂ffentlichungen betrachten. <br>  F√ºhren Sie zum √Ñndern des Dienstes die Bereitstellung des ge√§nderten Dienstes aus </p><br><pre> <code class="plaintext hljs">kubectl apply -f dashboard/service-nodeport.yaml</code> </pre> <br><p>  Der Administrator und das Token m√ºssen noch erstellt werden, um √ºber das Dashboard auf den Cluster zuzugreifen </p><br><pre> <code class="plaintext hljs">kubectl apply -f dashboard/rbac.yaml kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')</code> </pre> <br><p>  Danach k√∂nnen Sie sich unter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://10.73.71.25:30443</a> bei der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Systemsteuerung</a> anmelden <br><img src="https://habrastorage.org/webt/p7/zu/8q/p7zu8qv47mwdsmdydtvyo7gs_y4.png"><br>  Dashboard-Startbildschirm <br><img src="https://habrastorage.org/webt/h2/ks/jq/h2ksjq_7egqatf4ulnl0zkwqvqk.png"></p><br><p>  Gl√ºckwunsch!  Wenn Sie diesen Schritt erreicht haben, verf√ºgen Sie √ºber einen funktionierenden HA-Cluster von Kubernetes, der f√ºr die Bereitstellung Ihrer Anwendungen bereit ist. <br> Kubernetes    ,      .          . <br>       ,  GitHub,    ,    . <br> C ,  . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de462473/">https://habr.com/ru/post/de462473/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de462461/index.html">Ergebnisse der GOES-17-Crash-Untersuchung</a></li>
<li><a href="../de462465/index.html">Verwenden der nativen Orte von Apple</a></li>
<li><a href="../de462467/index.html">Frontend Weekly Digest (29. Juli - 4. August 2019)</a></li>
<li><a href="../de462469/index.html">Einige √úberlegungen zum gleichzeitigen Rechnen in R f√ºr "Unternehmens" -Aufgaben</a></li>
<li><a href="../de462471/index.html">L√∂sen eines Jobs mit pwnable.kr 16 - uaf. Verwendung nach freier Sicherheitsl√ºcke</a></li>
<li><a href="../de462475/index.html">Alexey Savvateev: Wie man Korruption mit Hilfe der Mathematik bek√§mpft (Nobelpreis f√ºr Wirtschaftswissenschaften f√ºr 2016)</a></li>
<li><a href="../de462477/index.html">Wissenschaftler behaupten, AI sei der Autor eines neuen Patents und versuchen, das Patentrecht zu √§ndern</a></li>
<li><a href="../de462479/index.html">Eskalation der lokalen Berechtigungen des Steam Windows-Clients 0 Tag</a></li>
<li><a href="../de462481/index.html">Geben Sie System-FAQs ein</a></li>
<li><a href="../de462483/index.html">Funktionale Programmierung: Ein verr√ºcktes Spielzeug, das die Arbeitsproduktivit√§t beeintr√§chtigt. Teil 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>