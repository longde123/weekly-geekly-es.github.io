<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíΩ üçÉ üíÉüèø AERODISK vAIR-Architektur oder Merkmale der nationalen Clusterbildung üíÉüèΩ üôåüèº ‚óæÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Khabrovchans! Wir machen Sie weiterhin mit dem russischen hyperkonvergenten System AERODISK vAIR vertraut. Dieser Artikel befasst sich mit der A...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>AERODISK vAIR-Architektur oder Merkmale der nationalen Clusterbildung</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/aerodisk/blog/475254/"><p><img src="https://habrastorage.org/webt/4c/_3/or/4c_3or6x9ro2_f_g8-waac1oen0.jpeg"></p><br><p>  Hallo Khabrovchans!  Wir machen Sie weiterhin mit dem russischen hyperkonvergenten System AERODISK vAIR vertraut.  Dieser Artikel befasst sich mit der Architektur dieses Systems.  Im letzten Artikel haben wir unser ARDFS-Dateisystem analysiert, und in diesem Artikel werden wir alle wichtigen Softwarekomponenten, aus denen vAIR besteht, und ihre Aufgaben behandeln. </p><a name="habracut"></a><br><p>  Wir beginnen die Beschreibung der Architektur von Grund auf - vom Speicher bis zur Verwaltung. </p><br><h2 id="faylovaya-sistema-ardfs--raft-cluster-driver">  ARDFS + Raft Cluster Driver-Dateisystem </h2><br><p>  Die Basis von vAIR ist das verteilte Dateisystem ARDFS, das die lokalen Festplatten aller Clusterknoten zu einem einzigen logischen Pool zusammenfasst, auf dessen Grundlage virtuelle Festplatten mit dem einen oder anderen Fehlertoleranzschema (Replikationsfaktor oder L√∂schcodierung) aus virtuellen 4-MB-Bl√∂cken gebildet werden.  Eine detailliertere Beschreibung der Arbeit von ARDFS finden Sie im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vorherigen Artikel.</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><br></a> <br>  Raft Cluster Driver ist ein interner ARDFS-Dienst, der das Problem der verteilten und zuverl√§ssigen Speicherung von Dateisystem-Metadaten l√∂st. </p><br><p>  ARDFS-Metadaten werden herk√∂mmlicherweise in zwei Klassen unterteilt. </p><br><ul><li>  Benachrichtigungen - Informationen √ºber Operationen mit Speicherobjekten und Informationen √ºber die Objekte selbst; </li><li>  Serviceinformationen - Festlegen von Sperren und Konfigurationsinformationen f√ºr Speicherknoten. </li></ul><br><p>  Der RCD-Dienst wird zum Verteilen dieser Daten verwendet.  Es weist automatisch einen Knoten mit der Rolle eines Leiters zu, dessen Aufgabe es ist, Metadaten zu erhalten und √ºber die Knoten zu verbreiten.  Ein F√ºhrer ist die einzig wahre Quelle dieser Information.  Zus√§tzlich organisiert der Anf√ºhrer einen Herzschlag, d.h.  pr√ºft die Verf√ºgbarkeit aller Speicherknoten (dies hat nichts mit der Verf√ºgbarkeit virtueller Maschinen zu tun, RCD ist nur ein Dienst f√ºr die Speicherung). </p><br><p>  Wenn der Anf√ºhrer aus irgendeinem Grund l√§nger als eine Sekunde f√ºr einen der normalen Knoten nicht verf√ºgbar ist, organisiert dieser normale Knoten eine Wiederwahl des Anf√ºhrers und fordert die Verf√ºgbarkeit des Anf√ºhrers von anderen normalen Knoten an.  Bei Beschlussf√§higkeit wird der Vorsitzende wiedergew√§hlt.  Nachdem der fr√ºhere Anf√ºhrer "aufgewacht" ist, wird er automatisch zu einem gew√∂hnlichen Knoten, weil  der neue Anf√ºhrer schickt ihm das entsprechende Team. </p><br><p> Die Logik des GGM selbst ist nicht neu.  Viele kommerzielle und kostenlose L√∂sungen von Drittanbietern lassen sich ebenfalls von dieser Logik leiten, aber diese L√∂sungen passten nicht zu uns (wie das vorhandene Open-Source-FS), da sie ziemlich schwer sind und es sehr schwierig ist, sie f√ºr unsere einfachen Aufgaben zu optimieren. Deshalb haben wir nur unsere eigenen geschrieben RCD-Service. <br>  Es mag den Anschein haben, als sei der Anf√ºhrer ein "schmaler Hals", der die Arbeit in gro√üen Clustern um Hunderte von Knoten verlangsamen kann, aber das ist nicht der Fall.  Der beschriebene Prozess l√§uft fast sofort ab und ‚Äûwiegt‚Äú sehr wenig, da wir ihn selbst geschrieben haben und nur die wichtigsten Funktionen enthalten.  Au√üerdem geschieht dies vollautomatisch und es verbleiben nur Nachrichten in den Protokollen. </p><br><h2 id="masterio--sluzhba-upravleniya-mnogopotochnym-vvodom-vyvodom">  MasterIO - Multithread-E / A-Verwaltungsdienst </h2><br><p>  Sobald ein ARDFS-Pool mit virtuellen Laufwerken organisiert ist, kann er f√ºr E / A verwendet werden.  An dieser Stelle stellt sich speziell f√ºr hyperkonvergierte Systeme die Frage, wie viel Systemressourcen (CPU / RAM) wir f√ºr IO spenden k√∂nnen. </p><br><p>  In klassischen Speichersystemen ist diese Frage nicht so akut, da die Speicheraufgabe nur darin besteht, Daten zu speichern (und die meisten Systemspeicherressourcen k√∂nnen sicher unter E / A bereitgestellt werden), und Hyperkonvergenzaufgaben umfassen neben Speicher auch die Ausf√ºhrung virtueller Maschinen.  Dementsprechend erfordert das GCS die Verwendung von CPU- und RAM-Ressourcen haupts√§chlich f√ºr virtuelle Maschinen.  Was ist mit I / O? </p><br><p>  Um dieses Problem zu l√∂sen, verwendet vAIR den E / A-Verwaltungsdienst: MasterIO.  Die Aufgabe des Dienstes ist einfach - <del>  "Nimm alles und teile" </del>  Es wird garantiert, dass es die n-te Anzahl von Systemressourcen f√ºr die Eingabe und Ausgabe aufnimmt und ausgehend von diesen die n-te Anzahl von Eingabe- / Ausgabestr√∂men startet. </p><br><p>  Zun√§chst wollten wir einen ‚Äûsehr intelligenten‚Äú Mechanismus f√ºr die Zuweisung von Ressourcen f√ºr E / A bereitstellen.  Wenn der Speicher beispielsweise nicht ausgelastet ist, k√∂nnen Systemressourcen f√ºr virtuelle Maschinen verwendet werden. Wenn die Auslastung auftritt, werden diese Ressourcen innerhalb vorgegebener Grenzen ‚Äûsanft‚Äú von den virtuellen Maschinen entfernt.  Dieser Versuch scheiterte jedoch teilweise.  Tests haben gezeigt, dass, wenn die Last schrittweise erh√∂ht wird, alles in Ordnung ist und Ressourcen (die f√ºr ein m√∂gliches Entfernen markiert sind) zugunsten von E / A schrittweise aus der VM entnommen werden.  Starke Speicherlasten f√ºhren jedoch zu einem weniger ‚Äûsanften‚Äú Entzug von Ressourcen aus virtuellen Maschinen. Infolgedessen h√§ufen sich Warteschlangen auf den Prozessoren und f√ºhren zu <del>  und die W√∂lfe sind hungrig und die Schafe sind tot </del>  und virtualka h√§ngen, und es gibt keine IOPS. </p><br><p>  Vielleicht werden wir in Zukunft auf dieses Problem zur√ºckkommen, aber vorerst haben wir die Ausgabe von Ressourcen f√ºr IO in den H√§nden des guten alten Gro√üvaters implementiert. </p><br><p>  Basierend auf den Auslegungsdaten weist der Administrator die n-te Anzahl von CPU-Kernen und RAM f√ºr den MasterIO-Dienst vorab zu.  Diesen Ressourcen wird ein Monopol zugewiesen, d. H.  Sie k√∂nnen in keiner Weise f√ºr die Anforderungen der VM verwendet werden, bis der Administrator dies zul√§sst.  Die Ressourcen werden gleichm√§√üig verteilt, d. H.  Jedem Knoten des Clusters wird die gleiche Menge an Systemressourcen entnommen.  Zuallererst sind Prozessorressourcen f√ºr MasterIO von Interesse (RAM ist weniger wichtig), insbesondere wenn wir Erasure-Codierung verwenden. </p><br><p>  Wenn bei der Dimensionierung ein Fehler aufgetreten ist und MasterIO zu viele Ressourcen zugewiesen wurden, kann die Situation einfach behoben werden, indem diese Ressourcen wieder in den VM-Ressourcenpool verschoben werden.  Wenn sich die Ressourcen im Leerlauf befinden, kehren sie fast sofort in den VM-Ressourcenpool zur√ºck. Wenn diese Ressourcen jedoch gel√∂scht werden, m√ºssen Sie eine Weile warten, bis MasterIO sie im Hintergrund freigibt. </p><br><p>  Die umgekehrte Situation ist komplizierter.  Wenn wir die Anzahl der Kerne f√ºr MasterIO erh√∂hen m√ºssen und diese mit virtuellen Kernen besch√§ftigt sind, m√ºssen wir mit virtuellen Kernen "verhandeln", dh sie mit Handles ausw√§hlen, da dieser Vorgang im automatischen Modus in einer Situation mit starker Lastschwankung mit VM-Einfrierungen und anderem launischem Verhalten behaftet ist. </p><br><p>  Dementsprechend muss der Dimensionierung der Leistung hyperkonvergierter IO-Systeme (nicht nur unserer) gro√üe Aufmerksamkeit gewidmet werden.  Wenig sp√§ter in einem der Artikel versprechen wir, dieses Problem ausf√ºhrlicher zu behandeln. </p><br><h2 id="gipervizor">  Hypervisor </h2><br><p>  Hypervisor Aist ist f√ºr die Ausf√ºhrung virtueller Maschinen in vAIR verantwortlich.  Dieser Hypervisor basiert auf dem bew√§hrten KVM-Hypervisor.  Im Prinzip wurde eine ganze Menge √ºber die Arbeit von KVM geschrieben, sodass es nicht erforderlich ist, diese zu malen. Zeigen Sie lediglich an, dass alle Standardfunktionen von KVM in Stork gespeichert sind und einwandfrei funktionieren. </p><br><p>  Daher werden wir hier die Hauptunterschiede zu dem Standard-KVM beschreiben, den wir in Stork implementiert haben.  Der Storch ist Teil des Systems (vorinstallierter Hypervisor) und wird von der gemeinsamen vAIR-Konsole √ºber die Web-GUI (russische und englische Version) und SSH (nat√ºrlich nur Englisch) gesteuert. </p><br><p><img src="https://habrastorage.org/webt/vi/ju/9e/viju9ee4sxkeq-ezvoobttyjvck.png"></p><br><p>  Dar√ºber hinaus werden die Hypervisor-Konfigurationen in der verteilten ConfigDB-Datenbank gespeichert (etwa etwas sp√§ter), die auch eine zentrale Kontrollstelle darstellt.  Das hei√üt, Sie k√∂nnen eine Verbindung zu einem beliebigen Knoten im Cluster herstellen und alle Knoten verwalten, ohne dass ein separater Verwaltungsserver erforderlich ist. </p><br><p>  Eine wichtige Erg√§nzung zur Standard-KVM-Funktionalit√§t ist das von uns entwickelte HA-Modul.  Dies ist die einfachste Implementierung eines Clusters hochverf√ºgbarer virtueller Maschinen, mit der Sie die virtuelle Maschine bei einem Knotenausfall automatisch auf einem anderen Clusterknoten neu starten k√∂nnen. </p><br><p>  Ein weiteres n√ºtzliches Feature ist die Massenbereitstellung von virtuellen Maschinen (relevant f√ºr VDI-Umgebungen), mit der die Bereitstellung von virtuellen Maschinen mit ihrer automatischen Verteilung auf die Knoten in Abh√§ngigkeit von deren Auslastung automatisiert wird. </p><br><p>  Die VM-Verteilung zwischen Knoten ist die Basis f√ºr den automatischen Lastausgleich (ala DRS).  Diese Funktion ist in der aktuellen Version noch nicht verf√ºgbar, wir arbeiten jedoch aktiv daran und sie wird definitiv in einem der n√§chsten Updates erscheinen. </p><br><p>  Der VMware ESXi-Hypervisor wird optional unterst√ºtzt. Derzeit wird er mit dem iSCSI-Protokoll implementiert. Eine NFS-Unterst√ºtzung ist auch f√ºr die Zukunft geplant. </p><br><h2 id="virtualnye-kommutatory">  Virtuelle Schalter </h2><br><p>  F√ºr die Software-Implementierung der Schalter ist eine separate Komponente vorgesehen - Fractal.  Wie bei unseren anderen Komponenten gehen wir von einfach zu komplex √ºber. In der ersten Version wird einfaches Switching implementiert, w√§hrend Routing und Firewall an Ger√§te von Drittanbietern √ºbertragen werden.  Das Funktionsprinzip ist Standard.  Die physische Schnittstelle des Servers ist √ºber eine Br√ºcke mit dem Fractal-Objekt verbunden - einer Gruppe von Ports.  Eine Gruppe von Ports mit den gew√ºnschten virtuellen Maschinen im Cluster.  Die Organisation von VLANs wird unterst√ºtzt und in einem der n√§chsten Releases wird die VxLAN-Unterst√ºtzung hinzugef√ºgt.  Alle erstellten Switches werden standardm√§√üig verteilt, d. H.  √úber alle Knoten des Clusters verteilt. Welche virtuellen Maschinen zu welchen Switches eine Verbindung zur VM herstellen, h√§ngt also nicht vom Standortknoten ab. Dies ist ausschlie√ülich Sache des Administrators. </p><br><h2 id="monitoring-i-statistika">  √úberwachung und Statistik </h2><br><p>  Die f√ºr die √úberwachung und Statistik verantwortliche Komponente (Arbeitstitel Monica) ist ein √ºberarbeiteter Klon aus dem ENGINE-Speichersystem.  Einmal hat er sich gut empfohlen, und wir haben beschlossen, es mit vAIR mit einfacher Abstimmung zu verwenden.  Monica wird wie alle anderen Komponenten gleichzeitig auf allen Knoten des Clusters ausgef√ºhrt und gespeichert. </p><br><p>  Monicas schwierige Aufgaben k√∂nnen wie folgt umrissen werden: </p><br><p>  Datenerfassung: </p><br><ul><li>  von Hardware-Sensoren (die Eisen √ºber IPMI geben k√∂nnen); </li><li>  aus logischen vAIR-Objekten (ARDFS, Stork, Fractal, MasterIO und anderen Objekten). </li></ul><br><p><img src="https://habrastorage.org/webt/1c/4k/u8/1c4ku82o_bkanp-348z-jbeioma.png"></p><br><p>  Sammeln von Daten in einer verteilten Datenbank; </p><br><p>  Interpretation von Daten in Form von: </p><br><ul><li>  Protokolle; </li><li>  Warnungen </li><li>  Zeitpl√§ne. </li></ul><br><p>  Externe Interaktion mit Systemen von Drittanbietern √ºber die Protokolle SMTP (Senden von E-Mail-Warnungen) und SNMP (Interaktion mit √úberwachungssystemen von Drittanbietern). </p><br><p><img src="https://habrastorage.org/webt/2k/-p/9o/2k-p9oah-yrta3n5ti0m5yfm22e.png"></p><br><h2 id="raspredelennaya-baza-konfiguraciy">  Verteilte Konfigurationsbasis </h2><br><p>  In den vorherigen Abs√§tzen wurde erw√§hnt, dass viele Daten gleichzeitig auf allen Knoten des Clusters gespeichert werden.  Um diese Speichermethode zu organisieren, wird eine spezielle verteilte ConfigDB-Datenbank bereitgestellt.  Wie der Name schon sagt, werden in der Datenbank die Konfigurationen aller Clusterobjekte gespeichert: Hypervisor, virtuelle Maschinen, HA-Modul, Switches, Dateisystem (nicht zu verwechseln mit der FS-Metadaten-Datenbank, dies ist eine andere Datenbank) sowie Statistiken.  Diese Daten werden synchron auf allen Knoten gespeichert und die Konsistenz dieser Daten ist eine Voraussetzung f√ºr den stabilen Betrieb von vAIR. </p><br><p>  Ein wichtiger Punkt: Obwohl die Funktionsweise von ConfigDB f√ºr den vAIR-Betrieb von entscheidender Bedeutung ist, wirkt sich ihr Ausfall, obwohl er den Cluster stoppt, nicht auf die Konsistenz der in ARDFS gespeicherten Daten aus, was unserer Meinung nach ein Plus f√ºr die Zuverl√§ssigkeit der gesamten L√∂sung darstellt. </p><br><p>  ConfigDB ist auch ein einziger Verwaltungspunkt. Sie k√∂nnen also nach IP-Adresse zu jedem Knoten des Clusters wechseln und alle Knoten des Clusters vollst√§ndig verwalten, was sehr praktisch ist. </p><br><p>  Dar√ºber hinaus bietet ConfigDB f√ºr den Zugriff auf externe Systeme eine Restful-API, √ºber die Sie die Integration in Systeme von Drittanbietern konfigurieren k√∂nnen.  Beispielsweise haben wir k√ºrzlich eine Pilotintegration mit mehreren russischen L√∂sungen in den Bereichen VDI und Informationssicherheit durchgef√ºhrt.  Wenn die Projekte abgeschlossen sind, schreiben wir hier gerne technische Details. </p><br><h2 id="kartina-v-celom">  Das ganze Bild </h2><br><p>  Als Ergebnis haben wir zwei Versionen der Systemarchitektur. </p><br><p>  Im ersten - Hauptfall - werden unsere KVM-basierten Aist-Hypervisor- und Fractal-Software-Switches verwendet. </p><br><p>  <strong>Szenario 1. Richtig</strong> </p><br><p><img src="https://habrastorage.org/webt/0-/pb/7j/0-pb7j1-zdrpqone3dlvw5un4py.png"></p><br><p>  Bei der zweiten - optionalen Option - ist das Schema etwas kompliziert, wenn Sie den ESXi-Hypervisor verwenden m√∂chten.  Um ESXi verwenden zu k√∂nnen, muss es auf den lokalen Laufwerken des Clusters auf die √ºbliche Weise installiert werden.  Als N√§chstes wird auf jedem ESXi-Knoten die virtuelle vAIR MasterVM-Maschine installiert, die eine spezielle vAIR-Distribution enth√§lt, die als virtuelle VMware-Maschine ausgef√ºhrt wird. </p><br><p>  ESXi stellt alle freien lokalen Festplatten durch direkte Weiterleitung an MasterVM zur Verf√ºgung.  Innerhalb von MasterVM werden diese Festplatten bereits standardm√§√üig in ARDFS formatiert und √ºber die dedizierten Schnittstellen in ESXi nach au√üen (bzw. zur√ºck zu ESXi) mit dem iSCSI-Protokoll (und in Zukunft wird es auch NFS geben) geliefert.  Dementsprechend werden virtuelle Maschinen und Softwarenetzwerke in diesem Fall von ESXi bereitgestellt. </p><br><p>  <strong>Szenario 2. ESXi</strong> </p><br><p><img src="https://habrastorage.org/webt/no/jf/cv/nojfcvippesbznyxdmzpvnmq2se.png"></p><br><p>  Daher haben wir alle Hauptkomponenten der vAIR-Architektur und ihre Aufgaben zerlegt.  Im n√§chsten Artikel werden wir √ºber die bereits implementierten Funktionen und Pl√§ne f√ºr die nahe Zukunft sprechen. </p><br><p>  Wir warten auf Kommentare und Vorschl√§ge. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de475254/">https://habr.com/ru/post/de475254/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de475242/index.html">Verwendung strenger Module in gro√üen Python-Projekten: Instagram-Erfahrung. Teil 2</a></li>
<li><a href="../de475244/index.html">Erwartete neue JavaScript-Funktionen, die Sie kennen sollten</a></li>
<li><a href="../de475246/index.html">Asynchrone Python-Programmierung: Ein kurzer √úberblick</a></li>
<li><a href="../de475248/index.html">Die Verwendung von Polyfills beim Schreiben browser√ºbergreifender Anwendungen</a></li>
<li><a href="../de475250/index.html">Als Redash ein Problem bemerkte und behebte, das zu einer Verschlechterung der Python-Code-Leistung f√ºhrte</a></li>
<li><a href="../de475260/index.html">Der Unterschied zwischen einer asynchronen Funktion und einer Funktion, die ein Versprechen zur√ºckgibt</a></li>
<li><a href="../de475262/index.html">Die Verdauung von frischen Materialien aus der Welt des Frontends f√ºr die letzte Woche Nr. 388 (4. - 10. November 2019)</a></li>
<li><a href="../de475264/index.html">Schn√ºffler, die es k√∂nnten: Wie die FakeSecurity-Familie Online-Shops infizierte</a></li>
<li><a href="../de475266/index.html">Wir kehren mobile 1s unter Android um. So f√ºgen Sie ein wenig Funktionalit√§t hinzu und lassen ein paar Abende aus</a></li>
<li><a href="../de475270/index.html">SwiftUI: Erweiterbare / reduzierbare Abschnitte in der Listenansicht erstellen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>