<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëáüèª üëèüèª üõ©Ô∏è Comprender las redes neuronales convolucionales a trav√©s de visualizaciones en PyTorch üë©‚Äçüé® üë®üèø‚Äçüé§ üõÄ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En nuestra era, las m√°quinas han logrado con √©xito una precisi√≥n del 99% en la comprensi√≥n y definici√≥n de caracter√≠sticas y objetos en las im√°genes. ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comprender las redes neuronales convolucionales a trav√©s de visualizaciones en PyTorch</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/436838/">  En nuestra era, las m√°quinas han logrado con √©xito una precisi√≥n del 99% en la comprensi√≥n y definici√≥n de caracter√≠sticas y objetos en las im√°genes.  Nos enfrentamos a esto todos los d√≠as, por ejemplo: reconocimiento facial en la c√°mara del tel√©fono inteligente, la capacidad de buscar fotos en Google, escanear texto de un c√≥digo de barras o libros a una buena velocidad, etc. Tal eficiencia de la m√°quina fue posible gracias a un tipo especial de red neuronal llamada neuronal convolucional la red  Si eres un entusiasta del aprendizaje profundo, es probable que hayas escuchado al respecto y podr√≠as desarrollar varios clasificadores de im√°genes.  Los marcos modernos de aprendizaje profundo como Tensorflow y PyTorch simplifican el aprendizaje autom√°tico de im√°genes.  Sin embargo, la pregunta sigue siendo: ¬øc√≥mo pasan los datos a trav√©s de las capas de la red neuronal y c√≥mo la computadora aprende de ellos?  Para obtener una vista clara desde cero, nos sumergimos en una convoluci√≥n, visualizando la imagen de cada capa. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/859/2c6/958/8592c6958985979587858374abd08f98.png" alt="imagen"><br><a name="habracut"></a><br><h2>  Redes neuronales convolucionales </h2><br>  Antes de comenzar a estudiar redes neuronales convolucionales (SNA), debe aprender a trabajar con redes neuronales.  Las redes neuronales imitan el cerebro humano para resolver problemas complejos y buscar patrones en los datos.  En los √∫ltimos a√±os, han reemplazado muchos algoritmos de aprendizaje autom√°tico y visi√≥n por computadora.  El modelo b√°sico de una red neuronal consiste en neuronas organizadas en capas.  Cada red neuronal tiene una capa de entrada y salida y se le agregan varias capas ocultas dependiendo de la complejidad del problema.  Al transmitir datos a trav√©s de capas, las neuronas se entrenan y reconocen los signos.  Esta representaci√≥n de una red neuronal se llama modelo.  Despu√©s de entrenar el modelo, le pedimos a la red que haga pron√≥sticos basados ‚Äã‚Äãen datos de prueba. <br><br>  El SNS es un tipo especial de red neuronal que funciona bien con im√°genes.  Ian Lekun los propuso en 1998, donde reconocieron el n√∫mero presente en la imagen de entrada.  SNA tambi√©n se utiliza para reconocimiento de voz, segmentaci√≥n de im√°genes y procesamiento de texto.  Antes de la creaci√≥n de redes neuronales convolucionales, se utilizaron perceptrones multicapa en la construcci√≥n de clasificadores de im√°genes.  La clasificaci√≥n de im√°genes se refiere a la tarea de extraer clases de una imagen r√°ster multicanal (color, blanco y negro).  Los perceptrones multicapa tardan mucho tiempo en buscar informaci√≥n en las im√°genes, ya que cada entrada debe estar asociada con cada neurona en la siguiente capa.  El SNA los rode√≥ usando un concepto llamado conectividad local.  Esto significa que conectaremos cada neurona solo a la regi√≥n de entrada local.  Esto minimiza el n√∫mero de par√°metros, permitiendo que varias partes de la red se especialicen en atributos de alto nivel, como la textura o el patr√≥n repetitivo.  Confundido?  Comparemos c√≥mo se transmiten las im√°genes a trav√©s de perceptrones multicapa (MP) y redes neuronales convolucionales. <br><br><h2>  Comparaci√≥n de MP y SNA </h2><br>  El n√∫mero total de entradas en la capa de entrada para el perceptr√≥n multicapa ser√° 784, ya que la imagen de entrada tiene un tama√±o de 28x28 = 784 (se considera el conjunto de datos MNIST).  La red debe poder predecir el n√∫mero en la imagen de entrada, lo que significa que la salida puede pertenecer a cualquiera de las siguientes clases en el rango de 0 a 9. En la capa de salida, devolvemos estimaciones de clase, digamos si esta entrada es la imagen con el n√∫mero "3", entonces, en la capa de salida, la neurona correspondiente "3" tiene un valor m√°s alto en comparaci√≥n con otras neuronas.  Nuevamente surge la pregunta: "¬øCu√°ntas capas ocultas necesitamos y cu√°ntas neuronas deber√≠an estar en cada una?"  Por ejemplo, tome el siguiente c√≥digo MP: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3f8/efc/e14/3f8efce14418a7df0be2e813399def5d.png" alt="imagen"><br><br>  El c√≥digo anterior se implementa utilizando un marco llamado Keras.  La primera capa oculta tiene 512 neuronas que est√°n conectadas a la capa de entrada de 784 neuronas.  La siguiente capa oculta: la capa de exclusi√≥n, que resuelve el problema del reciclaje.  0.2 significa que hay un 20% de posibilidades de no tener en cuenta las neuronas de la capa oculta anterior.  Nuevamente agregamos una segunda capa oculta con el mismo n√∫mero de neuronas que en la primera capa oculta (512), y luego otra capa exclusiva.  Finalmente, terminando este conjunto de capas con una capa de salida que consta de 10 clases.  La clase que m√°s importa ser√° el n√∫mero predicho por el modelo.  As√≠ es como se ve una red multicapa despu√©s de identificar todas las capas.  Uno de los inconvenientes del perceptr√≥n multinivel es que est√° completamente conectado, lo que requiere mucho tiempo y espacio. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/db3/df6/605/db3df6605d0ddb868eb14b347227b963.png" alt="imagen"><br><br>  Los convolts no usan capas completamente unidas.  Utilizan capas dispersas, que toman matrices como entrada, lo que les da una ventaja sobre MP.  En MP, cada nodo es responsable de comprender la imagen completa.  En el SCN, dividimos la imagen en √°reas (peque√±as √°reas locales de p√≠xeles).  La capa de salida combina los datos recibidos de cada nodo oculto para encontrar patrones.  A continuaci√≥n se muestra una imagen de c√≥mo est√°n conectadas las capas. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ae6/f7f/d61/ae6f7fd618d5296a0deecabdd2e06e77.png" alt="imagen"><br><br>  Ahora veamos c√≥mo el SCN encuentra informaci√≥n en las fotograf√≠as.  Antes de eso, debemos entender c√≥mo se extraen los signos.  En el SCN, utilizamos diferentes capas, cada capa conserva los signos de la imagen, por ejemplo, tiene en cuenta la imagen del perro, cuando la red necesita clasificar al perro, debe identificar todos los signos, como ojos, orejas, lengua, piernas, etc.  Estas se√±ales se rompen y se reconocen en los niveles de la red local mediante filtros y n√∫cleos. <br><br><h2>  ¬øC√≥mo miran las computadoras una imagen? </h2><br>  Una persona que mira una imagen y comprende su significado suena muy razonable.  Digamos que caminas y observas los muchos paisajes que te rodean.  ¬øC√≥mo entendemos la naturaleza en este caso?  Tomamos im√°genes del entorno utilizando nuestro √≥rgano sensorial principal: el ojo, y luego lo enviamos a la retina.  Todo parece bastante interesante, ¬øverdad?  Ahora imaginemos que una computadora hace lo mismo.  En las computadoras, las im√°genes se interpretan usando un conjunto de valores de p√≠xeles que van de 0 a 255. La computadora observa estos valores de p√≠xeles y los comprende.  A primera vista, no conoce objetos y colores.  Simplemente reconoce los valores de p√≠xeles, y la imagen es equivalente a un conjunto de valores de p√≠xeles para la computadora.  M√°s tarde, al analizar los valores de p√≠xeles, gradualmente aprende si la imagen es gris o de color.  Las im√°genes en escala de grises tienen solo un canal, ya que cada p√≠xel representa la intensidad de un color.  0 significa negro y 255 significa blanco, las otras variantes de blanco y negro, es decir, gris, se encuentran entre ellas. <br><br>  Las im√°genes en color tienen tres canales, rojo, verde y azul.  Representan la intensidad de 3 colores (matriz tridimensional), y cuando los valores cambian simult√°neamente, esto da un gran conjunto de colores, ¬°realmente una paleta de colores!  Despu√©s de eso, la computadora reconoce las curvas y contornos de los objetos en la imagen.  Todo esto puede estudiarse en la red neuronal convolucional.  Para esto, utilizaremos PyTorch para cargar un conjunto de datos y aplicar filtros a las im√°genes.  El siguiente es un fragmento de c√≥digo. <br><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Load the libraries import torch import numpy as np from torchvision import datasets import torchvision.transforms as transforms # Set the parameters num_workers = 0 batch_size = 20 # Converting the Images to tensors using Transforms transform = transforms.ToTensor() train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform) test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform) # Loading the Data train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers) test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers) import matplotlib.pyplot as plt %matplotlib inline dataiter = iter(train_loader) images, labels = dataiter.next() images = images.numpy() # Peeking into dataset fig = plt.figure(figsize=(25, 4)) for image in np.arange(20): ax = fig.add_subplot(2, 20/2, image+1, xticks=[], yticks=[]) ax.imshow(np.squeeze(images[image]), cmap='gray') ax.set_title(str(labels[image].item()))</span></span></code> </pre> <br><img src="https://habrastorage.org/getpro/habr/post_images/304/163/1ad/3041631ad58d7300a35af90b39b94584.png" alt="imagen"><br><br>  Ahora veamos c√≥mo se alimenta una sola imagen en una red neuronal. <br><br><pre> <code class="python hljs">img = np.squeeze(images[<span class="hljs-number"><span class="hljs-number">7</span></span>]) fig = plt.figure(figsize = (<span class="hljs-number"><span class="hljs-number">12</span></span>,<span class="hljs-number"><span class="hljs-number">12</span></span>)) ax = fig.add_subplot(<span class="hljs-number"><span class="hljs-number">111</span></span>) ax.imshow(img, cmap=<span class="hljs-string"><span class="hljs-string">'gray'</span></span>) width, height = img.shape thresh = img.max()/<span class="hljs-number"><span class="hljs-number">2.5</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(width): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(height): val = round(img[x][y],<span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> img[x][y] !=<span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> ax.annotate(str(val), xy=(y,x), color=<span class="hljs-string"><span class="hljs-string">'white'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> img[x][y]&lt;thresh <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">'black'</span></span>)</code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/264/f15/bff/264f15bffe653ae237f3e2fa1fc5c868.png" alt="imagen"><br><br>  As√≠ es como el n√∫mero "3" se divide en p√≠xeles.  Del conjunto de d√≠gitos escritos a mano, se selecciona "3" al azar, en el que se muestran los valores de p√≠xeles.  Aqu√≠ ToTensor () normaliza los valores de p√≠xeles reales (0‚Äì255) y los limita a un rango de 0 a 1. ¬øPor qu√© es esto?  Porque facilita los c√°lculos en las secciones posteriores, ya sea para interpretar im√°genes o para encontrar patrones comunes que existen en ellas. <br><br><h2>  Crea tu propio filtro </h2><br>  Los filtros, como su nombre lo indica, filtran la informaci√≥n.  En el caso de las redes neuronales convolucionales, cuando se trabaja con im√°genes, se filtra la informaci√≥n sobre los p√≠xeles.  ¬øPor qu√© deber√≠amos filtrar?  Recuerde que una computadora debe pasar por un proceso de aprendizaje para comprender las im√°genes, muy similar a c√≥mo lo hace un ni√±o.  ¬°En este caso, sin embargo, no necesitaremos muchos a√±os!  En resumen, aprende desde cero y luego avanza hacia el todo. <br><br>  Por lo tanto, la red debe conocer inicialmente todas las partes gruesas de la imagen, es decir, los bordes, contornos y otros elementos de bajo nivel.  Una vez que se descubren, se allana el camino para los s√≠ntomas complejos.  Para llegar a ellos, primero debemos extraer los atributos de bajo nivel, luego el medio y luego los de nivel superior.  Los filtros son una forma de extraer la informaci√≥n que el usuario necesita, y no solo la transferencia de datos a ciegas, por lo que la computadora no comprende la estructuraci√≥n de las im√°genes.  Al principio, las funciones de bajo nivel se pueden extraer en funci√≥n de un filtro espec√≠fico.  El filtro aqu√≠ tambi√©n es un conjunto de valores de p√≠xeles, similar a una imagen.  Se puede entender como los pesos que conectan las capas en la red neuronal convolucional.  Estos pesos o filtros se multiplican por los valores de entrada para producir im√°genes intermedias que representan la comprensi√≥n de la imagen por parte de la computadora.  Luego se multiplican por algunos filtros m√°s para expandir la vista.  Luego detecta los √≥rganos visibles de una persona (siempre que haya una persona presente en la imagen).  M√°s tarde, con la inclusi√≥n de varios filtros m√°s y varias capas, la computadora exclama: ‚Äú¬°Oh, s√≠!  Este es un hombre ". <br><br>  Si hablamos de filtros, entonces tenemos muchas opciones.  Es posible que desee desenfocar la imagen, luego aplicar un filtro de desenfoque, si necesita agregar nitidez, entonces un filtro de nitidez vendr√° al rescate, etc. <br><br>  Veamos algunos fragmentos de c√≥digo para comprender la funcionalidad de los filtros. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/679/6a4/bb4/6796a4bb4830bda29c6d14212274a286.png" alt="imagen"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/752/a89/805/752a89805fba54b5d0f9e90073ca9fde.png" alt="imagen"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/84f/8a1/f9c/84f8a1f9c92b1996b0e4eed4a2a7dd5b.png" alt="imagen"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/142/635/038/142635038ecef3606d53d5d9c85b26f8.png" alt="imagen"><br><br>  As√≠ es como se ve la imagen despu√©s de aplicar el filtro, en este caso utilizamos el filtro Sobel. <br><br><h2>  Redes neuronales convolucionales </h2><br>  Hasta ahora, hemos visto c√≥mo se utilizan los filtros para extraer caracter√≠sticas de las im√°genes.  Ahora, para completar toda la red neuronal convolucional, necesitamos conocer todas las capas que usamos para dise√±arla.  Las capas utilizadas en el SCN, <br><br><ol><li>  Capa convolucional </li><li>  Capa de agrupaci√≥n </li><li>  Capa completamente unida </li></ol><br>  Con las tres capas, el clasificador de imagen convolucional se ve as√≠: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8b4/927/c31/8b4927c31b5f951d7026b30d68695bea.png" alt="imagen"><br><br>  Ahora veamos qu√© hace cada capa. <br><br>  <b>La capa convolucional (CONV)</b> usa filtros que realizan operaciones de convoluci√≥n escaneando la imagen de entrada.  Sus hiperpar√°metros incluyen un tama√±o de filtro, que puede ser 2x2, 3x3, 4x4, 5x5 (pero no limitado a esto) y el paso S. El resultado O se denomina mapa de caracter√≠sticas o mapa de activaci√≥n en el que todas las caracter√≠sticas se calculan utilizando capas y filtros de entrada.  A continuaci√≥n se muestra una imagen de la generaci√≥n de mapas de caracter√≠sticas al aplicar convoluci√≥n, <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a2a/14d/aab/a2a14daab68c91f8d92ba0c54509493b.png" alt="imagen"><br><br>  <b>La capa de fusi√≥n (POOL) se</b> usa para compactar las caracter√≠sticas que generalmente se usan despu√©s de la capa de convoluci√≥n.  Hay dos tipos de operaciones de uni√≥n: esta es la uni√≥n m√°xima y media, donde se toman los valores m√°ximo y promedio de las caracter√≠sticas, respectivamente.  La siguiente es la operaci√≥n de operaciones de fusi√≥n, <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c2d/03f/2f8/c2d03f2f8734efade8cbc80d44d3767e.png" alt="imagen"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/01a/e5c/558/01ae5c558fa6647bfb9c19b9edabbb37.png" alt="imagen"><br><br>  <b>Las capas completamente conectadas (FC)</b> funcionan con una entrada plana, donde cada entrada est√° conectada a todas las neuronas.  Por lo general, se usan al final de la red para conectar capas ocultas a la capa de salida, lo que ayuda a optimizar los puntajes de la clase. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d28/558/188/d285581882fa97824cdc0ad6ecb31873.png" alt="imagen"><br><br><h3>  Visualizaci√≥n de SNA en PyTorch </h3><br>  Ahora que tenemos la ideolog√≠a completa de construir el SNA, implementemos el SNA usando el marco PyTorch de Facebook. <br><br>  <b>Paso 1</b> : descargue la imagen de entrada que se enviar√° a trav√©s de la red.  (Aqu√≠ lo hacemos con Numpy y OpenCV) <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt %matplotlib inline img_path = <span class="hljs-string"><span class="hljs-string">'dog.jpg'</span></span> bgr_img = cv2.imread(img_path) gray_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY) <span class="hljs-comment"><span class="hljs-comment"># Normalise gray_img = gray_img.astype("float32")/255 plt.imshow(gray_img, cmap='gray') plt.show()</span></span></code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/291/d0f/3d2/291d0f3d28f3091716c3aba41dc35c59.png" alt="imagen"><br><br>  <b>Paso 2</b> : Procesar filtros <br><br>  Visualicemos los filtros para comprender mejor cu√°les utilizaremos, <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np filter_vals = np.array([ [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>] ]) print(<span class="hljs-string"><span class="hljs-string">'Filter shape: '</span></span>, filter_vals.shape) <span class="hljs-comment"><span class="hljs-comment"># Defining the Filters filter_1 = filter_vals filter_2 = -filter_1 filter_3 = filter_1.T filter_4 = -filter_3 filters = np.array([filter_1, filter_2, filter_3, filter_4]) # Check the Filters fig = plt.figure(figsize=(10, 5)) for i in range(4): ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[]) ax.imshow(filters[i], cmap='gray') ax.set_title('Filter %s' % str(i+1)) width, height = filters[i].shape for x in range(width): for y in range(height): ax.annotate(str(filters[i][x][y]), xy=(y,x), color='white' if filters[i][x][y]&lt;0 else 'black')</span></span></code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/4c7/75f/1fc/4c775f1fc19bd461679d5a45831f1e2e.png" alt="imagen"><br><br>  <b>Paso 3</b> : determinar el SCN <br><br>  Este SNA tiene una capa convolucional y una capa de agrupaci√≥n con una funci√≥n m√°xima, y ‚Äã‚Äãlos pesos se inicializan utilizando los filtros que se muestran arriba, <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn.functional <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> F <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Net</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, weight)</span></span></span><span class="hljs-function">:</span></span> super(Net, self).__init__() <span class="hljs-comment"><span class="hljs-comment"># initializes the weights of the convolutional layer to be the weights of the 4 defined filters k_height, k_width = weight.shape[2:] # assumes there are 4 grayscale filters self.conv = nn.Conv2d(1, 4, kernel_size=(k_height, k_width), bias=False) # initializes the weights of the convolutional layer self.conv.weight = torch.nn.Parameter(weight) # define a pooling layer self.pool = nn.MaxPool2d(2, 2) def forward(self, x): # calculates the output of a convolutional layer # pre- and post-activation conv_x = self.conv(x) activated_x = F.relu(conv_x) # applies pooling layer pooled_x = self.pool(activated_x) # returns all layers return conv_x, activated_x, pooled_x # instantiate the model and set the weights weight = torch.from_numpy(filters).unsqueeze(1).type(torch.FloatTensor) model = Net(weight) # print out the layer in the network print(model)</span></span></code> </pre><br><blockquote><pre> <code class="plaintext hljs">Net( (conv): Conv2d(1, 4, kernel_size=(4, 4), stride=(1, 1), bias=False) (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) )</code> </pre> </blockquote>  <b>Paso 4</b> : Procesar filtros <br>  Un vistazo r√°pido a los filtros utilizados, <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">viz_layer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(layer, n_filters= </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">4</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> fig = plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_filters): ax = fig.add_subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, n_filters, i+<span class="hljs-number"><span class="hljs-number">1</span></span>) ax.imshow(np.squeeze(layer[<span class="hljs-number"><span class="hljs-number">0</span></span>,i].data.numpy()), cmap=<span class="hljs-string"><span class="hljs-string">'gray'</span></span>) ax.set_title(<span class="hljs-string"><span class="hljs-string">'Output %s'</span></span> % str(i+<span class="hljs-number"><span class="hljs-number">1</span></span>)) fig = plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>)) fig.subplots_adjust(left=<span class="hljs-number"><span class="hljs-number">0</span></span>, right=<span class="hljs-number"><span class="hljs-number">1.5</span></span>, bottom=<span class="hljs-number"><span class="hljs-number">0.8</span></span>, top=<span class="hljs-number"><span class="hljs-number">1</span></span>, hspace=<span class="hljs-number"><span class="hljs-number">0.05</span></span>, wspace=<span class="hljs-number"><span class="hljs-number">0.05</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">4</span></span>): ax = fig.add_subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, i+<span class="hljs-number"><span class="hljs-number">1</span></span>, xticks=[], yticks=[]) ax.imshow(filters[i], cmap=<span class="hljs-string"><span class="hljs-string">'gray'</span></span>) ax.set_title(<span class="hljs-string"><span class="hljs-string">'Filter %s'</span></span> % str(i+<span class="hljs-number"><span class="hljs-number">1</span></span>)) gray_img_tensor = torch.from_numpy(gray_img).unsqueeze(<span class="hljs-number"><span class="hljs-number">0</span></span>).unsqueeze(<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre><br>  Filtros: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/885/5c9/cac/8855c9cace448bed1d831c3dc4731828.png" alt="imagen"><br><br>  <b>Paso 5</b> : Resultados filtrados por capa <br><br>  Las im√°genes que aparecen en la capa CONV y POOL se muestran a continuaci√≥n. <br><br><pre> <code class="python hljs">viz_layer(activated_layer) viz_layer(pooled_layer)</code> </pre><br>  Capas convolucionales <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6bb/4c8/1bc/6bb4c81bc6ef16044dfc22e9e36bbaa6.png" alt="imagen"><br><br>  Capas de agrupaci√≥n <br><br><img src="https://habrastorage.org/getpro/habr/post_images/789/278/823/78927882302ae10f6403ba3a498669fd.png" alt="imagen"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Fuente</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es436838/">https://habr.com/ru/post/es436838/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es436800/index.html">T√©cnica de proyecto de bricolaje. Parte dos</a></li>
<li><a href="../es436822/index.html">Android Robotics hasta 2019: la historia real; en 5 partes; parte 3</a></li>
<li><a href="../es436828/index.html">La transici√≥n a Boost-1.65.1 y los errores que surgieron</a></li>
<li><a href="../es436830/index.html">Android Robotics hasta 2019: la historia real; en 5 partes; parte 5</a></li>
<li><a href="../es436836/index.html">Beneficios de analizar aplicaciones de nivel 7 en firewalls. Parte 2. Seguridad</a></li>
<li><a href="../es436840/index.html">El camino del brillo a la neurociencia: un podcast tem√°tico sobre carreras en medios y marketing de contenidos</a></li>
<li><a href="../es436842/index.html">Soluci√≥n Veeam para respaldo y recuperaci√≥n de m√°quinas virtuales en la plataforma Nutanix AHV. Parte 2</a></li>
<li><a href="../es436846/index.html">El resumen de materiales frescos del mundo del front-end para la √∫ltima semana No. 348 (14-20 de enero de 2019)</a></li>
<li><a href="../es436848/index.html">NSA anuncia lanzamiento de herramienta interna para ingenier√≠a inversa</a></li>
<li><a href="../es436850/index.html">Errores comunes al escribir pruebas unitarias. Conferencia de Yandex</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>