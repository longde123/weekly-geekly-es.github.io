<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏼‍🔧 👨🏽‍🤝‍👨🏻 👨🏾‍🏭 Filtrage linéaire optimal: de la descente de gradient aux filtres adaptatifs 🐧 🎑 🤕</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Développer le sujet des résumés dans la spécialité du master "Communication et traitement du signal" (TU Ilmenau), je voudrais poursuivre l'un des pri...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Filtrage linéaire optimal: de la descente de gradient aux filtres adaptatifs</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/455497/"><p>  Développer le sujet des résumés dans la spécialité du master "Communication et traitement du signal" (TU Ilmenau), je voudrais poursuivre l'un des principaux sujets du cours <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">"Traitement adaptatif et matriciel du signal"</a> .  À savoir, les bases du filtrage adaptatif. </p><br><p>  <u>Pour qui cet article a été écrit pour la première fois:</u> <u><br></u> <br>  1) pour une confrérie étudiante d'une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">spécialité autochtone</a> ; <br>  2) pour les enseignants qui préparent des séminaires pratiques, mais n'ont pas encore décidé des outils - voici des exemples en <strong>python</strong> et <strong>Matlab / Octave</strong> ; <br>  3) pour toute personne intéressée par le sujet du filtrage. </p><br><p>  <u>Que peut-on trouver sous la coupe:</u> <u><br></u> <br>  1) des informations issues de la théorie, que j'ai essayé d'organiser de manière aussi concise que possible, mais, me semble-t-il, informative; <br>  2) exemples d'utilisation de filtres: notamment dans le cadre de l'égaliseur du réseau d'antennes; <br>  3) des liens vers la littérature de base et les bibliothèques ouvertes (en python), qui peuvent être utiles pour la recherche. </p><br><p>  En général, bienvenue et trions tout par points. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/2f1/98b/d67/2f198bd673789161db58ad770c629faf.jpg"></p><a name="habracut"></a><br><p>  <em>La personne pensive sur la photo est familière à beaucoup, je pense, Norbert Wiener.</em>  <em>Pour la plupart, nous étudierons le filtre de son nom.</em>  <em>Cependant, on ne peut pas ne pas mentionner notre compatriote - Andrei Nikolaevich Kolmogorov, dont l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article de 1941 a</a> également apporté une contribution significative au développement de la théorie du filtrage optimal, qui même dans les sources anglaises est appelée la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">théorie du filtrage Kolmogorov-Wiener</a> .</em> </p><br><h2 id="chto-rassmatrivaem">  Que pensons-nous? </h2><br><p>  Aujourd'hui, nous envisageons un filtre classique à réponse impulsionnelle finie (FIR, réponse impulsionnelle finie), qui peut être décrit par le circuit simple suivant (Fig. 1). </p><br><p><img src="https://habrastorage.org/webt/to/or/u7/tooru7vj_f6aj0oe9wvpcde28kw.png"></p><br><p>  <em>Fig.1.</em>  <em>Le schéma de filtrage FIR pour étudier le filtre de Wiener [1.</em>  <em>p.117]</em> </p><br><p>  Nous écrirons sous forme matricielle ce qui sera exactement à la sortie de ce stand: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/43f/615/39b/43f61539ba1f68998b24c39a8c539706.svg" alt="e (n) = d (n) - \ hat {d} (n | \ mathcal {U} _n) = d (n) - \ mathbf {w} ^ H \ mathbf {u} \ qquad (1)"></div><br><p>  Déchiffrez la notation: </p><br><ul><li><img src="https://habrastorage.org/getpro/habr/post_images/6c5/882/04e/6c588204ed25c8bbb270106d7f08a4dd.svg" alt="e (n)">  Est la différence (erreur) entre les signaux donnés et reçus </li><li><img src="https://habrastorage.org/getpro/habr/post_images/771/807/bef/771807bef08a5612654d97e67695cf07.svg" alt="d (n)">  Est un signal prédéfini </li><li><img src="https://habrastorage.org/getpro/habr/post_images/960/b4e/a48/960b4ea48f3968f42c64eed1af640e1d.svg" alt="\ mathbf {u}">  Est un vecteur d'échantillons ou, en d'autres termes, un signal à l'entrée du filtre </li><li><img src="https://habrastorage.org/getpro/habr/post_images/75f/f22/a5a/75ff22a5a4f95cbe489056bf704597f0.svg" alt="\ hat {d} (n | \ mathcal {U} _n)">  Le signal est-il à la sortie du filtre </li><li><img src="https://habrastorage.org/getpro/habr/post_images/6f6/81f/9be/6f681f9be2ae30d1666fec498b59b3a3.svg" alt="\ mathbf {w} ^ H">  - c'est une conjugaison hermitienne du vecteur de coefficient de filtre - <u>c'est dans leur sélection optimale que réside l'adaptabilité du filtre</u> </li></ul><br><p>  Vous avez probablement déjà deviné que nous nous efforcerons de rechercher la plus petite différence entre le signal donné et le signal filtré, c'est-à-dire la plus petite erreur.  Cela signifie que nous sommes confrontés à une tâche d'optimisation. </p><br><h2 id="chto-budem-optimizirovat">  Qu'allons-nous optimiser? </h2><br><p>  Pour optimiser, ou plutôt minimiser, nous ne <strong>signifierons</strong> pas seulement <strong>l'</strong> erreur, l' <strong>erreur</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">quadratique moyenne</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MSE - Mean Sqared Error</a> ): </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b83/b62/193/b83b62193ca3490681c2cd8910e4d99a.svg" alt="MSE: J (\ mathbf {w}) = E \ {e (n) ^ 2 \} \ qquad (2)"></div><br><p>  où <img src="https://habrastorage.org/getpro/habr/post_images/c48/76a/b10/c4876ab1024579fa30ea997a45efd50a.svg" alt="J (\ mathbf {w})">  désigne la fonction de coût du vecteur de coefficients de filtre, et <img src="https://habrastorage.org/getpro/habr/post_images/f39/8e4/ded/f398e4ded6db9e55108d575a9b7d2f1f.svg" alt="E \ {* \}">  désigne le mat.  en attente. </p><br><p>  Le carré dans ce cas est très agréable, car cela signifie que nous sommes confrontés au problème de la <em>programmation convexe</em> (je n'ai recherché qu'un tel analogue de l' <em>optimisation convexe</em> anglaise), ce qui, à son tour, implique un <u>seul extremum</u> (dans notre cas, un minimum). </p><br><p><img src="https://habrastorage.org/webt/hr/xj/mb/hrxjmbmimv7c2uvvicnuklqn9y0.png"></p><br><p>  <em>Fig.2.</em>  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">La surface de l'erreur quadratique moyenne</a> .</em> </p><br><p>  Notre fonction d'erreur a une forme canonique [1, p. 121]: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5da/121/788/5da121788f801656b55ee459c2f4d56d.svg" alt="J (\ mathbf {w}) = \ sigma ^ 2_d - \ mathbf {w} ^ H \ mathbf {p} - \ mathbf {p} ^ H \ mathbf {w} + \ mathbf {w} ^ H \ mathbf { R} \ mathbf {w} \ qquad (3)"></div><br><p>  où <img src="https://habrastorage.org/getpro/habr/post_images/24f/50c/410/24f50c410ab0c2ca3dd302c630c734e8.svg" alt="\ sigma ^ 2_d">  Est la variance du signal attendu, <img src="https://habrastorage.org/getpro/habr/post_images/82c/861/559/82c86155992ccb2e83d6f9f3f9e92737.svg" alt="\ mathbf {p} = E \ {\ mathbf {u} (n) d ^ * (n) \}">  Est le vecteur de corrélation croisée entre le vecteur d'entrée et le signal attendu, et <img src="https://habrastorage.org/getpro/habr/post_images/664/01c/a88/66401ca883516093b2e73b7d519588ac.svg" alt="\ mathbf {R} = E \ {\ mathbf {u} (n) \ mathbf {u} ^ H (n) \}">  Est la matrice d'autocorrélation du signal d'entrée. </p><br><div class="spoiler">  <b class="spoiler_title">La conclusion de cette formule est là (je l'ai essayée plus clairement).</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/q6/sw/p5/q6swp5meopsxauj7yvygkwuc7-g.png" width="650"></div></div><br><p>  Comme nous l'avons noté ci-dessus, si nous parlons de programmation convexe, nous aurons alors un extremum (minimum).  Ainsi, pour trouver la valeur minimale de la fonction de coût (l'erreur quadratique moyenne minimale), il suffit de trouver la tangente de la pente de la tangente ou, en d'autres termes, la <u>dérivée partielle</u> par <u>rapport</u> à notre variable étudiée: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1cd/d0b/114/1cdd0b114b5457761dd27338b4ee57f4.svg" alt="\ frac {\ delta J (\ mathbf {w})} {\ delta w ^ *} = - \ mathbf {p} + \ mathbf {R} \ mathbf {w} \ qquad (4)"></div><br><p>  Dans le meilleur des cas ( <img src="https://habrastorage.org/getpro/habr/post_images/ac6/219/d1c/ac6219d1cc1885e6f5936e40b5c7a980.svg" alt="\ mathbf {w} = \ mathbf {w} _ {opt}">  ), l'erreur devrait, bien sûr, être minimale, ce qui signifie que nous assimilons la dérivée à zéro: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/777/46e/9a7/77746e9a7ab3ff9df3cf340da4b7238d.svg" alt="\ mathbf {R} \ mathbf {w} _ {opt} = \ mathbf {p} \ qquad (5)"></div><br><p>  En fait, le voici, notre poêle, à partir duquel nous danserons plus loin: devant nous est un <u>système d'équations linéaires</u> . </p><br><h2 id="kak-budem-reshat">  Comment allons-nous décider? </h2><br><p>  Il convient de noter tout de suite que les deux solutions, que nous examinerons ci-dessous, dans ce cas sont théoriques et pédagogiques, puisque <img src="https://habrastorage.org/getpro/habr/post_images/1cf/d71/499/1cfd714992b16fcc961ad10bcc855134.svg" alt="\ mathbf {R}">  et <img src="https://habrastorage.org/getpro/habr/post_images/1fa/0e8/e9e/1fa0e8e9e33d7dbd533901bbf025bd9f.svg" alt="\ mathbf {p}">  connu à l'avance (c'est-à-dire que nous avions la capacité présumée de collecter suffisamment de statistiques pour les calculer).  Cependant, l'analyse de tels exemples simplifiés est la meilleure à laquelle vous pouvez penser pour comprendre les approches de base. </p><br><h3 id="analiticheskoe-reshenie">  Solution analytique </h3><br><p>  Ce problème peut être résolu, pour ainsi dire, dans le front - en utilisant des matrices inverses: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/759/c3e/052/759c3e052c502aa04fe6da682a41ea2a.svg" alt="\ mathbf {w} _ {opt} = \ mathbf {R} ^ {- 1} \ mathbf {p} \ qquad (6)"></div><br><p>  Une telle expression est appelée <strong>l'</strong> équation de Wiener-Hopf - elle nous sert encore de référence. </p><br><blockquote>  Bien sûr, pour être complètement méticuleux, il serait probablement plus correct d'écrire ce cas de manière générale, c'est-à-dire  pas avec <img src="https://habrastorage.org/getpro/habr/post_images/bd8/f0f/04a/bd8f0f04a92fe1055c350d4e32a8a256.svg" alt="^ {-}">  , et avec <img src="https://habrastorage.org/getpro/habr/post_images/017/3d5/ed9/0173d5ed99b25d00ec4245287142f165.svg" alt="^ +">  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pseudo-inversion</a> ): <br><img src="https://habrastorage.org/getpro/habr/post_images/003/47f/bca/00347fbca15f40943c7fb7b20c38a3f9.svg" alt="\ mathbf {R} ^ + = \ mathbf {R} ^ H (\ mathbf {R} \ mathbf {R} ^ H) ^ {- 1}"><br><br>  Cependant, la matrice d'autocorrélation ne peut pas être non carrée ou singulière, donc, à juste titre, nous pensons qu'il n'y a pas de contradiction. </blockquote><p>  De cette équation, il est analytiquement possible de déduire ce que la valeur minimale de la fonction de coût sera égale (c'est-à-dire, dans notre cas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MMSE</a> - erreur quadratique moyenne minimale): </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c00/d2d/1f9/c00d2d1f9427762a17deae92ae3ea77c.svg" alt="J_ {min} = J (\ mathbf {w} _ {opt}) = \ sigma ^ 2_d - \ mathbf {p} ^ H \ mathbf {R} ^ {- 1} \ mathbf {p} \ qquad (7)"></div><br><div class="spoiler">  <b class="spoiler_title">La dérivation de la formule est là (j'ai aussi essayé de colorer plus coloré).</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/ds/vh/nx/dsvhnxoudmo_qd4hesg_qkysdg8.jpeg"></p></div></div><br><p>  Eh bien, il y a une solution. </p><br><h3 id="reshenie-iterativnym-metodom">  Solution itérative </h3><br><p>  Cependant, oui, il est possible de résoudre un système d'équations linéaires sans inverser la matrice d'autocorrélation - de manière itérative ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pour économiser les calculs</a> ).  À cette fin, considérons la <strong>méthode</strong> native et compréhensible <strong>de descente à gradient</strong> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">méthode de descente la plus raide / à gradient</a> ). </p><br><p>  L'essence de l'algorithme peut être réduite à ce qui suit: </p><br><ol><li>  Nous définissons la variable souhaitée sur une valeur par défaut (par exemple, <img src="https://habrastorage.org/getpro/habr/post_images/231/554/6f0/2315546f0e8aa127a8da693d41c53ff6.svg" alt="\ mathbf {w} (0) = \ mathbf {0}">  ) </li><li>  Choisissez une étape <img src="https://habrastorage.org/getpro/habr/post_images/849/a42/16c/849a4216c1bc55877bc86f4a97513f7a.svg" alt="\ mu">  (comment exactement nous choisissons, nous parlerons ci-dessous). </li><li>  Et puis, pour ainsi dire, nous descendons le long de notre surface d'origine (dans notre cas, c'est la surface MSE) avec une étape donnée <img src="https://habrastorage.org/getpro/habr/post_images/849/a42/16c/849a4216c1bc55877bc86f4a97513f7a.svg" alt="\ mu">  et une certaine vitesse déterminée par l'amplitude du gradient. </li></ol><br><p>  D'où le nom: <em>gradient</em> - gradient ou <em>plus raide</em> - <em>descente</em> pas à pas - descente. </p><br><p>  Le gradient dans notre cas est déjà connu: en fait, nous l'avons trouvé en différenciant la fonction de coût (la surface est concave, comparer avec [1, p. 220]).  Nous écrivons à quoi ressemblera la formule de mise à jour itérative de la variable souhaitée (coefficients de filtre) [1, p.  220]: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6d6/4ff/7eb/6d64ff7eb98c4274e05a33a3eb127933.svg" alt="\ mathbf {w} (n + 1) = \ mathbf {w} (n) - \ mu [- \ mathbf {p} + \ mathbf {R} \ mathbf {w} (n)] \ qquad (8)"></div><br><p>  où <img src="https://habrastorage.org/getpro/habr/post_images/fd6/0b2/b5b/fd60b2b5be4b7e93a0d905dd970c314f.svg" alt="n">  Est le numéro d'itération. </p><br><p>  Parlons maintenant du choix d'une taille de pas. </p><br><p>  Nous listons les prémisses évidentes: </p><br><ul><li>  l'étape ne peut pas être négative ou nulle </li><li>  le pas ne doit pas être trop grand, sinon l'algorithme ne convergera pas (il va, pour ainsi dire, sauter de bord en bord, sans tomber dans l'extrême) </li><li>  l'étape, bien sûr, peut être très petite, mais ce n'est pas non plus entièrement souhaitable - l'algorithme passera plus de temps </li></ul><br><p>  Concernant le filtre de Wiener, des restrictions ont bien sûr déjà été trouvées il y a longtemps [1, p. 222-226]: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/67d/64a/b6c/67d64ab6cf46d3438791ccea853421fb.svg" alt="0 <\ mu <\ frac {2} {\ lambda_ {max}} \ qquad (9)"></div><br><p>  où <img src="https://habrastorage.org/getpro/habr/post_images/e5d/fa8/35d/e5dfa835ded907ecd7cf2b56d7061307.svg" alt="\ lambda_ {max}">  Est la plus grande valeur propre de la matrice d'autocorrélation <img src="https://habrastorage.org/getpro/habr/post_images/1cf/d71/499/1cfd714992b16fcc961ad10bcc855134.svg" alt="\ mathbf {R}">  . </p><br><blockquote>  Soit dit en passant, les valeurs propres et les vecteurs sont un sujet intéressant distinct dans le contexte du filtrage linéaire.  Il existe même un <em>filtre propre</em> complet <em>pour</em> ce cas (voir l'annexe 1). </blockquote><p>  Mais ce n'est heureusement pas tout.  Il existe également une solution adaptative optimale: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f4c/3c2/fdf/f4c3c2fdf8ae926094bb67b391cd896b.svg" alt="\ mu (n) = \ frac {\ mathbf {\ gamma} (n) ^ H \ mathbf {\ gamma} (n)} {\ mathbf {\ gamma} (n) ^ H \ mathbf {R} \ mathbf { \ gamma} (n)} \ qquad (10)"></div><br><p>  où <img src="https://habrastorage.org/getpro/habr/post_images/1b0/4a6/a31/1b04a6a318f99ec501290f59c0f924ac.svg" alt="\ mathbf {\ gamma} (n) = \ mathbf {p} - \ mathbf {R} \ mathbf {w} (n)">  Est un gradient négatif.  Comme le montre la formule, l'étape est recalculée à chaque itération, c'est-à-dire qu'elle s'adapte. </p><br><div class="spoiler">  <b class="spoiler_title">La conclusion de la formule est ici (beaucoup de mathématiques - ne regardez que les mêmes nerds notoires comme moi).</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/au/hq/0s/auhq0sxrspaduxdkclqctns1xtw.jpeg"></p></div></div><br><p>  D'accord, pour la deuxième décision, nous avons également préparé le terrain. </p><br><h2 id="a-nelzya-li-na-primerah">  Mais est-ce possible avec des exemples? </h2><br><p>  Par souci de clarté, nous allons effectuer une petite simulation.  Nous utiliserons <strong>Python 3.6.4</strong> . </p><br><blockquote>  Je dirai tout de suite que ces exemples font partie de l'un des devoirs, chacun étant proposé aux étudiants pour solution dans les deux semaines.  J'ai réécrit la partie sous python (afin de vulgariser le langage auprès des ingénieurs radio).  Peut-être que vous rencontrerez d'autres options sur le Web d'autres anciens élèves. </blockquote><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> scipy.linalg <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> toeplitz <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">convmtx</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(h,n)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> toeplitz(np.hstack([h, np.zeros(n<span class="hljs-number"><span class="hljs-number">-1</span></span>)]),\ np.hstack([h[<span class="hljs-number"><span class="hljs-number">0</span></span>], np.zeros(n<span class="hljs-number"><span class="hljs-number">-1</span></span>)])) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">MSE_calc</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(sigmaS, R, p, w)</span></span></span><span class="hljs-function">:</span></span> w = w.reshape(w.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>) wH = np.conj(w).reshape(<span class="hljs-number"><span class="hljs-number">1</span></span>, w.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]) p = p.reshape(p.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>) pH = np.conj(p).reshape(<span class="hljs-number"><span class="hljs-number">1</span></span>, p.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]) MSE = sigmaS - np.dot(wH, p) - np.dot(pH, w) + np.dot(np.dot(wH, R), w) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> MSE[<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">mu_opt_calc</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(gamma, R)</span></span></span><span class="hljs-function">:</span></span> gamma = gamma.reshape(gamma.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>) gammaH = np.conj(gamma).reshape(<span class="hljs-number"><span class="hljs-number">1</span></span>, gamma.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]) mu_opt = np.dot(gammaH, gamma) / np.dot(np.dot(gammaH, R), gamma) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> mu_opt[<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>]</code> </pre> <br><p>  Nous utiliserons notre filtre linéaire pour le problème d' <u>égalisation de canal</u> , dont le but principal est de niveler les différents effets de ce canal sur le signal utile. </p><br><blockquote>  Le code source peut être téléchargé dans un fichier <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> ou <a href="">ici</a> (oui, j'avais un tel passe-temps - éditer Wikipedia). </blockquote><br><h3 id="model-sistemy">  Modèle de système </h3><br><p>  Supposons qu'il existe un réseau d'antennes (nous l'avons déjà examiné dans un article sur la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MUSIQUE</a> ). </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/61a/9c2/da7/61a9c2da745081459f9001d0252936f1.png"></p><br><p>  <em>Fig.</em>  <em>3. Réseau d'antennes linéaires non directionnelles (ULAA - réseau d'antennes linéaires uniformes) [2, p.</em>  <em>32].</em> </p><br><p>  Définissez les paramètres initiaux du réseau: </p><br><pre> <code class="python hljs">M = <span class="hljs-number"><span class="hljs-number">5</span></span> <span class="hljs-comment"><span class="hljs-comment">#    (number of sensors)</span></span></code> </pre> <br><p>  Dans cet article, nous considérerons quelque chose comme un <u>canal à large bande avec évanouissement</u> , dont une caractéristique est <u>la propagation par trajets multiples</u> .  Pour de tels cas, une approche est généralement appliquée dans laquelle chaque faisceau est modélisé en utilisant un retard d'une certaine amplitude (Fig. 4). </p><br><p><img src="https://habrastorage.org/webt/3t/tc/va/3ttcvau0o4njat-1beejefcudpy.png"></p><br><p>  <em>Fig.</em>  <em>4. Le modèle du canal large bande avec n retards fixes [3, p.</em>  <em>29].</em>  <em>Comme vous le comprenez, les désignations spécifiques ne jouent aucun rôle - dans ce qui suit, nous utiliserons des désignations légèrement différentes.</em> </p><br><p>  Le modèle du signal reçu pour un capteur est exprimé comme suit: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a3f/c3f/2f0/a3fc3f2f0cd278622946ce2da28005fd.svg" alt="x (n) = \ sum_ {l = 0} ^ Lh (l) s (n-l) + \ nu (n)"></div><br><p>  Dans ce cas <img src="https://habrastorage.org/getpro/habr/post_images/fd6/0b2/b5b/fd60b2b5be4b7e93a0d905dd970c314f.svg" alt="n">  indique le numéro de référence, <img src="https://habrastorage.org/getpro/habr/post_images/e9f/c39/8e5/e9fc398e58e24442ddc2cf11684debbc.svg" alt="h (l)">  Est la réponse du canal le long du <em>l-</em> ème faisceau, <em>L</em> est le nombre de registres de retard, <em>s</em> est le signal (utile) transmis, <img src="https://habrastorage.org/getpro/habr/post_images/270/81a/400/27081a40025995898a2b982ff59a7e39.svg" alt="\ nu (n)">  - bruit additif. </p><br><p>  Pour plusieurs capteurs, la formule prendra la forme: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c0e/835/aa7/c0e835aa74d5dccf4ccdf625ebcc88b4.svg" alt="\ mathbf {x} (n) = \ mathbf {H} \ mathbf {s} (n) + \ mathbf {\ nu} (n)"></div><br><p>  où <img src="https://habrastorage.org/getpro/habr/post_images/b0e/184/0ed/b0e1840edef9169f3e1f52974bea066d.svg" alt="\ mathbf {x} (n)">  et <img src="https://habrastorage.org/getpro/habr/post_images/270/81a/400/27081a40025995898a2b982ff59a7e39.svg" alt="\ mathbf {\ nu} (n)">  - avoir une dimension <img src="https://habrastorage.org/getpro/habr/post_images/132/724/0f2/1327240f26480a83dffca393bb730c44.svg" alt="M \ fois 1">  dimension <img src="https://habrastorage.org/getpro/habr/post_images/ed0/8c1/e77/ed08c1e77cfaf357d5e90e9e2ae918aa.svg" alt="\ mathbf {H}">  est égal à <img src="https://habrastorage.org/getpro/habr/post_images/25e/e63/fbb/25ee63fbb4a32f1a00d5c02aaea9b80c.svg" alt="M \ fois (M-L)">  et la dimension <img src="https://habrastorage.org/getpro/habr/post_images/5e3/d36/045/5e3d360455a5a33db6c17f93c119a694.svg" alt="\ mathbf {s} (n)">  est égal <img src="https://habrastorage.org/getpro/habr/post_images/6e2/fc1/636/6e2fc16365ac2698555dd979ed6b5eeb.svg" alt="(M-L) \ fois 1">  . </p><br><p>  Supposons que chaque capteur reçoive également un signal avec un certain retard, en raison de l'incidence de l'onde à un angle.  Matrix <img src="https://habrastorage.org/getpro/habr/post_images/ed0/8c1/e77/ed08c1e77cfaf357d5e90e9e2ae918aa.svg" alt="\ mathbf {H}">  dans notre cas, il s'agira d'une matrice convolutionnelle pour le vecteur de réponse de chaque rayon.  Je pense que le code sera plus clair: </p><br><pre> <code class="python hljs">h = np.array([<span class="hljs-number"><span class="hljs-number">0.722</span></span><span class="hljs-number"><span class="hljs-number">-1j</span></span>*<span class="hljs-number"><span class="hljs-number">0.779</span></span>, <span class="hljs-number"><span class="hljs-number">-0.257</span></span><span class="hljs-number"><span class="hljs-number">-1j</span></span>*<span class="hljs-number"><span class="hljs-number">0.722</span></span>, <span class="hljs-number"><span class="hljs-number">-0.789</span></span><span class="hljs-number"><span class="hljs-number">-1j</span></span>*<span class="hljs-number"><span class="hljs-number">1.862</span></span>]) L = len(h)<span class="hljs-number"><span class="hljs-number">-1</span></span> <span class="hljs-comment"><span class="hljs-comment"># number of signal sources H = convmtx(h,ML) print(H.shape) print(H)</span></span></code> </pre> <br><p>  La conclusion sera: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>) &gt;&gt;&gt; array([[ <span class="hljs-number"><span class="hljs-number">0.722</span></span><span class="hljs-number"><span class="hljs-number">-0.779j</span></span>, <span class="hljs-number"><span class="hljs-number">0.</span></span> +<span class="hljs-number"><span class="hljs-number">0.j</span></span> , <span class="hljs-number"><span class="hljs-number">0.</span></span> +<span class="hljs-number"><span class="hljs-number">0.j</span></span> ], [<span class="hljs-number"><span class="hljs-number">-0.257</span></span><span class="hljs-number"><span class="hljs-number">-0.722j</span></span>, <span class="hljs-number"><span class="hljs-number">0.722</span></span><span class="hljs-number"><span class="hljs-number">-0.779j</span></span>, <span class="hljs-number"><span class="hljs-number">0.</span></span> +<span class="hljs-number"><span class="hljs-number">0.j</span></span> ], [<span class="hljs-number"><span class="hljs-number">-0.789</span></span><span class="hljs-number"><span class="hljs-number">-1.862j</span></span>, <span class="hljs-number"><span class="hljs-number">-0.257</span></span><span class="hljs-number"><span class="hljs-number">-0.722j</span></span>, <span class="hljs-number"><span class="hljs-number">0.722</span></span><span class="hljs-number"><span class="hljs-number">-0.779j</span></span>], [ <span class="hljs-number"><span class="hljs-number">0.</span></span> +<span class="hljs-number"><span class="hljs-number">0.j</span></span> , <span class="hljs-number"><span class="hljs-number">-0.789</span></span><span class="hljs-number"><span class="hljs-number">-1.862j</span></span>, <span class="hljs-number"><span class="hljs-number">-0.257</span></span><span class="hljs-number"><span class="hljs-number">-0.722j</span></span>], [ <span class="hljs-number"><span class="hljs-number">0.</span></span> +<span class="hljs-number"><span class="hljs-number">0.j</span></span> , <span class="hljs-number"><span class="hljs-number">0.</span></span> +<span class="hljs-number"><span class="hljs-number">0.j</span></span> , <span class="hljs-number"><span class="hljs-number">-0.789</span></span><span class="hljs-number"><span class="hljs-number">-1.862j</span></span>]])</code> </pre> <br><p>  Ensuite, nous définissons les données initiales pour le signal et le bruit utiles: </p><br><pre> <code class="python hljs">sigmaS = <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-comment"><span class="hljs-comment">#    (the signal's s(n) power) sigmaN = 0.01 #   (the noise's n(n) power)</span></span></code> </pre> <br><p>  Passons maintenant aux corrélations. </p><br><pre> <code class="python hljs">Rxx = (sigmaS)*(np.dot(H,np.matrix(H).H))+(sigmaN)*np.identity(M) p = (sigmaS)*H[:,<span class="hljs-number"><span class="hljs-number">0</span></span>] p = p.reshape((len(p), <span class="hljs-number"><span class="hljs-number">1</span></span>))</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">La dérivation des formules ici (aussi une feuille pour les plus désespérés).</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/hi/lh/ks/hilhksxoc_rkum_5ibn3m42ukxc.jpeg"></p></div></div><br><p>  Nous trouvons une solution pour Wiener: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Solution of the Wiener-Hopf equation: wopt = np.dot(np.linalg.inv(Rxx), p) MSEopt = MSE_calc(sigmaS, Rxx, p, wopt)</span></span></code> </pre> <br><p>  Passons maintenant à la méthode de descente en gradient. </p><br><p>  Trouvez la plus grande valeur propre afin que la limite supérieure de l'étape puisse en être dérivée (voir formule (9)): </p><br><pre> <code class="python hljs">lamda_max = max(np.linalg.eigvals(Rxx))</code> </pre> <br><p>  Maintenant, définissons un tableau d'étapes qui représentera une certaine fraction du maximum: </p><br><pre> <code class="python hljs">coeff = np.array([<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0.9</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.2</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>]) mus = <span class="hljs-number"><span class="hljs-number">2</span></span>/lamda_max*coeff <span class="hljs-comment"><span class="hljs-comment"># different step sizes</span></span></code> </pre> <br><p>  Définissez le nombre maximum d'itérations: </p><br><pre> <code class="python hljs">N_steps = <span class="hljs-number"><span class="hljs-number">100</span></span></code> </pre> <br><p>  Exécutez l'algorithme: </p><br><pre> <code class="python hljs">MSE = np.empty((len(mus), N_steps), dtype=complex) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> mu_idx, mu <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(mus): w = np.zeros((M,<span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=complex) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> N_i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(N_steps): w = w - mu*(np.dot(Rxx, w) - p) MSE[mu_idx, N_i] = MSE_calc(sigmaS, Rxx, p, w)</code> </pre> <br><p>  Maintenant, nous ferons de même, mais pour l'étape adaptative (formule (10)): </p><br><pre> <code class="python hljs">MSEoptmu = np.empty((<span class="hljs-number"><span class="hljs-number">1</span></span>, N_steps), dtype=complex) w = np.zeros((M,<span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=complex) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> N_i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(N_steps): gamma = p - np.dot(Rxx,w) mu_opt = mu_opt_calc(gamma, Rxx) w = w - mu_opt*(np.dot(Rxx,w) - p) MSEoptmu[:, N_i] = MSE_calc(sigmaS, Rxx, p, w)</code> </pre> <br><p>  Vous devriez obtenir quelque chose comme ceci: </p><br><div class="spoiler">  <b class="spoiler_title">Dessin</b> <div class="spoiler_text"><pre> <code class="python hljs">x = [i <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, N_steps+<span class="hljs-number"><span class="hljs-number">1</span></span>)] plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>), dpi=<span class="hljs-number"><span class="hljs-number">300</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> idx, item <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(coeff): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> item == <span class="hljs-number"><span class="hljs-number">1</span></span>: item = <span class="hljs-string"><span class="hljs-string">''</span></span> plt.loglog(x, np.abs(MSE[idx, :]),\ label=<span class="hljs-string"><span class="hljs-string">'$\mu = '</span></span>+str(item)+<span class="hljs-string"><span class="hljs-string">'\mu_{max}$'</span></span>) plt.loglog(x, np.abs(MSEoptmu[<span class="hljs-number"><span class="hljs-number">0</span></span>, :]),\ label=<span class="hljs-string"><span class="hljs-string">'$\mu = \mu_{opt}$'</span></span>) plt.loglog(x, np.abs(MSEopt*np.ones((len(x), <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=complex)),\ label = <span class="hljs-string"><span class="hljs-string">'Wiener solution'</span></span>) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) plt.xlabel(<span class="hljs-string"><span class="hljs-string">'Number of steps'</span></span>) plt.ylabel(<span class="hljs-string"><span class="hljs-string">'Mean-Square Error'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'Steepest descent'</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'best'</span></span>) plt.minorticks_on() plt.grid(which=<span class="hljs-string"><span class="hljs-string">'major'</span></span>) plt.grid(which=<span class="hljs-string"><span class="hljs-string">'minor'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">':'</span></span>) plt.show()</code> </pre> </div></div><br><p><img src="https://habrastorage.org/webt/il/fa/8d/ilfa8dmoxgt4sjitiyvwbdjga6m.png"></p><br><p>  <em>Fig.</em>  <em>5. Courbes d'apprentissage pour les étapes de différentes tailles.</em> </p><br><p>  Fixations pour énoncer les principaux points de la descente en pente: </p><br><ul><li>  comme prévu, l'étape optimale donne la convergence la plus rapide; </li><li>  ne signifie plus mieux: après avoir dépassé la limite supérieure, nous n'avons pas du tout atteint la convergence. </li></ul><br><p>  Nous avons donc trouvé le vecteur optimal de coefficients de filtre qui égalisera le mieux les effets du canal - nous avons <u>formé l'égaliseur</u> . </p><br><h2 id="a-est-chto-to-bolee-blizkoe-k-realnosti">  Y a-t-il quelque chose de plus proche de la réalité? </h2><br><p>  Bien sûr!  Nous avons déjà dit à plusieurs reprises que la collecte de statistiques (c'est-à-dire le calcul de matrices de corrélation et de vecteurs) dans des systèmes en temps réel est loin d'être toujours un luxe abordable.  Cependant, l'humanité s'est adaptée à ces difficultés: au lieu d'une approche <em>déterministe</em> dans la pratique, <u>des</u> approches <u>adaptatives</u> sont utilisées.  Ils peuvent être divisés en deux grands groupes [1, p.  246]: </p><br><ul><li>  <em>probabiliste (stochastique)</em> (par exemple <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SG</a> - Gradient stochastique) </li><li>  et basé sur la méthode des <em>moindres carrés</em> (par exemple, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">LMS</a> - Least Mean Squares ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">RLS</a> - Recursive Least Squares) </li></ul><br><p>  Le sujet des filtres adaptatifs est bien représenté au sein de la communauté open-source (exemples pour python): </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pyroomacoustics</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">padasip</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">adaptfilt</a> </li></ul><br><blockquote>  Dans le deuxième exemple, j'aime particulièrement la documentation.  Attention cependant!  Lorsque j'ai testé le paquet <strong>padasip</strong> , j'ai rencontré des difficultés dans la gestion des nombres complexes (par défaut, float64 y est impliqué).  Peut-être que les mêmes problèmes peuvent survenir lorsque vous travaillez avec d'autres implémentations. </blockquote><p>  Bien entendu, les algorithmes ont leurs propres avantages et inconvénients, dont la somme détermine la portée de l'algorithme. </p><br><p>  Jetons un coup d'œil aux exemples: nous allons considérer les trois algorithmes <em>SG</em> , <em>LMS</em> et <em>RLS</em> que nous avons déjà mentionnés (nous modéliserons en langage MATLAB - j'avoue, il y avait déjà des blancs, et tout réécrire en uniformité python pour le bien de ... eh bien ...). </p><br><p>  Une description des algorithmes <em>LMS</em> et <em>RLS</em> peut être trouvée, par exemple, dans le dock <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">padasip</a> . </p><br><div class="spoiler">  <b class="spoiler_title">La description de SG peut être trouvée ici.</b> <div class="spoiler_text"><p>  La principale différence avec la descente du gradient est un gradient variable: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/76f/9e3/fbb/76f9e3fbbb4c7643f5af595103791091.svg" alt="\ mathbf {w} [n] = \ mathbf {w} [n-1] + \ mu \ left (\ mathbf {\ hat {p}} [n] - \ mathbf {\ hat {R}} _ {xx } [n] \ mathbf {w} [n-1] \ droite)"></div><br><p>  à </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/633/bac/0aa/633bac0aae95be15cd31a312b4e0d2c5.svg" alt="\ mathbf {\ hat {R}} _ {xx} [n] = \ frac {1} {n} \ left ((n-1) \ mathbf {\ hat {R}} _ {xx} [n-1 ] + \ mathbf {x} [n] \ mathbf {x} [n] ^ H \ droite)"></div><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4f7/bfd/f81/4f7bfdf81571e19eac474c7a8c380093.svg" alt="\ mathbf {\ hat {p}} [n] = \ frac {1} {n} \ left ((n-1) \ mathbf {\ hat {p}} [n-1] + \ mathbf {x} [ n] d [n] ^ * \ droite)"></div></div></div><br><p>  1) Un cas similaire à celui considéré ci-dessus. </p><br><div class="spoiler">  <b class="spoiler_title">Sources (MatLab / Octave).</b> <div class="spoiler_text"><p>  Les sources peuvent être téléchargées <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . </p></div></div><br><p><img src="https://habrastorage.org/webt/ff/zm/hq/ffzmhqsrnwvvc0hdrcyzhapropw.png"></p><br><p>  <em>Fig.</em>  <em>6. Courbes d'apprentissage pour LMS, RLS et SG.</em> </p><br><p>  On peut immédiatement noter qu'avec sa relative simplicité, l'algorithme LMS peut, en principe, ne pas aboutir à une solution optimale avec un pas relativement important.  RLS donne le résultat le plus rapide, mais il peut également échouer avec un <em>facteur d'oubli</em> relativement faible.  Jusqu'à présent, SG se porte bien, mais regardons un autre exemple. </p><br><p>  2) Le cas où le canal change dans le temps. </p><br><div class="spoiler">  <b class="spoiler_title">Sources (MatLab / Octave).</b> <div class="spoiler_text"><p>  Les sources peuvent être téléchargées <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . </p></div></div><br><p><img src="https://habrastorage.org/webt/v-/9d/sx/v-9dsxxwzr9jnbnswf0dmnvqrcu.png"></p><br><p>  <em>Fig.</em>  <em>7. Courbes d'apprentissage pour LMS, RLS et SG (changements de canaux dans le temps).</em> </p><br><p>  Et ici, l'image est déjà beaucoup plus intéressante: avec un fort changement dans la réponse du canal, le LMS semble déjà être la solution la plus fiable.  Qui aurait pensé.  Bien que RLS avec le bon facteur d'oubli fournisse également un résultat acceptable. </p><br><div class="spoiler">  <b class="spoiler_title">Quelques mots sur la performance.</b> <div class="spoiler_text"><p>  Oui, bien sûr, chaque algorithme a sa propre complexité de calcul, mais selon mes mesures, mon ancienne machine peut faire face à un ensemble pour environ 120 μs par itération dans le cas de LMS et SG et environ 250 μs par itération dans le cas de RLS.  Autrement dit, la différence est, en général, comparable. </p></div></div><br><p>  Et c'est tout pour aujourd'hui.  Merci à tous ceux qui ont regardé! </p><br><h2 id="literatura">  Littérature </h2><br><ol><li>  Théorie du filtre adaptatif Haykin SS.  - Pearson Education India, 2005. </li><li>  Haykin, Simon et KJ Ray Liu.  Manuel sur le traitement des réseaux et les réseaux de capteurs.  Vol.  63. John Wiley &amp; Sons, 2010. pp.  102-107 </li><li>  Arndt, D. (2015).  Modélisation On Channel pour la réception satellite terrestre mobile (thèse de doctorat). </li></ol><br><h2 id="prilozhenie-1">  Annexe 1 </h2><br><div class="spoiler">  <b class="spoiler_title">Filtre propre</b> <div class="spoiler_text"><p>  L'objectif principal d'un tel filtre est de maximiser le rapport signal / bruit (SNR). </p><br><p><img src="https://habrastorage.org/webt/kk/v_/uu/kkv_uu-08dppu5i4yhkucc_b0ww.jpeg"></p><br><p>  Mais à en juger par la présence de corrélations dans les calculs, il s'agit aussi davantage d'une construction théorique que d'une solution pratique. </p></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr455497/">https://habr.com/ru/post/fr455497/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr455483/index.html">Artiste Ai-Da: un robot humanoïde se prépare pour sa première exposition personnelle</a></li>
<li><a href="../fr455485/index.html">Check Point Scripts - exécutez des scripts directement à partir de la Smart Console</a></li>
<li><a href="../fr455487/index.html">Formation Cisco 200-125 CCNA v3.0. Jour 10. Changer les modes de fonctionnement du port</a></li>
<li><a href="../fr455489/index.html">Connexion de solutions audio et vidéo tierces à Microsoft Teams</a></li>
<li><a href="../fr455493/index.html">Nouveautés de la version Angular 8</a></li>
<li><a href="../fr455499/index.html">Extraction des dents de sagesse: comment cela se fait-il?</a></li>
<li><a href="../fr455501/index.html">Comment Hollywood utilise secrètement l'IA pour prendre des décisions de tournage clés</a></li>
<li><a href="../fr455503/index.html">19 concepts que vous devez apprendre pour devenir un développeur Angular efficace</a></li>
<li><a href="../fr455509/index.html">L'histoire de la raison pour laquelle j'utilise toujours jQuery</a></li>
<li><a href="../fr455511/index.html">Le sommeil est la principale ressource pour le cerveau d'un programmeur</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>