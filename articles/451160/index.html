<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üé™ üõ¢Ô∏è üè° Apache Kafka y Streaming con Spark Streaming üëèüèΩ üñïüèª üÜï</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola Habr! Hoy crearemos un sistema que utilizar√° Apark Kafka para procesar secuencias de mensajes utilizando Spark Streaming y escribir el resultado ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Apache Kafka y Streaming con Spark Streaming</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/451160/">  Hola Habr!  Hoy crearemos un sistema que utilizar√° Apark Kafka para procesar secuencias de mensajes utilizando Spark Streaming y escribir el resultado del procesamiento en la base de datos en la nube de AWS RDS. <br><br>  Imagine que cierta instituci√≥n de cr√©dito nos ha asignado la tarea de procesar transacciones entrantes sobre la marcha en todas sus sucursales.  Esto se puede hacer para calcular r√°pidamente la posici√≥n de moneda abierta para la tesorer√≠a, l√≠mites o resultados financieros para transacciones, etc. <br><br>  C√≥mo implementar este caso sin el uso de magia y hechizos m√°gicos: ¬°leemos debajo del corte!  Vamos! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/5w/sb/8v/5wsb8vvncrzhysct-pd6oqraqky.jpeg"></div><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">(Fuente de la imagen)</a> <br><a name="habracut"></a><br><h2>  Introduccion </h2><br>  Por supuesto, el procesamiento de una gran matriz de datos en tiempo real ofrece amplias oportunidades para su uso en sistemas modernos.  Una de las combinaciones m√°s populares para esto es el t√°ndem Apache Kafka y Spark Streaming, donde Kafka crea un flujo de paquetes de mensajes entrantes, y Spark Streaming procesa estos paquetes en un intervalo de tiempo espec√≠fico. <br><br>  Para aumentar la tolerancia a fallas de la aplicaci√≥n, utilizaremos puntos de control - puntos de control.  Usando este mecanismo, cuando el m√≥dulo Spark Streaming necesita recuperar datos perdidos, solo necesita regresar al √∫ltimo punto de control y reanudar los c√°lculos a partir de √©l. <br><br><h2>  Arquitectura del sistema en desarrollo. </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/od/ef/zc/odefzciug8ckvim4-ei6pdg49tw.png"></div><br><br>  Componentes utilizados: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><b>Apache Kafka</b></a> es un sistema de mensajer√≠a distribuido con publicaci√≥n y suscripci√≥n.  Adecuado para el consumo de mensajes fuera de l√≠nea y en l√≠nea.  Para evitar la p√©rdida de datos, los mensajes de Kafka se almacenan en el disco y se replican dentro del cl√∫ster.  El sistema Kafka est√° construido sobre el servicio de sincronizaci√≥n ZooKeeper; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><b>Apache Spark Streaming</b></a> : componente de Spark para procesar datos de transmisi√≥n.  El m√≥dulo Spark Streaming se construye utilizando la arquitectura de micro lotes, cuando el flujo de datos se interpreta como una secuencia continua de peque√±os paquetes de datos.  Spark Streaming recibe datos de varias fuentes y los combina en peque√±os paquetes.  Se crean nuevos paquetes a intervalos regulares.  Al comienzo de cada intervalo de tiempo, se crea un nuevo paquete y cualquier dato recibido durante este intervalo se incluye en el paquete.  Al final del intervalo, el crecimiento del paquete se detiene.  El tama√±o del intervalo est√° determinado por un par√°metro llamado intervalo de lote; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><b>Apache Spark SQL</b></a> : combina el procesamiento relacional con la programaci√≥n funcional de Spark.  Los datos estructurados se refieren a datos que tienen un esquema, es decir, un conjunto √∫nico de campos para todos los registros.  Spark SQL admite la entrada de una variedad de fuentes de datos estructurados y, gracias a la disponibilidad de informaci√≥n sobre el esquema, puede recuperar de manera eficiente solo los campos de registro requeridos, y tambi√©n proporciona las API de DataFrame; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><b>AWS RDS</b></a> es una base de datos relacional basada en la nube relativamente econ√≥mica, un servicio web que simplifica la configuraci√≥n, el funcionamiento y el escalado, y que Amazon administra directamente. </li></ul><br><h2>  Instalar e iniciar el servidor Kafka </h2><br>  Antes de usar Kafka directamente, debe asegurarse de que Java est√© disponible, ya que  JVM se usa para el trabajo: <br><br><pre><code class="bash hljs">sudo apt-get update sudo apt-get install default-jre java -version</code> </pre> <br>  Cree un nuevo usuario para trabajar con Kafka: <br><br><pre> <code class="bash hljs">sudo useradd kafka -m sudo passwd kafka sudo adduser kafka sudo</code> </pre><br>  A continuaci√≥n, descargue la distribuci√≥n del sitio web oficial de Apache Kafka: <br><br><pre> <code class="bash hljs">wget -P /YOUR_PATH <span class="hljs-string"><span class="hljs-string">"http://apache-mirror.rbc.ru/pub/apache/kafka/2.2.0/kafka_2.12-2.2.0.tgz"</span></span></code> </pre> <br>  Descomprima el archivo descargado: <br><pre> <code class="bash hljs">tar -xvzf /YOUR_PATH/kafka_2.12-2.2.0.tgz ln -s /YOUR_PATH/kafka_2.12-2.2.0 kafka</code> </pre><br>  El siguiente paso es opcional.  El hecho es que la configuraci√≥n predeterminada no permite el uso completo de todas las caracter√≠sticas de Apache Kafka.  Por ejemplo, elimine un tema, categor√≠a, grupo en el que se pueden publicar mensajes.  Para cambiar esto, edite el archivo de configuraci√≥n: <br><br><pre> <code class="bash hljs">vim ~/kafka/config/server.properties</code> </pre> <br>  Agregue lo siguiente al final del archivo: <br><br><pre> <code class="bash hljs">delete.topic.enable = <span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br>  Antes de iniciar el servidor Kafka, debe iniciar el servidor ZooKeeper, utilizaremos el script auxiliar que viene con la distribuci√≥n Kafka: <br><br><pre> <code class="bash hljs">Cd ~/kafka bin/zookeeper-server-start.sh config/zookeeper.properties</code> </pre><br>  Despu√©s de que ZooKeeper se inici√≥ con √©xito, en una terminal separada lanzamos el servidor Kafka: <br><br><pre> <code class="bash hljs">bin/kafka-server-start.sh config/server.properties</code> </pre> <br>  Cree un nuevo tema llamado Transacci√≥n: <br><br><pre> <code class="bash hljs">bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 3 --topic transaction</code> </pre> <br>  Aseg√∫rese de que se haya creado el tema con el n√∫mero correcto de particiones y replicaci√≥n: <br><br><pre> <code class="bash hljs">bin/kafka-topics.sh --describe --zookeeper localhost:2181</code> </pre> <br><img src="https://habrastorage.org/webt/s5/gh/bu/s5ghbuswhb0dcc0pmlvu_uloes4.png"><br><br>  Echaremos de menos los momentos de probar al productor y al consumidor para el tema reci√©n creado.  Para obtener m√°s detalles sobre c√≥mo probar el env√≠o y la recepci√≥n de mensajes, consulte la documentaci√≥n oficial: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">env√≠e algunos mensajes</a> .  Bueno, pasamos a escribir un productor en Python usando la API KafkaProducer. <br><br><h2>  Escritor de Productor </h2><br>  El productor generar√° datos aleatorios: 100 mensajes por segundo.  Por datos aleatorios nos referimos a un diccionario que consta de tres campos: <br><br><ul><li>  <b>Sucursal</b> : nombre del punto de venta de la entidad de cr√©dito; </li><li>  <b>Moneda</b> - <b>moneda de</b> transacci√≥n; </li><li>  <b>Monto</b> - monto de la transacci√≥n.  El monto ser√° un n√∫mero positivo si se trata de una compra de divisas por parte del Banco, y negativo si se trata de una venta. </li></ul><br>  El c√≥digo para el productor es el siguiente: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> numpy.random <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> choice, randint <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_random_value</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> new_dict = {} branch_list = [<span class="hljs-string"><span class="hljs-string">'Kazan'</span></span>, <span class="hljs-string"><span class="hljs-string">'SPB'</span></span>, <span class="hljs-string"><span class="hljs-string">'Novosibirsk'</span></span>, <span class="hljs-string"><span class="hljs-string">'Surgut'</span></span>] currency_list = [<span class="hljs-string"><span class="hljs-string">'RUB'</span></span>, <span class="hljs-string"><span class="hljs-string">'USD'</span></span>, <span class="hljs-string"><span class="hljs-string">'EUR'</span></span>, <span class="hljs-string"><span class="hljs-string">'GBP'</span></span>] new_dict[<span class="hljs-string"><span class="hljs-string">'branch'</span></span>] = choice(branch_list) new_dict[<span class="hljs-string"><span class="hljs-string">'currency'</span></span>] = choice(currency_list) new_dict[<span class="hljs-string"><span class="hljs-string">'amount'</span></span>] = randint(<span class="hljs-number"><span class="hljs-number">-100</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> new_dict</code> </pre><br>  A continuaci√≥n, utilizando el m√©todo de env√≠o, enviamos un mensaje al servidor, en el tema que necesitamos, en formato JSON: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> kafka <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> KafkaProducer producer = KafkaProducer(bootstrap_servers=[<span class="hljs-string"><span class="hljs-string">'localhost:9092'</span></span>], value_serializer=<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x:dumps(x).encode(<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>), compression_type=<span class="hljs-string"><span class="hljs-string">'gzip'</span></span>) my_topic = <span class="hljs-string"><span class="hljs-string">'transaction'</span></span> data = get_random_value() <span class="hljs-keyword"><span class="hljs-keyword">try</span></span>: future = producer.send(topic = my_topic, value = data) record_metadata = future.get(timeout=<span class="hljs-number"><span class="hljs-number">10</span></span>) print(<span class="hljs-string"><span class="hljs-string">'--&gt; The message has been sent to a topic: \ {}, partition: {}, offset: {}'</span></span> \ .format(record_metadata.topic, record_metadata.partition, record_metadata.offset )) <span class="hljs-keyword"><span class="hljs-keyword">except</span></span> Exception <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> e: print(<span class="hljs-string"><span class="hljs-string">'--&gt; It seems an Error occurred: {}'</span></span>.format(e)) <span class="hljs-keyword"><span class="hljs-keyword">finally</span></span>: producer.flush()</code> </pre><br>  Cuando ejecutamos el script, recibimos los siguientes mensajes en el terminal: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/_e/3g/zj/_e3gzjrmsycjb8ntjmur6ztaspw.png"></div><br>  Esto significa que todo funciona como quer√≠amos: el productor genera y env√≠a mensajes al tema que necesitamos. <br><br>  El siguiente paso es instalar Spark y procesar este flujo de mensajes. <br><br><h2>  Instalar Apache Spark </h2><br>  <b>Apache Spark</b> es una plataforma de computaci√≥n en cl√∫ster vers√°til y de alto rendimiento. <br><br>  En t√©rminos de rendimiento, Spark supera las implementaciones populares del modelo MapReduce, proporcionando simult√°neamente soporte para una gama m√°s amplia de tipos de c√°lculos, incluidas consultas interactivas y procesamiento de flujo.  La velocidad juega un papel importante en el procesamiento de grandes cantidades de datos, ya que es la velocidad la que le permite trabajar de manera interactiva sin perder minutos u horas de espera.  Una de las mayores fortalezas de Spark a una velocidad tan alta es su capacidad para realizar c√°lculos en memoria. <br><br>  Este marco est√° escrito en Scala, por lo que debe instalarlo primero: <br><br><pre> <code class="bash hljs">sudo apt-get install scala</code> </pre> <br>  Descargue la distribuci√≥n de Spark del sitio web oficial: <br><br><pre> <code class="bash hljs">wget <span class="hljs-string"><span class="hljs-string">"http://mirror.linux-ia64.org/apache/spark/spark-2.4.2/spark-2.4.2-bin-hadoop2.7.tgz"</span></span></code> </pre> <br>  Descomprima el archivo: <br><br><pre> <code class="bash hljs">sudo tar xvf spark-2.4.2/spark-2.4.2-bin-hadoop2.7.tgz -C /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/spark</code> </pre> <br>  Agregue la ruta al Spark en el archivo bash: <br><br><pre> <code class="bash hljs">vim ~/.bashrc</code> </pre> <br>  Agregue las siguientes l√≠neas a trav√©s del editor: <br><br><pre> <code class="bash hljs">SPARK_HOME=/usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/spark <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> PATH=<span class="hljs-variable"><span class="hljs-variable">$SPARK_HOME</span></span>/bin:<span class="hljs-variable"><span class="hljs-variable">$PATH</span></span></code> </pre><br>  Ejecute el siguiente comando despu√©s de realizar cambios en bashrc: <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">source</span></span> ~/.bashrc</code> </pre> <br><h2>  Implementaci√≥n de AWS PostgreSQL </h2><br>  Queda por implementar la base de datos, donde cargaremos la informaci√≥n procesada de las transmisiones.  Para esto utilizaremos el servicio AWS RDS. <br><br>  Vaya a la consola AWS -&gt; AWS RDS -&gt; Bases de datos -&gt; Crear base de datos: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/dg/os/m7/dgosm7dwnh3fr-uksjdt_xpltsk.png"></div><br>  Seleccione PostgreSQL y haga clic en el bot√≥n Siguiente: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/3y/d_/8r/3yd_8rsz2swfgaxaafpkyizthac.png"></div><br>  Porque  Este ejemplo se entiende √∫nicamente con fines educativos, utilizaremos un servidor gratuito "como m√≠nimo" (Nivel gratuito): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fn/6p/5b/fn6p5bjyitndy_ozs2cdcw_ssi0.png"></div><br>  A continuaci√≥n, marque el bloque Free Tier, y luego se nos ofrecer√° autom√°ticamente una instancia de la clase t2.micro, aunque d√©bil, es gratis y bastante adecuada para nuestra tarea: <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mj/jh/wg/mjjhwg3cknoehrq8wyxk3uw5v74.png"></div><br>  Siguen cosas muy importantes: el nombre de la instancia de la base de datos, el nombre del usuario maestro y su contrase√±a.  Pongamos nombre a la instancia: myHabrTest, el usuario maestro: <b>habr</b> , la contrase√±a: <b>habr12345</b> y haga clic en el bot√≥n Siguiente: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lg/jt/mf/lgjtmfdfst0pvqthojb_bdpeohc.png"></div><br><br>  La siguiente p√°gina contiene los par√°metros responsables de la disponibilidad de nuestro servidor de bases de datos desde el exterior (Accesibilidad p√∫blica) y la disponibilidad de puertos: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/40/z9/q7/40z9q7owar5kpnimyzrdj5laqgs.png"></div><br>  Creemos una nueva configuraci√≥n para el grupo de seguridad VPC, que nos permitir√° acceder a nuestro servidor de bases de datos desde el exterior a trav√©s del puerto 5432 (PostgreSQL). <br><br>  En una ventana separada del navegador, vaya a la consola de AWS en el Panel de VPC -&gt; Grupos de seguridad -&gt; secci√≥n Crear grupo de seguridad: <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fl/2i/ne/fl2inejlgnghwsh3itdrlcywdsu.png"></div><br>  Establezca el nombre para el grupo de Seguridad: PostgreSQL, una descripci√≥n, indique a qu√© VPC debe asociarse este grupo y haga clic en el bot√≥n Crear: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/js/8r/tv/js8rtvp8tudwjtpgso6xota5h-g.png"></div><br>  Complete el grupo de reglas de entrada reci√©n creado para el puerto 5432, como se muestra en la imagen a continuaci√≥n.  No tiene que especificar un puerto manual, pero seleccione PostgreSQL de la lista desplegable Tipo. <br><br>  Hablando estrictamente, el valor :: / 0 significa la disponibilidad de tr√°fico entrante para un servidor de todo el mundo, lo que can√≥nicamente no es del todo cierto, pero para analizar el ejemplo, usemos este enfoque: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ge/8j/bn/ge8jbntssnooajc8so36h0tjo80.png"></div><br>  Regresamos a la p√°gina del navegador, donde tenemos abierto "Configurar opciones avanzadas" y seleccionamos en la secci√≥n Grupos de seguridad de VPC -&gt; Elegir grupos de seguridad de VPC existentes -&gt; PostgreSQL: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nk/ae/-s/nkae-ste1tp3wgvmyilicvwlk8e.png"></div><br>  A continuaci√≥n, en la secci√≥n Opciones de la base de datos -&gt; Nombre de la base de datos -&gt; establezca el nombre - <b>habrDB</b> . <br><br>  Podemos dejar el resto de los par√°metros, con la excepci√≥n de deshabilitar la copia de seguridad (per√≠odo de retenci√≥n de la copia de seguridad - 0 d√≠as), la supervisi√≥n y las Estad√≠sticas de rendimiento, de forma predeterminada.  Haga clic en el bot√≥n <b>Crear base de datos</b> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ex/1p/po/ex1ppogq_vdsk3nnvywm7l8vq8i.png"></div><br><h2>  Controlador de flujo </h2><br>  El paso final ser√° el desarrollo de Spark-jobs, que procesar√° cada dos segundos nuevos datos procedentes de Kafka e ingresar√° el resultado en la base de datos. <br><br>  Como se se√±al√≥ anteriormente, los puntos de control son el mecanismo principal en SparkStreaming que debe configurarse para proporcionar tolerancia a fallas.  Usaremos puntos de control y, en caso de una ca√≠da del procedimiento, el m√≥dulo Spark Streaming solo necesitar√° regresar al √∫ltimo punto de control y reanudar los c√°lculos para restaurar los datos perdidos. <br><br>  Puede habilitar el punto de interrupci√≥n configurando el directorio en un sistema de archivos confiable y tolerante a fallas (por ejemplo, HDFS, S3, etc.), en el que se guardar√° la informaci√≥n del punto de interrupci√≥n.  Esto se hace usando, por ejemplo: <br><br><pre> <code class="python hljs">streamingContext.checkpoint(checkpointDirectory)</code> </pre> <br>  En nuestro ejemplo, utilizaremos el siguiente enfoque, es decir, si checkpointDirectory existe, entonces el contexto se recrear√° a partir de los datos del punto de control.  Si el directorio no existe (es decir, se ejecuta por primera vez), se llama a la funci√≥n functionToCreateContext para crear un nuevo contexto y configurar DStreams: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyspark.streaming <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> StreamingContext context = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext)</code> </pre><br>  Cree un objeto DirectStream para conectarse al tema "transacci√≥n" utilizando el m√©todo createDirectStream de la biblioteca KafkaUtils: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyspark.streaming.kafka <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> KafkaUtils sc = SparkContext(conf=conf) ssc = StreamingContext(sc, <span class="hljs-number"><span class="hljs-number">2</span></span>) broker_list = <span class="hljs-string"><span class="hljs-string">'localhost:9092'</span></span> topic = <span class="hljs-string"><span class="hljs-string">'transaction'</span></span> directKafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {<span class="hljs-string"><span class="hljs-string">"metadata.broker.list"</span></span>: broker_list})</code> </pre><br>  An√°lisis de datos entrantes en formato JSON: <br><br><pre> <code class="python hljs">rowRdd = rdd.map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> w: Row(branch=w[<span class="hljs-string"><span class="hljs-string">'branch'</span></span>], currency=w[<span class="hljs-string"><span class="hljs-string">'currency'</span></span>], amount=w[<span class="hljs-string"><span class="hljs-string">'amount'</span></span>])) testDataFrame = spark.createDataFrame(rowRdd) testDataFrame.createOrReplaceTempView(<span class="hljs-string"><span class="hljs-string">"treasury_stream"</span></span>)</code> </pre><br>  Usando Spark SQL, hacemos una agrupaci√≥n simple e imprimimos el resultado en la consola: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">select</span></span> from_unixtime(<span class="hljs-keyword"><span class="hljs-keyword">unix_timestamp</span></span>()) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> curr_time, t.branch <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> branch_name, t.currency <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> currency_code, <span class="hljs-keyword"><span class="hljs-keyword">sum</span></span>(amount) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> batch_value <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> treasury_stream t <span class="hljs-keyword"><span class="hljs-keyword">group</span></span> <span class="hljs-keyword"><span class="hljs-keyword">by</span></span> t.branch, t.currency</code> </pre><br>  Obtener texto de consulta y ejecutarlo a trav√©s de Spark SQL: <br><br><pre> <code class="python hljs">sql_query = get_sql_query() testResultDataFrame = spark.sql(sql_query) testResultDataFrame.show(n=<span class="hljs-number"><span class="hljs-number">5</span></span>)</code> </pre><br>  Y luego guardamos los datos agregados recibidos en una tabla en AWS RDS.  Para guardar los resultados de la agregaci√≥n en una tabla de base de datos, utilizaremos el m√©todo de escritura del objeto DataFrame: <br><br><pre> <code class="python hljs">testResultDataFrame.write \ .format(<span class="hljs-string"><span class="hljs-string">"jdbc"</span></span>) \ .mode(<span class="hljs-string"><span class="hljs-string">"append"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"driver"</span></span>, <span class="hljs-string"><span class="hljs-string">'org.postgresql.Driver'</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"url"</span></span>,<span class="hljs-string"><span class="hljs-string">"jdbc:postgresql://myhabrtest.ciny8bykwxeg.us-east-1.rds.amazonaws.com:5432/habrDB"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"dbtable"</span></span>, <span class="hljs-string"><span class="hljs-string">"transaction_flow"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"user"</span></span>, <span class="hljs-string"><span class="hljs-string">"habr"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"password"</span></span>, <span class="hljs-string"><span class="hljs-string">"habr12345"</span></span>) \ .save()</code> </pre><br><blockquote>  Algunas palabras sobre la configuraci√≥n de una conexi√≥n a AWS RDS.  Creamos el usuario y la contrase√±a en el paso "Implementaci√≥n de AWS PostgreSQL".  Para la URL del servidor de la base de datos, use Endpoint, que se muestra en la secci√≥n Conectividad y seguridad: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9n/sj/jd/9nsjjdun0hdy5qtwqub0xhvzunk.png"></div></blockquote><br>  Para conectar correctamente Spark y Kafka, debe ejecutar el trabajo mediante smark-submit utilizando el <b>artefacto spark-streaming-kafka-0-8_2.11</b> .  Adem√°s, tambi√©n aplicamos el artefacto para interactuar con la base de datos PostgreSQL, los transferiremos a trav√©s de --packages. <br><br>  Para la flexibilidad de la secuencia de comandos, tambi√©n sacamos el nombre del servidor de mensajes y el tema del que queremos recibir datos como par√°metros de entrada. <br><br>  Entonces, es hora de comenzar y probar el sistema: <br><br><pre> <code class="bash hljs">spark-submit \ --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.2,\ org.postgresql:postgresql:9.4.1207 \ spark_job.py localhost:9092 transaction</code> </pre><br>  ¬°Todo sali√≥ bien!  Como puede ver en la imagen a continuaci√≥n, durante el trabajo de la aplicaci√≥n, se muestran nuevos resultados de agregaci√≥n cada 2 segundos, porque establecemos el intervalo de procesamiento por lotes en 2 segundos al crear el objeto StreamingContext: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/cf/q1/25/cfq125zpzkyldktsuvdo175fazy.png"></div><br>  A continuaci√≥n, hacemos una consulta simple a la base de datos para verificar los registros en la tabla <b>transaction_flow</b> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7j/j9/qm/7jj9qmf4zpter3jkbblrmiqni2s.png"></div><br><h2>  Conclusi√≥n </h2><br>  Este art√≠culo examin√≥ un ejemplo de procesamiento de transmisi√≥n de informaci√≥n usando Spark Streaming junto con Apache Kafka y PostgreSQL.  Con el crecimiento de los datos de varias fuentes, es dif√≠cil sobreestimar el valor pr√°ctico de Spark Streaming para crear aplicaciones de transmisi√≥n y aplicaciones que operan en tiempo real. <br><br>  Puede encontrar el c√≥digo fuente completo en mi repositorio en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">GitHub</a> . <br><br>  Estoy listo para discutir este art√≠culo con placer, espero sus comentarios y tambi√©n espero cr√≠ticas constructivas de todos los lectores interesados. <br><br>  ¬°Te deseo √©xito! <br><br>  <b>PD:</b> originalmente se plane√≥ usar una base de datos local de PostgreSQL, pero dado mi amor por AWS, decid√≠ poner la base de datos en la nube.  En el pr√≥ximo art√≠culo sobre este tema, mostrar√© c√≥mo implementar todo el sistema descrito anteriormente en AWS utilizando AWS Kinesis y AWS EMR.  Sigue las noticias! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/451160/">https://habr.com/ru/post/451160/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../451148/index.html">Programaci√≥n Orientada a Objetos en Lenguajes Gr√°ficos</a></li>
<li><a href="../451150/index.html">Atr√°pame si puedes. Versi√≥n Manager</a></li>
<li><a href="../451152/index.html">La resistencia en el circuito de la puerta o c√≥mo hacerlo bien</a></li>
<li><a href="../451154/index.html">Sistema local de adquisici√≥n de datos aut√≥nomos (continuaci√≥n)</a></li>
<li><a href="../451158/index.html">Circuitos electricos. Tipos de circuito</a></li>
<li><a href="../451162/index.html">Correcci√≥n de errores: constantes f√≠sicas en las versiones actuales y nuevas del sistema internacional de unidades (SI)</a></li>
<li><a href="../451164/index.html">Buscando espacio de estacionamiento gratuito con Python</a></li>
<li><a href="../451166/index.html">¬øQu√© ofrecer√°n los nuevos repositorios para sistemas AI y MO?</a></li>
<li><a href="../451170/index.html">Jeff Bezos anunci√≥ planes para conquistar la luna</a></li>
<li><a href="../451172/index.html">Julia: funciones y estructuras como funciones</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>