<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>📲 🏷️ ✌️ vSAN di VMware Cloud 👩🏻‍🌾 👩🏽‍✈️ 🍰</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Tugas menyimpan dan mengakses data adalah titik yang menyakitkan bagi sistem informasi apa pun. Bahkan sistem penyimpanan yang dirancang dengan baik (...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>vSAN di VMware Cloud</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/selectel/blog/418753/"><img src="https://habrastorage.org/webt/pl/tj/to/pltjtoutsishswrt2dgxaktv5ka.png"><br><br>  Tugas menyimpan dan mengakses data adalah titik yang menyakitkan bagi sistem informasi apa pun.  Bahkan sistem penyimpanan yang dirancang dengan baik (selanjutnya disebut SHD) selama operasi mengungkapkan masalah yang terkait dengan penurunan kinerja.  Perhatian khusus harus diberikan pada serangkaian masalah penskalaan ketika jumlah sumber daya yang terlibat mendekati batas yang ditetapkan yang ditetapkan oleh pengembang penyimpanan. <br><br>  Alasan mendasar untuk terjadinya masalah ini adalah arsitektur tradisional berdasarkan ikatan yang erat dengan karakteristik perangkat keras dari perangkat penyimpanan yang digunakan.  Sebagian besar pelanggan masih memilih metode menyimpan dan mengakses data, dengan mempertimbangkan karakteristik antarmuka fisik (SAS / SATA / SCSI), dan bukan kebutuhan nyata dari aplikasi yang digunakan. <br><a name="habracut"></a><br>  Selusin tahun yang lalu, ini adalah keputusan yang logis.  Administrator sistem dengan cermat memilih perangkat penyimpanan informasi dengan spesifikasi yang diperlukan, misalnya, SATA / SAS, dan mengandalkan tingkat kinerja berdasarkan kemampuan perangkat keras pengontrol disk.  Pertarungan itu untuk volume cache controller RAID dan untuk opsi yang mencegah kehilangan data.  Sekarang pendekatan untuk menyelesaikan masalah ini tidak optimal. <br><br>  Dalam lingkungan saat ini, ketika memilih sistem penyimpanan, masuk akal untuk memulai bukan dari antarmuka fisik, tetapi dari kinerja yang dinyatakan dalam IOPS (jumlah operasi I / O per detik).  Menggunakan virtualisasi memungkinkan Anda untuk secara fleksibel menggunakan sumber daya perangkat keras yang ada dan menjamin tingkat kinerja yang diperlukan.  Untuk bagian kami, kami siap memberikan sumber daya dengan karakteristik yang benar-benar diperlukan untuk aplikasi. <br><br><h2>  Virtualisasi penyimpanan </h2><br>  Dengan pengembangan sistem virtualisasi, perlu untuk menemukan solusi inovatif untuk menyimpan dan mengakses data, sambil memastikan toleransi kesalahan.  Ini adalah titik awal untuk membuat SDS (Software-Defined Storage).  Untuk memenuhi kebutuhan bisnis, repositori ini dirancang dengan pemisahan perangkat lunak dan perangkat keras. <br><br>  Arsitektur SDS secara fundamental berbeda dari tradisional.  Logika penyimpanan telah diabstraksikan di tingkat perangkat lunak.  Organisasi penyimpanan menjadi lebih mudah karena penyatuan dan virtualisasi masing-masing komponen sistem tersebut. <br><br>  Apa faktor utama yang menghambat implementasi SDS di mana-mana?  Faktor ini paling sering adalah penilaian yang salah dari kebutuhan aplikasi yang digunakan dan penilaian risiko yang salah.  Untuk bisnis, pilihan solusi tergantung pada biaya implementasi, berdasarkan sumber daya yang dikonsumsi saat ini.  Sedikit orang berpikir - apa yang akan terjadi ketika jumlah informasi dan kinerja yang diperlukan melebihi kemampuan arsitektur yang dipilih.  Berpikir berdasarkan prinsip metodologis "seseorang tidak boleh berlipat ganda ada tanpa keharusan", lebih dikenal sebagai "pisau Occam", menentukan pilihan yang mendukung solusi tradisional. <br><br>  Hanya sedikit yang memahami bahwa kebutuhan skalabilitas dan keandalan penyimpanan data lebih penting daripada yang terlihat pada pandangan pertama.  Informasi adalah sumber daya, dan oleh karena itu, risiko kerugiannya harus diasuransikan.  Apa yang akan terjadi ketika sistem penyimpanan tradisional turun?  Anda perlu menggunakan garansi atau membeli peralatan baru.  Dan jika sistem penyimpanan dihentikan atau telah mengakhiri "masa hidup" (yang disebut EOL - End-of-Life)?  Ini bisa menjadi hari yang gelap bagi organisasi mana pun yang tidak dapat terus menggunakan layanannya sendiri yang sudah dikenal. <br><br>  Tidak ada sistem yang tidak memiliki satu titik kegagalan.  Tetapi ada sistem yang dapat dengan mudah selamat dari kegagalan satu atau lebih komponen.  Baik sistem penyimpanan virtual dan tradisional diciptakan dengan mempertimbangkan fakta bahwa cepat atau lambat kegagalan akan terjadi.  Itu hanya "batas kekuatan" dari sistem penyimpanan tradisional yang diletakkan dalam perangkat keras, tetapi dalam sistem penyimpanan virtual ditentukan dalam lapisan perangkat lunak. <br><br><h2>  Integrasi </h2><br>  Perubahan dramatis dalam infrastruktur TI selalu merupakan fenomena yang tidak diinginkan, penuh dengan downtime dan kehilangan dana.  Hanya kelancaran penerapan solusi baru yang memungkinkan untuk menghindari konsekuensi negatif dan meningkatkan kinerja layanan.  Itulah sebabnya Selectel merancang dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">meluncurkan cloud berbasis VMware</a> , pemimpin yang diakui di pasar virtualisasi.  Layanan yang kami buat akan memungkinkan setiap perusahaan untuk menyelesaikan seluruh jajaran tugas infrastruktur, termasuk penyimpanan data. <br><br>  Kami akan memberi tahu Anda dengan tepat bagaimana kami memutuskan pilihan sistem penyimpanan, serta keuntungan apa yang diberikan pilihan ini kepada kami.  Tentu saja, baik sistem penyimpanan tradisional dan SDS dipertimbangkan.  Untuk memahami dengan jelas semua aspek operasi dan risiko, kami menawarkan wawasan yang lebih mendalam tentang topik tersebut. <br><br>  Pada tahap desain, persyaratan berikut diberlakukan pada sistem penyimpanan: <br><br><ul><li>  <strong>toleransi kesalahan;</strong> </li><li>  <strong>kinerja</strong> </li><li>  <strong>scaling</strong> </li><li>  <strong>kemampuan untuk menjamin kecepatan;</strong> </li><li>  <strong>operasi yang benar di ekosistem VMware.</strong> </li></ul><br>  Penggunaan solusi perangkat keras tradisional tidak dapat memberikan tingkat skalabilitas yang diperlukan, karena tidak mungkin untuk terus meningkatkan volume penyimpanan karena keterbatasan arsitektur.  Pemesanan di tingkat seluruh pusat data juga sangat sulit.  Itu sebabnya kami mengalihkan perhatian ke SDS. <br><br>  Ada beberapa solusi perangkat lunak di pasar SDS yang cocok untuk kita untuk membangun cloud berdasarkan VMware vSphere.  Di antara solusi ini dapat dicatat: <br><br><ul><li>  <strong>Dell EMC ScaleIO;</strong> </li><li>  <strong>Datacore Hyper-konvergensi Virtual SAN;</strong> </li><li>  <strong>HPE StoreVirtual.</strong> </li></ul><br>  Solusi ini cocok untuk digunakan dengan VMware vSphere, namun, mereka tidak berintegrasi ke hypervisor dan berjalan secara terpisah.  Oleh karena itu, pilihan dibuat untuk VMware vSAN.  Mari kita pertimbangkan secara terperinci seperti apa arsitektur virtual dari solusi semacam itu. <br><br><h3>  Arsitektur </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hb/hu/nr/hbhunruzcic-pa1ggjw_-winfcg.png"></div><br>  <sup><i>Gambar diambil dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi resmi</a></i></sup> <br><br>  Tidak seperti sistem penyimpanan tradisional, semua informasi tidak disimpan pada satu titik.  Data mesin virtual tersebar secara merata di antara semua host, dan penskalaan dilakukan dengan menambahkan host atau menginstal disk drive tambahan pada mereka.  Dua opsi konfigurasi didukung: <br><br><ul><li>  <strong>Konfigurasi AllFlash</strong> (hanya solid state drive, baik untuk penyimpanan data dan untuk cache); </li><li>  <strong>Konfigurasi hibrid</strong> (penyimpanan magnetik dan cache keadaan padat). </li></ul><br>  Prosedur untuk menambah ruang disk tidak memerlukan pengaturan tambahan, misalnya, membuat LUN (Nomor Unit Logis, nomor disk logis) dan mengatur akses ke sana.  Segera setelah host ditambahkan ke cluster, ruang disknya menjadi tersedia untuk semua mesin virtual.  Pendekatan ini memiliki beberapa keunggulan signifikan: <br><br><ul><li>  <strong>kurangnya ikatan dengan produsen peralatan;</strong> </li><li>  <strong>peningkatan toleransi kesalahan;</strong> </li><li>  <strong>memastikan integritas data jika terjadi kegagalan;</strong> </li><li>  <strong>pusat kendali tunggal dari konsol vSphere;</strong> </li><li>  <strong>penskalaan horizontal dan vertikal yang nyaman.</strong> </li></ul><br>  Namun, arsitektur ini menempatkan permintaan tinggi pada infrastruktur jaringan.  Untuk memastikan throughput maksimum, di cloud kami jaringan dibangun di atas model Spine-Leaf. <br><br><h3>  Jaringan </h3><br>  Model jaringan tiga-tier tradisional (inti / agregasi / akses) memiliki sejumlah kelemahan signifikan.  Contoh yang mencolok adalah keterbatasan protokol Spanning-Tree. <br><br>  Model Spine-Leaf hanya menggunakan dua level, yang memberikan keuntungan sebagai berikut: <br><br><ul><li>  <strong>jarak antar perangkat yang dapat diprediksi;</strong> </li><li>  <strong>lalu lintas berjalan di sepanjang rute terbaik;</strong> </li><li>  <strong>kemudahan penskalaan;</strong> </li><li>  <strong>Pengecualian pembatasan protokol L2.</strong> </li></ul><br>  Fitur utama dari arsitektur semacam itu adalah bahwa arsitektur ini dioptimalkan untuk lalu lintas "horisontal".  Paket data hanya melewati satu hop, yang memungkinkan estimasi penundaan yang jelas. <br><br>  Koneksi fisik disediakan menggunakan beberapa tautan 10GbE per server, bandwidth yang digabungkan menggunakan protokol agregasi.  Dengan demikian, setiap host fisik menerima akses kecepatan tinggi ke semua objek penyimpanan. <br><br>  Pertukaran data diimplementasikan menggunakan protokol kepemilikan yang dibuat oleh VMware, yang memungkinkan operasi jaringan penyimpanan yang cepat dan andal pada Ethernet-transport (mulai 10GbE dan lebih tinggi). <br><br>  Transisi ke model objek penyimpanan data memungkinkan penyesuaian fleksibel penggunaan penyimpanan sesuai dengan persyaratan pelanggan.  Semua data disimpan dalam bentuk objek yang didistribusikan dengan cara tertentu di antara host cluster.  Kami mengklarifikasi nilai beberapa parameter yang dapat dikontrol. <br><br><h3>  Toleransi kesalahan </h3><br><ol><li>  <strong>FTT (Kegagalan Bertoleransi).</strong>  Mengindikasikan jumlah kegagalan host yang dapat ditangani oleh cluster tanpa mengganggu operasi reguler. </li><li>  <strong>FTM (Metode Toleransi Kegagalan).</strong>  Metode memastikan toleransi kesalahan pada tingkat disk. <br><br>  a.  <strong>Mirroring</strong> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jb/kv/lw/jbkvlwsmyvua7jrw2f8jj0820am.png"></div><br>  <sup><i>Gambar diambil dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">blog VMware.</a></i></sup> <br><br>  Merupakan duplikasi lengkap dari suatu objek, dan replika selalu terletak pada host fisik yang berbeda.  Analog terdekat dengan metode ini adalah RAID-1.  Penggunaannya memungkinkan cluster untuk secara rutin memproses hingga tiga kegagalan komponen apa pun (disk, host, kehilangan jaringan, dll.).  Parameter ini dikonfigurasi dengan mengatur opsi FTT. <br><br>  Secara default, opsi ini memiliki nilai 1, dan 1 replika dibuat untuk objek (hanya 2 instance pada host yang berbeda).  Saat nilainya meningkat, jumlah salinan akan menjadi N +1.  Jadi, dengan nilai maksimum FTT = 3, 4 instance objek akan berada pada host yang berbeda. <br><br>  Metode ini memungkinkan Anda untuk mencapai kinerja maksimum dengan mengorbankan efisiensi ruang disk.  Ini dapat digunakan dalam konfigurasi hybrid dan AllFlash. <br><br>  b.  <strong>Erasure Coding</strong> (analog dari RAID 5/6). <br><img src="https://habrastorage.org/webt/72/zq/hh/72zqhhyrlncoqga9qlzg0wdfgco.png"><br><br>  <sup><i>Gambar diambil dari blog <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">cormachogan.com.</a></i></sup> <br><br>  Pekerjaan metode ini didukung secara eksklusif pada konfigurasi AllFlash.  Dalam proses merekam setiap objek, blok paritas yang sesuai dihitung, yang memungkinkan pemulihan data secara unik jika terjadi kegagalan.  Pendekatan ini secara signifikan menghemat ruang disk dibandingkan dengan Mirroring. <br><br>  Tentu saja, pengoperasian metode ini meningkatkan overhead, yang dinyatakan dalam penurunan produktivitas.  Namun demikian, mengingat kinerja konfigurasi AllFlash, kelemahan ini diratakan, menjadikan penggunaan Erasure Coding sebagai opsi yang dapat diterima untuk sebagian besar tugas. <br><br>  Selain itu, VMware vSAN memperkenalkan konsep "domain kegagalan", yang merupakan pengelompokan logis dari rak server atau keranjang disk.  Segera setelah elemen yang diperlukan dikelompokkan, ini mengarah ke distribusi data di antara node yang berbeda dengan mempertimbangkan domain kegagalan.  Ini memungkinkan kluster untuk bertahan dari kehilangan seluruh domain, karena semua replika objek yang sesuai akan ditempatkan pada host lain di domain kegagalan yang berbeda. <br><br>  Domain kegagalan terkecil adalah grup disk, yang merupakan disk drive yang terhubung secara logis.  Setiap grup disk berisi dua jenis media - cache dan kapasitas.  Sebagai media cache, sistem memungkinkan hanya menggunakan disk solid-state, dan baik disk magnetik maupun solid-state dapat bertindak sebagai pembawa kapasitas.  Media cache membantu mempercepat disk magnetik dan mengurangi latensi saat mengakses data. </li></ol><br><h2>  Implementasi </h2><br>  Mari kita bicara tentang batasan apa yang ada dalam arsitektur VMware vSAN dan mengapa mereka diperlukan.  Terlepas dari platform perangkat keras yang digunakan, arsitektur menyediakan batasan berikut: <br><br><ul><li>  <strong>tidak lebih dari 5 grup disk per host;</strong> </li><li>  <strong>tidak lebih dari 7 pembawa kapasitas dalam grup disk;</strong> </li><li>  <strong>tidak lebih dari 1 cache-carrier dalam grup disk;</strong> </li><li>  <strong>tidak lebih dari 35 operator kapasitas per host;</strong> </li><li>  <strong>tidak lebih dari 9000 komponen per host (termasuk komponen saksi);</strong> </li><li>  <strong>tidak lebih dari 64 host di sebuah cluster;</strong> </li><li>  <strong>tidak lebih dari 1 vSAN-datastore per cluster.</strong> </li></ul><br>  Mengapa ini dibutuhkan?  Sampai batas yang ditentukan terlampaui, sistem akan beroperasi dengan kapasitas yang dinyatakan, menjaga keseimbangan antara kinerja dan kapasitas penyimpanan.  Ini memungkinkan Anda untuk menjamin operasi yang benar dari seluruh sistem penyimpanan virtual secara keseluruhan. <br><br>  Selain keterbatasan ini, satu fitur penting harus diingat.  Tidak disarankan untuk mengisi lebih dari 70% dari total volume penyimpanan.  Faktanya adalah ketika 80% tercapai, mekanisme penyeimbangan dimulai secara otomatis, dan sistem penyimpanan mulai mendistribusikan kembali data di semua host cluster.  Prosedur ini sangat intensif sumber daya dan dapat secara serius mempengaruhi kinerja subsistem disk. <br><br>  Untuk memenuhi kebutuhan berbagai pelanggan, kami telah menerapkan tiga kumpulan penyimpanan untuk kemudahan penggunaan dalam berbagai skenario.  Mari kita lihat masing-masing secara berurutan. <br><br><h3>  Kolam disk cepat </h3><br>  Prioritas untuk membuat kumpulan ini adalah untuk mendapatkan penyimpanan yang akan memberikan kinerja maksimum untuk hosting sistem yang sangat dimuat.  Server dari kumpulan ini menggunakan sepasang Intel P4600 sebagai cache dan 10 Intel P3520 untuk penyimpanan data.  Cache di kumpulan ini digunakan agar data dibaca langsung dari media, dan operasi penulisan terjadi melalui cache. <br><br>  Untuk meningkatkan kapasitas yang berguna dan memastikan toleransi kesalahan, model penyimpanan data yang disebut Erasure Coding digunakan.  Model ini mirip dengan array RAID 5/6 biasa, tetapi pada tingkat penyimpanan objek.  Untuk menghilangkan kemungkinan korupsi data, vSAN menggunakan mekanisme perhitungan checksum untuk setiap blok data 4K. <br><br>  Validasi dilakukan di latar belakang selama operasi baca / tulis, serta untuk data "dingin", akses yang tidak diminta selama tahun itu.  Ketika checksum mismatch terdeteksi, dan karenanya korupsi data terdeteksi, vSAN akan secara otomatis memulihkan file dengan menimpa. <br><br><h3>  Drive pool hybrid </h3><br>  Dalam hal kumpulan ini, tugas utamanya adalah menyediakan sejumlah besar data, sambil memastikan tingkat toleransi kesalahan yang baik.  Untuk banyak tugas, kecepatan akses data bukanlah prioritas, volume dan biaya penyimpanan jauh lebih penting.  Menggunakan solid-state drive karena penyimpanan seperti itu akan sangat mahal. <br><br>  Faktor ini adalah alasan untuk penciptaan kumpulan, yang merupakan hibrida dari caching solid-state drive (seperti di kumpulan lain itu adalah Intel P4600) dan hard drive tingkat perusahaan yang dikembangkan oleh HGST.  Alur kerja hybrid mempercepat akses ke data yang sering diminta dengan caching operasi baca dan tulis. <br><br>  Pada tingkat logis, data dicerminkan untuk menghilangkan kerugian jika terjadi kegagalan perangkat keras.  Setiap objek dibagi menjadi komponen yang identik dan sistem mendistribusikannya ke host yang berbeda. <br><br><h3>  Pool with Disaster Recovery </h3><br><img src="https://habrastorage.org/webt/hz/ne/xi/hznexild-sq7oxyvy6m9yvr-xm4.png"><br><br>  Tugas utama dari pool adalah untuk mencapai tingkat toleransi kesalahan dan kinerja maksimum.  Penggunaan teknologi <strong>Stretched vSAN</strong> memungkinkan kami untuk <strong>mendistribusikan</strong> penyimpanan antara pusat data Tsvetochnaya-2 di St. Petersburg dan Dubrovka-3 di Wilayah Leningrad.  Setiap server di kumpulan ini dilengkapi dengan sepasang drive Intel P4600 yang luas dan berkecepatan tinggi untuk operasi cache dan 6 drive Intel P3520 untuk penyimpanan data.  Pada level logis, ini adalah 2 grup disk per host. <br><br>  Konfigurasi AllFlash tidak memiliki kelemahan serius - penurunan tajam pada IOPS dan peningkatan antrian permintaan disk dengan peningkatan volume akses acak ke data.  Sama seperti di kumpulan dengan disk cepat, operasi tulis melalui cache, dan membaca dilakukan secara langsung. <br><br>  Sekarang tentang perbedaan utama dari sisa kolam.  Data dari setiap mesin virtual dicerminkan di dalam satu pusat data dan pada saat yang sama secara bersamaan direplikasi ke pusat data lain milik kami.  Dengan demikian, bahkan kecelakaan serius, seperti gangguan konektivitas penuh antara pusat data, tidak akan menjadi masalah.  Bahkan hilangnya pusat data sama sekali tidak akan memengaruhi data. <br><br>  Kecelakaan dengan kegagalan total situs - situasinya cukup langka, tetapi vSAN dapat bertahan dengan kehormatan tanpa kehilangan data.  Para tamu di acara <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">SelectelTechDay 2018</a> kami dapat melihat sendiri bagaimana cluster Stretched vSAN mengalami kegagalan situs total.  Mesin virtual tersedia hanya satu menit setelah semua server di salah satu situs dimatikan dengan listrik.  Semua mekanisme bekerja persis seperti yang direncanakan, tetapi data tetap tidak tersentuh. <br><br>  Pengabaian arsitektur penyimpanan yang sudah dikenal membutuhkan banyak perubahan.  Salah satu perubahan ini adalah munculnya "entitas" virtual baru, yang mencakup alat saksi.  Arti dari solusi ini adalah untuk melacak proses merekam replika data dan menentukan mana yang relevan.  Pada saat yang sama, data itu sendiri tidak disimpan pada komponen saksi, hanya metadata tentang proses perekaman. <br><br>  Mekanisme ini berlaku jika terjadi kecelakaan ketika terjadi kegagalan selama proses replikasi, yang mengakibatkan replika tidak sinkron. <br><br>  Untuk menentukan mana yang berisi informasi yang relevan, digunakan mekanisme penentuan kuorum.  Setiap komponen memiliki "hak suara" dan diberikan sejumlah suara (1 atau lebih).  "Hak suara" yang sama memiliki komponen saksi yang memainkan peran arbiter jika terjadi situasi kontroversial. <br><br>  Kuorum tercapai hanya ketika replika lengkap tersedia untuk suatu objek dan jumlah "suara" saat ini lebih dari 50%. <br><br><h2>  Kesimpulan </h2><br>  Pilihan VMware vSAN sebagai sistem penyimpanan telah menjadi keputusan penting bagi kami.  Opsi ini lulus pengujian stres dan pengujian toleransi kesalahan sebelum dimasukkan dalam proyek cloud berbasis VMware kami. <br><br>  Menurut hasil pengujian, menjadi jelas bahwa fungsionalitas yang dideklarasikan berfungsi seperti yang diharapkan dan memenuhi semua persyaratan infrastruktur cloud kami. <br><br>  Punya sesuatu untuk diceritakan berdasarkan pengalaman Anda sendiri dengan vSAN?  Selamat datang di komentar. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id418753/">https://habr.com/ru/post/id418753/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id418737/index.html">Ablasi laser, kaca telur dan dopant Er2O3</a></li>
<li><a href="../id418739/index.html">Pusat Teknologi Aditif: Printer 3D industri Sistem 3D, Stratasys, SLM, EOS</a></li>
<li><a href="../id418741/index.html">Tambahkan enkripsi dan dorong ke SIP biasa</a></li>
<li><a href="../id418743/index.html">Kisah Tempat Pertama di ML Boot Camp VI</a></li>
<li><a href="../id418751/index.html">Dex mata-mata Cina kurang ajar</a></li>
<li><a href="../id418755/index.html">Bagaimana e-commerce bertahan dalam promosi skala besar. Bersiap-siap untuk beban puncak di web [Bagian 1]</a></li>
<li><a href="../id418757/index.html">Kerangka waktu yang menipis (cryptocurrency, forex, exchange)</a></li>
<li><a href="../id418759/index.html">Berapa biaya bagi siswa untuk mengeluarkan chip?</a></li>
<li><a href="../id418761/index.html">Buku “Pure Python. Seluk-beluk pemrograman untuk pro »</a></li>
<li><a href="../id418763/index.html">Menengah / senior: bagaimana cara keluar dari rawa?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>