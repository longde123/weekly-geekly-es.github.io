<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëß üëàüèª üë®‚Äçüî¨ M√©todos de conjunto. Trecho do livro üçô üéüÔ∏è ‚ÜòÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° Khabrozhiteli, entregamos √† gr√°fica um novo livro "Machine Learning: Algorithms for Business" . Aqui est√° um trecho sobre os m√©todos de conjunto, ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>M√©todos de conjunto. Trecho do livro</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/445780/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><img src="https://habrastorage.org/webt/vk/fr/zn/vkfrzn9ctkjsd2wjx8puqifp980.jpeg" alt="imagem"></a> <br><br>  Ol√° Khabrozhiteli, entregamos √† gr√°fica um novo livro <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">"Machine Learning: Algorithms for Business"</a> .  Aqui est√° um trecho sobre os m√©todos de conjunto, cujo objetivo √© explicar o que os torna eficazes e como evitar erros comuns que levam ao mau uso das finan√ßas. <br><a name="habracut"></a><br><h3>  6.2  Tr√™s fontes de erro </h3><br>  Os modelos MO geralmente sofrem de tr√™s <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">erros</a> . <br><br>  1. Vi√©s: esse erro √© causado por suposi√ß√µes irrealistas.  Quando o vi√©s √© alto, isso significa que o algoritmo MO n√£o conseguiu reconhecer as rela√ß√µes importantes entre as caracter√≠sticas e os resultados.  Nesta situa√ß√£o, diz-se que o algoritmo est√° "mal ajustado". <br><br>  2. Dispers√£o: esse erro √© causado pela sensibilidade a pequenas altera√ß√µes no subconjunto de treinamento.  Quando a varia√ß√£o √© alta, isso significa que o algoritmo √© realinhado com o subconjunto de treinamento e, portanto, mesmo altera√ß√µes m√≠nimas no subconjunto de treinamento podem produzir previs√µes terrivelmente diferentes.  Em vez de modelar padr√µes gerais em um subconjunto de treinamento, o algoritmo recebe por engano ru√≠do para o sinal. <br><br>  3. Ru√≠do: Esse erro √© causado pela dispers√£o dos valores observados, como altera√ß√µes imprevis√≠veis ou erros de medi√ß√£o.  Este √© um erro fatal que n√£o pode ser explicado por nenhum modelo. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bc/8g/ag/bc8gaguh6e1w07vd3aoqcqcz4m0.png" alt="imagem"></div><br>  Um m√©todo de conjunto √© um m√©todo que combina muitos alunos fracos, que s√£o baseados no mesmo algoritmo de aprendizado, com o objetivo de criar um aluno (mais forte), cujo desempenho √© melhor do que qualquer um dos alunos.  As t√©cnicas de conjunto ajudam a reduzir o vi√©s e / ou dispers√£o. <br><br><h3>  6.3  Agrega√ß√£o de Bootstrap </h3><br>  O empacotamento (agrega√ß√£o) ou agrega√ß√£o de amostras de autoinicializa√ß√£o √© uma maneira eficaz de reduzir a varia√ß√£o nas previs√µes.  Funciona da seguinte forma: primeiro, √© necess√°rio gerar N subconjuntos de dados de treinamento usando amostragem aleat√≥ria com retorno.  Segundo, ajuste N avaliadores, um para cada subconjunto de treinamento.  Esses avaliadores s√£o ajustados independentemente um do outro, portanto, os modelos podem ser ajustados em paralelo.  Em terceiro lugar, a previs√£o do conjunto √© uma m√©dia aritm√©tica simples das previs√µes individuais dos modelos N.  No caso de vari√°veis ‚Äã‚Äãcateg√≥ricas, a probabilidade de uma observa√ß√£o pertencer a uma classe √© determinada pela parcela de avaliadores que classificam essa observa√ß√£o como membro dessa classe (por maioria de votos, ou seja, por maioria).  Quando o avaliador de base pode fazer previs√µes com a probabilidade de previs√£o, o classificador ensacado pode obter o valor m√©dio das probabilidades. <br><br>  Se voc√™ usar a classe baggingClassifier da biblioteca sklearn para calcular a precis√£o sem pacotes, dever√° saber sobre esse defeito: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://github.com/scikit-learn/scikitlearn/issues/8933</a> .  Uma solu√ß√£o alternativa √© renomear r√≥tulos em uma ordem seq√ºencial de n√∫mero inteiro. <br><br><h3>  6.3.1  Redu√ß√£o de dispers√£o </h3><br>  A principal vantagem do ensacamento √© que ele reduz a varia√ß√£o das previs√µes, ajudando assim a resolver o problema do excesso de ajuste.  A varia√ß√£o na previs√£o em saco (œÜi [c]) √© uma fun√ß√£o do n√∫mero de avaliadores em saco (N), a varia√ß√£o m√©dia da previs√£o realizada por um avaliador (œÉÃÑ) e a correla√ß√£o m√©dia entre suas previs√µes (œÅÃÑ): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pq/ly/9i/pqly9iyqdqu5m8ty8zahuoeomve.png" alt="imagem"></div><br>  O bootstraping seq√ºencial (cap√≠tulo 4) √© tornar a amostragem o mais independente poss√≠vel, reduzindo assim œÅÃÑ, o que deve reduzir a dispers√£o dos classificadores ensacados.  Na fig.  6.1, plotamos o diagrama de desvio padr√£o da previs√£o em bolsa em fun√ß√£o de N ‚àà [5, 30], œÅÃÑ ‚àà [0, 1] e œÉÃÑ = 1. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/na/bm/8z/nabm8zbq6st62mg82lmhac45y0i.png" alt="imagem"></div><br><h3>  6.3.2  Precis√£o aprimorada </h3><br>  Considere um classificador ensacado, que faz predi√ß√µes em k classes por um voto majorit√°rio entre N classificadores independentes.  Podemos designar previs√µes como {0,1}, em que 1 significa previs√£o correta.  A precis√£o do classificador √© a probabilidade p de marcar a previs√£o como 1. Em m√©dia, obtemos previs√µes de Np marcadas como 1 com uma varia√ß√£o de Np (1 - p).  A vota√ß√£o majorit√°ria faz a previs√£o correta quando a classe mais previs√≠vel √© observada.  Por exemplo, para N = 10 ek = 3, o classificador ensacado fez a previs√£o correta quando observado <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/g-/oq/oi/g-oqoilmsmjgpaukoouor90ndbo.png" alt="imagem"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wx/wk/cl/wxwkcl97h4cnn1n-yx8ljmdj14g.png" alt="imagem"></div><br>  Listagem 6.1.  A corre√ß√£o do classificador ensacado <br><br><pre><code class="plaintext hljs">from scipy.misc import comb N,p,k=100,1./3,3. p_=0 for i in xrange(0,int(N/k)+1): p_+=comb(N,i)*p**i*(1-p)**(Ni) print p,1-p_</code> </pre> <br>  Esse √© um argumento forte a favor de agrupar qualquer classificador no caso geral, quando os recursos computacionais permitirem.  No entanto, diferentemente do impulso, o empacotamento n√£o pode melhorar a precis√£o dos classificadores fracos: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9n/fz/i8/9nfzi86m4gvaqru13iicjf1q0bg.png" alt="imagem"></div><br>  Para uma an√°lise detalhada deste t√≥pico, aconselha-se o leitor a recorrer ao teorema do j√∫ri de Condorcet.  Embora esse teorema tenha sido obtido com a finalidade de vota√ß√£o majorit√°ria na ci√™ncia pol√≠tica, o problema abordado por esse teorema tem caracter√≠sticas comuns ao descrito acima. <br><br><h3>  6.3.3  Redund√¢ncia de observa√ß√µes </h3><br>  No cap√≠tulo 4, examinamos uma das raz√µes pelas quais as observa√ß√µes financeiras n√£o podem ser consideradas igualmente distribu√≠das e independentes entre si.  Observa√ß√µes excessivas t√™m dois efeitos prejudiciais no ensacamento.  Em primeiro lugar, √© mais prov√°vel que amostras colhidas com retorno sejam quase id√™nticas, mesmo que n√£o tenham observa√ß√µes comuns.  Faz <img src="https://habrastorage.org/webt/sx/i4/u0/sxi4u0afpswnvbe5nzxexnp_k80.png" alt="imagem">  e o empacotamento n√£o reduzir√£o a varia√ß√£o, independentemente de N. Por exemplo, se cada caso em t for marcado de acordo com um retorno financeiro entre t e t + 100, ser√° necess√°rio selecionar 1% dos casos por avaliador em saco, mas n√£o mais.  No cap√≠tulo 4, se√ß√£o 4.5, s√£o recomendadas tr√™s solu√ß√µes alternativas, uma das quais foi definir max_samples = out ['tW']. Mean () na implementa√ß√£o da classe de classificador ensacado na biblioteca sklearn.  Outra (melhor) solu√ß√£o foi a aplica√ß√£o do m√©todo de sele√ß√£o seq√ºencial de inicializa√ß√£o. <br><br>  O segundo efeito prejudicial da redund√¢ncia de observa√ß√£o √© que a precis√£o de pacotes extras ser√° inflada.  Isso se deve ao fato de que a amostragem aleat√≥ria com amostragem retorna √†s amostras do subconjunto de treinamento que s√£o muito semelhantes √†s de fora do pacote.  Nesse caso, a valida√ß√£o cruzada correta do bloco k estratificado sem embaralhar antes da divis√£o mostrar√° muito menos precis√£o no subconjunto de teste do que o avaliado fora da embalagem.  Por esse motivo, ao usar esta classe de biblioteca do sklearn, √© recomend√°vel definir stratifiedKFold (n_splits = k, shuffle = False), verificar o classificador ensacado e ignorar os resultados de precis√£o sem pacote.  Um k baixo √© prefer√≠vel a um k alto, pois a divis√£o excessiva colocar√° novamente padr√µes no subconjunto de teste que s√£o muito semelhantes aos usados ‚Äã‚Äãno subconjunto de treinamento. <br><br><h3>  6.4  Floresta aleat√≥ria </h3><br>  As √°rvores de decis√£o s√£o bem conhecidas, pois tendem a se ajustar demais, o que aumenta a varia√ß√£o das previs√µes.  Para resolver esse problema, um m√©todo de floresta aleat√≥ria (RF) foi desenvolvido para gerar previs√µes de conjuntos com menor varia√ß√£o. <br><br>  Uma floresta aleat√≥ria tem algumas semelhan√ßas comuns com o empacotamento, no sentido de treinar avaliadores individuais independentemente em subconjuntos de dados com inicializa√ß√£o.  A principal diferen√ßa do ensacamento √© que um segundo n√≠vel de aleatoriedade √© constru√≠do em florestas aleat√≥rias: durante a otimiza√ß√£o de cada fragmenta√ß√£o nodal, apenas uma subamostra aleat√≥ria (sem retorno) de atributos ser√° avaliada, a fim de correlacionar ainda mais os avaliadores. <br><br>  Como ensacamento, uma floresta aleat√≥ria reduz a varia√ß√£o das previs√µes sem superajustar (lembre-se disso at√©).  A segunda vantagem √© que uma floresta aleat√≥ria avalia a import√¢ncia dos atributos, que discutiremos em detalhes no Cap√≠tulo 8. A terceira vantagem √© que uma floresta aleat√≥ria fornece estimativas de precis√£o fora da embalagem, mas em aplica√ß√µes financeiras √© prov√°vel que elas sejam infladas (como descrito em Se√ß√£o 6.3.3).  Mas, como ensacamento, uma floresta aleat√≥ria n√£o necessariamente apresenta vi√©s menor do que as √°rvores de decis√£o individuais. <br><br>  Se um grande n√∫mero de amostras for redundante (n√£o igualmente distribu√≠do e independente mutuamente), ainda haver√° um reajuste: a amostragem aleat√≥ria com retorno criar√° um grande n√∫mero de √°rvores quase id√™nticas (), onde cada √°rvore de decis√£o √© super ajustada (uma desvantagem pela qual as √°rvores de decis√£o s√£o not√≥rias) .  Ao contr√°rio do empacotamento, uma floresta aleat√≥ria sempre define o tamanho das amostras de bootstrap de acordo com o tamanho do subconjunto de dados de treinamento.  Vejamos como podemos resolver esse problema de reajustar florestas aleat√≥rias na biblioteca sklearn.  Para fins de ilustra√ß√£o, vou me referir √†s classes da biblioteca sklearn;  no entanto, essas solu√ß√µes podem ser aplicadas a qualquer implementa√ß√£o: <br><br>  1. Defina o par√¢metro max_features como um valor mais baixo para obter uma discrep√¢ncia entre as √°rvores. <br><br>  2. Parada antecipada: defina o par√¢metro de regulariza√ß√£o min_weight_fraction_leaf para um valor suficientemente grande (por exemplo, 5%) para que a precis√£o fora do pacote converja para a corre√ß√£o fora da amostra (bloco k). <br><br>  3. Use o avaliador BaggingClassifier no avaliador b√°sico DecisionTreeClassifier, em que max_samples est√° definido como exclusividade m√©dia (avgU) entre as amostras. <br><br><ul><li>  clf = DecisionTreeClassifier (crit√©rio = 'entropia', max_features = 'auto', class_weight = 'equilibrado') </li><li>  bc = BaggingClassifier (base_estimator = clf, n_estimators = 1000, max_samples = avgU, max_features = 1.) </li></ul><br>  4. Use o avaliador BaggingClassifier no avaliador base RandomForestClassifier, em que max_samples est√° definido como exclusividade m√©dia (avgU) entre as amostras. <br><br><ul><li>  clf = RandomForestClassifier (n_estimators = 1, crit√©rio = 'entropia', autoinicializa√ß√£o = Falso, class_weight = 'balanced_subsample') </li><li>  bc = BaggingClassifier (base_estimator = clf, n_estimators = 1000, max_samples = avgU, max_features = 1.) </li></ul><br>  5. Modifique a classe de floresta aleat√≥ria para substituir as auto-inicializa√ß√£o padr√£o pelas auto-inicializa√ß√£o sequenciais. <br><br>  Para resumir, a Listagem 6.2 mostra tr√™s maneiras alternativas de configurar uma floresta aleat√≥ria usando classes diferentes. <br><br>  Listagem 6.2.  Tr√™s maneiras de configurar uma floresta aleat√≥ria <br><br><pre> <code class="plaintext hljs">clf0=RandomForestClassifier(n_estimators=1000, class_weight='balanced_ subsample', criterion='entropy') clf1=DecisionTreeClassifier(criterion='entropy', max_features='auto', class_weight='balanced') clf1=BaggingClassifier(base_estimator=clf1, n_estimators=1000, max_samples=avgU) clf2=RandomForestClassifier(n_estimators=1, criterion='entropy', bootstrap=False, class_weight='balanced_subsample') clf2=BaggingClassifier(base_estimator=clf2, n_estimators=1000, max_samples=avgU, max_features=1.)</code> </pre> <br>  Ao ajustar as √°rvores de decis√£o, a rota√ß√£o do espa√ßo do recurso na dire√ß√£o que coincide com os eixos reduz, em regra, o n√∫mero de n√≠veis necess√°rios para a √°rvore.  Por esse motivo, sugiro que voc√™ ajuste uma √°rvore aleat√≥ria no PCA de atributos, pois isso pode acelerar os c√°lculos e reduzir um pouco o reajuste (mais sobre isso no Cap√≠tulo 8).  Al√©m disso, conforme descrito no Cap√≠tulo 4, Se√ß√£o 4.8, o argumento class_weight = 'balanced_subsample' ajudar√° a impedir que as √°rvores classifiquem erroneamente classes minorit√°rias. <br><br><h3>  6.5  Impulso </h3><br>  Kearns e Valiant [1989] foram os primeiros a perguntar se avaliadores fracos poderiam ser combinados para obter um avaliador altamente preciso.  Logo depois, Schapire [1990] mostrou uma resposta afirmativa a essa pergunta usando um procedimento que chamamos de refor√ßo hoje (aumento, aumento, amplifica√ß√£o).  Em termos gerais, funciona da seguinte maneira: primeiro, gere um subconjunto de treinamento por sele√ß√£o aleat√≥ria com retorno de acordo com determinados pesos de amostra (inicializados por pesos uniformes).  Em segundo lugar, ajuste um avaliador usando este subconjunto de treinamento.  Em terceiro lugar, se um √∫nico avaliador alcan√ßa precis√£o que excede o limite de aceitabilidade (por exemplo, em um classificador bin√°rio √© de 50% para que o classificador funcione melhor do que a sorte), o avaliador permanece, caso contr√°rio, ele √© descartado.  Quarto, d√™ mais peso √†s observa√ß√µes classificadas incorretamente e menos peso √†s observa√ß√µes classificadas corretamente.  Quinto, repita as etapas anteriores at√© receber N avaliadores.  Sexto, a previs√£o do conjunto √© a m√©dia ponderada das previs√µes individuais dos modelos N, onde os pesos s√£o determinados pela precis√£o de cada avaliador.  Existem v√°rios algoritmos aprimorados, dos quais o aumento adaptativo do AdaBoost √© um dos mais populares (Geron [2017]).  A Figura 6.3 resume o fluxo de decis√£o na implementa√ß√£o padr√£o do algoritmo AdaBoost. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1d/u-/cr/1du-crm0gpkjgvy7lk45jj9f9ig.png" alt="imagem"></div><br><h3>  6.6  Bagging vs aumento de finan√ßas </h3><br>  A partir da descri√ß√£o acima, v√°rios aspectos tornam o impulso completamente diferente do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ensacamento</a> : <br><br><ul><li>  O ajuste dos classificadores individuais √© realizado sequencialmente. </li><li>  Classificadores ruins s√£o rejeitados. </li><li>  A cada itera√ß√£o, as observa√ß√µes s√£o ponderadas de maneira diferente. </li></ul><br>  A previs√£o do conjunto √© a m√©dia ponderada de cada aluno. <br><br>  A principal vantagem do aumento √© que ele reduz a varia√ß√£o e o vi√©s nas previs√µes.  No entanto, a corre√ß√£o do vi√©s ocorre devido a um maior risco de excesso de ajuste.  Pode-se argumentar que, em aplica√ß√µes financeiras, o ensacamento √© geralmente prefer√≠vel ao refor√ßo.  A ensacamento resolve o problema de ajuste excessivo, enquanto o aumento resolve o problema de ajuste excessivo.  Overfitting √© frequentemente um problema mais s√©rio do que underfitting, pois ajustar o algoritmo MO com muita for√ßa aos dados financeiros n√£o √© de todo dif√≠cil devido √† baixa rela√ß√£o sinal / ru√≠do.  Al√©m disso, o ensacamento pode ser paralelo, enquanto o refor√ßo geralmente requer execu√ß√£o sequencial. <br><br><h3>  6.7  Ensacamento para escalabilidade </h3><br>  Como voc√™ sabe, alguns algoritmos populares de MO n√£o escalam muito bem, dependendo do tamanho da amostra.  O m√©todo de m√°quinas de vetores de suporte (SVM) √© um excelente exemplo.  Se voc√™ tentar ajustar o avaliador SVM em mais de um milh√£o de observa√ß√µes, poder√° levar muito tempo at√© o algoritmo convergir.  E mesmo depois da converg√™ncia, n√£o h√° garantia de que a solu√ß√£o seja √≥tima global ou que n√£o seja realinhada. <br><br>  Uma abordagem pr√°tica √© criar um algoritmo ensacado, em que o avaliador de base pertence a uma classe que n√£o escala bem com o tamanho da amostra, como o SVM.  Ao definir esse avaliador b√°sico, introduzimos uma condi√ß√£o estrita para uma parada antecipada.  Por exemplo, na implementa√ß√£o de m√°quinas de vetores de suporte (SVMs) na biblioteca sklearn, voc√™ pode definir um valor baixo para o par√¢metro max_iter, por exemplo, itera√ß√µes 1E5.  O valor padr√£o √© max_iter = -1, que instrui o avaliador a continuar iterando at√© que os erros caiam abaixo do n√≠vel de toler√¢ncia.  Por outro lado, voc√™ pode aumentar o n√≠vel de toler√¢ncia com o par√¢metro tol, cujo padr√£o √© tol = iE-3.  Qualquer uma dessas duas op√ß√µes levar√° a uma parada antecipada.  √â poss√≠vel parar outros algoritmos com anteced√™ncia usando par√¢metros equivalentes, como o n√∫mero de n√≠veis em uma floresta aleat√≥ria (max_depth) ou a fra√ß√£o ponderada m√≠nima da soma total de pesos (todas as amostras de entrada) necess√°rias para estar em um n√≥ folha (min_weight_fraction_leaf). <br><br>  Dado que os algoritmos ensacados podem ser paralelos, transformamos uma grande tarefa seq√ºencial em uma s√©rie de tarefas menores que s√£o executadas simultaneamente.  Obviamente, uma parada antecipada aumentar√° a varia√ß√£o dos resultados dos avaliadores de base individuais;  no entanto, esse aumento pode ser mais do que compensado pela diminui√ß√£o da varia√ß√£o associada ao algoritmo ensacado.  Voc√™ pode controlar essa redu√ß√£o adicionando novos avaliadores de base independentes.  Utilizada dessa maneira, a embalagem permite obter estimativas r√°pidas e robustas em conjuntos de dados muito grandes. <br><br>  ¬ªMais informa√ß√µes sobre o livro podem ser encontradas no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">site do editor</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Conte√∫do</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Trecho</a> <br><br>  Desconto de 25% para os livros de pr√©-venda da Khabrozhiteley em um cupom - <b>Machine Learning</b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt445780/">https://habr.com/ru/post/pt445780/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt445762/index.html">A matem√°tica √© l√≥gica ou por que as teorias axiom√°ticas s√£o paradoxais?</a></li>
<li><a href="../pt445764/index.html">Minha maneira de criar componentes mestre na Figura</a></li>
<li><a href="../pt445766/index.html">Sobre o data center com toda a honestidade: como resolvemos o problema de poeira nas salas de servidores do data center</a></li>
<li><a href="../pt445772/index.html">Sistema de pagamento r√°pido ou imposs√≠vel √© poss√≠vel</a></li>
<li><a href="../pt445778/index.html">10 novos cursos gratuitos sobre servi√ßos cognitivos e Azure</a></li>
<li><a href="../pt445782/index.html">Uma sele√ß√£o de chaves de fenda geek e ferramentas m√∫ltiplas incomuns de Leatherman a Xiaomi</a></li>
<li><a href="../pt445784/index.html">Crescimento profissional dos funcion√°rios - o que √© e por que √© necess√°rio: nos comunicamos com a Dodo Pizza, Icons8 e Evil Martians</a></li>
<li><a href="../pt445786/index.html">Criptografia em Java. Classe KeyStore</a></li>
<li><a href="../pt445788/index.html">Vigil√¢ncia por v√≠deo em nuvem fa√ßa voc√™ mesmo: novos recursos do Ivideon Web SDK</a></li>
<li><a href="../pt445792/index.html">Como desenvolvemos a documenta√ß√£o em um projeto aberto da Embox</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>