<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🦁 💅🏼 🙍🏽 Die Idee der Trägheit (SGDm), die Idee der Skalierung (Adagrad) und der Regularisierung beim maschinellen Lernen am Beispiel des Klassifizierungsproblems 👹 🤯 👨🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Der nachfolgend verwendete Datensatz stammt aus einem bereits bestandenen Kaggle-Wettbewerb von hier . 
 Auf der Registerkarte Daten können Sie die Be...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Die Idee der Trägheit (SGDm), die Idee der Skalierung (Adagrad) und der Regularisierung beim maschinellen Lernen am Beispiel des Klassifizierungsproblems</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/473348/">  Der nachfolgend verwendete Datensatz stammt aus einem bereits bestandenen Kaggle-Wettbewerb <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">von hier</a> . <br>  Auf der Registerkarte Daten können Sie die Beschreibung aller Felder lesen. <br><br>  Der gesamte Quellcode <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ist hier</a> im Laptop-Format. <br><a name="habracut"></a><br>  Wir laden die Daten und überprüfen, ob wir im Allgemeinen Folgendes haben: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd dataset = pd.read_csv(<span class="hljs-string"><span class="hljs-string">'../input/ghouls-goblins-and-ghosts-boo/train.csv'</span></span>) <span class="hljs-comment"><span class="hljs-comment">#   X_test = pd.read_csv('../input/ghouls-goblins-and-ghosts-boo/test.csv') #   print(dataset.shape) print(dataset[:10])</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/ga/3u/p7/ga3up7ht-h4udt6hk9ptpnsw_cs.jpeg"><br><br>  Die Werte des Typfeldes (Ghoul, Ghost, Goblin) werden einfach durch 0, 1 und 2 ersetzt. <br><br>  Farbe - muss auch vorverarbeitet werden (wir benötigen nur numerische Werte, um das Modell zu erstellen).  Wir werden dafür LabelEncoder und OneHotEncoder verwenden.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Weitere Details</a> . <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.preprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LabelEncoder, OneHotEncoder labelencoder_X_1 = LabelEncoder() X_train[:, <span class="hljs-number"><span class="hljs-number">4</span></span>] = labelencoder_X_1.fit_transform(X_train[:, <span class="hljs-number"><span class="hljs-number">4</span></span>]) labelencoder_X_2 = LabelEncoder() X_test[:, <span class="hljs-number"><span class="hljs-number">4</span></span>] = labelencoder_X_2.fit_transform(X_test[:, <span class="hljs-number"><span class="hljs-number">4</span></span>]) labelencoder_Y_2 = LabelEncoder() Y_train = labelencoder_Y_2.fit_transform(Y_train) one_hot_encoder = OneHotEncoder(categorical_features = [<span class="hljs-number"><span class="hljs-number">4</span></span>]) X_train = one_hot_encoder.fit_transform(X_train).toarray() X_test = one_hot_encoder.fit_transform(X_test).toarray()</code> </pre><br>  Nun, zu diesem Zeitpunkt sind unsere Daten fertig.  Es bleibt unser Modell zu trainieren. <br><br>  Zuerst <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Adagrad auftragen</a> : <br><br>  Im Wesentlichen handelt es sich hierbei um eine Modifikation des stochastischen Gradientenabstiegs, über den ich das letzte Mal geschrieben habe: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">habr.com/en/post/472300</a> <br><br>  Diese Methode berücksichtigt den Verlauf aller vergangenen Gradienten für jeden einzelnen Parameter (die Idee der Skalierung).  Auf diese Weise können Sie die Größe des Lernschritts für Parameter mit einem großen Gradienten reduzieren: <br><br><img src="https://habrastorage.org/webt/_x/tl/an/_xtlannyj0jnvhq-smoh6yop4ly.jpeg"><br><br>  g ist der Skalierungsparameter (g0 = 0) <br>  θ - Parameter (Gewicht) <br>  epsilon ist eine kleine Konstante, die eingeführt wird, um eine Division durch Null zu verhindern <br><br>  Teilen Sie den Datensatz in 2 Teile: <br>  Trainingsmuster (Zug) und Validierung (val): <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size = <span class="hljs-number"><span class="hljs-number">0.2</span></span>)</code> </pre><br>  Eine kleine Vorbereitung für das Modelltraining: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np device = <span class="hljs-string"><span class="hljs-string">'cuda'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> torch.cuda.is_available() <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">'cpu'</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">make_train_step</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model, loss_fn, optimizer)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train_step</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x, y)</span></span></span><span class="hljs-function">:</span></span> model.train() yhat = model(x) loss = loss_fn(yhat, y) loss.backward() optimizer.step() optimizer.zero_grad() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> loss.item() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> train_step</code> </pre> <br>  Selbsttrainingsmodell: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> optim, nn model = torch.nn.Sequential( nn.Linear(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">270</span></span>), nn.ReLU(), nn.Linear(<span class="hljs-number"><span class="hljs-number">270</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>)) lr = <span class="hljs-number"><span class="hljs-number">0.01</span></span> n_epochs = <span class="hljs-number"><span class="hljs-number">500</span></span> loss_fn = torch.nn.CrossEntropyLoss() optimizer = optim.Adagrad(model.parameters(), lr=lr) train_step = make_train_step(model, loss_fn, optimizer) <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> shuffle <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> epoch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_epochs): x_train, y_train = shuffle(x_train, y_train) <span class="hljs-comment"><span class="hljs-comment">#    X = torch.FloatTensor(x_train) y = torch.LongTensor(y_train) loss = train_step(X, y) print(loss)</span></span></code> </pre> <br>  Modellbewertung: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  : test_var = torch.FloatTensor(x_val) with torch.no_grad(): result = model(test_var) values, labels = torch.max(result, 1) num_right = np.sum(labels.data.numpy() == y_val) print(' {:.2f}'.format(num_right / len(y_val)))</span></span></code> </pre> <br>  Hier haben wir zusätzlich zu den Ebenen (vorerst) nur 2 konfigurierbare Parameter: <br>  Lernrate und n_epochs (Anzahl der Epochen). <br><br>  Abhängig davon, wie wir diese beiden Parameter kombinieren, können drei Situationen auftreten: <br><br>  1 - alles ist gut, d.h.  Das Modell zeigt einen geringen Verlust bei der Trainingsprobe und eine hohe Genauigkeit bei der Validierung. <br><br>  2 - Unteranpassung - großer Verlust an der Trainingsprobe und geringe Genauigkeit an der Validierungsprobe. <br><br>  3 - Überanpassung - geringer Verlust bei der Trainingsprobe, aber geringe Genauigkeit bei der Validierung. <br><br>  Mit dem ersten ist alles klar :) <br><br>  Mit dem zweiten scheint es auch - mit der Lernrate und den n_epochs zu experimentieren. <br><br>  Und was tun mit dem dritten?  Die Antwort ist einfach - Regularisierung! <br><br>  Zuvor hatten wir eine Verlustfunktion der Form: <br>  L = MSE (Y, y) ohne zusätzliche Terme <br>  Das Wesen der Regularisierung besteht genau darin, dass durch Hinzufügen eines Begriffs zur Zielfunktion der Gradient „fein“ wird, wenn er zu groß ist.  Mit anderen Worten, wir beschränken unsere Zielfunktion. <br><br>  Es gibt viele Regularisierungsmethoden.  Weitere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Informationen zu</a> L1 und L2 - Regularisierung: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">craftappmobile.com/l1-vs-l2-regularization/#_L1_L2</a> <br><br>  Die Adagrad-Methode implementiert die L2-Regularisierung. Wenden wir sie an! <br><br>  Zur Verdeutlichung betrachten wir zunächst die Indikatoren des Modells ohne Regularisierung: <br><br>  lr = 0,01, n_epochs = 500: <br>  Verlust = 0,44 ... <br>  Genauigkeit: 0,71 <br><br>  lr = 0,01, n_epochs = 1000: <br>  Verlust = 0,41 ... <br>  Genauigkeit: 0,75 <br><br>  lr = 0,01, n_epochs = 2000: <br>  Verlust = 0,39 ... <br>  Genauigkeit: 0,75 <br><br>  lr = 0,01, n_epochs = 3000: <br>  Verlust = 0,367 ... <br>  Genauigkeit: 0,76 <br><br>  lr = 0,01, n_epochs = 4000: <br>  Verlust = 0,355 ... <br>  Genauigkeit: 0,72 <br><br>  lr = 0,01, n_epochs = 10000: <br>  Verlust = 0,285 ... <br>  Genauigkeit: 0,69 <br><br>  Hier können Sie sehen, dass bei 4k + Epochen das Modell bereits überpasst ist.  Versuchen wir nun, dies zu vermeiden: <br><br>  Fügen Sie dazu den Parameter weight_decay für unsere Optimierungsmethode hinzu: <br><br><pre> <code class="python hljs">optimizer = optim.Adagrad(model.parameters(), lr=lr, weight_decay = <span class="hljs-number"><span class="hljs-number">0.001</span></span>)</code> </pre> <br>  Mit lr = 0,01 ist m_epochs = 10000: <br>  Verlust = 0,367 ... <br>  Genauigkeit: 0,73 <br><br>  In 4000 Epochen: <br>  Verlust = 0,389 ... <br>  Genauigkeit: 0,75 <br><br>  Es ist viel besser geworden, aber wir haben nur 1 Parameter im Optimierer hinzugefügt :) <br><br>  Betrachten Sie nun SGDm (dies ist ein stochastischer Gradientenabstieg mit einer kleinen Ausdehnung - Heuristiken, wenn Sie möchten). <br><br>  Unter dem Strich aktualisiert <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SGD</a> die Parameter nach jeder Iteration ziemlich stark.  Es wäre logisch, den Gradienten mithilfe von Gradienten aus früheren Iterationen (der Idee der Trägheit) zu „glätten“: <br><br><img src="https://habrastorage.org/webt/qv/nq/ty/qvnqtydmtauek29nbjajagl-i6c.jpeg"><br><br>  θ - Parameter (Gewicht) <br>  µ - Trägheitshyperparameter <br><br>  SGD ohne Impulsparameter: <br><br><img src="https://habrastorage.org/webt/rj/rv/u8/rjrvu8nfx7_mcna815jgzx_hk3c.jpeg"><br><br>  SGD mit Impulsparameter: <br><br><img src="https://habrastorage.org/webt/xg/jq/-h/xgjq-hid1sb5cffzcxau7j_bclu.jpeg"><br><br>  Es stellte sich nicht viel besser heraus, aber der Punkt hier ist, dass es Methoden gibt, die sofort die Ideen der Skalierung und Trägheit verwenden.  Zum Beispiel Adam oder Adadelta, die jetzt gute Ergebnisse zeigen.  Nun, um diese Methoden zu verstehen, denke ich, dass es notwendig ist, einige grundlegende Ideen zu verstehen, die in einfacheren Methoden verwendet werden. <br><br>  Vielen Dank für Ihre Aufmerksamkeit! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de473348/">https://habr.com/ru/post/de473348/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de473338/index.html">TDD: Wie man Spezifikationen richtig schreibt (beschreibt)</a></li>
<li><a href="../de473340/index.html">Die Verdauung von frischen Materialien aus der Welt des Frontends für die letzte Woche Nr. 386 (21. - 27. Oktober 2019)</a></li>
<li><a href="../de473342/index.html">"Der lange Weg wartet auf Sie ..." oder Lösen des Prognoseproblems in C # mithilfe von Ml.NET (DataScience)</a></li>
<li><a href="../de473344/index.html">Konzerte und Events KudaGo in deinem Spiegel</a></li>
<li><a href="../de473346/index.html">Erstellen einer REST-API mit Node.js und einer Oracle-Datenbank. Teil 2</a></li>
<li><a href="../de473350/index.html">Erstellen einer REST-API mit Node.js und einer Oracle-Datenbank. Teil 3</a></li>
<li><a href="../de473352/index.html">Aktuelle Sammlungen in 10 Minuten</a></li>
<li><a href="../de473354/index.html">Über die Kuriositäten der Habrostatistik</a></li>
<li><a href="../de473358/index.html">Installieren und konfigurieren Sie Nexus Sonatype mithilfe der Infrastruktur als Code-Ansatz</a></li>
<li><a href="../de473362/index.html">GSoC-Erfahrung: Wie zwei (drei) Schüler den CRIU-Code wirklich verbessert haben</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>