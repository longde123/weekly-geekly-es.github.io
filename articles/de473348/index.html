<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü¶Å üíÖüèº üôçüèΩ Die Idee der Tr√§gheit (SGDm), die Idee der Skalierung (Adagrad) und der Regularisierung beim maschinellen Lernen am Beispiel des Klassifizierungsproblems üëπ ü§Ø üë®üèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Der nachfolgend verwendete Datensatz stammt aus einem bereits bestandenen Kaggle-Wettbewerb von hier . 
 Auf der Registerkarte Daten k√∂nnen Sie die Be...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Die Idee der Tr√§gheit (SGDm), die Idee der Skalierung (Adagrad) und der Regularisierung beim maschinellen Lernen am Beispiel des Klassifizierungsproblems</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/473348/">  Der nachfolgend verwendete Datensatz stammt aus einem bereits bestandenen Kaggle-Wettbewerb <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">von hier</a> . <br>  Auf der Registerkarte Daten k√∂nnen Sie die Beschreibung aller Felder lesen. <br><br>  Der gesamte Quellcode <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ist hier</a> im Laptop-Format. <br><a name="habracut"></a><br>  Wir laden die Daten und √ºberpr√ºfen, ob wir im Allgemeinen Folgendes haben: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd dataset = pd.read_csv(<span class="hljs-string"><span class="hljs-string">'../input/ghouls-goblins-and-ghosts-boo/train.csv'</span></span>) <span class="hljs-comment"><span class="hljs-comment">#   X_test = pd.read_csv('../input/ghouls-goblins-and-ghosts-boo/test.csv') #   print(dataset.shape) print(dataset[:10])</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/ga/3u/p7/ga3up7ht-h4udt6hk9ptpnsw_cs.jpeg"><br><br>  Die Werte des Typfeldes (Ghoul, Ghost, Goblin) werden einfach durch 0, 1 und 2 ersetzt. <br><br>  Farbe - muss auch vorverarbeitet werden (wir ben√∂tigen nur numerische Werte, um das Modell zu erstellen).  Wir werden daf√ºr LabelEncoder und OneHotEncoder verwenden.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Weitere Details</a> . <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.preprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LabelEncoder, OneHotEncoder labelencoder_X_1 = LabelEncoder() X_train[:, <span class="hljs-number"><span class="hljs-number">4</span></span>] = labelencoder_X_1.fit_transform(X_train[:, <span class="hljs-number"><span class="hljs-number">4</span></span>]) labelencoder_X_2 = LabelEncoder() X_test[:, <span class="hljs-number"><span class="hljs-number">4</span></span>] = labelencoder_X_2.fit_transform(X_test[:, <span class="hljs-number"><span class="hljs-number">4</span></span>]) labelencoder_Y_2 = LabelEncoder() Y_train = labelencoder_Y_2.fit_transform(Y_train) one_hot_encoder = OneHotEncoder(categorical_features = [<span class="hljs-number"><span class="hljs-number">4</span></span>]) X_train = one_hot_encoder.fit_transform(X_train).toarray() X_test = one_hot_encoder.fit_transform(X_test).toarray()</code> </pre><br>  Nun, zu diesem Zeitpunkt sind unsere Daten fertig.  Es bleibt unser Modell zu trainieren. <br><br>  Zuerst <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Adagrad auftragen</a> : <br><br>  Im Wesentlichen handelt es sich hierbei um eine Modifikation des stochastischen Gradientenabstiegs, √ºber den ich das letzte Mal geschrieben habe: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">habr.com/en/post/472300</a> <br><br>  Diese Methode ber√ºcksichtigt den Verlauf aller vergangenen Gradienten f√ºr jeden einzelnen Parameter (die Idee der Skalierung).  Auf diese Weise k√∂nnen Sie die Gr√∂√üe des Lernschritts f√ºr Parameter mit einem gro√üen Gradienten reduzieren: <br><br><img src="https://habrastorage.org/webt/_x/tl/an/_xtlannyj0jnvhq-smoh6yop4ly.jpeg"><br><br>  g ist der Skalierungsparameter (g0 = 0) <br>  Œ∏ - Parameter (Gewicht) <br>  epsilon ist eine kleine Konstante, die eingef√ºhrt wird, um eine Division durch Null zu verhindern <br><br>  Teilen Sie den Datensatz in 2 Teile: <br>  Trainingsmuster (Zug) und Validierung (val): <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size = <span class="hljs-number"><span class="hljs-number">0.2</span></span>)</code> </pre><br>  Eine kleine Vorbereitung f√ºr das Modelltraining: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np device = <span class="hljs-string"><span class="hljs-string">'cuda'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> torch.cuda.is_available() <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">'cpu'</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">make_train_step</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model, loss_fn, optimizer)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train_step</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x, y)</span></span></span><span class="hljs-function">:</span></span> model.train() yhat = model(x) loss = loss_fn(yhat, y) loss.backward() optimizer.step() optimizer.zero_grad() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> loss.item() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> train_step</code> </pre> <br>  Selbsttrainingsmodell: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> optim, nn model = torch.nn.Sequential( nn.Linear(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">270</span></span>), nn.ReLU(), nn.Linear(<span class="hljs-number"><span class="hljs-number">270</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>)) lr = <span class="hljs-number"><span class="hljs-number">0.01</span></span> n_epochs = <span class="hljs-number"><span class="hljs-number">500</span></span> loss_fn = torch.nn.CrossEntropyLoss() optimizer = optim.Adagrad(model.parameters(), lr=lr) train_step = make_train_step(model, loss_fn, optimizer) <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> shuffle <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> epoch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_epochs): x_train, y_train = shuffle(x_train, y_train) <span class="hljs-comment"><span class="hljs-comment">#    X = torch.FloatTensor(x_train) y = torch.LongTensor(y_train) loss = train_step(X, y) print(loss)</span></span></code> </pre> <br>  Modellbewertung: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  : test_var = torch.FloatTensor(x_val) with torch.no_grad(): result = model(test_var) values, labels = torch.max(result, 1) num_right = np.sum(labels.data.numpy() == y_val) print(' {:.2f}'.format(num_right / len(y_val)))</span></span></code> </pre> <br>  Hier haben wir zus√§tzlich zu den Ebenen (vorerst) nur 2 konfigurierbare Parameter: <br>  Lernrate und n_epochs (Anzahl der Epochen). <br><br>  Abh√§ngig davon, wie wir diese beiden Parameter kombinieren, k√∂nnen drei Situationen auftreten: <br><br>  1 - alles ist gut, d.h.  Das Modell zeigt einen geringen Verlust bei der Trainingsprobe und eine hohe Genauigkeit bei der Validierung. <br><br>  2 - Unteranpassung - gro√üer Verlust an der Trainingsprobe und geringe Genauigkeit an der Validierungsprobe. <br><br>  3 - √úberanpassung - geringer Verlust bei der Trainingsprobe, aber geringe Genauigkeit bei der Validierung. <br><br>  Mit dem ersten ist alles klar :) <br><br>  Mit dem zweiten scheint es auch - mit der Lernrate und den n_epochs zu experimentieren. <br><br>  Und was tun mit dem dritten?  Die Antwort ist einfach - Regularisierung! <br><br>  Zuvor hatten wir eine Verlustfunktion der Form: <br>  L = MSE (Y, y) ohne zus√§tzliche Terme <br>  Das Wesen der Regularisierung besteht genau darin, dass durch Hinzuf√ºgen eines Begriffs zur Zielfunktion der Gradient ‚Äûfein‚Äú wird, wenn er zu gro√ü ist.  Mit anderen Worten, wir beschr√§nken unsere Zielfunktion. <br><br>  Es gibt viele Regularisierungsmethoden.  Weitere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Informationen zu</a> L1 und L2 - Regularisierung: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">craftappmobile.com/l1-vs-l2-regularization/#_L1_L2</a> <br><br>  Die Adagrad-Methode implementiert die L2-Regularisierung. Wenden wir sie an! <br><br>  Zur Verdeutlichung betrachten wir zun√§chst die Indikatoren des Modells ohne Regularisierung: <br><br>  lr = 0,01, n_epochs = 500: <br>  Verlust = 0,44 ... <br>  Genauigkeit: 0,71 <br><br>  lr = 0,01, n_epochs = 1000: <br>  Verlust = 0,41 ... <br>  Genauigkeit: 0,75 <br><br>  lr = 0,01, n_epochs = 2000: <br>  Verlust = 0,39 ... <br>  Genauigkeit: 0,75 <br><br>  lr = 0,01, n_epochs = 3000: <br>  Verlust = 0,367 ... <br>  Genauigkeit: 0,76 <br><br>  lr = 0,01, n_epochs = 4000: <br>  Verlust = 0,355 ... <br>  Genauigkeit: 0,72 <br><br>  lr = 0,01, n_epochs = 10000: <br>  Verlust = 0,285 ... <br>  Genauigkeit: 0,69 <br><br>  Hier k√∂nnen Sie sehen, dass bei 4k + Epochen das Modell bereits √ºberpasst ist.  Versuchen wir nun, dies zu vermeiden: <br><br>  F√ºgen Sie dazu den Parameter weight_decay f√ºr unsere Optimierungsmethode hinzu: <br><br><pre> <code class="python hljs">optimizer = optim.Adagrad(model.parameters(), lr=lr, weight_decay = <span class="hljs-number"><span class="hljs-number">0.001</span></span>)</code> </pre> <br>  Mit lr = 0,01 ist m_epochs = 10000: <br>  Verlust = 0,367 ... <br>  Genauigkeit: 0,73 <br><br>  In 4000 Epochen: <br>  Verlust = 0,389 ... <br>  Genauigkeit: 0,75 <br><br>  Es ist viel besser geworden, aber wir haben nur 1 Parameter im Optimierer hinzugef√ºgt :) <br><br>  Betrachten Sie nun SGDm (dies ist ein stochastischer Gradientenabstieg mit einer kleinen Ausdehnung - Heuristiken, wenn Sie m√∂chten). <br><br>  Unter dem Strich aktualisiert <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SGD</a> die Parameter nach jeder Iteration ziemlich stark.  Es w√§re logisch, den Gradienten mithilfe von Gradienten aus fr√ºheren Iterationen (der Idee der Tr√§gheit) zu ‚Äûgl√§tten‚Äú: <br><br><img src="https://habrastorage.org/webt/qv/nq/ty/qvnqtydmtauek29nbjajagl-i6c.jpeg"><br><br>  Œ∏ - Parameter (Gewicht) <br>  ¬µ - Tr√§gheitshyperparameter <br><br>  SGD ohne Impulsparameter: <br><br><img src="https://habrastorage.org/webt/rj/rv/u8/rjrvu8nfx7_mcna815jgzx_hk3c.jpeg"><br><br>  SGD mit Impulsparameter: <br><br><img src="https://habrastorage.org/webt/xg/jq/-h/xgjq-hid1sb5cffzcxau7j_bclu.jpeg"><br><br>  Es stellte sich nicht viel besser heraus, aber der Punkt hier ist, dass es Methoden gibt, die sofort die Ideen der Skalierung und Tr√§gheit verwenden.  Zum Beispiel Adam oder Adadelta, die jetzt gute Ergebnisse zeigen.  Nun, um diese Methoden zu verstehen, denke ich, dass es notwendig ist, einige grundlegende Ideen zu verstehen, die in einfacheren Methoden verwendet werden. <br><br>  Vielen Dank f√ºr Ihre Aufmerksamkeit! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de473348/">https://habr.com/ru/post/de473348/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de473338/index.html">TDD: Wie man Spezifikationen richtig schreibt (beschreibt)</a></li>
<li><a href="../de473340/index.html">Die Verdauung von frischen Materialien aus der Welt des Frontends f√ºr die letzte Woche Nr. 386 (21. - 27. Oktober 2019)</a></li>
<li><a href="../de473342/index.html">"Der lange Weg wartet auf Sie ..." oder L√∂sen des Prognoseproblems in C # mithilfe von Ml.NET (DataScience)</a></li>
<li><a href="../de473344/index.html">Konzerte und Events KudaGo in deinem Spiegel</a></li>
<li><a href="../de473346/index.html">Erstellen einer REST-API mit Node.js und einer Oracle-Datenbank. Teil 2</a></li>
<li><a href="../de473350/index.html">Erstellen einer REST-API mit Node.js und einer Oracle-Datenbank. Teil 3</a></li>
<li><a href="../de473352/index.html">Aktuelle Sammlungen in 10 Minuten</a></li>
<li><a href="../de473354/index.html">√úber die Kuriosit√§ten der Habrostatistik</a></li>
<li><a href="../de473358/index.html">Installieren und konfigurieren Sie Nexus Sonatype mithilfe der Infrastruktur als Code-Ansatz</a></li>
<li><a href="../de473362/index.html">GSoC-Erfahrung: Wie zwei (drei) Sch√ºler den CRIU-Code wirklich verbessert haben</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>