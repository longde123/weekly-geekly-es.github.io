<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëèüèΩ üí• üçì Erstellen eines Bots zur Teilnahme am AI Mini Cup 2018 basierend auf einem wiederkehrenden neuronalen Netzwerk (Teil 2) üï∫üèæ ü§¥ üèôÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dies ist eine Fortsetzung des ersten Teils des Artikels. 


 Im ersten Teil des Artikels sprach der Autor auf mail.ru √ºber die Bedingungen des Wettbew...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Erstellen eines Bots zur Teilnahme am AI Mini Cup 2018 basierend auf einem wiederkehrenden neuronalen Netzwerk (Teil 2)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/417657/"><p><img src="https://habrastorage.org/webt/of/tw/ke/oftwke-wbh5-w_nlsm_p5rmhano.jpeg"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dies ist eine Fortsetzung des ersten Teils des Artikels.</a> </p><br><p> Im ersten Teil des Artikels sprach der Autor auf mail.ru √ºber die Bedingungen des Wettbewerbs f√ºr das Spiel Agario, die Struktur der Spielwelt und teilweise √ºber die Struktur des Bots.  Zum Teil, weil sie nur die Vorrichtung von Eingangssensoren und Befehlen am Ausgang des neuronalen Netzwerks betrafen (im Folgenden wird in Bildern und Text eine Abk√ºrzung NN verwendet).  Versuchen wir also, die Black Box zu √∂ffnen und zu verstehen, wie dort alles angeordnet ist. </p><a name="habracut"></a><br><p>  Und hier ist das erste Bild: </p><br><p><img src="https://habrastorage.org/webt/v2/pb/_s/v2pb_sha05gqjtqcqp9bv5mnm0o.jpeg"></p><br><p>  Es zeigt schematisch, was bei meinem Leser ein gelangweiltes L√§cheln hervorrufen sollte, sagen sie noch einmal in der ersten Klasse, sie wurden schon oft in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">verschiedenen Quellen gesehen</a> .  Aber wir wollen dieses Bild wirklich praktisch auf die Verwaltung des Bots anwenden, also schauen wir es uns nach dem wichtigen Hinweis genauer an. </p><br><p>  <strong>Wichtiger Hinweis:</strong> Es gibt eine Vielzahl vorgefertigter L√∂sungen (Frameworks) f√ºr die Arbeit mit neuronalen Netzen: </p><br><p><img src="https://habrastorage.org/webt/jt/il/97/jtil97rhtusvazj6iared1q_hnc.png"></p><br><p>  Alle diese Pakete l√∂sen die Hauptaufgaben f√ºr den Entwickler neuronaler Netze: den Aufbau und das Training von NN oder die Suche nach "optimalen" Gewichten.  Die Hauptmethode dieser Suche ist die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Backpropagation</a> .  Es wurde in den 70er Jahren des letzten Jahrhunderts erfunden, wie aus dem Artikel unter dem obigen Link hervorgeht. In dieser Zeit hat es als Boden des Schiffes verschiedene Verbesserungen erfahren, aber das Wesentliche ist dasselbe: Gewichtskoeffizienten mit einer Basis von Trainingsbeispielen zu finden, und es ist √§u√üerst w√ºnschenswert, dass jeder Diese Beispiele enthielten eine vorgefertigte Antwort in Form eines Ausgangssignals eines neuronalen Netzwerks.  Der Leser kann gegen mich protestieren.  dass selbstlernende Netzwerke verschiedener Klassen und Prinzipien bereits erfunden wurden, aber soweit ich wei√ü, l√§uft dort nicht alles reibungslos.  Nat√ºrlich gibt es Pl√§ne, diesen Zoo genauer zu studieren, aber ich denke, ich werde Gleichgesinnte finden, die feststellen, dass ein selbstgemachtes Fahrrad ohne spezielle Zeichnungen f√ºr den Sch√∂pfer noch st√§rker gebogen ist als ein F√∂rderklon eines idealen Fahrrads. <br>  Der Autor verstand, dass der Spieleserver h√∂chstwahrscheinlich nicht √ºber diese Bibliotheken verf√ºgt und die von den Organisatoren als 1 Prozessorkern zugewiesene Rechenleistung f√ºr ein umfangreiches Framework eindeutig nicht ausreicht, und entwickelte sein eigenes Fahrrad.  Ein wichtiger Kommentar dazu endete. </p><br><p>  Kehren wir zu dem Bild zur√ºck, das das wahrscheinlich einfachste der m√∂glichen neuronalen Netze mit einer verborgenen Schicht (auch bekannt als verborgene Schicht oder verborgene Schicht) zeigt.  Jetzt hat der Autor selbst das Bild mit Ideen zu diesem einfachen Beispiel stetig betrachtet, um dem Leser die Tiefen k√ºnstlicher neuronaler Netze zu offenbaren.  Wenn alles zu einem Primitiven vereinfacht ist, ist es einfacher, die Essenz zu verstehen.  Die Quintessenz ist, dass das Neuron der verborgenen Schicht nichts zusammenzufassen hat.  Und h√∂chstwahrscheinlich ist dies nicht einmal ein neuronales Netzwerk. In Lehrb√ºchern ist das einfachste NN ein Netzwerk mit zwei Eing√§ngen.  Hier sind wir also sozusagen die Entdecker des einfachsten der einfachsten Netzwerke. </p><br><p>  Versuchen wir, dieses neuronale Netzwerk (Pseudocode) zu beschreiben: <br>  Wir f√ºhren die Netzwerktopologie in Form eines Arrays ein, wobei jedes Element der Schicht und der Anzahl der darin enthaltenen Neuronen entspricht: </p><br><p><code>int array Topology= { 1, 1, 1}</code> <br>  Wir ben√∂tigen auch ein Float-Array von Gewichten des neuronalen Netzwerks W, wenn wir unser Netzwerk als "Feed-Forward-Neuronale Netzwerke (FF oder FFNN)" betrachten, bei dem jedes Neuron der aktuellen Schicht mit jedem Neuron der n√§chsten Schicht verbunden ist, erhalten wir die Dimension des Arrays W [Anzahl der Schichten , die Anzahl der Neuronen in der Schicht, die Anzahl der Neuronen in der Schicht].  Nicht ganz die optimale Codierung, aber angesichts des hei√üen Atems der GPU irgendwo sehr nah im Text ist es verst√§ndlich. <br>  Ein kurzes <code>CalculateSize</code> Verfahren zum Z√§hlen der Anzahl der <code>neuroncount</code> Neuronenanzahl und der Anzahl ihrer Verbindungen im neuronalen Netzwerk mit <code>dendritecount</code> wird dem Autor <code>neuroncount</code> die Art dieser Verbindungen besser erkl√§ren: </p><br><pre> <code class="hljs pgsql"><span class="hljs-type"><span class="hljs-type">void</span></span> CalculateSize(<span class="hljs-keyword"><span class="hljs-keyword">array</span></span> <span class="hljs-type"><span class="hljs-type">int</span></span> Topology, <span class="hljs-type"><span class="hljs-type">int</span></span> neuroncount, <span class="hljs-type"><span class="hljs-type">int</span></span> dendritecount) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-type"><span class="hljs-type">int</span></span> i : Topology) // i         neuroncount += i; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-type"><span class="hljs-type">int</span></span> layer = <span class="hljs-number"><span class="hljs-number">0</span></span>, layer &lt;Topology.Length - <span class="hljs-number"><span class="hljs-number">1</span></span>, layer++) //   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-type"><span class="hljs-type">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>, i &lt; Topology[layer] + <span class="hljs-number"><span class="hljs-number">1</span></span>, i++) //   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-type"><span class="hljs-type">int</span></span> j = <span class="hljs-number"><span class="hljs-number">0</span></span>, j &lt; Topology[layer + <span class="hljs-number"><span class="hljs-number">1</span></span>], j++) //   dendritecount++; }</code> </pre> <br><p>  Mein Leser, der dies alles bereits wei√ü, der Autor ist im ersten Artikel zu dieser Meinung gekommen, wird sicherlich nicht fragen: Warum in der dritten verschachtelten Schleife Topologie [Schicht1 + 1] anstelle von Topologie [Schicht1], die dem Neuron mehr gibt als in der Netzwerktopologie .  Ich werde nicht antworten.  Es ist auch n√ºtzlich f√ºr den Leser, Hausaufgaben zu machen. </p><br><p>  Wir sind fast einen Schritt vom Aufbau eines funktionierenden neuronalen Netzwerks entfernt.  Es bleibt die Funktion der Summierung der Signale am Eingang des Neurons und seiner Aktivierung hinzuzuf√ºgen.  Es gibt viele Aktivierungsfunktionen, aber diejenigen, die der Natur des Neurons am n√§chsten kommen, sind Sigmoid und Tangensoid <em>(wahrscheinlich ist es besser, es so zu nennen, obwohl dieser Name in der Literatur nicht verwendet wird, das Maximum tangential ist, aber dies ist der Name des Graphen, obwohl was ein Graph ist, wenn er nicht die Funktion widerspiegelt?)</em> </p><br><p>  Hier haben wir also die Neuronenaktivierungsfunktionen (sie sind im Bild im unteren Teil vorhanden). </p><br><pre> <code class="hljs go">float Sigmoid(float x) { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (x &lt; <span class="hljs-number"><span class="hljs-number">-10.0f</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">0.0f</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (x &gt; <span class="hljs-number"><span class="hljs-number">10.0f</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">1.0f</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (float)(<span class="hljs-number"><span class="hljs-number">1.0f</span></span> / (<span class="hljs-number"><span class="hljs-number">1.0f</span></span> + expf(-x))); }</code> </pre> <br><p>  Sigmoid gibt Werte von 0 bis 1 zur√ºck. </p><br><pre> <code class="hljs cs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">float</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Tanh</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params"> x</span></span></span><span class="hljs-function">)</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (x &lt; <span class="hljs-number"><span class="hljs-number">-10.0f</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">-1.0f</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (x &gt; <span class="hljs-number"><span class="hljs-number">10.0f</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">1.0f</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">float</span></span>)(tanhf(x)); }</code> </pre> <br><p>  Der Tangentoid gibt Werte von -1 bis 1 zur√ºck. </p><br><p>  Die Hauptidee eines Signals, das ein neuronales Netzwerk durchl√§uft, ist eine Welle: Ein Signal wird Eingangsneuronen zugef√ºhrt -&gt; √ºber neuronale Verbindungen gelangt das Signal zur zweiten Schicht -&gt; Neuronen der zweiten Schicht fassen die Signale zusammen, die sie erreicht haben, ge√§ndert durch interneuronale Gewichte -&gt; wird durch ein zus√§tzliches Vorspannungsgewicht hinzugef√ºgt -&gt; Wir verwenden die Aktivierungsfunktion-&gt; und gehen zur n√§chsten Schicht (lesen Sie den ersten Zyklus aus dem Beispiel nach Schichten), dh wir wiederholen die Kette von Anfang an, nur die Neuronen der n√§chsten Schicht werden zu Eingangsneuronen.  Zur Vereinfachung m√ºssen Sie nicht einmal die Werte der Neuronen des gesamten Netzwerks speichern, sondern nur die NN-Gewichte und die Werte der Neuronen der aktiven Schicht. </p><br><p>  Wir senden erneut ein Signal an den Eingang NN, die Welle lief durch die Schichten und auf der Ausgangsschicht entfernen wir den erhaltenen Wert. </p><br><p>  Nach dem Geschmack des Lesers ist es hier m√∂glich, das Programm mithilfe von Rekursion oder nur einem Dreifachzyklus wie dem des Autors zu l√∂sen. Um die Berechnungen zu beschleunigen, m√ºssen Sie keine Objekte in Form von Neuronen und deren Verbindungen und anderen OOPs umz√§unen.  Dies ist wiederum auf das Gef√ºhl enger GPU-Berechnungen zur√ºckzuf√ºhren, und bei GPUs bleibt OOP aufgrund ihrer Art der Massenparallelit√§t etwas stehen, dies ist relativ zu c # und C ++. </p><br><p>  Dar√ºber hinaus wird der Leser aufgefordert, unabh√§ngig den Weg zum Aufbau eines neuronalen Netzwerks in Code zu gehen, wobei sein Leser dies freiwillig w√ºnscht, dessen Fehlen dem Autor klar und vertraut ist, da es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">f√ºr Beispiele zum Erstellen von NN</a> von Grund auf viele Beispiele im Netzwerk gibt, so dass es schwierig sein wird, in die Irre zu gehen so einfach wie ein neuronales Netzwerk mit direkter Verteilung im obigen Bild. </p><br><p>  Aber wo wird der Leser ausrufen, der noch nicht von der vorherigen Passage abgewichen ist und in der Kindheit Recht haben wird? Der Autor hat den Wert des Buches durch Illustrationen dazu bestimmt.  Hier bitte: </p><br><p><img src="https://habrastorage.org/webt/03/jl/dw/03jldwzryoeaxscxxtfi3deakie.jpeg"></p><br><p>  Im Bild sehen wir ein wiederkehrendes Neuron und ein aus solchen Neuronen aufgebautes NN wird als wiederkehrendes oder RNN bezeichnet.  Das angegebene neuronale Netzwerk hat ein Kurzzeitged√§chtnis und wurde vom Autor f√ºr den Bot als das vielversprechendste in Bezug auf die Anpassung an den Spielprozess ausgew√§hlt.  Nat√ºrlich baute der Autor ein neuronales Netzwerk mit direkter Verteilung auf, aber auf der Suche nach einer ‚Äûeffektiven‚Äú L√∂sung wechselte er zu RNN. </p><br><p>  Ein wiederkehrendes Neuron hat einen zus√§tzlichen Zustand C, der nach dem ersten Durchgang eines Signals durch ein Neuron gebildet wird, Tick + 0 auf der Zeitachse.  In einfachen Worten ist dies eine Kopie des Ausgangssignals eines Neurons.  Lesen Sie im zweiten Schritt Tick + 1 (da das Netzwerk mit der Frequenz des Spielbot und des Servers arbeitet), kehrt der Wert C durch zus√§tzliche Gewichte zum Eingang der neuronalen Schicht zur√ºck und nimmt somit an der Signalbildung teil, jedoch bereits zum Zeitpunkt Tick + 1. </p><br><p>  <em>Hinweis: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">In der Arbeit von Forschungsgruppen zur Verwaltung von NN-Spiel-Bots</a> besteht die Tendenz, zwei Rhythmen f√ºr ein neuronales Netzwerk zu verwenden. Ein Rhythmus ist die Frequenz des Spiel-Ticks, der zweite Rhythmus ist beispielsweise doppelt so langsam wie der erste.</em>  <em>Verschiedene Teile des NN arbeiten mit unterschiedlichen Frequenzen, was eine unterschiedliche Sicht auf die Spielsituation innerhalb des NN erm√∂glicht und dadurch dessen Flexibilit√§t erh√∂ht.</em> </p><br><p>  Um RNN im Bot-Code zu erstellen, f√ºgen wir ein zus√§tzliches Array in die Topologie ein, wobei jedes Element der Schicht und der Anzahl der darin enthaltenen neuronalen Zust√§nde entspricht: </p><br><p> <code>int array TopologyNN= { numberofSensors, 16, 8, 4}</code> <br> <code>int array TopologyRNN= { 0, 16, 0, 0 }</code> </p> <br><p>  Aus der obigen Topologie ist ersichtlich, dass die zweite Schicht wiederkehrend ist, da sie neuronale Zust√§nde enth√§lt.  Wir f√ºhren auch zus√§tzliche Gewichte in Form eines Gleitkommas des WRR-Arrays ein, das dieselbe Dimension wie das W-Array hat. </p><br><p>  Die Anzahl der Verbindungen in unserem neuronalen Netzwerk wird sich etwas √§ndern: </p><br><pre> <code class="hljs matlab"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int layer = <span class="hljs-number"><span class="hljs-number">0</span></span>, layer &lt; TopologyNN.Length - <span class="hljs-number"><span class="hljs-number">1</span></span>, layer++) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int <span class="hljs-built_in"><span class="hljs-built_in">i</span></span> = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">i</span></span> &lt; TopologyNN[layer] + <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">i</span></span>++) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int <span class="hljs-built_in"><span class="hljs-built_in">j</span></span> = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">j</span></span> &lt; TopologyNN[layer + <span class="hljs-number"><span class="hljs-number">1</span></span>] , <span class="hljs-built_in"><span class="hljs-built_in">j</span></span>++) dendritecount++; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int layer = <span class="hljs-number"><span class="hljs-number">0</span></span>, layer &lt; TopologyRNN.Length - <span class="hljs-number"><span class="hljs-number">1</span></span>, layer++) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int <span class="hljs-built_in"><span class="hljs-built_in">i</span></span> = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">i</span></span>&lt; TopologyRNN[layer] + <span class="hljs-number"><span class="hljs-number">1</span></span> , <span class="hljs-built_in"><span class="hljs-built_in">i</span></span>++) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int <span class="hljs-built_in"><span class="hljs-built_in">j</span></span> = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">j</span></span>&lt; TopologyRNN[layer], <span class="hljs-built_in"><span class="hljs-built_in">j</span></span>++) dendritecount++;</code> </pre> <br><p>  Der Autor wird am Ende dieses Artikels den allgemeinen Code f√ºr ein wiederkehrendes neuronales Netzwerk anh√§ngen. Das Wichtigste ist jedoch das Prinzip: Der Durchgang einer Welle durch Schichten bei einem wiederkehrenden NN √§ndert nichts grundlegend, nur ein weiterer Begriff wird der Neuronenaktivierungsfunktion hinzugef√ºgt.  Dies ist der Begriff des Zustands der Neuronen auf dem vorherigen Tick multipliziert mit dem Gewicht der neuronalen Verbindung. </p><br><p>  Wir gehen davon aus, dass Theorie und Praxis neuronaler Netze aktualisiert wurden, aber der Autor ist sich klar dar√ºber im Klaren, dass er den Leser nicht n√§her an das Verst√§ndnis gebracht hat, wie man diese einfache Struktur neuronaler Netze lehrt, um Entscheidungen im Spiel zu treffen.  Wir haben keine Bibliotheken mit Beispielen f√ºr das Unterrichten von NN.  In den Internetgruppen der Bot-Entwickler gab es eine Meinung: Geben Sie uns eine Protokolldatei in Form von Koordinaten von Bots und anderen Spielinformationen, um eine Bibliothek mit Beispielen zu bilden.  Leider konnte der Autor nicht herausfinden, wie diese Protokolldatei f√ºr das Training von NN verwendet werden soll.  Ich werde dies gerne in den Kommentaren zum Artikel diskutieren.  Daher war die einzige Methode, die dem Autor zur Verf√ºgung stand, um die Trainingsmethode zu verstehen oder vielmehr "effektive" Neurobalances (Neuroconnections) zu finden, der genetische Algorithmus. </p><br><p>  Erstellt ein Bild √ºber die Prinzipien des genetischen Algorithmus: </p><br><p><img src="https://habrastorage.org/webt/-3/ex/ls/-3exlspxldh64avmkbi_3vuewou.jpeg"></p><br><p>  Also der <strong>genetische Algorithmus</strong> . </p><br><p>  Der Autor wird versuchen, sich nicht mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Theorie dieses Prozesses</a> zu befassen, sondern sich nur an das Minimum zu erinnern, das erforderlich ist, um den Artikel vollst√§ndig zu lesen. <br>  Im genetischen Algorithmus ist das Gen die Hauptarbeitsfl√ºssigkeit (DNA ist der Name des Molek√ºls).  Das Genom in unserem Fall ist ein sequentieller Satz von Genen oder eine eindimensionale Anordnung von ... </p><br><p>  In der Anfangsphase der Arbeit mit einem neu aufgebauten neuronalen Netzwerk muss es initialisiert werden.  Die Initialisierung bezieht sich auf die Zuordnung von Zufallswerten von -1 bis 1 zu neuronalen Gleichgewichten. Der Autor hat erw√§hnt, dass der Wertebereich von -1 bis 1 zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">extrem ist</a> und trainierte Netzwerke Gewichte in einem kleineren Bereich haben, beispielsweise von -0,5 bis 0,5, und dass Sie einen anf√§nglichen Wertebereich von ausgezeichnet nehmen sollten von -1 bis 1. Wir werden jedoch den klassischen Weg gehen, alle Schwierigkeiten in einem Gate zu sammeln und ein m√∂glichst breites Segment anf√§nglicher Zufallsvariablen als Grundlage f√ºr die Initialisierung des neuronalen Netzwerks zu verwenden. </p><br><p>  Nun erfolgt eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bijektion</a> .  Wir gehen davon aus, dass die L√§nge (Gr√∂√üe) des Bot-Genoms gleich der Gesamtl√§nge der Arrays des neuronalen Netzwerks TopologyNN.Length + TopologyRNN.Length ist, nicht umsonst, dass der Autor die Zeit des Lesers mit dem Verfahren zum Z√§hlen neuronaler Verbindungen verbracht hat. </p><br><p>  <em>Hinweis: Wie der Leser bereits selbst festgestellt hat, √ºbertragen wir nur die Gewichte des neuronalen Netzwerks auf den Genotyp, die Verbindungsstruktur, Aktivierungsfunktionen und Neuronenzust√§nde werden nicht √ºbertragen.</em>  <em>F√ºr einen genetischen Algorithmus sind nur neuronale Verbindungen ausreichend, was darauf hindeutet, dass sie die Informationstr√§ger sind.</em>  <em>Es gibt Entwicklungen, bei denen der genetische Algorithmus auch die Struktur von Verbindungen im neuronalen Netzwerk √§ndert und es recht einfach ist, sie zu implementieren.</em>  <em>Hier l√§sst der Autor dem Leser Raum f√ºr Kreativit√§t, obwohl er selbst mit Interesse dar√ºber nachdenken wird: Sie m√ºssen die Verwendung von zwei unabh√§ngigen Genomen und zwei Fitnessfunktionen (vereinfacht zwei unabh√§ngige genetische Algorithmen) verstehen, oder Sie k√∂nnen alle dasselbe Gen und denselben Algorithmus verwenden.</em> </p><br><p>  Und da wir NN mit Zufallsvariablen initialisiert haben, haben wir damit das Genom initialisiert.  Der umgekehrte Prozess ist ebenfalls m√∂glich: Initialisierung des Genotyps durch Zufallsvariablen und anschlie√üendes Kopieren in neuronale Gewichte.  Die zweite Option ist √ºblich.  Da der genetische Algorithmus im Programm oft au√üerhalb der Essenz selbst existiert und nur durch die Genomdaten und den Wert der Fitnessfunktion damit verbunden ist ... Stopp, Stopp, wird der Leser sagen, das Bild zeigt deutlich die Population und kein Wort √ºber das einzelne Genom. </p><br><p>  Ok, f√ºgen Sie dem Gedankenofen des Lesers einige Bilder hinzu: </p><br><p><img src="https://habrastorage.org/webt/h5/dz/ho/h5dzhojhlf6b1xqsol30qo63mdo.jpeg"></p><br><p>  Da der Autor die Bilder vor dem Schreiben des Textes des Artikels gemalt hat, unterst√ºtzen sie den Text, folgen jedoch nicht dem Buchstaben zum Buchstaben der aktuellen Geschichte. </p><br><p>  Aus den gewonnenen Informationen folgt, dass der Hauptarbeitsk√∂rper des genetischen Algorithmus eine Population von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Genomen ist</a> .  Dies steht etwas im Widerspruch zu dem, was der Autor zuvor gesagt hat, aber wie man in der realen Welt ohne kleine Widerspr√ºche auskommt.  Gestern drehte sich die Sonne um die Erde, und heute spricht der Autor √ºber das neuronale Netzwerk im Software-Bot.  Kein Wunder, dass er sich an den Ofen der Vernunft erinnerte. <br>  Ich vertraue darauf, dass der Leser selbst das Problem der Widerspr√ºche der Welt l√∂st.  Die Bot-Welt ist f√ºr den Artikel v√∂llig autark. </p><br><p>  Was der Autor in diesem Teil des Artikels jedoch bereits geschafft hat, ist die Bildung einer Population von Bots. <br>  Schauen wir es uns von der Softwareseite aus an: </p><br><p>  Es gibt einen Bot (es kann ein Objekt in OOP sein, eine Struktur, obwohl es wahrscheinlich auch ein Objekt oder nur ein Array von Daten ist).  Im Inneren enth√§lt der Bot Informationen zu Koordinaten, Geschwindigkeit, Masse und anderen Informationen, die f√ºr den Spielprozess n√ºtzlich sind. Die Hauptsache f√ºr uns ist jedoch, dass er je nach Implementierung einen Link zu seinem Genotyp oder Genotyp selbst enth√§lt.  Dann k√∂nnen Sie auf verschiedene Arten vorgehen, sich auf Arrays neuronaler Netzwerkgewichte beschr√§nken oder ein zus√§tzliches Array von Genotypen einf√ºhren, da es f√ºr den Leser bequem ist, sich dies in seiner Vorstellung vorzustellen.  In den ersten Phasen hat der Autor des Programms Arrays von Neurobalances und Genotypen zugewiesen.  Dann weigerte er sich, Informationen zu duplizieren und beschr√§nkte sich auf die Gewichte des neuronalen Netzwerks. </p><br><p>  Nach der Logik der Geschichte m√ºssen Sie sagen, dass die Population der Bots eine Reihe der oben genannten Bots ist.  Was f√ºr eine Spielschleife ... Wieder anhalten, welcher Spielzyklus?  Die Entwickler stellten h√∂flich einen Platz f√ºr nur einen Bot an Bord eines Spielwelt-Simulationsprogramms auf einem Server oder maximal vier Bots in einem lokalen Simulator zur Verf√ºgung.  Und wenn Sie sich an die vom Autor gew√§hlte Topologie des neuronalen Netzwerks erinnern: </p><br><p><img src="https://habrastorage.org/webt/vh/1d/xq/vh1dxqmwqoejzszyh3zpfyykve4.jpeg"></p><br><p>  Nehmen wir zur Vereinfachung der Geschichte an, dass der Genotyp ungef√§hr 1000 neuronale Verbindungen enth√§lt. Im Simulator sehen Genotypen √ºbrigens so aus (Rot ist ein negativer Genwert, Gr√ºn ist ein positiver Wert, jede Linie ist ein separates Genom): </p><br><p><img src="https://habrastorage.org/webt/ri/h0/s-/rih0s-ss3gaflldpts95rm9yo4u.jpeg"></p><br><p>  <em>Hinweis zum Foto: Im Laufe der Zeit √§ndert sich das Muster in Richtung der Dominanz einer der L√∂sungen. Vertikale Streifen sind h√§ufige Genotypgene.</em> </p><br><p>  Wir haben also 1000 Gene im Genotyp und maximal vier Bots im Spielwelt-Simulator-Programm der Organisatoren des Wettbewerbs.  Wie oft m√ºssen Sie eine Simulation eines Bots-Kampfes ausf√ºhren, damit selbst die kl√ºgsten mit brutaler Gewalt auf der Suche nach "effektiv" n√§her kommen? <br>  Genotyp, lesen Sie die "effektive" Kombination neuronaler Verbindungen, vorausgesetzt, dass jede neuronale Verbindung in Schritten von -1 bis 1 variiert, und welcher Schritt?  Die Initialisierung war zuf√§llig float, es sind 15 Dezimalstellen.  Der Schritt ist uns noch nicht klar.  In Bezug auf die Anzahl der Varianten von Kombinationen neuronaler Gewichte geht der Autor davon aus, dass dies eine unendliche Zahl ist, wenn eine bestimmte Schrittgr√∂√üe gew√§hlt wird, wahrscheinlich eine endliche Zahl. In jedem Fall sind diese Zahlen jedoch mehr als 4 Stellen im Simulator, selbst wenn der sequentielle Start aus der Warteschlange der Bots plus gleichzeitiger paralleler Start der offiziellen Simulatoren bis zu 10 ber√ºcksichtigt wird auf einem Computer (f√ºr Fans der Vintage-Programmierung: Computer). </p><br><p><img src="https://habrastorage.org/webt/it/-8/if/it-8ifszotccnx4wguexdtditpm.jpeg"></p><br><p>  Ich hoffe die Bilder helfen dem Leser. </p><br><p>  Hier m√ºssen Sie innehalten und √ºber die Architektur der Softwarel√∂sung sprechen.  Da die L√∂sung in Form eines separaten Software-Bots, der auf die Wettbewerbsseite hochgeladen wurde, nicht mehr geeignet war.  Es war notwendig, das Bot-Spiel nach den Regeln des Wettbewerbs im Rahmen des √ñkosystems der Organisatoren und des Programms zu trennen, um die Konfiguration des neuronalen Netzwerks f√ºr ihn zu finden.  Das folgende Diagramm stammt aus der Pr√§sentation f√ºr die Konferenz, spiegelt jedoch im Allgemeinen das tats√§chliche Bild wider. </p><br><p><img src="https://habrastorage.org/webt/-i/hb/ph/-ihbph2b0hlv3lfzxze7ihmu3-q.jpeg"></p><br><p>  Er erinnerte sich an einen b√§rtigen Witz: </p><br><p>  <em>Gro√üe Organisation.</em> <em><br></em>  <em>Um 18.00 Uhr arbeiten alle Mitarbeiter als einer.</em>  <em>Pl√∂tzlich schaltet einer der Mitarbeiter den Computer aus, zieht sich an und geht.</em> <em><br></em>  <em>Jeder folgt ihm mit einem √ºberraschten Blick.</em> <em><br></em>  <em>Am n√§chsten Tag.</em>  <em>Um 18.00 Uhr schaltet derselbe Mitarbeiter den Computer aus und geht.</em>  <em>Alle arbeiten weiter und beginnen unzufrieden zu fl√ºstern.</em> <em><br></em>  <em>Am n√§chsten Tag.</em>  <em>Um 18.00 Uhr schaltet derselbe Mitarbeiter den Computer aus ...</em> <em><br></em>  <em>Ein Kollege kommt auf ihn zu:</em> <em><br></em>  <em>-Wenn Sie sich nicht sch√§men, arbeiten wir am Ende des Quartals, so viele Berichte, wir wollen auch p√ºnktlich nach Hause und Sie sind so ein Individuum ...</em> <em><br></em>  <em>- Leute, ich mache generell Urlaub!</em> </p><br><p>  ‚Ä¶ Fortsetzung folgt. </p><br><p>  Ja, ich habe fast vergessen, den RNN-Berechnungsprozedurcode anzuh√§ngen. Er ist g√ºltig und unabh√§ngig geschrieben, sodass m√∂glicherweise Fehler darin enthalten sind.  Zur Verst√§rkung werde ich es so bringen, wie es ist, es ist in c ++, wie es auf CUDA (eine Bibliothek zum Berechnen auf der GPU) angewendet wird. </p><br><p>  Hinweis: Mehrdimensionale Arrays kommen auf GPUs nicht gut miteinander aus. Nat√ºrlich gibt es Texturen und Matrixberechnungen, aber sie empfehlen die Verwendung eindimensionaler Arrays. </p><br><p>  Ein Beispielarray [i, j] der Dimension M mal j wird zu einem Array der Form [i * M + j]. </p><br><div class="spoiler">  <b class="spoiler_title">Quellcode des Berechnungsverfahrens RNN</b> <div class="spoiler_text"><pre> <code class="hljs powershell">__global__ void cudaRNN(Bot *bot, argumentsRNN *RNN, ConstantStruct *Const, int *Topology, int *TopologyRNN, int numElements, int gameTick) { int tid = blockIdx.x * blockDim.x + threadIdx.x; int threadN = gridDim.x * blockDim.x; int TopologySize = Const-&gt;TopologySize; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int pos = tid; pos &lt; numElements; pos += threadN) { const int ii = pos; const int iiA = pos*Const-&gt;ArrayDim; int ArrayDim = Const-&gt;ArrayDim; const int iiAT = ii*TopologySize*ArrayDim; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (bot[<span class="hljs-type"><span class="hljs-type">pos</span></span>].TTF != <span class="hljs-number"><span class="hljs-number">0</span></span> &amp;&amp; bot[<span class="hljs-type"><span class="hljs-type">pos</span></span>].Mass&gt;<span class="hljs-number"><span class="hljs-number">0</span></span>) { RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">Topology</span></span>[<span class="hljs-number"><span class="hljs-number">0</span></span>]] = <span class="hljs-number"><span class="hljs-number">1</span></span>.f; //bias int neuroncount7 = Topology[<span class="hljs-number"><span class="hljs-number">0</span></span>]; neuroncount7++; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int layer1 = <span class="hljs-number"><span class="hljs-number">0</span></span>; layer1 &lt; TopologySize - <span class="hljs-number"><span class="hljs-number">1</span></span>; layer1++) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int j4 = <span class="hljs-number"><span class="hljs-number">0</span></span>; j4 &lt; Topology[<span class="hljs-type"><span class="hljs-type">layer1</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>]; j4++) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int i5 = <span class="hljs-number"><span class="hljs-number">0</span></span>; i5 &lt; Topology[<span class="hljs-type"><span class="hljs-type">layer1</span></span>] + <span class="hljs-number"><span class="hljs-number">1</span></span>; i5++) { RNN-&gt;sums[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">j4</span></span>] = RNN-&gt;sums[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">j4</span></span>] + RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">i5</span></span>] * RNN-&gt;NNweights[((<span class="hljs-type"><span class="hljs-type">ii</span></span>*<span class="hljs-type"><span class="hljs-type">TopologySize</span></span> + <span class="hljs-type"><span class="hljs-type">layer1</span></span>)*<span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> + <span class="hljs-type"><span class="hljs-type">i5</span></span>)*<span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> + <span class="hljs-type"><span class="hljs-type">j4</span></span>]; } } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (TopologyRNN[<span class="hljs-type"><span class="hljs-type">layer1</span></span>] &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int j14 = <span class="hljs-number"><span class="hljs-number">0</span></span>; j14 &lt; Topology[<span class="hljs-type"><span class="hljs-type">layer1</span></span>]; j14++) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int i15 = <span class="hljs-number"><span class="hljs-number">0</span></span>; i15 &lt; Topology[<span class="hljs-type"><span class="hljs-type">layer1</span></span>]; i15++) { RNN-&gt;sumsContext[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">j14</span></span>] = RNN-&gt;sumsContext[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">j14</span></span>] + RNN-&gt;neuronContext[<span class="hljs-type"><span class="hljs-type">iiAT</span></span> + <span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> * <span class="hljs-type"><span class="hljs-type">layer1</span></span> + <span class="hljs-type"><span class="hljs-type">i15</span></span>] * RNN-&gt;MNweights[((<span class="hljs-type"><span class="hljs-type">ii</span></span>*<span class="hljs-type"><span class="hljs-type">TopologySize</span></span> + <span class="hljs-type"><span class="hljs-type">layer1</span></span>)*<span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> + <span class="hljs-type"><span class="hljs-type">i15</span></span>)*<span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> + <span class="hljs-type"><span class="hljs-type">j14</span></span>]; } RNN-&gt;sumsContext[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">j14</span></span>] = RNN-&gt;sumsContext[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">j14</span></span>] + <span class="hljs-number"><span class="hljs-number">1.0</span></span>f* RNN-&gt;MNweights[((<span class="hljs-type"><span class="hljs-type">ii</span></span>*<span class="hljs-type"><span class="hljs-type">TopologySize</span></span> + <span class="hljs-type"><span class="hljs-type">layer1</span></span>)*<span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> + <span class="hljs-type"><span class="hljs-type">Topology</span></span>[<span class="hljs-type"><span class="hljs-type">layer1</span></span>])*<span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> + <span class="hljs-type"><span class="hljs-type">j14</span></span>]; //bias=<span class="hljs-number"><span class="hljs-number">1</span></span> } <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int t = <span class="hljs-number"><span class="hljs-number">0</span></span>; t &lt; Topology[<span class="hljs-type"><span class="hljs-type">layer1</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>]; t++) { RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">t</span></span>] = Tanh(RNN-&gt;sums[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">t</span></span>] + RNN-&gt;sumsContext[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">t</span></span>]); RNN-&gt;neuronContext[<span class="hljs-type"><span class="hljs-type">iiAT</span></span> + <span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> * <span class="hljs-type"><span class="hljs-type">layer1</span></span> + <span class="hljs-type"><span class="hljs-type">t</span></span>] = RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">t</span></span>]; } //SoftMax /* double sum = <span class="hljs-number"><span class="hljs-number">0.0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int k = <span class="hljs-number"><span class="hljs-number">0</span></span>; k &lt;ArrayDim; ++k) sum += exp(RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">k</span></span>]); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int k = <span class="hljs-number"><span class="hljs-number">0</span></span>; k &lt; ArrayDim; ++k) RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">k</span></span>] = exp(RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">k</span></span>]) / sum; */ } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int i1 = <span class="hljs-number"><span class="hljs-number">0</span></span>; i1 &lt; Topology[<span class="hljs-type"><span class="hljs-type">layer1</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>]; i1++) { RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">i1</span></span>] = Sigmoid(RNN-&gt;sums[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">i1</span></span>]); //sigma } } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (layer1 + <span class="hljs-number"><span class="hljs-number">1</span></span> != TopologySize - <span class="hljs-number"><span class="hljs-number">1</span></span>) { RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">Topology</span></span>[<span class="hljs-type"><span class="hljs-type">layer1</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>]] = <span class="hljs-number"><span class="hljs-number">1</span></span>.f; } <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int i2 = <span class="hljs-number"><span class="hljs-number">0</span></span>; i2 &lt; ArrayDim; i2++) { RNN-&gt;sums[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">i2</span></span>] = <span class="hljs-number"><span class="hljs-number">0</span></span>.f; RNN-&gt;sumsContext[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">i2</span></span>] = <span class="hljs-number"><span class="hljs-number">0</span></span>.f; } } } } }</code> </pre> </div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de417657/">https://habr.com/ru/post/de417657/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de417647/index.html">Eine Gesetzesvorlage zum Schutz personenbezogener Daten in Wei√ürussland - was ‚Äûdrin‚Äú ist</a></li>
<li><a href="../de417649/index.html">OpenAI √ºberwindet signifikante AI-Einschr√§nkungen f√ºr Dota 2</a></li>
<li><a href="../de417651/index.html">Was sollte der Leser tun, damit Sie mehr lesen k√∂nnen?</a></li>
<li><a href="../de417653/index.html">Offene Lektion "Grundlegende Konzepte von Datenbanken"</a></li>
<li><a href="../de417655/index.html">Hintergrund: Roscosmos State Corporation und ihre Arbeit</a></li>
<li><a href="../de417659/index.html">Neuropoet und andere Popstars der Zukunft</a></li>
<li><a href="../de417661/index.html">Aubrey de Gray besucht Joe Rogan</a></li>
<li><a href="../de417665/index.html">Englische Grammatik als Mathematik. Wo soll ich anfangen f√ºr diejenigen, die nicht trainiert haben?</a></li>
<li><a href="../de417667/index.html">AI. Tactical Barrier Tracker</a></li>
<li><a href="../de417671/index.html">Neue Funktionen der Programmiersprache ABAP in Webinaren von SAP</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>