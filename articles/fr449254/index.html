<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üö¥üèº üö¥üèº üêî FAQ sur l'architecture et le travail VKontakte ü•• üë©‚Äçüë©‚Äçüëß‚Äçüëß üèê</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="L'histoire de VKontakte est sur Wikip√©dia, raconte Pavel lui-m√™me. Il semble que tout le monde la connaisse d√©j√†. Pavel a parl√© de l'int√©rieur, de l'a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>FAQ sur l'architecture et le travail VKontakte</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/449254/">  L'histoire de VKontakte est sur Wikip√©dia, raconte Pavel lui-m√™me.  Il semble que tout le monde la connaisse d√©j√†.  Pavel a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">parl√©</a> de l'int√©rieur, de l'architecture et de la conception du site sur HighLoad ++ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">en 2010</a> .  Beaucoup de serveurs ont fui depuis lors, nous allons donc mettre √† jour les informations: nous diss√©quons, retirons l'int√©rieur, pesons - nous consid√©rons le dispositif VK d'un point de vue technique. <br><br><img src="https://habrastorage.org/webt/_x/zc/wp/_xzcwpb5ze_4e-yx_jw_-8nvnei.jpeg"><br><br>  <strong>Alexey Akulovich</strong> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">AterCattus</a> ) est un d√©veloppeur backend de l'√©quipe VKontakte.  La transcription de ce rapport est une r√©ponse collective aux questions fr√©quemment pos√©es sur le fonctionnement de la plateforme, l'infrastructure, les serveurs et l'interaction entre eux, mais pas sur le d√©veloppement, notamment <strong>sur le mat√©riel</strong> .  S√©par√©ment - sur les bases de donn√©es et ce que VK a √† leur place, sur la collecte des journaux et la surveillance de l'ensemble du projet dans son ensemble.  D√©tails sous la coupe. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/_GqcriadL-s" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><a name="habracut"></a><br>  Depuis plus de quatre ans, je fais toutes sortes de t√¢ches li√©es au backend. <br><br><ul><li>  T√©l√©chargement, stockage, traitement, distribution de m√©dias: vid√©o, streaming en direct, audio, photos, documents. </li><li>  Infrastructure, plate-forme, surveillance des d√©veloppeurs, journaux, caches r√©gionaux, CDN, protocole propri√©taire RPC. </li><li>  Int√©gration avec des services externes: push mailing, analyse de liens externes, flux RSS. </li><li>  Aidez vos coll√®gues sur diverses questions, pour les r√©ponses auxquelles vous devez plonger dans un code inconnu. </li></ul><br>  Pendant ce temps, j'ai particip√© √† de nombreuses composantes du site.  Je veux partager cette exp√©rience. <br><br><h2>  Architecture g√©n√©rale </h2><br>  Tout, comme d'habitude, commence par un serveur ou un groupe de serveurs qui acceptent les requ√™tes. <br><br><h3>  Serveur frontal </h3><br>  Le serveur frontal accepte les demandes via HTTPS, RTMP et WSS. <br><br>  <strong>Les HTTPS</strong> sont des demandes pour les versions Web principale et mobile du site: vk.com et m.vk.com, et d'autres clients officiels et non officiels de notre API: clients mobiles, messageries instantan√©es.  Nous avons du trafic <strong>RTMP</strong> pour les diffusions en direct avec des serveurs frontaux s√©par√©s et des connexions <strong>WSS</strong> pour l'API Streaming. <br><br>  Pour HTTPS et WSS, <strong>nginx est</strong> install√© sur les serveurs.  Pour les √©missions RTMP, nous avons r√©cemment opt√© pour notre propre solution <strong>Kive</strong> , mais cela d√©passe le cadre du rapport.  Pour la tol√©rance aux pannes, ces serveurs annoncent des adresses IP communes et agissent comme des groupes afin qu'en cas de probl√®me sur l'un des serveurs, les requ√™tes des utilisateurs ne soient pas perdues.  Pour HTTPS et WSS, ces m√™mes serveurs chiffrent le trafic pour prendre une partie de la charge CPU sur eux-m√™mes. <br><br>  De plus, nous ne parlerons pas de WSS et RTMP, mais seulement des requ√™tes HTTPS standard, qui sont g√©n√©ralement associ√©es √† un projet Web. <br><br><h3>  Backend </h3><br>  Derri√®re le front se trouvent g√©n√©ralement les serveurs principaux.  Ils g√®rent les demandes que le serveur frontal re√ßoit des clients. <br><br>  Ce sont des <strong>serveurs kPHP</strong> ex√©cutant le d√©mon HTTP car HTTPS est d√©j√† d√©crypt√©.  kPHP est un serveur qui fonctionne selon le <strong>mod√®le prefork</strong> : il d√©marre le processus ma√Ætre, un tas de processus enfants, leur transmet des sockets d'√©coute et ils traitent leurs requ√™tes.  Dans le m√™me temps, les processus ne sont pas red√©marr√©s entre chaque demande de l'utilisateur, mais r√©initialisent simplement leur √©tat √† l'√©tat initial √† valeur z√©ro - demande par demande, au lieu de red√©marrer. <br><br><h4>  Partage de charge </h4><br>  Tous nos backends ne sont pas un √©norme pool de machines capables de traiter n'importe quelle demande.  Nous les <strong>divisons en groupes distincts</strong> : g√©n√©ral, mobile, api, vid√©o, mise en sc√®ne ... Le probl√®me sur un groupe de machines distinct n'affectera pas tout le monde.  En cas de probl√®mes avec la vid√©o, l'utilisateur qui √©coute de la musique ne conna√Æt m√™me pas les probl√®mes.  Le backend auquel envoyer la requ√™te est r√©solu par nginx sur le devant de la configuration. <br><br><h4>  Collecte et r√©√©quilibrage des m√©triques </h4><br>  Pour comprendre le nombre de voitures dont vous avez besoin dans chaque groupe, nous <strong>ne comptons pas sur QPS</strong> .  Les backends sont diff√©rents, ils ont des demandes diff√©rentes, chaque demande a une complexit√© de calcul QPS diff√©rente.  Par cons√©quent, nous utilisons le <strong>concept de charge sur le serveur dans son ensemble - sur le CPU et la perf</strong> . <br><br>  Nous avons des milliers de tels serveurs.  Le groupe kPHP s'ex√©cute sur chaque serveur physique pour utiliser tous les noyaux (car kPHP est monothread). <br><br><h3>  Serveur de contenu </h3><br>  <strong>CS ou Content Server est un stockage</strong> .  CS est un serveur qui stocke les fichiers, et traite √©galement les fichiers t√©l√©charg√©s, toutes sortes de t√¢ches d'arri√®re-plan synchrones que le frontend Web principal lui pose. <br><br>  Nous avons des dizaines de milliers de serveurs physiques qui stockent des fichiers.  Les utilisateurs aiment t√©l√©charger des fichiers, et nous aimons les stocker et les partager.  Certains de ces serveurs sont ferm√©s par des serveurs sp√©ciaux pu / pp. <br><br><h3>  pu / pp </h3><br>  Si vous avez ouvert l'onglet r√©seau dans VK, vous avez vu pu / pp. <br><br><img src="https://habrastorage.org/webt/fd/am/xo/fdamxolkfxlplnc5h5flbihru3g.png"><br><br>  Qu'est-ce que pu / pp?  Si nous fermons un serveur apr√®s l'autre, il y a deux options pour t√©l√©charger et t√©l√©charger un fichier sur un serveur qui a √©t√© ferm√©: <strong>directement</strong> via <code>http://cs100500.userapi.com/path</code> ou <strong>via un serveur interm√©diaire</strong> - <code>http://pu.vk.com/c100500/path</code> . <br><br>  <strong>Pu est le nom historique du t√©l√©chargement de photos et pp est un proxy de photos</strong> .  Autrement dit, un serveur pour t√©l√©charger des photos, et un autre - pour donner.  D√©sormais, non seulement les photos sont charg√©es, mais le nom a √©t√© conserv√©. <br><br>  Ces serveurs <strong>mettent fin</strong> aux <strong>sessions HTTPS</strong> pour supprimer la charge du processeur du stockage.  De plus, comme les fichiers utilisateur sont trait√©s sur ces serveurs, moins les informations sensibles sont stock√©es sur ces machines, mieux c'est.  Par exemple, les cl√©s de chiffrement HTTPS. <br><br>  √âtant donn√© que les machines sont ferm√©es par nos autres machines, nous pouvons nous permettre de ne pas leur donner des adresses IP externes ¬´blanches¬ª et de <strong>donner</strong> des adresses IP <strong>¬´grises¬ª</strong> .  Nous avons donc √©conomis√© sur le pool IP et garanti de prot√©ger les machines contre l'acc√®s de l'ext√©rieur - il n'y a tout simplement pas d'adresse IP pour y acc√©der. <br><br>  <strong>Tol√©rance aux pannes via IP partag√©e</strong> .  En termes de tol√©rance aux pannes, le sch√©ma fonctionne de la m√™me mani√®re - plusieurs serveurs physiques ont une adresse IP physique commune, et le morceau de fer devant eux choisit o√π envoyer la demande.  Plus tard, je parlerai d'autres options. <br><br>  Le point controvers√© est que dans ce cas, le <strong>client d√©tient moins de connexions</strong> .  S'il y a la m√™me IP sur plusieurs machines - avec le m√™me h√¥te: pu.vk.com ou pp.vk.com, le navigateur client a une limite sur le nombre de demandes simultan√©es √† un h√¥te.  Mais pendant l'omnipr√©sent HTTP / 2, je pense que ce n'est plus le cas. <br><br>  L'inconv√©nient √©vident du sch√©ma est que vous devez <strong>pomper tout le trafic</strong> qui va vers le stockage via un autre serveur.  √âtant donn√© que nous pompons le trafic dans les voitures, nous ne pouvons pas encore pomper le trafic lourd de la m√™me mani√®re, par exemple, la vid√©o.  Nous le transf√©rons directement - une connexion directe distincte pour les r√©f√©rentiels individuels sp√©cifiquement pour la vid√©o.  Nous transmettons un contenu plus l√©ger via un proxy. <br><br>  Il n'y a pas si longtemps, nous avons une version am√©lior√©e du proxy.  Je vais maintenant vous expliquer en quoi ils diff√®rent des mod√®les ordinaires et pourquoi cela est n√©cessaire. <br><br><h3>  Soleil </h3><br>  En septembre 2017, Oracle, qui avait pr√©c√©demment achet√© Sun, a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">licenci√© un grand nombre d'employ√©s de Sun.</a>  On peut dire qu'√† ce moment la soci√©t√© a cess√© d'exister.  En choisissant un nom pour le nouveau syst√®me, nos administrateurs ont d√©cid√© de rendre hommage et de respect √† cette soci√©t√© et ont nomm√© le nouveau syst√®me Sun.  Entre nous, nous l'appelons simplement ¬´soleil¬ª. <br><br><img src="https://habrastorage.org/webt/d3/6f/0j/d36f0jjqbwlst9mk2-lcncltkq4.png"><br><br>  Pp a eu quelques probl√®mes.  <strong>Une adresse IP par groupe est un cache inefficace</strong> .  Plusieurs serveurs physiques ont une adresse IP commune, et il n'y a aucun moyen de contr√¥ler le serveur auquel la demande sera adress√©e.  Par cons√©quent, si diff√©rents utilisateurs viennent pour le m√™me fichier, alors s'il existe un cache sur ces serveurs, le fichier s'installe dans le cache de chaque serveur.  Il s'agit d'un sch√©ma tr√®s inefficace, mais rien ne pourrait √™tre fait. <br><br>  Par cons√©quent, <strong>nous ne pouvons pas partager le contenu</strong> , car nous ne pouvons pas s√©lectionner un serveur sp√©cifique pour ce groupe - ils ont une IP commune.  De plus, pour certaines raisons internes, nous <strong>n'avons pas eu la possibilit√© de mettre de tels serveurs dans les r√©gions</strong> .  Ils ne se tenaient qu'√† Saint-P√©tersbourg. <br><br>  Avec les soleils, nous avons chang√© le syst√®me de s√©lection.  Nous avons maintenant le <strong>routage anycast</strong> : <strong>routage</strong> dynamique, anycast, d√©mon d'auto-v√©rification.  Chaque serveur a sa propre IP individuelle, mais en m√™me temps un sous-r√©seau commun.  Tout est configur√© de mani√®re √† ce qu'en cas de perte d'un serveur, le trafic soit automatiquement r√©parti sur les autres serveurs du m√™me groupe.  Il est maintenant possible de s√©lectionner un serveur sp√©cifique, <strong>il n'y a pas de mise en cache excessive</strong> et la fiabilit√© n'est pas affect√©e. <br><br>  <strong>Support de poids</strong> .  Maintenant, nous pouvons nous permettre de mettre des voitures de capacit√©s diff√©rentes selon les besoins, et √©galement en cas de probl√®mes temporaires, de changer les poids des ¬´soleils¬ª de travail pour r√©duire la charge sur eux afin qu'ils se ¬´reposent¬ª et fonctionnent √† nouveau. <br><br>  <strong>Partage par identifiant de contenu</strong> .  Ce qui est amusant avec le sharding, c'est que nous partitionnons g√©n√©ralement le contenu afin que diff√©rents utilisateurs suivent le m√™me fichier √† travers le m√™me ¬´soleil¬ª afin qu'ils aient un cache commun. <br><br>  Nous avons r√©cemment lanc√© l'application Clover.  Il s'agit d'un quiz de diffusion en direct en ligne o√π le pr√©sentateur pose des questions et les utilisateurs r√©pondent en temps r√©el en choisissant des options.  L'application dispose d'un chat o√π les utilisateurs peuvent inonder.  <strong>Plus de 100 000 personnes</strong> peuvent se connecter simultan√©ment √† la diffusion.  Ils √©crivent tous des messages qui sont envoy√©s √† tous les participants, avec le message vient un autre avatar.  Si 100 000 personnes viennent pour un avatar dans un ¬´soleil¬ª, il peut parfois rouler sur un nuage. <br><br>  Pour r√©sister √† des rafales de demandes provenant du m√™me fichier, c'est pour une sorte de contenu que nous incluons un sch√©ma stupide qui r√©partit les fichiers sur tous les "soleils" disponibles dans la r√©gion. <br><br><h4>  Soleil √† l'int√©rieur </h4><br>  Proxy inverse vers nginx, cache en RAM ou disques rapides Optane / NVMe.  Exemple: <code>http://sun4-2.userapi.com/c100500/path</code> - lien vers le "soleil", qui se trouve dans la quatri√®me r√©gion, le deuxi√®me groupe de serveurs.  Il ferme le fichier de chemin, qui se trouve physiquement sur le serveur 100500. <br><br><h3>  Cache </h3><br>  Nous ajoutons un n≈ìud suppl√©mentaire √† notre sch√©ma architectural - l'environnement de mise en cache. <br><br><img src="https://habrastorage.org/webt/jp/4p/xv/jp4pxvydhstms-z9bpbmpjy0htk.png"><br><br>  Vous trouverez ci-dessous la disposition des <strong>caches r√©gionaux</strong> , il y en a environ 20.  Ce sont les endroits o√π se trouvent exactement les caches et les "soleils", qui peuvent mettre en cache le trafic √† travers eux-m√™mes. <br><br><img src="https://habrastorage.org/webt/yh/9i/qk/yh9iqkv3dyqd3uox9wgd2cmgqba.png"><br><br>  Il s'agit de la mise en cache du contenu multim√©dia, les donn√©es utilisateur ne sont pas stock√©es ici - juste de la musique, des vid√©os, des photos. <br><br>  Pour d√©terminer la r√©gion de l'utilisateur, nous <strong>collectons les pr√©fixes de r√©seau BGP annonc√©s dans les r√©gions</strong> .  Dans le cas de repli, nous avons toujours l'analyse de la base geoip, si nous ne pouvions pas trouver IP par pr√©fixes.  <strong>Par IP utilisateur, nous d√©terminons la r√©gion</strong> .  Dans le code, nous pouvons regarder une ou plusieurs r√©gions de l'utilisateur - les points dont il est g√©ographiquement le plus proche. <br><br><h4>  Comment √ßa marche? </h4><br>  <strong>Nous consid√©rons la popularit√© des fichiers par r√©gion</strong> .  Il y a un num√©ro de cache r√©gional o√π se trouve l'utilisateur et un identifiant de fichier - nous prenons cette paire et incr√©mentons la note pour chaque t√©l√©chargement. <br><br>  Dans le m√™me temps, les d√©mons - services dans les r√©gions - viennent de temps en temps √† l'API et disent: "J'ai tel ou tel cache, donnez-moi une liste des fichiers les plus populaires de ma r√©gion que je n'ai pas encore."  L'API donne un tas de fichiers tri√©s par note, le d√©mon les pompe, les transporte dans les r√©gions et leur donne des fichiers √† partir de l√†.  C'est une diff√©rence fondamentale entre pu / pp et Sun des caches: ils transmettent le fichier par eux-m√™mes imm√©diatement, m√™me si le fichier n'existe pas dans le cache, et le cache pompe d'abord le fichier pour lui-m√™me, puis il commence √† le r√©v√©ler. <br><br>  Dans le m√™me temps, nous <strong>rapprochons le contenu des utilisateurs</strong> et r√©duisons la charge du r√©seau.  Par exemple, uniquement √† partir du cache de Moscou, nous distribuons plus de 1 Tbit / s pendant les heures de pointe. <br><br>  Mais il y a des probl√®mes - <strong>les serveurs de cache ne sont pas en caoutchouc</strong> .  Pour le contenu super populaire, il n'y a parfois pas assez de r√©seau sur un serveur s√©par√©.  Nous avons des serveurs de cache de 40 √† 50 Gbit / s, mais il y a du contenu qui obstrue compl√®tement un tel canal.  Nous nous effor√ßons de r√©aliser le stockage de plus d'une copie de fichiers populaires dans la r√©gion.  J'esp√®re que nous le r√©aliserons d'ici la fin de l'ann√©e. <br><br>  Nous avons examin√© l'architecture g√©n√©rale. <br><br><ul><li>  Serveurs frontaux qui acceptent les demandes. </li><li>  Backends qui traitent les demandes. </li><li>  Des coffres ferm√©s par deux types de procurations. </li><li>  Caches r√©gionaux. </li></ul><br>  Qu'est-ce qui manque √† ce sch√©ma?  Bien s√ªr, les bases de donn√©es dans lesquelles nous stockons les donn√©es. <br><br><h2>  Bases de donn√©es ou moteurs </h2><br>  Nous ne les appelons pas des bases de donn√©es, mais des moteurs moteurs, car dans le sens g√©n√©ralement admis, nous n'avons pratiquement pas de bases de donn√©es. <br><br><img src="https://habrastorage.org/webt/n6/zm/lj/n6zmlj5pwxsnqoqp0xgfhxza_ic.png"><br><br>  <strong>C'est une mesure n√©cessaire</strong> .  Cela est arriv√© parce qu'en 2008-2009, lorsque VK a connu une croissance explosive de la popularit√©, le projet a pleinement fonctionn√© sur MySQL et Memcache, et il y avait des probl√®mes.  MySQL aimait tomber et ruiner des fichiers, apr√®s quoi il ne montait pas, et Memcache d√©gradait progressivement ses performances et devait √™tre red√©marr√©. <br><br>  Il s'av√®re que dans le projet qui gagnait en popularit√©, il y avait un stockage persistant qui corrompait les donn√©es et un cache qui ralentissait.  Dans ces conditions, il est difficile de d√©velopper un projet en pleine croissance.  Il a √©t√© d√©cid√© d'essayer de r√©√©crire les √©l√©ments essentiels sur lesquels le projet reposait sur leurs propres v√©los. <br><br>  <strong>La solution a r√©ussi</strong> .  La possibilit√© de le faire √©tait, tout comme un besoin urgent, car il n'existait pas √† l'√©poque d'autres m√©thodes de mise √† l'√©chelle.  Il n'y avait pas de tas de bases, NoSQL n'existait pas encore, il n'y avait que MySQL, Memcache, PostrgreSQL - et c'est tout. <br><br>  <strong>Fonctionnement universel</strong> .  Le d√©veloppement a √©t√© men√© par notre √©quipe de d√©veloppeurs C, et tout a √©t√© fait de la m√™me mani√®re.  Quel que soit le moteur, partout il y avait √† peu pr√®s le m√™me format des fichiers √©crits sur le disque, les m√™mes param√®tres de d√©marrage, les signaux √©taient trait√©s de la m√™me mani√®re et se comportaient de la m√™me mani√®re en cas de situations de bord et de probl√®mes.  Avec la croissance des moteurs, il est pratique pour les administrateurs de faire fonctionner le syst√®me - il n'y a pas de zoo √† entretenir et d'apprendre √† exploiter chaque nouvelle base tierce, ce qui a permis d'augmenter rapidement et facilement leur nombre. <br><br><h3>  Types de moteurs </h3><br>  L'√©quipe a √©crit pas mal de moteurs.  Voici quelques-uns d'entre eux: ami, conseils, image, ipdb, lettres, listes, journaux, memcached, meowdb, actualit√©s, nostradamus, photo, listes de lecture, pmemcached, sandbox, recherche, stockage, likes, t√¢ches, ... <br><br>  Pour chaque t√¢che n√©cessitant une structure de donn√©es sp√©cifique ou traitant des requ√™tes atypiques, l'√©quipe C √©crit un nouveau moteur.  Pourquoi pas. <br><br>  Nous avons un moteur <strong>memcached</strong> s√©par√©, qui est similaire √† celui habituel, mais avec un tas de petits pains, et qui ne ralentit pas.  Pas ClickHouse, mais fonctionne aussi.  Il y a <strong>pmemcached</strong> s√©par√©ment - c'est un <strong>memcached persistant</strong> qui peut stocker des donn√©es √©galement sur le disque, et plus que ce qu'il entre dans la RAM afin de ne pas perdre de donn√©es lors du red√©marrage.  Il existe diff√©rents moteurs pour des t√¢ches individuelles: files d'attente, listes, ensembles - tout ce qui est requis par notre projet. <br><br><h3>  Clusters </h3><br>  Du point de vue du code, il n'est pas n√©cessaire d'imaginer des moteurs ou des bases de donn√©es comme certains processus, entit√©s ou instances.  Le code fonctionne sp√©cifiquement avec les clusters, avec des groupes de moteurs - <strong>un type par cluster</strong> .  Disons qu'il y a un cluster memcached - c'est juste un groupe de machines. <br><br><blockquote>  Le code n'a pas besoin de conna√Ætre l'emplacement physique, la taille et le nombre de serveurs.  Il va au cluster par un identifiant. </blockquote><br>  Pour que cela fonctionne, vous devez ajouter une autre entit√©, situ√©e entre le code et les moteurs - <strong>proxy</strong> . <br><br><h3>  Proxy RPC </h3><br>  Proxy - un <strong>bus de connexion</strong> , qui ex√©cute presque tout le site.  Dans le m√™me temps, nous <strong>n'avons pas de d√©couverte de service</strong> - au lieu de cela, il y a une configuration de ce proxy, qui conna√Æt l'emplacement de tous les clusters et de tous les fragments de ce cluster.  Cela est fait par les administrateurs. <br><br>  Les programmeurs ne se soucient g√©n√©ralement pas de combien, o√π et ce que cela co√ªte - ils vont simplement au cluster.  Cela nous permet beaucoup.  D√®s r√©ception de la demande, le proxy redirige la demande, sachant o√π - il le d√©termine. <br><br><img src="https://habrastorage.org/webt/7k/pf/ia/7kpfiagxzy2a4mrosc_f4otqnw8.png"><br><br>  Dans le m√™me temps, le proxy est un point de protection contre les pannes de service.  Si un moteur ralentit ou tombe en panne, le proxy le comprend et r√©pond en cons√©quence du c√¥t√© client.  Cela vous permet de supprimer le d√©lai d'expiration - le code n'attend pas que le moteur r√©ponde, mais comprend qu'il ne fonctionne pas et que vous devez vous comporter diff√©remment.  Le code doit √™tre pr√©par√© pour le fait que les bases de donn√©es ne fonctionnent pas toujours. <br><br><h4>  Impl√©mentations sp√©cifiques </h4><br>  Parfois, nous voulons toujours vraiment avoir une sorte de solution personnalis√©e comme moteur.  En m√™me temps, il a √©t√© d√©cid√© de ne pas utiliser notre proxy rpc pr√™t √† l'emploi, cr√©√© sp√©cifiquement pour nos moteurs, mais de cr√©er un proxy s√©par√© pour la t√¢che. <br><br>  Pour MySQL, que nous avons encore √† certains endroits, nous utilisons db-proxy et pour ClickHouse - <strong>Kittenhouse</strong> . <br><br>  Cela fonctionne globalement comme √ßa.  Il y a un serveur, kPHP, Go, Python fonctionnent dessus - en g√©n√©ral, tout code pouvant suivre notre protocole RPC.  Le code va localement √† RPC-proxy - sur chaque serveur o√π il y a du code, son propre proxy local est lanc√©.  Sur demande, le mandataire sait o√π aller. <br><br><img src="https://habrastorage.org/webt/f-/dx/ro/f-dxrox3o97ckejzygz8mgf4tcs.png"><br><br>  Si un moteur veut passer √† un autre, m√™me s'il s'agit d'un voisin, il passe par un proxy, car le voisin peut se trouver dans un centre de donn√©es diff√©rent.  Le moteur ne doit pas √™tre li√© √† la connaissance de l'emplacement d'autre chose que lui-m√™me - nous avons cette solution standard.  Mais bien s√ªr, il y a des exceptions :) <br><br>  Un exemple de sch√©ma TL selon lequel tous les moteurs fonctionnent. <br><br><pre> <code class="plaintext hljs">memcache.not_found = memcache.Value; memcache.strvalue value:string flags:int = memcache.Value; memcache.addOrIncr key:string flags:int delay:int value:long = memcache.Value; tasks.task fields_mask:# flags:int tag:%(Vector int) data:string id:fields_mask.0?long retries:fields_mask.1?int scheduled_time:fields_mask.2?int deadline:fields_mask.3?int = tasks.Task; tasks.addTask type_name:string queue_id:%(Vector int) task:%tasks.Task = Long;</code> </pre> <br>  Il s'agit d'un protocole binaire, dont l'analogue le plus proche est <strong>protobuf.</strong>  Le sch√©ma d√©crit √† l'avance les champs facultatifs, les types complexes - les extensions de scalaires int√©gr√©s et les requ√™tes.  Tout fonctionne selon ce protocole. <br><br><h4>  RPC sur TL sur TCP / UDP ... UDP? </h4><br>  Nous avons un protocole RPC pour interroger le moteur, qui s'ex√©cute au-dessus du sch√©ma TL.  Tout cela fonctionne en plus de la connexion TCP / UDP.  TCP - il est clair pourquoi nous sommes souvent interrog√©s sur UDP. <br><br>  UDP permet d' <strong>√©viter le probl√®me d'un grand nombre de connexions entre les serveurs</strong> .  S'il y a un proxy RPC sur chaque serveur et en g√©n√©ral, il peut aller √† n'importe quel moteur, vous obtenez des dizaines de milliers de connexions TCP au serveur.  Il y a une charge, mais elle est inutile.  Dans le cas d'UDP, ce n'est pas un probl√®me. <br><br>  <strong>Aucune poign√©e de main TCP redondante</strong> .  Il s'agit d'un probl√®me typique: lorsqu'un nouveau moteur ou un nouveau serveur arrive, de nombreuses connexions TCP sont √©tablies en m√™me temps.  Pour les petites demandes l√©g√®res, par exemple, la charge utile UDP, toutes les communications entre le code et le moteur sont <strong>deux paquets UDP: l'</strong> un vole dans un sens, l'autre vole dans l'autre.  Un aller-retour - et le code a re√ßu une r√©ponse du moteur sans poign√©e de main. <br><br>  Oui, tout cela ne fonctionne <strong>qu'avec un tr√®s faible pourcentage de perte de paquets</strong> .  Le protocole prend en charge les retransmissions, les d√©lais d'attente, mais si nous perdons beaucoup, nous obtenons pratiquement TCP, ce qui n'est pas rentable.  √Ä travers les oc√©ans, ne conduisez pas UDP. <br><br>  Nous avons des milliers de ces serveurs, et le m√™me sch√©ma existe: un pack de moteurs est plac√© sur chaque serveur physique.  Fondamentalement, ils sont √† filetage unique pour fonctionner aussi rapidement que possible sans blocage, et sont d√©chiquet√©s en tant que solutions √† filetage unique.  Dans le m√™me temps, nous n'avons rien de plus fiable que ces moteurs, et une grande attention est accord√©e au stockage persistant des donn√©es. <br><br><h3>  Stockage de donn√©es persistant </h3><br>  <strong>Les moteurs √©crivent des binlogs</strong> .  Un binlog est un fichier √† la fin duquel un √©v√©nement est ajout√© pour changer un √©tat ou des donn√©es.  Dans diff√©rentes solutions, il est appel√© diff√©remment: log binaire, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">WAL</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">AOF</a> , mais le principe est un. <br><br>  Afin que le moteur ne relise pas l'int√©gralit√© du binlog lors d'un red√©marrage pendant plusieurs ann√©es, les moteurs √©crivent des <strong>instantan√©s - l'√©tat actuel</strong> .  Si n√©cessaire, ils lisent d'abord celui-ci, puis lisent √† partir du binlog.  Tous les binlogs sont √©crits dans le m√™me format binaire - selon le sch√©ma TL, afin que les administrateurs puissent les administrer √©galement avec leurs outils.  Il n'y a pas un tel besoin de clich√©s.  Il y a un titre g√©n√©ral qui indique dont l'instantan√© est l'int, la magie du moteur, et quel corps n'est important pour personne.  C'est le probl√®me du moteur qui a enregistr√© l'instantan√©. <br><br>  Je d√©crirai bri√®vement le principe du travail.  Il existe un serveur sur lequel le moteur fonctionne.  Il ouvre un nouveau binlog vide pour l'enregistrement, y √©crit un √©v√©nement de modification. <br><br><img src="https://habrastorage.org/webt/dd/w9/9p/ddw99p7g6upg9hci9ou6aln6d_c.png"><br><br>  √Ä un moment donn√©, il d√©cide soit de prendre un instantan√©, soit il re√ßoit un signal.  Le serveur cr√©e un nouveau fichier, y √©crit compl√®tement son √©tat, ajoute la taille actuelle du binlog - d√©cal√© √† la fin du fichier et continue d'√©crire davantage.  Un nouveau binlog n'est pas cr√©√©. <br><br><img src="https://habrastorage.org/webt/ec/fq/yt/ecfqytibh2tsm5ncd8mfli-b1ta.png"><br><br>  √Ä un certain moment, lorsque le moteur red√©marre, il y aura un binlog et un instantan√© sur le disque.  Le moteur lit en instantan√© complet, √©l√®ve son √©tat √† un certain point. <br><br><img src="https://habrastorage.org/webt/bg/ph/-u/bgph-uu68nqedhby4a2kf3r9c5u.png"><br><br>  Soustrait la position qui √©tait au moment de la cr√©ation de l'instantan√© et la taille du binlog. <br><br><img src="https://habrastorage.org/webt/ar/gs/lq/argslqv8ewosmtic8-zaobq4g5o.png"><br><br>  Lit la fin du binlog pour obtenir l'√©tat actuel et continue d'√©crire d'autres √©v√©nements.  C'est un sch√©ma simple, tous nos moteurs y travaillent. <br><br><h4>  R√©plication de donn√©es </h4><br>  Par cons√©quent, la r√©plication des donn√©es est <strong>bas√©e sur des instructions</strong> - nous n'√©crivons aucune modification de page dans le binlog, mais plut√¥t des <strong>demandes de modifications</strong> .  Tr√®s similaire √† ce qui vient sur le r√©seau, seulement un peu chang√©. <br><br>  Le m√™me sch√©ma est utilis√© non seulement pour la r√©plication, mais √©galement <strong>pour la cr√©ation de sauvegardes</strong> .  Nous avons un moteur - un ma√Ætre d'√©criture qui √©crit dans un binlog.  Dans tout autre endroit o√π les administrateurs s'installent, la copie de ce binlog augmente, et c'est tout - nous avons une sauvegarde. <br><br><img src="https://habrastorage.org/webt/og/al/sz/ogalszm0wfe3f_064sbjnpo4p9c.png"><br><br>  Si vous avez besoin d'une <strong>r√©plique de lecture</strong> afin de r√©duire la charge de lecture sur le CPU, le moteur de lecture monte simplement, qui lit la fin du binlog et ex√©cute ces commandes localement. <br><br>  Le d√©calage ici est tr√®s faible et il est possible de d√©couvrir √† quel point la r√©plique se trouve derri√®re le ma√Ætre. <br><br><h3>  Partage de donn√©es dans le proxy RPC </h3><br>  Comment fonctionne le sharding?  Comment le proxy comprend-il le fragment de cluster auquel envoyer?  Le code ne dit pas: "Envoyer √† 15 fragments!"  - non, il fait un proxy. <br><br>  <strong>Le sch√©ma le plus simple est firstint</strong> , le premier num√©ro de la demande. <br><br> <code>get(photo100_500) =&gt; 100 % N.</code> <br> <br>  Ceci est un exemple pour un protocole de texte memcached simple, mais, bien s√ªr, les demandes sont complexes, structur√©es.  L'exemple prend le premier nombre de la requ√™te et le reste de la division par la taille du cluster. <br><br>  Ceci est utile lorsque nous voulons avoir la localit√© des donn√©es d'une entit√©.  Disons que 100 est un ID d'utilisateur ou de groupe, et nous voulons que toutes les donn√©es d'une entit√© soient sur le m√™me fragment pour les requ√™tes complexes. <br><br>  Si nous ne nous soucions pas de la fa√ßon dont les demandes sont r√©parties dans le cluster, il existe une autre option: <strong>hacher l'int√©gralit√© du fragment</strong> . <br><br> <code>hash(photo100_500) =&gt; 3539886280 % N</code> <br> <br>  Nous obtenons √©galement le hachage, le reste de la division et le num√©ro du fragment. <br><br>  Ces deux options ne fonctionnent que si nous sommes pr√©par√©s au fait que lorsque nous augmentons la taille du cluster, nous le divisons ou l'augmentons plusieurs fois.  Par exemple, nous avions 16 fragments, nous manquons, nous en voulons plus - vous pouvez en obtenir 32 en toute s√©curit√© sans interruption.  Si nous voulons construire plusieurs fois, il y aura un temps d'arr√™t, car il ne sera pas possible de tout √©craser soigneusement sans perte.  Ces options sont utiles, mais pas toujours. <br><br>  Si nous devons ajouter ou supprimer un nombre arbitraire de serveurs, <strong>un hachage coh√©rent sur l'anneau √† la Ketama est utilis√©</strong> .  Mais en m√™me temps, nous perdons compl√®tement la localit√© des donn√©es, nous devons faire une demande de fusion au cluster afin que chaque pi√®ce renvoie sa petite r√©ponse, et d√©j√† combiner les r√©ponses au proxy. <br><br>  - .   : RPC-proxy  , ,       .     , ,     ,      .    proxy. <br><br><img src="https://habrastorage.org/webt/jx/6t/f9/jx6tf9jlkkmva1qfifzmwrx58wc.png"><br><br><h2>  </h2><br>     .     ‚Äî <strong>   memcache</strong> . <br><br> <code>ring-buffer: prefix.idx = line</code> <br> <br>    ‚Äî  , ,      ‚Äî  .     0     1.   memcache ‚Äî       .        . <br><br>    ,   <strong>Multi Get</strong>  ,   ,         .  ,   -      ,   ,         ,      . <br><br>         <strong>logs-engine</strong> .      ,       .       600   . <br><br>   ,  ,    6‚Äì7 .    ,    , ,    ClickHouse   . <br><br><h3>    ClickHouse </h3><br>   ,      . <br><br><img src="https://habrastorage.org/webt/jm/-j/s0/jm-js04tjh8lb8pii1_dzl_sfa4.png"><br><br>  ,   RPC    RPC-proxy,   ,    .       ClickHouse,        : <br><br><ul><li>  -   ClickHouse; </li><li>  RPC-proxy,      ClickHouse,  - ,  ,   RPC. </li></ul><br>    ‚Äî          ClickHouse. <br><br>     ClickHouse,   <strong>KittenHouse</strong> .      KittenHouse  ClickHouse ‚Äî   .   ,  HTTP-     .   ,    ClickHouse <strong>  reverse proxy</strong> ,   ,     .         . <br><br><img src="https://habrastorage.org/webt/zj/fy/5y/zjfy5yuay9-6wqe3nrgjkeznvny.png"><br><br>      RPC-   , ,  nginx.   KittenHouse      UDP. <br><br><img src="https://habrastorage.org/webt/hq/wl/v_/hqwlv_vnujb-maxakxksbmrf6xo.png"><br><br>         ,    UDP-      .       RPC     ,      UDP.      . <br><br><h2>  Suivi </h2><br>     : ,        ,     .     : <strong>  </strong> . <br><br><h3>   </h3><br>       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Netdata</a> ,        <strong>Graphite Carbon</strong> .      ClickHouse,   Whisper, .       ClickHouse,   <strong>Grafana</strong>  ,   .  ,   Netdata  Grafana  . <br><br><h3>   </h3><br>      . ,    ,    Counts, UniqueCounts   ,   - . <br><br><pre> <code class="plaintext hljs">statlogsCountEvent ( 'stat_name', $key1, $key2, ‚Ä¶) statlogsUniqueCount ( 'stat_name', $uid, $key1, $key2, ‚Ä¶) statlogsValuetEvent ( 'stat_name', $value, $key1, $key2, ‚Ä¶) $stats = statlogsStatData($params)</code> </pre><br>      ,    ,     ‚Äî  ,  Wathdogs. <br><br>    <strong> ,</strong>    600   1   .       <strong>   </strong> ,     .     ‚Äî  ,     . ,      . <br><br>    ,     <strong>  memcache</strong> ,    .         <strong>stats-daemon</strong>   .         <strong>logs-collectors</strong> ,       ,      . <br><br><img src="https://habrastorage.org/webt/ih/ab/oy/ihaboy4luh5hriorej9seodbx6u.png"><br><br>        logs-collectors. <br><br><img src="https://habrastorage.org/webt/fq/ta/bj/fqtabjgq556wqfdz5_kfq3mj94c.png"><br><br>          stas-daemom ‚Äî   ,      collector.  ,    -        memcache stats-daemon,   ,    . <br><br>  logs-collectors    <strong>meowDB</strong> ‚Äî   ,      . <br><br><img src="https://habrastorage.org/webt/v_/gb/_y/v_gb_ya-9ywkra7xdh5h_qtqsc4.png"><br><br>      ¬´-SQL¬ª  . <br><br><img src="https://habrastorage.org/webt/1q/gw/wp/1qgwwpyj3ewcwuonshvty_zcfhc.png"><br><br><h3>  </h3><br>  2018     ,          -,      ClickHouse.      ClickHouse ‚Äî    ? <br><br><img src="https://habrastorage.org/webt/wg/mz/kl/wgmzklw41x7ilj0-5hbr_kfdif8.png"><br><br>    ,     KittenHouse. <br><br><img src="https://habrastorage.org/webt/kq/s7/uj/kqs7ujzbhnqzt5f8djepldmwxia.png"><br><br>   <strong>     ¬´*House¬ª</strong> ,        ,       UDP.   *House    inserts,  ,   KittenHouse.        ClickHouse,     . <br><br><img src="https://habrastorage.org/webt/ff/k3/th/ffk3thypln9exuhyuhr-nuj9hr4.png"><br><br>   memcache, stats-daemon  logs-collectors    . <br><br><img src="https://habrastorage.org/webt/r4/g3/e9/r4g3e9yakpzbx5gscmgyl6keqsa.png"><br><br>   memcache, stats-daemon  logs-collectors    . <br><br><ul><li>     ,     StatsHouse. </li><li> StatsHouse   KittenHouse UDP-,    SQL-inserts, . </li><li> KittenHouse    ClickHouse. </li><li>     ,      StatsHouse ‚Äî   ClickHouse  SQL. </li></ul><br>    <strong></strong> ,   ,  .    , , ,    .     . <br><br>  <strong>  </strong> .   ,    stats-daemons  logs-collectors,  ClickHouse   ,  ,     . <strong>  ,       </strong> . <br><br><h2>  </h2><br>     PHP.    <strong>git</strong> :  <strong>GitLab</strong>  <strong>TeamCity</strong>  .     -,       ,   ‚Äî  . <br><br>        ,     diff  ‚Äî : , , .     binlog   copyfast,          .     ,  <strong>gossip replication</strong> ,       ,  ‚Äî  ,   .            .      ,       <strong>  </strong> .       . <br><br>     kPHP         <strong>git</strong>   .    <strong> HTTP-</strong> ,      diff ‚Äî     .     ‚Äî    <strong>binlog copyfast</strong> .     ,      .  <strong>  </strong> .  copyfast' ,   binlog   ,     gossip replication     ,    -,      .   <strong>graceful </strong>   . <br><br>   ,     ,   : <br><br><ul><li> git master branch; </li><li>   <strong>.deb</strong> ; </li><li>    binlog copyfast; </li><li>   ; </li><li>     .dep; </li><li> <strong>dpkg -i</strong> ; </li><li> graceful    . </li></ul><br>   ,        <strong>.deb</strong> ,     <strong>dpkg -i</strong>   .    kPHP  ,   ‚Äî dpkg?  .  ‚Äî  . <br><br> <b> :</b> <br><br><ul><li>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">¬´  Vkontakte. ?¬ª</a>    copyfast  gossip. </li><li>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">¬´ VK    CLickHouse    ¬ª</a> . </li><li>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">¬´     ¬ª</a> ,     ,   . </li></ul><br><blockquote>     ,       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">PHP Russia</a>  17          PHP-. ,     ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> (     PHP!) ‚Äî ,      PHP,   . </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr449254/">https://habr.com/ru/post/fr449254/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr449236/index.html">Ok Google: Comment puis-je passer par le captcha?</a></li>
<li><a href="../fr449240/index.html">L'histoire d'un jeune service Daida (abonnement art)</a></li>
<li><a href="../fr449246/index.html">AX200 - Intel Wi-Fi 6</a></li>
<li><a href="../fr449248/index.html">IDE moderne. Certainement D, dans une certaine mesure E, et certainement pas moi</a></li>
<li><a href="../fr449252/index.html">Projets Zombie - fusionnez les donn√©es des utilisateurs m√™me apr√®s sa mort</a></li>
<li><a href="../fr449256/index.html">J'ai lu 80 CV, j'ai des questions</a></li>
<li><a href="../fr449260/index.html">Qu'est-ce que l'apprentissage automatique automatis√© (AutoML)</a></li>
<li><a href="../fr449262/index.html">Derni√®re mise √† niveau IRM - Siebel vers IP17 +</a></li>
<li><a href="../fr449264/index.html">Cr√©ation d'un syst√®me de reporting pour 1C: ERP bas√© sur OLAP et Excel</a></li>
<li><a href="../fr449266/index.html">3 rapports avec RusCrypto: conf√©rences avec exp√©rience</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>